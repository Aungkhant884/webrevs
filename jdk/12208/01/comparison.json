{"files":[{"patch":"@@ -1296,0 +1296,4 @@\n+  \/\/ Vector Slide Instructions\n+  INSN(vslideup_vi,   0b1010111, 0b011, 0b001110);\n+  INSN(vslidedown_vi, 0b1010111, 0b011, 0b001111);\n+\n@@ -1321,0 +1325,10 @@\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorRegister Vs1, VectorMask vm = unmasked) { \\\n+    patch_VArith(op, Vd, funct3, Vs1->raw_encoding(), Vs2, vm, funct6);                            \\\n+  }\n+\n+  \/\/ Vector Register Gather Instructions\n+  INSN(vrgather_vv,     0b1010111, 0b000, 0b001100);\n+\n+#undef INSN\n+\n@@ -1569,0 +1583,10 @@\n+#define INSN(NAME, op, funct3, vm, funct6)                                   \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorRegister Vs1) {     \\\n+    patch_VArith(op, Vd, funct3, Vs1->raw_encoding(), Vs2, vm, funct6);      \\\n+  }\n+\n+  \/\/ Vector Integer Merge Instructions\n+  INSN(vmerge_vvm,  0b1010111, 0b000, 0b0, 0b010111);\n+\n+#undef INSN\n+\n@@ -1614,1 +1638,0 @@\n-#undef patch_VArith\n@@ -1656,3 +1679,3 @@\n-#define INSN(NAME, op, lumop, vm, mop, nf)                                           \\\n-  void NAME(VectorRegister Vd, Register Rs1, uint32_t width = 0, bool mew = false) { \\\n-    guarantee(is_unsigned_imm_in_range(width, 3, 0), \"width is invalid\");            \\\n+#define INSN(NAME, op, width, lumop, vm, mop, mew, nf)                               \\\n+  void NAME(VectorRegister Vd, Register Rs1) {                                       \\\n+    assert(is_unsigned_imm_in_range(width, 3, 0), \"width is invalid\");               \\\n@@ -1663,1 +1686,16 @@\n-  INSN(vl1re8_v, 0b0000111, 0b01000, 0b1, 0b00, g1);\n+  INSN(vl1re8_v,  0b0000111, 0b000, 0b01000, 0b1, 0b00, 0b0, g1);\n+  INSN(vl1re16_v, 0b0000111, 0b101, 0b01000, 0b1, 0b00, 0b0, g1);\n+  INSN(vl1re32_v, 0b0000111, 0b110, 0b01000, 0b1, 0b00, 0b0, g1);\n+  INSN(vl1re64_v, 0b0000111, 0b111, 0b01000, 0b1, 0b00, 0b0, g1);\n+  INSN(vl2re8_v,  0b0000111, 0b000, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vl2re16_v, 0b0000111, 0b101, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vl2re32_v, 0b0000111, 0b110, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vl2re64_v, 0b0000111, 0b111, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vl4re8_v,  0b0000111, 0b000, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vl4re16_v, 0b0000111, 0b101, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vl4re32_v, 0b0000111, 0b110, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vl4re64_v, 0b0000111, 0b111, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vl8re8_v,  0b0000111, 0b000, 0b01000, 0b1, 0b00, 0b0, g8);\n+  INSN(vl8re16_v, 0b0000111, 0b101, 0b01000, 0b1, 0b00, 0b0, g8);\n+  INSN(vl8re32_v, 0b0000111, 0b110, 0b01000, 0b1, 0b00, 0b0, g8);\n+  INSN(vl8re64_v, 0b0000111, 0b111, 0b01000, 0b1, 0b00, 0b0, g8);\n@@ -1674,0 +1712,3 @@\n+  INSN(vs2r_v, 0b0100111, 0b000, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vs4r_v, 0b0100111, 0b000, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vs8r_v, 0b0100111, 0b000, 0b01000, 0b1, 0b00, 0b0, g8);\n@@ -1745,0 +1786,29 @@\n+\/\/ ====================================\n+\/\/ RISC-V Vector Crypto Extension\n+\/\/ ====================================\n+\n+#define INSN(NAME, op, funct3, Vs1, funct6)                                    \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorMask vm = unmasked) { \\\n+    patch_VArith(op, Vd, funct3, Vs1, Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Bit-manipulation used in Cryptography (Zvkb) Extension\n+  INSN(vbrev8_v, 0b1010111, 0b010, 0b01000, 0b010010);\n+  INSN(vrev8_v,  0b1010111, 0b010, 0b01001, 0b010010);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, vm, funct6)                                   \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorRegister Vs1) {     \\\n+    patch_VArith(op, Vd, funct3, Vs1->raw_encoding(), Vs2, vm, funct6);      \\\n+  }\n+\n+  \/\/ Vector SHA-2 Secure Hash (Zvknh[ab]) Extension\n+  INSN(vsha2ms_vv,  0b1110111, 0b010, 0b1, 0b101101);\n+  INSN(vsha2ch_vv,  0b1110111, 0b010, 0b1, 0b101110);\n+  INSN(vsha2cl_vv,  0b1110111, 0b010, 0b1, 0b101111);\n+\n+#undef INSN\n+\n+#undef patch_VArith\n+\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":75,"deletions":5,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -113,0 +113,3 @@\n+  product(bool, UseZvkb, false, EXPERIMENTAL, \"Use Zvkb instructions\")           \\\n+  product(bool, UseZvknha, false, EXPERIMENTAL, \"Use Zvknha instructions\")       \\\n+  product(bool, UseZvknhb, false, EXPERIMENTAL, \"Use Zvknhb instructions\")       \\\n","filename":"src\/hotspot\/cpu\/riscv\/globals_riscv.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1276,0 +1276,12 @@\n+  inline void vnot_v(VectorRegister Vd, VectorRegister Vs, VectorMask vm = unmasked) {\n+    vxor_vi(Vd, Vs, -1, vm);\n+  }\n+\n+  inline void vmsltu_vi(VectorRegister Vd, VectorRegister Vs2, int32_t imm, VectorMask vm = unmasked) {\n+    vmsleu_vi(Vd, Vs2, imm-1, vm);\n+  }\n+\n+  inline void vmsgeu_vi(VectorRegister Vd, VectorRegister Vs2, int32_t imm, VectorMask vm = unmasked) {\n+    vmsgtu_vi(Vd, Vs2, imm-1, vm);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -3701,0 +3701,435 @@\n+\n+  \/\/ Arguments:\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - byte[]  source+offset\n+  \/\/   c_rarg1   - int[]   SHA.state\n+  \/\/   c_rarg2   - int     offset\n+  \/\/   c_rarg3   - int     limit\n+  \/\/\n+  address generate_sha256_implCompress(bool multi_block, const char *name) {\n+    static const uint32_t round_consts[64] = {\n+      0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5,\n+      0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,\n+      0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,\n+      0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,\n+      0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc,\n+      0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,\n+      0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7,\n+      0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,\n+      0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,\n+      0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,\n+      0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3,\n+      0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,\n+      0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5,\n+      0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,\n+      0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,\n+      0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2,\n+    };\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+    address start = __ pc();\n+\n+    Register buf   = c_rarg0;\n+    Register state = c_rarg1;\n+    Register ofs   = c_rarg2;\n+    Register limit = c_rarg3;\n+    Register consts = t0;\n+\n+    Label multi_block_loop;\n+\n+    __ enter();\n+\n+    \/\/ Register use in this function:\n+    \/\/\n+    \/\/ VECTORS\n+    \/\/  v10 - v13 (512-bits \/ 4*128 bits \/ 4*4*32 bits), hold the message\n+    \/\/             schedule words (Wt). They start with the message block\n+    \/\/             content (W0 to W15), then further words in the message\n+    \/\/             schedule generated via vsha2ms from previous Wt.\n+    \/\/   Initially:\n+    \/\/     v10 = W[  3:0] = { W3,  W2,  W1,  W0}\n+    \/\/     v11 = W[  7:4] = { W7,  W6,  W5,  W4}\n+    \/\/     v12 = W[ 11:8] = {W11, W10,  W9,  W8}\n+    \/\/     v13 = W[15:12] = {W15, W14, W13, W12}\n+    \/\/\n+    \/\/  v16 - v17 hold the working state variables (a, b, ..., h)\n+    \/\/    v16 = {a[t],b[t],e[t],f[t]}\n+    \/\/    v17 = {c[t],d[t],g[t],h[t]}\n+    \/\/   Initially:\n+    \/\/    v16 = {H5i-1, H4i-1, H1i-1 , H0i-1}\n+    \/\/    v17 = {H7i-i, H6i-1, H3i-1 , H2i-1}\n+    \/\/\n+    \/\/  v0 = masks for vrgather\/vmerge. Single value during the 16 rounds.\n+    \/\/\n+    \/\/  v14 = temporary, Wt+Kt\n+    \/\/  v15 = temporary, Kt\n+    \/\/\n+    \/\/  v18\/v19 = temporaries, in the epilogue, to re-arrange\n+    \/\/            and byte-swap v16\/v17\n+    \/\/\n+    \/\/  v26\/v27 = hold the initial values of the hash, byte-swapped.\n+    \/\/\n+    \/\/  v30\/v31 = used to generate masks, vrgather indices.\n+    \/\/\n+    \/\/ During most of the function the vector state is configured so that each\n+    \/\/ vector is interpreted as containing four 32 bits (e32) elements (128 bits).\n+\n+    \/\/ Set vectors as 4 * 32 bits\n+    \/\/\n+    \/\/ e32: vector of 32b\/4B elements\n+    \/\/ m1: LMUL=1\n+    \/\/ ta: tail agnostic (don't care about those lanes)\n+    \/\/ ma: mask agnostic (don't care about those lanes)\n+    \/\/ x0 is not written, we known the number of vector elements, 8.\n+    __ vsetivli(x0, 4, Assembler::e32, Assembler::m1, Assembler::ma, Assembler::ta);\n+\n+    \/\/ Load H[0..8] to produce\n+    \/\/  v16 = {a,b,e,f}\n+    \/\/  v17 = {c,d,g,h}\n+    __ vle32_v(v16, state);                          \/\/ v16 = {d,c,b,a}\n+    __ addi(state, state, 16);\n+    __ vle32_v(v17, state);                          \/\/ v17 = {h,g,f,e}\n+\n+    __ vid_v(v30);                                   \/\/ v30 = {3,2,1,0}\n+    __ vxor_vi(v30, v30, 0x3);                       \/\/ v30 = {0,1,2,3}\n+    __ vrgather_vv(v26, v16, v30);                   \/\/ v26 = {a,b,c,d}\n+    __ vrgather_vv(v27, v17, v30);                   \/\/ v27 = {e,f,g,h}\n+    __ vmsgeu_vi(v0, v30, 2);                        \/\/ v0  = {f,f,t,t}\n+    \/\/ Copy elements [3..2] of v26 ({d,c}) into elements [3..2] of v17.\n+    __ vslideup_vi(v17, v26, 2);                     \/\/ v17 = {c,d,_,_}\n+    \/\/ Merge elements [1..0] of v27 ({g,h}) into elements [1..0] of v17\n+    __ vmerge_vvm(v17, v17, v27);                    \/\/ v17 = {c,d,g,h}\n+    \/\/ Copy elements [1..0] of v27 ({f,e}) into elements [1..0] of v16.\n+    __ vslidedown_vi(v16, v27, 2);                   \/\/ v16 = {_,_,e,f}\n+    \/\/ Merge elements [3..2] of v26 ({a,b}) into elements [3..2] of v16\n+    __ vmerge_vvm(v16, v26, v16);                    \/\/ v16 = {a,b,e,f}\n+\n+    __ bind(multi_block_loop);\n+\n+    \/\/ Capture the initial H values in v26 and v27 to allow for computing\n+    \/\/ the resulting H', since H' = H+{a',b',c',...,h'}.\n+    __ vmv_v_v(v26, v16);\n+    __ vmv_v_v(v27, v17);\n+\n+    \/\/ Load the 512-bits of the message block in v10-v13 and perform\n+    \/\/ an endian swap on each 4 bytes element.\n+    \/\/\n+    \/\/ If Zvkb is not implemented, one can use vrgather with the right index\n+    \/\/ sequence. It requires loading in separate registers since the destination\n+    \/\/ of vrgather cannot overlap the source.\n+    \/\/    \/\/ We generate the lane (byte) index sequence\n+    \/\/    \/\/    v24 = [3 2 1 0   7 6 5 4  11 10 9 8   15 14 13 12]\n+    \/\/    \/\/ <https:\/\/oeis.org\/a104444> gives us \"N ^ 3\" as a nice formula to generate\n+    \/\/    \/\/ this sequence. 'vid' gives us the N.\n+    \/\/    \/\/\n+    \/\/    \/\/ We switch the vector type to SEW=8 temporarily.\n+    \/\/    vsetivli x0, 16, e8, m1, ta, ma\n+    \/\/    vid.v v24\n+    \/\/    vxor.vi v24, v24, 0x3\n+    \/\/    \/\/ Byteswap the bytes in each word of the text.\n+    \/\/    vrgather.vv v10, v20, v24\n+    \/\/    vrgather.vv v11, v21, v24\n+    \/\/    vrgather.vv v12, v22, v24\n+    \/\/    vrgather.vv v13, v23, v24\n+    \/\/    \/\/ Switch back to SEW=32\n+    \/\/    vsetivli x0, 4, e32, m1, ta, ma\n+    __ vle32_v(v10, buf);\n+    __ vrev8_v(v10, v10);\n+    __ addi(buf, buf, 16);\n+    __ vle32_v(v11, buf);\n+    __ vrev8_v(v11, v11);\n+    __ addi(buf, buf, 16);\n+    __ vle32_v(v12, buf);\n+    __ vrev8_v(v12, v12);\n+    __ addi(buf, buf, 16);\n+    __ vle32_v(v13, buf);\n+    __ vrev8_v(v13, v13);\n+\n+    \/\/ Set v0 up for the vmerge that replaces the first word (idx==0)\n+    __ vid_v(v0);\n+    __ vmseq_vi(v0, v0, 0x0);  \/\/ v0.mask[i] = (i == 0 ? 1 : 0)\n+\n+    __ la(consts, ExternalAddress((address)round_consts));\n+\n+    \/\/ Overview of the logic in each \"quad round\".\n+    \/\/\n+    \/\/ The code below repeats 16 times the logic implementing four rounds\n+    \/\/ of the SHA-256 core loop as documented by NIST. 16 \"quad rounds\"\n+    \/\/ to implementing the 64 single rounds.\n+    \/\/\n+    \/\/    \/\/ Load four word (u32) constants (K[t+3], K[t+2], K[t+1], K[t+0])\n+    \/\/    \/\/ Output:\n+    \/\/    \/\/   v15 = {K[t+3], K[t+2], K[t+1], K[t+0]}\n+    \/\/    vl1re32.v v15, ofs\n+    \/\/\n+    \/\/    \/\/ Increment word contant address by stride (16 bytes, 4*4B, 128b)\n+    \/\/    addi ofs, ofs, 16\n+    \/\/\n+    \/\/    \/\/ Add constants to message schedule words:\n+    \/\/    \/\/  Input\n+    \/\/    \/\/    v15 = {K[t+3], K[t+2], K[t+1], K[t+0]}\n+    \/\/    \/\/    v10 = {W[t+3], W[t+2], W[t+1], W[t+0]}; \/\/ Vt0 = W[3:0];\n+    \/\/    \/\/  Output\n+    \/\/    \/\/    v14 = {W[t+3]+K[t+3], W[t+2]+K[t+2], W[t+1]+K[t+1], W[t+0]+K[t+0]}\n+    \/\/    vadd.vv v14, v15, v10\n+    \/\/\n+    \/\/    \/\/  2 rounds of working variables updates.\n+    \/\/    \/\/     v17[t+4] <- v17[t], v16[t], v14[t]\n+    \/\/    \/\/  Input:\n+    \/\/    \/\/    v17 = {c[t],d[t],g[t],h[t]}   \" = v17[t] \"\n+    \/\/    \/\/    v16 = {a[t],b[t],e[t],f[t]}\n+    \/\/    \/\/    v14 = {W[t+3]+K[t+3], W[t+2]+K[t+2], W[t+1]+K[t+1], W[t+0]+K[t+0]}\n+    \/\/    \/\/  Output:\n+    \/\/    \/\/    v17 = {f[t+2],e[t+2],b[t+2],a[t+2]}  \" = v16[t+2] \"\n+    \/\/    \/\/        = {h[t+4],g[t+4],d[t+4],c[t+4]}  \" = v17[t+4] \"\n+    \/\/    vsha2cl.vv v17, v16, v14\n+    \/\/\n+    \/\/    \/\/  2 rounds of working variables updates.\n+    \/\/    \/\/     v16[t+4] <- v16[t], v16[t+2], v14[t]\n+    \/\/    \/\/  Input\n+    \/\/    \/\/   v16 = {a[t],b[t],e[t],f[t]}       \" = v16[t] \"\n+    \/\/    \/\/       = {h[t+2],g[t+2],d[t+2],c[t+2]}   \" = v17[t+2] \"\n+    \/\/    \/\/   v17 = {f[t+2],e[t+2],b[t+2],a[t+2]}   \" = v16[t+2] \"\n+    \/\/    \/\/   v14 = {W[t+3]+K[t+3], W[t+2]+K[t+2], W[t+1]+K[t+1], W[t+0]+K[t+0]}\n+    \/\/    \/\/  Output:\n+    \/\/    \/\/   v16 = {f[t+4],e[t+4],b[t+4],a[t+4]}   \" = v16[t+4] \"\n+    \/\/    vsha2ch.vv v16, v17, v14\n+    \/\/\n+    \/\/    \/\/ Combine 2QW into 1QW\n+    \/\/    \/\/\n+    \/\/    \/\/ To generate the next 4 words, \"new_v10\"\/\"v14\" from v10-v13, vsha2ms needs\n+    \/\/    \/\/     v10[0..3], v11[0], v12[1..3], v13[0, 2..3]\n+    \/\/    \/\/ and it can only take 3 vectors as inputs. Hence we need to combine\n+    \/\/    \/\/ v11[0] and v12[1..3] in a single vector.\n+    \/\/    \/\/\n+    \/\/    \/\/ vmerge Vt4, Vt1, Vt2, V0\n+    \/\/    \/\/ Input\n+    \/\/    \/\/  V0 = mask \/\/ first word from v12, 1..3 words from v11\n+    \/\/    \/\/  V12 = {Wt-8, Wt-7, Wt-6, Wt-5}\n+    \/\/    \/\/  V11 = {Wt-12, Wt-11, Wt-10, Wt-9}\n+    \/\/    \/\/ Output\n+    \/\/    \/\/  Vt4 = {Wt-12, Wt-7, Wt-6, Wt-5}\n+    \/\/    vmerge.vvm v14, v12, v11, v0\n+    \/\/\n+    \/\/    \/\/ Generate next Four Message Schedule Words (hence allowing for 4 more rounds)\n+    \/\/    \/\/ Input\n+    \/\/    \/\/  V10 = {W[t+ 3], W[t+ 2], W[t+ 1], W[t+ 0]}     W[ 3: 0]\n+    \/\/    \/\/  V13 = {W[t+15], W[t+14], W[t+13], W[t+12]}     W[15:12]\n+    \/\/    \/\/  V14 = {W[t+11], W[t+10], W[t+ 9], W[t+ 4]}     W[11: 9,4]\n+    \/\/    \/\/ Output (next four message schedule words)\n+    \/\/    \/\/  v10 = {W[t+19],  W[t+18],  W[t+17],  W[t+16]}  W[19:16]\n+    \/\/    vsha2ms.vv v10, v14, v13\n+    \/\/\n+    \/\/ BEFORE\n+    \/\/  v10 - v13 hold the message schedule words (initially the block words)\n+    \/\/    v10 = W[ 3: 0]   \"oldest\"\n+    \/\/    v11 = W[ 7: 4]\n+    \/\/    v12 = W[11: 8]\n+    \/\/    v13 = W[15:12]   \"newest\"\n+    \/\/\n+    \/\/  vt6 - vt7 hold the working state variables\n+    \/\/    v16 = {a[t],b[t],e[t],f[t]}   \/\/ initially {H5,H4,H1,H0}\n+    \/\/    v17 = {c[t],d[t],g[t],h[t]}   \/\/ initially {H7,H6,H3,H2}\n+    \/\/\n+    \/\/ AFTER\n+    \/\/  v10 - v13 hold the message schedule words (initially the block words)\n+    \/\/    v11 = W[ 7: 4]   \"oldest\"\n+    \/\/    v12 = W[11: 8]\n+    \/\/    v13 = W[15:12]\n+    \/\/    v10 = W[19:16]   \"newest\"\n+    \/\/\n+    \/\/  v16 and v17 hold the working state variables\n+    \/\/    v16 = {a[t+4],b[t+4],e[t+4],f[t+4]}\n+    \/\/    v17 = {c[t+4],d[t+4],g[t+4],h[t+4]}\n+    \/\/\n+    \/\/  The group of vectors v10,v11,v12,v13 is \"rotated\" by one in each quad-round,\n+    \/\/  hence the uses of those vectors rotate in each round, and we get back to the\n+    \/\/  initial configuration every 4 quad-rounds. We could avoid those changes at\n+    \/\/  the cost of moving those vectors at the end of each quad-rounds.\n+\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 0 (+0, v10->v11->v12->v13)\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v10);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    __ vmerge_vvm(v14, v12, v11);\n+    __ vsha2ms_vv(v10, v14, v13); \/\/ Generate W[19:16]\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 1 (+1, v11->v12->v13->v10)\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v11);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    __ vmerge_vvm(v14, v13, v12);\n+    __ vsha2ms_vv(v11, v14, v10); \/\/ Generate W[23:20]\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 2 (+2, v12->v13->v10->v11)\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v12);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    __ vmerge_vvm(v14, v10, v13);\n+    __ vsha2ms_vv(v12, v14, v11); \/\/ Generate W[27:24]\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 3 (+3, v13->v10->v11->v12)\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v13);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    __ vmerge_vvm(v14, v11, v10);\n+    __ vsha2ms_vv(v13, v14, v12); \/\/ Generate W[31:28]\n+\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 4 (+0, v10->v11->v12->v13)\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v10);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    __ vmerge_vvm(v14, v12, v11);\n+    __ vsha2ms_vv(v10, v14, v13); \/\/ Generate W[35:32]\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 5 (+1, v11->v12->v13->v10)\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v11);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    __ vmerge_vvm(v14, v13, v12);\n+    __ vsha2ms_vv(v11, v14, v10); \/\/ Generate W[39:36]\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 6 (+2, v12->v13->v10->v11)\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v12);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    __ vmerge_vvm(v14, v10, v13);\n+    __ vsha2ms_vv(v12, v14, v11); \/\/ Generate W[43:40]\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 7 (+3, v13->v10->v11->v12)\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v13);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    __ vmerge_vvm(v14, v11, v10);\n+    __ vsha2ms_vv(v13, v14, v12); \/\/ Generate W[47:44]\n+\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 8 (+0, v10->v11->v12->v13)\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v10);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    __ vmerge_vvm(v14, v12, v11);\n+    __ vsha2ms_vv(v10, v14, v13); \/\/ Generate W[51:48]\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 9 (+1, v11->v12->v13->v10)\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v11);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    __ vmerge_vvm(v14, v13, v12);\n+    __ vsha2ms_vv(v11, v14, v10); \/\/ Generate W[55:52]\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 10 (+2, v12->v13->v10->v11)\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v12);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    __ vmerge_vvm(v14, v10, v13);\n+    __ vsha2ms_vv(v12, v14, v11); \/\/ Generate W[59:56]\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 11 (+3, v13->v10->v11->v12)\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v13);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    __ vmerge_vvm(v14, v11, v10);\n+    __ vsha2ms_vv(v13, v14, v12); \/\/ Generate W[63:60]\n+\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 12 (+0, v10->v11->v12->v13)\n+    \/\/ Note that we stop generating new message schedule words (Wt, v10-13)\n+    \/\/ as we already generated all the words we end up consuming (i.e., W[63:60]).\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v10);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 13 (+1, v11->v12->v13->v10)\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v11);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 14 (+2, v12->v13->v10->v11)\n+    __ vl1re32_v(v15, consts);\n+    __ addi(consts, consts, 16);\n+    __ vadd_vv(v14, v15, v12);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Quad-round 15 (+3, v13->v10->v11->v12)\n+    __ vl1re32_v(v15, consts);\n+    \/\/ No consts increment needed\n+    __ vadd_vv(v14, v15, v13);\n+    __ vsha2cl_vv(v17, v16, v14);\n+    __ vsha2ch_vv(v16, v17, v14);\n+\n+    \/\/--------------------------------------------------------------------------------\n+    \/\/ Compute the updated hash value H'\n+    \/\/   H' = H + {h',g',...,b',a'}\n+    \/\/      = {h,g,...,b,a} + {h',g',...,b',a'}\n+    \/\/      = {h+h',g+g',...,b+b',a+a'}\n+\n+    __ vadd_vv(v16, v26, v16);\n+    __ vadd_vv(v17, v27, v17);\n+\n+    if (multi_block) {\n+      __ add(ofs, ofs, 64);\n+      __ ble(ofs, limit, multi_block_loop);\n+      __ mv(c_rarg0, ofs); \/\/ return ofs\n+    }\n+\n+    \/\/ Store H[0..8] = {a,b,c,d,e,f,g,h} from\n+    \/\/  v16 = {f,e,b,a}\n+    \/\/  v17 = {h,g,d,c}\n+    __ vid_v(v30);                                   \/\/ v30 = {3,2,1,0}\n+    __ vxor_vi(v30, v30, 0x3);                       \/\/ v30 = {0,1,2,3}\n+    __ vrgather_vv(v26, v16, v30);                   \/\/ v26 = {f,e,b,a}\n+    __ vrgather_vv(v27, v17, v30);                   \/\/ v27 = {h,g,d,c}\n+    __ vmsgeu_vi(v0, v30, 2);                        \/\/ v0  = {f,f,t,t}\n+    \/\/ Copy elements [3..2] of v26 ({f,e}) into elements [1..0] of v17.\n+    __ vslidedown_vi(v17, v26, 2);                   \/\/ v17 = {_,_,f,e}\n+    \/\/ Merge elements [3..2] of v27 ({g,h}) into elements [3..2] of v17\n+    __ vmerge_vvm(v17, v27, v17);                    \/\/ v17 = {h,g,f,e}\n+    \/\/ Copy elements [1..0] of v27 ({c,d}) into elements [3..2] of v16.\n+    __ vslideup_vi(v16, v27, 2);                     \/\/ v16 = {d,c,_,_}\n+    \/\/ Merge elements [1..0] of v26 ({a,b}) into elements [1..0] of v16\n+    __ vmerge_vvm(v16, v16, v26);                    \/\/ v16 = {d,c,b,a}\n+\n+    \/\/ Save the hash\n+    __ vse32_v(v17, state);\n+    __ addi(state, state, -16);\n+    __ vse32_v(v16, state);\n+\n+    __ leave();\n+    __ ret();\n+\n+    return start;\n+  }\n+\n@@ -4105,0 +4540,5 @@\n+    if (UseSHA256Intrinsics) {\n+      StubRoutines::_sha256_implCompress   = generate_sha256_implCompress(false, \"sha256_implCompress\");\n+      StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true,  \"sha256_implCompressMB\");\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":440,"deletions":0,"binary":false,"changes":440,"status":"modified"},{"patch":"@@ -134,1 +134,5 @@\n-  if (UseSHA256Intrinsics) {\n+  if (UseZvknha && UseZvkb) {\n+    if (FLAG_IS_DEFAULT(UseSHA256Intrinsics)) {\n+      FLAG_SET_DEFAULT(UseSHA256Intrinsics, true);\n+    }\n+  } else if (UseSHA256Intrinsics) {\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"}]}