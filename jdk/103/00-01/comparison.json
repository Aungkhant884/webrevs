{"files":[{"patch":"@@ -260,1 +260,1 @@\n-  ZHeapParIterator _par_iter;\n+  ZHeapIterator _iter;\n@@ -263,1 +263,1 @@\n-      _thread_num(thread_num), _heap(heap), _par_iter(_thread_num) {\n+      _thread_num(thread_num), _heap(heap), _iter(_thread_num) {\n@@ -265,1 +265,1 @@\n-    _heap->process_roots_for_par_iterate(&_par_iter, true \/* visit_weaks *\/);\n+    _heap->process_roots_for_par_iterate(&_iter, true \/* visit_weaks *\/);\n@@ -270,1 +270,1 @@\n-    _heap->par_references_iterate(cl, &_par_iter, worker_id, true \/* visit_weaks*\/);\n+    _heap->par_references_iterate(cl, &_iter, worker_id, true \/* visit_weaks*\/);\n","filename":"src\/hotspot\/share\/gc\/z\/zCollectedHeap.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/z\/zHeapIterator.hpp\"\n@@ -444,1 +443,1 @@\n-void ZHeap::process_roots_for_par_iterate(ZHeapParIterator* iter, bool visit_weaks) {\n+void ZHeap::process_roots_for_par_iterate(ZHeapIterator* iter, bool visit_weaks) {\n@@ -449,1 +448,1 @@\n-                                   ZHeapParIterator* iter,\n+                                   ZHeapIterator* iter,\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"gc\/z\/zHeapParIterator.hpp\"\n+#include \"gc\/z\/zHeapIterator.hpp\"\n@@ -145,2 +145,2 @@\n-  void process_roots_for_par_iterate(ZHeapParIterator* iter, bool visit_weaks);\n-  void par_references_iterate(ObjectClosure* cl, ZHeapParIterator* iter,\n+  void process_roots_for_par_iterate(ZHeapIterator* iter, bool visit_weaks);\n+  void par_references_iterate(ObjectClosure* cl, ZHeapIterator* iter,\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -55,0 +55,4 @@\n+\n+  inline bool par_try_set_bit(size_t index) {\n+    return _map.par_set_bit(index);\n+  }\n@@ -88,1 +92,1 @@\n-template <bool VisitReferents>\n+template <bool VisitReferents, bool ParallelIter>\n@@ -93,0 +97,1 @@\n+  ZHeapIterTaskQueue*     _queue;\n@@ -103,1 +108,1 @@\n-  ZHeapIteratorOopClosure(ZHeapIterator* iter, oop base) :\n+  ZHeapIteratorOopClosure(ZHeapIterator* iter, oop base, ZHeapIterTaskQueue* q = NULL) :\n@@ -106,1 +111,2 @@\n-      _base(base) {}\n+      _base(base),\n+      _queue(q) {}\n@@ -114,1 +120,5 @@\n-    _iter->push(obj);\n+    if (ParallelIter == false) {\n+      _iter->push(obj);\n+    } else {\n+      _iter->par_enqueue(obj, _queue);\n+    }\n@@ -128,1 +138,1 @@\n-ZHeapIterator::ZHeapIterator() :\n+ZHeapIterator::ZHeapIterator(uint num_workers) :\n@@ -130,1 +140,14 @@\n-    _visit_map(ZAddressOffsetMax) {}\n+    _visit_map(ZAddressOffsetMax),\n+    _num_workers(num_workers),\n+    _map_lock(),\n+    _task_queues(NULL) {\n+  if (_num_workers > 1) {\n+    \/\/ prepare process queue.\n+    _task_queues = new ZHeapIterTaskQueueSet((int) _num_workers);\n+    for (uint i = 0; i < _num_workers; i++) {\n+      ZHeapIterTaskQueue* q = new ZHeapIterTaskQueue();\n+      q->initialize();\n+      _task_queues->register_queue(i, q);\n+    }\n+  }\n+}\n@@ -137,0 +160,12 @@\n+  \/\/ reclaim task queues\n+  if (_task_queues != NULL) {\n+    for (uint i = 0; i < _num_workers; i++) {\n+      ZHeapIterTaskQueue* q = _task_queues->queue(i);\n+      if (q != NULL) {\n+        delete q;\n+        q = NULL;\n+      }\n+    }\n+    delete _task_queues;\n+    _task_queues = NULL;\n+  }\n@@ -155,2 +190,11 @@\n-    map = new ZHeapIteratorBitMap(object_index_max());\n-    _visit_map.put(offset, map);\n+    if (_num_workers > 1) {\n+      \/\/ Parallel iterate, holding lock to update _visit_map\n+      ZLocker<ZLock> locker(&_map_lock);\n+      if (map == NULL) {\n+        map = new ZHeapIteratorBitMap(object_index_max());\n+        _visit_map.put(offset, map);\n+      }\n+    } else {\n+      map = new ZHeapIteratorBitMap(object_index_max());\n+      _visit_map.put(offset, map);\n+    }\n@@ -158,1 +202,0 @@\n-\n@@ -162,0 +205,1 @@\n+\/\/ push objects in to _visit_stack, used by RootOopClosure.\n@@ -186,3 +230,3 @@\n-template <bool VisitReferents>\n-void ZHeapIterator::push_fields(oop obj) {\n-  ZHeapIteratorOopClosure<VisitReferents> cl(this, obj);\n+template <bool VisitReferents, bool ParallelIter>\n+void ZHeapIterator::push_fields(oop obj, ZHeapIterTaskQueue* queue) {\n+  ZHeapIteratorOopClosure<VisitReferents, ParallelIter> cl(this, obj, queue);\n@@ -216,0 +260,1 @@\n+\/\/ Used only in serial iteration\n@@ -223,0 +268,54 @@\n+\n+\/\/ Parallel iteration support\n+void ZHeapIterator::enqueue_roots(bool visit_weaks) {\n+  ZStatTimerDisable disable;\n+  \/\/ Push roots to visit\n+  push_roots<ZRootsIterator,                     false \/* Concurrent *\/, false \/* Weak *\/>();\n+  push_roots<ZConcurrentRootsIteratorClaimOther, true  \/* Concurrent *\/, false \/* Weak *\/>();\n+  if (visit_weaks) {\n+    push_roots<ZWeakRootsIterator,           false \/* Concurrent *\/, true  \/* Weak *\/>();\n+    push_roots<ZConcurrentWeakRootsIterator, true  \/* Concurrent *\/, true  \/* Weak *\/>();\n+  }\n+  \/\/ Divide roots into thread queue.\n+  size_t roots_num = _visit_stack.size();\n+  for (uint i = 0; i < roots_num; i++) {\n+    uint worker_id = i % _num_workers;\n+    oop obj = _visit_stack.pop();\n+    _task_queues->queue(worker_id)->push(obj);\n+  }\n+}\n+\n+void ZHeapIterator::drain_queue(ObjectClosure* cl, bool visit_weaks, uint worker_id) {\n+  ZStatTimerDisable disable;\n+  ZHeapIterTaskQueue* q = _task_queues->queue(worker_id);\n+  assert(q != NULL, \"Heap iteration task queue must not NULL\");\n+  \/\/ Drain stack\n+  oop obj;\n+  while (q->pop_overflow(obj) || q->pop_local(obj) || _task_queues->steal(worker_id, obj)) {\n+    \/\/ Visit object\n+    cl->do_object(obj);\n+    \/\/ Push fields to visit\n+    if (visit_weaks = true) {\n+      push_fields<true, true \/* ParallelIter *\/>(obj, q);\n+    } else {\n+      push_fields<false, true \/* ParallelIter *\/>(obj, q);\n+    }\n+  }\n+}\n+\n+void ZHeapIterator::par_enqueue(oop obj, ZHeapIterTaskQueue* q) {\n+   if (obj == NULL) {\n+    \/\/ Ignore\n+    return;\n+  }\n+\n+  ZHeapIteratorBitMap* const map = object_map(obj);\n+  const size_t index = object_index(obj);\n+  if (!map->par_try_set_bit(index)) {\n+    \/\/ Already pushed\n+    return;\n+  }\n+\n+  \/\/ Push\n+  q->push(obj);\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zHeapIterator.cpp","additions":111,"deletions":12,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/taskqueue.inline.hpp\"\n@@ -28,0 +29,1 @@\n+#include \"gc\/z\/zLock.inline.hpp\"\n@@ -33,0 +35,3 @@\n+\/\/ Queue and QueueSet for parallel iteration\n+typedef OverflowTaskQueue<oop, mtGC>                     ZHeapIterTaskQueue;\n+typedef GenericTaskQueueSet<ZHeapIterTaskQueue, mtGC>    ZHeapIterTaskQueueSet;\n@@ -36,1 +41,1 @@\n-  template<bool VisitReferents> friend class ZHeapIteratorOopClosure;\n+  template<bool VisitReferents, bool ParallelIter> friend class ZHeapIteratorOopClosure;\n@@ -42,1 +47,2 @@\n-\n+  \/\/ For parallel iteration, _visit_stack only contains roots.\n+  \/\/ For serial iteration, _visit_stack contains all references reached.\n@@ -45,0 +51,4 @@\n+  \/\/ For parallel iteration\n+  uint            _num_workers;\n+  ZLock           _map_lock;\n+  ZHeapIterTaskQueueSet* _task_queues;\n@@ -50,1 +60,3 @@\n-  template <bool VisitReferents> void push_fields(oop obj);\n+  \/\/ push_fields is different for serial iterate and parallel iterate.\n+  template <bool VisitReferents, bool ParallelIter = false>\n+  void push_fields(oop obj, ZHeapIterTaskQueue* queue = NULL);\n@@ -54,1 +66,1 @@\n-  ZHeapIterator();\n+  ZHeapIterator(uint num_workers = 0);\n@@ -58,0 +70,4 @@\n+  \/\/ For parallel iteration\n+  void par_enqueue(oop obj, ZHeapIterTaskQueue* q);\n+  void enqueue_roots(bool visit_weaks);\n+  void drain_queue(ObjectClosure* cl, bool visit_weaks, uint worker_id);\n","filename":"src\/hotspot\/share\/gc\/z\/zHeapIterator.hpp","additions":20,"deletions":4,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -1,274 +0,0 @@\n-\/*\n- * Copyright (C) 2020 THL A29 Limited, a Tencent company. All rights reserved.\n- * DO NOT ALTER OR REMOVE NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify\n- * it under the terms of the GNU General Public License version 2 as\n- * published by the Free Software Foundation. THL A29 Limited designates\n- * this particular file as subject to the \"Classpath\" exception as provided\n- * by Oracle in the LICENSE file that accompanied this code.\n- *\n- * This code is distributed in the hope that it will be useful,\n- * but WITHOUT ANY WARRANTY; without even the implied warranty of\n- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n- * GNU General Public License version 2 for more details.\n- *\n- * You should have received a copy of the GNU General Public License along\n- * with this program; if not, write to the Free Software Foundation, Inc.,\n- * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"classfile\/classLoaderData.hpp\"\n-#include \"classfile\/classLoaderDataGraph.hpp\"\n-#include \"gc\/shared\/taskqueue.inline.hpp\"\n-#include \"gc\/z\/zAddress.inline.hpp\"\n-#include \"gc\/z\/zBarrier.inline.hpp\"\n-#include \"gc\/z\/zGlobals.hpp\"\n-#include \"gc\/z\/zGranuleMap.inline.hpp\"\n-#include \"gc\/z\/zHeapParIterator.hpp\"\n-#include \"gc\/z\/zOop.inline.hpp\"\n-#include \"gc\/z\/zRootsIterator.hpp\"\n-#include \"gc\/z\/zStat.hpp\"\n-#include \"memory\/iterator.inline.hpp\"\n-#include \"utilities\/bitMap.inline.hpp\"\n-#include \"utilities\/stack.inline.hpp\"\n-\n-class ZHeapParIteratorBitMap : public CHeapObj<mtGC> {\n-private:\n-  CHeapBitMap _map;\n-\n-public:\n-  ZHeapParIteratorBitMap(size_t size_in_bits) :\n-      _map(size_in_bits) {}\n-\n-  bool try_set_bit(size_t index) {\n-    if (_map.at(index)) {\n-      return false;\n-    }\n-\n-    _map.set_bit(index);\n-    return true;\n-  }\n-\n-  inline bool par_try_set_bit(size_t index) {\n-    return _map.par_set_bit(index);\n-  }\n-};\n-\n-template <bool Concurrent, bool Weak>\n-class ZHeapParIteratorRootOopClosure : public ZRootsIteratorClosure {\n-private:\n-  ZHeapParIterator* const _iter;\n-\n-  oop load_oop(oop* p) {\n-    if (Weak) {\n-      return NativeAccess<AS_NO_KEEPALIVE | ON_PHANTOM_OOP_REF>::oop_load(p);\n-    }\n-\n-    if (Concurrent) {\n-      return NativeAccess<AS_NO_KEEPALIVE>::oop_load(p);\n-    }\n-\n-    return RawAccess<>::oop_load(p);\n-  }\n-\n-public:\n-  ZHeapParIteratorRootOopClosure(ZHeapParIterator* iter) :\n-      _iter(iter) {}\n-\n-  virtual void do_oop(oop* p) {\n-    const oop obj = load_oop(p);\n-    _iter->push(obj);\n-  }\n-\n-  virtual void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n-  }\n-};\n-\n-template <bool VisitReferents>\n-class ZHeapParIteratorOopClosure : public ClaimMetadataVisitingOopIterateClosure {\n-private:\n-  ZHeapParIterator*       _iter;\n-  const oop               _base;\n-  ZHeapIterTaskQueue*     _queue;\n-\n-  oop load_oop(oop* p) {\n-    if (VisitReferents) {\n-      return HeapAccess<AS_NO_KEEPALIVE | ON_UNKNOWN_OOP_REF>::oop_load_at(_base, _base->field_offset(p));\n-    }\n-\n-    return HeapAccess<AS_NO_KEEPALIVE>::oop_load(p);\n-  }\n-\n-public:\n-  ZHeapParIteratorOopClosure(ZHeapParIterator* iter, oop base, ZHeapIterTaskQueue* q) :\n-      ClaimMetadataVisitingOopIterateClosure(ClassLoaderData::_claim_other),\n-      _iter(iter),\n-      _base(base),\n-      _queue(q) {}\n-\n-  virtual ReferenceIterationMode reference_iteration_mode() {\n-    return VisitReferents ? DO_FIELDS : DO_FIELDS_EXCEPT_REFERENT;\n-  }\n-\n-  virtual void do_oop(oop* p) {\n-    const oop obj = load_oop(p);\n-    _iter->try_enqueue(obj, _queue);\n-  }\n-\n-  virtual void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n-  }\n-\n-#ifdef ASSERT\n-  virtual bool should_verify_oops() {\n-    return false;\n-  }\n-#endif\n-};\n-\n-ZHeapParIterator::ZHeapParIterator(uint num_workers) :\n-    _roots_stack(),\n-    _visit_map(ZAddressOffsetMax),\n-    _num_workers(num_workers),\n-    _map_lock() {\n-  \/\/ prepare process queue.\n-    _task_queues = new ZHeapIterTaskQueueSet((int) _num_workers);\n-    for (uint i = 0; i < _num_workers; i++) {\n-      ZHeapIterTaskQueue* q = new ZHeapIterTaskQueue();\n-      q->initialize();\n-      _task_queues->register_queue(i, q);\n-    }\n-  }\n-\n-ZHeapParIterator::~ZHeapParIterator() {\n-  ZVisitParMapIterator iter(&_visit_map);\n-  for (ZHeapParIteratorBitMap* map; iter.next(&map);) {\n-    delete map;\n-  }\n-  \/\/ reclaim task queues\n-  if (_task_queues != NULL) {\n-    for (uint i = 0; i < _num_workers; i++) {\n-      ZHeapIterTaskQueue* q = _task_queues->queue(i);\n-      if (q != NULL) {\n-        delete q;\n-        q = NULL;\n-      }\n-    }\n-    delete _task_queues;\n-    _task_queues = NULL;\n-  }\n-  ClassLoaderDataGraph::clear_claimed_marks(ClassLoaderData::_claim_other);\n-}\n-\n-static size_t object_index_max() {\n-  return ZGranuleSize >> ZObjectAlignmentSmallShift;\n-}\n-\n-static size_t object_index(oop obj) {\n-  const uintptr_t addr = ZOop::to_address(obj);\n-  const uintptr_t offset = ZAddress::offset(addr);\n-  const uintptr_t mask = ZGranuleSize - 1;\n-  return (offset & mask) >> ZObjectAlignmentSmallShift;\n-}\n-\n-ZHeapParIteratorBitMap* ZHeapParIterator::object_map(oop obj) {\n-  const uintptr_t offset = ZAddress::offset(ZOop::to_address(obj));\n-  ZHeapParIteratorBitMap* map = _visit_map.get(offset);\n-  if (map == NULL) {\n-    \/\/ holding lock to update _visit_map\n-    ZLocker<ZLock> locker(&_map_lock);\n-    if (map == NULL) {\n-      map = new ZHeapParIteratorBitMap(object_index_max());\n-      _visit_map.put(offset, map);\n-    }\n-  }\n-  return map;\n-}\n-\n-void ZHeapParIterator::enqueue_roots(bool visit_weaks) {\n-  ZStatTimerDisable disable;\n-  \/\/ Push roots to visit\n-  push_roots<ZRootsIterator,                     false \/* Concurrent *\/, false \/* Weak *\/>();\n-  push_roots<ZConcurrentRootsIteratorClaimOther, true  \/* Concurrent *\/, false \/* Weak *\/>();\n-  if (visit_weaks) {\n-    push_roots<ZWeakRootsIterator,           false \/* Concurrent *\/, true  \/* Weak *\/>();\n-    push_roots<ZConcurrentWeakRootsIterator, true  \/* Concurrent *\/, true  \/* Weak *\/>();\n-  }\n-  \/\/ Divide roots into thread queue.\n-  size_t roots_num = _roots_stack.size();\n-  for (uint i = 0; i < roots_num; i++) {\n-    uint worker_id = i % _num_workers;\n-    oop obj = _roots_stack.pop();\n-    _task_queues->queue(worker_id)->push(obj);\n-  }\n-}\n-\n-void ZHeapParIterator::drain_queue(ObjectClosure* cl, bool visit_weaks, uint worker_id) {\n-  ZStatTimerDisable disable;\n-  ZHeapIterTaskQueue* q = _task_queues->queue(worker_id);\n-  assert(q != NULL, \"Heap iteration task queue must not NULL\");\n-  \/\/ Drain stack\n-  oop obj;\n-  while (q->pop_overflow(obj) || q->pop_local(obj) || _task_queues->steal(worker_id, obj)) {\n-    \/\/ Visit object\n-    cl->do_object(obj);\n-    \/\/ Push fields to visit\n-    if (visit_weaks = true) {\n-      push_fields<true>(obj, q);\n-    } else {\n-      push_fields<false>(obj, q);\n-    }\n-  }\n-}\n-\n-\/\/ push objects in to _roots_stack, used by RootOopClosure.\n-void ZHeapParIterator::push(oop obj) {\n-  if (obj == NULL) {\n-    \/\/ Ignore\n-    return;\n-  }\n-\n-  ZHeapParIteratorBitMap* const map = object_map(obj);\n-  const size_t index = object_index(obj);\n-  if (!map->try_set_bit(index)) {\n-    \/\/ Already pushed\n-    return;\n-  }\n-\n-  \/\/ Push\n-  _roots_stack.push(obj);\n-}\n-\n-\/\/ push roots in to _roots_stack\n-template <typename RootsIterator, bool Concurrent, bool Weak>\n-void ZHeapParIterator::push_roots() {\n-  ZHeapParIteratorRootOopClosure<Concurrent, Weak> cl(this);\n-  RootsIterator roots;\n-  roots.oops_do(&cl);\n-}\n-\n-template <bool VisitReferents>\n-void ZHeapParIterator::push_fields(oop obj, ZHeapIterTaskQueue* q) {\n-  ZHeapParIteratorOopClosure<VisitReferents> cl(this, obj, q);\n-  obj->oop_iterate(&cl);\n-}\n-\n-void ZHeapParIterator::try_enqueue(oop obj, ZHeapIterTaskQueue* q) {\n-   if (obj == NULL) {\n-    \/\/ Ignore\n-    return;\n-  }\n-\n-  ZHeapParIteratorBitMap* const map = object_map(obj);\n-  const size_t index = object_index(obj);\n-  if (!map->par_try_set_bit(index)) {\n-    \/\/ Already pushed\n-    return;\n-  }\n-\n-  \/\/ Push\n-  q->push(obj);\n-}\n","filename":"src\/hotspot\/share\/gc\/z\/zHeapParIterator.cpp","additions":0,"deletions":274,"binary":false,"changes":274,"status":"deleted"},{"patch":"@@ -1,67 +0,0 @@\n-\/*\n- * Copyright (C) 2020 THL A29 Limited, a Tencent company. All rights reserved.\n- * DO NOT ALTER OR REMOVE NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify\n- * it under the terms of the GNU General Public License version 2 as\n- * published by the Free Software Foundation. THL A29 Limited designates\n- * this particular file as subject to the \"Classpath\" exception as provided\n- * by Oracle in the LICENSE file that accompanied this code.\n- *\n- * This code is distributed in the hope that it will be useful,\n- * but WITHOUT ANY WARRANTY; without even the implied warranty of\n- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n- * GNU General Public License version 2 for more details.\n- *\n- * You should have received a copy of the GNU General Public License along\n- * with this program; if not, write to the Free Software Foundation, Inc.,\n- * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\/\n-\n-#ifndef SHARE_GC_Z_ZHEAPPARITERATOR_HPP\n-#define SHARE_GC_Z_ZHEAPPARITERATOR_HPP\n-\n-#include \"gc\/shared\/taskqueue.hpp\"\n-#include \"gc\/z\/zLock.inline.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"utilities\/stack.hpp\"\n-\n-class ObjectClosure;\n-class ZHeapParIteratorBitMap;\n-\n-typedef OverflowTaskQueue<oop, mtGC>                     ZHeapIterTaskQueue;\n-typedef GenericTaskQueueSet<ZHeapIterTaskQueue, mtGC>    ZHeapIterTaskQueueSet;\n-\/\/ ZHeapIterator\n-class ZHeapParIterator : public StackObj {\n-  template<bool VisitReferents> friend class ZHeapParIteratorOopClosure;\n-  template <bool Concurrent, bool Weak> friend class ZHeapParIteratorRootOopClosure;\n-\n-private:\n-  typedef ZGranuleMap<ZHeapParIteratorBitMap*>             ZVisitParMap;\n-  typedef ZGranuleMapIterator<ZHeapParIteratorBitMap*>     ZVisitParMapIterator;\n-  typedef Stack<oop, mtGC>                                 ZVisitRootStack;\n-\n-  ZVisitRootStack _roots_stack;\n-  ZVisitParMap    _visit_map;\n-  uint            _num_workers;\n-  ZLock           _map_lock;\n-\n-  ZHeapIterTaskQueueSet* _task_queues;\n-\n-  ZHeapParIteratorBitMap* object_map(oop obj);\n-  void push(oop obj);\n-\n-  template <typename RootsIterator, bool Concurrent, bool Weak> void push_roots();\n-  template <bool VisitReferents> void push_fields(oop obj, ZHeapIterTaskQueue* queue);\n-  template <bool VisitReferents> void objects_do(ObjectClosure* cl);\n-\n-public:\n-  ZHeapParIterator(uint num_workers);\n-  ~ZHeapParIterator();\n-\n-  void try_enqueue(oop obj, ZHeapIterTaskQueue* q);\n-  void enqueue_roots(bool visit_weaks);\n-  void drain_queue(ObjectClosure* cl, bool visit_weaks, uint worker_id);\n-};\n-\n-#endif \/\/ SHARE_GC_Z_ZHEAPPARITERATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zHeapParIterator.hpp","additions":0,"deletions":67,"binary":false,"changes":67,"status":"deleted"}]}