{"files":[{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -352,1 +353,1 @@\n-  \/\/ Get index out of bytecode pointer, get_cache_entry_pointer_at_bcp\n+  \/\/ Get index out of bytecode pointer.\n@@ -354,1 +355,2 @@\n-  \/\/ Get address of invokedynamic array\n+\n+  \/\/ Get the address of the ResolvedIndyEntry array\n@@ -357,2 +359,10 @@\n-  \/\/ Scale the index to be the entry index * sizeof(ResolvedInvokeDynamicInfo)\n-  z_sllg(index, index, exact_log2(sizeof(ResolvedIndyEntry)));\n+\n+  \/\/ Scale the index to form a byte offset into the ResolvedIndyEntry array\n+  size_t entry_size = sizeof(ResolvedIndyEntry);\n+  if (is_power_of_2(entry_size)) {\n+    z_sllg(index, index, exact_log2(entry_size));\n+  } else {\n+    z_mghi(index, entry_size);\n+  }\n+\n+  \/\/ Calculate the final field address.\n@@ -362,0 +372,20 @@\n+void InterpreterMacroAssembler::load_field_entry(Register cache, Register index, int bcp_offset) {\n+  \/\/ Get field index out of bytecode pointer.\n+  get_cache_index_at_bcp(index, bcp_offset, sizeof(u2));\n+\n+  \/\/ Get the address of the ResolvedFieldEntry array.\n+  get_constant_pool_cache(cache);\n+  z_lg(cache, Address(cache, in_bytes(ConstantPoolCache::field_entries_offset())));\n+\n+  \/\/ Scale the index to form a byte offset into the ResolvedFieldEntry array\n+  size_t entry_size = sizeof(ResolvedFieldEntry);\n+  if (is_power_of_2(entry_size)) {\n+    z_sllg(index, index, exact_log2(entry_size));\n+  } else {\n+    z_mghi(index, entry_size);\n+  }\n+\n+  \/\/ Calculate the final field address.\n+  z_la(cache, Array<ResolvedFieldEntry>::base_offset_in_bytes(), index, cache);\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":34,"deletions":4,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -116,0 +116,1 @@\n+  void load_field_entry(Register cache, Register index, int bcp_offset = 1);\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -79,4 +80,7 @@\n-    __ z_larl(Z_R0, (int64_t)0);     \/* Check current address alignment. *\/    \\\n-    __ z_slgr(Z_R0, br_tab);         \/* Current Address must be equal    *\/    \\\n-    __ z_slgr(Z_R0, flags);          \/* to calculated branch target.     *\/    \\\n-    __ z_brc(Assembler::bcondLogZero, 3); \/* skip trap if ok. *\/               \\\n+    __ z_larl(br_tab_temp, (int64_t)0);  \/* Check current address alignment. *\/\\\n+    __ z_slgr(br_tab_temp, br_tab);      \/* Current Address must be equal    *\/\\\n+    if (tos_state != Z_R0_scratch) {                                           \\\n+      __ z_slgr(br_tab_temp, tos_state); \/* to calculated branch target.     *\/\\\n+    }                                                                          \\\n+    __ z_brc(Assembler::bcondLogZero, 4);\/* skip trap if ok. *\/                \\\n+    __ z_illtrap(0x55);                                                        \\\n@@ -254,2 +258,6 @@\n-        __ get_cache_and_index_and_bytecode_at_bcp(Z_R1_scratch, bc_reg,\n-                                                   temp_reg, byte_no, 1);\n+\n+        \/\/ Both registers are block-local temp regs. Their contents before and after is not used.\n+        Register index = bc_reg;\n+        Register cache = temp_reg;\n+\n+        __ load_field_entry(cache, index);\n@@ -257,2 +265,7 @@\n-        __ compareU32_and_branch(temp_reg, (intptr_t)0,\n-                                 Assembler::bcondZero, L_patch_done);\n+\n+        if (byte_no == f1_byte) {\n+          __ z_cli(Address(cache, in_bytes(ResolvedFieldEntry::get_code_offset())), 0);\n+        } else {\n+          __ z_cli(Address(cache, in_bytes(ResolvedFieldEntry::put_code_offset())), 0);\n+        }\n+        __ z_bre(L_patch_done);\n@@ -263,1 +276,1 @@\n-      \/\/ The pair bytecodes have already done the load.\n+      \/\/ The bytecode pair may have already performed the load.\n@@ -271,2 +284,1 @@\n-\n-    Label   L_fast_patch;\n+    NearLabel L_fast_patch;\n@@ -277,0 +289,1 @@\n+\n@@ -281,1 +294,1 @@\n-                      temp_reg, Z_R13, bc_reg);\n+                      temp_reg, Z_bcp, bc_reg);\n@@ -2345,1 +2358,1 @@\n-\/\/ NOTE: Cpe_offset is already computed as byte offset, so we must not\n+\/\/ NOTE: index is already computed as byte offset, so we must not\n@@ -2349,1 +2362,1 @@\n-                                            Register cpe_offset,\n+                                            Register index,\n@@ -2351,5 +2364,0 @@\n-  BLOCK_COMMENT(\"resolve_cache_and_index {\");\n-  NearLabel      resolved, clinit_barrier_slow;\n-  const Register bytecode_in_cpcache = Z_R1_scratch;\n-  const int      total_f1_offset = in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset());\n-  assert_different_registers(cache, cpe_offset, bytecode_in_cpcache);\n@@ -2357,0 +2365,5 @@\n+  assert_different_registers(cache, index, Z_R1_scratch);\n+  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n+\n+  const Register  bytecode_in_cpcache = Z_R1_scratch;\n+  NearLabel       resolved, clinit_barrier_slow;\n@@ -2358,6 +2371,0 @@\n-  switch (code) {\n-    case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;\n-    case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;\n-    default:\n-      break;\n-  }\n@@ -2365,6 +2372,1 @@\n-  {\n-    assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n-    __ get_cache_and_index_and_bytecode_at_bcp(cache, cpe_offset, bytecode_in_cpcache, byte_no, 1, index_size);\n-    \/\/ Have we resolved this bytecode?\n-    __ compare32_and_branch(bytecode_in_cpcache, (int)code, Assembler::bcondEqual, resolved);\n-  }\n+  BLOCK_COMMENT(\"resolve_cache_and_index {\");\n@@ -2372,1 +2374,5 @@\n-  \/\/ Resolve first time through.\n+  __ get_cache_and_index_and_bytecode_at_bcp(cache, index, bytecode_in_cpcache, byte_no, 1, index_size);\n+  \/\/ Have we resolved this bytecode?\n+  __ compare32_and_branch(bytecode_in_cpcache, (int)code, Assembler::bcondEqual, resolved);\n+\n+  \/\/ Resolve first time through via runtime call.\n@@ -2376,1 +2382,1 @@\n-  __ load_const_optimized(Z_ARG2, (int) code);\n+  __ load_const_optimized(Z_ARG2, (int)code);\n@@ -2378,1 +2384,0 @@\n-\n@@ -2380,1 +2385,2 @@\n-  __ get_cache_and_index_at_bcp(cache, cpe_offset, 1, index_size);\n+  __ get_cache_and_index_at_bcp(cache, index, 1, index_size);\n+\n@@ -2388,1 +2394,1 @@\n-    __ load_resolved_method_at_index(byte_no, cache, cpe_offset, method);\n+    __ load_resolved_method_at_index(byte_no, cache, index, method);\n@@ -2396,0 +2402,61 @@\n+void TemplateTable::resolve_cache_and_index_for_field(int byte_no,\n+                                                      Register cache,\n+                                                      Register index) {\n+\n+  assert_different_registers(cache, index, Z_R1_scratch);\n+  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n+\n+  NearLabel resolved;\n+\n+  Bytecodes::Code code = bytecode();\n+  switch (code) {\n+    case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;\n+    case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;\n+    default: break;\n+  }\n+\n+  __ load_field_entry(cache, index);\n+  if (byte_no == f1_byte) {\n+    __ z_cli(Address(cache, in_bytes(ResolvedFieldEntry::get_code_offset())), code);\n+  } else {\n+    __ z_cli(Address(cache, in_bytes(ResolvedFieldEntry::put_code_offset())), code);\n+  }\n+  __ z_bre(resolved);\n+\n+  \/\/ resolve first time through\n+  address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);\n+  __ load_const_optimized(Z_ARG2, (int)code);\n+  __ call_VM(noreg, entry, Z_ARG2);\n+\n+  \/\/ Update registers with resolved info.\n+  __ load_field_entry(cache, index);\n+\n+  __ bind(resolved);\n+}\n+\n+\/\/ The cache register (the only input reg) must be set before call.\n+void TemplateTable::load_resolved_field_entry(Register obj,\n+                                              Register cache,\n+                                              Register tos_state,\n+                                              Register offset,\n+                                              Register flags,\n+                                              bool is_static = false) {\n+  assert_different_registers(cache, tos_state, flags, offset);\n+\n+  \/\/ Field offset\n+  __ load_sized_value(offset, Address(cache, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+\n+  \/\/ Flags\n+  __ load_sized_value(flags, Address(cache, in_bytes(ResolvedFieldEntry::flags_offset())), sizeof(u1), false);\n+\n+  \/\/ TOS state\n+  __ load_sized_value(tos_state, Address(cache, in_bytes(ResolvedFieldEntry::type_offset())), sizeof(u1), false);\n+\n+  \/\/ Klass overwrite register\n+  if (is_static) {\n+    __ load_sized_value(obj, Address(cache, ResolvedFieldEntry::field_holder_offset()), sizeof(void*), false);\n+    __ load_sized_value(obj, Address(obj, in_bytes(Klass::java_mirror_offset())), sizeof(void*), false);\n+    __ resolve_oop_handle(obj);\n+  }\n+}\n+\n@@ -2523,2 +2590,4 @@\n-\/\/ The registers cache and index expected to be set before call.\n-\/\/ Correct values of the cache and index registers are preserved.\n+\/\/ The registers cache and index are set up if needed.\n+\/\/ However, the field entry must have been resolved before.\n+\/\/ If no jvmti post operation is performed, their contents remains unchanged.\n+\/\/ After a jvmti post operation, the registers are re-calculated by load_field_entry().\n@@ -2537,1 +2606,1 @@\n-  Label exit;\n+  Label dontPost;\n@@ -2540,5 +2609,2 @@\n-  __ load_and_test_int(Z_R0, Address(Z_tos));\n-  __ z_brz(exit);\n-\n-  \/\/ Index is returned as byte offset, do not shift!\n-  __ get_cache_and_index_at_bcp(Z_ARG3, Z_R1_scratch, 1);\n+  __ z_chsi(0, Z_tos, 0); \/\/ avoid loading data into a scratch register\n+  __ z_bre(dontPost);\n@@ -2547,3 +2613,1 @@\n-  __ add2reg_with_index(Z_ARG3,\n-                        in_bytes(ConstantPoolCache::base_offset()),\n-                        Z_ARG3, Z_R1_scratch);\n+  \/\/ __ load_field_entry(cache, index); \/\/ not required as already set by resolve_cache_and_index_for_field()\n@@ -2554,1 +2618,1 @@\n-    __ mem2reg_opt(Z_ARG2, at_tos());  \/\/ Get object pointer without popping it.\n+    __ load_ptr(0, Z_ARG2);  \/\/ Get object pointer without popping it.\n@@ -2557,0 +2621,1 @@\n+\n@@ -2558,1 +2623,1 @@\n-  \/\/ Z_ARG3: cache entry pointer\n+  \/\/ cache:  cache entry pointer\n@@ -2561,2 +2626,1 @@\n-             Z_ARG2, Z_ARG3);\n-  __ get_cache_and_index_at_bcp(cache, index, 1);\n+             Z_ARG2, cache);\n@@ -2564,1 +2628,4 @@\n-  __ bind(exit);\n+  \/\/ restore registers after runtime call.\n+  __ load_field_entry(cache, index);\n+\n+  __ bind(dontPost);\n@@ -2576,6 +2643,14 @@\n-  const Register cache = Z_tmp_1;\n-  const Register index = Z_tmp_2;\n-  const Register obj   = Z_tmp_1;\n-  const Register off   = Z_ARG2;\n-  const Register flags = Z_ARG1;\n-  const Register bc    = Z_tmp_1;  \/\/ Uses same reg as obj, so don't mix them.\n+  const Register obj           = Z_tmp_1;\n+  const Register off           = Z_tmp_2;\n+  const Register cache         = Z_tmp_1;\n+  const Register index         = Z_tmp_2;\n+  const Register flags         = Z_R1_scratch; \/\/ flags are not used in getfield\n+  const Register br_tab        = Z_R1_scratch;\n+  const Register tos_state     = Z_ARG4;\n+  const Register bc_reg        = Z_tmp_1;\n+  const Register patch_tmp     = Z_ARG4;\n+  const Register oopLoad_tmp1  = Z_R1_scratch;\n+  const Register oopLoad_tmp2  = Z_ARG5;\n+#ifdef ASSERT\n+  const Register br_tab_temp   = Z_R0_scratch;  \/\/ for branch table verification code only\n+#endif\n@@ -2583,1 +2658,13 @@\n-  resolve_cache_and_index(byte_no, cache, index, sizeof(u2));\n+\n+  \/\/ Register usage and life range\n+  \/\/\n+  \/\/  cache, index: short-lived. Their life ends after load_resolved_field_entry.\n+  \/\/  obj (overwrites cache): long-lived. Used in branch table entries.\n+  \/\/  off (overwrites index): long-lived. Used in branch table entries.\n+  \/\/  flags: unused in getfield.\n+  \/\/  br_tab: short-lived. Only used to address branch table, and for verification\n+  \/\/          in BTB_BEGIN macro.\n+  \/\/  tos_state: short-lived. Only used to index the branch table entry.\n+  \/\/  bc_reg: short-lived. Used as work register in patch_bytecode.\n+  \/\/\n+  resolve_cache_and_index_for_field(byte_no, cache, index);\n@@ -2585,1 +2672,1 @@\n-  load_field_cp_cache_entry(obj, cache, index, off, flags, is_static);\n+  load_resolved_field_entry(obj, cache, tos_state, off, flags, is_static);\n@@ -2592,1 +2679,1 @@\n-  \/\/ Displacement is 0, so any store instruction will be fine on any CPU.\n+  \/\/ Displacement is 0. No need to care about limited displacement range.\n@@ -2595,1 +2682,1 @@\n-  Label    is_Byte, is_Bool, is_Int, is_Short, is_Char,\n+  Label    is_Byte, is_Bool,  is_Int,    is_Short, is_Char,\n@@ -2597,3 +2684,2 @@\n-  Label    is_badState8, is_badState9, is_badStateA, is_badStateB,\n-           is_badStateC, is_badStateD, is_badStateE, is_badStateF,\n-           is_badState;\n+  Label    is_badState,  is_badState9, is_badStateA, is_badStateB,\n+           is_badStateC, is_badStateD, is_badStateE, is_badStateF;\n@@ -2601,1 +2687,0 @@\n-  Register br_tab       = Z_R1_scratch;\n@@ -2606,1 +2691,1 @@\n-  assert(btos == 0, \"change code, btos != 0\");\n+  assert((btos == 0) && (atos == 8), \"change branch table! ByteCodes may have changed\");\n@@ -2608,4 +2693,1 @@\n-  \/\/ Calculate branch table size. Generated code size depends on ASSERT and on bytecode rewriting.\n-#ifdef ASSERT\n-  const unsigned int bsize = dont_rewrite ? BTB_MINSIZE*1 : BTB_MINSIZE*4;\n-#else\n+  \/\/ Calculate branch table size.\n@@ -2613,1 +2695,0 @@\n-#endif\n@@ -2618,3 +2699,0 @@\n-    const int r_bitpos  = 63 - bit_shift;\n-    const int l_bitpos  = r_bitpos - ConstantPoolCacheEntry::tos_state_bits + 1;\n-    const int n_rotate  = (bit_shift-ConstantPoolCacheEntry::tos_state_shift);\n@@ -2622,1 +2700,7 @@\n-    __ rotate_then_insert(flags, flags, l_bitpos, r_bitpos, n_rotate, true);\n+    __ z_sllg(tos_state, tos_state, bit_shift);\n+    if (tos_state == Z_R0_scratch) {\n+      __ z_agr(br_tab, tos_state); \/\/ can't use tos_state with address calculation\n+      __ z_bcr(Assembler::bcondAlways, br_tab);\n+    } else {\n+      __ z_bc(Assembler::bcondAlways, 0, tos_state, br_tab);\n+    }\n@@ -2624,1 +2708,0 @@\n-  __ z_bc(Assembler::bcondAlways, 0, flags, br_tab);\n@@ -2635,1 +2718,1 @@\n-    patch_bytecode(Bytecodes::_fast_bgetfield, bc, Z_ARG5);\n+    patch_bytecode(Bytecodes::_fast_bgetfield, bc_reg, patch_tmp);\n@@ -2647,1 +2730,1 @@\n-    patch_bytecode(Bytecodes::_fast_bgetfield, bc, Z_ARG5);\n+    patch_bytecode(Bytecodes::_fast_bgetfield, bc_reg, patch_tmp);\n@@ -2659,1 +2742,1 @@\n-    patch_bytecode(Bytecodes::_fast_cgetfield, bc, Z_ARG5);\n+    patch_bytecode(Bytecodes::_fast_cgetfield, bc_reg, patch_tmp);\n@@ -2670,1 +2753,1 @@\n-    patch_bytecode(Bytecodes::_fast_sgetfield, bc, Z_ARG5);\n+    patch_bytecode(Bytecodes::_fast_sgetfield, bc_reg, patch_tmp);\n@@ -2681,1 +2764,1 @@\n-    patch_bytecode(Bytecodes::_fast_igetfield, bc, Z_ARG5);\n+    patch_bytecode(Bytecodes::_fast_igetfield, bc_reg, patch_tmp);\n@@ -2692,1 +2775,1 @@\n-    patch_bytecode(Bytecodes::_fast_lgetfield, bc, Z_ARG5);\n+    patch_bytecode(Bytecodes::_fast_lgetfield, bc_reg, patch_tmp);\n@@ -2703,1 +2786,1 @@\n-    patch_bytecode(Bytecodes::_fast_fgetfield, bc, Z_ARG5);\n+    patch_bytecode(Bytecodes::_fast_fgetfield, bc_reg, patch_tmp);\n@@ -2714,1 +2797,1 @@\n-    patch_bytecode(Bytecodes::_fast_dgetfield, bc, Z_ARG5);\n+    patch_bytecode(Bytecodes::_fast_dgetfield, bc_reg, patch_tmp);\n@@ -2725,4 +2808,0 @@\n-  BTB_BEGIN(is_badState8, bsize, \"getfield_or_static:is_badState8\");\n-  __ z_illtrap();\n-  __ z_bru(is_badState);\n-  BTB_END( is_badState8, bsize, \"getfield_or_static:is_badState8\");\n@@ -2775,1 +2854,1 @@\n-                      \/\/ to here is compensated for by the fallthru to \"Done\".\n+                      \/\/ to here is compensated for by the fallthrough to \"Done\".\n@@ -2778,1 +2857,1 @@\n-    do_oop_load(_masm, field, Z_tos, Z_tmp_2, Z_tmp_3, IN_HEAP);\n+    do_oop_load(_masm, field, Z_tos, oopLoad_tmp1, oopLoad_tmp2, IN_HEAP);\n@@ -2782,1 +2861,1 @@\n-      patch_bytecode(Bytecodes::_fast_agetfield, bc, Z_ARG5);\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc_reg, patch_tmp);\n@@ -2806,3 +2885,3 @@\n-\/\/ The registers cache and index expected to be set before call.  The\n-\/\/ function may destroy various registers, just not the cache and\n-\/\/ index registers.\n+\/\/ Register cache is expected to be set before the call.\n+\/\/ This function may destroy various registers.\n+\/\/ Only the contents of register cache is preserved\/restored.\n@@ -2819,6 +2898,4 @@\n-  \/\/ Check to see if a field modification watch has been set before\n-  \/\/ we take the time to call into the VM.\n-  Label    L1;\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n-  assert_different_registers(cache, index, Z_tos);\n-\n+  \/\/ Check to see if a field modification watch has been set\n+  \/\/ before we take the time to call into the VM.\n+  Label    dontPost;\n+  assert_different_registers(cache, index, Z_tos, Z_ARG2, Z_ARG3, Z_ARG4);\n@@ -2826,2 +2903,6 @@\n-  __ load_and_test_int(Z_R0, Address(Z_tos));\n-  __ z_brz(L1);\n+  __ z_chsi(0, Z_tos, 0); \/\/ avoid loading data into a scratch register\n+  __ z_bre(dontPost);\n+\n+  Register obj        = Z_ARG2;\n+  Register fieldEntry = Z_ARG3;\n+  Register value      = Z_ARG4;\n@@ -2829,2 +2910,2 @@\n-  \/\/ Index is returned as byte offset, do not shift!\n-  __ get_cache_and_index_at_bcp(Z_ARG3, Z_R1_scratch, 1);\n+  \/\/ Take a copy of cache entry pointer\n+  __ z_lgr(fieldEntry, cache);\n@@ -2833,2 +2914,2 @@\n-    \/\/ Life is simple. Null out the object pointer.\n-    __ clear_reg(Z_ARG2, true, false);   \/\/ Don't set CC.\n+    \/\/ Life is simple. NULL the object pointer.\n+    __ clear_reg(obj, true, false); \/\/ Don't set CC.\n@@ -2840,9 +2921,1 @@\n-    __ mem2reg_opt(Z_ARG4,\n-                   Address(Z_ARG3, Z_R1_scratch,\n-                           in_bytes(cp_base_offset + ConstantPoolCacheEntry::flags_offset()) +\n-                           (BytesPerLong - BytesPerInt)),\n-                   false);\n-    __ z_srl(Z_ARG4, ConstantPoolCacheEntry::tos_state_shift);\n-    \/\/ Make sure we don't need to mask Z_ARG4 for tos_state after the above shift.\n-    ConstantPoolCacheEntry::verify_tos_state_shift();\n-    __ mem2reg_opt(Z_ARG2, at_tos(1));  \/\/ Initially assume a one word jvalue.\n+    __ load_sized_value(value, Address(fieldEntry, in_bytes(ResolvedFieldEntry::type_offset())), sizeof(u1), false);\n@@ -2850,1 +2923,1 @@\n-    NearLabel   load_dtos, cont;\n+    __ mem2reg_opt(obj, at_tos(1)); \/\/ Initially assume a one word jvalue.\n@@ -2852,4 +2925,7 @@\n-    __ compareU32_and_branch(Z_ARG4, (intptr_t) ltos,\n-                              Assembler::bcondNotEqual, load_dtos);\n-    __ mem2reg_opt(Z_ARG2, at_tos(2)); \/\/ ltos (two word jvalue)\n-    __ z_bru(cont);\n+    if (VM_Version::has_LoadStoreConditional()) {\n+      __ z_chi(value, ltos);\n+      __ z_locg(obj, at_tos(2), Assembler::bcondEqual);\n+      __ z_chi(value, dtos);\n+      __ z_locg(obj, at_tos(2), Assembler::bcondEqual);\n+    } else {\n+      NearLabel load_dtos, cont;\n@@ -2857,3 +2933,4 @@\n-    __ bind(load_dtos);\n-    __ compareU32_and_branch(Z_ARG4, (intptr_t)dtos, Assembler::bcondNotEqual, cont);\n-    __ mem2reg_opt(Z_ARG2, at_tos(2)); \/\/ dtos (two word jvalue)\n+      __ z_chi(value, ltos);\n+      __ z_brne(load_dtos);\n+      __ mem2reg_opt(obj, at_tos(2)); \/\/ ltos (two word jvalue)\n+      __ z_bru(cont);\n@@ -2861,3 +2938,4 @@\n-    __ bind(cont);\n-  }\n-  \/\/ cache entry pointer\n+      __ bind(load_dtos);\n+      __ z_chi(value, dtos);\n+      __ z_brne(cont);\n+      __ mem2reg_opt(obj, at_tos(2)); \/\/ dtos (two word jvalue)\n@@ -2865,1 +2943,3 @@\n-  __ add2reg_with_index(Z_ARG3, in_bytes(cp_base_offset), Z_ARG3, Z_R1_scratch);\n+      __ bind(cont);\n+    }\n+  }\n@@ -2868,4 +2948,4 @@\n-  __ load_address(Z_ARG4, Address(Z_esp, Interpreter::stackElementSize));\n-  \/\/ Z_ARG2: object pointer set up above (null if static)\n-  \/\/ Z_ARG3: cache entry pointer\n-  \/\/ Z_ARG4: jvalue object on the stack\n+  __ load_address(value, Address(Z_esp, Interpreter::expr_offset_in_bytes(0)));\n+  \/\/ obj:        object pointer set up above (null if static)\n+  \/\/ fieldEntry: field entry pointer\n+  \/\/ value:      jvalue object on the stack\n@@ -2874,2 +2954,4 @@\n-             Z_ARG2, Z_ARG3, Z_ARG4);\n-  __ get_cache_and_index_at_bcp(cache, index, 1);\n+             obj, fieldEntry, value);\n+\n+  \/\/ Reload field entry\n+  __ load_field_entry(cache, index);\n@@ -2877,1 +2959,1 @@\n-  __ bind(L1);\n+  __ bind(dontPost);\n@@ -2885,2 +2967,0 @@\n-  const Register cache         = Z_tmp_1;\n-  const Register index         = Z_ARG5;\n@@ -2889,5 +2969,9 @@\n-  const Register flags         = Z_R1_scratch;\n-  const Register br_tab        = Z_ARG5;\n-  const Register bc            = Z_tmp_1;\n-  const Register oopStore_tmp1 = Z_R1_scratch;\n-  const Register oopStore_tmp2 = Z_ARG5;\n+  const Register cache         = Z_tmp_1;\n+  const Register index         = Z_tmp_2;\n+  const Register flags         = Z_R1_scratch; \/\/ non-volatile reg required, but none available.\n+  const Register br_tab        = Z_R1_scratch;\n+  const Register tos_state     = Z_ARG4;\n+  const Register bc_reg        = Z_tmp_1;\n+  const Register patch_tmp     = Z_ARG4;\n+  const Register oopStore_tmp1 = Z_R1_scratch; \/\/ tmp1 or tmp2 must be non-volatile reg\n+  const Register oopStore_tmp2 = Z_ARG5;       \/\/ tmp1 or tmp2 must be non-volatile reg\n@@ -2895,0 +2979,3 @@\n+#ifdef ASSERT\n+  const Register br_tab_temp   = Z_R0_scratch;  \/\/ for branch table verification code only\n+#endif\n@@ -2896,1 +2983,12 @@\n-  resolve_cache_and_index(byte_no, cache, index, sizeof(u2));\n+  \/\/ Register usage and life range\n+  \/\/\n+  \/\/  cache, index: short-lived. Their life ends after load_resolved_field_entry.\n+  \/\/  obj (overwrites cache): long-lived. Used in branch table entries.\n+  \/\/  off (overwrites index): long-lived. Used in branch table entries.\n+  \/\/  flags: long-lived. Has to survive until the end to determine volatility.\n+  \/\/  br_tab: short-lived. Only used to address branch table, and for verification\n+  \/\/          in BTB_BEGIN macro.\n+  \/\/  tos_state: short-lived. Only used to index the branch table entry.\n+  \/\/  bc_reg: short-lived. Used as work register in patch_bytecode.\n+  \/\/\n+  resolve_cache_and_index_for_field(byte_no, cache, index);\n@@ -2898,6 +2996,1 @@\n-  load_field_cp_cache_entry(obj, cache, index, off, flags, is_static);\n-  \/\/ begin of life for:\n-  \/\/   obj, off   long life range\n-  \/\/   flags      short life range, up to branch into branch table\n-  \/\/ end of life for:\n-  \/\/   cache, index\n+  load_resolved_field_entry(obj, cache, tos_state, off, flags, is_static);\n@@ -2905,0 +2998,1 @@\n+  \/\/ Displacement is 0. No need to care about limited displacement range.\n@@ -2906,1 +3000,2 @@\n-  Label is_Byte, is_Bool, is_Int, is_Short, is_Char,\n+\n+  Label is_Byte, is_Bool,  is_Int,    is_Short, is_Char,\n@@ -2908,3 +3003,2 @@\n-  Label is_badState8, is_badState9, is_badStateA, is_badStateB,\n-        is_badStateC, is_badStateD, is_badStateE, is_badStateF,\n-        is_badState;\n+  Label is_badState,  is_badState9, is_badStateA, is_badStateB,\n+        is_badStateC, is_badStateD, is_badStateE, is_badStateF;\n@@ -2916,2 +3010,1 @@\n-\n-  assert(btos == 0, \"change code, btos != 0\");\n+  assert((btos == 0) && (atos == 8), \"change branch table! ByteCodes may have changed\");\n@@ -2928,3 +3021,0 @@\n-    const int r_bitpos  = 63 - bit_shift;\n-    const int l_bitpos  = r_bitpos - ConstantPoolCacheEntry::tos_state_bits + 1;\n-    const int n_rotate  = (bit_shift-ConstantPoolCacheEntry::tos_state_shift);\n@@ -2932,2 +3022,7 @@\n-    __ rotate_then_insert(flags, flags, l_bitpos, r_bitpos, n_rotate, true);\n-    __ z_bc(Assembler::bcondAlways, 0, flags, br_tab);\n+    __ z_sllg(tos_state, tos_state, bit_shift);\n+    if (tos_state == Z_R0_scratch) {\n+      __ z_agr(br_tab, tos_state); \/\/ can't use tos_state with address calculation\n+      __ z_bcr(Assembler::bcondAlways, br_tab);\n+    } else {\n+      __ z_bc(Assembler::bcondAlways, 0, tos_state, br_tab);\n+    }\n@@ -2935,2 +3030,0 @@\n-  \/\/ end of life for:\n-  \/\/   flags, br_tab\n@@ -2949,1 +3042,1 @@\n-    patch_bytecode(Bytecodes::_fast_bputfield, bc, Z_ARG5, true, byte_no);\n+    patch_bytecode(Bytecodes::_fast_bputfield, bc_reg, patch_tmp, true, byte_no);\n@@ -2963,1 +3056,1 @@\n-    patch_bytecode(Bytecodes::_fast_zputfield, bc, Z_ARG5, true, byte_no);\n+    patch_bytecode(Bytecodes::_fast_zputfield, bc_reg, patch_tmp, true, byte_no);\n@@ -2976,1 +3069,1 @@\n-    patch_bytecode(Bytecodes::_fast_cputfield, bc, Z_ARG5, true, byte_no);\n+    patch_bytecode(Bytecodes::_fast_cputfield, bc_reg, patch_tmp, true, byte_no);\n@@ -2989,1 +3082,1 @@\n-    patch_bytecode(Bytecodes::_fast_sputfield, bc, Z_ARG5, true, byte_no);\n+    patch_bytecode(Bytecodes::_fast_sputfield, bc_reg, patch_tmp, true, byte_no);\n@@ -3002,1 +3095,1 @@\n-    patch_bytecode(Bytecodes::_fast_iputfield, bc, Z_ARG5, true, byte_no);\n+    patch_bytecode(Bytecodes::_fast_iputfield, bc_reg, patch_tmp, true, byte_no);\n@@ -3015,1 +3108,1 @@\n-    patch_bytecode(Bytecodes::_fast_lputfield, bc, Z_ARG5, true, byte_no);\n+    patch_bytecode(Bytecodes::_fast_lputfield, bc_reg, patch_tmp, true, byte_no);\n@@ -3028,1 +3121,1 @@\n-    patch_bytecode(Bytecodes::_fast_fputfield, bc, Z_ARG5, true, byte_no);\n+    patch_bytecode(Bytecodes::_fast_fputfield, bc_reg, patch_tmp, true, byte_no);\n@@ -3041,1 +3134,1 @@\n-    patch_bytecode(Bytecodes::_fast_dputfield, bc, Z_ARG5, true, byte_no);\n+    patch_bytecode(Bytecodes::_fast_dputfield, bc_reg, patch_tmp, true, byte_no);\n@@ -3052,4 +3145,0 @@\n-  BTB_BEGIN(is_badState8, bsize, \"putfield_or_static:is_badState8\");\n-  __ z_illtrap();\n-  __ z_bru(is_badState);\n-  BTB_END( is_badState8, bsize, \"putfield_or_static:is_badState8\");\n@@ -3102,0 +3191,1 @@\n+\n@@ -3107,1 +3197,1 @@\n-    do_oop_store(_masm, Address(obj, off), Z_tos,\n+    do_oop_store(_masm, field, Z_tos,\n@@ -3110,1 +3200,1 @@\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, Z_ARG5, true, byte_no);\n+      patch_bytecode(Bytecodes::_fast_aputfield, bc_reg, patch_tmp, true, byte_no);\n@@ -3112,1 +3202,1 @@\n-    \/\/ __ z_bru(Done); \/\/ fallthru\n+    \/\/ __ z_bru(Done); \/\/ fallthrough\n@@ -3119,1 +3209,2 @@\n-  Label notVolatile;\n+  \/\/ only if flags register is non-volatile\n+  NearLabel notVolatile;\n@@ -3121,2 +3212,4 @@\n-  __ testbit(Z_ARG4, ConstantPoolCacheEntry::is_volatile_shift);\n-  __ z_brz(notVolatile);\n+  if (!flags.is_volatile()) {\n+    __ testbit(flags, ConstantPoolCacheEntry::is_volatile_shift);\n+    __ z_brz(notVolatile);\n+  }\n@@ -3152,4 +3245,0 @@\n-  \/\/ Check to see if a field modification watch has been set before\n-  \/\/ we take the time to call into the VM.\n-  Label   exit;\n-\n@@ -3158,4 +3247,6 @@\n-  __ load_absolute_address(Z_R1_scratch,\n-                           (address) JvmtiExport::get_field_modification_count_addr());\n-  __ load_and_test_int(Z_R0_scratch, Address(Z_R1_scratch));\n-  __ z_brz(exit);\n+  \/\/ Check to see if a field modification watch has been set\n+  \/\/ before we take the time to call into the VM.\n+  Label dontPost;\n+  __ load_absolute_address(Z_R1_scratch, (address)JvmtiExport::get_field_modification_count_addr());\n+  __ z_chsi(0, Z_R1_scratch, 0); \/\/ avoid loading data into a scratch register\n+  __ z_bre(dontPost);\n@@ -3163,1 +3254,3 @@\n-  Register obj = Z_tmp_1;\n+  Register obj        = Z_ARG2;\n+  Register fieldEntry = Z_ARG3;\n+  Register value      = Z_ARG4;\n@@ -3165,3 +3258,3 @@\n-  __ pop_ptr(obj);                  \/\/ Copy the object pointer from tos.\n-  __ verify_oop(obj);\n-  __ push_ptr(obj);                 \/\/ Put the object pointer back on tos.\n+  __ load_ptr(0, obj);              \/\/ Copy the object pointer from tos.\n+  __ verify_oop(obj);               \/\/ and verify it\n+                                    \/\/ TODO: do we need to check twice (here and before call_VM?)\n@@ -3198,1 +3291,1 @@\n-  __ load_address(Z_ARG4, Address(Z_esp, Interpreter::stackElementSize));\n+  __ load_address(value, Address(Z_esp, Interpreter::expr_offset_in_bytes(0)));\n@@ -3200,1 +3293,1 @@\n-  __ get_cache_entry_pointer_at_bcp(Z_ARG3, Z_tos, 1);\n+  __ load_field_entry(fieldEntry, Z_tos, 1);\n@@ -3203,3 +3296,3 @@\n-  \/\/ obj   : object pointer copied above\n-  \/\/ Z_ARG3: cache entry pointer\n-  \/\/ Z_ARG4: jvalue object on the stack\n+  \/\/ obj        : object pointer copied above\n+  \/\/ fieldEntry : cache entry pointer\n+  \/\/ value      : jvalue object on the stack\n@@ -3208,1 +3301,1 @@\n-             obj, Z_ARG3, Z_ARG4);\n+             obj, fieldEntry, value);\n@@ -3234,1 +3327,1 @@\n-  __ bind(exit);\n+  __ bind(dontPost);\n@@ -3241,1 +3334,0 @@\n-  ByteSize base = ConstantPoolCache::base_offset();\n@@ -3245,3 +3337,6 @@\n-  Register cache = Z_tmp_1;\n-  Register index = Z_tmp_2;\n-  Register flags = Z_ARG5;\n+  Register obj       = Z_tmp_1;\n+  Register cache     = Z_tmp_1;\n+  Register index     = Z_tmp_2;\n+  Register off       = Z_tmp_2;\n+  Register flags     = Z_ARG5;\n+  Register tos_state = Z_R1_scratch;\n@@ -3250,9 +3345,3 @@\n-  __ get_cache_and_index_at_bcp(cache, index, 1);\n-\n-  \/\/ Test for volatile.\n-  assert(!flags->is_volatile(), \"do_oop_store could perform leaf RT call\");\n-  __ z_lg(flags, Address(cache, index, base + ConstantPoolCacheEntry::flags_offset()));\n-\n-  \/\/ Replace index with field offset from cache entry.\n-  Register field_offset = index;\n-  __ z_lg(field_offset, Address(cache, index, base + ConstantPoolCacheEntry::f2_offset()));\n+  __ load_field_entry(cache, index);\n+  \/\/ this call is for nonstatic. obj remains unchanged.\n+  load_resolved_field_entry(obj, cache, tos_state, off, flags, false);\n@@ -3261,2 +3350,0 @@\n-  Register   obj = cache;\n-\n@@ -3266,1 +3353,1 @@\n-  const Address   field(obj, field_offset);\n+  const Address field(obj, off);\n@@ -3271,1 +3358,1 @@\n-      do_oop_store(_masm, Address(obj, field_offset), Z_tos,\n+      do_oop_store(_masm, field, Z_tos,\n@@ -3314,1 +3401,1 @@\n-  Register obj = Z_tos;\n+  Register obj = Z_tos;  \/\/ Object ptr is in TOS\n@@ -3316,1 +3403,1 @@\n-  \/\/ Do the JVMTI work here to avoid disturbing the register state below\n+  \/\/ Do the JVMTI work here. There is no specific jvmti_post_fast_access() emitter.\n@@ -3318,3 +3405,6 @@\n-    \/\/ Check to see if a field access watch has been set before we\n-    \/\/ take the time to call into the VM.\n-    Label cont;\n+    \/\/ Check to see if a field modification watch has been set\n+    \/\/ before we take the time to call into the VM.\n+    BLOCK_COMMENT(\"jvmti_post_fast_field_access {\");\n+    Label    dontPost;\n+    Register cache = Z_ARG3;\n+    Register index = Z_tmp_2;\n@@ -3322,4 +3412,3 @@\n-    __ load_absolute_address(Z_R1_scratch,\n-                             (address)JvmtiExport::get_field_access_count_addr());\n-    __ load_and_test_int(Z_R0_scratch, Address(Z_R1_scratch));\n-    __ z_brz(cont);\n+    __ load_absolute_address(Z_R1_scratch, (address)JvmtiExport::get_field_access_count_addr());\n+    __ z_chsi(0, Z_R1_scratch, 0); \/\/ avoid loading data into a scratch register\n+    __ z_bre(dontPost);\n@@ -3328,0 +3417,1 @@\n+    __ load_field_entry(cache, index);\n@@ -3329,1 +3419,0 @@\n-    __ get_cache_entry_pointer_at_bcp(Z_ARG3, Z_tmp_1, 1);\n@@ -3331,1 +3420,1 @@\n-    __ push_ptr(obj);  \/\/ Save object pointer before call_VM() clobbers it.\n+    __ push_ptr(obj);   \/\/ Save object pointer before call_VM() clobbers it.\n@@ -3335,1 +3424,1 @@\n-    \/\/ Z_ARG3: cache entry pointer\n+    \/\/ cache: cache entry pointer\n@@ -3338,1 +3427,1 @@\n-               Z_ARG2, Z_ARG3);\n+               Z_ARG2, cache);\n@@ -3341,1 +3430,2 @@\n-    __ bind(cont);\n+    __ bind(dontPost);\n+    BLOCK_COMMENT(\"} jvmti_post_fast_field_access\");\n@@ -3345,2 +3435,3 @@\n-  Register   cache = Z_tmp_1;\n-  Register   index = Z_tmp_2;\n+  Register cache = Z_tmp_1;\n+  Register index = Z_tmp_2;\n+  Register off   = Z_tmp_2;\n@@ -3349,1 +3440,1 @@\n-  __ get_cache_and_index_at_bcp(cache, index, 1);\n+  __ load_field_entry(cache, index);\n@@ -3351,3 +3442,1 @@\n-  __ mem2reg_opt(index,\n-                 Address(cache, index,\n-                         ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f2_offset()));\n+  __ load_sized_value(off, Address(cache, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(jint), true);\n@@ -3358,1 +3447,1 @@\n-  Address field(obj, index);\n+  Address field(obj, off);\n@@ -3402,0 +3491,1 @@\n+  Register off   = Z_tmp_2;\n@@ -3404,1 +3494,1 @@\n-  __ get_cache_and_index_at_bcp(cache, index, 2);\n+  __ load_field_entry(cache, index, 2);\n@@ -3406,3 +3496,1 @@\n-  __ mem2reg_opt(index,\n-                 Address(cache, index,\n-                         ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f2_offset()));\n+  __ load_sized_value(off, Address(cache, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(jint), true);\n@@ -3414,0 +3502,3 @@\n+\n+  Address field(receiver, off);\n+\n@@ -3416,1 +3507,1 @@\n-      __ mem2reg_opt(Z_tos, Address(receiver, index), false);\n+      __ mem2reg_opt(Z_tos, field, false);\n@@ -3419,1 +3510,1 @@\n-      do_oop_load(_masm, Address(receiver, index), Z_tos, Z_tmp_1, Z_tmp_2, IN_HEAP);\n+      do_oop_load(_masm, field, Z_tos, Z_tmp_1, Z_tmp_2, IN_HEAP);\n@@ -3423,1 +3514,1 @@\n-      __ mem2freg_opt(Z_ftos, Address(receiver, index));\n+      __ mem2freg_opt(Z_ftos, field);\n","filename":"src\/hotspot\/cpu\/s390\/templateTable_s390.cpp","additions":335,"deletions":244,"binary":false,"changes":579,"status":"modified"}]}