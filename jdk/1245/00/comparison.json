{"files":[{"patch":"@@ -267,0 +267,1 @@\n+    DIVWU_OPCODE  = (31u << OPCODE_SHIFT | 459u << 1),\n@@ -524,0 +525,1 @@\n+    LXVL_OPCODE    = (31u << OPCODE_SHIFT |  269u << 1),\n@@ -525,0 +527,1 @@\n+    STXVL_OPCODE   = (31u << OPCODE_SHIFT |  397u << 1),\n@@ -528,0 +531,1 @@\n+    MTVSRDD_OPCODE = (31u << OPCODE_SHIFT |  435u << 1),\n@@ -1343,0 +1347,2 @@\n+  inline void divwu(  Register d, Register a, Register b);\n+  inline void divwu_( Register d, Register a, Register b);\n@@ -2257,0 +2263,2 @@\n+  inline void lxvl(     VectorSRegister d, Register a, Register b);\n+  inline void stxvl(    VectorSRegister d, Register a, Register b);\n@@ -2271,0 +2279,1 @@\n+  inline void mtvsrdd(  VectorSRegister d, Register a, Register b);\n","filename":"src\/hotspot\/cpu\/ppc\/assembler_ppc.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -130,0 +130,2 @@\n+inline void Assembler::divwu(  Register d, Register a, Register b) { emit_int32(DIVWU_OPCODE  | rt(d) | ra(a) | rb(b) | oe(0) | rc(0)); }\n+inline void Assembler::divwu_( Register d, Register a, Register b) { emit_int32(DIVWU_OPCODE  | rt(d) | ra(a) | rb(b) | oe(0) | rc(1)); }\n@@ -784,0 +786,2 @@\n+inline void Assembler::lxvl(    VectorSRegister d, Register s1, Register b)  { emit_int32( LXVL_OPCODE    | vsrt(d) | ra0mem(s1) | rb(b)); }\n+inline void Assembler::stxvl(   VectorSRegister d, Register s1, Register b)  { emit_int32( STXVL_OPCODE   | vsrt(d) | ra0mem(s1) | rb(b)); }\n@@ -789,0 +793,1 @@\n+inline void Assembler::mtvsrdd( VectorSRegister d, Register a, Register b)   { emit_int32( MTVSRDD_OPCODE | vsrt(d)  | ra(a) | rb(b)); }\n","filename":"src\/hotspot\/cpu\/ppc\/assembler_ppc.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -4019,0 +4019,527 @@\n+  void do_lxvl(VectorRegister vreg_ret, Register addr, Register length, Register gpr_tmp1, VectorRegister vreg_tmp1, VectorRegister vreg_tmp2) {\n+     Label no_clamp, lxvl_done, skip_second_load, no_load, done;\n+\n+     VectorRegister vreg_lo = vreg_tmp1;\n+     VectorRegister vreg_hi = vreg_tmp2;\n+     VectorRegister vreg_shift_amt = vreg_tmp1;\n+     VectorRegister vreg_mask = vreg_tmp2;\n+\n+     if (PowerArchitecturePPC64 >= 10) {\n+       \/\/ Shift the length into the upper 8 bits of tmp1 for lxvl\n+       __ rldicr(gpr_tmp1, length, 56, 7);\n+       __ lxvl(vreg_ret->to_vsr(), addr, gpr_tmp1);\n+     } else {\n+       \/\/ On P9, lxvl has performance problems in about 68% of the alignment\n+       \/\/ vs. length cases.  In those cases where it's slow, it's about\n+       \/\/ 5.4X slower.  So on P9, we replace lxvl with a conditional\n+       \/\/ unaligned load sequence, based on the alignment of the address\n+       \/\/ and the length of the data requested.\n+       \/\/\n+       \/\/ If the length is zero or negative, don't read anything.\n+       __ cmpdi(CCR7, length, 0);\n+       __ ble_predict_not_taken(CCR7, no_load);\n+\n+       \/\/ For an exact emulation of lxvl, we should clamp the length at 16,\n+       \/\/ but we know that this code is only used when the length is 16 or less.\n+\n+       \/\/ Do the first read\n+       __ lvx(vreg_hi, addr);\n+       \/\/ set the permute reg\n+       __ lvsr(vreg_ret, addr);\n+\n+       \/\/ max length of data readable for one lvx instruction = 16 - (addr & 0xf)\n+       \/\/\n+       \/\/ Note: When the second load isn't needed, skipping it is NOT\n+       \/\/ optional.  Reading past a page boundary into an inaccessible page\n+       \/\/ will cause a seg fault.\n+       \/\/\n+       __ andi_(gpr_tmp1, addr, 0xf);\n+       __ subfic(gpr_tmp1, gpr_tmp1, 16);\n+       __ cmpd(CCR7, length, gpr_tmp1);\n+       __ ble_predict_taken(CCR7, skip_second_load);\n+\n+       \/\/ Load at addr + 16 to get the next quadword\n+       __ li(gpr_tmp1, 16);\n+       __ lvx(vreg_lo, gpr_tmp1, addr);\n+\n+       __ bind(skip_second_load);\n+       __ vperm(vreg_ret, vreg_lo, vreg_hi, vreg_ret);\n+\n+       \/\/ Zero out the remaining bytes\n+       __ subfic(gpr_tmp1, length, 16);\n+       \/\/ The goal of the following two instructions is to get the shift amount\n+       \/\/ into bits 121:124 of vreg_shift_amt\n+       __ sldi(gpr_tmp1, gpr_tmp1, 3);\n+       __ mtvsrdd(vreg_shift_amt->to_vsr(), gpr_tmp1, gpr_tmp1);\n+       __ xxspltib(vreg_mask->to_vsr(), 0xff);\n+       __ vsro(vreg_mask, vreg_mask, vreg_shift_amt);\n+       __ vand(vreg_ret, vreg_ret, vreg_mask);\n+       __ b(done);\n+\n+       __ bind(no_load);\n+       \/\/ Zero out all of the bytes\n+       __ xxspltib(vreg_ret->to_vsr(), 0);\n+       __ bind(done);\n+\n+     }\n+  }\n+\n+\/\/ This algorithm is based on the methods described in this paper:\n+\/\/ http:\/\/0x80.pl\/notesen\/2016-01-12-sse-base64-encoding.html\n+\/\/\n+\/\/ The details of this implementation vary from the paper due to the\n+\/\/ difference in the ISA between SSE and AltiVec, especially in the\n+\/\/ splitting bytes section where there is no need on Power to mask after\n+\/\/ the shift because the shift is byte-wise rather than an entire an entire\n+\/\/ 128-bit word.\n+\/\/\n+\/\/ The \"Branchless code for lookup table\" algorithm uses the \"Single pshufb\n+\/\/ method\" described in the paper.\n+\/\/\n+\/\/\n+\/\/ Description of the ENCODE_CORE macro\n+\/\/\n+\/\/ Expand first 12 x 8-bit data bytes into 16 x 6-bit bytes (upper 2\n+\/\/ bits of each byte are zeros)\n+\/\/\n+\/\/ (Note: e7..e0 are not shown because they follow the same pattern as\n+\/\/ e8..e15)\n+\/\/\n+\/\/ In the table below, b0, b1, .. b15 are the bytes of unencoded\n+\/\/ binary data, the first line of each of the cells (except for\n+\/\/ the constants) uses the bit-field nomenclature from the\n+\/\/ above-linked paper, whereas the second line is more specific\n+\/\/ about which exact bits are present, and is constructed using the\n+\/\/ Power ISA 3.x document style, where:\n+\/\/\n+\/\/ * The specifier after the colon depicts which bits are there.\n+\/\/ * The bit numbering is big endian style (bit 0 is the most\n+\/\/   significant).\n+\/\/ * || is a concatenate operator.\n+\/\/ * Strings of 0's are a field of zeros with the shown length, and\n+\/\/   likewise for strings of 1's.\n+\/\/\n+\/\/ +==========================+=============+======================+======================+=============+=============+======================+======================+=============+\n+\/\/ |          Vector          |     e8      |          e9          |         e10          |     e11     |     e12     |         e13          |         e14          |     e15     |\n+\/\/ |         Element          |             |                      |                      |             |             |                      |                      |             |\n+\/\/ +==========================+=============+======================+======================+=============+=============+======================+======================+=============+\n+\/\/ |        after lxv         |  jjjjkkkk   |       iiiiiijj       |       gghhhhhh       |  ffffgggg   |  eeeeeeff   |       ccdddddd       |       bbbbcccc       |  aaaaaabb   |\n+\/\/ |                          |     b7      |          b6          |          b5          |     b4      |     b3      |          b2          |          b1          |     b0      |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |      xxperm indexes      |      0      |          10          |          11          |     12      |      0      |          13          |          14          |     15      |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |     (1) after xxperm     |             |       gghhhhhh       |       ffffgggg       |  eeeeeeff   |             |       ccdddddd       |       bbbbcccc       |  aaaaaabb   |\n+\/\/ |                          |    (b15)    |          b5          |          b4          |     b3      |    (b15)    |          b2          |          b1          |     b0      |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |      rshift_amount       |      0      |          6           |          4           |      2      |      0      |          6           |          4           |      2      |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |        after vsrb        |             |       000000gg       |       0000ffff       |  00eeeeee   |             |       000000cc       |       0000bbbb       |  00aaaaaa   |\n+\/\/ |                          |    (b15)    |   000000||b5:0..1    |    0000||b4:0..3     | 00||b3:0..5 |    (b15)    |   000000||b2:0..1    |    0000||b1:0..3     | 00||b0:0..5 |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |       rshift_mask        |  00000000   |      000000||11      |      0000||1111      | 00||111111  |  00000000   |      000000||11      |      0000||1111      | 00||111111  |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |    rshift after vand     |  00000000   |       000000gg       |       0000ffff       |  00eeeeee   |  00000000   |       000000cc       |       0000bbbb       |  00aaaaaa   |\n+\/\/ |                          |  00000000   |   000000||b5:0..1    |    0000||b4:0..3     | 00||b3:0..5 |  00000000   |   000000||b2:0..1    |    0000||b1:0..3     | 00||b0:0..5 |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |    1 octet lshift (1)    |  gghhhhhh   |       ffffgggg       |       eeeeeeff       |             |  ccdddddd   |       bbbbcccc       |       aaaaaabb       |  00000000   |\n+\/\/ |                          |     b5      |          b4          |          b3          |    (b15)    |     b2      |          b1          |          b0          |  00000000   |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |      lshift_amount       |      0      |          2           |          4           |      0      |      0      |          2           |          4           |      0      |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |        after vslb        |  gghhhhhh   |       ffgggg00       |       eeff0000       |             |  ccdddddd   |       bbcccc00       |       aabb0000       |  00000000   |\n+\/\/ |                          |     b5      |     b4:2..7||00      |    b3:4..7||0000     |    (b15)    |   b2:0..7   |     b1:2..7||00      |    b0:4..7||0000     |  00000000   |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |       lshift_mask        | 00||111111  |     00||1111||00     |     00||11||0000     |  00000000   | 00||111111  |     00||1111||00     |     00||11||0000     |  00000000   |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |    lshift after vand     |  00hhhhhh   |       00gggg00       |       00ff0000       |  00000000   |  00dddddd   |       00cccc00       |       00bb0000       |  00000000   |\n+\/\/ |                          | 00||b5:2..7 |   00||b4:4..7||00    |  00||b3:6..7||0000   |  00000000   | 00||b2:2..7 |   00||b1:4..7||00    |  00||b0:6..7||0000   |  00000000   |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ | after vor lshift, rshift |  00hhhhhh   |       00gggggg       |       00ffffff       |  00eeeeee   |  00dddddd   |       00cccccc       |       00bbbbbb       |  00aaaaaa   |\n+\/\/ |                          | 00||b5:2..7 | 00||b4:4..7||b5:0..1 | 00||b3:6..7||b4:0..3 | 00||b3:0..5 | 00||b2:2..7 | 00||b1:4..7||b2:0..1 | 00||b0:6..7||b1:0..3 | 00||b0:0..5 |\n+\/\/ +==========================+=============+======================+======================+=============+=============+======================+======================+=============+\n+\/\/\n+\/\/ Expand the first 12 bytes into 16 bytes, leaving every 4th byte\n+\/\/ blank for now.\n+\/\/ __ xxperm(input->to_vsr(), input->to_vsr(), expand_permute);\n+\/\/\n+\/\/ Generate two bit-shifted pieces - rshift and lshift - that will\n+\/\/ later be OR'd together.\n+\/\/\n+\/\/ First the right-shifted piece\n+\/\/ __ vsrb(rshift, input, expand_rshift);\n+\/\/ __ vand(rshift, rshift, expand_rshift_mask);\n+\/\/\n+\/\/ Now the left-shifted piece, which is done by octet shifting\n+\/\/ the input one byte to the left, then doing a variable shift,\n+\/\/ followed by a mask operation.\n+\/\/\n+\/\/ __ vslo(lshift, input, vec_8s);\n+\/\/ __ vslb(lshift, lshift, expand_lshift);\n+\/\/ __ vand(lshift, lshift, expand_lshift_mask);\n+\/\/\n+\/\/ Combine the two pieces by OR'ing\n+\/\/ __ vor(expanded, rshift, lshift);\n+\/\/\n+\/\/ This is the goal:\n+\/\/ +=============+=======+=======================+=======================+\n+\/\/ | 6-bit value | index |  offset (isURL == 0)  |  offset (isURL == 1)  |\n+\/\/ +=============+=======+=======================+=======================+\n+\/\/ |    0..25    |  13   | ord('A') + value - 0  |         same          |\n+\/\/ +-------------+-------+-----------------------+-----------------------+\n+\/\/ |   26..51    |   0   | ord('a') + value - 26 |         same          |\n+\/\/ +-------------+-------+-----------------------+-----------------------+\n+\/\/ |   52..61    | 1..10 | ord('0') + value - 52 |         same          |\n+\/\/ +-------------+-------+-----------------------+-----------------------+\n+\/\/ |     62      |  11   | ord('+') + value - 62 | ord('-') + value - 62 |\n+\/\/ +-------------+-------+-----------------------+-----------------------+\n+\/\/ |     63      |  12   | ord('\/') + value - 63 | ord('_') + value - 63 |\n+\/\/ +=============+=======+=======================+=======================+\n+\/\/\n+\/\/ Do a saturated subtract of 51's from expanded to produce an\n+\/\/ index into the offset table.  Any value less than 52 gets an\n+\/\/ index of zero for now.\n+\/\/ __ vsububs(indexes, expanded, vec_51s);\n+\/\/\n+\/\/ Distinguish between A-Z and a-z\n+\/\/ For those values that are between 0 and 25, set their index to 13\n+\/\/ __ vcmpgtub(is_A_to_Z, vec_26s, expanded);\n+\/\/\n+\/\/ All bytes in input that are between 0 and 25 have their\n+\/\/ corresponding byte in is_A_to_Z set to 0xff, while others are\n+\/\/ set to zero.  AND'ing with 0x13's sets all 0xff bytes to 0x13,\n+\/\/ and leaves the zeroed bytes alone.\n+\/\/ __ vand(is_A_to_Z, is_A_to_Z, vec_13s);\n+\/\/ __ vor(indexes, indexes, is_A_to_Z);\n+\/\/\n+\/\/ indexes is now fully initialized.  Use it to select the offsets.\n+\/\/ __ xxperm(offsets->to_vsr(), offsetLUT->to_vsr(), indexes->to_vsr());\n+\/\/\n+\/\/ Now simply add the offsets to expanded\n+\/\/ __ vaddubm(expanded, expanded, offsets);\n+\n+#define ENCODE_CORE                                                       \\\n+    __ xxperm(input->to_vsr(), input->to_vsr(), expand_permute);          \\\n+    __ vsrb(rshift, input, expand_rshift);                                \\\n+    __ vand(rshift, rshift, expand_rshift_mask);                          \\\n+    __ vslo(lshift, input, vec_8s);                                       \\\n+    __ vslb(lshift, lshift, expand_lshift);                               \\\n+    __ vand(lshift, lshift, expand_lshift_mask);                          \\\n+    __ vor(expanded, rshift, lshift);                                     \\\n+    __ vsububs(indexes, expanded, vec_51s);                               \\\n+    __ vcmpgtub(is_A_to_Z, vec_26s, expanded);                            \\\n+    __ vand(is_A_to_Z, is_A_to_Z, vec_13s);                               \\\n+    __ vor(indexes, indexes, is_A_to_Z);                                  \\\n+    __ xxperm(offsets->to_vsr(), offsetLUT->to_vsr(), indexes->to_vsr()); \\\n+    __ vaddubm(expanded, expanded, offsets);\n+\n+\/\/ Intrinsic function prototype in Base64.java:\n+\/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL) {\n+\n+  address generate_base64_encodeBlock() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"base64_encodeBlock\");\n+    address start   = __ function_entry();\n+\n+    static const __vector unsigned char expand_permute_val = {\n+      ARRAY_TO_LXV_ORDER(\n+      0,  4,  5,  6,\n+      0,  7,  8,  9,\n+      0, 10, 11, 12,\n+      0, 13, 14, 15 ) };\n+\n+    static const __vector unsigned char expand_rshift_val = {\n+      ARRAY_TO_LXV_ORDER(\n+      0, 6, 4, 2,\n+      0, 6, 4, 2,\n+      0, 6, 4, 2,\n+      0, 6, 4, 2 ) };\n+\n+    static const __vector unsigned char expand_rshift_mask_val = {\n+      ARRAY_TO_LXV_ORDER(\n+      0b00000000, 0b00000011, 0b00001111, 0b00111111,\n+      0b00000000, 0b00000011, 0b00001111, 0b00111111,\n+      0b00000000, 0b00000011, 0b00001111, 0b00111111,\n+      0b00000000, 0b00000011, 0b00001111, 0b00111111 ) };\n+\n+    static const __vector unsigned char expand_lshift_val = {\n+      ARRAY_TO_LXV_ORDER(\n+      0, 2, 4, 0,\n+      0, 2, 4, 0,\n+      0, 2, 4, 0,\n+      0, 2, 4, 0 ) };\n+\n+    static const __vector unsigned char expand_lshift_mask_val = {\n+      ARRAY_TO_LXV_ORDER(\n+      0b00111111, 0b00111100, 0b00110000, 0b00000000,\n+      0b00111111, 0b00111100, 0b00110000, 0b00000000,\n+      0b00111111, 0b00111100, 0b00110000, 0b00000000,\n+      0b00111111, 0b00111100, 0b00110000, 0b00000000 ) };\n+\n+    static const __vector unsigned char offsetLUT_val = {\n+      ARRAY_TO_LXV_ORDER(\n+      \/\/ 0\n+      (unsigned char)('a' - 26),\n+      \/\/ 1 .. 10\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      \/\/ 11\n+      (unsigned char)('+' - 62),\n+      \/\/ 12\n+      (unsigned char)('\/' - 63),\n+      \/\/ 13\n+      (unsigned char)('A' - 0),\n+      \/\/ 14 .. 15 (unused)\n+      0, 0 ) };\n+\n+    static const __vector unsigned char offsetLUT_URL_val = {\n+      ARRAY_TO_LXV_ORDER(\n+      \/\/ 0\n+      (unsigned char)('a' - 26),\n+      \/\/ 1 .. 10\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      (unsigned char)('0' - 52),\n+      \/\/ 11\n+      (unsigned char)('-' - 62),\n+      \/\/ 12\n+      (unsigned char)('_' - 63),\n+      \/\/ 13\n+      (unsigned char)('A' - 0),\n+      \/\/ 14 .. 15 (unused)\n+      0, 0 ) };\n+\n+    \/\/ Number of bytes to process in each pass through the main loop.\n+    \/\/ 12 of the 16 bytes from each lxv are encoded to 16 Base64 bytes.\n+    const unsigned block_size = 12;\n+\n+    \/\/ According to the ELF V2 ABI, registers r3-r12 are volatile and available for use without save\/restore\n+    Register src       = R3_ARG1; \/\/ source starting address of Base64 characters\n+    Register sp        = R4_ARG2; \/\/ source starting position\n+    Register sl        = R5_ARG3; \/\/ total source length of the Base64 characters to be processed\n+    Register dst       = R6_ARG4; \/\/ destination address\n+    Register dp        = R7_ARG5; \/\/ destination starting position\n+    Register isURL     = R8_ARG6; \/\/ boolean, if non-zero indicates use of RFC 4648 base64url encoding\n+\n+    \/\/ Local variables\n+    Register const_ptr     = R12; \/\/ used for loading constants (reuses isURL's register)\n+    Register tmp_reg       = R9;  \/\/ used for speeding up load_constant()\n+\n+    Register size           = R9;  \/\/ number of bytes to process (reuses tmp_reg's register)\n+    Register blocked_size   = R10; \/\/ number of bytes to process a block at a time\n+    Register block_modulo   = R12; \/\/ == block_size (reuse const_ptr)\n+    Register remaining      = R12; \/\/ bytes remaining to process after the blocks are completed (reuse block_modulo's reg)\n+    Register in             = R4;  \/\/ current input (source) pointer (reuse sp's register)\n+    Register num_blocks     = R11; \/\/ number of blocks to be processed by the unrolled loop\n+    Register out            = R8;  \/\/ current output (destination) pointer (reuse const_ptr's register)\n+    Register three          = R9;  \/\/ constant divisor (reuse size's register)\n+    Register bytes_to_write = R10; \/\/ number of bytes to write with the stxvl instr (reused blocked_size's register)\n+    Register lxvl_gpr_tmp1  = R7;  \/\/ temp register for do_lxvl (reuse dp's register)\n+    Register tmp1           = R7;  \/\/ gpr tmp (reuse lxvl_gpr_tmp1's register)\n+    Register modulo_chars   = R7;  \/\/ number of bytes written during the final write % 4 (reuse tmp1's register)\n+    Register pad_char       = R6;  \/\/ literal '=' (reuse dst's register)\n+\n+    \/\/ Volatile VSRS are 0..13, 32..51 (VR0..VR13)\n+    \/\/ VR Constants\n+    VectorRegister  vec_8s             = VR0;\n+    VectorRegister  vec_13s            = VR1;\n+    VectorRegister  vec_26s            = VR2;\n+    VectorRegister  vec_51s            = VR3;\n+    VectorRegister  expand_rshift      = VR4;\n+    VectorRegister  expand_rshift_mask = VR5;\n+    VectorRegister  expand_lshift      = VR6;\n+    VectorRegister  expand_lshift_mask = VR7;\n+    VectorRegister  offsetLUT          = VR8;\n+\n+    \/\/ VR variables for expand\n+    VectorRegister  input              = VR9;\n+    VectorRegister  rshift             = VR10;\n+    VectorRegister  lshift             = VR11;\n+    VectorRegister  expanded           = VR12;\n+\n+    \/\/ VR variables for lookup\n+    VectorRegister  indexes            = VR10; \/\/ reuse rshift's register\n+    VectorRegister  is_A_to_Z          = VR11; \/\/ reuse lshift's register\n+    VectorRegister  offsets            = VR13; \/\/ reuse lshift's register\n+\n+    \/\/ VR variables for lxvl emulation\n+    VectorRegister  lxvl_vreg_tmp1     = VR10; \/\/ reuse index's register\n+    VectorRegister  lxvl_vreg_tmp2     = VR11; \/\/ reuse is_A_to_Z's register\n+\n+    \/\/ VSR Constants\n+    VectorSRegister expand_permute     = VSR0;\n+\n+    Label not_URL, calculate_size, calculate_blocked_size, skip_loop;\n+    Label loop_start, le_16_to_write, no_pad, one_pad_char;\n+\n+    \/\/ The upper 32 bits of the non-pointer parameter registers are not\n+    \/\/ guaranteed to be zero, so mask off those upper bits.\n+    __ clrldi(sp, sp, 32);\n+    __ clrldi(sl, sl, 32);\n+    __ clrldi(dp, dp, 32);\n+    __ clrldi(isURL, isURL, 32);\n+\n+    \/\/ load up the constants\n+    __ load_const(const_ptr, (address)&expand_permute_val, tmp_reg);\n+    __ lxv(expand_permute, 0, const_ptr);\n+    __ load_const(const_ptr, (address)&expand_rshift_val, tmp_reg);\n+    __ lxv(expand_rshift->to_vsr(), 0, const_ptr);\n+    __ load_const(const_ptr, (address)&expand_rshift_mask_val, tmp_reg);\n+    __ lxv(expand_rshift_mask->to_vsr(), 0, const_ptr);\n+    __ load_const(const_ptr, (address)&expand_lshift_val, tmp_reg);\n+    __ lxv(expand_lshift->to_vsr(), 0, const_ptr);\n+    __ load_const(const_ptr, (address)&expand_lshift_mask_val, tmp_reg);\n+    __ lxv(expand_lshift_mask->to_vsr(), 0, const_ptr);\n+\n+    \/\/ Splat the constants that can use xxspltib\n+    __ xxspltib(vec_8s->to_vsr(), 8);\n+    __ xxspltib(vec_13s->to_vsr(), 13);\n+    __ xxspltib(vec_26s->to_vsr(), 26);\n+    __ xxspltib(vec_51s->to_vsr(), 51);\n+\n+    \/\/ Use a different offset lookup table depending on the\n+    \/\/ setting of isURL\n+    __ cmpdi(CCR0, isURL, 0);\n+    __ beq(CCR0, not_URL);\n+    __ load_const(const_ptr, (address)&offsetLUT_URL_val, tmp_reg);\n+    __ lxv(offsetLUT->to_vsr(), 0, const_ptr);\n+    __ b(calculate_size);\n+\n+    __ bind(not_URL);\n+    __ load_const(const_ptr, (address)&offsetLUT_val, tmp_reg);\n+    __ lxv(offsetLUT->to_vsr(), 0, const_ptr);\n+\n+    __ bind(calculate_size);\n+\n+    \/\/ size = sl - sp - 4 (*)\n+    \/\/ (*) Don't process the last four bytes in the main loop because\n+    \/\/ we don't want the lxv instruction to read past the end of the src\n+    \/\/ data, in case those four bytes are on the start of an unmapped or\n+    \/\/ otherwise inaccessible page.\n+    \/\/\n+    __ sub(size, sl, sp);\n+    __ subi(size, size, 4);\n+    __ cmpdi(CCR7, size, block_size);\n+    __ bgt(CCR7, calculate_blocked_size);\n+    __ mr(remaining, size);\n+    \/\/ Add the 4 back into remaining again\n+    __ addi(remaining, remaining, 4);\n+    \/\/ make \"in\" point to the beginning of the source data: in = src + sp\n+    __ add(in, src, sp);\n+    \/\/ out = dst + dp\n+    __ add(out, dst, dp);\n+    __ b(skip_loop);\n+\n+    __ bind(calculate_blocked_size);\n+    __ li(block_modulo, block_size);\n+    \/\/ num_blocks = size \/ block_modulo\n+    __ divwu(num_blocks, size, block_modulo);\n+    \/\/ blocked_size = num_blocks * size\n+    __ mullw(blocked_size, num_blocks, block_modulo);\n+    \/\/ remaining = size - blocked_size\n+    __ sub(remaining, size, blocked_size);\n+    __ mtctr(num_blocks);\n+\n+    \/\/ Add the 4 back in to remaining again\n+    __ addi(remaining, remaining, 4);\n+\n+    \/\/ make \"in\" point to the beginning of the source data: in = src + sp\n+    __ add(in, src, sp);\n+\n+    \/\/ out = dst + dp\n+    __ add(out, dst, dp);\n+\n+    __ align(32);\n+    __ bind(loop_start);\n+\n+    __ lxv(input->to_vsr(), 0, in);\n+\n+    ENCODE_CORE\n+\n+    __ stxv(expanded->to_vsr(), 0, out);\n+    __ addi(in, in, 12);\n+    __ addi(out, out, 16);\n+    __ bdnz(loop_start);\n+\n+    __ bind(skip_loop);\n+\n+    \/\/ When there are less than 16 bytes left, we need to be careful not to\n+    \/\/ read beyond the end of the src buffer, which might be in an unmapped\n+    \/\/ page.\n+    \/\/ Load the remaining bytes using lxvl.\n+    do_lxvl(input, in, remaining, lxvl_gpr_tmp1, lxvl_vreg_tmp1, lxvl_vreg_tmp2);\n+\n+    ENCODE_CORE\n+\n+    \/\/ bytes_to_write = ((remaining * 4) + 2) \/ 3\n+    __ li(three, 3);\n+    __ rlwinm(bytes_to_write, remaining, 2, 0, 29); \/\/ remaining * 4\n+    __ addi(bytes_to_write, bytes_to_write, 2);\n+    __ divwu(bytes_to_write, bytes_to_write, three);\n+\n+    __ cmpwi(CCR7, bytes_to_write, 16);\n+    __ ble_predict_taken(CCR7, le_16_to_write);\n+    __ stxv(expanded->to_vsr(), 0, out);\n+\n+    \/\/ We've processed 12 of the 13-15 data bytes, so advance the pointers,\n+    \/\/ and do one final pass for the remaining 1-3 bytes.\n+    __ addi(in, in, 12);\n+    __ addi(out, out, 16);\n+    __ subi(remaining, remaining, 12);\n+    __ subi(bytes_to_write, bytes_to_write, 16);\n+    do_lxvl(input, in, remaining, lxvl_gpr_tmp1, lxvl_vreg_tmp1, lxvl_vreg_tmp2);\n+\n+    ENCODE_CORE\n+\n+    __ bind(le_16_to_write);\n+    \/\/ shift bytes_to_write into the upper 8 bits of t1 for use by stxvl\n+    __ rldicr(tmp1, bytes_to_write, 56, 7);\n+    __ stxvl(expanded->to_vsr(), out, tmp1);\n+    __ add(out, out, bytes_to_write);\n+\n+    __ li(pad_char, '=');\n+    __ rlwinm_(modulo_chars, bytes_to_write, 0, 30, 31); \/\/ bytes_to_write % 4, set CCR0\n+    \/\/ Examples:\n+    \/\/    remaining  bytes_to_write  modulo_chars  num pad chars\n+    \/\/        0            0               0            0\n+    \/\/        1            2               2            2\n+    \/\/        2            3               3            1\n+    \/\/        3            4               0            0\n+    \/\/        4            6               2            2\n+    \/\/        5            7               3            1\n+    \/\/        ...\n+    \/\/       12           16               0            0\n+    \/\/       13           18               2            2\n+    \/\/       14           19               3            1\n+    \/\/       15           20               0            0\n+    __ beq(CCR0, no_pad);\n+    __ cmpwi(CCR7, modulo_chars, 3);\n+    __ beq(CCR7, one_pad_char);\n+\n+    \/\/ two pad chars\n+    __ stb(pad_char, out);\n+    __ addi(out, out, 1);\n+\n+    __ bind(one_pad_char);\n+    __ stb(pad_char, out);\n+\n+    __ bind(no_pad);\n+\n+    __ blr();\n+    return start;\n+  }\n+\n@@ -4124,0 +4651,1 @@\n+      StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":528,"deletions":0,"binary":false,"changes":528,"status":"modified"}]}