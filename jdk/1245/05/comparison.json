{"files":[{"patch":"@@ -267,0 +267,1 @@\n+    DIVWU_OPCODE  = (31u << OPCODE_SHIFT | 459u << 1),\n@@ -524,0 +525,1 @@\n+    LXVL_OPCODE    = (31u << OPCODE_SHIFT |  269u << 1),\n@@ -525,0 +527,1 @@\n+    STXVL_OPCODE   = (31u << OPCODE_SHIFT |  397u << 1),\n@@ -528,0 +531,1 @@\n+    MTVSRDD_OPCODE = (31u << OPCODE_SHIFT |  435u << 1),\n@@ -1343,0 +1347,2 @@\n+  inline void divwu(  Register d, Register a, Register b);\n+  inline void divwu_( Register d, Register a, Register b);\n@@ -2257,0 +2263,2 @@\n+  inline void lxvl(     VectorSRegister d, Register a, Register b);\n+  inline void stxvl(    VectorSRegister d, Register a, Register b);\n@@ -2271,0 +2279,1 @@\n+  inline void mtvsrdd(  VectorSRegister d, Register a, Register b);\n","filename":"src\/hotspot\/cpu\/ppc\/assembler_ppc.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -130,0 +130,2 @@\n+inline void Assembler::divwu(  Register d, Register a, Register b) { emit_int32(DIVWU_OPCODE  | rt(d) | ra(a) | rb(b) | oe(0) | rc(0)); }\n+inline void Assembler::divwu_( Register d, Register a, Register b) { emit_int32(DIVWU_OPCODE  | rt(d) | ra(a) | rb(b) | oe(0) | rc(1)); }\n@@ -784,0 +786,2 @@\n+inline void Assembler::lxvl(    VectorSRegister d, Register s1, Register b)  { emit_int32( LXVL_OPCODE    | vsrt(d) | ra0mem(s1) | rb(b)); }\n+inline void Assembler::stxvl(   VectorSRegister d, Register s1, Register b)  { emit_int32( STXVL_OPCODE   | vsrt(d) | ra0mem(s1) | rb(b)); }\n@@ -789,0 +793,1 @@\n+inline void Assembler::mtvsrdd( VectorSRegister d, Register a, Register b)   { emit_int32( MTVSRDD_OPCODE | vsrt(d)  | ra(a) | rb(b)); }\n","filename":"src\/hotspot\/cpu\/ppc\/assembler_ppc.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -4019,0 +4019,416 @@\n+\/\/ This algorithm is based on the methods described in this paper:\n+\/\/ http:\/\/0x80.pl\/notesen\/2016-01-12-sse-base64-encoding.html\n+\/\/\n+\/\/ The details of this implementation vary from the paper due to the\n+\/\/ difference in the ISA between SSE and AltiVec, especially in the\n+\/\/ splitting bytes section where there is no need on Power to mask after\n+\/\/ the shift because the shift is byte-wise rather than an entire an entire\n+\/\/ 128-bit word.\n+\/\/\n+\/\/ For the lookup part of the algorithm, different logic is used than\n+\/\/ described in the paper because of the availability of vperm, which can\n+\/\/ do a 64-byte table lookup in four instructions, while preserving the\n+\/\/ branchless nature.\n+\/\/\n+\/\/ Description of the ENCODE_CORE macro\n+\/\/\n+\/\/ Expand first 12 x 8-bit data bytes into 16 x 6-bit bytes (upper 2\n+\/\/ bits of each byte are zeros)\n+\/\/\n+\/\/ (Note: e7..e0 are not shown because they follow the same pattern as\n+\/\/ e8..e15)\n+\/\/\n+\/\/ In the table below, b0, b1, .. b15 are the bytes of unencoded\n+\/\/ binary data, the first line of each of the cells (except for\n+\/\/ the constants) uses the bit-field nomenclature from the\n+\/\/ above-linked paper, whereas the second line is more specific\n+\/\/ about which exact bits are present, and is constructed using the\n+\/\/ Power ISA 3.x document style, where:\n+\/\/\n+\/\/ * The specifier after the colon depicts which bits are there.\n+\/\/ * The bit numbering is big endian style (bit 0 is the most\n+\/\/   significant).\n+\/\/ * || is a concatenate operator.\n+\/\/ * Strings of 0's are a field of zeros with the shown length, and\n+\/\/   likewise for strings of 1's.\n+\/\/\n+\/\/ +==========================+=============+======================+======================+=============+=============+======================+======================+=============+\n+\/\/ |          Vector          |     e8      |          e9          |         e10          |     e11     |     e12     |         e13          |         e14          |     e15     |\n+\/\/ |         Element          |             |                      |                      |             |             |                      |                      |             |\n+\/\/ +==========================+=============+======================+======================+=============+=============+======================+======================+=============+\n+\/\/ |        after lxv         |  jjjjkkkk   |       iiiiiijj       |       gghhhhhh       |  ffffgggg   |  eeeeeeff   |       ccdddddd       |       bbbbcccc       |  aaaaaabb   |\n+\/\/ |                          |     b7      |          b6          |          b5          |     b4      |     b3      |          b2          |          b1          |     b0      |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |      xxperm indexes      |      0      |          10          |          11          |     12      |      0      |          13          |          14          |     15      |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |     (1) after xxperm     |             |       gghhhhhh       |       ffffgggg       |  eeeeeeff   |             |       ccdddddd       |       bbbbcccc       |  aaaaaabb   |\n+\/\/ |                          |    (b15)    |          b5          |          b4          |     b3      |    (b15)    |          b2          |          b1          |     b0      |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |      rshift_amount       |      0      |          6           |          4           |      2      |      0      |          6           |          4           |      2      |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |        after vsrb        |             |       000000gg       |       0000ffff       |  00eeeeee   |             |       000000cc       |       0000bbbb       |  00aaaaaa   |\n+\/\/ |                          |    (b15)    |   000000||b5:0..1    |    0000||b4:0..3     | 00||b3:0..5 |    (b15)    |   000000||b2:0..1    |    0000||b1:0..3     | 00||b0:0..5 |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |       rshift_mask        |  00000000   |      000000||11      |      0000||1111      | 00||111111  |  00000000   |      000000||11      |      0000||1111      | 00||111111  |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |    rshift after vand     |  00000000   |       000000gg       |       0000ffff       |  00eeeeee   |  00000000   |       000000cc       |       0000bbbb       |  00aaaaaa   |\n+\/\/ |                          |  00000000   |   000000||b5:0..1    |    0000||b4:0..3     | 00||b3:0..5 |  00000000   |   000000||b2:0..1    |    0000||b1:0..3     | 00||b0:0..5 |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |    1 octet lshift (1)    |  gghhhhhh   |       ffffgggg       |       eeeeeeff       |             |  ccdddddd   |       bbbbcccc       |       aaaaaabb       |  00000000   |\n+\/\/ |                          |     b5      |          b4          |          b3          |    (b15)    |     b2      |          b1          |          b0          |  00000000   |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |      lshift_amount       |      0      |          2           |          4           |      0      |      0      |          2           |          4           |      0      |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |        after vslb        |  gghhhhhh   |       ffgggg00       |       eeff0000       |             |  ccdddddd   |       bbcccc00       |       aabb0000       |  00000000   |\n+\/\/ |                          |     b5      |     b4:2..7||00      |    b3:4..7||0000     |    (b15)    |   b2:0..7   |     b1:2..7||00      |    b0:4..7||0000     |  00000000   |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |       lshift_mask        | 00||111111  |     00||1111||00     |     00||11||0000     |  00000000   | 00||111111  |     00||1111||00     |     00||11||0000     |  00000000   |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ |    lshift after vand     |  00hhhhhh   |       00gggg00       |       00ff0000       |  00000000   |  00dddddd   |       00cccc00       |       00bb0000       |  00000000   |\n+\/\/ |                          | 00||b5:2..7 |   00||b4:4..7||00    |  00||b3:6..7||0000   |  00000000   | 00||b2:2..7 |   00||b1:4..7||00    |  00||b0:6..7||0000   |  00000000   |\n+\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n+\/\/ | after vor lshift, rshift |  00hhhhhh   |       00gggggg       |       00ffffff       |  00eeeeee   |  00dddddd   |       00cccccc       |       00bbbbbb       |  00aaaaaa   |\n+\/\/ |                          | 00||b5:2..7 | 00||b4:4..7||b5:0..1 | 00||b3:6..7||b4:0..3 | 00||b3:0..5 | 00||b2:2..7 | 00||b1:4..7||b2:0..1 | 00||b0:6..7||b1:0..3 | 00||b0:0..5 |\n+\/\/ +==========================+=============+======================+======================+=============+=============+======================+======================+=============+\n+\/\/\n+\/\/ Expand the first 12 bytes into 16 bytes, leaving every 4th byte\n+\/\/ blank for now.\n+\/\/ __ xxperm(input->to_vsr(), input->to_vsr(), expand_permute);\n+\/\/\n+\/\/ Generate two bit-shifted pieces - rshift and lshift - that will\n+\/\/ later be OR'd together.\n+\/\/\n+\/\/ First the right-shifted piece\n+\/\/ __ vsrb(rshift, input, expand_rshift);\n+\/\/ __ vand(rshift, rshift, expand_rshift_mask);\n+\/\/\n+\/\/ Now the left-shifted piece, which is done by octet shifting\n+\/\/ the input one byte to the left, then doing a variable shift,\n+\/\/ followed by a mask operation.\n+\/\/\n+\/\/ __ vslo(lshift, input, vec_8s);\n+\/\/ __ vslb(lshift, lshift, expand_lshift);\n+\/\/ __ vand(lshift, lshift, expand_lshift_mask);\n+\/\/\n+\/\/ Combine the two pieces by OR'ing\n+\/\/ __ vor(expanded, rshift, lshift);\n+\/\/\n+\/\/ At this point, expanded is a vector containing a 6-bit value in each\n+\/\/ byte.  These values are used as indexes into a 64-byte lookup table that\n+\/\/ is contained in four vector registers.  The lookup operation is done\n+\/\/ using vperm instructions with the same indexes for the lower 32 and\n+\/\/ upper 32 bytes.  To figure out which of the two looked-up bytes to use\n+\/\/ at each location, all values in expanded are compared to 31.  Using\n+\/\/ vsel, values higher than 31 use the results from the upper 32 bytes of\n+\/\/ the lookup operation, while values less than or equal to 31 use the\n+\/\/ lower 32 bytes of the lookup operation.  Power10 and beyond can save the\n+\/\/ compare instruction, because the comparison is done within xxpermx\n+\/\/ itself. TODO: use xxpermx,xxpermx,vor on P10 when instruction prefixes are\n+\/\/ available in assembler_ppc.*\n+\n+#define ENCODE_CORE                                                        \\\n+    __ xxperm(input->to_vsr(), input->to_vsr(), expand_permute);           \\\n+    __ vsrb(rshift, input, expand_rshift);                                 \\\n+    __ vand(rshift, rshift, expand_rshift_mask);                           \\\n+    __ vslo(lshift, input, vec_8s);                                        \\\n+    __ vslb(lshift, lshift, expand_lshift);                                \\\n+    __ vand(lshift, lshift, expand_lshift_mask);                           \\\n+    __ vor(expanded, rshift, lshift);                                      \\\n+    __ vperm(encoded_00_31, vec_base64_00_15, vec_base64_16_31, expanded); \\\n+    __ vperm(encoded_32_63, vec_base64_32_47, vec_base64_48_63, expanded); \\\n+    __ vcmpgtub(gt_31, expanded, vec_31s);                                 \\\n+    __ vsel(expanded, encoded_00_31, encoded_32_63, gt_31);\n+\n+\/\/ Intrinsic function prototype in Base64.java:\n+\/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL) {\n+\n+  address generate_base64_encodeBlock() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"base64_encodeBlock\");\n+    address start   = __ function_entry();\n+\n+    typedef struct {\n+      unsigned char expand_permute_val[16];\n+      unsigned char expand_rshift_val[16];\n+      unsigned char expand_rshift_mask_val[16];\n+      unsigned char expand_lshift_val[16];\n+      unsigned char expand_lshift_mask_val[16];\n+      unsigned char base64_00_15_val[16];\n+      unsigned char base64_16_31_val[16];\n+      unsigned char base64_32_47_val[16];\n+      unsigned char base64_48_63_val[16];\n+      unsigned char base64_48_63_URL_val[16];\n+    } constant_block;\n+\n+    static const constant_block VEC_ALIGN const_block = {\n+      .expand_permute_val = {\n+        ARRAY_TO_LXV_ORDER(\n+        0,  4,  5,  6,\n+        0,  7,  8,  9,\n+        0, 10, 11, 12,\n+        0, 13, 14, 15 ) },\n+\n+      .expand_rshift_val = {\n+        ARRAY_TO_LXV_ORDER(\n+        0, 6, 4, 2,\n+        0, 6, 4, 2,\n+        0, 6, 4, 2,\n+        0, 6, 4, 2 ) },\n+\n+      .expand_rshift_mask_val = {\n+        ARRAY_TO_LXV_ORDER(\n+        0b00000000, 0b00000011, 0b00001111, 0b00111111,\n+        0b00000000, 0b00000011, 0b00001111, 0b00111111,\n+        0b00000000, 0b00000011, 0b00001111, 0b00111111,\n+        0b00000000, 0b00000011, 0b00001111, 0b00111111 ) },\n+\n+      .expand_lshift_val = {\n+        ARRAY_TO_LXV_ORDER(\n+        0, 2, 4, 0,\n+        0, 2, 4, 0,\n+        0, 2, 4, 0,\n+        0, 2, 4, 0 ) },\n+\n+      .expand_lshift_mask_val = {\n+        ARRAY_TO_LXV_ORDER(\n+        0b00111111, 0b00111100, 0b00110000, 0b00000000,\n+        0b00111111, 0b00111100, 0b00110000, 0b00000000,\n+        0b00111111, 0b00111100, 0b00110000, 0b00000000,\n+        0b00111111, 0b00111100, 0b00110000, 0b00000000 ) },\n+\n+      .base64_00_15_val = {\n+        ARRAY_TO_LXV_ORDER(\n+        'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P' ) },\n+\n+      .base64_16_31_val = {\n+        ARRAY_TO_LXV_ORDER(\n+        'Q','R','S','T','U','V','W','X','Y','Z','a','b','c','d','e','f' ) },\n+\n+      .base64_32_47_val = {\n+        ARRAY_TO_LXV_ORDER(\n+        'g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v' ) },\n+\n+      .base64_48_63_val = {\n+        ARRAY_TO_LXV_ORDER(\n+        'w','x','y','z','0','1','2','3','4','5','6','7','8','9','+','\/' ) },\n+\n+      .base64_48_63_URL_val = {\n+        ARRAY_TO_LXV_ORDER(\n+        'w','x','y','z','0','1','2','3','4','5','6','7','8','9','-','_' ) }\n+    };\n+    #define BLK_OFFSETOF(x) (offsetof(constant_block, x))\n+\n+    \/\/ Number of bytes to process in each pass through the main loop.\n+    \/\/ 12 of the 16 bytes from each lxv are encoded to 16 Base64 bytes.\n+    const unsigned block_size = 12;\n+\n+    \/\/ According to the ELF V2 ABI, registers r3-r12 are volatile and available for use without save\/restore\n+    Register src       = R3_ARG1; \/\/ source starting address of Base64 characters\n+    Register sp        = R4_ARG2; \/\/ source starting position\n+    Register sl        = R5_ARG3; \/\/ total source length of the Base64 characters to be processed\n+    Register dst       = R6_ARG4; \/\/ destination address\n+    Register dp        = R7_ARG5; \/\/ destination starting position\n+    Register isURL     = R8_ARG6; \/\/ boolean, if non-zero indicates use of RFC 4648 base64url encoding\n+\n+    \/\/ Local variables\n+    Register const_ptr     = R12; \/\/ used for loading constants (reuses isURL's register)\n+    Register tmp_reg       = R9;  \/\/ used for speeding up load_constant()\n+\n+    Register size           = R9;  \/\/ number of bytes to process (reuses tmp_reg's register)\n+    Register blocked_size   = R10; \/\/ number of bytes to process a block at a time\n+    Register block_modulo   = R12; \/\/ == block_size (reuse const_ptr)\n+    Register remaining      = R12; \/\/ bytes remaining to process after the blocks are completed (reuse block_modulo's reg)\n+    Register in             = R4;  \/\/ current input (source) pointer (reuse sp's register)\n+    Register num_blocks     = R11; \/\/ number of blocks to be processed by the unrolled loop\n+    Register out            = R8;  \/\/ current output (destination) pointer (reuse const_ptr's register)\n+    Register three          = R9;  \/\/ constant divisor (reuse size's register)\n+    Register bytes_to_write = R10; \/\/ number of bytes to write with the stxvl instr (reused blocked_size's register)\n+    Register tmp1           = R7;  \/\/ temp register for lxvl length (reuse dp's register)\n+    Register modulo_chars   = R7;  \/\/ number of bytes written during the final write % 4 (reuse tmp1's register)\n+    Register pad_char       = R6;  \/\/ literal '=' (reuse dst's register)\n+\n+    \/\/ Volatile VSRS are 0..13, 32..51 (VR0..VR13)\n+    \/\/ VR Constants\n+    VectorRegister  vec_8s             = VR0;\n+    VectorRegister  vec_31s            = VR1;\n+    VectorRegister  vec_base64_00_15   = VR2;\n+    VectorRegister  vec_base64_16_31   = VR3;\n+    VectorRegister  vec_base64_32_47   = VR4;\n+    VectorRegister  vec_base64_48_63   = VR5;\n+    VectorRegister  expand_rshift      = VR6;\n+    VectorRegister  expand_rshift_mask = VR7;\n+    VectorRegister  expand_lshift      = VR8;\n+    VectorRegister  expand_lshift_mask = VR9;\n+\n+    \/\/ VR variables for expand\n+    VectorRegister  input              = VR10;\n+    VectorRegister  rshift             = VR11;\n+    VectorRegister  lshift             = VR12;\n+    VectorRegister  expanded           = VR13;\n+\n+    \/\/ VR variables for lookup\n+    VectorRegister  encoded_00_31      = VR10; \/\/ (reuse input)\n+    VectorRegister  encoded_32_63      = VR11; \/\/ (reuse rshift)\n+    VectorRegister  gt_31              = VR12; \/\/ (reuse lshift)\n+\n+    \/\/ VSR Constants\n+    VectorSRegister expand_permute     = VSR0;\n+\n+    Label not_URL, calculate_size, calculate_blocked_size, skip_loop;\n+    Label loop_start, le_16_to_write, no_pad, one_pad_char;\n+\n+    \/\/ The upper 32 bits of the non-pointer parameter registers are not\n+    \/\/ guaranteed to be zero, so mask off those upper bits.\n+    __ clrldi(sp, sp, 32);\n+    __ clrldi(sl, sl, 32);\n+    __ clrldi(dp, dp, 32);\n+    __ clrldi(isURL, isURL, 32);\n+\n+    \/\/ load up the constants\n+    __ load_const_optimized(const_ptr, (address)&const_block, tmp_reg);\n+    __ lxv(expand_permute,               BLK_OFFSETOF(expand_permute_val),     const_ptr);\n+    __ lxv(expand_rshift->to_vsr(),      BLK_OFFSETOF(expand_rshift_val),      const_ptr);\n+    __ lxv(expand_rshift_mask->to_vsr(), BLK_OFFSETOF(expand_rshift_mask_val), const_ptr);\n+    __ lxv(expand_lshift->to_vsr(),      BLK_OFFSETOF(expand_lshift_val),      const_ptr);\n+    __ lxv(expand_lshift_mask->to_vsr(), BLK_OFFSETOF(expand_lshift_mask_val), const_ptr);\n+    __ lxv(vec_base64_00_15->to_vsr(),   BLK_OFFSETOF(base64_00_15_val),       const_ptr);\n+    __ lxv(vec_base64_16_31->to_vsr(),   BLK_OFFSETOF(base64_16_31_val),       const_ptr);\n+    __ lxv(vec_base64_32_47->to_vsr(),   BLK_OFFSETOF(base64_32_47_val),       const_ptr);\n+\n+    \/\/ Splat the constants that can use xxspltib\n+    __ xxspltib(vec_8s->to_vsr(), 8);\n+    __ xxspltib(vec_31s->to_vsr(), 31);\n+\n+\n+    \/\/ Use a different translation lookup table depending on the\n+    \/\/ setting of isURL\n+    __ cmpdi(CCR0, isURL, 0);\n+    __ beq(CCR0, not_URL);\n+    __ lxv(vec_base64_48_63->to_vsr(), BLK_OFFSETOF(base64_48_63_URL_val), const_ptr);\n+    __ b(calculate_size);\n+\n+    __ bind(not_URL);\n+    __ lxv(vec_base64_48_63->to_vsr(), BLK_OFFSETOF(base64_48_63_val), const_ptr);\n+\n+    __ bind(calculate_size);\n+\n+    \/\/ size = sl - sp - 4 (*)\n+    \/\/ (*) Don't process the last four bytes in the main loop because\n+    \/\/ we don't want the lxv instruction to read past the end of the src\n+    \/\/ data, in case those four bytes are on the start of an unmapped or\n+    \/\/ otherwise inaccessible page.\n+    \/\/\n+    __ sub(size, sl, sp);\n+    __ subi(size, size, 4);\n+    __ cmpdi(CCR7, size, block_size);\n+    __ bgt(CCR7, calculate_blocked_size);\n+    __ mr(remaining, size);\n+    \/\/ Add the 4 back into remaining again\n+    __ addi(remaining, remaining, 4);\n+    \/\/ make \"in\" point to the beginning of the source data: in = src + sp\n+    __ add(in, src, sp);\n+    \/\/ out = dst + dp\n+    __ add(out, dst, dp);\n+    __ b(skip_loop);\n+\n+    __ bind(calculate_blocked_size);\n+    __ li(block_modulo, block_size);\n+    \/\/ num_blocks = size \/ block_modulo\n+    __ divwu(num_blocks, size, block_modulo);\n+    \/\/ blocked_size = num_blocks * size\n+    __ mullw(blocked_size, num_blocks, block_modulo);\n+    \/\/ remaining = size - blocked_size\n+    __ sub(remaining, size, blocked_size);\n+    __ mtctr(num_blocks);\n+\n+    \/\/ Add the 4 back in to remaining again\n+    __ addi(remaining, remaining, 4);\n+\n+    \/\/ make \"in\" point to the beginning of the source data: in = src + sp\n+    __ add(in, src, sp);\n+\n+    \/\/ out = dst + dp\n+    __ add(out, dst, dp);\n+\n+    __ align(32);\n+    __ bind(loop_start);\n+\n+    __ lxv(input->to_vsr(), 0, in);\n+\n+    ENCODE_CORE\n+\n+    __ stxv(expanded->to_vsr(), 0, out);\n+    __ addi(in, in, 12);\n+    __ addi(out, out, 16);\n+    __ bdnz(loop_start);\n+\n+    __ bind(skip_loop);\n+\n+    \/\/ When there are less than 16 bytes left, we need to be careful not to\n+    \/\/ read beyond the end of the src buffer, which might be in an unmapped\n+    \/\/ page.\n+    \/\/ Load the remaining bytes using lxvl.\n+    __ rldicr(tmp1, remaining, 56, 7);\n+    __ lxvl(input->to_vsr(), in, tmp1);\n+\n+    ENCODE_CORE\n+\n+    \/\/ bytes_to_write = ((remaining * 4) + 2) \/ 3\n+    __ li(three, 3);\n+    __ rlwinm(bytes_to_write, remaining, 2, 0, 29); \/\/ remaining * 4\n+    __ addi(bytes_to_write, bytes_to_write, 2);\n+    __ divwu(bytes_to_write, bytes_to_write, three);\n+\n+    __ cmpwi(CCR7, bytes_to_write, 16);\n+    __ ble_predict_taken(CCR7, le_16_to_write);\n+    __ stxv(expanded->to_vsr(), 0, out);\n+\n+    \/\/ We've processed 12 of the 13-15 data bytes, so advance the pointers,\n+    \/\/ and do one final pass for the remaining 1-3 bytes.\n+    __ addi(in, in, 12);\n+    __ addi(out, out, 16);\n+    __ subi(remaining, remaining, 12);\n+    __ subi(bytes_to_write, bytes_to_write, 16);\n+    __ rldicr(tmp1, bytes_to_write, 56, 7);\n+    __ lxvl(input->to_vsr(), in, tmp1);\n+\n+    ENCODE_CORE\n+\n+    __ bind(le_16_to_write);\n+    \/\/ shift bytes_to_write into the upper 8 bits of t1 for use by stxvl\n+    __ rldicr(tmp1, bytes_to_write, 56, 7);\n+    __ stxvl(expanded->to_vsr(), out, tmp1);\n+    __ add(out, out, bytes_to_write);\n+\n+    __ li(pad_char, '=');\n+    __ rlwinm_(modulo_chars, bytes_to_write, 0, 30, 31); \/\/ bytes_to_write % 4, set CCR0\n+    \/\/ Examples:\n+    \/\/    remaining  bytes_to_write  modulo_chars  num pad chars\n+    \/\/        0            0               0            0\n+    \/\/        1            2               2            2\n+    \/\/        2            3               3            1\n+    \/\/        3            4               0            0\n+    \/\/        4            6               2            2\n+    \/\/        5            7               3            1\n+    \/\/        ...\n+    \/\/       12           16               0            0\n+    \/\/       13           18               2            2\n+    \/\/       14           19               3            1\n+    \/\/       15           20               0            0\n+    __ beq(CCR0, no_pad);\n+    __ cmpwi(CCR7, modulo_chars, 3);\n+    __ beq(CCR7, one_pad_char);\n+\n+    \/\/ two pad chars\n+    __ stb(pad_char, out);\n+    __ addi(out, out, 1);\n+\n+    __ bind(one_pad_char);\n+    __ stb(pad_char, out);\n+\n+    __ bind(no_pad);\n+\n+    __ blr();\n+    return start;\n+  }\n+\n@@ -4124,0 +4540,1 @@\n+      StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":417,"deletions":0,"binary":false,"changes":417,"status":"modified"}]}