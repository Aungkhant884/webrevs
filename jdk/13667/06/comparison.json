{"files":[{"patch":"@@ -90,0 +90,1 @@\n+  LOG_TAG(heapdump) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+  template(HeapDumpMerge)                         \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -246,4 +246,1 @@\n-    \/\/ Parallel thread number for heap dump, initialize based on active processor count.\n-    \/\/ Note the real number of threads used is also determined by active workers and compression\n-    \/\/ backend thread number. See heapDumper.cpp.\n-    uint parallel_thread_num = MAX2<uint>(1, (uint)os::initial_active_processor_count() * 3 \/ 8);\n+\n@@ -254,1 +251,1 @@\n-    dumper.dump(path, out, (int)level, false, (uint)parallel_thread_num);\n+    dumper.dump(path, out, (int)level, false, HeapDumper::default_num_of_dump_threads());\n","filename":"src\/hotspot\/share\/services\/attachListener.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -473,1 +473,3 @@\n-           \"BOOLEAN\", false, \"false\") {\n+           \"BOOLEAN\", false, \"false\"),\n+  _parallel(\"-parallel\", \"Number of parallel dump thread, it should be less than ParallelGCThread\",\n+            \"INT\", false, \"1\") {\n@@ -478,0 +480,1 @@\n+  _dcmdparser.add_dcmd_option(&_parallel);\n@@ -491,1 +494,0 @@\n-\n@@ -496,1 +498,2 @@\n-  dumper.dump(_filename.value(), output(), (int) level, _overwrite.value());\n+  uint num_dump_thread = _parallel.is_set() ? (uint)_parallel.value() : HeapDumper::default_num_of_dump_threads();\n+  dumper.dump(_filename.value(), output(), (int) level, _overwrite.value(), num_dump_thread);\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -323,0 +323,1 @@\n+  DCmdArgument<jlong> _parallel;\n@@ -324,1 +325,1 @@\n-  static int num_arguments() { return 4; }\n+  static int num_arguments() { return 5; }\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -57,0 +57,1 @@\n+#include \"runtime\/timerTrace.hpp\"\n@@ -383,1 +384,0 @@\n-\/\/ Supports I\/O operations for a dump\n@@ -385,1 +385,1 @@\n-class AbstractDumpWriter : public StackObj {\n+class AbstractDumpWriter : public ResourceObj {\n@@ -389,1 +389,0 @@\n-    io_buffer_max_waste = 10*K,\n@@ -402,2 +401,0 @@\n-  virtual void flush(bool force = false) = 0;\n-\n@@ -423,4 +420,0 @@\n-  \/\/ total number of bytes written to the disk\n-  virtual julong bytes_written() const = 0;\n-  virtual char const* error() const = 0;\n-\n@@ -445,12 +438,7 @@\n-  void finish_dump_segment(bool force_flush = false);\n-  \/\/ Refresh to get new buffer\n-  void refresh() {\n-    assert (_in_dump_segment ==false, \"Sanity check\");\n-    _buffer = nullptr;\n-    _size = io_buffer_max_size;\n-    _pos = 0;\n-    \/\/ Force flush to guarantee data from parallel dumper are written.\n-    flush(true);\n-  }\n-  \/\/ Called when finished to release the threads.\n-  virtual void deactivate() = 0;\n+  void finish_dump_segment();\n+  \/\/ Flush internal buffer to persistent storage\n+  virtual void flush() = 0;\n+  \/\/ Total number of bytes written to the disk\n+  virtual julong bytes_written() const = 0;\n+  \/\/ Return non-null if error occurred\n+  virtual char const* error() const = 0;\n@@ -551,1 +539,1 @@\n-void AbstractDumpWriter::finish_dump_segment(bool force_flush) {\n+void AbstractDumpWriter::finish_dump_segment() {\n@@ -569,1 +557,1 @@\n-    flush(force_flush);\n+    flush();\n@@ -589,0 +577,1 @@\n+    ResourceMark rm;\n@@ -614,21 +603,29 @@\n- private:\n-  CompressionBackend _backend; \/\/ Does the actual writing.\n- protected:\n-  void flush(bool force = false) override;\n-\n- public:\n-  \/\/ Takes ownership of the writer and compressor.\n-  DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor);\n-\n-  \/\/ total number of bytes written to the disk\n-  julong bytes_written() const override { return (julong) _backend.get_written(); }\n-\n-  char const* error() const override    { return _backend.error(); }\n-\n-  \/\/ Called by threads used for parallel writing.\n-  void writer_loop()                    { _backend.thread_loop(); }\n-  \/\/ Called when finish to release the threads.\n-  void deactivate() override            { flush(); _backend.deactivate(); }\n-  \/\/ Get the backend pointer, used by parallel dump writer.\n-  CompressionBackend* backend_ptr()     { return &_backend; }\n-\n+private:\n+ FileWriter* _writer;\n+ AbstractCompressor* _compressor;\n+ size_t _bytes_written;\n+ char* _error;\n+ \/\/ Compression support\n+ char* _out_buffer;\n+ size_t _out_size;\n+ size_t _out_pos;\n+ char* _tmp_buffer;\n+ size_t _tmp_size;\n+\n+private:\n+  void do_compress();\n+\n+public:\n+  DumpWriter(FileWriter* writer, AbstractCompressor* compressor);\n+  ~DumpWriter();\n+  julong bytes_written() const override        { return (julong) _bytes_written; }\n+  void set_bytes_written(julong bytes_written) { _bytes_written = bytes_written; }\n+  char const* error() const override           { return _error; }\n+  void set_error(const char* error)            { _error = (char*)error; }\n+  bool has_error() const                       { return _error != nullptr; }\n+  const char* get_file_path() const            { return _writer->get_file_path(); }\n+  AbstractCompressor* compressor()             { return _compressor; }\n+  void set_compressor(AbstractCompressor* p)   { _compressor = p; }\n+  bool is_overwrite() const                    { return _writer->is_overwrite(); }\n+\n+  void flush() override;\n@@ -637,2 +634,1 @@\n-\/\/ Check for error after constructing the object and destroy it in case of an error.\n-DumpWriter::DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor) :\n+DumpWriter::DumpWriter(FileWriter* writer, AbstractCompressor* compressor) :\n@@ -640,32 +636,22 @@\n-  _backend(writer, compressor, io_buffer_max_size, io_buffer_max_waste) {\n-  flush();\n-}\n-\n-\/\/ flush any buffered bytes to the file\n-void DumpWriter::flush(bool force) {\n-  _backend.get_new_buffer(&_buffer, &_pos, &_size, force);\n-}\n-\n-\/\/ Buffer queue used for parallel dump.\n-struct ParWriterBufferQueueElem {\n-  char* _buffer;\n-  size_t _used;\n-  ParWriterBufferQueueElem* _next;\n-};\n-\n-class ParWriterBufferQueue : public CHeapObj<mtInternal> {\n- private:\n-  ParWriterBufferQueueElem* _head;\n-  ParWriterBufferQueueElem* _tail;\n-  uint _length;\n- public:\n-  ParWriterBufferQueue() : _head(nullptr), _tail(nullptr), _length(0) { }\n-\n-  void enqueue(ParWriterBufferQueueElem* entry) {\n-    if (_head == nullptr) {\n-      assert(is_empty() && _tail == nullptr, \"Sanity check\");\n-      _head = _tail = entry;\n-    } else {\n-      assert ((_tail->_next == nullptr && _tail->_buffer != nullptr), \"Buffer queue is polluted\");\n-      _tail->_next = entry;\n-      _tail = entry;\n+  _writer(writer),\n+  _compressor(compressor),\n+  _bytes_written(0),\n+  _error(nullptr),\n+  _out_buffer(nullptr),\n+  _out_size(0),\n+  _out_pos(0),\n+  _tmp_buffer(nullptr),\n+  _tmp_size(0) {\n+  _error = (char*)_writer->open_writer();\n+  if (_error == nullptr) {\n+    _buffer = (char*)os::malloc(io_buffer_max_size, mtInternal);\n+    if (compressor != nullptr) {\n+      _error = (char*)_compressor->init(io_buffer_max_size, &_out_size, &_tmp_size);\n+      if (_error == nullptr) {\n+        if (_out_size > 0) {\n+          _out_buffer = (char*)os::malloc(_out_size, mtInternal);\n+        }\n+        if (_tmp_size > 0) {\n+          _tmp_buffer = (char*)os::malloc(_tmp_size, mtInternal);\n+        }\n+      }\n@@ -673,2 +659,0 @@\n-    _length++;\n-    assert(_tail->_next == nullptr, \"Buffer queue is polluted\");\n@@ -676,0 +660,4 @@\n+  \/\/ initialize internal buffer\n+  _pos = 0;\n+  _size = io_buffer_max_size;\n+}\n@@ -677,11 +665,3 @@\n-  ParWriterBufferQueueElem* dequeue() {\n-    if (_head == nullptr)  return nullptr;\n-    ParWriterBufferQueueElem* entry = _head;\n-    assert (entry->_buffer != nullptr, \"polluted buffer in writer list\");\n-    _head = entry->_next;\n-    if (_head == nullptr) {\n-      _tail = nullptr;\n-    }\n-    entry->_next = nullptr;\n-    _length--;\n-    return entry;\n+DumpWriter::~DumpWriter(){\n+  if (_buffer != nullptr) {\n+    os::free(_buffer);\n@@ -689,3 +669,2 @@\n-\n-  bool is_empty() {\n-    return _length == 0;\n+  if (_out_buffer != nullptr) {\n+    os::free(_out_buffer);\n@@ -693,32 +672,2 @@\n-\n-  uint length() { return _length; }\n-};\n-\n-\/\/ Support parallel heap dump.\n-class ParDumpWriter : public AbstractDumpWriter {\n- private:\n-  \/\/ Lock used to guarantee the integrity of multiple buffers writing.\n-  static Monitor* _lock;\n-  \/\/ Pointer of backend from global DumpWriter.\n-  CompressionBackend* _backend_ptr;\n-  char const * _err;\n-  ParWriterBufferQueue* _buffer_queue;\n-  size_t _internal_buffer_used;\n-  char* _buffer_base;\n-  bool _split_data;\n-  static const uint BackendFlushThreshold = 2;\n- protected:\n-  void flush(bool force = false) override {\n-    assert(_pos != 0, \"must not be zero\");\n-    if (_pos != 0) {\n-      refresh_buffer();\n-    }\n-\n-    if (_split_data || _is_huge_sub_record) {\n-      return;\n-    }\n-\n-    if (should_flush_buf_list(force)) {\n-      assert(!_in_dump_segment && !_split_data && !_is_huge_sub_record, \"incomplete data send to backend!\\n\");\n-      flush_to_backend(force);\n-    }\n+  if (_tmp_buffer != nullptr) {\n+    os::free(_tmp_buffer);\n@@ -726,0 +675,2 @@\n+  _bytes_written = -1;\n+}\n@@ -727,59 +678,4 @@\n- public:\n-  \/\/ Check for error after constructing the object and destroy it in case of an error.\n-  ParDumpWriter(DumpWriter* dw) :\n-    AbstractDumpWriter(),\n-    _backend_ptr(dw->backend_ptr()),\n-    _buffer_queue((new (std::nothrow) ParWriterBufferQueue())),\n-    _buffer_base(nullptr),\n-    _split_data(false) {\n-    \/\/ prepare internal buffer\n-    allocate_internal_buffer();\n-  }\n-\n-  ~ParDumpWriter() {\n-     assert(_buffer_queue != nullptr, \"Sanity check\");\n-     assert((_internal_buffer_used == 0) && (_buffer_queue->is_empty()),\n-            \"All data must be send to backend\");\n-     if (_buffer_base != nullptr) {\n-       os::free(_buffer_base);\n-       _buffer_base = nullptr;\n-     }\n-     delete _buffer_queue;\n-     _buffer_queue = nullptr;\n-  }\n-\n-  \/\/ total number of bytes written to the disk\n-  julong bytes_written() const override { return (julong) _backend_ptr->get_written(); }\n-  char const* error() const override    { return _err == nullptr ? _backend_ptr->error() : _err; }\n-\n-  static void before_work() {\n-    assert(_lock == nullptr, \"ParDumpWriter lock must be initialized only once\");\n-    _lock = new (std::nothrow) PaddedMonitor(Mutex::safepoint, \"ParallelHProfWriter_lock\");\n-  }\n-\n-  static void after_work() {\n-    assert(_lock != nullptr, \"ParDumpWriter lock is not initialized\");\n-    delete _lock;\n-    _lock = nullptr;\n-  }\n-\n-  \/\/ write raw bytes\n-  void write_raw(const void* s, size_t len) override {\n-    assert(!_in_dump_segment || (_sub_record_left >= len), \"sub-record too large\");\n-    debug_only(_sub_record_left -= len);\n-    assert(!_split_data, \"Invalid split data\");\n-    _split_data = true;\n-    \/\/ flush buffer to make room.\n-    while (len > buffer_size() - position()) {\n-      assert(!_in_dump_segment || _is_huge_sub_record,\n-             \"Cannot overflow in non-huge sub-record.\");\n-      size_t to_write = buffer_size() - position();\n-      memcpy(buffer() + position(), s, to_write);\n-      s = (void*) ((char*) s + to_write);\n-      len -= to_write;\n-      set_position(position() + to_write);\n-      flush();\n-    }\n-    _split_data = false;\n-    memcpy(buffer() + position(), s, len);\n-    set_position(position() + len);\n+\/\/ flush any buffered bytes to the file\n+void DumpWriter::flush() {\n+  if (_pos <= 0) {\n+    return;\n@@ -787,12 +683,1 @@\n-\n-  void deactivate() override { flush(true); _backend_ptr->deactivate(); }\n-\n- private:\n-  void allocate_internal_buffer() {\n-    assert(_buffer_queue != nullptr, \"Internal buffer queue is not ready when allocate internal buffer\");\n-    assert(_buffer == nullptr && _buffer_base == nullptr, \"current buffer must be null before allocate\");\n-    _buffer_base = _buffer = (char*)os::malloc(io_buffer_max_size, mtInternal);\n-    if (_buffer == nullptr) {\n-      set_error(\"Could not allocate buffer for writer\");\n-      return;\n-    }\n+  if (has_error()) {\n@@ -800,8 +685,1 @@\n-    _internal_buffer_used = 0;\n-    _size = io_buffer_max_size;\n-  }\n-\n-  void set_error(char const* new_error) {\n-    if ((new_error != nullptr) && (_err == nullptr)) {\n-      _err = new_error;\n-    }\n+    return;\n@@ -809,23 +687,9 @@\n-\n-  \/\/ Add buffer to internal list\n-  void refresh_buffer() {\n-    size_t expected_total = _internal_buffer_used + _pos;\n-    if (expected_total < io_buffer_max_size - io_buffer_max_waste) {\n-      \/\/ reuse current buffer.\n-      _internal_buffer_used = expected_total;\n-      assert(_size - _pos == io_buffer_max_size - expected_total, \"illegal resize of buffer\");\n-      _size -= _pos;\n-      _buffer += _pos;\n-      _pos = 0;\n-\n-      return;\n-    }\n-    \/\/ It is not possible here that expected_total is larger than io_buffer_max_size because\n-    \/\/ of limitation in write_xxx().\n-    assert(expected_total <= io_buffer_max_size, \"buffer overflow\");\n-    assert(_buffer - _buffer_base <= io_buffer_max_size, \"internal buffer overflow\");\n-    ParWriterBufferQueueElem* entry =\n-        (ParWriterBufferQueueElem*)os::malloc(sizeof(ParWriterBufferQueueElem), mtInternal);\n-    if (entry == nullptr) {\n-      set_error(\"Heap dumper can allocate memory\");\n-      return;\n+  char* result = nullptr;\n+  if (_compressor == nullptr) {\n+    result = (char*)_writer->write_buf(_buffer, _pos);\n+    _bytes_written += _pos;\n+  } else {\n+    do_compress();\n+    if (!has_error()) {\n+      result = (char*)_writer->write_buf(_out_buffer, _out_pos);\n+      _bytes_written += _out_pos;\n@@ -833,7 +697,0 @@\n-    entry->_buffer = _buffer_base;\n-    entry->_used = expected_total;\n-    entry->_next = nullptr;\n-    \/\/ add to internal buffer queue\n-    _buffer_queue->enqueue(entry);\n-    _buffer_base =_buffer = nullptr;\n-    allocate_internal_buffer();\n@@ -841,0 +698,1 @@\n+  _pos = 0; \/\/ reset pos to make internal buffer available\n@@ -842,12 +700,2 @@\n-  void reclaim_entry(ParWriterBufferQueueElem* entry) {\n-    assert(entry != nullptr && entry->_buffer != nullptr, \"Invalid entry to reclaim\");\n-    os::free(entry->_buffer);\n-    entry->_buffer = nullptr;\n-    os::free(entry);\n-  }\n-\n-  void flush_buffer(char* buffer, size_t used) {\n-    assert(_lock->owner() == Thread::current(), \"flush buffer must hold lock\");\n-    size_t max = io_buffer_max_size;\n-    \/\/ get_new_buffer\n-    _backend_ptr->flush_external_buffer(buffer, used, max);\n+  if (result != nullptr) {\n+    set_error(result);\n@@ -855,0 +703,1 @@\n+}\n@@ -856,3 +705,3 @@\n-  bool should_flush_buf_list(bool force) {\n-    return force || _buffer_queue->length() > BackendFlushThreshold;\n-  }\n+void DumpWriter::do_compress() {\n+  const char* msg = _compressor->compress(_buffer, _pos, _out_buffer, _out_size,\n+                                          _tmp_buffer, _tmp_size, &_out_pos);\n@@ -860,21 +709,2 @@\n-  void flush_to_backend(bool force) {\n-    \/\/ Guarantee there is only one writer updating the backend buffers.\n-    MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-    while (!_buffer_queue->is_empty()) {\n-      ParWriterBufferQueueElem* entry = _buffer_queue->dequeue();\n-      flush_buffer(entry->_buffer, entry->_used);\n-      \/\/ Delete buffer and entry.\n-      reclaim_entry(entry);\n-      entry = nullptr;\n-    }\n-    assert(_pos == 0, \"available buffer must be empty before flush\");\n-    \/\/ Flush internal buffer.\n-    if (_internal_buffer_used > 0) {\n-      flush_buffer(_buffer_base, _internal_buffer_used);\n-      os::free(_buffer_base);\n-      _pos = 0;\n-      _internal_buffer_used = 0;\n-      _buffer_base = _buffer = nullptr;\n-      \/\/ Allocate internal buffer for future use.\n-      allocate_internal_buffer();\n-    }\n+  if (msg != nullptr) {\n+    set_error(msg);\n@@ -882,3 +712,1 @@\n-};\n-\n-Monitor* ParDumpWriter::_lock = nullptr;\n+}\n@@ -1616,67 +1444,0 @@\n-\/\/ Large object heap dump support.\n-\/\/ To avoid memory consumption, when dumping large objects such as huge array and\n-\/\/ large objects whose size are larger than LARGE_OBJECT_DUMP_THRESHOLD, the scanned\n-\/\/ partial object\/array data will be sent to the backend directly instead of caching\n-\/\/ the whole object\/array in the internal buffer.\n-\/\/ The HeapDumpLargeObjectList is used to save the large object when dumper scans\n-\/\/ the heap. The large objects could be added (push) parallelly by multiple dumpers,\n-\/\/ But they will be removed (popped) serially only by the VM thread.\n-class HeapDumpLargeObjectList : public CHeapObj<mtInternal> {\n- private:\n-  class HeapDumpLargeObjectListElem : public CHeapObj<mtInternal> {\n-   public:\n-    HeapDumpLargeObjectListElem(oop obj) : _obj(obj), _next(nullptr) { }\n-    oop _obj;\n-    HeapDumpLargeObjectListElem* _next;\n-  };\n-\n-  volatile HeapDumpLargeObjectListElem* _head;\n-\n- public:\n-  HeapDumpLargeObjectList() : _head(nullptr) { }\n-\n-  void atomic_push(oop obj) {\n-    assert (obj != nullptr, \"sanity check\");\n-    HeapDumpLargeObjectListElem* entry = new HeapDumpLargeObjectListElem(obj);\n-    if (entry == nullptr) {\n-      warning(\"failed to allocate element for large object list\");\n-      return;\n-    }\n-    assert (entry->_obj != nullptr, \"sanity check\");\n-    while (true) {\n-      volatile HeapDumpLargeObjectListElem* old_head = Atomic::load_acquire(&_head);\n-      HeapDumpLargeObjectListElem* new_head = entry;\n-      if (Atomic::cmpxchg(&_head, old_head, new_head) == old_head) {\n-        \/\/ successfully push\n-        new_head->_next = (HeapDumpLargeObjectListElem*)old_head;\n-        return;\n-      }\n-    }\n-  }\n-\n-  oop pop() {\n-    if (_head == nullptr) {\n-      return nullptr;\n-    }\n-    HeapDumpLargeObjectListElem* entry = (HeapDumpLargeObjectListElem*)_head;\n-    _head = _head->_next;\n-    assert (entry != nullptr, \"illegal larger object list entry\");\n-    oop ret = entry->_obj;\n-    delete entry;\n-    assert (ret != nullptr, \"illegal oop pointer\");\n-    return ret;\n-  }\n-\n-  void drain(ObjectClosure* cl) {\n-    while (_head !=  nullptr) {\n-      cl->do_object(pop());\n-    }\n-  }\n-\n-  bool is_empty() {\n-    return _head == nullptr;\n-  }\n-\n-  static const size_t LargeObjectSizeThreshold = 1 << 20; \/\/ 1 MB\n-};\n-\n@@ -1689,2 +1450,0 @@\n-  HeapDumpLargeObjectList* _list;\n-\n@@ -1692,1 +1451,1 @@\n-  bool is_large(oop o);\n+\n@@ -1694,1 +1453,1 @@\n-  HeapObjectDumper(AbstractDumpWriter* writer, HeapDumpLargeObjectList* list = nullptr) {\n+  HeapObjectDumper(AbstractDumpWriter* writer) {\n@@ -1696,1 +1455,0 @@\n-    _list = list;\n@@ -1716,7 +1474,0 @@\n-  \/\/ If large object list exists and it is large object\/array,\n-  \/\/ add oop into the list and skip scan. VM thread will process it later.\n-  if (_list != nullptr && is_large(o)) {\n-    _list->atomic_push(o);\n-    return;\n-  }\n-\n@@ -1735,23 +1486,0 @@\n-bool HeapObjectDumper::is_large(oop o) {\n-  size_t size = 0;\n-  if (o->is_instance()) {\n-    \/\/ Use o->size() * 8 as the upper limit of instance size to avoid iterating static fields\n-    size = o->size() * 8;\n-  } else if (o->is_objArray()) {\n-    objArrayOop array = objArrayOop(o);\n-    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n-    assert(type >= T_BOOLEAN && type <= T_OBJECT, \"invalid array element type\");\n-    int length = array->length();\n-    int type_size = sizeof(address);\n-    size = (size_t)length * type_size;\n-  } else if (o->is_typeArray()) {\n-    typeArrayOop array = typeArrayOop(o);\n-    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n-    assert(type >= T_BOOLEAN && type <= T_OBJECT, \"invalid array element type\");\n-    int length = array->length();\n-    int type_size = type2aelembytes(type);\n-    size = (size_t)length * type_size;\n-  }\n-  return size > HeapDumpLargeObjectList::LargeObjectSizeThreshold;\n-}\n-\n@@ -1760,5 +1488,4 @@\n- private:\n-   bool     _started;\n-   Monitor* _lock;\n-   uint   _dumper_number;\n-   uint   _complete_number;\n+private:\n+  Monitor* _lock;\n+  uint   _dumper_number;\n+  uint   _complete_number;\n@@ -1766,39 +1493,42 @@\n- public:\n-   DumperController(uint number) :\n-     _started(false),\n-     _lock(new (std::nothrow) PaddedMonitor(Mutex::safepoint, \"DumperController_lock\")),\n-     _dumper_number(number),\n-     _complete_number(0) { }\n-\n-   ~DumperController() { delete _lock; }\n-\n-   void wait_for_start_signal() {\n-     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-     while (_started == false) {\n-       ml.wait();\n-     }\n-     assert(_started == true,  \"dumper woke up with wrong state\");\n-   }\n-\n-   void start_dump() {\n-     assert (_started == false, \"start dump with wrong state\");\n-     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-     _started = true;\n-     ml.notify_all();\n-   }\n-\n-   void dumper_complete() {\n-     assert (_started == true, \"dumper complete with wrong state\");\n-     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-     _complete_number++;\n-     ml.notify();\n-   }\n-\n-   void wait_all_dumpers_complete() {\n-     assert (_started == true, \"wrong state when wait for dumper complete\");\n-     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-     while (_complete_number != _dumper_number) {\n-        ml.wait();\n-     }\n-     _started = false;\n-   }\n+public:\n+  DumperController(uint number) :\n+    _lock(new (std::nothrow) PaddedMonitor(Mutex::safepoint, \"DumperController_lock\")),\n+    _dumper_number(number),\n+    _complete_number(0) { }\n+\n+  ~DumperController() { delete _lock; }\n+\n+  void dumper_complete(DumpWriter* local_writer, DumpWriter* global_writer) {\n+    MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n+    _complete_number++;\n+    \/\/ propagate local error to global if any\n+    if (local_writer->has_error()) {\n+    global_writer->set_error(local_writer->error());\n+    }\n+    ml.notify();\n+  }\n+\n+  void wait_all_dumpers_complete() {\n+    MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n+    while (_complete_number != _dumper_number) {\n+      ml.wait();\n+    }\n+  }\n+};\n+\n+\/\/ Merge segmented dump files into a complete one\n+class VM_HeapDumpMerge : public VM_Operation {\n+private:\n+  DumpWriter* _writer;\n+  const char* _path;\n+  bool _has_error;\n+\n+  void merge_file(char* path);\n+  void merge_done();\n+public:\n+  VM_HeapDumpMerge(const char* path, DumpWriter* writer)\n+    : _writer(writer), _path(path), _has_error(_writer->has_error()) {}\n+  VMOp_Type type() const { return VMOp_HeapDumpMerge; }\n+  \/\/ heap dump merge could happen outside safepoint\n+  virtual bool evaluate_at_safepoint() const { return false; }\n+  void doit();\n@@ -1821,1 +1551,0 @@\n-  uint                    _num_writer_threads;\n@@ -1824,6 +1553,0 @@\n-  HeapDumpLargeObjectList* _large_object_list;\n-\n-  \/\/ VMDumperType is for thread that dumps both heap and non-heap data.\n-  static const size_t VMDumperType = 0;\n-  static const size_t WriterType = 1;\n-  static const size_t DumperType = 2;\n@@ -1832,46 +1555,3 @@\n-\n-  size_t get_worker_type(uint worker_id) {\n-    assert(_num_writer_threads >= 1, \"Must be at least one writer\");\n-    \/\/ worker id of VMDumper that dump heap and non-heap data\n-    if (worker_id == VMDumperWorkerId) {\n-      return VMDumperType;\n-    }\n-\n-    \/\/ worker id of dumper starts from 1, which only dump heap datar\n-    if (worker_id < _num_dumper_threads) {\n-      return DumperType;\n-    }\n-\n-    \/\/ worker id of writer starts from _num_dumper_threads\n-    return WriterType;\n-  }\n-\n-  void prepare_parallel_dump(uint num_total) {\n-    assert (_dumper_controller == nullptr, \"dumper controller must be null\");\n-    assert (num_total > 0, \"active workers number must >= 1\");\n-    \/\/ Dumper threads number must not be larger than active workers number.\n-    if (num_total < _num_dumper_threads) {\n-      _num_dumper_threads = num_total - 1;\n-    }\n-    \/\/ Calculate dumper and writer threads number.\n-    _num_writer_threads = num_total - _num_dumper_threads;\n-    \/\/ If dumper threads number is 1, only the VMThread works as a dumper.\n-    \/\/ If dumper threads number is equal to active workers, need at lest one worker thread as writer.\n-    if (_num_dumper_threads > 0 && _num_writer_threads == 0) {\n-      _num_writer_threads = 1;\n-      _num_dumper_threads = num_total - _num_writer_threads;\n-    }\n-    \/\/ Prepare parallel writer.\n-    if (_num_dumper_threads > 1) {\n-      ParDumpWriter::before_work();\n-      \/\/ Number of dumper threads that only iterate heap.\n-      uint _heap_only_dumper_threads = _num_dumper_threads - 1 \/* VMDumper thread *\/;\n-      _dumper_controller = new (std::nothrow) DumperController(_heap_only_dumper_threads);\n-    }\n-  }\n-\n-  void finish_parallel_dump() {\n-    if (_num_dumper_threads > 1) {\n-      ParDumpWriter::after_work();\n-    }\n-  }\n+  \/\/ VM dumper dumps both heap and non-heap data, other dumpers dump heap-only data.\n+  static bool is_vm_dumper(uint worker_id) { return worker_id == VMDumperWorkerId; }\n+  static DumpWriter* create_dump_writer();\n@@ -1912,3 +1592,0 @@\n-  \/\/ large objects\n-  void dump_large_objects(ObjectClosure* writer);\n-\n@@ -1930,1 +1607,0 @@\n-    _large_object_list = new (std::nothrow) HeapDumpLargeObjectList();\n@@ -1957,1 +1633,0 @@\n-    delete _large_object_list;\n@@ -1959,1 +1634,10 @@\n-\n+  bool is_parallel_dump()  { return _num_dumper_threads > 1; }\n+  bool can_parallel_dump() {\n+    const char* base_path = writer()->get_file_path();\n+    assert(base_path != nullptr, \"sanity check\");\n+    if ((strlen(base_path) + 7\/*.p\\d\\d\\d\\d\\0*\/) >= JVM_MAXPATHLEN) {\n+      \/\/ no extra path room for separate heap dump files\n+      return false;\n+    }\n+    return true;\n+  }\n@@ -2184,0 +1868,2 @@\n+  uint  num_active_workers = workers != nullptr ? workers->active_workers() : 0;\n+  uint requested_num_dump_thread = _num_dumper_threads;\n@@ -2185,1 +1871,3 @@\n-  if (workers == nullptr) {\n+  if (num_active_workers <= 1 ||         \/\/ serial gc?\n+      requested_num_dump_thread <= 1 ||  \/\/ request serial dump?\n+     !can_parallel_dump()) {             \/\/ can not dump in parallel?\n@@ -2187,2 +1875,1 @@\n-    _num_dumper_threads=1;\n-    _num_writer_threads=1;\n+    _num_dumper_threads = 1;\n@@ -2191,10 +1878,8 @@\n-    prepare_parallel_dump(workers->active_workers());\n-    if (_num_dumper_threads > 1) {\n-      ParallelObjectIterator poi(_num_dumper_threads);\n-      _poi = &poi;\n-      workers->run_task(this);\n-      _poi = nullptr;\n-    } else {\n-      workers->run_task(this);\n-    }\n-    finish_parallel_dump();\n+    \/\/ Use parallel dump otherwise\n+    _num_dumper_threads = clamp(requested_num_dump_thread, 2U, num_active_workers);\n+    uint heap_only_dumper_threads = _num_dumper_threads - 1 \/* VMDumper thread *\/;\n+    _dumper_controller = new (std::nothrow) DumperController(heap_only_dumper_threads);\n+    ParallelObjectIterator poi(_num_dumper_threads);\n+    _poi = &poi;\n+    workers->run_task(this, _num_dumper_threads);\n+    _poi = nullptr;\n@@ -2208,8 +1893,55 @@\n-void VM_HeapDumper::work(uint worker_id) {\n-  if (worker_id != 0) {\n-    if (get_worker_type(worker_id) == WriterType) {\n-      writer()->writer_loop();\n-      return;\n-    }\n-    if (_num_dumper_threads > 1 && get_worker_type(worker_id) == DumperType) {\n-      _dumper_controller->wait_for_start_signal();\n+static int volatile dump_seq = 0;\n+\n+void VM_HeapDumpMerge::merge_done() {\n+  \/\/ Writes the HPROF_HEAP_DUMP_END record.\n+  if (!_has_error) {\n+    DumperSupport::end_of_dump(_writer);\n+    _writer->flush();\n+  }\n+  dump_seq = 0; \/\/reset\n+}\n+\n+void VM_HeapDumpMerge::merge_file(char* path) {\n+  assert(!SafepointSynchronize::is_at_safepoint(), \"merging happens outside safepoint\");\n+  TraceTime timer(\"Merge segmented heap file\", TRACETIME_LOG(Info, heapdump));\n+\n+  fileStream part_fs(path, \"r\");\n+  if (!part_fs.is_open()) {\n+    log_error(heapdump)(\"Can not open segmented heap file %s during merging\", path);\n+    _writer->set_error(\"Can not open segmented heap file during merging\");\n+    _has_error = true;\n+    return;\n+  }\n+\n+  jlong total = 0;\n+  int cnt = 0;\n+  char read_buf[4096];\n+  while ((cnt = part_fs.read(read_buf, 1, 4096)) != 0) {\n+    _writer->write_raw(read_buf, cnt);\n+    total += cnt;\n+  }\n+\n+  _writer->flush();\n+  if (part_fs.fileSize() != total) {\n+    log_error(heapdump)(\"Merged heap dump %s is incomplete\", path);\n+    _writer->set_error(\"Merged heap dump is incomplete\");\n+    _has_error = true;\n+  }\n+}\n+\n+void VM_HeapDumpMerge::doit() {\n+  assert(!SafepointSynchronize::is_at_safepoint(), \"merging happens outside safepoint\");\n+  TraceTime timer(\"Merge heap files complete\", TRACETIME_LOG(Info, heapdump));\n+\n+  \/\/ Since contents in segmented heap file were already zipped, we don't need to zip\n+  \/\/ them again during merging.\n+  AbstractCompressor* saved_compressor = _writer->compressor();\n+  _writer->set_compressor(nullptr);\n+\n+  \/\/ merge segmented heap file and remove it anyway\n+  char path[JVM_MAXPATHLEN];\n+  for (int i = 0; i < dump_seq; i++) {\n+    memset(path, 0, JVM_MAXPATHLEN);\n+    os::snprintf(path, JVM_MAXPATHLEN, \"%s.p%d\", _path, i);\n+    if (!_has_error) {\n+      merge_file(path);\n@@ -2217,2 +1949,25 @@\n-  } else {\n-    \/\/ The worker 0 on all non-heap data dumping and part of heap iteration.\n+    remove(path);\n+  }\n+\n+  \/\/ restore compressor for further use\n+  _writer->set_compressor(saved_compressor);\n+  merge_done();\n+}\n+\n+\/\/ prepare DumpWriter for every parallel dump thread\n+DumpWriter* VM_HeapDumper::create_dump_writer() {\n+  char* path = NEW_RESOURCE_ARRAY(char, JVM_MAXPATHLEN);\n+  memset(path, 0, JVM_MAXPATHLEN);\n+  const char* base_path = writer()->get_file_path();\n+  AbstractCompressor* compressor = writer()->compressor();\n+  int seq = Atomic::fetch_and_add(&dump_seq, 1);\n+  os::snprintf(path, JVM_MAXPATHLEN, \"%s.p%d\", base_path, seq);\n+  FileWriter* file_writer = new (std::nothrow) FileWriter(path, writer()->is_overwrite());\n+  DumpWriter* new_writer = new DumpWriter(file_writer, compressor);\n+  return new_writer;\n+}\n+\n+void VM_HeapDumper::work(uint worker_id) {\n+  \/\/ VM Dumper works on all non-heap data dumping and part of heap iteration.\n+  if (is_vm_dumper(worker_id)) {\n+    TraceTime timer(\"Dump non-objects\", TRACETIME_LOG(Info, heapdump));\n@@ -2262,0 +2017,2 @@\n+\n+  \/\/ Heap iteration.\n@@ -2268,1 +2025,4 @@\n-  if (_num_dumper_threads <= 1) {\n+  if (!is_parallel_dump()) {\n+    assert(worker_id == 0, \"must be\");\n+    \/\/ == Serial dump\n+    TraceTime timer(\"Dump heap objects\", TRACETIME_LOG(Info, heapdump));\n@@ -2271,0 +2031,4 @@\n+    writer()->finish_dump_segment();\n+    \/\/ Writes the HPROF_HEAP_DUMP_END record because merge does not happen in serial dump\n+    DumperSupport::end_of_dump(writer());\n+    writer()->flush();\n@@ -2272,28 +2036,13 @@\n-    assert(get_worker_type(worker_id) == DumperType\n-          || get_worker_type(worker_id) == VMDumperType,\n-          \"must be dumper thread to do heap iteration\");\n-    if (get_worker_type(worker_id) == VMDumperType) {\n-      \/\/ Clear global writer's buffer.\n-      writer()->finish_dump_segment(true);\n-      \/\/ Notify dumpers to start heap iteration.\n-      _dumper_controller->start_dump();\n-    }\n-    \/\/ Heap iteration.\n-    {\n-       ParDumpWriter pw(writer());\n-       {\n-         HeapObjectDumper obj_dumper(&pw, _large_object_list);\n-         _poi->object_iterate(&obj_dumper, worker_id);\n-       }\n-\n-       if (get_worker_type(worker_id) == VMDumperType) {\n-         _dumper_controller->wait_all_dumpers_complete();\n-         \/\/ clear internal buffer;\n-         pw.finish_dump_segment(true);\n-         \/\/ refresh the global_writer's buffer and position;\n-         writer()->refresh();\n-       } else {\n-         pw.finish_dump_segment(true);\n-         _dumper_controller->dumper_complete();\n-         return;\n-       }\n+    \/\/ == Parallel dump\n+    ResourceMark rm;\n+    TraceTime timer(\"Dump heap objects in parallel\", TRACETIME_LOG(Info, heapdump));\n+    DumpWriter* dw = is_vm_dumper(worker_id) ? writer() : create_dump_writer();\n+    HeapObjectDumper obj_dumper(dw);\n+    _poi->object_iterate(&obj_dumper, worker_id);\n+    dw->finish_dump_segment();\n+    dw->flush();\n+    if (is_vm_dumper(worker_id)) {\n+      _dumper_controller->wait_all_dumpers_complete();\n+    } else {\n+      _dumper_controller->dumper_complete(dw, writer());\n+      return;\n@@ -2302,9 +2051,2 @@\n-\n-  assert(get_worker_type(worker_id) == VMDumperType, \"Heap dumper must be VMDumper\");\n-  \/\/ Use writer() rather than ParDumpWriter to avoid memory consumption.\n-  HeapObjectDumper obj_dumper(writer());\n-  dump_large_objects(&obj_dumper);\n-  \/\/ Writes the HPROF_HEAP_DUMP_END record.\n-  DumperSupport::end_of_dump(writer());\n-  \/\/ We are done with writing. Release the worker threads.\n-  writer()->deactivate();\n+  \/\/ At this point, all fragments of the heapdump have been written to separate files.\n+  \/\/ We need to merge them into a complete heapdump and write HPROF_HEAP_DUMP_END at that time.\n@@ -2371,5 +2113,0 @@\n-\/\/ dump the large objects.\n-void VM_HeapDumper::dump_large_objects(ObjectClosure* cl) {\n-  _large_object_list->drain(cl);\n-}\n-\n@@ -2410,1 +2147,1 @@\n-  \/\/ generate the dump\n+  \/\/ generate the segmented heap dump into separate files\n@@ -2412,6 +2149,1 @@\n-  if (Thread::current()->is_VM_thread()) {\n-    assert(SafepointSynchronize::is_at_safepoint(), \"Expected to be called at a safepoint\");\n-    dumper.doit();\n-  } else {\n-    VMThread::execute(&dumper);\n-  }\n+  VMThread::execute(&dumper);\n@@ -2435,0 +2167,7 @@\n+  \/\/ merge segmented dump files into a complete one, this is not required for serial dump\n+  if (dumper.is_parallel_dump()) {\n+    VM_HeapDumpMerge op(path, &writer);\n+    VMThread::execute(&op);\n+    set_error(writer.error());\n+  }\n+\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":291,"deletions":552,"binary":false,"changes":843,"status":"modified"},{"patch":"@@ -32,12 +32,0 @@\n-\/\/ HeapDumper is used to dump the java heap to file in HPROF binary format:\n-\/\/\n-\/\/  { HeapDumper dumper(true \/* full GC before heap dump *\/);\n-\/\/    if (dumper.dump(\"\/export\/java.hprof\")) {\n-\/\/      ResourceMark rm;\n-\/\/      tty->print_cr(\"Dump failed: %s\", dumper.error_as_C_string());\n-\/\/    } else {\n-\/\/      \/\/ dump succeeded\n-\/\/    }\n-\/\/  }\n-\/\/\n-\n@@ -46,0 +34,1 @@\n+\/\/ HeapDumper is used to dump the java heap to file in HPROF binary format:\n@@ -83,0 +72,5 @@\n+\n+  \/\/ Parallel thread number for heap dump, initialize based on active processor count.\n+  static uint default_num_of_dump_threads() {\n+    return MAX2<uint>(1, (uint)os::initial_active_processor_count() * 3 \/ 8);\n+  }\n","filename":"src\/hotspot\/share\/services\/heapDumper.hpp","additions":6,"deletions":12,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright (c) 2023, Alibaba Group Holding Limited. All rights reserved.\n@@ -28,0 +29,1 @@\n+#include \"oops\/klass.inline.hpp\"\n@@ -32,0 +34,1 @@\n+#include \"services\/heapDumper.hpp\"\n@@ -34,1 +37,0 @@\n-\n@@ -143,360 +145,0 @@\n-\n-WorkList::WorkList() {\n-  _head._next = &_head;\n-  _head._prev = &_head;\n-}\n-\n-void WorkList::insert(WriteWork* before, WriteWork* work) {\n-  work->_prev = before;\n-  work->_next = before->_next;\n-  before->_next = work;\n-  work->_next->_prev = work;\n-}\n-\n-WriteWork* WorkList::remove(WriteWork* work) {\n-  if (work != nullptr) {\n-    assert(work->_next != work, \"Invalid next\");\n-    assert(work->_prev != work, \"Invalid prev\");\n-    work->_prev->_next = work->_next;;\n-    work->_next->_prev = work->_prev;\n-    work->_next = nullptr;\n-    work->_prev = nullptr;\n-  }\n-\n-  return work;\n-}\n-\n-void WorkList::add_by_id(WriteWork* work) {\n-  if (is_empty()) {\n-    add_first(work);\n-  } else {\n-    WriteWork* last_curr = &_head;\n-    WriteWork* curr = _head._next;\n-\n-    while (curr->_id < work->_id) {\n-      last_curr = curr;\n-      curr = curr->_next;\n-\n-      if (curr == &_head) {\n-        add_last(work);\n-        return;\n-      }\n-    }\n-\n-    insert(last_curr, work);\n-  }\n-}\n-\n-\n-\n-CompressionBackend::CompressionBackend(AbstractWriter* writer,\n-     AbstractCompressor* compressor, size_t block_size, size_t max_waste) :\n-  _active(false),\n-  _err(nullptr),\n-  _nr_of_threads(0),\n-  _works_created(0),\n-  _work_creation_failed(false),\n-  _id_to_write(0),\n-  _next_id(0),\n-  _in_size(block_size),\n-  _max_waste(max_waste),\n-  _out_size(0),\n-  _tmp_size(0),\n-  _written(0),\n-  _writer(writer),\n-  _compressor(compressor),\n-  _lock(new (std::nothrow) PaddedMonitor(Mutex::nosafepoint, \"HProfCompressionBackend_lock\")) {\n-  if (_writer == nullptr) {\n-    set_error(\"Could not allocate writer\");\n-  } else if (_lock == nullptr) {\n-    set_error(\"Could not allocate lock\");\n-  } else {\n-    set_error(_writer->open_writer());\n-  }\n-\n-  if (_compressor != nullptr) {\n-    set_error(_compressor->init(_in_size, &_out_size, &_tmp_size));\n-  }\n-\n-  _current = allocate_work(_in_size, _out_size, _tmp_size);\n-\n-  if (_current == nullptr) {\n-    set_error(\"Could not allocate memory for buffer\");\n-  }\n-\n-  _active = (_err == nullptr);\n-}\n-\n-CompressionBackend::~CompressionBackend() {\n-  assert(!_active, \"Must not be active by now\");\n-  assert(_nr_of_threads == 0, \"Must have no active threads\");\n-  assert(_to_compress.is_empty() && _finished.is_empty(), \"Still work to do\");\n-\n-  free_work_list(&_unused);\n-  free_work(_current);\n-  assert(_works_created == 0, \"All work must have been freed\");\n-\n-  delete _compressor;\n-  delete _writer;\n-  delete _lock;\n-}\n-\n-void CompressionBackend::flush_buffer(MonitorLocker* ml) {\n-\n-  \/\/ Make sure we write the last partially filled buffer.\n-  if ((_current != nullptr) && (_current->_in_used > 0)) {\n-    _current->_id = _next_id++;\n-    _to_compress.add_last(_current);\n-    _current = nullptr;\n-    ml->notify_all();\n-  }\n-\n-  \/\/ Wait for the threads to drain the compression work list and do some work yourself.\n-  while (!_to_compress.is_empty()) {\n-    do_foreground_work();\n-  }\n-}\n-\n-void CompressionBackend::flush_buffer() {\n-  assert(_active, \"Must be active\");\n-\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-  flush_buffer(&ml);\n-}\n-\n-void CompressionBackend::deactivate() {\n-  assert(_active, \"Must be active\");\n-\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-  flush_buffer(&ml);\n-\n-  _active = false;\n-  ml.notify_all();\n-}\n-\n-void CompressionBackend::thread_loop() {\n-  {\n-    MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-    _nr_of_threads++;\n-  }\n-\n-  WriteWork* work;\n-  while ((work = get_work()) != nullptr) {\n-    do_compress(work);\n-    finish_work(work);\n-  }\n-\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-  _nr_of_threads--;\n-  assert(_nr_of_threads >= 0, \"Too many threads finished\");\n-}\n-\n-void CompressionBackend::set_error(char const* new_error) {\n-  if ((new_error != nullptr) && (_err == nullptr)) {\n-    _err = new_error;\n-  }\n-}\n-\n-WriteWork* CompressionBackend::allocate_work(size_t in_size, size_t out_size,\n-                                             size_t tmp_size) {\n-  WriteWork* result = (WriteWork*) os::malloc(sizeof(WriteWork), mtInternal);\n-\n-  if (result == nullptr) {\n-    _work_creation_failed = true;\n-    return nullptr;\n-  }\n-\n-  _works_created++;\n-  result->_in = (char*) os::malloc(in_size, mtInternal);\n-  result->_in_max = in_size;\n-  result->_in_used = 0;\n-  result->_out = nullptr;\n-  result->_tmp = nullptr;\n-\n-  if (result->_in == nullptr) {\n-    goto fail;\n-  }\n-\n-  if (out_size > 0) {\n-    result->_out = (char*) os::malloc(out_size, mtInternal);\n-    result->_out_used = 0;\n-    result->_out_max = out_size;\n-\n-    if (result->_out == nullptr) {\n-      goto fail;\n-    }\n-  }\n-\n-  if (tmp_size > 0) {\n-    result->_tmp = (char*) os::malloc(tmp_size, mtInternal);\n-    result->_tmp_max = tmp_size;\n-\n-    if (result->_tmp == nullptr) {\n-      goto fail;\n-    }\n-  }\n-\n-  return result;\n-\n-fail:\n-  free_work(result);\n-  _work_creation_failed = true;\n-  return nullptr;\n-}\n-\n-void CompressionBackend::free_work(WriteWork* work) {\n-  if (work != nullptr) {\n-    os::free(work->_in);\n-    os::free(work->_out);\n-    os::free(work->_tmp);\n-    os::free(work);\n-    --_works_created;\n-  }\n-}\n-\n-void CompressionBackend::free_work_list(WorkList* list) {\n-  while (!list->is_empty()) {\n-    free_work(list->remove_first());\n-  }\n-}\n-\n-void CompressionBackend::do_foreground_work() {\n-  assert(!_to_compress.is_empty(), \"Must have work to do\");\n-  assert(_lock->owned_by_self(), \"Must have the lock\");\n-\n-  WriteWork* work = _to_compress.remove_first();\n-  MutexUnlocker mu(_lock, Mutex::_no_safepoint_check_flag);\n-  do_compress(work);\n-  finish_work(work);\n-}\n-\n-WriteWork* CompressionBackend::get_work() {\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-\n-  while (_active && _to_compress.is_empty()) {\n-    ml.wait();\n-  }\n-\n-  return _to_compress.remove_first();\n-}\n-\n-void CompressionBackend::flush_external_buffer(char* buffer, size_t used, size_t max) {\n-  assert(buffer != nullptr && used != 0 && max != 0, \"Invalid data send to compression backend\");\n-  assert(_active == true, \"Backend must be active when flushing external buffer\");\n-  char* buf;\n-  size_t tmp_used = 0;\n-  size_t tmp_max = 0;\n-\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-  \/\/ First try current buffer. Use it if empty.\n-  if (_current->_in_used == 0) {\n-    buf = _current->_in;\n-  } else {\n-    \/\/ If current buffer is not clean, flush it.\n-    MutexUnlocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-    get_new_buffer(&buf, &tmp_used, &tmp_max, true);\n-  }\n-  assert (_current->_in != nullptr && _current->_in_max >= max &&\n-          _current->_in_used == 0, \"Invalid buffer from compression backend\");\n-  \/\/ Copy data to backend buffer.\n-  memcpy(buf, buffer, used);\n-\n-  assert(_current->_in == buf, \"Must be current\");\n-  _current->_in_used += used;\n-}\n-\n-void CompressionBackend::get_new_buffer(char** buffer, size_t* used, size_t* max, bool force_reset) {\n-  if (_active) {\n-    MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-    if (*used > 0 || force_reset) {\n-      _current->_in_used += *used;\n-      \/\/ Check if we do not waste more than _max_waste. If yes, write the buffer.\n-      \/\/ Otherwise return the rest of the buffer as the new buffer.\n-      if (_current->_in_max - _current->_in_used <= _max_waste || force_reset) {\n-        _current->_id = _next_id++;\n-        _to_compress.add_last(_current);\n-        _current = nullptr;\n-        ml.notify_all();\n-      } else {\n-        *buffer = _current->_in + _current->_in_used;\n-        *used = 0;\n-        *max = _current->_in_max - _current->_in_used;\n-        return;\n-      }\n-    }\n-\n-    while ((_current == nullptr) && _unused.is_empty() && _active) {\n-      \/\/ Add more work objects if needed.\n-      if (!_work_creation_failed && (_works_created <= _nr_of_threads)) {\n-        WriteWork* work = allocate_work(_in_size, _out_size, _tmp_size);\n-\n-        if (work != nullptr) {\n-          _unused.add_first(work);\n-        }\n-      } else if (!_to_compress.is_empty() && (_nr_of_threads == 0)) {\n-        do_foreground_work();\n-      } else {\n-        ml.wait();\n-      }\n-    }\n-\n-    if (_current == nullptr) {\n-      _current = _unused.remove_first();\n-    }\n-\n-    if (_current != nullptr) {\n-      _current->_in_used = 0;\n-      _current->_out_used = 0;\n-      *buffer = _current->_in;\n-      *used = 0;\n-      *max = _current->_in_max;\n-\n-      return;\n-    }\n-  }\n-\n-  *buffer = nullptr;\n-  *used = 0;\n-  *max = 0;\n-\n-  return;\n-}\n-\n-void CompressionBackend::do_compress(WriteWork* work) {\n-  if (_compressor != nullptr) {\n-    char const* msg = _compressor->compress(work->_in, work->_in_used, work->_out,\n-                                            work->_out_max,\n-    work->_tmp, _tmp_size, &work->_out_used);\n-\n-    if (msg != nullptr) {\n-      MutexLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-      set_error(msg);\n-    }\n-  }\n-}\n-\n-void CompressionBackend::finish_work(WriteWork* work) {\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-\n-  _finished.add_by_id(work);\n-\n-  \/\/ Write all finished works as far as we can.\n-  while (!_finished.is_empty() && (_finished.first()->_id == _id_to_write)) {\n-    WriteWork* to_write = _finished.remove_first();\n-    size_t size = _compressor == nullptr ? to_write->_in_used : to_write->_out_used;\n-    char* p = _compressor == nullptr ? to_write->_in : to_write->_out;\n-    char const* msg = nullptr;\n-\n-    if (_err == nullptr) {\n-      _written += size;\n-      MutexUnlocker mu(_lock, Mutex::_no_safepoint_check_flag);\n-      msg = _writer->write_buf(p, (ssize_t) size);\n-    }\n-\n-    set_error(msg);\n-    _unused.add_first(to_write);\n-    _id_to_write++;\n-  }\n-\n-  ml.notify_all();\n-}\n","filename":"src\/hotspot\/share\/services\/heapDumperCompression.cpp","additions":3,"deletions":361,"binary":false,"changes":364,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright (c) 2023, Alibaba Group Holding Limited. All rights reserved.\n@@ -77,0 +78,4 @@\n+\n+  const char* get_file_path() { return _path; }\n+\n+  bool is_overwrite() const { return _overwrite; }\n@@ -100,141 +105,0 @@\n-\n-\/\/ The data needed to write a single buffer (and compress it optionally).\n-struct WriteWork {\n-  \/\/ The id of the work.\n-  int64_t _id;\n-\n-  \/\/ The input buffer where the raw data is\n-  char* _in;\n-  size_t _in_used;\n-  size_t _in_max;\n-\n-  \/\/ The output buffer where the compressed data is. Is null when compression is disabled.\n-  char* _out;\n-  size_t _out_used;\n-  size_t _out_max;\n-\n-  \/\/ The temporary space needed for compression. Is null when compression is disabled.\n-  char* _tmp;\n-  size_t _tmp_max;\n-\n-  \/\/ Used to link WriteWorks into lists.\n-  WriteWork* _next;\n-  WriteWork* _prev;\n-};\n-\n-\/\/ A list for works.\n-class WorkList {\n-private:\n-  WriteWork _head;\n-\n-  void insert(WriteWork* before, WriteWork* work);\n-  WriteWork* remove(WriteWork* work);\n-\n-public:\n-  WorkList();\n-\n-  \/\/ Return true if the list is empty.\n-  bool is_empty() { return _head._next == &_head; }\n-\n-  \/\/ Adds to the beginning of the list.\n-  void add_first(WriteWork* work) { insert(&_head, work); }\n-\n-  \/\/ Adds to the end of the list.\n-  void add_last(WriteWork* work) { insert(_head._prev, work); }\n-\n-  \/\/ Adds so the ids are ordered.\n-  void add_by_id(WriteWork* work);\n-\n-  \/\/ Returns the first element.\n-  WriteWork* first() { return is_empty() ? nullptr : _head._next; }\n-\n-  \/\/ Returns the last element.\n-  WriteWork* last() { return is_empty() ? nullptr : _head._prev; }\n-\n-  \/\/ Removes the first element. Returns null if empty.\n-  WriteWork* remove_first() { return remove(first()); }\n-\n-  \/\/ Removes the last element. Returns null if empty.\n-  WriteWork* remove_last() { return remove(first()); }\n-};\n-\n-\n-class Monitor;\n-\n-\/\/ This class is used by the DumpWriter class. It supplies the DumpWriter with\n-\/\/ chunks of memory to write the heap dump data into. When the DumpWriter needs a\n-\/\/ new memory chunk, it calls get_new_buffer(), which commits the old chunk used\n-\/\/ and returns a new chunk. The old chunk is then added to a queue to be compressed\n-\/\/ and then written in the background.\n-class CompressionBackend : StackObj {\n-  bool _active;\n-  char const * _err;\n-\n-  int _nr_of_threads;\n-  int _works_created;\n-  bool _work_creation_failed;\n-\n-  int64_t _id_to_write;\n-  int64_t _next_id;\n-\n-  size_t _in_size;\n-  size_t _max_waste;\n-  size_t _out_size;\n-  size_t _tmp_size;\n-\n-  size_t _written;\n-\n-  AbstractWriter* const _writer;\n-  AbstractCompressor* const _compressor;\n-\n-  Monitor* const _lock;\n-\n-  WriteWork* _current;\n-  WorkList _to_compress;\n-  WorkList _unused;\n-  WorkList _finished;\n-\n-  void set_error(char const* new_error);\n-\n-  WriteWork* allocate_work(size_t in_size, size_t out_size, size_t tmp_size);\n-  void free_work(WriteWork* work);\n-  void free_work_list(WorkList* list);\n-\n-  void do_foreground_work();\n-  WriteWork* get_work();\n-  void do_compress(WriteWork* work);\n-  void finish_work(WriteWork* work);\n-  void flush_buffer(MonitorLocker* ml);\n-\n-public:\n-  \/\/ compressor can be null if no compression is used.\n-  \/\/ Takes ownership of the writer and compressor.\n-  \/\/ block_size is the buffer size of a WriteWork.\n-  \/\/ max_waste is the maximum number of bytes to leave\n-  \/\/ empty in the buffer when it is written.\n-  CompressionBackend(AbstractWriter* writer, AbstractCompressor* compressor,\n-    size_t block_size, size_t max_waste);\n-\n-  ~CompressionBackend();\n-\n-  size_t get_written() const { return _written; }\n-\n-  char const* error() const { return _err; }\n-\n-  \/\/ Sets up an internal buffer, fills with external buffer, and sends to compressor.\n-  void flush_external_buffer(char* buffer, size_t used, size_t max);\n-\n-  \/\/ Commits the old buffer (using the value in *used) and sets up a new one.\n-  void get_new_buffer(char** buffer, size_t* used, size_t* max, bool force_reset = false);\n-\n-  \/\/ The entry point for a worker thread.\n-  void thread_loop();\n-\n-  \/\/ Shuts down the backend, releasing all threads.\n-  void deactivate();\n-\n-  \/\/ Flush all compressed data in buffer to file\n-  void flush_buffer();\n-};\n-\n-\n","filename":"src\/hotspot\/share\/services\/heapDumperCompression.hpp","additions":5,"deletions":141,"binary":false,"changes":146,"status":"modified"}]}