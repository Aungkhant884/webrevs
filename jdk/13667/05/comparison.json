{"files":[{"patch":"@@ -90,0 +90,1 @@\n+  LOG_TAG(heapdump) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+  template(HeapDumpMerge)                         \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -246,4 +246,1 @@\n-    \/\/ Parallel thread number for heap dump, initialize based on active processor count.\n-    \/\/ Note the real number of threads used is also determined by active workers and compression\n-    \/\/ backend thread number. See heapDumper.cpp.\n-    uint parallel_thread_num = MAX2<uint>(1, (uint)os::initial_active_processor_count() * 3 \/ 8);\n+\n@@ -254,1 +251,1 @@\n-    dumper.dump(path, out, (int)level, false, (uint)parallel_thread_num);\n+    dumper.dump(path, out, (int)level, false, HeapDumper::default_num_of_dump_threads());\n","filename":"src\/hotspot\/share\/services\/attachListener.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -473,1 +473,3 @@\n-           \"BOOLEAN\", false, \"false\") {\n+           \"BOOLEAN\", false, \"false\"),\n+  _parallel(\"-parallel\", \"Number of parallel dump thread, it should be less than ParallelGCThread\",\n+            \"INT\", false, \"1\") {\n@@ -478,0 +480,1 @@\n+  _dcmdparser.add_dcmd_option(&_parallel);\n@@ -491,1 +494,0 @@\n-\n@@ -496,1 +498,2 @@\n-  dumper.dump(_filename.value(), output(), (int) level, _overwrite.value());\n+  uint num_dump_thread = _parallel.is_set() ? (uint)_parallel.value() : HeapDumper::default_num_of_dump_threads();\n+  dumper.dump(_filename.value(), output(), (int) level, _overwrite.value(), num_dump_thread);\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -323,0 +323,1 @@\n+  DCmdArgument<jlong> _parallel;\n@@ -324,1 +325,1 @@\n-  static int num_arguments() { return 4; }\n+  static int num_arguments() { return 5; }\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -57,0 +57,1 @@\n+#include \"runtime\/timerTrace.hpp\"\n@@ -63,823 +64,0 @@\n-\/*\n- * HPROF binary format - description copied from:\n- *   src\/share\/demo\/jvmti\/hprof\/hprof_io.c\n- *\n- *\n- *  header    \"JAVA PROFILE 1.0.2\" (0-terminated)\n- *\n- *  u4        size of identifiers. Identifiers are used to represent\n- *            UTF8 strings, objects, stack traces, etc. They usually\n- *            have the same size as host pointers.\n- * u4         high word\n- * u4         low word    number of milliseconds since 0:00 GMT, 1\/1\/70\n- * [record]*  a sequence of records.\n- *\n- *\n- * Record format:\n- *\n- * u1         a TAG denoting the type of the record\n- * u4         number of *microseconds* since the time stamp in the\n- *            header. (wraps around in a little more than an hour)\n- * u4         number of bytes *remaining* in the record. Note that\n- *            this number excludes the tag and the length field itself.\n- * [u1]*      BODY of the record (a sequence of bytes)\n- *\n- *\n- * The following TAGs are supported:\n- *\n- * TAG           BODY       notes\n- *----------------------------------------------------------\n- * HPROF_UTF8               a UTF8-encoded name\n- *\n- *               id         name ID\n- *               [u1]*      UTF8 characters (no trailing zero)\n- *\n- * HPROF_LOAD_CLASS         a newly loaded class\n- *\n- *                u4        class serial number (> 0)\n- *                id        class object ID\n- *                u4        stack trace serial number\n- *                id        class name ID\n- *\n- * HPROF_UNLOAD_CLASS       an unloading class\n- *\n- *                u4        class serial_number\n- *\n- * HPROF_FRAME              a Java stack frame\n- *\n- *                id        stack frame ID\n- *                id        method name ID\n- *                id        method signature ID\n- *                id        source file name ID\n- *                u4        class serial number\n- *                i4        line number. >0: normal\n- *                                       -1: unknown\n- *                                       -2: compiled method\n- *                                       -3: native method\n- *\n- * HPROF_TRACE              a Java stack trace\n- *\n- *               u4         stack trace serial number\n- *               u4         thread serial number\n- *               u4         number of frames\n- *               [id]*      stack frame IDs\n- *\n- *\n- * HPROF_ALLOC_SITES        a set of heap allocation sites, obtained after GC\n- *\n- *               u2         flags 0x0001: incremental vs. complete\n- *                                0x0002: sorted by allocation vs. live\n- *                                0x0004: whether to force a GC\n- *               u4         cutoff ratio\n- *               u4         total live bytes\n- *               u4         total live instances\n- *               u8         total bytes allocated\n- *               u8         total instances allocated\n- *               u4         number of sites that follow\n- *               [u1        is_array: 0:  normal object\n- *                                    2:  object array\n- *                                    4:  boolean array\n- *                                    5:  char array\n- *                                    6:  float array\n- *                                    7:  double array\n- *                                    8:  byte array\n- *                                    9:  short array\n- *                                    10: int array\n- *                                    11: long array\n- *                u4        class serial number (may be zero during startup)\n- *                u4        stack trace serial number\n- *                u4        number of bytes alive\n- *                u4        number of instances alive\n- *                u4        number of bytes allocated\n- *                u4]*      number of instance allocated\n- *\n- * HPROF_START_THREAD       a newly started thread.\n- *\n- *               u4         thread serial number (> 0)\n- *               id         thread object ID\n- *               u4         stack trace serial number\n- *               id         thread name ID\n- *               id         thread group name ID\n- *               id         thread group parent name ID\n- *\n- * HPROF_END_THREAD         a terminating thread.\n- *\n- *               u4         thread serial number\n- *\n- * HPROF_HEAP_SUMMARY       heap summary\n- *\n- *               u4         total live bytes\n- *               u4         total live instances\n- *               u8         total bytes allocated\n- *               u8         total instances allocated\n- *\n- * HPROF_HEAP_DUMP          denote a heap dump\n- *\n- *               [heap dump sub-records]*\n- *\n- *                          There are four kinds of heap dump sub-records:\n- *\n- *               u1         sub-record type\n- *\n- *               HPROF_GC_ROOT_UNKNOWN         unknown root\n- *\n- *                          id         object ID\n- *\n- *               HPROF_GC_ROOT_THREAD_OBJ      thread object\n- *\n- *                          id         thread object ID  (may be 0 for a\n- *                                     thread newly attached through JNI)\n- *                          u4         thread sequence number\n- *                          u4         stack trace sequence number\n- *\n- *               HPROF_GC_ROOT_JNI_GLOBAL      JNI global ref root\n- *\n- *                          id         object ID\n- *                          id         JNI global ref ID\n- *\n- *               HPROF_GC_ROOT_JNI_LOCAL       JNI local ref\n- *\n- *                          id         object ID\n- *                          u4         thread serial number\n- *                          u4         frame # in stack trace (-1 for empty)\n- *\n- *               HPROF_GC_ROOT_JAVA_FRAME      Java stack frame\n- *\n- *                          id         object ID\n- *                          u4         thread serial number\n- *                          u4         frame # in stack trace (-1 for empty)\n- *\n- *               HPROF_GC_ROOT_NATIVE_STACK    Native stack\n- *\n- *                          id         object ID\n- *                          u4         thread serial number\n- *\n- *               HPROF_GC_ROOT_STICKY_CLASS    System class\n- *\n- *                          id         object ID\n- *\n- *               HPROF_GC_ROOT_THREAD_BLOCK    Reference from thread block\n- *\n- *                          id         object ID\n- *                          u4         thread serial number\n- *\n- *               HPROF_GC_ROOT_MONITOR_USED    Busy monitor\n- *\n- *                          id         object ID\n- *\n- *               HPROF_GC_CLASS_DUMP           dump of a class object\n- *\n- *                          id         class object ID\n- *                          u4         stack trace serial number\n- *                          id         super class object ID\n- *                          id         class loader object ID\n- *                          id         signers object ID\n- *                          id         protection domain object ID\n- *                          id         reserved\n- *                          id         reserved\n- *\n- *                          u4         instance size (in bytes)\n- *\n- *                          u2         size of constant pool\n- *                          [u2,       constant pool index,\n- *                           ty,       type\n- *                                     2:  object\n- *                                     4:  boolean\n- *                                     5:  char\n- *                                     6:  float\n- *                                     7:  double\n- *                                     8:  byte\n- *                                     9:  short\n- *                                     10: int\n- *                                     11: long\n- *                           vl]*      and value\n- *\n- *                          u2         number of static fields\n- *                          [id,       static field name,\n- *                           ty,       type,\n- *                           vl]*      and value\n- *\n- *                          u2         number of inst. fields (not inc. super)\n- *                          [id,       instance field name,\n- *                           ty]*      type\n- *\n- *               HPROF_GC_INSTANCE_DUMP        dump of a normal object\n- *\n- *                          id         object ID\n- *                          u4         stack trace serial number\n- *                          id         class object ID\n- *                          u4         number of bytes that follow\n- *                          [vl]*      instance field values (class, followed\n- *                                     by super, super's super ...)\n- *\n- *               HPROF_GC_OBJ_ARRAY_DUMP       dump of an object array\n- *\n- *                          id         array object ID\n- *                          u4         stack trace serial number\n- *                          u4         number of elements\n- *                          id         array class ID\n- *                          [id]*      elements\n- *\n- *               HPROF_GC_PRIM_ARRAY_DUMP      dump of a primitive array\n- *\n- *                          id         array object ID\n- *                          u4         stack trace serial number\n- *                          u4         number of elements\n- *                          u1         element type\n- *                                     4:  boolean array\n- *                                     5:  char array\n- *                                     6:  float array\n- *                                     7:  double array\n- *                                     8:  byte array\n- *                                     9:  short array\n- *                                     10: int array\n- *                                     11: long array\n- *                          [u1]*      elements\n- *\n- * HPROF_CPU_SAMPLES        a set of sample traces of running threads\n- *\n- *                u4        total number of samples\n- *                u4        # of traces\n- *               [u4        # of samples\n- *                u4]*      stack trace serial number\n- *\n- * HPROF_CONTROL_SETTINGS   the settings of on\/off switches\n- *\n- *                u4        0x00000001: alloc traces on\/off\n- *                          0x00000002: cpu sampling on\/off\n- *                u2        stack trace depth\n- *\n- *\n- * When the header is \"JAVA PROFILE 1.0.2\" a heap dump can optionally\n- * be generated as a sequence of heap dump segments. This sequence is\n- * terminated by an end record. The additional tags allowed by format\n- * \"JAVA PROFILE 1.0.2\" are:\n- *\n- * HPROF_HEAP_DUMP_SEGMENT  denote a heap dump segment\n- *\n- *               [heap dump sub-records]*\n- *               The same sub-record types allowed by HPROF_HEAP_DUMP\n- *\n- * HPROF_HEAP_DUMP_END      denotes the end of a heap dump\n- *\n- *\/\n-\n-\n-\/\/ HPROF tags\n-\n-enum hprofTag : u1 {\n-  \/\/ top-level records\n-  HPROF_UTF8                    = 0x01,\n-  HPROF_LOAD_CLASS              = 0x02,\n-  HPROF_UNLOAD_CLASS            = 0x03,\n-  HPROF_FRAME                   = 0x04,\n-  HPROF_TRACE                   = 0x05,\n-  HPROF_ALLOC_SITES             = 0x06,\n-  HPROF_HEAP_SUMMARY            = 0x07,\n-  HPROF_START_THREAD            = 0x0A,\n-  HPROF_END_THREAD              = 0x0B,\n-  HPROF_HEAP_DUMP               = 0x0C,\n-  HPROF_CPU_SAMPLES             = 0x0D,\n-  HPROF_CONTROL_SETTINGS        = 0x0E,\n-\n-  \/\/ 1.0.2 record types\n-  HPROF_HEAP_DUMP_SEGMENT       = 0x1C,\n-  HPROF_HEAP_DUMP_END           = 0x2C,\n-\n-  \/\/ field types\n-  HPROF_ARRAY_OBJECT            = 0x01,\n-  HPROF_NORMAL_OBJECT           = 0x02,\n-  HPROF_BOOLEAN                 = 0x04,\n-  HPROF_CHAR                    = 0x05,\n-  HPROF_FLOAT                   = 0x06,\n-  HPROF_DOUBLE                  = 0x07,\n-  HPROF_BYTE                    = 0x08,\n-  HPROF_SHORT                   = 0x09,\n-  HPROF_INT                     = 0x0A,\n-  HPROF_LONG                    = 0x0B,\n-\n-  \/\/ data-dump sub-records\n-  HPROF_GC_ROOT_UNKNOWN         = 0xFF,\n-  HPROF_GC_ROOT_JNI_GLOBAL      = 0x01,\n-  HPROF_GC_ROOT_JNI_LOCAL       = 0x02,\n-  HPROF_GC_ROOT_JAVA_FRAME      = 0x03,\n-  HPROF_GC_ROOT_NATIVE_STACK    = 0x04,\n-  HPROF_GC_ROOT_STICKY_CLASS    = 0x05,\n-  HPROF_GC_ROOT_THREAD_BLOCK    = 0x06,\n-  HPROF_GC_ROOT_MONITOR_USED    = 0x07,\n-  HPROF_GC_ROOT_THREAD_OBJ      = 0x08,\n-  HPROF_GC_CLASS_DUMP           = 0x20,\n-  HPROF_GC_INSTANCE_DUMP        = 0x21,\n-  HPROF_GC_OBJ_ARRAY_DUMP       = 0x22,\n-  HPROF_GC_PRIM_ARRAY_DUMP      = 0x23\n-};\n-\n-\/\/ Default stack trace ID (used for dummy HPROF_TRACE record)\n-enum {\n-  STACK_TRACE_ID = 1,\n-  INITIAL_CLASS_COUNT = 200\n-};\n-\n-\/\/ Supports I\/O operations for a dump\n-\/\/ Base class for dump and parallel dump\n-class AbstractDumpWriter : public StackObj {\n- protected:\n-  enum {\n-    io_buffer_max_size = 1*M,\n-    io_buffer_max_waste = 10*K,\n-    dump_segment_header_size = 9\n-  };\n-\n-  char* _buffer;    \/\/ internal buffer\n-  size_t _size;\n-  size_t _pos;\n-\n-  bool _in_dump_segment; \/\/ Are we currently in a dump segment?\n-  bool _is_huge_sub_record; \/\/ Are we writing a sub-record larger than the buffer size?\n-  DEBUG_ONLY(size_t _sub_record_left;) \/\/ The bytes not written for the current sub-record.\n-  DEBUG_ONLY(bool _sub_record_ended;) \/\/ True if we have called the end_sub_record().\n-\n-  virtual void flush(bool force = false) = 0;\n-\n-  char* buffer() const                          { return _buffer; }\n-  size_t buffer_size() const                    { return _size; }\n-  void set_position(size_t pos)                 { _pos = pos; }\n-\n-  \/\/ Can be called if we have enough room in the buffer.\n-  void write_fast(const void* s, size_t len);\n-\n-  \/\/ Returns true if we have enough room in the buffer for 'len' bytes.\n-  bool can_write_fast(size_t len);\n-\n-  void write_address(address a);\n-\n- public:\n-  AbstractDumpWriter() :\n-    _buffer(nullptr),\n-    _size(io_buffer_max_size),\n-    _pos(0),\n-    _in_dump_segment(false) { }\n-\n-  \/\/ total number of bytes written to the disk\n-  virtual julong bytes_written() const = 0;\n-  virtual char const* error() const = 0;\n-\n-  size_t position() const                       { return _pos; }\n-  \/\/ writer functions\n-  virtual void write_raw(const void* s, size_t len);\n-  void write_u1(u1 x);\n-  void write_u2(u2 x);\n-  void write_u4(u4 x);\n-  void write_u8(u8 x);\n-  void write_objectID(oop o);\n-  void write_rootID(oop* p);\n-  void write_symbolID(Symbol* o);\n-  void write_classID(Klass* k);\n-  void write_id(u4 x);\n-\n-  \/\/ Start a new sub-record. Starts a new heap dump segment if needed.\n-  void start_sub_record(u1 tag, u4 len);\n-  \/\/ Ends the current sub-record.\n-  void end_sub_record();\n-  \/\/ Finishes the current dump segment if not already finished.\n-  void finish_dump_segment(bool force_flush = false);\n-  \/\/ Refresh to get new buffer\n-  void refresh() {\n-    assert (_in_dump_segment ==false, \"Sanity check\");\n-    _buffer = nullptr;\n-    _size = io_buffer_max_size;\n-    _pos = 0;\n-    \/\/ Force flush to guarantee data from parallel dumper are written.\n-    flush(true);\n-  }\n-  \/\/ Called when finished to release the threads.\n-  virtual void deactivate() = 0;\n-};\n-\n-void AbstractDumpWriter::write_fast(const void* s, size_t len) {\n-  assert(!_in_dump_segment || (_sub_record_left >= len), \"sub-record too large\");\n-  assert(buffer_size() - position() >= len, \"Must fit\");\n-  debug_only(_sub_record_left -= len);\n-  memcpy(buffer() + position(), s, len);\n-  set_position(position() + len);\n-}\n-\n-bool AbstractDumpWriter::can_write_fast(size_t len) {\n-  return buffer_size() - position() >= len;\n-}\n-\n-\/\/ write raw bytes\n-void AbstractDumpWriter::write_raw(const void* s, size_t len) {\n-  assert(!_in_dump_segment || (_sub_record_left >= len), \"sub-record too large\");\n-  debug_only(_sub_record_left -= len);\n-\n-  \/\/ flush buffer to make room.\n-  while (len > buffer_size() - position()) {\n-    assert(!_in_dump_segment || _is_huge_sub_record,\n-           \"Cannot overflow in non-huge sub-record.\");\n-    size_t to_write = buffer_size() - position();\n-    memcpy(buffer() + position(), s, to_write);\n-    s = (void*) ((char*) s + to_write);\n-    len -= to_write;\n-    set_position(position() + to_write);\n-    flush();\n-  }\n-\n-  memcpy(buffer() + position(), s, len);\n-  set_position(position() + len);\n-}\n-\n-\/\/ Makes sure we inline the fast write into the write_u* functions. This is a big speedup.\n-#define WRITE_KNOWN_TYPE(p, len) do { if (can_write_fast((len))) write_fast((p), (len)); \\\n-                                      else write_raw((p), (len)); } while (0)\n-\n-void AbstractDumpWriter::write_u1(u1 x) {\n-  WRITE_KNOWN_TYPE(&x, 1);\n-}\n-\n-void AbstractDumpWriter::write_u2(u2 x) {\n-  u2 v;\n-  Bytes::put_Java_u2((address)&v, x);\n-  WRITE_KNOWN_TYPE(&v, 2);\n-}\n-\n-void AbstractDumpWriter::write_u4(u4 x) {\n-  u4 v;\n-  Bytes::put_Java_u4((address)&v, x);\n-  WRITE_KNOWN_TYPE(&v, 4);\n-}\n-\n-void AbstractDumpWriter::write_u8(u8 x) {\n-  u8 v;\n-  Bytes::put_Java_u8((address)&v, x);\n-  WRITE_KNOWN_TYPE(&v, 8);\n-}\n-\n-void AbstractDumpWriter::write_address(address a) {\n-#ifdef _LP64\n-  write_u8((u8)a);\n-#else\n-  write_u4((u4)a);\n-#endif\n-}\n-\n-void AbstractDumpWriter::write_objectID(oop o) {\n-  write_address(cast_from_oop<address>(o));\n-}\n-\n-void AbstractDumpWriter::write_rootID(oop* p) {\n-  write_address((address)p);\n-}\n-\n-void AbstractDumpWriter::write_symbolID(Symbol* s) {\n-  write_address((address)((uintptr_t)s));\n-}\n-\n-void AbstractDumpWriter::write_id(u4 x) {\n-#ifdef _LP64\n-  write_u8((u8) x);\n-#else\n-  write_u4(x);\n-#endif\n-}\n-\n-\/\/ We use java mirror as the class ID\n-void AbstractDumpWriter::write_classID(Klass* k) {\n-  write_objectID(k->java_mirror());\n-}\n-\n-void AbstractDumpWriter::finish_dump_segment(bool force_flush) {\n-  if (_in_dump_segment) {\n-    assert(_sub_record_left == 0, \"Last sub-record not written completely\");\n-    assert(_sub_record_ended, \"sub-record must have ended\");\n-\n-    \/\/ Fix up the dump segment length if we haven't written a huge sub-record last\n-    \/\/ (in which case the segment length was already set to the correct value initially).\n-    if (!_is_huge_sub_record) {\n-      assert(position() > dump_segment_header_size, \"Dump segment should have some content\");\n-      Bytes::put_Java_u4((address) (buffer() + 5),\n-                         (u4) (position() - dump_segment_header_size));\n-    } else {\n-      \/\/ Finish process huge sub record\n-      \/\/ Set _is_huge_sub_record to false so the parallel dump writer can flush data to file.\n-      _is_huge_sub_record = false;\n-    }\n-\n-    _in_dump_segment = false;\n-    flush(force_flush);\n-  }\n-}\n-\n-void AbstractDumpWriter::start_sub_record(u1 tag, u4 len) {\n-  if (!_in_dump_segment) {\n-    if (position() > 0) {\n-      flush();\n-    }\n-\n-    assert(position() == 0 && buffer_size() > dump_segment_header_size, \"Must be at the start\");\n-\n-    write_u1(HPROF_HEAP_DUMP_SEGMENT);\n-    write_u4(0); \/\/ timestamp\n-    \/\/ Will be fixed up later if we add more sub-records.  If this is a huge sub-record,\n-    \/\/ this is already the correct length, since we don't add more sub-records.\n-    write_u4(len);\n-    assert(Bytes::get_Java_u4((address)(buffer() + 5)) == len, \"Inconsistent size!\");\n-    _in_dump_segment = true;\n-    _is_huge_sub_record = len > buffer_size() - dump_segment_header_size;\n-  } else if (_is_huge_sub_record || (len > buffer_size() - position())) {\n-    \/\/ This object will not fit in completely or the last sub-record was huge.\n-    \/\/ Finish the current segment and try again.\n-    finish_dump_segment();\n-    start_sub_record(tag, len);\n-\n-    return;\n-  }\n-\n-  debug_only(_sub_record_left = len);\n-  debug_only(_sub_record_ended = false);\n-\n-  write_u1(tag);\n-}\n-\n-void AbstractDumpWriter::end_sub_record() {\n-  assert(_in_dump_segment, \"must be in dump segment\");\n-  assert(_sub_record_left == 0, \"sub-record not written completely\");\n-  assert(!_sub_record_ended, \"Must not have ended yet\");\n-  debug_only(_sub_record_ended = true);\n-}\n-\n-\/\/ Supports I\/O operations for a dump\n-\n-class DumpWriter : public AbstractDumpWriter {\n- private:\n-  CompressionBackend _backend; \/\/ Does the actual writing.\n- protected:\n-  void flush(bool force = false) override;\n-\n- public:\n-  \/\/ Takes ownership of the writer and compressor.\n-  DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor);\n-\n-  \/\/ total number of bytes written to the disk\n-  julong bytes_written() const override { return (julong) _backend.get_written(); }\n-\n-  char const* error() const override    { return _backend.error(); }\n-\n-  \/\/ Called by threads used for parallel writing.\n-  void writer_loop()                    { _backend.thread_loop(); }\n-  \/\/ Called when finish to release the threads.\n-  void deactivate() override            { flush(); _backend.deactivate(); }\n-  \/\/ Get the backend pointer, used by parallel dump writer.\n-  CompressionBackend* backend_ptr()     { return &_backend; }\n-\n-};\n-\n-\/\/ Check for error after constructing the object and destroy it in case of an error.\n-DumpWriter::DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor) :\n-  AbstractDumpWriter(),\n-  _backend(writer, compressor, io_buffer_max_size, io_buffer_max_waste) {\n-  flush();\n-}\n-\n-\/\/ flush any buffered bytes to the file\n-void DumpWriter::flush(bool force) {\n-  _backend.get_new_buffer(&_buffer, &_pos, &_size, force);\n-}\n-\n-\/\/ Buffer queue used for parallel dump.\n-struct ParWriterBufferQueueElem {\n-  char* _buffer;\n-  size_t _used;\n-  ParWriterBufferQueueElem* _next;\n-};\n-\n-class ParWriterBufferQueue : public CHeapObj<mtInternal> {\n- private:\n-  ParWriterBufferQueueElem* _head;\n-  ParWriterBufferQueueElem* _tail;\n-  uint _length;\n- public:\n-  ParWriterBufferQueue() : _head(nullptr), _tail(nullptr), _length(0) { }\n-\n-  void enqueue(ParWriterBufferQueueElem* entry) {\n-    if (_head == nullptr) {\n-      assert(is_empty() && _tail == nullptr, \"Sanity check\");\n-      _head = _tail = entry;\n-    } else {\n-      assert ((_tail->_next == nullptr && _tail->_buffer != nullptr), \"Buffer queue is polluted\");\n-      _tail->_next = entry;\n-      _tail = entry;\n-    }\n-    _length++;\n-    assert(_tail->_next == nullptr, \"Buffer queue is polluted\");\n-  }\n-\n-  ParWriterBufferQueueElem* dequeue() {\n-    if (_head == nullptr)  return nullptr;\n-    ParWriterBufferQueueElem* entry = _head;\n-    assert (entry->_buffer != nullptr, \"polluted buffer in writer list\");\n-    _head = entry->_next;\n-    if (_head == nullptr) {\n-      _tail = nullptr;\n-    }\n-    entry->_next = nullptr;\n-    _length--;\n-    return entry;\n-  }\n-\n-  bool is_empty() {\n-    return _length == 0;\n-  }\n-\n-  uint length() { return _length; }\n-};\n-\n-\/\/ Support parallel heap dump.\n-class ParDumpWriter : public AbstractDumpWriter {\n- private:\n-  \/\/ Lock used to guarantee the integrity of multiple buffers writing.\n-  static Monitor* _lock;\n-  \/\/ Pointer of backend from global DumpWriter.\n-  CompressionBackend* _backend_ptr;\n-  char const * _err;\n-  ParWriterBufferQueue* _buffer_queue;\n-  size_t _internal_buffer_used;\n-  char* _buffer_base;\n-  bool _split_data;\n-  static const uint BackendFlushThreshold = 2;\n- protected:\n-  void flush(bool force = false) override {\n-    assert(_pos != 0, \"must not be zero\");\n-    if (_pos != 0) {\n-      refresh_buffer();\n-    }\n-\n-    if (_split_data || _is_huge_sub_record) {\n-      return;\n-    }\n-\n-    if (should_flush_buf_list(force)) {\n-      assert(!_in_dump_segment && !_split_data && !_is_huge_sub_record, \"incomplete data send to backend!\\n\");\n-      flush_to_backend(force);\n-    }\n-  }\n-\n- public:\n-  \/\/ Check for error after constructing the object and destroy it in case of an error.\n-  ParDumpWriter(DumpWriter* dw) :\n-    AbstractDumpWriter(),\n-    _backend_ptr(dw->backend_ptr()),\n-    _buffer_queue((new (std::nothrow) ParWriterBufferQueue())),\n-    _buffer_base(nullptr),\n-    _split_data(false) {\n-    \/\/ prepare internal buffer\n-    allocate_internal_buffer();\n-  }\n-\n-  ~ParDumpWriter() {\n-     assert(_buffer_queue != nullptr, \"Sanity check\");\n-     assert((_internal_buffer_used == 0) && (_buffer_queue->is_empty()),\n-            \"All data must be send to backend\");\n-     if (_buffer_base != nullptr) {\n-       os::free(_buffer_base);\n-       _buffer_base = nullptr;\n-     }\n-     delete _buffer_queue;\n-     _buffer_queue = nullptr;\n-  }\n-\n-  \/\/ total number of bytes written to the disk\n-  julong bytes_written() const override { return (julong) _backend_ptr->get_written(); }\n-  char const* error() const override    { return _err == nullptr ? _backend_ptr->error() : _err; }\n-\n-  static void before_work() {\n-    assert(_lock == nullptr, \"ParDumpWriter lock must be initialized only once\");\n-    _lock = new (std::nothrow) PaddedMonitor(Mutex::safepoint, \"ParallelHProfWriter_lock\");\n-  }\n-\n-  static void after_work() {\n-    assert(_lock != nullptr, \"ParDumpWriter lock is not initialized\");\n-    delete _lock;\n-    _lock = nullptr;\n-  }\n-\n-  \/\/ write raw bytes\n-  void write_raw(const void* s, size_t len) override {\n-    assert(!_in_dump_segment || (_sub_record_left >= len), \"sub-record too large\");\n-    debug_only(_sub_record_left -= len);\n-    assert(!_split_data, \"Invalid split data\");\n-    _split_data = true;\n-    \/\/ flush buffer to make room.\n-    while (len > buffer_size() - position()) {\n-      assert(!_in_dump_segment || _is_huge_sub_record,\n-             \"Cannot overflow in non-huge sub-record.\");\n-      size_t to_write = buffer_size() - position();\n-      memcpy(buffer() + position(), s, to_write);\n-      s = (void*) ((char*) s + to_write);\n-      len -= to_write;\n-      set_position(position() + to_write);\n-      flush();\n-    }\n-    _split_data = false;\n-    memcpy(buffer() + position(), s, len);\n-    set_position(position() + len);\n-  }\n-\n-  void deactivate() override { flush(true); _backend_ptr->deactivate(); }\n-\n- private:\n-  void allocate_internal_buffer() {\n-    assert(_buffer_queue != nullptr, \"Internal buffer queue is not ready when allocate internal buffer\");\n-    assert(_buffer == nullptr && _buffer_base == nullptr, \"current buffer must be null before allocate\");\n-    _buffer_base = _buffer = (char*)os::malloc(io_buffer_max_size, mtInternal);\n-    if (_buffer == nullptr) {\n-      set_error(\"Could not allocate buffer for writer\");\n-      return;\n-    }\n-    _pos = 0;\n-    _internal_buffer_used = 0;\n-    _size = io_buffer_max_size;\n-  }\n-\n-  void set_error(char const* new_error) {\n-    if ((new_error != nullptr) && (_err == nullptr)) {\n-      _err = new_error;\n-    }\n-  }\n-\n-  \/\/ Add buffer to internal list\n-  void refresh_buffer() {\n-    size_t expected_total = _internal_buffer_used + _pos;\n-    if (expected_total < io_buffer_max_size - io_buffer_max_waste) {\n-      \/\/ reuse current buffer.\n-      _internal_buffer_used = expected_total;\n-      assert(_size - _pos == io_buffer_max_size - expected_total, \"illegal resize of buffer\");\n-      _size -= _pos;\n-      _buffer += _pos;\n-      _pos = 0;\n-\n-      return;\n-    }\n-    \/\/ It is not possible here that expected_total is larger than io_buffer_max_size because\n-    \/\/ of limitation in write_xxx().\n-    assert(expected_total <= io_buffer_max_size, \"buffer overflow\");\n-    assert(_buffer - _buffer_base <= io_buffer_max_size, \"internal buffer overflow\");\n-    ParWriterBufferQueueElem* entry =\n-        (ParWriterBufferQueueElem*)os::malloc(sizeof(ParWriterBufferQueueElem), mtInternal);\n-    if (entry == nullptr) {\n-      set_error(\"Heap dumper can allocate memory\");\n-      return;\n-    }\n-    entry->_buffer = _buffer_base;\n-    entry->_used = expected_total;\n-    entry->_next = nullptr;\n-    \/\/ add to internal buffer queue\n-    _buffer_queue->enqueue(entry);\n-    _buffer_base =_buffer = nullptr;\n-    allocate_internal_buffer();\n-  }\n-\n-  void reclaim_entry(ParWriterBufferQueueElem* entry) {\n-    assert(entry != nullptr && entry->_buffer != nullptr, \"Invalid entry to reclaim\");\n-    os::free(entry->_buffer);\n-    entry->_buffer = nullptr;\n-    os::free(entry);\n-  }\n-\n-  void flush_buffer(char* buffer, size_t used) {\n-    assert(_lock->owner() == Thread::current(), \"flush buffer must hold lock\");\n-    size_t max = io_buffer_max_size;\n-    \/\/ get_new_buffer\n-    _backend_ptr->flush_external_buffer(buffer, used, max);\n-  }\n-\n-  bool should_flush_buf_list(bool force) {\n-    return force || _buffer_queue->length() > BackendFlushThreshold;\n-  }\n-\n-  void flush_to_backend(bool force) {\n-    \/\/ Guarantee there is only one writer updating the backend buffers.\n-    MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-    while (!_buffer_queue->is_empty()) {\n-      ParWriterBufferQueueElem* entry = _buffer_queue->dequeue();\n-      flush_buffer(entry->_buffer, entry->_used);\n-      \/\/ Delete buffer and entry.\n-      reclaim_entry(entry);\n-      entry = nullptr;\n-    }\n-    assert(_pos == 0, \"available buffer must be empty before flush\");\n-    \/\/ Flush internal buffer.\n-    if (_internal_buffer_used > 0) {\n-      flush_buffer(_buffer_base, _internal_buffer_used);\n-      os::free(_buffer_base);\n-      _pos = 0;\n-      _internal_buffer_used = 0;\n-      _buffer_base = _buffer = nullptr;\n-      \/\/ Allocate internal buffer for future use.\n-      allocate_internal_buffer();\n-    }\n-  }\n-};\n-\n-Monitor* ParDumpWriter::_lock = nullptr;\n-\n@@ -1616,67 +794,0 @@\n-\/\/ Large object heap dump support.\n-\/\/ To avoid memory consumption, when dumping large objects such as huge array and\n-\/\/ large objects whose size are larger than LARGE_OBJECT_DUMP_THRESHOLD, the scanned\n-\/\/ partial object\/array data will be sent to the backend directly instead of caching\n-\/\/ the whole object\/array in the internal buffer.\n-\/\/ The HeapDumpLargeObjectList is used to save the large object when dumper scans\n-\/\/ the heap. The large objects could be added (push) parallelly by multiple dumpers,\n-\/\/ But they will be removed (popped) serially only by the VM thread.\n-class HeapDumpLargeObjectList : public CHeapObj<mtInternal> {\n- private:\n-  class HeapDumpLargeObjectListElem : public CHeapObj<mtInternal> {\n-   public:\n-    HeapDumpLargeObjectListElem(oop obj) : _obj(obj), _next(nullptr) { }\n-    oop _obj;\n-    HeapDumpLargeObjectListElem* _next;\n-  };\n-\n-  volatile HeapDumpLargeObjectListElem* _head;\n-\n- public:\n-  HeapDumpLargeObjectList() : _head(nullptr) { }\n-\n-  void atomic_push(oop obj) {\n-    assert (obj != nullptr, \"sanity check\");\n-    HeapDumpLargeObjectListElem* entry = new HeapDumpLargeObjectListElem(obj);\n-    if (entry == nullptr) {\n-      warning(\"failed to allocate element for large object list\");\n-      return;\n-    }\n-    assert (entry->_obj != nullptr, \"sanity check\");\n-    while (true) {\n-      volatile HeapDumpLargeObjectListElem* old_head = Atomic::load_acquire(&_head);\n-      HeapDumpLargeObjectListElem* new_head = entry;\n-      if (Atomic::cmpxchg(&_head, old_head, new_head) == old_head) {\n-        \/\/ successfully push\n-        new_head->_next = (HeapDumpLargeObjectListElem*)old_head;\n-        return;\n-      }\n-    }\n-  }\n-\n-  oop pop() {\n-    if (_head == nullptr) {\n-      return nullptr;\n-    }\n-    HeapDumpLargeObjectListElem* entry = (HeapDumpLargeObjectListElem*)_head;\n-    _head = _head->_next;\n-    assert (entry != nullptr, \"illegal larger object list entry\");\n-    oop ret = entry->_obj;\n-    delete entry;\n-    assert (ret != nullptr, \"illegal oop pointer\");\n-    return ret;\n-  }\n-\n-  void drain(ObjectClosure* cl) {\n-    while (_head !=  nullptr) {\n-      cl->do_object(pop());\n-    }\n-  }\n-\n-  bool is_empty() {\n-    return _head == nullptr;\n-  }\n-\n-  static const size_t LargeObjectSizeThreshold = 1 << 20; \/\/ 1 MB\n-};\n-\n@@ -1689,2 +800,0 @@\n-  HeapDumpLargeObjectList* _list;\n-\n@@ -1692,1 +801,1 @@\n-  bool is_large(oop o);\n+\n@@ -1694,1 +803,1 @@\n-  HeapObjectDumper(AbstractDumpWriter* writer, HeapDumpLargeObjectList* list = nullptr) {\n+  HeapObjectDumper(AbstractDumpWriter* writer) {\n@@ -1696,1 +805,0 @@\n-    _list = list;\n@@ -1716,7 +824,0 @@\n-  \/\/ If large object list exists and it is large object\/array,\n-  \/\/ add oop into the list and skip scan. VM thread will process it later.\n-  if (_list != nullptr && is_large(o)) {\n-    _list->atomic_push(o);\n-    return;\n-  }\n-\n@@ -1735,23 +836,0 @@\n-bool HeapObjectDumper::is_large(oop o) {\n-  size_t size = 0;\n-  if (o->is_instance()) {\n-    \/\/ Use o->size() * 8 as the upper limit of instance size to avoid iterating static fields\n-    size = o->size() * 8;\n-  } else if (o->is_objArray()) {\n-    objArrayOop array = objArrayOop(o);\n-    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n-    assert(type >= T_BOOLEAN && type <= T_OBJECT, \"invalid array element type\");\n-    int length = array->length();\n-    int type_size = sizeof(address);\n-    size = (size_t)length * type_size;\n-  } else if (o->is_typeArray()) {\n-    typeArrayOop array = typeArrayOop(o);\n-    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n-    assert(type >= T_BOOLEAN && type <= T_OBJECT, \"invalid array element type\");\n-    int length = array->length();\n-    int type_size = type2aelembytes(type);\n-    size = (size_t)length * type_size;\n-  }\n-  return size > HeapDumpLargeObjectList::LargeObjectSizeThreshold;\n-}\n-\n@@ -1761,1 +839,0 @@\n-   bool     _started;\n@@ -1768,1 +845,0 @@\n-     _started(false),\n@@ -1775,17 +851,1 @@\n-   void wait_for_start_signal() {\n-     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-     while (_started == false) {\n-       ml.wait();\n-     }\n-     assert(_started == true,  \"dumper woke up with wrong state\");\n-   }\n-\n-   void start_dump() {\n-     assert (_started == false, \"start dump with wrong state\");\n-     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-     _started = true;\n-     ml.notify_all();\n-   }\n-\n-   void dumper_complete() {\n-     assert (_started == true, \"dumper complete with wrong state\");\n+   void dumper_complete(DumpWriter* local_writer, DumpWriter* global_writer) {\n@@ -1794,0 +854,4 @@\n+     \/\/ propagate local error to global if any\n+     if (local_writer->has_error()) {\n+      global_writer->set_error(local_writer->error());\n+     }\n@@ -1798,1 +862,0 @@\n-     assert (_started == true, \"wrong state when wait for dumper complete\");\n@@ -1801,1 +864,1 @@\n-        ml.wait();\n+      ml.wait();\n@@ -1803,1 +866,0 @@\n-     _started = false;\n@@ -1807,0 +869,17 @@\n+class VM_HeapDumpMerge : public VM_Operation {\n+private:\n+  DumpWriter* _writer;\n+  const char* _path;\n+  bool _has_error;\n+\n+  void merge_file(char* path);\n+  void merge_done();\n+public:\n+  VM_HeapDumpMerge(const char* path, DumpWriter* writer)\n+    : _writer(writer), _path(path), _has_error(_writer->has_error()) {}\n+  VMOp_Type type() const { return VMOp_HeapDumpMerge; }\n+  \/\/ heap dump merge could happen outside safepoint\n+  virtual bool evaluate_at_safepoint() const { return false; }\n+  void doit();\n+};\n+\n@@ -1821,1 +900,0 @@\n-  uint                    _num_writer_threads;\n@@ -1824,6 +902,0 @@\n-  HeapDumpLargeObjectList* _large_object_list;\n-\n-  \/\/ VMDumperType is for thread that dumps both heap and non-heap data.\n-  static const size_t VMDumperType = 0;\n-  static const size_t WriterType = 1;\n-  static const size_t DumperType = 2;\n@@ -1832,46 +904,3 @@\n-\n-  size_t get_worker_type(uint worker_id) {\n-    assert(_num_writer_threads >= 1, \"Must be at least one writer\");\n-    \/\/ worker id of VMDumper that dump heap and non-heap data\n-    if (worker_id == VMDumperWorkerId) {\n-      return VMDumperType;\n-    }\n-\n-    \/\/ worker id of dumper starts from 1, which only dump heap datar\n-    if (worker_id < _num_dumper_threads) {\n-      return DumperType;\n-    }\n-\n-    \/\/ worker id of writer starts from _num_dumper_threads\n-    return WriterType;\n-  }\n-\n-  void prepare_parallel_dump(uint num_total) {\n-    assert (_dumper_controller == nullptr, \"dumper controller must be null\");\n-    assert (num_total > 0, \"active workers number must >= 1\");\n-    \/\/ Dumper threads number must not be larger than active workers number.\n-    if (num_total < _num_dumper_threads) {\n-      _num_dumper_threads = num_total - 1;\n-    }\n-    \/\/ Calculate dumper and writer threads number.\n-    _num_writer_threads = num_total - _num_dumper_threads;\n-    \/\/ If dumper threads number is 1, only the VMThread works as a dumper.\n-    \/\/ If dumper threads number is equal to active workers, need at lest one worker thread as writer.\n-    if (_num_dumper_threads > 0 && _num_writer_threads == 0) {\n-      _num_writer_threads = 1;\n-      _num_dumper_threads = num_total - _num_writer_threads;\n-    }\n-    \/\/ Prepare parallel writer.\n-    if (_num_dumper_threads > 1) {\n-      ParDumpWriter::before_work();\n-      \/\/ Number of dumper threads that only iterate heap.\n-      uint _heap_only_dumper_threads = _num_dumper_threads - 1 \/* VMDumper thread *\/;\n-      _dumper_controller = new (std::nothrow) DumperController(_heap_only_dumper_threads);\n-    }\n-  }\n-\n-  void finish_parallel_dump() {\n-    if (_num_dumper_threads > 1) {\n-      ParDumpWriter::after_work();\n-    }\n-  }\n+  \/\/ VM dumper dumps both heap and non-heap data, other dumpers dump heap-only data.\n+  static bool is_vm_dumper(uint worker_id) { return worker_id == VMDumperWorkerId; }\n+  static DumpWriter* create_dump_writer();\n@@ -1912,3 +941,0 @@\n-  \/\/ large objects\n-  void dump_large_objects(ObjectClosure* writer);\n-\n@@ -1930,1 +956,0 @@\n-    _large_object_list = new (std::nothrow) HeapDumpLargeObjectList();\n@@ -1957,1 +982,0 @@\n-    delete _large_object_list;\n@@ -1959,1 +983,10 @@\n-\n+  bool is_parallel_dump()  { return _num_dumper_threads > 1; }\n+  bool can_parallel_dump() {\n+    const char* base_path = writer()->get_file_path();\n+    assert(base_path != nullptr, \"sanity check\");\n+    if ((strlen(base_path) + 7\/*.p\\d\\d\\d\\d\\0*\/) >= JVM_MAXPATHLEN) {\n+      \/\/ no extra path room for separate heap dump files\n+      return false;\n+    }\n+    return true;\n+  }\n@@ -2184,0 +1217,2 @@\n+  uint  num_active_workers = workers != nullptr ? workers->active_workers() : 0;\n+  uint requested_num_dump_thread = _num_dumper_threads;\n@@ -2185,1 +1220,3 @@\n-  if (workers == nullptr) {\n+  if (num_active_workers <= 1 ||         \/\/ serial gc?\n+      requested_num_dump_thread <= 1 ||  \/\/ request serial dump?\n+     !can_parallel_dump()) {             \/\/ can not dump in parallel?\n@@ -2187,2 +1224,1 @@\n-    _num_dumper_threads=1;\n-    _num_writer_threads=1;\n+    _num_dumper_threads = 1;\n@@ -2191,10 +1227,8 @@\n-    prepare_parallel_dump(workers->active_workers());\n-    if (_num_dumper_threads > 1) {\n-      ParallelObjectIterator poi(_num_dumper_threads);\n-      _poi = &poi;\n-      workers->run_task(this);\n-      _poi = nullptr;\n-    } else {\n-      workers->run_task(this);\n-    }\n-    finish_parallel_dump();\n+    \/\/ Use parallel dump otherwise\n+    _num_dumper_threads = clamp(requested_num_dump_thread, 2U, num_active_workers);\n+    uint heap_only_dumper_threads = _num_dumper_threads - 1 \/* VMDumper thread *\/;\n+    _dumper_controller = new (std::nothrow) DumperController(heap_only_dumper_threads);\n+    ParallelObjectIterator poi(_num_dumper_threads);\n+    _poi = &poi;\n+    workers->run_task(this, _num_dumper_threads);\n+    _poi = nullptr;\n@@ -2208,8 +1242,55 @@\n-void VM_HeapDumper::work(uint worker_id) {\n-  if (worker_id != 0) {\n-    if (get_worker_type(worker_id) == WriterType) {\n-      writer()->writer_loop();\n-      return;\n-    }\n-    if (_num_dumper_threads > 1 && get_worker_type(worker_id) == DumperType) {\n-      _dumper_controller->wait_for_start_signal();\n+static int volatile dump_seq = 0;\n+\n+void VM_HeapDumpMerge::merge_done() {\n+  \/\/ Writes the HPROF_HEAP_DUMP_END record.\n+  if (!_has_error) {\n+    DumperSupport::end_of_dump(_writer);\n+    _writer->flush();\n+  }\n+  dump_seq = 0; \/\/reset\n+}\n+\n+void VM_HeapDumpMerge::merge_file(char* path) {\n+  assert(!SafepointSynchronize::is_at_safepoint(), \"merging happens outside safepoint\");\n+  TraceTime timer(\"Merge segmented heap file\", TRACETIME_LOG(Info, heapdump));\n+\n+  fileStream part_fs(path, \"r\");\n+  if (!part_fs.is_open()) {\n+    log_error(heapdump)(\"Can not open segmented heap file %s during merging\", path);\n+    _writer->set_error(\"Can not open segmented heap file during merging\");\n+    _has_error = true;\n+    return;\n+  }\n+\n+  jlong total = 0;\n+  int cnt = 0;\n+  char read_buf[4096];\n+  while ((cnt = part_fs.read(read_buf, 1, 4096)) != 0) {\n+    _writer->write_raw(read_buf, cnt);\n+    total += cnt;\n+  }\n+\n+  _writer->flush();\n+  if (part_fs.fileSize() != total) {\n+    log_error(heapdump)(\"Merged heap dump %s is incomplete\", path);\n+    _writer->set_error(\"Merged heap dump is incomplete\");\n+    _has_error = true;\n+  }\n+}\n+\n+void VM_HeapDumpMerge::doit() {\n+  assert(!SafepointSynchronize::is_at_safepoint(), \"merging happens outside safepoint\");\n+  TraceTime timer(\"Merge heap files complete\", TRACETIME_LOG(Info, heapdump));\n+\n+  \/\/ Since contents in segmented heap file were already zipped, we don't need to zip\n+  \/\/ them again during merging.\n+  AbstractCompressor* saved_compressor = _writer->compressor();\n+  _writer->set_compressor(nullptr);\n+\n+  \/\/ merge segmented heap file and remove it anyway\n+  char path[JVM_MAXPATHLEN];\n+  for (int i = 0; i < dump_seq; i++) {\n+    memset(path, 0, JVM_MAXPATHLEN);\n+    os::snprintf(path, JVM_MAXPATHLEN, \"%s.p%d\", _path, i);\n+    if (!_has_error) {\n+      merge_file(path);\n@@ -2217,2 +1298,25 @@\n-  } else {\n-    \/\/ The worker 0 on all non-heap data dumping and part of heap iteration.\n+    remove(path);\n+  }\n+\n+  \/\/ restore compressor for further use\n+  _writer->set_compressor(saved_compressor);\n+  merge_done();\n+}\n+\n+\/\/ prepare DumpWriter for every parallel dump thread\n+DumpWriter* VM_HeapDumper::create_dump_writer() {\n+  char* path = NEW_RESOURCE_ARRAY(char, JVM_MAXPATHLEN);\n+  memset(path, 0, JVM_MAXPATHLEN);\n+  const char* base_path = writer()->get_file_path();\n+  AbstractCompressor* compressor = writer()->compressor();\n+  int seq = Atomic::fetch_and_add(&dump_seq, 1);\n+  os::snprintf(path, JVM_MAXPATHLEN, \"%s.p%d\", base_path, seq);\n+  FileWriter* file_writer = new (std::nothrow) FileWriter(path, writer()->is_overwrite());\n+  DumpWriter* new_writer = new DumpWriter(file_writer, compressor);\n+  return new_writer;\n+}\n+\n+void VM_HeapDumper::work(uint worker_id) {\n+  \/\/ VM Dumper works on all non-heap data dumping and part of heap iteration.\n+  if (is_vm_dumper(worker_id)) {\n+    TraceTime timer(\"Dump non-objects\", TRACETIME_LOG(Info, heapdump));\n@@ -2262,0 +1366,2 @@\n+\n+  \/\/ Heap iteration.\n@@ -2268,1 +1374,4 @@\n-  if (_num_dumper_threads <= 1) {\n+  if (!is_parallel_dump()) {\n+    assert(worker_id == 0, \"must be\");\n+    \/\/ == Serial dump\n+    TraceTime timer(\"Dump heap objects\", TRACETIME_LOG(Info, heapdump));\n@@ -2271,0 +1380,4 @@\n+    writer()->finish_dump_segment();\n+    \/\/ Writes the HPROF_HEAP_DUMP_END record because merge does not happen in serial dump\n+    DumperSupport::end_of_dump(writer());\n+    writer()->flush();\n@@ -2272,28 +1385,13 @@\n-    assert(get_worker_type(worker_id) == DumperType\n-          || get_worker_type(worker_id) == VMDumperType,\n-          \"must be dumper thread to do heap iteration\");\n-    if (get_worker_type(worker_id) == VMDumperType) {\n-      \/\/ Clear global writer's buffer.\n-      writer()->finish_dump_segment(true);\n-      \/\/ Notify dumpers to start heap iteration.\n-      _dumper_controller->start_dump();\n-    }\n-    \/\/ Heap iteration.\n-    {\n-       ParDumpWriter pw(writer());\n-       {\n-         HeapObjectDumper obj_dumper(&pw, _large_object_list);\n-         _poi->object_iterate(&obj_dumper, worker_id);\n-       }\n-\n-       if (get_worker_type(worker_id) == VMDumperType) {\n-         _dumper_controller->wait_all_dumpers_complete();\n-         \/\/ clear internal buffer;\n-         pw.finish_dump_segment(true);\n-         \/\/ refresh the global_writer's buffer and position;\n-         writer()->refresh();\n-       } else {\n-         pw.finish_dump_segment(true);\n-         _dumper_controller->dumper_complete();\n-         return;\n-       }\n+    \/\/ == Parallel dump\n+    ResourceMark rm;\n+    TraceTime timer(\"Dump heap objects in parallel\", TRACETIME_LOG(Info, heapdump));\n+    DumpWriter* dw = is_vm_dumper(worker_id) ? writer() : create_dump_writer();\n+    HeapObjectDumper obj_dumper(dw);\n+    _poi->object_iterate(&obj_dumper, worker_id);\n+    dw->finish_dump_segment();\n+    dw->flush();\n+    if (is_vm_dumper(worker_id)) {\n+      _dumper_controller->wait_all_dumpers_complete();\n+    } else {\n+      _dumper_controller->dumper_complete(dw, writer());\n+      return;\n@@ -2302,9 +1400,2 @@\n-\n-  assert(get_worker_type(worker_id) == VMDumperType, \"Heap dumper must be VMDumper\");\n-  \/\/ Use writer() rather than ParDumpWriter to avoid memory consumption.\n-  HeapObjectDumper obj_dumper(writer());\n-  dump_large_objects(&obj_dumper);\n-  \/\/ Writes the HPROF_HEAP_DUMP_END record.\n-  DumperSupport::end_of_dump(writer());\n-  \/\/ We are done with writing. Release the worker threads.\n-  writer()->deactivate();\n+  \/\/ At this point, all fragments of the heapdump have been written to separate files.\n+  \/\/ We need to merge them into a complete heapdump and write HPROF_HEAP_DUMP_END at that time.\n@@ -2371,5 +1462,0 @@\n-\/\/ dump the large objects.\n-void VM_HeapDumper::dump_large_objects(ObjectClosure* cl) {\n-  _large_object_list->drain(cl);\n-}\n-\n@@ -2410,1 +1496,1 @@\n-  \/\/ generate the dump\n+  \/\/ generate the segmented heap dump into separate files\n@@ -2412,6 +1498,1 @@\n-  if (Thread::current()->is_VM_thread()) {\n-    assert(SafepointSynchronize::is_at_safepoint(), \"Expected to be called at a safepoint\");\n-    dumper.doit();\n-  } else {\n-    VMThread::execute(&dumper);\n-  }\n+  VMThread::execute(&dumper);\n@@ -2435,0 +1516,7 @@\n+  \/\/ merge segmented dump files into a complete one, this is not required for serial dump\n+  if (dumper.is_parallel_dump()) {\n+    VM_HeapDumpMerge op(path, &writer);\n+    VMThread::execute(&op);\n+    set_error(writer.error());\n+  }\n+\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":167,"deletions":1079,"binary":false,"changes":1246,"status":"modified"},{"patch":"@@ -32,11 +32,321 @@\n-\/\/ HeapDumper is used to dump the java heap to file in HPROF binary format:\n-\/\/\n-\/\/  { HeapDumper dumper(true \/* full GC before heap dump *\/);\n-\/\/    if (dumper.dump(\"\/export\/java.hprof\")) {\n-\/\/      ResourceMark rm;\n-\/\/      tty->print_cr(\"Dump failed: %s\", dumper.error_as_C_string());\n-\/\/    } else {\n-\/\/      \/\/ dump succeeded\n-\/\/    }\n-\/\/  }\n-\/\/\n+\/*\n+ * HPROF binary format - description copied from:\n+ *   src\/share\/demo\/jvmti\/hprof\/hprof_io.c\n+ *\n+ *\n+ *  header    \"JAVA PROFILE 1.0.2\" (0-terminated)\n+ *\n+ *  u4        size of identifiers. Identifiers are used to represent\n+ *            UTF8 strings, objects, stack traces, etc. They usually\n+ *            have the same size as host pointers.\n+ * u4         high word\n+ * u4         low word    number of milliseconds since 0:00 GMT, 1\/1\/70\n+ * [record]*  a sequence of records.\n+ *\n+ *\n+ * Record format:\n+ *\n+ * u1         a TAG denoting the type of the record\n+ * u4         number of *microseconds* since the time stamp in the\n+ *            header. (wraps around in a little more than an hour)\n+ * u4         number of bytes *remaining* in the record. Note that\n+ *            this number excludes the tag and the length field itself.\n+ * [u1]*      BODY of the record (a sequence of bytes)\n+ *\n+ *\n+ * The following TAGs are supported:\n+ *\n+ * TAG           BODY       notes\n+ *----------------------------------------------------------\n+ * HPROF_UTF8               a UTF8-encoded name\n+ *\n+ *               id         name ID\n+ *               [u1]*      UTF8 characters (no trailing zero)\n+ *\n+ * HPROF_LOAD_CLASS         a newly loaded class\n+ *\n+ *                u4        class serial number (> 0)\n+ *                id        class object ID\n+ *                u4        stack trace serial number\n+ *                id        class name ID\n+ *\n+ * HPROF_UNLOAD_CLASS       an unloading class\n+ *\n+ *                u4        class serial_number\n+ *\n+ * HPROF_FRAME              a Java stack frame\n+ *\n+ *                id        stack frame ID\n+ *                id        method name ID\n+ *                id        method signature ID\n+ *                id        source file name ID\n+ *                u4        class serial number\n+ *                i4        line number. >0: normal\n+ *                                       -1: unknown\n+ *                                       -2: compiled method\n+ *                                       -3: native method\n+ *\n+ * HPROF_TRACE              a Java stack trace\n+ *\n+ *               u4         stack trace serial number\n+ *               u4         thread serial number\n+ *               u4         number of frames\n+ *               [id]*      stack frame IDs\n+ *\n+ *\n+ * HPROF_ALLOC_SITES        a set of heap allocation sites, obtained after GC\n+ *\n+ *               u2         flags 0x0001: incremental vs. complete\n+ *                                0x0002: sorted by allocation vs. live\n+ *                                0x0004: whether to force a GC\n+ *               u4         cutoff ratio\n+ *               u4         total live bytes\n+ *               u4         total live instances\n+ *               u8         total bytes allocated\n+ *               u8         total instances allocated\n+ *               u4         number of sites that follow\n+ *               [u1        is_array: 0:  normal object\n+ *                                    2:  object array\n+ *                                    4:  boolean array\n+ *                                    5:  char array\n+ *                                    6:  float array\n+ *                                    7:  double array\n+ *                                    8:  byte array\n+ *                                    9:  short array\n+ *                                    10: int array\n+ *                                    11: long array\n+ *                u4        class serial number (may be zero during startup)\n+ *                u4        stack trace serial number\n+ *                u4        number of bytes alive\n+ *                u4        number of instances alive\n+ *                u4        number of bytes allocated\n+ *                u4]*      number of instance allocated\n+ *\n+ * HPROF_START_THREAD       a newly started thread.\n+ *\n+ *               u4         thread serial number (> 0)\n+ *               id         thread object ID\n+ *               u4         stack trace serial number\n+ *               id         thread name ID\n+ *               id         thread group name ID\n+ *               id         thread group parent name ID\n+ *\n+ * HPROF_END_THREAD         a terminating thread.\n+ *\n+ *               u4         thread serial number\n+ *\n+ * HPROF_HEAP_SUMMARY       heap summary\n+ *\n+ *               u4         total live bytes\n+ *               u4         total live instances\n+ *               u8         total bytes allocated\n+ *               u8         total instances allocated\n+ *\n+ * HPROF_HEAP_DUMP          denote a heap dump\n+ *\n+ *               [heap dump sub-records]*\n+ *\n+ *                          There are four kinds of heap dump sub-records:\n+ *\n+ *               u1         sub-record type\n+ *\n+ *               HPROF_GC_ROOT_UNKNOWN         unknown root\n+ *\n+ *                          id         object ID\n+ *\n+ *               HPROF_GC_ROOT_THREAD_OBJ      thread object\n+ *\n+ *                          id         thread object ID  (may be 0 for a\n+ *                                     thread newly attached through JNI)\n+ *                          u4         thread sequence number\n+ *                          u4         stack trace sequence number\n+ *\n+ *               HPROF_GC_ROOT_JNI_GLOBAL      JNI global ref root\n+ *\n+ *                          id         object ID\n+ *                          id         JNI global ref ID\n+ *\n+ *               HPROF_GC_ROOT_JNI_LOCAL       JNI local ref\n+ *\n+ *                          id         object ID\n+ *                          u4         thread serial number\n+ *                          u4         frame # in stack trace (-1 for empty)\n+ *\n+ *               HPROF_GC_ROOT_JAVA_FRAME      Java stack frame\n+ *\n+ *                          id         object ID\n+ *                          u4         thread serial number\n+ *                          u4         frame # in stack trace (-1 for empty)\n+ *\n+ *               HPROF_GC_ROOT_NATIVE_STACK    Native stack\n+ *\n+ *                          id         object ID\n+ *                          u4         thread serial number\n+ *\n+ *               HPROF_GC_ROOT_STICKY_CLASS    System class\n+ *\n+ *                          id         object ID\n+ *\n+ *               HPROF_GC_ROOT_THREAD_BLOCK    Reference from thread block\n+ *\n+ *                          id         object ID\n+ *                          u4         thread serial number\n+ *\n+ *               HPROF_GC_ROOT_MONITOR_USED    Busy monitor\n+ *\n+ *                          id         object ID\n+ *\n+ *               HPROF_GC_CLASS_DUMP           dump of a class object\n+ *\n+ *                          id         class object ID\n+ *                          u4         stack trace serial number\n+ *                          id         super class object ID\n+ *                          id         class loader object ID\n+ *                          id         signers object ID\n+ *                          id         protection domain object ID\n+ *                          id         reserved\n+ *                          id         reserved\n+ *\n+ *                          u4         instance size (in bytes)\n+ *\n+ *                          u2         size of constant pool\n+ *                          [u2,       constant pool index,\n+ *                           ty,       type\n+ *                                     2:  object\n+ *                                     4:  boolean\n+ *                                     5:  char\n+ *                                     6:  float\n+ *                                     7:  double\n+ *                                     8:  byte\n+ *                                     9:  short\n+ *                                     10: int\n+ *                                     11: long\n+ *                           vl]*      and value\n+ *\n+ *                          u2         number of static fields\n+ *                          [id,       static field name,\n+ *                           ty,       type,\n+ *                           vl]*      and value\n+ *\n+ *                          u2         number of inst. fields (not inc. super)\n+ *                          [id,       instance field name,\n+ *                           ty]*      type\n+ *\n+ *               HPROF_GC_INSTANCE_DUMP        dump of a normal object\n+ *\n+ *                          id         object ID\n+ *                          u4         stack trace serial number\n+ *                          id         class object ID\n+ *                          u4         number of bytes that follow\n+ *                          [vl]*      instance field values (class, followed\n+ *                                     by super, super's super ...)\n+ *\n+ *               HPROF_GC_OBJ_ARRAY_DUMP       dump of an object array\n+ *\n+ *                          id         array object ID\n+ *                          u4         stack trace serial number\n+ *                          u4         number of elements\n+ *                          id         array class ID\n+ *                          [id]*      elements\n+ *\n+ *               HPROF_GC_PRIM_ARRAY_DUMP      dump of a primitive array\n+ *\n+ *                          id         array object ID\n+ *                          u4         stack trace serial number\n+ *                          u4         number of elements\n+ *                          u1         element type\n+ *                                     4:  boolean array\n+ *                                     5:  char array\n+ *                                     6:  float array\n+ *                                     7:  double array\n+ *                                     8:  byte array\n+ *                                     9:  short array\n+ *                                     10: int array\n+ *                                     11: long array\n+ *                          [u1]*      elements\n+ *\n+ * HPROF_CPU_SAMPLES        a set of sample traces of running threads\n+ *\n+ *                u4        total number of samples\n+ *                u4        # of traces\n+ *               [u4        # of samples\n+ *                u4]*      stack trace serial number\n+ *\n+ * HPROF_CONTROL_SETTINGS   the settings of on\/off switches\n+ *\n+ *                u4        0x00000001: alloc traces on\/off\n+ *                          0x00000002: cpu sampling on\/off\n+ *                u2        stack trace depth\n+ *\n+ *\n+ * When the header is \"JAVA PROFILE 1.0.2\" a heap dump can optionally\n+ * be generated as a sequence of heap dump segments. This sequence is\n+ * terminated by an end record. The additional tags allowed by format\n+ * \"JAVA PROFILE 1.0.2\" are:\n+ *\n+ * HPROF_HEAP_DUMP_SEGMENT  denote a heap dump segment\n+ *\n+ *               [heap dump sub-records]*\n+ *               The same sub-record types allowed by HPROF_HEAP_DUMP\n+ *\n+ * HPROF_HEAP_DUMP_END      denotes the end of a heap dump\n+ *\n+ *\/\n+\n+\n+\/\/ HPROF tags\n+\n+enum hprofTag : u1 {\n+  \/\/ top-level records\n+  HPROF_UTF8                    = 0x01,\n+  HPROF_LOAD_CLASS              = 0x02,\n+  HPROF_UNLOAD_CLASS            = 0x03,\n+  HPROF_FRAME                   = 0x04,\n+  HPROF_TRACE                   = 0x05,\n+  HPROF_ALLOC_SITES             = 0x06,\n+  HPROF_HEAP_SUMMARY            = 0x07,\n+  HPROF_START_THREAD            = 0x0A,\n+  HPROF_END_THREAD              = 0x0B,\n+  HPROF_HEAP_DUMP               = 0x0C,\n+  HPROF_CPU_SAMPLES             = 0x0D,\n+  HPROF_CONTROL_SETTINGS        = 0x0E,\n+\n+  \/\/ 1.0.2 record types\n+  HPROF_HEAP_DUMP_SEGMENT       = 0x1C,\n+  HPROF_HEAP_DUMP_END           = 0x2C,\n+\n+  \/\/ field types\n+  HPROF_ARRAY_OBJECT            = 0x01,\n+  HPROF_NORMAL_OBJECT           = 0x02,\n+  HPROF_BOOLEAN                 = 0x04,\n+  HPROF_CHAR                    = 0x05,\n+  HPROF_FLOAT                   = 0x06,\n+  HPROF_DOUBLE                  = 0x07,\n+  HPROF_BYTE                    = 0x08,\n+  HPROF_SHORT                   = 0x09,\n+  HPROF_INT                     = 0x0A,\n+  HPROF_LONG                    = 0x0B,\n+\n+  \/\/ data-dump sub-records\n+  HPROF_GC_ROOT_UNKNOWN         = 0xFF,\n+  HPROF_GC_ROOT_JNI_GLOBAL      = 0x01,\n+  HPROF_GC_ROOT_JNI_LOCAL       = 0x02,\n+  HPROF_GC_ROOT_JAVA_FRAME      = 0x03,\n+  HPROF_GC_ROOT_NATIVE_STACK    = 0x04,\n+  HPROF_GC_ROOT_STICKY_CLASS    = 0x05,\n+  HPROF_GC_ROOT_THREAD_BLOCK    = 0x06,\n+  HPROF_GC_ROOT_MONITOR_USED    = 0x07,\n+  HPROF_GC_ROOT_THREAD_OBJ      = 0x08,\n+  HPROF_GC_CLASS_DUMP           = 0x20,\n+  HPROF_GC_INSTANCE_DUMP        = 0x21,\n+  HPROF_GC_OBJ_ARRAY_DUMP       = 0x22,\n+  HPROF_GC_PRIM_ARRAY_DUMP      = 0x23\n+};\n+\n+\/\/ Default stack trace ID (used for dummy HPROF_TRACE record)\n+enum {\n+  STACK_TRACE_ID = 1,\n+  INITIAL_CLASS_COUNT = 200\n+};\n+\n+\n@@ -46,0 +356,1 @@\n+\/\/ HeapDumper is used to dump the java heap to file in HPROF binary format:\n@@ -83,0 +394,5 @@\n+\n+  \/\/ Parallel thread number for heap dump, initialize based on active processor count.\n+  static uint default_num_of_dump_threads() {\n+    return MAX2<uint>(1, (uint)os::initial_active_processor_count() * 3 \/ 8);\n+  }\n","filename":"src\/hotspot\/share\/services\/heapDumper.hpp","additions":327,"deletions":11,"binary":false,"changes":338,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright (c) 2023, Alibaba Group Holding Limited. All rights reserved.\n@@ -28,0 +29,1 @@\n+#include \"oops\/klass.inline.hpp\"\n@@ -32,0 +34,1 @@\n+#include \"services\/heapDumper.hpp\"\n@@ -34,1 +37,0 @@\n-\n@@ -144,3 +146,7 @@\n-WorkList::WorkList() {\n-  _head._next = &_head;\n-  _head._prev = &_head;\n+\n+void AbstractDumpWriter::write_fast(const void* s, size_t len) {\n+  assert(!_in_dump_segment || (_sub_record_left >= len), \"sub-record too large\");\n+  assert(buffer_size() - position() >= len, \"Must fit\");\n+  debug_only(_sub_record_left -= len);\n+  memcpy(buffer() + position(), s, len);\n+  set_position(position() + len);\n@@ -149,5 +155,2 @@\n-void WorkList::insert(WriteWork* before, WriteWork* work) {\n-  work->_prev = before;\n-  work->_next = before->_next;\n-  before->_next = work;\n-  work->_next->_prev = work;\n+bool AbstractDumpWriter::can_write_fast(size_t len) {\n+  return buffer_size() - position() >= len;\n@@ -156,8 +159,15 @@\n-WriteWork* WorkList::remove(WriteWork* work) {\n-  if (work != nullptr) {\n-    assert(work->_next != work, \"Invalid next\");\n-    assert(work->_prev != work, \"Invalid prev\");\n-    work->_prev->_next = work->_next;;\n-    work->_next->_prev = work->_prev;\n-    work->_next = nullptr;\n-    work->_prev = nullptr;\n+\/\/ write raw bytes\n+void AbstractDumpWriter::write_raw(const void* s, size_t len) {\n+  assert(!_in_dump_segment || (_sub_record_left >= len), \"sub-record too large\");\n+  debug_only(_sub_record_left -= len);\n+\n+  \/\/ flush buffer to make room.\n+  while (len > buffer_size() - position()) {\n+    assert(!_in_dump_segment || _is_huge_sub_record,\n+           \"Cannot overflow in non-huge sub-record.\");\n+    size_t to_write = buffer_size() - position();\n+    memcpy(buffer() + position(), s, to_write);\n+    s = (void*) ((char*) s + to_write);\n+    len -= to_write;\n+    set_position(position() + to_write);\n+    flush();\n@@ -166,1 +176,2 @@\n-  return work;\n+  memcpy(buffer() + position(), s, len);\n+  set_position(position() + len);\n@@ -169,10 +180,3 @@\n-void WorkList::add_by_id(WriteWork* work) {\n-  if (is_empty()) {\n-    add_first(work);\n-  } else {\n-    WriteWork* last_curr = &_head;\n-    WriteWork* curr = _head._next;\n-\n-    while (curr->_id < work->_id) {\n-      last_curr = curr;\n-      curr = curr->_next;\n+\/\/ Makes sure we inline the fast write into the write_u* functions. This is a big speedup.\n+#define WRITE_KNOWN_TYPE(p, len) do { if (can_write_fast((len))) write_fast((p), (len)); \\\n+                                      else write_raw((p), (len)); } while (0)\n@@ -180,8 +184,2 @@\n-      if (curr == &_head) {\n-        add_last(work);\n-        return;\n-      }\n-    }\n-\n-    insert(last_curr, work);\n-  }\n+void AbstractDumpWriter::write_u1(u1 x) {\n+  WRITE_KNOWN_TYPE(&x, 1);\n@@ -190,38 +188,4 @@\n-\n-\n-CompressionBackend::CompressionBackend(AbstractWriter* writer,\n-     AbstractCompressor* compressor, size_t block_size, size_t max_waste) :\n-  _active(false),\n-  _err(nullptr),\n-  _nr_of_threads(0),\n-  _works_created(0),\n-  _work_creation_failed(false),\n-  _id_to_write(0),\n-  _next_id(0),\n-  _in_size(block_size),\n-  _max_waste(max_waste),\n-  _out_size(0),\n-  _tmp_size(0),\n-  _written(0),\n-  _writer(writer),\n-  _compressor(compressor),\n-  _lock(new (std::nothrow) PaddedMonitor(Mutex::nosafepoint, \"HProfCompressionBackend_lock\")) {\n-  if (_writer == nullptr) {\n-    set_error(\"Could not allocate writer\");\n-  } else if (_lock == nullptr) {\n-    set_error(\"Could not allocate lock\");\n-  } else {\n-    set_error(_writer->open_writer());\n-  }\n-\n-  if (_compressor != nullptr) {\n-    set_error(_compressor->init(_in_size, &_out_size, &_tmp_size));\n-  }\n-\n-  _current = allocate_work(_in_size, _out_size, _tmp_size);\n-\n-  if (_current == nullptr) {\n-    set_error(\"Could not allocate memory for buffer\");\n-  }\n-\n-  _active = (_err == nullptr);\n+void AbstractDumpWriter::write_u2(u2 x) {\n+  u2 v;\n+  Bytes::put_Java_u2((address)&v, x);\n+  WRITE_KNOWN_TYPE(&v, 2);\n@@ -230,12 +194,4 @@\n-CompressionBackend::~CompressionBackend() {\n-  assert(!_active, \"Must not be active by now\");\n-  assert(_nr_of_threads == 0, \"Must have no active threads\");\n-  assert(_to_compress.is_empty() && _finished.is_empty(), \"Still work to do\");\n-\n-  free_work_list(&_unused);\n-  free_work(_current);\n-  assert(_works_created == 0, \"All work must have been freed\");\n-\n-  delete _compressor;\n-  delete _writer;\n-  delete _lock;\n+void AbstractDumpWriter::write_u4(u4 x) {\n+  u4 v;\n+  Bytes::put_Java_u4((address)&v, x);\n+  WRITE_KNOWN_TYPE(&v, 4);\n@@ -244,14 +200,4 @@\n-void CompressionBackend::flush_buffer(MonitorLocker* ml) {\n-\n-  \/\/ Make sure we write the last partially filled buffer.\n-  if ((_current != nullptr) && (_current->_in_used > 0)) {\n-    _current->_id = _next_id++;\n-    _to_compress.add_last(_current);\n-    _current = nullptr;\n-    ml->notify_all();\n-  }\n-\n-  \/\/ Wait for the threads to drain the compression work list and do some work yourself.\n-  while (!_to_compress.is_empty()) {\n-    do_foreground_work();\n-  }\n+void AbstractDumpWriter::write_u8(u8 x) {\n+  u8 v;\n+  Bytes::put_Java_u8((address)&v, x);\n+  WRITE_KNOWN_TYPE(&v, 8);\n@@ -260,5 +206,6 @@\n-void CompressionBackend::flush_buffer() {\n-  assert(_active, \"Must be active\");\n-\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-  flush_buffer(&ml);\n+void AbstractDumpWriter::write_address(address a) {\n+#ifdef _LP64\n+  write_u8((u8)a);\n+#else\n+  write_u4((u4)a);\n+#endif\n@@ -267,8 +214,2 @@\n-void CompressionBackend::deactivate() {\n-  assert(_active, \"Must be active\");\n-\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-  flush_buffer(&ml);\n-\n-  _active = false;\n-  ml.notify_all();\n+void AbstractDumpWriter::write_objectID(oop o) {\n+  write_address(cast_from_oop<address>(o));\n@@ -277,15 +218,2 @@\n-void CompressionBackend::thread_loop() {\n-  {\n-    MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-    _nr_of_threads++;\n-  }\n-\n-  WriteWork* work;\n-  while ((work = get_work()) != nullptr) {\n-    do_compress(work);\n-    finish_work(work);\n-  }\n-\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-  _nr_of_threads--;\n-  assert(_nr_of_threads >= 0, \"Too many threads finished\");\n+void AbstractDumpWriter::write_rootID(oop* p) {\n+  write_address((address)p);\n@@ -294,4 +222,2 @@\n-void CompressionBackend::set_error(char const* new_error) {\n-  if ((new_error != nullptr) && (_err == nullptr)) {\n-    _err = new_error;\n-  }\n+void AbstractDumpWriter::write_symbolID(Symbol* s) {\n+  write_address((address)((uintptr_t)s));\n@@ -300,3 +226,7 @@\n-WriteWork* CompressionBackend::allocate_work(size_t in_size, size_t out_size,\n-                                             size_t tmp_size) {\n-  WriteWork* result = (WriteWork*) os::malloc(sizeof(WriteWork), mtInternal);\n+void AbstractDumpWriter::write_id(u4 x) {\n+#ifdef _LP64\n+  write_u8((u8) x);\n+#else\n+  write_u4(x);\n+#endif\n+}\n@@ -304,4 +234,4 @@\n-  if (result == nullptr) {\n-    _work_creation_failed = true;\n-    return nullptr;\n-  }\n+\/\/ We use java mirror as the class ID\n+void AbstractDumpWriter::write_classID(Klass* k) {\n+  write_objectID(k->java_mirror());\n+}\n@@ -309,6 +239,16 @@\n-  _works_created++;\n-  result->_in = (char*) os::malloc(in_size, mtInternal);\n-  result->_in_max = in_size;\n-  result->_in_used = 0;\n-  result->_out = nullptr;\n-  result->_tmp = nullptr;\n+void AbstractDumpWriter::finish_dump_segment() {\n+  if (_in_dump_segment) {\n+    assert(_sub_record_left == 0, \"Last sub-record not written completely\");\n+    assert(_sub_record_ended, \"sub-record must have ended\");\n+\n+    \/\/ Fix up the dump segment length if we haven't written a huge sub-record last\n+    \/\/ (in which case the segment length was already set to the correct value initially).\n+    if (!_is_huge_sub_record) {\n+      assert(position() > dump_segment_header_size, \"Dump segment should have some content\");\n+      Bytes::put_Java_u4((address) (buffer() + 5),\n+                         (u4) (position() - dump_segment_header_size));\n+    } else {\n+      \/\/ Finish process huge sub record\n+      \/\/ Set _is_huge_sub_record to false so the parallel dump writer can flush data to file.\n+      _is_huge_sub_record = false;\n+    }\n@@ -316,2 +256,2 @@\n-  if (result->_in == nullptr) {\n-    goto fail;\n+    _in_dump_segment = false;\n+    flush();\n@@ -319,0 +259,1 @@\n+}\n@@ -320,7 +261,4 @@\n-  if (out_size > 0) {\n-    result->_out = (char*) os::malloc(out_size, mtInternal);\n-    result->_out_used = 0;\n-    result->_out_max = out_size;\n-\n-    if (result->_out == nullptr) {\n-      goto fail;\n+void AbstractDumpWriter::start_sub_record(u1 tag, u4 len) {\n+  if (!_in_dump_segment) {\n+    if (position() > 0) {\n+      flush();\n@@ -328,1 +266,0 @@\n-  }\n@@ -330,7 +267,18 @@\n-  if (tmp_size > 0) {\n-    result->_tmp = (char*) os::malloc(tmp_size, mtInternal);\n-    result->_tmp_max = tmp_size;\n-\n-    if (result->_tmp == nullptr) {\n-      goto fail;\n-    }\n+    assert(position() == 0 && buffer_size() > dump_segment_header_size, \"Must be at the start\");\n+\n+    write_u1(HPROF_HEAP_DUMP_SEGMENT);\n+    write_u4(0); \/\/ timestamp\n+    \/\/ Will be fixed up later if we add more sub-records.  If this is a huge sub-record,\n+    \/\/ this is already the correct length, since we don't add more sub-records.\n+    write_u4(len);\n+    assert(Bytes::get_Java_u4((address)(buffer() + 5)) == len, \"Inconsistent size!\");\n+    _in_dump_segment = true;\n+    _is_huge_sub_record = len > buffer_size() - dump_segment_header_size;\n+    ResourceMark rm;\n+  } else if (_is_huge_sub_record || (len > buffer_size() - position())) {\n+    \/\/ This object will not fit in completely or the last sub-record was huge.\n+    \/\/ Finish the current segment and try again.\n+    finish_dump_segment();\n+    start_sub_record(tag, len);\n+\n+    return;\n@@ -339,1 +287,2 @@\n-  return result;\n+  debug_only(_sub_record_left = len);\n+  debug_only(_sub_record_ended = false);\n@@ -341,4 +290,1 @@\n-fail:\n-  free_work(result);\n-  _work_creation_failed = true;\n-  return nullptr;\n+  write_u1(tag);\n@@ -347,8 +293,5 @@\n-void CompressionBackend::free_work(WriteWork* work) {\n-  if (work != nullptr) {\n-    os::free(work->_in);\n-    os::free(work->_out);\n-    os::free(work->_tmp);\n-    os::free(work);\n-    --_works_created;\n-  }\n+void AbstractDumpWriter::end_sub_record() {\n+  assert(_in_dump_segment, \"must be in dump segment\");\n+  assert(_sub_record_left == 0, \"sub-record not written completely\");\n+  assert(!_sub_record_ended, \"Must not have ended yet\");\n+  debug_only(_sub_record_ended = true);\n@@ -357,3 +300,25 @@\n-void CompressionBackend::free_work_list(WorkList* list) {\n-  while (!list->is_empty()) {\n-    free_work(list->remove_first());\n+DumpWriter::DumpWriter(FileWriter* writer, AbstractCompressor* compressor) :\n+  AbstractDumpWriter(),\n+  _writer(writer),\n+  _compressor(compressor),\n+  _bytes_written(0),\n+  _error(nullptr),\n+  _out_buffer(nullptr),\n+  _out_size(0),\n+  _out_pos(0),\n+  _tmp_buffer(nullptr),\n+  _tmp_size(0) {\n+  _error = (char*)_writer->open_writer();\n+  if (_error == nullptr) {\n+    _buffer = (char*)os::malloc(io_buffer_max_size, mtInternal);\n+    if (compressor != nullptr) {\n+      _error = (char*)_compressor->init(io_buffer_max_size, &_out_size, &_tmp_size);\n+      if (_error == nullptr) {\n+        if (_out_size > 0) {\n+          _out_buffer = (char*)os::malloc(_out_size, mtInternal);\n+        }\n+        if (_tmp_size > 0) {\n+          _tmp_buffer = (char*)os::malloc(_tmp_size, mtInternal);\n+        }\n+      }\n+    }\n@@ -361,0 +326,3 @@\n+  \/\/ initialize internal buffer\n+  _pos = 0;\n+  _size = io_buffer_max_size;\n@@ -363,15 +331,3 @@\n-void CompressionBackend::do_foreground_work() {\n-  assert(!_to_compress.is_empty(), \"Must have work to do\");\n-  assert(_lock->owned_by_self(), \"Must have the lock\");\n-\n-  WriteWork* work = _to_compress.remove_first();\n-  MutexUnlocker mu(_lock, Mutex::_no_safepoint_check_flag);\n-  do_compress(work);\n-  finish_work(work);\n-}\n-\n-WriteWork* CompressionBackend::get_work() {\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-\n-  while (_active && _to_compress.is_empty()) {\n-    ml.wait();\n+DumpWriter::~DumpWriter(){\n+  if (_buffer != nullptr) {\n+    os::free(_buffer);\n@@ -379,19 +335,2 @@\n-\n-  return _to_compress.remove_first();\n-}\n-\n-void CompressionBackend::flush_external_buffer(char* buffer, size_t used, size_t max) {\n-  assert(buffer != nullptr && used != 0 && max != 0, \"Invalid data send to compression backend\");\n-  assert(_active == true, \"Backend must be active when flushing external buffer\");\n-  char* buf;\n-  size_t tmp_used = 0;\n-  size_t tmp_max = 0;\n-\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-  \/\/ First try current buffer. Use it if empty.\n-  if (_current->_in_used == 0) {\n-    buf = _current->_in;\n-  } else {\n-    \/\/ If current buffer is not clean, flush it.\n-    MutexUnlocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-    get_new_buffer(&buf, &tmp_used, &tmp_max, true);\n+  if (_out_buffer != nullptr) {\n+    os::free(_out_buffer);\n@@ -399,7 +338,4 @@\n-  assert (_current->_in != nullptr && _current->_in_max >= max &&\n-          _current->_in_used == 0, \"Invalid buffer from compression backend\");\n-  \/\/ Copy data to backend buffer.\n-  memcpy(buf, buffer, used);\n-\n-  assert(_current->_in == buf, \"Must be current\");\n-  _current->_in_used += used;\n+  if (_tmp_buffer != nullptr) {\n+    os::free(_tmp_buffer);\n+  }\n+  _bytes_written = -1;\n@@ -408,47 +344,18 @@\n-void CompressionBackend::get_new_buffer(char** buffer, size_t* used, size_t* max, bool force_reset) {\n-  if (_active) {\n-    MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-    if (*used > 0 || force_reset) {\n-      _current->_in_used += *used;\n-      \/\/ Check if we do not waste more than _max_waste. If yes, write the buffer.\n-      \/\/ Otherwise return the rest of the buffer as the new buffer.\n-      if (_current->_in_max - _current->_in_used <= _max_waste || force_reset) {\n-        _current->_id = _next_id++;\n-        _to_compress.add_last(_current);\n-        _current = nullptr;\n-        ml.notify_all();\n-      } else {\n-        *buffer = _current->_in + _current->_in_used;\n-        *used = 0;\n-        *max = _current->_in_max - _current->_in_used;\n-        return;\n-      }\n-    }\n-\n-    while ((_current == nullptr) && _unused.is_empty() && _active) {\n-      \/\/ Add more work objects if needed.\n-      if (!_work_creation_failed && (_works_created <= _nr_of_threads)) {\n-        WriteWork* work = allocate_work(_in_size, _out_size, _tmp_size);\n-\n-        if (work != nullptr) {\n-          _unused.add_first(work);\n-        }\n-      } else if (!_to_compress.is_empty() && (_nr_of_threads == 0)) {\n-        do_foreground_work();\n-      } else {\n-        ml.wait();\n-      }\n-    }\n-\n-    if (_current == nullptr) {\n-      _current = _unused.remove_first();\n-    }\n-\n-    if (_current != nullptr) {\n-      _current->_in_used = 0;\n-      _current->_out_used = 0;\n-      *buffer = _current->_in;\n-      *used = 0;\n-      *max = _current->_in_max;\n-\n-      return;\n+\/\/ flush any buffered bytes to the file\n+void DumpWriter::flush() {\n+  if (_pos <= 0) {\n+    return;\n+  }\n+  if (has_error()) {\n+    _pos = 0;\n+    return;\n+  }\n+  char* result = nullptr;\n+  if (_compressor == nullptr) {\n+    result = (char*)_writer->write_buf(_buffer, _pos);\n+    _bytes_written += _pos;\n+  } else {\n+    do_compress();\n+    if (!has_error()) {\n+      result = (char*)_writer->write_buf(_out_buffer, _out_pos);\n+      _bytes_written += _out_pos;\n@@ -457,0 +364,1 @@\n+  _pos = 0; \/\/ reset pos to make internal buffer available\n@@ -458,17 +366,2 @@\n-  *buffer = nullptr;\n-  *used = 0;\n-  *max = 0;\n-\n-  return;\n-}\n-\n-void CompressionBackend::do_compress(WriteWork* work) {\n-  if (_compressor != nullptr) {\n-    char const* msg = _compressor->compress(work->_in, work->_in_used, work->_out,\n-                                            work->_out_max,\n-    work->_tmp, _tmp_size, &work->_out_used);\n-\n-    if (msg != nullptr) {\n-      MutexLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-      set_error(msg);\n-    }\n+  if (result != nullptr) {\n+    set_error(result);\n@@ -478,17 +371,3 @@\n-void CompressionBackend::finish_work(WriteWork* work) {\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-\n-  _finished.add_by_id(work);\n-\n-  \/\/ Write all finished works as far as we can.\n-  while (!_finished.is_empty() && (_finished.first()->_id == _id_to_write)) {\n-    WriteWork* to_write = _finished.remove_first();\n-    size_t size = _compressor == nullptr ? to_write->_in_used : to_write->_out_used;\n-    char* p = _compressor == nullptr ? to_write->_in : to_write->_out;\n-    char const* msg = nullptr;\n-\n-    if (_err == nullptr) {\n-      _written += size;\n-      MutexUnlocker mu(_lock, Mutex::_no_safepoint_check_flag);\n-      msg = _writer->write_buf(p, (ssize_t) size);\n-    }\n+void DumpWriter::do_compress() {\n+  const char* msg = _compressor->compress(_buffer, _pos, _out_buffer, _out_size,\n+                                          _tmp_buffer, _tmp_size, &_out_pos);\n@@ -496,0 +375,1 @@\n+  if (msg != nullptr) {\n@@ -497,2 +377,0 @@\n-    _unused.add_first(to_write);\n-    _id_to_write++;\n@@ -500,2 +378,0 @@\n-\n-  ml.notify_all();\n","filename":"src\/hotspot\/share\/services\/heapDumperCompression.cpp","additions":180,"deletions":304,"binary":false,"changes":484,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright (c) 2023, Alibaba Group Holding Limited. All rights reserved.\n@@ -77,0 +78,4 @@\n+\n+  const char* get_file_path() { return _path; }\n+\n+  bool is_overwrite() const { return _overwrite; }\n@@ -100,59 +105,61 @@\n-\n-\/\/ The data needed to write a single buffer (and compress it optionally).\n-struct WriteWork {\n-  \/\/ The id of the work.\n-  int64_t _id;\n-\n-  \/\/ The input buffer where the raw data is\n-  char* _in;\n-  size_t _in_used;\n-  size_t _in_max;\n-\n-  \/\/ The output buffer where the compressed data is. Is null when compression is disabled.\n-  char* _out;\n-  size_t _out_used;\n-  size_t _out_max;\n-\n-  \/\/ The temporary space needed for compression. Is null when compression is disabled.\n-  char* _tmp;\n-  size_t _tmp_max;\n-\n-  \/\/ Used to link WriteWorks into lists.\n-  WriteWork* _next;\n-  WriteWork* _prev;\n-};\n-\n-\/\/ A list for works.\n-class WorkList {\n-private:\n-  WriteWork _head;\n-\n-  void insert(WriteWork* before, WriteWork* work);\n-  WriteWork* remove(WriteWork* work);\n-\n-public:\n-  WorkList();\n-\n-  \/\/ Return true if the list is empty.\n-  bool is_empty() { return _head._next == &_head; }\n-\n-  \/\/ Adds to the beginning of the list.\n-  void add_first(WriteWork* work) { insert(&_head, work); }\n-\n-  \/\/ Adds to the end of the list.\n-  void add_last(WriteWork* work) { insert(_head._prev, work); }\n-\n-  \/\/ Adds so the ids are ordered.\n-  void add_by_id(WriteWork* work);\n-\n-  \/\/ Returns the first element.\n-  WriteWork* first() { return is_empty() ? nullptr : _head._next; }\n-\n-  \/\/ Returns the last element.\n-  WriteWork* last() { return is_empty() ? nullptr : _head._prev; }\n-\n-  \/\/ Removes the first element. Returns null if empty.\n-  WriteWork* remove_first() { return remove(first()); }\n-\n-  \/\/ Removes the last element. Returns null if empty.\n-  WriteWork* remove_last() { return remove(first()); }\n+\/\/ Base class for dump and parallel dump\n+class AbstractDumpWriter : public ResourceObj {\n+ protected:\n+  enum {\n+    io_buffer_max_size = 1*M,\n+    dump_segment_header_size = 9\n+  };\n+\n+  char* _buffer;    \/\/ internal buffer\n+  size_t _size;\n+  size_t _pos;\n+\n+  bool _in_dump_segment; \/\/ Are we currently in a dump segment?\n+  bool _is_huge_sub_record; \/\/ Are we writing a sub-record larger than the buffer size?\n+  DEBUG_ONLY(size_t _sub_record_left;) \/\/ The bytes not written for the current sub-record.\n+  DEBUG_ONLY(bool _sub_record_ended;) \/\/ True if we have called the end_sub_record().\n+\n+  char* buffer() const                          { return _buffer; }\n+  size_t buffer_size() const                    { return _size; }\n+  void set_position(size_t pos)                 { _pos = pos; }\n+\n+  \/\/ Can be called if we have enough room in the buffer.\n+  void write_fast(const void* s, size_t len);\n+\n+  \/\/ Returns true if we have enough room in the buffer for 'len' bytes.\n+  bool can_write_fast(size_t len);\n+\n+  void write_address(address a);\n+\n+ public:\n+  AbstractDumpWriter() :\n+    _buffer(nullptr),\n+    _size(io_buffer_max_size),\n+    _pos(0),\n+    _in_dump_segment(false) { }\n+\n+  size_t position() const                       { return _pos; }\n+  \/\/ writer functions\n+  virtual void write_raw(const void* s, size_t len);\n+  void write_u1(u1 x);\n+  void write_u2(u2 x);\n+  void write_u4(u4 x);\n+  void write_u8(u8 x);\n+  void write_objectID(oop o);\n+  void write_rootID(oop* p);\n+  void write_symbolID(Symbol* o);\n+  void write_classID(Klass* k);\n+  void write_id(u4 x);\n+\n+  \/\/ Start a new sub-record. Starts a new heap dump segment if needed.\n+  void start_sub_record(u1 tag, u4 len);\n+  \/\/ Ends the current sub-record.\n+  void end_sub_record();\n+  \/\/ Finishes the current dump segment if not already finished.\n+  void finish_dump_segment();\n+  \/\/ Flush internal buffer to persistent storage\n+  virtual void flush() = 0;\n+  \/\/ Total number of bytes written to the disk\n+  virtual julong bytes_written() const = 0;\n+  \/\/ Return non-null if error occurred\n+  virtual char const* error() const = 0;\n@@ -162,1 +169,1 @@\n-class Monitor;\n+\/\/ Supports I\/O operations for a dump\n@@ -164,38 +171,12 @@\n-\/\/ This class is used by the DumpWriter class. It supplies the DumpWriter with\n-\/\/ chunks of memory to write the heap dump data into. When the DumpWriter needs a\n-\/\/ new memory chunk, it calls get_new_buffer(), which commits the old chunk used\n-\/\/ and returns a new chunk. The old chunk is then added to a queue to be compressed\n-\/\/ and then written in the background.\n-class CompressionBackend : StackObj {\n-  bool _active;\n-  char const * _err;\n-\n-  int _nr_of_threads;\n-  int _works_created;\n-  bool _work_creation_failed;\n-\n-  int64_t _id_to_write;\n-  int64_t _next_id;\n-\n-  size_t _in_size;\n-  size_t _max_waste;\n-  size_t _out_size;\n-  size_t _tmp_size;\n-\n-  size_t _written;\n-\n-  AbstractWriter* const _writer;\n-  AbstractCompressor* const _compressor;\n-\n-  Monitor* const _lock;\n-\n-  WriteWork* _current;\n-  WorkList _to_compress;\n-  WorkList _unused;\n-  WorkList _finished;\n-\n-  void set_error(char const* new_error);\n-\n-  WriteWork* allocate_work(size_t in_size, size_t out_size, size_t tmp_size);\n-  void free_work(WriteWork* work);\n-  void free_work_list(WorkList* list);\n+class DumpWriter : public AbstractDumpWriter {\n+private:\n+ FileWriter* _writer;\n+ AbstractCompressor* _compressor;\n+ size_t _bytes_written;\n+ char* _error;\n+ \/\/ Compression support\n+ char* _out_buffer;\n+ size_t _out_size;\n+ size_t _out_pos;\n+ char* _tmp_buffer;\n+ size_t _tmp_size;\n@@ -203,5 +184,2 @@\n-  void do_foreground_work();\n-  WriteWork* get_work();\n-  void do_compress(WriteWork* work);\n-  void finish_work(WriteWork* work);\n-  void flush_buffer(MonitorLocker* ml);\n+private:\n+  void do_compress();\n@@ -210,28 +188,13 @@\n-  \/\/ compressor can be null if no compression is used.\n-  \/\/ Takes ownership of the writer and compressor.\n-  \/\/ block_size is the buffer size of a WriteWork.\n-  \/\/ max_waste is the maximum number of bytes to leave\n-  \/\/ empty in the buffer when it is written.\n-  CompressionBackend(AbstractWriter* writer, AbstractCompressor* compressor,\n-    size_t block_size, size_t max_waste);\n-\n-  ~CompressionBackend();\n-\n-  size_t get_written() const { return _written; }\n-\n-  char const* error() const { return _err; }\n-\n-  \/\/ Sets up an internal buffer, fills with external buffer, and sends to compressor.\n-  void flush_external_buffer(char* buffer, size_t used, size_t max);\n-\n-  \/\/ Commits the old buffer (using the value in *used) and sets up a new one.\n-  void get_new_buffer(char** buffer, size_t* used, size_t* max, bool force_reset = false);\n-\n-  \/\/ The entry point for a worker thread.\n-  void thread_loop();\n-\n-  \/\/ Shuts down the backend, releasing all threads.\n-  void deactivate();\n-\n-  \/\/ Flush all compressed data in buffer to file\n-  void flush_buffer();\n+  DumpWriter(FileWriter* writer, AbstractCompressor* compressor);\n+  ~DumpWriter();\n+  julong bytes_written() const override        { return (julong) _bytes_written; }\n+  void set_bytes_written(julong bytes_written) { _bytes_written = bytes_written; }\n+  char const* error() const override           { return _error; }\n+  void set_error(const char* error)            { _error = (char*)error; }\n+  bool has_error() const                       { return _error != nullptr; }\n+  const char* get_file_path() const            { return _writer->get_file_path(); }\n+  AbstractCompressor* compressor()             { return _compressor; }\n+  void set_compressor(AbstractCompressor* p)   { _compressor = p; }\n+  bool is_overwrite() const                    { return _writer->is_overwrite(); }\n+\n+  void flush() override;\n@@ -240,1 +203,0 @@\n-\n","filename":"src\/hotspot\/share\/services\/heapDumperCompression.hpp","additions":94,"deletions":132,"binary":false,"changes":226,"status":"modified"}]}