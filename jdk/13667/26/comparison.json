{"files":[{"patch":"@@ -91,0 +91,1 @@\n+  LOG_TAG(heapdump) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -324,0 +324,1 @@\n+  virtual bool is_AttachListener_thread() const      { return false; }\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+  template(HeapDumpMerge)                         \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -112,0 +112,1 @@\n+#include \"services\/attachListener.hpp\"\n@@ -1251,0 +1252,1 @@\n+        declare_type(AttachListenerThread, JavaThread)                    \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -247,4 +247,1 @@\n-    \/\/ Parallel thread number for heap dump, initialize based on active processor count.\n-    \/\/ Note the real number of threads used is also determined by active workers and compression\n-    \/\/ backend thread number. See heapDumper.cpp.\n-    uint parallel_thread_num = MAX2<uint>(1, (uint)os::initial_active_processor_count() * 3 \/ 8);\n+\n@@ -255,1 +252,1 @@\n-    dumper.dump(path, out, (int)level, false, (uint)parallel_thread_num);\n+    dumper.dump(path, out, (int)level, false, HeapDumper::default_num_of_dump_threads());\n@@ -378,1 +375,1 @@\n-static void attach_listener_thread_entry(JavaThread* thread, TRAPS) {\n+void AttachListenerThread::thread_entry(JavaThread* thread, TRAPS) {\n@@ -463,1 +460,1 @@\n-  JavaThread* thread = new JavaThread(&attach_listener_thread_entry);\n+  JavaThread* thread = new AttachListenerThread();\n","filename":"src\/hotspot\/share\/services\/attachListener.cpp","additions":4,"deletions":7,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n@@ -61,0 +62,9 @@\n+class AttachListenerThread : public JavaThread {\n+private:\n+  static void thread_entry(JavaThread* thread, TRAPS);\n+\n+public:\n+  AttachListenerThread() : JavaThread(&AttachListenerThread::thread_entry) {}\n+  bool is_AttachListener_thread() const { return true; }\n+};\n+\n","filename":"src\/hotspot\/share\/services\/attachListener.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -472,1 +472,4 @@\n-           \"BOOLEAN\", false, \"false\") {\n+           \"BOOLEAN\", false, \"false\"),\n+  _parallel(\"-parallel\", \"Number of parallel threads to use for heap dump. The VM \"\n+                          \"will try to use the specified number of threads, but might use fewer.\",\n+            \"INT\", false, \"1\") {\n@@ -477,0 +480,1 @@\n+  _dcmdparser.add_dcmd_option(&_parallel);\n@@ -481,0 +485,1 @@\n+  jlong parallel = HeapDumper::default_num_of_dump_threads();\n@@ -491,0 +496,12 @@\n+  if (_parallel.is_set()) {\n+    parallel = _parallel.value();\n+\n+    if (parallel < 0) {\n+      output()->print_cr(\"Invalid number of parallel dump threads.\");\n+      return;\n+    } else if (parallel == 0) {\n+      \/\/ 0 implies to disable parallel heap dump, in such case, we use serial dump instead\n+      parallel = 1;\n+    }\n+  }\n+\n@@ -495,1 +512,1 @@\n-  dumper.dump(_filename.value(), output(), (int) level, _overwrite.value());\n+  dumper.dump(_filename.value(), output(), (int) level, _overwrite.value(), (uint)parallel);\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":19,"deletions":2,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -323,0 +323,1 @@\n+  DCmdArgument<jlong> _parallel;\n@@ -324,1 +325,1 @@\n-  static int num_arguments() { return 4; }\n+  static int num_arguments() { return 5; }\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright (c) 2023, Alibaba Group Holding Limited. All rights reserved.\n@@ -57,0 +58,1 @@\n+#include \"runtime\/timerTrace.hpp\"\n@@ -385,1 +387,1 @@\n-class AbstractDumpWriter : public StackObj {\n+class AbstractDumpWriter : public ResourceObj {\n@@ -389,1 +391,0 @@\n-    io_buffer_max_waste = 10*K,\n@@ -402,2 +403,0 @@\n-  virtual void flush(bool force = false) = 0;\n-\n@@ -423,1 +422,1 @@\n-  \/\/ total number of bytes written to the disk\n+  \/\/ Total number of bytes written to the disk\n@@ -425,0 +424,1 @@\n+  \/\/ Return non-null if error occurred\n@@ -445,12 +445,3 @@\n-  void finish_dump_segment(bool force_flush = false);\n-  \/\/ Refresh to get new buffer\n-  void refresh() {\n-    assert (_in_dump_segment ==false, \"Sanity check\");\n-    _buffer = nullptr;\n-    _size = io_buffer_max_size;\n-    _pos = 0;\n-    \/\/ Force flush to guarantee data from parallel dumper are written.\n-    flush(true);\n-  }\n-  \/\/ Called when finished to release the threads.\n-  virtual void deactivate() = 0;\n+  void finish_dump_segment();\n+  \/\/ Flush internal buffer to persistent storage\n+  virtual void flush() = 0;\n@@ -551,1 +542,1 @@\n-void AbstractDumpWriter::finish_dump_segment(bool force_flush) {\n+void AbstractDumpWriter::finish_dump_segment() {\n@@ -569,1 +560,1 @@\n-    flush(force_flush);\n+    flush();\n@@ -614,21 +605,29 @@\n- private:\n-  CompressionBackend _backend; \/\/ Does the actual writing.\n- protected:\n-  void flush(bool force = false) override;\n-\n- public:\n-  \/\/ Takes ownership of the writer and compressor.\n-  DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor);\n-\n-  \/\/ total number of bytes written to the disk\n-  julong bytes_written() const override { return (julong) _backend.get_written(); }\n-\n-  char const* error() const override    { return _backend.error(); }\n-\n-  \/\/ Called by threads used for parallel writing.\n-  void writer_loop()                    { _backend.thread_loop(); }\n-  \/\/ Called when finish to release the threads.\n-  void deactivate() override            { flush(); _backend.deactivate(); }\n-  \/\/ Get the backend pointer, used by parallel dump writer.\n-  CompressionBackend* backend_ptr()     { return &_backend; }\n-\n+private:\n+  FileWriter* _writer;\n+  AbstractCompressor* _compressor;\n+  size_t _bytes_written;\n+  char* _error;\n+  \/\/ Compression support\n+  char* _out_buffer;\n+  size_t _out_size;\n+  size_t _out_pos;\n+  char* _tmp_buffer;\n+  size_t _tmp_size;\n+\n+private:\n+  void do_compress();\n+\n+public:\n+  DumpWriter(const char* path, bool overwrite, AbstractCompressor* compressor);\n+  ~DumpWriter();\n+  julong bytes_written() const override        { return (julong) _bytes_written; }\n+  void set_bytes_written(julong bytes_written) { _bytes_written = bytes_written; }\n+  char const* error() const override           { return _error; }\n+  void set_error(const char* error)            { _error = (char*)error; }\n+  bool has_error() const                       { return _error != nullptr; }\n+  const char* get_file_path() const            { return _writer->get_file_path(); }\n+  AbstractCompressor* compressor()             { return _compressor; }\n+  void set_compressor(AbstractCompressor* p)   { _compressor = p; }\n+  bool is_overwrite() const                    { return _writer->is_overwrite(); }\n+\n+  void flush() override;\n@@ -637,2 +636,1 @@\n-\/\/ Check for error after constructing the object and destroy it in case of an error.\n-DumpWriter::DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor) :\n+DumpWriter::DumpWriter(const char* path, bool overwrite, AbstractCompressor* compressor) :\n@@ -640,32 +638,22 @@\n-  _backend(writer, compressor, io_buffer_max_size, io_buffer_max_waste) {\n-  flush();\n-}\n-\n-\/\/ flush any buffered bytes to the file\n-void DumpWriter::flush(bool force) {\n-  _backend.get_new_buffer(&_buffer, &_pos, &_size, force);\n-}\n-\n-\/\/ Buffer queue used for parallel dump.\n-struct ParWriterBufferQueueElem {\n-  char* _buffer;\n-  size_t _used;\n-  ParWriterBufferQueueElem* _next;\n-};\n-\n-class ParWriterBufferQueue : public CHeapObj<mtInternal> {\n- private:\n-  ParWriterBufferQueueElem* _head;\n-  ParWriterBufferQueueElem* _tail;\n-  uint _length;\n- public:\n-  ParWriterBufferQueue() : _head(nullptr), _tail(nullptr), _length(0) { }\n-\n-  void enqueue(ParWriterBufferQueueElem* entry) {\n-    if (_head == nullptr) {\n-      assert(is_empty() && _tail == nullptr, \"Sanity check\");\n-      _head = _tail = entry;\n-    } else {\n-      assert ((_tail->_next == nullptr && _tail->_buffer != nullptr), \"Buffer queue is polluted\");\n-      _tail->_next = entry;\n-      _tail = entry;\n+  _writer(new (std::nothrow) FileWriter(path, overwrite)),\n+  _compressor(compressor),\n+  _bytes_written(0),\n+  _error(nullptr),\n+  _out_buffer(nullptr),\n+  _out_size(0),\n+  _out_pos(0),\n+  _tmp_buffer(nullptr),\n+  _tmp_size(0) {\n+  _error = (char*)_writer->open_writer();\n+  if (_error == nullptr) {\n+    _buffer = (char*)os::malloc(io_buffer_max_size, mtInternal);\n+    if (compressor != nullptr) {\n+      _error = (char*)_compressor->init(io_buffer_max_size, &_out_size, &_tmp_size);\n+      if (_error == nullptr) {\n+        if (_out_size > 0) {\n+          _out_buffer = (char*)os::malloc(_out_size, mtInternal);\n+        }\n+        if (_tmp_size > 0) {\n+          _tmp_buffer = (char*)os::malloc(_tmp_size, mtInternal);\n+        }\n+      }\n@@ -673,2 +661,0 @@\n-    _length++;\n-    assert(_tail->_next == nullptr, \"Buffer queue is polluted\");\n@@ -676,0 +662,4 @@\n+  \/\/ initialize internal buffer\n+  _pos = 0;\n+  _size = io_buffer_max_size;\n+}\n@@ -677,11 +667,3 @@\n-  ParWriterBufferQueueElem* dequeue() {\n-    if (_head == nullptr)  return nullptr;\n-    ParWriterBufferQueueElem* entry = _head;\n-    assert (entry->_buffer != nullptr, \"polluted buffer in writer list\");\n-    _head = entry->_next;\n-    if (_head == nullptr) {\n-      _tail = nullptr;\n-    }\n-    entry->_next = nullptr;\n-    _length--;\n-    return entry;\n+DumpWriter::~DumpWriter(){\n+  if (_buffer != nullptr) {\n+    os::free(_buffer);\n@@ -689,3 +671,2 @@\n-\n-  bool is_empty() {\n-    return _length == 0;\n+  if (_out_buffer != nullptr) {\n+    os::free(_out_buffer);\n@@ -693,32 +674,2 @@\n-\n-  uint length() { return _length; }\n-};\n-\n-\/\/ Support parallel heap dump.\n-class ParDumpWriter : public AbstractDumpWriter {\n- private:\n-  \/\/ Lock used to guarantee the integrity of multiple buffers writing.\n-  static Monitor* _lock;\n-  \/\/ Pointer of backend from global DumpWriter.\n-  CompressionBackend* _backend_ptr;\n-  char const * _err;\n-  ParWriterBufferQueue* _buffer_queue;\n-  size_t _internal_buffer_used;\n-  char* _buffer_base;\n-  bool _split_data;\n-  static const uint BackendFlushThreshold = 2;\n- protected:\n-  void flush(bool force = false) override {\n-    assert(_pos != 0, \"must not be zero\");\n-    if (_pos != 0) {\n-      refresh_buffer();\n-    }\n-\n-    if (_split_data || _is_huge_sub_record) {\n-      return;\n-    }\n-\n-    if (should_flush_buf_list(force)) {\n-      assert(!_in_dump_segment && !_split_data && !_is_huge_sub_record, \"incomplete data send to backend!\\n\");\n-      flush_to_backend(force);\n-    }\n+  if (_tmp_buffer != nullptr) {\n+    os::free(_tmp_buffer);\n@@ -726,60 +677,2 @@\n-\n- public:\n-  \/\/ Check for error after constructing the object and destroy it in case of an error.\n-  ParDumpWriter(DumpWriter* dw) :\n-    AbstractDumpWriter(),\n-    _backend_ptr(dw->backend_ptr()),\n-    _buffer_queue((new (std::nothrow) ParWriterBufferQueue())),\n-    _buffer_base(nullptr),\n-    _split_data(false) {\n-    \/\/ prepare internal buffer\n-    allocate_internal_buffer();\n-  }\n-\n-  ~ParDumpWriter() {\n-     assert(_buffer_queue != nullptr, \"Sanity check\");\n-     assert((_internal_buffer_used == 0) && (_buffer_queue->is_empty()),\n-            \"All data must be send to backend\");\n-     if (_buffer_base != nullptr) {\n-       os::free(_buffer_base);\n-       _buffer_base = nullptr;\n-     }\n-     delete _buffer_queue;\n-     _buffer_queue = nullptr;\n-  }\n-\n-  \/\/ total number of bytes written to the disk\n-  julong bytes_written() const override { return (julong) _backend_ptr->get_written(); }\n-  char const* error() const override    { return _err == nullptr ? _backend_ptr->error() : _err; }\n-\n-  static void before_work() {\n-    assert(_lock == nullptr, \"ParDumpWriter lock must be initialized only once\");\n-    _lock = new (std::nothrow) PaddedMonitor(Mutex::safepoint, \"ParallelHProfWriter_lock\");\n-  }\n-\n-  static void after_work() {\n-    assert(_lock != nullptr, \"ParDumpWriter lock is not initialized\");\n-    delete _lock;\n-    _lock = nullptr;\n-  }\n-\n-  \/\/ write raw bytes\n-  void write_raw(const void* s, size_t len) override {\n-    assert(!_in_dump_segment || (_sub_record_left >= len), \"sub-record too large\");\n-    debug_only(_sub_record_left -= len);\n-    assert(!_split_data, \"Invalid split data\");\n-    _split_data = true;\n-    \/\/ flush buffer to make room.\n-    while (len > buffer_size() - position()) {\n-      assert(!_in_dump_segment || _is_huge_sub_record,\n-             \"Cannot overflow in non-huge sub-record.\");\n-      size_t to_write = buffer_size() - position();\n-      memcpy(buffer() + position(), s, to_write);\n-      s = (void*) ((char*) s + to_write);\n-      len -= to_write;\n-      set_position(position() + to_write);\n-      flush();\n-    }\n-    _split_data = false;\n-    memcpy(buffer() + position(), s, len);\n-    set_position(position() + len);\n+  if (_writer != NULL) {\n+    delete _writer;\n@@ -787,0 +680,2 @@\n+  _bytes_written = -1;\n+}\n@@ -788,11 +683,6 @@\n-  void deactivate() override { flush(true); _backend_ptr->deactivate(); }\n-\n- private:\n-  void allocate_internal_buffer() {\n-    assert(_buffer_queue != nullptr, \"Internal buffer queue is not ready when allocate internal buffer\");\n-    assert(_buffer == nullptr && _buffer_base == nullptr, \"current buffer must be null before allocate\");\n-    _buffer_base = _buffer = (char*)os::malloc(io_buffer_max_size, mtInternal);\n-    if (_buffer == nullptr) {\n-      set_error(\"Could not allocate buffer for writer\");\n-      return;\n-    }\n+\/\/ flush any buffered bytes to the file\n+void DumpWriter::flush() {\n+  if (_pos <= 0) {\n+    return;\n+  }\n+  if (has_error()) {\n@@ -800,2 +690,1 @@\n-    _internal_buffer_used = 0;\n-    _size = io_buffer_max_size;\n+    return;\n@@ -803,4 +692,9 @@\n-\n-  void set_error(char const* new_error) {\n-    if ((new_error != nullptr) && (_err == nullptr)) {\n-      _err = new_error;\n+  char* result = nullptr;\n+  if (_compressor == nullptr) {\n+    result = (char*)_writer->write_buf(_buffer, _pos);\n+    _bytes_written += _pos;\n+  } else {\n+    do_compress();\n+    if (!has_error()) {\n+      result = (char*)_writer->write_buf(_out_buffer, _out_pos);\n+      _bytes_written += _out_pos;\n@@ -809,0 +703,1 @@\n+  _pos = 0; \/\/ reset pos to make internal buffer available\n@@ -810,71 +705,2 @@\n-  \/\/ Add buffer to internal list\n-  void refresh_buffer() {\n-    size_t expected_total = _internal_buffer_used + _pos;\n-    if (expected_total < io_buffer_max_size - io_buffer_max_waste) {\n-      \/\/ reuse current buffer.\n-      _internal_buffer_used = expected_total;\n-      assert(_size - _pos == io_buffer_max_size - expected_total, \"illegal resize of buffer\");\n-      _size -= _pos;\n-      _buffer += _pos;\n-      _pos = 0;\n-\n-      return;\n-    }\n-    \/\/ It is not possible here that expected_total is larger than io_buffer_max_size because\n-    \/\/ of limitation in write_xxx().\n-    assert(expected_total <= io_buffer_max_size, \"buffer overflow\");\n-    assert(_buffer - _buffer_base <= io_buffer_max_size, \"internal buffer overflow\");\n-    ParWriterBufferQueueElem* entry =\n-        (ParWriterBufferQueueElem*)os::malloc(sizeof(ParWriterBufferQueueElem), mtInternal);\n-    if (entry == nullptr) {\n-      set_error(\"Heap dumper can allocate memory\");\n-      return;\n-    }\n-    entry->_buffer = _buffer_base;\n-    entry->_used = expected_total;\n-    entry->_next = nullptr;\n-    \/\/ add to internal buffer queue\n-    _buffer_queue->enqueue(entry);\n-    _buffer_base =_buffer = nullptr;\n-    allocate_internal_buffer();\n-  }\n-\n-  void reclaim_entry(ParWriterBufferQueueElem* entry) {\n-    assert(entry != nullptr && entry->_buffer != nullptr, \"Invalid entry to reclaim\");\n-    os::free(entry->_buffer);\n-    entry->_buffer = nullptr;\n-    os::free(entry);\n-  }\n-\n-  void flush_buffer(char* buffer, size_t used) {\n-    assert(_lock->owner() == Thread::current(), \"flush buffer must hold lock\");\n-    size_t max = io_buffer_max_size;\n-    \/\/ get_new_buffer\n-    _backend_ptr->flush_external_buffer(buffer, used, max);\n-  }\n-\n-  bool should_flush_buf_list(bool force) {\n-    return force || _buffer_queue->length() > BackendFlushThreshold;\n-  }\n-\n-  void flush_to_backend(bool force) {\n-    \/\/ Guarantee there is only one writer updating the backend buffers.\n-    MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-    while (!_buffer_queue->is_empty()) {\n-      ParWriterBufferQueueElem* entry = _buffer_queue->dequeue();\n-      flush_buffer(entry->_buffer, entry->_used);\n-      \/\/ Delete buffer and entry.\n-      reclaim_entry(entry);\n-      entry = nullptr;\n-    }\n-    assert(_pos == 0, \"available buffer must be empty before flush\");\n-    \/\/ Flush internal buffer.\n-    if (_internal_buffer_used > 0) {\n-      flush_buffer(_buffer_base, _internal_buffer_used);\n-      os::free(_buffer_base);\n-      _pos = 0;\n-      _internal_buffer_used = 0;\n-      _buffer_base = _buffer = nullptr;\n-      \/\/ Allocate internal buffer for future use.\n-      allocate_internal_buffer();\n-    }\n+  if (result != nullptr) {\n+    set_error(result);\n@@ -882,1 +708,1 @@\n-};\n+}\n@@ -884,1 +710,8 @@\n-Monitor* ParDumpWriter::_lock = nullptr;\n+void DumpWriter::do_compress() {\n+  const char* msg = _compressor->compress(_buffer, _pos, _out_buffer, _out_size,\n+                                          _tmp_buffer, _tmp_size, &_out_pos);\n+\n+  if (msg != nullptr) {\n+    set_error(msg);\n+  }\n+}\n@@ -1616,67 +1449,0 @@\n-\/\/ Large object heap dump support.\n-\/\/ To avoid memory consumption, when dumping large objects such as huge array and\n-\/\/ large objects whose size are larger than LARGE_OBJECT_DUMP_THRESHOLD, the scanned\n-\/\/ partial object\/array data will be sent to the backend directly instead of caching\n-\/\/ the whole object\/array in the internal buffer.\n-\/\/ The HeapDumpLargeObjectList is used to save the large object when dumper scans\n-\/\/ the heap. The large objects could be added (push) parallelly by multiple dumpers,\n-\/\/ But they will be removed (popped) serially only by the VM thread.\n-class HeapDumpLargeObjectList : public CHeapObj<mtInternal> {\n- private:\n-  class HeapDumpLargeObjectListElem : public CHeapObj<mtInternal> {\n-   public:\n-    HeapDumpLargeObjectListElem(oop obj) : _obj(obj), _next(nullptr) { }\n-    oop _obj;\n-    HeapDumpLargeObjectListElem* _next;\n-  };\n-\n-  volatile HeapDumpLargeObjectListElem* _head;\n-\n- public:\n-  HeapDumpLargeObjectList() : _head(nullptr) { }\n-\n-  void atomic_push(oop obj) {\n-    assert (obj != nullptr, \"sanity check\");\n-    HeapDumpLargeObjectListElem* entry = new HeapDumpLargeObjectListElem(obj);\n-    if (entry == nullptr) {\n-      warning(\"failed to allocate element for large object list\");\n-      return;\n-    }\n-    assert (entry->_obj != nullptr, \"sanity check\");\n-    while (true) {\n-      volatile HeapDumpLargeObjectListElem* old_head = Atomic::load_acquire(&_head);\n-      HeapDumpLargeObjectListElem* new_head = entry;\n-      if (Atomic::cmpxchg(&_head, old_head, new_head) == old_head) {\n-        \/\/ successfully push\n-        new_head->_next = (HeapDumpLargeObjectListElem*)old_head;\n-        return;\n-      }\n-    }\n-  }\n-\n-  oop pop() {\n-    if (_head == nullptr) {\n-      return nullptr;\n-    }\n-    HeapDumpLargeObjectListElem* entry = (HeapDumpLargeObjectListElem*)_head;\n-    _head = _head->_next;\n-    assert (entry != nullptr, \"illegal larger object list entry\");\n-    oop ret = entry->_obj;\n-    delete entry;\n-    assert (ret != nullptr, \"illegal oop pointer\");\n-    return ret;\n-  }\n-\n-  void drain(ObjectClosure* cl) {\n-    while (_head !=  nullptr) {\n-      cl->do_object(pop());\n-    }\n-  }\n-\n-  bool is_empty() {\n-    return _head == nullptr;\n-  }\n-\n-  static const size_t LargeObjectSizeThreshold = 1 << 20; \/\/ 1 MB\n-};\n-\n@@ -1689,2 +1455,0 @@\n-  HeapDumpLargeObjectList* _list;\n-\n@@ -1692,1 +1456,1 @@\n-  bool is_large(oop o);\n+\n@@ -1694,1 +1458,1 @@\n-  HeapObjectDumper(AbstractDumpWriter* writer, HeapDumpLargeObjectList* list = nullptr) {\n+  HeapObjectDumper(AbstractDumpWriter* writer) {\n@@ -1696,1 +1460,0 @@\n-    _list = list;\n@@ -1716,7 +1479,0 @@\n-  \/\/ If large object list exists and it is large object\/array,\n-  \/\/ add oop into the list and skip scan. VM thread will process it later.\n-  if (_list != nullptr && is_large(o)) {\n-    _list->atomic_push(o);\n-    return;\n-  }\n-\n@@ -1735,23 +1491,0 @@\n-bool HeapObjectDumper::is_large(oop o) {\n-  size_t size = 0;\n-  if (o->is_instance()) {\n-    \/\/ Use o->size() * 8 as the upper limit of instance size to avoid iterating static fields\n-    size = o->size() * 8;\n-  } else if (o->is_objArray()) {\n-    objArrayOop array = objArrayOop(o);\n-    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n-    assert(type >= T_BOOLEAN && type <= T_OBJECT, \"invalid array element type\");\n-    int length = array->length();\n-    int type_size = sizeof(address);\n-    size = (size_t)length * type_size;\n-  } else if (o->is_typeArray()) {\n-    typeArrayOop array = typeArrayOop(o);\n-    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n-    assert(type >= T_BOOLEAN && type <= T_OBJECT, \"invalid array element type\");\n-    int length = array->length();\n-    int type_size = type2aelembytes(type);\n-    size = (size_t)length * type_size;\n-  }\n-  return size > HeapDumpLargeObjectList::LargeObjectSizeThreshold;\n-}\n-\n@@ -1761,1 +1494,0 @@\n-   bool     _started;\n@@ -1763,1 +1495,1 @@\n-   uint   _dumper_number;\n+   const uint   _dumper_number;\n@@ -1768,1 +1500,0 @@\n-     _started(false),\n@@ -1775,17 +1506,1 @@\n-   void wait_for_start_signal() {\n-     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-     while (_started == false) {\n-       ml.wait();\n-     }\n-     assert(_started == true,  \"dumper woke up with wrong state\");\n-   }\n-\n-   void start_dump() {\n-     assert (_started == false, \"start dump with wrong state\");\n-     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-     _started = true;\n-     ml.notify_all();\n-   }\n-\n-   void dumper_complete() {\n-     assert (_started == true, \"dumper complete with wrong state\");\n+   void dumper_complete(DumpWriter* local_writer, DumpWriter* global_writer) {\n@@ -1794,0 +1509,4 @@\n+     \/\/ propagate local error to global if any\n+     if (local_writer->has_error()) {\n+       global_writer->set_error(local_writer->error());\n+     }\n@@ -1798,1 +1517,0 @@\n-     assert (_started == true, \"wrong state when wait for dumper complete\");\n@@ -1803,1 +1521,0 @@\n-     _started = false;\n@@ -1807,0 +1524,99 @@\n+\/\/ DumpMerger merges separate dump files into a complete one\n+class DumpMerger : public StackObj {\n+private:\n+  DumpWriter* _writer;\n+  const char* _path;\n+  bool _has_error;\n+  int _dump_seq;\n+\n+private:\n+  void merge_file(char* path);\n+  void merge_done();\n+\n+public:\n+  DumpMerger(const char* path, DumpWriter* writer, int dump_seq) :\n+    _writer(writer),\n+    _path(path),\n+    _has_error(_writer->has_error()),\n+    _dump_seq(dump_seq) {}\n+\n+  void do_merge();\n+};\n+\n+void DumpMerger::merge_done() {\n+  \/\/ Writes the HPROF_HEAP_DUMP_END record.\n+  if (!_has_error) {\n+    DumperSupport::end_of_dump(_writer);\n+    _writer->flush();\n+  }\n+  _dump_seq = 0; \/\/reset\n+}\n+\n+void DumpMerger::merge_file(char* path) {\n+  assert(!SafepointSynchronize::is_at_safepoint(), \"merging happens outside safepoint\");\n+  TraceTime timer(\"Merge segmented heap file\", TRACETIME_LOG(Info, heapdump));\n+\n+  fileStream segment_fs(path, \"rb\");\n+  if (!segment_fs.is_open()) {\n+    log_error(heapdump)(\"Can not open segmented heap file %s during merging\", path);\n+    _writer->set_error(\"Can not open segmented heap file during merging\");\n+    _has_error = true;\n+    return;\n+  }\n+\n+  jlong total = 0;\n+  size_t cnt = 0;\n+  char read_buf[4096];\n+  while ((cnt = segment_fs.read(read_buf, 1, 4096)) != 0) {\n+    _writer->write_raw(read_buf, cnt);\n+    total += cnt;\n+  }\n+\n+  _writer->flush();\n+  if (segment_fs.fileSize() != total) {\n+    log_error(heapdump)(\"Merged heap dump %s is incomplete, expect %ld but read \" JLONG_FORMAT \" bytes\",\n+                        path, segment_fs.fileSize(), total);\n+    _writer->set_error(\"Merged heap dump is incomplete\");\n+    _has_error = true;\n+  }\n+}\n+\n+void DumpMerger::do_merge() {\n+  assert(!SafepointSynchronize::is_at_safepoint(), \"merging happens outside safepoint\");\n+  TraceTime timer(\"Merge heap files complete\", TRACETIME_LOG(Info, heapdump));\n+\n+  \/\/ Since contents in segmented heap file were already zipped, we don't need to zip\n+  \/\/ them again during merging.\n+  AbstractCompressor* saved_compressor = _writer->compressor();\n+  _writer->set_compressor(nullptr);\n+\n+  \/\/ merge segmented heap file and remove it anyway\n+  char path[JVM_MAXPATHLEN];\n+  for (int i = 0; i < _dump_seq; i++) {\n+    memset(path, 0, JVM_MAXPATHLEN);\n+    os::snprintf(path, JVM_MAXPATHLEN, \"%s.p%d\", _path, i);\n+    if (!_has_error) {\n+      merge_file(path);\n+    }\n+    remove(path);\n+  }\n+\n+  \/\/ restore compressor for further use\n+  _writer->set_compressor(saved_compressor);\n+  merge_done();\n+}\n+\n+\/\/ The VM operation wraps DumpMerger so that it could be performed by VM thread\n+class VM_HeapDumpMerge : public VM_Operation {\n+private:\n+  DumpMerger* _merger;\n+public:\n+  VM_HeapDumpMerge(DumpMerger* merger) : _merger(merger) {}\n+  VMOp_Type type() const { return VMOp_HeapDumpMerge; }\n+  \/\/ heap dump merge could happen outside safepoint\n+  virtual bool evaluate_at_safepoint() const { return false; }\n+  void doit() {\n+    _merger->do_merge();\n+  }\n+};\n+\n@@ -1819,0 +1635,1 @@\n+  volatile int            _dump_seq;\n@@ -1821,1 +1638,0 @@\n-  uint                    _num_writer_threads;\n@@ -1824,6 +1640,0 @@\n-  HeapDumpLargeObjectList* _large_object_list;\n-\n-  \/\/ VMDumperType is for thread that dumps both heap and non-heap data.\n-  static const size_t VMDumperType = 0;\n-  static const size_t WriterType = 1;\n-  static const size_t DumperType = 2;\n@@ -1832,46 +1642,2 @@\n-\n-  size_t get_worker_type(uint worker_id) {\n-    assert(_num_writer_threads >= 1, \"Must be at least one writer\");\n-    \/\/ worker id of VMDumper that dump heap and non-heap data\n-    if (worker_id == VMDumperWorkerId) {\n-      return VMDumperType;\n-    }\n-\n-    \/\/ worker id of dumper starts from 1, which only dump heap datar\n-    if (worker_id < _num_dumper_threads) {\n-      return DumperType;\n-    }\n-\n-    \/\/ worker id of writer starts from _num_dumper_threads\n-    return WriterType;\n-  }\n-\n-  void prepare_parallel_dump(uint num_total) {\n-    assert (_dumper_controller == nullptr, \"dumper controller must be null\");\n-    assert (num_total > 0, \"active workers number must >= 1\");\n-    \/\/ Dumper threads number must not be larger than active workers number.\n-    if (num_total < _num_dumper_threads) {\n-      _num_dumper_threads = num_total - 1;\n-    }\n-    \/\/ Calculate dumper and writer threads number.\n-    _num_writer_threads = num_total - _num_dumper_threads;\n-    \/\/ If dumper threads number is 1, only the VMThread works as a dumper.\n-    \/\/ If dumper threads number is equal to active workers, need at lest one worker thread as writer.\n-    if (_num_dumper_threads > 0 && _num_writer_threads == 0) {\n-      _num_writer_threads = 1;\n-      _num_dumper_threads = num_total - _num_writer_threads;\n-    }\n-    \/\/ Prepare parallel writer.\n-    if (_num_dumper_threads > 1) {\n-      ParDumpWriter::before_work();\n-      \/\/ Number of dumper threads that only iterate heap.\n-      uint _heap_only_dumper_threads = _num_dumper_threads - 1 \/* VMDumper thread *\/;\n-      _dumper_controller = new (std::nothrow) DumperController(_heap_only_dumper_threads);\n-    }\n-  }\n-\n-  void finish_parallel_dump() {\n-    if (_num_dumper_threads > 1) {\n-      ParDumpWriter::after_work();\n-    }\n-  }\n+  \/\/ VM dumper dumps both heap and non-heap data, other dumpers dump heap-only data.\n+  static bool is_vm_dumper(uint worker_id) { return worker_id == VMDumperWorkerId; }\n@@ -1882,0 +1648,1 @@\n+\n@@ -1895,0 +1662,3 @@\n+  \/\/ create dump writer for every parallel dump thread\n+  DumpWriter* create_local_writer();\n+\n@@ -1912,3 +1682,0 @@\n-  \/\/ large objects\n-  void dump_large_objects(ObjectClosure* writer);\n-\n@@ -1927,0 +1694,1 @@\n+    _dump_seq = 0;\n@@ -1930,1 +1698,0 @@\n-    _large_object_list = new (std::nothrow) HeapDumpLargeObjectList();\n@@ -1957,1 +1724,0 @@\n-    delete _large_object_list;\n@@ -1959,0 +1725,3 @@\n+  int dump_seq()           { return _dump_seq; }\n+  bool is_parallel_dump()  { return _num_dumper_threads > 1; }\n+  bool can_parallel_dump(WorkerThreads* workers);\n@@ -2150,0 +1919,26 @@\n+bool VM_HeapDumper::can_parallel_dump(WorkerThreads* workers) {\n+  bool can_parallel = true;\n+  uint num_active_workers = workers != nullptr ? workers->active_workers() : 0;\n+  uint num_requested_dump_threads = _num_dumper_threads;\n+  \/\/ check if we can dump in parallel based on requested and active threads\n+  if (num_active_workers <= 1 || num_requested_dump_threads <= 1) {\n+    _num_dumper_threads = 1;\n+    can_parallel = false;\n+  } else {\n+    \/\/ check if we have extra path room to accommodate segmented heap files\n+    const char* base_path = writer()->get_file_path();\n+    assert(base_path != nullptr, \"sanity check\");\n+    if ((strlen(base_path) + 7\/*.p\\d\\d\\d\\d\\0*\/) >= JVM_MAXPATHLEN) {\n+      _num_dumper_threads = 1;\n+      can_parallel = false;\n+    } else {\n+      _num_dumper_threads = clamp(num_requested_dump_threads, 2U, num_active_workers);\n+    }\n+  }\n+\n+  log_info(heapdump)(\"Requested dump threads %u, active dump threads %u, \"\n+                     \"actual dump threads %u, parallelism %s\",\n+                     num_requested_dump_threads, num_active_workers,\n+                     _num_dumper_threads, can_parallel ? \"true\" : \"false\");\n+  return can_parallel;\n+}\n@@ -2196,6 +1991,2 @@\n-\n-  if (workers == nullptr) {\n-    \/\/ Use serial dump, set dumper threads and writer threads number to 1.\n-    _num_dumper_threads=1;\n-    _num_writer_threads=1;\n-    work(0);\n+  if (!can_parallel_dump(workers)) {\n+    work(VMDumperWorkerId);\n@@ -2203,10 +1994,6 @@\n-    prepare_parallel_dump(workers->active_workers());\n-    if (_num_dumper_threads > 1) {\n-      ParallelObjectIterator poi(_num_dumper_threads);\n-      _poi = &poi;\n-      workers->run_task(this);\n-      _poi = nullptr;\n-    } else {\n-      workers->run_task(this);\n-    }\n-    finish_parallel_dump();\n+    uint heap_only_dumper_threads = _num_dumper_threads - 1 \/* VMDumper thread *\/;\n+    _dumper_controller = new (std::nothrow) DumperController(heap_only_dumper_threads);\n+    ParallelObjectIterator poi(_num_dumper_threads);\n+    _poi = &poi;\n+    workers->run_task(this, _num_dumper_threads);\n+    _poi = nullptr;\n@@ -2220,0 +2007,16 @@\n+\/\/ prepare DumpWriter for every parallel dump thread\n+DumpWriter* VM_HeapDumper::create_local_writer() {\n+  char* path = NEW_RESOURCE_ARRAY(char, JVM_MAXPATHLEN);\n+  memset(path, 0, JVM_MAXPATHLEN);\n+\n+  \/\/ generate segmented heap file path\n+  const char* base_path = writer()->get_file_path();\n+  AbstractCompressor* compressor = writer()->compressor();\n+  int seq = Atomic::fetch_then_add(&_dump_seq, 1);\n+  os::snprintf(path, JVM_MAXPATHLEN, \"%s.p%d\", base_path, seq);\n+\n+  \/\/ create corresponding writer for that\n+  DumpWriter* local_writer = new DumpWriter(path, writer()->is_overwrite(), compressor);\n+  return local_writer;\n+}\n+\n@@ -2221,10 +2024,3 @@\n-  if (worker_id != 0) {\n-    if (get_worker_type(worker_id) == WriterType) {\n-      writer()->writer_loop();\n-      return;\n-    }\n-    if (_num_dumper_threads > 1 && get_worker_type(worker_id) == DumperType) {\n-      _dumper_controller->wait_for_start_signal();\n-    }\n-  } else {\n-    \/\/ The worker 0 on all non-heap data dumping and part of heap iteration.\n+  \/\/ VM Dumper works on all non-heap data dumping and part of heap iteration.\n+  if (is_vm_dumper(worker_id)) {\n+    TraceTime timer(\"Dump non-objects\", TRACETIME_LOG(Info, heapdump));\n@@ -2274,0 +2070,2 @@\n+\n+  \/\/ Heap iteration.\n@@ -2280,1 +2078,4 @@\n-  if (_num_dumper_threads <= 1) {\n+  if (!is_parallel_dump()) {\n+    assert(is_vm_dumper(worker_id), \"must be\");\n+    \/\/ == Serial dump\n+    TraceTime timer(\"Dump heap objects\", TRACETIME_LOG(Info, heapdump));\n@@ -2283,0 +2084,4 @@\n+    writer()->finish_dump_segment();\n+    \/\/ Writes the HPROF_HEAP_DUMP_END record because merge does not happen in serial dump\n+    DumperSupport::end_of_dump(writer());\n+    writer()->flush();\n@@ -2284,28 +2089,15 @@\n-    assert(get_worker_type(worker_id) == DumperType\n-          || get_worker_type(worker_id) == VMDumperType,\n-          \"must be dumper thread to do heap iteration\");\n-    if (get_worker_type(worker_id) == VMDumperType) {\n-      \/\/ Clear global writer's buffer.\n-      writer()->finish_dump_segment(true);\n-      \/\/ Notify dumpers to start heap iteration.\n-      _dumper_controller->start_dump();\n-    }\n-    \/\/ Heap iteration.\n-    {\n-       ParDumpWriter pw(writer());\n-       {\n-         HeapObjectDumper obj_dumper(&pw, _large_object_list);\n-         _poi->object_iterate(&obj_dumper, worker_id);\n-       }\n-\n-       if (get_worker_type(worker_id) == VMDumperType) {\n-         _dumper_controller->wait_all_dumpers_complete();\n-         \/\/ clear internal buffer;\n-         pw.finish_dump_segment(true);\n-         \/\/ refresh the global_writer's buffer and position;\n-         writer()->refresh();\n-       } else {\n-         pw.finish_dump_segment(true);\n-         _dumper_controller->dumper_complete();\n-         return;\n-       }\n+    \/\/ == Parallel dump\n+    ResourceMark rm;\n+    TraceTime timer(\"Dump heap objects in parallel\", TRACETIME_LOG(Info, heapdump));\n+    DumpWriter* local_writer = is_vm_dumper(worker_id) ? writer() : create_local_writer();\n+    if (!local_writer->has_error()) {\n+      HeapObjectDumper obj_dumper(local_writer);\n+      _poi->object_iterate(&obj_dumper, worker_id);\n+      local_writer->finish_dump_segment();\n+      local_writer->flush();\n+    }\n+    if (is_vm_dumper(worker_id)) {\n+      _dumper_controller->wait_all_dumpers_complete();\n+    } else {\n+      _dumper_controller->dumper_complete(local_writer, writer());\n+      return;\n@@ -2314,9 +2106,2 @@\n-\n-  assert(get_worker_type(worker_id) == VMDumperType, \"Heap dumper must be VMDumper\");\n-  \/\/ Use writer() rather than ParDumpWriter to avoid memory consumption.\n-  HeapObjectDumper obj_dumper(writer());\n-  dump_large_objects(&obj_dumper);\n-  \/\/ Writes the HPROF_HEAP_DUMP_END record.\n-  DumperSupport::end_of_dump(writer());\n-  \/\/ We are done with writing. Release the worker threads.\n-  writer()->deactivate();\n+  \/\/ At this point, all fragments of the heapdump have been written to separate files.\n+  \/\/ We need to merge them into a complete heapdump and write HPROF_HEAP_DUMP_END at that time.\n@@ -2383,5 +2168,0 @@\n-\/\/ dump the large objects.\n-void VM_HeapDumper::dump_large_objects(ObjectClosure* cl) {\n-  _large_object_list->drain(cl);\n-}\n-\n@@ -2411,1 +2191,1 @@\n-  DumpWriter writer(new (std::nothrow) FileWriter(path, overwrite), compressor);\n+  DumpWriter writer(path, overwrite, compressor);\n@@ -2422,1 +2202,1 @@\n-  \/\/ generate the dump\n+  \/\/ generate the segmented heap dump into separate files\n@@ -2424,6 +2204,1 @@\n-  if (Thread::current()->is_VM_thread()) {\n-    assert(SafepointSynchronize::is_at_safepoint(), \"Expected to be called at a safepoint\");\n-    dumper.doit();\n-  } else {\n-    VMThread::execute(&dumper);\n-  }\n+  VMThread::execute(&dumper);\n@@ -2434,0 +2209,25 @@\n+  \/\/ For serial dump, once VM_HeapDumper completes, the whole heap dump process\n+  \/\/ is done, no further phases needed. For parallel dump, the whole heap dump\n+  \/\/ process is done in two phases\n+  \/\/\n+  \/\/ Phase 1: Concurrent threads directly write heap data to multiple heap files.\n+  \/\/          This is done by VM_HeapDumper, which is performed within safepoint.\n+  \/\/\n+  \/\/ Phase 2: Merge multiple heap files into one complete heap dump file.\n+  \/\/          This is done by DumpMerger, which is performed outside safepoint\n+  if (dumper.is_parallel_dump()) {\n+    DumpMerger merger(path, &writer, dumper.dump_seq());\n+    Thread* current_thread = Thread::current();\n+    if (current_thread->is_AttachListener_thread()) {\n+      \/\/ perform heapdump file merge operation in the current thread prevents us\n+      \/\/ from occupying the VM Thread, which in turn affects the occurrence of\n+      \/\/ GC and other VM operations.\n+      merger.do_merge();\n+    } else {\n+      \/\/ otherwise, performs it by VM thread\n+      VM_HeapDumpMerge op(&merger);\n+      VMThread::execute(&op);\n+    }\n+    set_error(writer.error());\n+  }\n+\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":331,"deletions":531,"binary":false,"changes":862,"status":"modified"},{"patch":"@@ -32,12 +32,0 @@\n-\/\/ HeapDumper is used to dump the java heap to file in HPROF binary format:\n-\/\/\n-\/\/  { HeapDumper dumper(true \/* full GC before heap dump *\/);\n-\/\/    if (dumper.dump(\"\/export\/java.hprof\")) {\n-\/\/      ResourceMark rm;\n-\/\/      tty->print_cr(\"Dump failed: %s\", dumper.error_as_C_string());\n-\/\/    } else {\n-\/\/      \/\/ dump succeeded\n-\/\/    }\n-\/\/  }\n-\/\/\n-\n@@ -46,0 +34,1 @@\n+\/\/ HeapDumper is used to dump the java heap to file in HPROF binary format\n@@ -83,0 +72,5 @@\n+\n+  \/\/ Parallel thread number for heap dump, initialize based on active processor count.\n+  static uint default_num_of_dump_threads() {\n+    return MAX2<uint>(1, (uint)os::initial_active_processor_count() * 3 \/ 8);\n+  }\n","filename":"src\/hotspot\/share\/services\/heapDumper.hpp","additions":6,"deletions":12,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -137,360 +137,0 @@\n-\n-WorkList::WorkList() {\n-  _head._next = &_head;\n-  _head._prev = &_head;\n-}\n-\n-void WorkList::insert(WriteWork* before, WriteWork* work) {\n-  work->_prev = before;\n-  work->_next = before->_next;\n-  before->_next = work;\n-  work->_next->_prev = work;\n-}\n-\n-WriteWork* WorkList::remove(WriteWork* work) {\n-  if (work != nullptr) {\n-    assert(work->_next != work, \"Invalid next\");\n-    assert(work->_prev != work, \"Invalid prev\");\n-    work->_prev->_next = work->_next;;\n-    work->_next->_prev = work->_prev;\n-    work->_next = nullptr;\n-    work->_prev = nullptr;\n-  }\n-\n-  return work;\n-}\n-\n-void WorkList::add_by_id(WriteWork* work) {\n-  if (is_empty()) {\n-    add_first(work);\n-  } else {\n-    WriteWork* last_curr = &_head;\n-    WriteWork* curr = _head._next;\n-\n-    while (curr->_id < work->_id) {\n-      last_curr = curr;\n-      curr = curr->_next;\n-\n-      if (curr == &_head) {\n-        add_last(work);\n-        return;\n-      }\n-    }\n-\n-    insert(last_curr, work);\n-  }\n-}\n-\n-\n-\n-CompressionBackend::CompressionBackend(AbstractWriter* writer,\n-     AbstractCompressor* compressor, size_t block_size, size_t max_waste) :\n-  _active(false),\n-  _err(nullptr),\n-  _nr_of_threads(0),\n-  _works_created(0),\n-  _work_creation_failed(false),\n-  _id_to_write(0),\n-  _next_id(0),\n-  _in_size(block_size),\n-  _max_waste(max_waste),\n-  _out_size(0),\n-  _tmp_size(0),\n-  _written(0),\n-  _writer(writer),\n-  _compressor(compressor),\n-  _lock(new (std::nothrow) PaddedMonitor(Mutex::nosafepoint, \"HProfCompressionBackend_lock\")) {\n-  if (_writer == nullptr) {\n-    set_error(\"Could not allocate writer\");\n-  } else if (_lock == nullptr) {\n-    set_error(\"Could not allocate lock\");\n-  } else {\n-    set_error(_writer->open_writer());\n-  }\n-\n-  if (_compressor != nullptr) {\n-    set_error(_compressor->init(_in_size, &_out_size, &_tmp_size));\n-  }\n-\n-  _current = allocate_work(_in_size, _out_size, _tmp_size);\n-\n-  if (_current == nullptr) {\n-    set_error(\"Could not allocate memory for buffer\");\n-  }\n-\n-  _active = (_err == nullptr);\n-}\n-\n-CompressionBackend::~CompressionBackend() {\n-  assert(!_active, \"Must not be active by now\");\n-  assert(_nr_of_threads == 0, \"Must have no active threads\");\n-  assert(_to_compress.is_empty() && _finished.is_empty(), \"Still work to do\");\n-\n-  free_work_list(&_unused);\n-  free_work(_current);\n-  assert(_works_created == 0, \"All work must have been freed\");\n-\n-  delete _compressor;\n-  delete _writer;\n-  delete _lock;\n-}\n-\n-void CompressionBackend::flush_buffer(MonitorLocker* ml) {\n-\n-  \/\/ Make sure we write the last partially filled buffer.\n-  if ((_current != nullptr) && (_current->_in_used > 0)) {\n-    _current->_id = _next_id++;\n-    _to_compress.add_last(_current);\n-    _current = nullptr;\n-    ml->notify_all();\n-  }\n-\n-  \/\/ Wait for the threads to drain the compression work list and do some work yourself.\n-  while (!_to_compress.is_empty()) {\n-    do_foreground_work();\n-  }\n-}\n-\n-void CompressionBackend::flush_buffer() {\n-  assert(_active, \"Must be active\");\n-\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-  flush_buffer(&ml);\n-}\n-\n-void CompressionBackend::deactivate() {\n-  assert(_active, \"Must be active\");\n-\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-  flush_buffer(&ml);\n-\n-  _active = false;\n-  ml.notify_all();\n-}\n-\n-void CompressionBackend::thread_loop() {\n-  {\n-    MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-    _nr_of_threads++;\n-  }\n-\n-  WriteWork* work;\n-  while ((work = get_work()) != nullptr) {\n-    do_compress(work);\n-    finish_work(work);\n-  }\n-\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-  _nr_of_threads--;\n-  assert(_nr_of_threads >= 0, \"Too many threads finished\");\n-}\n-\n-void CompressionBackend::set_error(char const* new_error) {\n-  if ((new_error != nullptr) && (_err == nullptr)) {\n-    _err = new_error;\n-  }\n-}\n-\n-WriteWork* CompressionBackend::allocate_work(size_t in_size, size_t out_size,\n-                                             size_t tmp_size) {\n-  WriteWork* result = (WriteWork*) os::malloc(sizeof(WriteWork), mtInternal);\n-\n-  if (result == nullptr) {\n-    _work_creation_failed = true;\n-    return nullptr;\n-  }\n-\n-  _works_created++;\n-  result->_in = (char*) os::malloc(in_size, mtInternal);\n-  result->_in_max = in_size;\n-  result->_in_used = 0;\n-  result->_out = nullptr;\n-  result->_tmp = nullptr;\n-\n-  if (result->_in == nullptr) {\n-    goto fail;\n-  }\n-\n-  if (out_size > 0) {\n-    result->_out = (char*) os::malloc(out_size, mtInternal);\n-    result->_out_used = 0;\n-    result->_out_max = out_size;\n-\n-    if (result->_out == nullptr) {\n-      goto fail;\n-    }\n-  }\n-\n-  if (tmp_size > 0) {\n-    result->_tmp = (char*) os::malloc(tmp_size, mtInternal);\n-    result->_tmp_max = tmp_size;\n-\n-    if (result->_tmp == nullptr) {\n-      goto fail;\n-    }\n-  }\n-\n-  return result;\n-\n-fail:\n-  free_work(result);\n-  _work_creation_failed = true;\n-  return nullptr;\n-}\n-\n-void CompressionBackend::free_work(WriteWork* work) {\n-  if (work != nullptr) {\n-    os::free(work->_in);\n-    os::free(work->_out);\n-    os::free(work->_tmp);\n-    os::free(work);\n-    --_works_created;\n-  }\n-}\n-\n-void CompressionBackend::free_work_list(WorkList* list) {\n-  while (!list->is_empty()) {\n-    free_work(list->remove_first());\n-  }\n-}\n-\n-void CompressionBackend::do_foreground_work() {\n-  assert(!_to_compress.is_empty(), \"Must have work to do\");\n-  assert(_lock->owned_by_self(), \"Must have the lock\");\n-\n-  WriteWork* work = _to_compress.remove_first();\n-  MutexUnlocker mu(_lock, Mutex::_no_safepoint_check_flag);\n-  do_compress(work);\n-  finish_work(work);\n-}\n-\n-WriteWork* CompressionBackend::get_work() {\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-\n-  while (_active && _to_compress.is_empty()) {\n-    ml.wait();\n-  }\n-\n-  return _to_compress.remove_first();\n-}\n-\n-void CompressionBackend::flush_external_buffer(char* buffer, size_t used, size_t max) {\n-  assert(buffer != nullptr && used != 0 && max != 0, \"Invalid data send to compression backend\");\n-  assert(_active == true, \"Backend must be active when flushing external buffer\");\n-  char* buf;\n-  size_t tmp_used = 0;\n-  size_t tmp_max = 0;\n-\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-  \/\/ First try current buffer. Use it if empty.\n-  if (_current->_in_used == 0) {\n-    buf = _current->_in;\n-  } else {\n-    \/\/ If current buffer is not clean, flush it.\n-    MutexUnlocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-    get_new_buffer(&buf, &tmp_used, &tmp_max, true);\n-  }\n-  assert (_current->_in != nullptr && _current->_in_max >= max &&\n-          _current->_in_used == 0, \"Invalid buffer from compression backend\");\n-  \/\/ Copy data to backend buffer.\n-  memcpy(buf, buffer, used);\n-\n-  assert(_current->_in == buf, \"Must be current\");\n-  _current->_in_used += used;\n-}\n-\n-void CompressionBackend::get_new_buffer(char** buffer, size_t* used, size_t* max, bool force_reset) {\n-  if (_active) {\n-    MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-    if (*used > 0 || force_reset) {\n-      _current->_in_used += *used;\n-      \/\/ Check if we do not waste more than _max_waste. If yes, write the buffer.\n-      \/\/ Otherwise return the rest of the buffer as the new buffer.\n-      if (_current->_in_max - _current->_in_used <= _max_waste || force_reset) {\n-        _current->_id = _next_id++;\n-        _to_compress.add_last(_current);\n-        _current = nullptr;\n-        ml.notify_all();\n-      } else {\n-        *buffer = _current->_in + _current->_in_used;\n-        *used = 0;\n-        *max = _current->_in_max - _current->_in_used;\n-        return;\n-      }\n-    }\n-\n-    while ((_current == nullptr) && _unused.is_empty() && _active) {\n-      \/\/ Add more work objects if needed.\n-      if (!_work_creation_failed && (_works_created <= _nr_of_threads)) {\n-        WriteWork* work = allocate_work(_in_size, _out_size, _tmp_size);\n-\n-        if (work != nullptr) {\n-          _unused.add_first(work);\n-        }\n-      } else if (!_to_compress.is_empty() && (_nr_of_threads == 0)) {\n-        do_foreground_work();\n-      } else {\n-        ml.wait();\n-      }\n-    }\n-\n-    if (_current == nullptr) {\n-      _current = _unused.remove_first();\n-    }\n-\n-    if (_current != nullptr) {\n-      _current->_in_used = 0;\n-      _current->_out_used = 0;\n-      *buffer = _current->_in;\n-      *used = 0;\n-      *max = _current->_in_max;\n-\n-      return;\n-    }\n-  }\n-\n-  *buffer = nullptr;\n-  *used = 0;\n-  *max = 0;\n-\n-  return;\n-}\n-\n-void CompressionBackend::do_compress(WriteWork* work) {\n-  if (_compressor != nullptr) {\n-    char const* msg = _compressor->compress(work->_in, work->_in_used, work->_out,\n-                                            work->_out_max,\n-    work->_tmp, _tmp_size, &work->_out_used);\n-\n-    if (msg != nullptr) {\n-      MutexLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-      set_error(msg);\n-    }\n-  }\n-}\n-\n-void CompressionBackend::finish_work(WriteWork* work) {\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-\n-  _finished.add_by_id(work);\n-\n-  \/\/ Write all finished works as far as we can.\n-  while (!_finished.is_empty() && (_finished.first()->_id == _id_to_write)) {\n-    WriteWork* to_write = _finished.remove_first();\n-    size_t size = _compressor == nullptr ? to_write->_in_used : to_write->_out_used;\n-    char* p = _compressor == nullptr ? to_write->_in : to_write->_out;\n-    char const* msg = nullptr;\n-\n-    if (_err == nullptr) {\n-      _written += size;\n-      MutexUnlocker mu(_lock, Mutex::_no_safepoint_check_flag);\n-      msg = _writer->write_buf(p, (ssize_t) size);\n-    }\n-\n-    set_error(msg);\n-    _unused.add_first(to_write);\n-    _id_to_write++;\n-  }\n-\n-  ml.notify_all();\n-}\n","filename":"src\/hotspot\/share\/services\/heapDumperCompression.cpp","additions":0,"deletions":360,"binary":false,"changes":360,"status":"modified"},{"patch":"@@ -77,0 +77,4 @@\n+\n+  const char* get_file_path() { return _path; }\n+\n+  bool is_overwrite() const { return _overwrite; }\n@@ -100,141 +104,0 @@\n-\n-\/\/ The data needed to write a single buffer (and compress it optionally).\n-struct WriteWork {\n-  \/\/ The id of the work.\n-  int64_t _id;\n-\n-  \/\/ The input buffer where the raw data is\n-  char* _in;\n-  size_t _in_used;\n-  size_t _in_max;\n-\n-  \/\/ The output buffer where the compressed data is. Is null when compression is disabled.\n-  char* _out;\n-  size_t _out_used;\n-  size_t _out_max;\n-\n-  \/\/ The temporary space needed for compression. Is null when compression is disabled.\n-  char* _tmp;\n-  size_t _tmp_max;\n-\n-  \/\/ Used to link WriteWorks into lists.\n-  WriteWork* _next;\n-  WriteWork* _prev;\n-};\n-\n-\/\/ A list for works.\n-class WorkList {\n-private:\n-  WriteWork _head;\n-\n-  void insert(WriteWork* before, WriteWork* work);\n-  WriteWork* remove(WriteWork* work);\n-\n-public:\n-  WorkList();\n-\n-  \/\/ Return true if the list is empty.\n-  bool is_empty() { return _head._next == &_head; }\n-\n-  \/\/ Adds to the beginning of the list.\n-  void add_first(WriteWork* work) { insert(&_head, work); }\n-\n-  \/\/ Adds to the end of the list.\n-  void add_last(WriteWork* work) { insert(_head._prev, work); }\n-\n-  \/\/ Adds so the ids are ordered.\n-  void add_by_id(WriteWork* work);\n-\n-  \/\/ Returns the first element.\n-  WriteWork* first() { return is_empty() ? nullptr : _head._next; }\n-\n-  \/\/ Returns the last element.\n-  WriteWork* last() { return is_empty() ? nullptr : _head._prev; }\n-\n-  \/\/ Removes the first element. Returns null if empty.\n-  WriteWork* remove_first() { return remove(first()); }\n-\n-  \/\/ Removes the last element. Returns null if empty.\n-  WriteWork* remove_last() { return remove(first()); }\n-};\n-\n-\n-class Monitor;\n-\n-\/\/ This class is used by the DumpWriter class. It supplies the DumpWriter with\n-\/\/ chunks of memory to write the heap dump data into. When the DumpWriter needs a\n-\/\/ new memory chunk, it calls get_new_buffer(), which commits the old chunk used\n-\/\/ and returns a new chunk. The old chunk is then added to a queue to be compressed\n-\/\/ and then written in the background.\n-class CompressionBackend : StackObj {\n-  bool _active;\n-  char const * _err;\n-\n-  int _nr_of_threads;\n-  int _works_created;\n-  bool _work_creation_failed;\n-\n-  int64_t _id_to_write;\n-  int64_t _next_id;\n-\n-  size_t _in_size;\n-  size_t _max_waste;\n-  size_t _out_size;\n-  size_t _tmp_size;\n-\n-  size_t _written;\n-\n-  AbstractWriter* const _writer;\n-  AbstractCompressor* const _compressor;\n-\n-  Monitor* const _lock;\n-\n-  WriteWork* _current;\n-  WorkList _to_compress;\n-  WorkList _unused;\n-  WorkList _finished;\n-\n-  void set_error(char const* new_error);\n-\n-  WriteWork* allocate_work(size_t in_size, size_t out_size, size_t tmp_size);\n-  void free_work(WriteWork* work);\n-  void free_work_list(WorkList* list);\n-\n-  void do_foreground_work();\n-  WriteWork* get_work();\n-  void do_compress(WriteWork* work);\n-  void finish_work(WriteWork* work);\n-  void flush_buffer(MonitorLocker* ml);\n-\n-public:\n-  \/\/ compressor can be null if no compression is used.\n-  \/\/ Takes ownership of the writer and compressor.\n-  \/\/ block_size is the buffer size of a WriteWork.\n-  \/\/ max_waste is the maximum number of bytes to leave\n-  \/\/ empty in the buffer when it is written.\n-  CompressionBackend(AbstractWriter* writer, AbstractCompressor* compressor,\n-    size_t block_size, size_t max_waste);\n-\n-  ~CompressionBackend();\n-\n-  size_t get_written() const { return _written; }\n-\n-  char const* error() const { return _err; }\n-\n-  \/\/ Sets up an internal buffer, fills with external buffer, and sends to compressor.\n-  void flush_external_buffer(char* buffer, size_t used, size_t max);\n-\n-  \/\/ Commits the old buffer (using the value in *used) and sets up a new one.\n-  void get_new_buffer(char** buffer, size_t* used, size_t* max, bool force_reset = false);\n-\n-  \/\/ The entry point for a worker thread.\n-  void thread_loop();\n-\n-  \/\/ Shuts down the backend, releasing all threads.\n-  void deactivate();\n-\n-  \/\/ Flush all compressed data in buffer to file\n-  void flush_buffer();\n-};\n-\n-\n","filename":"src\/hotspot\/share\/services\/heapDumperCompression.hpp","additions":4,"deletions":141,"binary":false,"changes":145,"status":"modified"},{"patch":"@@ -0,0 +1,41 @@\n+\/*\n+ * Copyright (c) 2023, Alibaba Group Holding Limited. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.runtime;\n+\n+import java.io.*;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+\n+public class AttachListenerThread extends JavaThread {\n+\n+  public AttachListenerThread (Address addr) {\n+    super(addr);\n+  }\n+\n+  public boolean isJavaThread() { return false; }\n+\n+  public boolean isAttachListenerThread() { return true; }\n+\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/AttachListenerThread.java","additions":41,"deletions":0,"binary":false,"changes":41,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -88,0 +88,1 @@\n+  public boolean   isAttachListenerThread()      { return false; }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Thread.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -160,0 +160,1 @@\n+        virtualConstructor.addMapping(\"AttachListenerThread\", AttachListenerThread.class);\n@@ -167,2 +168,3 @@\n-      JvmtiAgentThread, NotificationThread, MonitorDeflationThread and ServiceThread.\n-      The latter four are subclasses of the former. Most operations\n+      JvmtiAgentThread, NotificationThread, MonitorDeflationThread,\n+      StringDedupThread, AttachListenerThread and ServiceThread.\n+      The latter seven subclasses of the former. Most operations\n@@ -174,1 +176,1 @@\n-      false for the four subclasses. FIXME: should reconsider the\n+      false for the seven subclasses. FIXME: should reconsider the\n@@ -198,1 +200,2 @@\n-            \" (expected type JavaThread, CompilerThread, MonitorDeflationThread, ServiceThread or JvmtiAgentThread)\", e);\n+            \" (expected type JavaThread, CompilerThread, MonitorDeflationThread, AttachListenerThread,\" +\n+            \" StringDedupThread, NotificationThread, ServiceThread or JvmtiAgentThread)\", e);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Threads.java","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -0,0 +1,140 @@\n+\/*\n+ * Copyright (c) 2023, Alibaba Group Holding Limited. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.List;\n+\n+import jdk.test.lib.Asserts;\n+import jdk.test.lib.JDKToolLauncher;\n+import jdk.test.lib.apps.LingeredApp;\n+import jdk.test.lib.dcmd.PidJcmdExecutor;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.process.ProcessTools;\n+\n+import jdk.test.lib.hprof.HprofParser;\n+\n+\/**\n+ * @test\n+ * @bug 8306441\n+ * @summary Verify the integrity of generated heap dump and capability of parallel dump\n+ * @library \/test\/lib\n+ * @run driver HeapDumpParallelTest\n+ *\/\n+\n+public class HeapDumpParallelTest {\n+\n+    private static void checkAndVerify(OutputAnalyzer dcmdOut, LingeredApp app, File heapDumpFile, boolean expectSerial) throws IOException {\n+        dcmdOut.shouldHaveExitValue(0);\n+        dcmdOut.shouldContain(\"Heap dump file created\");\n+        OutputAnalyzer appOut = new OutputAnalyzer(app.getProcessStdout());\n+        appOut.shouldContain(\"[heapdump]\");\n+        if (!expectSerial && Runtime.getRuntime().availableProcessors() > 1) {\n+            appOut.shouldContain(\"Dump heap objects in parallel\");\n+            appOut.shouldContain(\"Merge heap files complete\");\n+        } else {\n+            appOut.shouldNotContain(\"Dump heap objects in parallel\");\n+            appOut.shouldNotContain(\"Merge heap files complete\");\n+        }\n+        verifyHeapDump(heapDumpFile);\n+        if (heapDumpFile.exists()) {\n+            heapDumpFile.delete();\n+        }\n+    }\n+\n+    private static LingeredApp launchApp() throws IOException {\n+        LingeredApp theApp = new LingeredApp();\n+        LingeredApp.startApp(theApp, \"-Xlog:heapdump\", \"-Xmx512m\",\n+                             \"-XX:-UseDynamicNumberOfGCThreads\",\n+                             \"-XX:ParallelGCThreads=2\");\n+        return theApp;\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        String heapDumpFileName = \"parallelHeapDump.bin\";\n+\n+        File heapDumpFile = new File(heapDumpFileName);\n+        if (heapDumpFile.exists()) {\n+            heapDumpFile.delete();\n+        }\n+\n+        LingeredApp theApp = launchApp();\n+        try {\n+            \/\/ Expect error message\n+            OutputAnalyzer out = attachJcmdHeapDump(heapDumpFile, theApp.getPid(), \"-parallel=\" + -1);\n+            out.shouldContain(\"Invalid number of parallel dump threads.\");\n+\n+            \/\/ Expect serial dump because 0 implies to disable parallel dump\n+            test(heapDumpFile, \"-parallel=\" + 0, true);\n+\n+            \/\/ Expect serial dump\n+            test(heapDumpFile,  \"-parallel=\" + 1, true);\n+\n+            \/\/ Expect parallel dump\n+            test(heapDumpFile, \"-parallel=\" + Integer.MAX_VALUE, false);\n+\n+            \/\/ Expect parallel dump\n+            test(heapDumpFile, \"-gz=9 -overwrite -parallel=\" + Runtime.getRuntime().availableProcessors(), false);\n+        } finally {\n+            theApp.stopApp();\n+        }\n+    }\n+\n+    private static void test(File heapDumpFile, String arg, boolean expectSerial) throws Exception {\n+        LingeredApp theApp = launchApp();\n+        try {\n+            OutputAnalyzer dcmdOut = attachJcmdHeapDump(heapDumpFile, theApp.getPid(), arg);\n+            theApp.stopApp();\n+            checkAndVerify(dcmdOut, theApp, heapDumpFile, expectSerial);\n+        } finally {\n+            theApp.stopApp();\n+        }\n+    }\n+\n+    private static OutputAnalyzer attachJcmdHeapDump(File heapDumpFile, long lingeredAppPid, String arg) throws Exception {\n+        \/\/ e.g. jcmd <pid> GC.heap_dump -parallel=cpucount <file_path>\n+        System.out.println(\"Testing pid \" + lingeredAppPid);\n+        PidJcmdExecutor executor = new PidJcmdExecutor(\"\" + lingeredAppPid);\n+        return executor.execute(\"GC.heap_dump \" + arg + \" \" + heapDumpFile.getAbsolutePath());\n+    }\n+\n+    private static void verifyHeapDump(File dump) {\n+        Asserts.assertTrue(dump.exists() && dump.isFile(), \"Could not create dump file \" + dump.getAbsolutePath());\n+        try {\n+            File out = HprofParser.parse(dump);\n+\n+            Asserts.assertTrue(out != null && out.exists() && out.isFile(), \"Could not find hprof parser output file\");\n+            List<String> lines = Files.readAllLines(out.toPath());\n+            Asserts.assertTrue(lines.size() > 0, \"hprof parser output file is empty\");\n+            for (String line : lines) {\n+                Asserts.assertFalse(line.matches(\".*WARNING(?!.*Failed to resolve object.*constantPoolOop.*).*\"));\n+            }\n+\n+            out.delete();\n+        } catch (Exception e) {\n+            e.printStackTrace();\n+            Asserts.fail(\"Could not parse dump file \" + dump.getAbsolutePath());\n+        }\n+    }\n+}\n\\ No newline at end of file\n","filename":"test\/hotspot\/jtreg\/serviceability\/dcmd\/gc\/HeapDumpParallelTest.java","additions":140,"deletions":0,"binary":false,"changes":140,"status":"added"}]}