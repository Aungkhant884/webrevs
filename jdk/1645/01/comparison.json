{"files":[{"patch":"@@ -312,25 +312,6 @@\n-     *    the current thread was interrupted or the wait timed out. On\n-     *    multiprocessors, we use front-of-queue spinning: If a node\n-     *    appears to be the first unmatched node in the queue, it\n-     *    spins a bit before blocking. In either case, before blocking\n-     *    it tries to unsplice any nodes between the current \"head\"\n-     *    and the first unmatched node.\n-     *\n-     *    Front-of-queue spinning vastly improves performance of\n-     *    heavily contended queues. And so long as it is relatively\n-     *    brief and \"quiet\", spinning does not much impact performance\n-     *    of less-contended queues.  During spins threads check their\n-     *    interrupt status and generate a thread-local random number\n-     *    to decide to occasionally perform a Thread.yield. While\n-     *    yield has underdefined specs, we assume that it might help,\n-     *    and will not hurt, in limiting impact of spinning on busy\n-     *    systems.  We also use smaller (1\/2) spins for nodes that are\n-     *    not known to be front but whose predecessors have not\n-     *    blocked -- these \"chained\" spins avoid artifacts of\n-     *    front-of-queue rules which otherwise lead to alternating\n-     *    nodes spinning vs blocking. Further, front threads that\n-     *    represent phase changes (from data to request node or vice\n-     *    versa) compared to their predecessors receive additional\n-     *    chained spins, reflecting longer paths typically required to\n-     *    unblock threads during phase changes.\n-     *\n+     *    the current thread was interrupted or the wait timed out. To\n+     *    improve performance in common single-source \/ single-sink\n+     *    usages when there are more tasks that cores, an initial\n+     *    Thread.yield is tried when there is apparently only one\n+     *    waiter.  In other cases, waiters may help with some\n+     *    bookkeeping, then park\/unpark.\n@@ -372,24 +353,3 @@\n-     * won't help for case (1) anyway), we record a conservative\n-     * estimate of possible unsplice failures (in \"sweepVotes\").\n-     * We trigger a full sweep when the estimate exceeds a threshold\n-     * (\"SWEEP_THRESHOLD\") indicating the maximum number of estimated\n-     * removal failures to tolerate before sweeping through, unlinking\n-     * cancelled nodes that were not unlinked upon initial removal.\n-     * We perform sweeps by the thread hitting threshold (rather than\n-     * background threads or by spreading work to other threads)\n-     * because in the main contexts in which removal occurs, the\n-     * caller is timed-out or cancelled, which are not time-critical\n-     * enough to warrant the overhead that alternatives would impose\n-     * on other threads.\n-     *\n-     * Because the sweepVotes estimate is conservative, and because\n-     * nodes become unlinked \"naturally\" as they fall off the head of\n-     * the queue, and because we allow votes to accumulate even while\n-     * sweeps are in progress, there are typically significantly fewer\n-     * such nodes than estimated.  Choice of a threshold value\n-     * balances the likelihood of wasted effort and contention, versus\n-     * providing a worst-case bound on retention of interior nodes in\n-     * quiescent queues. The value defined below was chosen\n-     * empirically to balance these under various timeout scenarios.\n-     *\n-     * Because traversal operations on the linked list of nodes are a\n+     * won't help for case (1) anyway), we record the need to sweep the\n+     * next time any thread would otherwise block in awaitMatch. Also,\n+     * because traversal operations on the linked list of nodes are a\n@@ -408,14 +368,0 @@\n-    \/** True if on multiprocessor *\/\n-    private static final boolean MP =\n-        Runtime.getRuntime().availableProcessors() > 1;\n-\n-    \/**\n-     * The number of times to spin (with randomly interspersed calls\n-     * to Thread.yield) on multiprocessor before blocking when a node\n-     * is apparently the first waiter in the queue.  See above for\n-     * explanation. Must be a power of two. The value is empirically\n-     * derived -- it works pretty well across a variety of processors,\n-     * numbers of CPUs, and OSes.\n-     *\/\n-    private static final int FRONT_SPINS   = 1 << 7;\n-\n@@ -423,5 +369,3 @@\n-     * The number of times to spin before blocking when a node is\n-     * preceded by another node that is apparently spinning.  Also\n-     * serves as an increment to FRONT_SPINS on phase changes, and as\n-     * base average frequency for yielding during spins. Must be a\n-     * power of two.\n+     * The number of nanoseconds for which it is faster to spin\n+     * rather than to use timed park. A rough estimate suffices.\n+     * Using a power of two minus one simplifies some comparisons.\n@@ -429,1 +373,1 @@\n-    private static final int CHAINED_SPINS = FRONT_SPINS >>> 1;\n+    static final long SPIN_FOR_TIMEOUT_THRESHOLD = 1023L;\n@@ -445,1 +389,1 @@\n-    static final class Node {\n+    static final class Node implements ForkJoinPool.ManagedBlocker {\n@@ -490,18 +434,1 @@\n-            NEXT.set(this, next);\n-        }\n-\n-        \/**\n-         * Sets item (of a request node) to self and waiter to null,\n-         * to avoid garbage retention after matching or cancelling.\n-         * Uses relaxed writes because order is already constrained in\n-         * the only calling contexts: item is forgotten only after\n-         * volatile\/atomic mechanics that extract items, and visitors\n-         * of request nodes only ever check whether item is null.\n-         * Similarly, clearing waiter follows either CAS or return\n-         * from park (if ever parked; else we don't care).\n-         *\/\n-        final void forgetContents() {\n-            \/\/ assert isMatched();\n-            if (!isData)\n-                ITEM.set(this, this);\n-            WAITER.set(this, null);\n+            NEXT.setOpaque(this, next);\n@@ -537,0 +464,10 @@\n+        public final boolean isReleasable() {\n+            return (isData == (item == null)) ||\n+                Thread.currentThread().isInterrupted();\n+        }\n+\n+        public final boolean block() {\n+            while (!isReleasable()) LockSupport.park();\n+            return true;\n+        }\n+\n@@ -569,1 +506,1 @@\n-    private transient volatile int sweepVotes;\n+    private transient volatile boolean needSweep;\n@@ -581,5 +518,0 @@\n-    \/** Atomic version of ++sweepVotes. *\/\n-    private int incSweepVotes() {\n-        return (int) SWEEPVOTES.getAndAdd(this, 1) + 1;\n-    }\n-\n@@ -692,1 +624,1 @@\n-     * Spins\/yields\/blocks until node s is matched or caller gives up.\n+     * Possibly blocks until node s is matched or caller gives up.\n@@ -703,0 +635,1 @@\n+    @SuppressWarnings(\"unchecked\")\n@@ -704,0 +637,1 @@\n+        final boolean isData = s.isData;\n@@ -705,16 +639,9 @@\n-        Thread w = Thread.currentThread();\n-        int spins = -1; \/\/ initialized after first item and cancel checks\n-        ThreadLocalRandom randomYields = null; \/\/ bound if needed\n-\n-        for (;;) {\n-            final Object item;\n-            if ((item = s.item) != e) {       \/\/ matched\n-                \/\/ assert item != s;\n-                s.forgetContents();           \/\/ avoid garbage\n-                @SuppressWarnings(\"unchecked\") E itemE = (E) item;\n-                return itemE;\n-            }\n-            else if (w.isInterrupted() || (timed && nanos <= 0L)) {\n-                \/\/ try to cancel and unlink\n-                if (s.casItem(e, s.isData ? null : s)) {\n-                    unsplice(pred, s);\n+        final Thread w = Thread.currentThread();\n+        int stat = -1;                   \/\/ -1: may yield, +1: park, else 0\n+        Object item;\n+        while ((item = s.item) == e) {\n+            if (needSweep)               \/\/ help clean\n+                sweep();\n+            else if ((timed && nanos <= 0L) || w.isInterrupted()) {\n+                if (s.casItem(e, (e == null) ? s : null)) {\n+                    unsplice(pred, s);   \/\/ cancelled\n@@ -723,5 +650,0 @@\n-                \/\/ return normally if lost CAS\n-            }\n-            else if (spins < 0) {            \/\/ establish spins at\/near front\n-                if ((spins = spinsFor(pred, s.isData)) > 0)\n-                    randomYields = ThreadLocalRandom.current();\n@@ -729,4 +651,12 @@\n-            else if (spins > 0) {             \/\/ spin\n-                --spins;\n-                if (randomYields.nextInt(CHAINED_SPINS) == 0)\n-                    Thread.yield();           \/\/ occasionally yield\n+            else if (stat <= 0) {\n+                if (pred != null && pred.next == s) {\n+                    if (stat < 0 &&\n+                        (pred.isData != isData || pred.isMatched())) {\n+                        stat = 0;        \/\/ yield once if first\n+                        Thread.yield();\n+                    }\n+                    else {\n+                        stat = 1;\n+                        s.waiter = w;    \/\/ enable unpark\n+                    }\n+                }                        \/\/ else signal in progress\n@@ -734,2 +664,8 @@\n-            else if (s.waiter == null) {\n-                s.waiter = w;                 \/\/ request unpark then recheck\n+            else if ((item = s.item) != e)\n+                break;                   \/\/ recheck\n+            else if (!timed) {\n+                LockSupport.setCurrentBlocker(this);\n+                try {\n+                    ForkJoinPool.managedBlock(s);\n+                } catch (InterruptedException cannotHappen) { }\n+                LockSupport.setCurrentBlocker(null);\n@@ -737,1 +673,1 @@\n-            else if (timed) {\n+            else {\n@@ -739,1 +675,1 @@\n-                if (nanos > 0L)\n+                if (nanos > SPIN_FOR_TIMEOUT_THRESHOLD)\n@@ -742,18 +678,0 @@\n-            else {\n-                LockSupport.park(this);\n-            }\n-        }\n-    }\n-\n-    \/**\n-     * Returns spin\/yield value for a node with given predecessor and\n-     * data mode. See above for explanation.\n-     *\/\n-    private static int spinsFor(Node pred, boolean haveData) {\n-        if (MP && pred != null) {\n-            if (pred.isData != haveData)      \/\/ phase change\n-                return FRONT_SPINS + CHAINED_SPINS;\n-            if (pred.isMatched())             \/\/ probably at front\n-                return FRONT_SPINS;\n-            if (pred.waiter == null)          \/\/ pred apparently spinning\n-                return CHAINED_SPINS;\n@@ -761,1 +679,5 @@\n-        return 0;\n+        if (stat == 1)\n+            WAITER.set(s, null);\n+        if (!isData)\n+            ITEM.set(s, s);              \/\/ self-link to avoid garbage\n+        return (E) item;\n@@ -1184,2 +1106,1 @@\n-         * nor s are head or offlist, add to sweepVotes, and if enough\n-         * votes have accumulated, sweep.\n+         * nor s are head or offlist, set needSweep;\n@@ -1203,4 +1124,2 @@\n-                \/\/ sweep every SWEEP_THRESHOLD votes\n-                if (pred.next != pred && s.next != s \/\/ recheck if offlist\n-                    && (incSweepVotes() & (SWEEP_THRESHOLD - 1)) == 0)\n-                    sweep();\n+                if (pred.next != pred && s.next != s)\n+                    needSweep = true;\n@@ -1216,0 +1135,1 @@\n+        needSweep = false;\n@@ -1268,1 +1188,1 @@\n-        xfer(e, true, ASYNC, 0);\n+        xfer(e, true, ASYNC, 0L);\n@@ -1281,1 +1201,1 @@\n-        xfer(e, true, ASYNC, 0);\n+        xfer(e, true, ASYNC, 0L);\n@@ -1293,1 +1213,1 @@\n-        xfer(e, true, ASYNC, 0);\n+        xfer(e, true, ASYNC, 0L);\n@@ -1306,1 +1226,1 @@\n-        xfer(e, true, ASYNC, 0);\n+        xfer(e, true, ASYNC, 0L);\n@@ -1321,1 +1241,1 @@\n-        return xfer(e, true, NOW, 0) == null;\n+        return xfer(e, true, NOW, 0L) == null;\n@@ -1336,1 +1256,1 @@\n-        if (xfer(e, true, SYNC, 0) != null) {\n+        if (xfer(e, true, SYNC, 0L) != null) {\n@@ -1366,1 +1286,1 @@\n-        E e = xfer(null, false, SYNC, 0);\n+        E e = xfer(null, false, SYNC, 0L);\n@@ -1381,1 +1301,1 @@\n-        return xfer(null, false, NOW, 0);\n+        return xfer(null, false, NOW, 0L);\n@@ -1725,1 +1645,0 @@\n-    private static final VarHandle SWEEPVOTES;\n@@ -1736,2 +1655,0 @@\n-            SWEEPVOTES = l.findVarHandle(LinkedTransferQueue.class, \"sweepVotes\",\n-                                         int.class);\n","filename":"src\/java.base\/share\/classes\/java\/util\/concurrent\/LinkedTransferQueue.java","additions":77,"deletions":160,"binary":false,"changes":237,"status":"modified"},{"patch":"@@ -169,0 +169,12 @@\n+     *\n+     * The above steps improve throughput when many threads produce\n+     * and\/or consume data. But they don't help much with\n+     * single-source \/ single-sink usages in which one side or the\n+     * other is always transiently blocked, and so throughput is\n+     * mainly a function of thread scheduling. This is not usually\n+     * noticeably improved with bounded short spin-waits. Instead both\n+     * forms of transfer try Thread.yield if apparently the sole\n+     * waiter. This works well when there are more tasks that cores,\n+     * which is expected to be the main usage context of this mode. In\n+     * other cases, waiters may help with some bookkeeping, then\n+     * park\/unpark.\n@@ -191,17 +203,0 @@\n-    \/**\n-     * The number of times to spin before blocking in timed waits.\n-     * The value is empirically derived -- it works well across a\n-     * variety of processors and OSes. Empirically, the best value\n-     * seems not to vary with number of CPUs (beyond 2) so is just\n-     * a constant.\n-     *\/\n-    static final int MAX_TIMED_SPINS =\n-        (Runtime.getRuntime().availableProcessors() < 2) ? 0 : 32;\n-\n-    \/**\n-     * The number of times to spin before blocking in untimed waits.\n-     * This is greater than timed value because untimed waits spin\n-     * faster since they don't need to check times on each spin.\n-     *\/\n-    static final int MAX_UNTIMED_SPINS = MAX_TIMED_SPINS * 16;\n-\n@@ -212,1 +207,1 @@\n-    static final long SPIN_FOR_TIMEOUT_THRESHOLD = 1000L;\n+    static final long SPIN_FOR_TIMEOUT_THRESHOLD = 1023L;\n@@ -236,1 +231,1 @@\n-        static final class SNode {\n+        static final class SNode implements ForkJoinPool.ManagedBlocker {\n@@ -264,6 +259,6 @@\n-                if (match == null &&\n-                    SMATCH.compareAndSet(this, null, s)) {\n-                    Thread w = waiter;\n-                    if (w != null) {    \/\/ waiters need at most one unpark\n-                        waiter = null;\n-                        LockSupport.unpark(w);\n+                SNode m; Thread w;\n+                if ((m = match) == null) {\n+                    if (SMATCH.compareAndSet(this, null, s)) {\n+                        if ((w = waiter) != null)\n+                            LockSupport.unpark(w);\n+                        return true;\n@@ -271,1 +266,2 @@\n-                    return true;\n+                    else\n+                        m = match;\n@@ -273,1 +269,1 @@\n-                return match == s;\n+                return m == s;\n@@ -279,2 +275,2 @@\n-            void tryCancel() {\n-                SMATCH.compareAndSet(this, null, this);\n+            boolean tryCancel() {\n+                return SMATCH.compareAndSet(this, null, this);\n@@ -287,0 +283,13 @@\n+            public final boolean isReleasable() {\n+                return match != null || Thread.currentThread().isInterrupted();\n+            }\n+\n+            public final boolean block() {\n+                while (!isReleasable()) LockSupport.park();\n+                return true;\n+            }\n+\n+            void forgetWaiter() {\n+                SWAITER.setOpaque(this, null);\n+            }\n+\n@@ -290,0 +299,1 @@\n+            private static final VarHandle SWAITER;\n@@ -295,0 +305,1 @@\n+                    SWAITER = l.findVarHandle(SNode.class, \"waiter\", Thread.class);\n@@ -361,4 +372,30 @@\n-                        SNode m = awaitFulfill(s, timed, nanos);\n-                        if (m == s) {               \/\/ wait was cancelled\n-                            clean(s);\n-                            return null;\n+                        long deadline = timed ? System.nanoTime() + nanos : 0L;\n+                        Thread w = Thread.currentThread();\n+                        int stat = -1; \/\/ -1: may yield, +1: park, else 0\n+                        SNode m;                    \/\/ await fulfill or cancel\n+                        while ((m = s.match) == null) {\n+                            if ((timed &&\n+                                 (nanos = deadline - System.nanoTime()) <= 0) ||\n+                                w.isInterrupted()) {\n+                                if (s.tryCancel()) {\n+                                    clean(s);       \/\/ wait cancelled\n+                                    return null;\n+                                }\n+                            } else if ((m = s.match) != null) {\n+                                break;              \/\/ recheck\n+                            } else if (stat <= 0) {\n+                                if (stat < 0 && h == null && head == s) {\n+                                    stat = 0;       \/\/ yield once if was empty\n+                                    Thread.yield();\n+                                } else {\n+                                    stat = 1;\n+                                    s.waiter = w;   \/\/ enable signal\n+                                }\n+                            } else if (!timed) {\n+                                LockSupport.setCurrentBlocker(this);\n+                                try {\n+                                    ForkJoinPool.managedBlock(s);\n+                                } catch (InterruptedException cannotHappen) { }\n+                                LockSupport.setCurrentBlocker(null);\n+                            } else if (nanos > SPIN_FOR_TIMEOUT_THRESHOLD)\n+                                LockSupport.parkNanos(this, nanos);\n@@ -366,3 +403,6 @@\n-                        if ((h = head) != null && h.next == s)\n-                            casHead(h, s.next);     \/\/ help s's fulfiller\n-                        return (E) ((mode == REQUEST) ? m.item : s.item);\n+                        if (stat == 1)\n+                            s.forgetWaiter();\n+                        Object result = (mode == REQUEST) ? m.item : s.item;\n+                        if (h != null && h.next == s)\n+                            casHead(h, s.next);     \/\/ help fulfiller\n+                        return (E) result;\n@@ -404,71 +444,0 @@\n-        \/**\n-         * Spins\/blocks until node s is matched by a fulfill operation.\n-         *\n-         * @param s the waiting node\n-         * @param timed true if timed wait\n-         * @param nanos timeout value\n-         * @return matched node, or s if cancelled\n-         *\/\n-        SNode awaitFulfill(SNode s, boolean timed, long nanos) {\n-            \/*\n-             * When a node\/thread is about to block, it sets its waiter\n-             * field and then rechecks state at least one more time\n-             * before actually parking, thus covering race vs\n-             * fulfiller noticing that waiter is non-null so should be\n-             * woken.\n-             *\n-             * When invoked by nodes that appear at the point of call\n-             * to be at the head of the stack, calls to park are\n-             * preceded by spins to avoid blocking when producers and\n-             * consumers are arriving very close in time.  This can\n-             * happen enough to bother only on multiprocessors.\n-             *\n-             * The order of checks for returning out of main loop\n-             * reflects fact that interrupts have precedence over\n-             * normal returns, which have precedence over\n-             * timeouts. (So, on timeout, one last check for match is\n-             * done before giving up.) Except that calls from untimed\n-             * SynchronousQueue.{poll\/offer} don't check interrupts\n-             * and don't wait at all, so are trapped in transfer\n-             * method rather than calling awaitFulfill.\n-             *\/\n-            final long deadline = timed ? System.nanoTime() + nanos : 0L;\n-            Thread w = Thread.currentThread();\n-            int spins = shouldSpin(s)\n-                ? (timed ? MAX_TIMED_SPINS : MAX_UNTIMED_SPINS)\n-                : 0;\n-            for (;;) {\n-                if (w.isInterrupted())\n-                    s.tryCancel();\n-                SNode m = s.match;\n-                if (m != null)\n-                    return m;\n-                if (timed) {\n-                    nanos = deadline - System.nanoTime();\n-                    if (nanos <= 0L) {\n-                        s.tryCancel();\n-                        continue;\n-                    }\n-                }\n-                if (spins > 0) {\n-                    Thread.onSpinWait();\n-                    spins = shouldSpin(s) ? (spins - 1) : 0;\n-                }\n-                else if (s.waiter == null)\n-                    s.waiter = w; \/\/ establish waiter so can park next iter\n-                else if (!timed)\n-                    LockSupport.park(this);\n-                else if (nanos > SPIN_FOR_TIMEOUT_THRESHOLD)\n-                    LockSupport.parkNanos(this, nanos);\n-            }\n-        }\n-\n-        \/**\n-         * Returns true if node s is at head or there is an active\n-         * fulfiller.\n-         *\/\n-        boolean shouldSpin(SNode s) {\n-            SNode h = head;\n-            return (h == s || h == null || isFulfilling(h.mode));\n-        }\n-\n@@ -480,1 +449,1 @@\n-            s.waiter = null; \/\/ forget thread\n+            s.forgetWaiter();\n@@ -536,1 +505,1 @@\n-        static final class QNode {\n+        static final class QNode implements ForkJoinPool.ManagedBlocker {\n@@ -560,2 +529,2 @@\n-            void tryCancel(Object cmp) {\n-                QITEM.compareAndSet(this, cmp, this);\n+            boolean tryCancel(Object cmp) {\n+                return QITEM.compareAndSet(this, cmp, this);\n@@ -577,0 +546,20 @@\n+            void forgetWaiter() {\n+                QWAITER.setOpaque(this, null);\n+            }\n+\n+            boolean isFulfilled() {\n+                Object x;\n+                return isData == ((x = item) == null) || x == this;\n+            }\n+\n+            public final boolean isReleasable() {\n+                Object x;\n+                return isData == ((x = item) == null) || x == this ||\n+                    Thread.currentThread().isInterrupted();\n+            }\n+\n+            public final boolean block() {\n+                while (!isReleasable()) LockSupport.park();\n+                return true;\n+            }\n+\n@@ -580,0 +569,1 @@\n+            private static final VarHandle QWAITER;\n@@ -585,0 +575,1 @@\n+                    QWAITER = l.findVarHandle(QNode.class, \"waiter\", Thread.class);\n@@ -664,1 +655,1 @@\n-            QNode s = null; \/\/ constructed\/reused as needed\n+            QNode s = null;                  \/\/ constructed\/reused as needed\n@@ -666,1 +657,0 @@\n-\n@@ -668,10 +658,7 @@\n-                QNode t = tail;\n-                QNode h = head;\n-                if (t == null || h == null)         \/\/ saw uninitialized value\n-                    continue;                       \/\/ spin\n-\n-                if (h == t || t.isData == isData) { \/\/ empty or same-mode\n-                    QNode tn = t.next;\n-                    if (t != tail)                  \/\/ inconsistent read\n-                        continue;\n-                    if (tn != null) {               \/\/ lagging tail\n+                QNode t = tail, h = head, m, tn;         \/\/ m is node to fulfill\n+                if (t == null || h == null)\n+                    ;                                    \/\/ inconsistent\n+                else if (h == t || t.isData == isData) { \/\/ empty or same-mode\n+                    if (t != tail)                       \/\/ inconsistent\n+                        ;\n+                    else if ((tn = t.next) != null)      \/\/ lagging tail\n@@ -679,3 +666,1 @@\n-                        continue;\n-                    }\n-                    if (timed && nanos <= 0L)       \/\/ can't wait\n+                    else if (timed && nanos <= 0L)       \/\/ can't wait\n@@ -683,17 +668,46 @@\n-                    if (s == null)\n-                        s = new QNode(e, isData);\n-                    if (!t.casNext(null, s))        \/\/ failed to link in\n-                        continue;\n-\n-                    advanceTail(t, s);              \/\/ swing tail and wait\n-                    Object x = awaitFulfill(s, e, timed, nanos);\n-                    if (x == s) {                   \/\/ wait was cancelled\n-                        clean(t, s);\n-                        return null;\n-                    }\n-\n-                    if (!s.isOffList()) {           \/\/ not already unlinked\n-                        advanceHead(t, s);          \/\/ unlink if head\n-                        if (x != null)              \/\/ and forget fields\n-                            s.item = s;\n-                        s.waiter = null;\n+                    else if (t.casNext(null, (s != null) ? s :\n+                                       (s = new QNode(e, isData)))) {\n+                        advanceTail(t, s);\n+                        long deadline = timed ? System.nanoTime() + nanos : 0L;\n+                        Thread w = Thread.currentThread();\n+                        int stat = -1; \/\/ same idea as TransferStack\n+                        Object item;\n+                        while ((item = s.item) == e) {\n+                            if ((timed &&\n+                                 (nanos = deadline - System.nanoTime()) <= 0) ||\n+                                w.isInterrupted()) {\n+                                if (s.tryCancel(e)) {\n+                                    clean(t, s);\n+                                    return null;\n+                                }\n+                            } else if ((item = s.item) != e) {\n+                                break;                   \/\/ recheck\n+                            } else if (stat <= 0) {\n+                                if (t.next == s) {\n+                                    if (stat < 0 && t.isFulfilled()) {\n+                                        stat = 0;        \/\/ yield once if first\n+                                        Thread.yield();\n+                                    }\n+                                    else {\n+                                        stat = 1;\n+                                        s.waiter = w;\n+                                    }\n+                                }\n+                            } else if (!timed) {\n+                                LockSupport.setCurrentBlocker(this);\n+                                try {\n+                                    ForkJoinPool.managedBlock(s);\n+                                } catch (InterruptedException cannotHappen) { }\n+                                LockSupport.setCurrentBlocker(null);\n+                            }\n+                            else if (nanos > SPIN_FOR_TIMEOUT_THRESHOLD)\n+                                LockSupport.parkNanos(this, nanos);\n+                        }\n+                        if (stat == 1)\n+                            s.forgetWaiter();\n+                        if (!s.isOffList()) {            \/\/ not already unlinked\n+                            advanceHead(t, s);           \/\/ unlink if head\n+                            if (item != null)            \/\/ and forget fields\n+                                s.item = s;\n+                        }\n+                        return (item != null) ? (E)item : e;\n@@ -701,6 +715,0 @@\n-                    return (x != null) ? (E)x : e;\n-\n-                } else {                            \/\/ complementary-mode\n-                    QNode m = h.next;               \/\/ node to fulfill\n-                    if (t != tail || m == null || h != head)\n-                        continue;                   \/\/ inconsistent read\n@@ -708,0 +716,2 @@\n+                } else if ((m = h.next) != null && t == tail && h == head) {\n+                    Thread waiter;\n@@ -709,5 +719,7 @@\n-                    if (isData == (x != null) ||    \/\/ m already fulfilled\n-                        x == m ||                   \/\/ m cancelled\n-                        !m.casItem(x, e)) {         \/\/ lost CAS\n-                        advanceHead(h, m);          \/\/ dequeue and retry\n-                        continue;\n+                    boolean fulfilled = ((isData == (x == null)) &&\n+                                         x != m && m.casItem(x, e));\n+                    advanceHead(h, m);                    \/\/ (help) dequeue\n+                    if (fulfilled) {\n+                        if ((waiter = m.waiter) != null)\n+                            LockSupport.unpark(waiter);\n+                        return (x != null) ? (E)x : e;\n@@ -715,40 +727,0 @@\n-\n-                    advanceHead(h, m);              \/\/ successfully fulfilled\n-                    LockSupport.unpark(m.waiter);\n-                    return (x != null) ? (E)x : e;\n-                }\n-            }\n-        }\n-\n-        \/**\n-         * Spins\/blocks until node s is fulfilled.\n-         *\n-         * @param s the waiting node\n-         * @param e the comparison value for checking match\n-         * @param timed true if timed wait\n-         * @param nanos timeout value\n-         * @return matched item, or s if cancelled\n-         *\/\n-        Object awaitFulfill(QNode s, E e, boolean timed, long nanos) {\n-            \/* Same idea as TransferStack.awaitFulfill *\/\n-            final long deadline = timed ? System.nanoTime() + nanos : 0L;\n-            Thread w = Thread.currentThread();\n-            int spins = (head.next == s)\n-                ? (timed ? MAX_TIMED_SPINS : MAX_UNTIMED_SPINS)\n-                : 0;\n-            for (;;) {\n-                if (w.isInterrupted())\n-                    s.tryCancel(e);\n-                Object x = s.item;\n-                if (x != e)\n-                    return x;\n-                if (timed) {\n-                    nanos = deadline - System.nanoTime();\n-                    if (nanos <= 0L) {\n-                        s.tryCancel(e);\n-                        continue;\n-                    }\n-                }\n-                if (spins > 0) {\n-                    --spins;\n-                    Thread.onSpinWait();\n@@ -756,6 +728,0 @@\n-                else if (s.waiter == null)\n-                    s.waiter = w;\n-                else if (!timed)\n-                    LockSupport.park(this);\n-                else if (nanos > SPIN_FOR_TIMEOUT_THRESHOLD)\n-                    LockSupport.parkNanos(this, nanos);\n@@ -769,1 +735,1 @@\n-            s.waiter = null; \/\/ forget thread\n+            s.forgetWaiter();\n","filename":"src\/java.base\/share\/classes\/java\/util\/concurrent\/SynchronousQueue.java","additions":167,"deletions":201,"binary":false,"changes":368,"status":"modified"},{"patch":"@@ -64,1 +64,0 @@\n-    final int SWEEP_THRESHOLD;\n@@ -75,3 +74,0 @@\n-        SWEEP_THRESHOLD = (int)\n-            lookup.findStaticVarHandle(qClass, \"SWEEP_THRESHOLD\", int.class)\n-            .get();\n@@ -370,30 +366,0 @@\n-    public void cancelledNodeSweeping() throws Throwable {\n-        assertEquals(SWEEP_THRESHOLD & (SWEEP_THRESHOLD - 1), 0);\n-        LinkedTransferQueue q = new LinkedTransferQueue();\n-        Thread blockHead = null;\n-        if (rnd.nextBoolean()) {\n-            blockHead = new Thread(\n-                () -> { try { q.take(); } catch (InterruptedException ok) {}});\n-            blockHead.start();\n-            while (nodeCount(q) != 2) { Thread.yield(); }\n-            assertTrue(q.hasWaitingConsumer());\n-            assertEquals(q.getWaitingConsumerCount(), 1);\n-        }\n-        int initialNodeCount = nodeCount(q);\n-\n-        \/\/ Some dead nodes do in fact accumulate ...\n-        if (blockHead != null)\n-            while (nodeCount(q) < initialNodeCount + SWEEP_THRESHOLD \/ 2)\n-                q.poll(1L, TimeUnit.MICROSECONDS);\n-\n-        \/\/ ... but no more than SWEEP_THRESHOLD nodes accumulate\n-        for (int i = rnd.nextInt(SWEEP_THRESHOLD * 10); i-->0; )\n-            q.poll(1L, TimeUnit.MICROSECONDS);\n-        assertTrue(nodeCount(q) <= initialNodeCount + SWEEP_THRESHOLD);\n-\n-        if (blockHead != null) {\n-            blockHead.interrupt();\n-            blockHead.join();\n-        }\n-    }\n-\n","filename":"test\/jdk\/java\/util\/concurrent\/LinkedTransferQueue\/WhiteBox.java","additions":0,"deletions":34,"binary":false,"changes":34,"status":"modified"}]}