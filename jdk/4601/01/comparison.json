{"files":[{"patch":"@@ -3760,0 +3760,9 @@\n+void Assembler::vpermb(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x8D);\n+  emit_operand(dst, src);\n+}\n+\n@@ -3840,0 +3849,8 @@\n+void Assembler::evpmultishiftqb(XMMRegister dst, XMMRegister ctl, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), ctl->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x83, (unsigned char)(0xC0 | encode));\n+}\n+\n@@ -4138,0 +4155,9 @@\n+void Assembler::vpmaskmovd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+  assert((VM_Version::supports_avx2() && vector_len == AVX_256bit), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x8C);\n+  emit_operand(dst, src);\n+}\n+\n@@ -6567,0 +6593,7 @@\n+void Assembler::vpsubusb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD8, (0xC0 | encode));\n+}\n+\n@@ -6658,0 +6691,8 @@\n+void Assembler::vpmulhuw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert((vector_len == AVX_128bit && VM_Version::supports_avx()) ||\n+         (vector_len == AVX_256bit && VM_Version::supports_avx2()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE4, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -1691,0 +1691,1 @@\n+  void vpermb(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n@@ -1701,0 +1702,1 @@\n+  void evpmultishiftqb(XMMRegister dst, XMMRegister ctl, XMMRegister src, int vector_len);\n@@ -1749,0 +1751,1 @@\n+  void vpmaskmovd(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n@@ -2251,0 +2254,1 @@\n+  void vpsubusb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n@@ -2271,0 +2275,1 @@\n+  void vpmulhuw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -5289,4 +5289,4 @@\n-  \/\/base64 character set\n-  address base64_charset_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"base64_charset\");\n+  address base64_shuffle_addr()\n+  {\n+    __ align(64, (unsigned long long)__ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"shuffle_base64\");\n@@ -5294,73 +5294,10 @@\n-    __ emit_data64(0x0000004200000041, relocInfo::none);\n-    __ emit_data64(0x0000004400000043, relocInfo::none);\n-    __ emit_data64(0x0000004600000045, relocInfo::none);\n-    __ emit_data64(0x0000004800000047, relocInfo::none);\n-    __ emit_data64(0x0000004a00000049, relocInfo::none);\n-    __ emit_data64(0x0000004c0000004b, relocInfo::none);\n-    __ emit_data64(0x0000004e0000004d, relocInfo::none);\n-    __ emit_data64(0x000000500000004f, relocInfo::none);\n-    __ emit_data64(0x0000005200000051, relocInfo::none);\n-    __ emit_data64(0x0000005400000053, relocInfo::none);\n-    __ emit_data64(0x0000005600000055, relocInfo::none);\n-    __ emit_data64(0x0000005800000057, relocInfo::none);\n-    __ emit_data64(0x0000005a00000059, relocInfo::none);\n-    __ emit_data64(0x0000006200000061, relocInfo::none);\n-    __ emit_data64(0x0000006400000063, relocInfo::none);\n-    __ emit_data64(0x0000006600000065, relocInfo::none);\n-    __ emit_data64(0x0000006800000067, relocInfo::none);\n-    __ emit_data64(0x0000006a00000069, relocInfo::none);\n-    __ emit_data64(0x0000006c0000006b, relocInfo::none);\n-    __ emit_data64(0x0000006e0000006d, relocInfo::none);\n-    __ emit_data64(0x000000700000006f, relocInfo::none);\n-    __ emit_data64(0x0000007200000071, relocInfo::none);\n-    __ emit_data64(0x0000007400000073, relocInfo::none);\n-    __ emit_data64(0x0000007600000075, relocInfo::none);\n-    __ emit_data64(0x0000007800000077, relocInfo::none);\n-    __ emit_data64(0x0000007a00000079, relocInfo::none);\n-    __ emit_data64(0x0000003100000030, relocInfo::none);\n-    __ emit_data64(0x0000003300000032, relocInfo::none);\n-    __ emit_data64(0x0000003500000034, relocInfo::none);\n-    __ emit_data64(0x0000003700000036, relocInfo::none);\n-    __ emit_data64(0x0000003900000038, relocInfo::none);\n-    __ emit_data64(0x0000002f0000002b, relocInfo::none);\n-    return start;\n-  }\n-\n-  \/\/base64 url character set\n-  address base64url_charset_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"base64url_charset\");\n-    address start = __ pc();\n-    __ emit_data64(0x0000004200000041, relocInfo::none);\n-    __ emit_data64(0x0000004400000043, relocInfo::none);\n-    __ emit_data64(0x0000004600000045, relocInfo::none);\n-    __ emit_data64(0x0000004800000047, relocInfo::none);\n-    __ emit_data64(0x0000004a00000049, relocInfo::none);\n-    __ emit_data64(0x0000004c0000004b, relocInfo::none);\n-    __ emit_data64(0x0000004e0000004d, relocInfo::none);\n-    __ emit_data64(0x000000500000004f, relocInfo::none);\n-    __ emit_data64(0x0000005200000051, relocInfo::none);\n-    __ emit_data64(0x0000005400000053, relocInfo::none);\n-    __ emit_data64(0x0000005600000055, relocInfo::none);\n-    __ emit_data64(0x0000005800000057, relocInfo::none);\n-    __ emit_data64(0x0000005a00000059, relocInfo::none);\n-    __ emit_data64(0x0000006200000061, relocInfo::none);\n-    __ emit_data64(0x0000006400000063, relocInfo::none);\n-    __ emit_data64(0x0000006600000065, relocInfo::none);\n-    __ emit_data64(0x0000006800000067, relocInfo::none);\n-    __ emit_data64(0x0000006a00000069, relocInfo::none);\n-    __ emit_data64(0x0000006c0000006b, relocInfo::none);\n-    __ emit_data64(0x0000006e0000006d, relocInfo::none);\n-    __ emit_data64(0x000000700000006f, relocInfo::none);\n-    __ emit_data64(0x0000007200000071, relocInfo::none);\n-    __ emit_data64(0x0000007400000073, relocInfo::none);\n-    __ emit_data64(0x0000007600000075, relocInfo::none);\n-    __ emit_data64(0x0000007800000077, relocInfo::none);\n-    __ emit_data64(0x0000007a00000079, relocInfo::none);\n-    __ emit_data64(0x0000003100000030, relocInfo::none);\n-    __ emit_data64(0x0000003300000032, relocInfo::none);\n-    __ emit_data64(0x0000003500000034, relocInfo::none);\n-    __ emit_data64(0x0000003700000036, relocInfo::none);\n-    __ emit_data64(0x0000003900000038, relocInfo::none);\n-    __ emit_data64(0x0000005f0000002d, relocInfo::none);\n-\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x0405030401020001, relocInfo::none);\n+    __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n+    __ emit_data64(0x10110f100d0e0c0d, relocInfo::none);\n+    __ emit_data64(0x1617151613141213, relocInfo::none);\n+    __ emit_data64(0x1c1d1b1c191a1819, relocInfo::none);\n+    __ emit_data64(0x222321221f201e1f, relocInfo::none);\n+    __ emit_data64(0x2829272825262425, relocInfo::none);\n+    __ emit_data64(0x2e2f2d2e2b2c2a2b, relocInfo::none);\n@@ -5370,3 +5307,4 @@\n-  address base64_bswap_mask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"bswap_mask_base64\");\n+  address base64_avx2_shuffle_addr()\n+  {\n+    __ align(32);\n+    StubCodeMark mark(this, \"StubRoutines\", \"avx2_shuffle_base64\");\n@@ -5374,9 +5312,4 @@\n-    __ emit_data64(0x0504038002010080, relocInfo::none);\n-    __ emit_data64(0x0b0a098008070680, relocInfo::none);\n-    __ emit_data64(0x0908078006050480, relocInfo::none);\n-    __ emit_data64(0x0f0e0d800c0b0a80, relocInfo::none);\n-    __ emit_data64(0x0605048003020180, relocInfo::none);\n-    __ emit_data64(0x0c0b0a8009080780, relocInfo::none);\n-    __ emit_data64(0x0504038002010080, relocInfo::none);\n-    __ emit_data64(0x0b0a098008070680, relocInfo::none);\n-\n+    __ emit_data64(0x0809070805060405, relocInfo::none);\n+    __ emit_data64(0x0e0f0d0e0b0c0a0b, relocInfo::none);\n+    __ emit_data64(0x0405030401020001, relocInfo::none);\n+    __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n@@ -5386,3 +5319,4 @@\n-  address base64_right_shift_mask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"right_shift_mask\");\n+  address base64_avx2_input_mask_addr()\n+  {\n+    __ align(32);\n+    StubCodeMark mark(this, \"StubRoutines\", \"avx2_input_mask_base64\");\n@@ -5390,9 +5324,4 @@\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-\n+    __ emit_data64(0x8000000000000000, relocInfo::none);\n+    __ emit_data64(0x8000000080000000, relocInfo::none);\n+    __ emit_data64(0x8000000080000000, relocInfo::none);\n+    __ emit_data64(0x8000000080000000, relocInfo::none);\n@@ -5402,3 +5331,4 @@\n-  address base64_left_shift_mask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"left_shift_mask\");\n+  address base64_avx2_lut_addr()\n+  {\n+    __ align(32);\n+    StubCodeMark mark(this, \"StubRoutines\", \"avx2_lut_base64\");\n@@ -5406,9 +5336,10 @@\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-\n+    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+    __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n+    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+    __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n+\n+    \/\/ URL LUT\n+    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+    __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n+    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+    __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n@@ -5418,3 +5349,4 @@\n-  address base64_and_mask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"and_mask\");\n+  address base64_encoding_table_addr()\n+  {\n+    __ align(64, (unsigned long long)__ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"encoding_table_base64\");\n@@ -5422,10 +5354,10 @@\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    return start;\n-  }\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x4847464544434241, relocInfo::none);\n+    __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n+    __ emit_data64(0x5857565554535251, relocInfo::none);\n+    __ emit_data64(0x6665646362615a59, relocInfo::none);\n+    __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n+    __ emit_data64(0x767574737271706f, relocInfo::none);\n+    __ emit_data64(0x333231307a797877, relocInfo::none);\n+    __ emit_data64(0x2f2b393837363534, relocInfo::none);\n@@ -5433,5 +5365,9 @@\n-  address base64_gather_mask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"gather_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    \/\/ URL table\n+    __ emit_data64(0x4847464544434241, relocInfo::none);\n+    __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n+    __ emit_data64(0x5857565554535251, relocInfo::none);\n+    __ emit_data64(0x6665646362615a59, relocInfo::none);\n+    __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n+    __ emit_data64(0x767574737271706f, relocInfo::none);\n+    __ emit_data64(0x333231307a797877, relocInfo::none);\n+    __ emit_data64(0x5f2d393837363534, relocInfo::none);\n@@ -5441,4 +5377,6 @@\n-\/\/ Code for generating Base64 encoding.\n-\/\/ Intrinsic function prototype in Base64.java:\n-\/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL) {\n-  address generate_base64_encodeBlock() {\n+  \/\/ Code for generating Base64 encoding.\n+  \/\/ Intrinsic function prototype in Base64.java:\n+  \/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp,\n+  \/\/ boolean isURL) {\n+  address generate_base64_encodeBlock()\n+  {\n@@ -5457,1 +5395,1 @@\n-    const Register source = c_rarg0; \/\/ Source Array\n+    const Register source = c_rarg0;       \/\/ Source Array\n@@ -5459,2 +5397,2 @@\n-    const Register end_offset = c_rarg2; \/\/ end offset\n-    const Register dest = c_rarg3; \/\/ destination array\n+    const Register end_offset = c_rarg2;   \/\/ end offset\n+    const Register dest = c_rarg3;   \/\/ destination array\n@@ -5463,2 +5401,2 @@\n-    const Register dp = c_rarg4;  \/\/ Position for writing to dest array\n-    const Register isURL = c_rarg5;\/\/ Base64 or URL character set\n+    const Register dp = c_rarg4;    \/\/ Position for writing to dest array\n+    const Register isURL = c_rarg5; \/\/ Base64 or URL character set\n@@ -5466,1 +5404,2 @@\n-    const Address  dp_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+    const Address dp_mem(rbp,\n+             6 * wordSize); \/\/ length is on stack on Win64\n@@ -5468,1 +5407,1 @@\n-    const Register isURL = r10;      \/\/ pick the volatile windows register\n+    const Register isURL = r10; \/\/ pick the volatile windows register\n@@ -5475,1 +5414,3 @@\n-    Label L_process80, L_process32, L_process3, L_exit, L_processdata;\n+    const Register encode_table = r13;\n+    Label L_process3, L_exit, L_processdata, L_vbmiLoop, L_not512,\n+      L_32byteLoop;\n@@ -5483,5 +5424,253 @@\n-    __ lea(r11, ExternalAddress(StubRoutines::x86::base64_charset_addr()));\n-    \/\/ check if base64 charset(isURL=0) or base64 url charset(isURL=1) needs to be loaded\n-    __ cmpl(isURL, 0);\n-    __ jcc(Assembler::equal, L_processdata);\n-    __ lea(r11, ExternalAddress(StubRoutines::x86::base64url_charset_addr()));\n+    \/\/ Code for 512-bit VBMI encoding.  Encodes 48 input bytes into 64\n+    \/\/ output bytes. We read 64 input bytes and ignore the last 16, so be\n+    \/\/ sure not to read past the end of the input buffer.\n+    if (VM_Version::supports_avx512_vbmi()) {\n+      __ cmpl(length, 64); \/\/ Do not overrun input buffer.\n+      __ jcc(Assembler::below, L_not512);\n+\n+      __ shll(isURL, 6); \/\/ index into decode table based on isURL\n+      __ lea(encode_table, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n+      __ addptr(encode_table, isURL);\n+      __ shrl(isURL, 6); \/\/ restore isURL\n+\n+      __ mov64(rax, 0x3036242a1016040a); \/\/ Shifts\n+      __ evmovdquq(xmm3, ExternalAddress(StubRoutines::x86::base64_shuffle_addr()),\n+        Assembler::AVX_512bit, r15);\n+      __ evmovdquq(xmm2, Address(encode_table, 0), Assembler::AVX_512bit);\n+      __ evpbroadcastq(xmm1, rax, Assembler::AVX_512bit);\n+\n+      __ align(32);\n+      __ BIND(L_vbmiLoop);\n+\n+      __ vpermb(xmm0, xmm3, Address(source, start_offset), Assembler::AVX_512bit);\n+      __ subl(length, 48);\n+\n+      \/\/ Put the input bytes into the proper lanes for writing, then\n+      \/\/ encode them.\n+      __ evpmultishiftqb(xmm0, xmm1, xmm0, Assembler::AVX_512bit);\n+      __ vpermb(xmm0, xmm0, xmm2, Assembler::AVX_512bit);\n+\n+      \/\/ Write to destination\n+      __ evmovdquq(Address(dest, dp), xmm0, Assembler::AVX_512bit);\n+\n+      __ addptr(dest, 64);\n+      __ addptr(source, 48);\n+      __ cmpl(length, 64);\n+      __ jcc(Assembler::aboveEqual, L_vbmiLoop);\n+\n+      __ vzeroupper();\n+    }\n+\n+    __ BIND(L_not512);\n+    if (VM_Version::supports_avx2()\n+        && VM_Version::supports_avx512vlbw()) {\n+      \/*\n+      ** This AVX2 encoder is based off the paper at:\n+      **      https:\/\/dl.acm.org\/doi\/10.1145\/3132709\n+      **\n+      ** We use AVX2 SIMD instructions to encode 24 bytes into 32\n+      ** output bytes.\n+      **\n+      *\/\n+      \/\/ Lengths under 32 bytes are done with scalar routine\n+      __ cmpl(length, 31);\n+      __ jcc(Assembler::belowEqual, L_process3);\n+\n+      \/\/ Set up supporting constant table data\n+      __ vmovdqu(xmm9, ExternalAddress(StubRoutines::x86::base64_avx2_shuffle_addr()));\n+      \/\/ 6-bit mask for 2nd and 4th (and multiples) 6-bit values\n+      __ movl(rax, 0x0fc0fc00);\n+      __ vmovdqu(xmm1, ExternalAddress(StubRoutines::x86::base64_avx2_input_mask_addr()));\n+      __ evpbroadcastd(xmm8, rax, Assembler::AVX_256bit);\n+\n+      \/\/ Multiplication constant for \"shifting\" right by 6 and 10\n+      \/\/ bits\n+      __ movl(rax, 0x04000040);\n+\n+      __ subl(length, 24);\n+      __ evpbroadcastd(xmm7, rax, Assembler::AVX_256bit);\n+\n+      \/\/ For the first load, we mask off reading of the first 4\n+      \/\/ bytes into the register. This is so we can get 4 3-byte\n+      \/\/ chunks into each lane of the register, avoiding having to\n+      \/\/ handle end conditions.  We then shuffle these bytes into a\n+      \/\/ specific order so that manipulation is easier.\n+      \/\/\n+      \/\/ The initial read loads the XMM register like this:\n+      \/\/\n+      \/\/ Lower 128-bit lane:\n+      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+      \/\/ | XX | XX | XX | XX | A0 | A1 | A2 | B0 | B1 | B2 | C0 | C1\n+      \/\/ | C2 | D0 | D1 | D2 |\n+      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+      \/\/\n+      \/\/ Upper 128-bit lane:\n+      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+      \/\/ | E0 | E1 | E2 | F0 | F1 | F2 | G0 | G1 | G2 | H0 | H1 | H2\n+      \/\/ | XX | XX | XX | XX |\n+      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+      \/\/\n+      \/\/ Where A0 is the first input byte, B0 is the fourth, etc.\n+      \/\/ The alphabetical significance denotes the 3 bytes to be\n+      \/\/ consumed and encoded into 4 bytes.\n+      \/\/\n+      \/\/ We then shuffle the register so each 32-bit word contains\n+      \/\/ the sequence:\n+      \/\/    A1 A0 A2 A1, B1, B0, B2, B1, etc.\n+      \/\/ Each of these byte sequences are then manipulated into 4\n+      \/\/ 6-bit values ready for encoding.\n+      \/\/\n+      \/\/ If we focus on one set of 3-byte chunks, changing the\n+      \/\/ nomenclature such that A0 => a, A1 => b, and A2 => c, we\n+      \/\/ shuffle such that each 24-bit chunk contains:\n+      \/\/\n+      \/\/ b7 b6 b5 b4 b3 b2 b1 b0 | a7 a6 a5 a4 a3 a2 a1 a0 | c7 c6\n+      \/\/ c5 c4 c3 c2 c1 c0 | b7 b6 b5 b4 b3 b2 b1 b0\n+      \/\/ Explain this step.\n+      \/\/ b3 b2 b1 b0 c5 c4 c3 c2 | c1 c0 d5 d4 d3 d2 d1 d0 | a5 a4\n+      \/\/ a3 a2 a1 a0 b5 b4 | b3 b2 b1 b0 c5 c4 c3 c2\n+      \/\/\n+      \/\/ W first and off all but bits 4-9 and 16-21 (c5..c0 and\n+      \/\/ a5..a0) and shift them using a vector multiplication\n+      \/\/ operation (vpmulhuw) which effectively shifts c right by 6\n+      \/\/ bits and a right by 10 bits.  We similarly mask bits 10-15\n+      \/\/ (d5..d0) and 22-27 (b5..b0) and shift them left by 8 and 4\n+      \/\/ bits respecively.  This is done using vpmullw.  We end up\n+      \/\/ with 4 6-bit values, thus splitting the 3 input bytes,\n+      \/\/ ready for encoding:\n+      \/\/    0 0 d5..d0 0 0 c5..c0 0 0 b5..b0 0 0 a5..a0\n+      \/\/\n+      \/\/ For translation, we recognize that there are 5 distinct\n+      \/\/ ranges of legal Base64 characters as below:\n+      \/\/\n+      \/\/   +-------------+-------------+------------+\n+      \/\/   | 6-bit value | ASCII range |   offset   |\n+      \/\/   +-------------+-------------+------------+\n+      \/\/   |    0..25    |    A..Z     |     65     |\n+      \/\/   |   26..51    |    a..z     |     71     |\n+      \/\/   |   52..61    |    0..9     |     -4     |\n+      \/\/   |     62      |   + or -    | -19 or -17 |\n+      \/\/   |     63      |   \/ or _    | -16 or 32  |\n+      \/\/   +-------------+-------------+------------+\n+      \/\/\n+      \/\/ We note that vpshufb does a parallel lookup in a\n+      \/\/ destination register using the lower 4 bits of bytes from a\n+      \/\/ source register.  If we use a saturated subtraction and\n+      \/\/ subtract 51 from each 6-bit value, bytes from [0,51]\n+      \/\/ saturate to 0, and [52,63] map to a range of [1,12].  We\n+      \/\/ distinguish the [0,25] and [26,51] ranges by assigning a\n+      \/\/ value of 13 for all 6-bit values less than 26.  We end up\n+      \/\/ with:\n+      \/\/\n+      \/\/   +-------------+-------------+------------+\n+      \/\/   | 6-bit value |   Reduced   |   offset   |\n+      \/\/   +-------------+-------------+------------+\n+      \/\/   |    0..25    |     13      |     65     |\n+      \/\/   |   26..51    |      0      |     71     |\n+      \/\/   |   52..61    |    0..9     |     -4     |\n+      \/\/   |     62      |     11      | -19 or -17 |\n+      \/\/   |     63      |     12      | -16 or 32  |\n+      \/\/   +-------------+-------------+------------+\n+      \/\/\n+      \/\/ We then use a final vpshufb to add the appropriate offset,\n+      \/\/ translating the bytes.\n+      \/\/\n+      \/\/ Load input bytes - only 28 bytes.  Mask the first load to\n+      \/\/ not load into the full register.\n+      __ vpmaskmovd(xmm1, xmm1, Address(source, start_offset, Address::times_1, -4), Assembler::AVX_256bit);\n+\n+      \/\/ Move 3-byte chunks of input (12 bytes) into 16 bytes,\n+      \/\/ ordering by:\n+      \/\/   1, 0, 2, 1; 4, 3, 5, 4; etc.  This groups 6-bit chunks\n+      \/\/   for easy masking\n+      __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n+\n+      __ addl(start_offset, 24);\n+\n+      \/\/ Load masking register for first and third (and multiples)\n+      \/\/ 6-bit values.\n+      __ movl(rax, 0x003f03f0);\n+      __ evpbroadcastd(xmm6, rax, Assembler::AVX_256bit);\n+      \/\/ Multiplication constant for \"shifting\" left by 4 and 8 bits\n+      __ movl(rax, 0x01000010);\n+      __ evpbroadcastd(xmm5, rax, Assembler::AVX_256bit);\n+\n+      \/\/ Isolate 6-bit chunks of interest\n+      __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n+\n+      \/\/ Load constants for encoding\n+      __ movl(rax, 0x19191919);\n+      __ evpbroadcastd(xmm3, rax, Assembler::AVX_256bit);\n+      __ movl(rax, 0x33333333);\n+      __ evpbroadcastd(xmm4, rax, Assembler::AVX_256bit);\n+\n+      \/\/ Shift output bytes 0 and 2 into proper lanes\n+      __ vpmulhuw(xmm2, xmm0, xmm7, Assembler::AVX_256bit);\n+\n+      \/\/ Mask and shift output bytes 1 and 3 into proper lanes and\n+      \/\/ combine\n+      __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n+      __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n+      __ vpor(xmm0, xmm0, xmm2, Assembler::AVX_256bit);\n+\n+      \/\/ Find out which are 0..25.  This indicates which input\n+      \/\/ values fall in the range of 'A'-'Z', which require an\n+      \/\/ additional offset (see comments above)\n+      __ vpcmpgtb(xmm2, xmm0, xmm3, Assembler::AVX_256bit);\n+      __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n+      __ vpsubb(xmm1, xmm1, xmm2, Assembler::AVX_256bit);\n+\n+      \/\/ Load the proper lookup table\n+      __ lea(r11, ExternalAddress(StubRoutines::x86::base64_avx2_lut_addr()));\n+      __ movl(r15, isURL);\n+      __ shll(r15, 5);\n+      __ vmovdqu(xmm2, Address(r11, r15));\n+\n+      \/\/ Shuffle the offsets based on the range calculation done\n+      \/\/ above. This allows us to add the correct offset to the\n+      \/\/ 6-bit value corresponding to the range documented above.\n+      __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n+      __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n+\n+      \/\/ Store the encoded bytes\n+      __ vmovdqu(Address(dest, dp), xmm0);\n+      __ addl(dp, 32);\n+\n+      __ cmpl(length, 31);\n+      __ jcc(Assembler::belowEqual, L_process3);\n+\n+      __ align(32);\n+      __ BIND(L_32byteLoop);\n+\n+      \/\/ Get next 32 bytes\n+      __ vmovdqu(xmm1, Address(source, start_offset, Address::times_1, -4));\n+\n+      __ subl(length, 24);\n+      __ addl(start_offset, 24);\n+\n+      \/\/ This logic is identical to the above, with only constant\n+      \/\/ register loads removed.  Shuffle the input, mask off 6-bit\n+      \/\/ chunks, shift them into place, then add the offset to\n+      \/\/ encode.\n+      __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n+\n+      __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n+      __ vpmulhuw(xmm10, xmm0, xmm7, Assembler::AVX_256bit);\n+      __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n+      __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n+      __ vpor(xmm0, xmm0, xmm10, Assembler::AVX_256bit);\n+      __ vpcmpgtb(xmm10, xmm0, xmm3, Assembler::AVX_256bit);\n+      __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n+      __ vpsubb(xmm1, xmm1, xmm10, Assembler::AVX_256bit);\n+      __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n+      __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n+\n+      \/\/ Store the encoded bytes\n+      __ vmovdqu(Address(dest, dp), xmm0);\n+      __ addl(dp, 32);\n+\n+      __ cmpl(length, 31);\n+      __ jcc(Assembler::above, L_32byteLoop);\n+\n+      __ vzeroupper();\n+    }\n@@ -5489,147 +5678,0 @@\n-    \/\/ load masks required for encoding data\n-    __ BIND(L_processdata);\n-    __ movdqu(xmm16, ExternalAddress(StubRoutines::x86::base64_gather_mask_addr()));\n-    \/\/ Set 64 bits of K register.\n-    __ evpcmpeqb(k3, xmm16, xmm16, Assembler::AVX_512bit);\n-    __ evmovdquq(xmm12, ExternalAddress(StubRoutines::x86::base64_bswap_mask_addr()), Assembler::AVX_256bit, r13);\n-    __ evmovdquq(xmm13, ExternalAddress(StubRoutines::x86::base64_right_shift_mask_addr()), Assembler::AVX_512bit, r13);\n-    __ evmovdquq(xmm14, ExternalAddress(StubRoutines::x86::base64_left_shift_mask_addr()), Assembler::AVX_512bit, r13);\n-    __ evmovdquq(xmm15, ExternalAddress(StubRoutines::x86::base64_and_mask_addr()), Assembler::AVX_512bit, r13);\n-\n-    \/\/ Vector Base64 implementation, producing 96 bytes of encoded data\n-    __ BIND(L_process80);\n-    __ cmpl(length, 80);\n-    __ jcc(Assembler::below, L_process32);\n-    __ evmovdquq(xmm0, Address(source, start_offset, Address::times_1, 0), Assembler::AVX_256bit);\n-    __ evmovdquq(xmm1, Address(source, start_offset, Address::times_1, 24), Assembler::AVX_256bit);\n-    __ evmovdquq(xmm2, Address(source, start_offset, Address::times_1, 48), Assembler::AVX_256bit);\n-\n-    \/\/permute the input data in such a manner that we have continuity of the source\n-    __ vpermq(xmm3, xmm0, 148, Assembler::AVX_256bit);\n-    __ vpermq(xmm4, xmm1, 148, Assembler::AVX_256bit);\n-    __ vpermq(xmm5, xmm2, 148, Assembler::AVX_256bit);\n-\n-    \/\/shuffle input and group 3 bytes of data and to it add 0 as the 4th byte.\n-    \/\/we can deal with 12 bytes at a time in a 128 bit register\n-    __ vpshufb(xmm3, xmm3, xmm12, Assembler::AVX_256bit);\n-    __ vpshufb(xmm4, xmm4, xmm12, Assembler::AVX_256bit);\n-    __ vpshufb(xmm5, xmm5, xmm12, Assembler::AVX_256bit);\n-\n-    \/\/convert byte to word. Each 128 bit register will have 6 bytes for processing\n-    __ vpmovzxbw(xmm3, xmm3, Assembler::AVX_512bit);\n-    __ vpmovzxbw(xmm4, xmm4, Assembler::AVX_512bit);\n-    __ vpmovzxbw(xmm5, xmm5, Assembler::AVX_512bit);\n-\n-    \/\/ Extract bits in the following pattern 6, 4+2, 2+4, 6 to convert 3, 8 bit numbers to 4, 6 bit numbers\n-    __ evpsrlvw(xmm0, xmm3, xmm13,  Assembler::AVX_512bit);\n-    __ evpsrlvw(xmm1, xmm4, xmm13, Assembler::AVX_512bit);\n-    __ evpsrlvw(xmm2, xmm5, xmm13, Assembler::AVX_512bit);\n-\n-    __ evpsllvw(xmm3, xmm3, xmm14, Assembler::AVX_512bit);\n-    __ evpsllvw(xmm4, xmm4, xmm14, Assembler::AVX_512bit);\n-    __ evpsllvw(xmm5, xmm5, xmm14, Assembler::AVX_512bit);\n-\n-    __ vpsrlq(xmm0, xmm0, 8, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm1, xmm1, 8, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm2, xmm2, 8, Assembler::AVX_512bit);\n-\n-    __ vpsllq(xmm3, xmm3, 8, Assembler::AVX_512bit);\n-    __ vpsllq(xmm4, xmm4, 8, Assembler::AVX_512bit);\n-    __ vpsllq(xmm5, xmm5, 8, Assembler::AVX_512bit);\n-\n-    __ vpandq(xmm3, xmm3, xmm15, Assembler::AVX_512bit);\n-    __ vpandq(xmm4, xmm4, xmm15, Assembler::AVX_512bit);\n-    __ vpandq(xmm5, xmm5, xmm15, Assembler::AVX_512bit);\n-\n-    \/\/ Get the final 4*6 bits base64 encoding\n-    __ vporq(xmm3, xmm3, xmm0, Assembler::AVX_512bit);\n-    __ vporq(xmm4, xmm4, xmm1, Assembler::AVX_512bit);\n-    __ vporq(xmm5, xmm5, xmm2, Assembler::AVX_512bit);\n-\n-    \/\/ Shift\n-    __ vpsrlq(xmm3, xmm3, 8, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm4, xmm4, 8, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm5, xmm5, 8, Assembler::AVX_512bit);\n-\n-    \/\/ look up 6 bits in the base64 character set to fetch the encoding\n-    \/\/ we are converting word to dword as gather instructions need dword indices for looking up encoding\n-    __ vextracti64x4(xmm6, xmm3, 0);\n-    __ vpmovzxwd(xmm0, xmm6, Assembler::AVX_512bit);\n-    __ vextracti64x4(xmm6, xmm3, 1);\n-    __ vpmovzxwd(xmm1, xmm6, Assembler::AVX_512bit);\n-\n-    __ vextracti64x4(xmm6, xmm4, 0);\n-    __ vpmovzxwd(xmm2, xmm6, Assembler::AVX_512bit);\n-    __ vextracti64x4(xmm6, xmm4, 1);\n-    __ vpmovzxwd(xmm3, xmm6, Assembler::AVX_512bit);\n-\n-    __ vextracti64x4(xmm4, xmm5, 0);\n-    __ vpmovzxwd(xmm6, xmm4, Assembler::AVX_512bit);\n-\n-    __ vextracti64x4(xmm4, xmm5, 1);\n-    __ vpmovzxwd(xmm7, xmm4, Assembler::AVX_512bit);\n-\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm4, k2, Address(r11, xmm0, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm5, k2, Address(r11, xmm1, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm8, k2, Address(r11, xmm2, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm9, k2, Address(r11, xmm3, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm10, k2, Address(r11, xmm6, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm11, k2, Address(r11, xmm7, Address::times_4, 0), Assembler::AVX_512bit);\n-\n-    \/\/Down convert dword to byte. Final output is 16*6 = 96 bytes long\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 0), xmm4, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 16), xmm5, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 32), xmm8, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 48), xmm9, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 64), xmm10, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 80), xmm11, Assembler::AVX_512bit);\n-\n-    __ addq(dest, 96);\n-    __ addq(source, 72);\n-    __ subq(length, 72);\n-    __ jmp(L_process80);\n-\n-    \/\/ Vector Base64 implementation generating 32 bytes of encoded data\n-    __ BIND(L_process32);\n-    __ cmpl(length, 32);\n-    __ jcc(Assembler::below, L_process3);\n-    __ evmovdquq(xmm0, Address(source, start_offset), Assembler::AVX_256bit);\n-    __ vpermq(xmm0, xmm0, 148, Assembler::AVX_256bit);\n-    __ vpshufb(xmm6, xmm0, xmm12, Assembler::AVX_256bit);\n-    __ vpmovzxbw(xmm6, xmm6, Assembler::AVX_512bit);\n-    __ evpsrlvw(xmm2, xmm6, xmm13, Assembler::AVX_512bit);\n-    __ evpsllvw(xmm3, xmm6, xmm14, Assembler::AVX_512bit);\n-\n-    __ vpsrlq(xmm2, xmm2, 8, Assembler::AVX_512bit);\n-    __ vpsllq(xmm3, xmm3, 8, Assembler::AVX_512bit);\n-    __ vpandq(xmm3, xmm3, xmm15, Assembler::AVX_512bit);\n-    __ vporq(xmm1, xmm2, xmm3, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm1, xmm1, 8, Assembler::AVX_512bit);\n-    __ vextracti64x4(xmm9, xmm1, 0);\n-    __ vpmovzxwd(xmm6, xmm9, Assembler::AVX_512bit);\n-    __ vextracti64x4(xmm9, xmm1, 1);\n-    __ vpmovzxwd(xmm5, xmm9,  Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm8, k2, Address(r11, xmm6, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm10, k2, Address(r11, xmm5, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 0), xmm8, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 16), xmm10, Assembler::AVX_512bit);\n-    __ subq(length, 24);\n-    __ addq(dest, 32);\n-    __ addq(source, 24);\n-    __ jmp(L_process32);\n-\n-    \/\/ Scalar data processing takes 3 bytes at a time and produces 4 bytes of encoded data\n-    \/* This code corresponds to the scalar version of the following snippet in Base64.java\n-    ** int bits = (src[sp0++] & 0xff) << 16 |(src[sp0++] & 0xff) << 8 |(src[sp0++] & 0xff);\n-    ** dst[dp0++] = (byte)base64[(bits >> > 18) & 0x3f];\n-    ** dst[dp0++] = (byte)base64[(bits >> > 12) & 0x3f];\n-    ** dst[dp0++] = (byte)base64[(bits >> > 6) & 0x3f];\n-    ** dst[dp0++] = (byte)base64[bits & 0x3f];*\/\n@@ -5639,9 +5681,23 @@\n-    \/\/ Read 1 byte at a time\n-    __ movzbl(rax, Address(source, start_offset));\n-    __ shll(rax, 0x10);\n-    __ movl(r15, rax);\n-    __ movzbl(rax, Address(source, start_offset, Address::times_1, 1));\n-    __ shll(rax, 0x8);\n-    __ movzwl(rax, rax);\n-    __ orl(r15, rax);\n-    __ movzbl(rax, Address(source, start_offset, Address::times_1, 2));\n+\n+    \/\/ Load the encoding table based on isURL\n+    __ lea(r11, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n+    __ movl(r15, isURL);\n+    __ shll(r15, 5);\n+    __ addptr(r11, r15);\n+\n+    __ BIND(L_processdata);\n+\n+    \/\/ Load 3 bytes\n+    __ load_unsigned_byte(r15, Address(source, start_offset));\n+    __ load_unsigned_byte(r10, Address(source, start_offset, Address::times_1, 1));\n+    __ load_unsigned_byte(r13, Address(source, start_offset, Address::times_1, 2));\n+\n+    \/\/ Build a 32-bit word with bytes 1, 2, 0, 1\n+    __ movl(rax, r10);\n+    __ shll(r10, 24);\n+    __ orl(rax, r10);\n+\n+    __ subl(length, 3);\n+\n+    __ shll(r15, 8);\n+    __ shll(r13, 16);\n@@ -5649,10 +5705,23 @@\n-    \/\/ Save 3 bytes read in r15\n-    __ movl(r15, rax);\n-    __ shrl(rax, 0x12);\n-    __ andl(rax, 0x3f);\n-    \/\/ rax contains the index, r11 contains base64 lookup table\n-    __ movb(rax, Address(r11, rax, Address::times_4));\n-    \/\/ Write the encoded byte to destination\n-    __ movb(Address(dest, dp, Address::times_1, 0), rax);\n-    __ movl(rax, r15);\n-    __ shrl(rax, 0xc);\n+\n+    __ addl(start_offset, 3);\n+\n+    __ orl(rax, r13);\n+    \/\/ At this point, rax contains | byte1 | byte2 | byte0 | byte1\n+    \/\/ r13 has byte2 << 16 - need low-order 6 bits to translate.\n+    \/\/ This translated byte is the fourth output byte.\n+    __ shrl(r13, 16);\n+    __ andl(r13, 0x3f);\n+\n+    \/\/ The high-order 6 bits of r15 (byte0) is translated.\n+    \/\/ The translated byte is the first output byte.\n+    __ shrl(r15, 10);\n+\n+    __ load_unsigned_byte(r13, Address(r11, r13));\n+    __ load_unsigned_byte(r15, Address(r11, r15));\n+\n+    __ movb(Address(dest, dp, Address::times_1, 3), r13);\n+\n+    \/\/ Extract high-order 4 bits of byte1 and low-order 2 bits of byte0.\n+    \/\/ This translated byte is the second output byte.\n+    __ shrl(rax, 4);\n+    __ movl(r10, rax);\n@@ -5660,1 +5729,12 @@\n-    __ movb(rax, Address(r11, rax, Address::times_4));\n+\n+    __ movb(Address(dest, dp, Address::times_1, 0), r15);\n+\n+    __ load_unsigned_byte(rax, Address(r11, rax));\n+\n+    \/\/ Extract low-order 2 bits of byte1 and high-order 4 bits of byte2.\n+    \/\/ This translated byte is the third output byte.\n+    __ shrl(r10, 18);\n+    __ andl(r10, 0x3f);\n+\n+    __ load_unsigned_byte(r10, Address(r11, r10));\n+\n@@ -5662,13 +5742,6 @@\n-    __ movl(rax, r15);\n-    __ shrl(rax, 0x6);\n-    __ andl(rax, 0x3f);\n-    __ movb(rax, Address(r11, rax, Address::times_4));\n-    __ movb(Address(dest, dp, Address::times_1, 2), rax);\n-    __ movl(rax, r15);\n-    __ andl(rax, 0x3f);\n-    __ movb(rax, Address(r11, rax, Address::times_4));\n-    __ movb(Address(dest, dp, Address::times_1, 3), rax);\n-    __ subl(length, 3);\n-    __ addq(dest, 4);\n-    __ addq(source, 3);\n-    __ jmp(L_process3);\n+    __ movb(Address(dest, dp, Address::times_1, 2), r10);\n+\n+    __ addl(dp, 4);\n+    __ cmpl(length, 3);\n+    __ jcc(Assembler::aboveEqual, L_processdata);\n+\n@@ -7606,0 +7679,1 @@\n+\n@@ -7607,10 +7681,10 @@\n-      StubRoutines::x86::_and_mask = base64_and_mask_addr();\n-      StubRoutines::x86::_bswap_mask = base64_bswap_mask_addr();\n-      StubRoutines::x86::_base64_charset = base64_charset_addr();\n-      StubRoutines::x86::_url_charset = base64url_charset_addr();\n-      StubRoutines::x86::_gather_mask = base64_gather_mask_addr();\n-      StubRoutines::x86::_left_shift_mask = base64_left_shift_mask_addr();\n-      StubRoutines::x86::_right_shift_mask = base64_right_shift_mask_addr();\n-      StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();\n-      if(VM_Version::supports_avx512_vbmi() &&\n-         VM_Version::supports_avx512bw()) {\n+      if(VM_Version::supports_avx2() &&\n+         VM_Version::supports_avx512bw() &&\n+         VM_Version::supports_avx512vl()) {\n+        StubRoutines::x86::_avx2_shuffle_base64 = base64_avx2_shuffle_addr();\n+        StubRoutines::x86::_avx2_input_mask_base64 = base64_avx2_input_mask_addr();\n+        StubRoutines::x86::_avx2_lut_base64 = base64_avx2_lut_addr();\n+      }\n+      StubRoutines::x86::_encoding_table_base64 = base64_encoding_table_addr();\n+      if (VM_Version::supports_avx512_vbmi()) {\n+        StubRoutines::x86::_shuffle_base64 = base64_shuffle_addr();\n@@ -7627,0 +7701,1 @@\n+      StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":413,"deletions":338,"binary":false,"changes":751,"status":"modified"},{"patch":"@@ -68,7 +68,5 @@\n-address StubRoutines::x86::_bswap_mask = NULL;\n-address StubRoutines::x86::_base64_charset = NULL;\n-address StubRoutines::x86::_gather_mask = NULL;\n-address StubRoutines::x86::_right_shift_mask = NULL;\n-address StubRoutines::x86::_left_shift_mask = NULL;\n-address StubRoutines::x86::_and_mask = NULL;\n-address StubRoutines::x86::_url_charset = NULL;\n+address StubRoutines::x86::_encoding_table_base64 = NULL;\n+address StubRoutines::x86::_shuffle_base64 = NULL;\n+address StubRoutines::x86::_avx2_shuffle_base64 = NULL;\n+address StubRoutines::x86::_avx2_input_mask_base64 = NULL;\n+address StubRoutines::x86::_avx2_lut_base64 = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":5,"deletions":7,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -187,7 +187,5 @@\n-  static address _base64_charset;\n-  static address _bswap_mask;\n-  static address _gather_mask;\n-  static address _right_shift_mask;\n-  static address _left_shift_mask;\n-  static address _and_mask;\n-  static address _url_charset;\n+  static address _encoding_table_base64;\n+  static address _shuffle_base64;\n+  static address _avx2_shuffle_base64;\n+  static address _avx2_input_mask_base64;\n+  static address _avx2_lut_base64;\n@@ -340,7 +338,5 @@\n-  static address base64_charset_addr() { return _base64_charset; }\n-  static address base64url_charset_addr() { return _url_charset; }\n-  static address base64_bswap_mask_addr() { return _bswap_mask; }\n-  static address base64_gather_mask_addr() { return _gather_mask; }\n-  static address base64_right_shift_mask_addr() { return _right_shift_mask; }\n-  static address base64_left_shift_mask_addr() { return _left_shift_mask; }\n-  static address base64_and_mask_addr() { return _and_mask; }\n+  static address base64_encoding_table_addr() { return _encoding_table_base64; }\n+  static address base64_shuffle_addr() { return _shuffle_base64; }\n+  static address base64_avx2_shuffle_addr() { return _avx2_shuffle_base64; }\n+  static address base64_avx2_input_mask_addr() { return _avx2_input_mask_base64; }\n+  static address base64_avx2_lut_addr() { return _avx2_lut_base64; }\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":10,"deletions":14,"binary":false,"changes":24,"status":"modified"}]}