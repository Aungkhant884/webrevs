{"files":[{"patch":"@@ -5029,1 +5029,1 @@\n-    fill64_avx(base, 0, xtmp, use64byteVector);\n+    fill64(base, 0, xtmp, use64byteVector);\n@@ -5046,1 +5046,1 @@\n-    fill64_masked_avx(3, base, 0, xtmp, mask, cnt, rtmp, true);\n+    fill64_masked(3, base, 0, xtmp, mask, cnt, rtmp, true);\n@@ -5065,1 +5065,1 @@\n-    fill32_masked_avx(3, base, 0, xtmp, mask, cnt, rtmp);\n+    fill32_masked(3, base, 0, xtmp, mask, cnt, rtmp);\n@@ -5089,1 +5089,1 @@\n-    fill64_avx(base, i * 64, xtmp, use64byteVector);\n+    fill64(base, i * 64, xtmp, use64byteVector);\n@@ -5210,4 +5210,2 @@\n-#ifdef COMPILER2\n-#ifdef _LP64\n-  if(UseAVX > 2 &&\n-     MaxVectorSize >=32 &&\n+#if defined(COMPILER2) && defined(_LP64)\n+  if(MaxVectorSize >=32 &&\n@@ -5219,1 +5217,0 @@\n-#endif\n@@ -8275,0 +8272,8 @@\n+void MacroAssembler::fill_masked(BasicType bt, Address dst, XMMRegister xmm, KRegister mask,\n+                                 Register length, Register temp, int vec_enc) {\n+  \/\/ Computing mask for predicated vector store.\n+  movptr(temp, -1);\n+  bzhiq(temp, temp, length);\n+  kmov(mask, temp);\n+  evmovdqu(bt, mask, dst, xmm, vec_enc);\n+}\n@@ -8277,1 +8282,1 @@\n-void MacroAssembler::fill64_masked_avx(uint shift, Register dst, int disp,\n+void MacroAssembler::fill64_masked(uint shift, Register dst, int disp,\n@@ -8283,1 +8288,1 @@\n-    fill32_avx(dst, disp, xmm);\n+    fill32(dst, disp, xmm);\n@@ -8285,1 +8290,1 @@\n-    fill32_masked_avx(shift, dst, disp + 32, xmm, mask, length, temp);\n+    fill32_masked(shift, dst, disp + 32, xmm, mask, length, temp);\n@@ -8288,4 +8293,1 @@\n-    LP64_ONLY(mov64(temp, -1L)) NOT_LP64(movl(temp, -1));\n-    bzhiq(temp, temp, length);\n-    kmov(mask, temp);\n-    evmovdqu(type[shift], mask, Address(dst, disp), xmm, Assembler::AVX_512bit);\n+    fill_masked(type[shift], Address(dst, disp), xmm, mask, length, temp, Assembler::AVX_512bit);\n@@ -8296,1 +8298,1 @@\n-void MacroAssembler::fill32_masked_avx(uint shift, Register dst, int disp,\n+void MacroAssembler::fill32_masked(uint shift, Register dst, int disp,\n@@ -8301,4 +8303,1 @@\n-  LP64_ONLY(mov64(temp, -1L)) NOT_LP64(movl(temp, -1));\n-  bzhiq(temp, temp, length);\n-  kmov(mask, temp);\n-  evmovdqu(type[shift], mask, Address(dst, disp), xmm, Assembler::AVX_256bit);\n+  fill_masked(type[shift], Address(dst, disp), xmm, mask, length, temp, Assembler::AVX_256bit);\n@@ -8308,1 +8307,1 @@\n-void MacroAssembler::fill32_avx(Register dst, int disp, XMMRegister xmm) {\n+void MacroAssembler::fill32(Register dst, int disp, XMMRegister xmm) {\n@@ -8313,1 +8312,1 @@\n-void MacroAssembler::fill64_avx(Register dst, int disp, XMMRegister xmm, bool use64byteVector) {\n+void MacroAssembler::fill64(Register dst, int disp, XMMRegister xmm, bool use64byteVector) {\n@@ -8317,2 +8316,2 @@\n-    fill32_avx(dst, disp, xmm);\n-    fill32_avx(dst, disp + 32, xmm);\n+    fill32(dst, disp, xmm);\n+    fill32(dst, disp + 32, xmm);\n@@ -8367,1 +8366,1 @@\n-    fill32_masked_avx(shift, to, 0, xtmp, k2, count, rtmp);\n+    fill32_masked(shift, to, 0, xtmp, k2, count, rtmp);\n@@ -8373,1 +8372,1 @@\n-    fill64_masked_avx(shift, to, 0, xtmp, k2, count, rtmp);\n+    fill64_masked(shift, to, 0, xtmp, k2, count, rtmp);\n@@ -8379,1 +8378,1 @@\n-    fill64_avx(to, 0, xtmp);\n+    fill64(to, 0, xtmp);\n@@ -8381,1 +8380,1 @@\n-    fill32_masked_avx(shift, to, 64, xtmp, k2, count, rtmp);\n+    fill32_masked(shift, to, 64, xtmp, k2, count, rtmp);\n@@ -8387,2 +8386,2 @@\n-    fill64_avx(to, 0, xtmp);\n-    fill32_avx(to, 64, xtmp);\n+    fill64(to, 0, xtmp);\n+    fill32(to, 64, xtmp);\n@@ -8390,1 +8389,1 @@\n-    fill32_masked_avx(shift, to, 96, xtmp, k2, count, rtmp);\n+    fill32_masked(shift, to, 96, xtmp, k2, count, rtmp);\n@@ -8417,2 +8416,2 @@\n-      fill64_avx(to, 0, xtmp);\n-      fill64_avx(to, 64, xtmp);\n+      fill64(to, 0, xtmp);\n+      fill64(to, 64, xtmp);\n@@ -8443,1 +8442,1 @@\n-    fill64_masked_avx(shift, to, 0, xtmp, k2, count, rtmp, true);\n+    fill64_masked(shift, to, 0, xtmp, k2, count, rtmp, true);\n@@ -8449,1 +8448,1 @@\n-    fill64_avx(to, 0, xtmp, true);\n+    fill64(to, 0, xtmp, true);\n@@ -8451,1 +8450,1 @@\n-    fill64_masked_avx(shift, to, 64, xtmp, k2, count, rtmp, true);\n+    fill64_masked(shift, to, 64, xtmp, k2, count, rtmp, true);\n@@ -8457,2 +8456,2 @@\n-    fill64_avx(to, 0, xtmp, true);\n-    fill64_avx(to, 64, xtmp, true);\n+    fill64(to, 0, xtmp, true);\n+    fill64(to, 64, xtmp, true);\n@@ -8460,1 +8459,1 @@\n-    fill64_masked_avx(shift, to, 128, xtmp, k2, count, rtmp, true);\n+    fill64_masked(shift, to, 128, xtmp, k2, count, rtmp, true);\n@@ -8487,3 +8486,3 @@\n-      fill64_avx(to, 0, xtmp, true);\n-      fill64_avx(to, 64, xtmp, true);\n-      fill64_avx(to, 128, xtmp, true);\n+      fill64(to, 0, xtmp, true);\n+      fill64(to, 64, xtmp, true);\n+      fill64(to, 128, xtmp, true);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":42,"deletions":43,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -1842,1 +1842,4 @@\n-  void fill64_masked_avx(uint shift, Register dst, int disp,\n+  void fill_masked(BasicType bt, Address dst, XMMRegister xmm, KRegister mask,\n+                   Register length, Register temp, int vec_enc);\n+\n+  void fill64_masked(uint shift, Register dst, int disp,\n@@ -1846,1 +1849,1 @@\n-  void fill32_masked_avx(uint shift, Register dst, int disp,\n+  void fill32_masked(uint shift, Register dst, int disp,\n@@ -1850,1 +1853,1 @@\n-  void fill32_avx(Register dst, int disp, XMMRegister xmm);\n+  void fill32(Register dst, int disp, XMMRegister xmm);\n@@ -1852,1 +1855,1 @@\n-  void fill64_avx(Register dst, int dis, XMMRegister xmm, bool use64byteVector = false);\n+  void fill64(Register dst, int dis, XMMRegister xmm, bool use64byteVector = false);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"}]}