{"files":[{"patch":"@@ -5210,0 +5210,12 @@\n+#ifdef COMPILER2\n+#ifdef _LP64\n+  if(UseAVX > 2 &&\n+     MaxVectorSize >=32 &&\n+     VM_Version::supports_avx512vlbw() &&\n+     VM_Version::supports_bmi2()) {\n+    generate_fill_avx3(t, to, value, count, rtmp, xtmp);\n+    return;\n+  }\n+#endif\n+#endif\n+\n@@ -5430,0 +5442,24 @@\n+void MacroAssembler::evpbroadcast(BasicType type, XMMRegister dst, Register src, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+    case T_BOOLEAN:\n+      evpbroadcastb(dst, src, vector_len);\n+      break;\n+    case T_SHORT:\n+    case T_CHAR:\n+      evpbroadcastw(dst, src, vector_len);\n+      break;\n+    case T_INT:\n+    case T_FLOAT:\n+      evpbroadcastd(dst, src, vector_len);\n+      break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      evpbroadcastq(dst, src, vector_len);\n+      break;\n+    default:\n+      fatal(\"Unhandled type : %s\", type2name(type));\n+      break;\n+  }\n+}\n+\n@@ -8245,2 +8281,1 @@\n-  assert(shift != 0, \"shift value should be 1 (short),2(int) or 3(long)\");\n-  BasicType type[] = { T_BYTE, T_SHORT,  T_INT,   T_LONG};\n+  BasicType type[] = { T_BYTE, T_SHORT, T_INT, T_LONG};\n@@ -8253,4 +8288,3 @@\n-    movl(temp, 1);\n-    shlxl(temp, temp, length);\n-    subptr(temp, 1);\n-    kmovwl(mask, temp);\n+    LP64_ONLY(mov64(temp, -1L)) NOT_LP64(movl(temp, -1));\n+    bzhiq(temp, temp, length);\n+    kmov(mask, temp);\n@@ -8266,6 +8300,4 @@\n-  assert(shift != 0, \"shift value should be 1 (short), 2(int) or 3(long)\");\n-  BasicType type[] = { T_BYTE, T_SHORT,  T_INT,   T_LONG};\n-  movl(temp, 1);\n-  shlxl(temp, temp, length);\n-  subptr(temp, 1);\n-  kmovwl(mask, temp);\n+  BasicType type[] = { T_BYTE, T_SHORT, T_INT, T_LONG};\n+  LP64_ONLY(mov64(temp, -1L)) NOT_LP64(movl(temp, -1));\n+  bzhiq(temp, temp, length);\n+  kmov(mask, temp);\n@@ -8292,0 +8324,177 @@\n+#ifdef _LP64\n+void MacroAssembler::generate_fill_avx3(BasicType type, Register to, Register value,\n+                                        Register count, Register rtmp, XMMRegister xtmp) {\n+  Label L_exit;\n+  Label L_fill_start;\n+  Label L_fill_64_bytes;\n+  Label L_fill_96_bytes;\n+  Label L_fill_128_bytes;\n+  Label L_fill_128_bytes_loop;\n+  Label L_fill_128_loop_header;\n+  Label L_fill_128_bytes_loop_header;\n+  Label L_fill_128_bytes_loop_pre_header;\n+  Label L_fill_zmm_sequence;\n+\n+  int shift = -1;\n+  switch(type) {\n+    case T_BYTE:  shift = 0;\n+      break;\n+    case T_SHORT: shift = 1;\n+      break;\n+    case T_INT:   shift = 2;\n+      break;\n+    \/* Uncomment when LONG fill stubs are supported.\n+    case T_LONG:  shift = 3;\n+      break;\n+    *\/\n+    default:\n+      fatal(\"Unhandled type: %s\\n\", type2name(type));\n+  }\n+\n+  if (AVX3Threshold != 0  || MaxVectorSize == 32) {\n+\n+    if (MaxVectorSize == 64) {\n+      cmpq(count, AVX3Threshold >> shift);\n+      jcc(Assembler::greater, L_fill_zmm_sequence);\n+    }\n+\n+    evpbroadcast(type, xtmp, value, Assembler::AVX_256bit);\n+\n+    bind(L_fill_start);\n+\n+    cmpq(count, 32 >> shift);\n+    jccb(Assembler::greater, L_fill_64_bytes);\n+    fill32_masked_avx(shift, to, 0, xtmp, k2, count, rtmp);\n+    jmp(L_exit);\n+\n+    bind(L_fill_64_bytes);\n+    cmpq(count, 64 >> shift);\n+    jccb(Assembler::greater, L_fill_96_bytes);\n+    fill64_masked_avx(shift, to, 0, xtmp, k2, count, rtmp);\n+    jmp(L_exit);\n+\n+    bind(L_fill_96_bytes);\n+    cmpq(count, 96 >> shift);\n+    jccb(Assembler::greater, L_fill_128_bytes);\n+    fill64_avx(to, 0, xtmp);\n+    subq(count, 64 >> shift);\n+    fill32_masked_avx(shift, to, 64, xtmp, k2, count, rtmp);\n+    jmp(L_exit);\n+\n+    bind(L_fill_128_bytes);\n+    cmpq(count, 128 >> shift);\n+    jccb(Assembler::greater, L_fill_128_bytes_loop_pre_header);\n+    fill64_avx(to, 0, xtmp);\n+    fill32_avx(to, 64, xtmp);\n+    subq(count, 96 >> shift);\n+    fill32_masked_avx(shift, to, 96, xtmp, k2, count, rtmp);\n+    jmp(L_exit);\n+\n+    bind(L_fill_128_bytes_loop_pre_header);\n+    {\n+      mov(rtmp, to);\n+      andq(rtmp, 31);\n+      jccb(Assembler::zero, L_fill_128_bytes_loop_header);\n+      negq(rtmp);\n+      addq(rtmp, 32);\n+      mov64(r8, -1L);\n+      bzhiq(r8, r8, rtmp);\n+      kmovql(k2, r8);\n+      evmovdqu(T_BYTE, k2, Address(to, 0), xtmp, Assembler::AVX_256bit);\n+      addq(to, rtmp);\n+      shrq(rtmp, shift);\n+      subq(count, rtmp);\n+    }\n+\n+    cmpq(count, 128 >> shift);\n+    jcc(Assembler::less, L_fill_start);\n+\n+    bind(L_fill_128_bytes_loop_header);\n+    subq(count, 128 >> shift);\n+\n+    align32();\n+    bind(L_fill_128_bytes_loop);\n+      fill64_avx(to, 0, xtmp);\n+      fill64_avx(to, 64, xtmp);\n+      addq(to, 128);\n+      subq(count, 128 >> shift);\n+      jccb(Assembler::greaterEqual, L_fill_128_bytes_loop);\n+\n+    addq(count, 128 >> shift);\n+    jcc(Assembler::zero, L_exit);\n+    jmp(L_fill_start);\n+  }\n+\n+  if (MaxVectorSize == 64) {\n+    \/\/ Sequence using 64 byte ZMM register.\n+    Label L_fill_128_bytes_zmm;\n+    Label L_fill_192_bytes_zmm;\n+    Label L_fill_192_bytes_loop_zmm;\n+    Label L_fill_192_bytes_loop_header_zmm;\n+    Label L_fill_192_bytes_loop_pre_header_zmm;\n+    Label L_fill_start_zmm_sequence;\n+\n+    bind(L_fill_zmm_sequence);\n+    evpbroadcast(type, xtmp, value, Assembler::AVX_512bit);\n+\n+    bind(L_fill_start_zmm_sequence);\n+    cmpq(count, 64 >> shift);\n+    jccb(Assembler::greater, L_fill_128_bytes_zmm);\n+    fill64_masked_avx(shift, to, 0, xtmp, k2, count, rtmp, true);\n+    jmp(L_exit);\n+\n+    bind(L_fill_128_bytes_zmm);\n+    cmpq(count, 128 >> shift);\n+    jccb(Assembler::greater, L_fill_192_bytes_zmm);\n+    fill64_avx(to, 0, xtmp, true);\n+    subq(count, 64 >> shift);\n+    fill64_masked_avx(shift, to, 64, xtmp, k2, count, rtmp, true);\n+    jmp(L_exit);\n+\n+    bind(L_fill_192_bytes_zmm);\n+    cmpq(count, 192 >> shift);\n+    jccb(Assembler::greater, L_fill_192_bytes_loop_pre_header_zmm);\n+    fill64_avx(to, 0, xtmp, true);\n+    fill64_avx(to, 64, xtmp, true);\n+    subq(count, 128 >> shift);\n+    fill64_masked_avx(shift, to, 128, xtmp, k2, count, rtmp, true);\n+    jmp(L_exit);\n+\n+    bind(L_fill_192_bytes_loop_pre_header_zmm);\n+    {\n+      movq(rtmp, to);\n+      andq(rtmp, 63);\n+      jccb(Assembler::zero, L_fill_192_bytes_loop_header_zmm);\n+      negq(rtmp);\n+      addq(rtmp, 64);\n+      mov64(r8, -1L);\n+      bzhiq(r8, r8, rtmp);\n+      kmovql(k2, r8);\n+      evmovdqu(T_BYTE, k2, Address(to, 0), xtmp, Assembler::AVX_512bit);\n+      addq(to, rtmp);\n+      shrq(rtmp, shift);\n+      subq(count, rtmp);\n+    }\n+\n+    cmpq(count, 192 >> shift);\n+    jcc(Assembler::less, L_fill_start_zmm_sequence);\n+\n+    bind(L_fill_192_bytes_loop_header_zmm);\n+    subq(count, 192 >> shift);\n+\n+    align32();\n+    bind(L_fill_192_bytes_loop_zmm);\n+      fill64_avx(to, 0, xtmp, true);\n+      fill64_avx(to, 64, xtmp, true);\n+      fill64_avx(to, 128, xtmp, true);\n+      addq(to, 192);\n+      subq(count, 192 >> shift);\n+      jccb(Assembler::greaterEqual, L_fill_192_bytes_loop_zmm);\n+\n+    addq(count, 192 >> shift);\n+    jcc(Assembler::zero, L_exit);\n+    jmp(L_fill_start_zmm_sequence);\n+  }\n+  bind(L_exit);\n+}\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":221,"deletions":12,"binary":false,"changes":233,"status":"modified"},{"patch":"@@ -1308,0 +1308,1 @@\n+  void evpbroadcast(BasicType type, XMMRegister dst, Register src, int vector_len);\n@@ -1888,0 +1889,4 @@\n+\n+  void generate_fill_avx3(BasicType type, Register to, Register value,\n+                          Register count, Register rtmp, XMMRegister xtmp);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2116,1 +2116,1 @@\n-    const Register to       = c_rarg0;  \/\/ source array address\n+    const Register to       = c_rarg0;  \/\/ destination array address\n@@ -2119,0 +2119,1 @@\n+    __ mov(r11, count);\n@@ -2122,1 +2123,1 @@\n-    __ generate_fill(t, aligned, to, value, count, rax, xmm0);\n+    __ generate_fill(t, aligned, to, value, r11, rax, xmm0);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-  code_size2 = 35300 LP64_ONLY(+32000)          \/\/ simply increase if too small (assembler will crash if too small)\n+  code_size2 = 35300 LP64_ONLY(+35000)          \/\/ simply increase if too small (assembler will crash if too small)\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1472,0 +1472,8 @@\n+#ifdef COMPILER2\n+  if (FLAG_IS_DEFAULT(OptimizeFill)) {\n+    if (MaxVectorSize < 32 || !VM_Version::supports_avx512vlbw()) {\n+      OptimizeFill = false;\n+    }\n+  }\n+#endif\n+\n@@ -1588,6 +1596,0 @@\n-  if (FLAG_IS_DEFAULT(OptimizeFill)) {\n-    \/\/ 8247307: On x86, the auto-vectorized loop array fill code shows\n-    \/\/ better performance than the array fill stubs. We should reenable\n-    \/\/ this after the x86 stubs get improved.\n-    OptimizeFill = false;\n-  }\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,2 +39,2 @@\n-@BenchmarkMode(Mode.AverageTime)\n-@OutputTimeUnit(TimeUnit.NANOSECONDS)\n+@BenchmarkMode(Mode.Throughput)\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n@@ -44,1 +44,1 @@\n-    @Param({\"10\", \"266\", \"2048\"})\n+    @Param({\"10\", \"16\", \"31\", \"59\", \"89\", \"126\", \"250\", \"266\", \"511\", \"1021\", \"2047\", \"2048\", \"4095\", \"8195\"})\n","filename":"test\/micro\/org\/openjdk\/bench\/java\/util\/ArraysFill.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"}]}