{"files":[{"patch":"@@ -396,1 +396,78 @@\n-  scan_and_forward(this, cp);\n+  \/\/ Compute the new addresses for the live objects and store it in the mark\n+  \/\/ Used by universe::mark_sweep_phase2()\n+\n+  \/\/ We're sure to be here before any objects are compacted into this\n+  \/\/ space, so this is a good time to initialize this:\n+  set_compaction_top(bottom());\n+\n+  if (cp->space == NULL) {\n+    assert(cp->gen != NULL, \"need a generation\");\n+    assert(cp->threshold == NULL, \"just checking\");\n+    assert(cp->gen->first_compaction_space() == this, \"just checking\");\n+    cp->space = cp->gen->first_compaction_space();\n+    cp->threshold = cp->space->initialize_threshold();\n+    cp->space->set_compaction_top(cp->space->bottom());\n+  }\n+\n+  HeapWord* compact_top = cp->space->compaction_top(); \/\/ This is where we are currently compacting to.\n+\n+  DeadSpacer dead_spacer(this);\n+\n+  HeapWord*  end_of_live = bottom();  \/\/ One byte beyond the last byte of the last live object.\n+  HeapWord*  first_dead = NULL; \/\/ The first dead object.\n+\n+  const intx interval = PrefetchScanIntervalInBytes;\n+\n+  HeapWord* cur_obj = bottom();\n+  HeapWord* scan_limit = top();\n+\n+  while (cur_obj < scan_limit) {\n+    if (cast_to_oop(cur_obj)->is_gc_marked()) {\n+      \/\/ prefetch beyond cur_obj\n+      Prefetch::write(cur_obj, interval);\n+      size_t size = cast_to_oop(cur_obj)->size();\n+      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top);\n+      cur_obj += size;\n+      end_of_live = cur_obj;\n+    } else {\n+      \/\/ run over all the contiguous dead objects\n+      HeapWord* end = cur_obj;\n+      do {\n+        \/\/ prefetch beyond end\n+        Prefetch::write(end, interval);\n+        end += cast_to_oop(end)->size();\n+      } while (end < scan_limit && !cast_to_oop(end)->is_gc_marked());\n+\n+      \/\/ see if we might want to pretend this object is alive so that\n+      \/\/ we don't have to compact quite as often.\n+      if (cur_obj == compact_top && dead_spacer.insert_deadspace(cur_obj, end)) {\n+        oop obj = cast_to_oop(cur_obj);\n+        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top);\n+        end_of_live = end;\n+      } else {\n+        \/\/ otherwise, it really is a free region.\n+\n+        \/\/ cur_obj is a pointer to a dead object. Use this dead memory to store a pointer to the next live object.\n+        *(HeapWord**)cur_obj = end;\n+\n+        \/\/ see if this is the first dead region.\n+        if (first_dead == NULL) {\n+          first_dead = cur_obj;\n+        }\n+      }\n+\n+      \/\/ move on to the next object\n+      cur_obj = end;\n+    }\n+  }\n+\n+  assert(cur_obj == scan_limit, \"just checking\");\n+  _end_of_live = end_of_live;\n+  if (first_dead != NULL) {\n+    _first_dead = first_dead;\n+  } else {\n+    _first_dead = end_of_live;\n+  }\n+\n+  \/\/ save the compaction_top of the compaction space.\n+  cp->space->set_compaction_top(compact_top);\n@@ -405,1 +482,29 @@\n-  scan_and_adjust_pointers(this);\n+  \/\/ adjust all the interior pointers to point at the new locations of objects\n+  \/\/ Used by MarkSweep::mark_sweep_phase3()\n+\n+  HeapWord* cur_obj = bottom();\n+  HeapWord* const end_of_live = _end_of_live;  \/\/ Established by prepare_for_compaction().\n+  HeapWord* const first_dead = _first_dead;    \/\/ Established by prepare_for_compaction().\n+\n+  assert(first_dead <= end_of_live, \"Stands to reason, no?\");\n+\n+  const intx interval = PrefetchScanIntervalInBytes;\n+\n+  debug_only(HeapWord* prev_obj = NULL);\n+  while (cur_obj < end_of_live) {\n+    Prefetch::write(cur_obj, interval);\n+    if (cur_obj < first_dead || cast_to_oop(cur_obj)->is_gc_marked()) {\n+      \/\/ cur_obj is alive\n+      \/\/ point all the oops to the new location\n+      size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_obj));\n+      debug_only(prev_obj = cur_obj);\n+      cur_obj += size;\n+    } else {\n+      debug_only(prev_obj = cur_obj);\n+      \/\/ cur_obj is not a live object, instead it points at the next live object\n+      cur_obj = *(HeapWord**)cur_obj;\n+      assert(cur_obj > prev_obj, \"we should be moving forward through memory, cur_obj: \" PTR_FORMAT \", prev_obj: \" PTR_FORMAT, p2i(cur_obj), p2i(prev_obj));\n+    }\n+  }\n+\n+  assert(cur_obj == end_of_live, \"just checking\");\n@@ -409,1 +514,56 @@\n-  scan_and_compact(this);\n+  \/\/ Copy all live objects to their new location\n+  \/\/ Used by MarkSweep::mark_sweep_phase4()\n+\n+  verify_up_to_first_dead(this);\n+\n+  HeapWord* const start = bottom();\n+  HeapWord* const end_of_live = _end_of_live;\n+\n+  assert(_first_dead <= end_of_live, \"Invariant. _first_dead: \" PTR_FORMAT \" <= end_of_live: \" PTR_FORMAT, p2i(_first_dead), p2i(end_of_live));\n+  if (_first_dead == end_of_live && (start == end_of_live || !cast_to_oop(start)->is_gc_marked())) {\n+    \/\/ Nothing to compact. The space is either empty or all live object should be left in place.\n+    clear_empty_region(this);\n+    return;\n+  }\n+\n+  const intx scan_interval = PrefetchScanIntervalInBytes;\n+  const intx copy_interval = PrefetchCopyIntervalInBytes;\n+\n+  assert(start < end_of_live, \"bottom: \" PTR_FORMAT \" should be < end_of_live: \" PTR_FORMAT, p2i(start), p2i(end_of_live));\n+  HeapWord* cur_obj = start;\n+  if (_first_dead > cur_obj && !cast_to_oop(cur_obj)->is_gc_marked()) {\n+    \/\/ All object before _first_dead can be skipped. They should not be moved.\n+    \/\/ A pointer to the first live object is stored at the memory location for _first_dead.\n+    cur_obj = *(HeapWord**)(_first_dead);\n+  }\n+\n+  debug_only(HeapWord* prev_obj = NULL);\n+  while (cur_obj < end_of_live) {\n+    if (!cast_to_oop(cur_obj)->is_gc_marked()) {\n+      debug_only(prev_obj = cur_obj);\n+      \/\/ The first word of the dead object contains a pointer to the next live object or end of space.\n+      cur_obj = *(HeapWord**)cur_obj;\n+      assert(cur_obj > prev_obj, \"we should be moving forward through memory\");\n+    } else {\n+      \/\/ prefetch beyond q\n+      Prefetch::read(cur_obj, scan_interval);\n+\n+      \/\/ size and destination\n+      size_t size = cast_to_oop(cur_obj)->size();\n+      HeapWord* compaction_top = cast_from_oop<HeapWord*>(cast_to_oop(cur_obj)->forwardee());\n+\n+      \/\/ prefetch beyond compaction_top\n+      Prefetch::write(compaction_top, copy_interval);\n+\n+      \/\/ copy object and reinit its mark\n+      assert(cur_obj != compaction_top, \"everything in this pass should be moving\");\n+      Copy::aligned_conjoint_words(cur_obj, compaction_top, size);\n+      cast_to_oop(compaction_top)->init_mark();\n+      assert(cast_to_oop(compaction_top)->klass() != NULL, \"should have a class\");\n+\n+      debug_only(prev_obj = cur_obj);\n+      cur_obj += size;\n+    }\n+  }\n+\n+  clear_empty_region(this);\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":163,"deletions":3,"binary":false,"changes":166,"status":"modified"},{"patch":"@@ -315,5 +315,0 @@\n-\/\/ - scan_limit()\n-\/\/ - scanned_block_is_obj()\n-\/\/ - scanned_block_size()\n-\/\/ - adjust_obj_size()\n-\/\/ - obj_size()\n@@ -329,5 +324,0 @@\n-\/\/ Overrides\/definitions of\n-\/\/  - scan_limit\n-\/\/  - scanned_block_is_obj\n-\/\/  - scanned_block_size\n-\/\/ require override\/definition of prepare_for_compaction().\n@@ -335,6 +325,1 @@\n-\/\/  - adjust_obj_size  and adjust_pointers()\n-\/\/  - obj_size         and compact().\n-\/\/\n-\/\/ Additionally, this also means that changes to block_size() or block_is_obj() that\n-\/\/ should be effective during the compaction operations must provide a corresponding\n-\/\/ definition of scanned_block_size\/scanned_block_is_obj respectively.\n+\n@@ -347,7 +332,0 @@\n-  \/\/ Auxiliary functions for scan_and_{forward,adjust_pointers,compact} support.\n-  inline size_t adjust_obj_size(size_t size) const {\n-    return size;\n-  }\n-\n-  inline size_t obj_size(const HeapWord* addr) const;\n-\n@@ -454,21 +432,0 @@\n-\n-  \/\/ Below are template functions for scan_and_* algorithms (avoiding virtual calls).\n-  \/\/ The space argument should be a subclass of CompactibleSpace, implementing\n-  \/\/ scan_limit(), scanned_block_is_obj(), and scanned_block_size(),\n-  \/\/ and possibly also overriding obj_size(), and adjust_obj_size().\n-  \/\/ These functions should avoid virtual calls whenever possible.\n-\n-#if INCLUDE_SERIALGC\n-  \/\/ Frequently calls adjust_obj_size().\n-  template <class SpaceType>\n-  static inline void scan_and_adjust_pointers(SpaceType* space);\n-#endif\n-\n-  \/\/ Frequently calls obj_size().\n-  template <class SpaceType>\n-  static inline void scan_and_compact(SpaceType* space);\n-\n-  \/\/ Frequently calls scanned_block_is_obj() and scanned_block_size().\n-  \/\/ Requires the scan_limit() function.\n-  template <class SpaceType>\n-  static inline void scan_and_forward(SpaceType* space, CompactPoint* cp);\n@@ -483,16 +440,0 @@\n-  \/\/ Allow scan_and_forward function to call (private) overrides for auxiliary functions on this class\n-  template <typename SpaceType>\n-  friend void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp);\n-\n- private:\n-  \/\/ Auxiliary functions for scan_and_forward support.\n-  \/\/ See comments for CompactibleSpace for more information.\n-  inline HeapWord* scan_limit() const {\n-    return top();\n-  }\n-\n-  inline bool scanned_block_is_obj(const HeapWord* addr) const {\n-    return true; \/\/ Always true, since scan_limit is top\n-  }\n-\n-  inline size_t scanned_block_size(const HeapWord* addr) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":1,"deletions":60,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -79,4 +79,0 @@\n-size_t CompactibleSpace::obj_size(const HeapWord* addr) const {\n-  return cast_to_oop(addr)->size();\n-}\n-\n@@ -136,116 +132,0 @@\n-template <class SpaceType>\n-inline void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp) {\n-  \/\/ Compute the new addresses for the live objects and store it in the mark\n-  \/\/ Used by universe::mark_sweep_phase2()\n-\n-  \/\/ We're sure to be here before any objects are compacted into this\n-  \/\/ space, so this is a good time to initialize this:\n-  space->set_compaction_top(space->bottom());\n-\n-  if (cp->space == NULL) {\n-    assert(cp->gen != NULL, \"need a generation\");\n-    assert(cp->threshold == NULL, \"just checking\");\n-    assert(cp->gen->first_compaction_space() == space, \"just checking\");\n-    cp->space = cp->gen->first_compaction_space();\n-    cp->threshold = cp->space->initialize_threshold();\n-    cp->space->set_compaction_top(cp->space->bottom());\n-  }\n-\n-  HeapWord* compact_top = cp->space->compaction_top(); \/\/ This is where we are currently compacting to.\n-\n-  DeadSpacer dead_spacer(space);\n-\n-  HeapWord*  end_of_live = space->bottom();  \/\/ One byte beyond the last byte of the last live object.\n-  HeapWord*  first_dead = NULL; \/\/ The first dead object.\n-\n-  const intx interval = PrefetchScanIntervalInBytes;\n-\n-  HeapWord* cur_obj = space->bottom();\n-  HeapWord* scan_limit = space->scan_limit();\n-\n-  while (cur_obj < scan_limit) {\n-    if (space->scanned_block_is_obj(cur_obj) && cast_to_oop(cur_obj)->is_gc_marked()) {\n-      \/\/ prefetch beyond cur_obj\n-      Prefetch::write(cur_obj, interval);\n-      size_t size = space->scanned_block_size(cur_obj);\n-      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top);\n-      cur_obj += size;\n-      end_of_live = cur_obj;\n-    } else {\n-      \/\/ run over all the contiguous dead objects\n-      HeapWord* end = cur_obj;\n-      do {\n-        \/\/ prefetch beyond end\n-        Prefetch::write(end, interval);\n-        end += space->scanned_block_size(end);\n-      } while (end < scan_limit && (!space->scanned_block_is_obj(end) || !cast_to_oop(end)->is_gc_marked()));\n-\n-      \/\/ see if we might want to pretend this object is alive so that\n-      \/\/ we don't have to compact quite as often.\n-      if (cur_obj == compact_top && dead_spacer.insert_deadspace(cur_obj, end)) {\n-        oop obj = cast_to_oop(cur_obj);\n-        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top);\n-        end_of_live = end;\n-      } else {\n-        \/\/ otherwise, it really is a free region.\n-\n-        \/\/ cur_obj is a pointer to a dead object. Use this dead memory to store a pointer to the next live object.\n-        *(HeapWord**)cur_obj = end;\n-\n-        \/\/ see if this is the first dead region.\n-        if (first_dead == NULL) {\n-          first_dead = cur_obj;\n-        }\n-      }\n-\n-      \/\/ move on to the next object\n-      cur_obj = end;\n-    }\n-  }\n-\n-  assert(cur_obj == scan_limit, \"just checking\");\n-  space->_end_of_live = end_of_live;\n-  if (first_dead != NULL) {\n-    space->_first_dead = first_dead;\n-  } else {\n-    space->_first_dead = end_of_live;\n-  }\n-\n-  \/\/ save the compaction_top of the compaction space.\n-  cp->space->set_compaction_top(compact_top);\n-}\n-\n-template <class SpaceType>\n-inline void CompactibleSpace::scan_and_adjust_pointers(SpaceType* space) {\n-  \/\/ adjust all the interior pointers to point at the new locations of objects\n-  \/\/ Used by MarkSweep::mark_sweep_phase3()\n-\n-  HeapWord* cur_obj = space->bottom();\n-  HeapWord* const end_of_live = space->_end_of_live;  \/\/ Established by \"scan_and_forward\".\n-  HeapWord* const first_dead = space->_first_dead;    \/\/ Established by \"scan_and_forward\".\n-\n-  assert(first_dead <= end_of_live, \"Stands to reason, no?\");\n-\n-  const intx interval = PrefetchScanIntervalInBytes;\n-\n-  debug_only(HeapWord* prev_obj = NULL);\n-  while (cur_obj < end_of_live) {\n-    Prefetch::write(cur_obj, interval);\n-    if (cur_obj < first_dead || cast_to_oop(cur_obj)->is_gc_marked()) {\n-      \/\/ cur_obj is alive\n-      \/\/ point all the oops to the new location\n-      size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_obj));\n-      size = space->adjust_obj_size(size);\n-      debug_only(prev_obj = cur_obj);\n-      cur_obj += size;\n-    } else {\n-      debug_only(prev_obj = cur_obj);\n-      \/\/ cur_obj is not a live object, instead it points at the next live object\n-      cur_obj = *(HeapWord**)cur_obj;\n-      assert(cur_obj > prev_obj, \"we should be moving forward through memory, cur_obj: \" PTR_FORMAT \", prev_obj: \" PTR_FORMAT, p2i(cur_obj), p2i(prev_obj));\n-    }\n-  }\n-\n-  assert(cur_obj == end_of_live, \"just checking\");\n-}\n-\n@@ -264,1 +144,1 @@\n-       size_t size = space->obj_size(cur_obj);\n+       size_t size = cast_to_oop(cur_obj)->size();\n@@ -290,61 +170,0 @@\n-\n-template <class SpaceType>\n-inline void CompactibleSpace::scan_and_compact(SpaceType* space) {\n-  \/\/ Copy all live objects to their new location\n-  \/\/ Used by MarkSweep::mark_sweep_phase4()\n-\n-  verify_up_to_first_dead(space);\n-\n-  HeapWord* const bottom = space->bottom();\n-  HeapWord* const end_of_live = space->_end_of_live;\n-\n-  assert(space->_first_dead <= end_of_live, \"Invariant. _first_dead: \" PTR_FORMAT \" <= end_of_live: \" PTR_FORMAT, p2i(space->_first_dead), p2i(end_of_live));\n-  if (space->_first_dead == end_of_live && (bottom == end_of_live || !cast_to_oop(bottom)->is_gc_marked())) {\n-    \/\/ Nothing to compact. The space is either empty or all live object should be left in place.\n-    clear_empty_region(space);\n-    return;\n-  }\n-\n-  const intx scan_interval = PrefetchScanIntervalInBytes;\n-  const intx copy_interval = PrefetchCopyIntervalInBytes;\n-\n-  assert(bottom < end_of_live, \"bottom: \" PTR_FORMAT \" should be < end_of_live: \" PTR_FORMAT, p2i(bottom), p2i(end_of_live));\n-  HeapWord* cur_obj = bottom;\n-  if (space->_first_dead > cur_obj && !cast_to_oop(cur_obj)->is_gc_marked()) {\n-    \/\/ All object before _first_dead can be skipped. They should not be moved.\n-    \/\/ A pointer to the first live object is stored at the memory location for _first_dead.\n-    cur_obj = *(HeapWord**)(space->_first_dead);\n-  }\n-\n-  debug_only(HeapWord* prev_obj = NULL);\n-  while (cur_obj < end_of_live) {\n-    if (!cast_to_oop(cur_obj)->is_gc_marked()) {\n-      debug_only(prev_obj = cur_obj);\n-      \/\/ The first word of the dead object contains a pointer to the next live object or end of space.\n-      cur_obj = *(HeapWord**)cur_obj;\n-      assert(cur_obj > prev_obj, \"we should be moving forward through memory\");\n-    } else {\n-      \/\/ prefetch beyond q\n-      Prefetch::read(cur_obj, scan_interval);\n-\n-      \/\/ size and destination\n-      size_t size = space->obj_size(cur_obj);\n-      HeapWord* compaction_top = cast_from_oop<HeapWord*>(cast_to_oop(cur_obj)->forwardee());\n-\n-      \/\/ prefetch beyond compaction_top\n-      Prefetch::write(compaction_top, copy_interval);\n-\n-      \/\/ copy object and reinit its mark\n-      assert(cur_obj != compaction_top, \"everything in this pass should be moving\");\n-      Copy::aligned_conjoint_words(cur_obj, compaction_top, size);\n-      cast_to_oop(compaction_top)->init_mark();\n-      assert(cast_to_oop(compaction_top)->klass() != NULL, \"should have a class\");\n-\n-      debug_only(prev_obj = cur_obj);\n-      cur_obj += size;\n-    }\n-  }\n-\n-  clear_empty_region(space);\n-}\n-\n@@ -353,4 +172,0 @@\n-size_t ContiguousSpace::scanned_block_size(const HeapWord* addr) const {\n-  return cast_to_oop(addr)->size();\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/space.inline.hpp","additions":1,"deletions":186,"binary":false,"changes":187,"status":"modified"}]}