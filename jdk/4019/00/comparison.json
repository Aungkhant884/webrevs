{"files":[{"patch":"@@ -947,1 +947,6 @@\n-\n+  void gfmul_avx512(XMMRegister ghash, XMMRegister hkey);\n+  void generateHtbl_48_block_zmm(Register htbl);\n+  void ghash16_encrypt16_parallel(Register key, Register subkeyHtbl, XMMRegister ctr_blockx,\n+                                  XMMRegister aad_hashx, Register in, Register out, Register data, Register pos, bool reduction,\n+                                  XMMRegister addmask, bool no_ghash_input, Register rounds, Register ghash_pos,\n+                                  bool final_reduction, int index, Register isEncrypt, XMMRegister counter_inc_mask);\n@@ -953,0 +958,3 @@\n+  void aesgcm_encrypt(Register in, Register len, Register ct, Register out, Register key,\n+                      Register processInChunks, Register isEncrypt, Register state,\n+                      Register subkeyHtbl, Register counter);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1270,1 +1270,625 @@\n-#endif \/\/ _LP64\n+void MacroAssembler::gfmul_avx512(XMMRegister GH, XMMRegister HK) {\n+    const XMMRegister TMP1 = xmm0;\n+    const XMMRegister TMP2 = xmm1;\n+    const XMMRegister TMP3 = xmm2;\n+\n+    evpclmulqdq(TMP1, GH, HK, 0x11, Assembler::AVX_512bit);\n+    evpclmulqdq(TMP2, GH, HK, 0x00, Assembler::AVX_512bit);\n+    evpclmulqdq(TMP3, GH, HK, 0x01, Assembler::AVX_512bit);\n+    evpclmulqdq(GH, GH, HK, 0x10, Assembler::AVX_512bit);\n+    evpxorq(GH, GH, TMP3, Assembler::AVX_512bit);\n+    vpsrldq(TMP3, GH, 8, Assembler::AVX_512bit);\n+    vpslldq(GH, GH, 8, Assembler::AVX_512bit);\n+    evpxorq(TMP1, TMP1, TMP3, Assembler::AVX_512bit);\n+    evpxorq(GH, GH, TMP2, Assembler::AVX_512bit);\n+\n+    evmovdquq(TMP3, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr()), Assembler::AVX_512bit, r15);\n+    evpclmulqdq(TMP2, TMP3, GH, 0x01, Assembler::AVX_512bit);\n+    vpslldq(TMP2, TMP2, 8, Assembler::AVX_512bit);\n+    evpxorq(GH, GH, TMP2, Assembler::AVX_512bit);\n+    evpclmulqdq(TMP2, TMP3, GH, 0x00, Assembler::AVX_512bit);\n+    vpsrldq(TMP2, TMP2, 4, Assembler::AVX_512bit);\n+    evpclmulqdq(GH, TMP3, GH, 0x10, Assembler::AVX_512bit);\n+    vpslldq(GH, GH, 4, Assembler::AVX_512bit);\n+    vpternlogq(GH, 0x96, TMP1, TMP2, Assembler::AVX_512bit);\n+}\n+\n+void MacroAssembler::generateHtbl_48_block_zmm(Register htbl) {\n+    const XMMRegister HK = xmm6;\n+    const XMMRegister ZT5 = xmm4;\n+    const XMMRegister ZT7 = xmm7;\n+    const XMMRegister ZT8 = xmm8;\n+\n+    Label GFMUL_AVX512;\n+\n+    movdqu(HK, Address(htbl, 0));\n+    movdqu(xmm10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n+    vpshufb(HK, HK, xmm10, Assembler::AVX_128bit);\n+\n+    movdqu(xmm11, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr() + 64)); \/\/ Poly\n+    movdqu(xmm12, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr() + 80)); \/\/ Twoone\n+    \/\/ Compute H ^ 2 from the input subkeyH\n+    movdqu(xmm2, xmm6);\n+    vpsllq(xmm6, xmm6, 1, Assembler::AVX_128bit);\n+    vpsrlq(xmm2, xmm2, 63, Assembler::AVX_128bit);\n+    movdqu(xmm1, xmm2);\n+    vpslldq(xmm2, xmm2, 8, Assembler::AVX_128bit);\n+    vpsrldq(xmm1, xmm1, 8, Assembler::AVX_128bit);\n+    vpor(xmm6, xmm6, xmm2, Assembler::AVX_128bit);\n+\n+    vpshufd(xmm2, xmm1, 0x24, Assembler::AVX_128bit);\n+    vpcmpeqd(xmm2, xmm2, xmm12, AVX_128bit);\n+    vpand(xmm2, xmm2, xmm11, Assembler::AVX_128bit);\n+    vpxor(xmm6, xmm6, xmm2, Assembler::AVX_128bit);\n+    movdqu(Address(htbl, 16 * 56), xmm6); \/\/ H ^ 2\n+    \/\/ Compute the remaining three powers of H using XMM registers and all following powers using ZMM\n+    movdqu(ZT5, HK);\n+    vinserti32x4(ZT7, ZT7, HK, 3);\n+\n+    gfmul_avx512(ZT5, HK);\n+    movdqu(Address(htbl, 16 * 55), ZT5); \/\/ H ^ 2 * 2\n+    vinserti32x4(ZT7, ZT7, ZT5, 2);\n+\n+    gfmul_avx512(ZT5, HK);\n+    movdqu(Address(htbl, 16 * 54), ZT5); \/\/ H ^ 2 * 3\n+    vinserti32x4(ZT7, ZT7, ZT5, 1);\n+\n+    gfmul_avx512(ZT5, HK);\n+    movdqu(Address(htbl, 16 * 53), ZT5); \/\/ H ^ 2 * 4\n+    vinserti32x4(ZT7, ZT7, ZT5, 0);\n+\n+    evshufi64x2(ZT5, ZT5, ZT5, 0x00, Assembler::AVX_512bit);\n+    evmovdquq(ZT8, ZT7, Assembler::AVX_512bit);\n+    gfmul_avx512(ZT7, ZT5);\n+    evmovdquq(Address(htbl, 16 * 49), ZT7, Assembler::AVX_512bit);\n+    evshufi64x2(ZT5, ZT7, ZT7, 0x00, Assembler::AVX_512bit);\n+    gfmul_avx512(ZT8, ZT5);\n+    evmovdquq(Address(htbl, 16 * 45), ZT8, Assembler::AVX_512bit);\n+    gfmul_avx512(ZT7, ZT5);\n+    evmovdquq(Address(htbl, 16 * 41), ZT7, Assembler::AVX_512bit);\n+    gfmul_avx512(ZT8, ZT5);\n+    evmovdquq(Address(htbl, 16 * 37), ZT8, Assembler::AVX_512bit);\n+    gfmul_avx512(ZT7, ZT5);\n+    evmovdquq(Address(htbl, 16 * 33), ZT7, Assembler::AVX_512bit);\n+    gfmul_avx512(ZT8, ZT5);\n+    evmovdquq(Address(htbl, 16 * 29), ZT8, Assembler::AVX_512bit);\n+    gfmul_avx512(ZT7, ZT5);\n+    evmovdquq(Address(htbl, 16 * 25), ZT7, Assembler::AVX_512bit);\n+    gfmul_avx512(ZT8, ZT5);\n+    evmovdquq(Address(htbl, 16 * 21), ZT8, Assembler::AVX_512bit);\n+    gfmul_avx512(ZT7, ZT5);\n+    evmovdquq(Address(htbl, 16 * 17), ZT7, Assembler::AVX_512bit);\n+    gfmul_avx512(ZT8, ZT5);\n+    evmovdquq(Address(htbl, 16 * 13), ZT8, Assembler::AVX_512bit);\n+    gfmul_avx512(ZT7, ZT5);\n+    evmovdquq(Address(htbl, 16 * 9), ZT7, Assembler::AVX_512bit);\n+    ret(0);\n+}\n+\n+#define vclmul_reduce(out, poly, hi128, lo128, tmp0, tmp1) \\\n+evpclmulqdq(tmp0, poly, lo128, 0x01, Assembler::AVX_512bit); \\\n+vpslldq(tmp0, tmp0, 8, Assembler::AVX_512bit); \\\n+evpxorq(tmp0, lo128, tmp0, Assembler::AVX_512bit); \\\n+evpclmulqdq(tmp1, poly, tmp0, 0x00, Assembler::AVX_512bit); \\\n+vpsrldq(tmp1, tmp1, 4, Assembler::AVX_512bit); \\\n+evpclmulqdq(out, poly, tmp0, 0x10, Assembler::AVX_512bit); \\\n+vpslldq(out, out, 4, Assembler::AVX_512bit); \\\n+vpternlogq(out, 0x96, tmp1, hi128, Assembler::AVX_512bit); \\\n+\n+#define vhpxori4x128(reg, tmp) \\\n+vextracti64x4(tmp, reg, 1); \\\n+evpxorq(reg, reg, tmp, Assembler::AVX_256bit); \\\n+vextracti32x4(tmp, reg, 1); \\\n+evpxorq(reg, reg, tmp, Assembler::AVX_128bit); \\\n+\n+#define roundEncode(key, dst1, dst2, dst3, dst4) \\\n+vaesenc(dst1, dst1, key, Assembler::AVX_512bit); \\\n+vaesenc(dst2, dst2, key, Assembler::AVX_512bit); \\\n+vaesenc(dst3, dst3, key, Assembler::AVX_512bit); \\\n+vaesenc(dst4, dst4, key, Assembler::AVX_512bit); \\\n+\n+#define lastroundEncode(key, dst1, dst2, dst3, dst4) \\\n+vaesenclast(dst1, dst1, key, Assembler::AVX_512bit); \\\n+vaesenclast(dst2, dst2, key, Assembler::AVX_512bit); \\\n+vaesenclast(dst3, dst3, key, Assembler::AVX_512bit); \\\n+vaesenclast(dst4, dst4, key, Assembler::AVX_512bit); \\\n+\n+#define storeData(dst, position, src1, src2, src3, src4) \\\n+evmovdquq(Address(dst, position, Address::times_1, 0 * 64), src1, Assembler::AVX_512bit); \\\n+evmovdquq(Address(dst, position, Address::times_1, 1 * 64), src2, Assembler::AVX_512bit); \\\n+evmovdquq(Address(dst, position, Address::times_1, 2 * 64), src3, Assembler::AVX_512bit); \\\n+evmovdquq(Address(dst, position, Address::times_1, 3 * 64), src4, Assembler::AVX_512bit); \\\n+\n+#define loadData(src, position, dst1, dst2, dst3, dst4) \\\n+evmovdquq(dst1, Address(src, position, Address::times_1, 0 * 64), Assembler::AVX_512bit); \\\n+evmovdquq(dst2, Address(src, position, Address::times_1, 1 * 64), Assembler::AVX_512bit); \\\n+evmovdquq(dst3, Address(src, position, Address::times_1, 2 * 64), Assembler::AVX_512bit); \\\n+evmovdquq(dst4, Address(src, position, Address::times_1, 3 * 64), Assembler::AVX_512bit); \\\n+\n+#define carrylessMultiply(dst00, dst01, dst10, dst11, ghdata, hkey) \\\n+evpclmulqdq(dst00, ghdata, hkey, 0x00, Assembler::AVX_512bit); \\\n+evpclmulqdq(dst01, ghdata, hkey, 0x01, Assembler::AVX_512bit); \\\n+evpclmulqdq(dst10, ghdata, hkey, 0x10, Assembler::AVX_512bit); \\\n+evpclmulqdq(dst11, ghdata, hkey, 0x11, Assembler::AVX_512bit); \\\n+\n+#define shuffleExorRnd1Key(dst0, dst1, dst2, dst3, shufmask, rndkey) \\\n+vpshufb(dst0, dst0, shufmask, Assembler::AVX_512bit); \\\n+evpxorq(dst0, dst0, rndkey, Assembler::AVX_512bit); \\\n+vpshufb(dst1, dst1, shufmask, Assembler::AVX_512bit); \\\n+evpxorq(dst1, dst1, rndkey, Assembler::AVX_512bit); \\\n+vpshufb(dst2, dst2, shufmask, Assembler::AVX_512bit); \\\n+evpxorq(dst2, dst2, rndkey, Assembler::AVX_512bit); \\\n+vpshufb(dst3, dst3, shufmask, Assembler::AVX_512bit); \\\n+evpxorq(dst3, dst3, rndkey, Assembler::AVX_512bit); \\\n+\n+#define xorBeforeStore(dst0, dst1, dst2, dst3, src0, src1, src2, src3) \\\n+evpxorq(dst0, dst0, src0, Assembler::AVX_512bit); \\\n+evpxorq(dst1, dst1, src1, Assembler::AVX_512bit); \\\n+evpxorq(dst2, dst2, src2, Assembler::AVX_512bit); \\\n+evpxorq(dst3, dst3, src3, Assembler::AVX_512bit); \\\n+\n+#define xorGHASH(dst0, dst1, dst2, dst3, src02, src03, src12, src13, src22, src23, src32, src33) \\\n+vpternlogq(dst0, 0x96, src02, src03, Assembler::AVX_512bit); \\\n+vpternlogq(dst1, 0x96, src12, src13, Assembler::AVX_512bit); \\\n+vpternlogq(dst2, 0x96, src22, src23, Assembler::AVX_512bit); \\\n+vpternlogq(dst3, 0x96, src32, src33, Assembler::AVX_512bit); \\\n+\n+void MacroAssembler::ghash16_encrypt16_parallel(Register key, Register subkeyHtbl, XMMRegister ctr_blockx, XMMRegister aad_hashx,\n+    Register in, Register out, Register data, Register pos, bool first_time_reduction, XMMRegister addmask, bool ghash_input, Register rounds,\n+    Register ghash_pos, bool final_reduction, int i, Register isEncrypt, XMMRegister counter_inc_mask) {\n+\n+    Label AES_192, AES_256, LAST_AES_RND;\n+    const XMMRegister ZTMP0 = xmm0;\n+    const XMMRegister ZTMP1 = xmm3;\n+    const XMMRegister ZTMP2 = xmm4;\n+    const XMMRegister ZTMP3 = xmm5;\n+    const XMMRegister ZTMP5 = xmm7;\n+    const XMMRegister ZTMP6 = xmm10;\n+    const XMMRegister ZTMP7 = xmm11;\n+    const XMMRegister ZTMP8 = xmm12;\n+    const XMMRegister ZTMP9 = xmm13;\n+    const XMMRegister ZTMP10 = xmm15;\n+    const XMMRegister ZTMP11 = xmm16;\n+    const XMMRegister ZTMP12 = xmm17;\n+\n+    const XMMRegister ZTMP13 = xmm19;\n+    const XMMRegister ZTMP14 = xmm20;\n+    const XMMRegister ZTMP15 = xmm21;\n+    const XMMRegister ZTMP16 = xmm30;\n+    const XMMRegister ZTMP17 = xmm31;\n+    const XMMRegister ZTMP18 = xmm1;\n+    const XMMRegister ZTMP19 = xmm2;\n+    const XMMRegister ZTMP20 = xmm8;\n+    const XMMRegister ZTMP21 = xmm22;\n+    const XMMRegister ZTMP22 = xmm23;\n+\n+    \/\/ Pre increment counters\n+    vpaddd(ZTMP0, ctr_blockx, counter_inc_mask, Assembler::AVX_512bit);\n+    vpaddd(ZTMP1, ZTMP0, counter_inc_mask, Assembler::AVX_512bit);\n+    vpaddd(ZTMP2, ZTMP1, counter_inc_mask, Assembler::AVX_512bit);\n+    vpaddd(ZTMP3, ZTMP2, counter_inc_mask, Assembler::AVX_512bit);\n+    \/\/ Save counter value\n+    evmovdquq(ctr_blockx, ZTMP3, Assembler::AVX_512bit);\n+\n+    \/\/ Reuse ZTMP17 \/ ZTMP18 for loading AES Keys\n+    \/\/ Pre-load AES round keys\n+    ev_load_key(ZTMP17, key, 0, xmm29);\n+    ev_load_key(ZTMP18, key, 1 * 16, xmm29);\n+\n+    \/\/ ZTMP19 & ZTMP20 used for loading hash key\n+    \/\/ Pre-load hash key\n+    evmovdquq(ZTMP19, Address(subkeyHtbl, i * 64 + 144), Assembler::AVX_512bit);\n+    evmovdquq(ZTMP20, Address(subkeyHtbl, ++i * 64 + 144), Assembler::AVX_512bit);\n+    \/\/ Load data for computing ghash\n+    evmovdquq(ZTMP21, Address(data, ghash_pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n+    vpshufb(ZTMP21, ZTMP21, xmm24, Assembler::AVX_512bit);\n+\n+    \/\/ Xor cipher block 0 with input ghash, if available\n+    if (ghash_input) {\n+        evpxorq(ZTMP21, ZTMP21, aad_hashx, Assembler::AVX_512bit);\n+    }\n+    \/\/ Load data for computing ghash\n+    evmovdquq(ZTMP22, Address(data, ghash_pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n+    vpshufb(ZTMP22, ZTMP22, xmm24, Assembler::AVX_512bit);\n+\n+    \/\/ stitch AES rounds with GHASH\n+    \/\/ AES round 0, xmm24 has shuffle mask\n+    shuffleExorRnd1Key(ZTMP0, ZTMP1, ZTMP2, ZTMP3, xmm24, ZTMP17);\n+    \/\/ Reuse ZTMP17 \/ ZTMP18 for loading remaining AES Keys\n+    ev_load_key(ZTMP17, key, 2 * 16, xmm29);\n+    \/\/ GHASH 4 blocks\n+    carrylessMultiply(ZTMP6, ZTMP7, ZTMP8, ZTMP5, ZTMP21, ZTMP19);\n+    \/\/ Load the next hkey and Ghash data\n+    evmovdquq(ZTMP19, Address(subkeyHtbl, ++i * 64 + 144), Assembler::AVX_512bit);\n+    evmovdquq(ZTMP21, Address(data, ghash_pos, Address::times_1, 2 * 64), Assembler::AVX_512bit);\n+    vpshufb(ZTMP21, ZTMP21, xmm24, Assembler::AVX_512bit);\n+\n+    \/\/ AES round 1\n+    roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    ev_load_key(ZTMP18, key, 3 * 16, xmm29);\n+\n+    \/\/ GHASH 4 blocks(11 to 8)\n+    carrylessMultiply(ZTMP10, ZTMP12, ZTMP11, ZTMP9, ZTMP22, ZTMP20);\n+    \/\/ Load the next hkey and GDATA\n+    evmovdquq(ZTMP20, Address(subkeyHtbl, ++i * 64 + 144), Assembler::AVX_512bit);\n+    evmovdquq(ZTMP22, Address(data, ghash_pos, Address::times_1, 3 * 64), Assembler::AVX_512bit);\n+    vpshufb(ZTMP22, ZTMP22, xmm24, Assembler::AVX_512bit);\n+\n+    \/\/ AES round 2\n+    roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    ev_load_key(ZTMP17, key, 4 * 16, xmm29);\n+\n+    \/\/ GHASH 4 blocks(7 to 4)\n+    carrylessMultiply(ZTMP14, ZTMP16, ZTMP15, ZTMP13, ZTMP21, ZTMP19);\n+    \/\/ AES rounds 3\n+    roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    ev_load_key(ZTMP18, key, 5 * 16, xmm29);\n+\n+    \/\/ Gather(XOR) GHASH for 12 blocks\n+    xorGHASH(ZTMP5, ZTMP6, ZTMP8, ZTMP7, ZTMP9, ZTMP13, ZTMP10, ZTMP14, ZTMP12, ZTMP16, ZTMP11, ZTMP15);\n+\n+    \/\/ AES rounds 4\n+    roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    ev_load_key(ZTMP17, key, 6 * 16, xmm29);\n+\n+    \/\/ load plain \/ cipher text(recycle registers)\n+    loadData(in, pos, ZTMP13, ZTMP14, ZTMP15, ZTMP16);\n+\n+    \/\/ AES rounds 5\n+    roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    ev_load_key(ZTMP18, key, 7 * 16, xmm29);\n+    \/\/ GHASH 4 blocks(3 to 0)\n+    carrylessMultiply(ZTMP10, ZTMP12, ZTMP11, ZTMP9, ZTMP22, ZTMP20);\n+\n+    \/\/  AES round 6\n+    roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    ev_load_key(ZTMP17, key, 8 * 16, xmm29);\n+\n+    \/\/ gather GHASH in ZTMP6(low) and ZTMP5(high)\n+    if (first_time_reduction) {\n+        vpternlogq(ZTMP7, 0x96, ZTMP8, ZTMP12, Assembler::AVX_512bit);\n+        evpxorq(xmm25, ZTMP7, ZTMP11, Assembler::AVX_512bit);\n+        evpxorq(xmm27, ZTMP5, ZTMP9, Assembler::AVX_512bit);\n+        evpxorq(xmm26, ZTMP6, ZTMP10, Assembler::AVX_512bit);\n+    }\n+    else if (!first_time_reduction && !final_reduction) {\n+        xorGHASH(ZTMP7, xmm25, xmm27, xmm26, ZTMP8, ZTMP12, ZTMP7, ZTMP11, ZTMP5, ZTMP9, ZTMP6, ZTMP10);\n+    }\n+\n+    if (final_reduction) {\n+        \/\/ Phase one: Add mid products together\n+        \/\/ Also load polynomial constant for reduction\n+        vpternlogq(ZTMP7, 0x96, ZTMP8, ZTMP12, Assembler::AVX_512bit);\n+        vpternlogq(ZTMP7, 0x96, xmm25, ZTMP11, Assembler::AVX_512bit);\n+        vpsrldq(ZTMP11, ZTMP7, 8, Assembler::AVX_512bit);\n+        vpslldq(ZTMP7, ZTMP7, 8, Assembler::AVX_512bit);\n+        evmovdquq(ZTMP12, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr()), Assembler::AVX_512bit, rbx);\n+    }\n+    \/\/ AES round 7\n+    roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    ev_load_key(ZTMP18, key, 9 * 16, xmm29);\n+    if (final_reduction) {\n+        vpternlogq(ZTMP5, 0x96, ZTMP9, ZTMP11, Assembler::AVX_512bit);\n+        evpxorq(ZTMP5, ZTMP5, xmm27, Assembler::AVX_512bit);\n+        vpternlogq(ZTMP6, 0x96, ZTMP10, ZTMP7, Assembler::AVX_512bit);\n+        evpxorq(ZTMP6, ZTMP6, xmm26, Assembler::AVX_512bit);\n+    }\n+    \/\/ AES round 8\n+    roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    ev_load_key(ZTMP17, key, 10 * 16, xmm29);\n+\n+    \/\/ Horizontal xor of low and high 4*128\n+    if (final_reduction) {\n+        vhpxori4x128(ZTMP5, ZTMP9);\n+        vhpxori4x128(ZTMP6, ZTMP10);\n+    }\n+    \/\/ AES round 9\n+    roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    \/\/ First phase of reduction\n+    if (final_reduction) {\n+        evpclmulqdq(ZTMP10, ZTMP12, ZTMP6, 0x01, Assembler::AVX_128bit);\n+        vpslldq(ZTMP10, ZTMP10, 8, Assembler::AVX_128bit);\n+        evpxorq(ZTMP10, ZTMP6, ZTMP10, Assembler::AVX_128bit);\n+    }\n+    cmpl(rounds, 52);\n+    jcc(Assembler::greaterEqual, AES_192);\n+    jmp(LAST_AES_RND);\n+    \/\/ AES rounds upto 11 (AES192) or 13 (AES256)\n+    bind(AES_192);\n+    roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    ev_load_key(ZTMP18, key, 11 * 16, xmm29);\n+    roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    ev_load_key(ZTMP17, key, 12 * 16, xmm29);\n+    cmpl(rounds, 60);\n+    jcc(Assembler::aboveEqual, AES_256);\n+    jmp(LAST_AES_RND);\n+\n+    bind(AES_256);\n+    roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    ev_load_key(ZTMP18, key, 13 * 16, xmm29);\n+    roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    ev_load_key(ZTMP17, key, 14 * 16, xmm29);\n+\n+    bind(LAST_AES_RND);\n+    \/\/ Second phase of reduction\n+    if (final_reduction) {\n+        evpclmulqdq(ZTMP9, ZTMP12, ZTMP10, 0x00, Assembler::AVX_128bit);\n+        vpsrldq(ZTMP9, ZTMP9, 4, Assembler::AVX_128bit); \/\/ Shift-R 1-DW to obtain 2-DWs shift-R\n+        evpclmulqdq(ZTMP11, ZTMP12, ZTMP10, 0x10, Assembler::AVX_128bit);\n+        vpslldq(ZTMP11, ZTMP11, 4, Assembler::AVX_128bit); \/\/ Shift-L 1-DW for result\n+        \/\/ ZTMP5 = ZTMP5 X ZTMP11 X ZTMP9\n+        vpternlogq(ZTMP5, 0x96, ZTMP11, ZTMP9, Assembler::AVX_128bit);\n+    }\n+    \/\/ Last AES round\n+    lastroundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    \/\/ XOR against plain \/ cipher text\n+    xorBeforeStore(ZTMP0, ZTMP1, ZTMP2, ZTMP3, ZTMP13, ZTMP14, ZTMP15, ZTMP16);\n+    \/\/ store cipher \/ plain text\n+    storeData(out, pos, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+}\n+\n+void MacroAssembler::aesgcm_encrypt(Register in, Register len, Register ct, Register out, Register key,\n+                                    Register processInChunks, Register isEncrypt, Register state,\n+                                    Register subkeyHtbl, Register counter) {\n+\n+    Label ENC_DEC_DONE, GENERATE_HTBL_48_BLKS, AES_192, AES_256, STORE_CT, GHASH_LAST_32,\n+          AES_32_BLOCKS, GHASH_AES_PARALLEL, LOOP, ACCUMULATE, GHASH_16_AES_16;\n+    const XMMRegister CTR_BLOCKx = xmm9;\n+    const XMMRegister AAD_HASHx = xmm14;\n+    const Register pos = rax;\n+    const Register rounds = r15;\n+    Register ghash_pos;\n+#ifndef _WIN64\n+    ghash_pos = r14;\n+#else\n+    ghash_pos = r11;\n+#endif \/\/ !_WIN64\n+    const XMMRegister ZTMP0 = xmm0;\n+    const XMMRegister ZTMP1 = xmm3;\n+    const XMMRegister ZTMP2 = xmm4;\n+    const XMMRegister ZTMP3 = xmm5;\n+    const XMMRegister ZTMP4 = xmm6;\n+    const XMMRegister ZTMP5 = xmm7;\n+    const XMMRegister ZTMP6 = xmm10;\n+    const XMMRegister ZTMP7 = xmm11;\n+    const XMMRegister ZTMP8 = xmm12;\n+    const XMMRegister ZTMP9 = xmm13;\n+    const XMMRegister ZTMP10 = xmm15;\n+    const XMMRegister ZTMP11 = xmm16;\n+    const XMMRegister ZTMP12 = xmm17;\n+    const XMMRegister ZTMP13 = xmm19;\n+    const XMMRegister ZTMP14 = xmm20;\n+    const XMMRegister ZTMP15 = xmm21;\n+    const XMMRegister ZTMP16 = xmm30;\n+    const XMMRegister COUNTER_INC_MASK = xmm18;\n+\n+    movl(pos, 0); \/\/ Total length processed\n+    \/\/ Min data size processed = 768 bytes\n+    cmpl(len, 768);\n+    jcc(Assembler::less, ENC_DEC_DONE);\n+\n+    \/\/ Generate 48 constants for htbl\n+    call(GENERATE_HTBL_48_BLKS, relocInfo::none);\n+    int index = 0; \/\/ Index for choosing subkeyHtbl entry\n+    movl(ghash_pos, 0); \/\/ Pointer for ghash read and store operations\n+\n+    \/\/ Move initial counter value and STATE value into variables\n+    movdqu(CTR_BLOCKx, Address(counter, 0));\n+    movdqu(AAD_HASHx, Address(state, 0));\n+    \/\/ Load lswap mask for ghash\n+    movdqu(xmm24, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()), rbx);\n+    \/\/ Shuffle input state using lswap mask\n+    vpshufb(AAD_HASHx, AAD_HASHx, xmm24, Assembler::AVX_128bit);\n+\n+    \/\/ Compute #rounds for AES based on the length of the key array\n+    movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+    \/\/ Broadcast counter value to 512 bit register\n+    evshufi64x2(CTR_BLOCKx, CTR_BLOCKx, CTR_BLOCKx, 0, Assembler::AVX_512bit);\n+    \/\/ Load counter shuffle mask\n+    evmovdquq(xmm24, ExternalAddress(StubRoutines::x86::counter_mask_addr()), Assembler::AVX_512bit, rbx);\n+    \/\/ Shuffle counter\n+    vpshufb(CTR_BLOCKx, CTR_BLOCKx, xmm24, Assembler::AVX_512bit);\n+\n+    \/\/ Load mask for incrementing counter\n+    evmovdquq(COUNTER_INC_MASK, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, rbx);\n+    \/\/ Pre-increment counter\n+    vpaddd(ZTMP5, CTR_BLOCKx, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 64), Assembler::AVX_512bit, rbx);\n+    vpaddd(ZTMP6, ZTMP5, COUNTER_INC_MASK, Assembler::AVX_512bit);\n+    vpaddd(ZTMP7, ZTMP6, COUNTER_INC_MASK, Assembler::AVX_512bit);\n+    vpaddd(ZTMP8, ZTMP7, COUNTER_INC_MASK, Assembler::AVX_512bit);\n+\n+    \/\/ Begin 32 blocks of AES processing\n+    bind(AES_32_BLOCKS);\n+    \/\/ Save incremented counter before overwriting it with AES data\n+    evmovdquq(CTR_BLOCKx, ZTMP8, Assembler::AVX_512bit);\n+\n+    \/\/ Move 256 bytes of data\n+    loadData(in, pos, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    \/\/ Load key shuffle mask\n+    movdqu(xmm29, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()), rbx);\n+    \/\/ Load 0th AES round key\n+    ev_load_key(ZTMP4, key, 0, xmm29);\n+    \/\/ AES-ROUND0, xmm24 has the shuffle mask\n+    shuffleExorRnd1Key(ZTMP5, ZTMP6, ZTMP7, ZTMP8, xmm24, ZTMP4);\n+\n+    for (int j = 1; j < 10; j++) {\n+        ev_load_key(ZTMP4, key, j * 16, xmm29);\n+        roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+    }\n+    ev_load_key(ZTMP4, key, 10 * 16, xmm29);\n+    \/\/ AES rounds upto 11 (AES192) or 13 (AES256)\n+    cmpl(rounds, 52);\n+    jcc(Assembler::greaterEqual, AES_192);\n+    lastroundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+    jmp(STORE_CT);\n+\n+    bind(AES_192);\n+    roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+    ev_load_key(ZTMP4, key, 11 * 16, xmm29);\n+    roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+    cmpl(rounds, 60);\n+    jcc(Assembler::aboveEqual, AES_256);\n+    ev_load_key(ZTMP4, key, 12 * 16, xmm29);\n+    lastroundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+    jmp(STORE_CT);\n+\n+    bind(AES_256);\n+    ev_load_key(ZTMP4, key, 12 * 16, xmm29);\n+    roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+    ev_load_key(ZTMP4, key, 13 * 16, xmm29);\n+    roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+    ev_load_key(ZTMP4, key, 14 * 16, xmm29);\n+    \/\/ Last AES round\n+    lastroundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+\n+    bind(STORE_CT);\n+    \/\/ Xor the encrypted key with PT to obtain CT\n+    xorBeforeStore(ZTMP5, ZTMP6, ZTMP7, ZTMP8, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+    storeData(out, pos, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+    \/\/ 16 blocks encryption completed\n+    addl(pos, 256);\n+    cmpl(pos, 512);\n+    jcc(Assembler::aboveEqual, GHASH_AES_PARALLEL);\n+    vpaddd(ZTMP5, CTR_BLOCKx, COUNTER_INC_MASK, Assembler::AVX_512bit);\n+    vpaddd(ZTMP6, ZTMP5, COUNTER_INC_MASK, Assembler::AVX_512bit);\n+    vpaddd(ZTMP7, ZTMP6, COUNTER_INC_MASK, Assembler::AVX_512bit);\n+    vpaddd(ZTMP8, ZTMP7, COUNTER_INC_MASK, Assembler::AVX_512bit);\n+    jmp(AES_32_BLOCKS);\n+\n+    bind(GHASH_AES_PARALLEL);\n+    \/\/ Ghash16_encrypt16_parallel takes place in the order with three reduction values:\n+    \/\/ 1) First time -> cipher xor input ghash\n+    \/\/ 2) No reduction -> accumulate multiplication values\n+    \/\/ 3) Final reduction post 48 blocks -> new ghash value is computed for the next round\n+    \/\/ Reduction value = first time\n+    ghash16_encrypt16_parallel(key, subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, true, xmm24, true, rounds, ghash_pos, false, index, isEncrypt, COUNTER_INC_MASK);\n+    addl(pos, 256);\n+    addl(ghash_pos, 256);\n+    index += 4;\n+\n+    \/\/ At this point we have processed 768 bytes of AES and 256 bytes of GHASH.\n+    \/\/ If the remaining length is less than 768, process remaining 512 bytes of ghash in GHASH_LAST_32 code\n+    subl(len, 768);\n+    cmpl(len, 768);\n+    jcc(Assembler::less, GHASH_LAST_32);\n+\n+    \/\/ AES 16 blocks and GHASH 16 blocks in parallel\n+    \/\/ For multiples of 48 blocks we will do ghash16_encrypt16 interleaved multiple times\n+    \/\/ Reduction value = no reduction means that the carryless multiplication values are accumulated for further calculations\n+    \/\/ Each call uses 4 subkeyHtbl values, so increment the index by 4.\n+    bind(GHASH_16_AES_16);\n+    \/\/ Reduction value = no reduction\n+    ghash16_encrypt16_parallel(key, subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, false, xmm24, false, rounds, ghash_pos, false, index, isEncrypt, COUNTER_INC_MASK);\n+    addl(pos, 256);\n+    addl(ghash_pos, 256);\n+    index += 4;\n+    \/\/ Reduction value = final reduction means that the accumulated values have to be reduced as we have completed 48 blocks of ghash\n+    ghash16_encrypt16_parallel(key, subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, false, xmm24, false, rounds, ghash_pos, true, index, isEncrypt, COUNTER_INC_MASK);\n+    addl(pos, 256);\n+    addl(ghash_pos, 256);\n+    \/\/ Calculated ghash value needs to be moved to AAD_HASHX so that we can restart the ghash16-aes16 pipeline\n+    movdqu(AAD_HASHx, ZTMP5);\n+    index = 0; \/\/ Reset subkeyHtbl index\n+\n+    \/\/ Restart the pipeline\n+    \/\/ Reduction value = first time\n+    ghash16_encrypt16_parallel(key, subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, true, xmm24, true, rounds, ghash_pos, false, index, isEncrypt, COUNTER_INC_MASK);\n+    addl(pos, 256);\n+    addl(ghash_pos, 256);\n+    index += 4;\n+\n+    subl(len, 768);\n+    cmpl(len, 768);\n+    jcc(Assembler::greaterEqual, GHASH_16_AES_16);\n+\n+    \/\/ GHASH last 32 blocks processed here\n+    \/\/ GHASH products accumulated in ZMM27, ZMM25 and ZMM26 during GHASH16-AES16 operation is used\n+    bind(GHASH_LAST_32);\n+    \/\/ Use rbx as a pointer to the htbl; For last 32 blocks of GHASH, use key# 4-11 entry in subkeyHtbl\n+    movl(rbx, 256);\n+    \/\/ Load cipher blocks\n+    evmovdquq(ZTMP13, Address(ct, ghash_pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n+    evmovdquq(ZTMP14, Address(ct, ghash_pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n+    vpshufb(ZTMP13, ZTMP13, xmm24, Assembler::AVX_512bit);\n+    vpshufb(ZTMP14, ZTMP14, xmm24, Assembler::AVX_512bit);\n+    \/\/ Load ghash keys\n+    evmovdquq(ZTMP15, Address(subkeyHtbl, rbx, Address::times_1, 0 * 64 + 144), Assembler::AVX_512bit);\n+    evmovdquq(ZTMP16, Address(subkeyHtbl, rbx, Address::times_1, 1 * 64 + 144), Assembler::AVX_512bit);\n+\n+    \/\/ Ghash blocks 0 - 3\n+    carrylessMultiply(ZTMP2, ZTMP3, ZTMP4, ZTMP1, ZTMP13, ZTMP15);\n+    \/\/ Ghash blocks 4 - 7\n+    carrylessMultiply(ZTMP6, ZTMP7, ZTMP8, ZTMP5, ZTMP14, ZTMP16);\n+\n+    vpternlogq(ZTMP1, 0x96, ZTMP5, xmm27, Assembler::AVX_512bit); \/\/ ZTMP1 = ZTMP1 + ZTMP5 + zmm27\n+    vpternlogq(ZTMP2, 0x96, ZTMP6, xmm26, Assembler::AVX_512bit); \/\/ ZTMP2 = ZTMP2 + ZTMP6 + zmm26\n+    vpternlogq(ZTMP3, 0x96, ZTMP7, xmm25, Assembler::AVX_512bit); \/\/ ZTMP3 = ZTMP3 + ZTMP7 + zmm25\n+    evpxorq(ZTMP4, ZTMP4, ZTMP8, Assembler::AVX_512bit);          \/\/ ZTMP4 = ZTMP4 + ZTMP8\n+\n+    addl(ghash_pos, 128);\n+    addl(rbx, 128);\n+\n+    \/\/ Ghash remaining blocks\n+    bind(LOOP);\n+    cmpl(ghash_pos, pos);\n+    jcc(Assembler::aboveEqual, ACCUMULATE);\n+    \/\/ Load next cipher blocks and corresponding ghash keys\n+    evmovdquq(ZTMP13, Address(ct, ghash_pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n+    evmovdquq(ZTMP14, Address(ct, ghash_pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n+    vpshufb(ZTMP13, ZTMP13, xmm24, Assembler::AVX_512bit);\n+    vpshufb(ZTMP14, ZTMP14, xmm24, Assembler::AVX_512bit);\n+    evmovdquq(ZTMP15, Address(subkeyHtbl, rbx, Address::times_1, 0 * 64 + 144), Assembler::AVX_512bit);\n+    evmovdquq(ZTMP16, Address(subkeyHtbl, rbx, Address::times_1, 1 * 64 + 144), Assembler::AVX_512bit);\n+\n+    \/\/ ghash blocks 0 - 3\n+    carrylessMultiply(ZTMP6, ZTMP7, ZTMP8, ZTMP5, ZTMP13, ZTMP15);\n+\n+    \/\/ ghash blocks 4 - 7\n+    carrylessMultiply(ZTMP10, ZTMP11, ZTMP12, ZTMP9, ZTMP14, ZTMP16);\n+\n+    \/\/ update sums\n+    \/\/ ZTMP1 = ZTMP1 + ZTMP5 + ZTMP9\n+    \/\/ ZTMP2 = ZTMP2 + ZTMP6 + ZTMP10\n+    \/\/ ZTMP3 = ZTMP3 + ZTMP7 xor ZTMP11\n+    \/\/ ZTMP4 = ZTMP4 + ZTMP8 xor ZTMP12\n+    xorGHASH(ZTMP1, ZTMP2, ZTMP3, ZTMP4, ZTMP5, ZTMP9, ZTMP6, ZTMP10, ZTMP7, ZTMP11, ZTMP8, ZTMP12);\n+    addl(ghash_pos, 128);\n+    addl(rbx, 128);\n+    jmp(LOOP);\n+\n+    \/\/ Integrate ZTMP3\/ZTMP4 into ZTMP1 and ZTMP2\n+    bind(ACCUMULATE);\n+    evpxorq(ZTMP3, ZTMP3, ZTMP4, Assembler::AVX_512bit);\n+    vpsrldq(ZTMP7, ZTMP3, 8, Assembler::AVX_512bit);\n+    vpslldq(ZTMP8, ZTMP3, 8, Assembler::AVX_512bit);\n+    evpxorq(ZTMP1, ZTMP1, ZTMP7, Assembler::AVX_512bit);\n+    evpxorq(ZTMP2, ZTMP2, ZTMP8, Assembler::AVX_512bit);\n+\n+    \/\/ Add ZTMP1 and ZTMP2 128 - bit words horizontally\n+    vhpxori4x128(ZTMP1, ZTMP11);\n+    vhpxori4x128(ZTMP2, ZTMP12);\n+    \/\/ Load reduction polynomial and compute final reduction\n+    evmovdquq(ZTMP15, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr()), Assembler::AVX_512bit, rbx);\n+    vclmul_reduce(AAD_HASHx, ZTMP15, ZTMP1, ZTMP2, ZTMP3, ZTMP4);\n+\n+    \/\/ Pre-increment counter for next operation\n+    vpaddd(CTR_BLOCKx, CTR_BLOCKx, xmm18, Assembler::AVX_128bit);\n+    \/\/ Shuffle counter and save the updated value\n+    vpshufb(CTR_BLOCKx, CTR_BLOCKx, xmm24, Assembler::AVX_512bit);\n+    movdqu(Address(counter, 0), CTR_BLOCKx);\n+    \/\/ Load ghash lswap mask\n+    movdqu(xmm24, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n+    \/\/ Shuffle ghash using lbswap_mask and store it\n+    vpshufb(AAD_HASHx, AAD_HASHx, xmm24, Assembler::AVX_128bit);\n+    movdqu(Address(state, 0), AAD_HASHx);\n+    jmp(ENC_DEC_DONE);\n+\n+    bind(GENERATE_HTBL_48_BLKS);\n+    generateHtbl_48_block_zmm(subkeyHtbl);\n+\n+    bind(ENC_DEC_DONE);\n+    movq(rax, pos);\n+}\n+\n+#endif \/\/ _LP64\n\\ No newline at end of file\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_aes.cpp","additions":625,"deletions":1,"binary":false,"changes":626,"status":"modified"},{"patch":"@@ -4370,0 +4370,105 @@\n+  address ghash_polynomial512_addr() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly512_addr\");\n+    address start = __ pc();\n+    __ emit_data64(0x00000001C2000000, relocInfo::none); \/\/ POLY for reduction\n+    __ emit_data64(0xC200000000000000, relocInfo::none);\n+    __ emit_data64(0x00000001C2000000, relocInfo::none);\n+    __ emit_data64(0xC200000000000000, relocInfo::none);\n+    __ emit_data64(0x00000001C2000000, relocInfo::none);\n+    __ emit_data64(0xC200000000000000, relocInfo::none);\n+    __ emit_data64(0x00000001C2000000, relocInfo::none);\n+    __ emit_data64(0xC200000000000000, relocInfo::none);\n+    __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ POLY\n+    __ emit_data64(0xC200000000000000, relocInfo::none);\n+    __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ TWOONE\n+    __ emit_data64(0x0000000100000000, relocInfo::none);\n+    return start;\n+}\n+\n+  \/\/ Vector AES Galois Counter Mode implementation. Parameters:\n+  \/\/ Windows regs            |  Linux regs\n+  \/\/ in = c_rarg0 (rcx)      |  c_rarg0 (rsi)\n+  \/\/ len = c_rarg1 (rdx)     |  c_rarg1 (rdi)\n+  \/\/ ct = c_rarg2 (r8)       |  c_rarg2 (rdx)\n+  \/\/ out = c_rarg3 (r9)      |  c_rarg3 (rcx)\n+  \/\/ key = r10               |  c_rarg4 (r8)\n+  \/\/ processInChunks = r11   |  c_rarg5 (r9)\n+  \/\/ isEncrypt = r12         |  r10\n+  \/\/ state = r13             |  r13\n+  \/\/ subkeyHtbl = r14        |  r11\n+  \/\/ counter = rsi           |  r12\n+  \/\/ return - number of processed bytes\n+  address generate_galoisCounterMode_AESCrypt() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"galoisCounterMode_AESCrypt\");\n+    address start = __ pc();\n+    const Register in = c_rarg0;\n+    const Register len = c_rarg1;\n+    const Register ct = c_rarg2;\n+    const Register out = c_rarg3;\n+    \/\/ and updated with the incremented counter in the end\n+#ifndef _WIN64\n+    const Register key = c_rarg4;\n+    const Register processInChunks = c_rarg5;\n+    const Register isEncrypt = r10;\n+    const Address isEncrypt_mem(rbp, 2 * wordSize);\n+    const Address state_mem(rbp, 3 * wordSize);\n+    const Register state = r13;\n+    const Address subkeyH_mem(rbp, 4 * wordSize);\n+    const Register subkeyHtbl = r11;\n+    const Address counter_mem(rbp, 5 * wordSize);\n+    const Register counter = r12;\n+#else\n+    const Address key_mem(rbp, 6 * wordSize);\n+    const Register key = r10;\n+    const Register processInChunks = r11;\n+    const Address partial_mem(rbp, 7 * wordSize);\n+    const Address isEncrypt_mem(rbp, 8 * wordSize);\n+    const Register isEncrypt = r12;\n+    const Address state_mem(rbp, 9 * wordSize);\n+    const Register state = r13;\n+    const Address subkeyH_mem(rbp, 10 * wordSize);\n+    const Register subkeyHtbl = r14;\n+    const Address counter_mem(rbp, 11 * wordSize);\n+    const Register counter = rsi;\n+#endif\n+    __ enter();\n+   \/\/ Save state before entering routine\n+    __ push(r12);\n+    __ push(r13);\n+    __ push(r14);\n+    __ push(r15);\n+    __ push(rbx);\n+#ifdef _WIN64\n+    \/\/ on win64, fill len_reg from stack position\n+    __ push(rsi);\n+    __ movptr(key, key_mem);\n+    __ movl(processInChunks, partial_mem);\n+    __ movl(isEncrypt, isEncrypt_mem);\n+    __ movptr(state, state_mem);\n+    __ movptr(subkeyHtbl, subkeyH_mem);\n+    __ movptr(counter, counter_mem);\n+#else\n+    __ movptr(isEncrypt, isEncrypt_mem);\n+    __ movptr(state, state_mem);\n+    __ movptr(subkeyHtbl, subkeyH_mem);\n+    __ movptr(counter, counter_mem);\n+#endif\n+    __ aesgcm_encrypt(in, len, ct, out, key, processInChunks, isEncrypt, state, subkeyHtbl, counter);\n+\n+    \/\/ Restore state before leaving routine\n+#ifdef _WIN64\n+    __ pop(rsi);\n+#endif\n+    __ pop(rbx);\n+    __ pop(r15);\n+    __ pop(r14);\n+    __ pop(r13);\n+    __ pop(r12);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(0);\n+     return start;\n+  }\n+\n@@ -6879,0 +6984,8 @@\n+    if (UseAESIntrinsics) {\n+      if (VM_Version::supports_avx512_vaes() &&  VM_Version::supports_avx512vl() && VM_Version::supports_avx512dq()) {\n+        StubRoutines::x86::_counter_mask_addr = counter_mask_addr();\n+        StubRoutines::x86::_ghash_poly512_addr = ghash_polynomial512_addr();\n+        StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n+        StubRoutines::_galoisCounterMode_AESCrypt = generate_galoisCounterMode_AESCrypt();\n+      }\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":113,"deletions":0,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -76,0 +76,1 @@\n+address StubRoutines::x86::_ghash_poly512_addr = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -35,2 +35,2 @@\n-  code_size1 = 20000 LP64_ONLY(+10000),         \/\/ simply increase if too small (assembler will crash if too small)\n-  code_size2 = 35300 LP64_ONLY(+25000)          \/\/ simply increase if too small (assembler will crash if too small)\n+  code_size1 = 20000 LP64_ONLY(+12000),         \/\/ simply increase if too small (assembler will crash if too small)\n+  code_size2 = 35300 LP64_ONLY(+37000)          \/\/ simply increase if too small (assembler will crash if too small)\n@@ -172,0 +172,1 @@\n+  static address _ghash_poly512_addr;\n@@ -227,0 +228,1 @@\n+  static address ghash_polynomial512_addr() { return _ghash_poly512_addr; }\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -183,0 +183,1 @@\n+  case vmIntrinsics::_galoisCounterMode_AESCrypt:\n@@ -430,0 +431,3 @@\n+  case vmIntrinsics::_galoisCounterMode_AESCrypt:\n+    if (!UseAESIntrinsics) return true;\n+    break;\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -422,0 +422,5 @@\n+  do_class(com_sun_crypto_provider_galoisCounterMode, \"com\/sun\/crypto\/provider\/GaloisCounterMode\")                      \\\n+   do_intrinsic(_galoisCounterMode_AESCrypt, com_sun_crypto_provider_galoisCounterMode, gcm_crypt_name, aes_gcm_signature, F_R)   \\\n+   do_name(gcm_crypt_name, \"implGCMCrypt\")                                                                                 \\\n+   do_signature(aes_gcm_signature, \"([BII[BI[BIZZ[J[J[B)I\")                                                             \\\n+                                                                                                                        \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -334,0 +334,1 @@\n+  static_field(StubRoutines,                _galoisCounterMode_AESCrypt,                      address)                               \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -629,0 +629,1 @@\n+  case vmIntrinsics::_galoisCounterMode_AESCrypt:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1109,1 +1109,2 @@\n-                  strcmp(call->as_CallLeaf()->_name, \"vectorizedMismatch\") == 0)\n+                  strcmp(call->as_CallLeaf()->_name, \"vectorizedMismatch\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"galoisCounterMode_AESCrypt\") == 0 )\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2492,1 +2492,2 @@\n-                                  Node* parm6, Node* parm7) {\n+                                  Node* parm6, Node* parm7,\n+                                  Node* parm8, Node* parm9) {\n@@ -2536,1 +2537,3 @@\n-    \/* close each nested if ===> *\/  } } } } } } } }\n+  if (parm8 != NULL) { call->init_req(TypeFunc::Parms+8, parm8);\n+  if (parm9 != NULL) { call->init_req(TypeFunc::Parms+9, parm9);\n+  \/* close each nested if ===> *\/  } } } } } } } } } }\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -800,1 +800,2 @@\n-                          Node* parm6 = NULL, Node* parm7 = NULL);\n+                          Node* parm6 = NULL, Node* parm7 = NULL,\n+                          Node* parm8 = NULL, Node* parm9 = NULL);\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -546,0 +546,3 @@\n+  case vmIntrinsics::_galoisCounterMode_AESCrypt:\n+    return inline_galoisCounterMode_AESCrypt(intrinsic_id());\n+\n@@ -710,0 +713,2 @@\n+  case vmIntrinsics::_galoisCounterMode_AESCrypt:\n+    return inline_galoisCounterMode_AESCrypt_predicate();\n@@ -6506,0 +6511,127 @@\n+\/\/------------------------------inline_galoisCounterMode_AESCrypt-----------------------\n+bool LibraryCallKit::inline_galoisCounterMode_AESCrypt(vmIntrinsics::ID id) {\n+  assert(UseAES, \"need AES instruction support\");\n+  address stubAddr = NULL;\n+  const char *stubName = NULL;\n+  if (id == vmIntrinsics::_galoisCounterMode_AESCrypt) {\n+    stubAddr = StubRoutines::galoisCounterMode_AESCrypt();\n+    stubName = \"galoisCounterMode_AESCrypt\";\n+  }\n+  if (stubAddr == NULL) return false;\n+\n+  Node* galoisCounterMode_object = argument(0);\n+  Node* in      = argument(1);\n+  Node* inOfs   = argument(2);\n+  Node* len     = argument(3);\n+  Node* ct      = argument(4);\n+  Node* ctOfs   = argument(5);\n+  Node* out     = argument(6);\n+  Node* outOfs  = argument(7);\n+  Node* processInChunks = argument(8);\n+  Node* isEncrypt = argument(9);\n+  Node* state   = argument(10);\n+  Node* subkeyHtbl = argument(11);\n+  Node* counter = argument(12);\n+\n+  \/\/ (1) in, ct and out are arrays.\n+  const Type* in_type = in->Value(&_gvn);\n+  const Type* ct_type = ct->Value(&_gvn);\n+  const Type* out_type = out->Value(&_gvn);\n+  const TypeAryPtr* top_in = in_type->isa_aryptr();\n+  const TypeAryPtr* top_ct = ct_type->isa_aryptr();\n+  const TypeAryPtr* top_out = out_type->isa_aryptr();\n+  assert(top_in != NULL && top_in->klass() != NULL &&\n+         top_ct != NULL && top_ct->klass() != NULL &&\n+         top_out != NULL && top_out->klass() != NULL, \"args are strange\");\n+\n+  \/\/ checks are the responsibility of the caller\n+  Node* in_start = in;\n+  Node* ct_start = ct;\n+  Node* out_start = out;\n+  if (inOfs != NULL || ctOfs != NULL || outOfs != NULL) {\n+    assert(inOfs != NULL && ctOfs != NULL && outOfs != NULL, \"\");\n+    in_start = array_element_address(in, inOfs, T_BYTE);\n+    ct_start = array_element_address(ct, ctOfs, T_BYTE);\n+    out_start = array_element_address(out, outOfs, T_BYTE);\n+  }\n+\n+  \/\/ if we are in this set of code, we \"know\" the embeddedCipher is an AESCrypt object\n+  \/\/ (because of the predicated logic executed earlier).\n+  \/\/ so we cast it here safely.\n+  \/\/ this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java\n+  Node* embeddedCipherObj = load_field_from_object(galoisCounterMode_object, \"embeddedCipher\", \"Lcom\/sun\/crypto\/provider\/SymmetricCipher;\");\n+  if (embeddedCipherObj == NULL) return false;\n+  \/\/ cast it to what we know it will be at runtime\n+  const TypeInstPtr* tinst = _gvn.type(galoisCounterMode_object)->isa_instptr();\n+  assert(tinst != NULL, \"GCM obj is null\");\n+  assert(tinst->klass()->is_loaded(), \"GCM obj is not loaded\");\n+  ciKlass* klass_AESCrypt = tinst->klass()->as_instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  assert(klass_AESCrypt->is_loaded(), \"predicate checks that this class is loaded\");\n+  ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt->as_instance_klass();\n+  const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_AESCrypt);\n+  const TypeOopPtr* xtype = aklass->as_instance_type();\n+  Node* aescrypt_object = new CheckCastPPNode(control(), embeddedCipherObj, xtype);\n+  aescrypt_object = _gvn.transform(aescrypt_object);\n+  \/\/ we need to get the start of the aescrypt_object's expanded key array\n+  Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);\n+  if (k_start == NULL) return false;\n+\n+  \/\/ similarly, get the start address of the r vector\n+  Node* cnt_start = array_element_address(counter, intcon(0), T_BYTE);\n+  Node* state_start = array_element_address(state, intcon(0), T_LONG);\n+  Node* subkeyHtbl_start = array_element_address(subkeyHtbl, intcon(0), T_LONG);\n+\n+  \/\/ Call the stub, passing params\n+  Node* gcmCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                               OptoRuntime::galoisCounterMode_aescrypt_Type(),\n+                               stubAddr, stubName, TypePtr::BOTTOM,\n+                               in_start, len, ct_start, out_start, k_start, processInChunks, isEncrypt, state_start, subkeyHtbl_start, cnt_start);\n+\n+  \/\/ return cipher length (int)\n+  Node* retvalue = _gvn.transform(new ProjNode(gcmCrypt, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/----------------------------inline_galoisCounterMode_AESCrypt_predicate----------------------------\n+\/\/ Return node representing slow path of predicate check.\n+\/\/ the pseudo code we want to emulate with this predicate is:\n+\/\/ for encryption:\n+\/\/    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath\n+\/\/ for decryption:\n+\/\/    if ((embeddedCipherObj instanceof AESCrypt) && (cipher!=plain)) do_intrinsic, else do_javapath\n+\/\/    note cipher==plain is more conservative than the original java code but that's OK\n+\/\/\n+\n+Node* LibraryCallKit::inline_galoisCounterMode_AESCrypt_predicate() {\n+  \/\/ The receiver was checked for NULL already.\n+  Node* objGCM = argument(0);\n+  \/\/ Load embeddedCipher field of CipherBlockChaining object.\n+  Node* embeddedCipherObj = load_field_from_object(objGCM, \"embeddedCipher\", \"Lcom\/sun\/crypto\/provider\/SymmetricCipher;\");\n+  assert(embeddedCipherObj != NULL, \"embeddedCipherObj is null\");\n+\n+  \/\/ get AESCrypt klass for instanceOf check\n+  \/\/ AESCrypt might not be loaded yet if some other SymmetricCipher got us to this compile point\n+  \/\/ will have same classloader as CipherBlockChaining object\n+  const TypeInstPtr* tinst = _gvn.type(objGCM)->isa_instptr();\n+  assert(tinst != NULL, \"GCMobj is null\");\n+  assert(tinst->klass()->is_loaded(), \"GCMobj is not loaded\");\n+\n+  \/\/ we want to do an instanceof comparison against the AESCrypt class\n+  ciKlass* klass_AESCrypt = tinst->klass()->as_instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  if (!klass_AESCrypt->is_loaded()) {\n+    \/\/ if AESCrypt is not even loaded, we never take the intrinsic fast path\n+    Node* ctrl = control();\n+    set_control(top()); \/\/ no regular fast path\n+    return ctrl;\n+  }\n+\n+  ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt->as_instance_klass();\n+  Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));\n+  Node* cmp_instof = _gvn.transform(new CmpINode(instof, intcon(1)));\n+  Node* bool_instof = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));\n+  Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);\n+\n+  return instof_false; \/\/ even if it is NULL\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":132,"deletions":0,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -308,0 +308,2 @@\n+  bool inline_galoisCounterMode_AESCrypt(vmIntrinsics::ID id);\n+  Node* inline_galoisCounterMode_AESCrypt_predicate();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -931,0 +931,27 @@\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+  \/\/ returning cipher len (int)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+  return TypeFunc::make(domain, range);\n+}\n+\n+\/\/for counterMode calls of aescrypt encrypt\/decrypt, four pointers and a length, returning int\n+const TypeFunc* OptoRuntime::galoisCounterMode_aescrypt_Type() {\n+  \/\/ create input type (domain)\n+  int num_args = 10;\n+  int argcnt = num_args;\n+  const Type** fields = TypeTuple::fields(argcnt);\n+  int argp = TypeFunc::Parms;\n+  fields[argp++] = TypePtr::NOTNULL; \/\/ byte[] in + inOfs\n+  fields[argp++] = TypeInt::INT;     \/\/ int len\n+  fields[argp++] = TypePtr::NOTNULL; \/\/ byte[] ct + ctOfs\n+  fields[argp++] = TypePtr::NOTNULL; \/\/ byte[] out + outOfs\n+  fields[argp++] = TypePtr::NOTNULL; \/\/ byte[] key from AESCrypt obj\n+  fields[argp++] = TypeInt::BOOL;    \/\/ bool processInChunks\n+  fields[argp++] = TypeInt::BOOL;    \/\/ bool isEncrypt\n+  fields[argp++] = TypePtr::NOTNULL; \/\/ long[] state\n+  fields[argp++] = TypePtr::NOTNULL; \/\/ long[] subkeyHtbl\n+  fields[argp++] = TypePtr::NOTNULL; \/\/ byte[] counter\n+\n+  assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -277,0 +277,1 @@\n+  static const TypeFunc* galoisCounterMode_aescrypt_Type();\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -138,0 +138,1 @@\n+address StubRoutines::_galoisCounterMode_AESCrypt          = NULL;\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -220,0 +220,1 @@\n+  static address _galoisCounterMode_AESCrypt;\n@@ -428,0 +429,1 @@\n+  static address galoisCounterMode_AESCrypt()   { return _galoisCounterMode_AESCrypt; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -619,0 +619,1 @@\n+     static_field(StubRoutines,                _galoisCounterMode_AESCrypt,                   address)                               \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -153,1 +153,1 @@\n-        subkeyHtbl = new long[2*9];\n+        subkeyHtbl = new long[2*57]; \/\/ 48 keys for the interleaved implementation, 8 for avx-ghash implementation and one for the original key\n@@ -255,1 +255,1 @@\n-        if (subH.length != 18) {\n+        if (subH.length != 114) {\n@@ -282,0 +282,8 @@\n+\n+    long[] getState() {\n+        return state;\n+    }\n+\n+    long[] getSubkeyHtbl() {\n+        return subkeyHtbl;\n+    }\n","filename":"src\/java.base\/share\/classes\/com\/sun\/crypto\/provider\/GHASH.java","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+import jdk.internal.vm.annotation.IntrinsicCandidate;\n@@ -73,1 +74,1 @@\n-    private static final int TRIGGERLEN = 65536;  \/\/ 64k\n+    private static final int TRIGGERLEN = 768;  \/\/ Interleaved implementation can deal with 768 bytes or higher lengths.\n@@ -92,0 +93,3 @@\n+    private long[] subkeyHtbl = null;\n+    private long[] state = null;\n+    private byte[] counter = null;\n@@ -333,0 +337,4 @@\n+        this.subkeyHtbl = ghashAllToS.getSubkeyHtbl();\n+        this.state = ghashAllToS.getState();\n+        this.counter = gctrPAndC.counter;\n+\n@@ -407,1 +415,1 @@\n-\n+        int tlen = 0;\n@@ -416,19 +424,8 @@\n-        \/\/ Divide up larger data sizes to trigger CTR & GHASH intrinsic quicker\n-        if (len > TRIGGERLEN) {\n-            int i = 0;\n-            int tlen;  \/\/ incremental lengths\n-            final int plen = AES_BLOCK_SIZE * 6;\n-            \/\/ arbitrary formula to aid intrinsic without reaching buffer end\n-            final int count = len \/ 1024;\n-\n-            while (count > i) {\n-                tlen = gctrPAndC.update(in, inOfs, plen, out, outOfs);\n-                ghashAllToS.update(ct, ctOfs, tlen);\n-                inOfs += tlen;\n-                outOfs += tlen;\n-                ctOfs += tlen;\n-                i++;\n-            }\n-            ilen -= count * plen;\n-            processed += count * plen;\n-        }\n+          if (len > TRIGGERLEN) {\n+              tlen = implGCMCrypt(in, inOfs, len, ct, ctOfs, out, outOfs, true, isEncrypt, state, subkeyHtbl, counter);\n+              inOfs += tlen;\n+              outOfs += tlen;\n+              ctOfs += tlen;\n+              ilen -= tlen;\n+              processed += tlen;\n+          }\n@@ -508,0 +505,29 @@\n+    \/\/ Intrinsic for Vector AES Galois Counter Mode implementation.\n+    \/\/ AES and GHASH operations are interleaved in the intrinsic implementation.\n+    \/\/ return - number of processed bytes\n+    @IntrinsicCandidate\n+    private int implGCMCrypt(byte[] in, int inOfs, int len, byte[] ct, int ctOfs, byte[] out, int outOfs, boolean processInChunks, boolean isEncrypt,\n+                             long[] state, long[] subkeyHtbl, byte[] counter) {\n+        int processedBytes = 0;\n+        if (processInChunks) {\n+            int i = 0;\n+            int tlen = 0; \/\/ incremental length\n+            final int plen = AES_BLOCK_SIZE * 6;\n+            \/\/ arbitrary formula to aid intrinsic without reaching buffer end\n+            final int count = len \/ 1024;\n+            while (count > i) {\n+                tlen = gctrPAndC.update(in, inOfs, plen, out, outOfs);\n+                ghashAllToS.update(ct, ctOfs, tlen);\n+                inOfs += tlen;\n+                outOfs += tlen;\n+                ctOfs += tlen;\n+                processedBytes += tlen;\n+                i++;\n+            }\n+        } else {\n+            processedBytes = gctrPAndC.update(in, inOfs, len, out, outOfs);\n+            ghashAllToS.update(ct, ctOfs, processedBytes);\n+        }\n+        return processedBytes;\n+    }\n+\n@@ -599,3 +625,11 @@\n-        gctrPAndC.update(in, inOfs, len, out, outOfs);\n-        processed += len;\n-        ghashAllToS.update(out, outOfs, len);\n+        \/\/intrinsic processes 768 bytes or multiples of 768 bytes of data. Remaining length to be processed outside intrinsic.\n+        int tlen = implGCMCrypt(in, inOfs, len, out, outOfs, out, outOfs, false, true, state, subkeyHtbl, counter);\n+        processed += tlen;\n+        inOfs += tlen;\n+        outOfs += tlen;\n+        if (len > tlen) {\n+          int ilen = len - tlen;\n+          tlen = gctrPAndC.update(in, inOfs, ilen, out, outOfs);\n+          ghashAllToS.update(out, outOfs, tlen);\n+          processed += tlen;\n+        }\n","filename":"src\/java.base\/share\/classes\/com\/sun\/crypto\/provider\/GaloisCounterMode.java","additions":58,"deletions":24,"binary":false,"changes":82,"status":"modified"}]}