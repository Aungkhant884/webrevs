{"files":[{"patch":"@@ -2846,0 +2846,318 @@\n+uint64_t VM_Version::feature_flags() {\n+  uint64_t result = 0;\n+  if (_cpuid_info.std_cpuid1_edx.bits.cmpxchg8 != 0)\n+    result |= CPU_CX8;\n+  if (_cpuid_info.std_cpuid1_edx.bits.cmov != 0)\n+    result |= CPU_CMOV;\n+  if (_cpuid_info.std_cpuid1_edx.bits.clflush != 0)\n+    result |= CPU_FLUSH;\n+#ifdef _LP64\n+  \/\/ clflush should always be available on x86_64\n+  \/\/ if not we are in real trouble because we rely on it\n+  \/\/ to flush the code cache.\n+  assert ((result & CPU_FLUSH) != 0, \"clflush should be available\");\n+#endif\n+  if (_cpuid_info.std_cpuid1_edx.bits.fxsr != 0 || (is_amd_family() &&\n+      _cpuid_info.ext_cpuid1_edx.bits.fxsr != 0))\n+    result |= CPU_FXSR;\n+  \/\/ HT flag is set for multi-core processors also.\n+  if (threads_per_core() > 1)\n+    result |= CPU_HT;\n+  if (_cpuid_info.std_cpuid1_edx.bits.mmx != 0 || (is_amd_family() &&\n+      _cpuid_info.ext_cpuid1_edx.bits.mmx != 0))\n+    result |= CPU_MMX;\n+  if (_cpuid_info.std_cpuid1_edx.bits.sse != 0)\n+    result |= CPU_SSE;\n+  if (_cpuid_info.std_cpuid1_edx.bits.sse2 != 0)\n+    result |= CPU_SSE2;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.sse3 != 0)\n+    result |= CPU_SSE3;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.ssse3 != 0)\n+    result |= CPU_SSSE3;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.sse4_1 != 0)\n+    result |= CPU_SSE4_1;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.sse4_2 != 0)\n+    result |= CPU_SSE4_2;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.popcnt != 0)\n+    result |= CPU_POPCNT;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.avx != 0 &&\n+      _cpuid_info.std_cpuid1_ecx.bits.osxsave != 0 &&\n+      _cpuid_info.xem_xcr0_eax.bits.sse != 0 &&\n+      _cpuid_info.xem_xcr0_eax.bits.ymm != 0) {\n+    result |= CPU_AVX;\n+    result |= CPU_VZEROUPPER;\n+    if (_cpuid_info.sef_cpuid7_ebx.bits.avx2 != 0)\n+      result |= CPU_AVX2;\n+    if (_cpuid_info.sef_cpuid7_ebx.bits.avx512f != 0 &&\n+        _cpuid_info.xem_xcr0_eax.bits.opmask != 0 &&\n+        _cpuid_info.xem_xcr0_eax.bits.zmm512 != 0 &&\n+        _cpuid_info.xem_xcr0_eax.bits.zmm32 != 0) {\n+      result |= CPU_AVX512F;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512cd != 0)\n+        result |= CPU_AVX512CD;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512dq != 0)\n+        result |= CPU_AVX512DQ;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512pf != 0)\n+        result |= CPU_AVX512PF;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512er != 0)\n+        result |= CPU_AVX512ER;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512bw != 0)\n+        result |= CPU_AVX512BW;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512vl != 0)\n+        result |= CPU_AVX512VL;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vpopcntdq != 0)\n+        result |= CPU_AVX512_VPOPCNTDQ;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vpclmulqdq != 0)\n+        result |= CPU_AVX512_VPCLMULQDQ;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.vaes != 0)\n+        result |= CPU_AVX512_VAES;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.gfni != 0)\n+        result |= CPU_GFNI;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vnni != 0)\n+        result |= CPU_AVX512_VNNI;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_bitalg != 0)\n+        result |= CPU_AVX512_BITALG;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vbmi != 0)\n+        result |= CPU_AVX512_VBMI;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vbmi2 != 0)\n+        result |= CPU_AVX512_VBMI2;\n+    }\n+  }\n+  if (_cpuid_info.std_cpuid1_ecx.bits.hv != 0)\n+    result |= CPU_HV;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.bmi1 != 0)\n+    result |= CPU_BMI1;\n+  if (_cpuid_info.std_cpuid1_edx.bits.tsc != 0)\n+    result |= CPU_TSC;\n+  if (_cpuid_info.ext_cpuid7_edx.bits.tsc_invariance != 0)\n+    result |= CPU_TSCINV_BIT;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.aes != 0)\n+    result |= CPU_AES;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.erms != 0)\n+    result |= CPU_ERMS;\n+  if (_cpuid_info.sef_cpuid7_edx.bits.fast_short_rep_mov != 0)\n+    result |= CPU_FSRM;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.clmul != 0)\n+    result |= CPU_CLMUL;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.rtm != 0)\n+    result |= CPU_RTM;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.adx != 0)\n+     result |= CPU_ADX;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.bmi2 != 0)\n+    result |= CPU_BMI2;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.sha != 0)\n+    result |= CPU_SHA;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.fma != 0)\n+    result |= CPU_FMA;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.clflushopt != 0)\n+    result |= CPU_FLUSHOPT;\n+  if (_cpuid_info.ext_cpuid1_edx.bits.rdtscp != 0)\n+    result |= CPU_RDTSCP;\n+  if (_cpuid_info.sef_cpuid7_ecx.bits.rdpid != 0)\n+    result |= CPU_RDPID;\n+\n+  \/\/ AMD|Hygon features.\n+  if (is_amd_family()) {\n+    if ((_cpuid_info.ext_cpuid1_edx.bits.tdnow != 0) ||\n+        (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0))\n+      result |= CPU_3DNOW_PREFETCH;\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0)\n+      result |= CPU_LZCNT;\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.sse4a != 0)\n+      result |= CPU_SSE4A;\n+  }\n+\n+  \/\/ Intel features.\n+  if (is_intel()) {\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0) {\n+      result |= CPU_LZCNT;\n+    }\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0) {\n+      result |= CPU_3DNOW_PREFETCH;\n+    }\n+    if (_cpuid_info.sef_cpuid7_ebx.bits.clwb != 0) {\n+      result |= CPU_CLWB;\n+    }\n+    if (_cpuid_info.sef_cpuid7_edx.bits.serialize != 0)\n+      result |= CPU_SERIALIZE;\n+  }\n+\n+  \/\/ ZX features.\n+  if (is_zx()) {\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0) {\n+      result |= CPU_LZCNT;\n+    }\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0) {\n+      result |= CPU_3DNOW_PREFETCH;\n+    }\n+  }\n+\n+  \/\/ Composite features.\n+  if (supports_tscinv_bit() &&\n+      ((is_amd_family() && !is_amd_Barcelona()) ||\n+       is_intel_tsc_synched_at_init())) {\n+    result |= CPU_TSCINV;\n+  }\n+\n+  return result;\n+}\n+\n+bool VM_Version::os_supports_avx_vectors() {\n+  bool retVal = false;\n+  int nreg = 2 LP64_ONLY(+2);\n+  if (supports_evex()) {\n+    \/\/ Verify that OS save\/restore all bits of EVEX registers\n+    \/\/ during signal processing.\n+    retVal = true;\n+    for (int i = 0; i < 16 * nreg; i++) { \/\/ 64 bytes per zmm register\n+      if (_cpuid_info.zmm_save[i] != ymm_test_value()) {\n+        retVal = false;\n+        break;\n+      }\n+    }\n+  } else if (supports_avx()) {\n+    \/\/ Verify that OS save\/restore all bits of AVX registers\n+    \/\/ during signal processing.\n+    retVal = true;\n+    for (int i = 0; i < 8 * nreg; i++) { \/\/ 32 bytes per ymm register\n+      if (_cpuid_info.ymm_save[i] != ymm_test_value()) {\n+        retVal = false;\n+        break;\n+      }\n+    }\n+    \/\/ zmm_save will be set on a EVEX enabled machine even if we choose AVX code gen\n+    if (retVal == false) {\n+      \/\/ Verify that OS save\/restore all bits of EVEX registers\n+      \/\/ during signal processing.\n+      retVal = true;\n+      for (int i = 0; i < 16 * nreg; i++) { \/\/ 64 bytes per zmm register\n+        if (_cpuid_info.zmm_save[i] != ymm_test_value()) {\n+          retVal = false;\n+          break;\n+        }\n+      }\n+    }\n+  }\n+  return retVal;\n+}\n+\n+uint VM_Version::cores_per_cpu() {\n+  uint result = 1;\n+  if (is_intel()) {\n+    bool supports_topology = supports_processor_topology();\n+    if (supports_topology) {\n+      result = _cpuid_info.tpl_cpuidB1_ebx.bits.logical_cpus \/\n+               _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n+    }\n+    if (!supports_topology || result == 0) {\n+      result = (_cpuid_info.dcp_cpuid4_eax.bits.cores_per_cpu + 1);\n+    }\n+  } else if (is_amd_family()) {\n+    result = (_cpuid_info.ext_cpuid8_ecx.bits.cores_per_cpu + 1);\n+  } else if (is_zx()) {\n+    bool supports_topology = supports_processor_topology();\n+    if (supports_topology) {\n+      result = _cpuid_info.tpl_cpuidB1_ebx.bits.logical_cpus \/\n+               _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n+    }\n+    if (!supports_topology || result == 0) {\n+      result = (_cpuid_info.dcp_cpuid4_eax.bits.cores_per_cpu + 1);\n+    }\n+  }\n+  return result;\n+}\n+\n+uint VM_Version::threads_per_core() {\n+  uint result = 1;\n+  if (is_intel() && supports_processor_topology()) {\n+    result = _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n+  } else if (is_zx() && supports_processor_topology()) {\n+    result = _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n+  } else if (_cpuid_info.std_cpuid1_edx.bits.ht != 0) {\n+    if (cpu_family() >= 0x17) {\n+      result = _cpuid_info.ext_cpuid1E_ebx.bits.threads_per_core + 1;\n+    } else {\n+      result = _cpuid_info.std_cpuid1_ebx.bits.threads_per_cpu \/\n+                 cores_per_cpu();\n+    }\n+  }\n+  return (result == 0 ? 1 : result);\n+}\n+\n+intx VM_Version::L1_line_size() {\n+  intx result = 0;\n+  if (is_intel()) {\n+    result = (_cpuid_info.dcp_cpuid4_ebx.bits.L1_line_size + 1);\n+  } else if (is_amd_family()) {\n+    result = _cpuid_info.ext_cpuid5_ecx.bits.L1_line_size;\n+  } else if (is_zx()) {\n+    result = (_cpuid_info.dcp_cpuid4_ebx.bits.L1_line_size + 1);\n+  }\n+  if (result < 32) \/\/ not defined ?\n+    result = 32;   \/\/ 32 bytes by default on x86 and other x64\n+  return result;\n+}\n+\n+bool VM_Version::is_intel_tsc_synched_at_init() {\n+  if (is_intel_family_core()) {\n+    uint32_t ext_model = extended_cpu_model();\n+    if (ext_model == CPU_MODEL_NEHALEM_EP     ||\n+        ext_model == CPU_MODEL_WESTMERE_EP    ||\n+        ext_model == CPU_MODEL_SANDYBRIDGE_EP ||\n+        ext_model == CPU_MODEL_IVYBRIDGE_EP) {\n+      \/\/ <= 2-socket invariant tsc support. EX versions are usually used\n+      \/\/ in > 2-socket systems and likely don't synchronize tscs at\n+      \/\/ initialization.\n+      \/\/ Code that uses tsc values must be prepared for them to arbitrarily\n+      \/\/ jump forward or backward.\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+intx VM_Version::allocate_prefetch_distance(bool use_watermark_prefetch) {\n+  \/\/ Hardware prefetching (distance\/size in bytes):\n+  \/\/ Pentium 3 -  64 \/  32\n+  \/\/ Pentium 4 - 256 \/ 128\n+  \/\/ Athlon    -  64 \/  32 ????\n+  \/\/ Opteron   - 128 \/  64 only when 2 sequential cache lines accessed\n+  \/\/ Core      - 128 \/  64\n+  \/\/\n+  \/\/ Software prefetching (distance in bytes \/ instruction with best score):\n+  \/\/ Pentium 3 - 128 \/ prefetchnta\n+  \/\/ Pentium 4 - 512 \/ prefetchnta\n+  \/\/ Athlon    - 128 \/ prefetchnta\n+  \/\/ Opteron   - 256 \/ prefetchnta\n+  \/\/ Core      - 256 \/ prefetchnta\n+  \/\/ It will be used only when AllocatePrefetchStyle > 0\n+\n+  if (is_amd_family()) { \/\/ AMD | Hygon\n+    if (supports_sse2()) {\n+      return 256; \/\/ Opteron\n+    } else {\n+      return 128; \/\/ Athlon\n+    }\n+  } else { \/\/ Intel\n+    if (supports_sse3() && cpu_family() == 6) {\n+      if (supports_sse4_2() && supports_ht()) { \/\/ Nehalem based cpus\n+        return 192;\n+      } else if (use_watermark_prefetch) { \/\/ watermark prefetching on Core\n+#ifdef _LP64\n+        return 384;\n+#else\n+        return 320;\n+#endif\n+      }\n+    }\n+    if (supports_sse2()) {\n+      if (cpu_family() == 6) {\n+        return 256; \/\/ Pentium M, Core, Core2\n+      } else {\n+        return 512; \/\/ Pentium 4\n+      }\n+    } else {\n+      return 128; \/\/ Pentium 3 (and all other old CPUs)\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":318,"deletions":0,"binary":false,"changes":318,"status":"modified"},{"patch":"@@ -511,0 +511,1 @@\n+private:\n@@ -539,198 +540,2 @@\n-  static uint64_t feature_flags() {\n-    uint64_t result = 0;\n-    if (_cpuid_info.std_cpuid1_edx.bits.cmpxchg8 != 0)\n-      result |= CPU_CX8;\n-    if (_cpuid_info.std_cpuid1_edx.bits.cmov != 0)\n-      result |= CPU_CMOV;\n-    if (_cpuid_info.std_cpuid1_edx.bits.clflush != 0)\n-      result |= CPU_FLUSH;\n-#ifdef _LP64\n-    \/\/ clflush should always be available on x86_64\n-    \/\/ if not we are in real trouble because we rely on it\n-    \/\/ to flush the code cache.\n-    assert ((result & CPU_FLUSH) != 0, \"clflush should be available\");\n-#endif\n-    if (_cpuid_info.std_cpuid1_edx.bits.fxsr != 0 || (is_amd_family() &&\n-        _cpuid_info.ext_cpuid1_edx.bits.fxsr != 0))\n-      result |= CPU_FXSR;\n-    \/\/ HT flag is set for multi-core processors also.\n-    if (threads_per_core() > 1)\n-      result |= CPU_HT;\n-    if (_cpuid_info.std_cpuid1_edx.bits.mmx != 0 || (is_amd_family() &&\n-        _cpuid_info.ext_cpuid1_edx.bits.mmx != 0))\n-      result |= CPU_MMX;\n-    if (_cpuid_info.std_cpuid1_edx.bits.sse != 0)\n-      result |= CPU_SSE;\n-    if (_cpuid_info.std_cpuid1_edx.bits.sse2 != 0)\n-      result |= CPU_SSE2;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.sse3 != 0)\n-      result |= CPU_SSE3;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.ssse3 != 0)\n-      result |= CPU_SSSE3;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.sse4_1 != 0)\n-      result |= CPU_SSE4_1;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.sse4_2 != 0)\n-      result |= CPU_SSE4_2;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.popcnt != 0)\n-      result |= CPU_POPCNT;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.avx != 0 &&\n-        _cpuid_info.std_cpuid1_ecx.bits.osxsave != 0 &&\n-        _cpuid_info.xem_xcr0_eax.bits.sse != 0 &&\n-        _cpuid_info.xem_xcr0_eax.bits.ymm != 0) {\n-      result |= CPU_AVX;\n-      result |= CPU_VZEROUPPER;\n-      if (_cpuid_info.sef_cpuid7_ebx.bits.avx2 != 0)\n-        result |= CPU_AVX2;\n-      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512f != 0 &&\n-          _cpuid_info.xem_xcr0_eax.bits.opmask != 0 &&\n-          _cpuid_info.xem_xcr0_eax.bits.zmm512 != 0 &&\n-          _cpuid_info.xem_xcr0_eax.bits.zmm32 != 0) {\n-        result |= CPU_AVX512F;\n-        if (_cpuid_info.sef_cpuid7_ebx.bits.avx512cd != 0)\n-          result |= CPU_AVX512CD;\n-        if (_cpuid_info.sef_cpuid7_ebx.bits.avx512dq != 0)\n-          result |= CPU_AVX512DQ;\n-        if (_cpuid_info.sef_cpuid7_ebx.bits.avx512pf != 0)\n-          result |= CPU_AVX512PF;\n-        if (_cpuid_info.sef_cpuid7_ebx.bits.avx512er != 0)\n-          result |= CPU_AVX512ER;\n-        if (_cpuid_info.sef_cpuid7_ebx.bits.avx512bw != 0)\n-          result |= CPU_AVX512BW;\n-        if (_cpuid_info.sef_cpuid7_ebx.bits.avx512vl != 0)\n-          result |= CPU_AVX512VL;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vpopcntdq != 0)\n-          result |= CPU_AVX512_VPOPCNTDQ;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vpclmulqdq != 0)\n-          result |= CPU_AVX512_VPCLMULQDQ;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.vaes != 0)\n-          result |= CPU_AVX512_VAES;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.gfni != 0)\n-          result |= CPU_GFNI;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vnni != 0)\n-          result |= CPU_AVX512_VNNI;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_bitalg != 0)\n-          result |= CPU_AVX512_BITALG;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vbmi != 0)\n-          result |= CPU_AVX512_VBMI;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vbmi2 != 0)\n-          result |= CPU_AVX512_VBMI2;\n-      }\n-    }\n-    if (_cpuid_info.std_cpuid1_ecx.bits.hv != 0)\n-      result |= CPU_HV;\n-    if (_cpuid_info.sef_cpuid7_ebx.bits.bmi1 != 0)\n-      result |= CPU_BMI1;\n-    if (_cpuid_info.std_cpuid1_edx.bits.tsc != 0)\n-      result |= CPU_TSC;\n-    if (_cpuid_info.ext_cpuid7_edx.bits.tsc_invariance != 0)\n-      result |= CPU_TSCINV_BIT;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.aes != 0)\n-      result |= CPU_AES;\n-    if (_cpuid_info.sef_cpuid7_ebx.bits.erms != 0)\n-      result |= CPU_ERMS;\n-    if (_cpuid_info.sef_cpuid7_edx.bits.fast_short_rep_mov != 0)\n-      result |= CPU_FSRM;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.clmul != 0)\n-      result |= CPU_CLMUL;\n-    if (_cpuid_info.sef_cpuid7_ebx.bits.rtm != 0)\n-      result |= CPU_RTM;\n-    if (_cpuid_info.sef_cpuid7_ebx.bits.adx != 0)\n-       result |= CPU_ADX;\n-    if (_cpuid_info.sef_cpuid7_ebx.bits.bmi2 != 0)\n-      result |= CPU_BMI2;\n-    if (_cpuid_info.sef_cpuid7_ebx.bits.sha != 0)\n-      result |= CPU_SHA;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.fma != 0)\n-      result |= CPU_FMA;\n-    if (_cpuid_info.sef_cpuid7_ebx.bits.clflushopt != 0)\n-      result |= CPU_FLUSHOPT;\n-    if (_cpuid_info.ext_cpuid1_edx.bits.rdtscp != 0)\n-      result |= CPU_RDTSCP;\n-    if (_cpuid_info.sef_cpuid7_ecx.bits.rdpid != 0)\n-      result |= CPU_RDPID;\n-\n-    \/\/ AMD|Hygon features.\n-    if (is_amd_family()) {\n-      if ((_cpuid_info.ext_cpuid1_edx.bits.tdnow != 0) ||\n-          (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0))\n-        result |= CPU_3DNOW_PREFETCH;\n-      if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0)\n-        result |= CPU_LZCNT;\n-      if (_cpuid_info.ext_cpuid1_ecx.bits.sse4a != 0)\n-        result |= CPU_SSE4A;\n-    }\n-\n-    \/\/ Intel features.\n-    if (is_intel()) {\n-      if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0) {\n-        result |= CPU_LZCNT;\n-      }\n-      if (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0) {\n-        result |= CPU_3DNOW_PREFETCH;\n-      }\n-      if (_cpuid_info.sef_cpuid7_ebx.bits.clwb != 0) {\n-        result |= CPU_CLWB;\n-      }\n-      if (_cpuid_info.sef_cpuid7_edx.bits.serialize != 0)\n-        result |= CPU_SERIALIZE;\n-    }\n-\n-    \/\/ ZX features.\n-    if (is_zx()) {\n-      if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0) {\n-        result |= CPU_LZCNT;\n-      }\n-      if (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0) {\n-        result |= CPU_3DNOW_PREFETCH;\n-      }\n-    }\n-\n-    \/\/ Composite features.\n-    if (supports_tscinv_bit() &&\n-        ((is_amd_family() && !is_amd_Barcelona()) ||\n-         is_intel_tsc_synched_at_init())) {\n-      result |= CPU_TSCINV;\n-    }\n-\n-    return result;\n-  }\n-\n-  static bool os_supports_avx_vectors() {\n-    bool retVal = false;\n-    int nreg = 2 LP64_ONLY(+2);\n-    if (supports_evex()) {\n-      \/\/ Verify that OS save\/restore all bits of EVEX registers\n-      \/\/ during signal processing.\n-      retVal = true;\n-      for (int i = 0; i < 16 * nreg; i++) { \/\/ 64 bytes per zmm register\n-        if (_cpuid_info.zmm_save[i] != ymm_test_value()) {\n-          retVal = false;\n-          break;\n-        }\n-      }\n-    } else if (supports_avx()) {\n-      \/\/ Verify that OS save\/restore all bits of AVX registers\n-      \/\/ during signal processing.\n-      retVal = true;\n-      for (int i = 0; i < 8 * nreg; i++) { \/\/ 32 bytes per ymm register\n-        if (_cpuid_info.ymm_save[i] != ymm_test_value()) {\n-          retVal = false;\n-          break;\n-        }\n-      }\n-      \/\/ zmm_save will be set on a EVEX enabled machine even if we choose AVX code gen\n-      if (retVal == false) {\n-        \/\/ Verify that OS save\/restore all bits of EVEX registers\n-        \/\/ during signal processing.\n-        retVal = true;\n-        for (int i = 0; i < 16 * nreg; i++) { \/\/ 64 bytes per zmm register\n-          if (_cpuid_info.zmm_save[i] != ymm_test_value()) {\n-            retVal = false;\n-            break;\n-          }\n-        }\n-      }\n-    }\n-    return retVal;\n-  }\n-\n+  static uint64_t feature_flags();\n+  static bool os_supports_avx_vectors();\n@@ -770,1 +575,0 @@\n-\n@@ -814,56 +618,3 @@\n-  static uint cores_per_cpu()  {\n-    uint result = 1;\n-    if (is_intel()) {\n-      bool supports_topology = supports_processor_topology();\n-      if (supports_topology) {\n-        result = _cpuid_info.tpl_cpuidB1_ebx.bits.logical_cpus \/\n-                 _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n-      }\n-      if (!supports_topology || result == 0) {\n-        result = (_cpuid_info.dcp_cpuid4_eax.bits.cores_per_cpu + 1);\n-      }\n-    } else if (is_amd_family()) {\n-      result = (_cpuid_info.ext_cpuid8_ecx.bits.cores_per_cpu + 1);\n-    } else if (is_zx()) {\n-      bool supports_topology = supports_processor_topology();\n-      if (supports_topology) {\n-        result = _cpuid_info.tpl_cpuidB1_ebx.bits.logical_cpus \/\n-                 _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n-      }\n-      if (!supports_topology || result == 0) {\n-        result = (_cpuid_info.dcp_cpuid4_eax.bits.cores_per_cpu + 1);\n-      }\n-    }\n-    return result;\n-  }\n-\n-  static uint threads_per_core()  {\n-    uint result = 1;\n-    if (is_intel() && supports_processor_topology()) {\n-      result = _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n-    } else if (is_zx() && supports_processor_topology()) {\n-      result = _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n-    } else if (_cpuid_info.std_cpuid1_edx.bits.ht != 0) {\n-      if (cpu_family() >= 0x17) {\n-        result = _cpuid_info.ext_cpuid1E_ebx.bits.threads_per_core + 1;\n-      } else {\n-        result = _cpuid_info.std_cpuid1_ebx.bits.threads_per_cpu \/\n-                 cores_per_cpu();\n-      }\n-    }\n-    return (result == 0 ? 1 : result);\n-  }\n-\n-  static intx L1_line_size()  {\n-    intx result = 0;\n-    if (is_intel()) {\n-      result = (_cpuid_info.dcp_cpuid4_ebx.bits.L1_line_size + 1);\n-    } else if (is_amd_family()) {\n-      result = _cpuid_info.ext_cpuid5_ecx.bits.L1_line_size;\n-    } else if (is_zx()) {\n-      result = (_cpuid_info.dcp_cpuid4_ebx.bits.L1_line_size + 1);\n-    }\n-    if (result < 32) \/\/ not defined ?\n-      result = 32;   \/\/ 32 bytes by default on x86 and other x64\n-    return result;\n-  }\n+  static uint cores_per_cpu();\n+  static uint threads_per_core();\n+  static intx L1_line_size();\n@@ -943,17 +694,1 @@\n-  static bool is_intel_tsc_synched_at_init()  {\n-    if (is_intel_family_core()) {\n-      uint32_t ext_model = extended_cpu_model();\n-      if (ext_model == CPU_MODEL_NEHALEM_EP     ||\n-          ext_model == CPU_MODEL_WESTMERE_EP    ||\n-          ext_model == CPU_MODEL_SANDYBRIDGE_EP ||\n-          ext_model == CPU_MODEL_IVYBRIDGE_EP) {\n-        \/\/ <= 2-socket invariant tsc support. EX versions are usually used\n-        \/\/ in > 2-socket systems and likely don't synchronize tscs at\n-        \/\/ initialization.\n-        \/\/ Code that uses tsc values must be prepared for them to arbitrarily\n-        \/\/ jump forward or backward.\n-        return true;\n-      }\n-    }\n-    return false;\n-  }\n+  static bool is_intel_tsc_synched_at_init();\n@@ -989,45 +724,1 @@\n-  static intx allocate_prefetch_distance(bool use_watermark_prefetch) {\n-    \/\/ Hardware prefetching (distance\/size in bytes):\n-    \/\/ Pentium 3 -  64 \/  32\n-    \/\/ Pentium 4 - 256 \/ 128\n-    \/\/ Athlon    -  64 \/  32 ????\n-    \/\/ Opteron   - 128 \/  64 only when 2 sequential cache lines accessed\n-    \/\/ Core      - 128 \/  64\n-    \/\/\n-    \/\/ Software prefetching (distance in bytes \/ instruction with best score):\n-    \/\/ Pentium 3 - 128 \/ prefetchnta\n-    \/\/ Pentium 4 - 512 \/ prefetchnta\n-    \/\/ Athlon    - 128 \/ prefetchnta\n-    \/\/ Opteron   - 256 \/ prefetchnta\n-    \/\/ Core      - 256 \/ prefetchnta\n-    \/\/ It will be used only when AllocatePrefetchStyle > 0\n-\n-    if (is_amd_family()) { \/\/ AMD | Hygon\n-      if (supports_sse2()) {\n-        return 256; \/\/ Opteron\n-      } else {\n-        return 128; \/\/ Athlon\n-      }\n-    } else { \/\/ Intel\n-      if (supports_sse3() && cpu_family() == 6) {\n-        if (supports_sse4_2() && supports_ht()) { \/\/ Nehalem based cpus\n-          return 192;\n-        } else if (use_watermark_prefetch) { \/\/ watermark prefetching on Core\n-#ifdef _LP64\n-          return 384;\n-#else\n-          return 320;\n-#endif\n-        }\n-      }\n-      if (supports_sse2()) {\n-        if (cpu_family() == 6) {\n-          return 256; \/\/ Pentium M, Core, Core2\n-        } else {\n-          return 512; \/\/ Pentium 4\n-        }\n-      } else {\n-        return 128; \/\/ Pentium 3 (and all other old CPUs)\n-      }\n-    }\n-  }\n+  static intx allocate_prefetch_distance(bool use_watermark_prefetch);\n@@ -1066,1 +757,0 @@\n-\n@@ -1071,0 +761,1 @@\n+\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":9,"deletions":318,"binary":false,"changes":327,"status":"modified"}]}