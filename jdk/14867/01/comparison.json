{"files":[{"patch":"@@ -2140,0 +2140,5 @@\n+char* os::pd_attempt_reserve_memory_below(char* max, size_t bytes, size_t alignment,\n+                                          int max_attempts) {\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/os\/aix\/os_aix.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1795,0 +1795,5 @@\n+char* os::pd_attempt_reserve_memory_below(char* max, size_t bytes, size_t alignment,\n+                                          int max_attempts) {\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/os\/bsd\/os_bsd.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -4229,0 +4229,84 @@\n+char* os::pd_attempt_reserve_memory_below(char* max, size_t bytes, size_t alignment,\n+                                          int max_attempts) {\n+\n+  assert(is_aligned(alignment, os::vm_allocation_granularity()), \"alignment unaligned\");\n+  assert(is_aligned(bytes, os::vm_page_size()), \"size unaligned\");\n+\n+  \/\/ Scan \/proc\/self\/maps for an address space hole that is suitable for our reservation. If found,\n+  \/\/ attempt to map into that hole.\n+\n+  log_trace(os, map)(\"attempt_reserve_memory_below \" PTR_FORMAT \", bytes: \" SIZE_FORMAT_X\n+                     \", alignment: \" SIZE_FORMAT_X, p2i(max), bytes, alignment);\n+\n+\/\/ somewhat shorter\n+#define ALGNUP(x) align_up(x, alignment)\n+#define ALGNDWN(x) align_down(x, alignment)\n+\n+  FILE* f = os::fopen(\"\/proc\/self\/maps\", \"r\");\n+  if (f == nullptr) {\n+    return nullptr;\n+  }\n+\n+  char* const min_address = ALGNUP(os::get_lowest_attach_address());\n+  char* const max_address = MIN2(os::get_highest_attach_address(), max);\n+\n+  if (max_address < (min_address + bytes)) {\n+    return nullptr;\n+  }\n+\n+  char* result = nullptr;\n+\n+  char line[512];\n+  size_t len = 0;\n+  char* last_mapping_end = 0;\n+  int stop_after = max_attempts;\n+\n+  while (stop_after-- > 0 && result == nullptr && fgets(line, sizeof(line), f) != nullptr) {\n+\n+    char* mapping_start, *mapping_end;\n+    if (sscanf(line, \"%p-%p\", &mapping_start, &mapping_end) == 2) {\n+\n+      log_trace(os, map)(\"mapped block found [\" PTR_FORMAT \"-\" PTR_FORMAT \")\",\n+                         p2i(mapping_start), p2i(mapping_end));\n+\n+      assert(mapping_start >= last_mapping_end && mapping_end >= mapping_start, \"Sanity\");\n+\n+      \/\/ The hole is preceding this mapping\n+      char* const hole_start = ALGNUP(last_mapping_end);\n+      char* const hole_end = mapping_start;\n+\n+      if ((hole_start + bytes) > max_address) {\n+        break; \/\/ No need to continue\n+      }\n+\n+      char* const intersected_start = MAX2(hole_start, min_address);\n+      char* const intersected_end = MIN2(hole_end, max_address);\n+\n+      if ((intersected_start + bytes) < intersected_end) {\n+        log_trace(os, map)(\"candidate address space hole [\" PTR_FORMAT \"-\" PTR_FORMAT \")\",\n+                           p2i(intersected_start), p2i(intersected_end));\n+        \/\/ Found a candidate hole.\n+        \/\/ Attempt to allocate first at the end, failing that, at the start.\n+        char* const candidate = ALGNDWN(intersected_end - bytes);\n+        char* const candidate2 = intersected_start;\n+        result = os::attempt_reserve_memory_at(candidate, bytes, false);\n+        if (result == nullptr && candidate2 != candidate) {\n+          result = pd_attempt_reserve_memory_at(candidate2, bytes, false);\n+        };\n+        if (result != nullptr) {\n+          log_trace(os, map)(\"successfully reserved [\" PTR_FORMAT \"-\" PTR_FORMAT \")\",\n+                             p2i(result), p2i(result + bytes));\n+        }\n+      }\n+      last_mapping_end = mapping_end;\n+    }\n+  }\n+\n+  fclose(f);\n+\n+#undef ALGNUP\n+#undef ALGNDOWN\n+\n+  return result;\n+}\n+\n","filename":"src\/hotspot\/os\/linux\/os_linux.cpp","additions":84,"deletions":0,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -3392,0 +3392,52 @@\n+char* os::pd_attempt_reserve_memory_below(char* max, size_t bytes, size_t alignment,\n+                                          int max_attempts) {\n+\n+  assert(is_aligned(alignment, os::vm_allocation_granularity()), \"alignment unaligned\");\n+  assert(is_aligned(bytes, os::vm_page_size()), \"size unaligned\");\n+\n+  \/\/ Use find_mapping (ultimately, VirtualQuery) to find an address hole large enough\n+  \/\/ to hold bytes bytes. Give preference to higher addresses. Then attempt to map there.\n+\n+  log_trace(os, map)(\"attempt_reserve_memory_below \" PTR_FORMAT \", bytes: \" SIZE_FORMAT_X\n+                     \", alignment: \" SIZE_FORMAT_X, p2i(max), bytes, alignment);\n+\n+  \/\/ somewhat shorter\n+#define ALGNUP(x) align_up(x, alignment)\n+#define ALGNDWN(x) align_down(x, alignment)\n+\n+  char* const min_address = ALGNUP(os::get_lowest_attach_address());\n+  char* const max_address = MIN2(os::get_highest_attach_address(), max);\n+\n+  if (max_address < (min_address + bytes)) {\n+    return nullptr;\n+  }\n+\n+  char* result = nullptr;\n+  char* candidate = align_down(max_address - bytes, alignment);\n+  int stop_after = max_attempts;\n+\n+  while (result == nullptr && stop_after-- > 0) {\n+    os::win32::mapping_info_t mi;\n+    if (os::win32::find_mapping((address)candidate, &mi)) {\n+      \/\/ We found a mapping. Calculate a new candidate address bordering the lower\n+      \/\/ boundary of that mapping, then retry.\n+      log_trace(os, map)(\"mapped block found [\" PTR_FORMAT \"-\" PTR_FORMAT \")\",\n+                         p2i(mi.base), p2i(mi.base + mi.size));\n+      char* next_candidate = align_down((char*)mi.base - bytes, alignment);\n+      assert(next_candidate < candidate, \"odd result from VirtualQuery\");\n+      candidate = next_candidate;\n+    } else {\n+      result = pd_attempt_reserve_memory_at(candidate, bytes, false);\n+      if (result != nullptr) {\n+        log_trace(os, map)(\"successfully reserved [\" PTR_FORMAT \"-\" PTR_FORMAT \")\",\n+                           p2i(result), p2i(result + bytes));\n+      }\n+    }\n+  }\n+\n+#undef ALGNUP\n+#undef ALGNDOWN\n+\n+  return result;\n+}\n+\n","filename":"src\/hotspot\/os\/windows\/os_windows.cpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -592,0 +592,19 @@\n+  \/\/ First, attempt to reserve the class space such that zero-based addressing is possible.\n+  \/\/ If the heap has also be allocated in low address regions, this will - with a high probability -\n+  \/\/ cause the reserved range to nestle alongside the heap.\n+  {\n+    \/\/ First try for zero-base zero-shift (lower 4G); failing that, try for zero-based with max shift (lower 32G)\n+    constexpr int num_tries = 8;\n+    char* addr = os::attempt_reserve_memory_below((char*)UINT_MAX + 1, size, Metaspace::reserve_alignment(), num_tries);\n+    if (addr == nullptr) {\n+      addr = os::attempt_reserve_memory_below((char*)KlassEncodingMetaspaceMax + 1, size, Metaspace::reserve_alignment(), num_tries);\n+    }\n+    if (addr != nullptr && CompressedKlassPointers::is_valid_base((address)addr)) {\n+      ReservedSpace rs = ReservedSpace::space_for_range(addr, size, Metaspace::reserve_alignment(),\n+                                                        os::vm_page_size(), false, false);\n+      if (rs.is_reserved()) {\n+        return rs;\n+      }\n+    }\n+  }\n+\n@@ -593,1 +612,0 @@\n-  const size_t alignment = Metaspace::reserve_alignment();\n@@ -764,1 +782,0 @@\n-    address base = nullptr;\n@@ -771,1 +788,1 @@\n-      base = (address)CompressedClassSpaceBaseAddress;\n+      address base = (address)CompressedClassSpaceBaseAddress;\n@@ -789,19 +806,1 @@\n-    if (!rs.is_reserved()) {\n-      \/\/ If UseCompressedOops=1 and the java heap has been placed in coops-friendly\n-      \/\/  territory, i.e. its base is under 32G, then we attempt to place ccs\n-      \/\/  right above the java heap.\n-      \/\/ Otherwise the lower 32G are still free. We try to place ccs at the lowest\n-      \/\/ allowed mapping address.\n-      base = (UseCompressedOops && (uint64_t)CompressedOops::base() < OopEncodingHeapMax) ?\n-              CompressedOops::end() : (address)HeapBaseMinAddress;\n-      base = align_up(base, Metaspace::reserve_alignment());\n-\n-      if (base != nullptr) {\n-        if (CompressedKlassPointers::is_valid_base(base)) {\n-          rs = ReservedSpace(size, Metaspace::reserve_alignment(),\n-                             os::vm_page_size(), (char*)base);\n-        }\n-      }\n-    }\n-\n-    \/\/ ...failing that, reserve anywhere, but let platform do optimized placement:\n+    \/\/ ...otherwise let JVM chose the best placing:\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":21,"deletions":22,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -358,0 +358,8 @@\n+\/\/ Put a ReservedSpace over an existing range\n+ReservedSpace ReservedSpace::space_for_range(char* base, size_t size, size_t alignment,\n+                                             size_t page_size, bool special, bool executable) {\n+  ReservedSpace space;\n+  space.initialize_members(base, size, alignment, page_size, special, executable);\n+  return space;\n+}\n+\n@@ -549,2 +557,0 @@\n-    \/\/ But leave room for the compressed class pointers, which is allocated above\n-    \/\/ the heap.\n@@ -552,8 +558,0 @@\n-    const size_t class_space = align_up(CompressedClassSpaceSize, alignment);\n-    \/\/ For small heaps, save some space for compressed class pointer\n-    \/\/ space so it can be decoded with no base.\n-    if (UseCompressedClassPointers && !UseSharedSpaces && !DumpSharedSpaces &&\n-        OopEncodingHeapMax <= KlassEncodingMetaspaceMax &&\n-        (uint64_t)(aligned_heap_base_min_address + size + class_space) <= KlassEncodingMetaspaceMax) {\n-      zerobased_max = (char *)OopEncodingHeapMax - class_space;\n-    }\n","filename":"src\/hotspot\/share\/memory\/virtualspace.cpp","additions":8,"deletions":10,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -110,0 +110,4 @@\n+\n+  \/\/ Put a ReservedSpace over an existing range\n+  static ReservedSpace space_for_range(char* base, size_t size, size_t alignment,\n+                                       size_t page_size, bool special, bool executable);\n","filename":"src\/hotspot\/share\/memory\/virtualspace.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1767,0 +1767,1 @@\n+    log_debug(os)(\"Reserved memory at \" INTPTR_FORMAT \" for \" SIZE_FORMAT \" bytes.\", p2i(addr), bytes);\n@@ -1774,0 +1775,90 @@\n+char* os::get_lowest_attach_address() {\n+  return (char*)os::vm_allocation_granularity();\n+}\n+\n+char* os::get_highest_attach_address() {\n+  return (char*)(\n+#ifdef _LP64\n+  (128 * 1024 * G)\n+#else\n+  SIZE_MAX\n+#endif\n+  - os::vm_page_size());\n+}\n+\n+char* os::attempt_reserve_memory_in_range(char* min, char* max, size_t bytes,\n+                                          size_t alignment, int max_attempts) {\n+  assert(is_power_of_2(alignment), \"alignment not pow2\");\n+  assert(alignment <= 2 * G, \"alignment too large\");\n+  assert(max_attempts > 0, \"at least one attempt\");\n+  assert(is_aligned(bytes, os::vm_page_size()), \"bytes not page_aligned\");\n+\n+  log_trace(os, map)(\"attempt_reserve_memory_in_range \" PTR_FORMAT \"-\" PTR_FORMAT \", bytes: \" SIZE_FORMAT_X\n+                     \", alignment: \" SIZE_FORMAT_X, p2i(min), p2i(max), bytes, alignment);\n+\n+  alignment = align_up(alignment, os::vm_allocation_granularity());\n+\n+  \/\/ lowest, highest possible attach points\n+  char* lo_att = MAX2(min, os::get_lowest_attach_address());\n+  lo_att = align_up(lo_att, alignment);\n+\n+  char* hi_att = MIN2(max, os::get_highest_attach_address());\n+  hi_att -= bytes;\n+  hi_att = align_down(hi_att, alignment);\n+\n+  if (hi_att < lo_att) {\n+    return nullptr;\n+  }\n+\n+  const size_t range_size = hi_att - lo_att + alignment;\n+  const size_t num_attach_points = range_size \/ alignment;\n+  max_attempts = (int) MIN2((size_t)max_attempts, num_attach_points);\n+  const size_t step_size = align_up(range_size \/ max_attempts, alignment);\n+\n+  char* candidate, *result = nullptr;\n+  for (candidate = hi_att;\n+       candidate <= hi_att && candidate >= lo_att && result == nullptr;\n+       candidate -= step_size) {\n+    result = os::attempt_reserve_memory_at(candidate, bytes, false);\n+    if (result == nullptr) {\n+      log_trace(os, map)(\"failed to attach at \" PTR_FORMAT \".\", p2i(candidate));\n+    }\n+  }\n+\n+  if (result != nullptr) {\n+    assert(result >= min && (result + bytes) <= max, \"not in range\");\n+    assert(is_aligned(result, alignment), \"bad alignment\");\n+    log_trace(os, map)(\"successfully attached at [\" PTR_FORMAT \"-\" PTR_FORMAT \").\",\n+                       p2i(result), p2i(result + bytes));\n+  }\n+\n+  return result;\n+}\n+\n+char* os::attempt_reserve_memory_below(char* max, size_t bytes, size_t alignment, int max_attempts) {\n+  assert(is_power_of_2(alignment), \"alignment not pow2\");\n+  assert(alignment <= 2 * G, \"alignment too large\");\n+  assert(max_attempts > 0, \"at least one attempt\");\n+  assert(is_aligned(bytes, os::vm_page_size()), \"bytes not page_aligned\");\n+  alignment = MAX2(alignment, os::vm_allocation_granularity());\n+\n+  \/\/ First let platform try, it may have a better strategy:\n+  char* result = pd_attempt_reserve_memory_below(max, bytes, alignment, max_attempts);\n+\n+  if (result != nullptr) {\n+    MemTracker::record_virtual_memory_reserve((address)result, bytes, CALLER_PC);\n+  }\n+\n+  \/\/ Failing that, use a staggered ladder approach\n+  if (result == nullptr) {\n+    result = attempt_reserve_memory_in_range(nullptr, max, bytes, alignment, max_attempts);\n+  }\n+\n+  if (result != nullptr) {\n+    assert((result + bytes) <= max, \"not in range\");\n+    assert(is_aligned(result, alignment), \"bad alignment\");\n+  }\n+\n+  return result;\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":91,"deletions":0,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -197,0 +197,3 @@\n+  static char*  pd_attempt_reserve_memory_below(char* max, size_t bytes, size_t alignment,\n+                                                int max_attempts);\n+\n@@ -423,0 +426,4 @@\n+  \/\/ Returns the minimal and maximal address one should use for reserving memory\n+  static char* get_lowest_attach_address();\n+  static char* get_highest_attach_address();\n+\n@@ -435,0 +442,9 @@\n+  \/\/ Attempts to reserve memory below a given max address (for zero-based addressing needs) and\n+  \/\/ aligned to a given alignment.\n+  static char*  attempt_reserve_memory_below(char* max, size_t bytes, size_t alignment, int max_attempts);\n+\n+  \/\/ Given an address range [min, max) attempt to reserve memory within this range by probing a number\n+  \/\/ of attach points within this range. The maximum number of attempts is limited with max_attempts.\n+  static char*  attempt_reserve_memory_in_range(char* min, char* max, size_t bytes,\n+                                                size_t alignment, int max_attempts);\n+\n","filename":"src\/hotspot\/share\/runtime\/os.hpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -183,0 +183,5 @@\n+\/\/ Convert pointer to uintptr_t\n+inline uintptr_t p2u(const volatile void* p) {\n+  return (uintptr_t) p;\n+}\n+\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"testutils.hpp\"\n@@ -927,0 +928,163 @@\n+\n+static bool test_attempt_reserve_memory_in_range(char* from, char* to, size_t size,\n+                                                 size_t alignment, int tries) {\n+  bool rc = false;\n+  char* addr = os::attempt_reserve_memory_in_range(from, to, size, alignment, tries);\n+  if (addr != nullptr) {\n+#define ERRINFO \"addr: \" << ((void*)addr) << \" from: \" << ((void*)from) << \" to: \" << ((void*)to) \\\n+                 << \" size: \" << size << \" alignment: \" << alignment\n+    EXPECT_TRUE(is_aligned(addr, alignment)) << ERRINFO;\n+    EXPECT_LE(addr, to - size) << ERRINFO;\n+    EXPECT_LE(addr, os::get_highest_attach_address()) << ERRINFO;\n+    EXPECT_GE(addr, from) << ERRINFO;\n+    EXPECT_GE(addr, os::get_lowest_attach_address()) << ERRINFO;\n+#undef ERRINFO\n+    os::release_memory(addr, size);\n+    rc = true;\n+  }\n+  return rc;\n+}\n+\n+static bool test_attempt_reserve_memory_below(char* limit, size_t size,\n+                                              size_t alignment, int tries) {\n+  bool rc = false;\n+  char* addr = os::attempt_reserve_memory_below(limit, size, alignment, tries);\n+  if (addr != nullptr) {\n+#define ERRINFO \"addr: \" << ((void*)addr) << \"limit: \" << ((void*)limit) << \" size: \" << size << \" alignment: \" << alignment\n+    EXPECT_TRUE(is_aligned(addr, alignment)) << ERRINFO;\n+    EXPECT_LE(addr, limit - size) << ERRINFO;\n+    EXPECT_LT(addr, os::get_highest_attach_address()) << ERRINFO;\n+    EXPECT_GE(addr, os::get_lowest_attach_address()) << ERRINFO;\n+#undef ERRINFO\n+    os::release_memory(addr, size);\n+    rc = true;\n+  }\n+  return rc;\n+}\n+\n+TEST_VM(os, attempt_reserve_memory_in_below_or_in_range) {\n+\n+  const size_t pagesize = os::vm_page_size();\n+  static const struct { uintptr_t from; uintptr_t to; } some_ranges [] = {\n+      { 0, SIZE_MAX }, \/\/ everything\n+      { G, 4 * G },\n+      { G + 16, (4 * G) - 16 }, \/\/ not page aligned\n+#ifdef _LP64\n+      { nth_bit(32), nth_bit(48) },\n+#endif\n+      { 0, 0 } \/\/ last\n+  };\n+\n+  int num_success = 0;\n+\n+  for (int n = 0; some_ranges[n].to != 0; n++) {\n+    char* const from = (char*)(some_ranges[n].from);\n+    char* const to = (char*)(some_ranges[n].to);\n+    const size_t range_size = to - from;\n+    for (size_t size = os::vm_page_size(); size < MIN2(range_size, 128 * M); size *= 13) {\n+      for (size_t alignment = os::vm_page_size(); alignment < MIN2(range_size, 128 * M); alignment *= 8) {\n+        for (int tries = 1; tries <= 64; tries *= 8) {\n+          bool rc1 = test_attempt_reserve_memory_in_range(from, to, size, alignment, tries);\n+          bool rc2 = test_attempt_reserve_memory_below(to, size, alignment, tries);\n+          num_success += (rc1 && rc2) ? 1 : 0;\n+        }\n+      }\n+    }\n+  }\n+  ASSERT_GE(num_success, 10);\n+}\n+\n+TEST_VM(os, attempt_reserve_memory_in_range_cornercases) {\n+  char* const min = (char*)(128 * M);\n+  char* const max = min + M;\n+\n+  char* p = os::attempt_reserve_memory_at(min, M, false);\n+  if (p != nullptr) {\n+\n+    \/\/ For the duration of the test, we optimistically assume this range stays reservable.\n+\n+    assert(p == min, \"Sanity\");\n+    os::release_memory(p, M);\n+\n+    \/\/ 1) reservation of 1 M with 1 M alignment has exactly one fit in the range\n+    p = os::attempt_reserve_memory_in_range(min, max, M, M, 1);\n+    ASSERT_EQ(p, min);\n+    os::release_memory(p, M);\n+\n+    \/\/ 2) reservation of just one page, but with 1 M alignment, still has exactly one fit in the range\n+    p = os::attempt_reserve_memory_in_range(min, max, os::vm_page_size(), M, 1);\n+    ASSERT_EQ(p, min);\n+    os::release_memory(p, os::vm_page_size());\n+\n+    \/\/ 3) reservation of 1 M + page will not fit\n+    p = os::attempt_reserve_memory_in_range(min, max, M + os::vm_page_size(), 1, 1);\n+    ASSERT_NULL(p);\n+\n+    \/\/ 4) reservation of page with larger than 128M alignment will not fit\n+    p = os::attempt_reserve_memory_in_range(min, max, os::vm_page_size(), 256 * M, 1);\n+    ASSERT_NULL(p);\n+\n+  } else {\n+    tty->print_cr(\"Failed to reserve block; skipping test\");\n+  }\n+}\n+\n+#if defined(LINUX) || defined(_WIN32)\n+TEST_VM(os, attempt_reserve_memory_in_below_2) {\n+\n+  \/\/ First find the lowest mapped block in the process' address space. The mechanism\n+  \/\/ differs between Windows and Linux.\n+  char* first_mapping_start = nullptr;\n+\n+#ifdef LINUX\n+  \/\/ Do this test only if we have a large enough free area in lower address regions\n+  {\n+    FILE* f = os::fopen(\"\/proc\/self\/maps\", \"r\");\n+    ASSERT_NOT_NULL(f);\n+    ASSERT_EQ(::fscanf(f, \"%p-\", &first_mapping_start), 1);\n+    ::fclose(f);\n+  }\n+#endif\n+#ifdef _WIN32\n+  {\n+    char* p = os::get_lowest_attach_address();\n+    MEMORY_BASIC_INFORMATION minfo;\n+    memset(&minfo, 0, sizeof(minfo));\n+    if (::VirtualQuery(p, &minfo, sizeof(minfo))) {\n+      if (minfo.State == MEM_FREE) {\n+        \/\/ Note: for free regions, most of MEMORY_BASIC_INFORMATION is undefined.\n+        \/\/  Only region dimensions are not: use those to jump to the end of\n+        \/\/  the free range.\n+        first_mapping_start = (char*)minfo.BaseAddress + minfo.RegionSize;\n+      }\n+    }\n+  }\n+#endif\n+\n+  if (p2u(first_mapping_start) >= (128 * M)) {\n+    \/\/ address space hole large enough, the following should work\n+    char* const limit = first_mapping_start + os::vm_page_size(); \/\/ limit in the middle of the first mapping.\n+    char* p1 = os::attempt_reserve_memory_below(limit, M, M, 8);\n+    \/\/ it should have attempted first to allocate adjacent to the mapping\n+    char* expected_mapping_address = align_down(first_mapping_start - M, M);\n+    EXPECT_EQ(expected_mapping_address, p1);\n+\n+    \/\/ Attempt again, this time it should allocate adjacent to the last segment.\n+    char* p2 = os::attempt_reserve_memory_below(limit, M, M, 8);\n+    expected_mapping_address = p1 - M;\n+    EXPECT_EQ(expected_mapping_address, p2);\n+\n+    \/\/ Attempt again, this time with a higher alignment.\n+    char* p3 = os::attempt_reserve_memory_below(limit, M, 64 * M, 8);\n+    expected_mapping_address = align_down(p2 - M, 64 * M);\n+    EXPECT_EQ(expected_mapping_address, p3);\n+\n+    os::release_memory(p1, M);\n+    os::release_memory(p2, M);\n+    os::release_memory(p3, M);\n+  } else {\n+    tty->print_cr(\"Lower address range too populated (lowest mapping found \"\n+                  \"at @\" PTR_FORMAT \"), skipping test\", p2i(first_mapping_start));\n+  }\n+}\n+#endif \/\/ LINUX || WINDOWS\n","filename":"test\/hotspot\/gtest\/runtime\/test_os.cpp","additions":164,"deletions":0,"binary":false,"changes":164,"status":"modified"}]}