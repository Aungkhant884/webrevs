{"files":[{"patch":"@@ -1001,1 +1001,0 @@\n-  ContinuationEntry::setup_oopmap(map);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -876,1 +876,0 @@\n-  ContinuationEntry::setup_oopmap(map);\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1296,1 +1296,0 @@\n-  ContinuationEntry::setup_oopmap(map);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1911,1 +1911,1 @@\n-        if (ce == nullptr || ce->cont_oop() != java_lang_VirtualThread::continuation(_java_thread())) {\n+        if (ce == nullptr || ce->cont_oop(thread) != java_lang_VirtualThread::continuation(_java_thread())) {\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+          NULL \/* barrier_set_stack_chunk *\/,\n","filename":"src\/hotspot\/share\/gc\/epsilon\/epsilonBarrierSet.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/barrierSetStackChunk.hpp\"\n@@ -65,0 +66,8 @@\n+static BarrierSetStackChunk* select_barrier_set_stack_chunk(BarrierSetStackChunk* barrier_set_stack_chunk) {\n+  if (barrier_set_stack_chunk != NULL) {\n+    return barrier_set_stack_chunk;\n+  } else {\n+    return new BarrierSetStackChunk();\n+  }\n+}\n+\n@@ -69,0 +78,1 @@\n+                       BarrierSetStackChunk* barrier_set_stack_chunk,\n@@ -74,1 +84,2 @@\n-    _barrier_set_nmethod(select_barrier_set_nmethod(barrier_set_nmethod)) {\n+    _barrier_set_nmethod(select_barrier_set_nmethod(barrier_set_nmethod)),\n+    _barrier_set_stack_chunk(select_barrier_set_stack_chunk(barrier_set_stack_chunk)) {\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSet.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+class BarrierSetStackChunk;\n@@ -77,0 +78,1 @@\n+  BarrierSetStackChunk* _barrier_set_stack_chunk;\n@@ -101,0 +103,1 @@\n+             BarrierSetStackChunk* barrier_set_stack_chunk,\n@@ -168,0 +171,5 @@\n+  BarrierSetStackChunk* barrier_set_stack_chunk() {\n+    assert(_barrier_set_stack_chunk != NULL, \"should be set\");\n+    return _barrier_set_stack_chunk;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSet.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,100 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/barrierSetStackChunk.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/access.inline.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/stackChunkOop.inline.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+class UncompressOopsOopClosure : public OopClosure {\n+public:\n+  void do_oop(oop* p) override {\n+    assert(UseCompressedOops, \"Only needed with compressed oops\");\n+    oop obj = CompressedOops::decode(*(narrowOop*)p);\n+    assert(obj == nullptr || dbg_is_good_oop(obj), \"p: \" INTPTR_FORMAT \" obj: \" INTPTR_FORMAT, p2i(p), p2i((oopDesc*)obj));\n+    *p = obj;\n+  }\n+\n+  void do_oop(narrowOop* p) override {}\n+};\n+\n+class CompressOopsOopClosure : public OopClosure {\n+  stackChunkOop _chunk;\n+  BitMapView _bm;\n+\n+  void convert_oop_to_narrowOop(oop* p) {\n+    oop obj = *p;\n+    *p = nullptr;\n+    *(narrowOop*)p = CompressedOops::encode(obj);\n+  }\n+\n+  template <typename T>\n+  void do_oop_work(T* p) {\n+    BitMap::idx_t index = _chunk->bit_index_for(p);\n+    assert(!_bm.at(index), \"must not be set already\");\n+    _bm.set_bit(index);\n+  }\n+\n+public:\n+  CompressOopsOopClosure(stackChunkOop chunk)\n+    : _chunk(chunk), _bm(chunk->bitmap()) {}\n+\n+  virtual void do_oop(oop* p) override {\n+    if (UseCompressedOops) {\n+      \/\/ Convert all oops to narrow before marking the oop in the bitmap.\n+      convert_oop_to_narrowOop(p);\n+      do_oop_work((narrowOop*)p);\n+    } else {\n+      do_oop_work(p);\n+    }\n+  }\n+\n+  virtual void do_oop(narrowOop* p) override {\n+    do_oop_work(p);\n+  }\n+};\n+\n+void BarrierSetStackChunk::encode_gc_mode(stackChunkOop chunk, OopIterator* iterator) {\n+  CompressOopsOopClosure cl(chunk);\n+  iterator->oops_do(&cl);\n+}\n+\n+void BarrierSetStackChunk::decode_gc_mode(stackChunkOop chunk, OopIterator* iterator) {\n+  if (chunk->has_bitmap() && UseCompressedOops) {\n+    UncompressOopsOopClosure cl;\n+    iterator->oops_do(&cl);\n+  }\n+}\n+\n+oop BarrierSetStackChunk::load_oop(stackChunkOop chunk, oop* addr) {\n+  return RawAccess<>::oop_load(addr);\n+}\n+\n+oop BarrierSetStackChunk::load_oop(stackChunkOop chunk, narrowOop* addr) {\n+  return RawAccess<>::oop_load(addr);\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetStackChunk.cpp","additions":100,"deletions":0,"binary":false,"changes":100,"status":"added"},{"patch":"@@ -0,0 +1,44 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_BARRIERSETSTACKCHUNK_HPP\n+#define SHARE_GC_SHARED_BARRIERSETSTACKCHUNK_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class OopClosure;\n+\n+class BarrierSetStackChunk: public CHeapObj<mtGC> {\n+public:\n+  virtual void encode_gc_mode(stackChunkOop chunk, OopIterator* oop_iterator);\n+  virtual void decode_gc_mode(stackChunkOop chunk, OopIterator* oop_iterator);\n+\n+  virtual oop load_oop(stackChunkOop chunk, oop* addr);\n+  virtual oop load_oop(stackChunkOop chunk, narrowOop* addr);\n+};\n+\n+#endif \/\/ SHARE_GC_SHARED_BARRIERSETSTACKCHUNK_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetStackChunk.hpp","additions":44,"deletions":0,"binary":false,"changes":44,"status":"added"},{"patch":"@@ -256,1 +256,1 @@\n-HeapWord* MemAllocator::allocate_outside_tlab(Allocation& allocation) const {\n+HeapWord* MemAllocator::mem_allocate_outside_tlab(Allocation& allocation) const {\n@@ -270,1 +270,1 @@\n-HeapWord* MemAllocator::allocate_inside_tlab(Allocation& allocation) const {\n+HeapWord* MemAllocator::mem_allocate_inside_tlab(Allocation& allocation) const {\n@@ -274,1 +274,1 @@\n-  HeapWord* mem = allocate_inside_tlab_fast();\n+  HeapWord* mem = mem_allocate_inside_tlab_fast();\n@@ -280,1 +280,1 @@\n-  return allocate_inside_tlab_slow(allocation);\n+  return mem_allocate_inside_tlab_slow(allocation);\n@@ -283,1 +283,1 @@\n-HeapWord* MemAllocator::allocate_inside_tlab_fast() const {\n+HeapWord* MemAllocator::mem_allocate_inside_tlab_fast() const {\n@@ -287,1 +287,1 @@\n-HeapWord* MemAllocator::allocate_inside_tlab_slow(Allocation& allocation) const {\n+HeapWord* MemAllocator::mem_allocate_inside_tlab_slow(Allocation& allocation) const {\n@@ -354,1 +354,5 @@\n-HeapWord* MemAllocator::mem_allocate(Allocation& allocation) const {\n+\n+HeapWord* MemAllocator::mem_allocate_slow(Allocation& allocation) const {\n+  \/\/ Allocation of an oop can always invoke a safepoint.\n+  debug_only(JavaThread::cast(_thread)->check_for_valid_safepoint_state());\n+\n@@ -356,3 +360,4 @@\n-    HeapWord* result = allocate_inside_tlab(allocation);\n-    if (result != NULL) {\n-      return result;\n+    \/\/ Try refilling the TLAB and allocating the object in it.\n+    HeapWord* mem = mem_allocate_inside_tlab_slow(allocation);\n+    if (mem != NULL) {\n+      return mem;\n@@ -362,1 +367,1 @@\n-  return allocate_outside_tlab(allocation);\n+  return mem_allocate_outside_tlab(allocation);\n@@ -365,5 +370,4 @@\n-oop MemAllocator::allocate() const {\n-  oop obj = NULL;\n-  {\n-    Allocation allocation(*this, &obj);\n-    HeapWord* mem = mem_allocate(allocation);\n+HeapWord* MemAllocator::mem_allocate(Allocation& allocation) const {\n+  if (UseTLAB) {\n+    \/\/ Try allocating from an existing TLAB.\n+    HeapWord* mem = mem_allocate_inside_tlab_fast();\n@@ -371,5 +375,1 @@\n-      obj = initialize(mem);\n-    } else {\n-      \/\/ The unhandled oop detector will poison local variable obj,\n-      \/\/ so reset it to NULL if mem is NULL.\n-      obj = NULL;\n+      return mem;\n@@ -378,1 +378,2 @@\n-  return obj;\n+\n+  return mem_allocate_slow(allocation);\n@@ -381,1 +382,1 @@\n-oop MemAllocator::try_allocate_in_existing_tlab() {\n+oop MemAllocator::allocate() const {\n@@ -384,1 +385,2 @@\n-    HeapWord* mem = allocate_inside_tlab_fast();\n+    Allocation allocation(*this, &obj);\n+    HeapWord* mem = mem_allocate(allocation);\n@@ -450,17 +452,0 @@\n-\n-\/\/ Does the minimal amount of initialization needed for a TLAB allocation.\n-\/\/ We don't need to do a full initialization, as such an allocation need not be immediately walkable.\n-oop StackChunkAllocator::initialize(HeapWord* mem) const {\n-  assert(_stack_size > 0, \"\");\n-  assert(_stack_size <= max_jint, \"\");\n-  assert(_word_size > _stack_size, \"\");\n-\n-  \/\/ zero out fields (but not the stack)\n-  const size_t hs = oopDesc::header_size();\n-  Copy::fill_to_aligned_words(mem + hs, vmClasses::StackChunk_klass()->size_helper() - hs);\n-\n-  jdk_internal_vm_StackChunk::set_size(mem, (int)_stack_size);\n-  jdk_internal_vm_StackChunk::set_sp(mem, (int)_stack_size);\n-\n-  return finish(mem);\n-}\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":26,"deletions":41,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -45,0 +45,3 @@\n+  \/\/ Allocate from the current thread's TLAB, without taking a new TLAB (no safepoint).\n+ HeapWord* mem_allocate_inside_tlab_fast() const;\n+\n@@ -46,5 +49,9 @@\n-  \/\/ Allocate from the current thread's TLAB, with broken-out slow path.\n-  HeapWord* allocate_inside_tlab(Allocation& allocation) const;\n-  HeapWord* allocate_inside_tlab_fast() const;\n-  HeapWord* allocate_inside_tlab_slow(Allocation& allocation) const;\n-  HeapWord* allocate_outside_tlab(Allocation& allocation) const;\n+  \/\/ Allocate in a TLAB. Could allocate a new TLAB, and therefore potentially safepoint.\n+  HeapWord* mem_allocate_inside_tlab(Allocation& allocation) const;\n+  HeapWord* mem_allocate_inside_tlab_slow(Allocation& allocation) const;\n+\n+  \/\/ Allocate outside a TLAB. Could safepoint.\n+  HeapWord* mem_allocate_outside_tlab(Allocation& allocation) const;\n+\n+  \/\/ Fast-path TLAB allocation failed. Takes a slow-path and potentially safepoint.\n+  HeapWord* mem_allocate_slow(Allocation& allocation) const;\n@@ -88,0 +95,1 @@\n+\n@@ -89,0 +97,2 @@\n+\n+  using MemAllocator::allocate;\n@@ -104,0 +114,1 @@\n+\n@@ -105,0 +116,2 @@\n+\n+  using MemAllocator::allocate;\n@@ -111,5 +124,0 @@\n-  virtual oop initialize(HeapWord* mem) const;\n-};\n-\n-class StackChunkAllocator : public MemAllocator {\n-  const size_t _stack_size;\n@@ -117,4 +125,0 @@\n-public:\n-  StackChunkAllocator(Klass* klass, size_t word_size, size_t stack_size, Thread* thread = Thread::current())\n-    : MemAllocator(klass, word_size, thread),\n-      _stack_size(stack_size) {}\n@@ -122,0 +126,2 @@\n+\n+  using MemAllocator::allocate;\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.hpp","additions":20,"deletions":14,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+                 NULL \/* barrier_set_stack_chunk *\/,\n","filename":"src\/hotspot\/share\/gc\/shared\/modRefBarrierSet.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahBarrierSetStackChunk.hpp\"\n@@ -48,0 +49,1 @@\n+             new ShenandoahBarrierSetStackChunk(),\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,45 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSetStackChunk.hpp\"\n+\n+void ShenandoahBarrierSetStackChunk::encode_gc_mode(stackChunkOop chunk, OopIterator* oop_iterator) {\n+  \/\/ Nothing to do\n+}\n+\n+void ShenandoahBarrierSetStackChunk::decode_gc_mode(stackChunkOop chunk, OopIterator* oop_iterator) {\n+  \/\/ Nothing to do\n+}\n+\n+oop ShenandoahBarrierSetStackChunk::load_oop(stackChunkOop chunk, oop* addr) {\n+  oop result = BarrierSetStackChunk::load_oop(chunk, addr);\n+  return ShenandoahBarrierSet::barrier_set()->load_reference_barrier(result);\n+}\n+\n+oop ShenandoahBarrierSetStackChunk::load_oop(stackChunkOop chunk, narrowOop* addr) {\n+  oop result = BarrierSetStackChunk::load_oop(chunk, addr);\n+  return ShenandoahBarrierSet::barrier_set()->load_reference_barrier(result);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSetStackChunk.cpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -0,0 +1,39 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHBARRIERSETSTACKCHUNK_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHBARRIERSETSTACKCHUNK_HPP\n+\n+#include \"gc\/shared\/barrierSetStackChunk.hpp\"\n+\n+class ShenandoahBarrierSetStackChunk : public BarrierSetStackChunk {\n+public:\n+  virtual void encode_gc_mode(stackChunkOop chunk, OopIterator* oop_iterator) override;\n+  virtual void decode_gc_mode(stackChunkOop chunk, OopIterator* oop_iterator) override;\n+\n+  virtual oop load_oop(stackChunkOop chunk, oop* addr) override;\n+  virtual oop load_oop(stackChunkOop chunk, narrowOop* addr) override;\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHBARRIERSETSTACKCHUNK_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSetStackChunk.hpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"added"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/z\/zBarrierSetStackChunk.hpp\"\n@@ -49,0 +50,1 @@\n+               new ZBarrierSetStackChunk(),\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSet.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,47 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/z\/zBarrier.inline.hpp\"\n+#include \"gc\/z\/zBarrierSetStackChunk.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+void ZBarrierSetStackChunk::encode_gc_mode(stackChunkOop chunk, OopIterator* iterator) {\n+  \/\/ Do nothing\n+}\n+\n+void ZBarrierSetStackChunk::decode_gc_mode(stackChunkOop chunk, OopIterator* iterator) {\n+  \/\/ Do nothing\n+}\n+\n+oop ZBarrierSetStackChunk::load_oop(stackChunkOop chunk, oop* addr) {\n+  oop obj = Atomic::load(addr);\n+  return ZBarrier::load_barrier_on_oop_field_preloaded((volatile oop*)NULL, obj);\n+}\n+\n+oop ZBarrierSetStackChunk::load_oop(stackChunkOop chunk, narrowOop* addr) {\n+  ShouldNotReachHere();\n+  return NULL;\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSetStackChunk.cpp","additions":47,"deletions":0,"binary":false,"changes":47,"status":"added"},{"patch":"@@ -0,0 +1,44 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZBARRIERSETSTACKCHUNK_HPP\n+#define SHARE_GC_Z_ZBARRIERSETSTACKCHUNK_HPP\n+\n+#include \"gc\/shared\/barrierSetStackChunk.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class OopClosure;\n+\n+class ZBarrierSetStackChunk : public BarrierSetStackChunk {\n+public:\n+  virtual void encode_gc_mode(stackChunkOop chunk, OopIterator* iterator) override;\n+  virtual void decode_gc_mode(stackChunkOop chunk, OopIterator* iterator) override;\n+\n+  virtual oop load_oop(stackChunkOop chunk, oop* addr) override;\n+  virtual oop load_oop(stackChunkOop chunk, narrowOop* addr) override;\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZBARRIERSETSTACKCHUNK_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSetStackChunk.hpp","additions":44,"deletions":0,"binary":false,"changes":44,"status":"added"},{"patch":"@@ -126,0 +126,6 @@\n+\/\/ Interface for applying an OopClosure to a set of oops.\n+class OopIterator {\n+public:\n+  virtual void oops_do(OopClosure* cl) = 0;\n+};\n+\n","filename":"src\/hotspot\/share\/memory\/iterator.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -122,0 +122,1 @@\n+  static inline size_t gc_data_size(size_t stack_size_in_words); \/\/ In words\n","filename":"src\/hotspot\/share\/oops\/instanceStackChunkKlass.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-  return align_object_size(size_helper() + stack_size_in_words + bitmap_size(stack_size_in_words));\n+  return align_object_size(size_helper() + stack_size_in_words + gc_data_size(stack_size_in_words));\n@@ -51,0 +51,5 @@\n+inline size_t InstanceStackChunkKlass::gc_data_size(size_t stack_size_in_words) {\n+  \/\/ At the moment all GCs are okay with GC data big enough to fit a bit map\n+  return bitmap_size(stack_size_in_words);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/instanceStackChunkKlass.inline.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetStackChunk.hpp\"\n@@ -39,0 +41,22 @@\n+template <typename RegisterMapT>\n+class FrameOopIterator : public OopIterator {\n+private:\n+  const frame& _f;\n+  const RegisterMapT* _map;\n+\n+public:\n+  FrameOopIterator(const frame& f, const RegisterMapT* map)\n+    : _f(f),\n+      _map(map) {\n+  }\n+\n+  virtual void oops_do(OopClosure* cl) override {\n+    if (_f.is_interpreted_frame()) {\n+      _f.oops_interpreted_do(cl, nullptr);\n+    } else {\n+      OopMapDo<OopClosure, DerivedOopClosure, IncludeAllValues> visitor(cl, nullptr);\n+      visitor.oops_do(&_f, _map, _f.oop_map());\n+    }\n+  }\n+};\n+\n@@ -175,1 +199,2 @@\n-class FrameToDerivedPointerClosure {\n+class EncodeGCModeConcurrentFrameClosure {\n+  stackChunkOop _chunk;\n@@ -179,2 +204,4 @@\n-  FrameToDerivedPointerClosure(DerivedPointerClosureType* cl)\n-    : _cl(cl) {}\n+  EncodeGCModeConcurrentFrameClosure(stackChunkOop chunk, DerivedPointerClosureType* cl)\n+    : _chunk(chunk),\n+      _cl(cl) {\n+  }\n@@ -185,0 +212,6 @@\n+\n+    BarrierSetStackChunk* bs_chunk = BarrierSet::barrier_set()->barrier_set_stack_chunk();\n+    frame fr = f.to_frame();\n+    FrameOopIterator<RegisterMapT> iterator(fr, map);\n+    bs_chunk->encode_gc_mode(_chunk, &iterator);\n+\n@@ -259,1 +292,1 @@\n-  FrameToDerivedPointerClosure<decltype(derived_cl)> frame_cl(&derived_cl);\n+  EncodeGCModeConcurrentFrameClosure<decltype(derived_cl)> frame_cl(this, &derived_cl);\n@@ -265,40 +298,0 @@\n-enum class OopKind { Narrow, Wide };\n-\n-template <OopKind kind>\n-class CompressOopsAndBuildBitmapOopClosure : public OopClosure {\n-  stackChunkOop _chunk;\n-  BitMapView _bm;\n-\n-  void convert_oop_to_narrowOop(oop* p) {\n-    oop obj = *p;\n-    *p = nullptr;\n-    *(narrowOop*)p = CompressedOops::encode(obj);\n-  }\n-\n-  template <typename T>\n-  void do_oop_work(T* p) {\n-    BitMap::idx_t index = _chunk->bit_index_for(p);\n-    assert(!_bm.at(index), \"must not be set already\");\n-    _bm.set_bit(index);\n-  }\n-\n-public:\n-  CompressOopsAndBuildBitmapOopClosure(stackChunkOop chunk)\n-    : _chunk(chunk), _bm(chunk->bitmap()) {}\n-\n-  virtual void do_oop(oop* p) override {\n-    if (kind == OopKind::Narrow) {\n-      \/\/ Convert all oops to narrow before marking the oop in the bitmap.\n-      convert_oop_to_narrowOop(p);\n-      do_oop_work((narrowOop*)p);\n-    } else {\n-      do_oop_work(p);\n-    }\n-  }\n-\n-  virtual void do_oop(narrowOop* p) override {\n-    do_oop_work(p);\n-  }\n-};\n-\n-template <OopKind kind>\n@@ -316,2 +309,4 @@\n-    CompressOopsAndBuildBitmapOopClosure<kind> cl(_chunk);\n-    f.iterate_oops(&cl, map);\n+    BarrierSetStackChunk* bs_chunk = BarrierSet::barrier_set()->barrier_set_stack_chunk();\n+    frame fr = f.to_frame();\n+    FrameOopIterator<RegisterMapT> iterator(fr, map);\n+    bs_chunk->encode_gc_mode(_chunk, &iterator);\n@@ -331,7 +326,2 @@\n-  if (UseCompressedOops) {\n-    TransformStackChunkClosure<OopKind::Narrow> closure(this);\n-    iterate_stack(&closure);\n-  } else {\n-    TransformStackChunkClosure<OopKind::Wide> closure(this);\n-    iterate_stack(&closure);\n-  }\n+  TransformStackChunkClosure closure(this);\n+  iterate_stack(&closure);\n@@ -394,12 +384,0 @@\n-class UncompressOopsOopClosure : public OopClosure {\n-public:\n-  void do_oop(oop* p) override {\n-    assert(UseCompressedOops, \"Only needed with compressed oops\");\n-    oop obj = CompressedOops::decode(*(narrowOop*)p);\n-    assert(obj == nullptr || dbg_is_good_oop(obj), \"p: \" PTR_FORMAT \" obj: \" PTR_FORMAT, p2i(p), p2i(obj));\n-    *p = obj;\n-  }\n-\n-  void do_oop(narrowOop* p) override {}\n-};\n-\n@@ -412,9 +390,3 @@\n-  if (has_bitmap() && UseCompressedOops) {\n-    UncompressOopsOopClosure oop_closure;\n-    if (f.is_interpreted_frame()) {\n-      f.oops_interpreted_do(&oop_closure, nullptr);\n-    } else {\n-      OopMapDo<UncompressOopsOopClosure, DerivedOopClosure, SkipNullValue> visitor(&oop_closure, nullptr);\n-      visitor.oops_do(&f, map, f.oop_map());\n-    }\n-  }\n+  BarrierSetStackChunk* bs_chunk = BarrierSet::barrier_set()->barrier_set_stack_chunk();\n+  FrameOopIterator<RegisterMapT> iterator(f, map);\n+  bs_chunk->decode_gc_mode(this, &iterator);\n@@ -444,6 +416,0 @@\n-template <typename P>\n-static inline oop safe_load(P* addr) {\n-  oop obj = RawAccess<>::oop_load(addr);\n-  return NativeAccess<>::oop_load(&obj);\n-}\n-\n@@ -462,2 +428,2 @@\n-     _count++;\n-    oop obj = safe_load(p);\n+    _count++;\n+    oop obj = _chunk->load_oop(p);\n@@ -550,1 +516,1 @@\n-    oop obj = safe_load(p);\n+    oop obj = _chunk->load_oop(p);\n","filename":"src\/hotspot\/share\/oops\/stackChunkOop.cpp","additions":49,"deletions":83,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -71,2 +71,0 @@\n-  inline bool is_parent_null() const;\n-  template<typename P>\n@@ -97,1 +95,0 @@\n-  template<typename P> inline oop cont() const;\n@@ -157,0 +154,1 @@\n+  inline void* gc_data() const;\n@@ -188,0 +186,3 @@\n+\n+  template <typename OopT>\n+  inline oop load_oop(OopT* addr);\n","filename":"src\/hotspot\/share\/oops\/stackChunkOop.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -31,0 +31,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetStackChunk.hpp\"\n@@ -33,0 +35,1 @@\n+#include \"oops\/access.inline.hpp\"\n@@ -50,2 +53,0 @@\n-template<typename P>\n-inline bool stackChunkOopDesc::is_parent_null() const          { return jdk_internal_vm_StackChunk::is_parent_null<P>(as_oop()); }\n@@ -88,7 +89,1 @@\n-inline oop stackChunkOopDesc::cont() const              { return UseCompressedOops ? cont<narrowOop>() : cont<oop>(); \/* jdk_internal_vm_StackChunk::cont(as_oop()); *\/ }\n-template<typename P>\n-inline oop stackChunkOopDesc::cont() const              {\n-  oop obj = jdk_internal_vm_StackChunk::cont_raw<P>(as_oop());\n-  obj = (oop)NativeAccess<>::oop_load(&obj);\n-  return obj;\n-}\n+inline oop stackChunkOopDesc::cont() const                { return jdk_internal_vm_StackChunk::cont(as_oop()); }\n@@ -97,1 +92,1 @@\n-inline void stackChunkOopDesc::set_cont_raw(oop value)    {  jdk_internal_vm_StackChunk::set_cont_raw<P>(this, value); }\n+inline void stackChunkOopDesc::set_cont_raw(oop value)    { jdk_internal_vm_StackChunk::set_cont_raw<P>(this, value); }\n@@ -234,1 +229,1 @@\n-inline BitMapView stackChunkOopDesc::bitmap() const {\n+inline void* stackChunkOopDesc::gc_data() const {\n@@ -236,0 +231,5 @@\n+  assert(stack_sz != 0, \"stack should not be empty\");\n+\n+  \/\/ The gc data is located after the stack.\n+  return start_of_stack() + stack_sz;\n+}\n@@ -237,2 +237,3 @@\n-  \/\/ The bitmap is located after the stack.\n-  HeapWord* bitmap_addr = start_of_stack() + stack_sz;\n+inline BitMapView stackChunkOopDesc::bitmap() const {\n+  HeapWord* bitmap_addr = static_cast<HeapWord*>(gc_data());\n+  int stack_sz = stack_size();\n@@ -355,0 +356,5 @@\n+template <typename OopT>\n+inline oop stackChunkOopDesc::load_oop(OopT* addr) {\n+  return BarrierSet::barrier_set()->barrier_set_stack_chunk()->load_oop(this, addr);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/stackChunkOop.inline.hpp","additions":19,"deletions":13,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n@@ -630,0 +631,6 @@\n+\n+  \/\/ This could be a different thread to the current one. So we need to ensure that\n+  \/\/ processing has started before we are allowed to read the continuation oop of\n+  \/\/ another thread, as it is a direct root of that other thread.\n+  StackWatermarkSet::start_processing(java_thread, StackWatermarkKind::gc);\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnvBase.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -102,1 +102,1 @@\n-  oop cont()       override { return _vfst.continuation(); }\n+  oop cont() override { return _vfst.continuation(); }\n@@ -137,1 +137,1 @@\n-  oop cont() override { return continuation() != NULL ? continuation(): ContinuationEntry::cont_oop_or_null(_cont_entry); }\n+  oop cont() override { return continuation() != NULL ? continuation(): ContinuationEntry::cont_oop_or_null(_cont_entry, _map->thread()); }\n","filename":"src\/hotspot\/share\/prims\/stackwalk.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -64,10 +64,0 @@\n-const ContinuationEntry* Continuation::last_continuation(const JavaThread* thread, oop cont_scope) {\n-  \/\/ guarantee (thread->has_last_Java_frame(), \"\");\n-  for (ContinuationEntry* entry = thread->last_continuation(); entry != nullptr; entry = entry->parent()) {\n-    if (cont_scope == jdk_internal_vm_Continuation::scope(entry->cont_oop())) {\n-      return entry;\n-    }\n-  }\n-  return nullptr;\n-}\n-\n@@ -80,1 +70,1 @@\n-    if (continuation == entry->cont_oop()) {\n+    if (continuation == entry->cont_oop(thread)) {\n@@ -102,4 +92,0 @@\n-bool Continuation::is_continuation_scope_mounted(JavaThread* thread, oop cont_scope) {\n-  return is_on_stack(thread, last_continuation(thread, cont_scope));\n-}\n-\n@@ -196,1 +182,1 @@\n-  oop continuation = ce->cont_oop();\n+  oop continuation = ce->cont_oop(map->thread());\n@@ -269,1 +255,1 @@\n-    continuation = ce->cont_oop();\n+    continuation = ce->cont_oop(map->thread());\n","filename":"src\/hotspot\/share\/runtime\/continuation.cpp","additions":3,"deletions":17,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -75,1 +75,0 @@\n-  static const ContinuationEntry* last_continuation(const JavaThread* thread, oop cont_scope);\n@@ -81,1 +80,0 @@\n-  static bool is_continuation_scope_mounted(JavaThread* thread, oop cont_scope);\n","filename":"src\/hotspot\/share\/runtime\/continuation.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -93,5 +93,0 @@\n-void ContinuationEntry::setup_oopmap(OopMap* map) {\n-  map->set_oop(VMRegImpl::stack2reg(in_bytes(cont_offset())  \/ VMRegImpl::stack_slot_size));\n-  map->set_oop(VMRegImpl::stack2reg(in_bytes(chunk_offset()) \/ VMRegImpl::stack_slot_size));\n-}\n-\n","filename":"src\/hotspot\/share\/runtime\/continuationEntry.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -88,2 +88,0 @@\n-  static void setup_oopmap(OopMap* map);\n-\n@@ -129,3 +127,6 @@\n-  inline oop cont_oop() const;\n-  inline oop scope() const;\n-  inline static oop cont_oop_or_null(const ContinuationEntry* ce);\n+  inline oop cont_oop(const JavaThread* thread) const;\n+  inline oop scope(const JavaThread* thread) const;\n+  inline static oop cont_oop_or_null(const ContinuationEntry* ce, const JavaThread* thread);\n+\n+  oop* cont_addr() { return (oop*)&_cont; }\n+  oop* chunk_addr() { return (oop*)&_chunk; }\n","filename":"src\/hotspot\/share\/runtime\/continuationEntry.hpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -30,1 +30,2 @@\n-#include \"oops\/access.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"memory\/universe.hpp\"\n@@ -32,0 +33,2 @@\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n+#include \"runtime\/thread.hpp\"\n@@ -33,1 +36,0 @@\n-\n@@ -44,3 +46,15 @@\n-inline oop ContinuationEntry::cont_oop() const {\n-  oop snapshot = _cont;\n-  return NativeAccess<>::oop_load(&snapshot);\n+inline bool is_stack_watermark_processing_started(const JavaThread* thread) {\n+  StackWatermark* sw = StackWatermarkSet::get(const_cast<JavaThread*>(thread), StackWatermarkKind::gc);\n+\n+  if (sw == nullptr) {\n+    \/\/ No stale processing without stack watermarks\n+    return true;\n+  }\n+\n+  return sw->processing_started();\n+}\n+\n+inline oop ContinuationEntry::cont_oop(const JavaThread* thread) const {\n+  assert(!Universe::heap()->is_in((void*)&_cont), \"Should not be in the heap\");\n+  assert(is_stack_watermark_processing_started(thread != nullptr ? thread : JavaThread::current()), \"Not processed\");\n+  return *(oop*)&_cont;\n@@ -49,2 +63,2 @@\n-inline oop ContinuationEntry::cont_oop_or_null(const ContinuationEntry* ce) {\n-  return ce == nullptr ? nullptr : ce->cont_oop();\n+inline oop ContinuationEntry::cont_oop_or_null(const ContinuationEntry* ce, const JavaThread* thread) {\n+  return ce == nullptr ? nullptr : ce->cont_oop(thread);\n@@ -53,2 +67,2 @@\n-inline oop ContinuationEntry::scope() const {\n-  return Continuation::continuation_scope(cont_oop());\n+inline oop ContinuationEntry::scope(const JavaThread* thread) const {\n+  return Continuation::continuation_scope(cont_oop(thread));\n","filename":"src\/hotspot\/share\/runtime\/continuationEntry.inline.hpp","additions":23,"deletions":9,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -1216,1 +1216,5 @@\n-    ContinuationGCSupport::transform_stack_chunk(_cont.tail());\n+    if (UseShenandoahGC) {\n+      _cont.tail()->relativize_derived_pointers_concurrently();\n+    } else {\n+      ContinuationGCSupport::transform_stack_chunk(_cont.tail());\n+    }\n@@ -1250,0 +1254,78 @@\n+class StackChunkAllocator : public MemAllocator {\n+  const size_t                                 _stack_size;\n+  ContinuationWrapper&                         _continuation_wrapper;\n+  JvmtiSampledObjectAllocEventCollector* const _jvmti_event_collector;\n+  mutable bool                                 _took_slow_path;\n+\n+  \/\/ Does the minimal amount of initialization needed for a TLAB allocation.\n+  \/\/ We don't need to do a full initialization, as such an allocation need not be immediately walkable.\n+  virtual oop initialize(HeapWord* mem) const override {\n+    assert(_stack_size > 0, \"\");\n+    assert(_stack_size <= max_jint, \"\");\n+    assert(_word_size > _stack_size, \"\");\n+\n+    \/\/ zero out fields (but not the stack)\n+    const size_t hs = oopDesc::header_size();\n+    Copy::fill_to_aligned_words(mem + hs, vmClasses::StackChunk_klass()->size_helper() - hs);\n+\n+    jdk_internal_vm_StackChunk::set_size(mem, (int)_stack_size);\n+    jdk_internal_vm_StackChunk::set_sp(mem, (int)_stack_size);\n+\n+    return finish(mem);\n+  }\n+\n+  stackChunkOop allocate_fast() const {\n+    if (!UseTLAB) {\n+      return nullptr;\n+    }\n+\n+    HeapWord* const mem = MemAllocator::mem_allocate_inside_tlab_fast();\n+    if (mem == nullptr) {\n+      return nullptr;\n+    }\n+\n+    oop obj = initialize(mem);\n+    return stackChunkOopDesc::cast(obj);\n+  }\n+\n+public:\n+  StackChunkAllocator(Klass* klass,\n+                      size_t word_size,\n+                      Thread* thread,\n+                      size_t stack_size,\n+                      ContinuationWrapper& continuation_wrapper,\n+                      JvmtiSampledObjectAllocEventCollector* jvmti_event_collector)\n+    : MemAllocator(klass, word_size, thread),\n+      _stack_size(stack_size),\n+      _continuation_wrapper(continuation_wrapper),\n+      _jvmti_event_collector(jvmti_event_collector),\n+      _took_slow_path(false) {}\n+\n+  \/\/ Provides it's own, specialized allocation which skips instrumentation\n+  \/\/ if the memory can be allocated without going to a slow-path.\n+  stackChunkOop allocate() const {\n+    \/\/ First try to allocate without any slow-paths or instrumentation.\n+    stackChunkOop obj = allocate_fast();\n+    if (obj != nullptr) {\n+      return obj;\n+    }\n+\n+    \/\/ Now try full-blown allocation with all expensive operations,\n+    \/\/ including potentially safepoint operations.\n+    _took_slow_path = true;\n+\n+    \/\/ Protect unhandled Loom oops\n+    ContinuationWrapper::SafepointOp so(_thread, _continuation_wrapper);\n+\n+    \/\/ Can safepoint\n+    _jvmti_event_collector->start();\n+\n+    \/\/ Can safepoint\n+    return stackChunkOopDesc::cast(MemAllocator::allocate());\n+  }\n+\n+  bool took_slow_path() const {\n+    return _took_slow_path;\n+  }\n+};\n+\n@@ -1267,11 +1349,11 @@\n-  StackChunkAllocator allocator(klass, size_in_words, stack_size, current);\n-  oop fast_oop = allocator.try_allocate_in_existing_tlab();\n-  oop chunk_oop = fast_oop;\n-  if (chunk_oop == nullptr) {\n-    ContinuationWrapper::SafepointOp so(current, _cont);\n-    assert(_jvmti_event_collector != nullptr, \"\");\n-    _jvmti_event_collector->start();  \/\/ can safepoint\n-    chunk_oop = allocator.allocate(); \/\/ can safepoint\n-    if (chunk_oop == nullptr) {\n-      return nullptr; \/\/ OOME\n-    }\n+  \/\/ Allocate the chunk.\n+  \/\/\n+  \/\/ This might safepoint while allocating, but all safepointing due to\n+  \/\/ instrumentation have been deferred. This property is important for\n+  \/\/ some GCs, as this ensures that the allocated object is in the young\n+  \/\/ generation \/ newly allocated memory.\n+  StackChunkAllocator allocator(klass, size_in_words, current, stack_size, _cont, _jvmti_event_collector);\n+  stackChunkOop chunk = allocator.allocate();\n+\n+  if (chunk == nullptr) {\n+    return nullptr; \/\/ OOME\n@@ -1280,1 +1362,0 @@\n-  stackChunkOop chunk = stackChunkOopDesc::cast(chunk_oop);\n@@ -1296,1 +1377,8 @@\n-  assert(chunk->parent() == nullptr || chunk->parent()->is_stackChunk(), \"\");\n+#if INCLUDE_ZGC\n+ if (UseZGC) {\n+    assert(!chunk->requires_barriers(), \"ZGC always allocates in the young generation\");\n+    _barriers = false;\n+  } else\n+#endif\n+#if INCLUDE_SHENANDOAHGC\n+if (UseShenandoahGC) {\n@@ -1298,3 +1386,13 @@\n-  \/\/ Shenandoah: even continuation is good, it does not mean it is deeply good.\n-  if (UseShenandoahGC && chunk->requires_barriers()) {\n-    fast_oop = nullptr;\n+    _barriers = chunk->requires_barriers();\n+  } else\n+#endif\n+  {\n+    if (!allocator.took_slow_path()) {\n+      \/\/ Guaranteed to be in young gen \/ newly allocated memory\n+      assert(!chunk->requires_barriers(), \"Unfamiliar GC requires barriers on TLAB allocation\");\n+      _barriers = false;\n+    } else {\n+      \/\/ Some GCs could put direct allocations in old gen for slow-path\n+      \/\/ allocations; need to explicitly check if that was the case.\n+      _barriers = chunk->requires_barriers();\n+    }\n@@ -1303,9 +1401,2 @@\n-  if (fast_oop != nullptr) {\n-    assert(!chunk->requires_barriers(), \"Unfamiliar GC requires barriers on TLAB allocation\");\n-  } else {\n-    assert(!UseZGC || !chunk->requires_barriers(), \"Allocated ZGC object requires barriers\");\n-    _barriers = !UseZGC && chunk->requires_barriers();\n-\n-    if (_barriers) {\n-      log_develop_trace(continuations)(\"allocation requires barriers\");\n-    }\n+  if (_barriers) {\n+    log_develop_trace(continuations)(\"allocation requires barriers\");\n@@ -1313,0 +1404,3 @@\n+\n+  assert(chunk->parent() == nullptr || chunk->parent()->is_stackChunk(), \"\");\n+\n@@ -1418,2 +1512,2 @@\n-  oop oopCont = entry->cont_oop();\n-  assert(oopCont == current->last_continuation()->cont_oop(), \"\");\n+  oop oopCont = entry->cont_oop(current);\n+  assert(oopCont == current->last_continuation()->cont_oop(current), \"\");\n@@ -1426,1 +1520,1 @@\n-  assert(entry->is_virtual_thread() == (entry->scope() == java_lang_VirtualThread::vthread_scope()), \"\");\n+  assert(entry->is_virtual_thread() == (entry->scope(current) == java_lang_VirtualThread::vthread_scope()), \"\");\n@@ -1510,1 +1604,1 @@\n-      oop scope = jdk_internal_vm_Continuation::scope(entry->cont_oop());\n+      oop scope = jdk_internal_vm_Continuation::scope(entry->cont_oop(thread));\n@@ -1547,1 +1641,1 @@\n-  oop continuation = ce->cont_oop();\n+  oop continuation = ce->cont_oop(thread);\n@@ -1797,1 +1891,1 @@\n-  const bool is_last = empty && chunk->is_parent_null<typename ConfigT::OopT>();\n+  const bool is_last = empty && chunk->parent() == NULL;\n@@ -1878,1 +1972,0 @@\n-    \/\/ TODO ZGC: this is where we'd want to restore color to the oops\n@@ -2263,1 +2356,1 @@\n-  oop oopCont = entry->cont_oop();\n+  oop oopCont = entry->cont_oop(thread);\n@@ -2269,1 +2362,1 @@\n-  assert(entry->is_virtual_thread() == (entry->scope() == java_lang_VirtualThread::vthread_scope()), \"\");\n+  assert(entry->is_virtual_thread() == (entry->scope(thread) == java_lang_VirtualThread::vthread_scope()), \"\");\n","filename":"src\/hotspot\/share\/runtime\/continuationFreezeThaw.cpp","additions":128,"deletions":35,"binary":false,"changes":163,"status":"modified"},{"patch":"@@ -109,2 +109,0 @@\n-  static inline bool is_parent_null(oop chunk); \/\/ bypasses barriers for a faster test\n-  template<typename P>\n@@ -134,9 +132,7 @@\n- \/\/ cont oop's processing is essential for the chunk's GC protocol\n-  static inline oop cont(oop chunk);\n-  static inline void set_cont(oop chunk, oop value);\n-  template<typename P>\n-  static inline oop cont_raw(oop chunk);\n-  template<typename P>\n-  static inline void set_cont_raw(oop chunk, oop value);\n-  template<DecoratorSet decorators>\n-  static inline void set_cont_access(oop chunk, oop value);\n+  \/\/ cont oop's processing is essential for the chunk's GC protocol\n+   static inline oop cont(oop chunk);\n+   static inline void set_cont(oop chunk, oop value);\n+   template<typename P>\n+   static inline void set_cont_raw(oop chunk, oop value);\n+   template<DecoratorSet decorators>\n+   static inline void set_cont_access(oop chunk, oop value);\n","filename":"src\/hotspot\/share\/runtime\/continuationJavaClasses.hpp","additions":7,"deletions":11,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -90,5 +90,0 @@\n-template<typename P>\n-inline bool jdk_internal_vm_StackChunk::is_parent_null(oop chunk) {\n-  return (oop)RawAccess<>::oop_load(chunk->field_addr<P>(_parent_offset)) == NULL;\n-}\n-\n@@ -113,5 +108,0 @@\n-template<typename P>\n-inline oop jdk_internal_vm_StackChunk::cont_raw(oop chunk) {\n-  return (oop)RawAccess<>::oop_load(chunk->field_addr<P>(_cont_offset));\n-}\n-\n","filename":"src\/hotspot\/share\/runtime\/continuationJavaClasses.inline.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-  assert(_entry == nullptr || _continuation == _entry->cont_oop(),\n+  assert(_entry == nullptr || _continuation == _entry->cont_oop(map->thread()),\n@@ -48,1 +48,1 @@\n-    p2i( (oopDesc*)_continuation), p2i((oopDesc*)_entry->cont_oop()), p2i(entrySP()));\n+    p2i( (oopDesc*)_continuation), p2i((oopDesc*)_entry->cont_oop(map->thread())), p2i(entrySP()));\n","filename":"src\/hotspot\/share\/runtime\/continuationWrapper.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -82,1 +82,1 @@\n-    _tail = (stackChunkOop)badOop;\n+    *reinterpret_cast<intptr_t*>(&_tail) = badHeapOopVal;\n@@ -147,2 +147,0 @@\n-  assert(_continuation == _entry->cont_oop(), \"cont: \" INTPTR_FORMAT \" entry: \" INTPTR_FORMAT \" entry_sp: \"\n-         INTPTR_FORMAT, p2i((oopDesc*)_continuation), p2i((oopDesc*)_entry->cont_oop()), p2i(entrySP()));\n","filename":"src\/hotspot\/share\/runtime\/continuationWrapper.inline.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1387,0 +1387,7 @@\n+\n+  ContinuationEntry* entry = _cont_entry;\n+  while (entry != nullptr) {\n+    f->do_oop((oop*)entry->cont_addr());\n+    f->do_oop((oop*)entry->chunk_addr());\n+    entry = entry->parent();\n+  }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -44,12 +44,0 @@\n-\n-template <typename OopT>\n-static oop read_oop_local(OopT* p) {\n-  \/\/ We can't do a native access directly from p because load barriers\n-  \/\/ may self-heal. If that happens on a base pointer for compressed oops,\n-  \/\/ then there will be a crash later on. Only the stack watermark API is\n-  \/\/ allowed to heal oops, because it heals derived pointers before their\n-  \/\/ corresponding base pointers.\n-  oop obj = RawAccess<>::oop_load(p);\n-  return NativeAccess<>::oop_load(&obj);\n-}\n-\n@@ -64,2 +52,74 @@\n-template StackValue* StackValue::create_stack_value(ScopeValue*, address, const RegisterMap*);\n-template StackValue* StackValue::create_stack_value(ScopeValue*, address, const SmallRegisterMap*);\n+static oop oop_from_oop_location(stackChunkOop chunk, void* addr) {\n+  if (addr == nullptr) {\n+    return nullptr;\n+  }\n+\n+  if (UseCompressedOops) {\n+    \/\/ When compressed oops is enabled, an oop location may\n+    \/\/ contain narrow oop values - we deal with that here\n+\n+    if (chunk != NULL && chunk->has_bitmap()) {\n+      \/\/ Transformed stack chunk with narrow oops\n+      return chunk->load_oop((narrowOop*)addr);\n+    }\n+\n+#ifdef _LP64\n+    if (CompressedOops::is_base(*(void**)addr)) {\n+      \/\/ Compiled code may produce decoded oop = narrow_oop_base\n+      \/\/ when a narrow oop implicit null check is used.\n+      \/\/ The narrow_oop_base could be NULL or be the address\n+      \/\/ of the page below heap. Use NULL value for both cases.\n+      return nullptr;\n+    }\n+#endif\n+  }\n+\n+  if (chunk != NULL) {\n+    \/\/ Load oop from chunk\n+    return chunk->load_oop((oop*)addr);\n+  }\n+\n+  \/\/ Load oop from stack\n+  return *(oop*)addr;\n+}\n+\n+static oop oop_from_narrowOop_location(stackChunkOop chunk, void* addr, bool is_register) {\n+  assert(UseCompressedOops, \"Narrow oops should not exist\");\n+  assert(addr != nullptr, \"Not expecting null address\");\n+  narrowOop* narrow_addr;\n+  if (is_register) {\n+    \/\/ The callee has no clue whether the register holds an int,\n+    \/\/ long or is unused.  He always saves a long.  Here we know\n+    \/\/ a long was saved, but we only want an int back.  Narrow the\n+    \/\/ saved long to the int that the JVM wants.  We can't just\n+    \/\/ use narrow_oop_cast directly, because we don't know what\n+    \/\/ the high bits of the value might be.\n+    narrow_addr = ((narrowOop*)addr) BIG_ENDIAN_ONLY(+ 1);\n+  } else {\n+    narrow_addr = (narrowOop*)addr;\n+  }\n+\n+  if (chunk != NULL) {\n+    \/\/ Load oop from chunk\n+    return chunk->load_oop(narrow_addr);\n+  }\n+\n+  \/\/ Load oop from stack\n+  return CompressedOops::decode(*narrow_addr);\n+}\n+\n+StackValue* StackValue::create_stack_value_from_oop_location(stackChunkOop chunk, void* addr) {\n+  oop val = oop_from_oop_location(chunk, addr);\n+  assert(oopDesc::is_oop_or_null(val), \"bad oop found at \" INTPTR_FORMAT \" in_cont: %d compressed: %d\",\n+         p2i(addr), chunk != NULL, chunk != NULL && chunk->has_bitmap() && UseCompressedOops);\n+  Handle h(Thread::current(), val); \/\/ Wrap a handle around the oop\n+  return new StackValue(h);\n+}\n+\n+StackValue* StackValue::create_stack_value_from_narrowOop_location(stackChunkOop chunk, void* addr, bool is_register) {\n+  oop val = oop_from_narrowOop_location(chunk, addr, is_register);\n+  assert(oopDesc::is_oop_or_null(val), \"bad oop found at \" INTPTR_FORMAT \" in_cont: %d compressed: %d\",\n+         p2i(addr), chunk != NULL, chunk != NULL && chunk->has_bitmap() && UseCompressedOops);\n+  Handle h(Thread::current(), val); \/\/ Wrap a handle around the oop\n+  return new StackValue(h);\n+}\n@@ -69,0 +129,1 @@\n+  stackChunkOop chunk = reg_map->stack_chunk()();\n@@ -114,22 +175,2 @@\n-    case Location::narrowoop: {\n-      assert(UseCompressedOops, \"\");\n-      union { intptr_t p; narrowOop noop;} value;\n-      value.p = (intptr_t) CONST64(0xDEADDEAFDEADDEAF);\n-      if (loc.is_register()) {\n-        \/\/ The callee has no clue whether the register holds an int,\n-        \/\/ long or is unused.  He always saves a long.  Here we know\n-        \/\/ a long was saved, but we only want an int back.  Narrow the\n-        \/\/ saved long to the int that the JVM wants.  We can't just\n-        \/\/ use narrow_oop_cast directly, because we don't know what\n-        \/\/ the high bits of the value might be.\n-        static_assert(sizeof(narrowOop) == sizeof(juint), \"size mismatch\");\n-        juint narrow_value = (juint) *(julong*)value_addr;\n-        value.noop = CompressedOops::narrow_oop_cast(narrow_value);\n-      } else {\n-        value.noop = *(narrowOop*) value_addr;\n-      }\n-      \/\/ Decode narrowoop\n-      oop val = read_oop_local(&value.noop);\n-      Handle h(Thread::current(), val); \/\/ Wrap a handle around the oop\n-      return new StackValue(h);\n-    }\n+    case Location::narrowoop:\n+      return create_stack_value_from_narrowOop_location(reg_map->stack_chunk()(), (void*)value_addr, loc.is_register());\n@@ -137,22 +178,2 @@\n-    case Location::oop: {\n-      oop val;\n-      if (reg_map->in_cont() && reg_map->stack_chunk()->has_bitmap() && UseCompressedOops) {\n-        val = CompressedOops::decode(*(narrowOop*)value_addr);\n-      } else {\n-        val = *(oop *)value_addr;\n-      }\n-#ifdef _LP64\n-      if (CompressedOops::is_base(val)) {\n-         \/\/ Compiled code may produce decoded oop = narrow_oop_base\n-         \/\/ when a narrow oop implicit null check is used.\n-         \/\/ The narrow_oop_base could be NULL or be the address\n-         \/\/ of the page below heap. Use NULL value for both cases.\n-         val = (oop)NULL;\n-      }\n-#endif\n-      val = read_oop_local(&val);\n-      assert(oopDesc::is_oop_or_null(val), \"bad oop found at \" INTPTR_FORMAT \" in_cont: %d compressed: %d\",\n-        p2i(value_addr), reg_map->in_cont(), reg_map->in_cont() && reg_map->stack_chunk()->has_bitmap() && UseCompressedOops);\n-      Handle h(Thread::current(), val); \/\/ Wrap a handle around the oop\n-      return new StackValue(h);\n-    }\n+    case Location::oop:\n+      return create_stack_value_from_oop_location(reg_map->stack_chunk()(), (void*)value_addr);\n","filename":"src\/hotspot\/share\/runtime\/stackValue.cpp","additions":79,"deletions":58,"binary":false,"changes":137,"status":"modified"},{"patch":"@@ -111,0 +111,3 @@\n+  static StackValue* create_stack_value_from_oop_location(stackChunkOop chunk, void* addr);\n+  static StackValue* create_stack_value_from_narrowOop_location(stackChunkOop chunk, void* addr, bool is_register);\n+\n","filename":"src\/hotspot\/share\/runtime\/stackValue.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -335,2 +335,1 @@\n-  assert(index >= 0 &&\n-         index < oop_mask.number_of_entries(), \"invariant\");\n+  assert(index >= 0 && index < oop_mask.number_of_entries(), \"invariant\");\n@@ -340,11 +339,1 @@\n-    oop obj = NULL;\n-    if (addr != NULL) {\n-      if (chunk != NULL) {\n-        obj = (chunk->has_bitmap() && UseCompressedOops) ? (oop)HeapAccess<>::oop_load((narrowOop*)addr) : HeapAccess<>::oop_load((oop*)addr);\n-      } else {\n-        obj = *(oop*)addr;\n-      }\n-    }\n-    \/\/ reference (oop) \"r\"\n-    Handle h(Thread::current(), obj);\n-    return new StackValue(h);\n+    return StackValue::create_stack_value_from_oop_location(chunk, (void*)addr);\n","filename":"src\/hotspot\/share\/runtime\/vframe.cpp","additions":2,"deletions":13,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-    return _cont_entry->cont_oop();\n+    return _cont_entry->cont_oop(_reg_map.thread());\n@@ -85,1 +85,2 @@\n-      assert(_cont_entry->cont_oop() != NULL, \"_cont: \" INTPTR_FORMAT, p2i(_cont_entry));\n+      \/\/ Reading oops are only safe if process_frames() is true, and we fix the oops.\n+      assert(!_reg_map.process_frames() || _cont_entry->cont_oop(_reg_map.thread()) != NULL, \"_cont: \" INTPTR_FORMAT, p2i(_cont_entry));\n@@ -90,1 +91,1 @@\n-          (_continuation_scope.not_null() && _cont_entry->scope() == _continuation_scope())) {\n+          (_continuation_scope.not_null() && _cont_entry->scope(_reg_map.thread()) == _continuation_scope())) {\n","filename":"src\/hotspot\/share\/runtime\/vframe.inline.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"}]}