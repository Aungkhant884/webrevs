{"files":[{"patch":"@@ -246,0 +246,4 @@\n+    \/\/ Parallel thread number for heap dump, initialize based on active processor count.\n+    \/\/ Note the real number of threads used is also determined by active workers and compression\n+    \/\/ backend thread number. See heapDumper.cpp.\n+    uint parallel_thread_num = MAX2<uint>(1, (uint)os::initial_active_processor_count() * 3 \/ 8);\n@@ -250,1 +254,1 @@\n-    dumper.dump(op->arg(0), out, (int)level);\n+    dumper.dump(path, out, (int)level, false, (uint)parallel_thread_num);\n","filename":"src\/hotspot\/share\/services\/attachListener.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -384,3 +384,3 @@\n-\n-class DumpWriter : public StackObj {\n- private:\n+\/\/ Base class for dump and parallel dump\n+class AbstractDumpWriter : public StackObj {\n+ protected:\n@@ -402,3 +402,1 @@\n-  CompressionBackend _backend; \/\/ Does the actual writing.\n-\n-  void flush();\n+  virtual void flush(bool force = false) = 0;\n@@ -408,1 +406,0 @@\n-  size_t position() const                       { return _pos; }\n@@ -416,1 +413,0 @@\n-\n@@ -418,4 +414,5 @@\n-  \/\/ Takes ownership of the writer and compressor.\n-  DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor);\n-\n-  ~DumpWriter();\n+  AbstractDumpWriter() :\n+    _buffer(NULL),\n+    _size(io_buffer_max_size),\n+    _pos(0),\n+    _in_dump_segment(false) { }\n@@ -424,3 +421,2 @@\n-  julong bytes_written() const          { return (julong) _backend.get_written(); }\n-\n-  char const* error() const             { return _backend.error(); }\n+  virtual julong bytes_written() const = 0;\n+  virtual char const* error() const = 0;\n@@ -428,0 +424,1 @@\n+  size_t position() const                       { return _pos; }\n@@ -429,1 +426,1 @@\n-  void write_raw(void* s, size_t len);\n+  virtual void write_raw(void* s, size_t len);\n@@ -444,4 +441,10 @@\n-  void finish_dump_segment();\n-\n-  \/\/ Called by threads used for parallel writing.\n-  void writer_loop()                    { _backend.thread_loop(); }\n+  void finish_dump_segment(bool force_flush = false);\n+  \/\/ Refresh to get new buffer\n+  void refresh() {\n+    assert (_in_dump_segment ==false, \"Sanity check\");\n+    _buffer = NULL;\n+    _size = io_buffer_max_size;\n+    _pos = 0;\n+    \/\/ Force flush to guarantee data from parallel dumper are written.\n+    flush(true);\n+  }\n@@ -449,1 +452,1 @@\n-  void deactivate()                     { flush(); _backend.deactivate(); }\n+  virtual void deactivate() = 0;\n@@ -452,15 +455,1 @@\n-\/\/ Check for error after constructing the object and destroy it in case of an error.\n-DumpWriter::DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor) :\n-  _buffer(NULL),\n-  _size(0),\n-  _pos(0),\n-  _in_dump_segment(false),\n-  _backend(writer, compressor, io_buffer_max_size, io_buffer_max_waste) {\n-  flush();\n-}\n-\n-DumpWriter::~DumpWriter() {\n-  flush();\n-}\n-\n-void DumpWriter::write_fast(void* s, size_t len) {\n+void AbstractDumpWriter::write_fast(void* s, size_t len) {\n@@ -470,1 +459,0 @@\n-\n@@ -475,1 +463,1 @@\n-bool DumpWriter::can_write_fast(size_t len) {\n+bool AbstractDumpWriter::can_write_fast(size_t len) {\n@@ -480,1 +468,1 @@\n-void DumpWriter::write_raw(void* s, size_t len) {\n+void AbstractDumpWriter::write_raw(void* s, size_t len) {\n@@ -488,1 +476,0 @@\n-\n@@ -501,5 +488,0 @@\n-\/\/ flush any buffered bytes to the file\n-void DumpWriter::flush() {\n-  _backend.get_new_buffer(&_buffer, &_pos, &_size);\n-}\n-\n@@ -510,1 +492,1 @@\n-void DumpWriter::write_u1(u1 x) {\n+void AbstractDumpWriter::write_u1(u1 x) {\n@@ -514,1 +496,1 @@\n-void DumpWriter::write_u2(u2 x) {\n+void AbstractDumpWriter::write_u2(u2 x) {\n@@ -520,1 +502,1 @@\n-void DumpWriter::write_u4(u4 x) {\n+void AbstractDumpWriter::write_u4(u4 x) {\n@@ -526,1 +508,1 @@\n-void DumpWriter::write_u8(u8 x) {\n+void AbstractDumpWriter::write_u8(u8 x) {\n@@ -532,1 +514,1 @@\n-void DumpWriter::write_objectID(oop o) {\n+void AbstractDumpWriter::write_objectID(oop o) {\n@@ -541,1 +523,1 @@\n-void DumpWriter::write_symbolID(Symbol* s) {\n+void AbstractDumpWriter::write_symbolID(Symbol* s) {\n@@ -550,1 +532,1 @@\n-void DumpWriter::write_id(u4 x) {\n+void AbstractDumpWriter::write_id(u4 x) {\n@@ -559,1 +541,1 @@\n-void DumpWriter::write_classID(Klass* k) {\n+void AbstractDumpWriter::write_classID(Klass* k) {\n@@ -563,1 +545,1 @@\n-void DumpWriter::finish_dump_segment() {\n+void AbstractDumpWriter::finish_dump_segment(bool force_flush) {\n@@ -574,0 +556,4 @@\n+    } else {\n+      \/\/ Finish process huge sub record\n+      \/\/ Set _is_huge_sub_record to false so the parallel dump writer can flush data to file.\n+      _is_huge_sub_record = false;\n@@ -576,1 +562,0 @@\n-    flush();\n@@ -578,0 +563,1 @@\n+    flush(force_flush);\n@@ -581,1 +567,1 @@\n-void DumpWriter::start_sub_record(u1 tag, u4 len) {\n+void AbstractDumpWriter::start_sub_record(u1 tag, u4 len) {\n@@ -587,1 +573,1 @@\n-    assert(position() == 0, \"Must be at the start\");\n+    assert(position() == 0 && buffer_size() > dump_segment_header_size, \"Must be at the start\");\n@@ -594,0 +580,1 @@\n+    assert(Bytes::get_Java_u4((address)(buffer() + 5)) == len, \"Inconsitent size!\");\n@@ -611,1 +598,1 @@\n-void DumpWriter::end_sub_record() {\n+void AbstractDumpWriter::end_sub_record() {\n@@ -618,0 +605,275 @@\n+\/\/ Supports I\/O operations for a dump\n+\n+class DumpWriter : public AbstractDumpWriter {\n+ private:\n+  CompressionBackend _backend; \/\/ Does the actual writing.\n+ protected:\n+  virtual void flush(bool force = false);\n+\n+ public:\n+  \/\/ Takes ownership of the writer and compressor.\n+  DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor);\n+\n+  \/\/ total number of bytes written to the disk\n+  virtual julong bytes_written() const          { return (julong) _backend.get_written(); }\n+\n+  virtual char const* error() const             { return _backend.error(); }\n+\n+  \/\/ Called by threads used for parallel writing.\n+  void writer_loop()                    { _backend.thread_loop(); }\n+  \/\/ Called when finish to release the threads.\n+  virtual void deactivate()             { flush(); _backend.deactivate(); }\n+  \/\/ Get the backend pointer, used by parallel dump writer.\n+  CompressionBackend* backend_ptr() { return &_backend; }\n+\n+};\n+\n+\/\/ Check for error after constructing the object and destroy it in case of an error.\n+DumpWriter::DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor) :\n+  AbstractDumpWriter(),\n+  _backend(writer, compressor, io_buffer_max_size, io_buffer_max_waste) {\n+  flush();\n+}\n+\n+\/\/ flush any buffered bytes to the file\n+void DumpWriter::flush(bool force) {\n+  _backend.get_new_buffer(&_buffer, &_pos, &_size, force);\n+}\n+\n+\/\/ Buffer queue used for parallel dump.\n+struct ParWriterBufferQueueElem {\n+  char* _buffer;\n+  size_t _used;\n+  ParWriterBufferQueueElem* _next;\n+};\n+\n+class ParWriterBufferQueue : public CHeapObj<mtInternal> {\n+ private:\n+  ParWriterBufferQueueElem* _head;\n+  ParWriterBufferQueueElem* _tail;\n+  uint _length;\n+ public:\n+  ParWriterBufferQueue() : _head(NULL), _tail(NULL), _length(0) { }\n+\n+  void enqueue(ParWriterBufferQueueElem* entry) {\n+    if (_head == NULL) {\n+      assert(is_empty() && _tail == NULL, \"Sanity check\");\n+      _head = _tail = entry;\n+    } else {\n+      assert ((_tail->_next == NULL && _tail->_buffer != NULL), \"Buffer queue is polluted\");\n+      _tail->_next = entry;\n+      _tail = entry;\n+    }\n+    _length++;\n+    assert(_tail->_next == NULL, \"Bufer queue is polluted\");\n+  }\n+\n+  ParWriterBufferQueueElem* dequeue() {\n+    if (_head == NULL)  return NULL;\n+    ParWriterBufferQueueElem* entry = _head;\n+    assert (entry->_buffer != NULL, \"polluted buffer in writer list\");\n+    _head = entry->_next;\n+    if (_head == NULL) {\n+      _tail = NULL;\n+    }\n+    entry->_next = NULL;\n+    _length--;\n+    return entry;\n+  }\n+\n+  bool is_empty() {\n+    return _length == 0;\n+  }\n+\n+  uint length() { return _length; }\n+};\n+\n+\/\/ Support parallel heap dump.\n+class ParDumpWriter : public AbstractDumpWriter {\n+ private:\n+  \/\/ Lock used to guarantee the integrity of multiple buffers writing.\n+  static Monitor* _lock;\n+  \/\/ Pointer of backend from global DumpWriter.\n+  CompressionBackend* _backend_ptr;\n+  char const * _err;\n+  ParWriterBufferQueue* _buffer_queue;\n+  size_t _internal_buffer_used;\n+  char* _buffer_base;\n+  bool _split_data;\n+  static const uint BackendFlushThreshold = 2;\n+ protected:\n+  virtual void flush(bool force = false) {\n+    assert(_pos != 0, \"must not be zero\");\n+    if (_pos != 0) {\n+      refresh_buffer();\n+    }\n+\n+    if (_split_data || _is_huge_sub_record) {\n+      return;\n+    }\n+\n+    if (should_flush_buf_list(force)) {\n+      assert(!_in_dump_segment && !_split_data && !_is_huge_sub_record, \"incomplete data send to backend!\\n\");\n+      flush_to_backend(force);\n+    }\n+  }\n+\n+ public:\n+  \/\/ Check for error after constructing the object and destroy it in case of an error.\n+  ParDumpWriter(DumpWriter* dw) :\n+    AbstractDumpWriter(),\n+    _backend_ptr(dw->backend_ptr()),\n+    _buffer_queue((new (std::nothrow) ParWriterBufferQueue())),\n+    _buffer_base(NULL),\n+    _split_data(false) {\n+    \/\/ prepare internal buffer\n+    allocate_internal_buffer();\n+  }\n+\n+  ~ParDumpWriter() {\n+     assert(_buffer_queue != NULL, \"Sanity check\");\n+     assert((_internal_buffer_used == 0) && (_buffer_queue->is_empty()),\n+            \"All data must be send to backend\");\n+     if (_buffer_base != NULL) {\n+       os::free(_buffer_base);\n+       _buffer_base = NULL;\n+     }\n+     delete _buffer_queue;\n+     _buffer_queue = NULL;\n+  }\n+\n+  \/\/ total number of bytes written to the disk\n+  virtual julong bytes_written() const          { return (julong) _backend_ptr->get_written(); }\n+  virtual char const* error() const { return _err == NULL ? _backend_ptr->error() : _err; }\n+\n+  static void before_work() {\n+    assert(_lock == NULL, \"ParDumpWriter lock must be initialized only once\");\n+    _lock = new (std::nothrow) PaddedMonitor(Mutex::leaf, \"Parallel HProf writer lock\", Mutex::_safepoint_check_never);\n+  }\n+\n+  static void after_work() {\n+    assert(_lock != NULL, \"ParDumpWriter lock is not initialized\");\n+    delete _lock;\n+    _lock = NULL;\n+  }\n+\n+  \/\/ write raw bytes\n+  virtual void write_raw(void* s, size_t len) {\n+    assert(!_in_dump_segment || (_sub_record_left >= len), \"sub-record too large\");\n+    debug_only(_sub_record_left -= len);\n+    assert(!_split_data, \"Invalid split data\");\n+    _split_data = true;\n+    \/\/ flush buffer to make room.\n+    while (len > buffer_size() - position()) {\n+      assert(!_in_dump_segment || _is_huge_sub_record,\n+             \"Cannot overflow in non-huge sub-record.\");\n+      size_t to_write = buffer_size() - position();\n+      memcpy(buffer() + position(), s, to_write);\n+      s = (void*) ((char*) s + to_write);\n+      len -= to_write;\n+      set_position(position() + to_write);\n+      flush();\n+    }\n+    _split_data = false;\n+    memcpy(buffer() + position(), s, len);\n+    set_position(position() + len);\n+  }\n+\n+  virtual void deactivate()             { flush(true); _backend_ptr->deactivate(); }\n+\n+ private:\n+  void allocate_internal_buffer() {\n+    assert(_buffer_queue != NULL, \"Internal buffer queue is not ready when allocate internal buffer\");\n+    assert(_buffer == NULL && _buffer_base == NULL, \"current buffer must be NULL before allocate\");\n+    _buffer_base = _buffer = (char*)os::malloc(io_buffer_max_size, mtInternal);\n+    if (_buffer == NULL) {\n+      set_error(\"Could not allocate buffer for writer\");\n+      return;\n+    }\n+    _pos = 0;\n+    _internal_buffer_used = 0;\n+    _size = io_buffer_max_size;\n+  }\n+\n+  void set_error(char const* new_error) {\n+    if ((new_error != NULL) && (_err == NULL)) {\n+      _err = new_error;\n+    }\n+  }\n+\n+  \/\/ Add buffer to internal list\n+  void refresh_buffer() {\n+    size_t expected_total = _internal_buffer_used + _pos;\n+    if (expected_total < io_buffer_max_size - io_buffer_max_waste) {\n+      \/\/ reuse current buffer.\n+      _internal_buffer_used = expected_total;\n+      assert(_size - _pos == io_buffer_max_size - expected_total, \"illegal resize of buffer\");\n+      _size -= _pos;\n+      _buffer += _pos;\n+      _pos = 0;\n+\n+      return;\n+    }\n+    \/\/ It is not possible here that expected_total is larger than io_buffer_max_size because\n+    \/\/ of limitation in write_xxx().\n+    assert(expected_total <= io_buffer_max_size, \"buffer overflow\");\n+    assert(_buffer - _buffer_base <= io_buffer_max_size, \"internal buffer overflow\");\n+    ParWriterBufferQueueElem* entry =\n+        (ParWriterBufferQueueElem*)os::malloc(sizeof(ParWriterBufferQueueElem), mtInternal);\n+    if (entry == NULL) {\n+      set_error(\"Heap dumper can allocate memory\");\n+      return;\n+    }\n+    entry->_buffer = _buffer_base;\n+    entry->_used = expected_total;\n+    entry->_next = NULL;\n+    \/\/ add to internal buffer queue\n+    _buffer_queue->enqueue(entry);\n+    _buffer_base =_buffer = NULL;\n+    allocate_internal_buffer();\n+  }\n+\n+  void reclaim_entry(ParWriterBufferQueueElem* entry) {\n+    assert(entry != NULL && entry->_buffer != NULL, \"Invalid entry to reclaim\");\n+    os::free(entry->_buffer);\n+    entry->_buffer = NULL;\n+    os::free(entry);\n+  }\n+\n+  void flush_buffer(char* buffer, size_t used) {\n+    assert(_lock->owner() == Thread::current(), \"flush buffer must hold lock\");\n+    size_t max = io_buffer_max_size;\n+    \/\/ get_new_buffer\n+    _backend_ptr->flush_external_buffer(buffer, used, max);\n+  }\n+\n+  bool should_flush_buf_list(bool force) {\n+    return force || _buffer_queue->length() > BackendFlushThreshold;\n+  }\n+\n+  void flush_to_backend(bool force) {\n+    \/\/ Guarantee there is only one writer updating the backend buffers.\n+    MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n+    while (!_buffer_queue->is_empty()) {\n+      ParWriterBufferQueueElem* entry = _buffer_queue->dequeue();\n+      flush_buffer(entry->_buffer, entry->_used);\n+      \/\/ Delete buffer and entry.\n+      reclaim_entry(entry);\n+      entry = NULL;\n+    }\n+    assert(_pos == 0, \"available buffer must be empty before flush\");\n+    \/\/ Flush internal buffer.\n+    if (_internal_buffer_used > 0) {\n+      flush_buffer(_buffer_base, _internal_buffer_used);\n+      os::free(_buffer_base);\n+      _pos = 0;\n+      _internal_buffer_used = 0;\n+      _buffer_base = _buffer = NULL;\n+      \/\/ Allocate internal buffer for future use.\n+      allocate_internal_buffer();\n+    }\n+  }\n+};\n+\n+Monitor* ParDumpWriter::_lock = NULL;\n+\n@@ -624,1 +886,1 @@\n-  static void write_header(DumpWriter* writer, hprofTag tag, u4 len);\n+  static void write_header(AbstractDumpWriter* writer, hprofTag tag, u4 len);\n@@ -637,1 +899,1 @@\n-  static void dump_float(DumpWriter* writer, jfloat f);\n+  static void dump_float(AbstractDumpWriter* writer, jfloat f);\n@@ -639,1 +901,1 @@\n-  static void dump_double(DumpWriter* writer, jdouble d);\n+  static void dump_double(AbstractDumpWriter* writer, jdouble d);\n@@ -641,1 +903,1 @@\n-  static void dump_field_value(DumpWriter* writer, char type, oop obj, int offset);\n+  static void dump_field_value(AbstractDumpWriter* writer, char type, oop obj, int offset);\n@@ -645,1 +907,1 @@\n-  static void dump_static_fields(DumpWriter* writer, Klass* k);\n+  static void dump_static_fields(AbstractDumpWriter* writer, Klass* k);\n@@ -647,1 +909,1 @@\n-  static void dump_instance_fields(DumpWriter* writer, oop o);\n+  static void dump_instance_fields(AbstractDumpWriter* writer, oop o);\n@@ -651,1 +913,1 @@\n-  static void dump_instance_field_descriptors(DumpWriter* writer, Klass* k);\n+  static void dump_instance_field_descriptors(AbstractDumpWriter* writer, Klass* k);\n@@ -653,1 +915,1 @@\n-  static void dump_instance(DumpWriter* writer, oop o);\n+  static void dump_instance(AbstractDumpWriter* writer, oop o);\n@@ -656,1 +918,1 @@\n-  static void dump_class_and_array_classes(DumpWriter* writer, Klass* k);\n+  static void dump_class_and_array_classes(AbstractDumpWriter* writer, Klass* k);\n@@ -659,1 +921,1 @@\n-  static void dump_basic_type_array_class(DumpWriter* writer, Klass* k);\n+  static void dump_basic_type_array_class(AbstractDumpWriter* writer, Klass* k);\n@@ -662,1 +924,1 @@\n-  static void dump_object_array(DumpWriter* writer, objArrayOop array);\n+  static void dump_object_array(AbstractDumpWriter* writer, objArrayOop array);\n@@ -664,1 +926,1 @@\n-  static void dump_prim_array(DumpWriter* writer, typeArrayOop array);\n+  static void dump_prim_array(AbstractDumpWriter* writer, typeArrayOop array);\n@@ -666,1 +928,1 @@\n-  static void dump_stack_frame(DumpWriter* writer, int frame_serial_num, int class_serial_num, Method* m, int bci);\n+  static void dump_stack_frame(AbstractDumpWriter* writer, int frame_serial_num, int class_serial_num, Method* m, int bci);\n@@ -669,1 +931,1 @@\n-  static int calculate_array_max_length(DumpWriter* writer, arrayOop array, short header_size);\n+  static int calculate_array_max_length(AbstractDumpWriter* writer, arrayOop array, short header_size);\n@@ -672,1 +934,1 @@\n-  static void end_of_dump(DumpWriter* writer);\n+  static void end_of_dump(AbstractDumpWriter* writer);\n@@ -686,1 +948,1 @@\n-void DumperSupport:: write_header(DumpWriter* writer, hprofTag tag, u4 len) {\n+void DumperSupport:: write_header(AbstractDumpWriter* writer, hprofTag tag, u4 len) {\n@@ -740,1 +1002,1 @@\n-void DumperSupport::dump_float(DumpWriter* writer, jfloat f) {\n+void DumperSupport::dump_float(AbstractDumpWriter* writer, jfloat f) {\n@@ -754,1 +1016,1 @@\n-void DumperSupport::dump_double(DumpWriter* writer, jdouble d) {\n+void DumperSupport::dump_double(AbstractDumpWriter* writer, jdouble d) {\n@@ -769,1 +1031,1 @@\n-void DumperSupport::dump_field_value(DumpWriter* writer, char type, oop obj, int offset) {\n+void DumperSupport::dump_field_value(AbstractDumpWriter* writer, char type, oop obj, int offset) {\n@@ -887,1 +1149,1 @@\n-void DumperSupport::dump_static_fields(DumpWriter* writer, Klass* k) {\n+void DumperSupport::dump_static_fields(AbstractDumpWriter* writer, Klass* k) {\n@@ -930,1 +1192,1 @@\n-void DumperSupport::dump_instance_fields(DumpWriter* writer, oop o) {\n+void DumperSupport::dump_instance_fields(AbstractDumpWriter* writer, oop o) {\n@@ -953,1 +1215,1 @@\n-void DumperSupport::dump_instance_field_descriptors(DumpWriter* writer, Klass* k) {\n+void DumperSupport::dump_instance_field_descriptors(AbstractDumpWriter* writer, Klass* k) {\n@@ -968,1 +1230,1 @@\n-void DumperSupport::dump_instance(DumpWriter* writer, oop o) {\n+void DumperSupport::dump_instance(AbstractDumpWriter* writer, oop o) {\n@@ -991,1 +1253,1 @@\n-void DumperSupport::dump_class_and_array_classes(DumpWriter* writer, Klass* k) {\n+void DumperSupport::dump_class_and_array_classes(AbstractDumpWriter* writer, Klass* k) {\n@@ -1080,1 +1342,1 @@\n-void DumperSupport::dump_basic_type_array_class(DumpWriter* writer, Klass* k) {\n+void DumperSupport::dump_basic_type_array_class(AbstractDumpWriter* writer, Klass* k) {\n@@ -1115,1 +1377,1 @@\n-int DumperSupport::calculate_array_max_length(DumpWriter* writer, arrayOop array, short header_size) {\n+int DumperSupport::calculate_array_max_length(AbstractDumpWriter* writer, arrayOop array, short header_size) {\n@@ -1142,1 +1404,1 @@\n-void DumperSupport::dump_object_array(DumpWriter* writer, objArrayOop array) {\n+void DumperSupport::dump_object_array(AbstractDumpWriter* writer, objArrayOop array) {\n@@ -1176,1 +1438,1 @@\n-void DumperSupport::dump_prim_array(DumpWriter* writer, typeArrayOop array) {\n+void DumperSupport::dump_prim_array(AbstractDumpWriter* writer, typeArrayOop array) {\n@@ -1178,1 +1440,0 @@\n-\n@@ -1270,1 +1531,1 @@\n-void DumperSupport::dump_stack_frame(DumpWriter* writer,\n+void DumperSupport::dump_stack_frame(AbstractDumpWriter* writer,\n@@ -1299,2 +1560,2 @@\n-  DumpWriter* _writer;\n-  DumpWriter* writer() const                { return _writer; }\n+  AbstractDumpWriter* _writer;\n+  AbstractDumpWriter* writer() const                { return _writer; }\n@@ -1302,1 +1563,1 @@\n-  SymbolTableDumper(DumpWriter* writer)     { _writer = writer; }\n+  SymbolTableDumper(AbstractDumpWriter* writer)     { _writer = writer; }\n@@ -1322,1 +1583,1 @@\n-  DumpWriter* _writer;\n+  AbstractDumpWriter* _writer;\n@@ -1325,1 +1586,1 @@\n-  DumpWriter* writer() const                { return _writer; }\n+  AbstractDumpWriter* writer() const                { return _writer; }\n@@ -1327,1 +1588,1 @@\n-  JNILocalsDumper(DumpWriter* writer, u4 thread_serial_num) {\n+  JNILocalsDumper(AbstractDumpWriter* writer, u4 thread_serial_num) {\n@@ -1356,2 +1617,2 @@\n-  DumpWriter* _writer;\n-  DumpWriter* writer() const                { return _writer; }\n+  AbstractDumpWriter* _writer;\n+  AbstractDumpWriter* writer() const                { return _writer; }\n@@ -1360,1 +1621,1 @@\n-  JNIGlobalsDumper(DumpWriter* writer) {\n+  JNIGlobalsDumper(AbstractDumpWriter* writer) {\n@@ -1372,1 +1633,0 @@\n-\n@@ -1387,2 +1647,2 @@\n-  DumpWriter* _writer;\n-  DumpWriter* writer() const                { return _writer; }\n+  AbstractDumpWriter* _writer;\n+  AbstractDumpWriter* writer() const                { return _writer; }\n@@ -1390,1 +1650,1 @@\n-  StickyClassDumper(DumpWriter* writer) {\n+  StickyClassDumper(AbstractDumpWriter* writer) {\n@@ -1404,0 +1664,67 @@\n+\/\/ Large object heap dump support.\n+\/\/ To avoid memory consumption, when dumping large objects such as huge array and\n+\/\/ large objects whose size are larger than LARGE_OBJECT_DUMP_THRESHOLD, the scanned\n+\/\/ partial object\/array data will be sent to the backend directly instead of caching\n+\/\/ the whole object\/array in the internal buffer.\n+\/\/ The HeapDumpLargeObjectList is used to save the large object when dumper scans\n+\/\/ the heap. The large objects could be added (push) parallelly by multiple dumpers,\n+\/\/ But they will be removed (popped) serially only by the VM thread.\n+class HeapDumpLargeObjectList : public CHeapObj<mtInternal> {\n+ private:\n+  class HeapDumpLargeObjectListElem : public CHeapObj<mtInternal> {\n+   public:\n+    HeapDumpLargeObjectListElem(oop obj) : _obj(obj), _next(NULL) { }\n+    oop _obj;\n+    volatile HeapDumpLargeObjectListElem* _next;\n+  };\n+\n+  volatile HeapDumpLargeObjectListElem* _head;\n+\n+ public:\n+  HeapDumpLargeObjectList() : _head(NULL) { }\n+\n+  void atomic_push(oop obj) {\n+    assert (obj != NULL, \"sanity check\");\n+    HeapDumpLargeObjectListElem* entry = new HeapDumpLargeObjectListElem(obj);\n+    if (entry == NULL) {\n+      warning(\"failed to allocate element for large object list\");\n+      return;\n+    }\n+    assert (entry->_obj != NULL, \"sanity check\");\n+    while (true) {\n+      volatile HeapDumpLargeObjectListElem* old_head = Atomic::load_acquire(&_head);\n+      volatile HeapDumpLargeObjectListElem* new_head = entry;\n+      if (Atomic::cmpxchg(&_head, old_head, new_head) == old_head) {\n+        \/\/ successfully push\n+        new_head->_next = old_head;\n+        assert((oop)new_head->_obj == obj, \"must equal\");\n+        return;\n+      }\n+    }\n+  }\n+\n+  oop pop() {\n+    if (_head == NULL) {\n+      return NULL;\n+    }\n+    volatile HeapDumpLargeObjectListElem* entry = _head;\n+    _head = _head->_next;\n+    assert (entry != NULL, \"illegal larger object list entry\");\n+    oop ret = (oop)entry->_obj;\n+    delete entry;\n+    assert (ret != NULL, \"illegal oop pointer\");\n+    return ret;\n+  }\n+\n+  void drain(ObjectClosure* cl) {\n+    while (_head !=  NULL) {\n+      cl->do_object(pop());\n+    }\n+  }\n+\n+  bool is_empty() {\n+    return _head == NULL;\n+  }\n+\n+  static const size_t LargeObjectSizeThreshold = 1 << 20; \/\/ 1 MB\n+};\n@@ -1408,1 +1735,0 @@\n-\n@@ -1411,3 +1737,2 @@\n-  DumpWriter* _writer;\n-\n-  DumpWriter* writer()                  { return _writer; }\n+  AbstractDumpWriter* _writer;\n+  HeapDumpLargeObjectList* _list;\n@@ -1415,0 +1740,2 @@\n+  AbstractDumpWriter* writer()                  { return _writer; }\n+  bool is_large(oop o);\n@@ -1416,1 +1743,1 @@\n-  HeapObjectDumper(DumpWriter* writer) {\n+  HeapObjectDumper(AbstractDumpWriter* writer, HeapDumpLargeObjectList* list = NULL) {\n@@ -1418,0 +1745,1 @@\n+    _list = list;\n@@ -1437,0 +1765,7 @@\n+  \/\/ If large object list exists and it is large object\/array,\n+  \/\/ add oop into the list and skip scan. VM thread will process it later.\n+  if (_list != NULL && is_large(o)) {\n+    _list->atomic_push(o);\n+    return;\n+  }\n+\n@@ -1449,0 +1784,73 @@\n+bool HeapObjectDumper::is_large(oop o) {\n+  size_t size = 0;\n+  if (o->is_instance()) {\n+    \/\/ Use o->size() * 8 as the upper limit of instance size to avoid iterating static fields\n+    size = o->size() * 8;\n+  } else if (o->is_objArray()) {\n+    objArrayOop array = objArrayOop(o);\n+    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n+    assert(type >= T_BOOLEAN && type <= T_OBJECT, \"invalid array element type\");\n+    int length = array->length();\n+    int type_size = sizeof(address);\n+    size = (size_t)length * type_size;\n+  } else if (o->is_typeArray()) {\n+    typeArrayOop array = typeArrayOop(o);\n+    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n+    assert(type >= T_BOOLEAN && type <= T_OBJECT, \"invalid array element type\");\n+    int length = array->length();\n+    int type_size = type2aelembytes(type);\n+    size = (size_t)length * type_size;\n+  }\n+  return size > HeapDumpLargeObjectList::LargeObjectSizeThreshold;\n+}\n+\n+\/\/ The dumper controller for parallel heap dump\n+class DumperController : public CHeapObj<mtInternal> {\n+ private:\n+   bool     _started;\n+   Monitor* _lock;\n+   uint   _dumper_number;\n+   uint   _complete_number;\n+\n+ public:\n+   DumperController(uint number) :\n+     _started(false),\n+     _lock(new (std::nothrow) PaddedMonitor(Mutex::leaf, \"Dumper Controller lock\",\n+    Mutex::_safepoint_check_never)),\n+     _dumper_number(number),\n+     _complete_number(0) { }\n+\n+   ~DumperController() { delete _lock; }\n+\n+   void wait_for_start_signal() {\n+     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n+     while (_started == false) {\n+       ml.wait();\n+     }\n+     assert(_started == true,  \"dumper woke up with wrong state\");\n+   }\n+\n+   void start_dump() {\n+     assert (_started == false, \"start dump with wrong state\");\n+     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n+     _started = true;\n+     ml.notify_all();\n+   }\n+\n+   void dumper_complete() {\n+     assert (_started == true, \"dumper complete with wrong state\");\n+     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n+     _complete_number++;\n+     ml.notify();\n+   }\n+\n+   void wait_all_dumpers_complete() {\n+     assert (_started == true, \"wrong state when wait for dumper complete\");\n+     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n+     while (_complete_number != _dumper_number) {\n+        ml.wait();\n+     }\n+     _started = false;\n+   }\n+};\n+\n@@ -1452,9 +1860,69 @@\n-  static VM_HeapDumper* _global_dumper;\n-  static DumpWriter*    _global_writer;\n-  DumpWriter*           _local_writer;\n-  JavaThread*           _oome_thread;\n-  Method*               _oome_constructor;\n-  bool _gc_before_heap_dump;\n-  GrowableArray<Klass*>* _klass_map;\n-  ThreadStackTrace** _stack_traces;\n-  int _num_threads;\n+  static VM_HeapDumper*   _global_dumper;\n+  static DumpWriter*      _global_writer;\n+  DumpWriter*             _local_writer;\n+  JavaThread*             _oome_thread;\n+  Method*                 _oome_constructor;\n+  bool                    _gc_before_heap_dump;\n+  GrowableArray<Klass*>*  _klass_map;\n+  ThreadStackTrace**      _stack_traces;\n+  int                     _num_threads;\n+  \/\/ parallel heap dump support\n+  uint                    _num_dumper_threads;\n+  uint                    _num_writer_threads;\n+  DumperController*       _dumper_controller;\n+  ParallelObjectIterator* _poi;\n+  HeapDumpLargeObjectList* _large_object_list;\n+\n+  \/\/ VMDumperType is for thread that dumps both heap and non-heap data.\n+  static const size_t VMDumperType = 0;\n+  static const size_t WriterType = 1;\n+  static const size_t DumperType = 2;\n+  \/\/ worker id of VMDumper thread.\n+  static const size_t VMDumperWorkerId = 0;\n+\n+  size_t get_worker_type(uint worker_id) {\n+    assert(_num_writer_threads >= 1, \"Must be at least one writer\");\n+    \/\/ worker id of VMDumper that dump heap and non-heap data\n+    if (worker_id == VMDumperWorkerId) {\n+      return VMDumperType;\n+    }\n+\n+    \/\/ worker id of dumper starts from 1, which only dump heap datar\n+    if (worker_id < _num_dumper_threads) {\n+      return DumperType;\n+    }\n+\n+    \/\/ worker id of writer starts from _num_dumper_threads\n+    return WriterType;\n+  }\n+\n+  void prepare_parallel_dump(uint num_total) {\n+    assert (_dumper_controller == NULL, \"dumper controller must be NULL\");\n+    assert (num_total > 0, \"active workers number must >= 1\");\n+    \/\/ Dumper threads number must not be larger than active workers number.\n+    if (num_total < _num_dumper_threads) {\n+      _num_dumper_threads = num_total - 1;\n+    }\n+    \/\/ Calculate dumper and writer threads number.\n+    _num_writer_threads = num_total - _num_dumper_threads;\n+    \/\/ If dumper threads number is 1, only the VMThread works as a dumper.\n+    \/\/ If dumper threads number is equal to active workers, need at lest one worker thread as writer.\n+    if (_num_dumper_threads > 0 && _num_writer_threads == 0) {\n+      _num_writer_threads = 1;\n+      _num_dumper_threads = num_total - _num_writer_threads;\n+    }\n+    \/\/ Number of dumper threads that only iterate heap.\n+    uint _heap_only_dumper_threads = _num_dumper_threads - 1 \/* VMDumper thread *\/;\n+    \/\/ Prepare parallel writer.\n+    if (_num_dumper_threads > 1) {\n+      ParDumpWriter::before_work();\n+      _dumper_controller = new (std::nothrow) DumperController(_heap_only_dumper_threads);\n+      _poi = Universe::heap()->parallel_object_iterator(_num_dumper_threads);\n+    }\n+  }\n+\n+  void finish_parallel_dump() {\n+    if (_num_dumper_threads > 1) {\n+      ParDumpWriter::after_work();\n+    }\n+  }\n@@ -1501,0 +1969,3 @@\n+  \/\/ large objects\n+  void dump_large_objects(ObjectClosure* writer);\n+\n@@ -1502,1 +1973,1 @@\n-  VM_HeapDumper(DumpWriter* writer, bool gc_before_heap_dump, bool oome) :\n+  VM_HeapDumper(DumpWriter* writer, bool gc_before_heap_dump, bool oome, uint num_dump_threads) :\n@@ -1513,0 +1984,4 @@\n+    _num_dumper_threads = num_dump_threads;\n+    _dumper_controller = NULL;\n+    _poi = NULL;\n+    _large_object_list = new (std::nothrow) HeapDumpLargeObjectList();\n@@ -1526,0 +2001,1 @@\n+\n@@ -1533,0 +2009,8 @@\n+    if (_poi != NULL) {\n+      delete _poi;\n+      _poi = NULL;\n+    }\n+    if (_dumper_controller != NULL) {\n+      delete _dumper_controller;\n+      _dumper_controller = NULL;\n+    }\n@@ -1534,0 +2018,1 @@\n+    delete _large_object_list;\n@@ -1541,1 +2026,0 @@\n-\n@@ -1550,1 +2034,1 @@\n-void DumperSupport::end_of_dump(DumpWriter* writer) {\n+void DumperSupport::end_of_dump(AbstractDumpWriter* writer) {\n@@ -1774,0 +2258,1 @@\n+    prepare_parallel_dump(gang->active_workers());\n@@ -1775,0 +2260,1 @@\n+    finish_parallel_dump();\n@@ -1784,24 +2270,28 @@\n-    writer()->writer_loop();\n-    return;\n-  }\n-\n-  \/\/ Write the file header - we always use 1.0.2\n-  const char* header = \"JAVA PROFILE 1.0.2\";\n-\n-  \/\/ header is few bytes long - no chance to overflow int\n-  writer()->write_raw((void*)header, (int)strlen(header));\n-  writer()->write_u1(0); \/\/ terminator\n-  writer()->write_u4(oopSize);\n-  \/\/ timestamp is current time in ms\n-  writer()->write_u8(os::javaTimeMillis());\n-\n-  \/\/ HPROF_UTF8 records\n-  SymbolTableDumper sym_dumper(writer());\n-  SymbolTable::symbols_do(&sym_dumper);\n-\n-  \/\/ write HPROF_LOAD_CLASS records\n-  {\n-    LockedClassesDo locked_load_classes(&do_load_class);\n-    ClassLoaderDataGraph::classes_do(&locked_load_classes);\n-  }\n-  Universe::basic_type_classes_do(&do_load_class);\n+    if (get_worker_type(worker_id) == WriterType) {\n+      writer()->writer_loop();\n+      return;\n+    }\n+    if (_num_dumper_threads > 1 && get_worker_type(worker_id) == DumperType) {\n+      _dumper_controller->wait_for_start_signal();\n+    }\n+  } else {\n+    \/\/ The worker 0 on all non-heap data dumping and part of heap iteration.\n+    \/\/ Write the file header - we always use 1.0.2\n+    const char* header = \"JAVA PROFILE 1.0.2\";\n+\n+    \/\/ header is few bytes long - no chance to overflow int\n+    writer()->write_raw((void*)header, (int)strlen(header));\n+    writer()->write_u1(0); \/\/ terminator\n+    writer()->write_u4(oopSize);\n+    \/\/ timestamp is current time in ms\n+    writer()->write_u8(os::javaTimeMillis());\n+    \/\/ HPROF_UTF8 records\n+    SymbolTableDumper sym_dumper(writer());\n+    SymbolTable::symbols_do(&sym_dumper);\n+\n+    \/\/ write HPROF_LOAD_CLASS records\n+    {\n+      LockedClassesDo locked_load_classes(&do_load_class);\n+      ClassLoaderDataGraph::classes_do(&locked_load_classes);\n+    }\n+    Universe::basic_type_classes_do(&do_load_class);\n@@ -1809,3 +2299,3 @@\n-  \/\/ write HPROF_FRAME and HPROF_TRACE records\n-  \/\/ this must be called after _klass_map is built when iterating the classes above.\n-  dump_stack_traces();\n+    \/\/ write HPROF_FRAME and HPROF_TRACE records\n+    \/\/ this must be called after _klass_map is built when iterating the classes above.\n+    dump_stack_traces();\n@@ -1813,4 +2303,22 @@\n-  \/\/ Writes HPROF_GC_CLASS_DUMP records\n-  {\n-    LockedClassesDo locked_dump_class(&do_class_dump);\n-    ClassLoaderDataGraph::classes_do(&locked_dump_class);\n+    \/\/ Writes HPROF_GC_CLASS_DUMP records\n+    {\n+      LockedClassesDo locked_dump_class(&do_class_dump);\n+      ClassLoaderDataGraph::classes_do(&locked_dump_class);\n+    }\n+    Universe::basic_type_classes_do(&do_basic_type_array_class_dump);\n+\n+    \/\/ HPROF_GC_ROOT_THREAD_OBJ + frames + jni locals\n+    do_threads();\n+\n+    \/\/ HPROF_GC_ROOT_JNI_GLOBAL\n+    JNIGlobalsDumper jni_dumper(writer());\n+    JNIHandles::oops_do(&jni_dumper);\n+    \/\/ technically not jni roots, but global roots\n+    \/\/ for things like preallocated throwable backtraces\n+    Universe::vm_global()->oops_do(&jni_dumper);\n+\n+    \/\/ HPROF_GC_ROOT_STICKY_CLASS\n+    \/\/ These should be classes in the NULL class loader data, and not all classes\n+    \/\/ if !ClassUnloading\n+    StickyClassDumper class_dumper(writer());\n+    ClassLoaderData::the_null_class_loader_data()->classes_do(&class_dumper);\n@@ -1818,2 +2326,0 @@\n-  Universe::basic_type_classes_do(&do_basic_type_array_class_dump);\n-\n@@ -1826,18 +2332,36 @@\n-  HeapObjectDumper obj_dumper(writer());\n-  Universe::heap()->object_iterate(&obj_dumper);\n-\n-  \/\/ HPROF_GC_ROOT_THREAD_OBJ + frames + jni locals\n-  do_threads();\n-\n-  \/\/ HPROF_GC_ROOT_JNI_GLOBAL\n-  JNIGlobalsDumper jni_dumper(writer());\n-  JNIHandles::oops_do(&jni_dumper);\n-  \/\/ technically not jni roots, but global roots\n-  \/\/ for things like preallocated throwable backtraces\n-  Universe::vm_global()->oops_do(&jni_dumper);\n-\n-  \/\/ HPROF_GC_ROOT_STICKY_CLASS\n-  \/\/ These should be classes in the NULL class loader data, and not all classes\n-  \/\/ if !ClassUnloading\n-  StickyClassDumper class_dumper(writer());\n-  ClassLoaderData::the_null_class_loader_data()->classes_do(&class_dumper);\n+  if (_num_dumper_threads <= 1) {\n+    HeapObjectDumper obj_dumper(writer());\n+    Universe::heap()->object_iterate(&obj_dumper);\n+  } else {\n+    assert(get_worker_type(worker_id) == DumperType\n+          || get_worker_type(worker_id) == VMDumperType,\n+          \"must be dumper thread to do heap iteration\");\n+    if (get_worker_type(worker_id) == VMDumperType) {\n+      \/\/ Clear global writer's buffer.\n+      writer()->finish_dump_segment(true);\n+      \/\/ Notify dumpers to start heap iteration.\n+      _dumper_controller->start_dump();\n+    }\n+    \/\/ Heap iteration.\n+    {\n+       ParDumpWriter pw(writer());\n+       {\n+         HeapObjectDumper obj_dumper(&pw, _large_object_list);\n+         _poi->object_iterate(&obj_dumper, worker_id);\n+       }\n+\n+       if (get_worker_type(worker_id) == VMDumperType) {\n+         _dumper_controller->wait_all_dumpers_complete();\n+         \/\/ clear internal buffer;\n+         pw.finish_dump_segment(true);\n+\n+         \/\/ refresh the global_writer's buffer and position;\n+         writer()->refresh();\n+\n+       } else {\n+         pw.finish_dump_segment(true);\n+         _dumper_controller->dumper_complete();\n+         return;\n+       }\n+    }\n+  }\n@@ -1845,0 +2369,4 @@\n+  assert(get_worker_type(worker_id) == VMDumperType, \"Heap dumper must be VMDumper\");\n+  \/\/ Use writer() rather than ParDumpWriter to avoid memory consumption.\n+  HeapObjectDumper obj_dumper(writer());\n+  dump_large_objects(&obj_dumper);\n@@ -1847,1 +2375,0 @@\n-\n@@ -1910,0 +2437,5 @@\n+\/\/ dump the large objects.\n+void VM_HeapDumper::dump_large_objects(ObjectClosure* cl) {\n+  _large_object_list->drain(cl);\n+}\n+\n@@ -1911,1 +2443,1 @@\n-int HeapDumper::dump(const char* path, outputStream* out, int compression, bool overwrite) {\n+int HeapDumper::dump(const char* path, outputStream* out, int compression, bool overwrite, uint num_dump_threads) {\n@@ -1919,1 +2451,0 @@\n-\n@@ -1946,1 +2477,1 @@\n-  VM_HeapDumper dumper(&writer, _gc_before_heap_dump, _oome);\n+  VM_HeapDumper dumper(&writer, _gc_before_heap_dump, _oome, num_dump_threads);\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":704,"deletions":173,"binary":false,"changes":877,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -74,1 +74,2 @@\n-  int dump(const char* path, outputStream* out = NULL, int compression = -1, bool overwrite = false);\n+  \/\/ parallel_thread_num >= 0 indicates thread numbers of parallel object dump\n+  int dump(const char* path, outputStream* out = NULL, int compression = -1, bool overwrite = false, uint parallel_thread_num = 1);\n","filename":"src\/hotspot\/share\/services\/heapDumper.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -240,4 +240,1 @@\n-void CompressionBackend::deactivate() {\n-  assert(_active, \"Must be active\");\n-\n-  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n+void CompressionBackend::flush_buffer(MonitorLocker* ml) {\n@@ -250,1 +247,1 @@\n-    ml.notify_all();\n+    ml->notify_all();\n@@ -257,0 +254,14 @@\n+}\n+\n+void CompressionBackend::flush_buffer() {\n+  assert(_active, \"Must be active\");\n+\n+  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n+  flush_buffer(&ml);\n+}\n+\n+void CompressionBackend::deactivate() {\n+  assert(_active, \"Must be active\");\n+\n+  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n+  flush_buffer(&ml);\n@@ -368,1 +379,26 @@\n-void CompressionBackend::get_new_buffer(char** buffer, size_t* used, size_t* max) {\n+void CompressionBackend::flush_external_buffer(char* buffer, size_t used, size_t max) {\n+  assert(buffer != NULL && used != 0 && max != 0, \"Invalid data send to compression backend\");\n+  assert(_active == true, \"Backend must be active when flushing external buffer\");\n+  char* buf;\n+  size_t tmp_used = 0;\n+  size_t tmp_max = 0;\n+\n+  MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n+  \/\/ First try current buffer. Use it if empty.\n+  if (_current->_in_used == 0) {\n+    buf = _current->_in;\n+  } else {\n+    \/\/ If current buffer is not clean, flush it.\n+    MutexUnlocker ml(_lock, Mutex::_no_safepoint_check_flag);\n+    get_new_buffer(&buf, &tmp_used, &tmp_max, true);\n+  }\n+  assert (_current->_in != NULL && _current->_in_max >= max &&\n+          _current->_in_used == 0, \"Invalid buffer from compression backend\");\n+  \/\/ Copy data to backend buffer.\n+  memcpy(buf, buffer, used);\n+\n+  assert(_current->_in == buf, \"Must be current\");\n+  _current->_in_used += used;\n+}\n+\n+void CompressionBackend::get_new_buffer(char** buffer, size_t* used, size_t* max, bool force_reset) {\n@@ -371,2 +407,1 @@\n-\n-    if (*used > 0) {\n+    if (*used > 0 || force_reset) {\n@@ -374,1 +409,0 @@\n-\n@@ -377,1 +411,1 @@\n-      if (_current->_in_max - _current->_in_used <= _max_waste) {\n+      if (_current->_in_max - _current->_in_used <= _max_waste || force_reset) {\n@@ -386,1 +420,0 @@\n-\n","filename":"src\/hotspot\/share\/services\/heapDumperCompression.cpp","additions":44,"deletions":11,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -207,0 +207,1 @@\n+  void flush_buffer(MonitorLocker* ml);\n@@ -223,0 +224,3 @@\n+  \/\/ Sets up an internal buffer, fills with external buffer, and sends to compressor.\n+  void flush_external_buffer(char* buffer, size_t used, size_t max);\n+\n@@ -224,1 +228,1 @@\n-  void get_new_buffer(char** buffer, size_t* used, size_t* max);\n+  void get_new_buffer(char** buffer, size_t* used, size_t* max, bool force_reset = false);\n@@ -231,0 +235,3 @@\n+\n+  \/\/ Flush all compressed data in buffer to file\n+  void flush_buffer();\n","filename":"src\/hotspot\/share\/services\/heapDumperCompression.hpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -97,1 +97,2 @@\n-            } else if ((access = GzipRandomAccess.getAccess(heapFile, 16)) != null) {\n+            } else if ((i >>> 8) == GZIP_HEADER_MAGIC) {\n+                \/\/ Possible gziped file, try decompress it and get the stack trace.\n@@ -99,2 +100,19 @@\n-                try (BufferedInputStream gzBis = new BufferedInputStream(access.asStream(0));\n-                     PositionDataInputStream pdin = new PositionDataInputStream(gzBis)) {\n+                String deCompressedFile = \"heapdump\" + System.currentTimeMillis() + \".hprof\";\n+                File out = new File(deCompressedFile);\n+                \/\/ Decompress to get dump file.\n+                try (FileInputStream heapFis = new FileInputStream(heapFile);\n+                     GZIPInputStream gis = new GZIPInputStream(heapFis);\n+                     FileOutputStream fos = new FileOutputStream(out)) {\n+                    byte[] buffer = new byte[1024 * 1024];\n+                    int len = 0;\n+                    while ((len = gis.read(buffer)) > 0) {\n+                        fos.write(buffer, 0, len);\n+                    }\n+                } catch (Exception e) {\n+                    out.delete();\n+                    throw new IOException(\"Cannot decompress the compressed hprof file\", e);\n+                }\n+                \/\/ Check dump data header and print stack trace.\n+                try (FileInputStream outFis = new FileInputStream(out);\n+                     BufferedInputStream outBis = new BufferedInputStream(outFis);\n+                     PositionDataInputStream pdin = new PositionDataInputStream(outBis)) {\n@@ -103,3 +121,3 @@\n-                        Reader r\n-                            = new HprofReader(access.asFileBuffer(), pdin, dumpNumber,\n-                                              callStack, debugLevel);\n+                        HprofReader r\n+                            = new HprofReader(deCompressedFile, pdin, dumpNumber,\n+                                              true, debugLevel);\n@@ -108,1 +126,1 @@\n-                        throw new IOException(\"Wrong magic number in gzipped file: \" + i);\n+                        throw new IOException(\"Unrecognized magic number found in decompressed data: \" + i);\n@@ -110,0 +128,2 @@\n+                } finally {\n+                    out.delete();\n","filename":"test\/lib\/jdk\/test\/lib\/hprof\/parser\/Reader.java","additions":27,"deletions":7,"binary":false,"changes":34,"status":"modified"}]}