{"files":[{"patch":"@@ -8883,0 +8883,22 @@\n+instruct castFF(vRegF dst)\n+%{\n+  match(Set dst (CastFF dst));\n+\n+  size(0);\n+  format %{ \"# castFF of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct castDD(vRegD dst)\n+%{\n+  match(Set dst (CastDD dst));\n+\n+  size(0);\n+  format %{ \"# castDD of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":22,"deletions":0,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -5294,0 +5294,17 @@\n+instruct castFF( regF dst ) %{\n+  match(Set dst (CastFF dst));\n+  format %{ \"! castFF of $dst\" %}\n+  ins_encode( \/*empty encoding*\/ );\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castDD( regD dst ) %{\n+  match(Set dst (CastDD dst));\n+  format %{ \"! castDD of $dst\" %}\n+  ins_encode( \/*empty encoding*\/ );\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+\n","filename":"src\/hotspot\/cpu\/arm\/arm.ad","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -10455,0 +10455,16 @@\n+instruct castFF(regF dst) %{\n+  match(Set dst (CastFF dst));\n+  format %{ \" -- \\t\/\/ castFF of $dst\" %}\n+  size(0);\n+  ins_encode( \/*empty*\/ );\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct castDD(regD dst) %{\n+  match(Set dst (CastDD dst));\n+  format %{ \" -- \\t\/\/ castDD of $dst\" %}\n+  size(0);\n+  ins_encode( \/*empty*\/ );\n+  ins_pipe(pipe_class_default);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -5369,0 +5369,16 @@\n+instruct castFF(regF dst) %{\n+  match(Set dst (CastFF dst));\n+  size(0);\n+  format %{ \"# castFF of $dst\" %}\n+  ins_encode(\/*empty*\/);\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct castDD(regD dst) %{\n+  match(Set dst (CastDD dst));\n+  size(0);\n+  format %{ \"# castDD of $dst\" %}\n+  ins_encode(\/*empty*\/);\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -7249,0 +7249,16 @@\n+instruct castFF( regF dst ) %{\n+  match(Set dst (CastFF dst));\n+  format %{ \"#castFF of $dst\" %}\n+  ins_encode( \/*empty encoding*\/ );\n+  ins_cost(0);\n+  ins_pipe( empty );\n+%}\n+\n+instruct castDD( regD dst ) %{\n+  match(Set dst (CastDD dst));\n+  format %{ \"#castDD of $dst\" %}\n+  ins_encode( \/*empty encoding*\/ );\n+  ins_cost(0);\n+  ins_pipe( empty );\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -7648,0 +7648,22 @@\n+instruct castFF(regF dst)\n+%{\n+  match(Set dst (CastFF dst));\n+\n+  size(0);\n+  format %{ \"# castFF of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castDD(regD dst)\n+%{\n+  match(Set dst (CastDD dst));\n+\n+  size(0);\n+  format %{ \"# castDD of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":22,"deletions":0,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -118,0 +118,20 @@\n+class CastFFNode: public ConstraintCastNode {\n+public:\n+  CastFFNode(Node* n, const Type* t, bool carry_dependency = false)\n+          : ConstraintCastNode(n, t, carry_dependency){\n+    init_class_id(Class_CastFF);\n+  }\n+  virtual int Opcode() const;\n+  virtual uint ideal_reg() const { return Op_RegF; }\n+};\n+\n+class CastDDNode: public ConstraintCastNode {\n+public:\n+  CastDDNode(Node* n, const Type* t, bool carry_dependency = false)\n+          : ConstraintCastNode(n, t, carry_dependency){\n+    init_class_id(Class_CastDD);\n+  }\n+  virtual int Opcode() const;\n+  virtual uint ideal_reg() const { return Op_RegD; }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/castnode.hpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -64,0 +64,2 @@\n+macro(CastDD)\n+macro(CastFF)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4994,6 +4994,0 @@\n-  \/\/ if this is a load, check for anti-dependent stores\n-  \/\/ We use a conservative algorithm to identify potential interfering\n-  \/\/ instructions and for rescheduling the load.  The users of the memory\n-  \/\/ input of this load are examined.  Any use which is not a load and is\n-  \/\/ dominated by early is considered a potentially interfering store.\n-  \/\/ This can produce false positives.\n@@ -5001,31 +4995,44 @@\n-    int load_alias_idx = C->get_alias_index(n->adr_type());\n-    if (C->alias_type(load_alias_idx)->is_rewritable()) {\n-      Unique_Node_List worklist;\n-\n-      Node* mem = n->in(MemNode::Memory);\n-      for (DUIterator_Fast imax, i = mem->fast_outs(imax); i < imax; i++) {\n-        Node* s = mem->fast_out(i);\n-        worklist.push(s);\n-      }\n-      for (uint i = 0; i < worklist.size() && LCA != early; i++) {\n-        Node* s = worklist.at(i);\n-        if (s->is_Load() || s->Opcode() == Op_SafePoint ||\n-            (s->is_CallStaticJava() && s->as_CallStaticJava()->uncommon_trap_request() != 0) ||\n-            s->is_Phi()) {\n-          continue;\n-        } else if (s->is_MergeMem()) {\n-          for (DUIterator_Fast imax, i = s->fast_outs(imax); i < imax; i++) {\n-            Node* s1 = s->fast_out(i);\n-            worklist.push(s1);\n-          }\n-        } else {\n-          Node* sctrl = has_ctrl(s) ? get_ctrl(s) : s->in(0);\n-          assert(sctrl != NULL || !s->is_reachable_from_root(), \"must have control\");\n-          if (sctrl != NULL && !sctrl->is_top() && is_dominator(early, sctrl)) {\n-            const TypePtr* adr_type = s->adr_type();\n-            if (s->is_ArrayCopy()) {\n-              \/\/ Copy to known instance needs destination type to test for aliasing\n-              const TypePtr* dest_type = s->as_ArrayCopy()->_dest_type;\n-              if (dest_type != TypeOopPtr::BOTTOM) {\n-                adr_type = dest_type;\n-              }\n+    LCA = get_late_ctrl_with_anti_dep(n->as_Load(), early, LCA);\n+  }\n+\n+  assert(LCA == find_non_split_ctrl(LCA), \"unexpected late control\");\n+  return LCA;\n+}\n+\n+\/\/ if this is a load, check for anti-dependent stores\n+\/\/ We use a conservative algorithm to identify potential interfering\n+\/\/ instructions and for rescheduling the load.  The users of the memory\n+\/\/ input of this load are examined.  Any use which is not a load and is\n+\/\/ dominated by early is considered a potentially interfering store.\n+\/\/ This can produce false positives.\n+Node* PhaseIdealLoop::get_late_ctrl_with_anti_dep(LoadNode* n, Node* early, Node* LCA) {\n+  int load_alias_idx = C->get_alias_index(n->adr_type());\n+  if (C->alias_type(load_alias_idx)->is_rewritable()) {\n+    Unique_Node_List worklist;\n+\n+    Node* mem = n->in(MemNode::Memory);\n+    for (DUIterator_Fast imax, i = mem->fast_outs(imax); i < imax; i++) {\n+      Node* s = mem->fast_out(i);\n+      worklist.push(s);\n+    }\n+    for (uint i = 0; i < worklist.size() && LCA != early; i++) {\n+      Node* s = worklist.at(i);\n+      if (s->is_Load() || s->Opcode() == Op_SafePoint ||\n+          (s->is_CallStaticJava() && s->as_CallStaticJava()->uncommon_trap_request() != 0) ||\n+          s->is_Phi()) {\n+        continue;\n+      } else if (s->is_MergeMem()) {\n+        for (DUIterator_Fast imax, i = s->fast_outs(imax); i < imax; i++) {\n+          Node* s1 = s->fast_out(i);\n+          worklist.push(s1);\n+        }\n+      } else {\n+        Node* sctrl = has_ctrl(s) ? get_ctrl(s) : s->in(0);\n+        assert(sctrl != NULL || !s->is_reachable_from_root(), \"must have control\");\n+        if (sctrl != NULL && !sctrl->is_top() && is_dominator(early, sctrl)) {\n+          const TypePtr* adr_type = s->adr_type();\n+          if (s->is_ArrayCopy()) {\n+            \/\/ Copy to known instance needs destination type to test for aliasing\n+            const TypePtr* dest_type = s->as_ArrayCopy()->_dest_type;\n+            if (dest_type != TypeOopPtr::BOTTOM) {\n+              adr_type = dest_type;\n@@ -5033,7 +5040,12 @@\n-            if (C->can_alias(adr_type, load_alias_idx)) {\n-              LCA = dom_lca_for_get_late_ctrl(LCA, sctrl, n);\n-            } else if (s->is_CFG()) {\n-              for (DUIterator_Fast imax, i = s->fast_outs(imax); i < imax; i++) {\n-                Node* s1 = s->fast_out(i);\n-                if (_igvn.type(s1) == Type::MEMORY) {\n-                  worklist.push(s1);\n+          }\n+          if (C->can_alias(adr_type, load_alias_idx)) {\n+            LCA = dom_lca_for_get_late_ctrl(LCA, sctrl, n);\n+          } else if (s->is_CFG() && s->is_Multi()) {\n+            \/\/ Look for the memory use of s (that is the use of its memory projection)\n+            for (DUIterator_Fast imax, i = s->fast_outs(imax); i < imax; i++) {\n+              Node* s1 = s->fast_out(i);\n+              assert(s1->is_Proj(), \"projection expected\");\n+              if (_igvn.type(s1) == Type::MEMORY) {\n+                for (DUIterator_Fast jmax, j = s1->fast_outs(jmax); j < jmax; j++) {\n+                  Node* s2 = s1->fast_out(j);\n+                  worklist.push(s2);\n@@ -5065,2 +5077,0 @@\n-\n-  assert(LCA == find_non_split_ctrl(LCA), \"unexpected late control\");\n@@ -5094,1 +5104,1 @@\n-Node *PhaseIdealLoop::dom_lca_for_get_late_ctrl_internal( Node *n1, Node *n2, Node *tag ) {\n+Node *PhaseIdealLoop::dom_lca_for_get_late_ctrl_internal(Node *n1, Node *n2, Node *tag_node) {\n@@ -5097,0 +5107,1 @@\n+  jlong tag = tag_node->_idx | (((jlong)_dom_lca_tags_round) << 32);\n@@ -5101,1 +5112,1 @@\n-      _dom_lca_tags.map(n1->_idx, tag);\n+      _dom_lca_tags.at_put_grow(n1->_idx, tag);\n@@ -5106,2 +5117,2 @@\n-      Node *memo = _dom_lca_tags[n2->_idx];\n-      if( memo == tag ) {\n+      jlong memo = _dom_lca_tags.at_grow(n2->_idx, 0);\n+      if (memo == tag) {\n@@ -5110,1 +5121,1 @@\n-      _dom_lca_tags.map(n2->_idx, tag);\n+      _dom_lca_tags.at_put_grow(n2->_idx, tag);\n@@ -5119,1 +5130,1 @@\n-      _dom_lca_tags.map(n1->_idx, tag);\n+      _dom_lca_tags.at_put_grow(n1->_idx, tag);\n@@ -5123,1 +5134,1 @@\n-        _dom_lca_tags.map(t1->_idx, tag);\n+        _dom_lca_tags.at_put_grow(t1->_idx, tag);\n@@ -5127,1 +5138,1 @@\n-      _dom_lca_tags.map(n2->_idx, tag);\n+      _dom_lca_tags.at_put_grow(n2->_idx, tag);\n@@ -5131,1 +5142,1 @@\n-        _dom_lca_tags.map(t2->_idx, tag);\n+        _dom_lca_tags.at_put_grow(t2->_idx, tag);\n@@ -5150,16 +5161,2 @@\n-  _dom_lca_tags.map( limit, NULL );\n-#ifdef ASSERT\n-  for( uint i = 0; i < limit; ++i ) {\n-    assert(_dom_lca_tags[i] == NULL, \"Must be distinct from each node pointer\");\n-  }\n-#endif \/\/ ASSERT\n-}\n-\n-\/\/------------------------------clear_dom_lca_tags------------------------------\n-\/\/ Tag could be a node's integer index, 32bits instead of 64bits in some cases\n-\/\/ Intended use does not involve any growth for the array, so it could\n-\/\/ be of fixed size.\n-void PhaseIdealLoop::clear_dom_lca_tags() {\n-  uint limit = C->unique() + 1;\n-  _dom_lca_tags.map( limit, NULL );\n-  _dom_lca_tags.clear();\n+  _dom_lca_tags.at_grow(limit, 0);\n+  _dom_lca_tags_round = 0;\n@@ -5167,2 +5164,2 @@\n-  for( uint i = 0; i < limit; ++i ) {\n-    assert(_dom_lca_tags[i] == NULL, \"Must be distinct from each node pointer\");\n+  for (uint i = 0; i < limit; ++i) {\n+    assert(_dom_lca_tags.at(i) == 0, \"Must be distinct from each node pointer\");\n","filename":"src\/hotspot\/share\/opto\/loopnode.cpp","additions":70,"deletions":73,"binary":false,"changes":143,"status":"modified"},{"patch":"@@ -870,1 +870,2 @@\n-  Node_Array _dom_lca_tags;\n+  GrowableArray<jlong> _dom_lca_tags;\n+  uint _dom_lca_tags_round;\n@@ -872,1 +873,0 @@\n-  void   clear_dom_lca_tags();\n@@ -1066,1 +1066,0 @@\n-    _dom_lca_tags(arena()),  \/\/ Thread::resource_area\n@@ -1080,1 +1079,0 @@\n-    _dom_lca_tags(arena()),  \/\/ Thread::resource_area\n@@ -1487,1 +1485,1 @@\n-  Node *place_near_use( Node *useblock ) const;\n+  Node* place_near_use(Node* useblock, IdealLoopTree* loop) const;\n@@ -1615,1 +1613,9 @@\n-  bool is_safe_load_ctrl(Node* ctrl);\n+  Node* get_late_ctrl_with_anti_dep(LoadNode* n, Node* early, Node* LCA);\n+\n+  bool ctrl_of_use_out_of_loop(const Node* n, Node* n_ctrl, IdealLoopTree* n_loop, Node* ctrl);\n+\n+  bool ctrl_of_all_uses_out_of_loop(const Node* n, Node* n_ctrl, IdealLoopTree* n_loop);\n+\n+  Node* compute_early_ctrl(Node* n, Node* n_ctrl);\n+\n+  void try_sink_out_of_loop(Node* n);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":12,"deletions":6,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1137,5 +1137,6 @@\n-\/\/ For inner loop uses move it to the preheader area.\n-Node *PhaseIdealLoop::place_near_use(Node *useblock) const {\n-  IdealLoopTree *u_loop = get_loop( useblock );\n-  if (u_loop->_irreducible) {\n-    return useblock;\n+Node* PhaseIdealLoop::place_near_use(Node* useblock, IdealLoopTree* loop) const {\n+  Node* head = loop->_head;\n+  assert(!loop->is_member(get_loop(useblock)), \"must be outside loop\");\n+  if (head->is_Loop() && head->as_Loop()->is_strip_mined()) {\n+    loop = loop->_parent;\n+    assert(loop->_head->is_OuterStripMinedLoop(), \"malformed strip mined loop\");\n@@ -1143,3 +1144,6 @@\n-  if (u_loop->_child) {\n-    if (useblock == u_loop->_head && u_loop->_head->is_OuterStripMinedLoop()) {\n-      return u_loop->_head->in(LoopNode::EntryControl);\n+\n+  \/\/ Pick control right outside the loop\n+  for (;;) {\n+    Node* dom = idom(useblock);\n+    if (loop->is_member(get_loop(dom))) {\n+      break;\n@@ -1147,1 +1151,1 @@\n-    return useblock;\n+    useblock = dom;\n@@ -1149,1 +1153,2 @@\n-  return u_loop->_head->as_Loop()->skip_strip_mined()->in(LoopNode::EntryControl);\n+  assert(find_non_split_ctrl(useblock) == useblock, \"should be non split control\");\n+  return useblock;\n@@ -1403,5 +1408,25 @@\n-  \/\/ See if a shared loop-varying computation has no loop-varying uses.\n-  \/\/ Happens if something is only used for JVM state in uncommon trap exits,\n-  \/\/ like various versions of induction variable+offset.  Clone the\n-  \/\/ computation per usage to allow it to sink out of the loop.\n-  if (has_ctrl(n) && !n->in(0)) {\/\/ n not dead and has no control edge (can float about)\n+  try_sink_out_of_loop(n);\n+\n+  try_move_store_after_loop(n);\n+\n+  \/\/ Check for Opaque2's who's loop has disappeared - who's input is in the\n+  \/\/ same loop nest as their output.  Remove 'em, they are no longer useful.\n+  if( n_op == Op_Opaque2 &&\n+      n->in(1) != NULL &&\n+      get_loop(get_ctrl(n)) == get_loop(get_ctrl(n->in(1))) ) {\n+    _igvn.replace_node( n, n->in(1) );\n+  }\n+}\n+\n+\/\/ See if a shared loop-varying computation has no loop-varying uses.\n+\/\/ Happens if something is only used for JVM state in uncommon trap exits,\n+\/\/ like various versions of induction variable+offset.  Clone the\n+\/\/ computation per usage to allow it to sink out of the loop.\n+void PhaseIdealLoop::try_sink_out_of_loop(Node* n) {\n+  if (has_ctrl(n) &&\n+      !n->is_Phi() &&\n+      !n->is_Bool() &&\n+      !n->is_Proj() &&\n+      !n->is_MergeMem() &&\n+      !n->is_CMove() &&\n+      n->Opcode() != Op_Opaque4) {\n@@ -1410,47 +1435,27 @@\n-    if( n_loop != _ltree_root ) {\n-      DUIterator_Fast imax, i = n->fast_outs(imax);\n-      for (; i < imax; i++) {\n-        Node* u = n->fast_out(i);\n-        if( !has_ctrl(u) )     break; \/\/ Found control user\n-        IdealLoopTree *u_loop = get_loop(get_ctrl(u));\n-        if( u_loop == n_loop ) break; \/\/ Found loop-varying use\n-        if( n_loop->is_member( u_loop ) ) break; \/\/ Found use in inner loop\n-        if( u->Opcode() == Op_Opaque1 ) break; \/\/ Found loop limit, bugfix for 4677003\n-      }\n-      bool did_break = (i < imax);  \/\/ Did we break out of the previous loop?\n-      if (!did_break && n->outcnt() > 1) { \/\/ All uses in outer loops!\n-        Node *late_load_ctrl = NULL;\n-        if (n->is_Load()) {\n-          \/\/ If n is a load, get and save the result from get_late_ctrl(),\n-          \/\/ to be later used in calculating the control for n's clones.\n-          clear_dom_lca_tags();\n-          late_load_ctrl = get_late_ctrl(n, n_ctrl);\n-        }\n-        \/\/ If n is a load, and the late control is the same as the current\n-        \/\/ control, then the cloning of n is a pointless exercise, because\n-        \/\/ GVN will ensure that we end up where we started.\n-        if (!n->is_Load() || (late_load_ctrl != n_ctrl && is_safe_load_ctrl(late_load_ctrl))) {\n-          Node* outer_loop_clone = NULL;\n-          for (DUIterator_Last jmin, j = n->last_outs(jmin); j >= jmin; ) {\n-            Node *u = n->last_out(j); \/\/ Clone private computation per use\n-            _igvn.rehash_node_delayed(u);\n-            Node *x = n->clone(); \/\/ Clone computation\n-            Node *x_ctrl = NULL;\n-            if( u->is_Phi() ) {\n-              \/\/ Replace all uses of normal nodes.  Replace Phi uses\n-              \/\/ individually, so the separate Nodes can sink down\n-              \/\/ different paths.\n-              uint k = 1;\n-              while( u->in(k) != n ) k++;\n-              u->set_req( k, x );\n-              \/\/ x goes next to Phi input path\n-              x_ctrl = u->in(0)->in(k);\n-              --j;\n-            } else {              \/\/ Normal use\n-              \/\/ Replace all uses\n-              for( uint k = 0; k < u->req(); k++ ) {\n-                if( u->in(k) == n ) {\n-                  u->set_req( k, x );\n-                  --j;\n-                }\n-              }\n+    if (n_loop != _ltree_root && n->outcnt() > 1) {\n+      \/\/ Compute early control: needed for anti-dependence analysis. It's also possible that as a result of\n+      \/\/ previous transformations in this loop opts round, the node can be hoisted now: early control will tell us.\n+      Node* early_ctrl = compute_early_ctrl(n, n_ctrl);\n+      if (n_loop->is_member(get_loop(early_ctrl)) && \/\/ check that this one can't be hoisted now\n+          ctrl_of_all_uses_out_of_loop(n, early_ctrl, n_loop)) { \/\/ All uses in outer loops!\n+        assert(!n->is_Store() && !n->is_LoadStore(), \"no node with a side effect\");\n+        Node* outer_loop_clone = NULL;\n+        for (DUIterator_Last jmin, j = n->last_outs(jmin); j >= jmin;) {\n+          Node* u = n->last_out(j); \/\/ Clone private computation per use\n+          _igvn.rehash_node_delayed(u);\n+          Node* x = n->clone(); \/\/ Clone computation\n+          Node* x_ctrl = NULL;\n+          if (u->is_Phi()) {\n+            \/\/ Replace all uses of normal nodes.  Replace Phi uses\n+            \/\/ individually, so the separate Nodes can sink down\n+            \/\/ different paths.\n+            uint k = 1;\n+            while (u->in(k) != n) k++;\n+            u->set_req(k, x);\n+            \/\/ x goes next to Phi input path\n+            x_ctrl = u->in(0)->in(k);\n+            \/\/ Find control for 'x' next to use but not inside inner loops.\n+            x_ctrl = place_near_use(x_ctrl, n_loop);\n+            --j;\n+          } else {              \/\/ Normal use\n+            if (has_ctrl(u)) {\n@@ -1458,0 +1463,2 @@\n+            } else {\n+              x_ctrl = u->in(0);\n@@ -1459,1 +1466,0 @@\n-\n@@ -1461,41 +1467,12 @@\n-            \/\/ For inner loop uses get the preheader area.\n-            x_ctrl = place_near_use(x_ctrl);\n-\n-            if (n->is_Load()) {\n-              \/\/ For loads, add a control edge to a CFG node outside of the loop\n-              \/\/ to force them to not combine and return back inside the loop\n-              \/\/ during GVN optimization (4641526).\n-              \/\/\n-              \/\/ Because we are setting the actual control input, factor in\n-              \/\/ the result from get_late_ctrl() so we respect any\n-              \/\/ anti-dependences. (6233005).\n-              x_ctrl = dom_lca(late_load_ctrl, x_ctrl);\n-\n-              \/\/ Don't allow the control input to be a CFG splitting node.\n-              \/\/ Such nodes should only have ProjNodes as outs, e.g. IfNode\n-              \/\/ should only have IfTrueNode and IfFalseNode (4985384).\n-              x_ctrl = find_non_split_ctrl(x_ctrl);\n-\n-              IdealLoopTree* x_loop = get_loop(x_ctrl);\n-              Node* x_head = x_loop->_head;\n-              if (x_head->is_Loop() && (x_head->is_OuterStripMinedLoop() || x_head->as_Loop()->is_strip_mined())) {\n-                if (is_dominator(n_ctrl, x_head) && n_ctrl != x_head) {\n-                  \/\/ Anti dependence analysis is sometimes too\n-                  \/\/ conservative: a store in the outer strip mined loop\n-                  \/\/ can prevent a load from floating out of the outer\n-                  \/\/ strip mined loop but the load may not be referenced\n-                  \/\/ from the safepoint: loop strip mining verification\n-                  \/\/ code reports a problem in that case. Make sure the\n-                  \/\/ load is not moved in the outer strip mined loop in\n-                  \/\/ that case.\n-                  x_ctrl = x_head->as_Loop()->skip_strip_mined()->in(LoopNode::EntryControl);\n-                } else if (x_head->is_OuterStripMinedLoop()) {\n-                  \/\/ Do not add duplicate LoadNodes to the outer strip mined loop\n-                  if (outer_loop_clone != NULL) {\n-                    _igvn.replace_node(x, outer_loop_clone);\n-                    continue;\n-                  }\n-                  outer_loop_clone = x;\n-                }\n-              }\n-              assert(dom_depth(n_ctrl) <= dom_depth(x_ctrl), \"n is later than its clone\");\n+            x_ctrl = place_near_use(x_ctrl, n_loop);\n+            \/\/ Replace all uses\n+            if (u->is_ConstraintCast() && u->bottom_type()->higher_equal(_igvn.type(n)) && u->in(0) == x_ctrl) {\n+              \/\/ If we're sinking a chain of data nodes, we might have inserted a cast to pin the use which is not necessary\n+              \/\/ anymore now that we're going to pin n as well\n+              _igvn.replace_node(u, x);\n+              --j;\n+            } else {\n+              int nb = u->replace_edge(n, x, &_igvn);\n+              j -= nb;\n+            }\n+          }\n@@ -1503,1 +1480,15 @@\n-              x->set_req(0, x_ctrl);\n+          if (n->is_Load()) {\n+            \/\/ For loads, add a control edge to a CFG node outside of the loop\n+            \/\/ to force them to not combine and return back inside the loop\n+            \/\/ during GVN optimization (4641526).\n+            assert(x_ctrl == get_late_ctrl_with_anti_dep(x->as_Load(), early_ctrl, x_ctrl), \"anti-dependences were already checked\");\n+\n+            IdealLoopTree* x_loop = get_loop(x_ctrl);\n+            Node* x_head = x_loop->_head;\n+            if (x_head->is_Loop() && x_head->is_OuterStripMinedLoop()) {\n+              \/\/ Do not add duplicate LoadNodes to the outer strip mined loop\n+              if (outer_loop_clone != NULL) {\n+                _igvn.replace_node(x, outer_loop_clone);\n+                continue;\n+              }\n+              outer_loop_clone = x;\n@@ -1505,13 +1496,34 @@\n-            register_new_node(x, x_ctrl);\n-\n-            \/\/ Some institutional knowledge is needed here: 'x' is\n-            \/\/ yanked because if the optimizer runs GVN on it all the\n-            \/\/ cloned x's will common up and undo this optimization and\n-            \/\/ be forced back in the loop.\n-            \/\/ I tried setting control edges on the x's to force them to\n-            \/\/ not combine, but the matching gets worried when it tries\n-            \/\/ to fold a StoreP and an AddP together (as part of an\n-            \/\/ address expression) and the AddP and StoreP have\n-            \/\/ different controls.\n-            if (!x->is_Load() && !x->is_DecodeNarrowPtr()) {\n-              _igvn._worklist.yank(x);\n+            x->set_req(0, x_ctrl);\n+          } else if (n->in(0) != NULL){\n+            x->set_req(0, x_ctrl);\n+          }\n+          assert(dom_depth(n_ctrl) <= dom_depth(x_ctrl), \"n is later than its clone\");\n+          assert(!n_loop->is_member(get_loop(x_ctrl)), \"should have moved out of loop\");\n+          register_new_node(x, x_ctrl);\n+\n+          if (x->in(0) == NULL && !x->is_DecodeNarrowPtr()) {\n+            assert(!x->is_Load(), \"load should be pinned\");\n+            \/\/ Use a cast node to pin clone out of loop\n+            Node* cast = NULL;\n+            for (uint k = 0; k < x->req(); k++) {\n+              Node* in = x->in(k);\n+              if (in != NULL && n_loop->is_member(get_loop(get_ctrl(in)))) {\n+                const Type* in_t = _igvn.type(in);\n+                if (in_t->isa_int()) {\n+                  cast = new CastIINode(in, in_t, true);\n+                } else if (in_t->isa_long()) {\n+                  cast = new CastLLNode(in, in_t, true);\n+                } else if (in_t->isa_ptr()) {\n+                  cast = new CastPPNode(in, in_t, true);\n+                } else if (in_t->isa_float()) {\n+                  cast = new CastFFNode(in, in_t, true);\n+                } else if (in_t->isa_double()) {\n+                  cast = new CastDDNode(in, in_t, true);\n+                }\n+              }\n+              if (cast != NULL) {\n+                cast->set_req(0, x_ctrl);\n+                register_new_node(cast, x_ctrl);\n+                x->replace_edge(in, cast);\n+                break;\n+              }\n@@ -1519,0 +1531,1 @@\n+            assert(cast != NULL, \"must have added a cast to pin the node\");\n@@ -1520,1 +1533,0 @@\n-          _igvn.remove_dead_node(n);\n@@ -1522,0 +1534,1 @@\n+        _igvn.remove_dead_node(n);\n@@ -1523,0 +1536,1 @@\n+      _dom_lca_tags_round = 0;\n@@ -1525,0 +1539,1 @@\n+}\n@@ -1526,1 +1541,33 @@\n-  try_move_store_after_loop(n);\n+Node* PhaseIdealLoop::compute_early_ctrl(Node* n, Node* n_ctrl) {\n+  Node* early_ctrl = NULL;\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(n);\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* m = wq.at(i);\n+    Node* c = NULL;\n+    if (m->is_CFG()) {\n+      c = m;\n+    } else if (m->pinned()) {\n+      c = m->in(0);\n+    } else {\n+      for (uint j = 0; j < m->req(); j++) {\n+        Node* in = m->in(j);\n+        if (in == NULL) {\n+          continue;\n+        }\n+        wq.push(in);\n+      }\n+    }\n+    if (c != NULL) {\n+      assert(is_dominator(c, n_ctrl), \"\");\n+      if (early_ctrl == NULL) {\n+        early_ctrl = c;\n+      } else if (is_dominator(early_ctrl, c)) {\n+        early_ctrl = c;\n+      }\n+    }\n+  }\n+  assert(is_dominator(early_ctrl, n_ctrl), \"early control must dominate current control\");\n+  return early_ctrl;\n+}\n@@ -1528,6 +1575,23 @@\n-  \/\/ Check for Opaque2's who's loop has disappeared - who's input is in the\n-  \/\/ same loop nest as their output.  Remove 'em, they are no longer useful.\n-  if( n_op == Op_Opaque2 &&\n-      n->in(1) != NULL &&\n-      get_loop(get_ctrl(n)) == get_loop(get_ctrl(n->in(1))) ) {\n-    _igvn.replace_node( n, n->in(1) );\n+bool PhaseIdealLoop::ctrl_of_all_uses_out_of_loop(const Node* n, Node* n_ctrl, IdealLoopTree* n_loop) {\n+  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+    Node* u = n->fast_out(i);\n+    if (u->Opcode() == Op_Opaque1) {\n+      return false;  \/\/ Found loop limit, bugfix for 4677003\n+    }\n+    \/\/ We can't reuse tags in PhaseIdealLoop::dom_lca_for_get_late_ctrl_internal() so make sure calls to\n+    \/\/ get_late_ctrl_with_anti_dep() use their own tag\n+    _dom_lca_tags_round++;\n+    assert(_dom_lca_tags_round != 0, \"shouldn't wrap around\");\n+\n+    if (u->is_Phi()) {\n+      for (uint j = 1; j < u->req(); ++j) {\n+        if (u->in(j) == n && !ctrl_of_use_out_of_loop(n, n_ctrl, n_loop, u->in(0)->in(j))) {\n+          return false;\n+        }\n+      }\n+    } else {\n+      Node* ctrl = has_ctrl(u) ? get_ctrl(u) : u->in(0);\n+      if (!ctrl_of_use_out_of_loop(n, n_ctrl, n_loop, ctrl)) {\n+        return false;\n+      }\n+    }\n@@ -1535,0 +1599,1 @@\n+  return true;\n@@ -1537,3 +1602,10 @@\n-bool PhaseIdealLoop::is_safe_load_ctrl(Node* ctrl) {\n-  if (ctrl->is_Proj() && ctrl->in(0)->is_Call() && ctrl->has_out_with(Op_Catch)) {\n-    return false;\n+bool PhaseIdealLoop::ctrl_of_use_out_of_loop(const Node* n, Node* n_ctrl, IdealLoopTree* n_loop, Node* ctrl) {\n+  if (n->is_Load()) {\n+    ctrl = get_late_ctrl_with_anti_dep(n->as_Load(), n_ctrl, ctrl);\n+  }\n+  IdealLoopTree *u_loop = get_loop(ctrl);\n+  if (u_loop == n_loop) {\n+    return false; \/\/ Found loop-varying use\n+  }\n+  if (n_loop->is_member(u_loop)) {\n+    return false; \/\/ Found use in inner loop\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":201,"deletions":129,"binary":false,"changes":330,"status":"modified"},{"patch":"@@ -59,0 +59,2 @@\n+class CastFFNode;\n+class CastDDNode;\n@@ -692,0 +694,2 @@\n+        DEFINE_CLASS_ID(CastFF, ConstraintCast, 3)\n+        DEFINE_CLASS_ID(CastDD, ConstraintCast, 4)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"}]}