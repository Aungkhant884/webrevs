{"files":[{"patch":"@@ -1964,1 +1964,0 @@\n-  void updateBytesAdler32(Register adler32, Register buf, Register length, XMMRegister shuf0, XMMRegister shuf1, ExternalAddress scale);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1,211 +0,0 @@\n-\/*\n-* Copyright (c) 2021, Intel Corporation. All rights reserved.\n-*\n-* DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-*\n-* This code is free software; you can redistribute it and\/or modify it\n-* under the terms of the GNU General Public License version 2 only, as\n-* published by the Free Software Foundation.\n-*\n-* This code is distributed in the hope that it will be useful, but WITHOUT\n-* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-* version 2 for more details (a copy is included in the LICENSE file that\n-* accompanied this code).\n-*\n-* You should have received a copy of the GNU General Public License version\n-* 2 along with this work; if not, write to the Free Software Foundation,\n-* Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-*\n-* Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-* or visit www.oracle.com if you need additional information or have any\n-* questions.\n-*\n-*\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/assembler.hpp\"\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"macroAssembler_x86.hpp\"\n-\n-#ifdef _LP64\n-void MacroAssembler::updateBytesAdler32(Register init_d, Register data, Register size, XMMRegister yshuf0, XMMRegister yshuf1, ExternalAddress ascaletab)\n-{\n-      const int LIMIT = 5552;\n-      const int BASE = 65521;\n-      const int CHUNKSIZE =  16;\n-      const int CHUNKSIZE_M1 = CHUNKSIZE - 1;\n-\n-      const Register s = r11;\n-      const Register a_d = r12; \/\/r12d\n-      const Register b_d = r8; \/\/r8d\n-      const Register end = r13;\n-\n-      const XMMRegister ya = xmm0;\n-      const XMMRegister yb = xmm1;\n-      const XMMRegister ydata0 = xmm2;\n-      const XMMRegister ydata1 = xmm3;\n-      const XMMRegister ysa = xmm4;\n-      const XMMRegister ydata = ysa;\n-      const XMMRegister ytmp0 = ydata0;\n-      const XMMRegister ytmp1 = ydata1;\n-      const XMMRegister ytmp2 = xmm5;\n-      const XMMRegister xa = xmm0;\n-      const XMMRegister xb = xmm1;\n-      const XMMRegister xtmp0 = xmm2;\n-      const XMMRegister xtmp1 = xmm3;\n-      const XMMRegister xsa = xmm4;\n-      const XMMRegister xtmp2 = xmm5;\n-      assert_different_registers(init_d, data, size, s, a_d, b_d, end, rax);\n-\n-      Label SLOOP1, SLOOP1A, SKIP_LOOP_1A, FINISH, LT64, DO_FINAL, FINAL_LOOP, ZERO_SIZE, END;\n-\n-      push(r12);\n-      push(r13);\n-      push(r14);\n-      movl(b_d, init_d); \/\/adler\n-      shrl(b_d, 16);\n-      andl(init_d, 0xFFFF);\n-      cmpl(size, 32);\n-      jcc(Assembler::below, LT64);\n-      movdl(xa, init_d); \/\/vmovd - 32bit\n-      vpxor(yb, yb, yb, Assembler::AVX_256bit);\n-\n-      bind(SLOOP1);\n-      movl(s, LIMIT);\n-      cmpl(s, size);\n-      cmovl(Assembler::above, s, size); \/\/ s = min(size, LIMIT)\n-      lea(end, Address(s, data, Address::times_1, -CHUNKSIZE_M1));\n-      cmpptr(data, end);\n-      jcc(Assembler::aboveEqual, SKIP_LOOP_1A);\n-\n-      align32();\n-      bind(SLOOP1A);\n-      vbroadcastf128(ydata, Address(data, 0), Assembler::AVX_256bit);\n-      addptr(data, CHUNKSIZE);\n-      vpshufb(ydata0, ydata, yshuf0, Assembler::AVX_256bit);\n-      vpaddd(ya, ya, ydata0, Assembler::AVX_256bit);\n-      vpaddd(yb, yb, ya, Assembler::AVX_256bit);\n-      vpshufb(ydata1, ydata, yshuf1, Assembler::AVX_256bit);\n-      vpaddd(ya, ya, ydata1, Assembler::AVX_256bit);\n-      vpaddd(yb, yb, ya, Assembler::AVX_256bit);\n-      cmpptr(data, end);\n-      jcc(Assembler::below, SLOOP1A);\n-\n-      bind(SKIP_LOOP_1A);\n-      addptr(end, CHUNKSIZE_M1);\n-      testl(s, CHUNKSIZE_M1);\n-      jcc(Assembler::notEqual, DO_FINAL);\n-\n-      \/\/ either we're done, or we just did LIMIT\n-      subl(size, s);\n-\n-      \/\/ reduce\n-      vpslld(yb, yb, 3, Assembler::AVX_256bit); \/\/b is scaled by 8\n-      vpmulld(ysa, ya, ascaletab, Assembler::AVX_256bit, r14);\n-\n-      \/\/ compute horizontal sums of ya, yb, ysa\n-      vextracti128(xtmp0, ya, 1);\n-      vextracti128(xtmp1, yb, 1);\n-      vextracti128(xtmp2, ysa, 1);\n-      vpaddd(xa, xa, xtmp0, Assembler::AVX_128bit);\n-      vpaddd(xb, xb, xtmp1, Assembler::AVX_128bit);\n-      vpaddd(xsa, xsa, xtmp2, Assembler::AVX_128bit);\n-      vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n-      vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n-      vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n-      vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n-      vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n-      vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n-\n-      movdl(rax, xa);\n-      xorl(rdx, rdx);\n-      movl(rcx, BASE);\n-      divl(rcx); \/\/ divide edx:eax by ecx, quot->eax, rem->edx\n-      movl(a_d, rdx);\n-\n-      vpsubd(xb, xb, xsa, Assembler::AVX_128bit);\n-      movdl(rax, xb);\n-      addl(rax, b_d);\n-      xorl(rdx, rdx);\n-      movl(rcx, BASE);\n-      divl(rcx); \/\/ divide edx:eax by ecx, quot->eax, rem->edx\n-      movl(b_d, rdx);\n-\n-      testl(size, size);\n-      jcc(Assembler::zero, FINISH);\n-\n-      \/\/ continue loop\n-      movdl(xa, a_d);\n-      vpxor(yb, yb, yb, Assembler::AVX_256bit);\n-      jmp(SLOOP1);\n-\n-      bind(FINISH);\n-      movl(rax, b_d);\n-      shll(rax, 16);\n-      orl(rax, a_d);\n-      jmp(END);\n-\n-      bind(LT64);\n-      movl(a_d, init_d);\n-      lea(end, Address(data, size, Address::times_1));\n-      testl(size, size);\n-      jcc(Assembler::notZero, FINAL_LOOP);\n-      jmp(ZERO_SIZE);\n-\n-      \/\/ handle remaining 1...15 bytes\n-      bind(DO_FINAL);\n-      \/\/ reduce\n-      vpslld(yb, yb, 3, Assembler::AVX_256bit); \/\/b is scaled by 8\n-      vpmulld(ysa, ya, ascaletab, Assembler::AVX_256bit, r14); \/\/scaled a\n-\n-      vextracti128(xtmp0, ya, 1);\n-      vextracti128(xtmp1, yb, 1);\n-      vextracti128(xtmp2, ysa, 1);\n-      vpaddd(xa, xa, xtmp0, Assembler::AVX_128bit);\n-      vpaddd(xb, xb, xtmp1, Assembler::AVX_128bit);\n-      vpaddd(xsa, xsa, xtmp2, Assembler::AVX_128bit);\n-      vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n-      vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n-      vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n-      vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n-      vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n-      vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n-      vpsubd(xb, xb, xsa, Assembler::AVX_128bit);\n-\n-      movdl(a_d, xa);\n-      movdl(rax, xb);\n-      addl(b_d, rax);\n-\n-      align32();\n-      bind(FINAL_LOOP);\n-      movzbl(rax, Address(data, 0)); \/\/movzx   eax, byte[data]\n-      addl(a_d, rax);\n-      addptr(data, 1);\n-      addl(b_d, a_d);\n-      cmpptr(data, end);\n-      jcc(Assembler::below, FINAL_LOOP);\n-\n-      bind(ZERO_SIZE);\n-\n-      movl(rax, a_d);\n-      xorl(rdx, rdx);\n-      movl(rcx, BASE);\n-      divl(rcx); \/\/ div ecx -- divide edx:eax by ecx, quot->eax, rem->edx\n-      movl(a_d, rdx);\n-\n-      movl(rax, b_d);\n-      xorl(rdx, rdx);\n-      movl(rcx, BASE);\n-      divl(rcx); \/\/ divide edx:eax by ecx, quot->eax, rem->edx\n-      shll(rdx, 16);\n-      orl(rdx, a_d);\n-      movl(rax, rdx);\n-\n-      bind(END);\n-      pop(r14);\n-      pop(r13);\n-      pop(r12);\n-  }\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_adler.cpp","additions":0,"deletions":211,"binary":false,"changes":211,"status":"deleted"},{"patch":"@@ -6734,40 +6734,0 @@\n-\/***\n- *  Arguments:\n- *\n- *  Inputs:\n- *   c_rarg0   - int   adler\n- *   c_rarg1   - byte* buff\n- *   c_rarg2   - int   len\n- *\n- * Output:\n- *   rax   - int adler result\n- *\/\n-\n-address StubGenerator::generate_updateBytesAdler32() {\n-  assert(UseAdler32Intrinsics, \"need AVX2\");\n-\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"updateBytesAdler32\");\n-  address start = __ pc();\n-\n-  const Register data = r9;\n-  const Register size = r10;\n-\n-  const XMMRegister yshuf0 = xmm6;\n-  const XMMRegister yshuf1 = xmm7;\n-  assert_different_registers(c_rarg0, c_rarg1, c_rarg2, data, size);\n-\n-  BLOCK_COMMENT(\"Entry:\");\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-  __ vmovdqu(yshuf0, ExternalAddress((address) StubRoutines::x86::_adler32_shuf0_table), r9);\n-  __ vmovdqu(yshuf1, ExternalAddress((address) StubRoutines::x86::_adler32_shuf1_table), r9);\n-  __ movptr(data, c_rarg1); \/\/data\n-  __ movl(size, c_rarg2); \/\/length\n-  __ updateBytesAdler32(c_rarg0, data, size, yshuf0, yshuf1, ExternalAddress((address) StubRoutines::x86::_adler32_ascale_table));\n-  __ leave();\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":0,"deletions":40,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -0,0 +1,268 @@\n+\/*\n+* Copyright (c) 2021, Intel Corporation. All rights reserved.\n+*\n+* DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+*\n+* This code is free software; you can redistribute it and\/or modify it\n+* under the terms of the GNU General Public License version 2 only, as\n+* published by the Free Software Foundation.\n+*\n+* This code is distributed in the hope that it will be useful, but WITHOUT\n+* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+* version 2 for more details (a copy is included in the LICENSE file that\n+* accompanied this code).\n+*\n+* You should have received a copy of the GNU General Public License version\n+* 2 along with this work; if not, write to the Free Software Foundation,\n+* Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+*\n+* Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+* or visit www.oracle.com if you need additional information or have any\n+* questions.\n+*\n+*\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"macroAssembler_x86.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n+\n+#define __ _masm->\n+\n+ATTRIBUTE_ALIGNED(32) juint ADLER32_ASCALE_TABLE[] = {\n+    0x00000000UL, 0x00000001UL, 0x00000002UL, 0x00000003UL,\n+    0x00000004UL, 0x00000005UL, 0x00000006UL, 0x00000007UL\n+};\n+\n+ATTRIBUTE_ALIGNED(32) juint ADLER32_SHUF0_TABLE[] = {\n+    0xFFFFFF00UL, 0xFFFFFF01UL, 0xFFFFFF02UL, 0xFFFFFF03UL,\n+    0xFFFFFF04UL, 0xFFFFFF05UL, 0xFFFFFF06UL, 0xFFFFFF07UL\n+};\n+\n+ATTRIBUTE_ALIGNED(32) juint ADLER32_SHUF1_TABLE[] = {\n+    0xFFFFFF08UL, 0xFFFFFF09, 0xFFFFFF0AUL, 0xFFFFFF0BUL,\n+    0xFFFFFF0CUL, 0xFFFFFF0D, 0xFFFFFF0EUL, 0xFFFFFF0FUL\n+};\n+\n+\n+\/***\n+ *  Arguments:\n+ *\n+ *  Inputs:\n+ *   c_rarg0   - int   adler\n+ *   c_rarg1   - byte* buff\n+ *   c_rarg2   - int   len\n+ *\n+ * Output:\n+ *   rax   - int adler result\n+ *\/\n+address StubGenerator::generate_updateBytesAdler32() {\n+  assert(UseAdler32Intrinsics, \"\");\n+\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"updateBytesAdler32\");\n+  address start = __ pc();\n+\n+  const int LIMIT = 5552;\n+  const int BASE = 65521;\n+  const int CHUNKSIZE =  16;\n+  const int CHUNKSIZE_M1 = CHUNKSIZE - 1;\n+\n+\n+  const Register init_d = c_rarg0;\n+  const Register data = r9;\n+  const Register size = r10;\n+  const Register s = r11;\n+  const Register a_d = r12; \/\/r12d\n+  const Register b_d = r8; \/\/r8d\n+  const Register end = r13;\n+\n+  assert_different_registers(c_rarg0, c_rarg1, c_rarg2, data, size);\n+  assert_different_registers(init_d, data, size, s, a_d, b_d, end, rax);\n+\n+  const XMMRegister yshuf0 = xmm6;\n+  const XMMRegister yshuf1 = xmm7;\n+  const XMMRegister ya = xmm0;\n+  const XMMRegister yb = xmm1;\n+  const XMMRegister ydata0 = xmm2;\n+  const XMMRegister ydata1 = xmm3;\n+  const XMMRegister ysa = xmm4;\n+  const XMMRegister ydata = ysa;\n+  const XMMRegister ytmp0 = ydata0;\n+  const XMMRegister ytmp1 = ydata1;\n+  const XMMRegister ytmp2 = xmm5;\n+  const XMMRegister xa = xmm0;\n+  const XMMRegister xb = xmm1;\n+  const XMMRegister xtmp0 = xmm2;\n+  const XMMRegister xtmp1 = xmm3;\n+  const XMMRegister xsa = xmm4;\n+  const XMMRegister xtmp2 = xmm5;\n+\n+  Label SLOOP1, SLOOP1A, SKIP_LOOP_1A, FINISH, LT64, DO_FINAL, FINAL_LOOP, ZERO_SIZE, END;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+\n+  __ vmovdqu(yshuf0, ExternalAddress((address)ADLER32_SHUF0_TABLE), r14 \/*rscratch*\/);\n+  __ vmovdqu(yshuf1, ExternalAddress((address)ADLER32_SHUF1_TABLE), r14 \/*rscratch*\/);\n+  __ movptr(data, c_rarg1); \/\/data\n+  __ movl(size, c_rarg2); \/\/length\n+\n+  __ movl(b_d, init_d); \/\/adler\n+  __ shrl(b_d, 16);\n+  __ andl(init_d, 0xFFFF);\n+  __ cmpl(size, 32);\n+  __ jcc(Assembler::below, LT64);\n+  __ movdl(xa, init_d); \/\/vmovd - 32bit\n+  __ vpxor(yb, yb, yb, Assembler::AVX_256bit);\n+\n+  __ bind(SLOOP1);\n+  __ movl(s, LIMIT);\n+  __ cmpl(s, size);\n+  __ cmovl(Assembler::above, s, size); \/\/ s = min(size, LIMIT)\n+  __ lea(end, Address(s, data, Address::times_1, -CHUNKSIZE_M1));\n+  __ cmpptr(data, end);\n+  __ jcc(Assembler::aboveEqual, SKIP_LOOP_1A);\n+\n+  __ align32();\n+  __ bind(SLOOP1A);\n+  __ vbroadcastf128(ydata, Address(data, 0), Assembler::AVX_256bit);\n+  __ addptr(data, CHUNKSIZE);\n+  __ vpshufb(ydata0, ydata, yshuf0, Assembler::AVX_256bit);\n+  __ vpaddd(ya, ya, ydata0, Assembler::AVX_256bit);\n+  __ vpaddd(yb, yb, ya, Assembler::AVX_256bit);\n+  __ vpshufb(ydata1, ydata, yshuf1, Assembler::AVX_256bit);\n+  __ vpaddd(ya, ya, ydata1, Assembler::AVX_256bit);\n+  __ vpaddd(yb, yb, ya, Assembler::AVX_256bit);\n+  __ cmpptr(data, end);\n+  __ jcc(Assembler::below, SLOOP1A);\n+\n+  __ bind(SKIP_LOOP_1A);\n+  __ addptr(end, CHUNKSIZE_M1);\n+  __ testl(s, CHUNKSIZE_M1);\n+  __ jcc(Assembler::notEqual, DO_FINAL);\n+\n+  \/\/ either we're done, or we just did LIMIT\n+  __ subl(size, s);\n+\n+  \/\/ reduce\n+  __ vpslld(yb, yb, 3, Assembler::AVX_256bit); \/\/b is scaled by 8\n+  __ vpmulld(ysa, ya, ExternalAddress((address)ADLER32_ASCALE_TABLE), Assembler::AVX_256bit, r14 \/*rscratch*\/);\n+\n+  \/\/ compute horizontal sums of ya, yb, ysa\n+  __ vextracti128(xtmp0, ya, 1);\n+  __ vextracti128(xtmp1, yb, 1);\n+  __ vextracti128(xtmp2, ysa, 1);\n+  __ vpaddd(xa, xa, xtmp0, Assembler::AVX_128bit);\n+  __ vpaddd(xb, xb, xtmp1, Assembler::AVX_128bit);\n+  __ vpaddd(xsa, xsa, xtmp2, Assembler::AVX_128bit);\n+  __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n+  __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n+  __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n+  __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n+  __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n+  __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n+\n+  __ movdl(rax, xa);\n+  __ xorl(rdx, rdx);\n+  __ movl(rcx, BASE);\n+  __ divl(rcx); \/\/ divide edx:eax by ecx, quot->eax, rem->edx\n+  __ movl(a_d, rdx);\n+\n+  __ vpsubd(xb, xb, xsa, Assembler::AVX_128bit);\n+  __ movdl(rax, xb);\n+  __ addl(rax, b_d);\n+  __ xorl(rdx, rdx);\n+  __ movl(rcx, BASE);\n+  __ divl(rcx); \/\/ divide edx:eax by ecx, quot->eax, rem->edx\n+  __ movl(b_d, rdx);\n+\n+  __ testl(size, size);\n+  __ jcc(Assembler::zero, FINISH);\n+\n+  \/\/ continue loop\n+  __ movdl(xa, a_d);\n+  __ vpxor(yb, yb, yb, Assembler::AVX_256bit);\n+  __ jmp(SLOOP1);\n+\n+  __ bind(FINISH);\n+  __ movl(rax, b_d);\n+  __ shll(rax, 16);\n+  __ orl(rax, a_d);\n+  __ jmp(END);\n+\n+  __ bind(LT64);\n+  __ movl(a_d, init_d);\n+  __ lea(end, Address(data, size, Address::times_1));\n+  __ testl(size, size);\n+  __ jcc(Assembler::notZero, FINAL_LOOP);\n+  __ jmp(ZERO_SIZE);\n+\n+  \/\/ handle remaining 1...15 bytes\n+  __ bind(DO_FINAL);\n+  \/\/ reduce\n+  __ vpslld(yb, yb, 3, Assembler::AVX_256bit); \/\/b is scaled by 8\n+  __ vpmulld(ysa, ya, ExternalAddress((address)ADLER32_ASCALE_TABLE), Assembler::AVX_256bit, r14 \/*rscratch*\/); \/\/scaled a\n+\n+  __ vextracti128(xtmp0, ya, 1);\n+  __ vextracti128(xtmp1, yb, 1);\n+  __ vextracti128(xtmp2, ysa, 1);\n+  __ vpaddd(xa, xa, xtmp0, Assembler::AVX_128bit);\n+  __ vpaddd(xb, xb, xtmp1, Assembler::AVX_128bit);\n+  __ vpaddd(xsa, xsa, xtmp2, Assembler::AVX_128bit);\n+  __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n+  __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n+  __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n+  __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n+  __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n+  __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n+  __ vpsubd(xb, xb, xsa, Assembler::AVX_128bit);\n+\n+  __ movdl(a_d, xa);\n+  __ movdl(rax, xb);\n+  __ addl(b_d, rax);\n+\n+  __ align32();\n+  __ bind(FINAL_LOOP);\n+  __ movzbl(rax, Address(data, 0)); \/\/movzx   eax, byte[data]\n+  __ addl(a_d, rax);\n+  __ addptr(data, 1);\n+  __ addl(b_d, a_d);\n+  __ cmpptr(data, end);\n+  __ jcc(Assembler::below, FINAL_LOOP);\n+\n+  __ bind(ZERO_SIZE);\n+\n+  __ movl(rax, a_d);\n+  __ xorl(rdx, rdx);\n+  __ movl(rcx, BASE);\n+  __ divl(rcx); \/\/ div ecx -- divide edx:eax by ecx, quot->eax, rem->edx\n+  __ movl(a_d, rdx);\n+\n+  __ movl(rax, b_d);\n+  __ xorl(rdx, rdx);\n+  __ movl(rcx, BASE);\n+  __ divl(rcx); \/\/ divide edx:eax by ecx, quot->eax, rem->edx\n+  __ shll(rdx, 16);\n+  __ orl(rdx, a_d);\n+  __ movl(rax, rdx);\n+\n+  __ bind(END);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n+\n+  __ leave();\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_adler.cpp","additions":268,"deletions":0,"binary":false,"changes":268,"status":"added"},{"patch":"@@ -236,19 +236,0 @@\n-\n-juint StubRoutines::x86::_adler32_ascale_table[] =\n-{\n-    0x00000000UL, 0x00000001UL, 0x00000002UL, 0x00000003UL,\n-    0x00000004UL, 0x00000005UL, 0x00000006UL, 0x00000007UL\n-};\n-\n-juint StubRoutines::x86::_adler32_shuf0_table[] =\n-{\n-    0xFFFFFF00UL, 0xFFFFFF01UL, 0xFFFFFF02UL, 0xFFFFFF03UL,\n-    0xFFFFFF04UL, 0xFFFFFF05UL, 0xFFFFFF06UL, 0xFFFFFF07UL\n-};\n-\n-juint StubRoutines::x86::_adler32_shuf1_table[] =\n-{\n-    0xFFFFFF08UL, 0xFFFFFF09, 0xFFFFFF0AUL, 0xFFFFFF0BUL,\n-    0xFFFFFF0CUL, 0xFFFFFF0D, 0xFFFFFF0EUL, 0xFFFFFF0FUL\n-};\n-\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":0,"deletions":19,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -142,3 +142,0 @@\n-  static juint    _adler32_shuf0_table[];\n-  static juint    _adler32_shuf1_table[];\n-  static juint    _adler32_ascale_table[];\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"}]}