{"files":[{"patch":"@@ -13556,0 +13556,7 @@\n+void Assembler::vzeroall() {\n+  assert(VM_Version::supports_avx(), \"requires AVX\");\n+  InstructionAttr attributes(AVX_256bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  (void)vex_prefix_and_encode(0, 0, 0, VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x77);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2772,0 +2772,2 @@\n+  void vzeroall();\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -482,1 +482,1 @@\n-\/\/    C = a       \/\/ [0 0 0 0 0 0 0 a]\n+\/\/    CL = a       \/\/ [0 0 0 0 0 0 0 a]\n@@ -496,11 +496,11 @@\n-\/\/    T = r^4 || r^3 || r^2 || r\n-\/\/    B = limbs(T)           \/\/ [r^4  0  r^3  0  r^2  0  r^1  0 ]\n-\/\/    C = B >> 1             \/\/ [ 0  r^4  0  r^3  0  r^2  0  r^1]\n-\/\/    R = r^4 || r^4 || ..   \/\/ [r^4 r^4 r^4 r^4 r^4 r^4 r^4 r^4]\n-\/\/    B = B×R                \/\/ [r^8  0  r^7  0  r^6  0  r^5  0 ]\n-\/\/    B = B | C              \/\/ [r^8 r^4 r^7 r^3 r^6 r^2 r^5 r^1]\n-\/\/    push(B)\n-\/\/    R = r^8 || r^8 || ..   \/\/ [r^8 r^8 r^8 r^8 r^8 r^8 r^8 r^8]\n-\/\/    B = B × R              \/\/ [r^16 r^12 r^15 r^11 r^14 r^10 r^13 r^9]\n-\/\/    push(B)\n-\/\/    R = r^16 || r^16 || .. \/\/ [r^16 r^16 r^16 r^16 r^16 r^16 r^16 r^16]\n+\/\/    T  = r^4 || r^3 || r^2 || r\n+\/\/    B  = limbs(T)           \/\/ [r^4  0  r^3  0  r^2  0  r^1  0 ]\n+\/\/    CL = B >> 1             \/\/ [ 0  r^4  0  r^3  0  r^2  0  r^1]\n+\/\/    R  = r^4 || r^4 || ..   \/\/ [r^4 r^4 r^4 r^4 r^4 r^4 r^4 r^4]\n+\/\/    B  = B×R                \/\/ [r^8  0  r^7  0  r^6  0  r^5  0 ]\n+\/\/    B  = B | CL             \/\/ [r^8 r^4 r^7 r^3 r^6 r^2 r^5 r^1]\n+\/\/    CL = B\n+\/\/    R  = r^8 || r^8 || ..   \/\/ [r^8 r^8 r^8 r^8 r^8 r^8 r^8 r^8]\n+\/\/    B  = B × R              \/\/ [r^16 r^12 r^15 r^11 r^14 r^10 r^13 r^9]\n+\/\/    CH = B\n+\/\/    R = r^16 || r^16 || ..  \/\/ [r^16 r^16 r^16 r^16 r^16 r^16 r^16 r^16]\n@@ -516,4 +516,2 @@\n-\/\/  B = pop()\n-\/\/  R = pop()\n-\/\/  AL = AL × R\n-\/\/  AH = AH × B\n+\/\/  AL = AL × CL\n+\/\/  AH = AH × CH\n@@ -549,7 +547,7 @@\n-\/\/   TMP: xmm6\n-\/\/   C: xmm7-9\n-\/\/   D: xmm2-4\n-\/\/   T: xmm0-5\n-\/\/   A: xmm13-18\n-\/\/   B: xmm19-24\n-\/\/   R: xmm25-29\n+\/\/   D: xmm0-1\n+\/\/   TMP: xmm2\n+\/\/   T: xmm3-8\n+\/\/   A: xmm9-14\n+\/\/   B: xmm15-20\n+\/\/   C: xmm21-26\n+\/\/   R: xmm27-31\n@@ -569,38 +567,37 @@\n-  const XMMRegister TMP = xmm6;\n-\n-  const XMMRegister D0 = xmm2;\n-  const XMMRegister D1 = xmm3;\n-  const XMMRegister D2 = xmm4;\n-\n-  const XMMRegister C0 = xmm7;\n-  const XMMRegister C1 = xmm8;\n-  const XMMRegister C2 = xmm9;\n-\n-  const XMMRegister T0 = xmm0;\n-  const XMMRegister T1 = xmm1;\n-  const XMMRegister T2 = xmm2;\n-  const XMMRegister T3 = xmm3;\n-  const XMMRegister T4 = xmm4;\n-  const XMMRegister T5 = xmm5;\n-\n-  const XMMRegister A0 = xmm13;\n-  const XMMRegister A1 = xmm14;\n-  const XMMRegister A2 = xmm15;\n-  const XMMRegister A3 = xmm16;\n-  const XMMRegister A4 = xmm17;\n-  const XMMRegister A5 = xmm18;\n-\n-  const XMMRegister B0 = xmm19;\n-  const XMMRegister B1 = xmm20;\n-  const XMMRegister B2 = xmm21;\n-  const XMMRegister B3 = xmm22;\n-  const XMMRegister B4 = xmm23;\n-  const XMMRegister B5 = xmm24;\n-\n-  const XMMRegister R0 = xmm25;\n-  const XMMRegister R1 = xmm26;\n-  const XMMRegister R2 = xmm27;\n-  const XMMRegister R1P = xmm28;\n-  const XMMRegister R2P = xmm29;\n-\n-  __ subq(rsp, (512\/8)*6); \/\/ Make room to store 6 zmm registers (powers of R)\n+  const XMMRegister D0 = xmm0;\n+  const XMMRegister D1 = xmm1;\n+  const XMMRegister TMP = xmm2;\n+\n+  const XMMRegister T0 = xmm3;\n+  const XMMRegister T1 = xmm4;\n+  const XMMRegister T2 = xmm5;\n+  const XMMRegister T3 = xmm6;\n+  const XMMRegister T4 = xmm7;\n+  const XMMRegister T5 = xmm8;\n+\n+  const XMMRegister A0 = xmm9;\n+  const XMMRegister A1 = xmm10;\n+  const XMMRegister A2 = xmm11;\n+  const XMMRegister A3 = xmm12;\n+  const XMMRegister A4 = xmm13;\n+  const XMMRegister A5 = xmm14;\n+\n+  const XMMRegister B0 = xmm15;\n+  const XMMRegister B1 = xmm16;\n+  const XMMRegister B2 = xmm17;\n+  const XMMRegister B3 = xmm18;\n+  const XMMRegister B4 = xmm19;\n+  const XMMRegister B5 = xmm20;\n+\n+  const XMMRegister C0 = xmm21;\n+  const XMMRegister C1 = xmm22;\n+  const XMMRegister C2 = xmm23;\n+  const XMMRegister C3 = xmm24;\n+  const XMMRegister C4 = xmm25;\n+  const XMMRegister C5 = xmm26;\n+\n+  const XMMRegister R0 = xmm27;\n+  const XMMRegister R1 = xmm28;\n+  const XMMRegister R2 = xmm29;\n+  const XMMRegister R1P = xmm30;\n+  const XMMRegister R2P = xmm31;\n@@ -649,6 +646,6 @@\n-  \/\/ D0 to have bits 0-127 in 4 quadword pairs\n-  \/\/ D1 to have bits 128-129 in alternating 8 qwords\n-  __ vpxorq(D1, D1, D1, Assembler::AVX_512bit);\n-  __ movq(D2, r0);\n-  __ vpinsrq(D2, D2, r1, 1);\n-  __ vinserti32x4(D0, D0, D2, 3);\n+  \/\/ T0 to have bits 0-127 in 4 quadword pairs\n+  \/\/ T1 to have bits 128-129 in alternating 8 qwords\n+  __ vpxorq(T1, T1, T1, Assembler::AVX_512bit);\n+  __ movq(T2, r0);\n+  __ vpinsrq(T2, T2, r1, 1);\n+  __ vinserti32x4(T0, T0, T2, 3);\n@@ -664,5 +661,5 @@\n-  __ movq(D2, a0);\n-  __ vpinsrq(D2, D2, a1, 1);\n-  __ vinserti32x4(D0, D0, D2, 2);\n-  __ movq(D2, a2);\n-  __ vinserti32x4(D1, D1, D2, 2);\n+  __ movq(T2, a0);\n+  __ vpinsrq(T2, T2, a1, 1);\n+  __ vinserti32x4(T0, T0, T2, 2);\n+  __ movq(T2, a2);\n+  __ vinserti32x4(T1, T1, T2, 2);\n@@ -675,5 +672,5 @@\n-  __ movq(D2, a0);\n-  __ vpinsrq(D2, D2, a1, 1);\n-  __ vinserti32x4(D0, D0, D2, 1);\n-  __ movq(D2, a2);\n-  __ vinserti32x4(D1, D1, D2, 1);\n+  __ movq(T2, a0);\n+  __ vpinsrq(T2, T2, a1, 1);\n+  __ vinserti32x4(T0, T0, T2, 1);\n+  __ movq(T2, a2);\n+  __ vinserti32x4(T1, T1, T2, 1);\n@@ -686,5 +683,5 @@\n-  __ movq(D2, a0);\n-  __ vpinsrq(D2, D2, a1, 1);\n-  __ vinserti32x4(D0, D0, D2, 0);\n-  __ movq(D2, a2);\n-  __ vinserti32x4(D1, D1, D2, 0);\n+  __ movq(T2, a0);\n+  __ vpinsrq(T2, T2, a1, 1);\n+  __ vinserti32x4(T0, T0, T2, 0);\n+  __ movq(T2, a2);\n+  __ vinserti32x4(T1, T1, T2, 0);\n@@ -696,2 +693,2 @@\n-  __ vpxorq(D2, D2, D2, Assembler::AVX_512bit);\n-  poly1305_limbs_avx512(D0, D2, B0, B1, B2, false, TMP, rscratch);\n+  __ vpxorq(T2, T2, T2, Assembler::AVX_512bit);\n+  poly1305_limbs_avx512(T0, T2, B0, B1, B2, false, TMP, rscratch);\n@@ -699,3 +696,3 @@\n-  \/\/ D1 contains the 2 highest bits of the powers of R\n-  __ vpsllq(D1, D1, 40, Assembler::AVX_512bit);\n-  __ evporq(B2, B2, D1, Assembler::AVX_512bit);\n+  \/\/ T1 contains the 2 highest bits of the powers of R\n+  __ vpsllq(T1, T1, 40, Assembler::AVX_512bit);\n+  __ evporq(B2, B2, T1, Assembler::AVX_512bit);\n@@ -742,0 +739,5 @@\n+  \/\/ Store R^8-R for later use\n+  __ evmovdquq(C0, B0, Assembler::AVX_512bit);\n+  __ evmovdquq(C1, B1, Assembler::AVX_512bit);\n+  __ evmovdquq(C2, B2, Assembler::AVX_512bit);\n+\n@@ -755,5 +757,0 @@\n-  \/\/ Store R^8-R for later use\n-  __ evmovdquq(Address(rsp, 64*0), B0, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(rsp, 64*1), B1, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(rsp, 64*2), B2, Assembler::AVX_512bit);\n-\n@@ -766,3 +763,3 @@\n-  __ evmovdquq(Address(rsp, 64*3), B0, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(rsp, 64*4), B1, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(rsp, 64*5), B2, Assembler::AVX_512bit);\n+  __ evmovdquq(C3, B0, Assembler::AVX_512bit);\n+  __ evmovdquq(C4, B1, Assembler::AVX_512bit);\n+  __ evmovdquq(C5, B2, Assembler::AVX_512bit);\n@@ -819,9 +816,0 @@\n-  \/\/ Read R^16-R^9\n-  __ evmovdquq(B0, Address(rsp, 64*3), Assembler::AVX_512bit);\n-  __ evmovdquq(B1, Address(rsp, 64*4), Assembler::AVX_512bit);\n-  __ evmovdquq(B2, Address(rsp, 64*5), Assembler::AVX_512bit);\n-  \/\/ Read R^8-R\n-  __ evmovdquq(R0, Address(rsp, 64*0), Assembler::AVX_512bit);\n-  __ evmovdquq(R1, Address(rsp, 64*1), Assembler::AVX_512bit);\n-  __ evmovdquq(R2, Address(rsp, 64*2), Assembler::AVX_512bit);\n-\n@@ -829,6 +817,7 @@\n-  __ vpsllq(D0, B1, 2, Assembler::AVX_512bit);\n-  __ vpaddq(B3, B1, D0, Assembler::AVX_512bit); \/\/ R1' (R1*5)\n-  __ vpsllq(D0, B2, 2, Assembler::AVX_512bit);\n-  __ vpaddq(B4, B2, D0, Assembler::AVX_512bit); \/\/ R2' (R2*5)\n-  __ vpsllq(B3, B3, 2, Assembler::AVX_512bit);  \/\/ 4*5*R\n-  __ vpsllq(B4, B4, 2, Assembler::AVX_512bit);\n+  \/\/ Use D0 ~ R1P, D1 ~ R2P for higher powers\n+  __ vpsllq(R1P, C4, 2, Assembler::AVX_512bit);\n+  __ vpsllq(R2P, C5, 2, Assembler::AVX_512bit);\n+  __ vpaddq(R1P, R1P, C4, Assembler::AVX_512bit);    \/\/ 5*R^8\n+  __ vpaddq(R2P, R2P, C5, Assembler::AVX_512bit);\n+  __ vpsllq(D0, R1P, 2, Assembler::AVX_512bit);      \/\/ 4*5*R^8\n+  __ vpsllq(D1, R2P, 2, Assembler::AVX_512bit);\n@@ -837,5 +826,5 @@\n-  __ vpsllq(D0, R1, 2, Assembler::AVX_512bit);\n-  __ vpaddq(R1P, R1, D0, Assembler::AVX_512bit); \/\/ R1' (R1*5)\n-  __ vpsllq(D0, R2, 2, Assembler::AVX_512bit);\n-  __ vpaddq(R2P, R2, D0, Assembler::AVX_512bit); \/\/ R2' (R2*5)\n-  __ vpsllq(R1P, R1P, 2, Assembler::AVX_512bit); \/\/ 4*5*R\n+  __ vpsllq(R1P, C1, 2, Assembler::AVX_512bit);\n+  __ vpsllq(R2P, C2, 2, Assembler::AVX_512bit);\n+  __ vpaddq(R1P, R1P, C1, Assembler::AVX_512bit);    \/\/ 5*R^8\n+  __ vpaddq(R2P, R2P, C2, Assembler::AVX_512bit);\n+  __ vpsllq(R1P, R1P, 2, Assembler::AVX_512bit);     \/\/ 4*5*R^8\n@@ -845,1 +834,1 @@\n-                            B0, B1, B2, B3, B4,    \/\/ R^16-R^9, R1P, R2P\n+                            C3, C4, C5, D0, D1,    \/\/ R^16-R^9, R1P, R2P\n@@ -848,1 +837,1 @@\n-                            R0, R1, R2, R1P, R2P,  \/\/ R^8-R, R1P, R2P\n+                            C0, C1, C2, R1P, R2P,  \/\/ R^8-R, R1P, R2P\n@@ -858,6 +847,6 @@\n-  __ vextracti64x4(D0, A0, 1);\n-  __ vextracti64x4(D1, A1, 1);\n-  __ vextracti64x4(D2, A2, 1);\n-  __ vpaddq(A0, A0, D0, Assembler::AVX_256bit);\n-  __ vpaddq(A1, A1, D1, Assembler::AVX_256bit);\n-  __ vpaddq(A2, A2, D2, Assembler::AVX_256bit);\n+  __ vextracti64x4(T0, A0, 1);\n+  __ vextracti64x4(T1, A1, 1);\n+  __ vextracti64x4(T2, A2, 1);\n+  __ vpaddq(A0, A0, T0, Assembler::AVX_256bit);\n+  __ vpaddq(A1, A1, T1, Assembler::AVX_256bit);\n+  __ vpaddq(A2, A2, T2, Assembler::AVX_256bit);\n@@ -866,6 +855,6 @@\n-  __ vextracti32x4(D0, A0, 1);\n-  __ vextracti32x4(D1, A1, 1);\n-  __ vextracti32x4(D2, A2, 1);\n-  __ vpaddq(A0, A0, D0, Assembler::AVX_128bit);\n-  __ vpaddq(A1, A1, D1, Assembler::AVX_128bit);\n-  __ vpaddq(A2, A2, D2, Assembler::AVX_128bit);\n+  __ vextracti32x4(T0, A0, 1);\n+  __ vextracti32x4(T1, A1, 1);\n+  __ vextracti32x4(T2, A2, 1);\n+  __ vpaddq(A0, A0, T0, Assembler::AVX_128bit);\n+  __ vpaddq(A1, A1, T1, Assembler::AVX_128bit);\n+  __ vpaddq(A2, A2, T2, Assembler::AVX_128bit);\n@@ -874,3 +863,3 @@\n-  __ vpsrldq(D0, A0, 8, Assembler::AVX_128bit);\n-  __ vpsrldq(D1, A1, 8, Assembler::AVX_128bit);\n-  __ vpsrldq(D2, A2, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(T0, A0, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(T1, A1, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(T2, A2, 8, Assembler::AVX_128bit);\n@@ -881,3 +870,3 @@\n-  __ evpaddq(A0, k1, A0, D0, false, Assembler::AVX_512bit);\n-  __ evpaddq(A1, k1, A1, D1, false, Assembler::AVX_512bit);\n-  __ evpaddq(A2, k1, A2, D2, false, Assembler::AVX_512bit);\n+  __ evpaddq(A0, k1, A0, T0, false, Assembler::AVX_512bit);\n+  __ evpaddq(A1, k1, A1, T1, false, Assembler::AVX_512bit);\n+  __ evpaddq(A2, k1, A2, T2, false, Assembler::AVX_512bit);\n@@ -916,32 +905,5 @@\n-  __ vpxorq(xmm0, xmm0, xmm0, Assembler::AVX_512bit);\n-  __ vpxorq(xmm1, xmm1, xmm1, Assembler::AVX_512bit);\n-  __ vpxorq(D0, D0, D0, Assembler::AVX_512bit);\n-  __ vpxorq(D1, D1, D1, Assembler::AVX_512bit);\n-  __ vpxorq(D2, D2, D2, Assembler::AVX_512bit);\n-  __ vpxorq(C0, C0, C0, Assembler::AVX_512bit);\n-  __ vpxorq(C1, C1, C1, Assembler::AVX_512bit);\n-  __ vpxorq(C2, C2, C2, Assembler::AVX_512bit);\n-  __ vpxorq(A0, A0, A0, Assembler::AVX_512bit);\n-  __ vpxorq(A1, A1, A1, Assembler::AVX_512bit);\n-  __ vpxorq(A2, A2, A2, Assembler::AVX_512bit);\n-  __ vpxorq(A3, A3, A3, Assembler::AVX_512bit);\n-  __ vpxorq(A4, A4, A4, Assembler::AVX_512bit);\n-  __ vpxorq(A5, A5, A5, Assembler::AVX_512bit);\n-  __ vpxorq(B0, B0, B0, Assembler::AVX_512bit);\n-  __ vpxorq(B1, B1, B1, Assembler::AVX_512bit);\n-  __ vpxorq(B2, B2, B2, Assembler::AVX_512bit);\n-  __ vpxorq(B3, B3, B3, Assembler::AVX_512bit);\n-  __ vpxorq(B4, B4, B4, Assembler::AVX_512bit);\n-  __ vpxorq(B5, B5, B5, Assembler::AVX_512bit);\n-  __ vpxorq(R0, R0, R0, Assembler::AVX_512bit);\n-  __ vpxorq(R1, R1, R1, Assembler::AVX_512bit);\n-  __ vpxorq(R2, R2, R2, Assembler::AVX_512bit);\n-  __ vpxorq(R1P, R1P, R1P, Assembler::AVX_512bit);\n-  __ vpxorq(R2P, R2P, R2P, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(rsp, 64*3), A0, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(rsp, 64*4), A0, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(rsp, 64*5), A0, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(rsp, 64*0), A0, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(rsp, 64*1), A0, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(rsp, 64*2), A0, Assembler::AVX_512bit);\n-  __ addq(rsp, 512\/8*6); \/\/ (powers of R)\n+  \/\/ Zero out zmm0-zmm31.\n+  __ vzeroall();\n+  for (XMMRegister rxmm = xmm16; rxmm->is_valid(); rxmm = rxmm->successor()) {\n+    __ vpxorq(rxmm, rxmm, rxmm, Assembler::AVX_512bit);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_poly.cpp","additions":129,"deletions":167,"binary":false,"changes":296,"status":"modified"}]}