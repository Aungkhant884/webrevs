{"files":[{"patch":"@@ -395,2 +395,4 @@\n-  void poly1305_multiply_scalar(const Register A0, const Register A1, const Register A2,\n-                                const Register R0, const Register R1, const Register C1, bool only128);\n+  void poly1305_multiply_scalar(const Register a0, const Register a1, const Register a2,\n+                                const Register r0, const Register r1, const Register c1, bool only128,\n+                                const Register t0, const Register t1, const Register t2,\n+                                const Register mulql, const Register mulqh);\n@@ -398,3 +400,5 @@\n-                                 const XMMRegister R0, const XMMRegister R1, const XMMRegister R2, const XMMRegister R1P, const XMMRegister R2P);\n-  void poly1305_limbs(const Register limbs, const Register a0, const Register a1, const Register a2, const Register t1, const Register t2);\n-  void poly1305_limbs_out(const Register a0, const Register a1, const Register a2, const Register limbs, const Register t1, const Register t2);\n+                                 const XMMRegister R0, const XMMRegister R1, const XMMRegister R2, const XMMRegister R1P, const XMMRegister R2P,\n+                                 const XMMRegister P0L, const XMMRegister P0H, const XMMRegister P1L, const XMMRegister P1H, const XMMRegister P2L, const XMMRegister P2H,\n+                                 const XMMRegister TMP, const Register rscratch);\n+  void poly1305_limbs(const Register limbs, const Register a0, const Register a1, const Register a2, const Register t0, const Register t1);\n+  void poly1305_limbs_out(const Register a0, const Register a1, const Register a2, const Register limbs, const Register t0, const Register t1);\n@@ -403,1 +407,1 @@\n-                             const XMMRegister TMP1, const XMMRegister TMP2, const Register rscratch);\n+                             const XMMRegister TMP, const Register rscratch);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -55,26 +55,0 @@\n-\/\/ Register Map:\n-\/\/ GPRs:\n-\/\/   input        = rdi\n-\/\/   length       = rbx\n-\/\/   accumulator  = rcx\n-\/\/   R   = r8\n-\/\/   a0  = rsi\n-\/\/   a1  = r9\n-\/\/   a2  = r10\n-\/\/   r0  = r11\n-\/\/   r1  = r12\n-\/\/   c1  = r8;\n-\/\/   t1  = r13\n-\/\/   t2  = r14\n-\/\/   t3  = r15\n-\/\/   t0  = r14\n-\/\/   rscratch = r13\n-\/\/   stack(rsp, rbp)\n-\/\/   imul(rax, rdx)\n-\/\/ ZMMs:\n-\/\/   T: xmm0-6\n-\/\/   C: xmm7-9\n-\/\/   A: xmm13-18\n-\/\/   B: xmm19-24\n-\/\/   R: xmm25-29\n-\n@@ -151,1 +125,3 @@\n-  const XMMRegister R0, const XMMRegister R1, const XMMRegister R2, const XMMRegister R1P, const XMMRegister R2P)\n+  const XMMRegister R0, const XMMRegister R1, const XMMRegister R2, const XMMRegister R1P, const XMMRegister R2P,\n+  const XMMRegister P0L, const XMMRegister P0H, const XMMRegister P1L, const XMMRegister P1H, const XMMRegister P2L, const XMMRegister P2H,\n+  const XMMRegister TMP, const Register rscratch)\n@@ -153,8 +129,0 @@\n-  const XMMRegister P0_L = xmm0;\n-  const XMMRegister P0_H = xmm1;\n-  const XMMRegister P1_L = xmm2;\n-  const XMMRegister P1_H = xmm3;\n-  const XMMRegister P2_L = xmm4;\n-  const XMMRegister P2_H = xmm5;\n-  const XMMRegister TMP1 = xmm6;\n-  const Register rscratch = r13;\n@@ -163,6 +131,6 @@\n-  __ evpxorq(P0_L, P0_L, P0_L, Assembler::AVX_512bit);\n-  __ evpxorq(P0_H, P0_H, P0_H, Assembler::AVX_512bit);\n-  __ evpxorq(P1_L, P1_L, P1_L, Assembler::AVX_512bit);\n-  __ evpxorq(P1_H, P1_H, P1_H, Assembler::AVX_512bit);\n-  __ evpxorq(P2_L, P2_L, P2_L, Assembler::AVX_512bit);\n-  __ evpxorq(P2_H, P2_H, P2_H, Assembler::AVX_512bit);\n+  __ evpxorq(P0L, P0L, P0L, Assembler::AVX_512bit);\n+  __ evpxorq(P0H, P0H, P0H, Assembler::AVX_512bit);\n+  __ evpxorq(P1L, P1L, P1L, Assembler::AVX_512bit);\n+  __ evpxorq(P1H, P1H, P1H, Assembler::AVX_512bit);\n+  __ evpxorq(P2L, P2L, P2L, Assembler::AVX_512bit);\n+  __ evpxorq(P2H, P2H, P2H, Assembler::AVX_512bit);\n@@ -171,20 +139,29 @@\n-  __ evpmadd52luq(P0_L, A2, R1P, Assembler::AVX_512bit);\n-  __ evpmadd52huq(P0_H, A2, R1P, Assembler::AVX_512bit);\n-  __ evpmadd52luq(P1_L, A2, R2P, Assembler::AVX_512bit);\n-  __ evpmadd52huq(P1_H, A2, R2P, Assembler::AVX_512bit);\n-  __ evpmadd52luq(P2_L, A2, R0, Assembler::AVX_512bit);\n-  __ evpmadd52huq(P2_H, A2, R0, Assembler::AVX_512bit);\n-\n-  __ evpmadd52luq(P1_L, A0, R1, Assembler::AVX_512bit);\n-  __ evpmadd52huq(P1_H, A0, R1, Assembler::AVX_512bit);\n-  __ evpmadd52luq(P2_L, A0, R2, Assembler::AVX_512bit);\n-  __ evpmadd52huq(P2_H, A0, R2, Assembler::AVX_512bit);\n-  __ evpmadd52luq(P0_L, A0, R0, Assembler::AVX_512bit);\n-  __ evpmadd52huq(P0_H, A0, R0, Assembler::AVX_512bit);\n-\n-  __ evpmadd52luq(P0_L, A1, R2P, Assembler::AVX_512bit);\n-  __ evpmadd52huq(P0_H, A1, R2P, Assembler::AVX_512bit);\n-  __ evpmadd52luq(P1_L, A1, R0, Assembler::AVX_512bit);\n-  __ evpmadd52huq(P1_H, A1, R0, Assembler::AVX_512bit);\n-  __ evpmadd52luq(P2_L, A1, R1, Assembler::AVX_512bit);\n-  __ evpmadd52huq(P2_H, A1, R1, Assembler::AVX_512bit);\n+  \/\/ p0 = a2×r1'\n+  \/\/ p1 = a2×r2'\n+  \/\/ p2 = a2×r0\n+  __ evpmadd52luq(P0L, A2, R1P, Assembler::AVX_512bit);\n+  __ evpmadd52huq(P0H, A2, R1P, Assembler::AVX_512bit);\n+  __ evpmadd52luq(P1L, A2, R2P, Assembler::AVX_512bit);\n+  __ evpmadd52huq(P1H, A2, R2P, Assembler::AVX_512bit);\n+  __ evpmadd52luq(P2L, A2, R0, Assembler::AVX_512bit);\n+  __ evpmadd52huq(P2H, A2, R0, Assembler::AVX_512bit);\n+\n+  \/\/ p0 += a0×r0\n+  \/\/ p1 += a0×r1\n+  \/\/ p2 += a0×r2\n+  __ evpmadd52luq(P1L, A0, R1, Assembler::AVX_512bit);\n+  __ evpmadd52huq(P1H, A0, R1, Assembler::AVX_512bit);\n+  __ evpmadd52luq(P2L, A0, R2, Assembler::AVX_512bit);\n+  __ evpmadd52huq(P2H, A0, R2, Assembler::AVX_512bit);\n+  __ evpmadd52luq(P0L, A0, R0, Assembler::AVX_512bit);\n+  __ evpmadd52huq(P0H, A0, R0, Assembler::AVX_512bit);\n+\n+  \/\/ p0 += a1×r2'\n+  \/\/ p1 += a1×r0\n+  \/\/ p2 += a1×r1\n+  __ evpmadd52luq(P0L, A1, R2P, Assembler::AVX_512bit);\n+  __ evpmadd52huq(P0H, A1, R2P, Assembler::AVX_512bit);\n+  __ evpmadd52luq(P1L, A1, R0, Assembler::AVX_512bit);\n+  __ evpmadd52huq(P1H, A1, R0, Assembler::AVX_512bit);\n+  __ evpmadd52luq(P2L, A1, R1, Assembler::AVX_512bit);\n+  __ evpmadd52huq(P2H, A1, R1, Assembler::AVX_512bit);\n@@ -193,5 +170,5 @@\n-  \/\/ (Not quite aligned)                           | More mathematically correct:\n-  \/\/          P2_L   P1_L   P0_L                   |                  P2_L×2^88 + P1_L×2^44 + P0_L×2^0\n-  \/\/ + P2_H   P1_H   P0_H                          |   + P2_H×2^140 + P1_H×2^96 + P0_H×2^52\n-  \/\/ ---------------------------                   |   -----------------------------------------------\n-  \/\/ = P2_H    A2    A1     A0                     |   = P2_H×2^130 +   A2×2^88 +   A1×2^44 +   A0×2^0\n+  \/\/ (Not quite aligned)                         | More mathematically correct:\n+  \/\/         P2L   P1L   P0L                     |                 P2L×2^88 + P1L×2^44 + P0L×2^0\n+  \/\/ + P2H   P1H   P0H                           |   + P2H×2^140 + P1H×2^96 + P0H×2^52\n+  \/\/ ---------------------------                 |   -----------------------------------------------\n+  \/\/ = P2H    A2    A1    A0                     |   = P2H×2^130 + A2×2^88 +   A1×2^44 +  A0×2^0\n@@ -199,2 +176,2 @@\n-  __ vpsrlq(TMP1, P0_L, 44, Assembler::AVX_512bit);\n-  __ evpandq(A0, P0_L, ExternalAddress(poly1305_mask44()), Assembler::AVX_512bit, rscratch); \/\/ Clear top 20 bits\n+  __ vpsrlq(TMP, P0L, 44, Assembler::AVX_512bit);\n+  __ evpandq(A0, P0L, ExternalAddress(poly1305_mask44()), Assembler::AVX_512bit, rscratch); \/\/ Clear top 20 bits\n@@ -202,4 +179,4 @@\n-  __ vpsllq(P0_H, P0_H, 8, Assembler::AVX_512bit);\n-  __ vpaddq(P0_H, P0_H, TMP1, Assembler::AVX_512bit);\n-  __ vpaddq(P1_L, P1_L, P0_H, Assembler::AVX_512bit);\n-  __ evpandq(A1, P1_L, ExternalAddress(poly1305_mask44()), Assembler::AVX_512bit, rscratch); \/\/ Clear top 20 bits\n+  __ vpsllq(P0H, P0H, 8, Assembler::AVX_512bit);\n+  __ vpaddq(P0H, P0H, TMP, Assembler::AVX_512bit);\n+  __ vpaddq(P1L, P1L, P0H, Assembler::AVX_512bit);\n+  __ evpandq(A1, P1L, ExternalAddress(poly1305_mask44()), Assembler::AVX_512bit, rscratch); \/\/ Clear top 20 bits\n@@ -207,5 +184,5 @@\n-  __ vpsrlq(TMP1, P1_L, 44, Assembler::AVX_512bit);\n-  __ vpsllq(P1_H, P1_H, 8, Assembler::AVX_512bit);\n-  __ vpaddq(P1_H, P1_H, TMP1, Assembler::AVX_512bit);\n-  __ vpaddq(P2_L, P2_L, P1_H, Assembler::AVX_512bit);\n-  __ evpandq(A2, P2_L, ExternalAddress(poly1305_mask42()), Assembler::AVX_512bit, rscratch); \/\/ Clear top 22 bits\n+  __ vpsrlq(TMP, P1L, 44, Assembler::AVX_512bit);\n+  __ vpsllq(P1H, P1H, 8, Assembler::AVX_512bit);\n+  __ vpaddq(P1H, P1H, TMP, Assembler::AVX_512bit);\n+  __ vpaddq(P2L, P2L, P1H, Assembler::AVX_512bit);\n+  __ evpandq(A2, P2L, ExternalAddress(poly1305_mask42()), Assembler::AVX_512bit, rscratch); \/\/ Clear top 22 bits\n@@ -213,3 +190,3 @@\n-  __ vpsrlq(TMP1, P2_L, 42, Assembler::AVX_512bit);\n-  __ vpsllq(P2_H, P2_H, 10, Assembler::AVX_512bit);\n-  __ vpaddq(P2_H, P2_H, TMP1, Assembler::AVX_512bit);\n+  __ vpsrlq(TMP, P2L, 42, Assembler::AVX_512bit);\n+  __ vpsllq(P2H, P2H, 10, Assembler::AVX_512bit);\n+  __ vpaddq(P2H, P2H, TMP, Assembler::AVX_512bit);\n@@ -219,4 +196,4 @@\n-  __ vpaddq(A0, A0, P2_H, Assembler::AVX_512bit);\n-  __ vpsllq(P2_H, P2_H, 2, Assembler::AVX_512bit);\n-  __ vpaddq(A0, A0, P2_H, Assembler::AVX_512bit);\n-  __ vpsrlq(TMP1, A0, 44, Assembler::AVX_512bit);\n+  __ vpaddq(A0, A0, P2H, Assembler::AVX_512bit);\n+  __ vpsllq(P2H, P2H, 2, Assembler::AVX_512bit);\n+  __ vpaddq(A0, A0, P2H, Assembler::AVX_512bit);\n+  __ vpsrlq(TMP, A0, 44, Assembler::AVX_512bit);\n@@ -224,1 +201,1 @@\n-  __ vpaddq(A1, A1, TMP1, Assembler::AVX_512bit);\n+  __ vpaddq(A1, A1, TMP, Assembler::AVX_512bit);\n@@ -229,2 +206,2 @@\n-\/\/ - When only128 is set, Input [a2 a1 a0] is 128 bits (i.e. a2==0)\n-\/\/ - Output [a2 a1 a0] is at least 130 bits (i.e. a2 is used)\n+\/\/ - Input [a2 a1 a0]; when only128 is set, input is 128 bits (i.e. a2==0)\n+\/\/ - Output [a2 a1 a0] is at least 130 bits (i.e. a2 is used regardless of only128)\n@@ -246,1 +223,1 @@\n-\/\/   Registers:  t3:t2     t1:a0\n+\/\/   Registers:  t2:t1     t0:a0\n@@ -252,1 +229,1 @@\n-\/\/ t3 = L1H + L2L\n+\/\/ t2 = L1H + L2L\n@@ -255,1 +232,3 @@\n-  const Register r0, const Register r1, const Register c1, bool only128)\n+  const Register r0, const Register r1, const Register c1, bool only128,\n+  const Register t0, const Register t1, const Register t2,\n+  const Register mulql, const Register mulqh)\n@@ -257,4 +236,1 @@\n-  const Register t1 = r13;\n-  const Register t2 = r14;\n-  const Register t3 = r15;\n-  \/\/ Note mulq instruction requires\/clobers rax, rdx\n+  \/\/ mulq instruction requires\/clobers rax, rdx (mulql, mulqh)\n@@ -262,1 +238,1 @@\n-  \/\/ t3:t2 = (a0 * r1)\n+  \/\/ t2:t1 = (a0 * r1)\n@@ -265,2 +241,2 @@\n-  __ movq(t2, rax);\n-  __ movq(t3, rdx);\n+  __ movq(t1, rax);\n+  __ movq(t2, rdx);\n@@ -268,1 +244,1 @@\n-  \/\/ t1:a0 = (a0 * r0)\n+  \/\/ t0:a0 = (a0 * r0)\n@@ -272,1 +248,1 @@\n-  __ movq(t1, rdx);\n+  __ movq(t0, rdx);\n@@ -274,1 +250,1 @@\n-  \/\/ t3:t2 += (a1 * r0)\n+  \/\/ t2:t1 += (a1 * r0)\n@@ -277,2 +253,2 @@\n-  __ addq(t2, rax);\n-  __ adcq(t3, rdx);\n+  __ addq(t1, rax);\n+  __ adcq(t2, rdx);\n@@ -280,1 +256,1 @@\n-  \/\/ t1:a0 += (a1 * r1x5)\n+  \/\/ t0:a0 += (a1 * r1x5)\n@@ -284,1 +260,1 @@\n-  __ adcq(t1, rdx);\n+  __ adcq(t0, rdx);\n@@ -291,4 +267,4 @@\n-    \/\/ just move and add t1-t2 to a1\n-    __ movq(a1, t1);\n-    __ addq(a1, t2);\n-    __ adcq(t3, 0);\n+    \/\/ just move and add t0-t1 to a1\n+    __ movq(a1, t0);\n+    __ addq(a1, t1);\n+    __ adcq(t2, 0);\n@@ -296,1 +272,1 @@\n-    \/\/ t3:t2 += (a2 * r1x5)\n+    \/\/ t2:t1 += (a2 * r1x5)\n@@ -299,2 +275,2 @@\n-    __ addq(t2, a1);\n-    __ adcq(t3, 0);\n+    __ addq(t1, a1);\n+    __ adcq(t2, 0);\n@@ -302,1 +278,1 @@\n-    __ movq(a1, t1); \/\/ t1:a0 => a1:a0\n+    __ movq(a1, t0); \/\/ t0:a0 => a1:a0\n@@ -304,1 +280,1 @@\n-    \/\/ t3:a1 += (a2 * r0):t2\n+    \/\/ t2:a1 += (a2 * r0):t1\n@@ -306,2 +282,2 @@\n-    __ addq(a1, t2);\n-    __ adcq(t3, a2);\n+    __ addq(a1, t1);\n+    __ adcq(t2, a2);\n@@ -310,2 +286,2 @@\n-  \/\/ At this point, 3 64-bit limbs are in t3:a1:a0\n-  \/\/ t3 can span over more than 2 bits so final partial reduction step is needed.\n+  \/\/ At this point, 3 64-bit limbs are in t2:a1:a0\n+  \/\/ t2 can span over more than 2 bits so final partial reduction step is needed.\n@@ -314,2 +290,2 @@\n-  \/\/    a2 = t3 & 3\n-  \/\/    k = (t3 & ~3) + (t3 >> 2)\n+  \/\/    a2 = t2 & 3\n+  \/\/    k = (t2 & ~3) + (t2 >> 2)\n@@ -320,5 +296,5 @@\n-  __ movq(t1, t3);\n-  __ movl(a2, t3); \/\/ DWORD\n-  __ andq(t1, ~3);\n-  __ shrq(t3, 2);\n-  __ addq(t1, t3);\n+  __ movq(t0, t2);\n+  __ movl(a2, t2); \/\/ DWORD\n+  __ andq(t0, ~3);\n+  __ shrq(t2, 2);\n+  __ addq(t0, t2);\n@@ -327,2 +303,2 @@\n-  \/\/ a2:a1:a0 += k (kept in t1)\n-  __ addq(a0, t1);\n+  \/\/ a2:a1:a0 += k (kept in t0)\n+  __ addq(a0, t0);\n@@ -352,1 +328,1 @@\n-    const XMMRegister TMP1, const XMMRegister TMP2, const Register rscratch)\n+    const XMMRegister TMP, const Register rscratch)\n@@ -355,1 +331,1 @@\n-  __ evpunpckhqdq(TMP1, D0, D1, Assembler::AVX_512bit);\n+  __ evpunpckhqdq(TMP, D0, D1, Assembler::AVX_512bit);\n@@ -359,1 +335,1 @@\n-  __ vpsrlq(L2, TMP1, 24, Assembler::AVX_512bit);\n+  __ vpsrlq(L2, TMP, 24, Assembler::AVX_512bit);\n@@ -366,2 +342,2 @@\n-  __ vpsllq(TMP2, TMP1, 20, Assembler::AVX_512bit);\n-  __ vpternlogq(L1, 0xA8, TMP2, ExternalAddress(poly1305_mask44()), Assembler::AVX_512bit, rscratch); \/\/ (A OR B AND C)\n+  __ vpsllq(TMP, TMP, 20, Assembler::AVX_512bit);\n+  __ vpternlogq(L1, 0xA8, TMP, ExternalAddress(poly1305_mask44()), Assembler::AVX_512bit, rscratch); \/\/ (A OR B AND C)\n@@ -380,1 +356,1 @@\n-    const Register t1, const Register t2)\n+    const Register t0, const Register t1)\n@@ -383,7 +359,7 @@\n-  __ movq(t1, Address(limbs, 8));\n-  __ shlq(t1, 26);\n-  __ addq(a0, t1);\n-  __ movq(t1, Address(limbs, 16));\n-  __ movq(t2, Address(limbs, 24));\n-  __ movq(a1, t1);\n-  __ shlq(t1, 52);\n+  __ movq(t0, Address(limbs, 8));\n+  __ shlq(t0, 26);\n+  __ addq(a0, t0);\n+  __ movq(t0, Address(limbs, 16));\n+  __ movq(t1, Address(limbs, 24));\n+  __ movq(a1, t0);\n+  __ shlq(t0, 52);\n@@ -391,4 +367,4 @@\n-  __ shlq(t2, 14);\n-  __ addq(a0, t1);\n-  __ adcq(a1, t2);\n-  __ movq(t1, Address(limbs, 32));\n+  __ shlq(t1, 14);\n+  __ addq(a0, t0);\n+  __ adcq(a1, t1);\n+  __ movq(t0, Address(limbs, 32));\n@@ -396,1 +372,1 @@\n-    __ movq(a2, t1);\n+    __ movq(a2, t0);\n@@ -399,2 +375,2 @@\n-  __ shlq(t1, 40);\n-  __ addq(a1, t1);\n+  __ shlq(t0, 40);\n+  __ addq(a1, t0);\n@@ -408,2 +384,2 @@\n-  __ movq(t1, a2);\n-  __ andq(t1, ~3);\n+  __ movq(t0, a2);\n+  __ andq(t0, ~3);\n@@ -411,3 +387,3 @@\n-  __ movq(t2, t1);\n-  __ shrq(t2, 2);\n-  __ addq(t1, t2);\n+  __ movq(t1, t0);\n+  __ shrq(t1, 2);\n+  __ addq(t0, t1);\n@@ -415,1 +391,1 @@\n-  __ addq(a0, t1);\n+  __ addq(a0, t0);\n@@ -426,1 +402,1 @@\n-    const Register t1, const Register t2)\n+    const Register t0, const Register t1)\n@@ -430,2 +406,2 @@\n-  __ movq(t1, a2);\n-  __ andq(t1, ~3);\n+  __ movq(t0, a2);\n+  __ andq(t0, ~3);\n@@ -433,3 +409,3 @@\n-  __ movq(t2, t1);\n-  __ shrq(t2, 2);\n-  __ addq(t1, t2);\n+  __ movq(t1, t0);\n+  __ shrq(t1, 2);\n+  __ addq(t0, t1);\n@@ -437,1 +413,1 @@\n-  __ addq(a0, t1);\n+  __ addq(a0, t0);\n@@ -442,3 +418,3 @@\n-  __ movl(t1, a0);\n-  __ andl(t1, 0x3ffffff);\n-  __ movq(Address(limbs, 0), t1);\n+  __ movl(t0, a0);\n+  __ andl(t0, 0x3ffffff);\n+  __ movq(Address(limbs, 0), t0);\n@@ -447,3 +423,3 @@\n-  __ movl(t1, a0);\n-  __ andl(t1, 0x3ffffff);\n-  __ movq(Address(limbs, 8), t1);\n+  __ movl(t0, a0);\n+  __ andl(t0, 0x3ffffff);\n+  __ movq(Address(limbs, 8), t0);\n@@ -452,5 +428,5 @@\n-  __ movl(t1, a1);\n-  __ shll(t1, 12);\n-  __ addl(t1, a0);\n-  __ andl(t1, 0x3ffffff);\n-  __ movq(Address(limbs, 16), t1);\n+  __ movl(t0, a1);\n+  __ shll(t0, 12);\n+  __ addl(t0, a0);\n+  __ andl(t0, 0x3ffffff);\n+  __ movq(Address(limbs, 16), t0);\n@@ -462,3 +438,3 @@\n-  __ movl(t1, a1);\n-  __ andl(t1, 0x3ffffff);\n-  __ movq(Address(limbs, 24), t1);\n+  __ movl(t0, a1);\n+  __ andl(t0, 0x3ffffff);\n+  __ movq(Address(limbs, 24), t0);\n@@ -467,3 +443,3 @@\n-  __ movl(t1, a1);\n-  \/\/andl(t1, 0x3ffffff); doesnt have to be fully reduced, leave remaining bit(s)\n-  __ movq(Address(limbs, 32), t1);\n+  __ movl(t0, a1);\n+  \/\/andl(t0, 0x3ffffff); doesnt have to be fully reduced, leave remaining bit(s)\n+  __ movq(Address(limbs, 32), t0);\n@@ -477,1 +453,2 @@\n-\/\/    Put simply, main loop in this function multiplies each message block by r^16; why this works? 'Math' happens before and after.. why as follows:\n+\/\/    Main loop in this function multiplies each message block by r^16; And some glue before and after..\n+\/\/    Proof (for brevity, split into 4 'rows' instead of 16):\n@@ -482,1 +459,1 @@\n-\/\/          = m1*r^n     + m4*r^(n-4) + m8*r^(n-8) ...    \/\/ split into 4 groups for brevity, same applies to 16\n+\/\/          = m1*r^n     + m4*r^(n-4) + m8*r^(n-8) ...    \/\/ split into 4 groups for brevity, same applies to 16 blocks\n@@ -490,1 +467,1 @@\n-\/\/          + r^1 * (m4*r^(n-4) + m7*r^(n-8) + m11*r^(n-16) ... + mn_0)   \/\/ Note last message group has no multiplier\n+\/\/          + r^1 * (m4*r^(n-4) + m7*r^(n-8) + m11*r^(n-16) ... + mn_0)   \/\/ Note last column: message group has no multiplier\n@@ -492,4 +469,4 @@\n-\/\/          = r^4 * (((m1*r^4 + m4)*r^4 + m8 )*r^4 ... + mn_3)   \/\/ reverse Horner's rule, for each group\n-\/\/          + r^3 * (((m2*r^4 + m5)*r^4 + m9 )*r^4 ... + mn_2)\n-\/\/          + r^2 * (((m3*r^4 + m6)*r^4 + m10)*r^4 ... + mn_1)\n-\/\/          + r^1 * (((m4*r^4 + m7)*r^4 + m11)*r^4 ... + mn_0)\n+\/\/          = (((m1*r^4 + m4)*r^4 + m8 )*r^4 ... + mn_3) * r^4   \/\/ reverse Horner's rule, for each group\n+\/\/          + (((m2*r^4 + m5)*r^4 + m9 )*r^4 ... + mn_2) * r^3   \/\/ each column is multiplied by r^4, except last\n+\/\/          + (((m3*r^4 + m6)*r^4 + m10)*r^4 ... + mn_1) * r^2\n+\/\/          + (((m4*r^4 + m7)*r^4 + m11)*r^4 ... + mn_0) * r^1\n@@ -499,1 +476,1 @@\n-\/\/ Pseudocode for this function:\n+\/\/ Pseudocode:\n@@ -506,2 +483,2 @@\n-\/\/    AL = limbs(input)\n-\/\/    AH = limbs(input+8)\n+\/\/    AL = poly1305_limbs_avx512(input)\n+\/\/    AH = poly1305_limbs_avx512(input+8)\n@@ -532,2 +509,2 @@\n-\/\/     BL = limbs(input)\n-\/\/     BH = limbs(input+8)\n+\/\/     BL = poly1305_limbs_avx512(input)\n+\/\/     BH = poly1305_limbs_avx512(input+8)\n@@ -551,0 +528,28 @@\n+\/\/\n+\/\/ Register Map:\n+\/\/ GPRs:\n+\/\/   input        = rdi\n+\/\/   length       = rbx\n+\/\/   accumulator  = rcx\n+\/\/   R   = r8\n+\/\/   a0  = rsi\n+\/\/   a1  = r9\n+\/\/   a2  = r10\n+\/\/   r0  = r11\n+\/\/   r1  = r12\n+\/\/   c1  = r8;\n+\/\/   t0  = r13\n+\/\/   t1  = r14\n+\/\/   t2  = r15\n+\/\/   rscratch = r13\n+\/\/   stack(rsp, rbp)\n+\/\/   mulq(rax, rdx) in poly1305_multiply_scalar\n+\/\/\n+\/\/ ZMMs:\n+\/\/   TMP: xmm6\n+\/\/   C: xmm7-9\n+\/\/   D: xmm2-4\n+\/\/   T: xmm0-5\n+\/\/   A: xmm13-18\n+\/\/   B: xmm19-24\n+\/\/   R: xmm25-29\n@@ -557,6 +562,3 @@\n-  \/\/ Register Map:\n-  \/\/ reserved: rsp, rbp, rcx\n-  \/\/ PARAMs: rdi, rbx, rsi, r8-r12\n-  \/\/ poly1305_multiply_scalar clobbers: r13-r15, rax, rdx\n-  const Register t0 = r14;\n-  const Register t1 = r13;\n+  const Register t0 = r13;\n+  const Register t1 = r14;\n+  const Register t2 = r15;\n@@ -564,0 +566,2 @@\n+  const Register mulql = rax;\n+  const Register mulqh = rdx;\n@@ -565,3 +569,1 @@\n-  \/\/ poly1305_multiply8_avx512 clobbers: xmm0-xmm6\n-  const XMMRegister TMP1 = xmm0;\n-  const XMMRegister TMP2 = xmm1;\n+  const XMMRegister TMP = xmm6;\n@@ -569,3 +571,3 @@\n-  const XMMRegister T0 = xmm2;\n-  const XMMRegister T1 = xmm3;\n-  const XMMRegister T2 = xmm4;\n+  const XMMRegister D0 = xmm2;\n+  const XMMRegister D1 = xmm3;\n+  const XMMRegister D2 = xmm4;\n@@ -577,0 +579,7 @@\n+  const XMMRegister T0 = xmm0;\n+  const XMMRegister T1 = xmm1;\n+  const XMMRegister T2 = xmm2;\n+  const XMMRegister T3 = xmm3;\n+  const XMMRegister T4 = xmm4;\n+  const XMMRegister T5 = xmm5;\n+\n@@ -619,3 +628,3 @@\n-  __ evmovdquq(T0, Address(input, 0), Assembler::AVX_512bit);\n-  __ evmovdquq(T1, Address(input, 64), Assembler::AVX_512bit);\n-  poly1305_limbs_avx512(T0, T1, A0, A1, A2, true, TMP1, TMP2, rscratch);\n+  __ evmovdquq(D0, Address(input, 0), Assembler::AVX_512bit);\n+  __ evmovdquq(D1, Address(input, 64), Assembler::AVX_512bit);\n+  poly1305_limbs_avx512(D0, D1, A0, A1, A2, true, TMP, rscratch);\n@@ -632,3 +641,3 @@\n-  __ evmovdquq(T0, Address(input, 64*2), Assembler::AVX_512bit);\n-  __ evmovdquq(T1, Address(input, 64*3), Assembler::AVX_512bit);\n-  poly1305_limbs_avx512(T0, T1, A3, A4, A5, true, TMP1, TMP2, rscratch);\n+  __ evmovdquq(D0, Address(input, 64*2), Assembler::AVX_512bit);\n+  __ evmovdquq(D1, Address(input, 64*3), Assembler::AVX_512bit);\n+  poly1305_limbs_avx512(D0, D1, A3, A4, A5, true, TMP, rscratch);\n@@ -640,6 +649,6 @@\n-  \/\/ T0 to have bits 0-127 in 4 quadword pairs\n-  \/\/ T1 to have bits 128-129 in alternating 8 qwords\n-  __ vpxorq(T1, T1, T1, Assembler::AVX_512bit);\n-  __ movq(T2, r0);\n-  __ vpinsrq(T2, T2, r1, 1);\n-  __ vinserti32x4(T0, T0, T2, 3);\n+  \/\/ D0 to have bits 0-127 in 4 quadword pairs\n+  \/\/ D1 to have bits 128-129 in alternating 8 qwords\n+  __ vpxorq(D1, D1, D1, Assembler::AVX_512bit);\n+  __ movq(D2, r0);\n+  __ vpinsrq(D2, D2, r1, 1);\n+  __ vinserti32x4(D0, D0, D2, 3);\n@@ -651,1 +660,3 @@\n-  poly1305_multiply_scalar(a0, a1, a2, r0, r1, c1, true);\n+  poly1305_multiply_scalar(a0, a1, a2,\n+                           r0, r1, c1, true,\n+                           t0, t1, t2, mulql, mulqh);\n@@ -653,5 +664,5 @@\n-  __ movq(T2, a0);\n-  __ vpinsrq(T2, T2, a1, 1);\n-  __ vinserti32x4(T0, T0, T2, 2);\n-  __ movq(T2, a2);\n-  __ vinserti32x4(T1, T1, T2, 2);\n+  __ movq(D2, a0);\n+  __ vpinsrq(D2, D2, a1, 1);\n+  __ vinserti32x4(D0, D0, D2, 2);\n+  __ movq(D2, a2);\n+  __ vinserti32x4(D1, D1, D2, 2);\n@@ -660,1 +671,3 @@\n-  poly1305_multiply_scalar(a0, a1, a2, r0, r1, c1, false);\n+  poly1305_multiply_scalar(a0, a1, a2,\n+                           r0, r1, c1, false,\n+                           t0, t1, t2, mulql, mulqh);\n@@ -662,5 +675,5 @@\n-  __ movq(T2, a0);\n-  __ vpinsrq(T2, T2, a1, 1);\n-  __ vinserti32x4(T0, T0, T2, 1);\n-  __ movq(T2, a2);\n-  __ vinserti32x4(T1, T1, T2, 1);\n+  __ movq(D2, a0);\n+  __ vpinsrq(D2, D2, a1, 1);\n+  __ vinserti32x4(D0, D0, D2, 1);\n+  __ movq(D2, a2);\n+  __ vinserti32x4(D1, D1, D2, 1);\n@@ -669,1 +682,3 @@\n-  poly1305_multiply_scalar(a0, a1, a2, r0, r1, c1, false);\n+  poly1305_multiply_scalar(a0, a1, a2,\n+                           r0, r1, c1, false,\n+                           t0, t1, t2, mulql, mulqh);\n@@ -671,5 +686,5 @@\n-  __ movq(T2, a0);\n-  __ vpinsrq(T2, T2, a1, 1);\n-  __ vinserti32x4(T0, T0, T2, 0);\n-  __ movq(T2, a2);\n-  __ vinserti32x4(T1, T1, T2, 0);\n+  __ movq(D2, a0);\n+  __ vpinsrq(D2, D2, a1, 1);\n+  __ vinserti32x4(D0, D0, D2, 0);\n+  __ movq(D2, a2);\n+  __ vinserti32x4(D1, D1, D2, 0);\n@@ -681,2 +696,2 @@\n-  __ vpxorq(T2, T2, T2, Assembler::AVX_512bit);\n-  poly1305_limbs_avx512(T0, T2, B0, B1, B2, false, TMP1, TMP2, rscratch);\n+  __ vpxorq(D2, D2, D2, Assembler::AVX_512bit);\n+  poly1305_limbs_avx512(D0, D2, B0, B1, B2, false, TMP, rscratch);\n@@ -684,3 +699,3 @@\n-  \/\/ T1 contains the 2 highest bits of the powers of R\n-  __ vpsllq(T1, T1, 40, Assembler::AVX_512bit);\n-  __ evporq(B2, B2, T1, Assembler::AVX_512bit);\n+  \/\/ D1 contains the 2 highest bits of the powers of R\n+  __ vpsllq(D1, D1, 40, Assembler::AVX_512bit);\n+  __ evporq(B2, B2, D1, Assembler::AVX_512bit);\n@@ -719,1 +734,2 @@\n-                            R0, R1, R2, R1P, R2P);  \/\/ R^4..R^4, 4*5*R^4\n+                            R0, R1, R2, R1P, R2P,   \/\/ R^4..R^4, 4*5*R^4\n+                            T0, T1, T2, T3, T4, T5, TMP, rscratch);\n@@ -746,1 +762,2 @@\n-                            R0, R1, R2, R1P, R2P); \/\/ R^8..R^8, 4*5*R^8\n+                            R0, R1, R2, R1P, R2P,  \/\/ R^8..R^8, 4*5*R^8\n+                            T0, T1, T2, T3, T4, T5, TMP, rscratch);\n@@ -772,3 +789,3 @@\n-  __ evmovdquq(T0, Address(input, 0), Assembler::AVX_512bit);\n-  __ evmovdquq(T1, Address(input, 64), Assembler::AVX_512bit);\n-  poly1305_limbs_avx512(T0, T1, B0, B1, B2, true, TMP1, TMP2, rscratch);\n+  __ evmovdquq(D0, Address(input, 0), Assembler::AVX_512bit);\n+  __ evmovdquq(D1, Address(input, 64), Assembler::AVX_512bit);\n+  poly1305_limbs_avx512(D0, D1, B0, B1, B2, true, TMP, rscratch);\n@@ -777,3 +794,3 @@\n-  __ evmovdquq(T0, Address(input, 64*2), Assembler::AVX_512bit);\n-  __ evmovdquq(T1, Address(input, 64*3), Assembler::AVX_512bit);\n-  poly1305_limbs_avx512(T0, T1, B3, B4, B5, true, TMP1, TMP2, rscratch);\n+  __ evmovdquq(D0, Address(input, 64*2), Assembler::AVX_512bit);\n+  __ evmovdquq(D1, Address(input, 64*3), Assembler::AVX_512bit);\n+  poly1305_limbs_avx512(D0, D1, B3, B4, B5, true, TMP, rscratch);\n@@ -782,1 +799,2 @@\n-                            R0, R1, R2, R1P, R2P); \/\/ R^16..R^16, 4*5*R^16\n+                            R0, R1, R2, R1P, R2P,  \/\/ R^16..R^16, 4*5*R^16\n+                            T0, T1, T2, T3, T4, T5, TMP, rscratch);\n@@ -784,1 +802,2 @@\n-                            R0, R1, R2, R1P, R2P); \/\/ R^16..R^16, 4*5*R^16\n+                            R0, R1, R2, R1P, R2P,  \/\/ R^16..R^16, 4*5*R^16\n+                            T0, T1, T2, T3, T4, T5, TMP, rscratch);\n@@ -810,4 +829,4 @@\n-  __ vpsllq(T0, B1, 2, Assembler::AVX_512bit);\n-  __ vpaddq(B3, B1, T0, Assembler::AVX_512bit); \/\/ R1' (R1*5)\n-  __ vpsllq(T0, B2, 2, Assembler::AVX_512bit);\n-  __ vpaddq(B4, B2, T0, Assembler::AVX_512bit); \/\/ R2' (R2*5)\n+  __ vpsllq(D0, B1, 2, Assembler::AVX_512bit);\n+  __ vpaddq(B3, B1, D0, Assembler::AVX_512bit); \/\/ R1' (R1*5)\n+  __ vpsllq(D0, B2, 2, Assembler::AVX_512bit);\n+  __ vpaddq(B4, B2, D0, Assembler::AVX_512bit); \/\/ R2' (R2*5)\n@@ -818,4 +837,4 @@\n-  __ vpsllq(T0, R1, 2, Assembler::AVX_512bit);\n-  __ vpaddq(R1P, R1, T0, Assembler::AVX_512bit); \/\/ R1' (R1*5)\n-  __ vpsllq(T0, R2, 2, Assembler::AVX_512bit);\n-  __ vpaddq(R2P, R2, T0, Assembler::AVX_512bit); \/\/ R2' (R2*5)\n+  __ vpsllq(D0, R1, 2, Assembler::AVX_512bit);\n+  __ vpaddq(R1P, R1, D0, Assembler::AVX_512bit); \/\/ R1' (R1*5)\n+  __ vpsllq(D0, R2, 2, Assembler::AVX_512bit);\n+  __ vpaddq(R2P, R2, D0, Assembler::AVX_512bit); \/\/ R2' (R2*5)\n@@ -826,1 +845,2 @@\n-                            B0, B1, B2, B3, B4);   \/\/ R^16-R^9, R1P, R2P\n+                            B0, B1, B2, B3, B4,    \/\/ R^16-R^9, R1P, R2P\n+                            T0, T1, T2, T3, T4, T5, TMP, rscratch);\n@@ -828,1 +848,2 @@\n-                            R0, R1, R2, R1P, R2P); \/\/ R^8-R, R1P, R2P\n+                            R0, R1, R2, R1P, R2P,  \/\/ R^8-R, R1P, R2P\n+                            T0, T1, T2, T3, T4, T5, TMP, rscratch);\n@@ -837,6 +858,6 @@\n-  __ vextracti64x4(T0, A0, 1);\n-  __ vextracti64x4(T1, A1, 1);\n-  __ vextracti64x4(T2, A2, 1);\n-  __ vpaddq(A0, A0, T0, Assembler::AVX_256bit);\n-  __ vpaddq(A1, A1, T1, Assembler::AVX_256bit);\n-  __ vpaddq(A2, A2, T2, Assembler::AVX_256bit);\n+  __ vextracti64x4(D0, A0, 1);\n+  __ vextracti64x4(D1, A1, 1);\n+  __ vextracti64x4(D2, A2, 1);\n+  __ vpaddq(A0, A0, D0, Assembler::AVX_256bit);\n+  __ vpaddq(A1, A1, D1, Assembler::AVX_256bit);\n+  __ vpaddq(A2, A2, D2, Assembler::AVX_256bit);\n@@ -845,6 +866,6 @@\n-  __ vextracti32x4(T0, A0, 1);\n-  __ vextracti32x4(T1, A1, 1);\n-  __ vextracti32x4(T2, A2, 1);\n-  __ vpaddq(A0, A0, T0, Assembler::AVX_128bit);\n-  __ vpaddq(A1, A1, T1, Assembler::AVX_128bit);\n-  __ vpaddq(A2, A2, T2, Assembler::AVX_128bit);\n+  __ vextracti32x4(D0, A0, 1);\n+  __ vextracti32x4(D1, A1, 1);\n+  __ vextracti32x4(D2, A2, 1);\n+  __ vpaddq(A0, A0, D0, Assembler::AVX_128bit);\n+  __ vpaddq(A1, A1, D1, Assembler::AVX_128bit);\n+  __ vpaddq(A2, A2, D2, Assembler::AVX_128bit);\n@@ -853,3 +874,3 @@\n-  __ vpsrldq(T0, A0, 8, Assembler::AVX_128bit);\n-  __ vpsrldq(T1, A1, 8, Assembler::AVX_128bit);\n-  __ vpsrldq(T2, A2, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(D0, A0, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(D1, A1, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(D2, A2, 8, Assembler::AVX_128bit);\n@@ -860,3 +881,3 @@\n-  __ evpaddq(A0, k1, A0, T0, false, Assembler::AVX_512bit);\n-  __ evpaddq(A1, k1, A1, T1, false, Assembler::AVX_512bit);\n-  __ evpaddq(A2, k1, A2, T2, false, Assembler::AVX_512bit);\n+  __ evpaddq(A0, k1, A0, D0, false, Assembler::AVX_512bit);\n+  __ evpaddq(A1, k1, A1, D1, false, Assembler::AVX_512bit);\n+  __ evpaddq(A2, k1, A2, D2, false, Assembler::AVX_512bit);\n@@ -865,1 +886,1 @@\n-  __ vpsrlq(T0, A0, 44, Assembler::AVX_512bit);\n+  __ vpsrlq(D0, A0, 44, Assembler::AVX_512bit);\n@@ -867,2 +888,2 @@\n-  __ vpaddq(A1, A1, T0, Assembler::AVX_512bit);\n-  __ vpsrlq(T0, A1, 44, Assembler::AVX_512bit);\n+  __ vpaddq(A1, A1, D0, Assembler::AVX_512bit);\n+  __ vpsrlq(D0, A1, 44, Assembler::AVX_512bit);\n@@ -870,2 +891,2 @@\n-  __ vpaddq(A2, A2, T0, Assembler::AVX_512bit);\n-  __ vpsrlq(T0, A2, 42, Assembler::AVX_512bit);\n+  __ vpaddq(A2, A2, D0, Assembler::AVX_512bit);\n+  __ vpsrlq(D0, A2, 42, Assembler::AVX_512bit);\n@@ -873,3 +894,3 @@\n-  __ vpsllq(T1, T0, 2, Assembler::AVX_512bit);\n-  __ vpaddq(T0, T0, T1, Assembler::AVX_512bit);\n-  __ vpaddq(A0, A0, T0, Assembler::AVX_512bit);\n+  __ vpsllq(D1, D0, 2, Assembler::AVX_512bit);\n+  __ vpaddq(D0, D0, D1, Assembler::AVX_512bit);\n+  __ vpaddq(A0, A0, D0, Assembler::AVX_512bit);\n@@ -897,3 +918,3 @@\n-  __ vpxorq(T0, T0, T0, Assembler::AVX_512bit);\n-  __ vpxorq(T1, T1, T1, Assembler::AVX_512bit);\n-  __ vpxorq(T2, T2, T2, Assembler::AVX_512bit);\n+  __ vpxorq(D0, D0, D0, Assembler::AVX_512bit);\n+  __ vpxorq(D1, D1, D1, Assembler::AVX_512bit);\n+  __ vpxorq(D2, D2, D2, Assembler::AVX_512bit);\n@@ -949,0 +970,1 @@\n+  \/\/ Register Map\n@@ -954,1 +976,14 @@\n-  \/\/ void processBlocks(byte[] input, int len, int[5] a, int[5] r)\n+  const Register a0 = rsi;  \/\/ [in\/out] accumulator bits 63..0\n+  const Register a1 = r9;   \/\/ [in\/out] accumulator bits 127..64\n+  const Register a2 = r10;  \/\/ [in\/out] accumulator bits 195..128\n+  const Register r0 = r11;  \/\/ R constant bits 63..0\n+  const Register r1 = r12;  \/\/ R constant bits 127..64\n+  const Register c1 = r8;   \/\/ 5*R (upper limb only)\n+  const Register t0 = r13;\n+  const Register t1 = r14;\n+  const Register t2 = r15;\n+  const Register mulql = rax;\n+  const Register mulqh = rdx;\n+\n+  \/\/ Normalize input\n+  \/\/ pseudo-signature: void poly1305_processBlocks(byte[] input, int length, int[5] accumulator, int[5] R)\n@@ -977,9 +1012,0 @@\n-  const Register a0 = rsi;  \/\/ [in\/out] accumulator bits 63..0\n-  const Register a1 = r9;   \/\/ [in\/out] accumulator bits 127..64\n-  const Register a2 = r10;  \/\/ [in\/out] accumulator bits 195..128\n-  const Register r0 = r11;  \/\/ R constant bits 63..0\n-  const Register r1 = r12;  \/\/ R constant bits 127..64\n-  const Register c1 = r8;   \/\/ 5*R (upper limb only)\n-  const Register t1 = r13;\n-  const Register t2 = r14;\n-\n@@ -989,1 +1015,1 @@\n-  poly1305_limbs(R, r0, r1, noreg, t1, t2);\n+  poly1305_limbs(R, r0, r1, noreg, t0, t1);\n@@ -997,1 +1023,1 @@\n-  poly1305_limbs(accumulator, a0, a1, a2, t1, t2);\n+  poly1305_limbs(accumulator, a0, a1, a2, t0, t1);\n@@ -1015,1 +1041,3 @@\n-  poly1305_multiply_scalar(a0, a1, a2, r0, r1, c1, false);\n+  poly1305_multiply_scalar(a0, a1, a2,\n+                           r0, r1, c1, false,\n+                           t0, t1, t2, mulql, mulqh);\n@@ -1023,1 +1051,1 @@\n-  poly1305_limbs_out(a0, a1, a2, accumulator, t1, t2);\n+  poly1305_limbs_out(a0, a1, a2, accumulator, t0, t1);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_poly.cpp","additions":326,"deletions":298,"binary":false,"changes":624,"status":"modified"}]}