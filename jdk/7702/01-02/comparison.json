{"files":[{"patch":"@@ -395,1 +395,2 @@\n-    XMMRegister cVec, XMMRegister dVec, XMMRegister scratch, int vector_len);\n+    XMMRegister cVec, XMMRegister dVec, XMMRegister scratch,\n+    XMMRegister lrot8, XMMRegister lrot16, int vector_len);\n@@ -401,1 +402,0 @@\n-\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -84,0 +84,18 @@\n+\/**\n+ * The first 256 bits represents a byte-wise permutation\n+ * for an 8-bit left-rotation on 32-bit lanes.\n+ * The second 256 bits is a 16-bit rotation on 32-bit lanes.\n+ *\/\n+ATTRIBUTE_ALIGNED(64) uint64_t CC20_LROT_CONSTS[] = {\n+    0x0605040702010003UL, 0x0E0D0C0F0A09080BUL,\n+    0x0605040702010003UL, 0x0E0D0C0F0A09080BUL,\n+\n+    0x0504070601000302UL, 0x0D0C0F0E09080B0AUL,\n+    0x0504070601000302UL, 0x0D0C0F0E09080B0AUL\n+};\n+static address chacha20_lrot_consts() {\n+  return (address)CC20_LROT_CONSTS;\n+}\n+\n+\n+\n@@ -105,0 +123,1 @@\n+  const Register rotAddr      = r9;\n@@ -120,0 +139,2 @@\n+  const XMMRegister lrot8 = xmm14;\n+  const XMMRegister lrot16 = xmm15;\n@@ -140,0 +161,1 @@\n+  __ lea(rotAddr, ExternalAddress(chacha20_lrot_consts()));\n@@ -157,0 +179,2 @@\n+    __ movdqu(lrot8, Address(rotAddr, 0));      \/\/ Load 8-bit lrot const\n+    __ movdqu(lrot16, Address(rotAddr, 32));    \/\/ Load 16-bit lrot const\n@@ -178,0 +202,2 @@\n+    __ vmovdqu(lrot8, Address(rotAddr, 0));      \/\/ Load 8-bit lrot const\n+    __ vmovdqu(lrot16, Address(rotAddr, 32));    \/\/ Load 16-bit lrot const\n@@ -188,2 +214,4 @@\n-  cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch, vector_len);\n-  cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch, vector_len);\n+  cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch,\n+      lrot8, lrot16, vector_len);\n+  cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch,\n+      lrot8, lrot16, vector_len);\n@@ -202,2 +230,4 @@\n-  cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch, vector_len);\n-  cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch, vector_len);\n+  cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch,\n+      lrot8, lrot16, vector_len);\n+  cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch,\n+      lrot8, lrot16, vector_len);\n@@ -353,4 +383,8 @@\n-  cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch, Assembler::AVX_512bit);\n-  cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch, Assembler::AVX_512bit);\n-  cc20_quarter_round_avx(a3Vec, b3Vec, c3Vec, d3Vec, scratch, Assembler::AVX_512bit);\n-  cc20_quarter_round_avx(a4Vec, b4Vec, c4Vec, d4Vec, scratch, Assembler::AVX_512bit);\n+  cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch,\n+      xnoreg, xnoreg, Assembler::AVX_512bit);\n+  cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch,\n+      xnoreg, xnoreg, Assembler::AVX_512bit);\n+  cc20_quarter_round_avx(a3Vec, b3Vec, c3Vec, d3Vec, scratch,\n+      xnoreg, xnoreg, Assembler::AVX_512bit);\n+  cc20_quarter_round_avx(a4Vec, b4Vec, c4Vec, d4Vec, scratch,\n+      xnoreg, xnoreg, Assembler::AVX_512bit);\n@@ -371,4 +405,8 @@\n-  cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch, Assembler::AVX_512bit);\n-  cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch, Assembler::AVX_512bit);\n-  cc20_quarter_round_avx(a3Vec, b3Vec, c3Vec, d3Vec, scratch, Assembler::AVX_512bit);\n-  cc20_quarter_round_avx(a4Vec, b4Vec, c4Vec, d4Vec, scratch, Assembler::AVX_512bit);\n+  cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch,\n+      xnoreg, xnoreg, Assembler::AVX_512bit);\n+  cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch,\n+      xnoreg, xnoreg, Assembler::AVX_512bit);\n+  cc20_quarter_round_avx(a3Vec, b3Vec, c3Vec, d3Vec, scratch,\n+      xnoreg, xnoreg, Assembler::AVX_512bit);\n+  cc20_quarter_round_avx(a4Vec, b4Vec, c4Vec, d4Vec, scratch,\n+      xnoreg, xnoreg, Assembler::AVX_512bit);\n@@ -430,2 +468,1 @@\n- * Provide a macro for AVX and AVX2 implementations of the ChaCha20 quarter\n- * round function.\n+ * Provide a function that implements the ChaCha20 quarter round function.\n@@ -437,2 +474,4 @@\n- * @param scratch SIMD register used for left rotations other than 16-bit.\n- * @param vector_len the length of the vector (128 and 256 bit only)\n+ * @param scratch SIMD register used for non-byte-aligned left rotations\n+ * @param lrot8 shuffle control mask for an 8-byte left rotation (32-bit lane)\n+ * @param lrot16 shuffle control mask for a 16-byte left rotation (32-bit lane)\n+ * @param vector_len the length of the vector\n@@ -441,1 +480,2 @@\n-    XMMRegister cVec, XMMRegister dVec, XMMRegister scratch, int vector_len) {\n+    XMMRegister cVec, XMMRegister dVec, XMMRegister scratch,\n+    XMMRegister lrot8, XMMRegister lrot16, int vector_len) {\n@@ -449,2 +489,1 @@\n-    __ vpshufhw(dVec, dVec, 0xB1, vector_len);\n-    __ vpshuflw(dVec, dVec, 0xB1, vector_len);\n+    __ vpshufb(dVec, dVec, lrot16, vector_len);\n@@ -470,3 +509,1 @@\n-    __ vpsrld(scratch, dVec, 24, vector_len);\n-    __ vpslld(dVec, dVec, 8, vector_len);\n-    __ vpor(dVec, dVec, scratch, vector_len);\n+    __ vpshufb(dVec, dVec, lrot8, vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_chacha.cpp","additions":59,"deletions":22,"binary":false,"changes":81,"status":"modified"}]}