{"files":[{"patch":"@@ -0,0 +1,90 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"macroAssembler_aarch64.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+\n+\/**\n+ * Perform the quarter round calculations on values contained within\n+ * four SIMD registers.\n+ *\n+ * @param aVec the SIMD register containing only the \"a\" values\n+ * @param bVec the SIMD register containing only the \"b\" values\n+ * @param cVec the SIMD register containing only the \"c\" values\n+ * @param dVec the SIMD register containing only the \"d\" values\n+ * @param scratch scratch SIMD register used for 12 and 7 bit left rotations\n+ * @param table the SIMD register used as a table for 8 bit left rotations\n+ *\/\n+void MacroAssembler::cc20_quarter_round(FloatRegister aVec, FloatRegister bVec,\n+    FloatRegister cVec, FloatRegister dVec, FloatRegister scratch,\n+     FloatRegister table) {\n+\n+  \/\/ a += b, d ^= a, d <<<= 16\n+  addv(aVec, T4S, aVec, bVec);\n+  eor(dVec, T16B, dVec, aVec);\n+  rev32(dVec, T8H, dVec);\n+\n+  \/\/ c += d, b ^= c, b <<<= 12\n+  addv(cVec, T4S, cVec, dVec);\n+  eor(scratch, T16B, bVec, cVec);\n+  ushr(bVec, T4S, scratch, 20);\n+  sli(bVec, T4S, scratch, 12);\n+\n+  \/\/ a += b, d ^= a, d <<<= 8\n+  addv(aVec, T4S, aVec, bVec);\n+  eor(dVec, T16B, dVec, aVec);\n+  tbl(dVec, T16B, dVec,  1, table);\n+\n+  \/\/ c += d, b ^= c, b <<<= 7\n+  addv(cVec, T4S, cVec, dVec);\n+  eor(scratch, T16B, bVec, cVec);\n+  ushr(bVec, T4S, scratch, 25);\n+  sli(bVec, T4S, scratch, 7);\n+}\n+\n+\/**\n+ * Shift the b, c, and d vectors between columnar and diagonal representations.\n+ * Note that the \"a\" vector does not shift.\n+ *\n+ * @param bVec the SIMD register containing only the \"b\" values\n+ * @param cVec the SIMD register containing only the \"c\" values\n+ * @param dVec the SIMD register containing only the \"d\" values\n+ * @param colToDiag true if moving columnar to diagonal, false if\n+ *                  moving diagonal back to columnar.\n+ *\/\n+void MacroAssembler::cc20_shift_lane_org(FloatRegister bVec, FloatRegister cVec,\n+    FloatRegister dVec, bool colToDiag) {\n+  int bShift = colToDiag ? 4 : 12;\n+  int cShift = 8;\n+  int dShift = colToDiag ? 12 : 4;\n+\n+  ext(bVec, T16B, bVec, bVec, bShift);\n+  ext(cVec, T16B, cVec, cVec, cShift);\n+  ext(dVec, T16B, dVec, dVec, dShift);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64_chacha.cpp","additions":90,"deletions":0,"binary":false,"changes":90,"status":"added"},{"patch":"@@ -1,90 +0,0 @@\n-\/*\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-\n-#include \"asm\/assembler.hpp\"\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"macroAssembler_aarch64.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-\n-\/**\n- * Perform the quarter round calculations on values contained within\n- * four SIMD registers.\n- *\n- * @param aVec the SIMD register containing only the \"a\" values\n- * @param bVec the SIMD register containing only the \"b\" values\n- * @param cVec the SIMD register containing only the \"c\" values\n- * @param dVec the SIMD register containing only the \"d\" values\n- * @param scratch scratch SIMD register used for 12 and 7 bit left rotations\n- * @param table the SIMD register used as a table for 8 bit left rotations\n- *\/\n-void MacroAssembler::cc20_quarter_round(FloatRegister aVec, FloatRegister bVec,\n-        FloatRegister cVec, FloatRegister dVec, FloatRegister scratch,\n-        FloatRegister table) {\n-\n-    \/\/ a += b, d ^= a, d <<<= 16\n-    addv(aVec, T4S, aVec, bVec);\n-    eor(dVec, T16B, dVec, aVec);\n-    rev32(dVec, T8H, dVec);\n-\n-    \/\/ c += d, b ^= c, b <<<= 12\n-    addv(cVec, T4S, cVec, dVec);\n-    eor(scratch, T16B, bVec, cVec);\n-    ushr(bVec, T4S, scratch, 20);\n-    sli(bVec, T4S, scratch, 12);\n-\n-    \/\/ a += b, d ^= a, d <<<= 8\n-    addv(aVec, T4S, aVec, bVec);\n-    eor(dVec, T16B, dVec, aVec);\n-    tbl(dVec, T16B, dVec,  1, table);\n-\n-    \/\/ c += d, b ^= c, b <<<= 7\n-    addv(cVec, T4S, cVec, dVec);\n-    eor(scratch, T16B, bVec, cVec);\n-    ushr(bVec, T4S, scratch, 25);\n-    sli(bVec, T4S, scratch, 7);\n-}\n-\n-\/**\n- * Shift the b, c, and d vectors between columnar and diagonal representations.\n- * Note that the \"a\" vector does not shift.\n- *\n- * @param bVec the SIMD register containing only the \"b\" values\n- * @param cVec the SIMD register containing only the \"c\" values\n- * @param dVec the SIMD register containing only the \"d\" values\n- * @param colToDiag true if moving columnar to diagonal, false if\n- *                  moving diagonal back to columnar.\n- *\/\n-void MacroAssembler::cc20_shift_lane_org(FloatRegister bVec, FloatRegister cVec,\n-        FloatRegister dVec, bool colToDiag) {\n-    int bShift = colToDiag ? 4 : 12;\n-    int cShift = 8;\n-    int dShift = colToDiag ? 12 : 4;\n-\n-    ext(bVec, T16B, bVec, bVec, bShift);\n-    ext(cVec, T16B, cVec, cVec, cShift);\n-    ext(dVec, T16B, dVec, dVec, dShift);\n-}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64_chapoly.cpp","additions":0,"deletions":90,"binary":false,"changes":90,"status":"deleted"},{"patch":"@@ -4102,1 +4102,1 @@\n-     StubCodeMark mark(this, \"StubRoutines\", \"chacha20Block\");\n+    StubCodeMark mark(this, \"StubRoutines\", \"chacha20Block\");\n@@ -4137,2 +4137,2 @@\n-        __ ld4r(workSt[i], workSt[i + 1], workSt[i + 2], workSt[i + 3],\n-                __ T4S, __ post(tmpAddr, 16));\n+      __ ld4r(workSt[i], workSt[i + 1], workSt[i + 2], workSt[i + 3], __ T4S,\n+          __ post(tmpAddr, 16));\n@@ -4154,1 +4154,1 @@\n-            scratch, lrot8Tbl);\n+        scratch, lrot8Tbl);\n@@ -4156,1 +4156,1 @@\n-            scratch, lrot8Tbl);\n+        scratch, lrot8Tbl);\n@@ -4158,1 +4158,1 @@\n-            scratch, lrot8Tbl);\n+        scratch, lrot8Tbl);\n@@ -4160,1 +4160,1 @@\n-            scratch, lrot8Tbl);\n+        scratch, lrot8Tbl);\n@@ -4163,1 +4163,1 @@\n-            scratch, lrot8Tbl);\n+        scratch, lrot8Tbl);\n@@ -4165,1 +4165,1 @@\n-            scratch, lrot8Tbl);\n+        scratch, lrot8Tbl);\n@@ -4167,1 +4167,1 @@\n-            scratch, lrot8Tbl);\n+        scratch, lrot8Tbl);\n@@ -4169,1 +4169,1 @@\n-            scratch, lrot8Tbl);\n+        scratch, lrot8Tbl);\n@@ -4182,6 +4182,6 @@\n-        __ ld4r(stateFirst, stateSecond, stateThird, stateFourth,\n-                __ T4S, __ post(tmpAddr, 16));\n-        __ addv(workSt[i], __ T4S, workSt[i], stateFirst);\n-        __ addv(workSt[i + 1], __ T4S, workSt[i + 1], stateSecond);\n-        __ addv(workSt[i + 2], __ T4S, workSt[i + 2], stateThird);\n-        __ addv(workSt[i + 3], __ T4S, workSt[i + 3], stateFourth);\n+      __ ld4r(stateFirst, stateSecond, stateThird, stateFourth, __ T4S,\n+          __ post(tmpAddr, 16));\n+      __ addv(workSt[i], __ T4S, workSt[i], stateFirst);\n+      __ addv(workSt[i + 1], __ T4S, workSt[i + 1], stateSecond);\n+      __ addv(workSt[i + 2], __ T4S, workSt[i + 2], stateThird);\n+      __ addv(workSt[i + 3], __ T4S, workSt[i + 3], stateFourth);\n@@ -4195,4 +4195,4 @@\n-        for (j = 0; j < 16; j += 4) {\n-            __ st4(workSt[j], workSt[j + 1], workSt[j + 2], workSt[j + 3],\n-                    __ S, i, __ post(keystream, 16));\n-        }\n+      for (j = 0; j < 16; j += 4) {\n+        __ st4(workSt[j], workSt[j + 1], workSt[j + 2], workSt[j + 3], __ S, i,\n+            __ post(keystream, 16));\n+      }\n@@ -8047,1 +8047,1 @@\n-        StubRoutines::_chacha20Block = generate_chacha20Block_blockpar();\n+      StubRoutines::_chacha20Block = generate_chacha20Block_blockpar();\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":22,"deletions":22,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -989,10 +989,0 @@\n-  \/\/ ChaCha20-Poly1305 macroAssembler defs\n-  void cc20_quarter_round_avx(XMMRegister aVec, XMMRegister bVec,\n-          XMMRegister cVec, XMMRegister dVec, XMMRegister scratch,\n-          int vector_len);\n-  void cc20_shift_lane_org(XMMRegister bVec, XMMRegister cVec,\n-          XMMRegister dVec, int vector_len, bool colToDiag);\n-  void cc20_keystream_collate_avx512(XMMRegister aVec, XMMRegister bVec,\n-          XMMRegister cVec, XMMRegister dVec,\n-          Register baseAddr, int baseOffset);\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1,145 +0,0 @@\n-\/*\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/assembler.hpp\"\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"macroAssembler_x86.hpp\"\n-\n-\/**\n- * Provide a macro for AVX and AVX2 implementations of the ChaCha20 quarter\n- * round function.\n- *\n- * @param aVec the SIMD register containing only the \"a\" values\n- * @param bVec the SIMD register containing only the \"b\" values\n- * @param cVec the SIMD register containing only the \"c\" values\n- * @param dVec the SIMD register containing only the \"d\" values\n- * @param scratch SIMD register used for left rotations other than 16-bit.\n- * @param vector_len the length of the vector (128 and 256 bit only)\n- *\/\n-void MacroAssembler::cc20_quarter_round_avx(XMMRegister aVec, XMMRegister bVec,\n-        XMMRegister cVec, XMMRegister dVec, XMMRegister scratch,\n-        int vector_len) {\n-\n-    \/\/ a += b; d ^= a; d <<<= 16\n-    vpaddd(aVec, aVec, bVec, vector_len);\n-    vpxor(dVec, dVec, aVec, vector_len);\n-    if (vector_len == Assembler::AVX_512bit) {\n-        evprold(dVec, dVec, 16, vector_len);\n-    } else {\n-        vpshufhw(dVec, dVec, 0xB1, vector_len);\n-        vpshuflw(dVec, dVec, 0xB1, vector_len);\n-    }\n-\n-    \/\/ c += d; b ^= c; b <<<= 12 (b << 12 | scratch >>> 20)\n-    vpaddd(cVec, cVec, dVec, vector_len);\n-    vpxor(bVec, bVec, cVec, vector_len);\n-    if (vector_len == Assembler::AVX_512bit) {\n-        evprold(bVec, bVec, 12, vector_len);\n-    } else {\n-        vpsrld(scratch, bVec, 20, vector_len);\n-        vpslld(bVec, bVec, 12, vector_len);\n-        vpor(bVec, bVec, scratch, vector_len);\n-    }\n-\n-    \/\/ a += b; d ^= a; d <<<= 8 (d << 8 | scratch >>> 24)\n-    vpaddd(aVec, aVec, bVec, vector_len);\n-    vpxor(dVec, dVec, aVec, vector_len);\n-    if (vector_len == Assembler::AVX_512bit) {\n-        evprold(dVec, dVec, 8, vector_len);\n-    } else {\n-        vpsrld(scratch, dVec, 24, vector_len);\n-        vpslld(dVec, dVec, 8, vector_len);\n-        vpor(dVec, dVec, scratch, vector_len);\n-    }\n-\n-    \/\/ c += d; b ^= c; b <<<= 7 (b << 7 | scratch >>> 25)\n-    vpaddd(cVec, cVec, dVec, vector_len);\n-    vpxor(bVec, bVec, cVec, vector_len);\n-    if (vector_len == Assembler::AVX_512bit) {\n-        evprold(bVec, bVec, 7, vector_len);\n-    } else {\n-        vpsrld(scratch, bVec, 25, vector_len);\n-        vpslld(bVec, bVec, 7, vector_len);\n-        vpor(bVec, bVec, scratch, vector_len);\n-    }\n-}\n-\n-\/**\n- * Shift the b, c, and d vectors between columnar and diagonal representations.\n- * Note that the \"a\" vector does not shift.\n- *\n- * @param bVec the SIMD register containing only the \"b\" values\n- * @param cVec the SIMD register containing only the \"c\" values\n- * @param dVec the SIMD register containing only the \"d\" values\n- * @param vector_len the size of the SIMD register to operate upon\n- * @param colToDiag true if moving columnar to diagonal, false if\n- *                  moving diagonal back to columnar.\n- *\/\n-void MacroAssembler::cc20_shift_lane_org(XMMRegister bVec, XMMRegister cVec,\n-        XMMRegister dVec, int vector_len, bool colToDiag) {\n-    int bShift = colToDiag ? 0x39 : 0x93;\n-    int cShift = 0x4E;\n-    int dShift = colToDiag ? 0x93 : 0x39;\n-\n-    vpshufd(bVec, bVec, bShift, vector_len);\n-    vpshufd(cVec, cVec, cShift, vector_len);\n-    vpshufd(dVec, dVec, dShift, vector_len);\n-}\n-\n-\/**\n- * Write 256 bytes of keystream output held in 4 AVX512 SIMD registers\n- * in a quarter round parallel organization.\n- *\n- * @param aVec the SIMD register containing only the \"a\" values\n- * @param bVec the SIMD register containing only the \"b\" values\n- * @param cVec the SIMD register containing only the \"c\" values\n- * @param dVec the SIMD register containing only the \"d\" values\n- * @param baseAddr the register holding the base output address\n- * @param baseOffset the offset from baseAddr for writes\n- *\/\n-void MacroAssembler::cc20_keystream_collate_avx512(XMMRegister aVec,\n-        XMMRegister bVec, XMMRegister cVec, XMMRegister dVec,\n-        Register baseAddr, int baseOffset) {\n-    vextracti32x4(Address(baseAddr, baseOffset + 0), aVec, 0);\n-    vextracti32x4(Address(baseAddr, baseOffset + 64), aVec, 1);\n-    vextracti32x4(Address(baseAddr, baseOffset + 128), aVec, 2);\n-    vextracti32x4(Address(baseAddr, baseOffset + 192), aVec, 3);\n-\n-    vextracti32x4(Address(baseAddr, baseOffset + 16), bVec, 0);\n-    vextracti32x4(Address(baseAddr, baseOffset + 80), bVec, 1);\n-    vextracti32x4(Address(baseAddr, baseOffset + 144), bVec, 2);\n-    vextracti32x4(Address(baseAddr, baseOffset + 208), bVec, 3);\n-\n-    vextracti32x4(Address(baseAddr, baseOffset + 32), cVec, 0);\n-    vextracti32x4(Address(baseAddr, baseOffset + 96), cVec, 1);\n-    vextracti32x4(Address(baseAddr, baseOffset + 160), cVec, 2);\n-    vextracti32x4(Address(baseAddr, baseOffset + 224), cVec, 3);\n-\n-    vextracti32x4(Address(baseAddr, baseOffset + 48), dVec, 0);\n-    vextracti32x4(Address(baseAddr, baseOffset + 112), dVec, 1);\n-    vextracti32x4(Address(baseAddr, baseOffset + 176), dVec, 2);\n-    vextracti32x4(Address(baseAddr, baseOffset + 240), dVec, 3);\n-}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_chacha.cpp","additions":0,"deletions":145,"binary":false,"changes":145,"status":"deleted"},{"patch":"@@ -390,1 +390,1 @@\n-  \/\/ ChaCha20 stubs\n+  \/\/ ChaCha20 stubs and helper functions\n@@ -394,0 +394,7 @@\n+  void cc20_quarter_round_avx(XMMRegister aVec, XMMRegister bVec,\n+    XMMRegister cVec, XMMRegister dVec, XMMRegister scratch, int vector_len);\n+  void cc20_shift_lane_org(XMMRegister bVec, XMMRegister cVec,\n+    XMMRegister dVec, int vector_len, bool colToDiag);\n+  void cc20_keystream_collate_avx512(XMMRegister aVec, XMMRegister bVec,\n+    XMMRegister cVec, XMMRegister dVec, Register baseAddr, int baseOffset);\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-    return (address)CC20_COUNTER_ADD_AVX;\n+  return (address)CC20_COUNTER_ADD_AVX;\n@@ -81,1 +81,1 @@\n-    return (address)CC20_COUNTER_ADD_AVX512;\n+  return (address)CC20_COUNTER_ADD_AVX512;\n@@ -85,7 +85,6 @@\n-    \/\/ Generate ChaCha20 intrinsics code\n-    if (UseChaCha20Intrinsics) {\n-        if (VM_Version::supports_evex()) {\n-            StubRoutines::_chacha20Block = generate_chacha20Block_avx512();\n-        } else {    \/\/ Either AVX or AVX2 is supported\n-            StubRoutines::_chacha20Block = generate_chacha20Block_avx();\n-        }\n+  \/\/ Generate ChaCha20 intrinsics code\n+  if (UseChaCha20Intrinsics) {\n+    if (VM_Version::supports_evex()) {\n+      StubRoutines::_chacha20Block = generate_chacha20Block_avx512();\n+    } else {    \/\/ Either AVX or AVX2 is supported\n+      StubRoutines::_chacha20Block = generate_chacha20Block_avx();\n@@ -93,0 +92,1 @@\n+  }\n@@ -97,169 +97,169 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"chacha20Block\");\n-    address start = __ pc();\n-\n-    Label L_twoRounds;\n-    const Register state        = c_rarg0;\n-    const Register result       = c_rarg1;\n-    const Register loopCounter  = r8;\n-\n-    const XMMRegister aState = xmm0;\n-    const XMMRegister bState = xmm1;\n-    const XMMRegister cState = xmm2;\n-    const XMMRegister dState = xmm3;\n-    const XMMRegister a1Vec = xmm4;\n-    const XMMRegister b1Vec = xmm5;\n-    const XMMRegister c1Vec = xmm6;\n-    const XMMRegister d1Vec = xmm7;\n-    const XMMRegister a2Vec = xmm8;\n-    const XMMRegister b2Vec = xmm9;\n-    const XMMRegister c2Vec = xmm10;\n-    const XMMRegister d2Vec = xmm11;\n-    const XMMRegister scratch = xmm12;\n-    const XMMRegister d2State = xmm13;\n-\n-    int vector_len;\n-    int outlen;\n-\n-    \/\/ This function will only be called if AVX2 or AVX are supported\n-    \/\/ AVX512 uses a different function.\n-    if (VM_Version::supports_avx2()) {\n-        vector_len = Assembler::AVX_256bit;\n-        outlen = 256;\n-    } else if (VM_Version::supports_avx()) {\n-        vector_len = Assembler::AVX_128bit;\n-        outlen = 128;\n-    }\n-\n-    __ enter();\n-\n-    \/\/ Load the initial state in columnar orientation and then copy\n-    \/\/ that starting state to the working register set.\n-    \/\/ Also load the address of the add mask for later use in handling\n-    \/\/ multi-block counter increments.\n-    __ lea(rax, ExternalAddress(chacha20_ctradd_avx()));\n-    if (vector_len == Assembler::AVX_128bit) {\n-        __ movdqu(aState, Address(state, 0));       \/\/ Bytes 0 - 15 -> a1Vec\n-        __ movdqu(bState, Address(state, 16));      \/\/ Bytes 16 - 31 -> b1Vec\n-        __ movdqu(cState, Address(state, 32));      \/\/ Bytes 32 - 47 -> c1Vec\n-        __ movdqu(dState, Address(state, 48));      \/\/ Bytes 48 - 63 -> d1Vec\n-\n-        __ movdqu(a1Vec, aState);\n-        __ movdqu(b1Vec, bState);\n-        __ movdqu(c1Vec, cState);\n-        __ movdqu(d1Vec, dState);\n-\n-        __ movdqu(a2Vec, aState);\n-        __ movdqu(b2Vec, bState);\n-        __ movdqu(c2Vec, cState);\n-        __ vpaddd(d2State, dState, Address(rax, 16), vector_len);\n-        __ movdqu(d2Vec, d2State);\n-    } else {\n-        \/\/ We will broadcast each 128-bit segment of the state array into\n-        \/\/ the high and low halves of ymm state registers.  Then apply the add\n-        \/\/ mask to the dState register.  These will then be copied into the\n-        \/\/ a\/b\/c\/d1Vec working registers.\n-        __ vbroadcastf128(aState, Address(state, 0), vector_len);\n-        __ vbroadcastf128(bState, Address(state, 16), vector_len);\n-        __ vbroadcastf128(cState, Address(state, 32), vector_len);\n-        __ vbroadcastf128(dState, Address(state, 48), vector_len);\n-        __ vpaddd(dState, dState, Address(rax, 0), vector_len);\n-        __ vpaddd(d2State, dState, Address(rax, 32), vector_len);\n-\n-        __ vmovdqu(a1Vec, aState);\n-        __ vmovdqu(b1Vec, bState);\n-        __ vmovdqu(c1Vec, cState);\n-        __ vmovdqu(d1Vec, dState);\n-\n-        __ vmovdqu(a2Vec, aState);\n-        __ vmovdqu(b2Vec, bState);\n-        __ vmovdqu(c2Vec, cState);\n-        __ vmovdqu(d2Vec, d2State);\n-    }\n-\n-    __ movl(loopCounter, 10);                   \/\/ Set 10 2-round iterations\n-    __ BIND(L_twoRounds);\n-\n-    \/\/ The first quarter round macro call covers the first 4 QR operations:\n-    \/\/  Qround(state, 0, 4, 8,12)\n-    \/\/  Qround(state, 1, 5, 9,13)\n-    \/\/  Qround(state, 2, 6,10,14)\n-    \/\/  Qround(state, 3, 7,11,15)\n-    __ cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch, vector_len);\n-    __ cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch, vector_len);\n-\n-    \/\/ Shuffle the b1Vec\/c1Vec\/d1Vec to reorganize the state vectors\n-    \/\/ to diagonals.  The a1Vec does not need to change orientation.\n-    __ cc20_shift_lane_org(b1Vec, c1Vec, d1Vec, vector_len, true);\n-    __ cc20_shift_lane_org(b2Vec, c2Vec, d2Vec, vector_len, true);\n-\n-    \/\/ The second set of operations on the vectors covers the second 4 quarter\n-    \/\/ round operations, now acting on the diagonals:\n-    \/\/  Qround(state, 0, 5,10,15)\n-    \/\/  Qround(state, 1, 6,11,12)\n-    \/\/  Qround(state, 2, 7, 8,13)\n-    \/\/  Qround(state, 3, 4, 9,14)\n-    __ cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch, vector_len);\n-    __ cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch, vector_len);\n-\n-    \/\/ Before we start the next iteration, we need to perform shuffles\n-    \/\/ on the b\/c\/d vectors to move them back to columnar organizations\n-    \/\/ from their current diagonal orientation.\n-    __ cc20_shift_lane_org(b1Vec, c1Vec, d1Vec, vector_len, false);\n-    __ cc20_shift_lane_org(b2Vec, c2Vec, d2Vec, vector_len, false);\n-\n-    __ decrement(loopCounter);\n-    __ jcc(Assembler::notZero, L_twoRounds);\n-\n-    \/\/ Add the original start state back into the current state.\n-    __ vpaddd(a1Vec, a1Vec, aState, vector_len);\n-    __ vpaddd(b1Vec, b1Vec, bState, vector_len);\n-    __ vpaddd(c1Vec, c1Vec, cState, vector_len);\n-    __ vpaddd(d1Vec, d1Vec, dState, vector_len);\n-\n-    __ vpaddd(a2Vec, a2Vec, aState, vector_len);\n-    __ vpaddd(b2Vec, b2Vec, bState, vector_len);\n-    __ vpaddd(c2Vec, c2Vec, cState, vector_len);\n-    __ vpaddd(d2Vec, d2Vec, d2State, vector_len);\n-\n-    \/\/ Write the data to the keystream array\n-    if (vector_len == Assembler::AVX_128bit) {\n-        __ movdqu(Address(result, 0), a1Vec);\n-        __ movdqu(Address(result, 16), b1Vec);\n-        __ movdqu(Address(result, 32), c1Vec);\n-        __ movdqu(Address(result, 48), d1Vec);\n-        __ movdqu(Address(result, 64), a2Vec);\n-        __ movdqu(Address(result, 80), b2Vec);\n-        __ movdqu(Address(result, 96), c2Vec);\n-        __ movdqu(Address(result, 112), d2Vec);\n-    } else {\n-        \/\/ Each half of the YMM has to be written 64 bytes apart from\n-        \/\/ each other in memory so the final keystream buffer holds\n-        \/\/ two consecutive keystream blocks.\n-        __ vextracti128(Address(result, 0), a1Vec, 0);\n-        __ vextracti128(Address(result, 64), a1Vec, 1);\n-        __ vextracti128(Address(result, 16), b1Vec, 0);\n-        __ vextracti128(Address(result, 80), b1Vec, 1);\n-        __ vextracti128(Address(result, 32), c1Vec, 0);\n-        __ vextracti128(Address(result, 96), c1Vec, 1);\n-        __ vextracti128(Address(result, 48), d1Vec, 0);\n-        __ vextracti128(Address(result, 112), d1Vec, 1);\n-\n-        __ vextracti128(Address(result, 128), a2Vec, 0);\n-        __ vextracti128(Address(result, 192), a2Vec, 1);\n-        __ vextracti128(Address(result, 144), b2Vec, 0);\n-        __ vextracti128(Address(result, 208), b2Vec, 1);\n-        __ vextracti128(Address(result, 160), c2Vec, 0);\n-        __ vextracti128(Address(result, 224), c2Vec, 1);\n-        __ vextracti128(Address(result, 176), d2Vec, 0);\n-        __ vextracti128(Address(result, 240), d2Vec, 1);\n-    }\n-\n-    \/\/ This function will always write 128 or 256 bytes into the\n-    \/\/ key stream buffer, depending on the length of the SIMD\n-    \/\/ registers.  That length should be returned through %rax.\n-    __ mov64(rax, outlen);\n-\n-    __ leave();\n-    __ ret(0);\n-    return start;\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"chacha20Block\");\n+  address start = __ pc();\n+\n+  Label L_twoRounds;\n+  const Register state        = c_rarg0;\n+  const Register result       = c_rarg1;\n+  const Register loopCounter  = r8;\n+\n+  const XMMRegister aState = xmm0;\n+  const XMMRegister bState = xmm1;\n+  const XMMRegister cState = xmm2;\n+  const XMMRegister dState = xmm3;\n+  const XMMRegister a1Vec = xmm4;\n+  const XMMRegister b1Vec = xmm5;\n+  const XMMRegister c1Vec = xmm6;\n+  const XMMRegister d1Vec = xmm7;\n+  const XMMRegister a2Vec = xmm8;\n+  const XMMRegister b2Vec = xmm9;\n+  const XMMRegister c2Vec = xmm10;\n+  const XMMRegister d2Vec = xmm11;\n+  const XMMRegister scratch = xmm12;\n+  const XMMRegister d2State = xmm13;\n+\n+  int vector_len;\n+  int outlen;\n+\n+  \/\/ This function will only be called if AVX2 or AVX are supported\n+  \/\/ AVX512 uses a different function.\n+  if (VM_Version::supports_avx2()) {\n+    vector_len = Assembler::AVX_256bit;\n+    outlen = 256;\n+  } else if (VM_Version::supports_avx()) {\n+    vector_len = Assembler::AVX_128bit;\n+    outlen = 128;\n+  }\n+\n+  __ enter();\n+\n+  \/\/ Load the initial state in columnar orientation and then copy\n+  \/\/ that starting state to the working register set.\n+  \/\/ Also load the address of the add mask for later use in handling\n+  \/\/ multi-block counter increments.\n+  __ lea(rax, ExternalAddress(chacha20_ctradd_avx()));\n+  if (vector_len == Assembler::AVX_128bit) {\n+    __ movdqu(aState, Address(state, 0));       \/\/ Bytes 0 - 15 -> a1Vec\n+    __ movdqu(bState, Address(state, 16));      \/\/ Bytes 16 - 31 -> b1Vec\n+    __ movdqu(cState, Address(state, 32));      \/\/ Bytes 32 - 47 -> c1Vec\n+    __ movdqu(dState, Address(state, 48));      \/\/ Bytes 48 - 63 -> d1Vec\n+\n+    __ movdqu(a1Vec, aState);\n+    __ movdqu(b1Vec, bState);\n+    __ movdqu(c1Vec, cState);\n+    __ movdqu(d1Vec, dState);\n+\n+    __ movdqu(a2Vec, aState);\n+    __ movdqu(b2Vec, bState);\n+    __ movdqu(c2Vec, cState);\n+    __ vpaddd(d2State, dState, Address(rax, 16), vector_len);\n+    __ movdqu(d2Vec, d2State);\n+  } else {\n+    \/\/ We will broadcast each 128-bit segment of the state array into\n+    \/\/ the high and low halves of ymm state registers.  Then apply the add\n+    \/\/ mask to the dState register.  These will then be copied into the\n+    \/\/ a\/b\/c\/d1Vec working registers.\n+    __ vbroadcastf128(aState, Address(state, 0), vector_len);\n+    __ vbroadcastf128(bState, Address(state, 16), vector_len);\n+    __ vbroadcastf128(cState, Address(state, 32), vector_len);\n+    __ vbroadcastf128(dState, Address(state, 48), vector_len);\n+    __ vpaddd(dState, dState, Address(rax, 0), vector_len);\n+    __ vpaddd(d2State, dState, Address(rax, 32), vector_len);\n+\n+    __ vmovdqu(a1Vec, aState);\n+    __ vmovdqu(b1Vec, bState);\n+    __ vmovdqu(c1Vec, cState);\n+    __ vmovdqu(d1Vec, dState);\n+\n+    __ vmovdqu(a2Vec, aState);\n+    __ vmovdqu(b2Vec, bState);\n+    __ vmovdqu(c2Vec, cState);\n+    __ vmovdqu(d2Vec, d2State);\n+  }\n+\n+  __ movl(loopCounter, 10);                   \/\/ Set 10 2-round iterations\n+  __ BIND(L_twoRounds);\n+\n+  \/\/ The first quarter round macro call covers the first 4 QR operations:\n+  \/\/  Qround(state, 0, 4, 8,12)\n+  \/\/  Qround(state, 1, 5, 9,13)\n+  \/\/  Qround(state, 2, 6,10,14)\n+  \/\/  Qround(state, 3, 7,11,15)\n+  cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch, vector_len);\n+  cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch, vector_len);\n+\n+  \/\/ Shuffle the b1Vec\/c1Vec\/d1Vec to reorganize the state vectors\n+  \/\/ to diagonals.  The a1Vec does not need to change orientation.\n+  cc20_shift_lane_org(b1Vec, c1Vec, d1Vec, vector_len, true);\n+  cc20_shift_lane_org(b2Vec, c2Vec, d2Vec, vector_len, true);\n+\n+  \/\/ The second set of operations on the vectors covers the second 4 quarter\n+  \/\/ round operations, now acting on the diagonals:\n+  \/\/  Qround(state, 0, 5,10,15)\n+  \/\/  Qround(state, 1, 6,11,12)\n+  \/\/  Qround(state, 2, 7, 8,13)\n+  \/\/  Qround(state, 3, 4, 9,14)\n+  cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch, vector_len);\n+  cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch, vector_len);\n+\n+  \/\/ Before we start the next iteration, we need to perform shuffles\n+  \/\/ on the b\/c\/d vectors to move them back to columnar organizations\n+  \/\/ from their current diagonal orientation.\n+  cc20_shift_lane_org(b1Vec, c1Vec, d1Vec, vector_len, false);\n+  cc20_shift_lane_org(b2Vec, c2Vec, d2Vec, vector_len, false);\n+\n+  __ decrement(loopCounter);\n+  __ jcc(Assembler::notZero, L_twoRounds);\n+\n+  \/\/ Add the original start state back into the current state.\n+  __ vpaddd(a1Vec, a1Vec, aState, vector_len);\n+  __ vpaddd(b1Vec, b1Vec, bState, vector_len);\n+  __ vpaddd(c1Vec, c1Vec, cState, vector_len);\n+  __ vpaddd(d1Vec, d1Vec, dState, vector_len);\n+\n+  __ vpaddd(a2Vec, a2Vec, aState, vector_len);\n+  __ vpaddd(b2Vec, b2Vec, bState, vector_len);\n+  __ vpaddd(c2Vec, c2Vec, cState, vector_len);\n+  __ vpaddd(d2Vec, d2Vec, d2State, vector_len);\n+\n+  \/\/ Write the data to the keystream array\n+  if (vector_len == Assembler::AVX_128bit) {\n+    __ movdqu(Address(result, 0), a1Vec);\n+    __ movdqu(Address(result, 16), b1Vec);\n+    __ movdqu(Address(result, 32), c1Vec);\n+    __ movdqu(Address(result, 48), d1Vec);\n+    __ movdqu(Address(result, 64), a2Vec);\n+    __ movdqu(Address(result, 80), b2Vec);\n+    __ movdqu(Address(result, 96), c2Vec);\n+    __ movdqu(Address(result, 112), d2Vec);\n+  } else {\n+    \/\/ Each half of the YMM has to be written 64 bytes apart from\n+    \/\/ each other in memory so the final keystream buffer holds\n+    \/\/ two consecutive keystream blocks.\n+    __ vextracti128(Address(result, 0), a1Vec, 0);\n+    __ vextracti128(Address(result, 64), a1Vec, 1);\n+    __ vextracti128(Address(result, 16), b1Vec, 0);\n+    __ vextracti128(Address(result, 80), b1Vec, 1);\n+    __ vextracti128(Address(result, 32), c1Vec, 0);\n+    __ vextracti128(Address(result, 96), c1Vec, 1);\n+    __ vextracti128(Address(result, 48), d1Vec, 0);\n+    __ vextracti128(Address(result, 112), d1Vec, 1);\n+\n+    __ vextracti128(Address(result, 128), a2Vec, 0);\n+    __ vextracti128(Address(result, 192), a2Vec, 1);\n+    __ vextracti128(Address(result, 144), b2Vec, 0);\n+    __ vextracti128(Address(result, 208), b2Vec, 1);\n+    __ vextracti128(Address(result, 160), c2Vec, 0);\n+    __ vextracti128(Address(result, 224), c2Vec, 1);\n+    __ vextracti128(Address(result, 176), d2Vec, 0);\n+    __ vextracti128(Address(result, 240), d2Vec, 1);\n+  }\n+\n+  \/\/ This function will always write 128 or 256 bytes into the\n+  \/\/ key stream buffer, depending on the length of the SIMD\n+  \/\/ registers.  That length should be returned through %rax.\n+  __ mov64(rax, outlen);\n+\n+  __ leave();\n+  __ ret(0);\n+  return start;\n@@ -270,165 +270,157 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"chacha20Block\");\n-    address start = __ pc();\n-\n-    Label L_twoRounds;\n-    const Register state        = c_rarg0;\n-    const Register result       = c_rarg1;\n-    const Register loopCounter  = r8;\n-\n-    const XMMRegister aState = xmm0;\n-    const XMMRegister bState = xmm1;\n-    const XMMRegister cState = xmm2;\n-    const XMMRegister dState = xmm3;\n-    const XMMRegister a1Vec = xmm4;\n-    const XMMRegister b1Vec = xmm5;\n-    const XMMRegister c1Vec = xmm6;\n-    const XMMRegister d1Vec = xmm7;\n-    const XMMRegister a2Vec = xmm8;\n-    const XMMRegister b2Vec = xmm9;\n-    const XMMRegister c2Vec = xmm10;\n-    const XMMRegister d2Vec = xmm11;\n-    const XMMRegister a3Vec = xmm12;\n-    const XMMRegister b3Vec = xmm13;\n-    const XMMRegister c3Vec = xmm14;\n-    const XMMRegister d3Vec = xmm15;\n-    const XMMRegister a4Vec = xmm16;\n-    const XMMRegister b4Vec = xmm17;\n-    const XMMRegister c4Vec = xmm18;\n-    const XMMRegister d4Vec = xmm19;\n-    const XMMRegister d2State = xmm20;\n-    const XMMRegister d3State = xmm21;\n-    const XMMRegister d4State = xmm22;\n-    const XMMRegister scratch = xmm23;\n-\n-    __ enter();\n-\n-    \/\/ Load the initial state in columnar orientation.\n-    \/\/ We will broadcast each 128-bit segment of the state array into\n-    \/\/ all four double-quadword slots on ZMM State registers.  They will\n-    \/\/ be copied into the working ZMM registers and then added back in\n-    \/\/ at the very end of the block function.  The add mask should be\n-    \/\/ applied to the dState register so it does not need to be fetched\n-    \/\/ when adding the start state back into the final working state.\n-    __ lea(rax, ExternalAddress(chacha20_ctradd_avx512()));\n-    __ evbroadcasti32x4(aState, Address(state, 0), Assembler::AVX_512bit);\n-    __ evbroadcasti32x4(bState, Address(state, 16), Assembler::AVX_512bit);\n-    __ evbroadcasti32x4(cState, Address(state, 32), Assembler::AVX_512bit);\n-    __ evbroadcasti32x4(dState, Address(state, 48), Assembler::AVX_512bit);\n-    __ vpaddd(dState, dState, Address(rax, 0), Assembler::AVX_512bit);\n-    __ evmovdqul(scratch, Address(rax, 64), Assembler::AVX_512bit);\n-    __ vpaddd(d2State, dState, scratch, Assembler::AVX_512bit);\n-    __ vpaddd(d3State, d2State, scratch, Assembler::AVX_512bit);\n-    __ vpaddd(d4State, d3State, scratch, Assembler::AVX_512bit);\n-\n-    __ evmovdqul(a1Vec, aState, Assembler::AVX_512bit);\n-    __ evmovdqul(b1Vec, bState, Assembler::AVX_512bit);\n-    __ evmovdqul(c1Vec, cState, Assembler::AVX_512bit);\n-    __ evmovdqul(d1Vec, dState, Assembler::AVX_512bit);\n-\n-    __ evmovdqul(a2Vec, aState, Assembler::AVX_512bit);\n-    __ evmovdqul(b2Vec, bState, Assembler::AVX_512bit);\n-    __ evmovdqul(c2Vec, cState, Assembler::AVX_512bit);\n-    __ evmovdqul(d2Vec, d2State, Assembler::AVX_512bit);\n-\n-    __ evmovdqul(a3Vec, aState, Assembler::AVX_512bit);\n-    __ evmovdqul(b3Vec, bState, Assembler::AVX_512bit);\n-    __ evmovdqul(c3Vec, cState, Assembler::AVX_512bit);\n-    __ evmovdqul(d3Vec, d3State, Assembler::AVX_512bit);\n-\n-    __ evmovdqul(a4Vec, aState, Assembler::AVX_512bit);\n-    __ evmovdqul(b4Vec, bState, Assembler::AVX_512bit);\n-    __ evmovdqul(c4Vec, cState, Assembler::AVX_512bit);\n-    __ evmovdqul(d4Vec, d4State, Assembler::AVX_512bit);\n-\n-    __ movl(loopCounter, 10);                       \/\/ Set 10 2-round iterations\n-    __ BIND(L_twoRounds);\n-\n-    \/\/ The first set of operations on the vectors covers the first 4 quarter\n-    \/\/ round operations:\n-    \/\/  Qround(state, 0, 4, 8,12)\n-    \/\/  Qround(state, 1, 5, 9,13)\n-    \/\/  Qround(state, 2, 6,10,14)\n-    \/\/  Qround(state, 3, 7,11,15)\n-    __ cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch,\n-            Assembler::AVX_512bit);\n-    __ cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch,\n-            Assembler::AVX_512bit);\n-    __ cc20_quarter_round_avx(a3Vec, b3Vec, c3Vec, d3Vec, scratch,\n-            Assembler::AVX_512bit);\n-    __ cc20_quarter_round_avx(a4Vec, b4Vec, c4Vec, d4Vec, scratch,\n-            Assembler::AVX_512bit);\n-\n-    \/\/ Shuffle the b1Vec\/c1Vec\/d1Vec to reorganize the state vectors\n-    \/\/ to diagonals.  The a1Vec does not need to change orientation.\n-    __ cc20_shift_lane_org(b1Vec, c1Vec, d1Vec, Assembler::AVX_512bit, true);\n-    __ cc20_shift_lane_org(b2Vec, c2Vec, d2Vec, Assembler::AVX_512bit, true);\n-    __ cc20_shift_lane_org(b3Vec, c3Vec, d3Vec, Assembler::AVX_512bit, true);\n-    __ cc20_shift_lane_org(b4Vec, c4Vec, d4Vec, Assembler::AVX_512bit, true);\n-\n-    \/\/ The second set of operations on the vectors covers the second 4 quarter\n-    \/\/ round operations, now acting on the diagonals:\n-    \/\/  Qround(state, 0, 5,10,15)\n-    \/\/  Qround(state, 1, 6,11,12)\n-    \/\/  Qround(state, 2, 7, 8,13)\n-    \/\/  Qround(state, 3, 4, 9,14)\n-    __ cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch,\n-            Assembler::AVX_512bit);\n-    __ cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch,\n-            Assembler::AVX_512bit);\n-    __ cc20_quarter_round_avx(a3Vec, b3Vec, c3Vec, d3Vec, scratch,\n-            Assembler::AVX_512bit);\n-    __ cc20_quarter_round_avx(a4Vec, b4Vec, c4Vec, d4Vec, scratch,\n-            Assembler::AVX_512bit);\n-\n-    \/\/ Before we start the next iteration, we need to perform shuffles\n-    \/\/ on the b\/c\/d vectors to move them back to columnar organizations\n-    \/\/ from their current diagonal orientation.\n-    __ cc20_shift_lane_org(b1Vec, c1Vec, d1Vec, Assembler::AVX_512bit, false);\n-    __ cc20_shift_lane_org(b2Vec, c2Vec, d2Vec, Assembler::AVX_512bit, false);\n-    __ cc20_shift_lane_org(b3Vec, c3Vec, d3Vec, Assembler::AVX_512bit, false);\n-    __ cc20_shift_lane_org(b4Vec, c4Vec, d4Vec, Assembler::AVX_512bit, false);\n-\n-    __ decrement(loopCounter);\n-    __ jcc(Assembler::notZero, L_twoRounds);\n-\n-    \/\/ Add the initial state now held on the a\/b\/c\/dState registers to the\n-    \/\/ final working register values.  We will also add in the counter add\n-    \/\/ mask onto zmm3 after adding in the start state.\n-    __ vpaddd(a1Vec, a1Vec, aState, Assembler::AVX_512bit);\n-    __ vpaddd(b1Vec, b1Vec, bState, Assembler::AVX_512bit);\n-    __ vpaddd(c1Vec, c1Vec, cState, Assembler::AVX_512bit);\n-    __ vpaddd(d1Vec, d1Vec, dState, Assembler::AVX_512bit);\n-\n-    __ vpaddd(a2Vec, a2Vec, aState, Assembler::AVX_512bit);\n-    __ vpaddd(b2Vec, b2Vec, bState, Assembler::AVX_512bit);\n-    __ vpaddd(c2Vec, c2Vec, cState, Assembler::AVX_512bit);\n-    __ vpaddd(d2Vec, d2Vec, d2State, Assembler::AVX_512bit);\n-\n-    __ vpaddd(a3Vec, a3Vec, aState, Assembler::AVX_512bit);\n-    __ vpaddd(b3Vec, b3Vec, bState, Assembler::AVX_512bit);\n-    __ vpaddd(c3Vec, c3Vec, cState, Assembler::AVX_512bit);\n-    __ vpaddd(d3Vec, d3Vec, d3State, Assembler::AVX_512bit);\n-\n-    __ vpaddd(a4Vec, a4Vec, aState, Assembler::AVX_512bit);\n-    __ vpaddd(b4Vec, b4Vec, bState, Assembler::AVX_512bit);\n-    __ vpaddd(c4Vec, c4Vec, cState, Assembler::AVX_512bit);\n-    __ vpaddd(d4Vec, d4Vec, d4State, Assembler::AVX_512bit);\n-\n-    \/\/ Write the ZMM state registers out to the key stream buffer\n-    \/\/ Each ZMM is divided into 4 128-bit segments.  Each segment\n-    \/\/ is written to memory at 64-byte displacements from one\n-    \/\/ another.  The result is that all 4 blocks will be in their\n-    \/\/ proper order when serialized.\n-    __ cc20_keystream_collate_avx512(a1Vec, b1Vec, c1Vec, d1Vec, result, 0);\n-    __ cc20_keystream_collate_avx512(a2Vec, b2Vec, c2Vec, d2Vec, result, 256);\n-    __ cc20_keystream_collate_avx512(a3Vec, b3Vec, c3Vec, d3Vec, result, 512);\n-    __ cc20_keystream_collate_avx512(a4Vec, b4Vec, c4Vec, d4Vec, result, 768);\n-\n-    \/\/ This function will always write 1024 bytes into the key stream buffer\n-    \/\/ and that length should be returned through %rax.\n-    __ mov64(rax, 1024);\n-\n-    __ leave();\n-    __ ret(0);\n-    return start;\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"chacha20Block\");\n+  address start = __ pc();\n+\n+  Label L_twoRounds;\n+  const Register state        = c_rarg0;\n+  const Register result       = c_rarg1;\n+  const Register loopCounter  = r8;\n+\n+  const XMMRegister aState = xmm0;\n+  const XMMRegister bState = xmm1;\n+  const XMMRegister cState = xmm2;\n+  const XMMRegister dState = xmm3;\n+  const XMMRegister a1Vec = xmm4;\n+  const XMMRegister b1Vec = xmm5;\n+  const XMMRegister c1Vec = xmm6;\n+  const XMMRegister d1Vec = xmm7;\n+  const XMMRegister a2Vec = xmm8;\n+  const XMMRegister b2Vec = xmm9;\n+  const XMMRegister c2Vec = xmm10;\n+  const XMMRegister d2Vec = xmm11;\n+  const XMMRegister a3Vec = xmm12;\n+  const XMMRegister b3Vec = xmm13;\n+  const XMMRegister c3Vec = xmm14;\n+  const XMMRegister d3Vec = xmm15;\n+  const XMMRegister a4Vec = xmm16;\n+  const XMMRegister b4Vec = xmm17;\n+  const XMMRegister c4Vec = xmm18;\n+  const XMMRegister d4Vec = xmm19;\n+  const XMMRegister d2State = xmm20;\n+  const XMMRegister d3State = xmm21;\n+  const XMMRegister d4State = xmm22;\n+  const XMMRegister scratch = xmm23;\n+\n+  __ enter();\n+\n+  \/\/ Load the initial state in columnar orientation.\n+  \/\/ We will broadcast each 128-bit segment of the state array into\n+  \/\/ all four double-quadword slots on ZMM State registers.  They will\n+  \/\/ be copied into the working ZMM registers and then added back in\n+  \/\/ at the very end of the block function.  The add mask should be\n+  \/\/ applied to the dState register so it does not need to be fetched\n+  \/\/ when adding the start state back into the final working state.\n+  __ lea(rax, ExternalAddress(chacha20_ctradd_avx512()));\n+  __ evbroadcasti32x4(aState, Address(state, 0), Assembler::AVX_512bit);\n+  __ evbroadcasti32x4(bState, Address(state, 16), Assembler::AVX_512bit);\n+  __ evbroadcasti32x4(cState, Address(state, 32), Assembler::AVX_512bit);\n+  __ evbroadcasti32x4(dState, Address(state, 48), Assembler::AVX_512bit);\n+  __ vpaddd(dState, dState, Address(rax, 0), Assembler::AVX_512bit);\n+  __ evmovdqul(scratch, Address(rax, 64), Assembler::AVX_512bit);\n+  __ vpaddd(d2State, dState, scratch, Assembler::AVX_512bit);\n+  __ vpaddd(d3State, d2State, scratch, Assembler::AVX_512bit);\n+  __ vpaddd(d4State, d3State, scratch, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(a1Vec, aState, Assembler::AVX_512bit);\n+  __ evmovdqul(b1Vec, bState, Assembler::AVX_512bit);\n+  __ evmovdqul(c1Vec, cState, Assembler::AVX_512bit);\n+  __ evmovdqul(d1Vec, dState, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(a2Vec, aState, Assembler::AVX_512bit);\n+  __ evmovdqul(b2Vec, bState, Assembler::AVX_512bit);\n+  __ evmovdqul(c2Vec, cState, Assembler::AVX_512bit);\n+  __ evmovdqul(d2Vec, d2State, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(a3Vec, aState, Assembler::AVX_512bit);\n+  __ evmovdqul(b3Vec, bState, Assembler::AVX_512bit);\n+  __ evmovdqul(c3Vec, cState, Assembler::AVX_512bit);\n+  __ evmovdqul(d3Vec, d3State, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(a4Vec, aState, Assembler::AVX_512bit);\n+  __ evmovdqul(b4Vec, bState, Assembler::AVX_512bit);\n+  __ evmovdqul(c4Vec, cState, Assembler::AVX_512bit);\n+  __ evmovdqul(d4Vec, d4State, Assembler::AVX_512bit);\n+\n+  __ movl(loopCounter, 10);                       \/\/ Set 10 2-round iterations\n+  __ BIND(L_twoRounds);\n+\n+  \/\/ The first set of operations on the vectors covers the first 4 quarter\n+  \/\/ round operations:\n+  \/\/  Qround(state, 0, 4, 8,12)\n+  \/\/  Qround(state, 1, 5, 9,13)\n+  \/\/  Qround(state, 2, 6,10,14)\n+  \/\/  Qround(state, 3, 7,11,15)\n+  cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch, Assembler::AVX_512bit);\n+  cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch, Assembler::AVX_512bit);\n+  cc20_quarter_round_avx(a3Vec, b3Vec, c3Vec, d3Vec, scratch, Assembler::AVX_512bit);\n+  cc20_quarter_round_avx(a4Vec, b4Vec, c4Vec, d4Vec, scratch, Assembler::AVX_512bit);\n+\n+  \/\/ Shuffle the b1Vec\/c1Vec\/d1Vec to reorganize the state vectors\n+  \/\/ to diagonals.  The a1Vec does not need to change orientation.\n+  cc20_shift_lane_org(b1Vec, c1Vec, d1Vec, Assembler::AVX_512bit, true);\n+  cc20_shift_lane_org(b2Vec, c2Vec, d2Vec, Assembler::AVX_512bit, true);\n+  cc20_shift_lane_org(b3Vec, c3Vec, d3Vec, Assembler::AVX_512bit, true);\n+  cc20_shift_lane_org(b4Vec, c4Vec, d4Vec, Assembler::AVX_512bit, true);\n+\n+  \/\/ The second set of operations on the vectors covers the second 4 quarter\n+  \/\/ round operations, now acting on the diagonals:\n+  \/\/  Qround(state, 0, 5,10,15)\n+  \/\/  Qround(state, 1, 6,11,12)\n+  \/\/  Qround(state, 2, 7, 8,13)\n+  \/\/  Qround(state, 3, 4, 9,14)\n+  cc20_quarter_round_avx(a1Vec, b1Vec, c1Vec, d1Vec, scratch, Assembler::AVX_512bit);\n+  cc20_quarter_round_avx(a2Vec, b2Vec, c2Vec, d2Vec, scratch, Assembler::AVX_512bit);\n+  cc20_quarter_round_avx(a3Vec, b3Vec, c3Vec, d3Vec, scratch, Assembler::AVX_512bit);\n+  cc20_quarter_round_avx(a4Vec, b4Vec, c4Vec, d4Vec, scratch, Assembler::AVX_512bit);\n+\n+  \/\/ Before we start the next iteration, we need to perform shuffles\n+  \/\/ on the b\/c\/d vectors to move them back to columnar organizations\n+  \/\/ from their current diagonal orientation.\n+  cc20_shift_lane_org(b1Vec, c1Vec, d1Vec, Assembler::AVX_512bit, false);\n+  cc20_shift_lane_org(b2Vec, c2Vec, d2Vec, Assembler::AVX_512bit, false);\n+  cc20_shift_lane_org(b3Vec, c3Vec, d3Vec, Assembler::AVX_512bit, false);\n+  cc20_shift_lane_org(b4Vec, c4Vec, d4Vec, Assembler::AVX_512bit, false);\n+\n+  __ decrement(loopCounter);\n+  __ jcc(Assembler::notZero, L_twoRounds);\n+\n+  \/\/ Add the initial state now held on the a\/b\/c\/dState registers to the\n+  \/\/ final working register values.  We will also add in the counter add\n+  \/\/ mask onto zmm3 after adding in the start state.\n+  __ vpaddd(a1Vec, a1Vec, aState, Assembler::AVX_512bit);\n+  __ vpaddd(b1Vec, b1Vec, bState, Assembler::AVX_512bit);\n+  __ vpaddd(c1Vec, c1Vec, cState, Assembler::AVX_512bit);\n+  __ vpaddd(d1Vec, d1Vec, dState, Assembler::AVX_512bit);\n+\n+  __ vpaddd(a2Vec, a2Vec, aState, Assembler::AVX_512bit);\n+  __ vpaddd(b2Vec, b2Vec, bState, Assembler::AVX_512bit);\n+  __ vpaddd(c2Vec, c2Vec, cState, Assembler::AVX_512bit);\n+  __ vpaddd(d2Vec, d2Vec, d2State, Assembler::AVX_512bit);\n+\n+  __ vpaddd(a3Vec, a3Vec, aState, Assembler::AVX_512bit);\n+  __ vpaddd(b3Vec, b3Vec, bState, Assembler::AVX_512bit);\n+  __ vpaddd(c3Vec, c3Vec, cState, Assembler::AVX_512bit);\n+  __ vpaddd(d3Vec, d3Vec, d3State, Assembler::AVX_512bit);\n+\n+  __ vpaddd(a4Vec, a4Vec, aState, Assembler::AVX_512bit);\n+  __ vpaddd(b4Vec, b4Vec, bState, Assembler::AVX_512bit);\n+  __ vpaddd(c4Vec, c4Vec, cState, Assembler::AVX_512bit);\n+  __ vpaddd(d4Vec, d4Vec, d4State, Assembler::AVX_512bit);\n+\n+  \/\/ Write the ZMM state registers out to the key stream buffer\n+  \/\/ Each ZMM is divided into 4 128-bit segments.  Each segment\n+  \/\/ is written to memory at 64-byte displacements from one\n+  \/\/ another.  The result is that all 4 blocks will be in their\n+  \/\/ proper order when serialized.\n+  cc20_keystream_collate_avx512(a1Vec, b1Vec, c1Vec, d1Vec, result, 0);\n+  cc20_keystream_collate_avx512(a2Vec, b2Vec, c2Vec, d2Vec, result, 256);\n+  cc20_keystream_collate_avx512(a3Vec, b3Vec, c3Vec, d3Vec, result, 512);\n+  cc20_keystream_collate_avx512(a4Vec, b4Vec, c4Vec, d4Vec, result, 768);\n+\n+  \/\/ This function will always write 1024 bytes into the key stream buffer\n+  \/\/ and that length should be returned through %rax.\n+  __ mov64(rax, 1024);\n+\n+  __ leave();\n+  __ ret(0);\n+  return start;\n@@ -437,0 +429,114 @@\n+\/**\n+ * Provide a macro for AVX and AVX2 implementations of the ChaCha20 quarter\n+ * round function.\n+ *\n+ * @param aVec the SIMD register containing only the \"a\" values\n+ * @param bVec the SIMD register containing only the \"b\" values\n+ * @param cVec the SIMD register containing only the \"c\" values\n+ * @param dVec the SIMD register containing only the \"d\" values\n+ * @param scratch SIMD register used for left rotations other than 16-bit.\n+ * @param vector_len the length of the vector (128 and 256 bit only)\n+ *\/\n+void StubGenerator::cc20_quarter_round_avx(XMMRegister aVec, XMMRegister bVec,\n+    XMMRegister cVec, XMMRegister dVec, XMMRegister scratch, int vector_len) {\n+\n+  \/\/ a += b; d ^= a; d <<<= 16\n+  __ vpaddd(aVec, aVec, bVec, vector_len);\n+  __ vpxor(dVec, dVec, aVec, vector_len);\n+  if (vector_len == Assembler::AVX_512bit) {\n+    __ evprold(dVec, dVec, 16, vector_len);\n+  } else {\n+    __ vpshufhw(dVec, dVec, 0xB1, vector_len);\n+    __ vpshuflw(dVec, dVec, 0xB1, vector_len);\n+  }\n+\n+  \/\/ c += d; b ^= c; b <<<= 12 (b << 12 | scratch >>> 20)\n+  __ vpaddd(cVec, cVec, dVec, vector_len);\n+  __ vpxor(bVec, bVec, cVec, vector_len);\n+  if (vector_len == Assembler::AVX_512bit) {\n+    __ evprold(bVec, bVec, 12, vector_len);\n+  } else {\n+    __ vpsrld(scratch, bVec, 20, vector_len);\n+    __ vpslld(bVec, bVec, 12, vector_len);\n+    __ vpor(bVec, bVec, scratch, vector_len);\n+  }\n+\n+  \/\/ a += b; d ^= a; d <<<= 8 (d << 8 | scratch >>> 24)\n+  __ vpaddd(aVec, aVec, bVec, vector_len);\n+  __ vpxor(dVec, dVec, aVec, vector_len);\n+  if (vector_len == Assembler::AVX_512bit) {\n+    __ evprold(dVec, dVec, 8, vector_len);\n+  } else {\n+    __ vpsrld(scratch, dVec, 24, vector_len);\n+    __ vpslld(dVec, dVec, 8, vector_len);\n+    __ vpor(dVec, dVec, scratch, vector_len);\n+  }\n+\n+  \/\/ c += d; b ^= c; b <<<= 7 (b << 7 | scratch >>> 25)\n+  __ vpaddd(cVec, cVec, dVec, vector_len);\n+  __ vpxor(bVec, bVec, cVec, vector_len);\n+  if (vector_len == Assembler::AVX_512bit) {\n+    __ evprold(bVec, bVec, 7, vector_len);\n+  } else {\n+    __ vpsrld(scratch, bVec, 25, vector_len);\n+    __ vpslld(bVec, bVec, 7, vector_len);\n+    __ vpor(bVec, bVec, scratch, vector_len);\n+  }\n+}\n+\n+\/**\n+ * Shift the b, c, and d vectors between columnar and diagonal representations.\n+ * Note that the \"a\" vector does not shift.\n+ *\n+ * @param bVec the SIMD register containing only the \"b\" values\n+ * @param cVec the SIMD register containing only the \"c\" values\n+ * @param dVec the SIMD register containing only the \"d\" values\n+ * @param vector_len the size of the SIMD register to operate upon\n+ * @param colToDiag true if moving columnar to diagonal, false if\n+ *                  moving diagonal back to columnar.\n+ *\/\n+void StubGenerator::cc20_shift_lane_org(XMMRegister bVec, XMMRegister cVec,\n+    XMMRegister dVec, int vector_len, bool colToDiag) {\n+  int bShift = colToDiag ? 0x39 : 0x93;\n+  int cShift = 0x4E;\n+  int dShift = colToDiag ? 0x93 : 0x39;\n+\n+  __ vpshufd(bVec, bVec, bShift, vector_len);\n+  __ vpshufd(cVec, cVec, cShift, vector_len);\n+  __ vpshufd(dVec, dVec, dShift, vector_len);\n+}\n+\n+\/**\n+ * Write 256 bytes of keystream output held in 4 AVX512 SIMD registers\n+ * in a quarter round parallel organization.\n+ *\n+ * @param aVec the SIMD register containing only the \"a\" values\n+ * @param bVec the SIMD register containing only the \"b\" values\n+ * @param cVec the SIMD register containing only the \"c\" values\n+ * @param dVec the SIMD register containing only the \"d\" values\n+ * @param baseAddr the register holding the base output address\n+ * @param baseOffset the offset from baseAddr for writes\n+ *\/\n+void StubGenerator::cc20_keystream_collate_avx512(XMMRegister aVec, XMMRegister\n+bVec,\n+    XMMRegister cVec, XMMRegister dVec, Register baseAddr, int baseOffset) {\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 0), aVec, 0);\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 64), aVec, 1);\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 128), aVec, 2);\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 192), aVec, 3);\n+\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 16), bVec, 0);\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 80), bVec, 1);\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 144), bVec, 2);\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 208), bVec, 3);\n+\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 32), cVec, 0);\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 96), cVec, 1);\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 160), cVec, 2);\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 224), cVec, 3);\n+\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 48), dVec, 0);\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 112), dVec, 1);\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 176), dVec, 2);\n+  __ vextracti32x4(Address(baseAddr, baseOffset + 240), dVec, 3);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_chacha.cpp","additions":449,"deletions":343,"binary":false,"changes":792,"status":"modified"},{"patch":"@@ -533,1 +533,1 @@\n-   do_name(chacha20Block_name,                                 \"_chaCha20Block\")                                         \\\n+   do_name(chacha20Block_name,                                 \"implChaCha20Block\")                                         \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -854,1 +854,1 @@\n-        return _chaCha20Block(initState, result);\n+        return implChaCha20Block(initState, result);\n@@ -870,1 +870,1 @@\n-    private static int _chaCha20Block(int[] initState, byte[] result) {\n+    private static int implChaCha20Block(int[] initState, byte[] result) {\n","filename":"src\/java.base\/share\/classes\/com\/sun\/crypto\/provider\/ChaCha20Cipher.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}