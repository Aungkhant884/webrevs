{"files":[{"patch":"@@ -1967,1 +1967,1 @@\n-  __ remove_frame(framesize);\n+  __ remove_frame(framesize, C->needs_stack_repair(), C->output()->sp_inc_offset());\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -379,4 +379,1 @@\n-  \/\/ The frame_map records size in slots (32bit word)\n-\n-  \/\/ subtract two words to account for return address and link\n-  return (frame_map()->framesize() - (2*VMRegImpl::slots_per_word))  * VMRegImpl::stack_slot_size;\n+  return in_bytes(frame_map()->framesize_in_bytes());\n@@ -464,1 +461,2 @@\n-  __ remove_frame(initial_framesize, needs_stack_repair(), initial_framesize - wordSize);\n+  int sp_inc_offset = initial_framesize - 3*wordSize;  \/\/ Below saved FP and LR\n+  __ remove_frame(initial_framesize, needs_stack_repair(), sp_inc_offset);\n@@ -531,1 +529,2 @@\n-  __ remove_frame(initial_framesize, needs_stack_repair(), initial_framesize - wordSize);\n+  int sp_inc_offset = initial_framesize - 3*wordSize;  \/\/ Below saved FP and LR\n+  __ remove_frame(initial_framesize, needs_stack_repair(), sp_inc_offset);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":5,"deletions":6,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -352,1 +352,1 @@\n-  MacroAssembler::build_frame(frame_size_in_bytes + 2 * wordSize);\n+  MacroAssembler::build_frame(frame_size_in_bytes);\n@@ -355,1 +355,2 @@\n-    Unimplemented();\n+    int sp_inc_offset = frame_size_in_bytes - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+    save_stack_increment(sp_inc, sp_inc_offset);\n@@ -360,7 +361,5 @@\n-  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n-  \/\/ Make sure there is enough stack space for this method's activation.\n-  \/\/ Note that we do this before doing an enter().\n-  generate_stack_overflow_check(bang_size_in_bytes);\n-\n-  guarantee(needs_stack_repair == false, \"Stack repair should not be true\");\n-  if (verified_inline_entry_label != NULL) {\n+  if (has_scalarized_args) {\n+    \/\/ Initialize orig_pc to detect deoptimization during buffering in the entry points\n+    str(zr, Address(sp, sp_offset_for_orig_pc - frame_size_in_bytes));\n+  }\n+  if (!needs_stack_repair && verified_inline_entry_label != NULL) {\n@@ -370,0 +369,5 @@\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  \/\/ Note that we do this before creating a frame.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n@@ -375,0 +379,6 @@\n+\n+  if (needs_stack_repair && verified_inline_entry_label != NULL) {\n+    \/\/ Jump here from the scalarized entry points that require additional stack space\n+    \/\/ for packing scalarized arguments and therefore already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -379,2 +389,1 @@\n-  MacroAssembler::remove_frame(frame_size_in_bytes + 2 * wordSize,\n-                               needs_stack_repair, sp_inc_offset);\n+  MacroAssembler::remove_frame(frame_size_in_bytes, needs_stack_repair, sp_inc_offset);\n@@ -413,1 +422,1 @@\n-    Unimplemented();\n+    sp_inc = extend_stack_for_inline_args(args_on_stack);\n@@ -426,0 +435,8 @@\n+  \/\/ The runtime call returns the new array in r0 which is also j_rarg7\n+  \/\/ so we must avoid clobbering that. Temporarily save r0 in a\n+  \/\/ non-argument register and pass the buffered array in r20 instead.\n+  \/\/ This is safe because the runtime stub saves all registers.\n+  Register val_array = r20;\n+  Register tmp1 = r21;\n+  mov(tmp1, j_rarg7);\n+\n@@ -427,1 +444,1 @@\n-  mov(r1, (intptr_t) ces->method());\n+  mov(r19, (intptr_t) ces->method());\n@@ -435,0 +452,3 @@\n+  mov(val_array, r0);\n+  mov(j_rarg7, tmp1);\n+\n@@ -436,1 +456,1 @@\n-  add(sp, sp, frame_size_in_bytes);\n+  MacroAssembler::remove_frame(frame_size_in_bytes);\n@@ -441,1 +461,1 @@\n-                      sp_inc);\n+                      sp_inc, val_array);\n@@ -444,1 +464,3 @@\n-    Unimplemented();\n+    \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+    \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+    build_frame_helper(frame_size_in_bytes, sp_inc, true);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":38,"deletions":16,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -888,1 +888,1 @@\n-        Register method = r1;\n+        Register method = r19;   \/\/ Incoming\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -153,1 +153,0 @@\n-      sender_unextended_sp = sender_sp;\n@@ -156,2 +155,2 @@\n-      saved_fp = (intptr_t*) *(sender_sp - frame::sender_sp_offset);\n-    }\n+      intptr_t **saved_fp_addr = (intptr_t**) (sender_sp - frame::sender_sp_offset);\n+      saved_fp = *saved_fp_addr;\n@@ -159,0 +158,4 @@\n+      \/\/ Repair the sender sp if this is a method with scalarized inline type args\n+      sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+      sender_unextended_sp = sender_sp;\n+    }\n@@ -452,1 +455,0 @@\n-  intptr_t* unextended_sp = l_sender_sp;\n@@ -454,2 +456,3 @@\n-  \/\/ the return_address is always the word on the stack\n-  address sender_pc = (address) *(l_sender_sp-1);\n+#ifdef ASSERT\n+  address sender_pc_copy = (address) *(l_sender_sp-1);\n+#endif\n@@ -462,0 +465,16 @@\n+  \/\/ Repair the sender sp if the frame has been extended\n+  l_sender_sp = repair_sender_sp(l_sender_sp, saved_fp_addr);\n+\n+  \/\/ The return address is always the first word on the stack\n+  address sender_pc = (address) *(l_sender_sp-1);\n+\n+#ifdef ASSERT\n+  if (sender_pc != sender_pc_copy) {\n+    \/\/ When extending the stack in the callee method entry to make room for unpacking of value\n+    \/\/ type args, we keep a copy of the sender pc at the expected location in the callee frame.\n+    \/\/ If the sender pc is patched due to deoptimization, the copy is not consistent anymore.\n+    nmethod* nm = CodeCache::find_blob(sender_pc)->as_nmethod();\n+    assert(sender_pc == nm->deopt_mh_handler_begin() || sender_pc == nm->deopt_handler_begin(), \"unexpected sender pc\");\n+  }\n+#endif\n+\n@@ -466,1 +485,14 @@\n-    map->set_include_argument_oops(_cb->caller_must_gc_arguments(map->thread()));\n+    bool caller_args = _cb->caller_must_gc_arguments(map->thread());\n+#ifdef COMPILER1\n+    if (!caller_args) {\n+      nmethod* nm = _cb->as_nmethod_or_null();\n+      if (nm != NULL && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+          pc() < nm->verified_inline_entry_point()) {\n+        \/\/ The VEP and VIEP(RO) of C1-compiled methods call buffer_inline_args_xxx\n+        \/\/ before doing any argument shuffling, so we need to scan the oops\n+        \/\/ as the caller passes them.\n+        caller_args = true;\n+      }\n+    }\n+#endif\n+    map->set_include_argument_oops(caller_args);\n@@ -478,1 +510,1 @@\n-  return frame(l_sender_sp, unextended_sp, *saved_fp_addr, sender_pc);\n+  return frame(l_sender_sp, l_sender_sp, *saved_fp_addr, sender_pc);\n@@ -800,0 +832,17 @@\n+\/\/ Check for a method with scalarized inline type arguments that needs\n+\/\/ a stack repair and return the repaired sender stack pointer.\n+intptr_t* frame::repair_sender_sp(intptr_t* sender_sp, intptr_t** saved_fp_addr) const {\n+  CompiledMethod* cm = _cb->as_compiled_method_or_null();\n+  if (cm != NULL && cm->needs_stack_repair()) {\n+    \/\/ The stack increment resides just below the saved FP on the stack\n+    intptr_t* sp_inc_addr = (intptr_t*) (saved_fp_addr - 1);\n+    assert(*sp_inc_addr % StackAlignmentInBytes == 0, \"sp_inc not aligned\");\n+    int sp_inc = *sp_inc_addr \/ wordSize;\n+    int real_frame_size = _cb->frame_size() + sp_inc;\n+    assert(real_frame_size >= _cb->frame_size() && real_frame_size <= 1000000, \"invalid frame size\");\n+    assert(unextended_sp() + real_frame_size == sender_sp + sp_inc, \"sanity\");\n+    sender_sp = unextended_sp() + real_frame_size;\n+  }\n+  return sender_sp;\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":57,"deletions":8,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -131,0 +131,3 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  intptr_t* repair_sender_sp(intptr_t* sender_sp, intptr_t** saved_fp_addr) const;\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -196,2 +196,1 @@\n-  assert_different_registers(store_addr, new_val, thread, tmp, tmp2,\n-                             rscratch1);\n+  assert_different_registers(store_addr, new_val, thread, tmp, rscratch1);\n@@ -223,0 +222,2 @@\n+  assert_different_registers(store_addr, thread, tmp, tmp2, rscratch1);\n+\n@@ -293,1 +294,1 @@\n-  assert((decorators & IS_DEST_UNINITIALIZED) == 0, \"unsupported\");\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n@@ -295,1 +296,1 @@\n-  bool needs_pre_barrier = as_normal;\n+  bool needs_pre_barrier = as_normal && !dest_uninitialized;\n@@ -298,6 +299,4 @@\n-\n-   if (tmp3 == noreg) {\n-     tmp3 = rscratch2;\n-   }\n-   \/\/ assert_different_registers(val, tmp1, tmp2, tmp3, rscratch1, rscratch2);\n-   assert_different_registers(val, tmp1, tmp2, tmp3);\n+  if (tmp3 == noreg) {\n+    tmp3 = rscratch2;\n+  }\n+  assert_different_registers(val, tmp1, tmp2, tmp3);\n@@ -314,1 +313,0 @@\n-\n@@ -332,2 +330,1 @@\n-        \/\/ FIXME: Refactor the code to avoid usage of r19 and stay within tmpX\n-        new_val = r19;\n+        new_val = tmp3;\n@@ -336,1 +333,1 @@\n-   }\n+    }\n@@ -338,1 +335,1 @@\n-   BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg, noreg);\n+    BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg, noreg);\n@@ -341,8 +338,8 @@\n-       g1_write_barrier_post(masm,\n-                          tmp1 \/* store_adr *\/,\n-                          new_val \/* new_val *\/,\n-                          rthread \/* thread *\/,\n-                          tmp2 \/* tmp *\/,\n-                          tmp3 \/* tmp2 *\/);\n-   }\n- }\n+      g1_write_barrier_post(masm,\n+                            tmp1 \/* store_adr *\/,\n+                            new_val \/* new_val *\/,\n+                            rthread \/* thread *\/,\n+                            tmp2 \/* tmp *\/,\n+                            tmp3 \/* tmp2 *\/);\n+    }\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":21,"deletions":24,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -67,1 +67,1 @@\n-define_pd_global(bool, InlineTypePassFieldsAsArgs, false);\n+define_pd_global(bool, InlineTypePassFieldsAsArgs, true);\n","filename":"src\/hotspot\/cpu\/aarch64\/globals_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4784,1 +4784,2 @@\n-  assert(framesize > 0, \"framesize must be > 0\");\n+  assert(framesize >= 2 * wordSize, \"framesize must include space for FP\/LR\");\n+  assert(framesize % (2*wordSize) == 0, \"must preserve 2*wordSize alignment\");\n@@ -4803,1 +4804,2 @@\n-  assert(framesize > 0, \"framesize must be > 0\");\n+  assert(framesize >= 2 * wordSize, \"framesize must include space for FP\/LR\");\n+  assert(framesize % (2*wordSize) == 0, \"must preserve 2*wordSize alignment\");\n@@ -4818,0 +4820,45 @@\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {\n+  if (needs_stack_repair) {\n+    \/\/ Remove the extension of the caller's frame used for inline type unpacking\n+    \/\/\n+    \/\/ Right now the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Saved LR #1               |\n+    \/\/ | Saved FP #1               |\n+    \/\/ |---------------------------|\n+    \/\/ | sp_inc - 2*wordSize bytes |\n+    \/\/ |   of stack extension for  |\n+    \/\/ |   inline arg (un)packing  |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | Saved LR #2               |\n+    \/\/ | Saved FP #2               |\n+    \/\/ |---------------------------|  <-- FP\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ There are two copies of FP and LR on the stack. They will be identical\n+    \/\/ unless the caller has been deoptimized, in which case LR #1 will be patched\n+    \/\/ to point at the deopt blob, and LR #2 will still point into the old method.\n+\n+    \/\/ This pops the stack extension and regular frame in the wrong\n+    \/\/ order but it doesn't matter because the result is the same.\n+    ldr(rscratch1, Address(sp, sp_inc_offset));\n+    add(sp, sp, rscratch1);\n+  }\n+\n+  remove_frame(initial_framesize);\n+}\n+\n+void MacroAssembler::save_stack_increment(int sp_inc, int sp_inc_offset) {\n+  if (sp_inc == 0) {\n+    str(zr, Address(sp, sp_inc_offset));\n+  } else {\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    assert(sp_inc > 2*wordSize, \"must include FP\/LR space\");\n+    mov(rscratch1, sp_inc);\n+    str(rscratch1, Address(sp, sp_inc_offset));\n+  }\n+}\n@@ -5627,1 +5674,1 @@\n-\/\/ n.b. frame size includes space for return pc and rfp\n+  \/\/ n.b. frame size includes space for return pc and rfp\n@@ -5629,1 +5676,0 @@\n-  assert(framesize % (2 * wordSize) == 0, \"must preserve 2 * wordSize alignment\");\n@@ -5637,1 +5683,1 @@\n-     generate_stack_overflow_check(bangsize);\n+    generate_stack_overflow_check(bangsize);\n@@ -5642,1 +5688,1 @@\n-    Unimplemented();\n+    save_stack_increment(sp_inc, C->output()->sp_inc_offset());\n@@ -5736,1 +5782,7 @@\n-        mov(to->as_Register(), from->as_Register());\n+        if (from->is_Register() && to->is_Register()) {\n+          mov(to->as_Register(), from->as_Register());\n+        } else if (from->is_FloatRegister() && to->is_FloatRegister()) {\n+          fmovd(to->as_FloatRegister(), from->as_FloatRegister());\n+        } else {\n+          ShouldNotReachHere();\n+        }\n@@ -5738,1 +5790,1 @@\n-        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -5752,1 +5804,1 @@\n-      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size);\n@@ -5756,1 +5808,1 @@\n-             ldrd(to->as_FloatRegister(), from_addr);\n+            ldrd(to->as_FloatRegister(), from_addr);\n@@ -5765,1 +5817,1 @@\n-        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -5778,0 +5830,21 @@\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  int sp_inc = args_on_stack * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  assert(sp_inc > 0, \"sanity\");\n+\n+  \/\/ Save a copy of the FP and LR here for deoptimization patching and frame walking\n+  stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+\n+  \/\/ Adjust the stack pointer. This will be repaired on return by MacroAssembler::remove_frame\n+  if (sp_inc < (1 << 9)) {\n+    sub(sp, sp, sp_inc);   \/\/ Fits in an immediate\n+  } else {\n+    mov(rscratch1, sp_inc);\n+    sub(sp, sp, rscratch1);\n+  }\n+\n+  return sp_inc + 2 * wordSize;  \/\/ Account for the FP\/LR space\n+}\n+\n@@ -5784,0 +5857,1 @@\n+  Register tmp1 = r10, tmp2 = r11;\n@@ -5788,3 +5862,3 @@\n-    int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n-    ldr(r10, Address(sp, st_off));\n-    fromReg = r10;\n+    int st_off = from->reg2stack() * VMRegImpl::stack_slot_size;\n+    ldr(tmp1, Address(sp, st_off));\n+    fromReg = tmp1;\n@@ -5805,5 +5879,5 @@\n-     if (idx != from->value()) {\n-       mark_done = false;\n-     }\n-     done = false;\n-     continue;\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n@@ -5818,1 +5892,1 @@\n-      Register dst = toReg->is_stack() ? r13 : toReg->as_Register();\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n@@ -5826,1 +5900,1 @@\n-        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -5850,1 +5924,1 @@\n-                                        RegState reg_state[]) {\n+                                        RegState reg_state[], Register val_array) {\n@@ -5859,1 +5933,0 @@\n-  Register val_array = r0;\n@@ -5864,1 +5937,1 @@\n-  Register tmp3 = r1;\n+  Register tmp3 = r12;\n@@ -5867,0 +5940,2 @@\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n@@ -5893,1 +5968,1 @@\n-        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -5926,9 +6001,0 @@\n-void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {\n-  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  if (needs_stack_repair) {\n-    Unimplemented();\n-  } else {\n-    remove_frame(initial_framesize);\n-  }\n-}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":100,"deletions":34,"binary":false,"changes":134,"status":"modified"},{"patch":"@@ -1268,1 +1268,2 @@\n-                          RegState reg_state[]);\n+                          RegState reg_state[], Register val_array);\n+  int extend_stack_for_inline_args(int args_on_stack);\n@@ -1271,0 +1272,1 @@\n+  void save_stack_increment(int sp_inc, int sp_inc_offset);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -118,1 +118,5 @@\n-  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_offset() :\n+  \/\/ The following jump might pass an inline type argument that was erased to Object as oop to a\n+  \/\/ callee that expects inline type arguments to be passed as fields. We need to call the compiled\n+  \/\/ value entry (_code->inline_entry_point() or _adapter->c2i_inline_entry()) which will take care\n+  \/\/ of translating between the calling conventions.\n+  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_inline_offset() :\n","filename":"src\/hotspot\/cpu\/aarch64\/methodHandles_aarch64.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -520,21 +520,16 @@\n-static void gen_c2i_adapter_helper(MacroAssembler* masm, BasicType bt, const VMRegPair& reg_pair, int extraspace, const Address& to) {\n-\n-    assert(bt != T_INLINE_TYPE || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n-\n-    \/\/ int next_off = st_off - Interpreter::stackElementSize;\n-\n-    VMReg r_1 = reg_pair.first();\n-    VMReg r_2 = reg_pair.second();\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   Register tmp1,\n+                                   Register tmp2,\n+                                   Register tmp3,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  assert(bt != T_INLINE_TYPE || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n@@ -542,4 +537,21 @@\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      return;\n-    }\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"\");\n+    return;\n+  }\n@@ -547,0 +559,2 @@\n+  if (!r_1->is_FloatRegister()) {\n+    Register val = tmp3;\n@@ -548,15 +562,3 @@\n-      \/\/ memory to memory use rscratch1\n-      \/\/ words_pushed is always 0 so we don't use it.\n-      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace \/* + word_pushed * wordSize *\/);\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ ldrw(rscratch1, Address(sp, ld_off));\n-        __ str(rscratch1, to);\n-\n-      } else {\n-        __ ldr(rscratch1, Address(sp, ld_off));\n-        __ str(rscratch1, to);\n-      }\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      __ str(r, to);\n+      \/\/ memory to memory use tmp3 (scratch registers are used by store_heap_oop)\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n@@ -564,8 +566,16 @@\n-      assert(r_1->is_FloatRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ strs(r_1->as_FloatRegister(), to);\n-      } else {\n-        __ strd(r_1->as_FloatRegister(), to);\n-      }\n-   }\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, rscratch2, tmp1, tmp2);\n+    if (is_oop) {\n+      __ store_heap_oop(to, val, rscratch2, tmp1, tmp2, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else {\n+      \/\/ only a float use just part of the slot\n+      __ strs(r_1->as_FloatRegister(), to);\n+    }\n+  }\n@@ -593,1 +603,9 @@\n-  bool has_inline_argument = false;\n+  \/\/ Name some registers to be used in the following code. We can use\n+  \/\/ anything except r0-r7 which are arguments in the Java calling\n+  \/\/ convention, rmethod (r12), and r13 which holds the outgoing sender\n+  \/\/ SP for the interpreter.\n+  Register buf_array = r10;   \/\/ Array of buffered inline types\n+  Register buf_oop = r11;     \/\/ Buffered inline type oop\n+  Register tmp1 = r15;\n+  Register tmp2 = r16;\n+  Register tmp3 = r17;\n@@ -596,5 +614,6 @@\n-      \/\/ Is there an inline type argument?\n-     for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n-       has_inline_argument = (sig_extended->at(i)._bt == T_INLINE_TYPE);\n-     }\n-     if (has_inline_argument) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_INLINE_TYPE);\n+    }\n+    if (has_inline_argument) {\n@@ -610,1 +629,2 @@\n-      __ set_last_Java_frame(noreg, noreg, the_pc, rscratch1);\n+      Label retaddr;\n+      __ set_last_Java_frame(sp, noreg, retaddr, rscratch1);\n@@ -613,1 +633,1 @@\n-      __ mov(c_rarg1, r1);\n+      __ mov(c_rarg1, rmethod);\n@@ -618,0 +638,1 @@\n+      __ bind(retaddr);\n@@ -619,1 +640,1 @@\n-      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      oop_maps->add_gc_map(__ pc() - start, map);\n@@ -625,2 +646,2 @@\n-      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n-      __ cbz(r0, no_exception);\n+      __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));\n+      __ cbz(rscratch1, no_exception);\n@@ -635,2 +656,2 @@\n-      __ get_vm_result(r10, rthread);\n-      __ get_vm_result_2(r1, rthread); \/\/ TODO: required to keep the callee Method live?\n+      __ get_vm_result(buf_array, rthread);\n+      __ get_vm_result_2(rmethod, rthread); \/\/ TODO: required to keep the callee Method live?\n@@ -640,2 +661,0 @@\n-  int words_pushed = 0;\n-\n@@ -646,1 +665,1 @@\n-  int extraspace = (total_args_passed * Interpreter::stackElementSize) + wordSize;\n+  int extraspace = total_args_passed * Interpreter::stackElementSize;\n@@ -649,1 +668,1 @@\n-  extraspace = align_up(extraspace, 2 * wordSize);\n+  extraspace = align_up(extraspace, StackAlignmentInBytes);\n@@ -651,0 +670,1 @@\n+  \/\/ set senderSP value\n@@ -653,2 +673,1 @@\n-  if (extraspace)\n-    __ sub(sp, sp, extraspace);\n+  __ sub(sp, sp, extraspace);\n@@ -658,4 +677,14 @@\n-  int ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n-  bool has_oop_field = false;\n-\n-  for (int next_arg_comp = 0; next_arg_comp < total_args_passed; next_arg_comp++) {\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_INLINE_TYPE,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_INLINE_TYPE\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n@@ -663,3 +692,1 @@\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;\n-\n+    int st_off = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;\n@@ -667,13 +694,16 @@\n-      if (bt == T_VOID) {\n-         assert(next_arg_comp > 0 && (sig_extended->at(next_arg_comp - 1)._bt == T_LONG || sig_extended->at(next_arg_comp - 1)._bt == T_DOUBLE), \"missing half\");\n-         next_arg_int ++;\n-         continue;\n-       }\n-\n-       int next_off = st_off - Interpreter::stackElementSize;\n-       int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n-\n-       gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp], extraspace, Address(sp, offset));\n-       next_arg_int ++;\n-   } else {\n-       ignored++;\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(sp, offset), tmp1, tmp2, tmp3, extraspace, false);\n+      next_arg_int++;\n+#ifdef ASSERT\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov(rscratch1, CONST64(0xdeadffffdeadaaaa));\n+        __ str(rscratch1, Address(sp, st_off));\n+      }\n+#endif \/* ASSERT *\/\n+    } else {\n+      ignored++;\n@@ -682,3 +712,2 @@\n-      __ load_heap_oop(rscratch1, Address(r10, index));\n-      next_vt_arg++;\n-      next_arg_int++;\n+      __ load_heap_oop(buf_oop, Address(buf_array, index));\n+      next_vt_arg++; next_arg_int++;\n@@ -688,1 +717,1 @@\n-      \/\/ argument when we hit the T_VOID that acts as an end of value\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n@@ -705,5 +734,4 @@\n-\n-          bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n-          has_oop_field = has_oop_field || is_oop;\n-\n-          gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp - ignored], extraspace, Address(r11, off));\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(buf_oop, off), tmp1, tmp2, tmp3, extraspace, is_oop);\n@@ -713,17 +741,1 @@\n-      __ str(rscratch1, Address(sp, st_off));\n-   }\n-\n-  }\n-\n-\/\/ If an inline type was allocated and initialized, apply post barrier to all oop fields\n-  if (has_inline_argument && has_oop_field) {\n-    __ push(r13); \/\/ save senderSP\n-    __ push(r1); \/\/ save callee\n-    \/\/ Allocate argument register save area\n-    if (frame::arg_reg_save_area_bytes != 0) {\n-      __ sub(sp, sp, frame::arg_reg_save_area_bytes);\n-    }\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::apply_post_barriers), rthread, r10);\n-    \/\/ De-allocate argument register save area\n-    if (frame::arg_reg_save_area_bytes != 0) {\n-      __ add(sp, sp, frame::arg_reg_save_area_bytes);\n+      __ str(buf_oop, Address(sp, st_off));\n@@ -731,2 +743,0 @@\n-    __ pop(r1); \/\/ restore callee\n-    __ pop(r13); \/\/ restore sender SP\n@@ -811,1 +821,1 @@\n-  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_offset())));\n+  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_inline_offset())));\n@@ -1034,1 +1044,1 @@\n-  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words + 10, oop_maps, caller_must_gc_arguments);\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":127,"deletions":117,"binary":false,"changes":244,"status":"modified"},{"patch":"@@ -417,9 +417,1 @@\n-    \/\/ Two additional slots to account for return address\n-    sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n-    sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n-    \/\/ Save the return address, adjust the stack (make sure it is properly\n-    \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n-    \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n-    pop(r13);\n-    subptr(rsp, sp_inc);\n-    push(r13);\n+    sp_inc = extend_stack_for_inline_args(args_on_stack);\n@@ -454,1 +446,1 @@\n-                      sp_inc);\n+                      sp_inc, rax);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":2,"deletions":10,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -5575,0 +5575,16 @@\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  \/\/ Two additional slots to account for return address\n+  int sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  \/\/ Save the return address, adjust the stack (make sure it is properly\n+  \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+  \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n+  assert(sp_inc > 0, \"sanity\");\n+  pop(r13);\n+  subptr(rsp, sp_inc);\n+  push(r13);\n+  return sp_inc;\n+}\n+\n@@ -5646,1 +5662,1 @@\n-                                        RegState reg_state[]) {\n+                                        RegState reg_state[], Register val_array) {\n@@ -5655,1 +5671,0 @@\n-  Register val_array = rax;\n@@ -5663,0 +5678,2 @@\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":19,"deletions":2,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -1763,1 +1763,2 @@\n-                          RegState reg_state[]);\n+                          RegState reg_state[], Register val_array);\n+  int extend_stack_for_inline_args(int args_on_stack);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -135,14 +135,1 @@\n-    \/\/ Two additional slots to account for return address\n-    sp_inc = (args_on_stack_cc + 2) * VMRegImpl::stack_slot_size;\n-    sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n-    \/\/ Save the return address, adjust the stack (make sure it is properly\n-    \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n-    \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n-    assert(sp_inc > 0, \"sanity\");\n-#ifdef X86\n-    pop(r13);\n-    subptr(rsp, sp_inc);\n-    push(r13);\n-#else\n-    Unimplemented();\n-#endif\n+    sp_inc = extend_stack_for_inline_args(args_on_stack_cc);\n@@ -153,1 +140,1 @@\n-                      sp_inc);\n+                      sp_inc, noreg);\n@@ -162,1 +149,1 @@\n-                                         int sp_inc) {\n+                                         int sp_inc, Register val_array) {\n@@ -193,0 +180,1 @@\n+        assert(val_array != noreg, \"must be\");\n@@ -196,1 +184,1 @@\n-                                   reg_state);\n+                                   reg_state, val_array);\n","filename":"src\/hotspot\/share\/asm\/macroAssembler_common.cpp","additions":5,"deletions":17,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -49,1 +49,1 @@\n-                           int sp_inc);\n+                           int sp_inc, Register val_array);\n","filename":"src\/hotspot\/share\/asm\/macroAssembler_common.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2013,1 +2013,1 @@\n-  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypePassFieldsAsArgs)) {\n+  if (AMD64_ONLY(false &&) AARCH64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypePassFieldsAsArgs)) {\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -303,1 +303,1 @@\n-#ifdef ASSERT\n+#if defined ASSERT && !defined AARCH64   \/\/ Stub call site does not look like NativeCall on AArch64\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3547,25 +3547,0 @@\n-\/\/ TODO remove this once the AARCH64 dependency is gone\n-\/\/ Iterate over the array of heap allocated inline types and apply the GC post barrier to all reference fields.\n-\/\/ This is called from the C2I adapter after inline type arguments are heap allocated and initialized.\n-JRT_LEAF(void, SharedRuntime::apply_post_barriers(JavaThread* current, objArrayOopDesc* array))\n-{\n-  assert(InlineTypePassFieldsAsArgs, \"no reason to call this\");\n-  assert(oopDesc::is_oop(array), \"should be oop\");\n-  for (int i = 0; i < array->length(); ++i) {\n-    instanceOop valueOop = (instanceOop)array->obj_at(i);\n-    InlineKlass* vk = InlineKlass::cast(valueOop->klass());\n-    if (vk->contains_oops()) {\n-      const address dst_oop_addr = ((address) (void*) valueOop);\n-      OopMapBlock* map = vk->start_of_nonstatic_oop_maps();\n-      OopMapBlock* const end = map + vk->nonstatic_oop_map_count();\n-      while (map != end) {\n-        address doop_address = dst_oop_addr + map->offset();\n-        barrier_set_cast<ModRefBarrierSet>(BarrierSet::barrier_set())->\n-          write_ref_array((HeapWord*) doop_address, map->count());\n-        map++;\n-      }\n-    }\n-  }\n-}\n-JRT_END\n-\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":0,"deletions":25,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -536,1 +536,0 @@\n-  static void apply_post_barriers(JavaThread* current, objArrayOopDesc* array);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"}]}