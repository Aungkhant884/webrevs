{"files":[{"patch":"@@ -864,1 +864,1 @@\n-  BUILD_HOTSPOT_JTREG_EXCLUDE += exesigtest.c libterminatedThread.c libTestJNI.c libCompleteExit.c libTestPsig.c libnativeStack.c exeGetCreatedJavaVMs.c\n+  BUILD_HOTSPOT_JTREG_EXCLUDE += exesigtest.c libterminatedThread.c libTestJNI.c libCompleteExit.c libTestPsig.c exeGetCreatedJavaVMs.c\n@@ -866,0 +866,2 @@\n+  BUILD_HOTSPOT_JTREG_LIBRARIES_LIBS_libnativeStack := jvm.lib\n+  BUILD_HOTSPOT_JTREG_LIBRARIES_COPY_DEBUG_SYMBOLS_libnativeStack := true\n","filename":"make\/test\/JtregNativeHotspot.gmk","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2973,0 +2973,17 @@\n+  \/\/ Big-endian 128-bit + 64-bit -> 128-bit addition.\n+  \/\/ Inputs: 128-bits. in is preserved.\n+  \/\/ The least-significant 64-bit word is in the upper dword of each vector.\n+  \/\/ inc (the 64-bit increment) is preserved. Its lower dword must be zero.\n+  \/\/ Output: result\n+  void be_add_128_64(FloatRegister result, FloatRegister in,\n+                     FloatRegister inc, FloatRegister tmp) {\n+    assert_different_registers(result, tmp, inc);\n+\n+    __ addv(result, __ T2D, in, inc);      \/\/ Add inc to the least-significant dword of\n+                                           \/\/ input\n+    __ cm(__ HI, tmp, __ T2D, inc, result);\/\/ Check for result overflowing\n+    __ ext(tmp, __ T16B, tmp, tmp, 0x08);  \/\/ Swap LSD of comparison result to MSD and\n+                                           \/\/ MSD == 0 (must be!) to LSD\n+    __ subv(result, __ T2D, result, tmp);  \/\/ Subtract -1 from MSD if there was an overflow\n+  }\n+\n@@ -3082,1 +3099,1 @@\n-      __ ins(v4, __ S, v5, 3, 3); \/\/ v4 contains { 0, 0, 0, 1 }\n+      __ ins(v4, __ S, v5, 2, 2); \/\/ v4 contains { 0, 1 }\n@@ -3084,5 +3101,8 @@\n-      __ ld1(v0, __ T16B, counter); \/\/ Load the counter into v0\n-      __ rev32(v16, __ T16B, v0);\n-      __ addv(v16, __ T4S, v16, v4);\n-      __ rev32(v16, __ T16B, v16);\n-      __ st1(v16, __ T16B, counter); \/\/ Save the incremented counter back\n+      \/\/ 128-bit big-endian increment\n+      __ ld1(v0, __ T16B, counter);\n+      __ rev64(v16, __ T16B, v0);\n+      be_add_128_64(v16, v16, v4, \/*tmp*\/v5);\n+      __ rev64(v16, __ T16B, v16);\n+      __ st1(v16, __ T16B, counter);\n+      \/\/ Previous counter value is in v0\n+      \/\/ v4 contains { 0, 1 }\n@@ -3120,3 +3140,3 @@\n-        __ rev32(v16, __ T16B, v16);\n-        __ addv(v16, __ T4S, v16, v4);\n-        __ rev32(v16, __ T16B, v16);\n+        __ rev64(v16, __ T16B, v16);\n+        be_add_128_64(v16, v16, v4, \/*tmp*\/v5);\n+        __ rev64(v16, __ T16B, v16);\n@@ -3170,1 +3190,1 @@\n-    __ rev32(v16, __ T16B, v0); \/\/ v16 contains byte-reversed counter\n+    __ rev64(v16, __ T16B, v0); \/\/ v16 contains byte-reversed counter\n@@ -3180,1 +3200,1 @@\n-      __ ins(v8, __ S, v9, 3, 3); \/\/ v8 contains { 0, 0, 0, 1 }\n+      __ ins(v8, __ S, v9, 2, 2); \/\/ v8 contains { 0, 1 }\n@@ -3184,2 +3204,2 @@\n-        __ rev32(v0_ofs, __ T16B, v16);\n-        __ addv(v16, __ T4S, v16, v8);\n+        __ rev64(v0_ofs, __ T16B, v16);\n+        be_add_128_64(v16, v16, v8, \/*tmp*\/v9);\n@@ -3215,1 +3235,1 @@\n-    __ rev32(v16, __ T16B, v16);\n+    __ rev64(v16, __ T16B, v16);\n@@ -7250,1 +7270,0 @@\n-    __ resolve_global_jobject(r0, rscratch1, rscratch2);\n@@ -7279,0 +7298,1 @@\n+    __ resolve_global_jobject(r0, rscratch1, rscratch2);\n@@ -7292,0 +7312,38 @@\n+  \/\/ For c2: call to return a leased buffer.\n+  static RuntimeStub* generate_jfr_return_lease() {\n+    enum layout {\n+      rbp_off,\n+      rbpH_off,\n+      return_off,\n+      return_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    int insts_size = 1024;\n+    int locs_size = 64;\n+    CodeBuffer code(\"jfr_return_lease\", insts_size, locs_size);\n+    OopMapSet* oop_maps = new OopMapSet();\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+    MacroAssembler* _masm = masm;\n+\n+    address start = __ pc();\n+    __ enter();\n+    int frame_complete = __ pc() - start;\n+    address the_pc = __ pc();\n+    jfr_prologue(the_pc, _masm, rthread);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::return_lease), 1);\n+    jfr_epilogue(_masm);\n+\n+    __ leave();\n+    __ ret(lr);\n+\n+    OopMap* map = new OopMap(framesize, 1); \/\/ rfp\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    RuntimeStub* stub = \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+      RuntimeStub::new_runtime_stub(\"jfr_return_lease\", &code, frame_complete,\n+                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                    oop_maps, false);\n+    return stub;\n+  }\n+\n@@ -8425,2 +8483,1 @@\n-    JFR_ONLY(StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();)\n-    JFR_ONLY(StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();)\n+    JFR_ONLY(generate_jfr_stubs();)\n@@ -8429,0 +8486,9 @@\n+#if INCLUDE_JFR\n+  void generate_jfr_stubs() {\n+    StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();\n+    StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();\n+    StubRoutines::_jfr_return_lease_stub = generate_jfr_return_lease();\n+    StubRoutines::_jfr_return_lease = StubRoutines::_jfr_return_lease_stub->entry_point();\n+  }\n+#endif \/\/ INCLUDE_JFR\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":83,"deletions":17,"binary":false,"changes":100,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -10032,0 +10033,11 @@\n+void MacroAssembler::evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, AddressLiteral src, bool merge, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::evpaddq(dst, mask, nds, as_Address(src), merge, vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::evpaddq(dst, mask, nds, Address(rscratch, 0), merge, vector_len);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1851,0 +1851,3 @@\n+  using Assembler::evpaddq;\n+  void evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3771,0 +3771,41 @@\n+\/\/ For c2: call to return a leased buffer.\n+RuntimeStub* StubGenerator::generate_jfr_return_lease() {\n+  enum layout {\n+    rbp_off,\n+    rbpH_off,\n+    return_off,\n+    return_off2,\n+    framesize \/\/ inclusive of return address\n+  };\n+\n+  CodeBuffer code(\"jfr_return_lease\", 1024, 64);\n+  MacroAssembler* _masm = new MacroAssembler(&code);\n+  address start = __ pc();\n+\n+  __ enter();\n+  address the_pc = __ pc();\n+\n+  int frame_complete = the_pc - start;\n+\n+  __ set_last_Java_frame(rsp, rbp, the_pc, rscratch2);\n+  __ movptr(c_rarg0, r15_thread);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::return_lease), 1);\n+  __ reset_last_Java_frame(true);\n+\n+  __ leave();\n+  __ ret(0);\n+\n+  OopMapSet* oop_maps = new OopMapSet();\n+  OopMap* map = new OopMap(framesize, 1);\n+  oop_maps->add_gc_map(frame_complete, map);\n+\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(code.name(),\n+                                  &code,\n+                                  frame_complete,\n+                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                  oop_maps,\n+                                  false);\n+  return stub;\n+}\n+\n@@ -4124,2 +4165,1 @@\n-  JFR_ONLY(StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();)\n-  JFR_ONLY(StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();)\n+  JFR_ONLY(generate_jfr_stubs();)\n@@ -4128,0 +4168,9 @@\n+#if INCLUDE_JFR\n+void StubGenerator::generate_jfr_stubs() {\n+  StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();\n+  StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();\n+  StubRoutines::_jfr_return_lease_stub = generate_jfr_return_lease();\n+  StubRoutines::_jfr_return_lease = StubRoutines::_jfr_return_lease_stub->entry_point();\n+}\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":51,"deletions":2,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -367,1 +367,2 @@\n-\n+  void ev_add128(XMMRegister xmmdst, XMMRegister xmmsrc1, XMMRegister xmmsrc2,\n+                 int vector_len, KRegister ktmp, Register rscratch = noreg);\n@@ -523,1 +524,1 @@\n-\n+  void generate_jfr_stubs();\n@@ -528,1 +529,2 @@\n-\n+  \/\/ For c2: call to runtime to return a buffer lease.\n+  RuntimeStub* generate_jfr_return_lease();\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -83,0 +83,1 @@\n+ , _loop(nullptr)\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -489,4 +489,0 @@\n-#ifdef JFR_HAVE_INTRINSICS\n-  void do_getEventWriter(Intrinsic* x);\n-#endif\n-\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/compressedKlass.hpp\"\n@@ -1257,0 +1257,1 @@\n+#ifdef _LP64\n@@ -1261,0 +1262,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-enum {\n+enum : uint {\n@@ -70,1 +70,1 @@\n-    enum {\n+    enum : uint {\n","filename":"src\/hotspot\/share\/classfile\/verificationType.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/stackValue.hpp\"\n@@ -83,0 +84,14 @@\n+ScopeValue* DebugInfoReadStream::read_object_merge_value() {\n+  int id = read_int();\n+#ifdef ASSERT\n+  assert(_obj_pool != nullptr, \"object pool does not exist\");\n+  for (int i = _obj_pool->length() - 1; i >= 0; i--) {\n+    assert(_obj_pool->at(i)->as_ObjectValue()->id() != id, \"should not be read twice\");\n+  }\n+#endif\n+  ObjectMergeValue* result = new ObjectMergeValue(id);\n+  _obj_pool->push(result);\n+  result->read_object(this);\n+  return result;\n+}\n+\n@@ -101,1 +116,2 @@\n-                          AUTO_BOX_OBJECT_CODE = 7, MARKER_CODE = 8 };\n+                          AUTO_BOX_OBJECT_CODE = 7, MARKER_CODE = 8,\n+                          OBJECT_MERGE_CODE = 9 };\n@@ -113,0 +129,1 @@\n+   case OBJECT_MERGE_CODE:    result = stream->read_object_merge_value();                break;\n@@ -152,0 +169,1 @@\n+  _is_root = stream->read_bool();\n@@ -170,0 +188,1 @@\n+    stream->write_bool(_is_root);\n@@ -185,1 +204,1 @@\n-  st->print(\"%s[%d]\", is_auto_box() ? \"box_obj\" : \"obj\", _id);\n+  st->print(\"%s[%d]\", is_auto_box() ? \"box_obj\" : is_object_merge() ? \"merge_obj\" : \"obj\", _id);\n@@ -190,6 +209,28 @@\n-  if (_field_values.length() > 0) {\n-    _field_values.at(0)->print_on(st);\n-  }\n-  for (int i = 1; i < _field_values.length(); i++) {\n-    st->print(\", \");\n-    _field_values.at(i)->print_on(st);\n+  if (is_object_merge()) {\n+    ObjectMergeValue* omv = (ObjectMergeValue*)this;\n+    st->print(\"selector=\\\"\");\n+    omv->selector()->print_on(st);\n+    st->print(\"\\\"\");\n+    ScopeValue* merge_pointer = omv->merge_pointer();\n+    if (!(merge_pointer->is_object() && merge_pointer->as_ObjectValue()->value()() == nullptr) &&\n+        !(merge_pointer->is_constant_oop() && merge_pointer->as_ConstantOopReadValue()->value()() == nullptr)) {\n+      st->print(\", merge_pointer=\\\"\");\n+      merge_pointer->print_on(st);\n+      st->print(\"\\\"\");\n+    }\n+    GrowableArray<ScopeValue*>* possible_objects = omv->possible_objects();\n+    st->print(\", candidate_objs=[%d\", possible_objects->at(0)->as_ObjectValue()->id());\n+    int ncandidates = possible_objects->length();\n+    for (int i = 1; i < ncandidates; i++) {\n+      st->print(\", %d\", possible_objects->at(i)->as_ObjectValue()->id());\n+    }\n+    st->print(\"]\");\n+  } else {\n+    st->print(\"\\n        Fields: \");\n+    if (_field_values.length() > 0) {\n+      _field_values.at(0)->print_on(st);\n+    }\n+    for (int i = 1; i < _field_values.length(); i++) {\n+      st->print(\", \");\n+      _field_values.at(i)->print_on(st);\n+    }\n@@ -200,0 +241,63 @@\n+\n+\/\/ ObjectMergeValue\n+\n+\/\/ Returns the ObjectValue that should be used for the local that this\n+\/\/ ObjectMergeValue represents. ObjectMergeValue represents allocation\n+\/\/ merges in C2. This method will select which path the allocation merge\n+\/\/ took during execution of the Trap that triggered the rematerialization\n+\/\/ of the object.\n+ObjectValue* ObjectMergeValue::select(frame& fr, RegisterMap& reg_map) {\n+  StackValue* sv_selector = StackValue::create_stack_value(&fr, &reg_map, _selector);\n+  jint selector = sv_selector->get_int();\n+\n+  \/\/ If the selector is '-1' it means that execution followed the path\n+  \/\/ where no scalar replacement happened.\n+  \/\/ Otherwise, it is the index in _possible_objects array that holds\n+  \/\/ the description of the scalar replaced object.\n+  if (selector == -1) {\n+    StackValue* sv_merge_pointer = StackValue::create_stack_value(&fr, &reg_map, _merge_pointer);\n+    _selected = new ObjectValue(id());\n+\n+    \/\/ Retrieve the pointer to the real object and use it as if we had\n+    \/\/ allocated it during the deoptimization\n+    _selected->set_value(sv_merge_pointer->get_obj()());\n+\n+    \/\/ No need to rematerialize\n+    return nullptr;\n+  } else {\n+    assert(selector < _possible_objects.length(), \"sanity\");\n+    _selected = (ObjectValue*) _possible_objects.at(selector);\n+    return _selected;\n+  }\n+}\n+\n+void ObjectMergeValue::read_object(DebugInfoReadStream* stream) {\n+  _selector = read_from(stream);\n+  _merge_pointer = read_from(stream);\n+  int ncandidates = stream->read_int();\n+  for (int i = 0; i < ncandidates; i++) {\n+    ScopeValue* result = read_from(stream);\n+    assert(result->is_object(), \"Candidate is not an object!\");\n+    ObjectValue* obj = result->as_ObjectValue();\n+    _possible_objects.append(obj);\n+  }\n+}\n+\n+void ObjectMergeValue::write_on(DebugInfoWriteStream* stream) {\n+  if (is_visited()) {\n+    stream->write_int(OBJECT_ID_CODE);\n+    stream->write_int(_id);\n+  } else {\n+    set_visited(true);\n+    stream->write_int(OBJECT_MERGE_CODE);\n+    stream->write_int(_id);\n+    _selector->write_on(stream);\n+    _merge_pointer->write_on(stream);\n+    int ncandidates = _possible_objects.length();\n+    stream->write_int(ncandidates);\n+    for (int i = 0; i < ncandidates; i++) {\n+      _possible_objects.at(i)->as_ObjectValue()->write_on(stream);\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/code\/debugInfo.cpp","additions":112,"deletions":8,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+class ObjectMergeValue;\n@@ -53,0 +54,1 @@\n+  virtual bool is_object_merge() const { return false; }\n@@ -76,0 +78,5 @@\n+  ObjectMergeValue* as_ObjectMergeValue() {\n+    assert(is_object_merge(), \"must be\");\n+    return (ObjectMergeValue*)this;\n+  }\n+\n@@ -130,0 +137,4 @@\n+  bool                       _is_root;   \/\/ Will be true if this object is referred to\n+                                         \/\/ as a local\/expression\/monitor in the JVMs.\n+                                         \/\/ Otherwise false, meaning it's just a candidate\n+                                         \/\/ in an object allocation merge.\n@@ -137,1 +148,2 @@\n-     , _visited(false) {\n+     , _visited(false)\n+     , _is_root(true) {\n@@ -144,1 +156,1 @@\n-     , _is_init(nullptr)\n+     , _is_init(new MarkerValue())\n@@ -147,1 +159,2 @@\n-     , _visited(false) {}\n+     , _visited(false)\n+     , _is_root(true) {}\n@@ -150,13 +163,16 @@\n-  bool                        is_object() const         { return true; }\n-  int                         id() const                { return _id; }\n-  ScopeValue*                 klass() const             { return _klass; }\n-  ScopeValue*                 is_init() const           { return _is_init; }\n-  GrowableArray<ScopeValue*>* field_values()            { return &_field_values; }\n-  ScopeValue*                 field_at(int i) const     { return _field_values.at(i); }\n-  int                         field_size()              { return _field_values.length(); }\n-  Handle                      value() const             { return _value; }\n-  bool                        is_visited() const        { return _visited; }\n-  bool                        maybe_null() const        { return !_is_init->is_marker(); }\n-\n-  void                        set_value(oop value);\n-  void                        set_visited(bool visited) { _visited = visited; }\n+  bool                        is_object() const           { return true; }\n+  int                         id() const                  { return _id; }\n+  virtual ScopeValue*         klass() const               { return _klass; }\n+  virtual ScopeValue*         is_init() const             { return _is_init; }\n+  virtual GrowableArray<ScopeValue*>* field_values()      { return &_field_values; }\n+  virtual ScopeValue*         field_at(int i) const       { return _field_values.at(i); }\n+  virtual int                 field_size()                { return _field_values.length(); }\n+  virtual Handle              value() const               { return _value; }\n+  bool                        is_visited() const          { return _visited; }\n+  bool                        is_root() const             { return _is_root; }\n+  bool                        maybe_null() const          { return !_is_init->is_marker(); }\n+\n+  void                        set_id(int id)              { _id = id; }\n+  virtual void                set_value(oop value);\n+  void                        set_visited(bool visited)   { _visited = visited; }\n+  void                        set_root(bool root)         { _is_root = root; }\n@@ -173,0 +189,59 @@\n+\/\/ An ObjectMergeValue describes objects that were inputs to a Phi in C2 and at\n+\/\/ least one of them was scalar replaced.\n+\/\/ '_selector' is an integer value that will be '-1' if during the execution of\n+\/\/ the C2 compiled code the path taken was that of the Phi input that was NOT\n+\/\/ scalar replaced. In that case '_merge_pointer' is a pointer to an already\n+\/\/ allocated object. If '_selector' is not '-1' then it should be the index of\n+\/\/ an object in '_possible_objects'. That object is an ObjectValue describing an\n+\/\/ object that was scalar replaced.\n+\n+class ObjectMergeValue: public ObjectValue {\n+protected:\n+  ScopeValue*                _selector;\n+  ScopeValue*                _merge_pointer;\n+  GrowableArray<ScopeValue*> _possible_objects;\n+\n+  \/\/ This holds the ObjectValue that should be used in place of this\n+  \/\/ ObjectMergeValue. I.e., it's the ScopeValue from _possible_objects that was\n+  \/\/ selected by 'select()' or is a on-the-fly created ScopeValue representing\n+  \/\/ the _merge_pointer if _selector is -1.\n+  \/\/\n+  \/\/ We need to keep this reference around because there will be entries in\n+  \/\/ ScopeDesc that reference this ObjectMergeValue directly. After\n+  \/\/ rematerialization ObjectMergeValue will be just a wrapper for the\n+  \/\/ Objectvalue pointed by _selected.\n+  ObjectValue*               _selected;\n+public:\n+  ObjectMergeValue(int id, ScopeValue* merge_pointer, ScopeValue* selector)\n+     : ObjectValue(id)\n+     , _selector(selector)\n+     , _merge_pointer(merge_pointer)\n+     , _possible_objects()\n+     , _selected(nullptr) {}\n+\n+  ObjectMergeValue(int id)\n+     : ObjectValue(id)\n+     , _selector(nullptr)\n+     , _merge_pointer(nullptr)\n+     , _possible_objects()\n+     , _selected(nullptr) {}\n+\n+  bool                        is_object_merge() const         { return true; }\n+  ScopeValue*                 selector() const                { return _selector; }\n+  ScopeValue*                 merge_pointer() const           { return _merge_pointer; }\n+  GrowableArray<ScopeValue*>* possible_objects()              { return &_possible_objects; }\n+  ObjectValue*                select(frame& fr, RegisterMap& reg_map) ;\n+\n+  ScopeValue*                 klass() const                   { ShouldNotReachHere(); return nullptr; }\n+  GrowableArray<ScopeValue*>* field_values()                  { ShouldNotReachHere(); return nullptr; }\n+  ScopeValue*                 field_at(int i) const           { ShouldNotReachHere(); return nullptr; }\n+  int                         field_size()                    { ShouldNotReachHere(); return -1; }\n+\n+  Handle                      value() const                   { assert(_selected != nullptr, \"Should call select() first.\"); return _selected->value(); }\n+  void                        set_value(oop value)            { assert(_selected != nullptr, \"Should call select() first.\"); _selected->set_value(value); }\n+\n+  \/\/ Serialization of debugging information\n+  void read_object(DebugInfoReadStream* stream);\n+  void write_on(DebugInfoWriteStream* stream);\n+};\n+\n@@ -324,0 +399,1 @@\n+  ScopeValue* read_object_merge_value();\n","filename":"src\/hotspot\/share\/code\/debugInfo.hpp","additions":92,"deletions":16,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -119,1 +119,0 @@\n-  assert(result->length() == length, \"inconsistent debug information\");\n@@ -135,0 +134,32 @@\n+GrowableArray<ScopeValue*>* ScopeDesc::objects_to_rematerialize(frame& frm, RegisterMap& map) {\n+  if (_objects == nullptr) {\n+    return nullptr;\n+  }\n+\n+  GrowableArray<ScopeValue*>* result = new GrowableArray<ScopeValue*>();\n+  for (int i = 0; i < _objects->length(); i++) {\n+    assert(_objects->at(i)->is_object(), \"invalid debug information\");\n+    ObjectValue* sv = _objects->at(i)->as_ObjectValue();\n+\n+    \/\/ If the object is not referenced in current JVM state, then it's only\n+    \/\/ a candidate in an ObjectMergeValue, we don't need to rematerialize it\n+    \/\/ unless when\/if it's returned by 'select()' below.\n+    if (!sv->is_root()) {\n+      continue;\n+    }\n+\n+    if (sv->is_object_merge()) {\n+      sv = sv->as_ObjectMergeValue()->select(frm, map);\n+      \/\/ If select() returns nullptr, then the object doesn't need to be\n+      \/\/ rematerialized.\n+      if (sv == nullptr) {\n+        continue;\n+      }\n+    }\n+\n+    result->append_if_missing(sv);\n+  }\n+\n+  return result;\n+}\n+\n@@ -243,2 +274,6 @@\n-      st->print(\"    - %d: \", sv->id());\n-      st->print(\"%s \", java_lang_Class::as_Klass(sv->klass()->as_ConstantOopReadValue()->value()())->external_name());\n+      st->print(\"    - %d: %c \", i, sv->is_root() ? 'R' : ' ');\n+      sv->print_on(st);\n+      st->print(\", \");\n+      if (!sv->is_object_merge()) {\n+        st->print(\"%s\", java_lang_Class::as_Klass(sv->klass()->as_ConstantOopReadValue()->value()())->external_name());\n+      }\n","filename":"src\/hotspot\/share\/code\/scopeDesc.cpp","additions":38,"deletions":3,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -138,0 +138,1 @@\n+  GrowableArray<ScopeValue*>* objects_to_rematerialize(frame& frm, RegisterMap& map);\n","filename":"src\/hotspot\/share\/code\/scopeDesc.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -392,3 +392,4 @@\n-      int lo = Bytes::get_Java_u4(aligned_bcp + 1*jintSize);\n-      int hi = Bytes::get_Java_u4(aligned_bcp + 2*jintSize);\n-      int len = (int)(aligned_bcp - bcp) + (3 + hi - lo + 1)*jintSize;\n+      \/\/ Promote calculation to 64 bits to do range checks, used by the verifier.\n+      int64_t lo = (int)Bytes::get_Java_u4(aligned_bcp + 1*jintSize);\n+      int64_t hi = (int)Bytes::get_Java_u4(aligned_bcp + 2*jintSize);\n+      int64_t len = (aligned_bcp - bcp) + (3 + hi - lo + 1)*jintSize;\n@@ -397,1 +398,1 @@\n-      return (len > 0 && len == (int)len) ? len : -1;\n+      return (len > 0 && len == (int)len) ? (int)len : -1;\n@@ -407,2 +408,3 @@\n-      int npairs = Bytes::get_Java_u4(aligned_bcp + jintSize);\n-      int len = (int)(aligned_bcp - bcp) + (2 + 2*npairs)*jintSize;\n+      \/\/ Promote calculation to 64 bits to do range checks, used by the verifier.\n+      int64_t npairs = (int)Bytes::get_Java_u4(aligned_bcp + jintSize);\n+      int64_t len = (aligned_bcp - bcp) + (2 + 2*npairs)*jintSize;\n@@ -411,1 +413,1 @@\n-      return (len > 0 && len == (int)len) ? len : -1;\n+      return (len > 0 && len == (int)len) ? (int)len : -1;\n","filename":"src\/hotspot\/share\/interpreter\/bytecodes.cpp","additions":9,"deletions":7,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -290,3 +290,0 @@\n-    \/\/ We will reverse the bytecode rewriting _after_ adjusting them.\n-    \/\/ Adjust the cache index by offset to the invokedynamic entries in the\n-    \/\/ cpCache plus the delta if the invokedynamic bytecodes were adjusted.\n","filename":"src\/hotspot\/share\/interpreter\/rewriter.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1501,0 +1501,1 @@\n+              assert(!value->is_object_merge(), \"Should not be.\");\n@@ -1743,0 +1744,1 @@\n+        assert(!scopedValues->at(i2)->is_object_merge(), \"Should not be.\");\n@@ -1756,0 +1758,1 @@\n+        assert(!scopeExpressions->at(i2)->is_object_merge(), \"Should not be.\");\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -4107,1 +4107,1 @@\n-                      p2i(this),  p2i(superklass()));\n+                       p2i(this),  p2i(superklass()));\n@@ -4115,1 +4115,1 @@\n-                          p2i(InstanceKlass::cast(local_interfaces()->at(i))));\n+                           p2i(InstanceKlass::cast(local_interfaces()->at(i))));\n@@ -4127,3 +4127,3 @@\n-                        cfs->length(),\n-                        ClassLoader::crc32(0, (const char*)cfs->buffer(),\n-                        cfs->length()));\n+                         cfs->length(),\n+                         ClassLoader::crc32(0, (const char*)cfs->buffer(),\n+                         cfs->length()));\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,3 +40,0 @@\n-\/\/ If compressed klass pointers then use narrowKlass.\n-typedef juint  narrowKlass;\n-\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -185,3 +185,0 @@\n-  product(bool, PostLoopMultiversioning, false, EXPERIMENTAL,               \\\n-           \"Multi versioned post loops to eliminate range checks\")          \\\n-                                                                            \\\n@@ -473,0 +470,6 @@\n+  product(bool, ReduceAllocationMerges, true, DIAGNOSTIC,                   \\\n+          \"Try to simplify allocation merges before Scalar Replacement\")    \\\n+                                                                            \\\n+  notproduct(bool, TraceReduceAllocationMerges, false,                      \\\n+          \"Trace decision for simplifying allocation merges.\")              \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -55,0 +55,3 @@\n+const char* C2Compiler::retry_no_reduce_allocation_merges() {\n+  return \"retry without reducing allocation merges\";\n+}\n@@ -109,0 +112,1 @@\n+  bool do_reduce_allocation_merges = ReduceAllocationMerges;\n@@ -114,1 +118,1 @@\n-    Options options(subsume_loads, do_escape_analysis, do_iterative_escape_analysis, eliminate_boxing, do_locks_coarsening, install_code);\n+    Options options(subsume_loads, do_escape_analysis, do_iterative_escape_analysis, do_reduce_allocation_merges, eliminate_boxing, do_locks_coarsening, install_code);\n@@ -137,0 +141,6 @@\n+      if (C.failure_reason_is(retry_no_reduce_allocation_merges())) {\n+        assert(do_reduce_allocation_merges, \"must make progress\");\n+        do_reduce_allocation_merges = false;\n+        env->report_failure(C.failure_reason());\n+        continue;  \/\/ retry\n+      }\n@@ -679,0 +689,1 @@\n+  case vmIntrinsics::_jvm_commit:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -1164,0 +1164,6 @@\n+\/\/----------------------------is_uncommon_trap----------------------------\n+\/\/ Returns true if this is an uncommon trap.\n+bool CallStaticJavaNode::is_uncommon_trap() const {\n+  return (_name != nullptr && !strcmp(_name, \"uncommon_trap\"));\n+}\n+\n@@ -1167,4 +1173,1 @@\n-  if (_name != nullptr && !strcmp(_name, \"uncommon_trap\")) {\n-    return extract_uncommon_trap_request(this);\n-  }\n-  return 0;\n+  return is_uncommon_trap() ? extract_uncommon_trap_request(this) : 0;\n@@ -1671,6 +1674,1 @@\n-SafePointScalarObjectNode::SafePointScalarObjectNode(const TypeOopPtr* tp,\n-#ifdef ASSERT\n-                                                     Node* alloc,\n-#endif\n-                                                     uint first_index,\n-                                                     uint n_fields) :\n+SafePointScalarObjectNode::SafePointScalarObjectNode(const TypeOopPtr* tp, Node* alloc, uint first_index, uint n_fields) :\n@@ -1679,4 +1677,2 @@\n-  _n_fields(n_fields)\n-#ifdef ASSERT\n-  , _alloc(alloc)\n-#endif\n+  _n_fields(n_fields),\n+  _alloc(alloc)\n@@ -1685,2 +1681,1 @@\n-  if (alloc != nullptr && !alloc->is_Allocate()\n-      && !(alloc->Opcode() == Op_VectorBox)) {\n+  if (alloc != nullptr && !alloc->is_Allocate() && !(alloc->Opcode() == Op_VectorBox)) {\n@@ -1732,2 +1727,21 @@\n-  st->print(\" # fields@[%d..%d]\", first_index(),\n-             first_index() + n_fields() - 1);\n+  st->print(\" # fields@[%d..%d]\", first_index(), first_index() + n_fields() - 1);\n+}\n+#endif\n+\n+\/\/==============  SafePointScalarMergeNode  ==============\n+\n+SafePointScalarMergeNode::SafePointScalarMergeNode(const TypeOopPtr* tp, int merge_pointer_idx) :\n+  TypeNode(tp, 1), \/\/ 1 control input -- seems required.  Get from root.\n+  _merge_pointer_idx(merge_pointer_idx)\n+{\n+  init_class_id(Class_SafePointScalarMerge);\n+}\n+\n+\/\/ Do not allow value-numbering for SafePointScalarMerge node.\n+uint SafePointScalarMergeNode::hash() const { return NO_HASH; }\n+bool SafePointScalarMergeNode::cmp( const Node &n ) const {\n+  return (&n == this); \/\/ Always fail except on self\n+}\n+\n+uint SafePointScalarMergeNode::ideal_reg() const {\n+  return 0; \/\/ No matching to machine instruction\n@@ -1736,0 +1750,29 @@\n+const RegMask &SafePointScalarMergeNode::in_RegMask(uint idx) const {\n+  return *(Compile::current()->matcher()->idealreg2debugmask[in(idx)->ideal_reg()]);\n+}\n+\n+const RegMask &SafePointScalarMergeNode::out_RegMask() const {\n+  return RegMask::Empty;\n+}\n+\n+uint SafePointScalarMergeNode::match_edge(uint idx) const {\n+  return 0;\n+}\n+\n+SafePointScalarMergeNode*\n+SafePointScalarMergeNode::clone(Dict* sosn_map, bool& new_node) const {\n+  void* cached = (*sosn_map)[(void*)this];\n+  if (cached != nullptr) {\n+    new_node = false;\n+    return (SafePointScalarMergeNode*)cached;\n+  }\n+  new_node = true;\n+  SafePointScalarMergeNode* res = (SafePointScalarMergeNode*)Node::clone();\n+  sosn_map->Insert((void*)this, (void*)res);\n+  return res;\n+}\n+\n+#ifndef PRODUCT\n+void SafePointScalarMergeNode::dump_spec(outputStream *st) const {\n+  st->print(\" # merge_pointer_idx=%d, scalarized_objects=%d\", _merge_pointer_idx, req()-1);\n+}\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":61,"deletions":18,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -507,6 +507,6 @@\n-\n-  uint _first_index; \/\/ First input edge relative index of a SafePoint node where\n-                     \/\/ states of the scalarized object fields are collected.\n-                     \/\/ It is relative to the last (youngest) jvms->_scloff.\n-  uint _n_fields;    \/\/ Number of non-static fields of the scalarized object.\n-  DEBUG_ONLY(Node* _alloc;)\n+  uint _first_index;              \/\/ First input edge relative index of a SafePoint node where\n+                                  \/\/ states of the scalarized object fields are collected.\n+                                  \/\/ It is relative to the last (youngest) jvms->_scloff.\n+  uint _n_fields;                 \/\/ Number of non-static fields of the scalarized object.\n+\n+  Node* _alloc;                   \/\/ Just for debugging purposes.\n@@ -515,1 +515,1 @@\n-  virtual uint hash() const ; \/\/ { return NO_HASH; }\n+  virtual uint hash() const;\n@@ -521,5 +521,2 @@\n-  SafePointScalarObjectNode(const TypeOopPtr* tp,\n-#ifdef ASSERT\n-                            Node* alloc,\n-#endif\n-                            uint first_index, uint n_fields);\n+  SafePointScalarObjectNode(const TypeOopPtr* tp, Node* alloc, uint first_index, uint n_fields);\n+\n@@ -558,0 +555,86 @@\n+\/\/------------------------------SafePointScalarMergeNode----------------------\n+\/\/\n+\/\/ This class represents an allocation merge that is used as debug information\n+\/\/ and had at least one of its input scalar replaced.\n+\/\/\n+\/\/ The required inputs of this node, except the control, are pointers to\n+\/\/ SafePointScalarObjectNodes that describe scalarized inputs of the original\n+\/\/ allocation merge. The other(s) properties of the class are described below.\n+\/\/\n+\/\/ _merge_pointer_idx : index in the SafePointNode's input array where the\n+\/\/   description of the _allocation merge_ starts. The index is zero based and\n+\/\/   relative to the SafePoint's scloff. The two entries in the SafePointNode's\n+\/\/   input array starting at '_merge_pointer_idx` are Phi nodes representing:\n+\/\/\n+\/\/   1) The original merge Phi. During rematerialization this input will only be\n+\/\/   used if the \"selector Phi\" (see below) indicates that the execution of the\n+\/\/   Phi took the path of a non scalarized input.\n+\/\/\n+\/\/   2) A \"selector Phi\". The output of this Phi will be '-1' if the execution\n+\/\/   of the method exercised a non scalarized input of the original Phi.\n+\/\/   Otherwise, the output will be >=0, and it will indicate the index-1 in the\n+\/\/   SafePointScalarMergeNode input array where the description of the\n+\/\/   scalarized object that should be used is.\n+\/\/\n+\/\/ As an example, consider a Phi merging 3 inputs, of which the last 2 are\n+\/\/ scalar replaceable.\n+\/\/\n+\/\/    Phi(Region, NSR, SR, SR)\n+\/\/\n+\/\/ During scalar replacement the SR inputs will be changed to null:\n+\/\/\n+\/\/    Phi(Region, NSR, nullptr, nullptr)\n+\/\/\n+\/\/ A corresponding selector Phi will be created with a configuration like this:\n+\/\/\n+\/\/    Phi(Region, -1, 0, 1)\n+\/\/\n+\/\/ During execution of the compiled method, if the execution reaches a Trap, the\n+\/\/ output of the selector Phi will tell if we need to rematerialize one of the\n+\/\/ scalar replaced inputs or if we should just use the pointer returned by the\n+\/\/ original Phi.\n+\n+class SafePointScalarMergeNode: public TypeNode {\n+  int _merge_pointer_idx;         \/\/ This is the first input edge relative\n+                                  \/\/ index of a SafePoint node where metadata information relative\n+                                  \/\/ to restoring the merge is stored. The corresponding input\n+                                  \/\/ in the associated SafePoint will point to a Phi representing\n+                                  \/\/ potential non-scalar replaced objects.\n+\n+  virtual uint hash() const;\n+  virtual bool cmp( const Node &n ) const;\n+\n+public:\n+  SafePointScalarMergeNode(const TypeOopPtr* tp, int merge_pointer_idx);\n+\n+  virtual int            Opcode() const;\n+  virtual uint           ideal_reg() const;\n+  virtual const RegMask &in_RegMask(uint) const;\n+  virtual const RegMask &out_RegMask() const;\n+  virtual uint           match_edge(uint idx) const;\n+\n+  virtual uint size_of() const { return sizeof(*this); }\n+\n+  int merge_pointer_idx(JVMState* jvms) const {\n+    assert(jvms != nullptr, \"JVMS reference is null.\");\n+    return jvms->scloff() + _merge_pointer_idx;\n+  }\n+\n+  int selector_idx(JVMState* jvms) const {\n+    assert(jvms != nullptr, \"JVMS reference is null.\");\n+    return jvms->scloff() + _merge_pointer_idx + 1;\n+  }\n+\n+  \/\/ Assumes that \"this\" is an argument to a safepoint node \"s\", and that\n+  \/\/ \"new_call\" is being created to correspond to \"s\".  But the difference\n+  \/\/ between the start index of the jvmstates of \"new_call\" and \"s\" is\n+  \/\/ \"jvms_adj\".  Produce and return a SafePointScalarObjectNode that\n+  \/\/ corresponds appropriately to \"this\" in \"new_call\".  Assumes that\n+  \/\/ \"sosn_map\" is a map, specific to the translation of \"s\" to \"new_call\",\n+  \/\/ mapping old SafePointScalarObjectNodes to new, to avoid multiple copies.\n+  SafePointScalarMergeNode* clone(Dict* sosn_map, bool& new_node) const;\n+\n+#ifndef PRODUCT\n+  virtual void              dump_spec(outputStream *st) const;\n+#endif\n+};\n@@ -771,0 +854,1 @@\n+  bool is_uncommon_trap() const;\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":96,"deletions":12,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -316,0 +316,1 @@\n+macro(SafePointScalarMerge)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -535,0 +535,6 @@\n+  if (do_reduce_allocation_merges() != ReduceAllocationMerges && PrintOpto) {\n+    \/\/ Recompiling without reducing allocation merges\n+    tty->print_cr(\"*********************************************************\");\n+    tty->print_cr(\"** Bailout: Recompile without reduce allocation merges **\");\n+    tty->print_cr(\"*********************************************************\");\n+  }\n@@ -2801,1 +2807,0 @@\n-      if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n@@ -2805,0 +2810,1 @@\n+    print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n@@ -2826,2 +2832,4 @@\n-\n-        if (failing())  return;\n+\n+      ConnectionGraph::verify_ram_nodes(this, root());\n+      if (failing())  return;\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -181,0 +181,1 @@\n+  const bool _do_reduce_allocation_merges;  \/\/ Do try to reduce allocation merges.\n@@ -187,0 +188,1 @@\n+          bool do_reduce_allocation_merges,\n@@ -192,0 +194,1 @@\n+          _do_reduce_allocation_merges(do_reduce_allocation_merges),\n@@ -202,0 +205,1 @@\n+       \/* do_reduce_allocation_merges = *\/ false,\n@@ -575,0 +579,1 @@\n+  bool              do_reduce_allocation_merges() const  { return _options._do_reduce_allocation_merges; }\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -802,3 +802,4 @@\n-  GrowableArray<int>* bcis = new (C->node_arena()) GrowableArray<int>(C->node_arena(), 8, 0, -1);\n-  GrowableArray<const Type*>* extypes = new (C->node_arena()) GrowableArray<const Type*>(C->node_arena(), 8, 0, nullptr);\n-  GrowableArray<int>* saw_unloaded = new (C->node_arena()) GrowableArray<int>(C->node_arena(), 8, 0, 0);\n+  Arena tmp_mem{mtCompiler};\n+  GrowableArray<int> bcis(&tmp_mem, 8, 0, -1);\n+  GrowableArray<const Type*> extypes(&tmp_mem, 8, 0, nullptr);\n+  GrowableArray<int> saw_unloaded(&tmp_mem, 8, 0, -1);\n@@ -808,3 +809,3 @@\n-    ciExceptionHandler* h        = handlers.handler();\n-    int                 h_bci    = h->handler_bci();\n-    ciInstanceKlass*    h_klass  = h->is_catch_all() ? env()->Throwable_klass() : h->catch_klass();\n+    ciExceptionHandler* h       = handlers.handler();\n+    int                 h_bci   = h->handler_bci();\n+    ciInstanceKlass*    h_klass = h->is_catch_all() ? env()->Throwable_klass() : h->catch_klass();\n@@ -813,1 +814,1 @@\n-      if (saw_unloaded->contains(h_bci)) {\n+      if (saw_unloaded.contains(h_bci)) {\n@@ -819,1 +820,1 @@\n-        saw_unloaded->append(h_bci);\n+        saw_unloaded.append(h_bci);\n@@ -822,1 +823,1 @@\n-    const Type*         h_extype = TypeOopPtr::make_from_klass(h_klass);\n+    const Type* h_extype = TypeOopPtr::make_from_klass(h_klass);\n@@ -826,3 +827,3 @@\n-    \/\/ Note:  It's OK if the BCIs repeat themselves.\n-    bcis->append(h_bci);\n-    extypes->append(h_extype);\n+    \/\/ Note: It's OK if the BCIs repeat themselves.\n+    bcis.append(h_bci);\n+    extypes.append(h_extype);\n@@ -835,1 +836,1 @@\n-    bcis->append(-1);\n+    bcis.append(-1);\n@@ -838,1 +839,1 @@\n-    extypes->append(extype);\n+    extypes.append(extype);\n@@ -841,1 +842,1 @@\n-  int len = bcis->length();\n+  int len = bcis.length();\n@@ -852,1 +853,1 @@\n-    int handler_bci = bcis->at(i);\n+    int handler_bci = bcis.at(i);\n@@ -859,2 +860,2 @@\n-    const TypeInstPtr* extype = extypes->at(i)->is_instptr();\n-    Node *ex_oop = _gvn.transform(new CreateExNode(extypes->at(i), ctrl, i_o));\n+    const TypeInstPtr* extype = extypes.at(i)->is_instptr();\n+    Node* ex_oop = _gvn.transform(new CreateExNode(extypes.at(i), ctrl, i_o));\n@@ -863,1 +864,1 @@\n-    if (saw_unloaded->contains(handler_bci)) {\n+    if (saw_unloaded.contains(handler_bci)) {\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":20,"deletions":19,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"opto\/macro.hpp\"\n@@ -46,1 +47,6 @@\n-  _nodes(C->comp_arena(), C->unique(), C->unique(), nullptr),\n+  \/\/ If ReduceAllocationMerges is enabled we might call split_through_phi during\n+  \/\/ split_unique_types and that will create additional nodes that need to be\n+  \/\/ pushed to the ConnectionGraph. The code below bumps the initial capacity of\n+  \/\/ _nodes by 10% to account for these additional nodes. If capacity is exceeded\n+  \/\/ the array will be reallocated.\n+  _nodes(C->comp_arena(), ReduceAllocationMerges ? C->unique()*1.10 : C->unique(), C->unique(), nullptr),\n@@ -60,0 +66,1 @@\n+  set_not_scalar_replaceable(phantom_obj NOT_PRODUCT(COMMA \"Phantom object\"));\n@@ -65,0 +72,1 @@\n+  set_not_scalar_replaceable(null_obj NOT_PRODUCT(COMMA \"Null object\"));\n@@ -128,0 +136,1 @@\n+  Unique_Node_List reducible_merges;\n@@ -306,1 +315,1 @@\n-      adjust_scalar_replaceable_state(ptn);\n+      adjust_scalar_replaceable_state(ptn, reducible_merges);\n@@ -320,0 +329,9 @@\n+  \/\/ alloc_worklist will be processed in reverse push order.\n+  \/\/ Therefore the reducible Phis will be processed for last and that's what we\n+  \/\/ want because by then the scalarizable inputs of the merge will already have\n+  \/\/ an unique instance type.\n+  for (uint i = 0; i < reducible_merges.size(); i++ ) {\n+    Node* n = reducible_merges.at(i);\n+    alloc_worklist.append(n);\n+  }\n+\n@@ -373,1 +391,1 @@\n-    split_unique_types(alloc_worklist, arraycopy_worklist, mergemem_worklist);\n+    split_unique_types(alloc_worklist, arraycopy_worklist, mergemem_worklist, reducible_merges);\n@@ -393,0 +411,15 @@\n+  \/\/ 6. Remove reducible allocation merges from ideal graph\n+  if (ReduceAllocationMerges && reducible_merges.size() > 0) {\n+    bool delay = _igvn->delay_transform();\n+    _igvn->set_delay_transform(true);\n+    for (uint i = 0; i < reducible_merges.size(); i++ ) {\n+      Node* n = reducible_merges.at(i);\n+      reduce_phi(n->as_Phi());\n+      if (C->failing()) {\n+        NOT_PRODUCT(escape_state_statistics(java_objects_worklist);)\n+        return false;\n+      }\n+    }\n+    _igvn->set_delay_transform(delay);\n+  }\n+\n@@ -415,0 +448,341 @@\n+\/\/ Check if it's profitable to reduce the Phi passed as parameter.  Returns true\n+\/\/ if at least one scalar replaceable allocation participates in the merge and\n+\/\/ no input to the Phi is nullable.\n+bool ConnectionGraph::can_reduce_phi_check_inputs(PhiNode* ophi) const {\n+  \/\/ Check if there is a scalar replaceable allocate in the Phi\n+  bool found_sr_allocate = false;\n+\n+  for (uint i = 1; i < ophi->req(); i++) {\n+    \/\/ Right now we can't restore a \"null\" pointer during deoptimization\n+    const Type* inp_t = _igvn->type(ophi->in(i));\n+    if (inp_t == nullptr || inp_t->make_oopptr() == nullptr || inp_t->make_oopptr()->maybe_null()) {\n+      NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. Input %d is nullable.\", ophi->_idx, _invocation, i);)\n+      return false;\n+    }\n+\n+    \/\/ We are looking for at least one SR object in the merge\n+    JavaObjectNode* ptn = unique_java_object(ophi->in(i));\n+    if (ptn != nullptr && ptn->scalar_replaceable()) {\n+      assert(ptn->ideal_node() != nullptr && ptn->ideal_node()->is_Allocate(), \"sanity\");\n+      AllocateNode* alloc = ptn->ideal_node()->as_Allocate();\n+\n+      if (PhaseMacroExpand::can_eliminate_allocation(_igvn, alloc, nullptr)) {\n+        found_sr_allocate = true;\n+      } else {\n+        ptn->set_scalar_replaceable(false);\n+      }\n+    }\n+  }\n+\n+  NOT_PRODUCT(if (TraceReduceAllocationMerges && !found_sr_allocate) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. No SR Allocate as input.\", ophi->_idx, _invocation);)\n+  return found_sr_allocate;\n+}\n+\n+\/\/ Check if we are able to untangle the merge. Right now we only reduce Phis\n+\/\/ which are only used as debug information.\n+bool ConnectionGraph::can_reduce_phi_check_users(PhiNode* ophi) const {\n+  for (DUIterator_Fast imax, i = ophi->fast_outs(imax); i < imax; i++) {\n+    Node* use = ophi->fast_out(i);\n+\n+    if (use->is_SafePoint()) {\n+      if (use->is_Call() && use->as_Call()->has_non_debug_use(ophi)) {\n+        NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. Call has non_debug_use().\", ophi->_idx, _invocation);)\n+        return false;\n+      }\n+    } else if (use->is_AddP()) {\n+      Node* addp = use;\n+      for (DUIterator_Fast jmax, j = addp->fast_outs(jmax); j < jmax; j++) {\n+        Node* use_use = addp->fast_out(j);\n+        if (!use_use->is_Load() || !use_use->as_Load()->can_split_through_phi_base(_igvn)) {\n+          NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. AddP user isn't a [splittable] Load(): %s\", ophi->_idx, _invocation, use_use->Name());)\n+          return false;\n+        }\n+      }\n+    } else {\n+      NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. One of the uses is: %d %s\", ophi->_idx, _invocation, use->_idx, use->Name());)\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+\/\/ Returns true if: 1) It's profitable to reduce the merge, and 2) The Phi is\n+\/\/ only used in some certain code shapes. Check comments in\n+\/\/ 'can_reduce_phi_inputs' and 'can_reduce_phi_users' for more\n+\/\/ details.\n+bool ConnectionGraph::can_reduce_phi(PhiNode* ophi) const {\n+  \/\/ If there was an error attempting to reduce allocation merges for this\n+  \/\/ method we might have disabled the compilation and be retrying\n+  \/\/ with RAM disabled.\n+  if (!_compile->do_reduce_allocation_merges()) {\n+    return false;\n+  }\n+\n+  const Type* phi_t = _igvn->type(ophi);\n+  if (phi_t == nullptr || phi_t->make_ptr() == nullptr ||\n+                          phi_t->make_ptr()->isa_instptr() == nullptr ||\n+                          !phi_t->make_ptr()->isa_instptr()->klass_is_exact()) {\n+    NOT_PRODUCT(if (TraceReduceAllocationMerges) { tty->print_cr(\"Can NOT reduce Phi %d during invocation %d because it's nullable.\", ophi->_idx, _invocation); })\n+    return false;\n+  }\n+\n+  if (!can_reduce_phi_check_inputs(ophi) || !can_reduce_phi_check_users(ophi)) {\n+    return false;\n+  }\n+\n+  NOT_PRODUCT(if (TraceReduceAllocationMerges) { tty->print_cr(\"Can reduce Phi %d during invocation %d: \", ophi->_idx, _invocation); })\n+  return true;\n+}\n+\n+void ConnectionGraph::reduce_phi_on_field_access(PhiNode* ophi, GrowableArray<Node *>  &alloc_worklist) {\n+  \/\/ We'll pass this to 'split_through_phi' so that it'll do the split even\n+  \/\/ though the load doesn't have an unique instance type.\n+  bool ignore_missing_instance_id = true;\n+\n+  \/\/ Iterate over Phi outputs looking for an AddP\n+  for (int j = ophi->outcnt()-1; j >= 0;) {\n+    Node* previous_addp = ophi->raw_out(j);\n+    uint num_edges = 1;\n+    if (previous_addp->is_AddP()) {\n+      \/\/ All AddPs are present in the connection graph\n+      FieldNode* fn = ptnode_adr(previous_addp->_idx)->as_Field();\n+      num_edges = previous_addp->in(AddPNode::Address) == previous_addp->in(AddPNode::Base) ? 2 : 1;\n+\n+      \/\/ Iterate over AddP looking for a Load\n+      for (int k = previous_addp->outcnt()-1; k >= 0;) {\n+        Node* previous_load = previous_addp->raw_out(k);\n+        if (previous_load->is_Load()) {\n+          Node* data_phi = previous_load->as_Load()->split_through_phi(_igvn, ignore_missing_instance_id);\n+          _igvn->replace_node(previous_load, data_phi);\n+          assert(data_phi != nullptr, \"Output of split_through_phi is null.\");\n+          assert(data_phi != previous_load, \"Output of split_through_phi is same as input.\");\n+\n+          \/\/ Push the newly created AddP on alloc_worklist and patch\n+          \/\/ the connection graph. Note that the changes in the CG below\n+          \/\/ won't affect the ES of objects since the new nodes have the\n+          \/\/ same status as the old ones.\n+          if (data_phi != nullptr && data_phi->is_Phi()) {\n+            for (uint i = 1; i < data_phi->req(); i++) {\n+              Node* new_load = data_phi->in(i);\n+              if (new_load->is_Load()) {\n+                Node* new_addp = new_load->in(MemNode::Address);\n+                Node* base = get_addp_base(new_addp);\n+\n+                \/\/ The base might not be something that we can create an unique\n+                \/\/ type for. If that's the case we are done with that input.\n+                PointsToNode* jobj_ptn = unique_java_object(base);\n+                if (jobj_ptn == nullptr || !jobj_ptn->scalar_replaceable()) {\n+                  continue;\n+                }\n+\n+                \/\/ Push to alloc_worklist since the base has an unique_type\n+                alloc_worklist.append_if_missing(new_addp);\n+\n+                \/\/ Now let's add the node to the connection graph\n+                _nodes.at_grow(new_addp->_idx, nullptr);\n+                add_field(new_addp, fn->escape_state(), fn->offset());\n+                add_base(ptnode_adr(new_addp->_idx)->as_Field(), ptnode_adr(base->_idx));\n+\n+                \/\/ If the load doesn't load an object then it won't be\n+                \/\/ part of the connection graph\n+                PointsToNode* curr_load_ptn = ptnode_adr(previous_load->_idx);\n+                if (curr_load_ptn != nullptr) {\n+                  _nodes.at_grow(new_load->_idx, nullptr);\n+                  add_local_var(new_load, curr_load_ptn->escape_state());\n+                  add_edge(ptnode_adr(new_load->_idx), ptnode_adr(new_addp->_idx)->as_Field());\n+                }\n+              }\n+            }\n+          }\n+        }\n+        --k;\n+        k = MIN2(k, (int)previous_addp->outcnt()-1);\n+      }\n+\n+      \/\/ Remove the old AddP from the processing list because it's dead now\n+      alloc_worklist.remove_if_existing(previous_addp);\n+    }\n+    j -= num_edges;\n+    j = MIN2(j, (int)ophi->outcnt()-1);\n+  }\n+}\n+\n+\/\/ This method will create a SafePointScalarObjectNode for each combination of\n+\/\/ scalar replaceable allocation in 'ophi' and SafePoint node in 'safepoints'.\n+\/\/ The method will create a SafePointScalarMERGEnode for each combination of\n+\/\/ 'ophi' and SafePoint node in 'safepoints'.\n+\/\/ Each SafePointScalarMergeNode created here may describe multiple scalar\n+\/\/ replaced objects - check detailed description in SafePointScalarMergeNode\n+\/\/ class header.\n+\/\/\n+\/\/ This method will set entries in the Phi that are scalar replaceable to 'null'.\n+void ConnectionGraph::reduce_phi_on_safepoints(PhiNode* ophi, Unique_Node_List* safepoints) {\n+  Node* minus_one           = _igvn->register_new_node_with_optimizer(ConINode::make(-1));\n+  Node* selector            = _igvn->register_new_node_with_optimizer(PhiNode::make(ophi->region(), minus_one, TypeInt::INT));\n+  Node* null_ptr            = _igvn->makecon(TypePtr::NULL_PTR);\n+  const TypeOopPtr* merge_t = _igvn->type(ophi)->make_oopptr();\n+  uint number_of_sr_objects = 0;\n+  PhaseMacroExpand mexp(*_igvn);\n+\n+  _igvn->hash_delete(ophi);\n+\n+  \/\/ Fill in the 'selector' Phi. If index 'i' of the selector is:\n+  \/\/ -> a '-1' constant, the i'th input of the original Phi is NSR.\n+  \/\/ -> a 'x' constant >=0, the i'th input of of original Phi will be SR and the\n+  \/\/    info about the scalarized object will be at index x of\n+  \/\/    ObjectMergeValue::possible_objects\n+  for (uint i = 1; i < ophi->req(); i++) {\n+    Node* base          = ophi->in(i);\n+    JavaObjectNode* ptn = unique_java_object(base);\n+\n+    if (ptn != nullptr && ptn->scalar_replaceable()) {\n+      Node* sr_obj_idx = _igvn->register_new_node_with_optimizer(ConINode::make(number_of_sr_objects));\n+      selector->set_req(i, sr_obj_idx);\n+      number_of_sr_objects++;\n+    }\n+  }\n+\n+  \/\/ Update the debug information of all safepoints in turn\n+  for (uint spi = 0; spi < safepoints->size(); spi++) {\n+    SafePointNode* sfpt = safepoints->at(spi)->as_SafePoint();\n+    JVMState *jvms      = sfpt->jvms();\n+    uint merge_idx      = (sfpt->req() - jvms->scloff());\n+    int debug_start     = jvms->debug_start();\n+\n+    SafePointScalarMergeNode* smerge = new SafePointScalarMergeNode(merge_t, merge_idx);\n+    smerge->init_req(0, _compile->root());\n+    _igvn->register_new_node_with_optimizer(smerge);\n+\n+    \/\/ The next two inputs are:\n+    \/\/  (1) A copy of the original pointer to NSR objects.\n+    \/\/  (2) A selector, used to decide if we need to rematerialize an object\n+    \/\/      or use the pointer to a NSR object.\n+    \/\/ See more details of these fields in the declaration of SafePointScalarMergeNode\n+    sfpt->add_req(ophi);\n+    sfpt->add_req(selector);\n+\n+    for (uint i = 1; i < ophi->req(); i++) {\n+      Node* base          = ophi->in(i);\n+      JavaObjectNode* ptn = unique_java_object(base);\n+\n+      \/\/ If the base is not scalar replaceable we don't need to register information about\n+      \/\/ it at this time.\n+      if (ptn == nullptr || !ptn->scalar_replaceable()) {\n+        continue;\n+      }\n+\n+      AllocateNode* alloc = ptn->ideal_node()->as_Allocate();\n+      Unique_Node_List value_worklist;\n+      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt, &value_worklist);\n+      guarantee(value_worklist.size() == 0, \"Unimplemented: Valhalla support for 8287061\");\n+      if (sobj == nullptr) {\n+        _compile->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n+        return;\n+      }\n+\n+      \/\/ Now make a pass over the debug information replacing any references\n+      \/\/ to the allocated object with \"sobj\"\n+      Node* ccpp = alloc->result_cast();\n+      sfpt->replace_edges_in_range(ccpp, sobj, debug_start, jvms->debug_end(), _igvn);\n+\n+      \/\/ Register the scalarized object as a candidate for reallocation\n+      smerge->add_req(sobj);\n+    }\n+\n+    \/\/ Replaces debug information references to \"ophi\" in \"sfpt\" with references to \"smerge\"\n+    sfpt->replace_edges_in_range(ophi, smerge, debug_start, jvms->debug_end(), _igvn);\n+\n+    \/\/ The call to 'replace_edges_in_range' above might have removed the\n+    \/\/ reference to ophi that we need at _merge_pointer_idx. The line below make\n+    \/\/ sure the reference is maintained.\n+    sfpt->set_req(smerge->merge_pointer_idx(jvms), ophi);\n+    _igvn->_worklist.push(sfpt);\n+  }\n+\n+  \/\/ Now we can change ophi since we don't need to know the types\n+  \/\/ of the input allocations anymore.\n+  const Type* new_t = merge_t->meet(TypePtr::NULL_PTR);\n+  Node* new_phi = _igvn->register_new_node_with_optimizer(PhiNode::make(ophi->region(), null_ptr, new_t));\n+  for (uint i = 1; i < ophi->req(); i++) {\n+    Node* base          = ophi->in(i);\n+    JavaObjectNode* ptn = unique_java_object(base);\n+\n+    if (ptn != nullptr && ptn->scalar_replaceable()) {\n+      new_phi->set_req(i, null_ptr);\n+    } else {\n+      new_phi->set_req(i, ophi->in(i));\n+    }\n+  }\n+\n+  _igvn->replace_node(ophi, new_phi);\n+  _igvn->hash_insert(ophi);\n+  _igvn->_worklist.push(ophi);\n+}\n+\n+void ConnectionGraph::reduce_phi(PhiNode* ophi) {\n+  Unique_Node_List safepoints;\n+\n+  for (uint i = 0; i < ophi->outcnt(); i++) {\n+    Node* use = ophi->raw_out(i);\n+\n+    \/\/ All SafePoint nodes using the same Phi node use the same debug\n+    \/\/ information (regarding the Phi). Furthermore, reducing the Phi used by a\n+    \/\/ SafePoint requires changing the Phi. Therefore, I collect all safepoints\n+    \/\/ and patch them all at once later.\n+    if (use->is_SafePoint()) {\n+      safepoints.push(use->as_SafePoint());\n+    } else {\n+      assert(false, \"Unexpected use of reducible Phi.\");\n+    }\n+  }\n+\n+  if (safepoints.size() > 0) {\n+    reduce_phi_on_safepoints(ophi, &safepoints);\n+  }\n+}\n+\n+void ConnectionGraph::verify_ram_nodes(Compile* C, Node* root) {\n+  Unique_Node_List ideal_nodes;\n+\n+  ideal_nodes.map(C->live_nodes(), nullptr);  \/\/ preallocate space\n+  ideal_nodes.push(root);\n+\n+  for (uint next = 0; next < ideal_nodes.size(); ++next) {\n+    Node* n = ideal_nodes.at(next);\n+\n+    if (n->is_SafePointScalarMerge()) {\n+      SafePointScalarMergeNode* merge = n->as_SafePointScalarMerge();\n+\n+      \/\/ Validate inputs of merge\n+      for (uint i = 1; i < merge->req(); i++) {\n+        if (merge->in(i) != nullptr && !merge->in(i)->is_top() && !merge->in(i)->is_SafePointScalarObject()) {\n+          assert(false, \"SafePointScalarMerge inputs should be null\/top or SafePointScalarObject.\");\n+          C->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n+        }\n+      }\n+\n+      \/\/ Validate users of merge\n+      for (DUIterator_Fast imax, i = merge->fast_outs(imax); i < imax; i++) {\n+        Node* sfpt = merge->fast_out(i);\n+        if (sfpt->is_SafePoint()) {\n+          int merge_idx = merge->merge_pointer_idx(sfpt->as_SafePoint()->jvms());\n+\n+          if (sfpt->in(merge_idx) != nullptr && sfpt->in(merge_idx)->is_SafePointScalarMerge()) {\n+            assert(false, \"SafePointScalarMerge nodes can't be nested.\");\n+            C->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n+          }\n+        } else {\n+          assert(false, \"Only safepoints can use SafePointScalarMerge nodes.\");\n+          C->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n+        }\n+      }\n+    }\n+\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* m = n->fast_out(i);\n+      ideal_nodes.push(m);\n+    }\n+  }\n+}\n+\n@@ -606,1 +980,2 @@\n-      add_java_object(n, es);\n+      PointsToNode* ptn_con = add_java_object(n, es);\n+      set_not_scalar_replaceable(ptn_con NOT_PRODUCT(COMMA \"Constant pointer\"));\n@@ -698,1 +1073,2 @@\n-      add_java_object(n, PointsToNode::ArgEscape);\n+      PointsToNode* ptn_thr = add_java_object(n, PointsToNode::ArgEscape);\n+      set_not_scalar_replaceable(ptn_thr NOT_PRODUCT(COMMA \"Constant pointer\"));\n@@ -1078,0 +1454,3 @@\n+      if (es == PointsToNode::GlobalEscape) {\n+        set_not_scalar_replaceable(ptnode_adr(call->_idx) NOT_PRODUCT(COMMA \"object can be loaded from boxing cache\"));\n+      }\n@@ -1907,1 +2286,9 @@\n-void ConnectionGraph::adjust_scalar_replaceable_state(JavaObjectNode* jobj) {\n+void ConnectionGraph::adjust_scalar_replaceable_state(JavaObjectNode* jobj, Unique_Node_List &reducible_merges) {\n+  \/\/ A Phi 'x' is a _candidate_ to be reducible if 'can_reduce_phi(x)'\n+  \/\/ returns true. If one of the constraints in this method set 'jobj' to NSR\n+  \/\/ then the candidate Phi is discarded. If the Phi has another SR 'jobj' as\n+  \/\/ input, 'adjust_scalar_replaceable_state' will eventually be called with\n+  \/\/ that other object and the Phi will become a reducible Phi.\n+  \/\/ There could be multiple merges involving the same jobj.\n+  Unique_Node_List candidates;\n+\n@@ -1942,1 +2329,2 @@\n-    \/\/ 3. An object is not scalar replaceable if it is merged with other objects.\n+    \/\/ 3. An object is not scalar replaceable if it is merged with other objects\n+    \/\/ and we can't remove the merge\n@@ -1946,3 +2334,17 @@\n-        \/\/ Mark all objects.\n-        set_not_scalar_replaceable(jobj NOT_PRODUCT(COMMA trace_merged_message(ptn)));\n-        set_not_scalar_replaceable(ptn NOT_PRODUCT(COMMA trace_merged_message(jobj)));\n+        Node* use_n = use->ideal_node();\n+\n+        \/\/ If it's already a candidate or confirmed reducible merge we can skip verification\n+        if (candidates.member(use_n)) {\n+          continue;\n+        } else if (reducible_merges.member(use_n)) {\n+          candidates.push(use_n);\n+          continue;\n+        }\n+\n+        if (ReduceAllocationMerges && use_n->is_Phi() && can_reduce_phi(use_n->as_Phi())) {\n+          candidates.push(use_n);\n+        } else {\n+          \/\/ Mark all objects as NSR if we can't remove the merge\n+          set_not_scalar_replaceable(jobj NOT_PRODUCT(COMMA trace_merged_message(ptn)));\n+          set_not_scalar_replaceable(ptn NOT_PRODUCT(COMMA trace_merged_message(jobj)));\n+        }\n@@ -2011,1 +2413,1 @@\n-    if (field->base_count() > 1) {\n+    if (field->base_count() > 1 && candidates.size() == 0) {\n@@ -2023,0 +2425,4 @@\n+\n+      if (!jobj->scalar_replaceable()) {\n+        return;\n+      }\n@@ -2025,0 +2431,9 @@\n+\n+  \/\/ The candidate is truly a reducible merge only if none of the other\n+  \/\/ constraints ruled it as NSR. There could be multiple merges involving the\n+  \/\/ same jobj.\n+  assert(jobj->scalar_replaceable(), \"sanity\");\n+  for (uint i = 0; i < candidates.size(); i++ ) {\n+    Node* candidate = candidates.at(i);\n+    reducible_merges.push(candidate);\n+  }\n@@ -2296,1 +2711,1 @@\n-void ConnectionGraph::add_java_object(Node *n, PointsToNode::EscapeState es) {\n+PointsToNode* ConnectionGraph::add_java_object(Node *n, PointsToNode::EscapeState es) {\n@@ -2300,1 +2715,1 @@\n-    return;\n+    return ptadr;\n@@ -2305,0 +2720,1 @@\n+  return ptadr;\n@@ -2402,2 +2818,1 @@\n-JavaObjectNode* ConnectionGraph::unique_java_object(Node *n) {\n-  assert(!_collecting, \"should not call when constructed graph\");\n+JavaObjectNode* ConnectionGraph::unique_java_object(Node *n) const {\n@@ -3258,1 +3673,2 @@\n-                                         GrowableArray<MergeMemNode*> &mergemem_worklist) {\n+                                         GrowableArray<MergeMemNode*> &mergemem_worklist,\n+                                         Unique_Node_List &reducible_merges) {\n@@ -3405,1 +3821,6 @@\n-      JavaObjectNode* jobj = unique_java_object(get_addp_base(n));\n+      Node* addp_base = get_addp_base(n);\n+      if (addp_base != nullptr && reducible_merges.member(addp_base)) {\n+        \/\/ This AddP will go away when we reduce the the Phi\n+        continue;\n+      }\n+      JavaObjectNode* jobj = unique_java_object(addp_base);\n@@ -3426,0 +3847,6 @@\n+      \/\/ Reducible Phi's will be removed from the graph after split_unique_types finishes\n+      if (reducible_merges.member(n)) {\n+        \/\/ Split loads through phi\n+        reduce_phi_on_field_access(n->as_Phi(), alloc_worklist);\n+        continue;\n+      }\n@@ -3578,1 +4005,0 @@\n-  assert(unique_old == _compile->unique(), \"there should be no new ideal nodes after Phase 1\");\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":444,"deletions":18,"binary":false,"changes":462,"status":"modified"},{"patch":"@@ -263,1 +263,0 @@\n-#ifdef ASSERT\n@@ -265,1 +264,0 @@\n-#endif\n","filename":"src\/hotspot\/share\/opto\/inlinetypenode.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -502,0 +502,1 @@\n+  case vmIntrinsics::_jvm_commit:               return inline_native_jvm_commit();\n@@ -3220,0 +3221,130 @@\n+\/\/------------------------inline_native_jvm_commit------------------\n+bool LibraryCallKit::inline_native_jvm_commit() {\n+  enum { _true_path = 1, _false_path = 2, PATH_LIMIT };\n+\n+  \/\/ Save input memory and i_o state.\n+  Node* input_memory_state = reset_memory();\n+  set_all_memory(input_memory_state);\n+  Node* input_io_state = i_o();\n+\n+  \/\/ TLS.\n+  Node* tls_ptr = _gvn.transform(new ThreadLocalNode());\n+  \/\/ Jfr java buffer.\n+  Node* java_buffer_offset = _gvn.transform(new AddPNode(top(), tls_ptr, _gvn.transform(MakeConX(in_bytes(JAVA_BUFFER_OFFSET_JFR)))));\n+  Node* java_buffer = _gvn.transform(new LoadPNode(control(), input_memory_state, java_buffer_offset, TypePtr::BOTTOM, TypeRawPtr::NOTNULL, MemNode::unordered));\n+  Node* java_buffer_pos_offset = _gvn.transform(new AddPNode(top(), java_buffer, _gvn.transform(MakeConX(in_bytes(JFR_BUFFER_POS_OFFSET)))));\n+\n+  \/\/ Load the current value of the notified field in the JfrThreadLocal.\n+  Node* notified_offset = basic_plus_adr(top(), tls_ptr, in_bytes(NOTIFY_OFFSET_JFR));\n+  Node* notified = make_load(control(), notified_offset, TypeInt::BOOL, T_BOOLEAN, MemNode::unordered);\n+\n+  \/\/ Test for notification.\n+  Node* notified_cmp = _gvn.transform(new CmpINode(notified, _gvn.intcon(1)));\n+  Node* test_notified = _gvn.transform(new BoolNode(notified_cmp, BoolTest::eq));\n+  IfNode* iff_notified = create_and_map_if(control(), test_notified, PROB_MIN, COUNT_UNKNOWN);\n+\n+  \/\/ True branch, is notified.\n+  Node* is_notified = _gvn.transform(new IfTrueNode(iff_notified));\n+  set_control(is_notified);\n+\n+  \/\/ Reset notified state.\n+  Node* notified_reset_memory = store_to_memory(control(), notified_offset, _gvn.intcon(0), T_BOOLEAN, Compile::AliasIdxRaw, MemNode::unordered);\n+\n+  \/\/ Iff notified, the return address of the commit method is the current position of the backing java buffer. This is used to reset the event writer.\n+  Node* current_pos_X = _gvn.transform(new LoadXNode(control(), input_memory_state, java_buffer_pos_offset, TypeRawPtr::NOTNULL, TypeX_X, MemNode::unordered));\n+  \/\/ Convert the machine-word to a long.\n+  Node* current_pos = _gvn.transform(ConvX2L(current_pos_X));\n+\n+  \/\/ False branch, not notified.\n+  Node* not_notified = _gvn.transform(new IfFalseNode(iff_notified));\n+  set_control(not_notified);\n+  set_all_memory(input_memory_state);\n+\n+  \/\/ Arg is the next position as a long.\n+  Node* arg = argument(0);\n+  \/\/ Convert long to machine-word.\n+  Node* next_pos_X = _gvn.transform(ConvL2X(arg));\n+\n+  \/\/ Store the next_position to the underlying jfr java buffer.\n+  Node* commit_memory;\n+#ifdef _LP64\n+  commit_memory = store_to_memory(control(), java_buffer_pos_offset, next_pos_X, T_LONG, Compile::AliasIdxRaw, MemNode::release);\n+#else\n+  commit_memory = store_to_memory(control(), java_buffer_pos_offset, next_pos_X, T_INT, Compile::AliasIdxRaw, MemNode::release);\n+#endif\n+\n+  \/\/ Now load the flags from off the java buffer and decide if the buffer is a lease. If so, it needs to be returned post-commit.\n+  Node* java_buffer_flags_offset = _gvn.transform(new AddPNode(top(), java_buffer, _gvn.transform(MakeConX(in_bytes(JFR_BUFFER_FLAGS_OFFSET)))));\n+  Node* flags = make_load(control(), java_buffer_flags_offset, TypeInt::UBYTE, T_BYTE, MemNode::unordered);\n+  Node* lease_constant = _gvn.transform(_gvn.intcon(4));\n+\n+  \/\/ And flags with lease constant.\n+  Node* lease = _gvn.transform(new AndINode(flags, lease_constant));\n+\n+  \/\/ Branch on lease to conditionalize returning the leased java buffer.\n+  Node* lease_cmp = _gvn.transform(new CmpINode(lease, lease_constant));\n+  Node* test_lease = _gvn.transform(new BoolNode(lease_cmp, BoolTest::eq));\n+  IfNode* iff_lease = create_and_map_if(control(), test_lease, PROB_MIN, COUNT_UNKNOWN);\n+\n+  \/\/ False branch, not a lease.\n+  Node* not_lease = _gvn.transform(new IfFalseNode(iff_lease));\n+\n+  \/\/ True branch, is lease.\n+  Node* is_lease = _gvn.transform(new IfTrueNode(iff_lease));\n+  set_control(is_lease);\n+\n+  \/\/ Make a runtime call, which can safepoint, to return the leased buffer. This updates both the JfrThreadLocal and the Java event writer oop.\n+  Node* call_return_lease = make_runtime_call(RC_NO_LEAF,\n+                                              OptoRuntime::void_void_Type(),\n+                                              StubRoutines::jfr_return_lease(),\n+                                              \"return_lease\", TypePtr::BOTTOM);\n+  Node* call_return_lease_control = _gvn.transform(new ProjNode(call_return_lease, TypeFunc::Control));\n+\n+  RegionNode* lease_compare_rgn = new RegionNode(PATH_LIMIT);\n+  record_for_igvn(lease_compare_rgn);\n+  PhiNode* lease_compare_mem = new PhiNode(lease_compare_rgn, Type::MEMORY, TypePtr::BOTTOM);\n+  record_for_igvn(lease_compare_mem);\n+  PhiNode* lease_compare_io = new PhiNode(lease_compare_rgn, Type::ABIO);\n+  record_for_igvn(lease_compare_io);\n+  PhiNode* lease_result_value = new PhiNode(lease_compare_rgn, TypeLong::LONG);\n+  record_for_igvn(lease_result_value);\n+\n+  \/\/ Update control and phi nodes.\n+  lease_compare_rgn->init_req(_true_path, call_return_lease_control);\n+  lease_compare_rgn->init_req(_false_path, not_lease);\n+\n+  lease_compare_mem->init_req(_true_path, _gvn.transform(reset_memory()));\n+  lease_compare_mem->init_req(_false_path, commit_memory);\n+\n+  lease_compare_io->init_req(_true_path, i_o());\n+  lease_compare_io->init_req(_false_path, input_io_state);\n+\n+  lease_result_value->init_req(_true_path, null()); \/\/ if the lease was returned, return 0.\n+  lease_result_value->init_req(_false_path, arg); \/\/ if not lease, return new updated position.\n+\n+  RegionNode* result_rgn = new RegionNode(PATH_LIMIT);\n+  PhiNode* result_mem = new PhiNode(result_rgn, Type::MEMORY, TypePtr::BOTTOM);\n+  PhiNode* result_io = new PhiNode(result_rgn, Type::ABIO);\n+  PhiNode* result_value = new PhiNode(result_rgn, TypeLong::LONG);\n+\n+  \/\/ Update control and phi nodes.\n+  result_rgn->init_req(_true_path, is_notified);\n+  result_rgn->init_req(_false_path, _gvn.transform(lease_compare_rgn));\n+\n+  result_mem->init_req(_true_path, notified_reset_memory);\n+  result_mem->init_req(_false_path, _gvn.transform(lease_compare_mem));\n+\n+  result_io->init_req(_true_path, input_io_state);\n+  result_io->init_req(_false_path, _gvn.transform(lease_compare_io));\n+\n+  result_value->init_req(_true_path, current_pos);\n+  result_value->init_req(_false_path, _gvn.transform(lease_result_value));\n+\n+  \/\/ Set output state.\n+  set_control(_gvn.transform(result_rgn));\n+  set_all_memory(_gvn.transform(result_mem));\n+  set_i_o(_gvn.transform(result_io));\n+  set_result(result_rgn, result_value);\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":131,"deletions":0,"binary":false,"changes":131,"status":"modified"},{"patch":"@@ -274,0 +274,1 @@\n+  bool inline_native_jvm_commit();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -75,7 +75,6 @@\n-         IsMultiversioned      = 1<<12,\n-         StripMined            = 1<<13,\n-         SubwordLoop           = 1<<14,\n-         ProfileTripFailed     = 1<<15,\n-         LoopNestInnerLoop     = 1<<16,\n-         LoopNestLongOuterLoop = 1<<17,\n-         FlattenedArrays       = 1<<18};\n+         StripMined            = 1<<12,\n+         SubwordLoop           = 1<<13,\n+         ProfileTripFailed     = 1<<14,\n+         LoopNestInnerLoop     = 1<<15,\n+         LoopNestLongOuterLoop = 1<<16,\n+         FlattenedArrays       = 1<<17};\n@@ -84,2 +83,0 @@\n-  char _postloop_flags;\n-  enum { RCEPostLoop = 1 };\n@@ -97,1 +94,0 @@\n-  bool is_multiversioned() const { return _loop_flags & IsMultiversioned; }\n@@ -115,1 +111,0 @@\n-  void mark_is_multiversioned() { _loop_flags |= IsMultiversioned; }\n@@ -127,3 +122,0 @@\n-  int is_rce_post_loop() const { return _postloop_flags & RCEPostLoop; }\n-  void set_is_rce_post_loop() { _postloop_flags |= RCEPostLoop; }\n-\n@@ -140,1 +132,1 @@\n-      _postloop_flags(0), _profile_trip_cnt(COUNT_UNKNOWN)  {\n+      _profile_trip_cnt(COUNT_UNKNOWN) {\n@@ -328,2 +320,0 @@\n-  void set_slp_pack_count(int pack_count)    { _slp_vector_pack_count = pack_count; }\n-  int  slp_pack_count() const                { return _slp_vector_pack_count; }\n@@ -1311,3 +1301,0 @@\n-  \/\/ Add an RCE'd post loop which we will multi-version adapt for run time test path usage\n-  void insert_scalar_rced_post_loop( IdealLoopTree *loop, Node_List &old_new );\n-\n@@ -1408,7 +1395,0 @@\n-  \/\/ Process post loops which have range checks and try to build a multi-version\n-  \/\/ guard to safely determine if we can execute the post loop which was RCE'd.\n-  bool multi_version_post_loops(IdealLoopTree *rce_loop, IdealLoopTree *legacy_loop);\n-\n-  \/\/ Cause the rce'd post loop to optimized away, this happens if we cannot complete multiverioning\n-  void poison_rce_post_loop(IdealLoopTree *rce_loop);\n-\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":7,"deletions":27,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -1872,1 +1872,7 @@\n-                register_new_node(cast, x_ctrl);\n+                Node* prev = _igvn.hash_find_insert(cast);\n+                if (prev != nullptr) {\n+                  cast->destruct(&_igvn);\n+                  cast = prev;\n+                } else {\n+                  register_new_node(cast, x_ctrl);\n+                }\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -279,1 +279,1 @@\n-    Node* adr = _igvn.transform(new AddPNode(base, base, MakeConX(offset)));\n+    Node* adr = _igvn.transform(new AddPNode(base, base, _igvn.MakeConX(offset)));\n@@ -301,1 +301,1 @@\n-        adr = _igvn.transform(new AddPNode(base, base, MakeConX(off)));\n+        adr = _igvn.transform(new AddPNode(base, base, _igvn.MakeConX(off)));\n@@ -318,1 +318,1 @@\n-        diff = _igvn.transform(new LShiftXNode(diff, intcon(shift)));\n+        diff = _igvn.transform(new LShiftXNode(diff, _igvn.intcon(shift)));\n@@ -320,1 +320,1 @@\n-        Node* off = _igvn.transform(new AddXNode(MakeConX(offset), diff));\n+        Node* off = _igvn.transform(new AddXNode(_igvn.MakeConX(offset), diff));\n@@ -610,1 +610,1 @@\n-bool PhaseMacroExpand::can_eliminate_allocation(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints) {\n+bool PhaseMacroExpand::can_eliminate_allocation(PhaseIterGVN* igvn, AllocateNode *alloc, GrowableArray <SafePointNode *>* safepoints) {\n@@ -615,1 +615,2 @@\n-  bool  can_eliminate = true;\n+  bool can_eliminate = true;\n+  bool reduce_merge_precheck = (safepoints == nullptr);\n@@ -627,1 +628,1 @@\n-    res_type = _igvn.type(res)->isa_oopptr();\n+    res_type = igvn->type(res)->isa_oopptr();\n@@ -647,1 +648,1 @@\n-        const TypePtr* addp_type = _igvn.type(use)->is_ptr();\n+        const TypePtr* addp_type = igvn->type(use)->is_ptr();\n@@ -688,2 +689,2 @@\n-        } else {\n-          safepoints.append_if_missing(sfpt);\n+        } else if (!reduce_merge_precheck) {\n+          safepoints->append_if_missing(sfpt);\n@@ -712,0 +713,2 @@\n+      } else if (reduce_merge_precheck && (use->is_Phi() || use->is_EncodeP() || use->Opcode() == Op_MemBarRelease)) {\n+        \/\/ Nothing to do\n@@ -737,1 +740,1 @@\n-  if (PrintEliminateAllocations) {\n+  if (PrintEliminateAllocations && safepoints != nullptr) {\n@@ -762,11 +765,1 @@\n-\/\/ Do scalar replacement.\n-bool PhaseMacroExpand::scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints) {\n-  GrowableArray <SafePointNode *> safepoints_done;\n-\n-  ciInstanceKlass* iklass = nullptr;\n-  int nfields = 0;\n-  int array_base = 0;\n-  int element_size = 0;\n-  BasicType basic_elem_type = T_ILLEGAL;\n-  const Type* field_type = nullptr;\n-\n+void PhaseMacroExpand::undo_previous_scalarizations(GrowableArray <SafePointNode *> safepoints_done, AllocateNode* alloc) {\n@@ -774,0 +767,1 @@\n+  int nfields = 0;\n@@ -775,0 +769,51 @@\n+\n+  if (res != nullptr) {\n+    const TypeOopPtr* res_type = _igvn.type(res)->isa_oopptr();\n+\n+    if (res_type->isa_instptr()) {\n+      \/\/ find the fields of the class which will be needed for safepoint debug information\n+      ciInstanceKlass* iklass = res_type->is_instptr()->instance_klass();\n+      nfields = iklass->nof_nonstatic_fields();\n+    } else {\n+      \/\/ find the array's elements which will be needed for safepoint debug information\n+      nfields = alloc->in(AllocateNode::ALength)->find_int_con(-1);\n+      assert(nfields >= 0, \"must be an array klass.\");\n+    }\n+  }\n+\n+  \/\/ rollback processed safepoints\n+  while (safepoints_done.length() > 0) {\n+    SafePointNode* sfpt_done = safepoints_done.pop();\n+    \/\/ remove any extra entries we added to the safepoint\n+    uint last = sfpt_done->req() - 1;\n+    for (int k = 0;  k < nfields; k++) {\n+      sfpt_done->del_req(last--);\n+    }\n+    JVMState *jvms = sfpt_done->jvms();\n+    jvms->set_endoff(sfpt_done->req());\n+    \/\/ Now make a pass over the debug information replacing any references\n+    \/\/ to SafePointScalarObjectNode with the allocated object.\n+    int start = jvms->debug_start();\n+    int end   = jvms->debug_end();\n+    for (int i = start; i < end; i++) {\n+      if (sfpt_done->in(i)->is_SafePointScalarObject()) {\n+        SafePointScalarObjectNode* scobj = sfpt_done->in(i)->as_SafePointScalarObject();\n+        if (scobj->first_index(jvms) == sfpt_done->req() &&\n+            scobj->n_fields() == (uint)nfields) {\n+          assert(scobj->alloc() == alloc, \"sanity\");\n+          sfpt_done->set_req(i, res);\n+        }\n+      }\n+    }\n+    _igvn._worklist.push(sfpt_done);\n+  }\n+}\n+\n+SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt,\n+                                                                                  Unique_Node_List* value_worklist) {\n+  \/\/ Fields of scalar objs are referenced only at the end\n+  \/\/ of regular debuginfo at the last (youngest) JVMS.\n+  \/\/ Record relative start index.\n+  ciInstanceKlass* iklass    = nullptr;\n+  BasicType basic_elem_type  = T_ILLEGAL;\n+  const Type* field_type     = nullptr;\n@@ -776,0 +821,9 @@\n+  int nfields                = 0;\n+  int array_base             = 0;\n+  int element_size           = 0;\n+  uint first_ind             = (sfpt->req() - sfpt->jvms()->scloff());\n+  Node* res                  = alloc->result_cast();\n+\n+  assert(res == nullptr || res->is_CheckCastPP(), \"unexpected AllocateNode result\");\n+  assert(sfpt->jvms() != nullptr, \"missed JVMS\");\n+\n@@ -778,2 +832,0 @@\n-  }\n-  if (res != nullptr) {\n@@ -799,50 +851,26 @@\n-  \/\/\n-  \/\/ Process the safepoint uses\n-  \/\/\n-  assert(safepoints.length() == 0 || !res_type->is_inlinetypeptr(), \"Inline type allocations should not have safepoint uses\");\n-  Unique_Node_List value_worklist;\n-  while (safepoints.length() > 0) {\n-    SafePointNode* sfpt = safepoints.pop();\n-    Node* mem = sfpt->memory();\n-    Node* ctl = sfpt->control();\n-    assert(sfpt->jvms() != nullptr, \"missed JVMS\");\n-    \/\/ Fields of scalar objs are referenced only at the end\n-    \/\/ of regular debuginfo at the last (youngest) JVMS.\n-    \/\/ Record relative start index.\n-    uint first_ind = (sfpt->req() - sfpt->jvms()->scloff());\n-    SafePointScalarObjectNode* sobj = new SafePointScalarObjectNode(res_type,\n-#ifdef ASSERT\n-                                                 alloc,\n-#endif\n-                                                 first_ind, nfields);\n-    sobj->init_req(0, C->root());\n-    transform_later(sobj);\n-\n-    \/\/ Scan object's fields adding an input to the safepoint for each field.\n-    for (int j = 0; j < nfields; j++) {\n-      intptr_t offset;\n-      ciField* field = nullptr;\n-      if (iklass != nullptr) {\n-        field = iklass->nonstatic_field_at(j);\n-        offset = field->offset_in_bytes();\n-        ciType* elem_type = field->type();\n-        basic_elem_type = field->layout_type();\n-        assert(!field->is_flattened(), \"flattened inline type fields should not have safepoint uses\");\n-\n-        \/\/ The next code is taken from Parse::do_get_xxx().\n-        if (is_reference_type(basic_elem_type)) {\n-          if (!elem_type->is_loaded()) {\n-            field_type = TypeInstPtr::BOTTOM;\n-          } else if (field != nullptr && field->is_static_constant()) {\n-            ciObject* con = field->constant_value().as_object();\n-            \/\/ Do not \"join\" in the previous type; it doesn't add value,\n-            \/\/ and may yield a vacuous result if the field is of interface type.\n-            field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n-            assert(field_type != nullptr, \"field singleton type must be consistent\");\n-          } else {\n-            field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n-          }\n-          if (UseCompressedOops) {\n-            field_type = field_type->make_narrowoop();\n-            basic_elem_type = T_NARROWOOP;\n-          }\n+\n+  SafePointScalarObjectNode* sobj = new SafePointScalarObjectNode(res_type, alloc, first_ind, nfields);\n+  sobj->init_req(0, C->root());\n+  transform_later(sobj);\n+\n+  \/\/ Scan object's fields adding an input to the safepoint for each field.\n+  for (int j = 0; j < nfields; j++) {\n+    intptr_t offset;\n+    ciField* field = nullptr;\n+    if (iklass != nullptr) {\n+      field = iklass->nonstatic_field_at(j);\n+      offset = field->offset_in_bytes();\n+      ciType* elem_type = field->type();\n+      basic_elem_type = field->layout_type();\n+      assert(!field->is_flattened(), \"flattened inline type fields should not have safepoint uses\");\n+\n+      \/\/ The next code is taken from Parse::do_get_xxx().\n+      if (is_reference_type(basic_elem_type)) {\n+        if (!elem_type->is_loaded()) {\n+          field_type = TypeInstPtr::BOTTOM;\n+        } else if (field != nullptr && field->is_static_constant()) {\n+          ciObject* con = field->constant_value().as_object();\n+          \/\/ Do not \"join\" in the previous type; it doesn't add value,\n+          \/\/ and may yield a vacuous result if the field is of interface type.\n+          field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n+          assert(field_type != nullptr, \"field singleton type must be consistent\");\n@@ -850,1 +878,5 @@\n-          field_type = Type::get_const_basic_type(basic_elem_type);\n+          field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+        }\n+        if (UseCompressedOops) {\n+          field_type = field_type->make_narrowoop();\n+          basic_elem_type = T_NARROWOOP;\n@@ -853,1 +885,1 @@\n-        offset = array_base + j * (intptr_t)element_size;\n+        field_type = Type::get_const_basic_type(basic_elem_type);\n@@ -855,0 +887,3 @@\n+    } else {\n+      offset = array_base + j * (intptr_t)element_size;\n+    }\n@@ -856,8 +891,16 @@\n-      Node* field_val = nullptr;\n-      const TypeOopPtr* field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n-      if (res_type->is_flat()) {\n-        ciInlineKlass* vk = res_type->is_aryptr()->elem()->inline_klass();\n-        assert(vk->flatten_array(), \"must be flattened\");\n-        field_val = inline_type_from_mem(mem, ctl, vk, field_addr_type->isa_aryptr(), 0, alloc);\n-      } else {\n-        field_val = value_from_mem(mem, ctl, basic_elem_type, field_type, field_addr_type, alloc);\n+    Node* field_val = nullptr;\n+    const TypeOopPtr* field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n+    if (res_type->is_flat()) {\n+      ciInlineKlass* vk = res_type->is_aryptr()->elem()->inline_klass();\n+      assert(vk->flatten_array(), \"must be flattened\");\n+      field_val = inline_type_from_mem(sfpt->memory(), sfpt->control(), vk, field_addr_type->isa_aryptr(), 0, alloc);\n+    } else {\n+      field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n+    }\n+\n+    \/\/ We weren't able to find a value for this field,\n+    \/\/ give up on eliminating this allocation.\n+    if (field_val == nullptr) {\n+      uint last = sfpt->req() - 1;\n+      for (int k = 0;  k < j; k++) {\n+        sfpt->del_req(last--);\n@@ -865,36 +908,2 @@\n-      if (field_val == nullptr) {\n-        \/\/ We weren't able to find a value for this field,\n-        \/\/ give up on eliminating this allocation.\n-\n-        \/\/ Remove any extra entries we added to the safepoint.\n-        uint last = sfpt->req() - 1;\n-        for (int k = 0;  k < j; k++) {\n-          sfpt->del_req(last--);\n-        }\n-        _igvn._worklist.push(sfpt);\n-        \/\/ rollback processed safepoints\n-        while (safepoints_done.length() > 0) {\n-          SafePointNode* sfpt_done = safepoints_done.pop();\n-          \/\/ remove any extra entries we added to the safepoint\n-          last = sfpt_done->req() - 1;\n-          for (int k = 0;  k < nfields; k++) {\n-            sfpt_done->del_req(last--);\n-          }\n-          JVMState *jvms = sfpt_done->jvms();\n-          jvms->set_endoff(sfpt_done->req());\n-          \/\/ Now make a pass over the debug information replacing any references\n-          \/\/ to SafePointScalarObjectNode with the allocated object.\n-          int start = jvms->debug_start();\n-          int end   = jvms->debug_end();\n-          for (int i = start; i < end; i++) {\n-            if (sfpt_done->in(i)->is_SafePointScalarObject()) {\n-              SafePointScalarObjectNode* scobj = sfpt_done->in(i)->as_SafePointScalarObject();\n-              if (scobj->first_index(jvms) == sfpt_done->req() &&\n-                  scobj->n_fields() == (uint)nfields) {\n-                assert(scobj->alloc() == alloc, \"sanity\");\n-                sfpt_done->set_req(i, res);\n-              }\n-            }\n-          }\n-          _igvn._worklist.push(sfpt_done);\n-        }\n+      _igvn._worklist.push(sfpt);\n+\n@@ -902,16 +911,8 @@\n-        if (PrintEliminateAllocations) {\n-          if (field != nullptr) {\n-            tty->print(\"=== At SafePoint node %d can't find value of Field: \",\n-                       sfpt->_idx);\n-            field->print();\n-            int field_idx = C->get_alias_index(field_addr_type);\n-            tty->print(\" (alias_idx=%d)\", field_idx);\n-          } else { \/\/ Array's element\n-            tty->print(\"=== At SafePoint node %d can't find value of array element [%d]\",\n-                       sfpt->_idx, j);\n-          }\n-          tty->print(\", which prevents elimination of: \");\n-          if (res == nullptr)\n-            alloc->dump();\n-          else\n-            res->dump();\n+      if (PrintEliminateAllocations) {\n+        if (field != nullptr) {\n+          tty->print(\"=== At SafePoint node %d can't find value of field: \", sfpt->_idx);\n+          field->print();\n+          int field_idx = C->get_alias_index(field_addr_type);\n+          tty->print(\" (alias_idx=%d)\", field_idx);\n+        } else { \/\/ Array's element\n+          tty->print(\"=== At SafePoint node %d can't find value of array element [%d]\", sfpt->_idx, j);\n@@ -919,2 +920,5 @@\n-#endif\n-        return false;\n+        tty->print(\", which prevents elimination of: \");\n+        if (res == nullptr)\n+          alloc->dump();\n+        else\n+          res->dump();\n@@ -922,12 +926,12 @@\n-      if (UseCompressedOops && field_type->isa_narrowoop()) {\n-        \/\/ Enable \"DecodeN(EncodeP(Allocate)) --> Allocate\" transformation\n-        \/\/ to be able scalar replace the allocation.\n-        if (field_val->is_EncodeP()) {\n-          field_val = field_val->in(1);\n-        } else if (!field_val->is_InlineType()) {\n-          field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n-        }\n-      }\n-      if (field_val->is_InlineType()) {\n-        \/\/ Keep track of inline types to scalarize them later\n-        value_worklist.push(field_val);\n+#endif\n+\n+      return nullptr;\n+    }\n+\n+    if (UseCompressedOops && field_type->isa_narrowoop()) {\n+      \/\/ Enable \"DecodeN(EncodeP(Allocate)) --> Allocate\" transformation\n+      \/\/ to be able scalar replace the allocation.\n+      if (field_val->is_EncodeP()) {\n+        field_val = field_val->in(1);\n+      } else if (!field_val->is_InlineType()) {\n+        field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n@@ -935,3 +939,34 @@\n-      sfpt->add_req(field_val);\n-    JVMState *jvms = sfpt->jvms();\n-    jvms->set_endoff(sfpt->req());\n+    if (field_val->is_InlineType()) {\n+      \/\/ Keep track of inline types to scalarize them later\n+      value_worklist->push(field_val);\n+    }\n+    sfpt->add_req(field_val);\n+  }\n+\n+  sfpt->jvms()->set_endoff(sfpt->req());\n+\n+  return sobj;\n+}\n+\n+\/\/ Do scalar replacement.\n+bool PhaseMacroExpand::scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints) {\n+  GrowableArray <SafePointNode *> safepoints_done;\n+  Node* res = alloc->result_cast();\n+  assert(res == nullptr || res->is_CheckCastPP(), \"unexpected AllocateNode result\");\n+  const TypeOopPtr* res_type = nullptr;\n+  if (res != nullptr) { \/\/ Could be null when there are no users\n+    res_type = _igvn.type(res)->isa_oopptr();\n+  }\n+\n+  \/\/ Process the safepoint uses\n+  assert(safepoints.length() == 0 || !res_type->is_inlinetypeptr(), \"Inline type allocations should not have safepoint uses\");\n+  Unique_Node_List value_worklist;\n+  while (safepoints.length() > 0) {\n+    SafePointNode* sfpt = safepoints.pop();\n+    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt, &value_worklist);\n+\n+    if (sobj == nullptr) {\n+      undo_previous_scalarizations(safepoints_done, alloc);\n+      return false;\n+    }\n+\n@@ -941,3 +976,2 @@\n-    int start = jvms->debug_start();\n-    int end   = jvms->debug_end();\n-    sfpt->replace_edges_in_range(res, sobj, start, end, &_igvn);\n+    JVMState *jvms = sfpt->jvms();\n+    sfpt->replace_edges_in_range(res, sobj, jvms->debug_start(), jvms->debug_end(), &_igvn);\n@@ -945,1 +979,3 @@\n-    safepoints_done.append_if_missing(sfpt); \/\/ keep it for rollback\n+\n+    \/\/ keep it for rollback\n+    safepoints_done.append_if_missing(sfpt);\n@@ -1172,1 +1208,1 @@\n-  if (!can_eliminate_allocation(alloc, safepoints)) {\n+  if (!can_eliminate_allocation(&_igvn, alloc, &safepoints)) {\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":194,"deletions":158,"binary":false,"changes":352,"status":"modified"},{"patch":"@@ -103,2 +103,2 @@\n-  bool can_eliminate_allocation(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints);\n-  bool scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints_done);\n+  void undo_previous_scalarizations(GrowableArray <SafePointNode *> safepoints_done, AllocateNode* alloc);\n+  bool scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints);\n@@ -223,0 +223,4 @@\n+  SafePointScalarObjectNode* create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt, Unique_Node_List* value_worklist);\n+  static bool can_eliminate_allocation(PhaseIterGVN *igvn, AllocateNode *alloc, GrowableArray <SafePointNode *> *safepoints);\n+\n+\n","filename":"src\/hotspot\/share\/opto\/macro.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1549,0 +1549,29 @@\n+\n+\/\/------------------------------split_through_phi------------------------------\n+\/\/ Check whether a call to 'split_through_phi' would split this load through the\n+\/\/ Phi *base*. This method is essentially a copy of the validations performed\n+\/\/ by 'split_through_phi'. The first use of this method was in EA code as part\n+\/\/ of simplification of allocation merges.\n+bool LoadNode::can_split_through_phi_base(PhaseGVN* phase) {\n+  Node* mem        = in(Memory);\n+  Node* address    = in(Address);\n+  intptr_t ignore  = 0;\n+  Node*    base    = AddPNode::Ideal_base_and_offset(address, phase, ignore);\n+  bool base_is_phi = (base != nullptr) && base->is_Phi();\n+\n+  if (req() > 3 || !base_is_phi) {\n+    return false;\n+  }\n+\n+  if (!mem->is_Phi()) {\n+    if (!MemNode::all_controls_dominate(mem, base->in(0)))\n+      return false;\n+  } else if (base->in(0) != mem->in(0)) {\n+    if (!MemNode::all_controls_dominate(mem, base->in(0))) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n@@ -1551,1 +1580,1 @@\n-Node* LoadNode::split_through_phi(PhaseGVN* phase) {\n+Node* LoadNode::split_through_phi(PhaseGVN* phase, bool ignore_missing_instance_id) {\n@@ -1562,1 +1591,2 @@\n-         (t_oop->is_known_instance_field() ||\n+         (ignore_missing_instance_id ||\n+          t_oop->is_known_instance_field() ||\n@@ -1574,2 +1604,2 @@\n-        (load_boxed_values || t_oop->is_known_instance_field()))) {\n-    return nullptr; \/\/ memory is not Phi\n+        (ignore_missing_instance_id || load_boxed_values || t_oop->is_known_instance_field()))) {\n+    return nullptr; \/\/ Neither memory or base are Phi\n@@ -1619,1 +1649,1 @@\n-  assert(C->have_alias_type(t_oop), \"instance should have alias type\");\n+  assert(ignore_missing_instance_id || C->have_alias_type(t_oop), \"instance should have alias type\");\n@@ -1655,0 +1685,1 @@\n+  Node* phi = nullptr;\n@@ -1656,8 +1687,11 @@\n-  int this_index  = C->get_alias_index(t_oop);\n-  int this_offset = t_oop->offset();\n-  int this_iid    = t_oop->instance_id();\n-  if (!t_oop->is_known_instance() && load_boxed_values) {\n-    \/\/ Use _idx of address base for boxed values.\n-    this_iid = base->_idx;\n-  }\n-  Node* phi = new PhiNode(region, this_type, nullptr, mem->_idx, this_iid, this_index, this_offset);\n+  if (t_oop != nullptr && (t_oop->is_known_instance_field() || load_boxed_values)) {\n+    int this_index = C->get_alias_index(t_oop);\n+    int this_offset = t_oop->offset();\n+    int this_iid = t_oop->is_known_instance_field() ? t_oop->instance_id() : base->_idx;\n+    phi = new PhiNode(region, this_type, nullptr, mem->_idx, this_iid, this_index, this_offset);\n+  } else if (ignore_missing_instance_id) {\n+    phi = new PhiNode(region, this_type, nullptr, mem->_idx);\n+  } else {\n+    return nullptr;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":47,"deletions":13,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -251,0 +251,3 @@\n+  \/\/ Return true if it's possible to split the Load through a Phi merging the bases\n+  bool can_split_through_phi_base(PhaseGVN *phase);\n+\n@@ -252,1 +255,1 @@\n-  Node* split_through_phi(PhaseGVN *phase);\n+  Node* split_through_phi(PhaseGVN *phase, bool ignore_missing_instance_id = false);\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -164,0 +164,1 @@\n+class SafePointScalarMergeNode;\n@@ -737,0 +738,1 @@\n+      DEFINE_CLASS_ID(SafePointScalarMerge, Type, 9)\n@@ -969,0 +971,1 @@\n+  DEFINE_CLASS_QUERY(SafePointScalarMerge)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -798,1 +798,1 @@\n-    ObjectValue* sv = sv_for_node_id(objs, spobj->_idx);\n+    ObjectValue* sv = (ObjectValue*) sv_for_node_id(objs, spobj->_idx);\n@@ -831,0 +831,25 @@\n+  } else if (local->is_SafePointScalarMerge()) {\n+    SafePointScalarMergeNode* smerge = local->as_SafePointScalarMerge();\n+    ObjectMergeValue* mv = (ObjectMergeValue*) sv_for_node_id(objs, smerge->_idx);\n+\n+    if (mv == NULL) {\n+      GrowableArray<ScopeValue*> deps;\n+\n+      int merge_pointer_idx = smerge->merge_pointer_idx(sfpt->jvms());\n+      (void)FillLocArray(0, sfpt, sfpt->in(merge_pointer_idx), &deps, objs);\n+      assert(deps.length() == 1, \"missing value\");\n+\n+      int selector_idx = smerge->selector_idx(sfpt->jvms());\n+      (void)FillLocArray(1, NULL, sfpt->in(selector_idx), &deps, NULL);\n+      assert(deps.length() == 2, \"missing value\");\n+\n+      mv = new ObjectMergeValue(smerge->_idx, deps.at(0), deps.at(1));\n+      set_sv_for_object_node(objs, mv);\n+\n+      for (uint i = 1; i < smerge->req(); i++) {\n+        Node* obj_node = smerge->in(i);\n+        (void)FillLocArray(mv->possible_objects()->length(), sfpt, obj_node, mv->possible_objects(), objs);\n+      }\n+    }\n+    array->append(mv);\n+    return;\n@@ -996,0 +1021,12 @@\n+\/\/ Determine if there is a monitor that has 'ov' as its owner.\n+bool PhaseOutput::contains_as_owner(GrowableArray<MonitorValue*> *monarray, ObjectValue *ov) const {\n+  for (int k = 0; k < monarray->length(); k++) {\n+    MonitorValue* mv = monarray->at(k);\n+    if (mv->owner() == ov) {\n+      return true;\n+    }\n+  }\n+\n+  return false;\n+}\n+\n@@ -1130,0 +1167,15 @@\n+    \/\/ Mark ObjectValue nodes as root nodes if they are directly\n+    \/\/ referenced in the JVMS.\n+    for (int i = 0; i < objs->length(); i++) {\n+      ScopeValue* sv = objs->at(i);\n+      if (sv->is_object_merge()) {\n+        ObjectMergeValue* merge = sv->as_ObjectMergeValue();\n+\n+        for (int j = 0; j< merge->possible_objects()->length(); j++) {\n+          ObjectValue* ov = merge->possible_objects()->at(j)->as_ObjectValue();\n+          bool is_root = locarray->contains(ov) || exparray->contains(ov) || contains_as_owner(monarray, ov);\n+          ov->set_root(is_root);\n+        }\n+      }\n+    }\n+\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":53,"deletions":1,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -279,5 +279,1 @@\n-    Node* sobj = new SafePointScalarObjectNode(vec_box->box_type(),\n-#ifdef ASSERT\n-                                               vec_box,\n-#endif \/\/ ASSERT\n-                                               first_ind, n_fields);\n+    Node* sobj = new SafePointScalarObjectNode(vec_box->box_type(), vec_box, first_ind, n_fields);\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -372,0 +372,7 @@\n+WB_ENTRY(jboolean, WB_HasLibgraal(JNIEnv* env, jobject o))\n+#if INCLUDE_JVMCI\n+  return JVMCI::shared_library_exists();\n+#endif\n+  return false;\n+WB_END\n+\n@@ -2902,0 +2909,1 @@\n+  {CC\"hasLibgraal\",                       CC\"()Z\",    (void*)&WB_HasLibgraal },\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -2730,1 +2731,1 @@\n-      \/\/ For compatibility with classic. HotSpot refuses to load the old style agent.dll.\n+      warning(\"Option -Xnoagent was deprecated in JDK 22 and will likely be removed in a future release.\");\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -340,1 +340,1 @@\n-  GrowableArray<ScopeValue*>* objects = chunk->at(0)->scope()->objects();\n+  GrowableArray<ScopeValue*>* objects = chunk->at(0)->scope()->objects_to_rematerialize(deoptee, map);\n@@ -1661,0 +1661,1 @@\n+    assert(objects->at(i)->is_object(), \"invalid debug information\");\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -77,0 +77,1 @@\n+#include \"runtime\/os.inline.hpp\"\n@@ -1714,3 +1715,9 @@\n-    frame f = os::current_frame();\n-    VMError::print_native_stack(tty, f, this, true \/*print_source_info *\/,\n-                                -1 \/* max stack *\/, buf, O_BUFLEN);\n+    address lastpc = nullptr;\n+    if (os::platform_print_native_stack(tty, nullptr, buf, O_BUFLEN, lastpc)) {\n+      \/\/ We have printed the native stack in platform-specific code,\n+      \/\/ so nothing else to do in this case.\n+    } else {\n+      frame f = os::current_frame();\n+      VMError::print_native_stack(tty, f, this, true \/*print_source_info *\/,\n+                                  -1 \/* max stack *\/, buf, O_BUFLEN);\n+    }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -188,0 +188,2 @@\n+JFR_ONLY(RuntimeStub* StubRoutines::_jfr_return_lease_stub = nullptr;)\n+JFR_ONLY(address StubRoutines::_jfr_return_lease = nullptr;)\n@@ -282,37 +284,0 @@\n-#ifdef ASSERT\n-typedef void (*arraycopy_fn)(address src, address dst, int count);\n-\n-\/\/ simple tests of generated arraycopy functions\n-static void test_arraycopy_func(address func, int alignment) {\n-  int v = 0xcc;\n-  int v2 = 0x11;\n-  jlong lbuffer[8];\n-  jlong lbuffer2[8];\n-  address fbuffer  = (address) lbuffer;\n-  address fbuffer2 = (address) lbuffer2;\n-  unsigned int i;\n-  for (i = 0; i < sizeof(lbuffer); i++) {\n-    fbuffer[i] = v; fbuffer2[i] = v2;\n-  }\n-  \/\/ C++ does not guarantee jlong[] array alignment to 8 bytes.\n-  \/\/ Use middle of array to check that memory before it is not modified.\n-  address buffer  = align_up((address)&lbuffer[4], BytesPerLong);\n-  address buffer2 = align_up((address)&lbuffer2[4], BytesPerLong);\n-  \/\/ do an aligned copy\n-  ((arraycopy_fn)func)(buffer, buffer2, 0);\n-  for (i = 0; i < sizeof(lbuffer); i++) {\n-    assert(fbuffer[i] == v && fbuffer2[i] == v2, \"shouldn't have copied anything\");\n-  }\n-  \/\/ adjust destination alignment\n-  ((arraycopy_fn)func)(buffer, buffer2 + alignment, 0);\n-  for (i = 0; i < sizeof(lbuffer); i++) {\n-    assert(fbuffer[i] == v && fbuffer2[i] == v2, \"shouldn't have copied anything\");\n-  }\n-  \/\/ adjust source alignment\n-  ((arraycopy_fn)func)(buffer + alignment, buffer2, 0);\n-  for (i = 0; i < sizeof(lbuffer); i++) {\n-    assert(fbuffer[i] == v && fbuffer2[i] == v2, \"shouldn't have copied anything\");\n-  }\n-}\n-#endif \/\/ ASSERT\n-\n@@ -327,81 +292,0 @@\n-\n-#ifdef ASSERT\n-\n-  MACOS_AARCH64_ONLY(os::current_thread_enable_wx(WXExec));\n-\n-#define TEST_ARRAYCOPY(type)                                                    \\\n-  test_arraycopy_func(          type##_arraycopy(),          sizeof(type));     \\\n-  test_arraycopy_func(          type##_disjoint_arraycopy(), sizeof(type));     \\\n-  test_arraycopy_func(arrayof_##type##_arraycopy(),          sizeof(HeapWord)); \\\n-  test_arraycopy_func(arrayof_##type##_disjoint_arraycopy(), sizeof(HeapWord))\n-\n-  \/\/ Make sure all the arraycopy stubs properly handle zero count\n-  TEST_ARRAYCOPY(jbyte);\n-  TEST_ARRAYCOPY(jshort);\n-  TEST_ARRAYCOPY(jint);\n-  TEST_ARRAYCOPY(jlong);\n-\n-#undef TEST_ARRAYCOPY\n-\n-#define TEST_FILL(type)                                                                      \\\n-  if (_##type##_fill != nullptr) {                                                              \\\n-    union {                                                                                  \\\n-      double d;                                                                              \\\n-      type body[96];                                                                         \\\n-    } s;                                                                                     \\\n-                                                                                             \\\n-    int v = 32;                                                                              \\\n-    for (int offset = -2; offset <= 2; offset++) {                                           \\\n-      for (int i = 0; i < 96; i++) {                                                         \\\n-        s.body[i] = 1;                                                                       \\\n-      }                                                                                      \\\n-      type* start = s.body + 8 + offset;                                                     \\\n-      for (int aligned = 0; aligned < 2; aligned++) {                                        \\\n-        if (aligned) {                                                                       \\\n-          if (((intptr_t)start) % HeapWordSize == 0) {                                       \\\n-            ((void (*)(type*, int, int))StubRoutines::_arrayof_##type##_fill)(start, v, 80); \\\n-          } else {                                                                           \\\n-            continue;                                                                        \\\n-          }                                                                                  \\\n-        } else {                                                                             \\\n-          ((void (*)(type*, int, int))StubRoutines::_##type##_fill)(start, v, 80);           \\\n-        }                                                                                    \\\n-        for (int i = 0; i < 96; i++) {                                                       \\\n-          if (i < (8 + offset) || i >= (88 + offset)) {                                      \\\n-            assert(s.body[i] == 1, \"what?\");                                                 \\\n-          } else {                                                                           \\\n-            assert(s.body[i] == 32, \"what?\");                                                \\\n-          }                                                                                  \\\n-        }                                                                                    \\\n-      }                                                                                      \\\n-    }                                                                                        \\\n-  }                                                                                          \\\n-\n-  TEST_FILL(jbyte);\n-  TEST_FILL(jshort);\n-  TEST_FILL(jint);\n-\n-#undef TEST_FILL\n-\n-#define TEST_COPYRTN(type) \\\n-  test_arraycopy_func(CAST_FROM_FN_PTR(address, Copy::conjoint_##type##s_atomic),  sizeof(type)); \\\n-  test_arraycopy_func(CAST_FROM_FN_PTR(address, Copy::arrayof_conjoint_##type##s), (int)MAX2(sizeof(HeapWord), sizeof(type)))\n-\n-  \/\/ Make sure all the copy runtime routines properly handle zero count\n-  TEST_COPYRTN(jbyte);\n-  TEST_COPYRTN(jshort);\n-  TEST_COPYRTN(jint);\n-  TEST_COPYRTN(jlong);\n-\n-#undef TEST_COPYRTN\n-\n-  test_arraycopy_func(CAST_FROM_FN_PTR(address, Copy::conjoint_words), sizeof(HeapWord));\n-  test_arraycopy_func(CAST_FROM_FN_PTR(address, Copy::disjoint_words), sizeof(HeapWord));\n-  test_arraycopy_func(CAST_FROM_FN_PTR(address, Copy::disjoint_words_atomic), sizeof(HeapWord));\n-  \/\/ Aligned to BytesPerLong\n-  test_arraycopy_func(CAST_FROM_FN_PTR(address, Copy::aligned_conjoint_words), sizeof(jlong));\n-  test_arraycopy_func(CAST_FROM_FN_PTR(address, Copy::aligned_disjoint_words), sizeof(jlong));\n-\n-  MACOS_AARCH64_ONLY(os::current_thread_enable_wx(WXWrite));\n-\n-#endif\n@@ -410,1 +294,0 @@\n-\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":2,"deletions":119,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -263,0 +263,2 @@\n+  JFR_ONLY(static RuntimeStub* _jfr_return_lease_stub;)\n+  JFR_ONLY(static address _jfr_return_lease;)\n@@ -463,0 +465,1 @@\n+  JFR_ONLY(static address jfr_return_lease() { return _jfr_return_lease; })\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -330,0 +330,4 @@\n+  nonstatic_field(Annotations,                 _class_annotations,                            Array<u1>*)                            \\\n+  nonstatic_field(Annotations,                 _fields_annotations,                           Array<Array<u1>*>*)                    \\\n+  nonstatic_field(Annotations,                 _class_type_annotations,                       Array<u1>*)                            \\\n+  nonstatic_field(Annotations,                 _fields_type_annotations,                      Array<Array<u1>*>*)                    \\\n@@ -384,2 +388,2 @@\n-     static_field(CompressedKlassPointers,     _narrow_klass._base,                           address)                               \\\n-     static_field(CompressedKlassPointers,     _narrow_klass._shift,                          int)                                   \\\n+     static_field(CompressedKlassPointers,     _base,                                         address)                               \\\n+     static_field(CompressedKlassPointers,     _shift,                                        int)                                   \\\n@@ -970,0 +974,1 @@\n+  unchecked_nonstatic_field(Array<Array<u1>*>,        _data,                                  sizeof(Array<u1>*))                    \\\n@@ -1178,0 +1183,1 @@\n+    declare_type(Annotations, MetaspaceObj)                               \\\n@@ -1902,0 +1908,1 @@\n+            declare_type(Array<Array<u1>*>, MetaspaceObj)                 \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -412,0 +412,3 @@\n+#define EXACTFMT            SIZE_FORMAT \"%s\"\n+#define EXACTFMTARGS(s)     byte_size_in_exact_unit(s), exact_unit_for_byte_size(s)\n+\n@@ -587,5 +590,0 @@\n-const int LogKlassAlignmentInBytes = 3;\n-const int LogKlassAlignment        = LogKlassAlignmentInBytes - LogHeapWordSize;\n-const int KlassAlignmentInBytes    = 1 << LogKlassAlignmentInBytes;\n-const int KlassAlignment           = KlassAlignmentInBytes \/ HeapWordSize;\n-\n@@ -599,5 +597,0 @@\n-\/\/ Maximal size of compressed class space. Above this limit compression is not possible.\n-\/\/ Also upper bound for placement of zero based class space. (Class space is further limited\n-\/\/ to be < 3G, see arguments.cpp.)\n-const  uint64_t KlassEncodingMetaspaceMax = (uint64_t(max_juint) + 1) << LogKlassAlignmentInBytes;\n-\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":3,"deletions":10,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -271,15 +271,0 @@\n-    \/**\n-     * immutable table mapping primitive type names to corresponding\n-     * class objects\n-     *\/\n-    private static final Map<String, Class<?>> primClasses =\n-        Map.of(\"boolean\", boolean.class,\n-               \"byte\", byte.class,\n-               \"char\", char.class,\n-               \"short\", short.class,\n-               \"int\", int.class,\n-               \"long\", long.class,\n-               \"float\", float.class,\n-               \"double\", double.class,\n-               \"void\", void.class);\n-\n@@ -813,1 +798,1 @@\n-            Class<?> cl = primClasses.get(name);\n+            Class<?> cl = Class.forPrimitiveName(name);\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectInputStream.java","additions":1,"deletions":16,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -462,0 +462,3 @@\n+     * To obtain a {@code Class} object for a named primitive type\n+     * such as {@code int} or {@code long} use {@link\n+     * #forPrimitiveName(String)}.\n@@ -790,0 +793,35 @@\n+    \/**\n+     * {@return the {@code Class} object associated with the\n+     * {@linkplain #isPrimitive() primitive type} of the given name}\n+     * If the argument is not the name of a primitive type, {@code\n+     * null} is returned.\n+     *\n+     * @param primitiveName the name of the primitive type to find\n+     *\n+     * @throws NullPointerException if the argument is {@code null}\n+     *\n+     * @jls 4.2 Primitive Types and Values\n+     * @jls 15.8.2 Class Literals\n+     * @since 22\n+     *\/\n+    public static Class<?> forPrimitiveName(String primitiveName) {\n+        return switch(primitiveName) {\n+        \/\/ Integral types\n+        case \"int\"     -> int.class;\n+        case \"long\"    -> long.class;\n+        case \"short\"   -> short.class;\n+        case \"char\"    -> char.class;\n+        case \"byte\"    -> byte.class;\n+\n+        \/\/ Floating-point types\n+        case \"float\"   -> float.class;\n+        case \"double\"  -> double.class;\n+\n+        \/\/ Other types\n+        case \"boolean\" -> boolean.class;\n+        case \"void\"    -> void.class;\n+\n+        default        -> null;\n+        };\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Class.java","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -2854,0 +2854,5 @@\n+         * For a class or an interface, the name is the {@linkplain ClassLoader##binary-name binary name}.\n+         * For an array class of {@code n} dimensions, the name begins with {@code n} occurrences\n+         * of {@code '['} and followed by the element type as encoded in the\n+         * {@linkplain Class##nameFormat table} specified in {@link Class#getName}.\n+         * <p>\n@@ -2857,1 +2862,2 @@\n-         * @param targetName the fully qualified name of the class to be looked up.\n+         * @param targetName the {@linkplain ClassLoader##binary-name binary name} of the class\n+         *                   or the string representing an array class\n@@ -4233,1 +4239,1 @@\n-            return VarHandles.makeFieldHandle(getField, refc, getField.getFieldType(),\n+            return VarHandles.makeFieldHandle(getField, refc,\n@@ -8228,13 +8234,12 @@\n-     * If {@code R} is the return type of the filter (which cannot be void), the target var handle must accept a value of\n-     * type {@code R} as its coordinate in position {@code pos}, preceded and\/or followed by\n-     * any coordinate not passed to the filter.\n-     * No coordinates are reordered, and the result returned from the filter\n-     * replaces (in order) the whole subsequence of coordinates originally\n-     * passed to the adapter.\n-     * <p>\n-     * The argument types (if any) of the filter\n-     * replace zero or one coordinate types of the target var handle, at position {@code pos},\n-     * in the resulting adapted var handle.\n-     * The return type of the filter must be identical to the\n-     * coordinate type of the target var handle at position {@code pos}, and that target var handle\n-     * coordinate is supplied by the return value of the filter.\n+     * If {@code R} is the return type of the filter, then:\n+     * <ul>\n+     * <li>if {@code R} <em>is not<\/em> {@code void}, the target var handle must have a coordinate of type {@code R} in\n+     * position {@code pos}. The parameter types of the filter will replace the coordinate type at position {@code pos}\n+     * of the target var handle. When the returned var handle is invoked, it will be as if the filter is invoked first,\n+     * and its result is passed in place of the coordinate at position {@code pos} in a downstream invocation of the\n+     * target var handle.<\/li>\n+     * <li> if {@code R} <em>is<\/em> {@code void}, the parameter types (if any) of the filter will be inserted in the\n+     * coordinate type list of the target var handle at position {@code pos}. In this case, when the returned var handle\n+     * is invoked, the filter essentially acts as a side effect, consuming some of the coordinate values, before a\n+     * downstream invocation of the target var handle.<\/li>\n+     * <\/ul>\n@@ -8249,1 +8254,1 @@\n-     * @param pos the position of the coordinate to be filtered\n+     * @param pos the position in the coordinate list of the target var handle where the filter is to be inserted\n@@ -8254,1 +8259,1 @@\n-     * is void, or it is not the same as the {@code pos} coordinate of the target var handle,\n+     * is not void, and it is not the same as the {@code pos} coordinate of the target var handle,\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandles.java","additions":22,"deletions":17,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -476,1 +476,2 @@\n-     permits IndirectVarHandle, VarHandleSegmentViewBase,\n+     permits IndirectVarHandle, LazyInitializingVarHandle,\n+             VarHandleSegmentViewBase,\n@@ -525,2 +526,7 @@\n-    RuntimeException unsupported() {\n-        return new UnsupportedOperationException();\n+    \/**\n+     * Returns the target VarHandle.   Subclasses may override this method to implement\n+     * additional logic for example lazily initializing the declaring class of a static field var handle.\n+     *\/\n+    @ForceInline\n+    VarHandle target() {\n+        return asDirect();\n@@ -529,0 +535,8 @@\n+    \/**\n+     * Returns the direct target VarHandle.   Indirect VarHandle subclasses should implement\n+     * this method.\n+     *\n+     * @see #getMethodHandle(int)\n+     * @see #checkAccessModeThenIsDirect(AccessDescriptor)\n+     *\/\n+    @ForceInline\n@@ -2075,2 +2089,2 @@\n-     * the access mode of this VarHandle, then returns if this is a direct\n-     * method handle. These operations were grouped together to slightly\n+     * the access mode of this VarHandle, then returns if this is direct.\n+     * These operations were grouped together to slightly\n@@ -2079,0 +2093,5 @@\n+     * A direct VarHandle's VarForm has implementation MemberNames that can\n+     * be linked directly. If a VarHandle is indirect, it must override\n+     * {@link #isAccessModeSupported} and {@link #getMethodHandleUncached}\n+     * which access MemberNames.\n+     *\n@@ -2082,0 +2101,1 @@\n+     * @see #asDirect()\n@@ -2157,1 +2177,1 @@\n-            return mh.bindTo(this);\n+            return mh.bindTo(asDirect());\n@@ -2199,0 +2219,8 @@\n+    \/**\n+     * Computes a method handle that can be passed the {@linkplain #asDirect() direct}\n+     * var handle of this var handle with the given access mode. Pre\/postprocessing\n+     * such as argument or return value filtering should be done by the returned\n+     * method handle.\n+     *\n+     * @throws UnsupportedOperationException if the access mode is not supported\n+     *\/\n@@ -2414,1 +2442,1 @@\n-                case FIELD        -> lookup.findVarHandle((Class<?>) declaringClass.resolveConstantDesc(lookup),\n+                case FIELD        -> lookup.findVarHandle(declaringClass.resolveConstantDesc(lookup),\n@@ -2416,2 +2444,2 @@\n-                                                          (Class<?>) varType.resolveConstantDesc(lookup));\n-                case STATIC_FIELD -> lookup.findStaticVarHandle((Class<?>) declaringClass.resolveConstantDesc(lookup),\n+                                                          varType.resolveConstantDesc(lookup));\n+                case STATIC_FIELD -> lookup.findStaticVarHandle(declaringClass.resolveConstantDesc(lookup),\n@@ -2419,2 +2447,2 @@\n-                                                          (Class<?>) varType.resolveConstantDesc(lookup));\n-                case ARRAY        -> MethodHandles.arrayElementVarHandle((Class<?>) declaringClass.resolveConstantDesc(lookup));\n+                                                          varType.resolveConstantDesc(lookup));\n+                case ARRAY        -> MethodHandles.arrayElementVarHandle(declaringClass.resolveConstantDesc(lookup));\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/VarHandle.java","additions":39,"deletions":11,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -57,1 +57,1 @@\n-    static VarHandle makeFieldHandle(MemberName f, Class<?> refc, Class<?> type, boolean isWriteAllowedOnFinalFields) {\n+    static VarHandle makeFieldHandle(MemberName f, Class<?> refc, boolean isWriteAllowedOnFinalFields) {\n@@ -60,0 +60,1 @@\n+            Class<?> type = f.getFieldType();\n@@ -116,58 +117,13 @@\n-            \/\/ TODO This is not lazy on first invocation\n-            \/\/ and might cause some circular initialization issues\n-\n-            \/\/ Replace with something similar to direct method handles\n-            \/\/ where a barrier is used then elided after use\n-\n-            if (UNSAFE.shouldBeInitialized(refc))\n-                UNSAFE.ensureClassInitialized(refc);\n-\n-            Object base = MethodHandleNatives.staticFieldBase(f);\n-            long foffset = MethodHandleNatives.staticFieldOffset(f);\n-            if (!type.isPrimitive()) {\n-                if (f.isFlattened()) {\n-                    return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n-                            ? new VarHandleValues.FieldStaticReadOnly(decl, refc, foffset, type)\n-                            : new VarHandleValues.FieldStaticReadWrite(decl, refc, foffset, type));\n-                } else {\n-                    return f.isFinal() && !isWriteAllowedOnFinalFields\n-                            ? new VarHandleReferences.FieldStaticReadOnly(decl, base, foffset, type)\n-                            : new VarHandleReferences.FieldStaticReadWrite(decl, base, foffset, type);\n-                }\n-            }\n-            else if (type == boolean.class) {\n-                return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n-                       ? new VarHandleBooleans.FieldStaticReadOnly(decl, base, foffset)\n-                       : new VarHandleBooleans.FieldStaticReadWrite(decl, base, foffset));\n-            }\n-            else if (type == byte.class) {\n-                return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n-                       ? new VarHandleBytes.FieldStaticReadOnly(decl, base, foffset)\n-                       : new VarHandleBytes.FieldStaticReadWrite(decl, base, foffset));\n-            }\n-            else if (type == short.class) {\n-                return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n-                       ? new VarHandleShorts.FieldStaticReadOnly(decl, base, foffset)\n-                       : new VarHandleShorts.FieldStaticReadWrite(decl, base, foffset));\n-            }\n-            else if (type == char.class) {\n-                return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n-                       ? new VarHandleChars.FieldStaticReadOnly(decl, base, foffset)\n-                       : new VarHandleChars.FieldStaticReadWrite(decl, base, foffset));\n-            }\n-            else if (type == int.class) {\n-                return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n-                       ? new VarHandleInts.FieldStaticReadOnly(decl, base, foffset)\n-                       : new VarHandleInts.FieldStaticReadWrite(decl, base, foffset));\n-            }\n-            else if (type == long.class) {\n-                return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n-                       ? new VarHandleLongs.FieldStaticReadOnly(decl, base, foffset)\n-                       : new VarHandleLongs.FieldStaticReadWrite(decl, base, foffset));\n-            }\n-            else if (type == float.class) {\n-                return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n-                       ? new VarHandleFloats.FieldStaticReadOnly(decl, base, foffset)\n-                       : new VarHandleFloats.FieldStaticReadWrite(decl, base, foffset));\n-            }\n-            else if (type == double.class) {\n+            var vh = makeStaticFieldVarHandle(decl, f, isWriteAllowedOnFinalFields);\n+            return maybeAdapt(UNSAFE.shouldBeInitialized(decl)\n+                    ? new LazyInitializingVarHandle(vh, decl)\n+                    : vh);\n+        }\n+    }\n+\n+    static VarHandle makeStaticFieldVarHandle(Class<?> decl, MemberName f, boolean isWriteAllowedOnFinalFields) {\n+        Object base = MethodHandleNatives.staticFieldBase(f);\n+        long foffset = MethodHandleNatives.staticFieldOffset(f);\n+        Class<?> type = f.getFieldType();\n+        if (!type.isPrimitive()) {\n+            if (f.isFlattened()) {\n@@ -175,5 +131,6 @@\n-                       ? new VarHandleDoubles.FieldStaticReadOnly(decl, base, foffset)\n-                       : new VarHandleDoubles.FieldStaticReadWrite(decl, base, foffset));\n-            }\n-            else {\n-                throw new UnsupportedOperationException();\n+                        ? new VarHandleValues.FieldStaticReadOnly(decl, base, foffset, type)\n+                        : new VarHandleValues.FieldStaticReadWrite(decl, base, foffset, type));\n+            } else {\n+                return f.isFinal() && !isWriteAllowedOnFinalFields\n+                        ? new VarHandleReferences.FieldStaticReadOnly(decl, base, foffset, type)\n+                        : new VarHandleReferences.FieldStaticReadWrite(decl, base, foffset, type);\n@@ -183,0 +140,43 @@\n+        else if (type == boolean.class) {\n+            return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                    ? new VarHandleBooleans.FieldStaticReadOnly(decl, base, foffset)\n+                    : new VarHandleBooleans.FieldStaticReadWrite(decl, base, foffset));\n+        }\n+        else if (type == byte.class) {\n+            return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                    ? new VarHandleBytes.FieldStaticReadOnly(decl, base, foffset)\n+                    : new VarHandleBytes.FieldStaticReadWrite(decl, base, foffset));\n+        }\n+        else if (type == short.class) {\n+            return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                    ? new VarHandleShorts.FieldStaticReadOnly(decl, base, foffset)\n+                    : new VarHandleShorts.FieldStaticReadWrite(decl, base, foffset));\n+        }\n+        else if (type == char.class) {\n+            return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                    ? new VarHandleChars.FieldStaticReadOnly(decl, base, foffset)\n+                    : new VarHandleChars.FieldStaticReadWrite(decl, base, foffset));\n+        }\n+        else if (type == int.class) {\n+            return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                    ? new VarHandleInts.FieldStaticReadOnly(decl, base, foffset)\n+                    : new VarHandleInts.FieldStaticReadWrite(decl, base, foffset));\n+        }\n+        else if (type == long.class) {\n+            return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                    ? new VarHandleLongs.FieldStaticReadOnly(decl, base, foffset)\n+                    : new VarHandleLongs.FieldStaticReadWrite(decl, base, foffset));\n+        }\n+        else if (type == float.class) {\n+            return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                    ? new VarHandleFloats.FieldStaticReadOnly(decl, base, foffset)\n+                    : new VarHandleFloats.FieldStaticReadWrite(decl, base, foffset));\n+        }\n+        else if (type == double.class) {\n+            return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                    ? new VarHandleDoubles.FieldStaticReadOnly(decl, base, foffset)\n+                    : new VarHandleDoubles.FieldStaticReadWrite(decl, base, foffset));\n+        }\n+        else {\n+            throw new UnsupportedOperationException();\n+        }\n@@ -584,3 +584,1 @@\n-        } else if (filter.type().returnType() == void.class) {\n-            throw newIllegalArgumentException(\"Invalid filter type \" + filter.type() + \" ; filter cannot be void\");\n-        } else if (filter.type().returnType() != targetCoordinates.get(pos)) {\n+        } else if (filter.type().returnType() != void.class && filter.type().returnType() != targetCoordinates.get(pos)) {\n@@ -591,1 +589,3 @@\n-        newCoordinates.remove(pos);\n+        if (filter.type().returnType() != void.class) {\n+            newCoordinates.remove(pos);\n+        }\n@@ -752,10 +752,1 @@\n-\/\/        static class HandleType {\n-\/\/            final Class<?> receiver;\n-\/\/            final Class<?>[] intermediates;\n-\/\/            final Class<?> value;\n-\/\/\n-\/\/            HandleType(Class<?> receiver, Class<?> value, Class<?>... intermediates) {\n-\/\/                this.receiver = receiver;\n-\/\/                this.intermediates = intermediates;\n-\/\/                this.value = value;\n-\/\/            }\n+\/\/        record HandleType(Class<?> receiver, Class<?> value, Class<?>... intermediates) {\n@@ -837,4 +828,2 @@\n-\/\/            for (int i = 0; i < intermediates.length; i++) {\n-\/\/                params.add(intermediates[i]);\n-\/\/            }\n-\/\/            for (Parameter p : m.getParameters()) {\n+\/\/            java.util.Collections.addAll(params, intermediates);\n+\/\/            for (var p : m.getParameters()) {\n@@ -849,1 +838,1 @@\n-\/\/            LinkedHashMap<String, Class<?>> params = new LinkedHashMap<>();\n+\/\/            var params = new java.util.LinkedHashMap<String, Class<?>>();\n@@ -862,1 +851,1 @@\n-\/\/                    collect(joining(\", \"));\n+\/\/                    collect(java.util.stream.Collectors.joining(\", \"));\n@@ -872,2 +861,1 @@\n-\/\/            List<String> LINK_TO_STATIC_ARGS = params.keySet().stream().\n-\/\/                    collect(toList());\n+\/\/            List<String> LINK_TO_STATIC_ARGS = new ArrayList<>(params.keySet());\n@@ -876,2 +864,1 @@\n-\/\/            List<String> LINK_TO_INVOKER_ARGS = params.keySet().stream().\n-\/\/                    collect(toList());\n+\/\/            List<String> LINK_TO_INVOKER_ARGS = new ArrayList<>(params.keySet());\n@@ -905,4 +892,2 @@\n-\/\/                    replaceAll(\"<LINK_TO_STATIC_ARGS>\", LINK_TO_STATIC_ARGS.stream().\n-\/\/                            collect(joining(\", \"))).\n-\/\/                    replace(\"<LINK_TO_INVOKER_ARGS>\", LINK_TO_INVOKER_ARGS.stream().\n-\/\/                            collect(joining(\", \")))\n+\/\/                    replaceAll(\"<LINK_TO_STATIC_ARGS>\", String.join(\", \", LINK_TO_STATIC_ARGS)).\n+\/\/                    replace(\"<LINK_TO_INVOKER_ARGS>\", String.join(\", \", LINK_TO_INVOKER_ARGS))\n@@ -937,24 +922,1 @@\n-\/\/            if (pt == void.class) {\n-\/\/                return 'V';\n-\/\/            }\n-\/\/            else if (!pt.isPrimitive()) {\n-\/\/                return 'L';\n-\/\/            }\n-\/\/            else if (pt == boolean.class) {\n-\/\/                return 'Z';\n-\/\/            }\n-\/\/            else if (pt == int.class) {\n-\/\/                return 'I';\n-\/\/            }\n-\/\/            else if (pt == long.class) {\n-\/\/                return 'J';\n-\/\/            }\n-\/\/            else if (pt == float.class) {\n-\/\/                return 'F';\n-\/\/            }\n-\/\/            else if (pt == double.class) {\n-\/\/                return 'D';\n-\/\/            }\n-\/\/            else {\n-\/\/                throw new IllegalStateException(pt.getName());\n-\/\/            }\n+\/\/            return Wrapper.forBasicType(pt).basicTypeChar();\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/VarHandles.java","additions":78,"deletions":116,"binary":false,"changes":194,"status":"modified"},{"patch":"@@ -449,1 +449,1 @@\n-            FieldStaticReadOnly handle = (FieldStaticReadOnly)ob;\n+            FieldStaticReadOnly handle = (FieldStaticReadOnly) ob.target();\n@@ -456,1 +456,1 @@\n-            FieldStaticReadOnly handle = (FieldStaticReadOnly)ob;\n+            FieldStaticReadOnly handle = (FieldStaticReadOnly) ob.target();\n@@ -463,1 +463,1 @@\n-            FieldStaticReadOnly handle = (FieldStaticReadOnly)ob;\n+            FieldStaticReadOnly handle = (FieldStaticReadOnly) ob.target();\n@@ -470,1 +470,1 @@\n-            FieldStaticReadOnly handle = (FieldStaticReadOnly)ob;\n+            FieldStaticReadOnly handle = (FieldStaticReadOnly) ob.target();\n@@ -511,1 +511,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -519,1 +519,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -527,1 +527,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -535,1 +535,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -544,1 +544,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -554,1 +554,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -563,1 +563,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -572,1 +572,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -581,1 +581,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -590,1 +590,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -599,1 +599,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -608,1 +608,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -617,1 +617,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -625,1 +625,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -633,1 +633,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -643,1 +643,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -651,1 +651,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -659,1 +659,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -669,1 +669,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -677,1 +677,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -685,1 +685,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -693,1 +693,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -701,1 +701,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -709,1 +709,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -717,1 +717,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -725,1 +725,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n@@ -733,1 +733,1 @@\n-            FieldStaticReadWrite handle = (FieldStaticReadWrite)ob;\n+            FieldStaticReadWrite handle = (FieldStaticReadWrite) ob.target();\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/X-VarHandle.java.template","additions":31,"deletions":31,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1996, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1996, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -53,0 +53,1 @@\n+     * @implSpec\n@@ -56,4 +57,1 @@\n-     * <pre>\n-     * int[] x = {length};\n-     * Array.newInstance(componentType, x);\n-     * <\/pre>\n+     * {@code Array.newInstance(componentType, new int[]{length});}\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Array.java","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -326,2 +326,1 @@\n-        java.security.jgss,\n-        jdk.crypto.ec;\n+        java.security.jgss;\n@@ -334,1 +333,0 @@\n-        jdk.crypto.ec,\n@@ -338,1 +336,0 @@\n-        jdk.crypto.ec,\n@@ -344,1 +341,0 @@\n-        jdk.crypto.ec,\n@@ -363,1 +359,0 @@\n-        jdk.crypto.ec,\n@@ -368,5 +363,0 @@\n-    exports sun.security.util.math to\n-        jdk.crypto.ec;\n-    exports sun.security.util.math.intpoly to\n-        jdk.crypto.ec;\n-        jdk.crypto.ec,\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":1,"deletions":11,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1753,0 +1753,2 @@\n+                                    } else if (!constants.add(enumSym)) {\n+                                        log.error(label.pos(), Errors.DuplicateCaseLabel);\n@@ -1784,0 +1786,2 @@\n+                                    } else if (!constants.add(s)) {\n+                                        log.error(label.pos(), Errors.DuplicateCaseLabel);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4990,2 +4990,4 @@\n-            if (currentPattern instanceof JCBindingPattern) {\n-                return existingPattern instanceof JCBindingPattern;\n+            if (currentPattern instanceof JCBindingPattern ||\n+                currentPattern instanceof JCAnyPattern) {\n+                return existingPattern instanceof JCBindingPattern ||\n+                       existingPattern instanceof JCAnyPattern;\n@@ -4993,1 +4995,2 @@\n-                if (existingPattern instanceof JCBindingPattern) {\n+                if (existingPattern instanceof JCBindingPattern ||\n+                    existingPattern instanceof JCAnyPattern) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Check.java","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -778,1 +778,1 @@\n-            List<PatternDescription> patterns = List.from(patternSet);\n+            Set<PatternDescription> patterns = patternSet;\n@@ -782,1 +782,1 @@\n-                    List<PatternDescription> updatedPatterns;\n+                    Set<PatternDescription> updatedPatterns;\n@@ -786,1 +786,2 @@\n-                    repeat = updatedPatterns != patterns;\n+                    updatedPatterns = removeCoveredRecordPatterns(updatedPatterns);\n+                    repeat = !updatedPatterns.equals(patterns);\n@@ -799,1 +800,1 @@\n-        private boolean checkCovered(Type seltype, List<PatternDescription> patterns) {\n+        private boolean checkCovered(Type seltype, Iterable<PatternDescription> patterns) {\n@@ -835,1 +836,1 @@\n-        private List<PatternDescription> reduceBindingPatterns(Type selectorType, List<PatternDescription> patterns) {\n+        private Set<PatternDescription> reduceBindingPatterns(Type selectorType, Set<PatternDescription> patterns) {\n@@ -843,1 +844,0 @@\n-                    Set<PatternDescription> toRemove = new HashSet<>();\n@@ -849,0 +849,2 @@\n+                        clazz.complete();\n+\n@@ -874,1 +876,0 @@\n-                                    boolean reduces = false;\n@@ -891,1 +892,0 @@\n-                                            reduces = true;\n@@ -894,4 +894,0 @@\n-\n-                                    if (reduces) {\n-                                        bindings.append(pdOther);\n-                                    }\n@@ -902,1 +898,0 @@\n-                                toRemove.addAll(bindings);\n@@ -908,8 +903,4 @@\n-                    if (!toAdd.isEmpty() || !toRemove.isEmpty()) {\n-                        for (PatternDescription pd : toRemove) {\n-                            patterns = List.filter(patterns, pd);\n-                        }\n-                        for (PatternDescription pd : toAdd) {\n-                            patterns = patterns.prepend(pd);\n-                        }\n-                        return patterns;\n+                    if (!toAdd.isEmpty()) {\n+                        Set<PatternDescription> newPatterns = new HashSet<>(patterns);\n+                        newPatterns.addAll(toAdd);\n+                        return newPatterns;\n@@ -931,0 +922,2 @@\n+                current.complete();\n+\n@@ -957,1 +950,1 @@\n-        private List<PatternDescription> reduceNestedPatterns(List<PatternDescription> patterns) {\n+        private Set<PatternDescription> reduceNestedPatterns(Set<PatternDescription> patterns) {\n@@ -976,0 +969,1 @@\n+                Set<RecordPattern> current = new HashSet<>(e.getValue());\n@@ -982,1 +976,1 @@\n-                            e.getValue()\n+                            current\n@@ -1017,1 +1011,1 @@\n-                            var nestedPatterns = join.stream().map(rp -> rp.nested[mismatchingCandidateFin]).collect(List.collector());\n+                            var nestedPatterns = join.stream().map(rp -> rp.nested[mismatchingCandidateFin]).collect(Collectors.toSet());\n@@ -1021,0 +1015,1 @@\n+                            updatedPatterns = removeCoveredRecordPatterns(updatedPatterns);\n@@ -1023,11 +1018,2 @@\n-                            if (nestedPatterns != updatedPatterns) {\n-                                ListBuffer<PatternDescription> result = new ListBuffer<>();\n-                                Set<PatternDescription> toRemove = Collections.newSetFromMap(new IdentityHashMap<>());\n-\n-                                toRemove.addAll(join);\n-\n-                                for (PatternDescription p : patterns) {\n-                                    if (!toRemove.contains(p)) {\n-                                        result.append(p);\n-                                    }\n-                                }\n+                            if (!nestedPatterns.equals(updatedPatterns)) {\n+                                current.removeAll(join);\n@@ -1039,1 +1025,1 @@\n-                                    result.append(new RecordPattern(rpOne.recordType(),\n+                                    current.add(new RecordPattern(rpOne.recordType(),\n@@ -1043,1 +1029,0 @@\n-                                return result.toList();\n@@ -1048,0 +1033,7 @@\n+\n+                if (!current.equals(new HashSet<>(e.getValue()))) {\n+                    Set<PatternDescription> result = new HashSet<>(patterns);\n+                    result.removeAll(e.getValue());\n+                    result.addAll(current);\n+                    return result;\n+                }\n@@ -1057,2 +1049,2 @@\n-        private List<PatternDescription> reduceRecordPatterns(List<PatternDescription> patterns) {\n-            var newPatterns = new ListBuffer<PatternDescription>();\n+        private Set<PatternDescription> reduceRecordPatterns(Set<PatternDescription> patterns) {\n+            var newPatterns = new HashSet<PatternDescription>();\n@@ -1064,1 +1056,1 @@\n-                        newPatterns.append(reducedPattern);\n+                        newPatterns.add(reducedPattern);\n@@ -1069,1 +1061,1 @@\n-                newPatterns.append(pd);\n+                newPatterns.add(pd);\n@@ -1071,2 +1063,2 @@\n-            return modified ? newPatterns.toList() : patterns;\n-                }\n+            return modified ? newPatterns : patterns;\n+        }\n@@ -1104,0 +1096,17 @@\n+        private Set<PatternDescription> removeCoveredRecordPatterns(Set<PatternDescription> patterns) {\n+            Set<Symbol> existingBindings = patterns.stream()\n+                                                   .filter(pd -> pd instanceof BindingPattern)\n+                                                   .map(pd -> ((BindingPattern) pd).type.tsym)\n+                                                   .collect(Collectors.toSet());\n+            Set<PatternDescription> result = new HashSet<>(patterns);\n+\n+            for (Iterator<PatternDescription> it = result.iterator(); it.hasNext();) {\n+                PatternDescription pd = it.next();\n+                if (pd instanceof RecordPattern rp && existingBindings.contains(rp.recordType.tsym)) {\n+                    it.remove();\n+                }\n+            }\n+\n+            return result;\n+        }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Flow.java","additions":52,"deletions":43,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -913,0 +913,1 @@\n+            checkSourceLevel(Feature.UNNAMED_VARIABLES);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -76,0 +76,1 @@\n+    metadataConstructor.addMapping(\"Annotations\", Annotations.class);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Metadata.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+import sun.jvm.hotspot.utilities.U1Array;\n@@ -387,0 +388,16 @@\n+\n+  public U1Array getAnnotations() {\n+    return getConstMethod().getMethodAnnotations();\n+  }\n+\n+  public U1Array getParameterAnnotations() {\n+    return getConstMethod().getParameterAnnotations();\n+  }\n+\n+  public U1Array getTypeAnnotations() {\n+    return getConstMethod().getTypeAnnotations();\n+  }\n+\n+  public U1Array getAnnotationDefault() {\n+    return getConstMethod().getDefaultAnnotations();\n+  }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Method.java","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -410,0 +410,10 @@\n+            U1Array fieldAnnotations = klass.getFieldAnnotations(index);\n+            if (fieldAnnotations != null) {\n+                fieldAttributeCount++;\n+            }\n+\n+            U1Array fieldTypeAnnotations = klass.getFieldTypeAnnotations(index);\n+            if (fieldTypeAnnotations != null) {\n+                fieldAttributeCount++;\n+            }\n+\n@@ -429,0 +439,8 @@\n+\n+            if (fieldAnnotations != null) {\n+                writeAnnotationAttribute(\"RuntimeVisibleAnnotations\", fieldAnnotations);\n+            }\n+\n+            if (fieldTypeAnnotations != null) {\n+                writeAnnotationAttribute(\"RuntimeVisibleTypeAnnotations\", fieldTypeAnnotations);\n+            }\n@@ -496,0 +514,20 @@\n+        final U1Array annotations = m.getAnnotations();\n+        if (annotations != null) {\n+            methodAttributeCount++;\n+        }\n+\n+        final U1Array parameterAnnotations = m.getParameterAnnotations();\n+        if (parameterAnnotations != null) {\n+            methodAttributeCount++;\n+        }\n+\n+        final U1Array typeAnnotations = m.getTypeAnnotations();\n+        if (typeAnnotations != null) {\n+            methodAttributeCount++;\n+        }\n+\n+        final U1Array annotationDefault = m.getAnnotationDefault();\n+        if (annotationDefault != null) {\n+            methodAttributeCount++;\n+        }\n+\n@@ -691,0 +729,16 @@\n+\n+        if (annotationDefault != null) {\n+           writeAnnotationAttribute(\"AnnotationDefault\", annotationDefault);\n+        }\n+\n+        if (annotations != null) {\n+           writeAnnotationAttribute(\"RuntimeVisibleAnnotations\", annotations);\n+        }\n+\n+        if (parameterAnnotations != null) {\n+           writeAnnotationAttribute(\"RuntimeVisibleParameterAnnotations\", parameterAnnotations);\n+        }\n+\n+        if (typeAnnotations != null) {\n+           writeAnnotationAttribute(\"RuntimeVisibleTypeAnnotations\", typeAnnotations);\n+        }\n@@ -735,0 +789,10 @@\n+        U1Array classAnnotations = klass.getClassAnnotations();\n+        if (classAnnotations != null) {\n+            classAttributeCount++;\n+        }\n+\n+        U1Array classTypeAnnotations = klass.getClassTypeAnnotations();\n+        if (classTypeAnnotations != null) {\n+            classAttributeCount++;\n+        }\n+\n@@ -796,0 +860,21 @@\n+\n+        if (classAnnotations != null) {\n+           writeAnnotationAttribute(\"RuntimeVisibleAnnotations\", classAnnotations);\n+        }\n+\n+        if (classTypeAnnotations != null) {\n+           writeAnnotationAttribute(\"RuntimeVisibleTypeAnnotations\", classTypeAnnotations);\n+        }\n+    }\n+\n+    protected void writeAnnotationAttribute(String annotationName, U1Array annotation) throws IOException {\n+      int length = annotation.length();\n+      Short annotationNameIndex = utf8ToIndex.get(annotationName);\n+      if (Assert.ASSERTS_ENABLED) {\n+        Assert.that(annotationNameIndex != null, \"should not be null\");\n+      }\n+      writeIndex(annotationNameIndex.shortValue());\n+      dos.writeInt(length);\n+      for (int index = 0; index < length; index++) {\n+        dos.writeByte(annotation.at(index));\n+      }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/tools\/jcore\/ClassWriter.java","additions":85,"deletions":0,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -111,0 +111,1 @@\n+applications\/ctw\/modules\/jdk_crypto_ec.java 8312194 generic-all\n@@ -128,0 +129,1 @@\n+serviceability\/jvmti\/vthread\/VThreadTLSTest\/VThreadTLSTest.java#id1 8300051 generic-all\n@@ -209,2 +211,0 @@\n-\n-vmTestbase\/nsk\/jdb\/interrupt\/interrupt001\/interrupt001.java 8310551 linux-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -201,1 +201,1 @@\n-        String optoRegex = \"(.*precise .*\\\\R((.*(?i:mov|xorl|nop|spill).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n+        String optoRegex = \"(.*precise .*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n@@ -207,1 +207,1 @@\n-        String regex = \"(.*precise .*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|xorl|nop|spill).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n+        String regex = \"(.*precise .*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n@@ -213,1 +213,1 @@\n-        String optoRegex = \"(.*precise \\\\[.*\\\\R((.*(?i:mov|xor|nop|spill).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n+        String optoRegex = \"(.*precise \\\\[.*\\\\R((.*(?i:mov|mv|xor|nop|spill).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n@@ -219,1 +219,1 @@\n-        String regex = \"(.*precise \\\\[.*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|xorl|nop|spill).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n+        String regex = \"(.*precise \\\\[.*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n@@ -280,1 +280,1 @@\n-        String regex = \"(((?i:cmp|CLFI|CLR).*precise \\\\[.*:|.*(?i:mov|or).*precise \\\\[.*:.*\\\\R.*(cmp|CMP|CLR))\" + END;\n+        String regex = \"(((?i:cmp|CLFI|CLR).*precise \\\\[.*:|.*(?i:mov|mv|or).*precise \\\\[.*:.*\\\\R.*(cmp|CMP|CLR))\" + END;\n@@ -286,1 +286,1 @@\n-        String regex = \"(((?i:cmp|CLFI|CLR).*precise \\\\[.*\" + IS_REPLACED + \":|.*(?i:mov|or).*precise \\\\[.*\" + IS_REPLACED + \":.*\\\\R.*(cmp|CMP|CLR))\" + END;\n+        String regex = \"(((?i:cmp|CLFI|CLR).*precise \\\\[.*\" + IS_REPLACED + \":|.*(?i:mov|mv|or).*precise \\\\[.*\" + IS_REPLACED + \":.*\\\\R.*(cmp|CMP|CLR))\" + END;\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -579,2 +579,0 @@\n-sun\/security\/tools\/keytool\/ListKeychainStore.sh                 8156889 macosx-all\n-\n","filename":"test\/jdk\/ProblemList.txt","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -148,1 +148,3 @@\n-        assertTrue(re.isInstance(_e), String.format(\"%sIncorrect throwable thrown, %s. Expected %s\", message, _e, re));\n+        if (!re.isInstance(_e)) {\n+            fail(String.format(\"%sIncorrect throwable thrown, %s. Expected %s\", message, _e, re), _e);\n+        }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleBaseTest.java","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -105,0 +105,1 @@\n+        map.put(\"vm.jvmci.enabled\", this::vmJvmciEnabled);\n@@ -125,0 +126,4 @@\n+        \/\/ jdk.hasLibgraal is true if the libgraal shared library file is present\n+        map.put(\"jdk.hasLibgraal\", this::hasLibgraal);\n+        \/\/ vm.libgraal.enabled is true if libgraal is used as JIT\n+        map.put(\"vm.libgraal.enabled\", this::isLibgraalEnabled);\n@@ -269,0 +274,14 @@\n+\n+    \/**\n+     * @return true if JVMCI is enabled\n+     *\/\n+    protected String vmJvmciEnabled() {\n+        \/\/ builds with jvmci have this flag\n+        if (\"false\".equals(vmJvmci())) {\n+            return \"false\";\n+        }\n+\n+        return \"\" + Compiler.isJVMCIEnabled();\n+    }\n+\n+\n@@ -475,0 +494,18 @@\n+    \/**\n+     * Check if the libgraal shared library file is present.\n+     *\n+     * @return true if the libgraal shared library file is present.\n+     *\/\n+    protected String hasLibgraal() {\n+        return \"\" + WB.hasLibgraal();\n+    }\n+\n+    \/**\n+     * Check if libgraal is used as JIT compiler.\n+     *\n+     * @return true if libgraal is used as JIT compiler.\n+     *\/\n+    protected String isLibgraalEnabled() {\n+        return \"\" + Compiler.isLibgraalEnabled();\n+    }\n+\n@@ -698,0 +735,1 @@\n+        Collections.sort(lines);\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -320,0 +320,3 @@\n+\n+  \/\/ Determines if the libgraal shared library file is present.\n+  public native boolean hasLibgraal();\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"}]}