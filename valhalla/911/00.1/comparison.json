{"files":[{"patch":"@@ -751,0 +751,1 @@\n+BUILD_HOTSPOT_JTREG_LIBRARIES_CFLAGS_libobjmonusage007 := $(NSK_JVMTI_AGENT_INCLUDES)\n@@ -1461,0 +1462,1 @@\n+  BUILD_HOTSPOT_JTREG_LIBRARIES_LIBS_libobjmonusage007 += -lpthread\n","filename":"make\/test\/JtregNativeHotspot.gmk","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -57,0 +58,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +61,1 @@\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -1124,0 +1127,35 @@\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  ldr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  ldr(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  ldr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass, temp_reg);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset);\n+  load_heap_oop(obj, field, inline_klass, rscratch2);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n@@ -1473,1 +1511,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1506,1 +1548,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1604,0 +1650,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -1649,0 +1699,110 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  assert_different_registers(markword, rscratch2);\n+  andr(markword, markword, markWord::inline_type_mask_in_place);\n+  mov(rscratch2, markWord::inline_type_pattern);\n+  cmp(markword, rscratch2);\n+  br(Assembler::EQ, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  andr(temp_reg, temp_reg, JVM_ACC_VALUE);\n+  cbnz(temp_reg, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  assert_different_registers(tmp, rscratch1);\n+  cbz(object, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  ldr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  mov(rscratch1, is_inline_type_mask);\n+  andr(tmp, tmp, rscratch1);\n+  cmp(tmp, rscratch1);\n+  br(Assembler::NE, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  ldrw(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  andr(temp_reg, temp_reg, InstanceKlassFlags::is_empty_inline_type_value());\n+  cbnz(temp_reg, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_null_free_inline_type_shift, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbz(flags, ConstantPoolCacheEntry::is_null_free_inline_type_shift, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_inlined_shift, is_flattened);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  ldr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  tst(temp_reg, markWord::unlocked_value);\n+  br(Assembler::NE, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  load_prototype_header(temp_reg, oop);\n+\n+  bind(test_mark_word);\n+  andr(temp_reg, temp_reg, test_bit);\n+  if (jmp_set) {\n+    cbnz(temp_reg, jmp_label);\n+  } else {\n+    cbz(temp_reg, jmp_label);\n+  }\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label& is_flattened_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flattened_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::NE, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::EQ, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {\n+  tst(lh, Klass::_lh_null_free_array_bit_inplace);\n+  br(Assembler::NE, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {\n+  tst(lh, Klass::_lh_null_free_array_bit_inplace);\n+  br(Assembler::EQ, is_non_null_free_array);\n+}\n+\n@@ -4314,0 +4474,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -4372,0 +4540,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src) {\n+  load_klass(dst, src);\n+  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -4696,0 +4869,40 @@\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  ldr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  ldrw(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    add(data, data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rscratch1, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  ldrw(rscratch1, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  lsr(rscratch1, rscratch1, Klass::_lh_log2_element_size_shift);\n+  andr(rscratch1, rscratch1, Klass::_lh_log2_element_size_mask);\n+  lslv(index, index, rscratch1);\n+\n+  add(data, array, index);\n+  add(data, data, arrayOopDesc::base_offset_in_bytes(T_PRIMITIVE_OBJECT));\n+}\n+\n@@ -4772,0 +4985,96 @@\n+\/\/ Object \/ value buffer allocation...\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == r0, \"needs to be r0\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  ldrw(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  tst(layout_size, Klass::_lh_instance_slow_path_bit);\n+  br(Assembler::NE, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  if (UseTLAB) {\n+    push(klass);\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      b(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      b(initialize_object);\n+    }\n+\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      subs(layout_size, layout_size, sizeof(oopDesc));\n+      br(Assembler::EQ, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      tst(layout_size, 7);\n+      br(Assembler::EQ, L);\n+      stop(\"object size is not multiple of 8 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      lsr(layout_size, layout_size, LogBytesPerLong);\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        Register base = t2;\n+\n+        bind(loop);\n+        add(rscratch1, new_obj, layout_size, Assembler::LSL, LogBytesPerLong);\n+        str(zr, Address(rscratch1, sizeof(oopDesc) - 1*oopSize));\n+        subs(layout_size, layout_size, 1);\n+        br(Assembler::NE, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    ldr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes ()));\n+    store_klass_gap(new_obj, zr);  \/\/ zero klass gap for compressed oops\n+    mov(t2, klass);         \/\/ preserve klass\n+    store_klass(new_obj, t2);  \/\/ src klass reg is potentially compressed\n+\n+    \/\/ TODO: Valhalla removed SharedRuntime::dtrace_object_alloc from here ?\n+\n+    b(done);\n+  }\n+\n+  if (UseTLAB) {\n+    bind(slow_case);\n+    pop(klass);\n+  }\n+  bind(slow_case_no_pop);\n+  b(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -4811,0 +5120,13 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  ldr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cbnz(inline_klass, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  ldr(inline_klass, Address(inline_klass, index, Address::lsl(3)));\n+}\n+\n@@ -4936,0 +5258,51 @@\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  if (needs_stack_repair) {\n+    \/\/ Remove the extension of the caller's frame used for inline type unpacking\n+    \/\/\n+    \/\/ Right now the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Saved LR #1               |\n+    \/\/ | Saved FP #1               |\n+    \/\/ |---------------------------|\n+    \/\/ | Extension space for       |\n+    \/\/ |   inline arg (un)packing  |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | Saved LR #2               |\n+    \/\/ | Saved FP #2               |\n+    \/\/ |---------------------------|  <-- FP\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ There are two copies of FP and LR on the stack. They will be identical\n+    \/\/ unless the caller has been deoptimized, in which case LR #1 will be patched\n+    \/\/ to point at the deopt blob, and LR #2 will still point into the old method.\n+    \/\/\n+    \/\/ The sp_inc stack slot holds the total size of the frame including the\n+    \/\/ extension space minus two words for the saved FP and LR.\n+\n+    int sp_inc_offset = initial_framesize - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+    ldr(rscratch1, Address(sp, sp_inc_offset));\n+    add(sp, sp, rscratch1);\n+    ldp(rfp, lr, Address(post(sp, 2 * wordSize)));\n+  } else {\n+    remove_frame(initial_framesize);\n+  }\n+}\n+\n+void MacroAssembler::save_stack_increment(int sp_inc, int frame_size) {\n+  int real_frame_size = frame_size + sp_inc;\n+  assert(sp_inc == 0 || sp_inc > 2*wordSize, \"invalid sp_inc value\");\n+  assert(real_frame_size >= 2*wordSize, \"frame size must include FP\/LR space\");\n+  assert((real_frame_size & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+\n+  int sp_inc_offset = frame_size - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+  \/\/ Subtract two words for the saved FP and LR as these will be popped\n+  \/\/ separately. See remove_frame above.\n+  mov(rscratch1, real_frame_size - 2*wordSize);\n+  str(rscratch1, Address(sp, sp_inc_offset));\n+}\n@@ -5850,0 +6223,441 @@\n+#ifdef COMPILER2\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+\n+    mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n+    clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    bind(L_skip_barrier);\n+  }\n+\n+  if (C->max_vector_size() > 0) {\n+    reinitialize_ptrue();\n+  }\n+\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  if (C->output()->need_stack_bang(bangsize))\n+    generate_stack_overflow_check(bangsize);\n+\n+  \/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->output()->frame_size_in_bytes();\n+  build_frame(framesize);\n+\n+  if (C->needs_stack_repair()) {\n+    save_stack_increment(sp_inc, framesize);\n+  }\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+#endif \/\/ COMPILER2\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  tbz(r0, 0, skip);\n+  int call_offset = -1;\n+\n+  \/\/ Be careful not to clobber r1-7 which hold returned fields\n+  \/\/ Also do not use callee-saved registers as these may be live in the interpreter\n+  Register tmp1 = r13, tmp2 = r14, klass = r15, r0_preserved = r12;\n+\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != NULL, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(r0_preserved, r0); \/\/ save r0 for slow_case since *_allocate may corrupt it when allocation failed\n+\n+  if (vk != NULL) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(klass, (intptr_t)vk->get_InlineKlass());\n+    jint obj_size = vk->layout_helper();\n+    assert(obj_size != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB) {\n+      tlab_allocate(r0, noreg, obj_size, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+    andr(klass, r0, -2);\n+    ldrw(tmp2, Address(klass, Klass::layout_helper_offset()));\n+    if (UseTLAB) {\n+      tlab_allocate(r0, tmp2, 0, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = r0;\n+    mov(rscratch1, (intptr_t)markWord::inline_type_prototype().value());\n+    str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+    store_klass_gap(buffer_obj, zr);\n+    if (vk == NULL) {\n+      \/\/ store_klass corrupts klass, so save it for later use (interpreter case only).\n+      mov(tmp1, klass);\n+    }\n+    store_klass(buffer_obj, klass);\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != NULL) {\n+      far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      \/\/ tmp1 holds klass preserved above\n+      ldr(tmp1, Address(tmp1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      ldr(tmp1, Address(tmp1, InlineKlass::pack_handler_offset()));\n+      blr(tmp1);\n+    }\n+\n+    membar(Assembler::StoreStore);\n+    b(skip);\n+  } else {\n+    \/\/ Must have already branched to slow_case above.\n+    DEBUG_ONLY(should_not_reach_here());\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(r0, r0_preserved);\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    far_call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+  membar(Assembler::StoreStore);\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_Register() && to->is_Register()) {\n+          mov(to->as_Register(), from->as_Register());\n+        } else if (from->is_FloatRegister() && to->is_FloatRegister()) {\n+          fmovd(to->as_FloatRegister(), from->as_FloatRegister());\n+        } else {\n+          ShouldNotReachHere();\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+            ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  int sp_inc = args_on_stack * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  assert(sp_inc > 0, \"sanity\");\n+\n+  \/\/ Save a copy of the FP and LR here for deoptimization patching and frame walking\n+  stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+\n+  \/\/ Adjust the stack pointer. This will be repaired on return by MacroAssembler::remove_frame\n+  if (sp_inc < (1 << 9)) {\n+    sub(sp, sp, sp_inc);   \/\/ Fits in an immediate\n+  } else {\n+    mov(rscratch1, sp_inc);\n+    sub(sp, sp, rscratch1);\n+  }\n+\n+  return sp_inc + 2 * wordSize;  \/\/ Account for the FP\/LR space\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r11;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(tmp1, Address(sp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        cbz(fromReg, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        mov(tmp2, 1);\n+        str(tmp2, Address(sp, st_off));\n+      } else {\n+        mov(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_FloatRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr, rscratch1, rscratch2);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        str(dst, Address(sp, st_off));\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      ldrd(toReg->as_FloatRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      ldrs(toReg->as_FloatRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      b(L_notNull);\n+      bind(L_null);\n+      \/\/ Set IsInit field to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+            str(zr, Address(sp, st_off));\n+          } else {\n+            mov(toReg->as_Register(), zr);\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_PRIMITIVE_OBJECT, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ The GC barrier expanded by store_heap_oop below may call into the\n+  \/\/ runtime so use callee-saved registers for any values that need to be\n+  \/\/ preserved. The GC barrier assembler should take care of saving the\n+  \/\/ Java argument registers.\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r21;\n+  Register from_reg_tmp = r22;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r12;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_PRIMITIVE_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index), tmp1, tmp2);\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldrb(tmp2, Address(sp, ld_off));\n+        cbnz(tmp2, L_notNull);\n+      } else {\n+        cbnz(fromReg->as_Register(), L_notNull);\n+      }\n+      mov(val_obj, 0);\n+      b(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+\n+    if (!fromReg->is_FloatRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        load_sized_value(src, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      strd(fromReg->as_FloatRegister(), dst);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      strs(fromReg->as_FloatRegister(), dst);\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v8->as_VMReg() : r14->as_VMReg();\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":816,"deletions":2,"binary":false,"changes":818,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -36,0 +37,4 @@\n+#include \"runtime\/signature.hpp\"\n+\n+\n+class ciInlineKlass;\n@@ -617,0 +622,31 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type);\n+  void test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type);\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type);\n+\n+  \/\/ Get the default value oop for the given InlineKlass\n+  void get_default_value_oop(Register inline_klass, Register temp_reg, Register obj);\n+  \/\/ The empty value oop, for the given InlineKlass (\"empty\" as in no instance fields)\n+  \/\/ get_default_value_oop with extra assertion for empty inline klass\n+  void get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj);\n+\n+  void test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free);\n+  void test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free);\n+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened);\n+\n+  \/\/ Check oops for special arrays, i.e. flattened and\/or null-free\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flattened_array_oop(Register klass, Register temp_reg, Label& is_flattened_array);\n+  void test_non_flattened_array_oop(Register oop, Register temp_reg, Label&is_non_flattened_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flatten or null-free arrays...\n+  void test_flattened_array_layout(Register lh, Label& is_flattened_array);\n+  void test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array);\n+  void test_null_free_array_layout(Register lh, Label& is_null_free_array);\n+  void test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array);\n+\n@@ -853,0 +889,2 @@\n+  void load_metadata(Register dst, Register src);\n+\n@@ -867,0 +905,9 @@\n+  void access_value_copy(DecoratorSet decorators, Register src, Register dst, Register inline_klass);\n+\n+  \/\/ inline type data payload offsets...\n+  void first_field_offset(Register inline_klass, Register offset);\n+  void data_for_oop(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -880,0 +927,2 @@\n+  void load_prototype_header(Register dst, Register src);\n+\n@@ -927,0 +976,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -937,0 +995,3 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n@@ -1320,0 +1381,18 @@\n+  void verified_entry(Compile* C, int sp_inc);\n+\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[], Register val_array);\n+  int extend_stack_for_inline_args(int args_on_stack);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair);\n+  VMReg spill_reg_for(VMReg reg);\n+  void save_stack_increment(int sp_inc, int frame_size);\n+\n@@ -1384,0 +1463,2 @@\n+  void fill_words(Register base, uint64_t cnt, Register value);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":81,"deletions":0,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -315,1 +315,1 @@\n-    \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+    \/\/ T_OBJECT, T_PRIMITIVE_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n@@ -318,4 +318,13 @@\n-    __ ldr(j_rarg2, result);\n-    Label is_long, is_float, is_double, exit;\n-    __ ldr(j_rarg1, result_type);\n-    __ cmp(j_rarg1, (u1)T_OBJECT);\n+    \/\/ All of j_rargN may be used to return inline type fields so be careful\n+    \/\/ not to clobber those.\n+    \/\/ SharedRuntime::generate_buffered_inline_type_adapter() knows the register\n+    \/\/ assignment of Rresult below.\n+    Register Rresult = r14, Rresult_type = r15;\n+    __ ldr(Rresult, result);\n+    Label is_long, is_float, is_double, check_prim, exit;\n+    __ ldr(Rresult_type, result_type);\n+    __ cmp(Rresult_type, (u1)T_OBJECT);\n+    __ br(Assembler::EQ, check_prim);\n+    __ cmp(Rresult_type, (u1)T_PRIMITIVE_OBJECT);\n+    __ br(Assembler::EQ, check_prim);\n+    __ cmp(Rresult_type, (u1)T_LONG);\n@@ -323,3 +332,1 @@\n-    __ cmp(j_rarg1, (u1)T_LONG);\n-    __ br(Assembler::EQ, is_long);\n-    __ cmp(j_rarg1, (u1)T_FLOAT);\n+    __ cmp(Rresult_type, (u1)T_FLOAT);\n@@ -327,1 +334,1 @@\n-    __ cmp(j_rarg1, (u1)T_DOUBLE);\n+    __ cmp(Rresult_type, (u1)T_DOUBLE);\n@@ -331,1 +338,1 @@\n-    __ strw(r0, Address(j_rarg2));\n+    __ strw(r0, Address(Rresult));\n@@ -379,0 +386,11 @@\n+    __ BIND(check_prim);\n+    if (InlineTypeReturnedAsFields) {\n+      \/\/ Check for scalarized return value\n+      __ tbz(r0, 0, is_long);\n+      \/\/ Load pack handler address\n+      __ andr(rscratch1, r0, -2);\n+      __ ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(rscratch1, Address(rscratch1, InlineKlass::pack_handler_jobject_offset()));\n+      __ blr(rscratch1);\n+      __ b(exit);\n+    }\n@@ -381,1 +399,1 @@\n-    __ str(r0, Address(j_rarg2, 0));\n+    __ str(r0, Address(Rresult, 0));\n@@ -385,1 +403,1 @@\n-    __ strs(j_farg0, Address(j_rarg2, 0));\n+    __ strs(j_farg0, Address(Rresult, 0));\n@@ -389,1 +407,1 @@\n-    __ strd(j_farg0, Address(j_rarg2, 0));\n+    __ strd(j_farg0, Address(Rresult, 0));\n@@ -2202,0 +2220,8 @@\n+    \/\/ Check for flat inline type array -> return -1\n+    __ tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+    __ br(Assembler::NE, L_failed);\n+\n+    \/\/ Check for null-free (non-flat) inline type array -> handle as object array\n+    __ tst(lh, Klass::_lh_null_free_array_bit_inplace);\n+    __ br(Assembler::NE, L_failed);\n+\n@@ -8259,0 +8285,128 @@\n+  \/\/ Call here from the interpreter or compiled code to either load\n+  \/\/ multiple returned values from the inline type instance being\n+  \/\/ returned to registers or to store returned values to a newly\n+  \/\/ allocated inline type instance.\n+  address generate_return_value_stub(address destination, const char* name, bool has_res) {\n+    \/\/ We need to save all registers the calling convention may use so\n+    \/\/ the runtime calls read or update those registers. This needs to\n+    \/\/ be in sync with SharedRuntime::java_return_convention().\n+    \/\/ n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0\n+    enum layout {\n+      j_rarg7_off = 0, j_rarg7_2,    \/\/ j_rarg7 is r0\n+      j_rarg6_off, j_rarg6_2,\n+      j_rarg5_off, j_rarg5_2,\n+      j_rarg4_off, j_rarg4_2,\n+      j_rarg3_off, j_rarg3_2,\n+      j_rarg2_off, j_rarg2_2,\n+      j_rarg1_off, j_rarg1_2,\n+      j_rarg0_off, j_rarg0_2,\n+\n+      j_farg7_off, j_farg7_2,\n+      j_farg6_off, j_farg6_2,\n+      j_farg5_off, j_farg5_2,\n+      j_farg4_off, j_farg4_2,\n+      j_farg3_off, j_farg3_2,\n+      j_farg2_off, j_farg2_2,\n+      j_farg1_off, j_farg1_2,\n+      j_farg0_off, j_farg0_2,\n+\n+      rfp_off, rfp_off2,\n+      return_off, return_off2,\n+\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    CodeBuffer code(name, 512, 64);\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+\n+    int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+    assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+    int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+    int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+    OopMapSet* oop_maps = new OopMapSet();\n+    OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg7_off), j_rarg7->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg6_off), j_rarg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+    address start = __ pc();\n+\n+    __ enter(); \/\/ Save FP and LR before call\n+\n+    __ stpd(j_farg1, j_farg0, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg3, j_farg2, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg5, j_farg4, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg7, j_farg6, Address(__ pre(sp, -2 * wordSize)));\n+\n+    __ stp(j_rarg1, j_rarg0, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg3, j_rarg2, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg5, j_rarg4, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg7, j_rarg6, Address(__ pre(sp, -2 * wordSize)));\n+\n+    int frame_complete = __ offset();\n+\n+    \/\/ Set up last_Java_sp and last_Java_fp\n+    address the_pc = __ pc();\n+    __ set_last_Java_frame(sp, noreg, the_pc, rscratch1);\n+\n+    \/\/ Call runtime\n+    __ mov(c_rarg1, r0);\n+    __ mov(c_rarg0, rthread);\n+\n+    __ mov(rscratch1, destination);\n+    __ blr(rscratch1);\n+\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    __ reset_last_Java_frame(false);\n+\n+    __ ldp(j_rarg7, j_rarg6, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg5, j_rarg4, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg3, j_rarg2, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg1, j_rarg0, Address(__ post(sp, 2 * wordSize)));\n+\n+    __ ldpd(j_farg7, j_farg6, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg5, j_farg4, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg3, j_farg2, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg1, j_farg0, Address(__ post(sp, 2 * wordSize)));\n+\n+    __ leave();\n+\n+    \/\/ check for pending exceptions\n+    Label pending;\n+    __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ cbnz(rscratch1, pending);\n+\n+    if (has_res) {\n+      __ get_vm_result(r0, rthread);\n+    }\n+\n+    __ ret(lr);\n+\n+    __ bind(pending);\n+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+    \/\/ -------------\n+    \/\/ make sure all code is generated\n+    masm->flush();\n+\n+    RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &code, frame_complete, frame_size_in_words, oop_maps, false);\n+    return stub->entry_point();\n+  }\n+\n@@ -8314,0 +8468,7 @@\n+\n+    if (InlineTypeReturnedAsFields) {\n+      StubRoutines::_load_inline_type_fields_in_regs =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), \"load_inline_type_fields_in_regs\", false);\n+      StubRoutines::_store_inline_type_fields_to_buf =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), \"store_inline_type_fields_to_buf\", true);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":174,"deletions":13,"binary":false,"changes":187,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -54,0 +55,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -56,0 +58,4 @@\n+#include \"vmreg_x86.inline.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1682,0 +1688,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2856,0 +2866,140 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  testl(temp_reg, JVM_ACC_VALUE);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  testptr(object, object);\n+  jcc(Assembler::zero, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  movl(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  testl(temp_reg, InstanceKlassFlags::is_empty_inline_type_value());\n+  jcc(Assembler::notZero, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_null_free_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_null_free_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::zero, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inlined_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inlined);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,\n+                                              Label&is_flattened_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flattened_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flattened_array_layout(temp_reg, is_flattened_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flattened_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flattened_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flattened_array_layout(temp_reg, is_non_flattened_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_null_free_array_layout(temp_reg, is_null_free_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_null_free_array_layout(temp_reg, is_non_null_free_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_array_bit_inplace);\n+  jcc(Assembler::notZero, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_array_bit_inplace);\n+  jcc(Assembler::zero, is_non_null_free_array);\n+}\n+\n+\n@@ -3960,0 +4110,114 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  push(klass);\n+  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(klass);\n+#ifndef _LP64\n+  if (UseTLAB) {\n+    get_thread(thread);\n+  }\n+#endif \/\/ _LP64\n+\n+  if (UseTLAB) {\n+    tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    jmp(slow_case);\n+  }\n+\n+  \/\/ If UseTLAB is true, the object is created above and there is an initialize need.\n+  \/\/ Otherwise, skip and go to the slow path.\n+  if (UseTLAB) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      decrement(layout_size, sizeof(oopDesc));\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 1*oopSize), zero);\n+        NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 2*oopSize), zero));\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+#ifdef _LP64\n+    xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+#endif\n+    movptr(t2, klass);         \/\/ preserve klass\n+    store_klass(new_obj, t2, rscratch1);  \/\/ src klass reg is potentially compressed\n+\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -4212,0 +4476,50 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  movptr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(inline_klass, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  movptr(inline_klass, Address(inline_klass, index, Address::times_ptr));\n+}\n+\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  movptr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  movl(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  movptr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset, Address::times_1);\n+  load_heap_oop(obj, field);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n+\n@@ -4679,1 +4993,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4741,1 +5059,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -5228,0 +5550,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5237,1 +5567,6 @@\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+}\n+\n+void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {\n+  load_klass(dst, src, tmp);\n+  movptr(dst, Address(dst, Klass::prototype_header_offset()));\n@@ -5276,0 +5611,40 @@\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_PRIMITIVE_OBJECT)));\n+}\n+\n@@ -5615,1 +5990,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, KRegister mask) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, KRegister mask) {\n@@ -5621,1 +5996,1 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_512bit);\n+    evpbroadcastq(xtmp, val, AVX_512bit);\n@@ -5623,1 +5998,3 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -5625,1 +6002,2 @@\n-    pxor(xtmp, xtmp);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n@@ -5648,1 +6026,1 @@\n-    fill64_masked(3, base, 0, xtmp, mask, cnt, rtmp, true);\n+    fill64_masked(3, base, 0, xtmp, mask, cnt, val, true);\n@@ -5667,1 +6045,1 @@\n-    fill32_masked(3, base, 0, xtmp, mask, cnt, rtmp);\n+    fill32_masked(3, base, 0, xtmp, mask, cnt, val);\n@@ -5680,0 +6058,398 @@\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(rscratch1, rax); \/\/ save rax for slow_case since *_allocate may corrupt it when allocation failed\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+    jint obj_size = vk->layout_helper();\n+    assert(obj_size != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB) {\n+      tlab_allocate(r15_thread, rax, noreg, obj_size, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+    mov(rbx, rax);\n+    andptr(rbx, -2);\n+    movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+    if (UseTLAB) {\n+      tlab_allocate(r15_thread, rax, r14, 0, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = rax;\n+    movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+    xorl(r13, r13);\n+    store_klass_gap(buffer_obj, r13);\n+    if (vk == nullptr) {\n+      \/\/ store_klass corrupts rbx(klass), so save it in r13 for later use (interpreter case only).\n+      mov(r13, rbx);\n+    }\n+    store_klass(buffer_obj, rbx, rscratch1);\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(r13, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(rax, rscratch1);\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  \/\/ Two additional slots to account for return address\n+  int sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  \/\/ Save the return address, adjust the stack (make sure it is properly\n+  \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+  \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n+  assert(sp_inc > 0, \"sanity\");\n+  pop(r13);\n+  subptr(rsp, sp_inc);\n+  push(r13);\n+  return sp_inc;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(tmp1, Address(rsp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        testptr(fromReg, fromReg);\n+        jcc(Assembler::zero, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), 1);\n+      } else {\n+        movq(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      jmp(L_notNull);\n+      bind(L_null);\n+      \/\/ Set IsInit field to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+            movq(Address(rsp, st_off), 0);\n+          } else {\n+            xorq(toReg->as_Register(), toReg->as_Register());\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_PRIMITIVE_OBJECT, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14;\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_PRIMITIVE_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        testb(Address(rsp, ld_off), 1);\n+      } else {\n+        testb(fromReg->as_Register(), 1);\n+      }\n+      jcc(Assembler::notZero, L_notNull);\n+      movptr(val_obj, 0);\n+      jmp(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    movq(rbp, Address(rsp, initial_framesize));\n+    \/\/ The stack increment resides just below the saved rbp\n+    addq(rsp, Address(rsp, initial_framesize - wordSize));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n@@ -5769,2 +6545,2 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp,\n-                               bool is_large, KRegister mask) {\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp,\n+                               bool is_large, bool word_copy_only, KRegister mask) {\n@@ -5775,1 +6551,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"val register must be eax for rep stos\");\n@@ -5781,3 +6557,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n@@ -5797,1 +6570,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -5806,1 +6579,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -5810,1 +6583,1 @@\n-    xmm_clear_mem(base, cnt, tmp, xtmp, mask);\n+    xmm_clear_mem(base, cnt, val, xtmp, mask);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":791,"deletions":18,"binary":false,"changes":809,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -36,0 +37,2 @@\n+class ciInlineKlass;\n+\n@@ -105,0 +108,31 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type);\n+  void test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type);\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type);\n+\n+  \/\/ Get the default value oop for the given InlineKlass\n+  void get_default_value_oop(Register inline_klass, Register temp_reg, Register obj);\n+  \/\/ The empty value oop, for the given InlineKlass (\"empty\" as in no instance fields)\n+  \/\/ get_default_value_oop with extra assertion for empty inline klass\n+  void get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj);\n+\n+  void test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free);\n+  void test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free);\n+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined);\n+\n+  \/\/ Check oops for special arrays, i.e. flattened and\/or null-free\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flattened_array_oop(Register oop, Register temp_reg, Label&is_flattened_array);\n+  void test_non_flattened_array_oop(Register oop, Register temp_reg, Label&is_non_flattened_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flatten or null-free arrays...\n+  void test_flattened_array_layout(Register lh, Label& is_flattened_array);\n+  void test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array);\n+  void test_null_free_array_layout(Register lh, Label& is_null_free_array);\n+  void test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array);\n+\n@@ -366,0 +400,1 @@\n+  void load_metadata(Register dst, Register src);\n@@ -374,0 +409,10 @@\n+  void access_value_copy(DecoratorSet decorators, Register src, Register dst, Register inline_klass);\n+\n+  \/\/ inline type data payload offsets...\n+  void first_field_offset(Register inline_klass, Register offset);\n+  void data_for_oop(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n+\n@@ -385,0 +430,2 @@\n+  void load_prototype_header(Register dst, Register src, Register tmp);\n+\n@@ -586,0 +633,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -597,0 +653,3 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n@@ -757,1 +816,2 @@\n-  void andptr(Register src1, Register src2) { LP64_ONLY(andq(src1, src2)) NOT_LP64(andl(src1, src2)) ; }\n+  void andptr(Register dst, Register src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n+  void andptr(Register dst, Address src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n@@ -1857,0 +1917,15 @@\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[], Register val_array);\n+  int extend_stack_for_inline_args(int args_on_stack);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair);\n+  VMReg spill_reg_for(VMReg reg);\n+\n@@ -1859,1 +1934,1 @@\n-  void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large, KRegister mask=knoreg);\n+  void clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only, KRegister mask=knoreg);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":77,"deletions":2,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"asm\/assembler.hpp\"\n@@ -39,0 +40,2 @@\n+#include \"utilities\/macros.hpp\"\n+#include \"vmreg_x86.inline.hpp\"\n@@ -317,5 +320,9 @@\n-  \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n-  __ movptr(c_rarg0, result);\n-  Label is_long, is_float, is_double, exit;\n-  __ movl(c_rarg1, result_type);\n-  __ cmpl(c_rarg1, T_OBJECT);\n+  \/\/ T_OBJECT, T_PRIMITIVE_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+  __ movptr(r13, result);\n+  Label is_long, is_float, is_double, check_prim, exit;\n+  __ movl(rbx, result_type);\n+  __ cmpl(rbx, T_OBJECT);\n+  __ jcc(Assembler::equal, check_prim);\n+  __ cmpl(rbx, T_PRIMITIVE_OBJECT);\n+  __ jcc(Assembler::equal, check_prim);\n+  __ cmpl(rbx, T_LONG);\n@@ -323,3 +330,1 @@\n-  __ cmpl(c_rarg1, T_LONG);\n-  __ jcc(Assembler::equal, is_long);\n-  __ cmpl(c_rarg1, T_FLOAT);\n+  __ cmpl(rbx, T_FLOAT);\n@@ -327,1 +332,1 @@\n-  __ cmpl(c_rarg1, T_DOUBLE);\n+  __ cmpl(rbx, T_DOUBLE);\n@@ -331,1 +336,1 @@\n-  __ movl(Address(c_rarg0, 0), rax);\n+  __ movl(Address(r13, 0), rax);\n@@ -395,0 +400,13 @@\n+  __ BIND(check_prim);\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check for scalarized return value\n+    __ testptr(rax, 1);\n+    __ jcc(Assembler::zero, is_long);\n+    \/\/ Load pack handler address\n+    __ andptr(rax, -2);\n+    __ movptr(rax, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    __ movptr(rbx, Address(rax, InlineKlass::pack_handler_jobject_offset()));\n+    \/\/ Call pack handler to initialize the buffer\n+    __ call(rbx);\n+    __ jmp(exit);\n+  }\n@@ -396,1 +414,1 @@\n-  __ movq(Address(c_rarg0, 0), rax);\n+  __ movq(Address(r13, 0), rax);\n@@ -400,1 +418,1 @@\n-  __ movflt(Address(c_rarg0, 0), xmm0);\n+  __ movflt(Address(r13, 0), xmm0);\n@@ -404,1 +422,1 @@\n-  __ movdbl(Address(c_rarg0, 0), xmm0);\n+  __ movdbl(Address(r13, 0), xmm0);\n@@ -3922,0 +3940,10 @@\n+  \/\/ Generate these first because they are called from other stubs\n+  if (InlineTypeReturnedAsFields) {\n+    StubRoutines::_load_inline_type_fields_in_regs =\n+      generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs),\n+                                 \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf =\n+      generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf),\n+                                 \"store_inline_type_fields_to_buf\", true);\n+  }\n+\n@@ -3987,0 +4015,144 @@\n+\/\/ Call here from the interpreter or compiled code to either load\n+\/\/ multiple returned values from the inline type instance being\n+\/\/ returned to registers or to store returned values to a newly\n+\/\/ allocated inline type instance.\n+\/\/ Register is a class, but it would be assigned numerical value.\n+\/\/ \"0\" is assigned for xmm0. Thus we need to ignore -Wnonnull.\n+PRAGMA_DIAG_PUSH\n+PRAGMA_NONNULL_IGNORED\n+address StubGenerator::generate_return_value_stub(address destination, const char* name, bool has_res) {\n+  \/\/ We need to save all registers the calling convention may use so\n+  \/\/ the runtime calls read or update those registers. This needs to\n+  \/\/ be in sync with SharedRuntime::java_return_convention().\n+  enum layout {\n+    pad_off = frame::arg_reg_save_area_bytes\/BytesPerInt, pad_off_2,\n+    rax_off, rax_off_2,\n+    j_rarg5_off, j_rarg5_2,\n+    j_rarg4_off, j_rarg4_2,\n+    j_rarg3_off, j_rarg3_2,\n+    j_rarg2_off, j_rarg2_2,\n+    j_rarg1_off, j_rarg1_2,\n+    j_rarg0_off, j_rarg0_2,\n+    j_farg0_off, j_farg0_2,\n+    j_farg1_off, j_farg1_2,\n+    j_farg2_off, j_farg2_2,\n+    j_farg3_off, j_farg3_2,\n+    j_farg4_off, j_farg4_2,\n+    j_farg5_off, j_farg5_2,\n+    j_farg6_off, j_farg6_2,\n+    j_farg7_off, j_farg7_2,\n+    rbp_off, rbp_off_2,\n+    return_off, return_off_2,\n+\n+    framesize\n+  };\n+\n+  CodeBuffer buffer(name, 1000, 512);\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+\n+  int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+  assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+  int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+  int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+  OopMapSet *oop_maps = new OopMapSet();\n+  OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+  map->set_callee_saved(VMRegImpl::stack2reg(rax_off), rax->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+  int start = __ offset();\n+\n+  __ subptr(rsp, frame_size_in_bytes - 8 \/* return address*\/);\n+\n+  __ movptr(Address(rsp, rbp_off * BytesPerInt), rbp);\n+  __ movdbl(Address(rsp, j_farg7_off * BytesPerInt), j_farg7);\n+  __ movdbl(Address(rsp, j_farg6_off * BytesPerInt), j_farg6);\n+  __ movdbl(Address(rsp, j_farg5_off * BytesPerInt), j_farg5);\n+  __ movdbl(Address(rsp, j_farg4_off * BytesPerInt), j_farg4);\n+  __ movdbl(Address(rsp, j_farg3_off * BytesPerInt), j_farg3);\n+  __ movdbl(Address(rsp, j_farg2_off * BytesPerInt), j_farg2);\n+  __ movdbl(Address(rsp, j_farg1_off * BytesPerInt), j_farg1);\n+  __ movdbl(Address(rsp, j_farg0_off * BytesPerInt), j_farg0);\n+\n+  __ movptr(Address(rsp, j_rarg0_off * BytesPerInt), j_rarg0);\n+  __ movptr(Address(rsp, j_rarg1_off * BytesPerInt), j_rarg1);\n+  __ movptr(Address(rsp, j_rarg2_off * BytesPerInt), j_rarg2);\n+  __ movptr(Address(rsp, j_rarg3_off * BytesPerInt), j_rarg3);\n+  __ movptr(Address(rsp, j_rarg4_off * BytesPerInt), j_rarg4);\n+  __ movptr(Address(rsp, j_rarg5_off * BytesPerInt), j_rarg5);\n+  __ movptr(Address(rsp, rax_off * BytesPerInt), rax);\n+\n+  int frame_complete = __ offset();\n+\n+  __ set_last_Java_frame(noreg, noreg, nullptr, rscratch1);\n+\n+  __ mov(c_rarg0, r15_thread);\n+  __ mov(c_rarg1, rax);\n+\n+  __ call(RuntimeAddress(destination));\n+\n+  \/\/ Set an oopmap for the call site.\n+\n+  oop_maps->add_gc_map( __ offset() - start, map);\n+\n+  \/\/ clear last_Java_sp\n+  __ reset_last_Java_frame(false);\n+\n+  __ movptr(rbp, Address(rsp, rbp_off * BytesPerInt));\n+  __ movdbl(j_farg7, Address(rsp, j_farg7_off * BytesPerInt));\n+  __ movdbl(j_farg6, Address(rsp, j_farg6_off * BytesPerInt));\n+  __ movdbl(j_farg5, Address(rsp, j_farg5_off * BytesPerInt));\n+  __ movdbl(j_farg4, Address(rsp, j_farg4_off * BytesPerInt));\n+  __ movdbl(j_farg3, Address(rsp, j_farg3_off * BytesPerInt));\n+  __ movdbl(j_farg2, Address(rsp, j_farg2_off * BytesPerInt));\n+  __ movdbl(j_farg1, Address(rsp, j_farg1_off * BytesPerInt));\n+  __ movdbl(j_farg0, Address(rsp, j_farg0_off * BytesPerInt));\n+\n+  __ movptr(j_rarg0, Address(rsp, j_rarg0_off * BytesPerInt));\n+  __ movptr(j_rarg1, Address(rsp, j_rarg1_off * BytesPerInt));\n+  __ movptr(j_rarg2, Address(rsp, j_rarg2_off * BytesPerInt));\n+  __ movptr(j_rarg3, Address(rsp, j_rarg3_off * BytesPerInt));\n+  __ movptr(j_rarg4, Address(rsp, j_rarg4_off * BytesPerInt));\n+  __ movptr(j_rarg5, Address(rsp, j_rarg5_off * BytesPerInt));\n+  __ movptr(rax, Address(rsp, rax_off * BytesPerInt));\n+\n+  __ addptr(rsp, frame_size_in_bytes-8);\n+\n+  \/\/ check for pending exceptions\n+  Label pending;\n+  __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+  __ jcc(Assembler::notEqual, pending);\n+\n+  if (has_res) {\n+    __ get_vm_result(rax, r15_thread);\n+  }\n+\n+  __ ret(0);\n+\n+  __ bind(pending);\n+\n+  __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+  \/\/ -------------\n+  \/\/ make sure all code is generated\n+  _masm->flush();\n+\n+  RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_in_words, oop_maps, false);\n+  return stub->entry_point();\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":185,"deletions":13,"binary":false,"changes":198,"status":"modified"},{"patch":"@@ -553,0 +553,3 @@\n+  \/\/ interpreter or compiled code marshalling registers to\/from inline type instance\n+  address generate_return_value_stub(address destination, const char* name, bool has_res);\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -215,0 +217,2 @@\n+  assert(!_gen->in_conditional_code(), \"LIRItem cannot be loaded in conditional code\");\n+\n@@ -610,1 +614,2 @@\n-void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info) {\n+void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no,\n+                                 CodeEmitInfo* info_for_exception, CodeEmitInfo* info, CodeStub* throw_imse_stub) {\n@@ -613,1 +618,1 @@\n-  CodeStub* slow_path = new MonitorEnterStub(object, lock, info);\n+  CodeStub* slow_path = new MonitorEnterStub(object, lock, info, throw_imse_stub, scratch);\n@@ -616,1 +621,1 @@\n-  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception);\n+  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception, throw_imse_stub);\n@@ -640,4 +645,9 @@\n-void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n-  klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n-  \/\/ If klass is not loaded we do not know if the klass has finalizers:\n-  if (UseFastNewInstance && klass->is_loaded()\n+void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, bool allow_inline, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n+  if (allow_inline) {\n+    assert(!is_unresolved && klass->is_loaded(), \"inline type klass should be resolved\");\n+    __ metadata2reg(klass->constant_encoding(), klass_reg);\n+  } else {\n+    klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n+  }\n+  \/\/ If klass is not loaded we do not know if the klass has finalizers or is an unexpected inline klass\n+  if (UseFastNewInstance && klass->is_loaded() && (allow_inline || !klass->is_inlinetype())\n@@ -657,2 +667,2 @@\n-    CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, Runtime1::new_instance_id);\n-    __ branch(lir_cond_always, slow_path);\n+    CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, allow_inline ? Runtime1::new_instance_id : Runtime1::new_instance_no_inline_id);\n+    __ jump(slow_path);\n@@ -758,0 +768,10 @@\n+  if (!src->is_loaded_flattened_array() && !dst->is_loaded_flattened_array()) {\n+    flags &= ~LIR_OpArrayCopy::always_slow_path;\n+  }\n+  if (!src->maybe_flattened_array()) {\n+    flags &= ~LIR_OpArrayCopy::src_inlinetype_check;\n+  }\n+  if (!dst->maybe_flattened_array() && !dst->maybe_null_free_array()) {\n+    flags &= ~LIR_OpArrayCopy::dst_inlinetype_check;\n+  }\n+\n@@ -1563,2 +1583,4 @@\n-  _constants.append(c);\n-  _reg_for_constants.append(result);\n+  if (!in_conditional_code()) {\n+    _constants.append(c);\n+    _reg_for_constants.append(result);\n+  }\n@@ -1568,0 +1590,6 @@\n+void LIRGenerator::set_in_conditional_code(bool v) {\n+  assert(v != _in_conditional_code, \"must change state\");\n+  _in_conditional_code = v;\n+}\n+\n+\n@@ -1659,0 +1687,5 @@\n+  if (!inline_type_field_access_prolog(x)) {\n+    \/\/ Field store will always deopt due to unloaded field or holder klass\n+    return;\n+  }\n+\n@@ -1680,0 +1713,173 @@\n+\/\/ FIXME -- I can't find any other way to pass an address to access_load_at().\n+class TempResolvedAddress: public Instruction {\n+ public:\n+  TempResolvedAddress(ValueType* type, LIR_Opr addr) : Instruction(type) {\n+    set_operand(addr);\n+  }\n+  virtual void input_values_do(ValueVisitor*) {}\n+  virtual void visit(InstructionVisitor* v)   {}\n+  virtual const char* name() const  { return \"TempResolvedAddress\"; }\n+};\n+\n+LIR_Opr LIRGenerator::get_and_load_element_address(LIRItem& array, LIRItem& index) {\n+  ciType* array_type = array.value()->declared_type();\n+  ciFlatArrayKlass* flat_array_klass = array_type->as_flat_array_klass();\n+  assert(flat_array_klass->is_loaded(), \"must be\");\n+\n+  int array_header_size = flat_array_klass->array_header_in_bytes();\n+  int shift = flat_array_klass->log2_element_size();\n+\n+#ifndef _LP64\n+  LIR_Opr index_op = new_register(T_INT);\n+  \/\/ FIXME -- on 32-bit, the shift below can overflow, so we need to check that\n+  \/\/ the top (shift+1) bits of index_op must be zero, or\n+  \/\/ else throw ArrayIndexOutOfBoundsException\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::intConst(const_index << shift), index_op);\n+  } else {\n+    __ shift_left(index_op, shift, index.result());\n+  }\n+#else\n+  LIR_Opr index_op = new_register(T_LONG);\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::longConst(const_index << shift), index_op);\n+  } else {\n+    __ convert(Bytecodes::_i2l, index.result(), index_op);\n+    \/\/ Need to shift manually, as LIR_Address can scale only up to 3.\n+    __ shift_left(index_op, shift, index_op);\n+  }\n+#endif\n+\n+  LIR_Opr elm_op = new_pointer_register();\n+  LIR_Address* elm_address = generate_address(array.result(), index_op, 0, array_header_size, T_ADDRESS);\n+  __ leal(LIR_OprFact::address(elm_address), elm_op);\n+  return elm_op;\n+}\n+\n+void LIRGenerator::access_sub_element(LIRItem& array, LIRItem& index, LIR_Opr& result, ciField* field, int sub_offset) {\n+  assert(field != nullptr, \"Need a subelement type specified\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  BasicType subelt_type = field->type()->basic_type();\n+  TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(subelt_type), elm_op);\n+  LIRItem elm_item(elm_resolved_addr, this);\n+\n+  DecoratorSet decorators = IN_HEAP;\n+  access_load_at(decorators, subelt_type,\n+                     elm_item, LIR_OprFact::intConst(sub_offset), result,\n+                     nullptr, nullptr);\n+\n+  if (field->is_null_free()) {\n+    assert(field->type()->is_loaded(), \"Must be\");\n+    assert(field->type()->is_inlinetype(), \"Must be if loaded\");\n+    assert(field->type()->as_inline_klass()->is_initialized(), \"Must be\");\n+    LabelObj* L_end = new LabelObj();\n+    __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(nullptr));\n+    __ branch(lir_cond_notEqual, L_end->label());\n+    set_in_conditional_code(true);\n+    Constant* default_value = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+    if (default_value->is_pinned()) {\n+      __ move(LIR_OprFact::value_type(default_value->type()), result);\n+    } else {\n+      __ move(load_constant(default_value), result);\n+    }\n+    __ branch_destination(L_end->label());\n+    set_in_conditional_code(false);\n+  }\n+}\n+\n+void LIRGenerator::access_flattened_array(bool is_load, LIRItem& array, LIRItem& index, LIRItem& obj_item,\n+                                          ciField* field, int sub_offset) {\n+  assert(sub_offset == 0 || field != nullptr, \"Sanity check\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  ciInlineKlass* elem_klass = nullptr;\n+  if (field != nullptr) {\n+    elem_klass = field->type()->as_inline_klass();\n+  } else {\n+    elem_klass = array.value()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+  }\n+  for (int i = 0; i < elem_klass->nof_nonstatic_fields(); i++) {\n+    ciField* inner_field = elem_klass->nonstatic_field_at(i);\n+    assert(!inner_field->is_flattened(), \"flattened fields must have been expanded\");\n+    int obj_offset = inner_field->offset_in_bytes();\n+    int elm_offset = obj_offset - elem_klass->first_field_offset() + sub_offset; \/\/ object header is not stored in array.\n+    BasicType field_type = inner_field->type()->basic_type();\n+\n+    \/\/ Types which are smaller than int are still passed in an int register.\n+    BasicType reg_type = field_type;\n+    switch (reg_type) {\n+    case T_BYTE:\n+    case T_BOOLEAN:\n+    case T_SHORT:\n+    case T_CHAR:\n+      reg_type = T_INT;\n+      break;\n+    default:\n+      break;\n+    }\n+\n+    LIR_Opr temp = new_register(reg_type);\n+    TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(field_type), elm_op);\n+    LIRItem elm_item(elm_resolved_addr, this);\n+\n+    DecoratorSet decorators = IN_HEAP;\n+    if (is_load) {\n+      access_load_at(decorators, field_type,\n+                     elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                     nullptr, nullptr);\n+      access_store_at(decorators, field_type,\n+                      obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                      nullptr, nullptr);\n+    } else {\n+      access_load_at(decorators, field_type,\n+                     obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                     nullptr, nullptr);\n+      access_store_at(decorators, field_type,\n+                      elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                      nullptr, nullptr);\n+    }\n+  }\n+}\n+\n+void LIRGenerator::check_flattened_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path) {\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_flattened_array(array, value, tmp, slow_path);\n+}\n+\n+void LIRGenerator::check_null_free_array(LIRItem& array, LIRItem& value, CodeEmitInfo* info) {\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+  __ branch(lir_cond_equal, L_end->label());\n+  __ null_check(value.result(), info);\n+  __ branch_destination(L_end->label());\n+}\n+\n+bool LIRGenerator::needs_flattened_array_store_check(StoreIndexed* x) {\n+  if (x->elt_type() == T_OBJECT && x->array()->maybe_flattened_array()) {\n+    ciType* type = x->value()->declared_type();\n+    if (type != nullptr && type->is_klass()) {\n+      ciKlass* klass = type->as_klass();\n+      if (!klass->can_be_inline_klass() || (klass->is_inlinetype() && !klass->as_inline_klass()->flatten_array())) {\n+        \/\/ This is known to be a non-flattened object. If the array is flattened,\n+        \/\/ it will be caught by the code generated by array_store_check().\n+        return false;\n+      }\n+    }\n+    \/\/ We're not 100% sure, so let's do the flattened_array_store_check.\n+    return true;\n+  }\n+  return false;\n+}\n+\n+bool LIRGenerator::needs_null_free_array_store_check(StoreIndexed* x) {\n+  return x->elt_type() == T_OBJECT && x->array()->maybe_null_free_array();\n+}\n+\n@@ -1682,0 +1888,2 @@\n+  assert(x->elt_type() != T_ARRAY, \"never used\");\n+  bool is_loaded_flattened_array = x->array()->is_loaded_flattened_array();\n@@ -1685,3 +1893,3 @@\n-  bool needs_store_check = obj_store && (x->value()->as_Constant() == nullptr ||\n-                                         !get_jobject_constant(x->value())->is_null_object() ||\n-                                         x->should_profile());\n+  bool needs_store_check = obj_store && !(is_loaded_flattened_array && x->is_exact_flattened_array_store()) &&\n+                                        (x->value()->as_Constant() == nullptr ||\n+                                         !get_jobject_constant(x->value())->is_null_object());\n@@ -1700,2 +1908,3 @@\n-\n-  if (needs_store_check || x->check_boolean()) {\n+\n+  if (needs_store_check || x->check_boolean()\n+      || is_loaded_flattened_array || needs_flattened_array_store_check(x) || needs_null_free_array_store_check(x)) {\n@@ -1730,0 +1939,16 @@\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flattened_array()) {\n+      \/\/ No need to profile a store to a flattened array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      ciMethodData* md = nullptr;\n+      ciArrayLoadStoreData* load_store = nullptr;\n+      profile_array_type(x, md, load_store);\n+      if (x->array()->maybe_null_free_array()) {\n+        profile_null_free_array(array, md, load_store);\n+      }\n+      profile_element_type(x->value(), md, load_store);\n+    }\n+  }\n+\n@@ -1732,1 +1957,1 @@\n-    array_store_check(value.result(), array.result(), store_check_info, x->profiled_method(), x->profiled_bci());\n+    array_store_check(value.result(), array.result(), store_check_info, nullptr, -1);\n@@ -1735,4 +1960,21 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-  if (x->check_boolean()) {\n-    decorators |= C1_MASK_BOOLEAN;\n-  }\n+  if (is_loaded_flattened_array) {\n+    if (!x->value()->is_null_free()) {\n+      __ null_check(value.result(), new CodeEmitInfo(range_check_info));\n+    }\n+    \/\/ If array element is an empty inline type, no need to copy anything\n+    if (!x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+      access_flattened_array(false, array, index, value);\n+    }\n+  } else {\n+    StoreFlattenedArrayStub* slow_path = nullptr;\n+\n+    if (needs_flattened_array_store_check(x)) {\n+      \/\/ Check if we indeed have a flattened array\n+      index.load_item();\n+      slow_path = new StoreFlattenedArrayStub(array.result(), index.result(), value.result(), state_for(x, x->state_before()));\n+      check_flattened_array(array.result(), value.result(), slow_path);\n+      set_in_conditional_code(true);\n+    } else if (needs_null_free_array_store_check(x)) {\n+      CodeEmitInfo* info = new CodeEmitInfo(range_check_info);\n+      check_null_free_array(array, value, info);\n+    }\n@@ -1740,2 +1982,12 @@\n-  access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n-                  nullptr, null_check_info);\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    if (x->check_boolean()) {\n+      decorators |= C1_MASK_BOOLEAN;\n+    }\n+\n+    access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n+                    nullptr, null_check_info);\n+    if (slow_path != nullptr) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+  }\n@@ -1822,0 +2074,25 @@\n+bool LIRGenerator::inline_type_field_access_prolog(AccessField* x) {\n+  ciField* field = x->field();\n+  assert(!field->is_flattened(), \"Flattened field access should have been expanded\");\n+  if (!field->is_null_free()) {\n+    return true; \/\/ Not an inline type field\n+  }\n+  \/\/ Deoptimize if the access is non-static and requires patching (holder not loaded\n+  \/\/ or not accessible) because then we only have partial field information and the\n+  \/\/ field could be flattened (see ciField constructor).\n+  bool could_be_flat = !x->is_static() && x->needs_patching();\n+  \/\/ Deoptimize if we load from a static field with an uninitialized type because we\n+  \/\/ need to throw an exception if initialization of the type failed.\n+  bool not_initialized = x->is_static() && x->as_LoadField() != nullptr &&\n+      !field->type()->as_instance_klass()->is_initialized();\n+  if (could_be_flat || not_initialized) {\n+    CodeEmitInfo* info = state_for(x, x->state_before());\n+    CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),\n+                                        Deoptimization::Reason_unloaded,\n+                                        Deoptimization::Action_make_not_entrant);\n+    __ jump(stub);\n+    return false;\n+  }\n+  return true;\n+}\n+\n@@ -1851,0 +2128,7 @@\n+  if (!inline_type_field_access_prolog(x)) {\n+    \/\/ Field load will always deopt due to unloaded field or holder klass\n+    LIR_Opr result = rlock_result(x, field_type);\n+    __ move(LIR_OprFact::oopConst(nullptr), result);\n+    return;\n+  }\n+\n@@ -1879,0 +2163,34 @@\n+\n+  ciField* field = x->field();\n+  if (field->is_null_free()) {\n+    \/\/ Load from non-flattened inline type field requires\n+    \/\/ a null check to replace null with the default value.\n+    ciInstanceKlass* holder = field->holder();\n+    if (field->is_static() && holder->is_loaded()) {\n+      ciObject* val = holder->java_mirror()->field_value(field).as_object();\n+      if (!val->is_null_object()) {\n+        \/\/ Static field is initialized, we don't need to perform a null check.\n+        return;\n+      }\n+    }\n+    ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+    if (inline_klass->is_initialized()) {\n+      LabelObj* L_end = new LabelObj();\n+      __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(nullptr));\n+      __ branch(lir_cond_notEqual, L_end->label());\n+      set_in_conditional_code(true);\n+      Constant* default_value = new Constant(new InstanceConstant(inline_klass->default_instance()));\n+      if (default_value->is_pinned()) {\n+        __ move(LIR_OprFact::value_type(default_value->type()), result);\n+      } else {\n+        __ move(load_constant(default_value), result);\n+      }\n+      __ branch_destination(L_end->label());\n+      set_in_conditional_code(false);\n+    } else {\n+      info = state_for(x, x->state_before());\n+      __ cmp(lir_cond_equal, result, LIR_OprFact::oopConst(nullptr));\n+      __ branch(lir_cond_equal, new DeoptimizeStub(info, Deoptimization::Reason_uninitialized,\n+                                                         Deoptimization::Action_make_not_entrant));\n+    }\n+  }\n@@ -2023,1 +2341,68 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  ciMethodData* md = nullptr;\n+  ciArrayLoadStoreData* load_store = nullptr;\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flattened_array()) {\n+      \/\/ No need to profile a load from a flattened array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      profile_array_type(x, md, load_store);\n+    }\n+  }\n+\n+  Value element;\n+  if (x->vt() != nullptr) {\n+    assert(x->array()->is_loaded_flattened_array(), \"must be\");\n+    \/\/ Find the destination address (of the NewInlineTypeInstance).\n+    LIRItem obj_item(x->vt(), this);\n+\n+    access_flattened_array(true, array, index, obj_item,\n+                           x->delayed() == nullptr ? 0 : x->delayed()->field(),\n+                           x->delayed() == nullptr ? 0 : x->delayed()->offset());\n+    set_no_result(x);\n+  } else if (x->delayed() != nullptr) {\n+    assert(x->array()->is_loaded_flattened_array(), \"must be\");\n+    LIR_Opr result = rlock_result(x, x->delayed()->field()->type()->basic_type());\n+    access_sub_element(array, index, result, x->delayed()->field(), x->delayed()->offset());\n+  } else if (x->array() != nullptr && x->array()->is_loaded_flattened_array() &&\n+             x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_initialized() &&\n+             x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+    \/\/ Load the default instance instead of reading the element\n+    ciInlineKlass* elem_klass = x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    assert(elem_klass->is_initialized(), \"Must be\");\n+    Constant* default_value = new Constant(new InstanceConstant(elem_klass->default_instance()));\n+    if (default_value->is_pinned()) {\n+      __ move(LIR_OprFact::value_type(default_value->type()), result);\n+    } else {\n+      __ move(load_constant(default_value), result);\n+    }\n+  } else {\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    LoadFlattenedArrayStub* slow_path = nullptr;\n+\n+    if (x->should_profile() && x->array()->maybe_null_free_array()) {\n+      profile_null_free_array(array, md, load_store);\n+    }\n+\n+    if (x->elt_type() == T_OBJECT && x->array()->maybe_flattened_array()) {\n+      assert(x->delayed() == nullptr, \"Delayed LoadIndexed only apply to loaded_flattened_arrays\");\n+      index.load_item();\n+      \/\/ if we are loading from flattened array, load it using a runtime call\n+      slow_path = new LoadFlattenedArrayStub(array.result(), index.result(), result, state_for(x, x->state_before()));\n+      check_flattened_array(array.result(), LIR_OprFact::illegalOpr, slow_path);\n+      set_in_conditional_code(true);\n+    }\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    access_load_at(decorators, x->elt_type(),\n+                   array, index.result(), result,\n+                   nullptr, null_check_info);\n+\n+    if (slow_path != nullptr) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+\n+    element = x;\n+  }\n@@ -2025,4 +2410,3 @@\n-  LIR_Opr result = rlock_result(x, x->elt_type());\n-  access_load_at(decorators, x->elt_type(),\n-                 array, index.result(), result,\n-                 nullptr, null_check_info);\n+  if (x->should_profile()) {\n+    profile_element_type(element, md, load_store);\n+  }\n@@ -2031,0 +2415,12 @@\n+void LIRGenerator::do_Deoptimize(Deoptimize* x) {\n+  \/\/ This happens only when a class X uses the withfield\/aconst_init bytecode\n+  \/\/ to refer to an inline class V, where V has not yet been loaded\/resolved.\n+  \/\/ This is not a common case. Let's just deoptimize.\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+  CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),\n+                                      Deoptimization::Reason_unloaded,\n+                                      Deoptimization::Action_make_not_entrant);\n+  __ jump(stub);\n+  LIR_Opr reg = rlock_result(x, T_OBJECT);\n+  __ move(LIR_OprFact::oopConst(nullptr), reg);\n+}\n@@ -2530,1 +2926,1 @@\n-  if (do_update) {\n+  if (do_update && signature_at_call_k != nullptr) {\n@@ -2615,0 +3011,46 @@\n+void LIRGenerator::profile_flags(ciMethodData* md, ciProfileData* data, int flag, LIR_Condition condition) {\n+  assert(md != nullptr && data != nullptr, \"should have been initialized\");\n+  LIR_Opr mdp = new_register(T_METADATA);\n+  __ metadata2reg(md->constant_encoding(), mdp);\n+  LIR_Address* addr = new LIR_Address(mdp, md->byte_offset_of_slot(data, DataLayout::flags_offset()), T_BYTE);\n+  LIR_Opr flags = new_register(T_INT);\n+  __ move(addr, flags);\n+  if (condition != lir_cond_always) {\n+    LIR_Opr update = new_register(T_INT);\n+    __ cmove(condition, LIR_OprFact::intConst(0), LIR_OprFact::intConst(flag), update, T_INT);\n+  } else {\n+    __ logical_or(flags, LIR_OprFact::intConst(flag), flags);\n+  }\n+  __ store(flags, addr);\n+}\n+\n+void LIRGenerator::profile_null_free_array(LIRItem array, ciMethodData* md, ciArrayLoadStoreData* load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+\n+  profile_flags(md, load_store, ArrayLoadStoreData::null_free_array_byte_constant(), lir_cond_equal);\n+}\n+\n+void LIRGenerator::profile_array_type(AccessIndexed* x, ciMethodData*& md, ciArrayLoadStoreData*& load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  int bci = x->profiled_bci();\n+  md = x->profiled_method()->method_data();\n+  assert(md != nullptr, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(bci);\n+  assert(data != nullptr && data->is_ArrayLoadStoreData(), \"incorrect profiling entry\");\n+  load_store = (ciArrayLoadStoreData*)data;\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_store, ArrayLoadStoreData::array_offset()), 0,\n+               load_store->array()->type(), x->array(), mdp, true, nullptr, nullptr);\n+}\n+\n+void LIRGenerator::profile_element_type(Value element, ciMethodData* md, ciArrayLoadStoreData* load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  assert(md != nullptr && load_store != nullptr, \"should have been initialized\");\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_store, ArrayLoadStoreData::element_offset()), 0,\n+               load_store->element()->type(), element, mdp, false, nullptr, nullptr);\n+}\n+\n@@ -2695,0 +3137,8 @@\n+  if (method()->has_scalarized_args()) {\n+    \/\/ Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized inline type arguments\n+    \/\/ in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.\n+    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, 0), nullptr, false);\n+    CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);\n+    __ append(new LIR_Op0(lir_check_orig_pc));\n+    __ branch(lir_cond_notEqual, deopt_stub);\n+  }\n@@ -2710,0 +3160,14 @@\n+void LIRGenerator::invoke_load_one_argument(LIRItem* param, LIR_Opr loc) {\n+  if (loc->is_register()) {\n+    param->load_item_force(loc);\n+  } else {\n+    LIR_Address* addr = loc->as_address_ptr();\n+    param->load_for_store(addr->type());\n+    assert(addr->type() != T_PRIMITIVE_OBJECT, \"not supported yet\");\n+    if (addr->type() == T_OBJECT) {\n+      __ move_wide(param->result(), addr);\n+    } else {\n+      __ move(param->result(), addr);\n+    }\n+  }\n+}\n@@ -2717,10 +3181,1 @@\n-    if (loc->is_register()) {\n-      param->load_item_force(loc);\n-    } else {\n-      LIR_Address* addr = loc->as_address_ptr();\n-      param->load_for_store(addr->type());\n-      if (addr->type() == T_OBJECT) {\n-        __ move_wide(param->result(), addr);\n-      } else\n-        __ move(param->result(), addr);\n-    }\n+    invoke_load_one_argument(param, loc);\n@@ -2892,1 +3347,1 @@\n-  if (can_inline_as_constant(right.value())) {\n+  if (can_inline_as_constant(right.value()) && !x->substitutability_check()) {\n@@ -2895,0 +3350,1 @@\n+    \/\/ substitutability_check() needs to use right as a base register.\n@@ -2902,3 +3358,60 @@\n-  LIR_Opr reg = rlock_result(x);\n-  __ cmp(lir_cond(x->cond()), left.result(), right.result());\n-  __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  if (x->substitutability_check()) {\n+    substitutability_check(x, left, right, t_val, f_val);\n+  } else {\n+    LIR_Opr reg = rlock_result(x);\n+    __ cmp(lir_cond(x->cond()), left.result(), right.result());\n+    __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  }\n+}\n+\n+void LIRGenerator::substitutability_check(IfOp* x, LIRItem& left, LIRItem& right, LIRItem& t_val, LIRItem& f_val) {\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  bool is_acmpeq = (x->cond() == If::eql);\n+  LIR_Opr equal_result     = is_acmpeq ? t_val.result() : f_val.result();\n+  LIR_Opr not_equal_result = is_acmpeq ? f_val.result() : t_val.result();\n+  LIR_Opr result = rlock_result(x);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+}\n+\n+void LIRGenerator::substitutability_check(If* x, LIRItem& left, LIRItem& right) {\n+  LIR_Opr equal_result     = LIR_OprFact::intConst(1);\n+  LIR_Opr not_equal_result = LIR_OprFact::intConst(0);\n+  LIR_Opr result = new_register(T_INT);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  __ cmp(lir_cond(x->cond()), result, equal_result);\n+}\n+\n+void LIRGenerator::substitutability_check_common(Value left_val, Value right_val, LIRItem& left, LIRItem& right,\n+                                                 LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result,\n+                                                 CodeEmitInfo* info) {\n+  LIR_Opr tmp1 = LIR_OprFact::illegalOpr;\n+  LIR_Opr tmp2 = LIR_OprFact::illegalOpr;\n+  LIR_Opr left_klass_op = LIR_OprFact::illegalOpr;\n+  LIR_Opr right_klass_op = LIR_OprFact::illegalOpr;\n+\n+  ciKlass* left_klass  = left_val ->as_loaded_klass_or_null();\n+  ciKlass* right_klass = right_val->as_loaded_klass_or_null();\n+\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    init_temps_for_substitutability_check(tmp1, tmp2);\n+  }\n+\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+  } else {\n+    BasicType t_klass = UseCompressedOops ? T_INT : T_METADATA;\n+    left_klass_op = new_register(t_klass);\n+    right_klass_op = new_register(t_klass);\n+  }\n+\n+  CodeStub* slow_path = new SubstitutabilityCheckStub(left.result(), right.result(), info);\n+  __ substitutability_check(result, left.result(), right.result(), equal_result, not_equal_result,\n+                            tmp1, tmp2,\n+                            left_klass, right_klass, left_klass_op, right_klass_op, info, slow_path);\n@@ -3179,1 +3692,1 @@\n-    ciReturnTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n+    ciSingleTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n@@ -3200,0 +3713,47 @@\n+bool LIRGenerator::profile_inline_klass(ciMethodData* md, ciProfileData* data, Value value, int flag) {\n+  ciKlass* klass = value->as_loaded_klass_or_null();\n+  if (klass != nullptr) {\n+    if (klass->is_inlinetype()) {\n+      profile_flags(md, data, flag, lir_cond_always);\n+    } else if (klass->can_be_inline_klass()) {\n+      return false;\n+    }\n+  } else {\n+    return false;\n+  }\n+  return true;\n+}\n+\n+\n+void LIRGenerator::do_ProfileACmpTypes(ProfileACmpTypes* x) {\n+  ciMethod* method = x->method();\n+  assert(method != nullptr, \"method should be set if branch is profiled\");\n+  ciMethodData* md = method->method_data_or_null();\n+  assert(md != nullptr, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(x->bci());\n+  assert(data != nullptr, \"must have profiling data\");\n+  assert(data->is_ACmpData(), \"need BranchData for two-way branches\");\n+  ciACmpData* acmp = (ciACmpData*)data;\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()), 0,\n+               acmp->left()->type(), x->left(), mdp, !x->left_maybe_null(), nullptr, nullptr);\n+  int flags_offset = md->byte_offset_of_slot(data, DataLayout::flags_offset());\n+  if (!profile_inline_klass(md, acmp, x->left(), ACmpData::left_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->left(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::left_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()),\n+               in_bytes(ACmpData::right_offset()) - in_bytes(ACmpData::left_offset()),\n+               acmp->right()->type(), x->right(), mdp, !x->right_maybe_null(), nullptr, nullptr);\n+  if (!profile_inline_klass(md, acmp, x->right(), ACmpData::right_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->right(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::right_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":604,"deletions":44,"binary":false,"changes":648,"status":"modified"},{"patch":"@@ -172,0 +172,1 @@\n+  bool          _in_conditional_code;\n@@ -198,0 +199,1 @@\n+  void set_in_conditional_code(bool v);\n@@ -217,0 +219,1 @@\n+  bool in_conditional_code() { return _in_conditional_code; }\n@@ -276,0 +279,14 @@\n+  bool inline_type_field_access_prolog(AccessField* x);\n+  void access_flattened_array(bool is_load, LIRItem& array, LIRItem& index, LIRItem& obj_item, ciField* field = nullptr, int offset = 0);\n+  void access_sub_element(LIRItem& array, LIRItem& index, LIR_Opr& result, ciField* field, int sub_offset);\n+  LIR_Opr get_and_load_element_address(LIRItem& array, LIRItem& index);\n+  bool needs_flattened_array_store_check(StoreIndexed* x);\n+  void check_flattened_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path);\n+  bool needs_null_free_array_store_check(StoreIndexed* x);\n+  void check_null_free_array(LIRItem& array, LIRItem& value,  CodeEmitInfo* info);\n+  void substitutability_check(IfOp* x, LIRItem& left, LIRItem& right, LIRItem& t_val, LIRItem& f_val);\n+  void substitutability_check(If* x, LIRItem& left, LIRItem& right);\n+  void substitutability_check_common(Value left_val, Value right_val, LIRItem& left, LIRItem& right,\n+                                     LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result, CodeEmitInfo* info);\n+  void init_temps_for_substitutability_check(LIR_Opr& tmp1, LIR_Opr& tmp2);\n+\n@@ -329,1 +346,1 @@\n-\n+  void invoke_load_one_argument(LIRItem* param, LIR_Opr loc);\n@@ -365,1 +382,1 @@\n-  void monitor_enter (LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info);\n+  void monitor_enter (LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info, CodeStub* throw_imse_stub);\n@@ -368,1 +385,1 @@\n-  void new_instance    (LIR_Opr  dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr  scratch1, LIR_Opr  scratch2, LIR_Opr  scratch3,  LIR_Opr scratch4, LIR_Opr  klass_reg, CodeEmitInfo* info);\n+  void new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, bool allow_inline, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info);\n@@ -480,0 +497,5 @@\n+  void profile_flags(ciMethodData* md, ciProfileData* load_store, int flag, LIR_Condition condition = lir_cond_always);\n+  void profile_null_free_array(LIRItem array, ciMethodData* md, ciArrayLoadStoreData* load_store);\n+  void profile_array_type(AccessIndexed* x, ciMethodData*& md, ciArrayLoadStoreData*& load_store);\n+  void profile_element_type(Value element, ciMethodData* md, ciArrayLoadStoreData* load_store);\n+  bool profile_inline_klass(ciMethodData* md, ciProfileData* data, Value value, int flag);\n@@ -507,0 +529,1 @@\n+    , _in_conditional_code(false)\n@@ -560,0 +583,1 @@\n+  virtual void do_NewInlineTypeInstance(NewInlineTypeInstance* x);\n@@ -563,0 +587,1 @@\n+  virtual void do_Deoptimize     (Deoptimize*      x);\n@@ -585,0 +610,1 @@\n+  virtual void do_ProfileACmpTypes(ProfileACmpTypes* x);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":29,"deletions":3,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -65,0 +65,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -71,3 +71,3 @@\n-      \/\/ Bottom two bits determine if the type is a reference, primitive,\n-      \/\/ uninitialized or a query-type.\n-      TypeMask           = 0x00000003,\n+      \/\/ Bottom three bits determine if the type is a reference, inline type,\n+      \/\/ primitive, uninitialized or a query-type.\n+      TypeMask           = 0x00000007,\n@@ -76,1 +76,1 @@\n-      Reference          = 0x0,        \/\/ _sym contains the name\n+      Reference          = 0x0,        \/\/ _sym contains the name of an object\n@@ -80,0 +80,1 @@\n+      InlineType         = 0x4,        \/\/ _sym contains the name of an inline type\n@@ -86,0 +87,2 @@\n+      InlineTypeFlag     = 0x08,       \/\/ For inline type query types\n+      NonScalarFlag      = 0x10,       \/\/ For either inline type or reference queries\n@@ -117,1 +120,3 @@\n-      Category2_2ndQuery = (Category2_2ndFlag << 1 * BitsPerByte) | TypeQuery\n+      Category2_2ndQuery = (Category2_2ndFlag << 1 * BitsPerByte) | TypeQuery,\n+      InlineTypeQuery    = (InlineTypeFlag    << 1 * BitsPerByte) | TypeQuery,\n+      NonScalarQuery     = (NonScalarFlag     << 1 * BitsPerByte) | TypeQuery\n@@ -150,0 +155,2 @@\n+  static VerificationType inline_type_check()\n+    { return VerificationType(InlineTypeQuery); }\n@@ -156,0 +163,2 @@\n+  static VerificationType nonscalar_check()\n+    { return VerificationType(NonScalarQuery); }\n@@ -159,1 +168,1 @@\n-      assert(((uintptr_t)sh & 0x3) == 0, \"Symbols must be aligned\");\n+      assert(((uintptr_t)sh & TypeMask) == 0, \"Symbols must be aligned\");\n@@ -170,0 +179,11 @@\n+  \/\/ For inline types, store the actual Symbol* and set the 3rd bit.\n+  \/\/ Provides a way for an inline type to be distinguished from a reference type.\n+  static VerificationType inline_type(Symbol* sh) {\n+      assert(((uintptr_t)sh & TypeMask) == 0, \"Symbols must be aligned\");\n+      assert((uintptr_t)sh != 0, \"Null is not a valid inline type\");\n+      \/\/ If the above assert fails in the future because oop* isn't aligned,\n+      \/\/ then this type encoding system will have to change to have a tag value\n+      \/\/ to discriminate between oops and primitives.\n+      return VerificationType((uintptr_t)sh | InlineType);\n+  }\n+\n@@ -185,1 +205,2 @@\n-  bool is_reference() const { return ((_u._data & TypeMask) == Reference); }\n+  bool is_reference() const { return (((_u._data & TypeMask) == Reference) && !is_inline_type_check()); }\n+  bool is_inline_type() const { return ((_u._data & TypeMask) == InlineType); }\n@@ -188,2 +209,2 @@\n-    \/\/ primitives, and references (including uninitialized refs).  Though\n-    \/\/ the 'query' types should technically return 'false' here, if we\n+    \/\/ primitives, references (including uninitialized refs) and inline types.\n+    \/\/ Though the 'query' types should technically return 'false' here, if we\n@@ -203,0 +224,2 @@\n+  bool is_inline_type_check() const { return _u._data == InlineTypeQuery; }\n+  bool is_nonscalar_check() const { return _u._data == NonScalarQuery; }\n@@ -221,0 +244,1 @@\n+  bool is_inline_type_array() const { return is_x_array(JVM_SIGNATURE_PRIMITIVE_OBJECT); }\n@@ -223,0 +247,2 @@\n+  bool is_nonscalar_array() const\n+    { return is_object_array() || is_array_array() || is_inline_type_array(); }\n@@ -239,0 +265,6 @@\n+  static VerificationType change_ref_to_inline_type(VerificationType ref) {\n+    assert(ref.is_reference(), \"Bad arg\");\n+    assert(!ref.is_null(), \"Unexpected nullptr\");\n+    return inline_type(ref.name());\n+  }\n+\n@@ -245,2 +277,2 @@\n-    assert(is_reference() && !is_null(), \"Must be a non-null reference\");\n-    return _u._sym;\n+    assert(!is_null() && (is_reference() || is_inline_type()), \"Must be a non-null reference or an inline type\");\n+    return (is_reference() ? _u._sym : ((Symbol*)(_u._data & ~(uintptr_t)InlineType)));\n@@ -251,2 +283,4 @@\n-      (is_reference() && t.is_reference() && !is_null() && !t.is_null() &&\n-       name() == t.name()));\n+            (((is_reference() && t.is_reference()) ||\n+             (is_inline_type() && t.is_inline_type())) &&\n+              !is_null() && !t.is_null() && name() == t.name()));\n+\n@@ -281,0 +315,5 @@\n+        case NonScalarQuery:\n+          return from.is_reference() || from.is_uninitialized() ||\n+                 from.is_inline_type();\n+        case InlineTypeQuery:\n+          return from.is_inline_type();\n@@ -288,1 +327,5 @@\n-          if (is_reference() && from.is_reference()) {\n+          if (is_inline_type()) {\n+            return is_inline_type_assignable_from(from);\n+          } else if (is_reference() && from.is_inline_type()) {\n+            return is_ref_assignable_from_inline_type(from, context, THREAD);\n+          } else if (is_reference() && from.is_reference()) {\n@@ -336,0 +379,5 @@\n+  bool is_inline_type_assignable_from(const VerificationType& from) const;\n+\n+  bool is_ref_assignable_from_inline_type(const VerificationType& from, ClassVerifier* context, TRAPS) const;\n+\n+\n","filename":"src\/hotspot\/share\/classfile\/verificationType.hpp","additions":62,"deletions":14,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -171,0 +171,1 @@\n+  _is_init = read_from(stream);\n@@ -189,0 +190,5 @@\n+    if (_is_init == nullptr) {\n+      \/\/ MarkerValue is used for null-free objects\n+      _is_init = new MarkerValue();\n+    }\n+    _is_init->write_on(stream);\n","filename":"src\/hotspot\/share\/code\/debugInfo.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -133,0 +133,1 @@\n+  ScopeValue*                _is_init;\n@@ -141,1 +142,1 @@\n-  ObjectValue(int id, ScopeValue* klass)\n+  ObjectValue(int id, ScopeValue* klass, ScopeValue* is_init = nullptr)\n@@ -144,0 +145,1 @@\n+     , _is_init(is_init)\n@@ -154,0 +156,1 @@\n+     , _is_init(new MarkerValue())\n@@ -163,0 +166,1 @@\n+  virtual ScopeValue*         is_init() const             { return _is_init; }\n@@ -169,0 +173,1 @@\n+  bool                        maybe_null() const          { return !_is_init->is_marker(); }\n","filename":"src\/hotspot\/share\/code\/debugInfo.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+  _return_scalarized = pd->return_scalarized();\n@@ -56,0 +57,1 @@\n+  _return_scalarized = false;\n","filename":"src\/hotspot\/share\/code\/scopeDesc.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+  bool return_scalarized() const { return _return_scalarized; }\n@@ -108,0 +109,1 @@\n+  bool          _return_scalarized;\n@@ -111,1 +113,0 @@\n-\n","filename":"src\/hotspot\/share\/code\/scopeDesc.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+  def(_fast_qgetfield            , \"fast_qgetfield\"            , \"bJJ\"  , nullptr    , T_OBJECT ,  0, true , _getfield          ) \\\n@@ -43,0 +44,1 @@\n+  def(_fast_qputfield            , \"fast_qputfield\"            , \"bJJ\"  , nullptr    , T_OBJECT ,  0, true , _putfield          ) \\\n@@ -283,0 +285,2 @@\n+  def(_aconst_init     , \"aconst_init\"     , \"bkk\"  , nullptr    , T_OBJECT ,  1, true , _aconst_init    ) \\\n+  def(_withfield       , \"withfield\"       , \"bJJ\"  , nullptr    , T_OBJECT , -1, true , _withfield      ) \\\n","filename":"src\/hotspot\/share\/interpreter\/bytecodes.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -182,1 +182,1 @@\n-    int  cp_index    = Bytes::get_Java_u2(p);\n+    int cp_index    = Bytes::get_Java_u2(p);\n@@ -218,1 +218,0 @@\n-\n@@ -433,1 +432,1 @@\n-                  if (!method->is_static_initializer()) {\n+                  if (!method->is_class_initializer()) {\n@@ -437,1 +436,1 @@\n-                  if (!method->is_object_initializer()) {\n+                  if (!method->is_object_constructor()) {\n@@ -449,0 +448,1 @@\n+      case Bytecodes::_withfield     : \/\/ fall through but may require more checks for correctness\n","filename":"src\/hotspot\/share\/interpreter\/rewriter.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1124,0 +1124,1 @@\n+      const bool return_scalarized     = false;\n@@ -1127,1 +1128,1 @@\n-                                      has_ea_local_in_scope, arg_escape,\n+                                      return_scalarized, has_ea_local_in_scope, arg_escape,\n@@ -1266,0 +1267,2 @@\n+      _offsets.set_value(CodeOffsets::Verified_Inline_Entry, pc_offset);\n+      _offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, pc_offset);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -135,1 +135,1 @@\n-      _jca->push_oop(next_arg(T_OBJECT));\n+      (type == T_PRIMITIVE_OBJECT) ? _jca->push_oop(next_arg(T_PRIMITIVE_OBJECT)) : _jca->push_oop(next_arg(T_OBJECT));\n@@ -1494,1 +1494,1 @@\n-              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false);\n+              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false, CHECK_NULL);\n@@ -1734,1 +1734,1 @@\n-  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false);\n+  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false, THREAD);\n@@ -2058,1 +2058,1 @@\n-    if (m->is_initializer() && !m->is_static()) {\n+    if (m->is_object_constructor()) {\n@@ -2085,1 +2085,1 @@\n-    if (!m->is_initializer() && !m->is_overpass()) {\n+    if (!(m->is_object_constructor() || m->is_class_initializer()) && !m->is_overpass()) {\n@@ -2798,2 +2798,1 @@\n-  if (m->is_initializer()) {\n-    if (m->is_static_initializer()) {\n+  if (m->is_class_initializer()) {\n@@ -2802,1 +2801,2 @@\n-    }\n+  }\n+  else if (m->is_object_constructor() || m->is_static_vnew_factory()) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -169,0 +170,4 @@\n+bool InstanceKlass::field_is_null_free_inline_type(int index) const {\n+  return Signature::basic_type(field_signature(index)) == T_PRIMITIVE_OBJECT;\n+}\n+\n@@ -441,1 +446,3 @@\n-                                       parser.is_interface());\n+                                       parser.is_interface(),\n+                                       parser.has_inline_fields() ? parser.java_fields_count() : 0,\n+                                       parser.is_inline_type());\n@@ -463,0 +470,3 @@\n+  } else if (parser.is_inline_type()) {\n+    \/\/ inline type\n+    ik = new (loader_data, size, THREAD) InlineKlass(parser);\n@@ -474,0 +484,6 @@\n+#ifdef ASSERT\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -477,0 +493,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = nullptr;\n+  address end = nullptr;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -516,1 +555,4 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _inline_type_field_klasses(nullptr),\n+  _preload_classes(nullptr),\n+  _adr_inlineklass_fixed_block(nullptr)\n@@ -523,0 +565,3 @@\n+  if (parser.has_inline_fields()) {\n+    set_has_inline_type_fields();\n+  }\n@@ -527,0 +572,4 @@\n+\n+  if (has_inline_type_fields()) {\n+    _inline_type_field_klasses = (const Klass**) adr_inline_type_field_klasses();\n+  }\n@@ -696,0 +745,6 @@\n+  if (preload_classes() != nullptr &&\n+      preload_classes() != Universe::the_empty_short_array() &&\n+      !preload_classes()->is_shared()) {\n+    MetadataFactory::free_array<jushort>(loader_data, preload_classes());\n+  }\n+\n@@ -852,0 +907,78 @@\n+\n+  \/\/ If a class declares a method that uses an inline class as an argument\n+  \/\/ type or return inline type, this inline class must be loaded during the\n+  \/\/ linking of this class because size and properties of the inline class\n+  \/\/ must be known in order to be able to perform inline type optimizations.\n+  \/\/ The implementation below is an approximation of this rule, the code\n+  \/\/ iterates over all methods of the current class (including overridden\n+  \/\/ methods), not only the methods declared by this class. This\n+  \/\/ approximation makes the code simpler, and doesn't change the semantic\n+  \/\/ because classes declaring methods overridden by the current class are\n+  \/\/ linked (and have performed their own pre-loading) before the linking\n+  \/\/ of the current class.\n+\n+\n+  \/\/ Note:\n+  \/\/ Inline class types are loaded during\n+  \/\/ the loading phase (see ClassFileParser::post_process_parsed_stream()).\n+  \/\/ Inline class types used as element types for array creation\n+  \/\/ are not pre-loaded. Their loading is triggered by either anewarray\n+  \/\/ or multianewarray bytecodes.\n+\n+  \/\/ Could it be possible to do the following processing only if the\n+  \/\/ class uses inline types?\n+  if (EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    if (EnablePrimitiveClasses) {\n+      for (int i = 0; i < methods()->length(); i++) {\n+        Method* m = methods()->at(i);\n+        for (SignatureStream ss(m->signature()); !ss.is_done(); ss.next()) {\n+          if (ss.is_reference()) {\n+            if (ss.is_array()) {\n+              continue;\n+            }\n+            if (ss.type() == T_PRIMITIVE_OBJECT) {\n+              Symbol* symb = ss.as_symbol();\n+              if (symb == name()) continue;\n+              oop loader = class_loader();\n+              oop protection_domain = this->protection_domain();\n+              Klass* klass = SystemDictionary::resolve_or_fail(symb,\n+                                                              Handle(THREAD, loader), Handle(THREAD, protection_domain), true,\n+                                                              CHECK_false);\n+              if (klass == nullptr) {\n+                THROW_(vmSymbols::java_lang_LinkageError(), false);\n+              }\n+              if (!klass->is_inline_klass()) {\n+                Exceptions::fthrow(\n+                  THREAD_AND_LOCATION,\n+                  vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                  \"class %s is not an inline type\",\n+                  klass->external_name());\n+              }\n+            }\n+          }\n+        }\n+      }\n+    }\n+    \/\/ Aggressively preloading all classes from the Preload attribute\n+    if (preload_classes() != nullptr) {\n+      for (int i = 0; i < preload_classes()->length(); i++) {\n+        if (constants()->tag_at(preload_classes()->at(i)).is_klass()) continue;\n+        Symbol* class_name = constants()->klass_at_noresolve(preload_classes()->at(i));\n+        if (class_name == name()) continue;\n+        oop loader = class_loader();\n+        oop protection_domain = this->protection_domain();\n+        Klass* klass = SystemDictionary::resolve_or_null(class_name,\n+                                                          Handle(THREAD, loader), Handle(THREAD, protection_domain), THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          CLEAR_PENDING_EXCEPTION;\n+        }\n+        if (klass != nullptr) {\n+          log_info(class, preload)(\"Preloading class %s during linking of class %s because of its Preload attribute\", class_name->as_C_string(), name()->as_C_string());\n+        } else {\n+          log_warning(class, preload)(\"Preloading of class %s during linking of class %s (Preload attribute) failed\", class_name->as_C_string(), name()->as_C_string());\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1113,0 +1246,19 @@\n+  \/\/ Pre-allocating an instance of the default value\n+  if (is_inline_klass()) {\n+      InlineKlass* vk = InlineKlass::cast(this);\n+      oop val = vk->allocate_instance(THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          {\n+              EXCEPTION_MARK;\n+              add_initialization_error(THREAD, e);\n+              \/\/ Locks object, set state, and notify all waiting threads\n+              set_initialization_state_and_notify(initialization_error, THREAD);\n+              CLEAR_PENDING_EXCEPTION;\n+          }\n+          THROW_OOP(e());\n+      }\n+      vk->set_default_value(val);\n+  }\n+\n@@ -1145,1 +1297,41 @@\n-\n+  \/\/ Initialize classes of inline fields\n+  if (EnablePrimitiveClasses) {\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_PRIMITIVE_OBJECT) {\n+        Klass* klass = get_inline_type_field_klass_or_null(fs.index());\n+        if (fs.access_flags().is_static() && klass == nullptr) {\n+          klass = SystemDictionary::resolve_or_fail(field_signature(fs.index())->fundamental_name(THREAD),\n+              Handle(THREAD, class_loader()),\n+              Handle(THREAD, protection_domain()),\n+              true, THREAD);\n+          set_inline_type_field_klass(fs.index(), klass);\n+        }\n+\n+        if (!HAS_PENDING_EXCEPTION) {\n+          assert(klass != nullptr, \"Must  be\");\n+          InstanceKlass::cast(klass)->initialize(THREAD);\n+          if (fs.access_flags().is_static()) {\n+            if (java_mirror()->obj_field(fs.offset()) == nullptr) {\n+              java_mirror()->obj_field_put(fs.offset(), InlineKlass::cast(klass)->default_value());\n+            }\n+          }\n+        }\n+\n+        if (HAS_PENDING_EXCEPTION) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          {\n+            EXCEPTION_MARK;\n+            add_initialization_error(THREAD, e);\n+            \/\/ Locks object, set state, and notify all waiting threads\n+            set_initialization_state_and_notify(initialization_error, THREAD);\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+          THROW_OOP(e());\n+        }\n+      }\n+    }\n+  }\n+\n+\n+  \/\/ Step 9\n@@ -1168,1 +1360,1 @@\n-  \/\/ Step 9\n+  \/\/ Step 10\n@@ -1174,1 +1366,1 @@\n-    \/\/ Step 10 and 11\n+    \/\/ Step 11 and 12\n@@ -1500,1 +1692,2 @@\n-        ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, CHECK_NULL);\n+        ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this,\n+                                                                  false, false, CHECK_NULL);\n@@ -1507,2 +1700,2 @@\n-  ObjArrayKlass* oak = array_klasses();\n-  return oak->array_klass(n, THREAD);\n+  ArrayKlass* ak = array_klasses();\n+  return ak->array_klass(n, THREAD);\n@@ -1513,2 +1706,2 @@\n-  ObjArrayKlass* oak = array_klasses_acquire();\n-  if (oak == nullptr) {\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == nullptr) {\n@@ -1517,1 +1710,1 @@\n-    return oak->array_klass_or_null(n);\n+    return ak->array_klass_or_null(n);\n@@ -1534,1 +1727,1 @@\n-  if (clinit != nullptr && clinit->has_valid_initializer_flags()) {\n+  if (clinit != nullptr && clinit->is_class_initializer()) {\n@@ -1583,1 +1776,1 @@\n-    MutexLocker x(OopMapCacheAlloc_lock);\n+    MutexLocker x(OopMapCacheAlloc_lock,  Mutex::_no_safepoint_check_flag);\n@@ -1595,4 +1788,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n@@ -1680,0 +1869,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->first_field_offset() && offset < (vk->first_field_offset() + vk->get_exact_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -2072,0 +2270,4 @@\n+    if (name == vmSymbols::object_initializer_name() ||\n+        name == vmSymbols::inline_factory_name()) {\n+      break;  \/\/ <init> and <vnew> is never inherited\n+    }\n@@ -2552,0 +2754,1 @@\n+  it->push(&_preload_classes);\n@@ -2553,0 +2756,6 @@\n+\n+  if (has_inline_type_fields()) {\n+    for (int i = 0; i < java_fields_count(); i++) {\n+      it->push(&((Klass**)adr_inline_type_field_klasses())[i]);\n+    }\n+  }\n@@ -2596,1 +2805,9 @@\n-  \/\/ These are not allocated from metaspace. They are safe to set to null.\n+  if (has_inline_type_fields()) {\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_PRIMITIVE_OBJECT) {\n+        reset_inline_type_field_klass(fs.index());\n+      }\n+    }\n+  }\n+\n+  \/\/ These are not allocated from metaspace. They are safe to set to nullptr.\n@@ -2680,0 +2897,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2906,0 +3127,2 @@\n+  return signature_name_of_carrier(JVM_SIGNATURE_CLASS);\n+}\n@@ -2907,0 +3130,1 @@\n+const char* InstanceKlass::signature_name_of_carrier(char c) const {\n@@ -2913,1 +3137,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -2915,1 +3139,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = c;\n@@ -3257,2 +3481,1 @@\n-  \/\/ Remember to strip ACC_SUPER bit\n-  return (access & (~JVM_ACC_SUPER)) & JVM_ACC_WRITTEN_FLAGS;\n+  return (access & JVM_ACC_WRITTEN_FLAGS);\n@@ -3515,1 +3738,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3519,0 +3745,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3522,0 +3753,6 @@\n+    } else if (self != nullptr && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3528,1 +3765,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(nullptr, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3569,15 +3827,3 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);                  st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);      st->cr();\n-  if (Verbose && default_methods() != nullptr) {\n-    Array<Method*>* method_array = default_methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n+  st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3585,1 +3831,1 @@\n-    st->print(BULLET\"default vtable indices:   \"); default_vtable_indices()->print_value_on(st);       st->cr();\n+    st->print(BULLET\"default vtable indices:   \"); print_array_on(st, default_vtable_indices());\n@@ -3587,2 +3833,2 @@\n-  st->print(BULLET\"local interfaces:  \"); local_interfaces()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"trans. interfaces: \"); transitive_interfaces()->print_value_on(st); st->cr();\n+  st->print(BULLET\"local interfaces:  \"); print_array_on(st, local_interfaces());\n+  st->print(BULLET\"trans. interfaces: \"); print_array_on(st, transitive_interfaces());\n@@ -3634,0 +3880,1 @@\n+  st->print(BULLET\"preload classes:     \"); preload_classes()->print_value_on(st); st->cr();\n@@ -3644,1 +3891,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(nullptr, start_of_itable(), itable_length(), st);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":290,"deletions":43,"binary":false,"changes":333,"status":"modified"},{"patch":"@@ -151,6 +151,8 @@\n-bool oopDesc::is_instance_noinline()    const { return is_instance();    }\n-bool oopDesc::is_instanceRef_noinline() const { return is_instanceRef(); }\n-bool oopDesc::is_stackChunk_noinline()  const { return is_stackChunk();  }\n-bool oopDesc::is_array_noinline()       const { return is_array();       }\n-bool oopDesc::is_objArray_noinline()    const { return is_objArray();    }\n-bool oopDesc::is_typeArray_noinline()   const { return is_typeArray();   }\n+bool oopDesc::is_instance_noinline()        const { return is_instance();         }\n+bool oopDesc::is_instanceRef_noinline()     const { return is_instanceRef();      }\n+bool oopDesc::is_stackChunk_noinline()      const { return is_stackChunk();       }\n+bool oopDesc::is_array_noinline()           const { return is_array();            }\n+bool oopDesc::is_objArray_noinline()        const { return is_objArray();         }\n+bool oopDesc::is_typeArray_noinline()       const { return is_typeArray();        }\n+bool oopDesc::is_flatArray_noinline()       const { return is_flatArray();        }\n+bool oopDesc::is_null_free_array_noinline() const { return is_null_free_array();  }\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -45,0 +45,10 @@\n+\/\/\n+\/\/ oopDesc::_mark - the \"oop mark word\" encoding to be found separately in markWord.hpp\n+\/\/\n+\/\/ oopDesc::_metadata - encodes the object's klass pointer, as a raw pointer in \"_klass\"\n+\/\/                      or compressed pointer in \"_compressed_klass\"\n+\/\/\n+\/\/ The overall size of the _metadata field is dependent on \"UseCompressedClassPointers\",\n+\/\/ hence the terms \"narrow\" (32 bits) vs \"wide\" (64 bits).\n+\/\/\n+\n@@ -114,6 +124,9 @@\n-  inline bool is_instance()    const;\n-  inline bool is_instanceRef() const;\n-  inline bool is_stackChunk()  const;\n-  inline bool is_array()       const;\n-  inline bool is_objArray()    const;\n-  inline bool is_typeArray()   const;\n+  inline bool is_instance()         const;\n+  inline bool is_inline_type()      const;\n+  inline bool is_instanceRef()      const;\n+  inline bool is_stackChunk()       const;\n+  inline bool is_array()            const;\n+  inline bool is_objArray()         const;\n+  inline bool is_typeArray()        const;\n+  inline bool is_flatArray()        const;\n+  inline bool is_null_free_array()  const;\n@@ -122,6 +135,8 @@\n-  bool is_instance_noinline()    const;\n-  bool is_instanceRef_noinline() const;\n-  bool is_stackChunk_noinline()  const;\n-  bool is_array_noinline()       const;\n-  bool is_objArray_noinline()    const;\n-  bool is_typeArray_noinline()   const;\n+  bool is_instance_noinline()         const;\n+  bool is_instanceRef_noinline()      const;\n+  bool is_stackChunk_noinline()       const;\n+  bool is_array_noinline()            const;\n+  bool is_objArray_noinline()         const;\n+  bool is_typeArray_noinline()        const;\n+  bool is_flatArray_noinline()        const;\n+  bool is_null_free_array_noinline()  const;\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":27,"deletions":12,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -86,1 +86,1 @@\n-  set_mark(markWord::prototype());\n+  set_mark(Klass::default_prototype_header(klass()));\n@@ -212,0 +212,15 @@\n+bool oopDesc::is_inline_type() const { return mark().is_inline_type(); }\n+#ifdef _LP64\n+bool oopDesc::is_flatArray() const {\n+  markWord mrk = mark();\n+  return (mrk.is_unlocked()) ? mrk.is_flat_array() : klass()->is_flatArray_klass();\n+}\n+bool oopDesc::is_null_free_array() const {\n+  markWord mrk = mark();\n+  return (mrk.is_unlocked()) ? mrk.is_null_free_array() : klass()->is_null_free_array_klass();\n+}\n+#else\n+bool oopDesc::is_flatArray()       const { return klass()->is_flatArray_klass(); }\n+bool oopDesc::is_null_free_array() const { return klass()->is_null_free_array_klass(); }\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+typedef class     flatArrayOopDesc*           flatArrayOop;\n@@ -158,0 +159,1 @@\n+DEF_OOP(flatArray);\n@@ -190,0 +192,1 @@\n+class     InlineKlass;\n@@ -197,0 +200,1 @@\n+class     FlatArrayKlass;\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -758,0 +758,6 @@\n+  product(bool, UseArrayLoadStoreProfile, true,                             \\\n+          \"Take advantage of profiling at array load\/store\")                \\\n+                                                                            \\\n+  product(bool, UseACmpProfile, true,                                       \\\n+          \"Take advantage of profiling at acmp\")                            \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -593,0 +593,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -602,0 +604,1 @@\n+  case vmIntrinsics::_getValue:\n@@ -611,0 +614,1 @@\n+  case vmIntrinsics::_putValue:\n@@ -696,0 +700,4 @@\n+  case vmIntrinsics::_asPrimaryType:\n+  case vmIntrinsics::_asPrimaryTypeArg:\n+  case vmIntrinsics::_asValueType:\n+  case vmIntrinsics::_asValueTypeArg:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -46,0 +48,1 @@\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -81,1 +84,1 @@\n-Node *StartNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *StartNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -105,11 +108,0 @@\n-\/\/------------------------------StartOSRNode----------------------------------\n-\/\/ The method start node for an on stack replacement adapter\n-\n-\/\/------------------------------osr_domain-----------------------------\n-const TypeTuple *StartOSRNode::osr_domain() {\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n-\n-  return TypeTuple::make(TypeFunc::Parms+1, fields);\n-}\n-\n@@ -501,0 +493,8 @@\n+      } else if (cik->is_flat_array_klass()) {\n+        ciKlass* cie = cik->as_flat_array_klass()->base_element_klass();\n+        cie->print_name_on(st);\n+        st->print(\"[%d]\", spobj->n_fields());\n+        int ndim = cik->as_array_klass()->dimension() - 1;\n+        while (ndim-- > 0) {\n+          st->print(\"[]\");\n+        }\n@@ -506,0 +506,7 @@\n+        if (iklass != nullptr && iklass->is_inlinetype()) {\n+          Node* init_node = mcall->in(first_ind++);\n+          if (!init_node->is_top()) {\n+            st->print(\" [is_init\");\n+            format_helper(regalloc, st, init_node, \":\", -1, nullptr);\n+          }\n+        }\n@@ -719,1 +726,1 @@\n-const Type *CallNode::bottom_type() const { return tf()->range(); }\n+const Type *CallNode::bottom_type() const { return tf()->range_cc(); }\n@@ -721,2 +728,4 @@\n-  if (phase->type(in(0)) == Type::TOP)  return Type::TOP;\n-  return tf()->range();\n+  if (!in(0) || phase->type(in(0)) == Type::TOP) {\n+    return Type::TOP;\n+  }\n+  return tf()->range_cc();\n@@ -727,0 +736,7 @@\n+  if (_entry_point == StubRoutines::store_inline_type_fields_to_buf()) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -735,27 +751,26 @@\n-Node *CallNode::match( const ProjNode *proj, const Matcher *match ) {\n-  switch (proj->_con) {\n-  case TypeFunc::Control:\n-  case TypeFunc::I_O:\n-  case TypeFunc::Memory:\n-    return new MachProjNode(this,proj->_con,RegMask::Empty,MachProjNode::unmatched_proj);\n-\n-  case TypeFunc::Parms+1:       \/\/ For LONG & DOUBLE returns\n-    assert(tf()->range()->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n-    \/\/ 2nd half of doubles and longs\n-    return new MachProjNode(this,proj->_con, RegMask::Empty, (uint)OptoReg::Bad);\n-\n-  case TypeFunc::Parms: {       \/\/ Normal returns\n-    uint ideal_reg = tf()->range()->field_at(TypeFunc::Parms)->ideal_reg();\n-    OptoRegPair regs = Opcode() == Op_CallLeafVector\n-      ? match->vector_return_value(ideal_reg)      \/\/ Calls into assembly vector routine\n-      : is_CallRuntime()\n-        ? match->c_return_value(ideal_reg)  \/\/ Calls into C runtime\n-        : match->  return_value(ideal_reg); \/\/ Calls into compiled Java code\n-    RegMask rm = RegMask(regs.first());\n-\n-    if (Opcode() == Op_CallLeafVector) {\n-      \/\/ If the return is in vector, compute appropriate regmask taking into account the whole range\n-      if(ideal_reg >= Op_VecS && ideal_reg <= Op_VecZ) {\n-        if(OptoReg::is_valid(regs.second())) {\n-          for (OptoReg::Name r = regs.first(); r <= regs.second(); r = OptoReg::add(r, 1)) {\n-            rm.Insert(r);\n+Node *CallNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n+  uint con = proj->_con;\n+  const TypeTuple* range_cc = tf()->range_cc();\n+  if (con >= TypeFunc::Parms) {\n+    if (tf()->returns_inline_type_as_fields()) {\n+      \/\/ The call returns multiple values (inline type fields): we\n+      \/\/ create one projection per returned value.\n+      assert(con <= TypeFunc::Parms+1 || InlineTypeReturnedAsFields, \"only for multi value return\");\n+      uint ideal_reg = range_cc->field_at(con)->ideal_reg();\n+      return new MachProjNode(this, con, mask[con-TypeFunc::Parms], ideal_reg);\n+    } else {\n+      if (con == TypeFunc::Parms) {\n+        uint ideal_reg = range_cc->field_at(TypeFunc::Parms)->ideal_reg();\n+        OptoRegPair regs = Opcode() == Op_CallLeafVector\n+          ? match->vector_return_value(ideal_reg)      \/\/ Calls into assembly vector routine\n+          : match->c_return_value(ideal_reg);\n+        RegMask rm = RegMask(regs.first());\n+\n+        if (Opcode() == Op_CallLeafVector) {\n+          \/\/ If the return is in vector, compute appropriate regmask taking into account the whole range\n+          if(ideal_reg >= Op_VecS && ideal_reg <= Op_VecZ) {\n+            if(OptoReg::is_valid(regs.second())) {\n+              for (OptoReg::Name r = regs.first(); r <= regs.second(); r = OptoReg::add(r, 1)) {\n+                rm.Insert(r);\n+              }\n+            }\n@@ -764,0 +779,9 @@\n+\n+        if (OptoReg::is_valid(regs.second())) {\n+          rm.Insert(regs.second());\n+        }\n+        return new MachProjNode(this,con,rm,ideal_reg);\n+      } else {\n+        assert(con == TypeFunc::Parms+1, \"only one return value\");\n+        assert(range_cc->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n+        return new MachProjNode(this,con, RegMask::Empty, (uint)OptoReg::Bad);\n@@ -766,4 +790,0 @@\n-\n-    if( OptoReg::is_valid(regs.second()) )\n-      rm.Insert( regs.second() );\n-    return new MachProjNode(this,proj->_con,rm,ideal_reg);\n@@ -772,0 +792,6 @@\n+  switch (con) {\n+  case TypeFunc::Control:\n+  case TypeFunc::I_O:\n+  case TypeFunc::Memory:\n+    return new MachProjNode(this,proj->_con,RegMask::Empty,MachProjNode::unmatched_proj);\n+\n@@ -792,1 +818,1 @@\n-    const TypeTuple* args = _tf->domain();\n+    const TypeTuple* args = _tf->domain_sig();\n@@ -841,1 +867,1 @@\n-      const TypeTuple* d = tf()->domain();\n+      const TypeTuple* d = tf()->domain_cc();\n@@ -856,2 +882,2 @@\n-bool CallNode::has_non_debug_use(Node *n) {\n-  const TypeTuple * d = tf()->domain();\n+bool CallNode::has_non_debug_use(Node* n) {\n+  const TypeTuple* d = tf()->domain_cc();\n@@ -859,2 +885,1 @@\n-    Node *arg = in(i);\n-    if (arg == n) {\n+    if (in(i) == n) {\n@@ -867,0 +892,11 @@\n+bool CallNode::has_debug_use(Node* n) {\n+  if (jvms() != nullptr) {\n+    for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+      if (in(i) == n) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -898,10 +934,15 @@\n-void CallNode::extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts) {\n-  projs->fallthrough_proj      = nullptr;\n-  projs->fallthrough_catchproj = nullptr;\n-  projs->fallthrough_ioproj    = nullptr;\n-  projs->catchall_ioproj       = nullptr;\n-  projs->catchall_catchproj    = nullptr;\n-  projs->fallthrough_memproj   = nullptr;\n-  projs->catchall_memproj      = nullptr;\n-  projs->resproj               = nullptr;\n-  projs->exobj                 = nullptr;\n+CallProjections* CallNode::extract_projections(bool separate_io_proj, bool do_asserts) {\n+  uint max_res = TypeFunc::Parms-1;\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    ProjNode *pn = fast_out(i)->as_Proj();\n+    max_res = MAX2(max_res, pn->_con);\n+  }\n+\n+  assert(max_res < _tf->range_cc()->cnt(), \"result out of bounds\");\n+\n+  uint projs_size = sizeof(CallProjections);\n+  if (max_res > TypeFunc::Parms) {\n+    projs_size += (max_res-TypeFunc::Parms)*sizeof(Node*);\n+  }\n+  char* projs_storage = resource_allocate_bytes(projs_size);\n+  CallProjections* projs = new(projs_storage)CallProjections(max_res - TypeFunc::Parms + 1);\n@@ -953,1 +994,1 @@\n-      projs->resproj = pn;\n+      projs->resproj[0] = pn;\n@@ -956,1 +997,3 @@\n-      assert(false, \"unexpected projection from allocation node.\");\n+      assert(pn->_con <= max_res, \"unexpected projection from allocation node.\");\n+      projs->resproj[pn->_con-TypeFunc::Parms] = pn;\n+      break;\n@@ -963,1 +1006,1 @@\n-  assert(projs->fallthrough_proj      != nullptr, \"must be found\");\n+  assert(!do_asserts || projs->fallthrough_proj      != nullptr, \"must be found\");\n@@ -973,0 +1016,1 @@\n+  return projs;\n@@ -1004,2 +1048,2 @@\n-  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain()->cnt() : (uint)TypeFunc::Parms+1;\n-  uint new_dbg_start = tf()->domain()->cnt();\n+  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain_sig()->cnt() : (uint)TypeFunc::Parms+1;\n+  uint new_dbg_start = tf()->domain_sig()->cnt();\n@@ -1046,0 +1090,4 @@\n+  Bytecodes::Code bc = jvms()->method()->java_code_at_bci(jvms()->bci());\n+  if (EnableValhalla && (bc == Bytecodes::_if_acmpeq || bc == Bytecodes::_if_acmpne)) {\n+    return true;\n+  }\n@@ -1079,0 +1127,10 @@\n+  if (can_reshape && uncommon_trap_request() != 0) {\n+    if (remove_useless_allocation(phase, in(0), in(TypeFunc::Memory), in(TypeFunc::Parms))) {\n+      if (!in(0)->is_Region()) {\n+        PhaseIterGVN* igvn = phase->is_IterGVN();\n+        igvn->replace_input_of(this, 0, phase->C->top());\n+      }\n+      return this;\n+    }\n+  }\n+\n@@ -1131,0 +1189,124 @@\n+bool CallStaticJavaNode::remove_useless_allocation(PhaseGVN *phase, Node* ctl, Node* mem, Node* unc_arg) {\n+  \/\/ Split if can cause the flattened array branch of an array load to\n+  \/\/ end in an uncommon trap. In that case, the allocation of the\n+  \/\/ loaded value and its initialization is useless. Eliminate it. use\n+  \/\/ the jvm state of the allocation to create a new uncommon trap\n+  \/\/ call at the load.\n+  if (ctl == nullptr || ctl->is_top() || mem == nullptr || mem->is_top() || !mem->is_MergeMem()) {\n+    return false;\n+  }\n+  PhaseIterGVN* igvn = phase->is_IterGVN();\n+  if (ctl->is_Region()) {\n+    bool res = false;\n+    for (uint i = 1; i < ctl->req(); i++) {\n+      MergeMemNode* mm = mem->clone()->as_MergeMem();\n+      for (MergeMemStream mms(mm); mms.next_non_empty(); ) {\n+        Node* m = mms.memory();\n+        if (m->is_Phi() && m->in(0) == ctl) {\n+          mms.set_memory(m->in(i));\n+        }\n+      }\n+      if (remove_useless_allocation(phase, ctl->in(i), mm, unc_arg)) {\n+        res = true;\n+        if (!ctl->in(i)->is_Region()) {\n+          igvn->replace_input_of(ctl, i, phase->C->top());\n+        }\n+      }\n+      igvn->remove_dead_node(mm);\n+    }\n+    return res;\n+  }\n+  \/\/ verify the control flow is ok\n+  Node* call = ctl;\n+  MemBarNode* membar = nullptr;\n+  for (;;) {\n+    if (call == nullptr || call->is_top()) {\n+      return false;\n+    }\n+    if (call->is_Proj() || call->is_Catch() || call->is_MemBar()) {\n+      call = call->in(0);\n+    } else if (call->Opcode() == Op_CallStaticJava &&\n+               call->as_Call()->entry_point() == OptoRuntime::load_unknown_inline_Java()) {\n+      assert(call->in(0)->is_Proj() && call->in(0)->in(0)->is_MemBar(), \"missing membar\");\n+      membar = call->in(0)->in(0)->as_MemBar();\n+      break;\n+    } else {\n+      return false;\n+    }\n+  }\n+\n+  JVMState* jvms = call->jvms();\n+  if (phase->C->too_many_traps(jvms->method(), jvms->bci(), Deoptimization::trap_request_reason(uncommon_trap_request()))) {\n+    return false;\n+  }\n+\n+  Node* alloc_mem = call->in(TypeFunc::Memory);\n+  if (alloc_mem == nullptr || alloc_mem->is_top()) {\n+    return false;\n+  }\n+  if (!alloc_mem->is_MergeMem()) {\n+    alloc_mem = MergeMemNode::make(alloc_mem);\n+    igvn->register_new_node_with_optimizer(alloc_mem);\n+  }\n+\n+  \/\/ and that there's no unexpected side effect\n+  for (MergeMemStream mms2(mem->as_MergeMem(), alloc_mem->as_MergeMem()); mms2.next_non_empty2(); ) {\n+    Node* m1 = mms2.is_empty() ? mms2.base_memory() : mms2.memory();\n+    Node* m2 = mms2.memory2();\n+\n+    for (uint i = 0; i < 100; i++) {\n+      if (m1 == m2) {\n+        break;\n+      } else if (m1->is_Proj()) {\n+        m1 = m1->in(0);\n+      } else if (m1->is_MemBar()) {\n+        m1 = m1->in(TypeFunc::Memory);\n+      } else if (m1->Opcode() == Op_CallStaticJava &&\n+                 m1->as_Call()->entry_point() == OptoRuntime::load_unknown_inline_Java()) {\n+        if (m1 != call) {\n+          return false;\n+        }\n+        break;\n+      } else if (m1->is_MergeMem()) {\n+        MergeMemNode* mm = m1->as_MergeMem();\n+        int idx = mms2.alias_idx();\n+        if (idx == Compile::AliasIdxBot) {\n+          m1 = mm->base_memory();\n+        } else {\n+          m1 = mm->memory_at(idx);\n+        }\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+  if (alloc_mem->outcnt() == 0) {\n+    igvn->remove_dead_node(alloc_mem);\n+  }\n+\n+  \/\/ Remove membar preceding the call\n+  membar->remove(igvn);\n+\n+  address call_addr = SharedRuntime::uncommon_trap_blob()->entry_point();\n+  CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\", nullptr);\n+  unc->init_req(TypeFunc::Control, call->in(0));\n+  unc->init_req(TypeFunc::I_O, call->in(TypeFunc::I_O));\n+  unc->init_req(TypeFunc::Memory, call->in(TypeFunc::Memory));\n+  unc->init_req(TypeFunc::FramePtr,  call->in(TypeFunc::FramePtr));\n+  unc->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  unc->init_req(TypeFunc::Parms+0, unc_arg);\n+  unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+  unc->copy_call_debug_info(igvn, call->as_CallStaticJava());\n+\n+  igvn->replace_input_of(call, 0, phase->C->top());\n+\n+  igvn->register_new_node_with_optimizer(unc);\n+\n+  Node* ctrl = phase->transform(new ProjNode(unc, TypeFunc::Control));\n+  Node* halt = phase->transform(new HaltNode(ctrl, call->in(TypeFunc::FramePtr), \"uncommon trap returned which should never happen\"));\n+  igvn->add_input_to(phase->C->root(), halt);\n+\n+  return true;\n+}\n+\n+\n@@ -1235,0 +1417,7 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -1240,1 +1429,1 @@\n-  assert(tf()->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte == _num_bits,\n+  assert(tf()->range_sig()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte == _num_bits,\n@@ -1242,1 +1431,1 @@\n-  const TypeTuple* d = tf()->domain();\n+  const TypeTuple* d = tf()->domain_sig();\n@@ -1266,0 +1455,6 @@\n+uint CallLeafNoFPNode::match_edge(uint idx) const {\n+  \/\/ Null entry point is a special case for which the target is in a\n+  \/\/ register. Need to match that edge.\n+  return entry_point() == nullptr && idx == TypeFunc::Parms;\n+}\n+\n@@ -1316,1 +1511,14 @@\n-  return remove_dead_region(phase, can_reshape) ? this : nullptr;\n+  if (remove_dead_region(phase, can_reshape)) {\n+    return this;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  if (phase->C->scalarize_in_safepoints() && can_reshape && jvms() != nullptr) {\n+    for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+      Node* n = in(i)->uncast();\n+      if (n->is_InlineType()) {\n+        n->as_InlineType()->make_scalar_in_safepoints(phase->is_IterGVN());\n+      }\n+    }\n+  }\n+  return nullptr;\n@@ -1473,1 +1681,1 @@\n-  if (!alloc->is_Allocate() && !(alloc->Opcode() == Op_VectorBox)) {\n+  if (alloc != nullptr && !alloc->is_Allocate() && !(alloc->Opcode() == Op_VectorBox)) {\n@@ -1578,1 +1786,3 @@\n-                           Node *size, Node *klass_node, Node *initial_test)\n+                           Node *size, Node *klass_node,\n+                           Node* initial_test,\n+                           InlineTypeNode* inline_type_node)\n@@ -1586,0 +1796,1 @@\n+  _larval = false;\n@@ -1598,0 +1809,3 @@\n+  init_req( InlineType     , inline_type_node);\n+  \/\/ DefaultValue defaults to nullptr\n+  \/\/ RawDefaultValue defaults to nullptr\n@@ -1604,3 +1818,2 @@\n-         initializer->is_initializer() &&\n-         !initializer->is_static(),\n-             \"unexpected initializer method\");\n+         initializer->is_object_constructor_or_class_initializer(),\n+         \"unexpected initializer method\");\n@@ -1617,1 +1830,2 @@\n-Node *AllocateNode::make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem) {\n+\n+Node* AllocateNode::make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem) {\n@@ -1619,3 +1833,10 @@\n-  \/\/ For now only enable fast locking for non-array types\n-  mark_node = phase->MakeConX(markWord::prototype().value());\n-  return mark_node;\n+  if (EnableValhalla) {\n+    Node* klass_node = in(AllocateNode::KlassNode);\n+    Node* proto_adr = phase->transform(new AddPNode(klass_node, klass_node, phase->MakeConX(in_bytes(Klass::prototype_header_offset()))));\n+    mark_node = LoadNode::make(*phase, control, mem, proto_adr, TypeRawPtr::BOTTOM, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  } else {\n+    mark_node = phase->MakeConX(markWord::prototype().value());\n+  }\n+  mark_node = phase->transform(mark_node);\n+  \/\/ Avoid returning a constant (old node) here because this method is used by LoadNode::Ideal\n+  return new OrXNode(mark_node, phase->MakeConX(_larval ? markWord::larval_bit_in_place : 0));\n@@ -2000,1 +2221,2 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() && !obj_type->is_inlinetypeptr()) {\n@@ -2196,1 +2418,2 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() && !obj_type->is_inlinetypeptr()) {\n@@ -2276,1 +2499,2 @@\n-    dest_t = dest_t->add_offset(Type::OffsetBot)->is_oopptr();\n+    dest_t = dest_t->is_aryptr()->with_field_offset(Type::OffsetBot)->add_offset(Type::OffsetBot)->is_oopptr();\n+    t_oop = t_oop->is_aryptr()->with_field_offset(Type::OffsetBot);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":306,"deletions":82,"binary":false,"changes":388,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n@@ -93,1 +93,0 @@\n-  static  const TypeTuple *osr_domain();\n@@ -645,1 +644,1 @@\n-class CallProjections : public StackObj {\n+class CallProjections {\n@@ -654,1 +653,19 @@\n-  Node* resproj;\n+  uint nb_resproj;\n+  Node* resproj[1]; \/\/ at least one projection\n+\n+  CallProjections(uint nbres) {\n+    fallthrough_proj      = nullptr;\n+    fallthrough_catchproj = nullptr;\n+    fallthrough_memproj   = nullptr;\n+    fallthrough_ioproj    = nullptr;\n+    catchall_catchproj    = nullptr;\n+    catchall_memproj      = nullptr;\n+    catchall_ioproj       = nullptr;\n+    exobj                 = nullptr;\n+    nb_resproj            = nbres;\n+    resproj[0]            = nullptr;\n+    for (uint i = 1; i < nb_resproj; i++) {\n+      resproj[i]          = nullptr;\n+    }\n+  }\n+\n@@ -677,1 +694,1 @@\n-    : SafePointNode(tf->domain()->cnt(), jvms, adr_type),\n+    : SafePointNode(tf->domain_cc()->cnt(), jvms, adr_type),\n@@ -704,1 +721,1 @@\n-  virtual Node*       match(const ProjNode* proj, const Matcher* m);\n+  virtual Node*       match(const ProjNode* proj, const Matcher* m, const RegMask* mask);\n@@ -718,0 +735,1 @@\n+  bool                has_debug_use(Node* n);\n@@ -724,2 +742,3 @@\n-    const TypeTuple* r = tf()->range();\n-    return (r->cnt() > TypeFunc::Parms &&\n+    const TypeTuple* r = tf()->range_sig();\n+    return (!tf()->returns_inline_type_as_fields() &&\n+            r->cnt() > TypeFunc::Parms &&\n@@ -732,1 +751,1 @@\n-  void extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts = true);\n+  CallProjections* extract_projections(bool separate_io_proj, bool do_asserts = true);\n@@ -802,0 +821,3 @@\n+\n+  bool remove_useless_allocation(PhaseGVN *phase, Node* ctl, Node* mem, Node* unc_arg);\n+\n@@ -810,0 +832,11 @@\n+    const TypeTuple *r = tf->range_sig();\n+    if (InlineTypeReturnedAsFields &&\n+        method != nullptr &&\n+        method->is_method_handle_intrinsic() &&\n+        r->cnt() > TypeFunc::Parms &&\n+        r->field_at(TypeFunc::Parms)->isa_oopptr() &&\n+        r->field_at(TypeFunc::Parms)->is_oopptr()->can_be_inline_type()) {\n+      \/\/ Make sure this call is processed by PhaseMacroExpand::expand_mh_intrinsic_return\n+      init_flags(Flag_is_macro);\n+      C->add_macro_node(this);\n+    }\n@@ -920,0 +953,1 @@\n+  virtual uint match_edge(uint idx) const;\n@@ -962,0 +996,3 @@\n+    InlineType,                       \/\/ InlineTypeNode if this is an inline type allocation\n+    DefaultValue,                     \/\/ default value in case of non-flattened inline type array\n+    RawDefaultValue,                  \/\/ same as above but as raw machine word\n@@ -972,0 +1009,3 @@\n+    fields[InlineType] = Type::BOTTOM;\n+    fields[DefaultValue] = TypeInstPtr::NOTNULL;\n+    fields[RawDefaultValue] = TypeX_X;\n@@ -989,0 +1029,1 @@\n+  bool _larval;\n@@ -992,1 +1033,2 @@\n-               Node *size, Node *klass_node, Node *initial_test);\n+               Node *size, Node *klass_node, Node *initial_test,\n+               InlineTypeNode* inline_type_node = nullptr);\n@@ -1057,1 +1099,1 @@\n-  Node* make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem);\n+  Node* make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem);\n@@ -1067,1 +1109,2 @@\n-                    Node* initial_test, Node* count_val, Node* valid_length_test)\n+                    Node* initial_test, Node* count_val, Node* valid_length_test,\n+                    Node* default_value, Node* raw_default_value)\n@@ -1074,0 +1117,2 @@\n+    init_req(AllocateNode::DefaultValue,  default_value);\n+    init_req(AllocateNode::RawDefaultValue, raw_default_value);\n@@ -1193,1 +1238,1 @@\n-    return TypeFunc::make(domain,range);\n+    return TypeFunc::make(domain, range);\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":58,"deletions":13,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -185,0 +185,1 @@\n+macro(FlatArrayCheck)\n@@ -373,0 +374,1 @@\n+macro(InlineType)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -395,0 +396,3 @@\n+  if (dead->is_InlineType()) {\n+    remove_inline_type(dead);\n+  }\n@@ -436,0 +440,3 @@\n+    if (n->outcnt() == 0) {\n+      worklist.push(n);\n+    }\n@@ -443,0 +450,6 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n+#ifdef ASSERT\n+  if (_modified_nodes != nullptr) {\n+    _modified_nodes->remove_useless_nodes(useful.member_set());\n+  }\n+#endif\n@@ -619,0 +632,1 @@\n+                  _has_circular_inline_type(false),\n@@ -638,0 +652,1 @@\n+                  _inline_type_nodes (comp_arena(), 8, 0, nullptr),\n@@ -746,4 +761,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -756,1 +769,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -883,0 +896,10 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n+  \/\/ TODO 8284443 Only reserve extra slot if needed\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ One extra slot to hold the IsInit information for a nullable\n+    \/\/ inline type return if we run out of registers.\n+    next_slot += 2;\n+  }\n@@ -916,0 +939,1 @@\n+    _has_circular_inline_type(false),\n@@ -1045,0 +1069,4 @@\n+  _has_flattened_accesses = false;\n+  _flattened_accesses_share_alias = true;\n+  _scalarize_in_safepoints = false;\n+\n@@ -1333,1 +1361,2 @@\n-    assert(InlineUnsafeOps || StressReflectiveCode, \"indeterminate pointers come only from unsafe ops\");\n+    bool default_value_load = EnableValhalla && tj->is_instptr()->instance_klass() == ciEnv::current()->Class_klass();\n+    assert(InlineUnsafeOps || StressReflectiveCode || default_value_load, \"indeterminate pointers come only from unsafe ops\");\n@@ -1346,0 +1375,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1359,0 +1397,2 @@\n+    \/\/ For flattened inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1399,1 +1439,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n@@ -1403,1 +1443,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInstPtr::BOTTOM && _flattened_accesses_share_alias) {\n+      const TypeAry* tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size(), \/* stable= *\/ false, \/* flat= *\/ true);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1410,1 +1455,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1460,1 +1505,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, Type::Offset(offset));\n@@ -1475,1 +1520,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, nullptr, offset, to->instance_id());\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, nullptr, Type::Offset(offset), to->instance_id());\n@@ -1477,1 +1522,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, nullptr, offset);\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, nullptr, Type::Offset(offset));\n@@ -1493,1 +1538,1 @@\n-                                       offset);\n+                                       Type::Offset(offset));\n@@ -1499,1 +1544,1 @@\n-        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), offset);\n+        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), Type::Offset(offset));\n@@ -1501,1 +1546,1 @@\n-        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, offset);\n+        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, Type::Offset(offset), tk->is_not_flat(), tk->is_not_null_free(), tk->is_null_free());\n@@ -1504,1 +1549,0 @@\n-\n@@ -1634,1 +1678,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1639,3 +1683,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = nullptr;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1691,0 +1738,1 @@\n+    ciField* field = nullptr;\n@@ -1697,0 +1745,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1698,1 +1747,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (flat->is_flat() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->first_field_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1710,0 +1766,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1720,1 +1778,0 @@\n-      ciField* field;\n@@ -1727,0 +1784,4 @@\n+      } else if (tinst->is_inlinetypeptr()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1731,7 +1792,14 @@\n-      assert(field == nullptr ||\n-             original_field == nullptr ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset_in_bytes() == original_field->offset_in_bytes() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != nullptr)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == nullptr ||\n+           original_field == nullptr ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset_in_bytes() == original_field->offset_in_bytes() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != nullptr) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1742,3 +1810,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1746,6 +1815,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == nullptr) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == nullptr) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1869,0 +1939,418 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineType()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    } else if (n->is_CheckCastPP()) {\n+      wq.push(n->in(1));\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make sure that the return value does not keep an otherwise unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = nullptr;\n+    for (uint i = 1; i < root()->req(); i++) {\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == nullptr, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != nullptr) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  if (_inline_type_nodes.length() == 0) {\n+    return;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  set_scalarize_in_safepoints(true);\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    _inline_type_nodes.at(i)->as_InlineType()->make_scalar_in_safepoints(&igvn);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes by replacing them with their oop input\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeNode* vt = _inline_type_nodes.pop()->as_InlineType();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+        continue;\n+      }\n+      for (DUIterator i = vt->outs(); vt->has_out(i); i++) {\n+        DEBUG_ONLY(bool must_be_buffered = false);\n+        Node* u = vt->out(i);\n+        \/\/ Check if any users are blackholes. If so, rewrite them to use either the\n+        \/\/ allocated buffer, or individual components, instead of the inline type node\n+        \/\/ that goes away.\n+        if (u->is_Blackhole()) {\n+          BlackholeNode* bh = u->as_Blackhole();\n+\n+          \/\/ Unlink the old input\n+          int idx = bh->find_edge(vt);\n+          assert(idx != -1, \"The edge should be there\");\n+          bh->del_req(idx);\n+          --i;\n+\n+          if (vt->is_allocated(&igvn)) {\n+            \/\/ Already has the allocated instance, blackhole that\n+            bh->add_req(vt->get_oop());\n+          } else {\n+            \/\/ Not allocated yet, blackhole the components\n+            for (uint c = 0; c < vt->field_count(); c++) {\n+              bh->add_req(vt->field_value(c));\n+            }\n+          }\n+\n+          \/\/ Node modified, record for IGVN\n+          igvn.record_for_igvn(bh);\n+        }\n+#ifdef ASSERT\n+        \/\/ Verify that inline type is buffered when replacing by oop\n+        else if (u->is_InlineType()) {\n+          InlineTypeNode* vt2 = u->as_InlineType();\n+          for (uint i = 0; i < vt2->field_count(); ++i) {\n+            if (vt2->field_value(i) == vt && !vt2->field_is_flattened(i)) {\n+              \/\/ Use in non-flat field\n+              must_be_buffered = true;\n+            }\n+          }\n+        } else if (u->is_Phi()) {\n+          \/\/ TODO 8302217 Remove this once InlineTypeNodes are reliably pushed through\n+        } else if (u->Opcode() != Op_Return || !tf()->returns_inline_type_as_fields()) {\n+          must_be_buffered = true;\n+        }\n+        if (must_be_buffered && !vt->is_allocated(&igvn)) {\n+          vt->dump(0);\n+          u->dump(0);\n+          assert(false, \"Should have been buffered\");\n+        }\n+#endif\n+      }\n+      igvn.replace_node(vt, vt->get_oop());\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flattened_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flattened_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flattened array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flattened array) correct. We're done with parsing so we\n+  \/\/ now know all flattened array accesses in this compile\n+  \/\/ unit. Let's move flattened array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flattened memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flattened array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = nullptr;\n+      if (n->Opcode() == Op_StoreCM) {\n+        adr_type = get_adr_type(get_alias_index(n->in(MemNode::OopStore)->adr_type()));\n+      } else {\n+        adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      }\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != nullptr) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flattened_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flattened array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != nullptr &&\n+          ace->_adr_type->is_flat()) {\n+        ace->_adr_type = nullptr;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the nullptr adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = nullptr;\n+      if (m->Opcode() == Op_StoreCM) {\n+        adr_type = m->in(MemNode::OopStore)->adr_type();\n+        if (adr_type != TypeAryPtr::INLINES) {\n+          \/\/ store was optimized out and we lost track of the adr_type\n+          Node* clone = new StoreCMNode(m->in(MemNode::Control), m->in(MemNode::Memory), m->in(MemNode::Address),\n+                                        m->adr_type(), m->in(MemNode::ValueIn), m->in(MemNode::OopStore),\n+                                        get_alias_index(adr_type));\n+          igvn.register_new_node_with_optimizer(clone);\n+          igvn.replace_node(m, clone);\n+        }\n+      } else {\n+        adr_type = m->adr_type();\n+#ifdef ASSERT\n+        m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      }\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flattened array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = nullptr;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = nullptr;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == nullptr) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const TypePtr* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flattened arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flattened array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const TypePtr* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, nullptr);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != nullptr) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const TypePtr* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flattened_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != nullptr) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n@@ -2154,1 +2642,4 @@\n-  assert(_modified_nodes == nullptr, \"not allowed\");\n+#ifdef ASSERT\n+  Unique_Node_List* modified_nodes = _modified_nodes;\n+  _modified_nodes = nullptr;\n+#endif\n@@ -2167,0 +2658,1 @@\n+  DEBUG_ONLY( _modified_nodes = modified_nodes; )\n@@ -2304,0 +2796,5 @@\n+  \/\/ Process inline type nodes now that all inlining is over\n+  process_inline_types(igvn);\n+\n+  adjust_flattened_array_access_aliases(igvn);\n+\n@@ -2419,0 +2916,8 @@\n+  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n+\n+  if (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual and MH calls.\n+    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n+    process_late_inline_calls_no_inline(igvn);\n+  }\n+\n@@ -2429,0 +2934,4 @@\n+  \/\/ Process inline type nodes again and remove them. From here\n+  \/\/ on we don't need to keep track of field values anymore.\n+  process_inline_types(igvn, \/* remove= *\/ true);\n+\n@@ -2444,0 +2953,1 @@\n+  DEBUG_ONLY( _late_inlines.clear(); )\n@@ -2446,8 +2956,0 @@\n-\n-  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n-\n-  if (_late_inlines.length() > 0) {\n-    \/\/ More opportunities to optimize virtual and MH calls.\n-    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n-    process_late_inline_calls_no_inline(igvn);\n-  }\n@@ -3076,0 +3578,1 @@\n+\n@@ -3228,1 +3731,16 @@\n-      n->add_prec(prec);\n+      if (prec->is_MergeMem()) {\n+        MergeMemNode* mm = prec->as_MergeMem();\n+        Node* base = mm->base_memory();\n+        for (int i = AliasIdxRaw + 1; i < num_alias_types(); i++) {\n+          const TypePtr* adr_type = get_adr_type(i);\n+          if (adr_type->is_flat()) {\n+            Node* m = mm->memory_at(i);\n+            n->add_prec(m);\n+          }\n+        }\n+        if (mm->outcnt() == 0) {\n+          mm->disconnect_inputs(this);\n+        }\n+      } else {\n+        n->add_prec(prec);\n+      }\n@@ -3824,0 +4342,7 @@\n+#ifdef ASSERT\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n+#endif\n@@ -4205,2 +4730,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -4214,1 +4739,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4270,0 +4795,1 @@\n+               (n->is_Allocate() && i >= AllocateNode::InlineType) ||\n@@ -4272,1 +4798,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+              \"only region, phi, arraycopy, allocate, unlock or membar nodes have null data edges\");\n@@ -4406,0 +4932,8 @@\n+\n+    \/\/ Do not fold the subtype check to an array klass pointer comparison for [V? arrays.\n+    \/\/ [QMyValue is a subtype of [LMyValue but the klass for [QMyValue is not equal to\n+    \/\/ the klass for [LMyValue. Perform a full test.\n+    if (!superk->is_aryklassptr()->is_null_free() && superk->is_aryklassptr()->elem()->isa_instklassptr() &&\n+        superk->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->is_inlinetype()) {\n+      return SSC_full_test;\n+    }\n@@ -4957,0 +5491,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == nullptr || tb == nullptr ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are nullptr.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(nullptr, a));\n+    b = phase->transform(new CastP2XNode(nullptr, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":607,"deletions":52,"binary":false,"changes":659,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+class CallNode;\n@@ -94,0 +95,1 @@\n+class InlineTypeNode;\n@@ -323,0 +325,1 @@\n+  bool                  _has_circular_inline_type; \/\/ True if method loads an inline type with a circular, non-flattened field\n@@ -349,0 +352,3 @@\n+  bool                  _has_flattened_accesses; \/\/ Any known flattened array accesses?\n+  bool                  _flattened_accesses_share_alias; \/\/ Initially all flattened array share a single slice\n+  bool                  _scalarize_in_safepoints; \/\/ Scalarize inline types in safepoint debug info\n@@ -364,0 +370,1 @@\n+  GrowableArray<Node*>  _inline_type_nodes;     \/\/ List of InlineType nodes\n@@ -623,0 +630,2 @@\n+  bool              has_circular_inline_type() const { return _has_circular_inline_type; }\n+  void          set_has_circular_inline_type(bool z) { _has_circular_inline_type = z; }\n@@ -659,0 +668,10 @@\n+  void          set_flattened_accesses()         { _has_flattened_accesses = true; }\n+  bool          flattened_accesses_share_alias() const { return _flattened_accesses_share_alias; }\n+  void          set_flattened_accesses_share_alias(bool z) { _flattened_accesses_share_alias = z; }\n+  bool          scalarize_in_safepoints() const { return _scalarize_in_safepoints; }\n+  void          set_scalarize_in_safepoints(bool z) { _scalarize_in_safepoints = z; }\n+\n+  \/\/ Support for scalarized inline type calling convention\n+  bool              has_scalarized_args() const  { return _method != nullptr && _method->has_scalarized_args(); }\n+  bool              needs_stack_repair()  const  { return _method != nullptr && _method->get_Method()->c2_needs_stack_repair(); }\n+\n@@ -771,0 +790,7 @@\n+  \/\/ Keep track of inline type nodes for later processing\n+  void add_inline_type(Node* n);\n+  void remove_inline_type(Node* n);\n+  void process_inline_types(PhaseIterGVN &igvn, bool remove = false);\n+\n+  void adjust_flattened_array_access_aliases(PhaseIterGVN& igvn);\n+\n@@ -913,1 +939,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }\n@@ -917,1 +943,1 @@\n-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }\n+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, nullptr, uncached)->index(); }\n@@ -1159,1 +1185,1 @@\n-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);\n+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);\n@@ -1235,1 +1261,3 @@\n-  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  Node* optimize_acmp(PhaseGVN* phase, Node* a, Node* b);\n+\n+  \/\/ Auxiliary method for randomized fuzzing\/stressing\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -587,1 +588,1 @@\n-  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_initializer()) {\n+  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_constructor()) {\n@@ -717,1 +718,1 @@\n-          \/\/ It's OK for a method  to return a value that is discarded.\n+          \/\/ It's OK for a method to return a value that is discarded.\n@@ -729,0 +730,3 @@\n+            if (declared_signature->returns_null_free_inline_type()) {\n+              sig_type = sig_type->join_speculative(TypePtr::NOTNULL);\n+            }\n@@ -775,0 +779,5 @@\n+    if (rtype->is_inlinetype() && !peek()->is_InlineType()) {\n+      Node* retnode = pop();\n+      retnode = InlineTypeNode::make_from_oop(this, retnode, rtype->as_inline_klass(), !gvn().type(retnode)->maybe_null());\n+      push_node(T_PRIMITIVE_OBJECT, retnode);\n+    }\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -164,0 +165,10 @@\n+    if ((n->Opcode() == Op_LoadX || n->Opcode() == Op_StoreX) &&\n+        !n->in(MemNode::Address)->is_AddP() &&\n+        _igvn->type(n->in(MemNode::Address))->isa_oopptr()) {\n+      \/\/ Load\/Store at mark work address is at offset 0 so has no AddP which confuses EA\n+      Node* addp = new AddPNode(n->in(MemNode::Address), n->in(MemNode::Address), _igvn->MakeConX(0));\n+      _igvn->register_new_node_with_optimizer(addp);\n+      _igvn->replace_input_of(n, MemNode::Address, addp);\n+      ideal_nodes.push(addp);\n+      _nodes.at_put_grow(addp->_idx, nullptr, nullptr);\n+    }\n@@ -665,1 +676,3 @@\n-      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt);\n+      Unique_Node_List value_worklist;\n+      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt, &value_worklist);\n+      guarantee(value_worklist.size() == 0, \"Unimplemented: Valhalla support for 8287061\");\n@@ -823,1 +836,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_sig();\n@@ -897,0 +910,11 @@\n+      } else if (n->as_Call()->tf()->returns_inline_type_as_fields()) {\n+        bool returns_oop = false;\n+        for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax && !returns_oop; i++) {\n+          ProjNode* pn = n->fast_out(i)->as_Proj();\n+          if (pn->_con >= TypeFunc::Parms && pn->bottom_type()->isa_ptr()) {\n+            returns_oop = true;\n+          }\n+        }\n+        if (returns_oop) {\n+          add_call_node(n->as_Call());\n+        }\n@@ -928,0 +952,1 @@\n+    case Op_InlineType:\n@@ -999,2 +1024,4 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_Call() &&\n+          (n->in(0)->as_Call()->returns_pointer() || n->bottom_type()->isa_ptr())) {\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+               n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -1102,0 +1129,1 @@\n+    case Op_InlineType:\n@@ -1156,2 +1184,2 @@\n-      assert(n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-             n->in(0)->as_Call()->returns_pointer(), \"Unexpected node type\");\n+      assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+             n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -1333,1 +1361,1 @@\n-  assert(call->returns_pointer(), \"only for call which returns pointer\");\n+  assert(call->returns_pointer() || call->tf()->returns_inline_type_as_fields(), \"only for call which returns pointer\");\n@@ -1409,1 +1437,2 @@\n-      assert(strncmp(name, \"_multianewarray\", 15) == 0, \"TODO: add failed case check\");\n+      assert(strncmp(name, \"_multianewarray\", 15) == 0 ||\n+             strncmp(name, \"_load_unknown_inline\", 20) == 0, \"TODO: add failed case check\");\n@@ -1440,1 +1469,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -1488,1 +1517,1 @@\n-      const TypeTuple * d = call->tf()->domain();\n+      const TypeTuple * d = call->tf()->domain_sig();\n@@ -1519,1 +1548,4 @@\n-                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != nullptr)));\n+                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != nullptr)) ||\n+                               (aat->isa_aryptr() && aat->isa_aryptr()->elem() != nullptr &&\n+                                                               aat->isa_aryptr()->is_flat() &&\n+                                                               aat->isa_aryptr()->elem()->inline_klass()->contains_oops()));\n@@ -1568,0 +1600,3 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"vectorizedMismatch\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"load_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_unknown_inline\") == 0 ||\n@@ -1630,1 +1665,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -1674,1 +1709,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_cc();\n@@ -2087,0 +2122,1 @@\n+  PointsToNode* init_val = phantom_obj;\n@@ -2092,1 +2128,8 @@\n-    return 0;\n+    if (alloc->as_Allocate()->in(AllocateNode::DefaultValue) != nullptr) {\n+      \/\/ Non-flattened inline type arrays are initialized with\n+      \/\/ the default value instead of null. Handle them here.\n+      init_val = ptnode_adr(alloc->as_Allocate()->in(AllocateNode::DefaultValue)->_idx);\n+      assert(init_val != nullptr, \"default value should be registered\");\n+    } else {\n+      return 0;\n+    }\n@@ -2094,1 +2137,2 @@\n-  assert(pta->arraycopy_dst() || alloc->as_CallStaticJava(), \"sanity\");\n+  \/\/ Non-escaped allocation returned from Java or runtime call has unknown values in fields.\n+  assert(pta->arraycopy_dst() || alloc->is_CallStaticJava() || init_val != phantom_obj, \"sanity\");\n@@ -2096,1 +2140,1 @@\n-  if (!pta->arraycopy_dst() && alloc->as_CallStaticJava()->method() == nullptr) {\n+  if (alloc->is_CallStaticJava() && alloc->as_CallStaticJava()->method() == nullptr) {\n@@ -2098,1 +2142,2 @@\n-    assert(strncmp(name, \"_multianewarray\", 15) == 0, \"sanity\");\n+    assert(strncmp(name, \"_multianewarray\", 15) == 0 ||\n+           strncmp(name, \"_load_unknown_inline\", 20) == 0, \"sanity\");\n@@ -2106,1 +2151,1 @@\n-      if (add_edge(field, phantom_obj)) {\n+      if (add_edge(field, init_val)) {\n@@ -2121,1 +2166,1 @@\n-  if (!alloc->is_Allocate()) {\n+  if (!alloc->is_Allocate() || alloc->as_Allocate()->in(AllocateNode::DefaultValue) != nullptr) {\n@@ -2207,1 +2252,1 @@\n-                tty->print_cr(\"----------missed referernce to object-----------\");\n+                tty->print_cr(\"----------missed reference to object------------\");\n@@ -2209,1 +2254,1 @@\n-                tty->print_cr(\"----------object referernced by init store -----\");\n+                tty->print_cr(\"----------object referenced by init store-------\");\n@@ -2511,1 +2556,2 @@\n-          if (not_global_escape(alock->obj_node())) {\n+          const Type* obj_type = igvn->type(alock->obj_node());\n+          if (not_global_escape(alock->obj_node()) && !obj_type->is_inlinetypeptr()) {\n@@ -2552,5 +2598,10 @@\n-      MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n-      mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n-      mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n-      igvn->register_new_node_with_optimizer(mb);\n-      igvn->replace_node(storestore, mb);\n+      if (alloc->in(AllocateNode::InlineType) != nullptr) {\n+        \/\/ Non-escaping inline type buffer allocations don't require a membar\n+        storestore->as_MemBar()->remove(_igvn);\n+      } else {\n+        MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n+        mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n+        mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n+        igvn->register_new_node_with_optimizer(mb);\n+        igvn->replace_node(storestore, mb);\n+      }\n@@ -2710,0 +2761,1 @@\n+  int field_offset = adr_type->isa_aryptr() ? adr_type->isa_aryptr()->field_offset().get() : Type::OffsetBot;\n@@ -2711,1 +2763,1 @@\n-  if (offset == Type::OffsetBot) {\n+  if (offset == Type::OffsetBot && field_offset == Type::OffsetBot) {\n@@ -2723,1 +2775,1 @@\n-      ciField* field = _compile->alias_type(adr_type->isa_instptr())->field();\n+      ciField* field = _compile->alias_type(adr_type->is_ptr())->field();\n@@ -2742,2 +2794,8 @@\n-        const Type* elemtype = adr_type->isa_aryptr()->elem();\n-        bt = elemtype->array_element_basic_type();\n+        const Type* elemtype = adr_type->is_aryptr()->elem();\n+        if (adr_type->is_aryptr()->is_flat() && field_offset != Type::OffsetBot) {\n+          ciInlineKlass* vk = elemtype->inline_klass();\n+          field_offset += vk->first_field_offset();\n+          bt = vk->get_field_by_offset(field_offset, false)->layout_type();\n+        } else {\n+          bt = elemtype->array_element_basic_type();\n+        }\n@@ -2926,3 +2984,1 @@\n-  const TypePtr *t_ptr = adr_type->isa_ptr();\n-  assert(t_ptr != nullptr, \"must be a pointer type\");\n-  return t_ptr->offset();\n+  return adr_type->is_ptr()->flattened_offset();\n@@ -3082,1 +3138,8 @@\n-    t = base_t->add_offset(offs)->is_oopptr();\n+    if (base_t->isa_aryptr() != nullptr) {\n+      \/\/ In the case of a flattened inline type array, each field has its\n+      \/\/ own slice so we need to extract the field being accessed from\n+      \/\/ the address computation\n+      t = base_t->isa_aryptr()->add_field_offset_and_offset(offs)->is_oopptr();\n+    } else {\n+      t = base_t->add_offset(offs)->is_oopptr();\n+    }\n@@ -3084,1 +3147,1 @@\n-  int inst_id =  base_t->instance_id();\n+  int inst_id = base_t->instance_id();\n@@ -3098,1 +3161,1 @@\n-  \/\/ It could happened when CHA type is different from MDO type on a dead path\n+  \/\/ It could happen when CHA type is different from MDO type on a dead path\n@@ -3108,1 +3171,12 @@\n-  const TypeOopPtr *tinst = base_t->add_offset(t->offset())->is_oopptr();\n+  const TypePtr* tinst = base_t->add_offset(t->offset());\n+  if (tinst->isa_aryptr() && t->isa_aryptr()) {\n+    \/\/ In the case of a flattened inline type array, each field has its\n+    \/\/ own slice so we need to keep track of the field being accessed.\n+    tinst = tinst->is_aryptr()->with_field_offset(t->is_aryptr()->field_offset().get());\n+    \/\/ Keep array properties (not flat\/null-free)\n+    tinst = tinst->is_aryptr()->update_properties(t->is_aryptr());\n+    if (tinst == nullptr) {\n+      return false; \/\/ Skip dead path with inconsistent properties\n+    }\n+  }\n+\n@@ -3802,0 +3876,7 @@\n+          if (tn_t->isa_aryptr()) {\n+            \/\/ Keep array properties (not flat\/null-free)\n+            tinst = tinst->is_aryptr()->update_properties(tn_t->is_aryptr());\n+            if (tinst == nullptr) {\n+              continue; \/\/ Skip dead path with inconsistent properties\n+            }\n+          }\n@@ -3827,1 +3908,1 @@\n-      if(use->is_Mem() && use->in(MemNode::Address) == n) {\n+      if (use->is_Mem() && use->in(MemNode::Address) == n) {\n@@ -3863,0 +3944,3 @@\n+      } else if (use->Opcode() == Op_Return) {\n+        \/\/ Allocation is referenced by field of returned inline type\n+        assert(_compile->tf()->returns_inline_type_as_fields(), \"EA: unexpected reference by ReturnNode\");\n@@ -3876,1 +3960,1 @@\n-              op == Op_SubTypeCheck ||\n+              op == Op_SubTypeCheck || op == Op_InlineType || op == Op_FlatArrayCheck ||\n@@ -3945,0 +4029,3 @@\n+    } else if (n->is_CallLeaf() && n->as_CallLeaf()->_name != nullptr &&\n+               strcmp(n->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+      n = n->as_CallLeaf()->proj_out(TypeFunc::Memory);\n@@ -3987,1 +4074,1 @@\n-      } else if(use->is_Mem()) {\n+      } else if (use->is_Mem()) {\n@@ -3996,0 +4083,4 @@\n+      } else if (use->is_CallLeaf() && use->as_CallLeaf()->_name != nullptr &&\n+                 strcmp(use->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+        \/\/ store_unknown_inline overwrites destination array\n+        memnode_worklist.append_if_missing(use);\n@@ -4005,1 +4096,1 @@\n-              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {\n+              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar || op == Op_FlatArrayCheck)) {\n@@ -4092,1 +4183,1 @@\n-  \/\/ chains as is done in split_memory_phi() since they  will\n+  \/\/ chains as is done in split_memory_phi() since they will\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":134,"deletions":43,"binary":false,"changes":177,"status":"modified"},{"patch":"@@ -0,0 +1,1257 @@\n+\/*\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"opto\/addnode.hpp\"\n+#include \"opto\/castnode.hpp\"\n+#include \"opto\/graphKit.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n+#include \"opto\/rootnode.hpp\"\n+#include \"opto\/phaseX.hpp\"\n+\n+\/\/ Clones the inline type to handle control flow merges involving multiple inline types.\n+\/\/ The inputs are replaced by PhiNodes to represent the merged values for the given region.\n+InlineTypeNode* InlineTypeNode::clone_with_phis(PhaseGVN* gvn, Node* region, bool is_init) {\n+  InlineTypeNode* vt = clone()->as_InlineType();\n+  const Type* t = Type::get_const_type(inline_klass());\n+  gvn->set_type(vt, t);\n+  vt->as_InlineType()->set_type(t);\n+\n+  \/\/ Create a PhiNode for merging the oop values\n+  PhiNode* oop = PhiNode::make(region, vt->get_oop(), t);\n+  gvn->set_type(oop, t);\n+  gvn->record_for_igvn(oop);\n+  vt->set_oop(oop);\n+\n+  \/\/ Create a PhiNode for merging the is_buffered values\n+  t = Type::get_const_basic_type(T_BOOLEAN);\n+  Node* is_buffered_node = PhiNode::make(region, vt->get_is_buffered(), t);\n+  gvn->set_type(is_buffered_node, t);\n+  gvn->record_for_igvn(is_buffered_node);\n+  vt->set_req(IsBuffered, is_buffered_node);\n+\n+  \/\/ Create a PhiNode for merging the is_init values\n+  Node* is_init_node;\n+  if (is_init) {\n+    is_init_node = gvn->intcon(1);\n+  } else {\n+    t = Type::get_const_basic_type(T_BOOLEAN);\n+    is_init_node = PhiNode::make(region, vt->get_is_init(), t);\n+    gvn->set_type(is_init_node, t);\n+    gvn->record_for_igvn(is_init_node);\n+  }\n+  vt->set_req(IsInit, is_init_node);\n+\n+  \/\/ Create a PhiNode each for merging the field values\n+  for (uint i = 0; i < vt->field_count(); ++i) {\n+    ciType* type = vt->field_type(i);\n+    Node*  value = vt->field_value(i);\n+    \/\/ We limit scalarization for inline types with circular fields and can therefore observe nodes\n+    \/\/ of the same type but with different scalarization depth during IGVN. To avoid inconsistencies\n+    \/\/ during merging, make sure that we only create Phis for fields that are guaranteed to be scalarized.\n+    bool no_circularity = !gvn->C->has_circular_inline_type() || !gvn->is_IterGVN() || field_is_flattened(i);\n+    if (value->is_InlineType() && no_circularity) {\n+      \/\/ Handle inline type fields recursively\n+      value = value->as_InlineType()->clone_with_phis(gvn, region);\n+    } else {\n+      t = Type::get_const_type(type);\n+      value = PhiNode::make(region, value, t);\n+      gvn->set_type(value, t);\n+      gvn->record_for_igvn(value);\n+    }\n+    vt->set_field_value(i, value);\n+  }\n+  gvn->record_for_igvn(vt);\n+  return vt;\n+}\n+\n+\/\/ Checks if the inputs of the InlineTypeNode were replaced by PhiNodes\n+\/\/ for the given region (see InlineTypeNode::clone_with_phis).\n+bool InlineTypeNode::has_phi_inputs(Node* region) {\n+  \/\/ Check oop input\n+  bool result = get_oop()->is_Phi() && get_oop()->as_Phi()->region() == region;\n+#ifdef ASSERT\n+  if (result) {\n+    \/\/ Check all field value inputs for consistency\n+    for (uint i = Values; i < field_count(); ++i) {\n+      Node* n = in(i);\n+      if (n->is_InlineType()) {\n+        assert(n->as_InlineType()->has_phi_inputs(region), \"inconsistent phi inputs\");\n+      } else {\n+        assert(n->is_Phi() && n->as_Phi()->region() == region, \"inconsistent phi inputs\");\n+      }\n+    }\n+  }\n+#endif\n+  return result;\n+}\n+\n+\/\/ Merges 'this' with 'other' by updating the input PhiNodes added by 'clone_with_phis'\n+InlineTypeNode* InlineTypeNode::merge_with(PhaseGVN* gvn, const InlineTypeNode* other, int pnum, bool transform) {\n+  \/\/ Merge oop inputs\n+  PhiNode* phi = get_oop()->as_Phi();\n+  phi->set_req(pnum, other->get_oop());\n+  if (transform) {\n+    set_oop(gvn->transform(phi));\n+  }\n+\n+  \/\/ Merge is_buffered inputs\n+  phi = get_is_buffered()->as_Phi();\n+  phi->set_req(pnum, other->get_is_buffered());\n+  if (transform) {\n+    set_req(IsBuffered, gvn->transform(phi));\n+  }\n+\n+  \/\/ Merge is_init inputs\n+  Node* is_init = get_is_init();\n+  if (is_init->is_Phi()) {\n+    phi = is_init->as_Phi();\n+    phi->set_req(pnum, other->get_is_init());\n+    if (transform) {\n+      set_req(IsInit, gvn->transform(phi));\n+    }\n+  } else {\n+    assert(is_init->find_int_con(0) == 1, \"only with a non null inline type\");\n+  }\n+\n+  \/\/ Merge field values\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* val1 =        field_value(i);\n+    Node* val2 = other->field_value(i);\n+    if (val1->is_InlineType()) {\n+      val1->as_InlineType()->merge_with(gvn, val2->as_InlineType(), pnum, transform);\n+    } else {\n+      assert(val1->is_Phi(), \"must be a phi node\");\n+      val1->set_req(pnum, val2);\n+    }\n+    if (transform) {\n+      set_field_value(i, gvn->transform(val1));\n+    }\n+  }\n+  return this;\n+}\n+\n+\/\/ Adds a new merge path to an inline type node with phi inputs\n+void InlineTypeNode::add_new_path(Node* region) {\n+  assert(has_phi_inputs(region), \"must have phi inputs\");\n+\n+  PhiNode* phi = get_oop()->as_Phi();\n+  phi->add_req(nullptr);\n+  assert(phi->req() == region->req(), \"must be same size as region\");\n+\n+  phi = get_is_buffered()->as_Phi();\n+  phi->add_req(nullptr);\n+  assert(phi->req() == region->req(), \"must be same size as region\");\n+\n+  phi = get_is_init()->as_Phi();\n+  phi->add_req(nullptr);\n+  assert(phi->req() == region->req(), \"must be same size as region\");\n+\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* val = field_value(i);\n+    if (val->is_InlineType()) {\n+      val->as_InlineType()->add_new_path(region);\n+    } else {\n+      val->as_Phi()->add_req(nullptr);\n+      assert(val->req() == region->req(), \"must be same size as region\");\n+    }\n+  }\n+}\n+\n+Node* InlineTypeNode::field_value(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  return in(Values + index);\n+}\n+\n+\/\/ Get the value of the field at the given offset.\n+\/\/ If 'recursive' is true, flattened inline type fields will be resolved recursively.\n+Node* InlineTypeNode::field_value_by_offset(int offset, bool recursive) const {\n+  \/\/ If the field at 'offset' belongs to a flattened inline type field, 'index' refers to the\n+  \/\/ corresponding InlineTypeNode input and 'sub_offset' is the offset in flattened inline type.\n+  int index = inline_klass()->field_index_by_offset(offset);\n+  int sub_offset = offset - field_offset(index);\n+  Node* value = field_value(index);\n+  assert(value != nullptr, \"field value not found\");\n+  if (recursive && value->is_InlineType()) {\n+    if (field_is_flattened(index)) {\n+      \/\/ Flattened inline type field\n+      InlineTypeNode* vt = value->as_InlineType();\n+      sub_offset += vt->inline_klass()->first_field_offset(); \/\/ Add header size\n+      return vt->field_value_by_offset(sub_offset, recursive);\n+    } else {\n+      assert(sub_offset == 0, \"should not have a sub offset\");\n+      return value;\n+    }\n+  }\n+  assert(!(recursive && value->is_InlineType()), \"should not be an inline type\");\n+  assert(sub_offset == 0, \"offset mismatch\");\n+  return value;\n+}\n+\n+void InlineTypeNode::set_field_value(uint index, Node* value) {\n+  assert(index < field_count(), \"index out of bounds\");\n+  set_req(Values + index, value);\n+}\n+\n+void InlineTypeNode::set_field_value_by_offset(int offset, Node* value) {\n+  set_field_value(field_index(offset), value);\n+}\n+\n+int InlineTypeNode::field_offset(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  return inline_klass()->declared_nonstatic_field_at(index)->offset_in_bytes();\n+}\n+\n+uint InlineTypeNode::field_index(int offset) const {\n+  uint i = 0;\n+  for (; i < field_count() && field_offset(i) != offset; i++) { }\n+  assert(i < field_count(), \"field not found\");\n+  return i;\n+}\n+\n+ciType* InlineTypeNode::field_type(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  return inline_klass()->declared_nonstatic_field_at(index)->type();\n+}\n+\n+bool InlineTypeNode::field_is_flattened(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  ciField* field = inline_klass()->declared_nonstatic_field_at(index);\n+  assert(!field->is_flattened() || field->type()->is_inlinetype(), \"must be an inline type\");\n+  return field->is_flattened();\n+}\n+\n+bool InlineTypeNode::field_is_null_free(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  ciField* field = inline_klass()->declared_nonstatic_field_at(index);\n+  assert(!field->is_flattened() || field->type()->is_inlinetype(), \"must be an inline type\");\n+  return field->is_null_free();\n+}\n+\n+void InlineTypeNode::make_scalar_in_safepoint(PhaseIterGVN* igvn, Unique_Node_List& worklist, SafePointNode* sfpt) {\n+  ciInlineKlass* vk = inline_klass();\n+  uint nfields = vk->nof_nonstatic_fields();\n+  JVMState* jvms = sfpt->jvms();\n+  \/\/ Replace safepoint edge by SafePointScalarObjectNode and add field values\n+  assert(jvms != nullptr, \"missing JVMS\");\n+  uint first_ind = (sfpt->req() - jvms->scloff());\n+  SafePointScalarObjectNode* sobj = new SafePointScalarObjectNode(type()->isa_instptr(),\n+                                                                  nullptr,\n+                                                                  first_ind, nfields);\n+  sobj->init_req(0, igvn->C->root());\n+  \/\/ Nullable inline types have an IsInit field that needs\n+  \/\/ to be checked before using the field values.\n+  if (!igvn->type(get_is_init())->is_int()->is_con(1)) {\n+    sfpt->add_req(get_is_init());\n+  } else {\n+    sfpt->add_req(igvn->C->top());\n+  }\n+  \/\/ Iterate over the inline type fields in order of increasing\n+  \/\/ offset and add the field values to the safepoint.\n+  for (uint j = 0; j < nfields; ++j) {\n+    int offset = vk->nonstatic_field_at(j)->offset_in_bytes();\n+    Node* value = field_value_by_offset(offset, true \/* include flattened inline type fields *\/);\n+    if (value->is_InlineType()) {\n+      \/\/ Add inline type field to the worklist to process later\n+      worklist.push(value);\n+    }\n+    sfpt->add_req(value);\n+  }\n+  jvms->set_endoff(sfpt->req());\n+  sobj = igvn->transform(sobj)->as_SafePointScalarObject();\n+  igvn->rehash_node_delayed(sfpt);\n+  for (uint i = jvms->debug_start(); i < jvms->debug_end(); i++) {\n+    Node* debug = sfpt->in(i);\n+    if (debug != nullptr && debug->uncast() == this) {\n+      sfpt->set_req(i, sobj);\n+    }\n+  }\n+}\n+\n+void InlineTypeNode::make_scalar_in_safepoints(PhaseIterGVN* igvn, bool allow_oop) {\n+  \/\/ If the inline type has a constant or loaded oop, use the oop instead of scalarization\n+  \/\/ in the safepoint to avoid keeping field loads live just for the debug info.\n+  Node* oop = get_oop();\n+  bool use_oop = allow_oop && is_allocated(igvn) &&\n+                 (oop->is_Con() || oop->is_Parm() || oop->is_Load() || (oop->isa_DecodeN() && oop->in(1)->is_Load()));\n+\n+  ResourceMark rm;\n+  Unique_Node_List safepoints;\n+  Unique_Node_List vt_worklist;\n+  Unique_Node_List worklist;\n+  worklist.push(this);\n+  while (worklist.size() > 0) {\n+    Node* n = worklist.pop();\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* use = n->fast_out(i);\n+      if (use->is_SafePoint() && !use->is_CallLeaf() && (!use->is_Call() || use->as_Call()->has_debug_use(n))) {\n+        safepoints.push(use);\n+      } else if (use->is_ConstraintCast()) {\n+        worklist.push(use);\n+      }\n+    }\n+  }\n+\n+  \/\/ Process all safepoint uses and scalarize inline type\n+  while (safepoints.size() > 0) {\n+    SafePointNode* sfpt = safepoints.pop()->as_SafePoint();\n+    if (use_oop) {\n+      for (uint i = sfpt->jvms()->debug_start(); i < sfpt->jvms()->debug_end(); i++) {\n+        Node* debug = sfpt->in(i);\n+        if (debug != nullptr && debug->uncast() == this) {\n+          sfpt->set_req(i, get_oop());\n+        }\n+      }\n+      igvn->rehash_node_delayed(sfpt);\n+    } else {\n+      make_scalar_in_safepoint(igvn, vt_worklist, sfpt);\n+    }\n+  }\n+  \/\/ Now scalarize non-flattened fields\n+  for (uint i = 0; i < vt_worklist.size(); ++i) {\n+    InlineTypeNode* vt = vt_worklist.at(i)->isa_InlineType();\n+    vt->make_scalar_in_safepoints(igvn);\n+  }\n+  if (outcnt() == 0) {\n+    igvn->_worklist.push(this);\n+  }\n+}\n+\n+const TypePtr* InlineTypeNode::field_adr_type(Node* base, int offset, ciInstanceKlass* holder, DecoratorSet decorators, PhaseGVN& gvn) const {\n+  const TypeAryPtr* ary_type = gvn.type(base)->isa_aryptr();\n+  const TypePtr* adr_type = nullptr;\n+  bool is_array = ary_type != nullptr;\n+  if ((decorators & C2_MISMATCHED) != 0) {\n+    adr_type = TypeRawPtr::BOTTOM;\n+  } else if (is_array) {\n+    \/\/ In the case of a flattened inline type array, each field has its own slice\n+    adr_type = ary_type->with_field_offset(offset)->add_offset(Type::OffsetBot);\n+  } else {\n+    ciField* field = holder->get_field_by_offset(offset, false);\n+    assert(field != nullptr, \"field not found\");\n+    adr_type = gvn.C->alias_type(field)->adr_type();\n+  }\n+  return adr_type;\n+}\n+\n+\/\/ We limit scalarization for inline types with circular fields and can therefore observe\n+\/\/ nodes of same type but with different scalarization depth during GVN. This method adjusts\n+\/\/ the scalarization depth to avoid inconsistencies during merging.\n+InlineTypeNode* InlineTypeNode::adjust_scalarization_depth(GraphKit* kit) {\n+  if (!kit->C->has_circular_inline_type()) {\n+    return this;\n+  }\n+  GrowableArray<ciType*> visited;\n+  visited.push(inline_klass());\n+  return adjust_scalarization_depth_impl(kit, visited);\n+}\n+\n+InlineTypeNode* InlineTypeNode::adjust_scalarization_depth_impl(GraphKit* kit, GrowableArray<ciType*>& visited) {\n+  InlineTypeNode* val = this;\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* value = field_value(i);\n+    Node* new_value = value;\n+    ciType* ft = field_type(i);\n+    if (value->is_InlineType()) {\n+      if (!field_is_flattened(i) && visited.contains(ft)) {\n+        new_value = value->as_InlineType()->buffer(kit)->get_oop();\n+      } else {\n+        int old_len = visited.length();\n+        visited.push(ft);\n+        new_value = value->as_InlineType()->adjust_scalarization_depth_impl(kit, visited);\n+        visited.trunc_to(old_len);\n+      }\n+    } else if (ft->is_inlinetype() && !visited.contains(ft)) {\n+      int old_len = visited.length();\n+      visited.push(ft);\n+      new_value = make_from_oop_impl(kit, value, ft->as_inline_klass(), field_is_null_free(i), visited);\n+      visited.trunc_to(old_len);\n+    }\n+    if (value != new_value) {\n+      if (val == this) {\n+        val = clone()->as_InlineType();\n+      }\n+      val->set_field_value(i, new_value);\n+    }\n+  }\n+  return (val == this) ? this : kit->gvn().transform(val)->as_InlineType();\n+}\n+\n+void InlineTypeNode::load(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, GrowableArray<ciType*>& visited, int holder_offset, DecoratorSet decorators) {\n+  \/\/ Initialize the inline type by loading its field values from\n+  \/\/ memory and adding the values as input edges to the node.\n+  for (uint i = 0; i < field_count(); ++i) {\n+    int offset = holder_offset + field_offset(i);\n+    Node* value = nullptr;\n+    ciType* ft = field_type(i);\n+    bool null_free = field_is_null_free(i);\n+    if (null_free && ft->as_inline_klass()->is_empty()) {\n+      \/\/ Loading from a field of an empty inline type. Just return the default instance.\n+      value = make_default_impl(kit->gvn(), ft->as_inline_klass(), visited);\n+    } else if (field_is_flattened(i)) {\n+      \/\/ Recursively load the flattened inline type field\n+      value = make_from_flattened_impl(kit, ft->as_inline_klass(), base, ptr, holder, offset, decorators, visited);\n+    } else {\n+      const TypeOopPtr* oop_ptr = kit->gvn().type(base)->isa_oopptr();\n+      bool is_array = (oop_ptr->isa_aryptr() != nullptr);\n+      bool mismatched = (decorators & C2_MISMATCHED) != 0;\n+      if (base->is_Con() && !is_array && !mismatched) {\n+        \/\/ If the oop to the inline type is constant (static final field), we can\n+        \/\/ also treat the fields as constants because the inline type is immutable.\n+        ciObject* constant_oop = oop_ptr->const_oop();\n+        ciField* field = holder->get_field_by_offset(offset, false);\n+        assert(field != nullptr, \"field not found\");\n+        ciConstant constant = constant_oop->as_instance()->field_value(field);\n+        const Type* con_type = Type::make_from_constant(constant, \/*require_const=*\/ true);\n+        assert(con_type != nullptr, \"type not found\");\n+        value = kit->gvn().transform(kit->makecon(con_type));\n+        \/\/ Check type of constant which might be more precise than the static field type\n+        if (con_type->is_inlinetypeptr() && !con_type->is_zero_type()) {\n+          ft = con_type->inline_klass();\n+          null_free = true;\n+        }\n+      } else {\n+        \/\/ Load field value from memory\n+        const TypePtr* adr_type = field_adr_type(base, offset, holder, decorators, kit->gvn());\n+        Node* adr = kit->basic_plus_adr(base, ptr, offset);\n+        BasicType bt = type2field[ft->basic_type()];\n+        assert(is_java_primitive(bt) || adr->bottom_type()->is_ptr_to_narrowoop() == UseCompressedOops, \"inconsistent\");\n+        const Type* val_type = Type::get_const_type(ft);\n+        value = kit->access_load_at(base, adr, adr_type, val_type, bt, is_array ? (decorators | IS_ARRAY) : decorators);\n+      }\n+      \/\/ Loading a non-flattened inline type from memory\n+      if (visited.contains(ft)) {\n+        kit->C->set_has_circular_inline_type(true);\n+      } else if (ft->is_inlinetype()) {\n+        int old_len = visited.length();\n+        visited.push(ft);\n+        value = make_from_oop_impl(kit, value, ft->as_inline_klass(), null_free, visited);\n+        visited.trunc_to(old_len);\n+      }\n+    }\n+    set_field_value(i, value);\n+  }\n+}\n+\n+void InlineTypeNode::store_flattened(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, int holder_offset, DecoratorSet decorators) const {\n+  if (kit->gvn().type(base)->isa_aryptr()) {\n+    kit->C->set_flattened_accesses();\n+  }\n+  \/\/ The inline type is embedded into the object without an oop header. Subtract the\n+  \/\/ offset of the first field to account for the missing header when storing the values.\n+  if (holder == nullptr) {\n+    holder = inline_klass();\n+  }\n+  holder_offset -= inline_klass()->first_field_offset();\n+  store(kit, base, ptr, holder, holder_offset, decorators);\n+}\n+\n+void InlineTypeNode::store(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, int holder_offset, DecoratorSet decorators) const {\n+  \/\/ Write field values to memory\n+  for (uint i = 0; i < field_count(); ++i) {\n+    int offset = holder_offset + field_offset(i);\n+    Node* value = field_value(i);\n+    ciType* ft = field_type(i);\n+    if (field_is_flattened(i)) {\n+      \/\/ Recursively store the flattened inline type field\n+      value->as_InlineType()->store_flattened(kit, base, ptr, holder, offset, decorators);\n+    } else {\n+      \/\/ Store field value to memory\n+      const TypePtr* adr_type = field_adr_type(base, offset, holder, decorators, kit->gvn());\n+      Node* adr = kit->basic_plus_adr(base, ptr, offset);\n+      BasicType bt = type2field[ft->basic_type()];\n+      assert(is_java_primitive(bt) || adr->bottom_type()->is_ptr_to_narrowoop() == UseCompressedOops, \"inconsistent\");\n+      const Type* val_type = Type::get_const_type(ft);\n+      bool is_array = (kit->gvn().type(base)->isa_aryptr() != nullptr);\n+      kit->access_store_at(base, adr, adr_type, value, val_type, bt, is_array ? (decorators | IS_ARRAY) : decorators);\n+    }\n+  }\n+}\n+\n+InlineTypeNode* InlineTypeNode::buffer(GraphKit* kit, bool safe_for_replace) {\n+  if (kit->gvn().find_int_con(get_is_buffered(), 0) == 1) {\n+    \/\/ Already buffered\n+    return this;\n+  }\n+\n+  \/\/ Check if inline type is already buffered\n+  Node* not_buffered_ctl = kit->top();\n+  Node* not_null_oop = kit->null_check_oop(get_oop(), &not_buffered_ctl, \/* never_see_null = *\/ false, safe_for_replace);\n+  if (not_buffered_ctl->is_top()) {\n+    \/\/ Already buffered\n+    InlineTypeNode* vt = clone()->as_InlineType();\n+    vt->set_is_buffered(kit->gvn());\n+    vt = kit->gvn().transform(vt)->as_InlineType();\n+    if (safe_for_replace) {\n+      kit->replace_in_map(this, vt);\n+    }\n+    return vt;\n+  }\n+  Node* buffered_ctl = kit->control();\n+  kit->set_control(not_buffered_ctl);\n+\n+  \/\/ Inline type is not buffered, check if it is null.\n+  Node* null_ctl = kit->top();\n+  kit->null_check_common(get_is_init(), T_INT, false, &null_ctl);\n+  bool null_free = null_ctl->is_top();\n+\n+  RegionNode* region = new RegionNode(4);\n+  PhiNode* oop = PhiNode::make(region, not_null_oop, type()->join_speculative(null_free ? TypePtr::NOTNULL : TypePtr::BOTTOM));\n+\n+  \/\/ InlineType is already buffered\n+  region->init_req(1, buffered_ctl);\n+  oop->init_req(1, not_null_oop);\n+\n+  \/\/ InlineType is null\n+  region->init_req(2, null_ctl);\n+  oop->init_req(2, kit->gvn().zerocon(T_OBJECT));\n+\n+  PhiNode* io  = PhiNode::make(region, kit->i_o(), Type::ABIO);\n+  PhiNode* mem = PhiNode::make(region, kit->merged_memory(), Type::MEMORY, TypePtr::BOTTOM);\n+\n+  int bci = kit->bci();\n+  bool reexecute = kit->jvms()->should_reexecute();\n+  if (!kit->stopped()) {\n+    assert(!is_allocated(&kit->gvn()), \"already buffered\");\n+\n+    \/\/ Allocate and initialize buffer\n+    PreserveJVMState pjvms(kit);\n+    \/\/ Propagate re-execution state and bci\n+    kit->set_bci(bci);\n+    kit->jvms()->set_bci(bci);\n+    kit->jvms()->set_should_reexecute(reexecute);\n+\n+    kit->kill_dead_locals();\n+    ciInlineKlass* vk = inline_klass();\n+    Node* klass_node = kit->makecon(TypeKlassPtr::make(vk));\n+    Node* alloc_oop  = kit->new_instance(klass_node, nullptr, nullptr, \/* deoptimize_on_exception *\/ true, this);\n+    store(kit, alloc_oop, alloc_oop, vk);\n+\n+    \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+    \/\/ store that would make this buffer accessible by other threads.\n+    AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_oop);\n+    assert(alloc != nullptr, \"must have an allocation node\");\n+    kit->insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+\n+    region->init_req(3, kit->control());\n+    oop   ->init_req(3, alloc_oop);\n+    io    ->init_req(3, kit->i_o());\n+    mem   ->init_req(3, kit->merged_memory());\n+  }\n+\n+  \/\/ Update GraphKit\n+  kit->set_control(kit->gvn().transform(region));\n+  kit->set_i_o(kit->gvn().transform(io));\n+  kit->set_all_memory(kit->gvn().transform(mem));\n+  kit->record_for_igvn(region);\n+  kit->record_for_igvn(oop);\n+  kit->record_for_igvn(io);\n+  kit->record_for_igvn(mem);\n+\n+  \/\/ Use cloned InlineTypeNode to propagate oop from now on\n+  Node* res_oop = kit->gvn().transform(oop);\n+  InlineTypeNode* vt = clone()->as_InlineType();\n+  vt->set_oop(res_oop);\n+  vt->set_is_buffered(kit->gvn());\n+  vt = kit->gvn().transform(vt)->as_InlineType();\n+  if (safe_for_replace) {\n+    kit->replace_in_map(this, vt);\n+  }\n+  \/\/ InlineTypeNode::remove_redundant_allocations piggybacks on split if.\n+  \/\/ Make sure it gets a chance to remove this allocation.\n+  kit->C->set_has_split_ifs(true);\n+  return vt;\n+}\n+\n+bool InlineTypeNode::is_allocated(PhaseGVN* phase) const {\n+  if (phase->find_int_con(get_is_buffered(), 0) == 1) {\n+    return true;\n+  }\n+  Node* oop = get_oop();\n+  const Type* oop_type = (phase != nullptr) ? phase->type(oop) : oop->bottom_type();\n+  return !oop_type->maybe_null();\n+}\n+\n+\/\/ When a call returns multiple values, it has several result\n+\/\/ projections, one per field. Replacing the result of the call by an\n+\/\/ inline type node (after late inlining) requires that for each result\n+\/\/ projection, we find the corresponding inline type field.\n+void InlineTypeNode::replace_call_results(GraphKit* kit, CallNode* call, Compile* C, bool null_free) {\n+  ciInlineKlass* vk = inline_klass();\n+  for (DUIterator_Fast imax, i = call->fast_outs(imax); i < imax; i++) {\n+    ProjNode* pn = call->fast_out(i)->as_Proj();\n+    uint con = pn->_con;\n+    Node* field = nullptr;\n+    if (con == TypeFunc::Parms) {\n+      field = get_oop();\n+    } else if (!null_free && con == (call->tf()->range_cc()->cnt() - 1)) {\n+      field = get_is_init();\n+    } else if (con > TypeFunc::Parms) {\n+      uint field_nb = con - (TypeFunc::Parms+1);\n+      int extra = 0;\n+      for (uint j = 0; j < field_nb - extra; j++) {\n+        ciField* f = vk->nonstatic_field_at(j);\n+        BasicType bt = f->type()->basic_type();\n+        if (bt == T_LONG || bt == T_DOUBLE) {\n+          extra++;\n+        }\n+      }\n+      ciField* f = vk->nonstatic_field_at(field_nb - extra);\n+      field = field_value_by_offset(f->offset_in_bytes(), true);\n+    }\n+    if (field != nullptr) {\n+      C->gvn_replace_by(pn, field);\n+      C->initial_gvn()->hash_delete(pn);\n+      pn->set_req(0, C->top());\n+      --i; --imax;\n+    }\n+  }\n+}\n+\n+Node* InlineTypeNode::allocate_fields(GraphKit* kit) {\n+  InlineTypeNode* vt = clone()->as_InlineType();\n+  for (uint i = 0; i < field_count(); i++) {\n+     Node* value = field_value(i);\n+     if (field_is_flattened(i)) {\n+       \/\/ Flattened inline type field\n+       vt->set_field_value(i, value->as_InlineType()->allocate_fields(kit));\n+     } else if (value->is_InlineType()) {\n+       \/\/ Non-flattened inline type field\n+       vt->set_field_value(i, value->as_InlineType()->buffer(kit));\n+     }\n+  }\n+  vt = kit->gvn().transform(vt)->as_InlineType();\n+  kit->replace_in_map(this, vt);\n+  return vt;\n+}\n+\n+\/\/ Replace a buffer allocation by a dominating allocation\n+static void replace_allocation(PhaseIterGVN* igvn, Node* res, Node* dom) {\n+  \/\/ Remove initializing stores and GC barriers\n+  for (DUIterator_Fast imax, i = res->fast_outs(imax); i < imax; i++) {\n+    Node* use = res->fast_out(i);\n+    if (use->is_AddP()) {\n+      for (DUIterator_Fast jmax, j = use->fast_outs(jmax); j < jmax; j++) {\n+        Node* store = use->fast_out(j)->isa_Store();\n+        if (store != nullptr) {\n+          igvn->rehash_node_delayed(store);\n+          igvn->replace_in_uses(store, store->in(MemNode::Memory));\n+        }\n+      }\n+    } else if (use->Opcode() == Op_CastP2X) {\n+      if (UseG1GC && use->find_out_with(Op_XorX)->in(1) != use) {\n+        \/\/ The G1 pre-barrier uses a CastP2X both for the pointer of the object\n+        \/\/ we store into, as well as the value we are storing. Skip if this is a\n+        \/\/ barrier for storing 'res' into another object.\n+        continue;\n+      }\n+      BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+      bs->eliminate_gc_barrier(igvn, use);\n+      --i; --imax;\n+    }\n+  }\n+  igvn->replace_node(res, dom);\n+}\n+\n+Node* InlineTypeNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* oop = get_oop();\n+  if (!is_larval(phase) &&\n+      is_default(phase) &&\n+      inline_klass()->is_initialized() &&\n+      (!oop->is_Con() || phase->type(oop)->is_zero_type())) {\n+    \/\/ Use the pre-allocated oop for default inline types\n+    set_oop(default_oop(*phase, inline_klass()));\n+    assert(is_allocated(phase), \"should now be allocated\");\n+    return this;\n+  }\n+  if (oop->isa_InlineType() && !phase->type(oop)->maybe_null()) {\n+    InlineTypeNode* vtptr = oop->as_InlineType();\n+    set_oop(vtptr->get_oop());\n+    set_is_buffered(*phase);\n+    set_is_init(*phase);\n+    for (uint i = Values; i < vtptr->req(); ++i) {\n+      set_req(i, vtptr->in(i));\n+    }\n+    return this;\n+  }\n+  if (!is_allocated(phase)) {\n+    \/\/ Save base oop if fields are loaded from memory and the inline\n+    \/\/ type is not buffered (in this case we should not use the oop).\n+    Node* base = is_loaded(phase);\n+    if (base != nullptr && !phase->type(base)->maybe_null()) {\n+      set_oop(base);\n+      assert(is_allocated(phase), \"should now be allocated\");\n+      return this;\n+    }\n+  }\n+\n+  if (can_reshape) {\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+    if (is_allocated(phase)) {\n+      \/\/ Search for and remove re-allocations of this inline type. Ignore scalar replaceable ones,\n+      \/\/ they will be removed anyway and changing the memory chain will confuse other optimizations.\n+      \/\/ This can happen with late inlining when we first allocate an inline type argument\n+      \/\/ but later decide to inline the call after the callee code also triggered allocation.\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        AllocateNode* alloc = fast_out(i)->isa_Allocate();\n+        if (alloc != nullptr && alloc->in(AllocateNode::InlineType) == this && !alloc->_is_scalar_replaceable) {\n+          \/\/ Found a re-allocation\n+          Node* res = alloc->result_cast();\n+          if (res != nullptr && res->is_CheckCastPP()) {\n+            \/\/ Replace allocation by oop and unlink AllocateNode\n+            replace_allocation(igvn, res, oop);\n+            igvn->replace_input_of(alloc, AllocateNode::InlineType, igvn->C->top());\n+            --i; --imax;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_uninitialized(PhaseGVN& gvn, ciInlineKlass* vk, bool null_free) {\n+  \/\/ Create a new InlineTypeNode with uninitialized values and nullptr oop\n+  Node* oop = (vk->is_empty() && vk->is_initialized()) ? default_oop(gvn, vk) : gvn.zerocon(T_PRIMITIVE_OBJECT);\n+  InlineTypeNode* vt = new InlineTypeNode(vk, oop, null_free);\n+  vt->set_is_buffered(gvn, vk->is_empty() && vk->is_initialized());\n+  vt->set_is_init(gvn);\n+  return vt;\n+}\n+\n+Node* InlineTypeNode::default_oop(PhaseGVN& gvn, ciInlineKlass* vk) {\n+  \/\/ Returns the constant oop of the default inline type allocation\n+  return gvn.makecon(TypeInstPtr::make(vk->default_instance()));\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_default(PhaseGVN& gvn, ciInlineKlass* vk) {\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_default_impl(gvn, vk, visited);\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_default_impl(PhaseGVN& gvn, ciInlineKlass* vk, GrowableArray<ciType*>& visited) {\n+  \/\/ Create a new InlineTypeNode with default values\n+  Node* oop = vk->is_initialized() ? default_oop(gvn, vk) : gvn.zerocon(T_PRIMITIVE_OBJECT);\n+  InlineTypeNode* vt = new InlineTypeNode(vk, oop, \/* null_free= *\/ true);\n+  vt->set_is_buffered(gvn, vk->is_initialized());\n+  vt->set_is_init(gvn);\n+  for (uint i = 0; i < vt->field_count(); ++i) {\n+    ciType* ft = vt->field_type(i);\n+    Node* value = gvn.zerocon(ft->basic_type());\n+    if (!vt->field_is_flattened(i) && visited.contains(ft)) {\n+      gvn.C->set_has_circular_inline_type(true);\n+    } else if (ft->is_inlinetype()) {\n+      int old_len = visited.length();\n+      visited.push(ft);\n+      ciInlineKlass* vk = ft->as_inline_klass();\n+      if (vt->field_is_null_free(i)) {\n+        value = make_default_impl(gvn, vk, visited);\n+      } else {\n+        value = make_null_impl(gvn, vk, visited);\n+      }\n+      visited.trunc_to(old_len);\n+    }\n+    vt->set_field_value(i, value);\n+  }\n+  vt = gvn.transform(vt)->as_InlineType();\n+  assert(vt->is_default(&gvn), \"must be the default inline type\");\n+  return vt;\n+}\n+\n+bool InlineTypeNode::is_default(PhaseGVN* gvn) const {\n+  const Type* tinit = gvn->type(in(IsInit));\n+  if (!tinit->isa_int() || !tinit->is_int()->is_con(1)) {\n+    return false; \/\/ May be null\n+  }\n+  for (uint i = 0; i < field_count(); ++i) {\n+    ciType* ft = field_type(i);\n+    Node* value = field_value(i);\n+    if (field_is_null_free(i)) {\n+      if (!value->is_InlineType() || !value->as_InlineType()->is_default(gvn)) {\n+        return false;\n+      }\n+      continue;\n+    } else if (value->is_InlineType()) {\n+      value = value->as_InlineType()->get_oop();\n+    }\n+    if (!gvn->type(value)->is_zero_type()) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_from_oop(GraphKit* kit, Node* oop, ciInlineKlass* vk, bool null_free) {\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_from_oop_impl(kit, oop, vk, null_free, visited);\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_from_oop_impl(GraphKit* kit, Node* oop, ciInlineKlass* vk, bool null_free, GrowableArray<ciType*>& visited) {\n+  PhaseGVN& gvn = kit->gvn();\n+\n+  if (vk->is_empty() && null_free) {\n+    InlineTypeNode* def = make_default_impl(gvn, vk, visited);\n+    kit->record_for_igvn(def);\n+    return def;\n+  }\n+  \/\/ Create and initialize an InlineTypeNode by loading all field\n+  \/\/ values from a heap-allocated version and also save the oop.\n+  InlineTypeNode* vt = nullptr;\n+\n+  if (oop->isa_InlineType()) {\n+    return oop->as_InlineType();\n+  } else if (gvn.type(oop)->maybe_null()) {\n+    \/\/ Add a null check because the oop may be null\n+    Node* null_ctl = kit->top();\n+    Node* not_null_oop = kit->null_check_oop(oop, &null_ctl);\n+    if (kit->stopped()) {\n+      \/\/ Constant null\n+      kit->set_control(null_ctl);\n+      if (null_free) {\n+        vt = make_default_impl(gvn, vk, visited);\n+      } else {\n+        vt = make_null_impl(gvn, vk, visited);\n+      }\n+      kit->record_for_igvn(vt);\n+      return vt;\n+    }\n+    vt = new InlineTypeNode(vk, not_null_oop, null_free);\n+    vt->set_is_buffered(gvn);\n+    vt->set_is_init(gvn);\n+    vt->load(kit, not_null_oop, not_null_oop, vk, visited);\n+\n+    if (null_ctl != kit->top()) {\n+      InlineTypeNode* null_vt = nullptr;\n+      if (null_free) {\n+        null_vt = make_default_impl(gvn, vk, visited);\n+      } else {\n+        null_vt = make_null_impl(gvn, vk, visited);\n+      }\n+      Node* region = new RegionNode(3);\n+      region->init_req(1, kit->control());\n+      region->init_req(2, null_ctl);\n+\n+      vt = vt->clone_with_phis(&gvn, region);\n+      vt->merge_with(&gvn, null_vt, 2, true);\n+      if (!null_free) {\n+        vt->set_oop(oop);\n+      }\n+      kit->set_control(gvn.transform(region));\n+    }\n+  } else {\n+    \/\/ Oop can never be null\n+    vt = new InlineTypeNode(vk, oop, \/* null_free= *\/ true);\n+    Node* init_ctl = kit->control();\n+    vt->set_is_buffered(gvn);\n+    vt->set_is_init(gvn);\n+    vt->load(kit, oop, oop, vk, visited);\n+\/\/ TODO 8284443\n+\/\/    assert(!null_free || vt->as_InlineType()->is_default(&gvn) || init_ctl != kit->control() || !gvn.type(oop)->is_inlinetypeptr() || oop->is_Con() || oop->Opcode() == Op_InlineType ||\n+\/\/           AllocateNode::Ideal_allocation(oop, &gvn) != nullptr || vt->as_InlineType()->is_loaded(&gvn) == oop, \"inline type should be loaded\");\n+  }\n+  assert(!null_free || vt->is_allocated(&gvn), \"inline type should be allocated\");\n+  kit->record_for_igvn(vt);\n+  return gvn.transform(vt)->as_InlineType();\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_from_flattened(GraphKit* kit, ciInlineKlass* vk, Node* obj, Node* ptr, ciInstanceKlass* holder, int holder_offset, DecoratorSet decorators) {\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_from_flattened_impl(kit, vk, obj, ptr, holder, holder_offset, decorators, visited);\n+}\n+\n+\/\/ GraphKit wrapper for the 'make_from_flattened' method\n+InlineTypeNode* InlineTypeNode::make_from_flattened_impl(GraphKit* kit, ciInlineKlass* vk, Node* obj, Node* ptr, ciInstanceKlass* holder, int holder_offset, DecoratorSet decorators, GrowableArray<ciType*>& visited) {\n+  if (kit->gvn().type(obj)->isa_aryptr()) {\n+    kit->C->set_flattened_accesses();\n+  }\n+  \/\/ Create and initialize an InlineTypeNode by loading all field values from\n+  \/\/ a flattened inline type field at 'holder_offset' or from an inline type array.\n+  InlineTypeNode* vt = make_uninitialized(kit->gvn(), vk);\n+  \/\/ The inline type is flattened into the object without an oop header. Subtract the\n+  \/\/ offset of the first field to account for the missing header when loading the values.\n+  holder_offset -= vk->first_field_offset();\n+  vt->load(kit, obj, ptr, holder, visited, holder_offset, decorators);\n+  assert(vt->is_loaded(&kit->gvn()) != obj, \"holder oop should not be used as flattened inline type oop\");\n+  return kit->gvn().transform(vt)->as_InlineType();\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_from_multi(GraphKit* kit, MultiNode* multi, ciInlineKlass* vk, uint& base_input, bool in, bool null_free) {\n+  InlineTypeNode* vt = make_uninitialized(kit->gvn(), vk, null_free);\n+  if (!in) {\n+    \/\/ Keep track of the oop. The returned inline type might already be buffered.\n+    Node* oop = kit->gvn().transform(new ProjNode(multi, base_input++));\n+    vt->set_oop(oop);\n+  }\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  vt->initialize_fields(kit, multi, base_input, in, null_free, nullptr, visited);\n+  return kit->gvn().transform(vt)->as_InlineType();\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_larval(GraphKit* kit, bool allocate) const {\n+  ciInlineKlass* vk = inline_klass();\n+  InlineTypeNode* res = make_uninitialized(kit->gvn(), vk);\n+  for (uint i = 1; i < req(); ++i) {\n+    res->set_req(i, in(i));\n+  }\n+\n+  if (allocate) {\n+    \/\/ Re-execute if buffering triggers deoptimization\n+    PreserveReexecuteState preexecs(kit);\n+    kit->jvms()->set_should_reexecute(true);\n+    Node* klass_node = kit->makecon(TypeKlassPtr::make(vk));\n+    Node* alloc_oop  = kit->new_instance(klass_node, nullptr, nullptr, true);\n+    AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_oop);\n+    alloc->_larval = true;\n+\n+    store(kit, alloc_oop, alloc_oop, vk);\n+    res->set_oop(alloc_oop);\n+  }\n+  \/\/ TODO 8239003\n+  \/\/res->set_type(TypeInlineType::make(vk, true));\n+  res = kit->gvn().transform(res)->as_InlineType();\n+  assert(!allocate || res->is_allocated(&kit->gvn()), \"must be allocated\");\n+  return res;\n+}\n+\n+InlineTypeNode* InlineTypeNode::finish_larval(GraphKit* kit) const {\n+  Node* obj = get_oop();\n+  Node* mark_addr = kit->basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+  Node* mark = kit->make_load(nullptr, mark_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  mark = kit->gvn().transform(new AndXNode(mark, kit->MakeConX(~markWord::larval_bit_in_place)));\n+  kit->store_to_memory(kit->control(), mark_addr, mark, TypeX_X->basic_type(), kit->gvn().type(mark_addr)->is_ptr(), MemNode::unordered);\n+\n+  \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+  \/\/ store that would make this buffer accessible by other threads.\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(obj);\n+  assert(alloc != nullptr, \"must have an allocation node\");\n+  kit->insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+\n+  ciInlineKlass* vk = inline_klass();\n+  InlineTypeNode* res = make_uninitialized(kit->gvn(), vk);\n+  for (uint i = 1; i < req(); ++i) {\n+    res->set_req(i, in(i));\n+  }\n+  \/\/ TODO 8239003\n+  \/\/res->set_type(TypeInlineType::make(vk, false));\n+  res = kit->gvn().transform(res)->as_InlineType();\n+  return res;\n+}\n+\n+bool InlineTypeNode::is_larval(PhaseGVN* gvn) const {\n+  if (!is_allocated(gvn)) {\n+    return false;\n+  }\n+\n+  Node* oop = get_oop();\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(oop);\n+  return alloc != nullptr && alloc->_larval;\n+}\n+\n+Node* InlineTypeNode::is_loaded(PhaseGVN* phase, ciInlineKlass* vk, Node* base, int holder_offset) {\n+  if (vk == nullptr) {\n+    vk = inline_klass();\n+  }\n+  if (field_count() == 0 && vk->is_initialized()) {\n+    const Type* tinit = phase->type(in(IsInit));\n+    if (tinit->isa_int() && tinit->is_int()->is_con(1)) {\n+      assert(is_allocated(phase), \"must be allocated\");\n+      return get_oop();\n+    } else {\n+      \/\/ TODO 8284443\n+      return nullptr;\n+    }\n+  }\n+  for (uint i = 0; i < field_count(); ++i) {\n+    int offset = holder_offset + field_offset(i);\n+    Node* value = field_value(i);\n+    if (value->is_InlineType()) {\n+      InlineTypeNode* vt = value->as_InlineType();\n+      if (vt->type()->inline_klass()->is_empty()) {\n+        continue;\n+      } else if (field_is_flattened(i) && vt->is_InlineType()) {\n+        \/\/ Check inline type field load recursively\n+        base = vt->as_InlineType()->is_loaded(phase, vk, base, offset - vt->type()->inline_klass()->first_field_offset());\n+        if (base == nullptr) {\n+          return nullptr;\n+        }\n+        continue;\n+      } else {\n+        value = vt->get_oop();\n+        if (value->Opcode() == Op_CastPP) {\n+          \/\/ Skip CastPP\n+          value = value->in(1);\n+        }\n+      }\n+    }\n+    if (value->isa_DecodeN()) {\n+      \/\/ Skip DecodeN\n+      value = value->in(1);\n+    }\n+    if (value->isa_Load()) {\n+      \/\/ Check if base and offset of field load matches inline type layout\n+      intptr_t loffset = 0;\n+      Node* lbase = AddPNode::Ideal_base_and_offset(value->in(MemNode::Address), phase, loffset);\n+      if (lbase == nullptr || (lbase != base && base != nullptr) || loffset != offset) {\n+        return nullptr;\n+      } else if (base == nullptr) {\n+        \/\/ Set base and check if pointer type matches\n+        base = lbase;\n+        const TypeInstPtr* vtptr = phase->type(base)->isa_instptr();\n+        if (vtptr == nullptr || !vtptr->instance_klass()->equals(vk)) {\n+          return nullptr;\n+        }\n+      }\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+  return base;\n+}\n+\n+Node* InlineTypeNode::tagged_klass(ciInlineKlass* vk, PhaseGVN& gvn) {\n+  const TypeKlassPtr* tk = TypeKlassPtr::make(vk);\n+  intptr_t bits = tk->get_con();\n+  set_nth_bit(bits, 0);\n+  return gvn.longcon((jlong)bits);\n+}\n+\n+void InlineTypeNode::pass_fields(GraphKit* kit, Node* n, uint& base_input, bool in, bool null_free) {\n+  if (!null_free && in) {\n+    n->init_req(base_input++, get_is_init());\n+  }\n+  for (uint i = 0; i < field_count(); i++) {\n+    Node* arg = field_value(i);\n+    if (field_is_flattened(i)) {\n+      \/\/ Flattened inline type field\n+      arg->as_InlineType()->pass_fields(kit, n, base_input, in);\n+    } else {\n+      if (arg->is_InlineType()) {\n+        \/\/ Non-flattened inline type field\n+        InlineTypeNode* vt = arg->as_InlineType();\n+        assert(n->Opcode() != Op_Return || vt->is_allocated(&kit->gvn()), \"inline type field should be allocated on return\");\n+        arg = vt->buffer(kit);\n+      }\n+      \/\/ Initialize call\/return arguments\n+      n->init_req(base_input++, arg);\n+      if (field_type(i)->size() == 2) {\n+        n->init_req(base_input++, kit->top());\n+      }\n+    }\n+  }\n+  \/\/ The last argument is used to pass IsInit information to compiled code and not required here.\n+  if (!null_free && !in) {\n+    n->init_req(base_input++, kit->top());\n+  }\n+}\n+\n+void InlineTypeNode::initialize_fields(GraphKit* kit, MultiNode* multi, uint& base_input, bool in, bool null_free, Node* null_check_region, GrowableArray<ciType*>& visited) {\n+  PhaseGVN& gvn = kit->gvn();\n+  Node* is_init = nullptr;\n+  if (!null_free) {\n+    \/\/ Nullable inline type\n+    if (in) {\n+      \/\/ Set IsInit field\n+      if (multi->is_Start()) {\n+        is_init = gvn.transform(new ParmNode(multi->as_Start(), base_input));\n+      } else {\n+        is_init = multi->as_Call()->in(base_input);\n+      }\n+      set_req(IsInit, is_init);\n+      base_input++;\n+    }\n+    \/\/ Add a null check to make subsequent loads dependent on\n+    assert(null_check_region == nullptr, \"already set\");\n+    if (is_init == nullptr) {\n+      \/\/ Will only be initialized below, use dummy node for now\n+      is_init = new Node(1);\n+      gvn.set_type_bottom(is_init);\n+    }\n+    Node* null_ctrl = kit->top();\n+    kit->null_check_common(is_init, T_INT, false, &null_ctrl);\n+    Node* non_null_ctrl = kit->control();\n+    null_check_region = new RegionNode(3);\n+    null_check_region->init_req(1, non_null_ctrl);\n+    null_check_region->init_req(2, null_ctrl);\n+    null_check_region = gvn.transform(null_check_region);\n+    kit->set_control(null_check_region);\n+  }\n+\n+  for (uint i = 0; i < field_count(); ++i) {\n+    ciType* type = field_type(i);\n+    Node* parm = nullptr;\n+    if (field_is_flattened(i)) {\n+      \/\/ Flattened inline type field\n+      InlineTypeNode* vt = make_uninitialized(gvn, type->as_inline_klass());\n+      vt->initialize_fields(kit, multi, base_input, in, true, null_check_region, visited);\n+      parm = gvn.transform(vt);\n+    } else {\n+      if (multi->is_Start()) {\n+        assert(in, \"return from start?\");\n+        parm = gvn.transform(new ParmNode(multi->as_Start(), base_input));\n+      } else if (in) {\n+        parm = multi->as_Call()->in(base_input);\n+      } else {\n+        parm = gvn.transform(new ProjNode(multi->as_Call(), base_input));\n+      }\n+      \/\/ Non-flattened inline type field\n+      if (type->is_inlinetype()) {\n+        if (null_check_region != nullptr) {\n+          if (parm->is_InlineType() && kit->C->has_circular_inline_type()) {\n+            parm = parm->as_InlineType()->get_oop();\n+          }\n+          \/\/ Holder is nullable, set field to nullptr if holder is nullptr to avoid loading from uninitialized memory\n+          parm = PhiNode::make(null_check_region, parm, TypeInstPtr::make(TypePtr::BotPTR, type->as_inline_klass()));\n+          parm->set_req(2, kit->zerocon(T_OBJECT));\n+          parm = gvn.transform(parm);\n+        }\n+        if (visited.contains(type)) {\n+          kit->C->set_has_circular_inline_type(true);\n+        } else if (!parm->is_InlineType()) {\n+          int old_len = visited.length();\n+          visited.push(type);\n+          parm = make_from_oop_impl(kit, parm, type->as_inline_klass(), field_is_null_free(i), visited);\n+          visited.trunc_to(old_len);\n+        }\n+      }\n+      base_input += type->size();\n+    }\n+    assert(parm != nullptr, \"should never be null\");\n+    assert(field_value(i) == nullptr, \"already set\");\n+    set_field_value(i, parm);\n+    gvn.record_for_igvn(parm);\n+  }\n+  \/\/ The last argument is used to pass IsInit information to compiled code\n+  if (!null_free && !in) {\n+    Node* cmp = is_init->raw_out(0);\n+    is_init = gvn.transform(new ProjNode(multi->as_Call(), base_input));\n+    set_req(IsInit, is_init);\n+    gvn.hash_delete(cmp);\n+    cmp->set_req(1, is_init);\n+    gvn.hash_find_insert(cmp);\n+    base_input++;\n+  }\n+}\n+\n+\/\/ Search for multiple allocations of this inline type and try to replace them by dominating allocations.\n+\/\/ Equivalent InlineTypeNodes are merged by GVN, so we just need to search for AllocateNode users to find redundant allocations.\n+void InlineTypeNode::remove_redundant_allocations(PhaseIdealLoop* phase) {\n+  PhaseIterGVN* igvn = &phase->igvn();\n+  \/\/ Search for allocations of this inline type. Ignore scalar replaceable ones, they\n+  \/\/ will be removed anyway and changing the memory chain will confuse other optimizations.\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    AllocateNode* alloc = fast_out(i)->isa_Allocate();\n+    if (alloc != nullptr && alloc->in(AllocateNode::InlineType) == this && !alloc->_is_scalar_replaceable) {\n+      Node* res = alloc->result_cast();\n+      if (res == nullptr || !res->is_CheckCastPP()) {\n+        break; \/\/ No unique CheckCastPP\n+      }\n+      assert((!is_default(igvn) || !inline_klass()->is_initialized()) && !is_allocated(igvn), \"re-allocation should be removed by Ideal transformation\");\n+      \/\/ Search for a dominating allocation of the same inline type\n+      Node* res_dom = res;\n+      for (DUIterator_Fast jmax, j = fast_outs(jmax); j < jmax; j++) {\n+        AllocateNode* alloc_other = fast_out(j)->isa_Allocate();\n+        if (alloc_other != nullptr && alloc_other->in(AllocateNode::InlineType) == this && !alloc_other->_is_scalar_replaceable) {\n+          Node* res_other = alloc_other->result_cast();\n+          if (res_other != nullptr && res_other->is_CheckCastPP() && res_other != res_dom &&\n+              phase->is_dominator(res_other->in(0), res_dom->in(0))) {\n+            res_dom = res_other;\n+          }\n+        }\n+      }\n+      if (res_dom != res) {\n+        \/\/ Replace allocation by dominating one.\n+        replace_allocation(igvn, res, res_dom);\n+        \/\/ The result of the dominated allocation is now unused and will be removed\n+        \/\/ later in PhaseMacroExpand::eliminate_allocate_node to not confuse loop opts.\n+        igvn->_worklist.push(alloc);\n+      }\n+    }\n+  }\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_null(PhaseGVN& gvn, ciInlineKlass* vk) {\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_null_impl(gvn, vk, visited);\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_null_impl(PhaseGVN& gvn, ciInlineKlass* vk, GrowableArray<ciType*>& visited) {\n+  InlineTypeNode* vt = new InlineTypeNode(vk, gvn.zerocon(T_OBJECT), \/* null_free= *\/ false);\n+  vt->set_is_buffered(gvn);\n+  vt->set_is_init(gvn, false);\n+  for (uint i = 0; i < vt->field_count(); i++) {\n+    ciType* ft = vt->field_type(i);\n+    Node* value = gvn.zerocon(ft->basic_type());\n+    if (!vt->field_is_flattened(i) && visited.contains(ft)) {\n+      gvn.C->set_has_circular_inline_type(true);\n+    } else if (ft->is_inlinetype()) {\n+      int old_len = visited.length();\n+      visited.push(ft);\n+      value = make_null_impl(gvn, ft->as_inline_klass(), visited);\n+      visited.trunc_to(old_len);\n+    }\n+    vt->set_field_value(i, value);\n+  }\n+  return gvn.transform(vt)->as_InlineType();\n+}\n+\n+Node* InlineTypeNode::Identity(PhaseGVN* phase) {\n+  if (get_oop()->is_InlineType()) {\n+    return get_oop();\n+  }\n+  return this;\n+}\n+\n+const Type* InlineTypeNode::Value(PhaseGVN* phase) const {\n+  Node* oop = get_oop();\n+  const Type* toop = phase->type(oop);\n+#ifdef ASSERT\n+  if (oop->is_Con() && toop->is_zero_type() && _type->isa_oopptr()->is_known_instance()) {\n+    \/\/ We are not allocated (anymore) and should therefore not have an instance id\n+    dump(1);\n+    assert(false, \"Unbuffered inline type should not have known instance id\");\n+  }\n+#endif\n+  const Type* t = toop->filter_speculative(_type);\n+  if (t->singleton()) {\n+    \/\/ Don't replace InlineType by a constant\n+    t = _type;\n+  }\n+  const Type* tinit = phase->type(in(IsInit));\n+  if (tinit == Type::TOP) {\n+    return Type::TOP;\n+  }\n+  if (tinit->isa_int() && tinit->is_int()->is_con(1)) {\n+    t = t->join_speculative(TypePtr::NOTNULL);\n+  }\n+  return t;\n+}\n","filename":"src\/hotspot\/share\/opto\/inlinetypenode.cpp","additions":1257,"deletions":0,"binary":false,"changes":1257,"status":"added"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -324,0 +325,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -333,0 +336,1 @@\n+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_PRIMITIVE_OBJECT,Relaxed, false);\n@@ -343,0 +347,1 @@\n+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_PRIMITIVE_OBJECT,Relaxed, false);\n@@ -529,0 +534,5 @@\n+  case vmIntrinsics::_asPrimaryType:\n+  case vmIntrinsics::_asPrimaryTypeArg:\n+  case vmIntrinsics::_asValueType:\n+  case vmIntrinsics::_asValueTypeArg:           return inline_primitive_Class_conversion(intrinsic_id());\n+\n@@ -2024,1 +2034,1 @@\n-    } else if (type == T_OBJECT) {\n+    } else if (type == T_OBJECT || type == T_PRIMITIVE_OBJECT) {\n@@ -2203,0 +2213,1 @@\n+  bool null_free = false;\n@@ -2208,0 +2219,1 @@\n+      null_free = alias_type->field()->is_null_free();\n@@ -2216,0 +2228,1 @@\n+      null_free = adr_type->is_aryptr()->is_null_free();\n@@ -2228,0 +2241,3 @@\n+    if (null_free) {\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n@@ -2282,2 +2298,2 @@\n-      assert(rtype == type, \"getter must return the expected value\");\n-      assert(sig->count() == 2, \"oop getter has 2 arguments\");\n+      assert(rtype == type || (rtype == T_OBJECT && type == T_PRIMITIVE_OBJECT), \"getter must return the expected value\");\n+      assert(sig->count() == 2 || (type == T_PRIMITIVE_OBJECT && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n@@ -2289,1 +2305,1 @@\n-      assert(sig->count() == 3, \"oop putter has 3 arguments\");\n+      assert(sig->count() == 3 || (type == T_PRIMITIVE_OBJECT && sig->count() == 4), \"oop putter has 3 arguments\");\n@@ -2293,1 +2309,1 @@\n-      assert(vtype == type, \"putter must accept the expected value\");\n+      assert(vtype == type || (type == T_PRIMITIVE_OBJECT && vtype == T_OBJECT), \"putter must accept the expected value\");\n@@ -2315,0 +2331,55 @@\n+\n+  ciInlineKlass* inline_klass = nullptr;\n+  if (type == T_PRIMITIVE_OBJECT) {\n+    const TypeInstPtr* cls = _gvn.type(argument(4))->isa_instptr();\n+    if (cls == nullptr || cls->const_oop() == nullptr) {\n+      return false;\n+    }\n+    ciType* mirror_type = cls->const_oop()->as_instance()->java_mirror_type();\n+    if (!mirror_type->is_inlinetype()) {\n+      return false;\n+    }\n+    inline_klass = mirror_type->as_inline_klass();\n+  }\n+\n+  if (base->is_InlineType()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (is_store) {\n+      if (!vt->is_allocated(&_gvn)) {\n+        return false;\n+      }\n+      base = vt->get_oop();\n+    } else {\n+      if (offset->is_Con()) {\n+        long off = find_long_con(offset, 0);\n+        ciInlineKlass* vk = vt->type()->inline_klass();\n+        if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+          return false;\n+        }\n+\n+        ciField* field = vk->get_non_flattened_field_by_offset(off);\n+        if (field != nullptr) {\n+          BasicType bt = type2field[field->type()->basic_type()];\n+          if (bt == T_ARRAY || bt == T_NARROWOOP || (bt == T_PRIMITIVE_OBJECT && !field->is_flattened())) {\n+            bt = T_OBJECT;\n+          }\n+          if (bt == type && (bt != T_PRIMITIVE_OBJECT || field->type() == inline_klass)) {\n+            Node* value = vt->field_value_by_offset(off, false);\n+            if (value->is_InlineType()) {\n+              value = value->as_InlineType()->adjust_scalarization_depth(this);\n+            }\n+            set_result(value);\n+            return true;\n+          }\n+        }\n+      }\n+      {\n+        \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        vt = vt->buffer(this);\n+      }\n+      base = vt->get_oop();\n+    }\n+  }\n+\n@@ -2325,1 +2396,1 @@\n-    if (type != T_OBJECT) {\n+    if (type != T_OBJECT && (inline_klass == nullptr || !inline_klass->has_object_fields())) {\n@@ -2343,1 +2414,1 @@\n-  Node* val = is_store ? argument(4) : nullptr;\n+  Node* val = is_store ? argument(4 + (type == T_PRIMITIVE_OBJECT ? 1 : 0)) : nullptr;\n@@ -2364,1 +2435,25 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = nullptr;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != nullptr &&\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flattened_field_by_offset(off);\n+    }\n+    if (field != nullptr) {\n+      bt = type2field[field->type()->basic_type()];\n+    }\n+    assert(bt == alias_type->basic_type() || bt == T_PRIMITIVE_OBJECT, \"should match\");\n+    if (field != nullptr && bt == T_PRIMITIVE_OBJECT && !field->is_flattened()) {\n+      bt = T_OBJECT;\n+    }\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2367,0 +2462,3 @@\n+    if (adr_type->is_flat()) {\n+      bt = T_PRIMITIVE_OBJECT;\n+    }\n@@ -2372,1 +2470,1 @@\n-    if (is_reference_type(bt, true)) {\n+    if (bt != T_PRIMITIVE_OBJECT && is_reference_type(bt, true)) {\n@@ -2387,0 +2485,23 @@\n+  if (type == T_PRIMITIVE_OBJECT) {\n+    if (adr_type->isa_instptr()) {\n+      if (field == nullptr || field->type() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else if (adr_type->isa_aryptr()) {\n+      const Type* elem = adr_type->is_aryptr()->elem();\n+      if (!adr_type->is_flat() || elem->inline_klass() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else {\n+      mismatched = true;\n+    }\n+    if (is_store) {\n+      const Type* val_t = _gvn.type(val);\n+      if (!val_t->is_inlinetypeptr() || val_t->inline_klass() != inline_klass) {\n+        set_map(old_map);\n+        set_sp(old_sp);\n+        return false;\n+      }\n+    }\n+  }\n+\n@@ -2388,1 +2509,1 @@\n-  assert(!mismatched || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n+  assert(!mismatched || type == T_PRIMITIVE_OBJECT || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n@@ -2400,4 +2521,8 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != nullptr) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != nullptr) {\n+        value_type = tjp;\n+      }\n+    } else if (type == T_PRIMITIVE_OBJECT) {\n+      value_type = nullptr;\n@@ -2419,2 +2544,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !field->is_flattened() && !mismatched) {\n@@ -2426,1 +2551,16 @@\n-      p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      if (type == T_PRIMITIVE_OBJECT) {\n+        if (adr_type->isa_instptr() && !mismatched) {\n+          ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+          int offset = adr_type->is_instptr()->offset();\n+          p = InlineTypeNode::make_from_flattened(this, inline_klass, base, base, holder, offset, decorators);\n+        } else {\n+          p = InlineTypeNode::make_from_flattened(this, inline_klass, base, adr, nullptr, 0, decorators);\n+        }\n+      } else {\n+        p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+        const TypeOopPtr* ptr = value_type->make_oopptr();\n+        if (ptr != nullptr && ptr->is_inlinetypeptr()) {\n+          \/\/ Load a non-flattened inline type from memory\n+          p = InlineTypeNode::make_from_oop(this, p, ptr->inline_klass(), !ptr->maybe_null());\n+        }\n+      }\n@@ -2464,1 +2604,17 @@\n-    access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    if (type == T_PRIMITIVE_OBJECT) {\n+      if (adr_type->isa_instptr() && !mismatched) {\n+        ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+        int offset = adr_type->is_instptr()->offset();\n+        val->as_InlineType()->store_flattened(this, base, base, holder, offset, decorators);\n+      } else {\n+        val->as_InlineType()->store_flattened(this, base, adr, nullptr, 0, decorators);\n+      }\n+    } else {\n+      access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    }\n+  }\n+\n+  if (argument(1)->is_InlineType() && is_store) {\n+    InlineTypeNode* value = InlineTypeNode::make_from_oop(this, base, _gvn.type(argument(1))->inline_klass());\n+    value = value->make_larval(this, false);\n+    replace_in_map(argument(1), value);\n@@ -2470,0 +2626,40 @@\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+  if (!value->is_InlineType()) {\n+    return false;\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  set_result(value->as_InlineType()->make_larval(this, true));\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+  if (!buffer->is_InlineType()) {\n+    return false;\n+  }\n+  InlineTypeNode* vt = buffer->as_InlineType();\n+  if (!vt->is_allocated(&_gvn)) {\n+    return false;\n+  }\n+  \/\/ TODO 8239003 Why is this needed?\n+  if (AllocateNode::Ideal_allocation(vt->get_oop()) == nullptr) {\n+    return false;\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  set_result(vt->finish_larval(this));\n+  return true;\n+}\n+\n@@ -2675,0 +2871,13 @@\n+    if (oldval != nullptr && oldval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      oldval = oldval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+    if (newval != nullptr && newval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      newval = newval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+\n@@ -2836,2 +3045,7 @@\n-\n-  Node* obj = new_instance(kls, test);\n+  Node* obj = nullptr;\n+  const TypeInstKlassPtr* tkls = _gvn.type(kls)->isa_instklassptr();\n+  if (tkls != nullptr && tkls->instance_klass()->is_inlinetype()) {\n+    obj = InlineTypeNode::make_default(_gvn, tkls->instance_klass()->as_inline_klass())->buffer(this);\n+  } else {\n+    obj = new_instance(kls, test);\n+  }\n@@ -3609,1 +3823,1 @@\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3614,1 +3828,1 @@\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n@@ -3632,9 +3846,0 @@\n-\/\/---------------------------load_mirror_from_klass----------------------------\n-\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {\n-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n-  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n-  \/\/ mirror = ((OopHandle)mirror)->resolve();\n-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n-}\n-\n@@ -3683,0 +3888,1 @@\n+\n@@ -3876,0 +4082,31 @@\n+\/\/-------------------------inline_primitive_Class_conversion-------------------\n+\/\/               Class<T> java.lang.Class                  .asPrimaryType()\n+\/\/ public static Class<T> jdk.internal.value.PrimitiveClass.asPrimaryType(Class<T>)\n+\/\/               Class<T> java.lang.Class                  .asValueType()\n+\/\/ public static Class<T> jdk.internal.value.PrimitiveClass.asValueType(Class<T>)\n+bool LibraryCallKit::inline_primitive_Class_conversion(vmIntrinsics::ID id) {\n+  Node* mirror = argument(0); \/\/ Receiver\/argument Class\n+  const TypeInstPtr* mirror_con = _gvn.type(mirror)->isa_instptr();\n+  if (mirror_con == nullptr) {\n+    return false;\n+  }\n+\n+  bool is_val_mirror = true;\n+  ciType* tm = mirror_con->java_mirror_type(&is_val_mirror);\n+  if (tm != nullptr) {\n+    Node* result = mirror;\n+    if ((id == vmIntrinsics::_asPrimaryType || id == vmIntrinsics::_asPrimaryTypeArg) && is_val_mirror) {\n+      result = _gvn.makecon(TypeInstPtr::make(tm->as_inline_klass()->ref_mirror()));\n+    } else if (id == vmIntrinsics::_asValueType || id == vmIntrinsics::_asValueTypeArg) {\n+      if (!tm->is_inlinetype()) {\n+        return false; \/\/ Throw UnsupportedOperationException\n+      } else if (!is_val_mirror) {\n+        result = _gvn.makecon(TypeInstPtr::make(tm->as_inline_klass()->val_mirror()));\n+      }\n+    }\n+    set_result(result);\n+    return true;\n+  }\n+  return false;\n+}\n+\n@@ -3891,1 +4128,2 @@\n-  ciType* tm = mirror_con->java_mirror_type();\n+  bool requires_null_check = false;\n+  ciType* tm = mirror_con->java_mirror_type(&requires_null_check);\n@@ -3901,0 +4139,3 @@\n+        if (requires_null_check) {\n+          obj = null_check(obj);\n+        }\n@@ -3921,0 +4162,3 @@\n+  if (requires_null_check) {\n+    obj = null_check(obj);\n+  }\n@@ -3927,2 +4171,2 @@\n-  \/\/ Not-subtype or the mirror's klass ptr is null (in case it is a primitive).\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  \/\/ Not-subtype or the mirror's klass ptr is nullptr (in case it is a primitive).\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -3938,0 +4182,2 @@\n+  Node* io = i_o();\n+  Node* mem = merged_memory();\n@@ -3939,0 +4185,21 @@\n+    if (EnableValhalla && !requires_null_check) {\n+      \/\/ Check if we are casting to QMyValue\n+      Node* ctrl_val_mirror = generate_fair_guard(is_val_mirror(mirror), nullptr);\n+      if (ctrl_val_mirror != nullptr) {\n+        RegionNode* r = new RegionNode(3);\n+        record_for_igvn(r);\n+        r->init_req(1, control());\n+\n+        \/\/ Casting to QMyValue, check for null\n+        set_control(ctrl_val_mirror);\n+        { \/\/ PreserveJVMState because null check replaces obj in map\n+          PreserveJVMState pjvms(this);\n+          Node* null_ctr = top();\n+          null_check_oop(obj, &null_ctr);\n+          region->init_req(_npe_path, null_ctr);\n+          r->init_req(2, control());\n+        }\n+        set_control(_gvn.transform(r));\n+      }\n+    }\n+\n@@ -3945,1 +4212,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -3949,0 +4217,3 @@\n+    \/\/ Set IO and memory because gen_checkcast may override them when buffering inline types\n+    set_i_o(io);\n+    set_all_memory(mem);\n@@ -3982,0 +4253,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -3984,0 +4256,1 @@\n+  record_for_igvn(prim_region);\n@@ -4008,2 +4281,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -4019,0 +4295,3 @@\n+    \/\/ If superc is an inline mirror, we also need to check if superc == subc because LMyValue\n+    \/\/ is not a subtype of QMyValue but due to subk == superk the subtype check will pass.\n+    generate_fair_guard(is_val_mirror(args[0]), prim_region);\n@@ -4026,1 +4305,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -4031,1 +4311,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -4062,2 +4342,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind) {\n@@ -4069,9 +4348,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -4082,4 +4352,11 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;\n+      case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -4095,0 +4372,21 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case ObjectArray:\n+    case NonObjectArray: {\n+      value = Klass::_lh_array_tag_obj_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == ObjectArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -4096,4 +4394,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -4101,3 +4396,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -4110,1 +4402,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -4256,1 +4548,13 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    const TypeKlassPtr* tklass = _gvn.type(klass_node)->is_klassptr();\n+    bool exclude_flat = UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == nullptr || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        tklass->can_be_inline_array() && (!tklass->is_flat() || tklass->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_objArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n@@ -4260,1 +4564,1 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n@@ -4281,0 +4585,32 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != nullptr && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_fair_guard(flat_array_test(klass_node, \/* flat = *\/ false), bailout);\n+        }\n+      } else if (UseFlatArray && (orig_t == nullptr || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!tklass->is_flat() && tklass->can_be_inline_array()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_fair_guard(flat_array_test(load_object_klass(original)), bailout);\n+        if (orig_t != nullptr) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(null_free_array_test(klass_node), bailout);\n+      }\n+    }\n+\n@@ -4323,1 +4659,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -4409,1 +4745,1 @@\n-    const TypeTuple* range = tf->range();\n+    const TypeTuple* range = tf->range_cc();\n@@ -4413,1 +4749,1 @@\n-    tf = TypeFunc::make(tf->domain(), new_range);\n+    tf = TypeFunc::make(tf->domain_cc(), new_range);\n@@ -4470,1 +4806,6 @@\n-  Node* obj = nullptr;\n+  Node* obj = argument(0);\n+\n+  if (gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -4480,1 +4821,0 @@\n-    obj = argument(0);\n@@ -4520,1 +4860,2 @@\n-  Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+  \/\/ This also serves as guard against inline types\n+  Node *lock_mask      = _gvn.MakeConX(markWord::inline_type_mask_in_place);\n@@ -4586,1 +4927,10 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    const Type* t = _gvn.type(obj);\n+    if (t->maybe_null()) {\n+      null_check(obj);\n+    }\n+    set_result(makecon(TypeInstPtr::make(t->inline_klass()->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -4951,1 +5301,2 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    obj = null_check_receiver();\n@@ -4961,1 +5312,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -4991,0 +5343,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -4996,3 +5353,0 @@\n-      Node* obj_length = load_array_length(obj);\n-      Node* obj_size  = nullptr;\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size, \/*deoptimize_on_exception=*\/true);\n@@ -5001,20 +5355,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n-        if (is_obja != nullptr) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          \/\/ Clones are always tightly coupled.\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Expansion) &&\n+          obj_type->can_be_inline_array() &&\n+          (ary_ptr == nullptr || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flattened inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_fair_guard(flat_array_test(obj_klass), slow_region);\n@@ -5022,7 +5363,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -5031,7 +5365,43 @@\n-        copy_to_clone(obj, alloc_obj, obj_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(obj);\n+        Node* obj_size  = nullptr;\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n+          if (is_obja != nullptr) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            \/\/ Clones are always tightly coupled.\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, obj_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -5041,4 +5411,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -5214,2 +5580,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -5218,1 +5583,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -5260,1 +5625,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -5461,1 +5826,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -5488,0 +5853,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -5491,0 +5858,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -5506,2 +5875,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -5550,0 +5918,2 @@\n+      slow_region->add_req(not_subtype_ctrl);\n+    }\n@@ -5551,6 +5921,28 @@\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n+    src_type = _gvn.type(src);\n+    top_src  = src_type->isa_aryptr();\n+\n+    \/\/ Handle flat inline type arrays (null-free arrays are handled by the subtype check above)\n+    if (!stopped() && UseFlatArray) {\n+      \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check). Handle flat src here.\n+      assert(top_dest == nullptr || !top_dest->is_flat() || top_src->is_flat(), \"src array must be flat\");\n+      if (top_src != nullptr && top_src->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (top_dest != nullptr && !top_dest->is_flat()) {\n+          generate_fair_guard(flat_array_test(dest_klass, \/* flat = *\/ false), slow_region);\n+          \/\/ Since dest is flat and src <: dest, dest must have the same type as src.\n+          top_dest = top_src->cast_to_exactness(false);\n+          assert(top_dest->is_flat(), \"dest must be flat\");\n+          dest = _gvn.transform(new CheckCastPPNode(control(), dest, top_dest));\n+        }\n+      } else if (top_src == nullptr || !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        assert(top_dest == nullptr || !top_dest->is_flat(), \"dest array must not be flat\");\n+        generate_fair_guard(flat_array_test(src), slow_region);\n+        if (top_src != nullptr) {\n+          top_src = top_src->cast_to_not_flat();\n+          src = _gvn.transform(new CheckCastPPNode(control(), src, top_src));\n+        }\n@@ -5559,0 +5951,1 @@\n+\n@@ -5566,4 +5959,0 @@\n-\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n-    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":520,"deletions":131,"binary":false,"changes":651,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -108,3 +109,11 @@\n-    if (!stopped() && result() != nullptr) {\n-      BasicType bt = result()->bottom_type()->basic_type();\n-      push_node(bt, result());\n+    Node* res = result();\n+    if (!stopped() && res != nullptr) {\n+      BasicType bt = res->bottom_type()->basic_type();\n+      if (C->inlining_incrementally() && res->is_InlineType()) {\n+        \/\/ The caller expects an oop when incrementally inlining an intrinsic that returns an\n+        \/\/ inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        res = res->as_InlineType()->buffer(this);\n+      }\n+      push_node(bt, res);\n@@ -141,1 +150,0 @@\n-  Node* load_mirror_from_klass(Node* klass);\n@@ -163,0 +171,9 @@\n+\n+  enum ArrayKind {\n+    AnyArray,\n+    NonArray,\n+    ObjectArray,\n+    NonObjectArray,\n+    TypeArray\n+  };\n+\n@@ -164,0 +181,1 @@\n+\n@@ -165,1 +183,1 @@\n-    return generate_array_guard_common(kls, region, false, false);\n+    return generate_array_guard_common(kls, region, AnyArray);\n@@ -168,1 +186,1 @@\n-    return generate_array_guard_common(kls, region, false, true);\n+    return generate_array_guard_common(kls, region, NonArray);\n@@ -171,1 +189,1 @@\n-    return generate_array_guard_common(kls, region, true, false);\n+    return generate_array_guard_common(kls, region, ObjectArray);\n@@ -174,1 +192,4 @@\n-    return generate_array_guard_common(kls, region, true, true);\n+    return generate_array_guard_common(kls, region, NonObjectArray);\n+  }\n+  Node* generate_typeArray_guard(Node* kls, RegionNode* region) {\n+    return generate_array_guard_common(kls, region, TypeArray);\n@@ -176,2 +197,1 @@\n-  Node* generate_array_guard_common(Node* kls, RegionNode* region,\n-                                    bool obj_array, bool not_array);\n+  Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);\n@@ -234,0 +254,2 @@\n+  bool inline_unsafe_make_private_buffer();\n+  bool inline_unsafe_finish_private_buffer();\n@@ -256,0 +278,1 @@\n+  bool inline_primitive_Class_conversion(vmIntrinsics::ID id);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":33,"deletions":10,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -79,1 +79,2 @@\n-         LoopNestLongOuterLoop = 1<<16 };\n+         LoopNestLongOuterLoop = 1<<16,\n+         FlattenedArrays       = 1<<17};\n@@ -102,0 +103,1 @@\n+  bool is_flattened_arrays() const { return _loop_flags & FlattenedArrays; }\n@@ -115,0 +117,1 @@\n+  void mark_flattened_arrays() { _loop_flags |= FlattenedArrays; }\n@@ -1396,3 +1399,3 @@\n-                                        Node_List &old_new,\n-                                        IfNode* unswitch_iff,\n-                                        CloneLoopMode mode);\n+                                      Node_List &old_new,\n+                                      Node_List &unswitch_iffs,\n+                                      CloneLoopMode mode);\n@@ -1416,1 +1419,1 @@\n-  IfNode* find_unswitching_candidate(const IdealLoopTree *loop) const;\n+  IfNode* find_unswitching_candidate(const IdealLoopTree *loop, Node_List& unswitch_iffs) const;\n@@ -1536,0 +1539,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1537,0 +1541,1 @@\n+  bool flatten_array_element_type_check(Node *n);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -65,0 +66,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return nullptr;\n+  }\n+\n@@ -701,0 +708,4 @@\n+      if (inp->isa_InlineType()) {\n+        \/\/ TODO 8302217 This prevents PhiNode::push_inline_types_through\n+        return nullptr;\n+      }\n@@ -1026,0 +1037,49 @@\n+\/\/ If UseArrayMarkWordCheck is enabled, we can't use immutable memory for the flat array check\n+\/\/ because we are loading the mark word which is mutable. Although the bits we are interested in\n+\/\/ are immutable (we check for markWord::unlocked_value), we need to use raw memory to not break\n+\/\/ anti dependency analysis. Below code will attempt to still move flat array checks out of loops,\n+\/\/ mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::ArrayOrKlass)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -1038,0 +1098,6 @@\n+\n+  if (UseArrayMarkWordCheck && n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1315,0 +1381,98 @@\n+bool PhaseIdealLoop::flatten_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flattened array and the load of the value\n+  \/\/ happens with a flattened array check then: push the type check\n+  \/\/ through the phi of the flattened array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == nullptr) {\n+    return false;\n+  }\n+\n+  assert(obj != nullptr && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != nullptr, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      const Type* tcast = cast_clone->Value(&_igvn);\n+      _igvn.set_type(cast_clone, tcast);\n+      cast_clone->as_Type()->set_type(tcast);\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1321,0 +1485,4 @@\n+  if (flatten_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1453,0 +1621,5 @@\n+\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(this);\n+  }\n@@ -1886,1 +2059,9 @@\n-  Node *sample_cmp = sample_bool->in(1);\n+  Node* sample_cmp = sample_bool->in(1);\n+  const Type* t = Type::TOP;\n+  const TypePtr* at = nullptr;\n+  if (sample_cmp->is_FlatArrayCheck()) {\n+    \/\/ Left input of a FlatArrayCheckNode is memory, set the (adr) type of the phi accordingly\n+    assert(sample_cmp->in(1)->bottom_type() == Type::MEMORY, \"unexpected input type\");\n+    t = Type::MEMORY;\n+    at = TypeRawPtr::BOTTOM;\n+  }\n@@ -1889,1 +2070,1 @@\n-  PhiNode *phi1 = new PhiNode(phi->in(0), Type::TOP);\n+  PhiNode *phi1 = new PhiNode(phi->in(0), t, at);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":183,"deletions":2,"binary":false,"changes":185,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -56,0 +58,1 @@\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -85,12 +88,0 @@\n-void PhaseMacroExpand::migrate_outs(Node *old, Node *target) {\n-  assert(old != nullptr, \"sanity\");\n-  for (DUIterator_Fast imax, i = old->fast_outs(imax); i < imax; i++) {\n-    Node* use = old->fast_out(i);\n-    _igvn.rehash_node_delayed(use);\n-    imax -= replace_input(use, old, target);\n-    \/\/ back up iterator\n-    --i;\n-  }\n-  assert(old->outcnt() == 0, \"all uses must be deleted\");\n-}\n-\n@@ -159,1 +150,1 @@\n-  bs->eliminate_gc_barrier(this, p2x);\n+  bs->eliminate_gc_barrier(&_igvn, p2x);\n@@ -213,1 +204,1 @@\n-        int adr_offset = atype->offset();\n+        int adr_offset = atype->flattened_offset();\n@@ -258,1 +249,1 @@\n-   } else if (mem->Opcode() == Op_StrInflatedCopy) {\n+    } else if (mem->Opcode() == Op_StrInflatedCopy) {\n@@ -303,1 +294,5 @@\n-      const TypePtr* adr_type = nullptr;\n+      Node* base = ac->in(ArrayCopyNode::Src);\n+      const TypeAryPtr* adr_type = _igvn.type(base)->is_aryptr();\n+      if (adr_type->is_flat()) {\n+        shift = adr_type->flat_log_elem_size();\n+      }\n@@ -306,1 +301,0 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n@@ -308,1 +302,2 @@\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(off);\n+        adr_type = _igvn.type(adr)->is_aryptr();\n+        assert(adr_type == _igvn.type(base)->is_aryptr()->add_field_offset_and_offset(off), \"incorrect address type\");\n@@ -311,1 +306,1 @@\n-          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type->isa_oopptr(), alloc);\n+          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type, alloc);\n@@ -314,0 +309,5 @@\n+        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n+          \/\/ Non constant offset in the array: we can't statically\n+          \/\/ determine the value\n+          return nullptr;\n+        }\n@@ -321,7 +321,5 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(Type::OffsetBot);\n-        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n-          \/\/ Non constant offset in the array: we can't statically\n-          \/\/ determine the value\n-          return nullptr;\n-        }\n+        \/\/ In the case of a flattened inline type array, each field has its\n+        \/\/ own slice so we need to extract the field being accessed from\n+        \/\/ the address computation\n+        adr_type = adr_type->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot)->is_aryptr();\n+        adr = _igvn.transform(new CastPPNode(adr, adr_type));\n@@ -338,0 +336,1 @@\n+      assert(res->isa_DecodeN(), \"should be narrow oop\");\n@@ -353,1 +352,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flattened_offset();\n@@ -388,1 +387,1 @@\n-    } else  {\n+    } else {\n@@ -392,1 +391,7 @@\n-        values.at_put(j, _igvn.zerocon(ft));\n+        Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+        if (default_value != nullptr) {\n+          values.at_put(j, default_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawDefaultValue) == nullptr, \"default value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -412,1 +417,7 @@\n-        values.at_put(j, _igvn.zerocon(ft));\n+        Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+        if (default_value != nullptr) {\n+          values.at_put(j, default_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawDefaultValue) == nullptr, \"default value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -456,1 +467,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flattened_offset();\n@@ -458,1 +469,0 @@\n-  Node *alloc_ctrl = alloc->in(TypeFunc::Control);\n@@ -474,1 +484,1 @@\n-        done = true; \/\/ Something go wrong.\n+        done = true; \/\/ Something went wrong.\n@@ -484,1 +494,1 @@\n-             atype->is_known_instance_field() && atype->offset() == offset &&\n+             atype->is_known_instance_field() && atype->flattened_offset() == offset &&\n@@ -517,0 +527,5 @@\n+      Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+      if (default_value != nullptr) {\n+        return default_value;\n+      }\n+      assert(alloc->in(AllocateNode::RawDefaultValue) == nullptr, \"default value may not be null\");\n@@ -548,1 +563,1 @@\n-  \/\/ Something go wrong.\n+  \/\/ Something went wrong.\n@@ -552,0 +567,42 @@\n+\/\/ Search the last value stored into the inline type's fields.\n+Node* PhaseMacroExpand::inline_type_from_mem(Node* mem, Node* ctl, ciInlineKlass* vk, const TypeAryPtr* adr_type, int offset, AllocateNode* alloc) {\n+  \/\/ Subtract the offset of the first field to account for the missing oop header\n+  offset -= vk->first_field_offset();\n+  \/\/ Create a new InlineTypeNode and retrieve the field values from memory\n+  InlineTypeNode* vt = InlineTypeNode::make_uninitialized(_igvn, vk);\n+  transform_later(vt);\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); ++i) {\n+    ciType* field_type = vt->field_type(i);\n+    int field_offset = offset + vt->field_offset(i);\n+    Node* value = nullptr;\n+    if (vt->field_is_flattened(i)) {\n+      value = inline_type_from_mem(mem, ctl, field_type->as_inline_klass(), adr_type, field_offset, alloc);\n+    } else {\n+      const Type* ft = Type::get_const_type(field_type);\n+      BasicType bt = type2field[field_type->basic_type()];\n+      if (UseCompressedOops && !is_java_primitive(bt)) {\n+        ft = ft->make_narrowoop();\n+        bt = T_NARROWOOP;\n+      }\n+      \/\/ Each inline type field has its own memory slice\n+      adr_type = adr_type->with_field_offset(field_offset);\n+      value = value_from_mem(mem, ctl, bt, ft, adr_type, alloc);\n+      if (value != nullptr && ft->isa_narrowoop()) {\n+        assert(UseCompressedOops, \"unexpected narrow oop\");\n+        if (value->is_EncodeP()) {\n+          value = value->in(1);\n+        } else {\n+          value = transform_later(new DecodeNNode(value, value->get_ptr_type()));\n+        }\n+      }\n+    }\n+    if (value != nullptr) {\n+      vt->set_field_value(i, value);\n+    } else {\n+      \/\/ We might have reached the TrackedInitializationLimit\n+      return nullptr;\n+    }\n+  }\n+  return vt;\n+}\n+\n@@ -561,0 +618,1 @@\n+  Unique_Node_List worklist;\n@@ -569,0 +627,1 @@\n+    worklist.push(res);\n@@ -582,1 +641,1 @@\n-  if (can_eliminate && res != nullptr) {\n+  while (can_eliminate && worklist.size() > 0) {\n@@ -584,2 +643,2 @@\n-    for (DUIterator_Fast jmax, j = res->fast_outs(jmax);\n-                               j < jmax && can_eliminate; j++) {\n+    res = worklist.pop();\n+    for (DUIterator_Fast jmax, j = res->fast_outs(jmax); j < jmax && can_eliminate; j++) {\n@@ -633,0 +692,21 @@\n+      } else if (use->is_InlineType() && use->as_InlineType()->get_oop() == res) {\n+        \/\/ Look at uses\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (u->is_InlineType()) {\n+            \/\/ Use in flat field can be eliminated\n+            InlineTypeNode* vt = u->as_InlineType();\n+            for (uint i = 0; i < vt->field_count(); ++i) {\n+              if (vt->field_value(i) == use && !vt->field_is_flattened(i)) {\n+                can_eliminate = false; \/\/ Use in non-flattened field\n+                break;\n+              }\n+            }\n+          } else {\n+            \/\/ Add other uses to the worklist to process individually\n+            worklist.push(u);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(res_type->is_inlinetypeptr(), \"Unexpected store to mark word\");\n@@ -652,0 +732,3 @@\n+      } else {\n+        assert(use->Opcode() == Op_CastP2X, \"should be\");\n+        assert(!use->has_out_with(Op_OrL), \"should have been removed because oop is never null\");\n@@ -664,1 +747,1 @@\n-    } else if (alloc->_is_scalar_replaceable) {\n+    } else {\n@@ -729,1 +812,2 @@\n-SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt) {\n+SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt,\n+                                                                                  Unique_Node_List* value_worklist) {\n@@ -761,0 +845,4 @@\n+      if (res_type->is_flat()) {\n+        \/\/ Flattened inline type array\n+        element_size = res_type->is_aryptr()->flat_elem_size();\n+      }\n@@ -777,0 +865,1 @@\n+      assert(!field->is_flattened(), \"flattened inline type fields should not have safepoint uses\");\n@@ -802,3 +891,9 @@\n-    const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n-\n-    Node *field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n+    Node* field_val = nullptr;\n+    const TypeOopPtr* field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n+    if (res_type->is_flat()) {\n+      ciInlineKlass* vk = res_type->is_aryptr()->elem()->inline_klass();\n+      assert(vk->flatten_array(), \"must be flattened\");\n+      field_val = inline_type_from_mem(sfpt->memory(), sfpt->control(), vk, field_addr_type->isa_aryptr(), 0, alloc);\n+    } else {\n+      field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n+    }\n@@ -841,1 +936,1 @@\n-      } else {\n+      } else if (!field_val->is_InlineType()) {\n@@ -844,0 +939,4 @@\n+    if (field_val->is_InlineType()) {\n+      \/\/ Keep track of inline types to scalarize them later\n+      value_worklist->push(field_val);\n+    }\n@@ -857,0 +956,4 @@\n+  const TypeOopPtr* res_type = nullptr;\n+  if (res != nullptr) { \/\/ Could be null when there are no users\n+    res_type = _igvn.type(res)->isa_oopptr();\n+  }\n@@ -859,0 +962,2 @@\n+  assert(safepoints.length() == 0 || !res_type->is_inlinetypeptr(), \"Inline type allocations should not have safepoint uses\");\n+  Unique_Node_List value_worklist;\n@@ -861,1 +966,1 @@\n-    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt);\n+    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt, &value_worklist);\n@@ -878,1 +983,8 @@\n-\n+  \/\/ Scalarize inline types that were added to the safepoint.\n+  \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+  \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+  bool allow_oop = (res_type != nullptr) && !res_type->is_flat();\n+  for (uint i = 0; i < value_worklist.size(); ++i) {\n+    InlineTypeNode* vt = value_worklist.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&_igvn, allow_oop);\n+  }\n@@ -894,1 +1006,2 @@\n-void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc) {\n+void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc, bool inline_alloc) {\n+  Unique_Node_List worklist;\n@@ -897,0 +1010,4 @@\n+    worklist.push(res);\n+  }\n+  while (worklist.size() > 0) {\n+    res = worklist.pop();\n@@ -906,10 +1023,7 @@\n-#ifdef ASSERT\n-            \/\/ Verify that there is no dependent MemBarVolatile nodes,\n-            \/\/ they should be removed during IGVN, see MemBarNode::Ideal().\n-            for (DUIterator_Fast pmax, p = n->fast_outs(pmax);\n-                                       p < pmax; p++) {\n-              Node* mb = n->fast_out(p);\n-              assert(mb->is_Initialize() || !mb->is_MemBar() ||\n-                     mb->req() <= MemBarNode::Precedent ||\n-                     mb->in(MemBarNode::Precedent) != n,\n-                     \"MemBarVolatile should be eliminated for non-escaping object\");\n+            for (DUIterator_Fast pmax, p = n->fast_outs(pmax); p < pmax; p++) {\n+              MemBarNode* mb = n->fast_out(p)->isa_MemBar();\n+              if (mb != nullptr && mb->req() <= MemBarNode::Precedent && mb->in(MemBarNode::Precedent) == n) {\n+                \/\/ MemBarVolatiles should have been removed by MemBarNode::Ideal() for non-inline allocations\n+                assert(inline_alloc, \"MemBarVolatile should be eliminated for non-escaping object\");\n+                mb->remove(&_igvn);\n+              }\n@@ -917,1 +1031,0 @@\n-#endif\n@@ -941,2 +1054,1 @@\n-          CallProjections callprojs;\n-          ac->extract_projections(&callprojs, true);\n+          CallProjections* callprojs = ac->extract_projections(true);\n@@ -944,3 +1056,3 @@\n-          _igvn.replace_node(callprojs.fallthrough_ioproj, ac->in(TypeFunc::I_O));\n-          _igvn.replace_node(callprojs.fallthrough_memproj, ac->in(TypeFunc::Memory));\n-          _igvn.replace_node(callprojs.fallthrough_catchproj, ac->in(TypeFunc::Control));\n+          _igvn.replace_node(callprojs->fallthrough_ioproj, ac->in(TypeFunc::I_O));\n+          _igvn.replace_node(callprojs->fallthrough_memproj, ac->in(TypeFunc::Memory));\n+          _igvn.replace_node(callprojs->fallthrough_catchproj, ac->in(TypeFunc::Control));\n@@ -963,0 +1075,19 @@\n+      } else if (use->is_InlineType()) {\n+        assert(use->as_InlineType()->get_oop() == res, \"unexpected inline type ptr use\");\n+        \/\/ Cut off oop input and remove known instance id from type\n+        _igvn.rehash_node_delayed(use);\n+        use->as_InlineType()->set_oop(_igvn.zerocon(T_PRIMITIVE_OBJECT));\n+        const TypeOopPtr* toop = _igvn.type(use)->is_oopptr()->cast_to_instance_id(TypeOopPtr::InstanceBot);\n+        _igvn.set_type(use, toop);\n+        use->as_InlineType()->set_type(toop);\n+        \/\/ Process users\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (!u->is_InlineType()) {\n+            worklist.push(u);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(inline_alloc, \"Unexpected store to mark word\");\n+        _igvn.replace_node(use, use->in(MemNode::Memory));\n@@ -975,1 +1106,1 @@\n-  if (_callprojs.resproj != nullptr && _callprojs.resproj->outcnt() != 0) {\n+  if (_callprojs->resproj[0] != nullptr && _callprojs->resproj[0]->outcnt() != 0) {\n@@ -979,2 +1110,2 @@\n-    for (DUIterator_Fast jmax, j = _callprojs.resproj->fast_outs(jmax);  j < jmax; j++) {\n-      Node* use = _callprojs.resproj->fast_out(j);\n+    for (DUIterator_Fast jmax, j = _callprojs->resproj[0]->fast_outs(jmax);  j < jmax; j++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(j);\n@@ -987,3 +1118,3 @@\n-    for (DUIterator_Last jmin, j = _callprojs.resproj->last_outs(jmin); j >= jmin; ) {\n-      Node* use = _callprojs.resproj->last_out(j);\n-      uint oc1 = _callprojs.resproj->outcnt();\n+    for (DUIterator_Last jmin, j = _callprojs->resproj[0]->last_outs(jmin); j >= jmin; ) {\n+      Node* use = _callprojs->resproj[0]->last_out(j);\n+      uint oc1 = _callprojs->resproj[0]->outcnt();\n@@ -1000,1 +1131,1 @@\n-          assert(tmp == nullptr || tmp == _callprojs.fallthrough_catchproj, \"allocation control projection\");\n+          assert(tmp == nullptr || tmp == _callprojs->fallthrough_catchproj, \"allocation control projection\");\n@@ -1008,1 +1139,1 @@\n-            assert(mem->in(TypeFunc::Memory) == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem->in(TypeFunc::Memory) == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1010,1 +1141,1 @@\n-            assert(mem == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1015,0 +1146,4 @@\n+      } else if (use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarStoreStore\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1018,1 +1153,1 @@\n-      j -= (oc1 - _callprojs.resproj->outcnt());\n+      j -= (oc1 - _callprojs->resproj[0]->outcnt());\n@@ -1021,2 +1156,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, alloc->in(TypeFunc::Control));\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, alloc->in(TypeFunc::Control));\n@@ -1024,2 +1159,2 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_memproj, alloc->in(TypeFunc::Memory));\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_memproj, alloc->in(TypeFunc::Memory));\n@@ -1027,2 +1162,2 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_memproj, C->top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_memproj, C->top());\n@@ -1030,2 +1165,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -1033,2 +1168,2 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_ioproj, C->top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_ioproj, C->top());\n@@ -1036,2 +1171,2 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_catchproj, C->top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_catchproj, C->top());\n@@ -1047,1 +1182,1 @@\n-  if (!EliminateAllocations || !alloc->_is_non_escaping) {\n+  if (!EliminateAllocations) {\n@@ -1052,1 +1187,8 @@\n-  Node* res = alloc->result_cast();\n+\n+  \/\/ Attempt to eliminate inline type buffer allocations\n+  \/\/ regardless of usage and escape\/replaceable status.\n+  bool inline_alloc = tklass->isa_instklassptr() &&\n+                      tklass->is_instklassptr()->instance_klass()->is_inlinetype();\n+  if (!alloc->_is_non_escaping && !inline_alloc) {\n+    return false;\n+  }\n@@ -1055,1 +1197,2 @@\n-  bool boxing_alloc = C->eliminate_boxing() &&\n+  Node* res = alloc->result_cast();\n+  bool boxing_alloc = (res == nullptr) && C->eliminate_boxing() &&\n@@ -1062,1 +1205,1 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1070,1 +1213,1 @@\n-    assert(res == nullptr, \"sanity\");\n+    assert(res == nullptr || inline_alloc, \"sanity\");\n@@ -1075,0 +1218,1 @@\n+      assert(!inline_alloc, \"Inline type allocations should not have safepoint uses\");\n@@ -1095,1 +1239,1 @@\n-  process_users_of_allocation(alloc);\n+  process_users_of_allocation(alloc, inline_alloc);\n@@ -1117,1 +1261,1 @@\n-  boxing->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = boxing->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1119,1 +1263,1 @@\n-  const TypeTuple* r = boxing->tf()->range();\n+  const TypeTuple* r = boxing->tf()->range_sig();\n@@ -1303,1 +1447,1 @@\n-    IfNode *toobig_iff = new IfNode(ctrl, initial_slow_test, PROB_MIN, COUNT_UNKNOWN);\n+    IfNode* toobig_iff = new IfNode(ctrl, initial_slow_test, PROB_MIN, COUNT_UNKNOWN);\n@@ -1306,1 +1450,1 @@\n-    Node *toobig_true = new IfTrueNode( toobig_iff );\n+    Node* toobig_true = new IfTrueNode(toobig_iff);\n@@ -1309,1 +1453,1 @@\n-    toobig_false = new IfFalseNode( toobig_iff );\n+    toobig_false = new IfFalseNode(toobig_iff);\n@@ -1348,0 +1492,1 @@\n+\n@@ -1405,0 +1550,3 @@\n+  } else {\n+    \/\/ Let the runtime know if this is a larval allocation\n+    call->init_req(TypeFunc::Parms+1, _igvn.intcon(alloc->_larval));\n@@ -1436,1 +1584,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1442,2 +1590,2 @@\n-  if (expand_fast_path && _callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, result_phi_rawmem);\n+  if (expand_fast_path && _callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, result_phi_rawmem);\n@@ -1447,4 +1595,4 @@\n-  if (_callprojs.catchall_memproj != nullptr ) {\n-    if (_callprojs.fallthrough_memproj == nullptr) {\n-      _callprojs.fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n-      transform_later(_callprojs.fallthrough_memproj);\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    if (_callprojs->fallthrough_memproj == nullptr) {\n+      _callprojs->fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n+      transform_later(_callprojs->fallthrough_memproj);\n@@ -1452,2 +1600,2 @@\n-    migrate_outs(_callprojs.catchall_memproj, _callprojs.fallthrough_memproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_memproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_memproj, _callprojs->fallthrough_memproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_memproj);\n@@ -1461,2 +1609,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, result_phi_i_o);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, result_phi_i_o);\n@@ -1466,4 +1614,4 @@\n-  if (_callprojs.catchall_ioproj != nullptr ) {\n-    if (_callprojs.fallthrough_ioproj == nullptr) {\n-      _callprojs.fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n-      transform_later(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    if (_callprojs->fallthrough_ioproj == nullptr) {\n+      _callprojs->fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n+      transform_later(_callprojs->fallthrough_ioproj);\n@@ -1471,2 +1619,2 @@\n-    migrate_outs(_callprojs.catchall_ioproj, _callprojs.fallthrough_ioproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_ioproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_ioproj, _callprojs->fallthrough_ioproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_ioproj);\n@@ -1491,2 +1639,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    ctrl = _callprojs.fallthrough_catchproj->clone();\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    ctrl = _callprojs->fallthrough_catchproj->clone();\n@@ -1494,1 +1642,1 @@\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, result_region);\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, result_region);\n@@ -1499,1 +1647,1 @@\n-  if (_callprojs.resproj == nullptr) {\n+  if (_callprojs->resproj[0] == nullptr) {\n@@ -1503,1 +1651,1 @@\n-    slow_result = _callprojs.resproj->clone();\n+    slow_result = _callprojs->resproj[0]->clone();\n@@ -1505,1 +1653,1 @@\n-    _igvn.replace_node(_callprojs.resproj, result_phi_rawoop);\n+    _igvn.replace_node(_callprojs->resproj[0], result_phi_rawoop);\n@@ -1515,1 +1663,1 @@\n-  result_phi_rawmem->init_req(slow_result_path, _callprojs.fallthrough_memproj);\n+  result_phi_rawmem->init_req(slow_result_path, _callprojs->fallthrough_memproj);\n@@ -1527,4 +1675,4 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  if (_callprojs.resproj != nullptr) {\n-    for (DUIterator_Fast imax, i = _callprojs.resproj->fast_outs(imax); i < imax; i++) {\n-      Node* use = _callprojs.resproj->fast_out(i);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  if (_callprojs->resproj[0] != nullptr) {\n+    for (DUIterator_Fast imax, i = _callprojs->resproj[0]->fast_outs(imax); i < imax; i++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(i);\n@@ -1535,2 +1683,2 @@\n-    assert(_callprojs.resproj->outcnt() == 0, \"all uses must be deleted\");\n-    _igvn.remove_dead_node(_callprojs.resproj);\n+    assert(_callprojs->resproj[0]->outcnt() == 0, \"all uses must be deleted\");\n+    _igvn.remove_dead_node(_callprojs->resproj[0]);\n@@ -1538,3 +1686,3 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_catchproj, ctrl);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_catchproj);\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_catchproj, ctrl);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_catchproj);\n@@ -1542,3 +1690,3 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_catchproj);\n-    _callprojs.catchall_catchproj->set_req(0, top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_catchproj);\n+    _callprojs->catchall_catchproj->set_req(0, top());\n@@ -1546,2 +1694,2 @@\n-  if (_callprojs.fallthrough_proj != nullptr) {\n-    Node* catchnode = _callprojs.fallthrough_proj->unique_ctrl_out();\n+  if (_callprojs->fallthrough_proj != nullptr) {\n+    Node* catchnode = _callprojs->fallthrough_proj->unique_ctrl_out();\n@@ -1549,1 +1697,1 @@\n-    _igvn.remove_dead_node(_callprojs.fallthrough_proj);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_proj);\n@@ -1551,3 +1699,3 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, mem);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_memproj);\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, mem);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_memproj);\n@@ -1555,3 +1703,3 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, i_o);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, i_o);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_ioproj);\n@@ -1559,3 +1707,3 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_memproj);\n-    _callprojs.catchall_memproj->set_req(0, top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_memproj);\n+    _callprojs->catchall_memproj->set_req(0, top());\n@@ -1563,3 +1711,3 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_ioproj);\n-    _callprojs.catchall_ioproj->set_req(0, top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_ioproj);\n+    _callprojs->catchall_ioproj->set_req(0, top());\n@@ -1683,5 +1831,4 @@\n-Node*\n-PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n-                                    Node* control, Node* rawmem, Node* object,\n-                                    Node* klass_node, Node* length,\n-                                    Node* size_in_bytes) {\n+Node* PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n+                                          Node* control, Node* rawmem, Node* object,\n+                                          Node* klass_node, Node* length,\n+                                          Node* size_in_bytes) {\n@@ -1690,1 +1837,1 @@\n-  Node* mark_node = alloc->make_ideal_mark(&_igvn, object, control, rawmem);\n+  Node* mark_node = alloc->make_ideal_mark(&_igvn, control, rawmem);\n@@ -1726,0 +1873,2 @@\n+                                            alloc->in(AllocateNode::DefaultValue),\n+                                            alloc->in(AllocateNode::RawDefaultValue),\n@@ -2095,0 +2244,43 @@\n+void PhaseMacroExpand::inline_type_guard(Node** ctrl, LockNode* lock) {\n+  Node* obj = lock->obj_node();\n+  const TypePtr* obj_type = _igvn.type(obj)->make_ptr();\n+  if (!obj_type->can_be_inline_type()) {\n+    return;\n+  }\n+  Node* mark = make_load(*ctrl, lock->memory(), obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X->basic_type());\n+  Node* value_mask = _igvn.MakeConX(markWord::inline_type_pattern);\n+  Node* is_value = _igvn.transform(new AndXNode(mark, value_mask));\n+  Node* cmp = _igvn.transform(new CmpXNode(is_value, value_mask));\n+  Node* bol = _igvn.transform(new BoolNode(cmp, BoolTest::eq));\n+  Node* unc_ctrl = generate_slow_guard(ctrl, bol, nullptr);\n+\n+  int trap_request = Deoptimization::make_trap_request(Deoptimization::Reason_class_check, Deoptimization::Action_none);\n+  address call_addr = SharedRuntime::uncommon_trap_blob()->entry_point();\n+  const TypePtr* no_memory_effects = nullptr;\n+  CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\",\n+                                         no_memory_effects);\n+  unc->init_req(TypeFunc::Control, unc_ctrl);\n+  unc->init_req(TypeFunc::I_O, lock->i_o());\n+  unc->init_req(TypeFunc::Memory, lock->memory());\n+  unc->init_req(TypeFunc::FramePtr,  lock->in(TypeFunc::FramePtr));\n+  unc->init_req(TypeFunc::ReturnAdr, lock->in(TypeFunc::ReturnAdr));\n+  unc->init_req(TypeFunc::Parms+0, _igvn.intcon(trap_request));\n+  unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+  unc->copy_call_debug_info(&_igvn, lock);\n+\n+  assert(unc->peek_monitor_box() == lock->box_node(), \"wrong monitor\");\n+  assert((obj_type->is_inlinetypeptr() && unc->peek_monitor_obj()->is_SafePointScalarObject()) ||\n+         (obj->is_InlineType() && obj->in(1) == unc->peek_monitor_obj()) ||\n+         (obj == unc->peek_monitor_obj()), \"wrong monitor\");\n+\n+  \/\/ pop monitor and push obj back on stack: we trap before the monitorenter\n+  unc->pop_monitor();\n+  unc->grow_stack(unc->jvms(), 1);\n+  unc->set_stack(unc->jvms(), unc->jvms()->stk_size()-1, obj);\n+  _igvn.register_new_node_with_optimizer(unc);\n+\n+  unc_ctrl = _igvn.transform(new ProjNode(unc, TypeFunc::Control));\n+  Node* halt = _igvn.transform(new HaltNode(unc_ctrl, lock->in(TypeFunc::FramePtr), \"monitor enter on inline type\"));\n+  _igvn.add_input_to(C->root(), halt);\n+}\n+\n@@ -2126,1 +2318,1 @@\n-  alock->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alock->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2130,2 +2322,2 @@\n-         _callprojs.fallthrough_proj != nullptr &&\n-         _callprojs.fallthrough_memproj != nullptr,\n+         _callprojs->fallthrough_proj != nullptr &&\n+         _callprojs->fallthrough_memproj != nullptr,\n@@ -2134,2 +2326,2 @@\n-  Node* fallthroughproj = _callprojs.fallthrough_proj;\n-  Node* memproj_fallthrough = _callprojs.fallthrough_memproj;\n+  Node* fallthroughproj = _callprojs->fallthrough_proj;\n+  Node* memproj_fallthrough = _callprojs->fallthrough_memproj;\n@@ -2141,0 +2333,3 @@\n+    \/\/ Deoptimize and re-execute if object is an inline type\n+    inline_type_guard(&ctrl, alock->as_Lock());\n+\n@@ -2201,0 +2396,3 @@\n+  \/\/ Deoptimize and re-execute if object is an inline type\n+  inline_type_guard(&slow_path, lock);\n+\n@@ -2206,1 +2404,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2212,2 +2410,2 @@\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2218,1 +2416,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2220,2 +2418,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2225,1 +2423,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2233,1 +2431,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2266,3 +2464,3 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2274,1 +2472,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2276,2 +2474,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2281,1 +2479,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2288,1 +2486,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2291,0 +2489,211 @@\n+\/\/ An inline type might be returned from the call but we don't know its\n+\/\/ type. Either we get a buffered inline type (and nothing needs to be done)\n+\/\/ or one of the values being returned is the klass of the inline type\n+\/\/ and we need to allocate an inline type instance of that type and\n+\/\/ initialize it with other values being returned. In that case, we\n+\/\/ first try a fast path allocation and initialize the value with the\n+\/\/ inline klass's pack handler or we fall back to a runtime call.\n+void PhaseMacroExpand::expand_mh_intrinsic_return(CallStaticJavaNode* call) {\n+  assert(call->method()->is_method_handle_intrinsic(), \"must be a method handle intrinsic call\");\n+  Node* ret = call->proj_out_or_null(TypeFunc::Parms);\n+  if (ret == nullptr) {\n+    return;\n+  }\n+  const TypeFunc* tf = call->_tf;\n+  const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+  const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+  call->_tf = new_tf;\n+  \/\/ Make sure the change of type is applied before projections are processed by igvn\n+  _igvn.set_type(call, call->Value(&_igvn));\n+  _igvn.set_type(ret, ret->Value(&_igvn));\n+\n+  \/\/ Before any new projection is added:\n+  CallProjections* projs = call->extract_projections(true, true);\n+\n+  \/\/ Create temporary hook nodes that will be replaced below.\n+  \/\/ Add an input to prevent hook nodes from being dead.\n+  Node* ctl = new Node(call);\n+  Node* mem = new Node(ctl);\n+  Node* io = new Node(ctl);\n+  Node* ex_ctl = new Node(ctl);\n+  Node* ex_mem = new Node(ctl);\n+  Node* ex_io = new Node(ctl);\n+  Node* res = new Node(ctl);\n+\n+  \/\/ Allocate a new buffered inline type only if a new one is not returned\n+  Node* cast = transform_later(new CastP2XNode(ctl, res));\n+  Node* mask = MakeConX(0x1);\n+  Node* masked = transform_later(new AndXNode(cast, mask));\n+  Node* cmp = transform_later(new CmpXNode(masked, mask));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* allocation_iff = new IfNode(ctl, bol, PROB_MAX, COUNT_UNKNOWN);\n+  transform_later(allocation_iff);\n+  Node* allocation_ctl = transform_later(new IfTrueNode(allocation_iff));\n+  Node* no_allocation_ctl = transform_later(new IfFalseNode(allocation_iff));\n+  Node* no_allocation_res = transform_later(new CheckCastPPNode(no_allocation_ctl, res, TypeInstPtr::BOTTOM));\n+\n+  \/\/ Try to allocate a new buffered inline instance either from TLAB or eden space\n+  Node* needgc_ctrl = nullptr; \/\/ needgc means slowcase, i.e. allocation failed\n+  CallLeafNoFPNode* handler_call;\n+  const bool alloc_in_place = UseTLAB;\n+  if (alloc_in_place) {\n+    Node* fast_oop_ctrl = nullptr;\n+    Node* fast_oop_rawmem = nullptr;\n+    Node* mask2 = MakeConX(-2);\n+    Node* masked2 = transform_later(new AndXNode(cast, mask2));\n+    Node* rawklassptr = transform_later(new CastX2PNode(masked2));\n+    Node* klass_node = transform_later(new CheckCastPPNode(allocation_ctl, rawklassptr, TypeInstKlassPtr::OBJECT_OR_NULL));\n+    Node* layout_val = make_load(nullptr, mem, klass_node, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);\n+    Node* size_in_bytes = ConvI2X(layout_val);\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    Node* fast_oop = bs->obj_allocate(this, mem, allocation_ctl, size_in_bytes, io, needgc_ctrl,\n+                                      fast_oop_ctrl, fast_oop_rawmem,\n+                                      AllocateInstancePrefetchLines);\n+    \/\/ Allocation succeed, initialize buffered inline instance header firstly,\n+    \/\/ and then initialize its fields with an inline class specific handler\n+    Node* mark_node = makecon(TypeRawPtr::make((address)markWord::inline_type_prototype().value()));\n+    fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::mark_offset_in_bytes(), mark_node, T_ADDRESS);\n+    fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+    if (UseCompressedClassPointers) {\n+      fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_gap_offset_in_bytes(), intcon(0), T_INT);\n+    }\n+    Node* fixed_block  = make_load(fast_oop_ctrl, fast_oop_rawmem, klass_node, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    Node* pack_handler = make_load(fast_oop_ctrl, fast_oop_rawmem, fixed_block, in_bytes(InlineKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    handler_call = new CallLeafNoFPNode(OptoRuntime::pack_inline_type_Type(),\n+                                        nullptr,\n+                                        \"pack handler\",\n+                                        TypeRawPtr::BOTTOM);\n+    handler_call->init_req(TypeFunc::Control, fast_oop_ctrl);\n+    handler_call->init_req(TypeFunc::Memory, fast_oop_rawmem);\n+    handler_call->init_req(TypeFunc::I_O, top());\n+    handler_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+    handler_call->init_req(TypeFunc::ReturnAdr, top());\n+    handler_call->init_req(TypeFunc::Parms, pack_handler);\n+    handler_call->init_req(TypeFunc::Parms+1, fast_oop);\n+  } else {\n+    needgc_ctrl = allocation_ctl;\n+  }\n+\n+  \/\/ Allocation failed, fall back to a runtime call\n+  CallStaticJavaNode* slow_call = new CallStaticJavaNode(OptoRuntime::store_inline_type_fields_Type(),\n+                                                         StubRoutines::store_inline_type_fields_to_buf(),\n+                                                         \"store_inline_type_fields\",\n+                                                         TypePtr::BOTTOM);\n+  slow_call->init_req(TypeFunc::Control, needgc_ctrl);\n+  slow_call->init_req(TypeFunc::Memory, mem);\n+  slow_call->init_req(TypeFunc::I_O, io);\n+  slow_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+  slow_call->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  slow_call->init_req(TypeFunc::Parms, res);\n+\n+  Node* slow_ctl = transform_later(new ProjNode(slow_call, TypeFunc::Control));\n+  Node* slow_mem = transform_later(new ProjNode(slow_call, TypeFunc::Memory));\n+  Node* slow_io = transform_later(new ProjNode(slow_call, TypeFunc::I_O));\n+  Node* slow_res = transform_later(new ProjNode(slow_call, TypeFunc::Parms));\n+  Node* slow_catc = transform_later(new CatchNode(slow_ctl, slow_io, 2));\n+  Node* slow_norm = transform_later(new CatchProjNode(slow_catc, CatchProjNode::fall_through_index, CatchProjNode::no_handler_bci));\n+  Node* slow_excp = transform_later(new CatchProjNode(slow_catc, CatchProjNode::catch_all_index,    CatchProjNode::no_handler_bci));\n+\n+  Node* ex_r = new RegionNode(3);\n+  Node* ex_mem_phi = new PhiNode(ex_r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* ex_io_phi = new PhiNode(ex_r, Type::ABIO);\n+  ex_r->init_req(1, slow_excp);\n+  ex_mem_phi->init_req(1, slow_mem);\n+  ex_io_phi->init_req(1, slow_io);\n+  ex_r->init_req(2, ex_ctl);\n+  ex_mem_phi->init_req(2, ex_mem);\n+  ex_io_phi->init_req(2, ex_io);\n+  transform_later(ex_r);\n+  transform_later(ex_mem_phi);\n+  transform_later(ex_io_phi);\n+\n+  \/\/ We don't know how many values are returned. This assumes the\n+  \/\/ worst case, that all available registers are used.\n+  for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+    if (domain->field_at(i) == Type::HALF) {\n+      slow_call->init_req(i, top());\n+      if (alloc_in_place) {\n+        handler_call->init_req(i+1, top());\n+      }\n+      continue;\n+    }\n+    Node* proj = transform_later(new ProjNode(call, i));\n+    slow_call->init_req(i, proj);\n+    if (alloc_in_place) {\n+      handler_call->init_req(i+1, proj);\n+    }\n+  }\n+  \/\/ We can safepoint at that new call\n+  slow_call->copy_call_debug_info(&_igvn, call);\n+  transform_later(slow_call);\n+  if (alloc_in_place) {\n+    transform_later(handler_call);\n+  }\n+\n+  Node* fast_ctl = nullptr;\n+  Node* fast_res = nullptr;\n+  MergeMemNode* fast_mem = nullptr;\n+  if (alloc_in_place) {\n+    fast_ctl = transform_later(new ProjNode(handler_call, TypeFunc::Control));\n+    Node* rawmem = transform_later(new ProjNode(handler_call, TypeFunc::Memory));\n+    fast_res = transform_later(new ProjNode(handler_call, TypeFunc::Parms));\n+    fast_mem = MergeMemNode::make(mem);\n+    fast_mem->set_memory_at(Compile::AliasIdxRaw, rawmem);\n+    transform_later(fast_mem);\n+  }\n+\n+  Node* r = new RegionNode(alloc_in_place ? 4 : 3);\n+  Node* mem_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* io_phi = new PhiNode(r, Type::ABIO);\n+  Node* res_phi = new PhiNode(r, TypeInstPtr::BOTTOM);\n+  r->init_req(1, no_allocation_ctl);\n+  mem_phi->init_req(1, mem);\n+  io_phi->init_req(1, io);\n+  res_phi->init_req(1, no_allocation_res);\n+  r->init_req(2, slow_norm);\n+  mem_phi->init_req(2, slow_mem);\n+  io_phi->init_req(2, slow_io);\n+  res_phi->init_req(2, slow_res);\n+  if (alloc_in_place) {\n+    r->init_req(3, fast_ctl);\n+    mem_phi->init_req(3, fast_mem);\n+    io_phi->init_req(3, io);\n+    res_phi->init_req(3, fast_res);\n+  }\n+  transform_later(r);\n+  transform_later(mem_phi);\n+  transform_later(io_phi);\n+  transform_later(res_phi);\n+\n+  \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+  \/\/ store that would make this buffer accessible by other threads.\n+  MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);\n+  transform_later(mb);\n+  mb->init_req(TypeFunc::Memory, mem_phi);\n+  mb->init_req(TypeFunc::Control, r);\n+  r = new ProjNode(mb, TypeFunc::Control);\n+  transform_later(r);\n+  mem_phi = new ProjNode(mb, TypeFunc::Memory);\n+  transform_later(mem_phi);\n+\n+  assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+  _igvn.replace_in_uses(projs->fallthrough_catchproj, r);\n+  _igvn.replace_in_uses(projs->fallthrough_memproj, mem_phi);\n+  _igvn.replace_in_uses(projs->fallthrough_ioproj, io_phi);\n+  _igvn.replace_in_uses(projs->resproj[0], res_phi);\n+  _igvn.replace_in_uses(projs->catchall_catchproj, ex_r);\n+  _igvn.replace_in_uses(projs->catchall_memproj, ex_mem_phi);\n+  _igvn.replace_in_uses(projs->catchall_ioproj, ex_io_phi);\n+  \/\/ The CatchNode should not use the ex_io_phi. Re-connect it to the catchall_ioproj.\n+  Node* cn = projs->fallthrough_catchproj->in(0);\n+  _igvn.replace_input_of(cn, 1, projs->catchall_ioproj);\n+\n+  _igvn.replace_node(ctl, projs->fallthrough_catchproj);\n+  _igvn.replace_node(mem, projs->fallthrough_memproj);\n+  _igvn.replace_node(io, projs->fallthrough_ioproj);\n+  _igvn.replace_node(res, projs->resproj[0]);\n+  _igvn.replace_node(ex_ctl, projs->catchall_catchproj);\n+  _igvn.replace_node(ex_mem, projs->catchall_memproj);\n+  _igvn.replace_node(ex_io, projs->catchall_ioproj);\n+ }\n+\n@@ -2316,1 +2725,1 @@\n-      subklass = _igvn.transform(LoadKlassNode::make(_igvn, nullptr, C->immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+      subklass = _igvn.transform(LoadKlassNode::make(_igvn, nullptr, C->immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -2328,0 +2737,101 @@\n+\/\/ FlatArrayCheckNode (array1 array2 ...) is expanded into:\n+\/\/\n+\/\/ long mark = array1.mark | array2.mark | ...;\n+\/\/ long locked_bit = markWord::unlocked_value & array1.mark & array2.mark & ...;\n+\/\/ if (locked_bit == 0) {\n+\/\/   \/\/ One array is locked, load prototype header from the klass\n+\/\/   mark = array1.klass.proto | array2.klass.proto | ...\n+\/\/ }\n+\/\/ if ((mark & markWord::flat_array_bit_in_place) == 0) {\n+\/\/    ...\n+\/\/ }\n+void PhaseMacroExpand::expand_flatarraycheck_node(FlatArrayCheckNode* check) {\n+  bool array_inputs = _igvn.type(check->in(FlatArrayCheckNode::ArrayOrKlass))->isa_oopptr() != nullptr;\n+  if (UseArrayMarkWordCheck && array_inputs) {\n+    Node* mark = MakeConX(0);\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    Node* mem = check->in(FlatArrayCheckNode::Memory);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* ary = check->in(i);\n+      const TypeOopPtr* t = _igvn.type(ary)->isa_oopptr();\n+      assert(t != nullptr, \"Mixing array and klass inputs\");\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      Node* mark_adr = basic_plus_adr(ary, oopDesc::mark_offset_in_bytes());\n+      Node* mark_load = _igvn.transform(LoadNode::make(_igvn, nullptr, mem, mark_adr, mark_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+      mark = _igvn.transform(new OrXNode(mark, mark_load));\n+      locked_bit = _igvn.transform(new AndXNode(locked_bit, mark_load));\n+    }\n+    assert(!mark->is_Con(), \"Should have been optimized out\");\n+    Node* cmp = _igvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _igvn.transform(new BoolNode(cmp, BoolTest::ne));\n+\n+    \/\/ BoolNode might be shared, replace each if user\n+    Node* old_bol = check->unique_out();\n+    assert(old_bol->is_Bool() && old_bol->as_Bool()->_test._test == BoolTest::ne, \"unexpected condition\");\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      IfNode* old_iff = old_bol->last_out(i)->as_If();\n+      Node* ctrl = old_iff->in(0);\n+      RegionNode* region = new RegionNode(3);\n+      Node* mark_phi = new PhiNode(region, TypeX_X);\n+\n+      \/\/ Check if array is unlocked\n+      IfNode* iff = _igvn.transform(new IfNode(ctrl, is_unlocked, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+\n+      \/\/ Unlocked: Use bits from mark word\n+      region->init_req(1, _igvn.transform(new IfTrueNode(iff)));\n+      mark_phi->init_req(1, mark);\n+\n+      \/\/ Locked: Load prototype header from klass\n+      ctrl = _igvn.transform(new IfFalseNode(iff));\n+      Node* proto = MakeConX(0);\n+      for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+        Node* ary = check->in(i);\n+        \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+        Node* klass_adr = basic_plus_adr(ary, oopDesc::klass_offset_in_bytes());\n+        Node* klass = _igvn.transform(LoadKlassNode::make(_igvn, ctrl, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+        Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+        Node* proto_load = _igvn.transform(LoadNode::make(_igvn, ctrl, C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+        proto = _igvn.transform(new OrXNode(proto, proto_load));\n+      }\n+      region->init_req(2, ctrl);\n+      mark_phi->init_req(2, proto);\n+\n+      \/\/ Check if flat array bits are set\n+      Node* mask = MakeConX(markWord::flat_array_bit_in_place);\n+      Node* masked = _igvn.transform(new AndXNode(_igvn.transform(mark_phi), mask));\n+      cmp = _igvn.transform(new CmpXNode(masked, MakeConX(0)));\n+      Node* is_not_flat = _igvn.transform(new BoolNode(cmp, BoolTest::eq));\n+\n+      ctrl = _igvn.transform(region);\n+      iff = _igvn.transform(new IfNode(ctrl, is_not_flat, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+      _igvn.replace_node(old_iff, iff);\n+    }\n+    _igvn.replace_node(check, C->top());\n+  } else {\n+    \/\/ Fall back to layout helper check\n+    Node* lhs = intcon(0);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* array_or_klass = check->in(i);\n+      Node* klass = nullptr;\n+      const TypePtr* t = _igvn.type(array_or_klass)->is_ptr();\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      if (t->isa_oopptr() != nullptr) {\n+        Node* klass_adr = basic_plus_adr(array_or_klass, oopDesc::klass_offset_in_bytes());\n+        klass = transform_later(LoadKlassNode::make(_igvn, nullptr, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+      } else {\n+        assert(t->isa_aryklassptr(), \"Unexpected input type\");\n+        klass = array_or_klass;\n+      }\n+      Node* lh_addr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+      Node* lh_val = _igvn.transform(LoadNode::make(_igvn, nullptr, C->immutable_memory(), lh_addr, lh_addr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+      lhs = _igvn.transform(new OrINode(lhs, lh_val));\n+    }\n+    Node* masked = transform_later(new AndINode(lhs, intcon(Klass::_lh_array_tag_flat_value_bit_inplace)));\n+    Node* cmp = transform_later(new CmpINode(masked, intcon(0)));\n+    Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+    Node* old_bol = check->unique_out();\n+    _igvn.replace_node(old_bol, bol);\n+    _igvn.replace_node(check, C->top());\n+  }\n+}\n+\n@@ -2388,2 +2898,5 @@\n-      case Node::Class_CallStaticJava:\n-        success = eliminate_boxing_node(n->as_CallStaticJava());\n+      case Node::Class_CallStaticJava: {\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          success = eliminate_boxing_node(n->as_CallStaticJava());\n+        }\n@@ -2391,0 +2904,1 @@\n+      }\n@@ -2404,0 +2918,2 @@\n+      case Node::Class_FlatArrayCheck:\n+        break;\n@@ -2446,4 +2962,7 @@\n-        \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n-        C->remove_macro_node(n);\n-        _igvn._worklist.push(n);\n-        success = true;\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n+          C->remove_macro_node(n);\n+          _igvn._worklist.push(n);\n+          success = true;\n+        }\n@@ -2564,0 +3083,7 @@\n+    case Node::Class_CallStaticJava:\n+      expand_mh_intrinsic_return(n->as_CallStaticJava());\n+      C->remove_macro_node(n);\n+      break;\n+    case Node::Class_FlatArrayCheck:\n+      expand_flatarraycheck_node(n->as_FlatArrayCheck());\n+      break;\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":707,"deletions":181,"binary":false,"changes":888,"status":"modified"},{"patch":"@@ -84,1 +84,1 @@\n-  CallProjections _callprojs;\n+  CallProjections* _callprojs;\n@@ -99,0 +99,1 @@\n+  Node* inline_type_from_mem(Node* mem, Node* ctl, ciInlineKlass* vk, const TypeAryPtr* adr_type, int offset, AllocateNode* alloc);\n@@ -104,1 +105,1 @@\n-  void process_users_of_allocation(CallNode *alloc);\n+  void process_users_of_allocation(CallNode *alloc, bool inline_alloc = false);\n@@ -111,0 +112,1 @@\n+  void inline_type_guard(Node** ctrl, LockNode* lock);\n@@ -112,0 +114,1 @@\n+  void expand_mh_intrinsic_return(CallStaticJavaNode* call);\n@@ -121,0 +124,1 @@\n+  Node* generate_fair_guard(Node** ctrl, Node* test, RegionNode* region);\n@@ -131,0 +135,4 @@\n+  Node* array_lh_test(Node* array, jint mask);\n+  Node* generate_flat_array_guard(Node** ctrl, Node* array, RegionNode* region);\n+  Node* generate_null_free_array_guard(Node** ctrl, Node* array, RegionNode* region);\n+\n@@ -140,0 +148,1 @@\n+                           Node* dest_length,\n@@ -146,0 +155,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -181,1 +192,3 @@\n-\n+  const TypePtr* adjust_for_flat_array(const TypeAryPtr* top_dest, Node*& src_offset,\n+                                       Node*& dest_offset, Node*& length, BasicType& dest_elem,\n+                                       Node*& dest_length);\n@@ -186,0 +199,2 @@\n+  void expand_flatarraycheck_node(FlatArrayCheckNode* check);\n+\n@@ -208,1 +223,1 @@\n-  SafePointScalarObjectNode* create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt);\n+  SafePointScalarObjectNode* create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt, Unique_Node_List* value_worklist);\n","filename":"src\/hotspot\/share\/opto\/macro.hpp","additions":19,"deletions":4,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -278,0 +278,2 @@\n+  RegMask* return_values_mask(const TypeFunc* tf);\n+\n@@ -409,1 +411,1 @@\n-  RegMask                     _return_value_mask;\n+  RegMask*            _return_values_mask;\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -41,0 +43,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -234,0 +237,2 @@\n+                     ->cast_to_not_flat(t_oop->is_aryptr()->is_not_flat())\n+                     ->cast_to_not_null_free(t_oop->is_aryptr()->is_not_null_free())\n@@ -260,1 +265,1 @@\n-               tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n+        tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n@@ -928,0 +933,1 @@\n+  case T_PRIMITIVE_OBJECT:\n@@ -1023,1 +1029,1 @@\n-      uint shift  = exact_log2(type2aelembytes(ary_elem));\n+      uint shift  = ary_t->is_flat() ? ary_t->flat_log_elem_size() : exact_log2(type2aelembytes(ary_elem));\n@@ -1144,1 +1150,1 @@\n-        const TypeVect* out_vt = as_LoadVector()->vect_type();\n+        const TypeVect* out_vt = is_Load() ? as_LoadVector()->vect_type() : as_StoreVector()->vect_type();\n@@ -1162,0 +1168,6 @@\n+      assert(memory_type() != T_PRIMITIVE_OBJECT, \"should not be used for inline types\");\n+      Node* default_value = ld_alloc->in(AllocateNode::DefaultValue);\n+      if (default_value != nullptr) {\n+        return default_value;\n+      }\n+      assert(ld_alloc->in(AllocateNode::RawDefaultValue) == nullptr, \"default value may not be null\");\n@@ -1229,0 +1241,17 @@\n+  \/\/ Loading from an InlineType? The InlineType has the values of\n+  \/\/ all fields as input. Look for the field with matching offset.\n+  Node* addr = in(Address);\n+  intptr_t offset;\n+  Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);\n+  if (base != nullptr && base->is_InlineType() && offset > oopDesc::klass_offset_in_bytes()) {\n+    Node* value = base->as_InlineType()->field_value_by_offset((int)offset, true);\n+    if (value != nullptr) {\n+      if (Opcode() == Op_LoadN) {\n+        \/\/ Encode oop value if we are loading a narrow oop\n+        assert(!phase->type(value)->isa_narrowoop(), \"should already be decoded\");\n+        value = phase->transform(new EncodePNode(value, bottom_type()));\n+      }\n+      return value;\n+    }\n+  }\n+\n@@ -1987,0 +2016,1 @@\n+        && !ary->is_flat()\n@@ -2022,0 +2052,2 @@\n+            \/\/ Default value load\n+            tp->is_instptr()->instance_klass() == ciEnv::current()->Class_klass() ||\n@@ -2027,1 +2059,3 @@\n-    \/\/ Optimize loads from constant fields.\n+    BasicType bt = memory_type();\n+\n+    \/\/ Optimize loads from constant fields.\n@@ -2031,1 +2065,19 @@\n-      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), memory_type());\n+      ciType* mirror_type = const_oop->as_instance()->java_mirror_type();\n+      if (mirror_type != nullptr) {\n+        const Type* const_oop = nullptr;\n+        ciInlineKlass* vk = mirror_type->is_inlinetype() ? mirror_type->as_inline_klass() : nullptr;\n+        \/\/ Fold default value loads\n+        if (vk != nullptr && off == vk->default_value_offset()) {\n+          const_oop = TypeInstPtr::make(vk->default_instance());\n+        }\n+        \/\/ Fold class mirror loads\n+        if (off == java_lang_Class::primary_mirror_offset()) {\n+          const_oop = (vk == nullptr) ? TypePtr::NULL_PTR : TypeInstPtr::make(vk->ref_instance());\n+        } else if (off == java_lang_Class::secondary_mirror_offset()) {\n+          const_oop = (vk == nullptr) ? TypePtr::NULL_PTR : TypeInstPtr::make(vk->val_instance());\n+        }\n+        if (const_oop != nullptr) {\n+          return (bt == T_NARROWOOP) ? const_oop->make_narrowoop() : const_oop;\n+        }\n+      }\n+      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), bt);\n@@ -2046,15 +2098,31 @@\n-  } else if (tp->base() == Type::RawPtr && adr->is_Load() && off == 0) {\n-    \/* With mirrors being an indirect in the Klass*\n-     * the VM is now using two loads. LoadKlass(LoadP(LoadP(Klass, mirror_offset), zero_offset))\n-     * The LoadP from the Klass has a RawPtr type (see LibraryCallKit::load_mirror_from_klass).\n-     *\n-     * So check the type and klass of the node before the LoadP.\n-     *\/\n-    Node* adr2 = adr->in(MemNode::Address);\n-    const TypeKlassPtr* tkls = phase->type(adr2)->isa_klassptr();\n-    if (tkls != nullptr && !StressReflectiveCode) {\n-      if (tkls->is_loaded() && tkls->klass_is_exact() && tkls->offset() == in_bytes(Klass::java_mirror_offset())) {\n-        ciKlass* klass = tkls->exact_klass();\n-        assert(adr->Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n-        assert(Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n-        return TypeInstPtr::make(klass->java_mirror());\n+  } else if (tp->base() == Type::RawPtr && !StressReflectiveCode) {\n+    if (adr->is_Load() && off == 0) {\n+      \/* With mirrors being an indirect in the Klass*\n+       * the VM is now using two loads. LoadKlass(LoadP(LoadP(Klass, mirror_offset), zero_offset))\n+       * The LoadP from the Klass has a RawPtr type (see LibraryCallKit::load_mirror_from_klass).\n+       *\n+       * So check the type and klass of the node before the LoadP.\n+       *\/\n+      Node* adr2 = adr->in(MemNode::Address);\n+      const TypeKlassPtr* tkls = phase->type(adr2)->isa_klassptr();\n+      if (tkls != nullptr) {\n+        if (tkls->is_loaded() && tkls->klass_is_exact() && tkls->offset() == in_bytes(Klass::java_mirror_offset())) {\n+          ciKlass* klass = tkls->exact_klass();\n+          assert(adr->Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n+          assert(Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n+          return TypeInstPtr::make(klass->java_mirror());\n+        }\n+      }\n+    } else {\n+      \/\/ Check for a load of the default value offset from the InlineKlassFixedBlock:\n+      \/\/ LoadI(LoadP(inline_klass, adr_inlineklass_fixed_block_offset), default_value_offset_offset)\n+      intptr_t offset = 0;\n+      Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+      if (base != nullptr && base->is_Load() && offset == in_bytes(InlineKlass::default_value_offset_offset())) {\n+        const TypeKlassPtr* tkls = phase->type(base->in(MemNode::Address))->isa_klassptr();\n+        if (tkls != nullptr && tkls->is_loaded() && tkls->klass_is_exact() && tkls->exact_klass()->is_inlinetype() &&\n+            tkls->offset() == in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())) {\n+          assert(base->Opcode() == Op_LoadP, \"must load an oop from klass\");\n+          assert(Opcode() == Op_LoadI, \"must load an int from fixed block\");\n+          return TypeInt::make(tkls->exact_klass()->as_inline_klass()->default_value_offset());\n+        }\n@@ -2166,1 +2234,0 @@\n-\n@@ -2169,1 +2236,10 @@\n-    return TypeX::make(markWord::prototype().value());\n+    if (EnableValhalla) {\n+      \/\/ The mark word may contain property bits (inline, flat, null-free)\n+      Node* klass_node = alloc->in(AllocateNode::KlassNode);\n+      const TypeKlassPtr* tkls = phase->type(klass_node)->isa_klassptr();\n+      if (tkls != nullptr && tkls->is_loaded() && tkls->klass_is_exact()) {\n+        return TypeX::make(tkls->exact_klass()->prototype_header().value());\n+      }\n+    } else {\n+      return TypeX::make(markWord::prototype().value());\n+    }\n@@ -2320,1 +2396,2 @@\n-Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at, const TypeKlassPtr* tk) {\n+Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at,\n+                          const TypeKlassPtr* tk) {\n@@ -2367,1 +2444,2 @@\n-      ciType* t = tinst->java_mirror_type();\n+      bool null_free = false;\n+      ciType* t = tinst->java_mirror_type(&null_free);\n@@ -2377,1 +2455,1 @@\n-          return TypeKlassPtr::make(ciArrayKlass::make(t), Type::trust_interfaces);\n+          return TypeKlassPtr::make(ciArrayKlass::make(t, null_free), Type::trust_interfaces);\n@@ -2396,1 +2474,1 @@\n-  const TypeAryPtr *tary = tp->isa_aryptr();\n+  const TypeAryPtr* tary = tp->isa_aryptr();\n@@ -2622,0 +2700,1 @@\n+  case T_PRIMITIVE_OBJECT:\n@@ -2672,1 +2751,1 @@\n-  {\n+  if (phase->C->get_adr_type(phase->C->get_alias_index(adr_type())) != TypeAryPtr::INLINES) {\n@@ -2692,0 +2771,1 @@\n+             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreN) ||\n@@ -2788,2 +2868,1 @@\n-  if (result == this &&\n-      ReduceFieldZeroing && phase->type(val)->is_zero_type()) {\n+  if (result == this && ReduceFieldZeroing) {\n@@ -2791,1 +2870,2 @@\n-    if (mem->is_Proj() && mem->in(0)->is_Allocate()) {\n+    if (mem->is_Proj() && mem->in(0)->is_Allocate() &&\n+        (phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::DefaultValue) == val)) {\n@@ -2795,1 +2875,1 @@\n-    if (result == this) {\n+    if (result == this && phase->type(val)->is_zero_type()) {\n@@ -2980,3 +3060,7 @@\n-    Node* mem = my_store->as_MergeMem()->memory_at(oop_alias_idx());\n-    set_req_X(MemNode::OopStore, mem, phase);\n-    return this;\n+    if (oop_alias_idx() != phase->C->get_alias_index(TypeAryPtr::INLINES) ||\n+        phase->C->flattened_accesses_share_alias()) {\n+      \/\/ The alias that was recorded is no longer accurate enough.\n+      Node* mem = my_store->as_MergeMem()->memory_at(oop_alias_idx());\n+      set_req_X(MemNode::OopStore, mem, phase);\n+      return this;\n+    }\n@@ -3153,1 +3237,1 @@\n-    return new ClearArrayNode(in(0), in(1), in(2), in(3), true);\n+    return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);\n@@ -3171,1 +3255,1 @@\n-  Node *zero = phase->makecon(TypeLong::ZERO);\n+  Node *val = in(4);\n@@ -3173,1 +3257,1 @@\n-  mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+  mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -3178,1 +3262,1 @@\n-    mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+    mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -3212,0 +3296,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -3222,1 +3308,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -3229,1 +3321,1 @@\n-  return clear_memory(ctl, mem, dest, phase->MakeConX(offset), end_offset, phase);\n+  return clear_memory(ctl, mem, dest, raw_val, phase->MakeConX(offset), end_offset, phase);\n@@ -3233,0 +3325,1 @@\n+                                   Node* raw_val,\n@@ -3255,1 +3348,4 @@\n-  mem = new ClearArrayNode(ctl, mem, zsize, adr, false);\n+  if (raw_val == nullptr) {\n+    raw_val = phase->MakeConX(0);\n+  }\n+  mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);\n@@ -3260,0 +3356,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -3274,1 +3372,1 @@\n-    mem = clear_memory(ctl, mem, dest,\n+    mem = clear_memory(ctl, mem, dest, val, raw_val,\n@@ -3281,1 +3379,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -3426,1 +3530,1 @@\n-Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {\n+Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {\n@@ -3713,1 +3817,3 @@\n-  if (init == nullptr || init->is_complete())  return false;\n+  if (init == nullptr || init->is_complete()) {\n+    return false;\n+  }\n@@ -3891,0 +3997,6 @@\n+                if (base->is_Phi()) {\n+                  \/\/ In rare case, base may be a PhiNode and it may read\n+                  \/\/ the same memory slice between InitializeNode and store.\n+                  failed = true;\n+                  break;\n+                }\n@@ -4477,0 +4589,2 @@\n+                                              allocation()->in(AllocateNode::DefaultValue),\n+                                              allocation()->in(AllocateNode::RawDefaultValue),\n@@ -4536,0 +4650,2 @@\n+                                            allocation()->in(AllocateNode::DefaultValue),\n+                                            allocation()->in(AllocateNode::RawDefaultValue),\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":161,"deletions":45,"binary":false,"changes":206,"status":"modified"},{"patch":"@@ -129,0 +129,4 @@\n+#ifdef ASSERT\n+  void set_adr_type(const TypePtr* adr_type) { _adr_type = adr_type; }\n+#endif\n+\n@@ -551,1 +555,0 @@\n-\n@@ -1090,0 +1093,1 @@\n+  bool _word_copy_only;\n@@ -1091,2 +1095,3 @@\n-  ClearArrayNode( Node *ctrl, Node *arymem, Node *word_cnt, Node *base, bool is_large)\n-    : Node(ctrl,arymem,word_cnt,base), _is_large(is_large) {\n+  ClearArrayNode( Node *ctrl, Node *arymem, Node *word_cnt, Node *base, Node* val, bool is_large)\n+    : Node(ctrl, arymem, word_cnt, base, val), _is_large(is_large),\n+      _word_copy_only(val->bottom_type()->isa_long() && (!val->bottom_type()->is_long()->is_con() || val->bottom_type()->is_long()->get_con() != 0)) {\n@@ -1104,0 +1109,1 @@\n+  bool word_copy_only() const { return _word_copy_only; }\n@@ -1110,0 +1116,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -1114,0 +1122,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -1118,0 +1128,1 @@\n+                            Node* raw_val,\n@@ -1169,1 +1180,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+class FlatArrayCheckNode;\n@@ -117,0 +118,1 @@\n+class MachPrologNode;\n@@ -123,0 +125,1 @@\n+class MachVEPNode;\n@@ -170,0 +173,1 @@\n+class InlineTypeNode;\n@@ -679,0 +683,1 @@\n+        DEFINE_CLASS_ID(Blackhole,        MemBar, 2)\n@@ -700,0 +705,2 @@\n+      DEFINE_CLASS_ID(MachProlog,       Mach, 8)\n+      DEFINE_CLASS_ID(MachVEP,          Mach, 9)\n@@ -726,1 +733,2 @@\n-        DEFINE_CLASS_ID(Reduction, Vector, 7)\n+      DEFINE_CLASS_ID(InlineType, Type, 8)\n+        DEFINE_CLASS_ID(Reduction, Vector, 9)\n@@ -728,1 +736,1 @@\n-      DEFINE_CLASS_ID(Con, Type, 8)\n+      DEFINE_CLASS_ID(Con, Type, 10)\n@@ -768,3 +776,4 @@\n-        DEFINE_CLASS_ID(FastLock,   Cmp, 0)\n-        DEFINE_CLASS_ID(FastUnlock, Cmp, 1)\n-        DEFINE_CLASS_ID(SubTypeCheck,Cmp, 2)\n+        DEFINE_CLASS_ID(FastLock,       Cmp, 0)\n+        DEFINE_CLASS_ID(FastUnlock,     Cmp, 1)\n+        DEFINE_CLASS_ID(SubTypeCheck,   Cmp, 2)\n+        DEFINE_CLASS_ID(FlatArrayCheck, Cmp, 3)\n@@ -869,0 +878,1 @@\n+  DEFINE_CLASS_QUERY(Blackhole)\n@@ -898,0 +908,1 @@\n+  DEFINE_CLASS_QUERY(FlatArrayCheck)\n@@ -930,0 +941,1 @@\n+  DEFINE_CLASS_QUERY(MachProlog)\n@@ -936,0 +948,1 @@\n+  DEFINE_CLASS_QUERY(MachVEP)\n@@ -964,0 +977,1 @@\n+  DEFINE_CLASS_QUERY(InlineType)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":19,"deletions":5,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -244,1 +245,9 @@\n-    _orig_pc_slot = C->fixed_slots() - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n+    int fixed_slots = C->fixed_slots();\n+    if (C->needs_stack_repair()) {\n+      fixed_slots -= 2;\n+    }\n+    \/\/ TODO 8284443 Only reserve extra slot if needed\n+    if (InlineTypeReturnedAsFields) {\n+      fixed_slots -= 2;\n+    }\n+    _orig_pc_slot = fixed_slots - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n@@ -285,1 +294,2 @@\n-  MachPrologNode *prolog = new MachPrologNode();\n+  Label verified_entry;\n+  MachPrologNode* prolog = new MachPrologNode(&verified_entry);\n@@ -291,3 +301,2 @@\n-\n-  if( C->is_osr_compilation() ) {\n-    if( PoisonOSREntry ) {\n+  if (C->is_osr_compilation()) {\n+    if (PoisonOSREntry) {\n@@ -298,3 +307,14 @@\n-    if( C->method() && !C->method()->flags().is_static() ) {\n-      \/\/ Insert unvalidated entry point\n-      C->cfg()->insert( broot, 0, new MachUEPNode() );\n+    if (C->method()) {\n+      if (C->method()->has_scalarized_args()) {\n+        \/\/ Add entry point to unpack all inline type arguments\n+        C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true, \/* receiver_only *\/ false));\n+        if (!C->method()->is_static()) {\n+          \/\/ Add verified\/unverified entry points to only unpack inline type receiver at interface calls\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ false));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true,  \/* receiver_only *\/ true));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ true));\n+        }\n+      } else if (!C->method()->is_static()) {\n+        \/\/ Insert unvalidated entry point\n+        C->cfg()->insert(broot, 0, new MachUEPNode());\n+      }\n@@ -302,1 +322,0 @@\n-\n@@ -342,0 +361,25 @@\n+  if (!C->is_osr_compilation() && C->has_scalarized_args()) {\n+    \/\/ Compute the offsets of the entry points required by the inline type calling convention\n+    if (!C->method()->is_static()) {\n+      \/\/ We have entries at the beginning of the method, implemented by the first 4 nodes.\n+      \/\/ Entry                     (unverified) @ offset 0\n+      \/\/ Verified_Inline_Entry_RO\n+      \/\/ Inline_Entry              (unverified)\n+      \/\/ Verified_Inline_Entry\n+      uint offset = 0;\n+      _code_offsets.set_value(CodeOffsets::Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(0))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(1))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Inline_Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(2))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, offset);\n+    } else {\n+      _code_offsets.set_value(CodeOffsets::Entry, -1); \/\/ will be patched later\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, 0);\n+    }\n+  }\n+\n@@ -502,1 +546,3 @@\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != nullptr) {\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -757,0 +803,17 @@\n+      uint first_ind = spobj->first_index(sfpt->jvms());\n+      \/\/ Nullable, scalarized inline types have an is_init input\n+      \/\/ that needs to be checked before using the field values.\n+      ScopeValue* is_init = nullptr;\n+      if (cik->is_inlinetype()) {\n+        Node* init_node = sfpt->in(first_ind++);\n+        assert(init_node != nullptr, \"is_init node not found\");\n+        if (!init_node->is_top()) {\n+          const TypeInt* init_type = init_node->bottom_type()->is_int();\n+          if (init_node->is_Con()) {\n+            is_init = new ConstantIntValue(init_type->get_con());\n+          } else {\n+            OptoReg::Name init_reg = C->regalloc()->get_reg_first(init_node);\n+            is_init = new_loc_value(C->regalloc(), init_reg, Location::normal);\n+          }\n+        }\n+      }\n@@ -758,1 +821,1 @@\n-                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()));\n+                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), is_init);\n@@ -761,1 +824,0 @@\n-      uint first_ind = spobj->first_index(sfpt->jvms());\n@@ -980,0 +1042,1 @@\n+  bool return_scalarized = false;\n@@ -1000,1 +1063,1 @@\n-    if (mcall->returns_pointer()) {\n+    if (mcall->returns_pointer() || mcall->returns_scalarized()) {\n@@ -1003,0 +1066,3 @@\n+    if (mcall->returns_scalarized()) {\n+      return_scalarized = true;\n+    }\n@@ -1141,0 +1207,1 @@\n+      return_scalarized,\n@@ -1516,2 +1583,4 @@\n-          \/\/ This destination address is NOT PC-relative\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != nullptr) {\n+            \/\/ This destination address is NOT PC-relative\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -1681,1 +1750,0 @@\n-\n@@ -3082,0 +3150,13 @@\n+\n+      \/\/ Do not allow a CheckCastPP node whose input is a raw pointer to\n+      \/\/ float past a safepoint.  This can occur when a buffered inline\n+      \/\/ type is allocated in a loop and the CheckCastPP from that\n+      \/\/ allocation is reused outside the loop.  If the use inside the\n+      \/\/ loop is scalarized the CheckCastPP will no longer be connected\n+      \/\/ to the loop safepoint.  See JDK-8264340.\n+      if (m->is_Mach() && m->as_Mach()->ideal_Opcode() == Op_CheckCastPP) {\n+        Node *def = m->in(1);\n+        if (def != nullptr && def->bottom_type()->base() == Type::RawPtr) {\n+          last_safept_node->add_prec(m);\n+        }\n+      }\n@@ -3240,0 +3321,19 @@\n+    if (C->has_scalarized_args()) {\n+      \/\/ Inline type entry points (MachVEPNodes) require lots of space for GC barriers and oop verification\n+      \/\/ when loading object fields from the buffered argument. Increase scratch buffer size accordingly.\n+      ciMethod* method = C->method();\n+      int barrier_size = UseZGC ? 200 : (7 DEBUG_ONLY(+ 37));\n+      int arg_num = 0;\n+      if (!method->is_static()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += method->holder()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+      for (ciSignatureStream str(method->signature()); !str.at_return_type(); str.next()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += str.type()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+    }\n@@ -3310,1 +3410,2 @@\n-  if (is_branch) \/\/ Restore label.\n+  \/\/ Restore label.\n+  if (is_branch) {\n@@ -3312,0 +3413,1 @@\n+  }\n@@ -3355,0 +3457,9 @@\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry_RO) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);\n+      }\n@@ -3359,14 +3470,14 @@\n-                                     entry_bci,\n-                                     &_code_offsets,\n-                                     _orig_pc_slot_offset_in_bytes,\n-                                     code_buffer(),\n-                                     frame_size_in_words(),\n-                                     oop_map_set(),\n-                                     &_handler_table,\n-                                     inc_table(),\n-                                     compiler,\n-                                     has_unsafe_access,\n-                                     SharedRuntime::is_wide_vector(C->max_vector_size()),\n-                                     C->has_monitors(),\n-                                     0,\n-                                     C->rtm_state());\n+                              entry_bci,\n+                              &_code_offsets,\n+                              _orig_pc_slot_offset_in_bytes,\n+                              code_buffer(),\n+                              frame_size_in_words(),\n+                              _oop_map_set,\n+                              &_handler_table,\n+                              inc_table(),\n+                              compiler,\n+                              has_unsafe_access,\n+                              SharedRuntime::is_wide_vector(C->max_vector_size()),\n+                              C->has_monitors(),\n+                              0,\n+                              C->rtm_state());\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":142,"deletions":31,"binary":false,"changes":173,"status":"modified"},{"patch":"@@ -209,1 +209,1 @@\n-      for (uint i = TypeFunc::Parms; i < call->tf()->domain()->cnt(); i++) {\n+      for (uint i = TypeFunc::Parms; i < call->tf()->domain_sig()->cnt(); i++) {\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -62,0 +63,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -68,0 +70,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -1895,0 +1898,87 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+class CollectOops : public BasicOopIterateClosure {\n+ public:\n+  GrowableArray<Handle>* array;\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < array->length(); i++) {\n+      result_array->obj_at_put(i, array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    \/\/ Value might be oop, but JLS can't see as Object, just iterate through it...\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      array->append(oh);\n+    }\n+  }\n+\n+  void do_oop(oop* o) { add_oop(HeapAccess<>::oop_load(o)); }\n+  void do_oop(narrowOop* v) { add_oop(HeapAccess<>::oop_load(v)); }\n+};\n+\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  StackFrameStream sfs(thread, false \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\n@@ -2781,0 +2871,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":96,"deletions":0,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+#include <string.h>\n@@ -1859,1 +1860,0 @@\n-unsigned int patch_mod_count = 0;\n@@ -1906,0 +1906,10 @@\n+  if (AMD64_ONLY(false &&) AARCH64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypePassFieldsAsArgs)) {\n+    FLAG_SET_CMDLINE(InlineTypePassFieldsAsArgs, false);\n+    warning(\"InlineTypePassFieldsAsArgs is not supported on this platform\");\n+  }\n+\n+  if (AMD64_ONLY(false &&) AARCH64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypeReturnedAsFields)) {\n+    FLAG_SET_CMDLINE(InlineTypeReturnedAsFields, false);\n+    warning(\"InlineTypeReturnedAsFields is not supported on this platform\");\n+  }\n+\n@@ -2042,2 +2052,0 @@\n-  bool patch_mod_javabase = false;\n-\n@@ -2057,1 +2065,1 @@\n-  jint result = parse_each_vm_init_arg(vm_options_args, &patch_mod_javabase, JVMFlagOrigin::JIMAGE_RESOURCE);\n+  jint result = parse_each_vm_init_arg(vm_options_args, JVMFlagOrigin::JIMAGE_RESOURCE);\n@@ -2064,1 +2072,1 @@\n-  result = parse_each_vm_init_arg(java_tool_options_args, &patch_mod_javabase, JVMFlagOrigin::ENVIRON_VAR);\n+  result = parse_each_vm_init_arg(java_tool_options_args, JVMFlagOrigin::ENVIRON_VAR);\n@@ -2070,1 +2078,1 @@\n-  result = parse_each_vm_init_arg(cmd_line_args, &patch_mod_javabase, JVMFlagOrigin::COMMAND_LINE);\n+  result = parse_each_vm_init_arg(cmd_line_args, JVMFlagOrigin::COMMAND_LINE);\n@@ -2077,1 +2085,1 @@\n-  result = parse_each_vm_init_arg(java_options_args, &patch_mod_javabase, JVMFlagOrigin::ENVIRON_VAR);\n+  result = parse_each_vm_init_arg(java_options_args, JVMFlagOrigin::ENVIRON_VAR);\n@@ -2098,1 +2106,1 @@\n-  result = finalize_vm_init_args(patch_mod_javabase);\n+  result = finalize_vm_init_args();\n@@ -2151,1 +2159,1 @@\n-int Arguments::process_patch_mod_option(const char* patch_mod_tail, bool* patch_mod_javabase) {\n+int Arguments::process_patch_mod_option(const char* patch_mod_tail) {\n@@ -2167,1 +2175,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1, patch_mod_javabase);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/);\n@@ -2169,3 +2177,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2179,0 +2184,64 @@\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, each module may have value classes that\n+  \/\/ are to be patched into the module.\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (enable_preview() && EnableValhalla) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module or --enable-preview\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2233,1 +2302,1 @@\n-jint Arguments::parse_each_vm_init_arg(const JavaVMInitArgs* args, bool* patch_mod_javabase, JVMFlagOrigin origin) {\n+jint Arguments::parse_each_vm_init_arg(const JavaVMInitArgs* args, JVMFlagOrigin origin) {\n@@ -2360,1 +2429,1 @@\n-      int res = process_patch_mod_option(tail, patch_mod_javabase);\n+      int res = process_patch_mod_option(tail);\n@@ -2887,0 +2956,6 @@\n+  if (!EnableValhalla && EnablePrimitiveClasses) {\n+    jio_fprintf(defaultStream::error_stream(),\n+                \"Cannot specify -XX:+EnablePrimitiveClasses without -XX:+EnableValhalla\");\n+    return JNI_EINVAL;\n+  }\n+\n@@ -2901,12 +2976,3 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool* patch_mod_javabase) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (*patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      *patch_mod_javabase = true;\n-    }\n-  }\n+bool match_module(void *module_name, ModulePatchPath *patch) {\n+  return (strcmp((char *)module_name, patch->module_name()) == 0);\n+}\n@@ -2914,0 +2980,5 @@\n+bool Arguments::patch_mod_javabase() {\n+    return _patch_mod_prefix != nullptr && _patch_mod_prefix->find((void*)JAVA_BASE_NAME, match_module) >= 0;\n+}\n+\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append) {\n@@ -2919,1 +2990,16 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find((void*)module_name, match_module);\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -2962,1 +3048,1 @@\n-jint Arguments::finalize_vm_init_args(bool patch_mod_javabase) {\n+jint Arguments::finalize_vm_init_args() {\n@@ -3036,0 +3122,5 @@\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n+    return JNI_ERR;\n+  }\n+\n@@ -3077,1 +3168,1 @@\n-  if (UseSharedSpaces && patch_mod_javabase) {\n+  if (UseSharedSpaces && patch_mod_javabase()) {\n@@ -4033,0 +4124,7 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    InlineTypePassFieldsAsArgs = false;\n+    InlineTypeReturnedAsFields = false;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":128,"deletions":30,"binary":false,"changes":158,"status":"modified"},{"patch":"@@ -52,0 +52,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n@@ -57,0 +59,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -308,1 +311,1 @@\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+    assert(obj.not_null() || k->is_inline_klass() || realloc_failures, \"reallocation was missed\");\n@@ -310,1 +313,5 @@\n-      st.print(\" allocation failed\");\n+      if (k->is_inline_klass()) {\n+        st.print(\" is null\");\n+      } else {\n+        st.print(\" allocation failed\");\n+      }\n@@ -344,2 +351,13 @@\n-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n-  Handle return_value;\n+  ScopeDesc* scope = chunk->at(0)->scope();\n+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n+  \/\/ In case of the return of multiple values, we must take care\n+  \/\/ of all oop return values.\n+  GrowableArray<Handle> return_oops;\n+  InlineKlass* vk = nullptr;\n+  if (save_oop_result && scope->return_scalarized()) {\n+    vk = InlineKlass::returned_inline_klass(map);\n+    if (vk != nullptr) {\n+      vk->save_oop_fields(map, return_oops);\n+      save_oop_result = false;\n+    }\n+  }\n@@ -351,1 +369,1 @@\n-    return_value = Handle(thread, result);\n+    return_oops.push(Handle(thread, result));\n@@ -358,1 +376,1 @@\n-  if (objects != nullptr) {\n+  if (objects != nullptr || vk != nullptr) {\n@@ -363,1 +381,8 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, CHECK_AND_CLEAR_(true));\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+        bool skip_internal = (compiled_method != nullptr) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, CHECK_AND_CLEAR_(true));\n+      }\n@@ -368,1 +393,8 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+        bool skip_internal = (compiled_method != nullptr) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, THREAD);\n+      }\n@@ -371,2 +403,0 @@\n-    bool skip_internal = (compiled_method != nullptr) && !compiled_method->is_compiled_by_jvmci();\n-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal);\n@@ -377,1 +407,1 @@\n-  if (save_oop_result) {\n+  if (save_oop_result || vk != nullptr) {\n@@ -379,1 +409,2 @@\n-    deoptee.set_saved_oop_result(&map, return_value());\n+    assert(return_oops.length() == 1, \"no inline type\");\n+    deoptee.set_saved_oop_result(&map, return_oops.pop()());\n@@ -714,1 +745,1 @@\n-  \/\/ If the sender is deoptimized the we must retrieve the address of the handler\n+  \/\/ If the sender is deoptimized we must retrieve the address of the handler\n@@ -1214,2 +1245,12 @@\n-\n-    oop obj = nullptr;\n+    \/\/ Check if the object may be null and has an additional is_init input that needs\n+    \/\/ to be checked before using the field values. Skip re-allocation if it is null.\n+    if (sv->maybe_null()) {\n+      assert(k->is_inline_klass(), \"must be an inline klass\");\n+      intptr_t init_value = StackValue::create_stack_value(fr, reg_map, sv->is_init())->get_int();\n+      jint is_init = (jint)*((jint*)&init_value);\n+      if (is_init == 0) {\n+        continue;\n+      }\n+    }\n+\n+    oop obj = nullptr;\n@@ -1248,0 +1289,4 @@\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* ak = FlatArrayKlass::cast(k);\n+      \/\/ Inline type array must be zeroed because not all memory is reassigned\n+      obj = ak->allocate(sv->field_size(), THREAD);\n@@ -1277,0 +1322,15 @@\n+\/\/ We're deoptimizing at the return of a call, inline type fields are\n+\/\/ in registers. When we go back to the interpreter, it will expect a\n+\/\/ reference to an inline type instance. Allocate and initialize it from\n+\/\/ the register values here.\n+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {\n+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);\n+  if (new_vt == nullptr) {\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);\n+  }\n+  return_oops.clear();\n+  return_oops.push(Handle(THREAD, new_vt));\n+  return false;\n+}\n+\n@@ -1449,0 +1509,1 @@\n+  InstanceKlass* _klass;\n@@ -1453,0 +1514,1 @@\n+    _klass = nullptr;\n@@ -1462,1 +1524,1 @@\n-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal) {\n+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal, int base_offset, TRAPS) {\n@@ -1471,0 +1533,8 @@\n+        if (fs.signature()->is_Q_signature()) {\n+          if (fs.is_inlined()) {\n+            \/\/ Resolve klass of flattened inline type field\n+            field._klass = InlineKlass::cast(klass->get_inline_type_field_klass(fs.index()));\n+          } else {\n+            field._type = T_OBJECT;\n+          }\n+        }\n@@ -1478,0 +1548,11 @@\n+    BasicType type = fields->at(i)._type;\n+    int offset = base_offset + fields->at(i)._offset;\n+    \/\/ Check for flattened inline type field before accessing the ScopeValue because it might not have any fields\n+    if (type == T_PRIMITIVE_OBJECT) {\n+      \/\/ Recursively re-assign flattened inline type fields\n+      InstanceKlass* vk = fields->at(i)._klass;\n+      assert(vk != nullptr, \"must be resolved\");\n+      offset -= InlineKlass::cast(vk)->first_field_offset(); \/\/ Adjust offset to omit oop header\n+      svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, skip_internal, offset, CHECK_0);\n+      continue; \/\/ Continue because we don't need to increment svIndex\n+    }\n@@ -1481,3 +1562,2 @@\n-    int offset = fields->at(i)._offset;\n-    BasicType type = fields->at(i)._type;\n-      case T_OBJECT: case T_ARRAY:\n+      case T_OBJECT:\n+      case T_ARRAY:\n@@ -1564,0 +1644,14 @@\n+\/\/ restore fields of an eliminated inline type array\n+void Deoptimization::reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool skip_internal, TRAPS) {\n+  InlineKlass* vk = vak->element_klass();\n+  assert(vk->flatten_array(), \"should only be used for flattened inline type arrays\");\n+  \/\/ Adjust offset to omit oop header\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_PRIMITIVE_OBJECT) - InlineKlass::cast(vk)->first_field_offset();\n+  \/\/ Initialize all elements of the flattened inline type array\n+  for (int i = 0; i < sv->field_size(); i++) {\n+    ScopeValue* val = sv->field_at(i);\n+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));\n+    reassign_fields_by_klass(vk, fr, reg_map, val->as_ObjectValue(), 0, (oop)obj, skip_internal, offset, CHECK);\n+  }\n+}\n+\n@@ -1565,1 +1659,1 @@\n-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal) {\n+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal, TRAPS) {\n@@ -1571,1 +1665,1 @@\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+    assert(obj.not_null() || realloc_failures || sv->maybe_null(), \"reallocation was missed\");\n@@ -1611,1 +1705,4 @@\n-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal);\n+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal, 0, CHECK);\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      reassign_flat_array_elements(fr, reg_map, sv, (flatArrayOop) obj(), vak, skip_internal, CHECK);\n@@ -1771,1 +1868,1 @@\n-  \/\/ Deoptimize only if the frame comes from compile code.\n+  \/\/ Deoptimize only if the frame comes from compiled code.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":120,"deletions":23,"binary":false,"changes":143,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -173,0 +173,3 @@\n+address StubRoutines::_load_inline_type_fields_in_regs = nullptr;\n+address StubRoutines::_store_inline_type_fields_to_buf = nullptr;\n+\n@@ -423,0 +426,1 @@\n+  case T_PRIMITIVE_OBJECT:\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -266,0 +266,3 @@\n+  static address _load_inline_type_fields_in_regs;\n+  static address _store_inline_type_fields_to_buf;\n+\n@@ -483,0 +486,3 @@\n+\n+  static address load_inline_type_fields_in_regs() { return _load_inline_type_fields_in_regs; }\n+  static address store_inline_type_fields_to_buf() { return _store_inline_type_fields_to_buf; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -71,0 +71,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -228,1 +230,1 @@\n-  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ObjArrayKlass*)                        \\\n+  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ArrayKlass*)                        \\\n@@ -1166,0 +1168,1 @@\n+           declare_type(FlatArrayKlass, ArrayKlass)                       \\\n@@ -1169,0 +1172,1 @@\n+        declare_type(InlineKlass, InstanceKlass)                          \\\n@@ -1545,0 +1549,1 @@\n+  declare_c2_type(MachVEPNode, MachIdealNode)                             \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -633,0 +633,9 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Prototyping\n+\/\/ \"Code Missing Here\" macro, un-define when integrating back from prototyping stage and break\n+\/\/ compilation on purpose (i.e. \"forget me not\")\n+#define PROTOTYPE\n+#ifdef PROTOTYPE\n+#define CMH(m)\n+#endif\n+\n@@ -718,6 +727,7 @@\n-  T_VOID        = 14,\n-  T_ADDRESS     = 15,\n-  T_NARROWOOP   = 16,\n-  T_METADATA    = 17,\n-  T_NARROWKLASS = 18,\n-  T_CONFLICT    = 19, \/\/ for stack value type with conflicting contents\n+  T_PRIMITIVE_OBJECT = 14,\n+  T_VOID        = 15,\n+  T_ADDRESS     = 16,\n+  T_NARROWOOP   = 17,\n+  T_METADATA    = 18,\n+  T_NARROWKLASS = 19,\n+  T_CONFLICT    = 20, \/\/ for stack value type with conflicting contents\n@@ -738,0 +748,1 @@\n+    F(JVM_SIGNATURE_PRIMITIVE_OBJECT, T_PRIMITIVE_OBJECT, N) \\\n@@ -767,1 +778,1 @@\n-  return (t == T_OBJECT || t == T_ARRAY || (include_narrow_oop && t == T_NARROWOOP));\n+  return (t == T_OBJECT || t == T_ARRAY || t == T_PRIMITIVE_OBJECT || (include_narrow_oop && t == T_NARROWOOP));\n@@ -825,1 +836,2 @@\n-  T_VOID_size        = 0\n+  T_VOID_size        = 0,\n+  T_PRIMITIVE_OBJECT_size = 1\n@@ -855,0 +867,1 @@\n+  T_PRIMITIVE_OBJECT_aelem_bytes = 8,\n@@ -858,0 +871,1 @@\n+  T_PRIMITIVE_OBJECT_aelem_bytes = 4,\n@@ -950,1 +964,1 @@\n-  vtos = 9,             \/\/ tos not cached\n+  vtos = 9,             \/\/ tos not cached,\n@@ -967,1 +981,2 @@\n-    case T_ARRAY  : \/\/ fall through\n+    case T_PRIMITIVE_OBJECT: \/\/ fall through\n+    case T_ARRAY  :   \/\/ fall through\n@@ -1337,0 +1352,6 @@\n+\/\/ TEMP!!!!\n+\/\/ This should be removed after LW2 arrays are implemented (JDK-8220790).\n+\/\/ It's an alias to (EnablePrimitiveClasses && (FlatArrayElementMaxSize != 0)),\n+\/\/ which is actually not 100% correct, but works for the current set of C1\/C2\n+\/\/ implementation and test cases.\n+#define UseFlatArray (EnablePrimitiveClasses && (FlatArrayElementMaxSize != 0))\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":31,"deletions":10,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -220,0 +220,2 @@\n+ * Value objects cannot be `java.io.Externalizable` because value objects are\n+ * immutable and `Externalizable.readExternal` is unable to modify the fields of the value.\n@@ -245,0 +247,5 @@\n+ * <p>Value objects are deserialized differently than ordinary serializable objects or records.\n+ * See <a href=\"{@docRoot}\/..\/specs\/serialization\/serial-arch.html#serialization-of-value-objects\">\n+ * <cite>Java Object Serialization Specification,<\/cite> Section 1.14,\n+ * \"Serialization of Value Objects\"<\/a> for additional information.\n+ *\n@@ -2254,1 +2261,2 @@\n-        passHandle = handles.assign(unshared ? unsharedMarker : obj);\n+        \/\/ Assign the handle and initially set to null or the unsharedMarker\n+        passHandle = handles.assign(unshared ? unsharedMarker : null);\n@@ -2267,0 +2275,6 @@\n+            if (desc.isValue()) {\n+                throw new NotSerializableException(\"Externalizable not valid for value class \"\n+                        + cl.getName());\n+            }\n+            if (!unshared)\n+                handles.setObject(passHandle, obj);\n@@ -2268,0 +2282,7 @@\n+        } else if (desc.isValue()) {\n+            \/\/ For value objects, read the fields and finish the buffer before publishing the ref\n+            assert obj != null : \"obj == null: \" + desc;\n+            readSerialData(obj, desc);\n+            obj = desc.finishValue(obj);\n+            if (!unshared)\n+                handles.setObject(passHandle, obj);\n@@ -2269,0 +2290,3 @@\n+            \/\/ For all other objects, publish the ref and then read the data\n+            if (!unshared)\n+                handles.setObject(passHandle, obj);\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectInputStream.java","additions":25,"deletions":1,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -84,0 +85,1 @@\n+import jdk.internal.value.PrimitiveClass;\n@@ -232,3 +234,3 @@\n-    private static final int ANNOTATION= 0x00002000;\n-    private static final int ENUM      = 0x00004000;\n-    private static final int SYNTHETIC = 0x00001000;\n+    private static final int ANNOTATION = 0x00002000;\n+    private static final int ENUM       = 0x00004000;\n+    private static final int SYNTHETIC  = 0x00001000;\n@@ -266,2 +268,16 @@\n-        String kind = isInterface() ? \"interface \" : isPrimitive() ? \"\" : \"class \";\n-        return kind.concat(getName());\n+        String s = getName();\n+        if (isPrimitive()) {\n+            return s;\n+        }\n+        \/\/ Avoid invokedynamic based String concat, might be not available\n+        \/\/ Prepend type of class\n+        s = (isInterface() ? \"interface \" : \"class \").concat(s);\n+        if (isValue()) {\n+            \/\/ prepend value class type\n+            s = (isPrimitiveClass() ? \"primitive \" : \"value \").concat(s);\n+            if (isPrimitiveClass() && isPrimaryType()) {\n+                \/\/ Append .ref\n+                s = s.concat(\".ref\");\n+            }\n+        }\n+        return s;\n@@ -320,0 +336,2 @@\n+                \/\/ Modifier.toString() below mis-interprets SYNCHRONIZED, STRICT, and VOLATILE bits\n+                modifiers &= ~(Modifier.SYNCHRONIZED | Modifier.STRICT | Modifier.VOLATILE);\n@@ -328,0 +346,3 @@\n+                if (isValue()) {\n+                    sb.append(isPrimitiveClass() ? \"primitive \" : \"value \");\n+                }\n@@ -542,2 +563,2 @@\n-                                            ClassLoader loader,\n-                                            Class<?> caller)\n+                                    ClassLoader loader,\n+                                    Class<?> caller)\n@@ -634,0 +655,138 @@\n+    \/\/ set by VM if this class is an exotic type such as primitive class\n+    \/\/ otherwise, these two fields are null\n+    private transient Class<T> primaryType;\n+    private transient Class<T> secondaryType;\n+\n+    \/**\n+     * Returns {@code true} if this class is a primitive class.\n+     * <p>\n+     * Each primitive class has a {@linkplain #isPrimaryType() primary type}\n+     * representing the <em>primitive reference type<\/em> and a\n+     * {@linkplain #isPrimitiveValueType() secondary type} representing\n+     * the <em>primitive value type<\/em>.  The primitive reference type\n+     * and primitive value type can be obtained by calling the\n+     * {@link #asPrimaryType()} and {@link #asValueType} method\n+     * of a primitive class respectively.\n+     * <p>\n+     * A primitive class is a {@linkplain #isValue() value class}.\n+     *\n+     * @return {@code true} if this class is a primitive class, otherwise {@code false}\n+     * @see #isValue()\n+     * @see #asPrimaryType()\n+     * @see #asValueType()\n+     * @since Valhalla\n+     *\/\n+    \/* package *\/ boolean isPrimitiveClass() {\n+        return (this.getModifiers() & PrimitiveClass.PRIMITIVE_CLASS) != 0;\n+    }\n+\n+    \/**\n+     * {@return {@code true} if this {@code Class} object represents an identity\n+     * class or interface; otherwise {@code false}}\n+     *\n+     * If this {@code Class} object represents an array type, then this method\n+     * returns {@code true}.\n+     * If this {@code Class} object represents a primitive type, or {@code void},\n+     * then this method returns {@code false}.\n+     *\n+     * @since Valhalla\n+     *\/\n+    @PreviewFeature(feature = PreviewFeature.Feature.VALUE_OBJECTS)\n+    public native boolean isIdentity();\n+\n+    \/**\n+     * {@return {@code true} if this {@code Class} object represents a value\n+     * class or interface; otherwise {@code false}}\n+     *\n+     * If this {@code Class} object represents an array type, a primitive type, or\n+     * {@code void}, then this method returns {@code false}.\n+     *\n+     * @since Valhalla\n+     *\/\n+    @PreviewFeature(feature = PreviewFeature.Feature.VALUE_OBJECTS)\n+    public boolean isValue() {\n+        return (this.getModifiers() & Modifier.VALUE) != 0;\n+    }\n+\n+    \/**\n+     * Returns a {@code Class} object representing the primary type\n+     * of this class or interface.\n+     * <p>\n+     * If this {@code Class} object represents a primitive type or an array type,\n+     * then this method returns this class.\n+     * <p>\n+     * If this {@code Class} object represents a {@linkplain #isPrimitiveClass()\n+     * primitive class}, then this method returns the <em>primitive reference type<\/em>\n+     * type of this primitive class.\n+     * <p>\n+     * Otherwise, this {@code Class} object represents a non-primitive class or interface\n+     * and this method returns this class.\n+     *\n+     * @return the {@code Class} representing the primary type of\n+     *         this class or interface\n+     * @since Valhalla\n+     *\/\n+    @IntrinsicCandidate\n+    \/* package *\/ Class<?> asPrimaryType() {\n+        return isPrimitiveClass() ? primaryType : this;\n+    }\n+\n+    \/**\n+     * Returns a {@code Class} object representing the <em>primitive value type<\/em>\n+     * of this class if this class is a {@linkplain #isPrimitiveClass() primitive class}.\n+     *\n+     * @apiNote Alternatively, this method returns null if this class is not\n+     *          a primitive class rather than throwing UOE.\n+     *\n+     * @return the {@code Class} representing the {@linkplain #isPrimitiveValueType()\n+     * primitive value type} of this class if this class is a primitive class\n+     * @throws UnsupportedOperationException if this class or interface\n+     *         is not a primitive class\n+     * @since Valhalla\n+     *\/\n+    @IntrinsicCandidate\n+    \/* package *\/ Class<?> asValueType() {\n+        if (isPrimitiveClass())\n+            return secondaryType;\n+\n+        throw new UnsupportedOperationException(this.getName().concat(\" is not a primitive class\"));\n+    }\n+\n+    \/**\n+     * Returns {@code true} if this {@code Class} object represents the primary type\n+     * of this class or interface.\n+     * <p>\n+     * If this {@code Class} object represents a primitive type or an array type,\n+     * then this method returns {@code true}.\n+     * <p>\n+     * If this {@code Class} object represents a {@linkplain #isPrimitiveClass()\n+     * primitive}, then this method returns {@code true} if this {@code Class}\n+     * object represents a primitive reference type, or returns {@code false}\n+     * if this {@code Class} object represents a primitive value type.\n+     * <p>\n+     * If this {@code Class} object represents a non-primitive class or interface,\n+     * then this method returns {@code true}.\n+     *\n+     * @return {@code true} if this {@code Class} object represents\n+     * the primary type of this class or interface\n+     * @since Valhalla\n+     *\/\n+    \/* package *\/ boolean isPrimaryType() {\n+        if (isPrimitiveClass()) {\n+            return this == primaryType;\n+        }\n+        return true;\n+    }\n+\n+    \/**\n+     * Returns {@code true} if this {@code Class} object represents\n+     * a {@linkplain #isPrimitiveClass() primitive} value type.\n+     *\n+     * @return {@code true} if this {@code Class} object represents\n+     * the value type of a primitive class\n+     * @since Valhalla\n+     *\/\n+    \/* package *\/ boolean isPrimitiveValueType() {\n+        return isPrimitiveClass() && this == secondaryType;\n+    }\n+\n@@ -1391,0 +1550,2 @@\n+     * The modifiers also include the Java Virtual Machine's constants for\n+     * {@code identity class} and {@code value class}.\n@@ -1425,1 +1586,1 @@\n-    \/**\n+   \/**\n@@ -1428,0 +1589,1 @@\n+     * The {@code AccessFlags} may depend on the class file format version of the class.\n@@ -1457,4 +1619,8 @@\n-        return AccessFlag.maskToAccessFlags((location == AccessFlag.Location.CLASS) ?\n-                                            getClassAccessFlagsRaw() :\n-                                            getModifiers(),\n-                                            location);\n+        int accessFlags = (location == AccessFlag.Location.CLASS) ?\n+                getClassAccessFlagsRaw() : getModifiers();\n+        var cffv = ClassFileFormatVersion.fromMajor(getClassFileVersion() & 0xffff);\n+        if (cffv.compareTo(ClassFileFormatVersion.latest()) >= 0) {\n+            \/\/ Ignore unspecified (0x800) access flag for current version\n+            accessFlags &= ~0x0800;\n+        }\n+        return AccessFlag.maskToAccessFlags(accessFlags, location, cffv);\n@@ -1463,1 +1629,1 @@\n-    \/**\n+   \/**\n@@ -1473,1 +1639,0 @@\n-\n@@ -1479,1 +1644,0 @@\n-\n@@ -1620,1 +1784,1 @@\n-        boolean isConstructor() { return !isPartial() && ConstantDescs.INIT_NAME.equals(name); }\n+        boolean isObjectConstructor() { return !isPartial() && ConstantDescs.INIT_NAME.equals(name); }\n@@ -1622,1 +1786,5 @@\n-        boolean isMethod() { return !isPartial() && !isConstructor() && !ConstantDescs.CLASS_INIT_NAME.equals(name); }\n+        boolean isValueFactoryMethod() { return !isPartial() && ConstantDescs.VNEW_NAME.equals(name); }\n+\n+        boolean isMethod() { return !isPartial() && !isObjectConstructor()\n+                                        && !isValueFactoryMethod()\n+                                        && !ConstantDescs.CLASS_INIT_NAME.equals(name); }\n@@ -1679,1 +1847,1 @@\n-            if (!enclosingInfo.isConstructor())\n+            if (!enclosingInfo.isObjectConstructor() && !enclosingInfo.isValueFactoryMethod())\n@@ -1862,1 +2030,1 @@\n-                return cl.getName().concat(\"[]\".repeat(dimensions));\n+                return cl.getTypeName().concat(\"[]\".repeat(dimensions));\n@@ -1865,1 +2033,6 @@\n-        return getName();\n+        if (isPrimitiveClass()) {\n+            \/\/ TODO: null-default\n+            return isPrimaryType() ? getName().concat(\".ref\") : getName();\n+        } else {\n+            return getName();\n+        }\n@@ -3807,1 +3980,1 @@\n-        throw new NoSuchMethodException(methodToString(\"<init>\", parameterTypes));\n+        throw new NoSuchMethodException(methodToString(isValue() ? \"<vnew>\" : \"<init>\", parameterTypes));\n@@ -4112,0 +4285,3 @@\n+        if (isPrimitiveValueType() && obj == null)\n+            throw new NullPointerException(getName() + \" is a primitive value type\");\n+\n@@ -4407,1 +4583,1 @@\n-         return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n+        return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n@@ -4617,1 +4793,3 @@\n-        } else if (isHidden()) {\n+        }\n+        char typeDesc = isPrimitiveValueType() ? 'Q' : 'L';\n+        if (isHidden()) {\n@@ -4621,1 +4799,1 @@\n-                    .append('L')\n+                    .append(typeDesc)\n@@ -4630,1 +4808,1 @@\n-                    .append('L')\n+                    .append(typeDesc)\n@@ -4808,1 +4986,2 @@\n-    private int getClassFileVersion() {\n+    \/* package-private *\/\n+    int getClassFileVersion() {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Class.java","additions":205,"deletions":26,"binary":false,"changes":231,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import jdk.internal.value.PrimitiveClass;\n@@ -716,2 +717,2 @@\n-     * with special names ({@value ConstantDescs#INIT_NAME} and {@value\n-     * ConstantDescs#CLASS_INIT_NAME}).\n+     * with special names ({@value ConstantDescs#INIT_NAME},\n+     * {@value ConstantDescs#VNEW_NAME} and {@value ConstantDescs#CLASS_INIT_NAME}).\n@@ -1641,0 +1642,1 @@\n+            assert PrimitiveClass.isPrimaryType(lookupClass);\n@@ -2821,0 +2823,2 @@\n+         *\n+         *\n@@ -2836,0 +2840,3 @@\n+            if (type.returnType() != void.class) {\n+                throw new NoSuchMethodException(\"Constructors must have void return type: \" + refc.getName());\n+            }\n@@ -3531,1 +3538,1 @@\n-            assert(ctor.isConstructor());\n+            assert(ctor.isObjectConstructor() || ctor.isStaticValueFactoryMethod());\n@@ -3534,1 +3541,10 @@\n-            return lookup.getDirectConstructorNoSecurityManager(ctor.getDeclaringClass(), ctor);\n+            Class<?> defc = c.getDeclaringClass();\n+            if (ctor.isObjectConstructor()) {\n+                assert(ctor.getMethodType().returnType() == void.class);\n+                return lookup.getDirectConstructorNoSecurityManager(defc, ctor);\n+            } else {\n+                \/\/ static init factory is a static method\n+                assert(ctor.isMethod() && ctor.getMethodType().returnType() == defc && ctor.getReferenceKind() == REF_invokeStatic) : ctor.toString();\n+                assert(!MethodHandleNatives.isCallerSensitive(ctor));  \/\/ must not be caller-sensitive\n+                return lookup.getDirectMethodNoSecurityManager(ctor.getReferenceKind(), defc, ctor, lookup);\n+            }\n@@ -3779,1 +3795,1 @@\n-            if (name.startsWith(\"<\") && refKind != REF_newInvokeSpecial) {\n+            if (isIllegalMethodName(refKind, name)) {\n@@ -3782,0 +3798,1 @@\n+\n@@ -3801,0 +3818,12 @@\n+        \/*\n+         * \"<init>\" can only be invoked via invokespecial\n+         * \"<vnew>\" factory can only invoked via invokestatic\n+         *\/\n+        boolean isIllegalMethodName(byte refKind, String name) {\n+            if (name.startsWith(\"<\")) {\n+                return MemberName.VALUE_FACTORY_NAME.equals(name) ? refKind != REF_invokeStatic\n+                                                                  : refKind != REF_newInvokeSpecial;\n+            }\n+            return false;\n+        }\n+\n@@ -3803,2 +3832,3 @@\n-            if (name.startsWith(\"<\") && refKind != REF_newInvokeSpecial)\n-                throw new NoSuchMethodException(\"illegal method name: \"+name);\n+            if (isIllegalMethodName(refKind, name)) {\n+                throw new NoSuchMethodException(\"illegal method name: \" + name + \" \" + refKind);\n+            }\n@@ -3908,1 +3938,1 @@\n-            if (!fullPrivilegeLookup && defc != refc) {\n+            if (!fullPrivilegeLookup && PrimitiveClass.asPrimaryType(defc) != PrimitiveClass.asPrimaryType(refc)) {\n@@ -3916,1 +3946,1 @@\n-            if (m.isConstructor())\n+            if (m.isObjectConstructor())\n@@ -3995,1 +4025,1 @@\n-                               (defc == refc ||\n+                               (PrimitiveClass.asPrimaryType(defc) == PrimitiveClass.asPrimaryType(refc) ||\n@@ -4000,1 +4030,1 @@\n-                           (defc == refc ||\n+                           (PrimitiveClass.asPrimaryType(defc) == PrimitiveClass.asPrimaryType(refc) ||\n@@ -4077,1 +4107,0 @@\n-\n@@ -4226,1 +4255,1 @@\n-            assert(ctor.isConstructor());\n+            assert(ctor.isObjectConstructor());\n@@ -5132,1 +5161,1 @@\n-            if (value == null)\n+            if (!PrimitiveClass.isPrimitiveValueType(type) && value == null)\n@@ -5177,1 +5206,9 @@\n-        return type.isPrimitive() ?  zero(Wrapper.forPrimitiveType(type), type) : zero(Wrapper.OBJECT, type);\n+        if (type.isPrimitive()) {\n+            return zero(Wrapper.forPrimitiveType(type), type);\n+        } else if (PrimitiveClass.isPrimitiveValueType(type)) {\n+            \/\/ singleton default value\n+            Object value = UNSAFE.uninitializedDefaultValue(type);\n+            return identity(type).bindTo(value);\n+        } else {\n+            return zero(Wrapper.OBJECT, type);\n+        }\n@@ -5207,1 +5244,1 @@\n-        MethodType mtype = methodType(ptype, ptype);\n+        MethodType mtype = MethodType.methodType(ptype, ptype);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandles.java","additions":53,"deletions":16,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+import jdk.internal.value.PrimitiveClass;\n@@ -509,1 +510,4 @@\n-             VarHandleShorts.FieldStaticReadOnly {\n+             VarHandleShorts.FieldStaticReadOnly,\n+             VarHandleValues.Array,\n+             VarHandleValues.FieldInstanceReadOnly,\n+             VarHandleValues.FieldStaticReadOnly {\n@@ -1693,0 +1697,6 @@\n+            \/\/ the field type (value) is mapped to the return type of MethodType\n+            \/\/ the receiver type is mapped to a parameter type of MethodType\n+            \/\/ So use the value type if it's a primitive class\n+            if (receiver != null && PrimitiveClass.isPrimitiveClass(receiver)) {\n+                receiver = PrimitiveClass.asValueType(receiver);\n+            }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/VarHandle.java","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import jdk.internal.value.PrimitiveClass;\n@@ -61,1 +62,6 @@\n-                return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                if (f.isFlattened()) {\n+                    return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                        ? new VarHandleValues.FieldInstanceReadOnly(refc, foffset, type)\n+                        : new VarHandleValues.FieldInstanceReadWrite(refc, foffset, type));\n+                } else {\n+                    return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n@@ -64,0 +70,1 @@\n+                }\n@@ -122,3 +129,9 @@\n-            return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n-                    ? new VarHandleReferences.FieldStaticReadOnly(decl, base, foffset, type)\n-                    : new VarHandleReferences.FieldStaticReadWrite(decl, base, foffset, type));\n+            if (f.isFlattened()) {\n+                return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                        ? new VarHandleValues.FieldStaticReadOnly(decl, base, foffset, type)\n+                        : new VarHandleValues.FieldStaticReadWrite(decl, base, foffset, type));\n+            } else {\n+                return f.isFinal() && !isWriteAllowedOnFinalFields\n+                        ? new VarHandleReferences.FieldStaticReadOnly(decl, base, foffset, type)\n+                        : new VarHandleReferences.FieldStaticReadWrite(decl, base, foffset, type);\n+            }\n@@ -213,1 +226,6 @@\n-            return maybeAdapt(new VarHandleReferences.Array(aoffset, ashift, arrayClass));\n+            \/\/ the redundant componentType.isPrimitiveValueType() check is\n+            \/\/ there to minimize the performance impact to non-value array.\n+            \/\/ It should be removed when Unsafe::isFlattenedArray is intrinsified.\n+            return maybeAdapt(PrimitiveClass.isPrimitiveValueType(componentType) && UNSAFE.isFlattenedArray(arrayClass)\n+                ? new VarHandleValues.Array(aoffset, ashift, arrayClass)\n+                : new VarHandleReferences.Array(aoffset, ashift, arrayClass));\n@@ -632,1 +650,1 @@\n-            } else if (MethodHandleNatives.refKindIsConstructor(refKind)) {\n+            } else if (MethodHandleNatives.refKindIsObjectConstructor(refKind)) {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/VarHandles.java","additions":24,"deletions":6,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -97,1 +97,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -104,1 +104,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -111,1 +111,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -118,1 +118,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -125,1 +125,0 @@\n-\n@@ -149,0 +148,7 @@\n+#if[Object]\n+        @ForceInline\n+        static Object checkCast(FieldInstanceReadWrite handle, $type$ value) {\n+            return handle.fieldType.cast(value);\n+        }\n+#end[Object]\n+\n@@ -153,2 +159,2 @@\n-                             handle.fieldOffset,\n-                             {#if[Object]?handle.fieldType.cast(value):value});\n+                             handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                             {#if[Object]?checkCast(handle, value):value});\n@@ -161,2 +167,2 @@\n-                                     handle.fieldOffset,\n-                                     {#if[Object]?handle.fieldType.cast(value):value});\n+                                     handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                     {#if[Object]?checkCast(handle, value):value});\n@@ -169,2 +175,2 @@\n-                                   handle.fieldOffset,\n-                                   {#if[Object]?handle.fieldType.cast(value):value});\n+                                   handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                   {#if[Object]?checkCast(handle, value):value});\n@@ -177,2 +183,2 @@\n-                                    handle.fieldOffset,\n-                                    {#if[Object]?handle.fieldType.cast(value):value});\n+                                    handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                    {#if[Object]?checkCast(handle, value):value});\n@@ -186,3 +192,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -195,3 +201,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -204,3 +210,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -213,3 +219,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -222,3 +228,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -231,3 +237,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -240,3 +246,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -249,3 +255,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -258,2 +264,2 @@\n-                                          handle.fieldOffset,\n-                                          {#if[Object]?handle.fieldType.cast(value):value});\n+                                          handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                          {#if[Object]?checkCast(handle, value):value});\n@@ -266,2 +272,2 @@\n-                                          handle.fieldOffset,\n-                                          {#if[Object]?handle.fieldType.cast(value):value});\n+                                          handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                          {#if[Object]?checkCast(handle, value):value});\n@@ -274,2 +280,2 @@\n-                                          handle.fieldOffset,\n-                                          {#if[Object]?handle.fieldType.cast(value):value});\n+                                          handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                          {#if[Object]?checkCast(handle, value):value});\n@@ -445,1 +451,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -452,1 +458,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -459,1 +465,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -466,1 +472,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -497,0 +503,6 @@\n+#if[Object]\n+        static Object checkCast(FieldStaticReadWrite handle, $type$ value) {\n+            return handle.fieldType.cast(value);\n+        }\n+#end[Object]\n+\n@@ -501,2 +513,2 @@\n-                             handle.fieldOffset,\n-                             {#if[Object]?handle.fieldType.cast(value):value});\n+                             handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                             {#if[Object]?checkCast(handle, value):value});\n@@ -509,2 +521,2 @@\n-                                     handle.fieldOffset,\n-                                     {#if[Object]?handle.fieldType.cast(value):value});\n+                                     handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                     {#if[Object]?checkCast(handle, value):value});\n@@ -517,2 +529,2 @@\n-                                   handle.fieldOffset,\n-                                   {#if[Object]?handle.fieldType.cast(value):value});\n+                                   handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                   {#if[Object]?checkCast(handle, value):value});\n@@ -525,2 +537,2 @@\n-                                    handle.fieldOffset,\n-                                    {#if[Object]?handle.fieldType.cast(value):value});\n+                                    handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                    {#if[Object]?checkCast(handle, value):value});\n@@ -534,3 +546,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -544,3 +556,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -553,3 +565,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -562,3 +574,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -571,3 +583,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -580,3 +592,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -589,3 +601,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -598,3 +610,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -607,2 +619,2 @@\n-                                          handle.fieldOffset,\n-                                          {#if[Object]?handle.fieldType.cast(value):value});\n+                                          handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                          {#if[Object]?checkCast(handle, value):value});\n@@ -615,2 +627,2 @@\n-                                          handle.fieldOffset,\n-                                          {#if[Object]?handle.fieldType.cast(value):value});\n+                                          handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                          {#if[Object]?checkCast(handle, value):value});\n@@ -623,2 +635,2 @@\n-                                          handle.fieldOffset,\n-                                          {#if[Object]?handle.fieldType.cast(value):value});\n+                                          handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                          {#if[Object]?checkCast(handle, value):value});\n@@ -731,0 +743,8 @@\n+#if[Reference]\n+    static VarHandle makeVarHandleValuesArray(Class<?> arrayClass) {\n+        Class<?> componentType = arrayClass.getComponentType();\n+        assert UNSAFE.isFlattenedArray(arrayClass);\n+        \/\/ should cache these VarHandle for performance\n+        return VarHandles.makeArrayElementHandle(arrayClass);\n+    }\n+#end[Reference]\n@@ -823,1 +843,9 @@\n-            array[index] = {#if[Object]?handle.componentType.cast(value):value};\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                vh.set(oarray, index, reflectiveTypeCheck(array, value));\n+                return;\n+            }\n+#end[Reference]\n+            array[index] = {#if[Object]?runtimeTypeCheck(handle, array, value):value};\n@@ -834,0 +862,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.getVolatile(oarray, index);\n+            }\n+#end[Reference]\n@@ -835,1 +870,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase);\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType});\n@@ -846,0 +881,8 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                vh.setVolatile(oarray, index, reflectiveTypeCheck(array, value));\n+                return;\n+            }\n+#end[Reference]\n@@ -847,1 +890,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -859,0 +902,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.getOpaque(oarray, index);\n+            }\n+#end[Reference]\n@@ -860,1 +910,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase);\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType});\n@@ -871,0 +921,8 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                vh.setOpaque(oarray, index, reflectiveTypeCheck(array, value));\n+                return;\n+            }\n+#end[Reference]\n@@ -872,1 +930,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -884,0 +942,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.getAcquire(oarray, index);\n+            }\n+#end[Reference]\n@@ -885,1 +950,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase);\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType});\n@@ -896,0 +961,8 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                vh.setRelease(oarray, index, reflectiveTypeCheck(array, value));\n+                return;\n+            }\n+#end[Reference]\n@@ -897,1 +970,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -910,0 +983,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.compareAndSet(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -911,1 +991,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -924,0 +1004,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.compareAndExchange(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -925,1 +1012,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -938,0 +1025,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.compareAndExchangeAcquire(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -939,1 +1033,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -952,0 +1046,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.compareAndExchangeRelease(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -953,1 +1054,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -966,0 +1067,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.weakCompareAndSetPlain(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -967,1 +1075,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -980,0 +1088,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.weakCompareAndSet(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -981,1 +1096,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -994,0 +1109,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.weakCompareAndSetAcquire(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -995,1 +1117,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -1008,0 +1130,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.weakCompareAndSetRelease(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -1009,1 +1138,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -1022,0 +1151,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.getAndSet(oarray, index, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -1023,1 +1159,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -1035,0 +1171,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.getAndSetAcquire(oarray, index, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -1036,1 +1179,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -1048,0 +1191,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.getAndSetRelease(oarray, index, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -1049,1 +1199,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/X-VarHandle.java.template","additions":253,"deletions":103,"binary":false,"changes":356,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import java.util.Objects;\n@@ -143,2 +144,12 @@\n-    public static native Object get(Object array, int index)\n-        throws IllegalArgumentException, ArrayIndexOutOfBoundsException;\n+    public static Object get(Object array, int index)\n+        throws IllegalArgumentException, ArrayIndexOutOfBoundsException {\n+        Class<?> componentType = array.getClass().getComponentType();\n+        if (componentType != null && !componentType.isPrimitive()) {\n+            Object[] objArray = (Object[]) array.getClass().cast(array);\n+            return objArray[index];\n+        } else {\n+            return getReferenceOrPrimitive(array, index);\n+        }\n+    }\n+\n+    private static native Object getReferenceOrPrimitive(Object array, int index);\n@@ -315,2 +326,12 @@\n-    public static native void set(Object array, int index, Object value)\n-        throws IllegalArgumentException, ArrayIndexOutOfBoundsException;\n+    public static void set(Object array, int index, Object value)\n+        throws IllegalArgumentException, ArrayIndexOutOfBoundsException {\n+        Class<?> componentType = array.getClass().getComponentType();\n+        if (componentType != null && !componentType.isPrimitive()) {\n+            Object[] objArray = (Object[]) array.getClass().cast(array);\n+            objArray[index] = componentType.cast(value);\n+        } else {\n+            setReferenceOrPrimitive(array, index, value);\n+        }\n+    }\n+\n+    private static native void setReferenceOrPrimitive(Object array, int index, Object value);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Array.java","additions":25,"deletions":4,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -134,1 +134,0 @@\n-\n@@ -275,0 +274,2 @@\n+    exports jdk.internal.value to  \/\/ Needed by Unsafe\n+        jdk.unsupported;\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+import com.sun.tools.javac.code.Type.ClassType.Flavor;\n@@ -169,0 +170,1 @@\n+        allowPrimitiveClasses = Feature.PRIMITIVE_CLASSES.allowedInSource(source) && options.isSet(\"enablePrimitiveClasses\");\n@@ -187,0 +189,4 @@\n+    \/** Switch: allow primitive classes ?\n+     *\/\n+    boolean allowPrimitiveClasses;\n+\n@@ -275,1 +281,1 @@\n-            ((owner.name == names.init ||    \/\/ i.e. we are in a constructor\n+            ((names.isInitOrVNew(owner.name) ||    \/\/ i.e. we are in a constructor\n@@ -802,1 +808,1 @@\n-                List<Type> bounds = List.of(attribType(tvar.bounds.head, env));\n+                List<Type> bounds = List.of(chk.checkRefType(tvar.bounds.head, attribType(tvar.bounds.head, env), false));\n@@ -804,1 +810,1 @@\n-                    bounds = bounds.prepend(attribType(bound, env));\n+                    bounds = bounds.prepend(chk.checkRefType(bound, attribType(bound, env), false));\n@@ -1074,1 +1080,1 @@\n-                if (tree.name == names.init) {\n+                if (names.isInitOrVNew(tree.name)) {\n@@ -1189,1 +1195,1 @@\n-                if (tree.name == names.init && owner.type != syms.objectType) {\n+                if (names.isInitOrVNew(tree.name) && owner.type != syms.objectType) {\n@@ -1192,1 +1198,1 @@\n-                            TreeInfo.getConstructorInvocationName(body.stats, names) == names.empty) {\n+                            TreeInfo.getConstructorInvocationName(body.stats, names, true) == names.empty) {\n@@ -1205,0 +1211,6 @@\n+                    } else if ((env.enclClass.sym.flags() & VALUE_CLASS) != 0 &&\n+                        (tree.mods.flags & GENERATEDCONSTR) == 0 &&\n+                        TreeInfo.isSuperCall(body.stats.head)) {\n+                        \/\/ value constructors are not allowed to call super directly,\n+                        \/\/ but tolerate compiler generated ones, these are ignored during code generation\n+                        log.error(tree.body.stats.head.pos(), Errors.CallToSuperNotAllowedInValueCtor);\n@@ -1292,0 +1304,3 @@\n+            \/* Don't want constant propagation\/folding for instance fields of primitive classes,\n+               as these can undergo updates via copy on write.\n+            *\/\n@@ -1293,1 +1308,1 @@\n-                if ((v.flags_field & FINAL) == 0 ||\n+                if ((v.flags_field & FINAL) == 0 || ((v.flags_field & STATIC) == 0 && v.owner.isValueClass()) ||\n@@ -1333,1 +1348,2 @@\n-        return false;\n+        \/\/ isValueObject is not included in Object yet so we need a work around\n+        return name == names.isValueObject;\n@@ -1550,1 +1566,1 @@\n-                Type base = types.asSuper(exprType, syms.iterableType.tsym);\n+                Type base = types.asSuper(exprType.referenceProjectionOrSelf(), syms.iterableType.tsym);\n@@ -1566,1 +1582,1 @@\n-                    if (types.asSuper(iterSymbol.type.getReturnType(), syms.iteratorType.tsym) == null) {\n+                    if (types.asSuper(iterSymbol.type.getReturnType().referenceProjectionOrSelf(), syms.iteratorType.tsym) == null) {\n@@ -1905,1 +1921,1 @@\n-        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env));\n+        chk.checkIdentityType(tree.pos(), attribExpr(tree.lock, env));\n@@ -1996,1 +2012,1 @@\n-            types.asSuper(resource, syms.autoCloseableType.tsym) != null &&\n+            types.asSuper(resource.referenceProjectionOrSelf(), syms.autoCloseableType.tsym) != null &&\n@@ -2185,1 +2201,2 @@\n-            \/\/ Those were all the cases that could result in a primitive\n+            \/\/ Those were all the cases that could result in a primitive. See if primitive boxing and primitive\n+            \/\/ value conversions bring about a convergence.\n@@ -2187,1 +2204,2 @@\n-                                 .map(t -> t.isPrimitive() ? types.boxedClass(t).type : t)\n+                                 .map(t -> t.isPrimitive() ? types.boxedClass(t).type\n+                                         : t.isReferenceProjection() ? t.valueProjection() : t)\n@@ -2198,1 +2216,1 @@\n-                                 .map(t -> chk.checkNonVoid(posIt.next(), t))\n+                                 .map(t -> chk.checkNonVoid(posIt.next(), allowPrimitiveClasses && t.isPrimitiveClass() ? t.referenceProjection() : t))\n@@ -2201,1 +2219,1 @@\n-            \/\/ both are known to be reference types.  The result is\n+            \/\/ both are known to be reference types (or projections).  The result is\n@@ -2639,0 +2657,4 @@\n+                \/\/ Special treatment for primitive classes: Given an expression v of type V where\n+                \/\/ V is a primitive class, v.getClass() is typed to be Class<? extends |V.ref|>\n+                Type wcb = types.erasure(allowPrimitiveClasses && qualifierType.isPrimitiveClass() ?\n+                                         qualifierType.referenceProjection() : qualifierType.baseType());\n@@ -2640,1 +2662,1 @@\n-                        List.of(new WildcardType(types.erasure(qualifierType.baseType()),\n+                        List.of(new WildcardType(wcb,\n@@ -2644,1 +2666,2 @@\n-                        restype.getMetadata());\n+                        restype.getMetadata(),\n+                        restype.getFlavor());\n@@ -2664,1 +2687,1 @@\n-            if (enclMethod != null && enclMethod.name == names.init) {\n+            if (enclMethod != null && names.isInitOrVNew(enclMethod.name)) {\n@@ -2813,0 +2836,10 @@\n+            \/\/ Check that it is an instantiation of a class and not a projection type\n+            if (allowPrimitiveClasses) {\n+                if (clazz.hasTag(SELECT)) {\n+                    JCFieldAccess fieldAccess = (JCFieldAccess) clazz;\n+                    if (fieldAccess.selected.type.isPrimitiveClass() &&\n+                            (fieldAccess.name == names.ref || fieldAccess.name == names.val)) {\n+                        log.error(tree.pos(), Errors.ProjectionCantBeInstantiated);\n+                    }\n+                }\n+            }\n@@ -2837,1 +2870,2 @@\n-                                               clazztype.getMetadata());\n+                                               clazztype.getMetadata(),\n+                                               clazztype.getFlavor());\n@@ -2991,0 +3025,3 @@\n+                    if (allowPrimitiveClasses) {\n+                        chk.checkParameterizationByPrimitiveClass(tree, clazztype);\n+                    }\n@@ -3063,0 +3100,3 @@\n+        \/\/ Likewise arg can't be null if it is a primitive class instance.\n+        if (allowPrimitiveClasses && arg.type.isPrimitiveClass())\n+            return arg;\n@@ -3552,1 +3592,2 @@\n-                    for (Symbol s : enclClass.members_field.getSymbolsByName(names.init)) {\n+                    Name constructorName = owner.isConcreteValueClass() ? names.vnew : names.init;\n+                    for (Symbol s : enclClass.members_field.getSymbolsByName(constructorName)) {\n@@ -3615,0 +3656,1 @@\n+            Symbol lhsSym = TreeInfo.symbol(that.expr);\n@@ -3616,0 +3658,4 @@\n+                \/\/ TODO - a bit hacky but...\n+                if (lhsSym != null && lhsSym.isConcreteValueClass() && that.name == names.init) {\n+                    that.name = names.vnew;\n+                }\n@@ -3621,1 +3667,0 @@\n-                Symbol lhsSym = TreeInfo.symbol(that.expr);\n@@ -3705,1 +3750,1 @@\n-            that.sym = refSym.isConstructor() ? refSym.baseSymbol() : refSym;\n+            that.sym = refSym.isInitOrVNew() ? refSym.baseSymbol() : refSym;\n@@ -4394,0 +4439,10 @@\n+        Assert.check(site == tree.selected.type);\n+        if (allowPrimitiveClasses && tree.name == names._class && site.isPrimitiveClass()) {\n+            \/* JDK-8269956: Where a reflective (class) literal is needed, the unqualified Point.class is\n+             * always the \"primary\" mirror - representing the primitive reference runtime type - thereby\n+             * always matching the behavior of Object::getClass\n+             *\/\n+             if (!tree.selected.hasTag(SELECT) || ((JCFieldAccess) tree.selected).name != names.val) {\n+                 tree.selected.setType(site = site.referenceProjection());\n+             }\n+        }\n@@ -4406,1 +4461,1 @@\n-                return ;\n+                return;\n@@ -4510,1 +4565,1 @@\n-                Type site1 = types.asSuper(env.enclClass.sym.type, site.tsym);\n+                Type site1 = types.asSuper(env.enclClass.sym.type.referenceProjectionOrSelf(), site.tsym);\n@@ -4553,0 +4608,2 @@\n+                } else if (allowPrimitiveClasses && site.isPrimitiveClass() && isType(location) && resultInfo.pkind.contains(KindSelector.TYP) && (name == names.ref || name == names.val)) {\n+                    return site.tsym;\n@@ -4656,1 +4713,1 @@\n-                \/\/ except for two situations:\n+                \/\/ except for three situations:\n@@ -4659,0 +4716,3 @@\n+                    if (allowPrimitiveClasses) {\n+                        Assert.check(owntype.getFlavor() != Flavor.X_Typeof_X);\n+                    }\n@@ -4662,1 +4722,8 @@\n-                    \/\/ (a) If the symbol's type is parameterized, erase it\n+                    \/\/ (a) If symbol is a primitive class and its reference projection\n+                    \/\/ is requested via the .ref notation, then adjust the computed type to\n+                    \/\/ reflect this.\n+                    if (allowPrimitiveClasses && owntype.isPrimitiveClass() && tree.hasTag(SELECT) && ((JCFieldAccess) tree).name == names.ref) {\n+                        owntype = new ClassType(owntype.getEnclosingType(), owntype.getTypeArguments(), (TypeSymbol)sym, owntype.getMetadata(), Flavor.L_TypeOf_Q);\n+                    }\n+\n+                    \/\/ (b) If the symbol's type is parameterized, erase it\n@@ -4669,1 +4736,1 @@\n-                    \/\/ (b) If the symbol's type is an inner class, then\n+                    \/\/ (c) If the symbol's type is an inner class, then\n@@ -4689,1 +4756,1 @@\n-                                owntype.getMetadata());\n+                                owntype.getMetadata(), owntype.getFlavor());\n@@ -4752,1 +4819,1 @@\n-            if (sym.name != names.init || tree.hasTag(REFERENCE)) {\n+            if (!names.isInitOrVNew(sym.name) || tree.hasTag(REFERENCE)) {\n@@ -5006,0 +5073,31 @@\n+    public void visitDefaultValue(JCDefaultValue tree) {\n+        if (!allowPrimitiveClasses) {\n+            log.error(DiagnosticFlag.SOURCE_LEVEL, tree.pos(),\n+                    Feature.PRIMITIVE_CLASSES.error(sourceName));\n+        }\n+\n+        \/\/ Attribute the qualifier expression, and determine its symbol (if any).\n+        Type site = attribTree(tree.clazz, env, new ResultInfo(KindSelector.TYP_PCK, Type.noType));\n+        if (!pkind().contains(KindSelector.TYP_PCK))\n+            site = capture(site); \/\/ Capture field access\n+        if (!allowPrimitiveClasses) {\n+            result = types.createErrorType(names._default, site.tsym, site);\n+        } else {\n+            Symbol sym = switch (site.getTag()) {\n+                case WILDCARD -> throw new AssertionError(tree);\n+                case PACKAGE -> {\n+                    log.error(tree.pos, Errors.CantResolveLocation(Kinds.KindName.CLASS, site.tsym.getQualifiedName(), null, null,\n+                            Fragments.Location(Kinds.typeKindName(env.enclClass.type), env.enclClass.type, null)));\n+                    yield syms.errSymbol;\n+                }\n+                case ERROR -> types.createErrorType(names._default, site.tsym, site).tsym;\n+                default -> new VarSymbol(STATIC, names._default, site, site.tsym);\n+            };\n+\n+            if (site.hasTag(TYPEVAR) && sym.kind != ERR) {\n+                site = types.skipTypeVars(site, true);\n+            }\n+            result = checkId(tree, site, sym, env, resultInfo);\n+        }\n+    }\n+\n@@ -5093,1 +5191,1 @@\n-                                        clazztype.getMetadata());\n+                                        clazztype.getMetadata(), clazztype.getFlavor());\n@@ -5220,1 +5318,1 @@\n-                make.Modifiers(PUBLIC | ABSTRACT),\n+                make.Modifiers(PUBLIC | ABSTRACT | (extending != null && TreeInfo.symbol(extending).isPrimitiveClass() ? PRIMITIVE_CLASS : 0)),\n@@ -5243,1 +5341,1 @@\n-        result = check(tree, new WildcardType(chk.checkRefType(tree.pos(), type),\n+        result = check(tree, new WildcardType(chk.checkRefType(tree.pos(), type, false),\n@@ -5361,0 +5459,5 @@\n+            if (allowPrimitiveClasses && c.type.isPrimitiveClass()) {\n+                final Env<AttrContext> env = typeEnvs.get(c);\n+                if (env != null && env.tree != null && env.tree.hasTag(CLASSDEF))\n+                    chk.checkNonCyclicMembership((JCClassDecl)env.tree);\n+            }\n@@ -5481,1 +5584,1 @@\n-            } else {\n+            } else if ((c.flags_field & Flags.COMPOUND) == 0) {\n@@ -5536,0 +5639,5 @@\n+                if (c.isValueClass()) {\n+                    Assert.check(env.tree.hasTag(CLASSDEF));\n+                    chk.checkConstraintsOfValueClass(env.tree.pos(), c);\n+                }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":142,"deletions":34,"binary":false,"changes":176,"status":"modified"},{"patch":"@@ -187,0 +187,1 @@\n+        allowPrimitiveClasses = Feature.PRIMITIVE_CLASSES.allowedInSource(source) && options.isSet(\"enablePrimitiveClasses\");\n@@ -230,0 +231,4 @@\n+    \/** Are primitive classes allowed\n+     *\/\n+    private final boolean allowPrimitiveClasses;\n+\n@@ -632,0 +637,5 @@\n+        } else {\n+            if (allowPrimitiveClasses && found.hasTag(CLASS)) {\n+                if (inferenceContext != infer.emptyContext)\n+                    checkParameterizationByPrimitiveClass(pos, found);\n+            }\n@@ -762,0 +772,51 @@\n+    void checkConstraintsOfValueClass(DiagnosticPosition pos, ClassSymbol c) {\n+        for (Type st : types.closure(c.type)) {\n+            if (st == null || st.tsym == null || st.tsym.kind == ERR)\n+                continue;\n+            if  (st.tsym == syms.objectType.tsym || st.tsym == syms.recordType.tsym || st.isInterface())\n+                continue;\n+            if (!st.tsym.isAbstract()) {\n+                if (c != st.tsym) {\n+                    log.error(pos, Errors.ConcreteSupertypeForValueClass(c, st));\n+                }\n+                continue;\n+            }\n+            \/\/ dealing with an abstract value or value super class below.\n+            Fragment fragment = c.isAbstract() && c.isValueClass() && c == st.tsym ? Fragments.AbstractValueClass(c) : Fragments.SuperclassOfValueClass(c, st);\n+            if ((st.tsym.flags() & HASINITBLOCK) != 0) {\n+                log.error(pos, Errors.AbstractValueClassDeclaresInitBlock(fragment));\n+            }\n+            Type encl = st.getEnclosingType();\n+            if (encl != null && encl.hasTag(CLASS)) {\n+                log.error(pos, Errors.AbstractValueClassCannotBeInner(fragment));\n+            }\n+            for (Symbol s : st.tsym.members().getSymbols(NON_RECURSIVE)) {\n+                switch (s.kind) {\n+                case VAR:\n+                    if ((s.flags() & STATIC) == 0) {\n+                        log.error(pos, Errors.InstanceFieldNotAllowed(s, fragment));\n+                    }\n+                    break;\n+                case MTH:\n+                    if ((s.flags() & (SYNCHRONIZED | STATIC)) == SYNCHRONIZED) {\n+                        log.error(pos, Errors.SuperClassMethodCannotBeSynchronized(s, c, st));\n+                    } else if (s.isInitOrVNew()) {\n+                        MethodSymbol m = (MethodSymbol)s;\n+                        if (m.getParameters().size() > 0) {\n+                            log.error(pos, Errors.AbstractValueClassConstructorCannotTakeArguments(m, fragment));\n+                        } else if (m.getTypeParameters().size() > 0) {\n+                            log.error(pos, Errors.AbstractValueClassConstructorCannotBeGeneric(m, fragment));\n+                        } else if (m.type.getThrownTypes().size() > 0) {\n+                            log.error(pos, Errors.AbstractValueClassConstructorCannotThrow(m, fragment));\n+                        } else if (protection(m.flags()) > protection(m.owner.flags())) {\n+                            log.error(pos, Errors.AbstractValueClassConstructorHasWeakerAccess(m, fragment));\n+                        } else if ((m.flags() & EMPTYNOARGCONSTR) == 0) {\n+                                log.error(pos, Errors.AbstractValueClassNoArgConstructorMustBeEmpty(m, fragment));\n+                        }\n+                    }\n+                    break;\n+                }\n+            }\n+        }\n+    }\n+\n@@ -764,2 +825,2 @@\n-    Type checkConstructorRefType(DiagnosticPosition pos, Type t) {\n-        t = checkClassOrArrayType(pos, t);\n+    Type checkConstructorRefType(JCExpression expr, Type t) {\n+        t = checkClassOrArrayType(expr, t);\n@@ -768,1 +829,1 @@\n-                log.error(pos, Errors.AbstractCantBeInstantiated(t.tsym));\n+                log.error(expr, Errors.AbstractCantBeInstantiated(t.tsym));\n@@ -771,1 +832,1 @@\n-                log.error(pos, Errors.EnumCantBeInstantiated);\n+                log.error(expr, Errors.EnumCantBeInstantiated);\n@@ -774,1 +835,10 @@\n-                t = checkClassType(pos, t, true);\n+                \/\/ Projection types may not be mentioned in constructor references\n+                if (expr.hasTag(SELECT)) {\n+                    JCFieldAccess fieldAccess = (JCFieldAccess) expr;\n+                    if (allowPrimitiveClasses && fieldAccess.selected.type.isPrimitiveClass() &&\n+                            (fieldAccess.name == names.ref || fieldAccess.name == names.val)) {\n+                        log.error(expr, Errors.ProjectionCantBeInstantiated);\n+                        t = types.createErrorType(t);\n+                    }\n+                }\n+                t = checkClassType(expr, t, true);\n@@ -778,1 +848,1 @@\n-                log.error(pos, Errors.GenericArrayCreation);\n+                log.error(expr, Errors.GenericArrayCreation);\n@@ -809,0 +879,1 @@\n+     *  @param primitiveClassOK       If false, a primitive class does not qualify\n@@ -810,2 +881,2 @@\n-    Type checkRefType(DiagnosticPosition pos, Type t) {\n-        if (t.isReference())\n+    Type checkRefType(DiagnosticPosition pos, Type t, boolean primitiveClassOK) {\n+        if (t.isReference() && (!allowPrimitiveClasses || primitiveClassOK || !t.isPrimitiveClass()))\n@@ -819,0 +890,31 @@\n+    \/** Check that type is an identity type, i.e. not a primitive\/value type\n+     *  nor its reference projection. When not discernible statically,\n+     *  give it the benefit of doubt and defer to runtime.\n+     *\n+     *  @param pos           Position to be used for error reporting.\n+     *  @param t             The type to be checked.\n+     *\/\n+    void checkIdentityType(DiagnosticPosition pos, Type t) {\n+        if (t.hasTag(TYPEVAR)) {\n+            t = types.skipTypeVars(t, false);\n+        }\n+        if (t.isIntersection()) {\n+            IntersectionClassType ict = (IntersectionClassType)t;\n+            for (Type component : ict.getExplicitComponents()) {\n+                checkIdentityType(pos, component);\n+            }\n+            return;\n+        }\n+        if (t.isPrimitive() || t.isValueClass() || t.isValueInterface() || t.isReferenceProjection())\n+            typeTagError(pos, diags.fragment(Fragments.TypeReqIdentity), t);\n+    }\n+\n+    \/** Check that type is a reference type, i.e. a class, interface or array type\n+     *  or a type variable.\n+     *  @param pos           Position to be used for error reporting.\n+     *  @param t             The type to be checked.\n+     *\/\n+    Type checkRefType(DiagnosticPosition pos, Type t) {\n+        return checkRefType(pos, t, true);\n+    }\n+\n@@ -827,1 +929,1 @@\n-            l.head = checkRefType(tl.head.pos(), l.head);\n+            l.head = checkRefType(tl.head.pos(), l.head, false);\n@@ -863,0 +965,49 @@\n+    void checkParameterizationByPrimitiveClass(DiagnosticPosition pos, Type t) {\n+        parameterizationByPrimitiveClassChecker.visit(t, pos);\n+    }\n+\n+    \/** parameterizationByPrimitiveClassChecker: A type visitor that descends down the given type looking for instances of primitive classes\n+     *  being used as type arguments and issues error against those usages.\n+     *\/\n+    private final Types.SimpleVisitor<Void, DiagnosticPosition> parameterizationByPrimitiveClassChecker =\n+            new Types.SimpleVisitor<Void, DiagnosticPosition>() {\n+\n+        @Override\n+        public Void visitType(Type t, DiagnosticPosition pos) {\n+            return null;\n+        }\n+\n+        @Override\n+        public Void visitClassType(ClassType t, DiagnosticPosition pos) {\n+            for (Type targ : t.allparams()) {\n+                if (allowPrimitiveClasses && targ.isPrimitiveClass()) {\n+                    log.error(pos, Errors.GenericParameterizationWithPrimitiveClass(t));\n+                }\n+                visit(targ, pos);\n+            }\n+            return null;\n+        }\n+\n+        @Override\n+        public Void visitTypeVar(TypeVar t, DiagnosticPosition pos) {\n+             return null;\n+        }\n+\n+        @Override\n+        public Void visitCapturedType(CapturedType t, DiagnosticPosition pos) {\n+            return null;\n+        }\n+\n+        @Override\n+        public Void visitArrayType(ArrayType t, DiagnosticPosition pos) {\n+            return visit(t.elemtype, pos);\n+        }\n+\n+        @Override\n+        public Void visitWildcardType(WildcardType t, DiagnosticPosition pos) {\n+            return visit(t.type, pos);\n+        }\n+    };\n+\n+\n+\n@@ -995,1 +1146,1 @@\n-                (s.isConstructor() ||\n+                (s.isInitOrVNew() ||\n@@ -1011,1 +1162,5 @@\n-        return types.upward(t, types.captures(t)).baseType();\n+        Type varType = types.upward(t, types.captures(t)).baseType();\n+        if (allowPrimitiveClasses && varType.hasTag(CLASS)) {\n+            checkParameterizationByPrimitiveClass(pos, varType);\n+        }\n+        return varType;\n@@ -1034,0 +1189,1 @@\n+        \/\/ TODO - is enum so <init>\n@@ -1209,1 +1365,1 @@\n-            else\n+            else {\n@@ -1211,0 +1367,4 @@\n+                if (sym.owner.type.isValueClass() && (flags & STATIC) == 0) {\n+                    implicit |= FINAL;\n+                }\n+            }\n@@ -1213,1 +1373,1 @@\n-            if (sym.name == names.init) {\n+            if (names.isInitOrVNew(sym.name)) {\n@@ -1236,1 +1396,2 @@\n-                mask = RecordMethodFlags;\n+                mask = ((sym.owner.flags_field & VALUE_CLASS) != 0 && (flags & Flags.STATIC) == 0) ?\n+                        RecordMethodFlags & ~SYNCHRONIZED : RecordMethodFlags;\n@@ -1238,1 +1399,3 @@\n-                mask = MethodFlags;\n+                \/\/ value objects do not have an associated monitor\/lock\n+                mask = ((sym.owner.flags_field & VALUE_CLASS) != 0 && (flags & Flags.STATIC) == 0) ?\n+                        MethodFlags & ~SYNCHRONIZED : MethodFlags;\n@@ -1255,1 +1418,1 @@\n-                mask = staticOrImplicitlyStatic && allowRecords && (flags & ANNOTATION) == 0 ? StaticLocalFlags : LocalClassFlags;\n+                mask = staticOrImplicitlyStatic && allowRecords && (flags & ANNOTATION) == 0 ? ExtendedStaticLocalClassFlags : ExtendedLocalClassFlags;\n@@ -1275,2 +1438,2 @@\n-                \/\/ enums can't be declared abstract, final, sealed or non-sealed\n-                mask &= ~(ABSTRACT | FINAL | SEALED | NON_SEALED);\n+                \/\/ enums can't be declared abstract, final, sealed or non-sealed or primitive\/value\n+                mask &= ~(ABSTRACT | FINAL | SEALED | NON_SEALED | PRIMITIVE_CLASS | VALUE_CLASS);\n@@ -1289,0 +1452,17 @@\n+\n+            \/\/ primitive classes are implicitly final value classes.\n+            if ((flags & PRIMITIVE_CLASS) != 0)\n+                implicit |= VALUE_CLASS | FINAL;\n+\n+            \/\/ concrete value classes are implicitly final\n+            if ((flags & (ABSTRACT | INTERFACE | VALUE_CLASS)) == VALUE_CLASS) {\n+                implicit |= FINAL;\n+                if ((flags & NON_SEALED) != 0) {\n+                    \/\/ cant declare a final value class non-sealed\n+                    log.error(pos,\n+                            Errors.ModNotAllowedHere(asFlagSet(NON_SEALED)));\n+                }\n+            }\n+\n+            \/\/ TYPs can't be declared synchronized\n+            mask &= ~SYNCHRONIZED;\n@@ -1317,1 +1497,5 @@\n-                               FINAL | NATIVE | SYNCHRONIZED)\n+                               FINAL | NATIVE | SYNCHRONIZED | PRIMITIVE_CLASS)\n+                 &&\n+                 checkDisjoint(pos, flags,\n+                        IDENTITY_TYPE,\n+                        PRIMITIVE_CLASS | VALUE_CLASS)\n@@ -1327,1 +1511,1 @@\n-                 checkDisjoint(pos, flags,\n+                 checkDisjoint(pos, (flags | implicit), \/\/ complain against volatile & implcitly final entities too.\n@@ -1343,1 +1527,7 @@\n-                                ANNOTATION)) {\n+                                ANNOTATION)\n+                 && checkDisjoint(pos, flags,\n+                                IDENTITY_TYPE,\n+                                ANNOTATION)\n+                && checkDisjoint(pos, flags,\n+                                VALUE_CLASS,\n+                                ANNOTATION) ) {\n@@ -1516,1 +1706,2 @@\n-                tree.selected.type.isParameterized()) {\n+                tree.selected.type.isParameterized() &&\n+                    (tree.name != names.ref || !tree.type.isReferenceProjection())) {\n@@ -1520,0 +1711,2 @@\n+                \/\/ Tolerate the pseudo-select V.ref: V<T>.ref will be static if V<T> is and\n+                \/\/ should not be confused as selecting a static member of a parameterized type.\n@@ -1583,1 +1776,1 @@\n-                    env.enclMethod != null && env.enclMethod.name == names.init;\n+                    env.enclMethod != null && names.isInitOrVNew(env.enclMethod.name);\n@@ -2184,1 +2377,1 @@\n-                (env.info.isAnonymousDiamond && !m.isConstructor() && !m.isPrivate());\n+                (env.info.isAnonymousDiamond && !m.isInitOrVNew() && !m.isPrivate());\n@@ -2340,0 +2533,39 @@\n+    \/\/ A primitive class cannot contain a field of its own type either or indirectly.\n+    void checkNonCyclicMembership(JCClassDecl tree) {\n+        if (allowPrimitiveClasses) {\n+            Assert.check((tree.sym.flags_field & LOCKED) == 0);\n+            try {\n+                tree.sym.flags_field |= LOCKED;\n+                for (List<? extends JCTree> l = tree.defs; l.nonEmpty(); l = l.tail) {\n+                    if (l.head.hasTag(VARDEF)) {\n+                        JCVariableDecl field = (JCVariableDecl) l.head;\n+                        if (cyclePossible(field.sym)) {\n+                            checkNonCyclicMembership((ClassSymbol) field.type.tsym, field.pos());\n+                        }\n+                    }\n+                }\n+            } finally {\n+                tree.sym.flags_field &= ~LOCKED;\n+            }\n+        }\n+    }\n+    \/\/ where\n+    private void checkNonCyclicMembership(ClassSymbol c, DiagnosticPosition pos) {\n+        if ((c.flags_field & LOCKED) != 0) {\n+            log.error(pos, Errors.CyclicPrimitiveClassMembership(c));\n+            return;\n+        }\n+        try {\n+            c.flags_field |= LOCKED;\n+            for (Symbol fld : c.members().getSymbols(s -> s.kind == VAR && cyclePossible((VarSymbol) s), NON_RECURSIVE)) {\n+                checkNonCyclicMembership((ClassSymbol) fld.type.tsym, pos);\n+            }\n+        } finally {\n+            c.flags_field &= ~LOCKED;\n+        }\n+    }\n+        \/\/ where\n+        private boolean cyclePossible(VarSymbol symbol) {\n+            return (symbol.flags() & STATIC) == 0 && allowPrimitiveClasses && symbol.type.isPrimitiveClass();\n+        }\n+\n@@ -2588,0 +2820,22 @@\n+\n+        boolean cIsValue = (c.tsym.flags() & VALUE_CLASS) != 0;\n+        boolean cHasIdentity = (c.tsym.flags() & IDENTITY_TYPE) != 0;\n+        Type identitySuper = null, valueSuper = null;\n+        for (Type t : types.closure(c)) {\n+            if (t != c) {\n+                if ((t.tsym.flags() & IDENTITY_TYPE) != 0)\n+                    identitySuper = t;\n+                else if ((t.tsym.flags() & VALUE_CLASS) != 0)\n+                    valueSuper = t;\n+                if (cIsValue &&  identitySuper != null) {\n+                    log.error(pos, Errors.ValueTypeHasIdentitySuperType(c, identitySuper));\n+                    break;\n+                } else if (cHasIdentity &&  valueSuper != null) {\n+                    log.error(pos, Errors.IdentityTypeHasValueSuperType(c, valueSuper));\n+                    break;\n+                } else if (identitySuper != null && valueSuper != null) {\n+                    log.error(pos, Errors.MutuallyIncompatibleSupers(c, identitySuper, valueSuper));\n+                    break;\n+                }\n+            }\n+        }\n@@ -2683,1 +2937,1 @@\n-                     !s.isConstructor();\n+                     !s.isInitOrVNew();\n@@ -2742,1 +2996,1 @@\n-                     !s.isConstructor();\n+                     !s.isInitOrVNew();\n@@ -3625,1 +3879,1 @@\n-                if (s.kind == MTH && !s.isConstructor())\n+                if (s.kind == MTH && !s.isInitOrVNew())\n@@ -3633,1 +3887,1 @@\n-                if (s.kind == MTH && s.isConstructor())\n+                if (s.kind == MTH && s.isInitOrVNew())\n@@ -3652,1 +3906,1 @@\n-                        (s.kind == MTH && !s.isConstructor() &&\n+                        (s.kind == MTH && !s.isInitOrVNew() &&\n@@ -3654,1 +3908,1 @@\n-                        (s.kind == MTH && s.isConstructor())) {\n+                        (s.kind == MTH && s.isInitOrVNew())) {\n@@ -4977,1 +5231,1 @@\n-                    if (sym.isConstructor() &&\n+                    if (sym.isInitOrVNew() &&\n@@ -5005,1 +5259,1 @@\n-                        if (sym.isConstructor()) {\n+                        if (sym.isInitOrVNew()) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Check.java","additions":286,"deletions":32,"binary":false,"changes":318,"status":"modified"},{"patch":"@@ -58,0 +58,2 @@\n+import static com.sun.tools.javac.comp.Flow.ThisExposability.ALLOWED;\n+import static com.sun.tools.javac.comp.Flow.ThisExposability.BANNED;\n@@ -1599,1 +1601,1 @@\n-                    if (types.asSuper(sup, syms.autoCloseableType.tsym) != null) {\n+                    if (types.asSuper(sup.referenceProjectionOrSelf(), syms.autoCloseableType.tsym) != null) {\n@@ -2015,0 +2017,8 @@\n+    \/** Enum to model whether constructors allowed to \"leak\" this reference before\n+        all instance fields are DA.\n+     *\/\n+    enum ThisExposability {\n+        ALLOWED,     \/\/ identity Object classes - NOP\n+        BANNED,      \/\/ primitive\/value classes - Error\n+    }\n+\n@@ -2103,0 +2113,3 @@\n+        \/\/ Are constructors allowed to leak this reference ?\n+        ThisExposability thisExposability = ALLOWED;\n+\n@@ -2228,0 +2241,22 @@\n+        void checkEmbryonicThisExposure(JCTree node) {\n+            if (this.thisExposability == ALLOWED || classDef == null)\n+                return;\n+\n+            \/\/ Note: for non-initial constructors, firstadr is post all instance fields.\n+            for (int i = firstadr; i < nextadr; i++) {\n+                VarSymbol sym = vardecls[i].sym;\n+                if (sym.owner != classDef.sym)\n+                    continue;\n+                if ((sym.flags() & (FINAL | HASINIT | STATIC | PARAMETER)) != FINAL)\n+                    continue;\n+                if (sym.pos < startPos || sym.adr < firstadr)\n+                    continue;\n+                if (!inits.isMember(sym.adr)) {\n+                    if (this.thisExposability == BANNED) {\n+                        log.error(node, Errors.ThisExposedPrematurely);\n+                    }\n+                    return; \/\/ don't flog a dead horse.\n+                }\n+            }\n+        }\n+\n@@ -2433,0 +2468,1 @@\n+            ThisExposability priorThisExposability = this.thisExposability;\n@@ -2447,0 +2483,6 @@\n+                        this.thisExposability = ALLOWED;\n+                    } else {\n+                        if (tree.sym.owner.type.isValueClass())\n+                            this.thisExposability = BANNED;\n+                        else\n+                            this.thisExposability = ALLOWED;\n@@ -2509,0 +2551,1 @@\n+                this.thisExposability = priorThisExposability;\n@@ -2977,0 +3020,5 @@\n+            if (tree.meth.hasTag(IDENT)) {\n+                JCIdent ident = (JCIdent) tree.meth;\n+                if (ident.name != names._super && !ident.sym.isStatic())\n+                    checkEmbryonicThisExposure(tree);\n+            }\n@@ -2983,0 +3031,6 @@\n+            if (classDef != null && tree.encl == null && tree.clazz.hasTag(IDENT)) {\n+                JCIdent clazz = (JCIdent) tree.clazz;\n+                if (!clazz.sym.isStatic() && clazz.type.getEnclosingType().tsym == classDef.sym) {\n+                    checkEmbryonicThisExposure(tree);\n+                }\n+            }\n@@ -3052,1 +3106,8 @@\n-            super.visitSelect(tree);\n+            ThisExposability priorThisExposability = this.thisExposability;\n+            try {\n+                if (tree.name == names._this && classDef != null && tree.sym.owner == classDef.sym) {\n+                    checkEmbryonicThisExposure(tree);\n+                } else if (tree.sym.kind == VAR || tree.sym.isStatic()) {\n+                    this.thisExposability = ALLOWED;\n+                }\n+                super.visitSelect(tree);\n@@ -3055,1 +3116,4 @@\n-                checkInit(tree.pos(), (VarSymbol)tree.sym);\n+                    checkInit(tree.pos(), (VarSymbol)tree.sym);\n+                }\n+            } finally {\n+                 this.thisExposability = priorThisExposability;\n@@ -3119,0 +3183,3 @@\n+            if (tree.name == names._this) {\n+                checkEmbryonicThisExposure(tree);\n+            }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Flow.java","additions":70,"deletions":3,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+import com.sun.tools.javac.code.Flags.Flag;\n@@ -55,0 +56,1 @@\n+import static com.sun.tools.javac.code.Flags.asFlagSet;\n@@ -63,0 +65,1 @@\n+import static com.sun.tools.javac.parser.Tokens.TokenKind.SYNCHRONIZED;\n@@ -195,0 +198,2 @@\n+        this.allowPrimitiveClasses = Feature.PRIMITIVE_CLASSES.allowedInSource(source) && fac.options.isSet(\"enablePrimitiveClasses\");\n+        this.allowValueClasses = Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -218,0 +223,2 @@\n+        this.allowPrimitiveClasses = parser.allowPrimitiveClasses;\n+        this.allowValueClasses = parser.allowValueClasses;\n@@ -255,0 +262,8 @@\n+    \/** Switch: are primitive classes allowed in this source level?\n+     *\/\n+     boolean allowPrimitiveClasses;\n+\n+    \/** Switch: are value classes allowed in this source level?\n+     *\/\n+    boolean allowValueClasses;\n+\n@@ -518,0 +533,16 @@\n+    \/** If next input token matches one of the two given tokens, skip it, otherwise report\n+     *  an error.\n+     *\n+     * @return The actual token kind.\n+     *\/\n+    public TokenKind accept2(TokenKind tk1, TokenKind tk2) {\n+        TokenKind returnValue = token.kind;\n+        if (token.kind == tk1 || token.kind == tk2) {\n+            nextToken();\n+        } else {\n+            setErrorEndPos(token.pos);\n+            reportSyntaxError(S.prevToken().endPos, Errors.Expected2(tk1, tk2));\n+        }\n+        return returnValue;\n+    }\n+\n@@ -1508,0 +1539,6 @@\n+                            case DEFAULT:\n+                                if (typeArgs != null) return illegal();\n+                                selectExprMode();\n+                                t = to(F.at(pos).DefaultValue(t));\n+                                nextToken();\n+                                break loop;\n@@ -1571,3 +1608,4 @@\n-                        if (!isMode(TYPE) && isUnboundMemberRef()) {\n-                            \/\/this is an unbound method reference whose qualifier\n-                            \/\/is a generic type i.e. A<S>::m\n+                        if (!isMode(TYPE) && isParameterizedTypePrefix()) {\n+                            \/\/this is either an unbound method reference whose qualifier\n+                            \/\/is a generic type i.e. A<S>::m or a default value creation of\n+                            \/\/the form ValueType<S>.default\n@@ -1586,0 +1624,6 @@\n+                                if (token.kind == DEFAULT) {\n+                                    t =  toP(F.at(token.pos).DefaultValue(t));\n+                                    nextToken();\n+                                    selectExprMode();\n+                                    return term3Rest(t, typeArgs);\n+                                }\n@@ -1816,1 +1860,2 @@\n-     * method reference or a binary expression. To disambiguate, look for a\n+     * method reference or a default value creation that uses a parameterized type\n+     * or a binary expression. To disambiguate, look for a\n@@ -1820,1 +1865,1 @@\n-    boolean isUnboundMemberRef() {\n+    boolean isParameterizedTypePrefix() {\n@@ -2427,1 +2472,1 @@\n-            accept(CLASS);\n+            TokenKind selector = accept2(CLASS, DEFAULT);\n@@ -2445,1 +2490,5 @@\n-                t = toP(F.at(pos).Select(t, names._class));\n+                if (selector == CLASS) {\n+                    t = toP(F.at(pos).Select(t, names._class));\n+                } else {\n+                    t = toP(F.at(pos).DefaultValue(t));\n+                }\n@@ -2477,0 +2526,1 @@\n+            \/\/ TODO - will be converted in Attr\n@@ -2489,2 +2539,2 @@\n-        List<JCAnnotation> newAnnotations = typeAnnotationsOpt();\n-\n+        final JCModifiers mods = modifiersOpt();\n+        List<JCAnnotation> newAnnotations = mods.annotations;\n@@ -2494,0 +2544,3 @@\n+            if (mods.flags != 0) {\n+                log.error(token.pos, Errors.ModNotAllowedHere(asFlagSet(mods.flags)));\n+            }\n@@ -2562,0 +2615,3 @@\n+            long badModifiers = mods.flags & ~(Flags.PRIMITIVE_CLASS | Flags.VALUE_CLASS | Flags.FINAL);\n+            if (badModifiers != 0)\n+                log.error(token.pos, Errors.ModNotAllowedHere(asFlagSet(badModifiers)));\n@@ -2566,1 +2622,5 @@\n-            return classCreatorRest(newpos, null, typeArgs, t);\n+            JCNewClass newClass = classCreatorRest(newpos, null, typeArgs, t, mods.flags);\n+            if ((newClass.def == null) && (mods.flags != 0)) {\n+                log.error(newClass.pos, Errors.ModNotAllowedHere(asFlagSet(mods.flags)));\n+            }\n+            return newClass;\n@@ -2591,1 +2651,1 @@\n-        return classCreatorRest(newpos, encl, typeArgs, t);\n+        return classCreatorRest(newpos, encl, typeArgs, t, 0);\n@@ -2669,1 +2729,2 @@\n-                                  JCExpression t)\n+                                  JCExpression t,\n+                                  long flags)\n@@ -2676,1 +2737,1 @@\n-            JCModifiers mods = F.at(Position.NOPOS).Modifiers(0);\n+            JCModifiers mods = F.at(Position.NOPOS).Modifiers(flags);\n@@ -2679,1 +2740,2 @@\n-        return toP(F.at(newpos).NewClass(encl, typeArgs, t, args, body));\n+        JCNewClass newClass = toP(F.at(newpos).NewClass(encl, typeArgs, t, args, body));\n+        return newClass;\n@@ -2910,0 +2972,4 @@\n+        if ((isPrimitiveModifier() && allowPrimitiveClasses) || (isValueModifier() || isIdentityModifier()) && allowValueClasses) {\n+            dc = token.comment(CommentStyle.JAVADOC);\n+            return List.of(classOrRecordOrInterfaceOrEnumDeclaration(modifiersOpt(), dc));\n+        }\n@@ -3401,1 +3467,4 @@\n-                return variableDeclarators(modifiersOpt(), t, stats, true).toList();\n+                pos = token.pos;\n+                JCModifiers mods = F.at(Position.NOPOS).Modifiers(0);\n+                F.at(pos);\n+                return variableDeclarators(mods, t, stats, true).toList();\n@@ -3498,0 +3567,12 @@\n+                if (isPrimitiveModifier()) {\n+                    flag = Flags.PRIMITIVE_CLASS;\n+                    break;\n+                }\n+                if (isValueModifier()) {\n+                    flag = Flags.VALUE_CLASS;\n+                    break;\n+                }\n+                if (isIdentityModifier()) {\n+                    flag = Flags.IDENTITY_TYPE;\n+                    break;\n+                }\n@@ -3764,0 +3845,19 @@\n+        if (name == names.primitive) {\n+            if (allowPrimitiveClasses) {\n+                return Source.JDK18;\n+            }\n+        }\n+        if (name == names.value) {\n+            if (allowValueClasses) {\n+                return Source.JDK18;\n+            } else if (shouldWarn) {\n+                log.warning(pos, Warnings.RestrictedTypeNotAllowedPreview(name, Source.JDK18));\n+            }\n+        }\n+        if (name == names.identity) {\n+            if (allowPrimitiveClasses) {\n+                return Source.JDK18;\n+            } else if (shouldWarn) {\n+                log.warning(pos, Warnings.RestrictedTypeNotAllowedPreview(name, Source.JDK18));\n+            }\n+        }\n@@ -4288,1 +4388,2 @@\n-                if (methDef.name == names.init && methDef.params.isEmpty() && (methDef.mods.flags & Flags.COMPACT_RECORD_CONSTRUCTOR) != 0) {\n+                \/\/ TODO - specifically for record.\n+                if (names.isInitOrVNew(methDef.name) && methDef.params.isEmpty() && (methDef.mods.flags & Flags.COMPACT_RECORD_CONSTRUCTOR) != 0) {\n@@ -4829,0 +4930,78 @@\n+    protected boolean isPrimitiveModifier() {\n+        if (token.kind == IDENTIFIER && token.name() == names.primitive) {\n+            boolean isPrimitiveModifier = false;\n+            Token next = S.token(1);\n+            switch (next.kind) {\n+                case PRIVATE: case PROTECTED: case PUBLIC: case STATIC: case TRANSIENT:\n+                case FINAL: case ABSTRACT: case NATIVE: case VOLATILE: case SYNCHRONIZED:\n+                case STRICTFP: case MONKEYS_AT: case DEFAULT: case BYTE: case SHORT:\n+                case CHAR: case INT: case LONG: case FLOAT: case DOUBLE: case BOOLEAN: case VOID:\n+                case CLASS: case INTERFACE: case ENUM:\n+                    isPrimitiveModifier = true;\n+                    break;\n+                case IDENTIFIER: \/\/ primitive record R || primitive primitive || primitive identity || primitive value || new primitive Comparable() {}\n+                    if (next.name() == names.record || next.name() == names.primitive || next.name() == names.identity\n+                            || next.name() == names.value || (mode & EXPR) != 0)\n+                        isPrimitiveModifier = true;\n+                    break;\n+            }\n+            if (isPrimitiveModifier) {\n+                checkSourceLevel(Feature.PRIMITIVE_CLASSES);\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n+    protected boolean isValueModifier() {\n+        if (token.kind == IDENTIFIER && token.name() == names.value) {\n+            boolean isValueModifier = false;\n+            Token next = S.token(1);\n+            switch (next.kind) {\n+                case PRIVATE: case PROTECTED: case PUBLIC: case STATIC: case TRANSIENT:\n+                case FINAL: case ABSTRACT: case NATIVE: case VOLATILE: case SYNCHRONIZED:\n+                case STRICTFP: case MONKEYS_AT: case DEFAULT: case BYTE: case SHORT:\n+                case CHAR: case INT: case LONG: case FLOAT: case DOUBLE: case BOOLEAN: case VOID:\n+                case CLASS: case INTERFACE: case ENUM:\n+                    isValueModifier = true;\n+                    break;\n+                case IDENTIFIER: \/\/ value record R || value value || value identity || value primitive || new value Comparable() {} ??\n+                    if (next.name() == names.record || next.name() == names.value || next.name() == names.identity\n+                            || next.name() == names.primitive || (mode & EXPR) != 0)\n+                        isValueModifier = true;\n+                    break;\n+            }\n+            if (isValueModifier) {\n+                checkSourceLevel(Feature.VALUE_CLASSES);\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n+    protected boolean isIdentityModifier() {\n+        if (token.kind == IDENTIFIER && token.name() == names.identity) {\n+            boolean isIdentityModifier = false;\n+            Token next = S.token(1);\n+            switch (next.kind) {\n+                case PRIVATE: case PROTECTED: case PUBLIC: case STATIC: case TRANSIENT:\n+                case FINAL: case ABSTRACT: case NATIVE: case VOLATILE: case SYNCHRONIZED:\n+                case STRICTFP: case MONKEYS_AT: case DEFAULT: case BYTE: case SHORT:\n+                case CHAR: case INT: case LONG: case FLOAT: case DOUBLE: case BOOLEAN: case VOID:\n+                case CLASS: case INTERFACE: case ENUM:\n+                    isIdentityModifier = true;\n+                    break;\n+                case IDENTIFIER: \/\/ identity record R || identity primitive || || identity identity || identity value || new identity Comparable() {}\n+                    if (next.name() == names.record || next.name() == names.primitive || next.name() == names.identity\n+                            || next.name() == names.value || (mode & EXPR) != 0)\n+                        isIdentityModifier = true;\n+                    break;\n+            }\n+            if (isIdentityModifier) {\n+                checkSourceLevel(Feature.VALUE_CLASSES);\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n@@ -4856,1 +5035,4 @@\n-                case IDENTIFIER -> isNonSealedIdentifier(next, currentIsNonSealed ? 3 : 1) || next.name() == names.sealed;\n+                case IDENTIFIER -> isNonSealedIdentifier(next, currentIsNonSealed ? 3 : 1) ||\n+                        next.name() == names.sealed ||\n+                        next.name() == names.value ||\n+                        next.name() == names.identity;\n@@ -4887,1 +5069,1 @@\n-            if (!isRecord || name != names.init || token.kind == LPAREN) {\n+            if (!isRecord || !names.isInitOrVNew(name) || token.kind == LPAREN) {\n@@ -5343,1 +5525,4 @@\n-        if (preview.isPreview(feature) && !preview.isEnabled()) {\n+        if (feature == Feature.PRIMITIVE_CLASSES && !allowPrimitiveClasses) {\n+            \/\/ primitive classes are special\n+            log.error(DiagnosticFlag.SOURCE_LEVEL, pos, feature.error(source.name));\n+        } else if (preview.isPreview(feature) && !preview.isEnabled()) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":204,"deletions":19,"binary":false,"changes":223,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+    metadataConstructor.addMapping(\"InlineKlass\", InlineKlass.class);\n@@ -69,0 +70,1 @@\n+    metadataConstructor.addMapping(\"FlatArrayKlass\", FlatArrayKlass.class);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Metadata.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+    valueFactoryName = null;\n@@ -100,0 +101,1 @@\n+  private static String valueFactoryName;\n@@ -107,0 +109,6 @@\n+  private static String valueFactoryName() {\n+    if (valueFactoryName == null) {\n+      valueFactoryName = \"<vnew>\";\n+    }\n+    return classInitializerName;\n+  }\n@@ -256,1 +264,1 @@\n-     return (!isStatic()) && getName().equals(objectInitializerName());\n+     return (!isStatic()) && (getName().equals(objectInitializerName()) || getName().equals(valueFactoryName()));\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Method.java","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -364,0 +364,1 @@\n+        int nb_interfaces = len;\n@@ -365,1 +366,1 @@\n-        if (DEBUG) debugMessage(\"number of interfaces = \" + len);\n+        if (DEBUG) debugMessage(\"number of interfaces = \" + nb_interfaces);\n@@ -368,1 +369,1 @@\n-        dos.writeShort((short) len);\n+        dos.writeShort((short) nb_interfaces);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/tools\/jcore\/ClassWriter.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -74,0 +74,3 @@\n+compiler\/gcbarriers\/TestZGCBarrierElision.java#ZGen 8313737 generic-all\n+compiler\/valhalla\/inlinetypes\/TestArrays.java 8313667 generic-all\n+\n@@ -92,0 +95,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -112,0 +116,4 @@\n+# Valhalla\n+runtime\/AccModule\/ConstModule.java 8294051 generic-all\n+runtime\/valhalla\/inlinetypes\/InlineOops.java#ZGen 8313607 linux-aarch64,macosx-aarch64\n+\n@@ -134,0 +142,27 @@\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":35,"deletions":0,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -1627,1 +1627,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -1659,1 +1659,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -700,0 +700,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -755,0 +759,6 @@\n+jdk\/classfile\/SwapTest.java                                     8308778 generic-all\n+jdk\/classfile\/LowAdaptTest.java                                 8308778 generic-all\n+jdk\/classfile\/BuilderBlockTest.java                             8308778 generic-all\n+jdk\/classfile\/BuilderTryCatchTest.java                          8308778 generic-all\n+jdk\/classfile\/PrimitiveClassConstantTest.java                   8310649 generic-all\n+\n","filename":"test\/jdk\/ProblemList.txt","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -447,0 +447,4 @@\n+\n+        public String toString() {\n+            return super.toString() + \", vh:\" + vh;\n+        }\n@@ -467,0 +471,4 @@\n+\n+        public String toString() {\n+            return super.toString() + \", vh:\" + vh + \", f: \" + f;\n+        }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleBaseTest.java","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -386,0 +386,1 @@\n+        vmOptFinalFlag(map, \"TieredCompilation\");\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -154,0 +154,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"}]}