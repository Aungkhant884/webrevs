{"files":[{"patch":"@@ -7,0 +7,3 @@\n+      - lworld\n+      - type-restrictions\n+      - jep390\n@@ -12,1 +15,1 @@\n-        default: \"Linux additional (hotspot only), Linux x64, Linux x86, Windows x64, macOS x64\"\n+        default: \"Linux x64, Windows x64, macOS x64\"\n@@ -21,2 +24,0 @@\n-      platform_linux_additional: ${{ steps.check_platforms.outputs.platform_linux_additional }}\n-      platform_linux_x86: ${{ steps.check_platforms.outputs.platform_linux_x86 }}\n@@ -36,2 +37,0 @@\n-          echo \"::set-output name=platform_linux_additional::${{ contains(github.event.inputs.platforms, 'linux additional (hotspot only)') || (github.event.inputs.platforms == '' && (secrets.JDK_SUBMIT_PLATFORMS == '' || contains(secrets.JDK_SUBMIT_PLATFORMS, 'linux additional (hotspot only)'))) }}\"\n-          echo \"::set-output name=platform_linux_x86::${{ contains(github.event.inputs.platforms, 'linux x86') || (github.event.inputs.platforms == '' && (secrets.JDK_SUBMIT_PLATFORMS == '' || contains(secrets.JDK_SUBMIT_PLATFORMS, 'linux x86'))) }}\"\n@@ -105,1 +104,1 @@\n-    if: needs.prerequisites.outputs.should_run != 'false' && (needs.prerequisites.outputs.platform_linux_x64 != 'false' || needs.prerequisites.outputs.platform_linux_additional == 'true')\n+    if: needs.prerequisites.outputs.should_run != 'false' && needs.prerequisites.outputs.platform_linux_x64 != 'false'\n@@ -381,468 +380,0 @@\n-  linux_additional_build:\n-    name: Linux additional\n-    runs-on: \"ubuntu-20.04\"\n-    needs:\n-      - prerequisites\n-      - linux_x64_build\n-    if: needs.prerequisites.outputs.should_run != 'false' && needs.prerequisites.outputs.platform_linux_additional != 'false'\n-\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        flavor:\n-          - hs x64 build only\n-          - hs x64 zero build only\n-          - hs x64 minimal build only\n-          - hs x64 optimized build only\n-          - hs aarch64 build only\n-          - hs arm build only\n-          - hs s390x build only\n-          - hs ppc64le build only\n-        include:\n-          - flavor: hs x64 build only\n-            flags: --enable-debug --disable-precompiled-headers\n-          - flavor: hs x64 zero build only\n-            flags: --enable-debug --disable-precompiled-headers --with-jvm-variants=zero\n-          - flavor: hs x64 minimal build only\n-            flags: --enable-debug --disable-precompiled-headers --with-jvm-variants=minimal\n-          - flavor: hs x64 optimized build only\n-            flags: --with-debug-level=optimized --disable-precompiled-headers\n-          - flavor: hs aarch64 build only\n-            flags: --enable-debug --disable-precompiled-headers\n-            debian-arch: arm64\n-            gnu-arch: aarch64\n-          - flavor: hs arm build only\n-            flags: --enable-debug --disable-precompiled-headers\n-            debian-arch: armhf\n-            gnu-arch: arm\n-            gnu-flavor: eabihf\n-          - flavor: hs s390x build only\n-            flags: --enable-debug --disable-precompiled-headers\n-            debian-arch: s390x\n-            gnu-arch: s390x\n-          - flavor: hs ppc64le build only\n-            flags: --enable-debug --disable-precompiled-headers\n-            debian-arch: ppc64el\n-            gnu-arch: powerpc64le\n-\n-    env:\n-      JDK_VERSION: \"${{ fromJson(needs.prerequisites.outputs.dependencies).DEFAULT_VERSION_FEATURE }}\"\n-      BOOT_JDK_VERSION: \"${{ fromJson(needs.prerequisites.outputs.dependencies).BOOT_JDK_VERSION }}\"\n-      BOOT_JDK_FILENAME: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_FILENAME }}\"\n-      BOOT_JDK_URL: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_URL }}\"\n-      BOOT_JDK_SHA256: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_SHA256 }}\"\n-\n-    steps:\n-      - name: Checkout the source\n-        uses: actions\/checkout@v2\n-        with:\n-          path: jdk\n-\n-      - name: Restore boot JDK from cache\n-        id: bootjdk\n-        uses: actions\/cache@v2\n-        with:\n-          path: ~\/bootjdk\/${{ env.BOOT_JDK_VERSION }}\n-          key: bootjdk-${{ runner.os }}-${{ env.BOOT_JDK_VERSION }}-${{ env.BOOT_JDK_SHA256 }}-v1\n-\n-      - name: Download boot JDK\n-        run: |\n-          mkdir -p \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\"\n-          wget -O \"${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" \"${BOOT_JDK_URL}\"\n-          echo \"${BOOT_JDK_SHA256} ${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" | sha256sum -c >\/dev\/null -\n-          tar -xf \"${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" -C \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\"\n-          mv \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\/\"*\/* \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\/\"\n-        if: steps.bootjdk.outputs.cache-hit != 'true'\n-\n-      - name: Restore build JDK\n-        id: build_restore\n-        uses: actions\/download-artifact@v2\n-        with:\n-          name: transient_jdk-linux-x64_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: ~\/jdk-linux-x64\n-        continue-on-error: true\n-\n-      - name: Restore build JDK (retry)\n-        uses: actions\/download-artifact@v2\n-        with:\n-          name: transient_jdk-linux-x64_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: ~\/jdk-linux-x64\n-        if: steps.build_restore.outcome == 'failure'\n-\n-      - name: Unpack build JDK\n-        run: |\n-          mkdir -p \"${HOME}\/jdk-linux-x64\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x64_bin\"\n-          tar -xf \"${HOME}\/jdk-linux-x64\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x64_bin.tar.gz\" -C \"${HOME}\/jdk-linux-x64\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x64_bin\"\n-\n-      - name: Find root of build JDK image dir\n-        run: |\n-          build_jdk_root=`find ${HOME}\/jdk-linux-x64\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x64_bin -name release -type f`\n-          echo \"build_jdk_root=`dirname ${build_jdk_root}`\" >> $GITHUB_ENV\n-\n-      - name: Update apt\n-        run: sudo apt-get update\n-\n-      - name: Install native host dependencies\n-        run: |\n-          sudo apt-get install gcc-10=10.2.0-5ubuntu1~20.04 g++-10=10.2.0-5ubuntu1~20.04 libxrandr-dev libxtst-dev libcups2-dev libasound2-dev\n-          sudo update-alternatives --install \/usr\/bin\/gcc gcc \/usr\/bin\/gcc-10 100 --slave \/usr\/bin\/g++ g++ \/usr\/bin\/g++-10\n-        if: matrix.debian-arch == ''\n-\n-      - name: Install cross-compilation host dependencies\n-        run: sudo apt-get install gcc-10-${{ matrix.gnu-arch }}-linux-gnu${{ matrix.gnu-flavor}}=10.2.0-5ubuntu1~20.04cross1 g++-10-${{ matrix.gnu-arch }}-linux-gnu${{ matrix.gnu-flavor}}=10.2.0-5ubuntu1~20.04cross1\n-        if: matrix.debian-arch != ''\n-\n-      - name: Cache sysroot\n-        id: cache-sysroot\n-        uses: actions\/cache@v2\n-        with:\n-          path: ~\/sysroot-${{ matrix.debian-arch }}\/\n-          key: sysroot-${{ matrix.debian-arch }}-${{ hashFiles('jdk\/.github\/workflows\/submit.yml') }}\n-        if: matrix.debian-arch != ''\n-\n-      - name: Install sysroot host dependencies\n-        run: sudo apt-get install debootstrap qemu-user-static\n-        if: matrix.debian-arch != '' && steps.cache-sysroot.outputs.cache-hit != 'true'\n-\n-      - name: Create sysroot\n-        run: >\n-          sudo qemu-debootstrap\n-          --arch=${{ matrix.debian-arch }}\n-          --verbose\n-          --include=fakeroot,symlinks,build-essential,libx11-dev,libxext-dev,libxrender-dev,libxrandr-dev,libxtst-dev,libxt-dev,libcups2-dev,libfontconfig1-dev,libasound2-dev,libfreetype6-dev,libpng-dev\n-          --resolve-deps\n-          buster\n-          ~\/sysroot-${{ matrix.debian-arch }}\n-          http:\/\/httpredir.debian.org\/debian\/\n-        if: matrix.debian-arch != '' && steps.cache-sysroot.outputs.cache-hit != 'true'\n-\n-      - name: Prepare sysroot for caching\n-        run: |\n-          sudo chroot ~\/sysroot-${{ matrix.debian-arch }} symlinks -cr .\n-          sudo chown ${USER} -R ~\/sysroot-${{ matrix.debian-arch }}\n-          rm -rf ~\/sysroot-${{ matrix.debian-arch }}\/{dev,proc,run,sys}\n-        if: matrix.debian-arch != '' && steps.cache-sysroot.outputs.cache-hit != 'true'\n-\n-      - name: Configure cross compiler\n-        run: |\n-          echo \"CC=${{ matrix.gnu-arch }}-linux-gnu${{ matrix.gnu-flavor}}-gcc-10\" >> $GITHUB_ENV\n-          echo \"CXX=${{ matrix.gnu-arch }}-linux-gnu${{ matrix.gnu-flavor}}-g++-10\" >> $GITHUB_ENV\n-        if: matrix.debian-arch != ''\n-\n-      - name: Configure cross specific flags\n-        run: >\n-          echo \"cross_flags=\n-          --openjdk-target=${{ matrix.gnu-arch }}-linux-gnu${{ matrix.gnu-flavor}}\n-          --with-sysroot=${HOME}\/sysroot-${{ matrix.debian-arch }}\/\n-          --with-toolchain-path=${HOME}\/sysroot-${{ matrix.debian-arch }}\/\n-          --with-freetype-lib=${HOME}\/sysroot-${{ matrix.debian-arch }}\/usr\/lib\/${{ matrix.gnu-arch }}-linux-gnu${{ matrix.gnu-flavor}}\/\n-          --with-freetype-include=${HOME}\/sysroot-${{ matrix.debian-arch }}\/usr\/include\/freetype2\/\n-          --x-libraries=${HOME}\/sysroot-${{ matrix.debian-arch }}\/usr\/lib\/${{ matrix.gnu-arch }}-linux-gnu${{ matrix.gnu-flavor}}\/\n-          \" >> $GITHUB_ENV\n-        if: matrix.debian-arch != ''\n-\n-      - name: Configure\n-        run: >\n-          bash configure\n-          --with-conf-name=linux-${{ matrix.gnu-arch }}-hotspot\n-          ${{ matrix.flags }}\n-          ${{ env.cross_flags }}\n-          --with-version-opt=${GITHUB_ACTOR}-${GITHUB_SHA}\n-          --with-version-build=0\n-          --with-boot-jdk=${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\n-          --with-build-jdk=${{ env.build_jdk_root }}\n-          --with-default-make-target=\"hotspot\"\n-          --with-zlib=system\n-        working-directory: jdk\n-\n-      - name: Build\n-        run: make CONF_NAME=linux-${{ matrix.gnu-arch }}-hotspot\n-        working-directory: jdk\n-\n-  linux_x86_build:\n-    name: Linux x86\n-    runs-on: \"ubuntu-20.04\"\n-    needs: prerequisites\n-    if: needs.prerequisites.outputs.should_run != 'false' && needs.prerequisites.outputs.platform_linux_x86 != 'false'\n-\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        flavor:\n-          - build release\n-          - build debug\n-        include:\n-          - flavor: build debug\n-            flags: --enable-debug\n-            artifact: -debug\n-\n-    # Reduced 32-bit build uses the same boot JDK as 64-bit build\n-    env:\n-      JDK_VERSION: \"${{ fromJson(needs.prerequisites.outputs.dependencies).DEFAULT_VERSION_FEATURE }}\"\n-      BOOT_JDK_VERSION: \"${{ fromJson(needs.prerequisites.outputs.dependencies).BOOT_JDK_VERSION }}\"\n-      BOOT_JDK_FILENAME: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_FILENAME }}\"\n-      BOOT_JDK_URL: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_URL }}\"\n-      BOOT_JDK_SHA256: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_SHA256 }}\"\n-\n-    steps:\n-      - name: Checkout the source\n-        uses: actions\/checkout@v2\n-        with:\n-          path: jdk\n-\n-      - name: Restore boot JDK from cache\n-        id: bootjdk\n-        uses: actions\/cache@v2\n-        with:\n-          path: ~\/bootjdk\/${{ env.BOOT_JDK_VERSION }}\n-          key: bootjdk-${{ runner.os }}-${{ env.BOOT_JDK_VERSION }}-${{ env.BOOT_JDK_SHA256 }}-v1\n-\n-      - name: Download boot JDK\n-        run: |\n-          mkdir -p \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\"\n-          wget -O \"${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" \"${BOOT_JDK_URL}\"\n-          echo \"${BOOT_JDK_SHA256} ${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" | sha256sum -c >\/dev\/null -\n-          tar -xf \"${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" -C \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\"\n-          mv \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\/\"*\/* \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\/\"\n-        if: steps.bootjdk.outputs.cache-hit != 'true'\n-\n-      - name: Restore jtreg artifact\n-        id: jtreg_restore\n-        uses: actions\/download-artifact@v2\n-        with:\n-          name: transient_jtreg_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: ~\/jtreg\/\n-        continue-on-error: true\n-\n-      - name: Restore jtreg artifact (retry)\n-        uses: actions\/download-artifact@v2\n-        with:\n-          name: transient_jtreg_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: ~\/jtreg\/\n-        if: steps.jtreg_restore.outcome == 'failure'\n-\n-      - name: Checkout gtest sources\n-        uses: actions\/checkout@v2\n-        with:\n-          repository: \"google\/googletest\"\n-          ref: \"release-${{ fromJson(needs.prerequisites.outputs.dependencies).GTEST_VERSION }}\"\n-          path: gtest\n-\n-      # Roll in the multilib environment and its dependencies.\n-      # Some multilib libraries do not have proper inter-dependencies, so we have to\n-      # install their dependencies manually.\n-      - name: Install dependencies\n-        run: |\n-          sudo dpkg --add-architecture i386\n-          sudo apt-get update\n-          sudo apt-get install gcc-10-multilib g++-10-multilib libfreetype6-dev:i386 libxrandr-dev:i386 libxtst-dev:i386 libtiff-dev:i386 libcupsimage2-dev:i386 libcups2-dev:i386 libasound2-dev:i386\n-          sudo update-alternatives --install \/usr\/bin\/gcc gcc \/usr\/bin\/gcc-10 100 --slave \/usr\/bin\/g++ g++ \/usr\/bin\/g++-10\n-\n-      - name: Configure\n-        run: >\n-          bash configure\n-          --with-conf-name=linux-x86\n-          --with-target-bits=32\n-          ${{ matrix.flags }}\n-          --with-version-opt=${GITHUB_ACTOR}-${GITHUB_SHA}\n-          --with-version-build=0\n-          --with-boot-jdk=${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\n-          --with-jtreg=${HOME}\/jtreg\n-          --with-gtest=${GITHUB_WORKSPACE}\/gtest\n-          --with-default-make-target=\"product-bundles test-bundles\"\n-          --with-zlib=system\n-          --enable-jtreg-failure-handler\n-        working-directory: jdk\n-\n-      - name: Build\n-        run: make CONF_NAME=linux-x86\n-        working-directory: jdk\n-\n-      - name: Persist test bundles\n-        uses: actions\/upload-artifact@v2\n-        with:\n-          name: transient_jdk-linux-x86${{ matrix.artifact }}_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: |\n-            jdk\/build\/linux-x86\/bundles\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin${{ matrix.artifact }}.tar.gz\n-            jdk\/build\/linux-x86\/bundles\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin-tests${{ matrix.artifact }}.tar.gz\n-\n-  linux_x86_test:\n-    name: Linux x86\n-    runs-on: \"ubuntu-20.04\"\n-    needs:\n-      - prerequisites\n-      - linux_x86_build\n-\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        test:\n-          - jdk\/tier1 part 1\n-          - jdk\/tier1 part 2\n-          - jdk\/tier1 part 3\n-          - langtools\/tier1\n-          - hs\/tier1 common\n-          - hs\/tier1 compiler\n-          - hs\/tier1 gc\n-          - hs\/tier1 runtime\n-          - hs\/tier1 serviceability\n-        include:\n-          - test: jdk\/tier1 part 1\n-            suites: test\/jdk\/:tier1_part1\n-          - test: jdk\/tier1 part 2\n-            suites: test\/jdk\/:tier1_part2\n-          - test: jdk\/tier1 part 3\n-            suites: test\/jdk\/:tier1_part3\n-          - test: langtools\/tier1\n-            suites: test\/langtools\/:tier1\n-          - test: hs\/tier1 common\n-            suites: test\/hotspot\/jtreg\/:tier1_common\n-            artifact: -debug\n-          - test: hs\/tier1 compiler\n-            suites: test\/hotspot\/jtreg\/:tier1_compiler\n-            artifact: -debug\n-          - test: hs\/tier1 gc\n-            suites: test\/hotspot\/jtreg\/:tier1_gc\n-            artifact: -debug\n-          - test: hs\/tier1 runtime\n-            suites: test\/hotspot\/jtreg\/:tier1_runtime\n-            artifact: -debug\n-          - test: hs\/tier1 serviceability\n-            suites: test\/hotspot\/jtreg\/:tier1_serviceability\n-            artifact: -debug\n-\n-    # Reduced 32-bit build uses the same boot JDK as 64-bit build\n-    env:\n-      JDK_VERSION: \"${{ fromJson(needs.prerequisites.outputs.dependencies).DEFAULT_VERSION_FEATURE }}\"\n-      BOOT_JDK_VERSION: \"${{ fromJson(needs.prerequisites.outputs.dependencies).BOOT_JDK_VERSION }}\"\n-      BOOT_JDK_FILENAME: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_FILENAME }}\"\n-      BOOT_JDK_URL: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_URL }}\"\n-      BOOT_JDK_SHA256: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_SHA256 }}\"\n-\n-    steps:\n-      - name: Checkout the source\n-        uses: actions\/checkout@v2\n-\n-      - name: Restore boot JDK from cache\n-        id: bootjdk\n-        uses: actions\/cache@v2\n-        with:\n-          path: ~\/bootjdk\/${{ env.BOOT_JDK_VERSION }}\n-          key: bootjdk-${{ runner.os }}-${{ env.BOOT_JDK_VERSION }}-${{ env.BOOT_JDK_SHA256 }}-v1\n-\n-      - name: Download boot JDK\n-        run: |\n-          mkdir -p \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\"\n-          wget -O \"${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" \"${BOOT_JDK_URL}\"\n-          echo \"${BOOT_JDK_SHA256} ${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" | sha256sum -c >\/dev\/null -\n-          tar -xf \"${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" -C \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\"\n-          mv \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\/\"*\/* \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\/\"\n-        if: steps.bootjdk.outputs.cache-hit != 'true'\n-\n-      - name: Restore jtreg artifact\n-        id: jtreg_restore\n-        uses: actions\/download-artifact@v2\n-        with:\n-          name: transient_jtreg_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: ~\/jtreg\/\n-        continue-on-error: true\n-\n-      - name: Restore jtreg artifact (retry)\n-        uses: actions\/download-artifact@v2\n-        with:\n-          name: transient_jtreg_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: ~\/jtreg\/\n-        if: steps.jtreg_restore.outcome == 'failure'\n-\n-      - name: Restore build artifacts\n-        id: build_restore\n-        uses: actions\/download-artifact@v2\n-        with:\n-          name: transient_jdk-linux-x86${{ matrix.artifact }}_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: ~\/jdk-linux-x86${{ matrix.artifact }}\n-        continue-on-error: true\n-\n-      - name: Restore build artifacts (retry)\n-        uses: actions\/download-artifact@v2\n-        with:\n-          name: transient_jdk-linux-x86${{ matrix.artifact }}_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: ~\/jdk-linux-x86${{ matrix.artifact }}\n-        if: steps.build_restore.outcome == 'failure'\n-\n-      - name: Unpack jdk\n-        run: |\n-          mkdir -p \"${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin${{ matrix.artifact }}\"\n-          tar -xf \"${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin${{ matrix.artifact }}.tar.gz\" -C \"${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin${{ matrix.artifact }}\"\n-\n-      - name: Unpack tests\n-        run: |\n-          mkdir -p \"${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin-tests${{ matrix.artifact }}\"\n-          tar -xf \"${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin-tests${{ matrix.artifact }}.tar.gz\" -C \"${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin-tests${{ matrix.artifact }}\"\n-\n-      - name: Find root of jdk image dir\n-        run: |\n-          imageroot=`find ${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin${{ matrix.artifact }} -name release -type f`\n-          echo \"imageroot=`dirname ${imageroot}`\" >> $GITHUB_ENV\n-\n-      - name: Run tests\n-        run: >\n-          JDK_IMAGE_DIR=${{ env.imageroot }}\n-          TEST_IMAGE_DIR=${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin-tests${{ matrix.artifact }}\n-          BOOT_JDK=${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\n-          JT_HOME=${HOME}\/jtreg\n-          make test-prebuilt\n-          CONF_NAME=run-test-prebuilt\n-          LOG_CMDLINES=true\n-          JTREG_VERBOSE=fail,error,time\n-          TEST=\"${{ matrix.suites }}\"\n-          TEST_OPTS_JAVA_OPTIONS=\n-          JTREG_KEYWORDS=\"!headful\"\n-          JTREG=\"JAVA_OPTIONS=-XX:-CreateCoredumpOnCrash\"\n-\n-      - name: Check that all tests executed successfully\n-        if: always()\n-        run: >\n-          if ! grep --include=test-summary.txt -lqr build\/*\/test-results -e \"TEST SUCCESS\" ; then\n-            cat build\/*\/test-results\/*\/text\/newfailures.txt ;\n-            exit 1 ;\n-          fi\n-\n-      - name: Create suitable test log artifact name\n-        if: always()\n-        run: echo \"logsuffix=`echo ${{ matrix.test }} | sed -e 's!\/!_!'g -e 's! !_!'g`\" >> $GITHUB_ENV\n-\n-      - name: Package test results\n-        if: always()\n-        working-directory: build\/run-test-prebuilt\/test-results\/\n-        run: >\n-          zip -r9\n-          \"$HOME\/linux-x86${{ matrix.artifact }}_testresults_${{ env.logsuffix }}.zip\"\n-          .\n-        continue-on-error: true\n-\n-      - name: Package test support\n-        if: always()\n-        working-directory: build\/run-test-prebuilt\/test-support\/\n-        run: >\n-          zip -r9\n-          \"$HOME\/linux-x86${{ matrix.artifact }}_testsupport_${{ env.logsuffix }}.zip\"\n-          .\n-          -i *.jtr\n-          -i *\/hs_err*.log\n-          -i *\/replay*.log\n-        continue-on-error: true\n-\n-      - name: Persist test results\n-        if: always()\n-        uses: actions\/upload-artifact@v2\n-        with:\n-          path: ~\/linux-x86${{ matrix.artifact }}_testresults_${{ env.logsuffix }}.zip\n-        continue-on-error: true\n-\n-      - name: Persist test outputs\n-        if: always()\n-        uses: actions\/upload-artifact@v2\n-        with:\n-          path: ~\/linux-x86${{ matrix.artifact }}_testsupport_${{ env.logsuffix }}.zip\n-        continue-on-error: true\n-\n@@ -952,0 +483,1 @@\n+          --disable-precompiled-headers\n@@ -1465,2 +997,0 @@\n-      - linux_additional_build\n-      - linux_x86_test\n","filename":".github\/workflows\/submit.yml","additions":6,"deletions":476,"binary":false,"changes":482,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-project=jdk\n+project=valhalla\n@@ -6,5 +6,1 @@\n-error=author,committer,reviewers,merge,issues,executable,symlink,message,hg-tag,whitespace,problemlists\n-\n-[repository]\n-tags=(?:jdk-(?:[1-9]([0-9]*)(?:\\.(?:0|[1-9][0-9]*)){0,4})(?:\\+(?:(?:[0-9]+))|(?:-ga)))|(?:jdk[4-9](?:u\\d{1,3})?-(?:(?:b\\d{2,3})|(?:ga)))|(?:hs\\d\\d(?:\\.\\d{1,2})?-b\\d\\d)\n-branches=\n+error=author,committer,executable,symlink,whitespace\n@@ -20,7 +16,0 @@\n-[checks \"merge\"]\n-message=Merge\n-\n-[checks \"reviewers\"]\n-reviewers=1\n-ignore=duke\n-\n@@ -29,6 +18,0 @@\n-\n-[checks \"issues\"]\n-pattern=^([124-8][0-9]{6}): (\\S.*)$\n-\n-[checks \"problemlists\"]\n-dirs=test\/jdk|test\/langtools|test\/lib-test|test\/hotspot\/jtreg|test\/jaxp\n","filename":".jcheck\/conf","additions":2,"deletions":19,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -1894,0 +1894,2 @@\n+  __ verified_entry(C, 0);\n+  __ bind(*_verified_entry);\n@@ -1942,6 +1944,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -2004,5 +2000,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {\n-  \/\/ Variable size. Determine dynamically.\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2291,1 +2282,23 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n+\n+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+{\n+  MacroAssembler _masm(&cbuf);\n+\n+  if (!_verified) {\n+    Label skip;\n+    __ cmp_klass(j_rarg0, rscratch2, rscratch1);\n+    __ br(Assembler::EQ, skip);\n+      __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+    __ bind(skip);\n@@ -2293,0 +2306,11 @@\n+  } else {\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    __ b(*_verified_entry);\n+  }\n+}\n+\n+\/\/=============================================================================\n@@ -2314,0 +2338,1 @@\n+  Label skip;\n@@ -2315,0 +2340,1 @@\n+  \/\/ UseCompressedClassPointers logic are inside cmp_klass\n@@ -2316,1 +2342,1 @@\n-  Label skip;\n+\n@@ -2324,5 +2350,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2755,1 +2776,0 @@\n-\n@@ -8700,0 +8720,15 @@\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -14675,1 +14710,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n@@ -14677,1 +14712,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":56,"deletions":21,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -244,1 +246,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), needs_stack_repair(), NULL);\n@@ -459,1 +461,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -510,0 +512,15 @@\n+  ciMethod* method = compilation()->method();\n+\n+  ciType* return_type = method->return_type();\n+  if (InlineTypeReturnedAsFields && return_type->is_inlinetype()) {\n+    ciInlineKlass* vk = return_type->as_inline_klass();\n+    if (vk->can_be_returned_as_fields()) {\n+      address unpack_handler = vk->unpack_handler();\n+      assert(unpack_handler != NULL, \"must be\");\n+      __ far_call(RuntimeAddress(unpack_handler));\n+      \/\/ At this point, rax points to the value object (for interpreter or C1 caller).\n+      \/\/ The fields of the object are copied into registers (for C2 caller).\n+    }\n+  }\n+\n+\n@@ -511,1 +528,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -523,0 +540,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -568,0 +589,1 @@\n+    case T_INLINE_TYPE:\n@@ -569,3 +591,1 @@\n-        if (patch_code == lir_patch_none) {\n-          jobject2reg(c->as_jobject(), dest->as_register());\n-        } else {\n+        if (patch_code != lir_patch_none) {\n@@ -573,0 +593,2 @@\n+        } else {\n+          jobject2reg(c->as_jobject(), dest->as_register());\n@@ -614,0 +636,1 @@\n+  case T_INLINE_TYPE:\n@@ -680,0 +703,1 @@\n+  case T_INLINE_TYPE:\n@@ -682,0 +706,2 @@\n+    \/\/ Non-null case is not handled on aarch64 but handled on x86\n+    \/\/ FIXME: do we need to add it here?\n@@ -720,1 +746,1 @@\n-    if (src->type() == T_OBJECT) {\n+    if (src->type() == T_OBJECT || src->type() == T_INLINE_TYPE) {\n@@ -820,0 +846,1 @@\n+    case T_INLINE_TYPE: \/\/ fall through\n@@ -949,1 +976,1 @@\n-  if (addr->base()->type() == T_OBJECT) {\n+  if (addr->base()->type() == T_OBJECT || addr->base()->type() == T_INLINE_TYPE) {\n@@ -973,0 +1000,1 @@\n+    case T_INLINE_TYPE: \/\/ fall through\n@@ -1038,0 +1066,1 @@\n+      __ andr(dest->as_register(), dest->as_register(), oopDesc::compressed_klass_mask());\n@@ -1039,0 +1068,2 @@\n+    } else {\n+      __   ubfm(dest->as_register(), dest->as_register(), 0, 63 - oopDesc::storage_props_nof_bits);\n@@ -1043,0 +1074,14 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, NULL);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n@@ -1234,1 +1279,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->type() == T_INLINE_TYPE ||\n@@ -1546,0 +1591,122 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing an array that *may* be a flattened array (the declared type\n+  \/\/ Object[], interface[], or VT?[]). If this array is flattened, take slow path.\n+\n+  __ load_storage_props(op->tmp()->as_register(), op->array()->as_register());\n+  __ tst(op->tmp()->as_register(), ArrayStorageProperties::flattened_value);\n+  __ br(Assembler::NE, *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ We are storing into the array.\n+    Label skip;\n+    __ tst(op->tmp()->as_register(), ArrayStorageProperties::null_free_value);\n+    __ br(Assembler::EQ, skip);\n+    \/\/ The array is not flattened, but it is null_free. If we are storing\n+    \/\/ a null, take the slow path (which will throw NPE).\n+    __ cbz(op->value()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ This is called when we use aastore into a an array declared as \"[LVT;\",\n+  \/\/ where we know VT is not flattened (due to FlatArrayElementMaxSize, etc).\n+  \/\/ However, we need to do a NULL check if the actual array is a \"[QVT;\".\n+\n+  __ load_storage_props(op->tmp()->as_register(), op->array()->as_register());\n+  __ mov(rscratch1, (uint64_t) ArrayStorageProperties::null_free_value);\n+  __ cmp(op->tmp()->as_register(), rscratch1);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmp(left, right);\n+  __ br(Assembler::EQ, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  {\n+    __ cbz(left, L_oops_not_equal);\n+    __ cbz(right, L_oops_not_equal);\n+  }\n+\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Value object check -- if either of the operands is not a value object,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are value objects\n+  if ((left_klass == NULL || right_klass == NULL) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = rscratch1; \/* op->tmp1()->as_register(); *\/\n+    Register tmp2  = rscratch2; \/* op->tmp2()->as_register(); *\/\n+\n+    __ mov(tmp1, (intptr_t)markWord::always_locked_pattern);\n+\n+    __ ldr(tmp2, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, tmp2);\n+\n+    __ ldr(tmp2, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, tmp2);\n+\n+    __ mov(tmp2, (intptr_t)markWord::always_locked_pattern);\n+    __ cmp(tmp1, tmp2);\n+    __ br(Assembler::NE, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != NULL && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ b(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedOops) {\n+      __ ldrw(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldrw(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpw(left_klass_op, right_klass_op);\n+    } else {\n+      __ ldr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmp(left_klass_op, right_klass_op);\n+    }\n+\n+    __ br(Assembler::EQ, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  \/\/ We've returned from the stub. op->result_opr() contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+\n+  if (op->result_opr()->type() == T_LONG) {\n+    __ cbzw(op->result_opr()->as_register(), L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  } else {\n+    __ cbz(op->result_opr()->as_register(), L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  }\n+\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+\n+}\n+\n+\n@@ -1989,0 +2156,1 @@\n+      case T_INLINE_TYPE:\n@@ -2162,0 +2330,1 @@\n+    case T_INLINE_TYPE:\n@@ -2198,0 +2367,1 @@\n+    case T_INLINE_TYPE:\n@@ -2242,0 +2412,13 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest) {\n+  __ load_storage_props(tmp, obj);\n+  if (is_dest) {\n+    \/\/ We also take slow path if it's a null_free destination array, just in case the source array\n+    \/\/ contains NULLs.\n+    __ tst(tmp, ArrayStorageProperties::flattened_value | ArrayStorageProperties::null_free_value);\n+  } else {\n+    __ tst(tmp, ArrayStorageProperties::flattened_value);\n+  }\n+  __ br(Assembler::NE, *slow_path->entry());\n+}\n+\n+\n@@ -2263,0 +2446,16 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ b(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false);\n+  }\n+\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true);\n+  }\n+\n+\n+\n@@ -2875,0 +3074,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n@@ -3161,0 +3363,1 @@\n+  case T_INLINE_TYPE:\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":212,"deletions":9,"binary":false,"changes":221,"status":"modified"},{"patch":"@@ -731,0 +731,1 @@\n+    case new_flat_array_id:\n@@ -738,1 +739,2 @@\n-        } else {\n+        }\n+        else if (id == new_object_array_id) {\n@@ -741,0 +743,3 @@\n+        else {\n+          __ set_info(\"new_flat_array\", dont_gc_arguments);\n+        }\n@@ -749,3 +754,8 @@\n-          int tag = ((id == new_type_array_id)\n-                     ? Klass::_lh_array_tag_type_value\n-                     : Klass::_lh_array_tag_obj_value);\n+\n+          int tag = 0;\n+          switch (id) {\n+           case new_type_array_id: tag = Klass::_lh_array_tag_type_value; break;\n+           case new_object_array_id: tag = Klass::_lh_array_tag_obj_value; break;\n+           case new_flat_array_id: tag = Klass::_lh_array_tag_vt_value; break;\n+           default:  ShouldNotReachHere();\n+          }\n@@ -812,0 +822,1 @@\n+          \/\/ Runtime1::new_object_array handles both object and flat arrays\n@@ -847,0 +858,78 @@\n+    case buffer_inline_args_id:\n+    case buffer_inline_args_no_receiver_id:\n+    {\n+        const char* name = (id == buffer_inline_args_id) ?\n+          \"buffer_inline_args\" : \"buffer_inline_args_no_receiver\";\n+        StubFrame f(sasm, name, dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 2);\n+        Register method = r1;\n+        address entry = (id == buffer_inline_args_id) ?\n+          CAST_FROM_FN_PTR(address, buffer_inline_args) :\n+          CAST_FROM_FN_PTR(address, buffer_inline_args_no_receiver);\n+        int call_offset = __ call_RT(r0, noreg, entry, method);\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+        __ verify_oop(r0);  \/\/ r0: an array of buffered value objects\n+     }\n+     break;\n+\n+    case load_flattened_array_id:\n+      {\n+        StubFrame f(sasm, \"load_flattened_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 3);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, r0); \/\/ r0,: array\n+        f.load_argument(0, r1); \/\/ r1,: index\n+        int call_offset = __ call_RT(r0, noreg, CAST_FROM_FN_PTR(address, load_flattened_array), r0, r1);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+\n+        \/\/ r0: loaded element at array[index]\n+        __ verify_oop(r0);\n+      }\n+      break;\n+\n+    case store_flattened_array_id:\n+      {\n+        StubFrame f(sasm, \"store_flattened_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 4);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(2, r0); \/\/ r0: array\n+        f.load_argument(1, r1); \/\/ r1: index\n+        f.load_argument(0, r2); \/\/ r2: value\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, store_flattened_array), r0, r1, r2);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+      }\n+      break;\n+\n+      case substitutability_check_id:\n+      {\n+        StubFrame f(sasm, \"substitutability_check\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 3);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, r0); \/\/ r0,: left\n+        f.load_argument(0, r1); \/\/ r1,: right\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, substitutability_check), r0, r1);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+\n+        \/\/ r0,: are the two operands substitutable\n+      }\n+      break;\n+\n+\n+\n@@ -886,1 +975,1 @@\n-      { StubFrame f(sasm, \"throw_incompatible_class_cast_exception\", dont_gc_arguments);\n+      { StubFrame f(sasm, \"throw_incompatible_class_change_exception\", dont_gc_arguments);\n@@ -891,0 +980,6 @@\n+    case throw_illegal_monitor_state_exception_id:\n+      { StubFrame f(sasm, \"throw_illegal_monitor_state_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_illegal_monitor_state_exception), false);\n+      }\n+      break;\n+\n@@ -1094,0 +1189,2 @@\n+      \/\/ FIXME: For unhandled trap_id this code fails with assert during vm intialization\n+      \/\/ rather than insert a call to unimplemented_entry\n@@ -1102,0 +1199,2 @@\n+\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":104,"deletions":5,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -289,1 +289,16 @@\n-                                         Address dst, Register val, Register tmp1, Register tmp2) {\n+                                         Address dst, Register val, Register tmp1, Register tmp2, Register tmp3) {\n+\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  bool as_normal = (decorators & AS_NORMAL) != 0;\n+  assert((decorators & IS_DEST_UNINITIALIZED) == 0, \"unsupported\");\n+\n+  bool needs_pre_barrier = as_normal;\n+  bool needs_post_barrier = (val != noreg && in_heap);\n+\n+\n+   if (tmp3 == noreg) {\n+     tmp3 = rscratch2;\n+   }\n+   \/\/ assert_different_registers(val, tmp1, tmp2, tmp3, rscratch1, rscratch2);\n+   assert_different_registers(val, tmp1, tmp2, tmp3);\n+\n@@ -292,2 +307,2 @@\n-    if (dst.base() != r3) {\n-      __ mov(r3, dst.base());\n+    if (dst.base() != tmp1) {\n+      __ mov(tmp1, dst.base());\n@@ -296,1 +311,1 @@\n-    __ lea(r3, dst);\n+    __ lea(tmp1, dst);\n@@ -299,2 +314,4 @@\n-  g1_write_barrier_pre(masm,\n-                       r3 \/* obj *\/,\n+\n+  if (needs_pre_barrier) {\n+      g1_write_barrier_pre(masm,\n+                       tmp1 \/* obj *\/,\n@@ -303,1 +320,1 @@\n-                       tmp1  \/* tmp *\/,\n+                       tmp3  \/* tmp *\/,\n@@ -306,0 +323,1 @@\n+  }\n@@ -308,1 +326,1 @@\n-    BarrierSetAssembler::store_at(masm, decorators, type, Address(r3, 0), noreg, noreg, noreg);\n+    BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), noreg, noreg, noreg, noreg);\n@@ -312,7 +330,13 @@\n-    if (UseCompressedOops) {\n-      new_val = rscratch2;\n-      __ mov(new_val, val);\n-    }\n-    BarrierSetAssembler::store_at(masm, decorators, type, Address(r3, 0), val, noreg, noreg);\n-    g1_write_barrier_post(masm,\n-                          r3 \/* store_adr *\/,\n+    if (needs_post_barrier) {\n+      if (UseCompressedOops) {\n+        \/\/ FIXME: Refactor the code to avoid usage of r19 and stay within tmpX\n+        new_val = r19;\n+        __ mov(new_val, val);\n+      }\n+   }\n+\n+   BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg, noreg);\n+\n+    if (needs_post_barrier) {\n+       g1_write_barrier_post(masm,\n+                          tmp1 \/* store_adr *\/,\n@@ -321,3 +345,4 @@\n-                          tmp1 \/* tmp *\/,\n-                          tmp2 \/* tmp2 *\/);\n-  }\n+                          tmp2 \/* tmp *\/,\n+                          tmp3 \/* tmp2 *\/);\n+   }\n+ }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":43,"deletions":18,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -111,1 +111,2 @@\n-                                        Register tmp2) {\n+                                        Register tmp2,\n+                                        Register tmp3) {\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zBarrierSetAssembler_aarch64.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -686,0 +687,1 @@\n+\n@@ -702,0 +704,27 @@\n+\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    Label skip;\n+    \/\/ Test if the return type is an inline type\n+    ldr(rscratch1, Address(rfp, frame::interpreter_frame_method_offset * wordSize));\n+    ldr(rscratch1, Address(rscratch1, Method::const_offset()));\n+    ldrb(rscratch1, Address(rscratch1, ConstMethod::result_type_offset()));\n+    cmpw(rscratch1, (u1) T_INLINE_TYPE);\n+    br(Assembler::NE, skip);\n+\n+    \/\/ We are returning an inline type, load its fields into registers\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+\n+    load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+    cbz(rscratch1, skip);\n+\n+    blr(rscratch1);\n+\n+    \/\/ call above kills the value in r1. Reload it.\n+    ldr(r1, Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n+\n+\n@@ -762,0 +791,5 @@\n+    if (EnableValhalla && !UseBiasedLocking) {\n+      \/\/ For slow path is_always_locked, using biased, which is never natural for !UseBiasLocking\n+      andr(swap_reg, swap_reg, ~((int) markWord::biased_lock_bit_in_place));\n+    }\n+\n@@ -1741,1 +1775,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1787,1 +1821,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":36,"deletions":2,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -1287,1 +1288,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1317,1 +1322,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1409,0 +1418,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -1458,0 +1471,33 @@\n+void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label& is_value) {\n+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  andr(temp_reg, temp_reg, JVM_ACC_INLINE);\n+  cbnz(temp_reg, is_value);\n+}\n+\n+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_inline_field_shift, is_inline);\n+}\n+\n+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbz(flags, ConstantPoolCacheEntry::is_inline_field_shift, not_inline);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label& is_flattened_array) {\n+  load_storage_props(temp_reg, oop);\n+  andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);\n+  cbnz(temp_reg, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  load_storage_props(temp_reg, oop);\n+  andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);\n+  cbnz(temp_reg, is_null_free_array);\n+}\n+\n@@ -3762,1 +3808,1 @@\n-void MacroAssembler::load_klass(Register dst, Register src) {\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n@@ -3765,1 +3811,0 @@\n-    decode_klass_not_null(dst);\n@@ -3771,0 +3816,10 @@\n+void MacroAssembler::load_klass(Register dst, Register src) {\n+  load_metadata(dst, src);\n+  if (UseCompressedClassPointers) {\n+    andr(dst, dst, oopDesc::compressed_klass_mask());\n+    decode_klass_not_null(dst);\n+  } else {\n+    ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);\n+  }\n+}\n+\n@@ -3802,0 +3857,9 @@\n+void MacroAssembler::load_storage_props(Register dst, Register src) {\n+  load_metadata(dst, src);\n+  if (UseCompressedClassPointers) {\n+    asrw(dst, dst, oopDesc::narrow_storage_props_shift);\n+  } else {\n+    asr(dst, dst, oopDesc::wide_storage_props_shift);\n+  }\n+}\n+\n@@ -4139,1 +4203,2 @@\n-                                     Register tmp1, Register thread_tmp) {\n+                                     Register tmp1, Register thread_tmp, Register tmp3) {\n+\n@@ -4144,1 +4209,1 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4146,1 +4211,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4170,2 +4235,2 @@\n-                                    Register thread_tmp, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);\n+                                    Register thread_tmp, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4176,1 +4241,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -5272,0 +5337,339 @@\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+\n+\/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->frame_size_in_bytes();\n+  assert(framesize % (2 * wordSize) == 0, \"must preserve 2 * wordSize alignment\");\n+\n+  \/\/ insert a nop at the start of the prolog so we can patch in a\n+  \/\/ branch if we need to invalidate the method later\n+  nop();\n+\n+  int bangsize = C->bang_size_in_bytes();\n+  if (C->need_stack_bang(bangsize) && UseStackBanging)\n+     generate_stack_overflow_check(bangsize);\n+\n+  build_frame(framesize);\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  cmp(r0, (u1) 1);\n+  br(Assembler::EQ, skip);\n+  int call_offset = -1;\n+\n+  Label slow_case;\n+\n+  \/\/ Try to allocate a new buffered inline type (from the heap)\n+  if (UseTLAB) {\n+\n+    if (vk != NULL) {\n+      \/\/ Called from C1, where the return type is statically known.\n+      mov(r1, (intptr_t)vk->get_InlineKlass());\n+      jint lh = vk->layout_helper();\n+      assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+      mov(r14, lh);\n+    } else {\n+       \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+       andr(r1, r0, -2);\n+       \/\/ get obj size\n+       ldrw(r14, Address(rscratch1 \/*klass*\/, Klass::layout_helper_offset()));\n+    }\n+\n+     ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+\n+     \/\/ check whether we have space in TLAB,\n+     \/\/ rscratch1 contains pointer to just allocated obj\n+      lea(r14, Address(r13, r14));\n+      ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));\n+\n+      cmp(r14, rscratch1);\n+      br(Assembler::GT, slow_case);\n+\n+      \/\/ OK we have room in TLAB,\n+      \/\/ Set new TLAB top\n+      str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+\n+      \/\/ Set new class always locked\n+      mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());\n+      str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));\n+\n+      store_klass_gap(r13, zr);  \/\/ zero klass gap for compressed oops\n+      if (vk == NULL) {\n+        \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n+         mov(r0, r1);\n+      }\n+\n+      store_klass(r13, r1);  \/\/ klass\n+\n+      if (vk != NULL) {\n+        \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+        mov(r0, r13);\n+        far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+      } else {\n+\n+        \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+        ldr(r1, Address(r0, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+        ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));\n+\n+        \/\/ Mov new class to r0 and call pack_handler\n+        mov(r0, r13);\n+        blr(r1);\n+      }\n+      b(skip);\n+  }\n+\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    ldr(rscratch1, RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    blr(rscratch1);\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        mov(to->as_Register(), from->as_Register());\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,\n+                                          int& to_index, RegState reg_state[]) {\n+  Register fromReg = from->is_reg() ? from->as_Register() : noreg;\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+\n+\n+  int vt = 1;\n+  bool done = true;\n+  bool mark_done = true;\n+  do {\n+    sig_index--;\n+    BasicType bt = sig->at(sig_index)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      vt--;\n+    } else if (bt == T_VOID &&\n+               sig->at(sig_index-1)._bt != T_LONG &&\n+               sig->at(sig_index-1)._bt != T_DOUBLE) {\n+      vt++;\n+    } else {\n+      assert(to_index >= 0, \"invalid to_index\");\n+      VMRegPair pair_to = regs_to[to_index--];\n+      VMReg to = pair_to.first();\n+\n+      if (bt == T_VOID) continue;\n+\n+      int idx = (int) to->value();\n+      if (reg_state[idx] == reg_readonly) {\n+         if (idx != from->value()) {\n+           mark_done = false;\n+         }\n+         done = false;\n+         continue;\n+      } else if (reg_state[idx] == reg_written) {\n+        continue;\n+      } else {\n+        assert(reg_state[idx] == reg_writable, \"must be writable\");\n+        reg_state[idx] = reg_written;\n+      }\n+\n+      if (fromReg == noreg) {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        ldr(rscratch2, Address(sp, st_off));\n+        fromReg = rscratch2;\n+      }\n+\n+      int off = sig->at(sig_index)._offset;\n+      assert(off > 0, \"offset in object should be positive\");\n+      bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+\n+      Address fromAddr = Address(fromReg, off);\n+      bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+\n+      if (!to->is_FloatRegister()) {\n+\n+        Register dst = to->is_stack() ? rscratch1 : to->as_Register();\n+\n+        if (is_oop) {\n+          load_heap_oop(dst, fromAddr);\n+        } else {\n+          load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+        }\n+        if (to->is_stack()) {\n+          int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+          str(dst, Address(sp, st_off));\n+        }\n+      } else {\n+        if (bt == T_DOUBLE) {\n+          ldrd(to->as_FloatRegister(), fromAddr);\n+        } else {\n+          assert(bt == T_FLOAT, \"must be float\");\n+          ldrs(to->as_FloatRegister(), fromAddr);\n+        }\n+     }\n+\n+    }\n+\n+  } while (vt != 0);\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"must be\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  Register val_array = r0;\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r10;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r1;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);\n+  VMRegPair from_pair;\n+  BasicType bt;\n+\n+  while (stream.next(from_pair, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    VMReg from_r1 = from_pair.first();\n+    VMReg from_r2 = from_pair.second();\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+\n+    if (!from_r1->is_FloatRegister()) {\n+      Register from_reg;\n+      if (from_r1->is_stack()) {\n+        from_reg = from_reg_tmp;\n+        int ld_off = from_r1->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        from_reg = from_r1->as_Register();\n+      }\n+\n+      if (is_oop) {\n+        DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;\n+        store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);\n+      } else {\n+        store_sized_value(dst, from_reg, size_in_bytes);\n+      }\n+    } else {\n+      if (from_r2->is_valid()) {\n+        strd(from_r1->as_FloatRegister(), dst);\n+      } else {\n+        strs(from_r1->as_FloatRegister(), dst);\n+      }\n+    }\n+\n+    reg_state[from_r1->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v0->as_VMReg() : r14->as_VMReg();\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":414,"deletions":10,"binary":false,"changes":424,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -32,0 +33,4 @@\n+#include \"runtime\/signature.hpp\"\n+\n+\n+class ciInlineKlass;\n@@ -614,0 +619,10 @@\n+  void test_klass_is_value(Register klass, Register temp_reg, Label& is_value);\n+\n+  void test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline);\n+  void test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline);\n+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened);\n+\n+  \/\/ Check klass\/oops is flat inline type array (oop->_klass->_layout_helper & vt_bit)\n+  void test_flattened_array_oop(Register klass, Register temp_reg, Label& is_flattened_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+\n@@ -821,0 +836,3 @@\n+  void load_metadata(Register dst, Register src);\n+  void load_storage_props(Register dst, Register src);\n+\n@@ -833,1 +851,1 @@\n-                       Register tmp1, Register tmp_thread);\n+                       Register tmp1, Register tmp_thread, Register tmp3 = noreg);\n@@ -845,1 +863,1 @@\n-                      Register tmp_thread = noreg, DecoratorSet decorators = 0);\n+                      Register tmp_thread = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);\n@@ -1195,0 +1213,14 @@\n+  void verified_entry(Compile* C, int sp_inc);\n+\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[]);\n+  void restore_stack(Compile* C);\n+  VMReg spill_reg_for(VMReg reg);\n+\n@@ -1259,0 +1291,2 @@\n+  void fill_words(Register base, uint64_t cnt, Register value);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":36,"deletions":2,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -321,0 +322,1 @@\n+    case T_INLINE_TYPE:\n@@ -354,0 +356,84 @@\n+\n+\/\/ const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_int = 6;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt, VMRegPair *regs, int total_args_passed) {\n+\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  \/\/ r1, r2 used to address klasses and states, exclude it from return convention to avoid colision\n+\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+     r0 \/* j_rarg7 *\/, j_rarg6, j_rarg5, j_rarg4, j_rarg3, j_rarg2\n+  };\n+\n+  static const FloatRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3, j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        \/\/ Should we have gurantee here?\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+      \/\/ Should T_METADATA be added to java_calling_convention as well ?\n+    case T_METADATA:\n+    case T_INLINE_TYPE:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -387,19 +473,40 @@\n-static void gen_c2i_adapter(MacroAssembler *masm,\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n-                            const VMRegPair *regs,\n-                            Label& skip_fixup) {\n-  \/\/ Before we get into the guts of the C2I adapter, see if we should be here\n-  \/\/ at all.  We've come from compiled code and are attempting to jump to the\n-  \/\/ interpreter, which means the caller made a static call to get here\n-  \/\/ (vcalls always get a compiled target if there is one).  Check for a\n-  \/\/ compiled target.  If there is one, we need to patch the caller's call.\n-  patch_callers_callsite(masm);\n-\n-  __ bind(skip_fixup);\n-\n-  int words_pushed = 0;\n-\n-  \/\/ Since all args are passed on the stack, total_args_passed *\n-  \/\/ Interpreter::stackElementSize is the space we need.\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+     for (int i = 0; i < sig_extended->length(); i++) {\n+       BasicType bt = sig_extended->at(i)._bt;\n+       if (bt == T_INLINE_TYPE) {\n+         \/\/ In sig_extended, an inline type argument starts with:\n+         \/\/ T_INLINE_TYPE, followed by the types of the fields of the\n+         \/\/ inline type and T_VOID to mark the end of the value\n+         \/\/ type. Inline types are flattened so, for instance, in the\n+         \/\/ case of an inline type with an int field and an inline type\n+         \/\/ field that itself has 2 fields, an int and a long:\n+         \/\/ T_INLINE_TYPE T_INT T_INLINE_TYPE T_INT T_LONG T_VOID (second\n+         \/\/ slot for the T_LONG) T_VOID (inner T_INLINE_TYPE) T_VOID\n+         \/\/ (outer T_INLINE_TYPE)\n+         total_args_passed++;\n+         int vt = 1;\n+         do {\n+           i++;\n+           BasicType bt = sig_extended->at(i)._bt;\n+           BasicType prev_bt = sig_extended->at(i-1)._bt;\n+           if (bt == T_INLINE_TYPE) {\n+             vt++;\n+           } else if (bt == T_VOID &&\n+                      prev_bt != T_LONG &&\n+                      prev_bt != T_DOUBLE) {\n+             vt--;\n+           }\n+         } while (vt != 0);\n+       } else {\n+         total_args_passed++;\n+       }\n+     }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n@@ -407,1 +514,2 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+  return total_args_passed;\n+}\n@@ -409,3 +517,1 @@\n-  __ mov(r13, sp);\n-  \/\/ stack is aligned, keep it that way\n-  extraspace = align_up(extraspace, 2*wordSize);\n+static void gen_c2i_adapter_helper(MacroAssembler* masm, BasicType bt, const VMRegPair& reg_pair, int extraspace, const Address& to) {\n@@ -414,13 +520,1 @@\n-  if (extraspace)\n-    __ sub(sp, sp, extraspace);\n-\n-  \/\/ Now write the args into the outgoing interpreter space\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n+    assert(bt != T_INLINE_TYPE || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n@@ -441,2 +535,5 @@\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n+    \/\/ int next_off = st_off - Interpreter::stackElementSize;\n+\n+    VMReg r_1 = reg_pair.first();\n+    VMReg r_2 = reg_pair.second();\n+\n@@ -445,1 +542,1 @@\n-      continue;\n+      return;\n@@ -447,0 +544,1 @@\n+\n@@ -449,3 +547,2 @@\n-      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size\n-                    + extraspace\n-                    + words_pushed * wordSize);\n+      \/\/ words_pushed is always 0 so we don't use it.\n+      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace \/* + word_pushed * wordSize *\/);\n@@ -455,1 +552,1 @@\n-        __ str(rscratch1, Address(sp, st_off));\n+        __ str(rscratch1, to);\n@@ -458,16 +555,1 @@\n-\n-\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ str(rscratch1, Address(sp, next_off));\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaaaull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ str(rscratch1, Address(sp, st_off));\n-        }\n+        __ str(rscratch1, to);\n@@ -478,19 +560,1 @@\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ str(r, Address(sp, st_off));\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ jlong\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaabull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-          __ str(r, Address(sp, next_off));\n-        } else {\n-          __ str(r, Address(sp, st_off));\n-        }\n-      }\n+      __ str(r, to);\n@@ -501,1 +565,1 @@\n-        __ strs(r_1->as_FloatRegister(), Address(sp, st_off));\n+        __ strs(r_1->as_FloatRegister(), to);\n@@ -503,6 +567,1 @@\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaacull);\n-        __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        __ strd(r_1->as_FloatRegister(), Address(sp, next_off));\n+        __ strd(r_1->as_FloatRegister(), to);\n@@ -510,0 +569,153 @@\n+   }\n+}\n+\n+static void gen_c2i_adapter(MacroAssembler *masm,\n+                            const GrowableArray<SigEntry>* sig_extended,\n+                            const VMRegPair *regs,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+\n+  \/\/ Before we get into the guts of the C2I adapter, see if we should be here\n+  \/\/ at all.  We've come from compiled code and are attempting to jump to the\n+  \/\/ interpreter, which means the caller made a static call to get here\n+  \/\/ (vcalls always get a compiled target if there is one).  Check for a\n+  \/\/ compiled target.  If there is one, we need to patch the caller's call.\n+  patch_callers_callsite(masm);\n+\n+  __ bind(skip_fixup);\n+\n+  bool has_inline_argument = false;\n+\n+  if (InlineTypePassFieldsAsArgs) {\n+      \/\/ Is there an inline type argument?\n+     for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+       has_inline_argument = (sig_extended->at(i)._bt == T_INLINE_TYPE);\n+     }\n+     if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);\n+\n+      frame_complete = __ offset();\n+      address the_pc = __ pc();\n+\n+      __ set_last_Java_frame(noreg, noreg, the_pc, rscratch1);\n+\n+      __ mov(c_rarg0, rthread);\n+      __ mov(c_rarg1, r1);\n+      __ mov(c_rarg2, (int64_t)alloc_inline_receiver);\n+\n+      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+      __ blr(rscratch1);\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ cbz(r0, no_exception);\n+\n+      __ str(zr, Address(rthread, JavaThread::vm_result_offset()));\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ b(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(r10, rthread);\n+      __ get_vm_result_2(r1, rthread); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n+  int words_pushed = 0;\n+\n+  \/\/ Since all args are passed on the stack, total_args_passed *\n+  \/\/ Interpreter::stackElementSize is the space we need.\n+\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n+  int extraspace = (total_args_passed * Interpreter::stackElementSize) + wordSize;\n+\n+  \/\/ stack is aligned, keep it that way\n+  extraspace = align_up(extraspace, 2 * wordSize);\n+\n+  __ mov(r13, sp);\n+\n+  if (extraspace)\n+    __ sub(sp, sp, extraspace);\n+\n+  \/\/ Now write the args into the outgoing interpreter space\n+\n+  int ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+  bool has_oop_field = false;\n+\n+  for (int next_arg_comp = 0; next_arg_comp < total_args_passed; next_arg_comp++) {\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    \/\/ offset to start parameters\n+    int st_off   = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;\n+\n+    if (!InlineTypePassFieldsAsArgs || bt != T_INLINE_TYPE) {\n+      if (bt == T_VOID) {\n+         assert(next_arg_comp > 0 && (sig_extended->at(next_arg_comp - 1)._bt == T_LONG || sig_extended->at(next_arg_comp - 1)._bt == T_DOUBLE), \"missing half\");\n+         next_arg_int ++;\n+         continue;\n+       }\n+\n+       int next_off = st_off - Interpreter::stackElementSize;\n+       int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+\n+       gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp], extraspace, Address(sp, offset));\n+       next_arg_int ++;\n+   } else {\n+       ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_INLINE_TYPE);\n+      __ load_heap_oop(rscratch1, Address(r10, index));\n+      next_vt_arg++;\n+      next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of value\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp - 1)._bt;\n+        if (bt == T_INLINE_TYPE) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n+        } else {\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          assert(off > 0, \"offset in object should be positive\");\n+\n+          bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+          has_oop_field = has_oop_field || is_oop;\n+\n+          gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp - ignored], extraspace, Address(r11, off));\n+        }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ str(rscratch1, Address(sp, st_off));\n+   }\n+\n+  }\n+\n+\/\/ If an inline type was allocated and initialized, apply post barrier to all oop fields\n+  if (has_inline_argument && has_oop_field) {\n+    __ push(r13); \/\/ save senderSP\n+    __ push(r1); \/\/ save callee\n+    \/\/ Allocate argument register save area\n+    if (frame::arg_reg_save_area_bytes != 0) {\n+      __ sub(sp, sp, frame::arg_reg_save_area_bytes);\n@@ -511,0 +723,7 @@\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::apply_post_barriers), rthread, r10);\n+    \/\/ De-allocate argument register save area\n+    if (frame::arg_reg_save_area_bytes != 0) {\n+      __ add(sp, sp, frame::arg_reg_save_area_bytes);\n+    }\n+    __ pop(r1); \/\/ restore callee\n+    __ pop(r13); \/\/ restore sender SP\n@@ -519,0 +738,1 @@\n+void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm, int comp_args_on_stack, const GrowableArray<SigEntry>* sig, const VMRegPair *regs) {\n@@ -520,5 +740,0 @@\n-void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,\n-                                    int total_args_passed,\n-                                    int comp_args_on_stack,\n-                                    const BasicType *sig_bt,\n-                                    const VMRegPair *regs) {\n@@ -584,1 +799,1 @@\n-  int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)>>LogBytesPerWord;\n+  int comp_words_on_stack = 0;\n@@ -586,2 +801,3 @@\n-    __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n-    __ andr(sp, rscratch1, -16);\n+     comp_words_on_stack = align_up(comp_args_on_stack * VMRegImpl::stack_slot_size, wordSize) >> LogBytesPerWord;\n+     __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n+     __ andr(sp, rscratch1, -16);\n@@ -606,0 +822,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -608,2 +826,5 @@\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+    BasicType bt = sig->at(i)._bt;\n+\n+    assert(bt != T_INLINE_TYPE, \"i2c adapter doesn't unpack inline typ args\");\n+    if (bt == T_VOID) {\n+      assert(i > 0 && (sig->at(i - 1)._bt == T_LONG || sig->at(i - 1)._bt == T_DOUBLE), \"missing half\");\n@@ -614,0 +835,1 @@\n+    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(), \"scrambled load targets?\");\n@@ -615,3 +837,1 @@\n-    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),\n-            \"scrambled load targets?\");\n-    int ld_off = (total_args_passed - i - 1)*Interpreter::stackElementSize;\n+    int ld_off = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n@@ -632,1 +852,1 @@\n-      int st_off = regs[i].first()->reg2stack()*VMRegImpl::stack_slot_size;\n+      int st_off = regs[i].first()->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -649,2 +869,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n@@ -653,11 +872,28 @@\n-        __ str(rscratch2, Address(sp, st_off));\n-      }\n-    } else if (r_1->is_Register()) {  \/\/ Register argument\n-      Register r = r_1->as_Register();\n-      if (r_2->is_valid()) {\n-        \/\/\n-        \/\/ We are using two VMRegs. This can be either T_OBJECT,\n-        \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n-        \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n-        \/\/ So we must adjust where to pick up the data to match the\n-        \/\/ interpreter.\n+         __ str(rscratch2, Address(sp, st_off));\n+       }\n+     } else if (r_1->is_Register()) {  \/\/ Register argument\n+       Register r = r_1->as_Register();\n+       if (r_2->is_valid()) {\n+         \/\/\n+         \/\/ We are using two VMRegs. This can be either T_OBJECT,\n+         \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n+         \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n+         \/\/ So we must adjust where to pick up the data to match the\n+         \/\/ interpreter.\n+\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n+\n+         \/\/ this can be a misaligned move\n+         __ ldr(r, Address(esp, offset));\n+       } else {\n+         \/\/ sign extend and use a full word?\n+         __ ldrw(r, Address(esp, ld_off));\n+       }\n+     } else {\n+       if (!r_2->is_valid()) {\n+         __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n+       } else {\n+         __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n+       }\n+     }\n+   }\n@@ -665,17 +901,0 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n-\n-        \/\/ this can be a misaligned move\n-        __ ldr(r, Address(esp, offset));\n-      } else {\n-        \/\/ sign extend and use a full word?\n-        __ ldrw(r, Address(esp, ld_off));\n-      }\n-    } else {\n-      if (!r_2->is_valid()) {\n-        __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n-      } else {\n-        __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n-      }\n-    }\n-  }\n@@ -694,1 +913,0 @@\n-\n@@ -698,13 +916,1 @@\n-\/\/ ---------------------------------------------------------------\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                                            int total_args_passed,\n-                                                            int comp_args_on_stack,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n-  address i2c_entry = __ pc();\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n-\n-  address c2i_unverified_entry = __ pc();\n-  Label skip_fixup;\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n@@ -745,0 +951,34 @@\n+}\n+\n+\n+\/\/ ---------------------------------------------------------------\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n+                                                            int comp_args_on_stack,\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n+\n+  address i2c_entry = __ pc();\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n+\n+  address c2i_unverified_entry = __ pc();\n+  Label skip_fixup;\n+\n+  gen_inline_cache_check(masm, skip_fixup);\n+\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    Label unused;\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+    skip_fixup = unused;\n+  }\n@@ -746,0 +986,1 @@\n+  \/\/ Scalarized c2i adapter\n@@ -750,0 +991,1 @@\n+\n@@ -752,4 +994,4 @@\n-\n-      __ ldrw(rscratch1, Address(rmethod, Method::access_flags_offset()));\n-      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n-      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n+        Register flags  = rscratch1;\n+      __ ldrw(flags, Address(rmethod, Method::access_flags_offset()));\n+      __ tst(flags, JVM_ACC_STATIC);\n+      __ br(Assembler::NE, L_skip_barrier); \/\/ non-static\n@@ -759,3 +1001,6 @@\n-    __ load_method_holder(rscratch2, rmethod);\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    Register klass = rscratch1;\n+    __ load_method_holder(klass, rmethod);\n+    \/\/ We pass rthread to this function on x86\n+    __ clinit_barrier(klass, rscratch2, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n@@ -772,0 +1017,14 @@\n+  address c2i_unverified_inline_entry = c2i_unverified_entry;\n+\n+ \/\/ Non-scalarized c2i adapter\n+  address c2i_inline_entry = c2i_entry;\n+  if (regs != regs_cc) {\n+    Label inline_entry_skip_fixup;\n+    c2i_unverified_inline_entry = __ pc();\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    Label unused;\n+    gen_c2i_adapter(masm, sig, regs, inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+  }\n+\n@@ -773,1 +1032,8 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+\n+  \/\/ The c2i adapter might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+\n+  bool caller_must_gc_arguments = (regs != regs_cc);\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words + 10, oop_maps, caller_must_gc_arguments);\n+\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -816,0 +1082,1 @@\n+      case T_INLINE_TYPE:\n@@ -1634,0 +1901,1 @@\n+      case T_INLINE_TYPE:\n@@ -1821,0 +2089,1 @@\n+  case T_INLINE_TYPE:\n@@ -3071,0 +3340,105 @@\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler _masm(&buffer);\n+  MacroAssembler* masm = &_masm;\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ strs(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_DOUBLE) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(r0, val);\n+      \/\/ We don't need barriers because the destination is a newly allocated object.\n+      \/\/ Also, we cannot use store_heap_oop(to, val) because it uses r8 as tmp.\n+      if (UseCompressedOops) {\n+        __ encode_heap_oop(val);\n+        __ str(val, to);\n+      } else {\n+        __ str(val, to);\n+      }\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ store_sized_value(to, r_1->as_Register(), size_in_bytes);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(lr);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ ldrs(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ ldrd(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+       assert_different_registers(r0, r_1->as_Register());\n+       __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(lr);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, unpack_fields_off);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":525,"deletions":151,"binary":false,"changes":676,"status":"modified"},{"patch":"@@ -307,1 +307,1 @@\n-    \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+    \/\/ T_OBJECT, T_INLINE_TYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n@@ -311,1 +311,1 @@\n-    Label is_long, is_float, is_double, exit;\n+    Label is_long, is_float, is_double, is_value, exit;\n@@ -315,0 +315,2 @@\n+    __ cmp(j_rarg1, (u1)T_INLINE_TYPE);\n+    __ br(Assembler::EQ, is_value);\n@@ -369,0 +371,13 @@\n+    __ BIND(is_value);\n+    if (InlineTypeReturnedAsFields) {\n+      \/\/ Check for flattened return value\n+      __ cbz(r0, is_long);\n+      \/\/ Initialize pre-allocated buffer\n+      __ mov(r1, r0);\n+      __ andr(r1, r1, -2);\n+      __ ldr(r1, Address(r1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));\n+      __ ldr(r0, Address(j_rarg2, 0));\n+      __ blr(r1);\n+      __ b(exit);\n+    }\n@@ -1819,1 +1834,1 @@\n-    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, AS_RAW);  \/\/ store the oop\n+    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n@@ -6485,0 +6500,178 @@\n+  \/\/ Call here from the interpreter or compiled code to either load\n+  \/\/ multiple returned values from the inline type instance being\n+  \/\/ returned to registers or to store returned values to a newly\n+  \/\/ allocated inline type instance.\n+  address generate_return_value_stub(address destination, const char* name, bool has_res) {\n+\n+    \/\/ Information about frame layout at time of blocking runtime call.\n+    \/\/ Note that we only have to preserve callee-saved registers since\n+    \/\/ the compilers are responsible for supplying a continuation point\n+    \/\/ if they expect all registers to be preserved.\n+    \/\/ n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0\n+    enum layout {\n+      rfp_off = 0, rfp_off2,\n+\n+      j_rarg7_off, j_rarg7_2,\n+      j_rarg6_off, j_rarg6_2,\n+      j_rarg5_off, j_rarg5_2,\n+      j_rarg4_off, j_rarg4_2,\n+      j_rarg3_off, j_rarg3_2,\n+      j_rarg2_off, j_rarg2_2,\n+      j_rarg1_off, j_rarg1_2,\n+      j_rarg0_off, j_rarg0_2,\n+\n+      j_farg0_off, j_farg0_2,\n+      j_farg1_off, j_farg1_2,\n+      j_farg2_off, j_farg2_2,\n+      j_farg3_off, j_farg3_2,\n+      j_farg4_off, j_farg4_2,\n+      j_farg5_off, j_farg5_2,\n+      j_farg6_off, j_farg6_2,\n+      j_farg7_off, j_farg7_2,\n+\n+      return_off, return_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    int insts_size = 512;\n+    int locs_size  = 64;\n+\n+    CodeBuffer code(name, insts_size, locs_size);\n+    OopMapSet* oop_maps  = new OopMapSet();\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+\n+    address start = __ pc();\n+\n+    const Address f7_save       (rfp, j_farg7_off * wordSize);\n+    const Address f6_save       (rfp, j_farg6_off * wordSize);\n+    const Address f5_save       (rfp, j_farg5_off * wordSize);\n+    const Address f4_save       (rfp, j_farg4_off * wordSize);\n+    const Address f3_save       (rfp, j_farg3_off * wordSize);\n+    const Address f2_save       (rfp, j_farg2_off * wordSize);\n+    const Address f1_save       (rfp, j_farg1_off * wordSize);\n+    const Address f0_save       (rfp, j_farg0_off * wordSize);\n+\n+    const Address r0_save      (rfp, j_rarg0_off * wordSize);\n+    const Address r1_save      (rfp, j_rarg1_off * wordSize);\n+    const Address r2_save      (rfp, j_rarg2_off * wordSize);\n+    const Address r3_save      (rfp, j_rarg3_off * wordSize);\n+    const Address r4_save      (rfp, j_rarg4_off * wordSize);\n+    const Address r5_save      (rfp, j_rarg5_off * wordSize);\n+    const Address r6_save      (rfp, j_rarg6_off * wordSize);\n+    const Address r7_save      (rfp, j_rarg7_off * wordSize);\n+\n+    \/\/ Generate oop map\n+    OopMap* map = new OopMap(framesize, 0);\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(rfp_off), rfp->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg7_off), j_rarg7->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg6_off), j_rarg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+    \/\/ This is an inlined and slightly modified version of call_VM\n+    \/\/ which has the ability to fetch the return PC out of\n+    \/\/ thread-local storage and also sets up last_Java_sp slightly\n+    \/\/ differently than the real call_VM\n+\n+    __ enter(); \/\/ Save FP and LR before call\n+\n+    assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n+\n+    \/\/ lr and fp are already in place\n+    __ sub(sp, rfp, ((unsigned)framesize - 4) << LogBytesPerInt); \/\/ prolog\n+\n+    __ strd(j_farg7, f7_save);\n+    __ strd(j_farg6, f6_save);\n+    __ strd(j_farg5, f5_save);\n+    __ strd(j_farg4, f4_save);\n+    __ strd(j_farg3, f3_save);\n+    __ strd(j_farg2, f2_save);\n+    __ strd(j_farg1, f1_save);\n+    __ strd(j_farg0, f0_save);\n+\n+    __ str(j_rarg0, r0_save);\n+    __ str(j_rarg1, r1_save);\n+    __ str(j_rarg2, r2_save);\n+    __ str(j_rarg3, r3_save);\n+    __ str(j_rarg4, r4_save);\n+    __ str(j_rarg5, r5_save);\n+    __ str(j_rarg6, r6_save);\n+    __ str(j_rarg7, r7_save);\n+\n+    int frame_complete = __ pc() - start;\n+\n+    \/\/ Set up last_Java_sp and last_Java_fp\n+    address the_pc = __ pc();\n+    __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n+\n+    \/\/ Call runtime\n+    __ mov(c_rarg0, rthread);\n+    __ mov(c_rarg1, r0);\n+\n+    BLOCK_COMMENT(\"call runtime_entry\");\n+    __ mov(rscratch1, destination);\n+    __ blr(rscratch1);\n+\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    __ reset_last_Java_frame(false);\n+    __ maybe_isb();\n+\n+    __ ldrd(j_farg7, f7_save);\n+    __ ldrd(j_farg6, f6_save);\n+    __ ldrd(j_farg5, f5_save);\n+    __ ldrd(j_farg4, f4_save);\n+    __ ldrd(j_farg3, f3_save);\n+    __ ldrd(j_farg3, f2_save);\n+    __ ldrd(j_farg1, f1_save);\n+    __ ldrd(j_farg0, f0_save);\n+\n+    __ ldr(j_rarg0, r0_save);\n+    __ ldr(j_rarg1, r1_save);\n+    __ ldr(j_rarg2, r2_save);\n+    __ ldr(j_rarg3, r3_save);\n+    __ ldr(j_rarg4, r4_save);\n+    __ ldr(j_rarg5, r5_save);\n+    __ ldr(j_rarg6, r6_save);\n+    __ ldr(j_rarg7, r7_save);\n+\n+    __ leave();\n+\n+    \/\/ check for pending exceptions\n+    Label pending;\n+    __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ cmp(rscratch1, (u1)NULL_WORD);\n+    __ br(Assembler::NE, pending);\n+\n+    if (has_res) {\n+      __ get_vm_result(r0, rthread);\n+    }\n+    __ ret(lr);\n+\n+    __ bind(pending);\n+    __ ldr(r0, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+\n+    \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+    int frame_size_in_words = (framesize >> (LogBytesPerWord - LogBytesPerInt));\n+    RuntimeStub* stub =\n+      RuntimeStub::new_runtime_stub(name, &code, frame_complete, frame_size_in_words, oop_maps, false);\n+\n+    return stub->entry_point();\n+  }\n+\n@@ -6535,0 +6728,5 @@\n+    StubRoutines::_load_inline_type_fields_in_regs =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), \"store_inline_type_fields_to_buf\", true);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":201,"deletions":3,"binary":false,"changes":204,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -438,0 +439,5 @@\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(NULL, true);\n+  }\n+\n@@ -556,0 +562,1 @@\n+  case T_INLINE_TYPE: \/\/ fall through (value types are handled with oops)\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -3196,0 +3196,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1886,1 +1886,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n@@ -1927,1 +1927,1 @@\n-    profile_obj_type(ret, R28_mdx, -in_bytes(ReturnTypeEntry::size()), tmp1, tmp2);\n+    profile_obj_type(ret, R28_mdx, -in_bytes(SingleTypeEntry::size()), tmp1, tmp2);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -519,0 +519,5 @@\n+  if (EnableValhalla) {\n+    assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n+    \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+    andptr(tmpReg, ~((int) markWord::inline_type_bit_in_place));\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -139,1 +139,0 @@\n-      sender_unextended_sp = sender_sp;\n@@ -143,2 +142,2 @@\n-      saved_fp = (intptr_t*) *(sender_sp - frame::sender_sp_offset);\n-    }\n+      intptr_t** saved_fp_addr = (intptr_t**) (sender_sp - frame::sender_sp_offset);\n+      saved_fp = *saved_fp_addr;\n@@ -146,0 +145,4 @@\n+      \/\/ Repair the sender sp if this is a method with scalarized inline type args\n+      sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+      sender_unextended_sp = sender_sp;\n+    }\n@@ -448,3 +451,3 @@\n-  intptr_t* unextended_sp = sender_sp;\n-  \/\/ On Intel the return_address is always the word on the stack\n-  address sender_pc = (address) *(sender_sp-1);\n+#ifdef ASSERT\n+  address sender_pc_copy = (address) *(sender_sp-1);\n+#endif\n@@ -457,0 +460,16 @@\n+  \/\/ Repair the sender sp if the frame has been extended\n+  sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+\n+  \/\/ On Intel the return_address is always the word on the stack\n+  address sender_pc = (address) *(sender_sp-1);\n+\n+#ifdef ASSERT\n+  if (sender_pc != sender_pc_copy) {\n+    \/\/ When extending the stack in the callee method entry to make room for unpacking of value\n+    \/\/ type args, we keep a copy of the sender pc at the expected location in the callee frame.\n+    \/\/ If the sender pc is patched due to deoptimization, the copy is not consistent anymore.\n+    nmethod* nm = CodeCache::find_blob(sender_pc)->as_nmethod();\n+    assert(sender_pc == nm->deopt_mh_handler_begin() || sender_pc == nm->deopt_handler_begin(), \"unexpected sender pc\");\n+  }\n+#endif\n+\n@@ -461,1 +480,20 @@\n-    map->set_include_argument_oops(_cb->caller_must_gc_arguments(map->thread()));\n+    bool caller_args = _cb->caller_must_gc_arguments(map->thread());\n+#ifdef COMPILER1\n+    if (!caller_args) {\n+      nmethod* nm = _cb->as_nmethod_or_null();\n+      if (nm != NULL && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+          pc() < nm->verified_inline_entry_point()) {\n+        \/\/ The VEP and VIEP(RO) of C1-compiled methods call buffer_inline_args_xxx\n+        \/\/ before doing any argument shuffling, so we need to scan the oops\n+        \/\/ as the caller passes them.\n+        caller_args = true;\n+#ifdef ASSERT\n+        NativeCall* call = nativeCall_before(pc());\n+        address dest = call->destination();\n+        assert(dest == Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id) ||\n+               dest == Runtime1::entry_for(Runtime1::buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+      }\n+    }\n+#endif\n+    map->set_include_argument_oops(caller_args);\n@@ -473,1 +511,1 @@\n-  return frame(sender_sp, unextended_sp, *saved_fp_addr, sender_pc);\n+  return frame(sender_sp, sender_sp, *saved_fp_addr, sender_pc);\n@@ -480,1 +518,1 @@\n-  \/\/ Default is we done have to follow them. The sender_for_xxx will\n+  \/\/ Default is we don't have to follow them. The sender_for_xxx will\n@@ -585,0 +623,1 @@\n+    case T_INLINE_TYPE:\n@@ -686,0 +725,15 @@\n+\/\/ Check for a method with scalarized inline type arguments that needs\n+\/\/ a stack repair and return the repaired sender stack pointer.\n+intptr_t* frame::repair_sender_sp(intptr_t* sender_sp, intptr_t** saved_fp_addr) const {\n+  CompiledMethod* cm = _cb->as_compiled_method_or_null();\n+  if (cm != NULL && cm->needs_stack_repair()) {\n+    \/\/ The stack increment resides just below the saved rbp on the stack\n+    \/\/ and does not account for the return address.\n+    intptr_t* real_frame_size_addr = (intptr_t*) (saved_fp_addr - 1);\n+    int real_frame_size = ((*real_frame_size_addr) + wordSize) \/ wordSize;\n+    assert(real_frame_size >= _cb->frame_size() && real_frame_size <= 1000000, \"invalid frame size\");\n+    sender_sp = unextended_sp() + real_frame_size;\n+  }\n+  return sender_sp;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":63,"deletions":9,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -193,1 +193,0 @@\n-\n@@ -207,8 +206,12 @@\n-  \/\/ save the live input values\n-  if(tosca_live) __ push(rax);\n-\n-  if (obj != noreg && obj != rax)\n-    __ push(obj);\n-\n-  if (pre_val != rax)\n-    __ push(pre_val);\n+  \/\/ Barriers might be emitted when converting between (scalarized) calling conventions for inline\n+  \/\/ types. Save all argument registers before calling into the runtime.\n+  __ pusha();\n+  __ subptr(rsp, 64);\n+  __ movdbl(Address(rsp, 0),  j_farg0);\n+  __ movdbl(Address(rsp, 8),  j_farg1);\n+  __ movdbl(Address(rsp, 16), j_farg2);\n+  __ movdbl(Address(rsp, 24), j_farg3);\n+  __ movdbl(Address(rsp, 32), j_farg4);\n+  __ movdbl(Address(rsp, 40), j_farg5);\n+  __ movdbl(Address(rsp, 48), j_farg6);\n+  __ movdbl(Address(rsp, 56), j_farg7);\n@@ -228,2 +231,0 @@\n-  NOT_LP64( __ push(thread); )\n-\n@@ -248,10 +249,11 @@\n-  NOT_LP64( __ pop(thread); )\n-\n-  \/\/ save the live input values\n-  if (pre_val != rax)\n-    __ pop(pre_val);\n-\n-  if (obj != noreg && obj != rax)\n-    __ pop(obj);\n-\n-  if(tosca_live) __ pop(rax);\n+  \/\/ Restore registers\n+  __ movdbl(j_farg0, Address(rsp, 0));\n+  __ movdbl(j_farg1, Address(rsp, 8));\n+  __ movdbl(j_farg2, Address(rsp, 16));\n+  __ movdbl(j_farg3, Address(rsp, 24));\n+  __ movdbl(j_farg4, Address(rsp, 32));\n+  __ movdbl(j_farg5, Address(rsp, 40));\n+  __ movdbl(j_farg6, Address(rsp, 48));\n+  __ movdbl(j_farg7, Address(rsp, 56));\n+  __ addptr(rsp, 64);\n+  __ popa();\n@@ -330,3 +332,13 @@\n-  \/\/ save the live input values\n-  __ push(store_addr);\n-  __ push(new_val);\n+  \/\/ Barriers might be emitted when converting between (scalarized) calling conventions for inline\n+  \/\/ types. Save all argument registers before calling into the runtime.\n+  __ pusha();\n+  __ subptr(rsp, 64);\n+  __ movdbl(Address(rsp, 0),  j_farg0);\n+  __ movdbl(Address(rsp, 8),  j_farg1);\n+  __ movdbl(Address(rsp, 16), j_farg2);\n+  __ movdbl(Address(rsp, 24), j_farg3);\n+  __ movdbl(Address(rsp, 32), j_farg4);\n+  __ movdbl(Address(rsp, 40), j_farg5);\n+  __ movdbl(Address(rsp, 48), j_farg6);\n+  __ movdbl(Address(rsp, 56), j_farg7);\n+\n@@ -340,2 +352,12 @@\n-  __ pop(new_val);\n-  __ pop(store_addr);\n+\n+  \/\/ Restore registers\n+  __ movdbl(j_farg0, Address(rsp, 0));\n+  __ movdbl(j_farg1, Address(rsp, 8));\n+  __ movdbl(j_farg2, Address(rsp, 16));\n+  __ movdbl(j_farg3, Address(rsp, 24));\n+  __ movdbl(j_farg4, Address(rsp, 32));\n+  __ movdbl(j_farg5, Address(rsp, 40));\n+  __ movdbl(j_farg6, Address(rsp, 48));\n+  __ movdbl(j_farg7, Address(rsp, 56));\n+  __ addptr(rsp, 64);\n+  __ popa();\n@@ -347,1 +369,1 @@\n-                                         Address dst, Register val, Register tmp1, Register tmp2) {\n+                                         Address dst, Register val, Register tmp1, Register tmp2, Register tmp3) {\n@@ -350,1 +372,1 @@\n-  assert((decorators & IS_DEST_UNINITIALIZED) == 0, \"unsupported\");\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n@@ -352,1 +374,1 @@\n-  bool needs_pre_barrier = as_normal;\n+  bool needs_pre_barrier = as_normal && !dest_uninitialized;\n@@ -355,1 +377,3 @@\n-  Register tmp3 = LP64_ONLY(r8) NOT_LP64(rsi);\n+  if (tmp3 == noreg) {\n+    tmp3 = LP64_ONLY(r8) NOT_LP64(rsi);\n+  }\n@@ -384,1 +408,1 @@\n-    BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg);\n+    BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg, noreg);\n@@ -394,1 +418,1 @@\n-    BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg);\n+    BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg, noreg);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":56,"deletions":32,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -197,1 +197,2 @@\n-                                    Register tmp2) {\n+                                    Register tmp2,\n+                                    Register tmp3) {\n@@ -200,0 +201,1 @@\n+  assert(type != T_INLINE_TYPE, \"Not supported yet\");\n@@ -215,1 +217,1 @@\n-  BarrierSetAssembler::store_at(masm, decorators, type, dst, src, tmp1, tmp2);\n+  BarrierSetAssembler::store_at(masm, decorators, type, dst, src, tmp1, tmp2, tmp3);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zBarrierSetAssembler_x86.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -48,0 +49,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -51,0 +53,1 @@\n+#include \"vmreg_x86.inline.hpp\"\n@@ -52,0 +55,3 @@\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1622,0 +1628,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2650,0 +2660,140 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  testl(temp_reg, JVM_ACC_INLINE);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  testptr(object, object);\n+  jcc(Assembler::equal, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  movl(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  testl(temp_reg, InstanceKlass::misc_flags_is_empty_inline_type());\n+  jcc(Assembler::notZero, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::zero, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inlined_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inlined);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,\n+                                              Label&is_flattened_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flattened_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flattened_array_layout(temp_reg, is_flattened_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flattened_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flattened_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flattened_array_layout(temp_reg, is_non_flattened_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::nullfree_array_bit_in_place, true, is_null_free_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_null_free_array_layout(temp_reg, is_null_free_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::nullfree_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_null_free_array_layout(temp_reg, is_non_null_free_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_bit_inplace);\n+  jcc(Assembler::notZero, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_bit_inplace);\n+  jcc(Assembler::zero, is_non_null_free_array);\n+}\n+\n+\n@@ -3461,0 +3611,129 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax, according to barrier asm eden_allocate\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+    jcc(Assembler::equal, L);\n+    stop(\"klass not initialized\");\n+    bind(L);\n+  }\n+#endif\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+  const bool allow_shared_alloc =\n+    Universe::heap()->supports_inline_contig_alloc();\n+\n+  push(klass);\n+  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(klass);\n+#ifndef _LP64\n+  if (UseTLAB || allow_shared_alloc) {\n+    get_thread(thread);\n+  }\n+#endif \/\/ _LP64\n+\n+  if (UseTLAB) {\n+    tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    \/\/ Allocation in the shared Eden, if allowed.\n+    \/\/\n+    eden_allocate(thread, new_obj, layout_size, 0, t2, slow_case);\n+  }\n+\n+  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n+  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n+  if (UseTLAB || allow_shared_alloc) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      decrement(layout_size, sizeof(oopDesc));\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 1*oopSize), zero);\n+        NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 2*oopSize), zero));\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+#ifdef _LP64\n+    xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+#endif\n+    movptr(t2, klass);         \/\/ preserve klass\n+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    store_klass(new_obj, t2, tmp_store_klass);  \/\/ src klass reg is potentially compressed\n+\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -3538,0 +3817,50 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  movptr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(inline_klass, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  movptr(inline_klass, Address(inline_klass, index, Address::times_ptr));\n+}\n+\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  movptr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  movl(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  movptr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset, Address::times_1);\n+  load_heap_oop(obj, field);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n+\n@@ -3886,1 +4215,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -3945,1 +4278,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4447,0 +4784,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -4456,1 +4801,1 @@\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -4489,1 +4834,1 @@\n-                                     Register tmp1, Register tmp2) {\n+                                     Register tmp1, Register tmp2, Register tmp3) {\n@@ -4494,1 +4839,23 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+  } else {\n+    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+  }\n+}\n+\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n@@ -4496,1 +4863,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2);\n+    lea(data, Address(oop, offset));\n@@ -4500,0 +4867,18 @@\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE)));\n+}\n+\n@@ -4521,2 +4906,2 @@\n-                                    Register tmp2, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2);\n+                                    Register tmp2, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2, tmp3);\n@@ -4527,1 +4912,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -4841,0 +5226,1 @@\n+#ifdef COMPILER2\n@@ -4842,1 +5228,5 @@\n-void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n@@ -4895,0 +5285,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, C->output()->sp_inc_offset()), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -4923,5 +5319,1 @@\n-\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->nmethod_entry_barrier(this);\n-  }\n+#endif \/\/ COMPILER2\n@@ -4931,1 +5323,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, XMMRegister xtmp) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp) {\n@@ -4935,0 +5327,1 @@\n+  movdq(xtmp, val);\n@@ -4936,1 +5329,2 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -4938,1 +5332,1 @@\n-    pxor(xtmp, xtmp);\n+    punpcklqdq(xtmp, xtmp);\n@@ -4982,1 +5376,297 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp, bool is_large) {\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  Label slow_case;\n+\n+  \/\/ Try to allocate a new buffered inline type (from the heap)\n+  if (UseTLAB) {\n+    \/\/ FIXME -- for smaller code, the inline allocation (and the slow case) should be moved inside the pack handler.\n+    if (vk != NULL) {\n+      \/\/ Called from C1, where the return type is statically known.\n+      movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+      jint lh = vk->layout_helper();\n+      assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+      movl(r14, lh);\n+    } else {\n+      \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+      mov(rbx, rax);\n+      andptr(rbx, -2);\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+    }\n+\n+    movptr(r13, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));\n+    lea(r14, Address(r13, r14, Address::times_1));\n+    cmpptr(r14, Address(r15_thread, in_bytes(JavaThread::tlab_end_offset())));\n+    jcc(Assembler::above, slow_case);\n+    movptr(Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())), r14);\n+    movptr(Address(r13, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+\n+    xorl(rax, rax); \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(r13, rax);  \/\/ zero klass gap for compressed oops\n+\n+    if (vk == NULL) {\n+      \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n+      mov(rax, rbx);\n+    }\n+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    store_klass(r13, rbx, tmp_store_klass);  \/\/ klass\n+\n+    \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+    if (vk != NULL) {\n+      \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+      mov(rax, r13);\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      mov(rax, r13);\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must bevalid\");\n+  Register fromReg;\n+  if (from->is_reg()) {\n+    fromReg = from->as_Register();\n+  } else {\n+    int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+    movq(r10, Address(rsp, st_off));\n+    fromReg = r10;\n+  }\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  while (stream.next(toReg, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+     if (idx != from->value()) {\n+       mark_done = false;\n+     }\n+     done = false;\n+     continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    } else {\n+      assert(reg_state[idx] == reg_writable, \"must be writable\");\n+      reg_state[idx] = reg_written;\n+    }\n+\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? r13 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  Register val_array = rax;\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14; \/\/ Be careful with r14 because it's used for spilling\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  while (stream.next(fromReg, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+    reg_state[fromReg->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    movq(rbp, Address(rsp, initial_framesize));\n+    addq(rsp, Address(rsp, sp_inc_offset));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only) {\n@@ -4987,1 +5677,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"tmp register must be eax for rep stos\");\n@@ -4994,4 +5684,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n-\n@@ -5010,1 +5696,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -5019,1 +5705,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -5023,2 +5709,1 @@\n-    movptr(tmp, base);\n-    xmm_clear_mem(tmp, cnt, xtmp);\n+    xmm_clear_mem(base, cnt, val, xtmp);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":713,"deletions":28,"binary":false,"changes":741,"status":"modified"},{"patch":"@@ -31,0 +31,3 @@\n+#include \"runtime\/signature.hpp\"\n+\n+class ciInlineKlass;\n@@ -101,0 +104,31 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type);\n+  void test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type);\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type);\n+\n+  \/\/ Get the default value oop for the given InlineKlass\n+  void get_default_value_oop(Register inline_klass, Register temp_reg, Register obj);\n+  \/\/ The empty value oop, for the given InlineKlass (\"empty\" as in no instance fields)\n+  \/\/ get_default_value_oop with extra assertion for empty inline klass\n+  void get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj);\n+\n+  void test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline);\n+  void test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline);\n+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined);\n+\n+  \/\/ Check oops for special arrays, i.e. flattened and\/or null-free\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flattened_array_oop(Register oop, Register temp_reg, Label&is_flattened_array);\n+  void test_non_flattened_array_oop(Register oop, Register temp_reg, Label&is_non_flattened_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flatten or null-free arrays...\n+  void test_flattened_array_layout(Register lh, Label& is_flattened_array);\n+  void test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array);\n+  void test_null_free_array_layout(Register lh, Label& is_null_free_array);\n+  void test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array);\n+\n@@ -318,0 +352,1 @@\n+  void load_metadata(Register dst, Register src);\n@@ -324,1 +359,11 @@\n-                       Register tmp1, Register tmp2);\n+                       Register tmp1, Register tmp2, Register tmp3 = noreg);\n+\n+  void access_value_copy(DecoratorSet decorators, Register src, Register dst, Register inline_klass);\n+\n+  \/\/ inline type data payload offsets...\n+  void first_field_offset(Register inline_klass, Register offset);\n+  void data_for_oop(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -335,1 +380,1 @@\n-                      Register tmp2 = noreg, DecoratorSet decorators = 0);\n+                      Register tmp2 = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);\n@@ -511,0 +556,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -530,0 +584,3 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n@@ -695,1 +752,2 @@\n-  void andptr(Register src1, Register src2) { LP64_ONLY(andq(src1, src2)) NOT_LP64(andl(src1, src2)) ; }\n+  void andptr(Register dst, Register src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n+  void andptr(Register dst, Address src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n@@ -1677,1 +1735,15 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(Compile* C, int sp_inc = 0);\n+\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[]);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset);\n+  VMReg spill_reg_for(VMReg reg);\n@@ -1681,1 +1753,1 @@\n-  void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large);\n+  void clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only);\n@@ -1684,1 +1756,1 @@\n-  void xmm_clear_mem(Register base, Register cnt, XMMRegister xtmp);\n+  void xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":78,"deletions":6,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -152,1 +152,5 @@\n-  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_offset() :\n+  \/\/ The following jump might pass an inline type argument that was erased to Object as oop to a\n+  \/\/ callee that expects inline type arguments to be passed as fields. We need to call the compiled\n+  \/\/ value entry (_code->inline_entry_point() or _adapter->c2i_inline_entry()) which will take care\n+  \/\/ of translating between the calling conventions.\n+  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_inline_offset() :\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -510,0 +511,1 @@\n+    case T_INLINE_TYPE:\n@@ -543,0 +545,82 @@\n+\/\/ Same as java_calling_convention() but for multiple return\n+\/\/ values. There's no way to store them on the stack so if we don't\n+\/\/ have enough registers, multiple values can't be returned.\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    rax, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+  static const XMMRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3,\n+    j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_INLINE_TYPE:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+    case T_METADATA:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -585,0 +669,106 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+    for (int i = 0; i < sig_extended->length(); i++) {\n+      BasicType bt = sig_extended->at(i)._bt;\n+      if (bt == T_INLINE_TYPE) {\n+        \/\/ In sig_extended, an inline type argument starts with:\n+        \/\/ T_INLINE_TYPE, followed by the types of the fields of the\n+        \/\/ inline type and T_VOID to mark the end of the value\n+        \/\/ type. Inline types are flattened so, for instance, in the\n+        \/\/ case of an inline type with an int field and an inline type\n+        \/\/ field that itself has 2 fields, an int and a long:\n+        \/\/ T_INLINE_TYPE T_INT T_INLINE_TYPE T_INT T_LONG T_VOID (second\n+        \/\/ slot for the T_LONG) T_VOID (inner T_INLINE_TYPE) T_VOID\n+        \/\/ (outer T_INLINE_TYPE)\n+        total_args_passed++;\n+        int vt = 1;\n+        do {\n+          i++;\n+          BasicType bt = sig_extended->at(i)._bt;\n+          BasicType prev_bt = sig_extended->at(i-1)._bt;\n+          if (bt == T_INLINE_TYPE) {\n+            vt++;\n+          } else if (bt == T_VOID &&\n+                     prev_bt != T_LONG &&\n+                     prev_bt != T_DOUBLE) {\n+            vt--;\n+          }\n+        } while (vt != 0);\n+      } else {\n+        total_args_passed++;\n+      }\n+    }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  assert(bt != T_INLINE_TYPE || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"must be invalid\");\n+    return;\n+  }\n+\n+  if (!r_1->is_XMMRegister()) {\n+    Register val = rax;\n+    if (r_1->is_stack()) {\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, rscratch1);\n+    if (is_oop) {\n+      __ push(r13);\n+      __ push(rbx);\n+      __ store_heap_oop(to, val, rscratch1, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      __ pop(rbx);\n+      __ pop(r13);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    }\n+  }\n+}\n@@ -587,3 +777,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -591,1 +779,6 @@\n-                            Label& skip_fixup) {\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n@@ -601,0 +794,42 @@\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_INLINE_TYPE);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types.\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ false);\n+\n+      frame_complete = __ offset();\n+\n+      __ set_last_Java_frame(noreg, noreg, NULL);\n+\n+      __ mov(c_rarg0, r15_thread);\n+      __ mov(c_rarg1, rbx);\n+      __ mov64(c_rarg2, (int64_t)alloc_inline_receiver);\n+      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+      __ jcc(Assembler::equal, no_exception);\n+\n+      __ movptr(Address(r15_thread, JavaThread::vm_result_offset()), (int)NULL_WORD);\n+      __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+      __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(rscratch2, r15_thread); \/\/ Use rscratch2 (r11) as temporary because rscratch1 (r10) is trashed by movptr()\n+      __ get_vm_result_2(rbx, r15_thread); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n@@ -605,1 +840,1 @@\n-\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n@@ -623,46 +858,24 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n-\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n-    }\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rax\n-      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ movl(rax, Address(rsp, ld_off));\n-        __ movptr(Address(rsp, st_off), rax);\n-\n-      } else {\n-\n-        __ movq(rax, Address(rsp, ld_off));\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ movq(Address(rsp, next_off), rax);\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_INLINE_TYPE,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_INLINE_TYPE\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_INLINE_TYPE) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(rsp, offset), extraspace, false);\n+      next_arg_int++;\n@@ -671,7 +884,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n-          __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ movq(Address(rsp, st_off), rax);\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n+        __ movptr(Address(rsp, st_off), rax);\n@@ -679,16 +889,25 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ movl(Address(rsp, st_off), r);\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ long\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaab));\n-          __ movptr(Address(rsp, st_off), rax);\n-          __ movq(Address(rsp, next_off), r);\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_INLINE_TYPE);\n+      __ load_heap_oop(r14, Address(rscratch2, index));\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp-1)._bt;\n+        if (bt == T_INLINE_TYPE) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID &&\n+                   prev_bt != T_LONG &&\n+                   prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -697,1 +916,6 @@\n-          __ movptr(Address(rsp, st_off), r);\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(r14, off), extraspace, is_oop);\n@@ -699,14 +923,3 @@\n-      }\n-    } else {\n-      assert(r_1->is_XMMRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ movflt(Address(rsp, st_off), r_1->as_XMMRegister());\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov64(rax, CONST64(0xdeadffffdeadaaac));\n-        __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        __ movdbl(Address(rsp, next_off), r_1->as_XMMRegister());\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ movptr(Address(rsp, st_off), r14);\n@@ -735,2 +948,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>* sig,\n@@ -829,1 +1041,1 @@\n-  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_offset())));\n+  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_inline_offset())));\n@@ -843,0 +1055,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -846,1 +1060,3 @@\n-    if (sig_bt[i] == T_VOID) {\n+    BasicType bt = sig->at(i)._bt;\n+    assert(bt != T_INLINE_TYPE, \"i2c adapter doesn't unpack inline type args\");\n+    if (bt == T_VOID) {\n@@ -849,1 +1065,2 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      BasicType prev_bt = (i > 0) ? sig->at(i-1)._bt : T_ILLEGAL;\n+      assert(i > 0 && (prev_bt == T_LONG || prev_bt == T_DOUBLE), \"missing half\");\n@@ -891,1 +1108,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -906,1 +1123,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -937,1 +1154,1 @@\n-  \/\/ only needed becaus eof c2 resolve stubs return Method* as a result in\n+  \/\/ only needed because of c2 resolve stubs return Method* as a result in\n@@ -943,0 +1160,22 @@\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Label ok;\n+\n+  Register holder = rax;\n+  Register receiver = j_rarg0;\n+  Register temp = rbx;\n+\n+  __ load_klass(temp, receiver, rscratch1);\n+  __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n+  __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n+  __ jcc(Assembler::equal, ok);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+\n+  __ bind(ok);\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);\n+  __ jcc(Assembler::equal, skip_fixup);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n+\n@@ -945,4 +1184,8 @@\n-                                                            int total_args_passed,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n@@ -951,2 +1194,1 @@\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -965,11 +1207,1 @@\n-  Label ok;\n-\n-  Register holder = rax;\n-  Register receiver = j_rarg0;\n-  Register temp = rbx;\n-  {\n-    __ load_klass(temp, receiver, rscratch1);\n-    __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ jcc(Assembler::equal, ok);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -978,7 +1210,9 @@\n-    __ bind(ok);\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);\n-    __ jcc(Assembler::equal, skip_fixup);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+    skip_fixup.reset();\n@@ -987,0 +1221,1 @@\n+  \/\/ Scalarized c2i adapter\n@@ -1015,1 +1250,14 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, true);\n+\n+  address c2i_unverified_inline_entry = c2i_unverified_entry;\n+\n+  \/\/ Non-scalarized c2i adapter\n+  address c2i_inline_entry = c2i_entry;\n+  if (regs != regs_cc) {\n+    Label inline_entry_skip_fixup;\n+    c2i_unverified_inline_entry = __ pc();\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+  }\n@@ -1018,1 +1266,7 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+\n+  \/\/ The c2i adapters might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  bool caller_must_gc_arguments = (regs != regs_cc);\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n+\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -1076,0 +1330,1 @@\n+      case T_INLINE_TYPE:\n@@ -2115,0 +2370,1 @@\n+      case T_INLINE_TYPE:\n@@ -2250,0 +2506,6 @@\n+    if (EnableValhalla) {\n+      assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      __ andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+    }\n+\n@@ -2309,0 +2571,1 @@\n+  case T_INLINE_TYPE:           \/\/ Really a handle\n@@ -4027,0 +4290,111 @@\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  __ movptr(rax, Address(r13, 0));\n+  __ resolve_jobject(rax \/* value *\/,\n+                     r15_thread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ movptr(Address(r13, 0), rax);\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r14, r13, rbx, rscratch1);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r14, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(0);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(rax, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(rax, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  if (StressInlineTypeReturnedAsFields) {\n+    __ load_klass(rax, rax, rscratch1);\n+    __ orptr(rax, 1);\n+  }\n+\n+  __ ret(0);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":497,"deletions":123,"binary":false,"changes":620,"status":"modified"},{"patch":"@@ -339,5 +339,5 @@\n-    \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n-    __ movptr(c_rarg0, result);\n-    Label is_long, is_float, is_double, exit;\n-    __ movl(c_rarg1, result_type);\n-    __ cmpl(c_rarg1, T_OBJECT);\n+    \/\/ T_OBJECT, T_INLINE_TYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+    __ movptr(r13, result);\n+    Label is_long, is_float, is_double, is_value, exit;\n+    __ movl(rbx, result_type);\n+    __ cmpl(rbx, T_OBJECT);\n@@ -345,1 +345,3 @@\n-    __ cmpl(c_rarg1, T_LONG);\n+    __ cmpl(rbx, T_INLINE_TYPE);\n+    __ jcc(Assembler::equal, is_value);\n+    __ cmpl(rbx, T_LONG);\n@@ -347,1 +349,1 @@\n-    __ cmpl(c_rarg1, T_FLOAT);\n+    __ cmpl(rbx, T_FLOAT);\n@@ -349,1 +351,1 @@\n-    __ cmpl(c_rarg1, T_DOUBLE);\n+    __ cmpl(rbx, T_DOUBLE);\n@@ -353,1 +355,1 @@\n-    __ movl(Address(c_rarg0, 0), rax);\n+    __ movl(Address(r13, 0), rax);\n@@ -415,0 +417,13 @@\n+    __ BIND(is_value);\n+    if (InlineTypeReturnedAsFields) {\n+      \/\/ Check for flattened return value\n+      __ testptr(rax, 1);\n+      __ jcc(Assembler::zero, is_long);\n+      \/\/ Load pack handler address\n+      __ andptr(rax, -2);\n+      __ movptr(rax, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ movptr(rbx, Address(rax, InlineKlass::pack_handler_jobject_offset()));\n+      \/\/ Call pack handler to initialize the buffer\n+      __ call(rbx);\n+      __ jmp(exit);\n+    }\n@@ -416,1 +431,1 @@\n-    __ movq(Address(c_rarg0, 0), rax);\n+    __ movq(Address(r13, 0), rax);\n@@ -420,1 +435,1 @@\n-    __ movflt(Address(c_rarg0, 0), xmm0);\n+    __ movflt(Address(r13, 0), xmm0);\n@@ -424,1 +439,1 @@\n-    __ movdbl(Address(c_rarg0, 0), xmm0);\n+    __ movdbl(Address(r13, 0), xmm0);\n@@ -2823,1 +2838,1 @@\n-    __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, AS_RAW);  \/\/ store the oop\n+    __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n@@ -3118,0 +3133,8 @@\n+    \/\/ Check for flat inline type array -> return -1\n+    __ testl(rax_lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+    __ jcc(Assembler::notZero, L_failed);\n+\n+    \/\/ Check for null-free (non-flat) inline type array -> handle as object array\n+    __ testl(rax_lh, Klass::_lh_null_free_bit_inplace);\n+    __ jcc(Assembler::notZero, L_objArray);\n+\n@@ -3127,2 +3150,4 @@\n-      __ cmpl(rax_lh, (Klass::_lh_array_tag_type_value << Klass::_lh_array_tag_shift));\n-      __ jcc(Assembler::greaterEqual, L);\n+      __ movl(rklass_tmp, rax_lh);\n+      __ sarl(rklass_tmp, Klass::_lh_array_tag_shift);\n+      __ cmpl(rklass_tmp, Klass::_lh_array_tag_type_value);\n+      __ jcc(Assembler::equal, L);\n@@ -3236,0 +3261,1 @@\n+      \/\/ This check also fails for flat\/null-free arrays which are not supported.\n@@ -3239,0 +3265,13 @@\n+#ifdef ASSERT\n+      {\n+        BLOCK_COMMENT(\"assert not null-free array {\");\n+        Label L;\n+        __ movl(rklass_tmp, Address(rax, lh_offset));\n+        __ testl(rklass_tmp, Klass::_lh_null_free_bit_inplace);\n+        __ jcc(Assembler::zero, L);\n+        __ stop(\"unexpected null-free array\");\n+        __ bind(L);\n+        BLOCK_COMMENT(\"} assert not null-free array\");\n+      }\n+#endif\n+\n@@ -6675,0 +6714,140 @@\n+  \/\/ Call here from the interpreter or compiled code to either load\n+  \/\/ multiple returned values from the inline type instance being\n+  \/\/ returned to registers or to store returned values to a newly\n+  \/\/ allocated inline type instance.\n+  address generate_return_value_stub(address destination, const char* name, bool has_res) {\n+    \/\/ We need to save all registers the calling convention may use so\n+    \/\/ the runtime calls read or update those registers. This needs to\n+    \/\/ be in sync with SharedRuntime::java_return_convention().\n+    enum layout {\n+      pad_off = frame::arg_reg_save_area_bytes\/BytesPerInt, pad_off_2,\n+      rax_off, rax_off_2,\n+      j_rarg5_off, j_rarg5_2,\n+      j_rarg4_off, j_rarg4_2,\n+      j_rarg3_off, j_rarg3_2,\n+      j_rarg2_off, j_rarg2_2,\n+      j_rarg1_off, j_rarg1_2,\n+      j_rarg0_off, j_rarg0_2,\n+      j_farg0_off, j_farg0_2,\n+      j_farg1_off, j_farg1_2,\n+      j_farg2_off, j_farg2_2,\n+      j_farg3_off, j_farg3_2,\n+      j_farg4_off, j_farg4_2,\n+      j_farg5_off, j_farg5_2,\n+      j_farg6_off, j_farg6_2,\n+      j_farg7_off, j_farg7_2,\n+      rbp_off, rbp_off_2,\n+      return_off, return_off_2,\n+\n+      framesize\n+    };\n+\n+    CodeBuffer buffer(name, 1000, 512);\n+    MacroAssembler* masm = new MacroAssembler(&buffer);\n+\n+    int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+    assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+    int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+    int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+    OopMapSet *oop_maps = new OopMapSet();\n+    OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(rax_off), rax->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+    int start = __ offset();\n+\n+    __ subptr(rsp, frame_size_in_bytes - 8 \/* return address*\/);\n+\n+    __ movptr(Address(rsp, rbp_off * BytesPerInt), rbp);\n+    __ movdbl(Address(rsp, j_farg7_off * BytesPerInt), j_farg7);\n+    __ movdbl(Address(rsp, j_farg6_off * BytesPerInt), j_farg6);\n+    __ movdbl(Address(rsp, j_farg5_off * BytesPerInt), j_farg5);\n+    __ movdbl(Address(rsp, j_farg4_off * BytesPerInt), j_farg4);\n+    __ movdbl(Address(rsp, j_farg3_off * BytesPerInt), j_farg3);\n+    __ movdbl(Address(rsp, j_farg2_off * BytesPerInt), j_farg2);\n+    __ movdbl(Address(rsp, j_farg1_off * BytesPerInt), j_farg1);\n+    __ movdbl(Address(rsp, j_farg0_off * BytesPerInt), j_farg0);\n+\n+    __ movptr(Address(rsp, j_rarg0_off * BytesPerInt), j_rarg0);\n+    __ movptr(Address(rsp, j_rarg1_off * BytesPerInt), j_rarg1);\n+    __ movptr(Address(rsp, j_rarg2_off * BytesPerInt), j_rarg2);\n+    __ movptr(Address(rsp, j_rarg3_off * BytesPerInt), j_rarg3);\n+    __ movptr(Address(rsp, j_rarg4_off * BytesPerInt), j_rarg4);\n+    __ movptr(Address(rsp, j_rarg5_off * BytesPerInt), j_rarg5);\n+    __ movptr(Address(rsp, rax_off * BytesPerInt), rax);\n+\n+    int frame_complete = __ offset();\n+\n+    __ set_last_Java_frame(noreg, noreg, NULL);\n+\n+    __ mov(c_rarg0, r15_thread);\n+    __ mov(c_rarg1, rax);\n+\n+    __ call(RuntimeAddress(destination));\n+\n+    \/\/ Set an oopmap for the call site.\n+\n+    oop_maps->add_gc_map( __ offset() - start, map);\n+\n+    \/\/ clear last_Java_sp\n+    __ reset_last_Java_frame(false);\n+\n+    __ movptr(rbp, Address(rsp, rbp_off * BytesPerInt));\n+    __ movdbl(j_farg7, Address(rsp, j_farg7_off * BytesPerInt));\n+    __ movdbl(j_farg6, Address(rsp, j_farg6_off * BytesPerInt));\n+    __ movdbl(j_farg5, Address(rsp, j_farg5_off * BytesPerInt));\n+    __ movdbl(j_farg4, Address(rsp, j_farg4_off * BytesPerInt));\n+    __ movdbl(j_farg3, Address(rsp, j_farg3_off * BytesPerInt));\n+    __ movdbl(j_farg2, Address(rsp, j_farg2_off * BytesPerInt));\n+    __ movdbl(j_farg1, Address(rsp, j_farg1_off * BytesPerInt));\n+    __ movdbl(j_farg0, Address(rsp, j_farg0_off * BytesPerInt));\n+\n+    __ movptr(j_rarg0, Address(rsp, j_rarg0_off * BytesPerInt));\n+    __ movptr(j_rarg1, Address(rsp, j_rarg1_off * BytesPerInt));\n+    __ movptr(j_rarg2, Address(rsp, j_rarg2_off * BytesPerInt));\n+    __ movptr(j_rarg3, Address(rsp, j_rarg3_off * BytesPerInt));\n+    __ movptr(j_rarg4, Address(rsp, j_rarg4_off * BytesPerInt));\n+    __ movptr(j_rarg5, Address(rsp, j_rarg5_off * BytesPerInt));\n+    __ movptr(rax, Address(rsp, rax_off * BytesPerInt));\n+\n+    __ addptr(rsp, frame_size_in_bytes-8);\n+\n+    \/\/ check for pending exceptions\n+    Label pending;\n+    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+    __ jcc(Assembler::notEqual, pending);\n+\n+    if (has_res) {\n+      __ get_vm_result(rax, r15_thread);\n+    }\n+\n+    __ ret(0);\n+\n+    __ bind(pending);\n+\n+    __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+    \/\/ -------------\n+    \/\/ make sure all code is generated\n+    masm->flush();\n+\n+    RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_in_words, oop_maps, false);\n+    return stub->entry_point();\n+  }\n+\n@@ -6690,2 +6869,5 @@\n-    StubRoutines::_call_stub_entry =\n-      generate_call_stub(StubRoutines::_call_stub_return_address);\n+    \/\/ Generate these first because they are called from other stubs\n+    StubRoutines::_load_inline_type_fields_in_regs = generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf = generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), \"store_inline_type_fields_to_buf\", true);\n+\n+    StubRoutines::_call_stub_entry = generate_call_stub(StubRoutines::_call_stub_return_address);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":199,"deletions":17,"binary":false,"changes":216,"status":"modified"},{"patch":"@@ -1523,1 +1523,1 @@\n-  if (!UseFastStosb && UseSSE >= 2 && UseUnalignedLoadStores) {\n+  if (UseSSE >= 2 && UseUnalignedLoadStores) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -873,3 +873,0 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n@@ -891,1 +888,7 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL);\n+  __ verified_entry(C);\n+  __ bind(*_verified_entry);\n+\n+  if (C->stub_function() == NULL) {\n+    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->nmethod_entry_barrier(&_masm);\n+  }\n@@ -903,6 +906,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -956,23 +953,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    emit_opcode(cbuf, Assembler::REX_W);\n-    if (framesize < 0x80) {\n-      emit_opcode(cbuf, 0x83); \/\/ addq rsp, #framesize\n-      emit_rm(cbuf, 0x3, 0x00, RSP_enc);\n-      emit_d8(cbuf, framesize);\n-    } else {\n-      emit_opcode(cbuf, 0x81); \/\/ addq rsp, #framesize\n-      emit_rm(cbuf, 0x3, 0x00, RSP_enc);\n-      emit_d32(cbuf, framesize);\n-    }\n-  }\n-\n-  \/\/ popq rbp\n-  emit_opcode(cbuf, 0x58 | RBP_enc);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair(), C->output()->sp_inc_offset());\n@@ -996,6 +973,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1537,0 +1508,30 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+{\n+  MacroAssembler _masm(&cbuf);\n+  if (!_verified) {  \n+    uint insts_size = cbuf.insts_size();\n+    if (UseCompressedClassPointers) {\n+      __ load_klass(rscratch1, j_rarg0, rscratch2);\n+      __ cmpptr(rax, rscratch1);\n+    } else {\n+      __ cmpptr(rax, Address(j_rarg0, oopDesc::klass_offset_in_bytes()));\n+    }\n+    __ jump_cc(Assembler::notEqual, RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  } else {\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    __ jmp(*_verified_entry);\n+  }\n+}\n+\n@@ -1579,7 +1580,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n-\n@@ -3847,0 +3841,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -4189,1 +4199,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -6643,0 +6653,13 @@\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -10747,1 +10770,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -10750,3 +10773,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);\n@@ -10755,1 +10778,0 @@\n-    $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n@@ -10769,2 +10791,59 @@\n-       $$emit$$\"mov     rdi,rax\\n\\t\"\n-       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n@@ -10773,2 +10852,2 @@\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n@@ -10781,1 +10860,1 @@\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n@@ -10800,2 +10879,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n@@ -10806,1 +10885,1 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -10809,3 +10888,3 @@\n-  predicate(((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);\n@@ -10815,1 +10894,0 @@\n-       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n@@ -10819,2 +10897,49 @@\n-       $$emit$$\"mov     rdi,rax\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val, \n+                        Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n@@ -10823,2 +10948,2 @@\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n@@ -10831,1 +10956,1 @@\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n@@ -10845,1 +10970,0 @@\n-       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n@@ -10850,2 +10974,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register, \n+                 $tmp$$XMMRegister, true, true);\n@@ -12421,0 +12545,15 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == NULL);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -12423,0 +12562,1 @@\n+  predicate(n->as_Call()->entry_point() != NULL);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":212,"deletions":72,"binary":false,"changes":284,"status":"modified"},{"patch":"@@ -795,1 +795,1 @@\n-  return  false;\n+  return false;\n@@ -885,1 +885,2 @@\n-      strcmp(_matrule->_opType,\"Halt\"      )==0 )\n+      strcmp(_matrule->_opType,\"Halt\"      )==0 ||\n+      strcmp(_matrule->_opType,\"CallLeafNoFP\")==0)\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -659,0 +661,11 @@\n+  \/\/ Record this newly allocated object\n+  void new_instance(NewInlineTypeInstance* object) {\n+    int index = _newobjects.length();\n+    _newobjects.append(object);\n+    if (_fields.at_grow(index, NULL) == NULL) {\n+      _fields.at_put(index, new FieldBuffer());\n+    } else {\n+      _fields.at(index)->kill();\n+    }\n+  }\n+\n@@ -941,0 +954,7 @@\n+  if (x->as_NewInlineTypeInstance() != NULL && x->as_NewInlineTypeInstance()->in_larval_state()) {\n+    if (x->as_NewInlineTypeInstance()->on_stack_count() == 1) {\n+      x->as_NewInlineTypeInstance()->set_not_larva_anymore();\n+    } else {\n+      x->as_NewInlineTypeInstance()->increment_on_stack_count();\n+    }\n+  }\n@@ -947,0 +967,4 @@\n+  if (x->as_NewInlineTypeInstance() != NULL) {\n+    x->as_NewInlineTypeInstance()->set_local_index(index);\n+    x->as_NewInlineTypeInstance()->decrement_on_stack_count();\n+  }\n@@ -974,0 +998,1 @@\n+  x->set_local_index(index);\n@@ -975,0 +1000,4 @@\n+  if (x->as_NewInlineTypeInstance() != NULL) {\n+    x->as_NewInlineTypeInstance()->set_local_index(index);\n+    x->as_NewInlineTypeInstance()->decrement_on_stack_count();\n+  }\n@@ -980,1 +1009,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = NULL;\n+  int array_idx = state()->stack_size() - 2;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flattened_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flattened arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -990,1 +1027,44 @@\n-  push(as_ValueType(type), append(new LoadIndexed(array, index, length, type, state_before)));\n+\n+  LoadIndexed* load_indexed = NULL;\n+  Instruction* result = NULL;\n+  if (array->is_loaded_flattened_array()) {\n+    ciType* array_type = array->declared_type();\n+    ciInlineKlass* elem_klass = array_type->as_flat_array_klass()->element_klass()->as_inline_klass();\n+\n+    ciBytecodeStream s(method());\n+    s.force_bci(bci());\n+    s.next();\n+    if (s.cur_bc() == Bytecodes::_getfield) {\n+      \/\/ potentially optimizable array access, storing information for delayed decision\n+      LoadIndexed* li = new LoadIndexed(array, index, length, type, state_before);\n+      DelayedLoadIndexed* dli = new DelayedLoadIndexed(li, state_before);\n+      li->set_delayed(dli);\n+      set_pending_load_indexed(dli);\n+      return; \/\/ Nothing else to do for now\n+    } else {\n+      if (elem_klass->is_empty()) {\n+        \/\/ No need to create a new instance, the default instance will be used instead\n+        load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+        apush(append(load_indexed));\n+      } else {\n+        NewInlineTypeInstance* new_instance = new NewInlineTypeInstance(elem_klass, state_before);\n+        _memory->new_instance(new_instance);\n+        apush(append_split(new_instance));\n+        load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+        load_indexed->set_vt(new_instance);\n+      }\n+    }\n+  } else {\n+    load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+  }\n+  if (profile_array_accesses() && is_reference_type(type) && !array->is_loaded_flattened_array()) {\n+    compilation()->set_would_profile(true);\n+    load_indexed->set_should_profile(true);\n+    load_indexed->set_profiled_method(method());\n+    load_indexed->set_profiled_bci(bci());\n+  }\n+  result = append(load_indexed);\n+  assert(!(profile_array_accesses() && is_reference_type(type)) || load_indexed == result, \"should not be optimized out\");\n+  if (!array->is_loaded_flattened_array()) {\n+    push(as_ValueType(type), result);\n+  }\n@@ -996,1 +1076,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = NULL;\n+  int array_idx = state()->stack_size() - 3;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flattened_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flattened arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -1002,0 +1090,1 @@\n+  value->set_escaped();\n@@ -1019,5 +1108,2 @@\n-  StoreIndexed* result = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n-  append(result);\n-  _memory->store_value(value);\n-  if (type == T_OBJECT && is_profiling()) {\n-    \/\/ Note that we'd collect profile data in this method if we wanted it.\n+  StoreIndexed* store_indexed = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n+  if (profile_array_accesses() && is_reference_type(type)) {\n@@ -1026,6 +1112,3 @@\n-\n-    if (profile_checkcasts()) {\n-      result->set_profiled_method(method());\n-      result->set_profiled_bci(bci());\n-      result->set_should_profile(true);\n-    }\n+    store_indexed->set_should_profile(true);\n+    store_indexed->set_profiled_method(method());\n+    store_indexed->set_profiled_bci(bci());\n@@ -1033,1 +1116,3 @@\n-}\n+  Instruction* result = append(store_indexed);\n+  assert(!store_indexed->should_profile() || store_indexed == result, \"should not be optimized out\");\n+  _memory->store_value(value);\n@@ -1035,0 +1120,1 @@\n+}\n@@ -1039,1 +1125,2 @@\n-      { state()->raw_pop();\n+      { Value w = state()->raw_pop();\n+        update_larva_stack_count(w);\n@@ -1043,2 +1130,4 @@\n-      { state()->raw_pop();\n-        state()->raw_pop();\n+      { Value w1 = state()->raw_pop();\n+        Value w2 = state()->raw_pop();\n+        update_larva_stack_count(w1);\n+        update_larva_stack_count(w2);\n@@ -1049,0 +1138,1 @@\n+        update_larval_state(w);\n@@ -1056,0 +1146,1 @@\n+        update_larval_state(w1);\n@@ -1065,0 +1156,11 @@\n+        \/\/ special handling for the dup_x2\/pop sequence (see JDK-8251046)\n+        if (w1 != NULL && w1->as_NewInlineTypeInstance() != NULL) {\n+          ciBytecodeStream s(method());\n+          s.force_bci(bci());\n+          s.next();\n+          if (s.cur_bc() != Bytecodes::_pop) {\n+            w1->as_NewInlineTypeInstance()->set_not_larva_anymore();\n+          }  else {\n+            w1->as_NewInlineTypeInstance()->increment_on_stack_count();\n+           }\n+        }\n@@ -1074,0 +1176,2 @@\n+        update_larval_state(w1);\n+        update_larval_state(w2);\n@@ -1084,0 +1188,2 @@\n+        update_larval_state(w1);\n+        update_larval_state(w2);\n@@ -1096,0 +1202,2 @@\n+        update_larval_state(w1);\n+        update_larval_state(w2);\n@@ -1227,0 +1335,27 @@\n+\n+  bool subst_check = false;\n+  if (EnableValhalla && (stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne)) {\n+    ValueType* left_vt = x->type();\n+    ValueType* right_vt = y->type();\n+    if (left_vt->is_object()) {\n+      assert(right_vt->is_object(), \"must be\");\n+      ciKlass* left_klass = x->as_loaded_klass_or_null();\n+      ciKlass* right_klass = y->as_loaded_klass_or_null();\n+\n+      if (left_klass == NULL || right_klass == NULL) {\n+        \/\/ The klass is still unloaded, or came from a Phi node. Go slow case;\n+        subst_check = true;\n+      } else if (left_klass->can_be_inline_klass() || right_klass->can_be_inline_klass()) {\n+        \/\/ Either operand may be a value object, but we're not sure. Go slow case;\n+        subst_check = true;\n+      } else {\n+        \/\/ No need to do substitutability check\n+      }\n+    }\n+  }\n+  if ((stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne) &&\n+      is_profiling() && profile_branches()) {\n+    compilation()->set_would_profile(true);\n+    append(new ProfileACmpTypes(method(), bci(), x, y));\n+  }\n+\n@@ -1229,1 +1364,1 @@\n-  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic()) ? state_before : NULL, is_bb));\n+  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic() || subst_check) ? state_before : NULL, is_bb, subst_check));\n@@ -1480,1 +1615,1 @@\n-  if (method()->name() == ciSymbol::object_initializer_name() &&\n+  if (method()->is_object_constructor() &&\n@@ -1631,0 +1766,16 @@\n+void GraphBuilder::copy_inline_content(ciInlineKlass* vk, Value src, int src_off, Value dest, int dest_off,\n+                                       ValueStack* state_before, bool needs_patching) {\n+  assert(!needs_patching, \"Can't patch flattened inline type field access\");\n+  assert(vk->nof_nonstatic_fields() > 0, \"Empty inline type access should be removed\");\n+  src->set_escaped();\n+  for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+    ciField* inner_field = vk->nonstatic_field_at(i);\n+    assert(!inner_field->is_flattened(), \"the iteration over nested fields is handled by the loop itself\");\n+    int off = inner_field->offset() - vk->first_field_offset();\n+    LoadField* load = new LoadField(src, src_off + off, inner_field, false, state_before, needs_patching);\n+    Value replacement = append(load);\n+    StoreField* store = new StoreField(dest, dest_off + off, inner_field, replacement, false, state_before, needs_patching);\n+    append(store);\n+  }\n+}\n+\n@@ -1637,0 +1788,1 @@\n+\n@@ -1640,1 +1792,1 @@\n-                              PatchALot;\n+                              (!field->is_flattened() && PatchALot);\n@@ -1659,1 +1811,1 @@\n-  if (field->is_final() && (code == Bytecodes::_putfield)) {\n+  if (field->is_final() && code == Bytecodes::_putfield) {\n@@ -1680,0 +1832,3 @@\n+      } else if (field_type == T_INLINE_TYPE && field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Loading from a field of an empty inline type. Just return the default instance.\n+        constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n@@ -1687,2 +1842,3 @@\n-        push(type, append(new LoadField(append(obj), offset, field, true,\n-                                        state_before, needs_patching)));\n+        LoadField* load_field = new LoadField(append(obj), offset, field, true,\n+                                        state_before, needs_patching);\n+        push(type, append(load_field));\n@@ -1694,0 +1850,1 @@\n+      val->set_escaped();\n@@ -1697,1 +1854,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1701,0 +1858,4 @@\n+      if (field_type == T_INLINE_TYPE && field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Storing to a field of an empty inline type. Ignore.\n+        break;\n+      }\n@@ -1707,14 +1868,30 @@\n-      obj = apop();\n-      ObjectType* obj_type = obj->type()->as_ObjectType();\n-      if (field->is_constant() && obj_type->is_constant() && !PatchALot) {\n-        ciObject* const_oop = obj_type->constant_value();\n-        if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n-          ciConstant field_value = field->constant_value_of(const_oop);\n-          if (field_value.is_valid()) {\n-            constant = make_constant(field_value, field);\n-            \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n-            if (field->is_call_site_target()) {\n-              ciCallSite* call_site = const_oop->as_call_site();\n-              if (!call_site->is_fully_initialized_constant_call_site()) {\n-                ciMethodHandle* target = field_value.as_object()->as_method_handle();\n-                dependency_recorder()->assert_call_site_target_value(call_site, target);\n+      if (state_before == NULL && field->is_flattened()) {\n+        \/\/ Save the entire state and re-execute on deopt when accessing flattened fields\n+        assert(Interpreter::bytecode_should_reexecute(code), \"should reexecute\");\n+        state_before = copy_state_before();\n+      }\n+      if (!has_pending_field_access() && !has_pending_load_indexed()) {\n+        obj = apop();\n+        ObjectType* obj_type = obj->type()->as_ObjectType();\n+        if (field_type == T_INLINE_TYPE && field->type()->as_inline_klass()->is_empty()) {\n+          \/\/ Loading from a field of an empty inline type. Just return the default instance.\n+          null_check(obj);\n+          constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+        } else if (field->is_constant() && !field->is_flattened() && obj_type->is_constant() && !PatchALot) {\n+          ciObject* const_oop = obj_type->constant_value();\n+          if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n+            ciConstant field_value = field->constant_value_of(const_oop);\n+            if (field_value.is_valid()) {\n+              if (field->signature()->is_Q_signature() && field_value.is_null_or_zero()) {\n+                \/\/ Non-flattened inline type field. Replace null by the default value.\n+                constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+              } else {\n+                constant = make_constant(field_value, field);\n+              }\n+              \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n+              if (field->is_call_site_target()) {\n+                ciCallSite* call_site = const_oop->as_call_site();\n+                if (!call_site->is_fully_initialized_constant_call_site()) {\n+                  ciMethodHandle* target = field_value.as_object()->as_method_handle();\n+                  dependency_recorder()->assert_call_site_target_value(call_site, target);\n+                }\n@@ -1732,19 +1909,15 @@\n-        LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n-        Value replacement = !needs_patching ? _memory->load(load) : load;\n-        if (replacement != load) {\n-          assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n-          \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n-          \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n-          BasicType bt = field->type()->basic_type();\n-          switch (bt) {\n-          case T_BOOLEAN:\n-          case T_BYTE:\n-            replacement = append(new Convert(Bytecodes::_i2b, replacement, as_ValueType(bt)));\n-            break;\n-          case T_CHAR:\n-            replacement = append(new Convert(Bytecodes::_i2c, replacement, as_ValueType(bt)));\n-            break;\n-          case T_SHORT:\n-            replacement = append(new Convert(Bytecodes::_i2s, replacement, as_ValueType(bt)));\n-            break;\n-          default:\n+        if (!field->is_flattened()) {\n+          LoadField* load;\n+          if (has_pending_field_access()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            load = new LoadField(pending_field_access()->obj(),\n+                                 pending_field_access()->offset() + offset - field->holder()->as_inline_klass()->first_field_offset(),\n+                                 field, false, state_before, needs_patching);\n+            set_pending_field_access(NULL);\n+          } else if (has_pending_load_indexed()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+            LoadIndexed* li = pending_load_indexed()->load_instr();\n+            li->set_type(type);\n+            push(type, append(li));\n+            set_pending_load_indexed(NULL);\n@@ -1752,0 +1925,25 @@\n+          } else {\n+            load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n+          }\n+          Value replacement = !needs_patching ? _memory->load(load) : load;\n+          if (replacement != load) {\n+            assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n+            \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n+            \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n+            switch (field_type) {\n+            case T_BOOLEAN:\n+            case T_BYTE:\n+              replacement = append(new Convert(Bytecodes::_i2b, replacement, type));\n+              break;\n+            case T_CHAR:\n+              replacement = append(new Convert(Bytecodes::_i2c, replacement, type));\n+              break;\n+            case T_SHORT:\n+              replacement = append(new Convert(Bytecodes::_i2s, replacement, type));\n+              break;\n+            default:\n+              break;\n+            }\n+            push(type, replacement);\n+          } else {\n+            push(type, append(load));\n@@ -1753,2 +1951,58 @@\n-          push(type, replacement);\n-          push(type, append(load));\n+          \/\/ Look at the next bytecode to check if we can delay the field access\n+          bool can_delay_access = false;\n+          ciBytecodeStream s(method());\n+          s.force_bci(bci());\n+          s.next();\n+          if (s.cur_bc() == Bytecodes::_getfield && !needs_patching) {\n+            ciField* next_field = s.get_field(will_link);\n+            bool next_needs_patching = !next_field->holder()->is_loaded() ||\n+                                       !next_field->will_link(method(), code) ||\n+                                       PatchALot;\n+            can_delay_access = !next_needs_patching;\n+          }\n+          if (can_delay_access) {\n+            if (has_pending_load_indexed()) {\n+              pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+            } else if (has_pending_field_access()) {\n+              pending_field_access()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+            } else {\n+              null_check(obj);\n+              DelayedFieldAccess* dfa = new DelayedFieldAccess(obj, field, field->offset());\n+              set_pending_field_access(dfa);\n+            }\n+          } else {\n+            ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+            scope()->set_wrote_final();\n+            scope()->set_wrote_fields();\n+            if (inline_klass->is_empty()) {\n+              apush(append(new Constant(new InstanceConstant(inline_klass->default_instance()))));\n+              if (has_pending_field_access()) {\n+                set_pending_field_access(NULL);\n+              } else if (has_pending_load_indexed()) {\n+                set_pending_load_indexed(NULL);\n+              }\n+            } else if (has_pending_load_indexed()) {\n+              assert(!needs_patching, \"Can't patch delayed field access\");\n+              pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+              NewInlineTypeInstance* vt = new NewInlineTypeInstance(inline_klass, pending_load_indexed()->state_before());\n+              _memory->new_instance(vt);\n+              pending_load_indexed()->load_instr()->set_vt(vt);\n+              apush(append_split(vt));\n+              append(pending_load_indexed()->load_instr());\n+              set_pending_load_indexed(NULL);\n+            } else {\n+              NewInlineTypeInstance* new_instance = new NewInlineTypeInstance(inline_klass, state_before);\n+              _memory->new_instance(new_instance);\n+              apush(append_split(new_instance));\n+              if (has_pending_field_access()) {\n+                copy_inline_content(inline_klass, pending_field_access()->obj(),\n+                                    pending_field_access()->offset() + field->offset() - field->holder()->as_inline_klass()->first_field_offset(),\n+                                    new_instance, inline_klass->first_field_offset(),\n+                                    state_before, needs_patching);\n+                set_pending_field_access(NULL);\n+              } else {\n+                copy_inline_content(inline_klass, obj, field->offset(), new_instance, inline_klass->first_field_offset(),\n+                                    state_before, needs_patching);\n+              }\n+            }\n+          }\n@@ -1762,0 +2016,1 @@\n+      val->set_escaped();\n@@ -1766,1 +2021,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1770,4 +2025,12 @@\n-      StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n-      if (!needs_patching) store = _memory->store(store);\n-      if (store != NULL) {\n-        append(store);\n+      if (field_type == T_INLINE_TYPE && field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Storing to a field of an empty inline type. Ignore.\n+        null_check(obj);\n+      } else if (!field->is_flattened()) {\n+        StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n+        if (!needs_patching) store = _memory->store(store);\n+        if (store != NULL) {\n+          append(store);\n+        }\n+      } else {\n+        ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+        copy_inline_content(inline_klass, val, inline_klass->first_field_offset(), obj, offset, state_before, needs_patching);\n@@ -1783,0 +2046,80 @@\n+\/\/ Baseline version of withfield, allocate every time\n+void GraphBuilder::withfield(int field_index)\n+{\n+  bool will_link;\n+  ciField* field_modify = stream()->get_field(will_link);\n+  ciInstanceKlass* holder = field_modify->holder();\n+  BasicType field_type = field_modify->type()->basic_type();\n+  ValueType* type = as_ValueType(field_type);\n+\n+  \/\/ call will_link again to determine if the field is valid.\n+  const bool needs_patching = !holder->is_loaded() ||\n+                              !field_modify->will_link(method(), Bytecodes::_withfield) ||\n+                              PatchALot;\n+\n+  scope()->set_wrote_final();\n+  scope()->set_wrote_fields();\n+\n+  const int offset = !needs_patching ? field_modify->offset() : -1;\n+\n+  ValueStack* state_before = copy_state_before();\n+  if (!holder->is_loaded()\n+      || needs_patching \/* FIXME: 8228634 - field_modify->will_link() may incorrectly return false *\/\n+      ) {\n+    Value val = pop(type);\n+    Value obj = apop();\n+    apush(append_split(new WithField(state_before)));\n+    return;\n+  }\n+\n+  Value val = pop(type);\n+  Value obj = apop();\n+\n+  assert(holder->is_inlinetype(), \"must be a value klass\");\n+  \/\/ Save the entire state and re-execute on deopt when executing withfield\n+  state_before->set_should_reexecute(true);\n+  NewInlineTypeInstance* new_instance;\n+  if (obj->as_NewInlineTypeInstance() != NULL && obj->as_NewInlineTypeInstance()->in_larval_state()) {\n+    new_instance = obj->as_NewInlineTypeInstance();\n+    apush(append_split(new_instance));\n+  } else {\n+    new_instance = new NewInlineTypeInstance(holder->as_inline_klass(), state_before);\n+    _memory->new_instance(new_instance);\n+    apush(append_split(new_instance));\n+\n+    for (int i = 0; i < holder->nof_nonstatic_fields(); i++) {\n+      ciField* field = holder->nonstatic_field_at(i);\n+      int off = field->offset();\n+\n+      if (field->offset() != offset) {\n+        if (field->is_flattened()) {\n+          ciInlineKlass* vk = field->type()->as_inline_klass();\n+          if (!vk->is_empty()) {\n+            copy_inline_content(vk, obj, off, new_instance, vk->first_field_offset(), state_before, needs_patching);\n+          }\n+        } else {\n+          \/\/ Only load those fields who are not modified\n+          LoadField* load = new LoadField(obj, off, field, false, state_before, needs_patching);\n+          Value replacement = append(load);\n+          StoreField* store = new StoreField(new_instance, off, field, replacement, false, state_before, needs_patching);\n+          append(store);\n+        }\n+      }\n+    }\n+  }\n+\n+  \/\/ Field to modify\n+  if (field_modify->type()->basic_type() == T_BOOLEAN) {\n+    Value mask = append(new Constant(new IntConstant(1)));\n+    val = append(new LogicOp(Bytecodes::_iand, val, mask));\n+  }\n+  if (field_modify->is_flattened()) {\n+    ciInlineKlass* vk = field_modify->type()->as_inline_klass();\n+    if (!vk->is_empty()) {\n+      copy_inline_content(vk, val, vk->first_field_offset(), new_instance, field_modify->offset(), state_before, needs_patching);\n+    }\n+  } else {\n+    StoreField* store = new StoreField(new_instance, offset, field_modify, val, false, state_before, needs_patching);\n+    append(store);\n+  }\n+}\n@@ -1868,1 +2211,1 @@\n-  if (bc_raw == Bytecodes::_invokespecial && !target->is_object_initializer()) {\n+  if (bc_raw == Bytecodes::_invokespecial && !target->is_object_constructor()) {\n@@ -2123,1 +2466,9 @@\n-  Invoke* result = new Invoke(code, result_type, recv, args, vtable_index, target, state_before);\n+  if (recv != NULL) {\n+    recv->set_escaped();\n+  }\n+  for (int i=0; i<args->length(); i++) {\n+    args->at(0)->set_escaped();\n+  }\n+\n+  Invoke* result = new Invoke(code, result_type, recv, args, vtable_index, target, state_before,\n+                              declared_signature->return_type()->is_inlinetype());\n@@ -2150,0 +2501,11 @@\n+void GraphBuilder::default_value(int klass_index) {\n+  bool will_link;\n+  ciKlass* klass = stream()->get_klass(will_link);\n+  if (!stream()->is_unresolved_klass() && klass->is_inlinetype()) {\n+    ciInlineKlass* vk = klass->as_inline_klass();\n+    apush(append(new Constant(new InstanceConstant(vk->default_instance()))));\n+  } else {\n+    ValueStack* state_before = copy_state_before();\n+    apush(append_split(new DefaultValue(state_before)));\n+  }\n+}\n@@ -2160,0 +2522,1 @@\n+  bool null_free = stream()->is_inline_klass();\n@@ -2161,1 +2524,1 @@\n-  NewArray* n = new NewObjectArray(klass, ipop(), state_before);\n+  NewArray* n = new NewObjectArray(klass, ipop(), state_before, null_free);\n@@ -2186,0 +2549,1 @@\n+  bool null_free = stream()->is_inline_klass();\n@@ -2187,1 +2551,1 @@\n-  CheckCast* c = new CheckCast(klass, apop(), state_before);\n+  CheckCast* c = new CheckCast(klass, apop(), state_before, null_free);\n@@ -2226,0 +2590,19 @@\n+  bool maybe_inlinetype = false;\n+  if (bci == InvocationEntryBci) {\n+    \/\/ Called by GraphBuilder::inline_sync_entry.\n+#ifdef ASSERT\n+    ciType* obj_type = x->declared_type();\n+    assert(obj_type == NULL || !obj_type->is_inlinetype(), \"inline types cannot have synchronized methods\");\n+#endif\n+  } else {\n+    \/\/ We are compiling a monitorenter bytecode\n+    if (EnableValhalla) {\n+      ciType* obj_type = x->declared_type();\n+      if (obj_type == NULL || obj_type->as_klass()->can_be_inline_klass()) {\n+        \/\/ If we're (possibly) locking on an inline type, check for markWord::always_locked_pattern\n+        \/\/ and throw IMSE. (obj_type is null for Phi nodes, so let's just be conservative).\n+        maybe_inlinetype = true;\n+      }\n+    }\n+  }\n+\n@@ -2228,1 +2611,1 @@\n-  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before), bci);\n+  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before, maybe_inlinetype), bci);\n@@ -2402,1 +2785,3 @@\n-    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci(), \"invalid bci\");\n+    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci()\n+           || has_pending_field_access() || has_pending_load_indexed(), \"invalid bci\");\n+\n@@ -2890,0 +3275,2 @@\n+      case Bytecodes::_defaultvalue   : default_value(s.get_index_u2()); break;\n+      case Bytecodes::_withfield      : withfield(s.get_index_u2()); break;\n@@ -3175,1 +3562,2 @@\n-    state->store_local(idx, new Local(method()->holder(), objectType, idx, true));\n+    state->store_local(idx, new Local(method()->holder(), objectType, idx,\n+             \/*receiver*\/ true, \/*null_free*\/ method()->holder()->is_flat_array_klass()));\n@@ -3187,1 +3575,1 @@\n-    state->store_local(idx, new Local(type, vt, idx, false));\n+    state->store_local(idx, new Local(type, vt, idx, false, type->is_inlinetype()));\n@@ -3207,0 +3595,2 @@\n+  , _pending_field_access(NULL)\n+  , _pending_load_indexed(NULL)\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":463,"deletions":73,"binary":false,"changes":536,"status":"modified"},{"patch":"@@ -209,0 +209,1 @@\n+  bool                          _should_reexecute;\n@@ -216,1 +217,2 @@\n-                   IRScopeDebugInfo*             caller):\n+                   IRScopeDebugInfo*             caller,\n+                   bool                          should_reexecute):\n@@ -222,1 +224,2 @@\n-    , _caller(caller) {}\n+    , _caller(caller)\n+    , _should_reexecute(should_reexecute) {}\n@@ -235,1 +238,1 @@\n-  void record_debug_info(DebugInformationRecorder* recorder, int pc_offset, bool topmost, bool is_method_handle_invoke = false) {\n+  void record_debug_info(DebugInformationRecorder* recorder, int pc_offset, bool topmost, bool is_method_handle_invoke = false, bool maybe_return_as_fields = false) {\n@@ -246,0 +249,5 @@\n+    bool return_vt = false;\n+    if (maybe_return_as_fields) {\n+      return_oop = true;\n+      return_vt = true;\n+    }\n@@ -251,1 +259,1 @@\n-                             reexecute, rethrow_exception, is_method_handle_invoke, is_opt_native, return_oop,\n+                             reexecute, rethrow_exception, is_method_handle_invoke, is_opt_native, return_oop, return_vt,\n@@ -288,1 +296,1 @@\n-  void record_debug_info(DebugInformationRecorder* recorder, int pc_offset);\n+  void record_debug_info(DebugInformationRecorder* recorder, int pc_offset, bool maybe_return_as_fields = false);\n","filename":"src\/hotspot\/share\/c1\/c1_IR.hpp","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,0 +32,1 @@\n+#include \"oops\/flatArrayOop.hpp\"\n@@ -46,0 +47,1 @@\n+  ciArray(flatArrayHandle h_a) : ciObject(h_a), _length(h_a()->length()) {}\n","filename":"src\/hotspot\/share\/ci\/ciArray.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+class       ciInlineKlass;\n@@ -67,0 +68,1 @@\n+class       ciFlatArrayKlass;\n@@ -119,0 +121,1 @@\n+friend class ciInlineKlass;            \\\n@@ -120,0 +123,1 @@\n+friend class ciFlatArrayKlass;         \\\n","filename":"src\/hotspot\/share\/ci\/ciClassList.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -460,1 +461,3 @@\n-      (sym->char_at(1) == JVM_SIGNATURE_ARRAY || sym->char_at(1) == JVM_SIGNATURE_CLASS)) {\n+      (sym->char_at(1) == JVM_SIGNATURE_ARRAY ||\n+       sym->char_at(1) == JVM_SIGNATURE_CLASS ||\n+       sym->char_at(1) == JVM_SIGNATURE_INLINE_TYPE )) {\n@@ -473,1 +476,1 @@\n-      return ciObjArrayKlass::make_impl(elem_klass);\n+      return ciArrayKlass::make(elem_klass);\n@@ -499,0 +502,15 @@\n+  int i = 0;\n+  while (sym->char_at(i) == JVM_SIGNATURE_ARRAY) {\n+    i++;\n+  }\n+  if (i > 0 && sym->char_at(i) == JVM_SIGNATURE_INLINE_TYPE) {\n+    \/\/ An unloaded array class of inline types is an ObjArrayKlass, an\n+    \/\/ unloaded inline type class is an InstanceKlass. For consistency,\n+    \/\/ make the signature of the unloaded array of inline type use L\n+    \/\/ rather than Q.\n+    char *new_name = CURRENT_THREAD_ENV->name_buffer(sym->utf8_length()+1);\n+    strncpy(new_name, (char*)sym->base(), sym->utf8_length());\n+    new_name[i] = JVM_SIGNATURE_CLASS;\n+    new_name[sym->utf8_length()] = '\\0';\n+    return get_unloaded_klass(accessing_klass, ciSymbol::make(new_name));\n+  }\n@@ -529,1 +547,1 @@\n-    klass =  ConstantPool::klass_at_if_loaded(cpool, index);\n+    klass = ConstantPool::klass_at_if_loaded(cpool, index);\n@@ -581,0 +599,8 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciEnv::is_inline_klass\n+\/\/\n+\/\/ Check if the klass is an inline klass.\n+bool ciEnv::is_inline_klass(const constantPoolHandle& cpool, int index) {\n+  GUARDED_VM_ENTRY(return cpool->klass_name_at(index)->is_Q_signature();)\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":29,"deletions":3,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -137,0 +137,2 @@\n+  bool       is_inline_klass(const constantPoolHandle& cpool,\n+                             int klass_index);\n@@ -201,0 +203,4 @@\n+  ciFlatArrayKlass* get_flat_array_klass(Klass* o) {\n+    if (o == NULL) return NULL;\n+    return get_metadata(o)->as_flat_array_klass();\n+  }\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -47,1 +47,2 @@\n-  friend class ciSignature;\n+  friend class ciSignature;\n+  friend class ciFlatArrayKlass;\n@@ -110,0 +111,8 @@\n+  virtual bool can_be_inline_klass(bool is_exact = false) {\n+    return false;\n+  }\n+\n+  virtual bool can_be_inline_array_klass() {\n+    return EnableValhalla && is_java_lang_Object();\n+  }\n+\n","filename":"src\/hotspot\/share\/ci\/ciKlass.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -656,0 +657,33 @@\n+bool ciMethod::array_access_profiled_type(int bci, ciKlass*& array_type, ciKlass*& element_type, ProfilePtrKind& element_ptr, bool &flat_array, bool &null_free_array) {\n+  if (method_data() != NULL && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != NULL && data->is_ArrayLoadStoreData()) {\n+      ciArrayLoadStoreData* array_access = (ciArrayLoadStoreData*)data->as_ArrayLoadStoreData();\n+      array_type = array_access->array()->valid_type();\n+      element_type = array_access->element()->valid_type();\n+      element_ptr = array_access->element()->ptr_kind();\n+      flat_array = array_access->flat_array();\n+      null_free_array = array_access->null_free_array();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ciMethod::acmp_profiled_type(int bci, ciKlass*& left_type, ciKlass*& right_type, ProfilePtrKind& left_ptr, ProfilePtrKind& right_ptr, bool &left_inline_type, bool &right_inline_type) {\n+  if (method_data() != NULL && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != NULL && data->is_ACmpData()) {\n+      ciACmpData* acmp = (ciACmpData*)data->as_ACmpData();\n+      left_type = acmp->left()->valid_type();\n+      right_type = acmp->right()->valid_type();\n+      left_ptr = acmp->left()->ptr_kind();\n+      right_ptr = acmp->right()->ptr_kind();\n+      left_inline_type = acmp->left_inline_type();\n+      right_inline_type = acmp->right_inline_type();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -942,1 +976,1 @@\n-\/\/ ciMethod::is_object_initializer\n+\/\/ ciMethod::is_object_constructor\n@@ -944,2 +978,15 @@\n-bool ciMethod::is_object_initializer() const {\n-   return name() == ciSymbol::object_initializer_name();\n+bool ciMethod::is_object_constructor() const {\n+   return (name() == ciSymbol::object_initializer_name()\n+           && signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciMethod::is_static_init_factory\n+\/\/\n+bool ciMethod::is_static_init_factory() const {\n+   return (name() == ciSymbol::object_initializer_name()\n+           && !signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n@@ -1225,1 +1272,1 @@\n-bool ciMethod::is_initializer () const {         FETCH_FLAG_FROM_VM(is_initializer); }\n+bool ciMethod::is_object_constructor_or_class_initializer() const { FETCH_FLAG_FROM_VM(is_object_constructor_or_class_initializer); }\n@@ -1376,0 +1423,1 @@\n+  if (bt == T_INLINE_TYPE)   return T_OBJECT;\n@@ -1463,0 +1511,13 @@\n+\n+bool ciMethod::has_scalarized_args() const {\n+  VM_ENTRY_MARK;\n+  return get_Method()->has_scalarized_args();\n+}\n+\n+const GrowableArray<SigEntry>* ciMethod::get_sig_cc() {\n+  VM_ENTRY_MARK;\n+  if (get_Method()->adapter() == NULL) {\n+    return NULL;\n+  }\n+  return get_Method()->adapter()->get_sig_cc();\n+}\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":65,"deletions":4,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+  ProfileUnknownNull,\n@@ -202,1 +203,1 @@\n-  bool is_static_initializer() const { return get_Method()->is_static_initializer(); }\n+  bool is_class_initializer()  const { return get_Method()->is_class_initializer(); }\n@@ -262,1 +263,4 @@\n-\n+  bool          array_access_profiled_type(int bci, ciKlass*& array_type, ciKlass*& element_type, ProfilePtrKind& element_ptr, bool &flat_array, bool &null_free);\n+  bool          acmp_profiled_type(int bci, ciKlass*& left_type, ciKlass*& right_type,\n+                                   ProfilePtrKind& left_ptr, ProfilePtrKind& right_ptr,\n+                                   bool &left_inline_type, bool &right_inline_type);\n@@ -334,0 +338,1 @@\n+  bool has_vararg     () const                   { return flags().has_vararg(); }\n@@ -345,1 +350,0 @@\n-  bool is_initializer () const;\n@@ -350,0 +354,3 @@\n+  bool is_object_constructor() const;\n+  bool is_static_init_factory() const;\n+  bool is_object_constructor_or_class_initializer() const;\n@@ -351,1 +358,0 @@\n-  bool is_object_initializer() const;\n@@ -371,0 +377,4 @@\n+\n+  \/\/ Support for the inline type calling convention\n+  bool has_scalarized_args() const;\n+  const GrowableArray<SigEntry>* get_sig_cc();\n","filename":"src\/hotspot\/share\/ci\/ciMethod.hpp","additions":14,"deletions":4,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+#include \"ci\/ciFlatArray.hpp\"\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -44,0 +46,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -360,0 +363,3 @@\n+  } else if (o->is_flatArray()) {\n+    flatArrayHandle h_ta(THREAD, (flatArrayOop)o);\n+    return new (arena()) ciFlatArray(h_ta);\n@@ -379,1 +385,3 @@\n-    if (k->is_instance_klass()) {\n+    if (k->is_inline_klass()) {\n+      return new (arena()) ciInlineKlass(k);\n+    } else if (k->is_instance_klass()) {\n@@ -381,0 +389,2 @@\n+    } else if (k->is_flatArray_klass()) {\n+      return new (arena()) ciFlatArrayKlass(k);\n@@ -490,1 +500,1 @@\n-    if (element_type == T_OBJECT) {\n+    if (element_type == T_OBJECT || element_type == T_INLINE_TYPE) {\n","filename":"src\/hotspot\/share\/ci\/ciObjectFactory.cpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -73,0 +73,16 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciSignature::maybe_returns_inline_type\n+\/\/\n+\/\/ True if we statically know that the return value is never null, or\n+\/\/ if the return type has a Q signature but is not yet loaded, in which case\n+\/\/ it could be a never-null type.\n+bool ciSignature::maybe_returns_inline_type() const {\n+  ciType* ret_type = return_type();\n+  if (ret_type->is_inlinetype()) {\n+    return true;\n+  } else if (ret_type->is_instance_klass() && !ret_type->as_instance_klass()->is_loaded()) {\n+    GUARDED_VM_ENTRY(if (get_symbol()->is_Q_method_signature()) { return true; })\n+  }\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciSignature.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+  bool      maybe_returns_inline_type() const;\n","filename":"src\/hotspot\/share\/ci\/ciSignature.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -222,0 +222,1 @@\n+  bool is_inline_klass() const;\n","filename":"src\/hotspot\/share\/ci\/ciStreams.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -79,0 +79,14 @@\n+bool ciSymbol::starts_with(char prefix_char) const {\n+  GUARDED_VM_ENTRY(return get_symbol()->starts_with(prefix_char);)\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciSymbol::ends_with\n+\/\/\n+\/\/ Tests if the symbol ends with the given suffix.\n+bool ciSymbol::ends_with(const char* suffix, int len) const {\n+  GUARDED_VM_ENTRY(return get_symbol()->ends_with(suffix, len);)\n+}\n+bool ciSymbol::ends_with(char suffix_char) const {\n+  GUARDED_VM_ENTRY(return get_symbol()->ends_with(suffix_char);)\n+}\n@@ -98,0 +112,6 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciSymbol::is_Q_signature\n+bool ciSymbol::is_Q_signature() {\n+  GUARDED_VM_ENTRY(return get_symbol()->is_Q_signature();)\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciSymbol.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+  friend class ciFlatArrayKlass;\n@@ -85,0 +86,7 @@\n+  bool starts_with(char prefix_char) const;\n+\n+  \/\/ Tests if the symbol ends with the given suffix.\n+  bool ends_with(const char* suffix, int len) const;\n+  bool ends_with(char suffix_char) const;\n+\n+  bool        is_Q_signature();\n","filename":"src\/hotspot\/share\/ci\/ciSymbol.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-\/\/ This class represents a Java reference or primitive type.\n+\/\/ This class represents a Java reference, inline type or primitive type.\n@@ -49,1 +49,1 @@\n-  _basic_type = k->is_array_klass() ? T_ARRAY : T_OBJECT;\n+  _basic_type = k->is_array_klass() ? T_ARRAY : (k->is_inline_klass() ? T_INLINE_TYPE : T_OBJECT);\n","filename":"src\/hotspot\/share\/ci\/ciType.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-\/\/ This class represents a Java reference or primitive type.\n+\/\/ This class represents a Java reference, inline type or primitive type.\n","filename":"src\/hotspot\/share\/ci\/ciType.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -279,1 +280,1 @@\n-    \/\/ is T.  null_type meet null_type is null_type.\n+    \/\/ is T (except for inline types).  null_type meet null_type is null_type.\n@@ -281,1 +282,4 @@\n-      if (!t2->is_primitive_type() || t2->equals(null_type())) {\n+      if (t2->is_inlinetype()) {\n+        \/\/ Inline types are null-free, return the super type\n+        return t2->as_inline_klass()->super();\n+      } else if (!t2->is_primitive_type() || t2->equals(null_type())) {\n@@ -285,1 +289,4 @@\n-      if (!t1->is_primitive_type()) {\n+      if (t1->is_inlinetype()) {\n+        \/\/ Inline types are null-free, return the super type\n+        return t1->as_inline_klass()->super();\n+      } else if (!t1->is_primitive_type()) {\n@@ -293,35 +300,36 @@\n-  } else {\n-    \/\/ Both types are non-top non-primitive types.  That is,\n-    \/\/ both types are either instanceKlasses or arrayKlasses.\n-    ciKlass* object_klass = analyzer->env()->Object_klass();\n-    ciKlass* k1 = t1->as_klass();\n-    ciKlass* k2 = t2->as_klass();\n-    if (k1->equals(object_klass) || k2->equals(object_klass)) {\n-      return object_klass;\n-    } else if (!k1->is_loaded() || !k2->is_loaded()) {\n-      \/\/ Unloaded classes fall to java.lang.Object at a merge.\n-      return object_klass;\n-    } else if (k1->is_interface() != k2->is_interface()) {\n-      \/\/ When an interface meets a non-interface, we get Object;\n-      \/\/ This is what the verifier does.\n-      return object_klass;\n-    } else if (k1->is_array_klass() || k2->is_array_klass()) {\n-      \/\/ When an array meets a non-array, we get Object.\n-      \/\/ When objArray meets typeArray, we also get Object.\n-      \/\/ And when typeArray meets different typeArray, we again get Object.\n-      \/\/ But when objArray meets objArray, we look carefully at element types.\n-      if (k1->is_obj_array_klass() && k2->is_obj_array_klass()) {\n-        \/\/ Meet the element types, then construct the corresponding array type.\n-        ciKlass* elem1 = k1->as_obj_array_klass()->element_klass();\n-        ciKlass* elem2 = k2->as_obj_array_klass()->element_klass();\n-        ciKlass* elem  = type_meet_internal(elem1, elem2, analyzer)->as_klass();\n-        \/\/ Do an easy shortcut if one type is a super of the other.\n-        if (elem == elem1) {\n-          assert(k1 == ciObjArrayKlass::make(elem), \"shortcut is OK\");\n-          return k1;\n-        } else if (elem == elem2) {\n-          assert(k2 == ciObjArrayKlass::make(elem), \"shortcut is OK\");\n-          return k2;\n-        } else {\n-          return ciObjArrayKlass::make(elem);\n-        }\n+  }\n+\n+  \/\/ Both types are non-top non-primitive types.  That is,\n+  \/\/ both types are either instanceKlasses or arrayKlasses.\n+  ciKlass* object_klass = analyzer->env()->Object_klass();\n+  ciKlass* k1 = t1->as_klass();\n+  ciKlass* k2 = t2->as_klass();\n+  if (k1->equals(object_klass) || k2->equals(object_klass)) {\n+    return object_klass;\n+  } else if (!k1->is_loaded() || !k2->is_loaded()) {\n+    \/\/ Unloaded classes fall to java.lang.Object at a merge.\n+    return object_klass;\n+  } else if (k1->is_interface() != k2->is_interface()) {\n+    \/\/ When an interface meets a non-interface, we get Object;\n+    \/\/ This is what the verifier does.\n+    return object_klass;\n+  } else if (k1->is_array_klass() || k2->is_array_klass()) {\n+    \/\/ When an array meets a non-array, we get Object.\n+    \/\/ When (obj\/flat)Array meets typeArray, we also get Object.\n+    \/\/ And when typeArray meets different typeArray, we again get Object.\n+    \/\/ But when (obj\/flat)Array meets (obj\/flat)Array, we look carefully at element types.\n+    if ((k1->is_obj_array_klass() || k1->is_flat_array_klass()) &&\n+        (k2->is_obj_array_klass() || k2->is_flat_array_klass())) {\n+      ciType* elem1 = k1->as_array_klass()->element_klass();\n+      ciType* elem2 = k2->as_array_klass()->element_klass();\n+      ciType* elem = elem1;\n+      if (elem1 != elem2) {\n+        elem = type_meet_internal(elem1, elem2, analyzer)->as_klass();\n+      }\n+      \/\/ Do an easy shortcut if one type is a super of the other.\n+      if (elem == elem1) {\n+        assert(k1 == ciArrayKlass::make(elem), \"shortcut is OK\");\n+        return k1;\n+      } else if (elem == elem2) {\n+        assert(k2 == ciArrayKlass::make(elem), \"shortcut is OK\");\n+        return k2;\n@@ -329,1 +337,1 @@\n-        return object_klass;\n+        return ciArrayKlass::make(elem);\n@@ -332,4 +340,1 @@\n-      \/\/ Must be two plain old instance klasses.\n-      assert(k1->is_instance_klass(), \"previous cases handle non-instances\");\n-      assert(k2->is_instance_klass(), \"previous cases handle non-instances\");\n-      return k1->least_common_ancestor(k2);\n+      return object_klass;\n@@ -337,0 +342,5 @@\n+  } else {\n+    \/\/ Must be two plain old instance klasses.\n+    assert(k1->is_instance_klass(), \"previous cases handle non-instances\");\n+    assert(k2->is_instance_klass(), \"previous cases handle non-instances\");\n+    return k1->least_common_ancestor(k2);\n@@ -550,2 +560,2 @@\n-\/\/ ciTypeFlow::StateVector::do_aaload\n-void ciTypeFlow::StateVector::do_aaload(ciBytecodeStream* str) {\n+\/\/ ciTypeFlow::StateVector::do_aload\n+void ciTypeFlow::StateVector::do_aload(ciBytecodeStream* str) {\n@@ -553,1 +563,1 @@\n-  ciObjArrayKlass* array_klass = pop_objArray();\n+  ciArrayKlass* array_klass = pop_objOrFlatArray();\n@@ -555,1 +565,1 @@\n-    \/\/ Did aaload on a null reference; push a null and ignore the exception.\n+    \/\/ Did aload on a null reference; push a null and ignore the exception.\n@@ -591,6 +601,13 @@\n-    \/\/ VM's interpreter will not load 'klass' if object is NULL.\n-    \/\/ Type flow after this block may still be needed in two situations:\n-    \/\/ 1) C2 uses do_null_assert() and continues compilation for later blocks\n-    \/\/ 2) C2 does an OSR compile in a later block (see bug 4778368).\n-    pop_object();\n-    do_null_assert(klass);\n+    if (str->is_inline_klass()) {\n+      trap(str, klass,\n+           Deoptimization::make_trap_request\n+           (Deoptimization::Reason_unloaded,\n+            Deoptimization::Action_reinterpret));\n+    } else {\n+      \/\/ VM's interpreter will not load 'klass' if object is NULL.\n+      \/\/ Type flow after this block may still be needed in two situations:\n+      \/\/ 1) C2 uses do_null_assert() and continues compilation for later blocks\n+      \/\/ 2) C2 does an OSR compile in a later block (see bug 4778368).\n+      pop_object();\n+      do_null_assert(klass);\n+    }\n@@ -765,1 +782,13 @@\n-  if (!will_link || str->is_unresolved_klass()) {\n+  if (!will_link || str->is_unresolved_klass() || klass->is_inlinetype()) {\n+    trap(str, klass, str->get_klass_index());\n+  } else {\n+    push_object(klass);\n+  }\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciTypeFlow::StateVector::do_defaultvalue\n+void ciTypeFlow::StateVector::do_defaultvalue(ciBytecodeStream* str) {\n+  bool will_link;\n+  ciKlass* klass = str->get_klass(will_link);\n+  if (!will_link || !klass->is_inlinetype()) {\n@@ -772,0 +801,22 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciTypeFlow::StateVector::do_withfield\n+void ciTypeFlow::StateVector::do_withfield(ciBytecodeStream* str) {\n+  bool will_link;\n+  ciField* field = str->get_field(will_link);\n+  ciKlass* klass = field->holder();\n+  if (!will_link) {\n+    trap(str, klass, str->get_field_holder_index());\n+  } else {\n+    ciType* type = pop_value();\n+    ciType* field_type = field->type();\n+    if (field_type->is_two_word()) {\n+      ciType* type2 = pop_value();\n+      assert(type2->is_two_word(), \"must be 2nd half\");\n+      assert(type == half_type(type2), \"must be 2nd half\");\n+    }\n+    pop_object();\n+    assert(klass->is_inlinetype(), \"should be inline type\");\n+    push_object(klass);\n+  }\n+}\n+\n@@ -877,1 +928,1 @@\n-  case Bytecodes::_aaload: do_aaload(str);                       break;\n+  case Bytecodes::_aaload: do_aload(str);                           break;\n@@ -883,1 +934,1 @@\n-      pop_objArray();\n+      pop_objOrFlatArray();\n@@ -905,1 +956,1 @@\n-        push_object(ciObjArrayKlass::make(element_klass));\n+        push_object(ciArrayKlass::make(element_klass));\n@@ -1437,0 +1488,3 @@\n+  case Bytecodes::_defaultvalue: do_defaultvalue(str);              break;\n+  case Bytecodes::_withfield: do_withfield(str);                    break;\n+\n@@ -1464,0 +1518,1 @@\n+\n@@ -1746,3 +1801,6 @@\n-      case Bytecodes::_athrow:     case Bytecodes::_ireturn:\n-      case Bytecodes::_lreturn:    case Bytecodes::_freturn:\n-      case Bytecodes::_dreturn:    case Bytecodes::_areturn:\n+      case Bytecodes::_athrow:\n+      case Bytecodes::_ireturn:\n+      case Bytecodes::_lreturn:\n+      case Bytecodes::_freturn:\n+      case Bytecodes::_dreturn:\n+      case Bytecodes::_areturn:\n","filename":"src\/hotspot\/share\/ci\/ciTypeFlow.cpp","additions":118,"deletions":60,"binary":false,"changes":178,"status":"modified"},{"patch":"@@ -334,2 +334,2 @@\n-    \/\/ pop_objArray and pop_typeArray narrow the tos to ciObjArrayKlass\n-    \/\/ or ciTypeArrayKlass (resp.).  In the rare case that an explicit\n+    \/\/ pop_objOrFlatArray and pop_typeArray narrow the tos to ciObjArrayKlass,\n+    \/\/ ciFlatArrayKlass or ciTypeArrayKlass (resp.). In the rare case that an explicit\n@@ -337,1 +337,1 @@\n-    ciObjArrayKlass* pop_objArray() {\n+    ciArrayKlass* pop_objOrFlatArray() {\n@@ -340,2 +340,3 @@\n-      assert(array->is_obj_array_klass(), \"must be object array type\");\n-      return array->as_obj_array_klass();\n+      assert(array->is_obj_array_klass() || array->is_flat_array_klass(),\n+             \"must be a flat or an object array type\");\n+      return array->as_array_klass();\n@@ -355,1 +356,1 @@\n-    void do_aaload(ciBytecodeStream* str);\n+    void do_aload(ciBytecodeStream* str);\n@@ -364,0 +365,2 @@\n+    void do_defaultvalue(ciBytecodeStream* str);\n+    void do_withfield(ciBytecodeStream* str);\n","filename":"src\/hotspot\/share\/ci\/ciTypeFlow.hpp","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -47,0 +47,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -48,1 +50,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -1007,1 +1009,6 @@\n-      if (k->is_typeArray_klass()) {\n+      if (k->is_flatArray_klass()) {\n+        Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+        assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+        InlineKlass* vk = InlineKlass::cast(InstanceKlass::cast(element_klass));\n+        comp_mirror = Handle(THREAD, vk->java_mirror());\n+      } else if (k->is_typeArray_klass()) {\n@@ -1106,0 +1113,1 @@\n+      case T_INLINE_TYPE:\n@@ -1189,0 +1197,6 @@\n+  if (k->is_inline_klass()) {\n+    \/\/ Inline types have a val type mirror and a ref type mirror. Don't handle this for now. TODO:CDS\n+    k->clear_java_mirror_handle();\n+    return NULL;\n+  }\n+\n@@ -1516,0 +1530,1 @@\n+  bool is_value = false;\n@@ -1521,0 +1536,1 @@\n+    is_value = k->is_inline_klass();\n@@ -1527,1 +1543,7 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    if (is_value) {\n+      st->print(\"Q\");\n+    } else {\n+      st->print(\"L\");\n+    }\n+  }\n@@ -1549,1 +1571,1 @@\n-      int         siglen = (int) strlen(sigstr);\n+      int siglen = (int) strlen(sigstr);\n@@ -2514,2 +2536,2 @@\n-      \/\/ This is simlar to classic VM.\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      \/\/ This is similar to classic VM (before HotSpot).\n+      if (method->is_object_constructor() &&\n@@ -3948,1 +3970,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n@@ -4774,0 +4796,71 @@\n+\/\/ jdk_internal_vm_jni_SubElementSelector\n+\n+int jdk_internal_vm_jni_SubElementSelector::_arrayElementType_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_subElementType_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_offset_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_isInlined_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_isInlineType_offset;\n+\n+#define SUBELEMENT_SELECTOR_FIELDS_DO(macro) \\\n+  macro(_arrayElementType_offset,  k, \"arrayElementType\", class_signature, false); \\\n+  macro(_subElementType_offset,    k, \"subElementType\",   class_signature, false); \\\n+  macro(_offset_offset,            k, \"offset\",           int_signature,   false); \\\n+  macro(_isInlined_offset,         k, \"isInlined\",        bool_signature,  false); \\\n+  macro(_isInlineType_offset,      k, \"isInlineType\",     bool_signature,  false);\n+\n+void jdk_internal_vm_jni_SubElementSelector::compute_offsets() {\n+  InstanceKlass* k = SystemDictionary::jdk_internal_vm_jni_SubElementSelector_klass();\n+  SUBELEMENT_SELECTOR_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+}\n+\n+#if INCLUDE_CDS\n+void jdk_internal_vm_jni_SubElementSelector::serialize_offsets(SerializeClosure* f) {\n+  SUBELEMENT_SELECTOR_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+#undef SUBELEMENT_SELECTOR_FIELDS_DO\n+\n+Symbol* jdk_internal_vm_jni_SubElementSelector::symbol() {\n+  return vmSymbols::jdk_internal_vm_jni_SubElementSelector();\n+}\n+\n+oop jdk_internal_vm_jni_SubElementSelector::getArrayElementType(oop obj) {\n+  return obj->obj_field(_arrayElementType_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setArrayElementType(oop obj, oop type) {\n+  obj->obj_field_put(_arrayElementType_offset, type);\n+}\n+\n+oop jdk_internal_vm_jni_SubElementSelector::getSubElementType(oop obj) {\n+  return obj->obj_field(_subElementType_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setSubElementType(oop obj, oop type) {\n+  obj->obj_field_put(_subElementType_offset, type);\n+}\n+\n+int jdk_internal_vm_jni_SubElementSelector::getOffset(oop obj) {\n+  return obj->int_field(_offset_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setOffset(oop obj, int offset) {\n+  obj->int_field_put(_offset_offset, offset);\n+}\n+\n+bool jdk_internal_vm_jni_SubElementSelector::getIsInlined(oop obj) {\n+  return obj->bool_field(_isInlined_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setIsInlined(oop obj, bool b) {\n+  obj->bool_field_put(_isInlined_offset, b);\n+}\n+\n+bool jdk_internal_vm_jni_SubElementSelector::getIsInlineType(oop obj) {\n+  return obj->bool_field(_isInlineType_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setIsInlineType(oop obj, bool b) {\n+  obj->bool_field_put(_isInlineType_offset, b);\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":100,"deletions":7,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+  f(jdk_internal_vm_jni_SubElementSelector) \\\n@@ -309,0 +310,1 @@\n+  static int component_mirror_offset()     { CHECK_INIT(_component_mirror_offset); }\n@@ -325,2 +327,0 @@\n-  static int component_mirror_offset() { return _component_mirror_offset; }\n-\n@@ -1146,1 +1146,1 @@\n-    MN_IS_CONSTRUCTOR        = 0x00020000, \/\/ constructor\n+    MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, \/\/ constructor\n@@ -1693,1 +1693,0 @@\n-\n@@ -1709,0 +1708,25 @@\n+class jdk_internal_vm_jni_SubElementSelector : AllStatic {\n+ private:\n+  static int _arrayElementType_offset;\n+  static int _subElementType_offset;\n+  static int _offset_offset;\n+  static int _isInlined_offset;\n+  static int _isInlineType_offset;\n+ public:\n+  static Symbol* symbol();\n+  static void compute_offsets();\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+\n+  static oop getArrayElementType(oop obj);\n+  static void setArrayElementType(oop obj, oop type);\n+  static oop getSubElementType(oop obj);\n+  static void setSubElementType(oop obj, oop type);\n+  static int getOffset(oop obj);\n+  static void setOffset(oop obj, int offset);\n+  static bool getIsInlined(oop obj);\n+  static void setIsInlined(oop obj, bool b);\n+  static bool getIsInlineType(oop obj);\n+  static void setIsInlineType(oop obj, bool b);\n+};\n+\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":28,"deletions":4,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+class AllFieldStream;\n@@ -105,0 +106,1 @@\n+  do_klass(IdentityObject_klass,                        java_lang_IdentityObject                              ) \\\n@@ -177,0 +179,1 @@\n+  do_klass(ValueBootstrapMethods_klass,                 java_lang_invoke_ValueBootstrapMethods                ) \\\n@@ -226,0 +229,1 @@\n+  do_klass(jdk_internal_vm_jni_SubElementSelector_klass, jdk_internal_vm_jni_SubElementSelector               ) \\\n@@ -286,0 +290,6 @@\n+  static Klass* resolve_inline_type_field_or_fail(AllFieldStream* fs,\n+                                                  Handle class_loader,\n+                                                  Handle protection_domain,\n+                                                  bool throw_error,\n+                                                  TRAPS);\n+\n@@ -375,0 +385,1 @@\n+  static InstanceKlass* check_klass_ValhallaClasses(InstanceKlass* k) { return k; }\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -552,0 +552,2 @@\n+  do_signature(getValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;)Ljava\/lang\/Object;\")                   \\\n+  do_signature(putValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;Ljava\/lang\/Object;)V\")                  \\\n@@ -562,0 +564,3 @@\n+  do_name(getValue_name,\"getValue\")             do_name(putValue_name,\"putValue\")                                       \\\n+  do_name(makePrivateBuffer_name,\"makePrivateBuffer\")                                                                   \\\n+  do_name(finishPrivateBuffer_name,\"finishPrivateBuffer\")                                                               \\\n@@ -572,0 +577,1 @@\n+  do_intrinsic(_getValue,           jdk_internal_misc_Unsafe,     getValue_name, getValue_signature,             F_RN)  \\\n@@ -581,0 +587,4 @@\n+  do_intrinsic(_putValue,           jdk_internal_misc_Unsafe,     putValue_name, putValue_signature,             F_RN)  \\\n+                                                                                                                        \\\n+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \\\n+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+  template(java_lang_IdentityObject,                  \"java\/lang\/IdentityObject\")                 \\\n@@ -67,0 +68,1 @@\n+  template(java_lang_NonTearable,                     \"java\/lang\/NonTearable\")                    \\\n@@ -479,0 +481,2 @@\n+  template(default_value_name,                        \".default\")                                 \\\n+  template(empty_marker_name,                         \".empty\")                                   \\\n@@ -551,0 +555,1 @@\n+  template(object_object_boolean_signature,           \"(Ljava\/lang\/Object;Ljava\/lang\/Object;)Z\") \\\n@@ -698,0 +703,5 @@\n+  template(java_lang_invoke_ValueBootstrapMethods, \"java\/lang\/invoke\/ValueBootstrapMethods\")                      \\\n+  template(isSubstitutable_name,                   \"isSubstitutable\")                                             \\\n+  template(inlineObjectHashCode_name,              \"inlineObjectHashCode\")                                        \\\n+                                                                                                                  \\\n+  template(jdk_internal_vm_jni_SubElementSelector, \"jdk\/internal\/vm\/jni\/SubElementSelector\")                      \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -361,7 +361,0 @@\n-    SimpleScopeDesc ssd(this, pc);\n-    if (ssd.is_optimized_linkToNative()) return; \/\/ call was replaced\n-    Bytecode_invoke call(methodHandle(Thread::current(), ssd.method()), ssd.bci());\n-    bool has_receiver = call.has_receiver();\n-    bool has_appendix = call.has_appendix();\n-    Symbol* signature = call.signature();\n-\n@@ -371,0 +364,3 @@\n+    bool has_receiver = false;\n+    bool has_appendix = false;\n+    Symbol* signature = NULL;\n@@ -375,0 +371,18 @@\n+\n+      \/\/ If inline types are passed as fields, use the extended signature\n+      \/\/ which contains the types of all (oop) fields of the inline type.\n+      if (this->is_compiled_by_c2() && callee->has_scalarized_args()) {\n+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();\n+        assert(sig != NULL, \"sig should never be null\");\n+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);\n+        has_receiver = false; \/\/ The extended signature contains the receiver type\n+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);\n+        return;\n+      }\n+    } else {\n+      SimpleScopeDesc ssd(this, pc);\n+      if (ssd.is_optimized_linkToNative()) return; \/\/ call was replaced\n+      Bytecode_invoke call(methodHandle(Thread::current(), ssd.method()), ssd.bci());\n+      has_receiver = call.has_receiver();\n+      has_appendix = call.has_appendix();\n+      signature = call.signature();\n","filename":"src\/hotspot\/share\/code\/compiledMethod.cpp","additions":21,"deletions":7,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -292,0 +292,1 @@\n+                                              bool        return_vt,\n@@ -311,0 +312,1 @@\n+  last_pd->set_return_vt(return_vt);\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -109,0 +109,1 @@\n+                      bool        return_vt  = false,\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -643,0 +643,6 @@\n+\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\"); \/\/ for the next 3 fields\n+    _inline_entry_point       = _entry_point;\n+    _verified_inline_entry_point = _verified_entry_point;\n+    _verified_inline_ro_entry_point = _verified_entry_point;\n+\n@@ -816,0 +822,3 @@\n+    _inline_entry_point       = code_begin()         + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point = code_begin()      + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin()   + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n@@ -935,0 +944,3 @@\n+static nmethod* _nmethod_to_print = NULL;\n+static const CompiledEntrySignature* _nmethod_to_print_ces = NULL;\n+\n@@ -936,0 +948,6 @@\n+  ResourceMark rm;\n+  CompiledEntrySignature ces(method());\n+  ces.compute_calling_conventions();\n+  \/\/ ces.compute_calling_conventions() needs to grab the ProtectionDomainSet_lock, so we\n+  \/\/ can't do that (inside nmethod::print_entry_parameters) while holding the ttyLocker.\n+  \/\/ Hence we have do compute it here and pass via a global. Yuck.\n@@ -937,0 +955,3 @@\n+  assert(_nmethod_to_print == NULL && _nmethod_to_print_ces == NULL, \"no nesting\");\n+  _nmethod_to_print = this;\n+  _nmethod_to_print_ces = &ces;\n@@ -1016,0 +1037,3 @@\n+\n+  _nmethod_to_print = NULL;\n+  _nmethod_to_print_ces = NULL;\n@@ -3096,0 +3120,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -3097,0 +3122,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -3106,0 +3133,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -3108,4 +3145,13 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != NULL) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != NULL) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n+      }\n@@ -3115,6 +3161,34 @@\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != NULL) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n+  if (_nmethod_to_print != this) {\n+    return;\n+  }\n+  Method* m = method();\n+  if (m == NULL || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN4(entry_point(), verified_entry_point(), verified_inline_entry_point(), verified_inline_ro_entry_point());\n+  low = MIN2(low, inline_entry_point());\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  {\n+    const CompiledEntrySignature* ces = _nmethod_to_print_ces;\n+    const GrowableArray<SigEntry>* sig_cc;\n+    const VMRegPair* regs;\n+    if (block_begin == verified_entry_point()) {\n+      sig_cc = &ces->sig_cc();\n+      regs = ces->regs_cc();\n+    } else if (block_begin == verified_inline_entry_point()) {\n+      sig_cc = &ces->sig();\n+      regs = ces->regs();\n+    } else if (block_begin == verified_inline_ro_entry_point()) {\n+      sig_cc = &ces->sig_cc_ro();\n+      regs = ces->regs_cc_ro();\n+    } else {\n+      return;\n@@ -3122,19 +3196,12 @@\n-    if (m != NULL && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+\n+    ResourceMark rm;\n+    int sizeargs = 0;\n+    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, 256);\n+    TempNewSymbol sig = SigEntry::create_symbol(sig_cc);\n+    for (SignatureStream ss(sig); !ss.at_return_type(); ss.next()) {\n+      BasicType t = ss.type();\n+      sig_bt[sizeargs++] = t;\n+      if (type2size[t] == 2) {\n+        sig_bt[sizeargs++] = T_VOID;\n+      } else {\n+        assert(type2size[t] == 1, \"size is 1 or 2\");\n@@ -3142,45 +3209,29 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      intptr_t out_preserve = SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n+    }\n+    bool has_this = !m->is_static();\n+    if (ces->has_inline_recv() && block_begin == verified_entry_point()) {\n+      \/\/ <this> argument is scalarized for verified_entry_point()\n+      has_this = false;\n+    }\n+    const char* spname = \"sp\"; \/\/ make arch-specific?\n+    int stack_slot_offset = this->frame_size() * wordSize;\n+    int tab1 = 14, tab2 = 24;\n+    int sig_index = 0;\n+    int arg_index = has_this ? -1 : 0;\n+    bool did_old_sp = false;\n+    for (SignatureStream ss(sig); !ss.at_return_type(); ) {\n+      bool at_this = (arg_index == -1);\n+      bool at_old_sp = false;\n+      BasicType t = ss.type();\n+      assert(t == sig_bt[sig_index], \"sigs in sync\");\n+      if (at_this) {\n+        stream->print(\"  # this: \");\n+      } else {\n+        stream->print(\"  # parm%d: \", arg_index);\n+      }\n+      stream->move_to(tab1);\n+      VMReg fst = regs[sig_index].first();\n+      VMReg snd = regs[sig_index].second();\n+      if (fst->is_reg()) {\n+        stream->print(\"%s\", fst->name());\n+        if (snd->is_valid())  {\n+          stream->print(\":%s\", snd->name());\n@@ -3188,3 +3239,18 @@\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n+      } else if (fst->is_stack()) {\n+        int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+        if (offset == stack_slot_offset)  at_old_sp = true;\n+        stream->print(\"[%s+0x%x]\", spname, offset);\n+      } else {\n+        stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+      }\n+      stream->print(\" \");\n+      stream->move_to(tab2);\n+      stream->print(\"= \");\n+      if (at_this) {\n+        m->method_holder()->print_value_on(stream);\n+      } else {\n+        bool did_name = false;\n+        if (ss.is_reference()) {\n+          Symbol* name = ss.as_symbol();\n+          name->print_value_on(stream);\n+          did_name = true;\n@@ -3192,4 +3258,2 @@\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+        if (!did_name)\n+          stream->print(\"%s\", type2name(t));\n@@ -3197,4 +3261,1 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+      if (at_old_sp) {\n@@ -3202,1 +3263,1 @@\n-        stream->cr();\n+        did_old_sp = true;\n@@ -3204,0 +3265,11 @@\n+      stream->cr();\n+      sig_index += type2size[t];\n+      arg_index += 1;\n+      ss.next();\n+    }\n+    if (!did_old_sp) {\n+      stream->print(\"  # \");\n+      stream->move_to(tab1);\n+      stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+      stream->print(\"  (%s of caller)\", spname);\n+      stream->cr();\n@@ -3328,1 +3400,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_vt=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_vt());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":159,"deletions":87,"binary":false,"changes":246,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"compiler\/compilerDefinitions.hpp\"\n@@ -196,0 +197,3 @@\n+  address _inline_entry_point;               \/\/ inline type entry point (unpack all inline type args) with class check\n+  address _verified_inline_entry_point;      \/\/ inline type entry point (unpack all inline type args) without class check\n+  address _verified_inline_ro_entry_point;   \/\/ inline type entry point (unpack receiver only) without class check\n@@ -456,2 +460,5 @@\n-  address entry_point() const                     { return _entry_point;             } \/\/ normal entry point\n-  address verified_entry_point() const            { return _verified_entry_point;    } \/\/ if klass is correct\n+  address entry_point() const                     { return _entry_point;             }        \/\/ normal entry point\n+  address verified_entry_point() const            { return _verified_entry_point;    }        \/\/ normal entry point without class check\n+  address inline_entry_point() const              { return _inline_entry_point; }             \/\/ inline type entry point (unpack all inline type args)\n+  address verified_inline_entry_point() const     { return _verified_inline_entry_point; }    \/\/ inline type entry point (unpack all inline type args) without class check\n+  address verified_inline_ro_entry_point() const  { return _verified_inline_ro_entry_point; } \/\/ inline type entry point (only unpack receiver) without class check\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -48,1 +48,2 @@\n-    PCDESC_is_optimized_linkToNative = 1 << 6\n+    PCDESC_is_optimized_linkToNative = 1 << 6,\n+    PCDESC_return_vt                 = 1 << 7\n@@ -98,0 +99,2 @@\n+  bool     return_vt()                     const { return (_flags & PCDESC_return_vt) != 0;     }\n+  void set_return_vt(bool z)                     { set_flag(PCDESC_return_vt, z); }\n","filename":"src\/hotspot\/share\/code\/pcDesc.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+  bool return_vt()        const { return _return_vt; }\n@@ -111,0 +112,1 @@\n+  bool          _return_vt;\n@@ -114,1 +116,0 @@\n-\n","filename":"src\/hotspot\/share\/code\/scopeDesc.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -104,1 +104,1 @@\n-    number_of_result_handlers = 10                              \/\/ number of result handlers for native calls\n+    number_of_result_handlers = 11                              \/\/ number of result handlers for native calls\n","filename":"src\/hotspot\/share\/interpreter\/abstractInterpreter.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -53,1 +53,2 @@\n-  T_OBJECT\n+  T_OBJECT ,\n+  T_INLINE_TYPE\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreterGenerator.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1196,1 +1196,1 @@\n-  _debug_recorder->describe_scope(pc_offset, method, NULL, bci, reexecute, throw_exception, is_mh_invoke, is_opt_native, return_oop,\n+  _debug_recorder->describe_scope(pc_offset, method, NULL, bci, reexecute, throw_exception, is_mh_invoke, is_opt_native, return_oop, false,\n@@ -1363,0 +1363,2 @@\n+        _offsets.set_value(CodeOffsets::Verified_Inline_Entry, pc_offset);\n+        _offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, pc_offset);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -53,0 +53,2 @@\n+  address _c2i_inline_ro_entry_trampoline;\n+  address _c2i_inline_entry_trampoline;\n@@ -56,0 +58,2 @@\n+  address c2i_inline_ro_entry_trampoline() { return _c2i_inline_ro_entry_trampoline; }\n+  address c2i_inline_entry_trampoline() { return _c2i_inline_entry_trampoline; }\n@@ -58,0 +62,2 @@\n+  void set_c2i_inline_ro_entry_trampoline(address addr) { _c2i_inline_ro_entry_trampoline = addr; }\n+  void set_c2i_inline_entry_trampoline(address addr) { _c2i_inline_entry_trampoline = addr; }\n@@ -346,1 +352,0 @@\n-    assert(type == _method_entry_ref, \"only special type allowed for now\");\n@@ -349,1 +354,1 @@\n-    _builder->add_special_ref(type, src_obj, field_offset);\n+    _builder->add_special_ref(type, src_obj, field_offset, ref->size() * BytesPerWord);\n@@ -393,4 +398,0 @@\n-void ArchiveBuilder::add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset) {\n-  _special_refs->append(SpecialRefInfo(type, src_obj, field_offset));\n-}\n-\n@@ -542,2 +543,18 @@\n-    assert(s.type() == MetaspaceClosure::_method_entry_ref, \"only special type allowed for now\");\n-    assert(*src_p == *dst_p, \"must be a copy\");\n+\n+    MetaspaceClosure::assert_valid(s.type());\n+    switch (s.type()) {\n+    case MetaspaceClosure::_method_entry_ref:\n+      assert(*src_p == *dst_p, \"must be a copy\");\n+      break;\n+    case MetaspaceClosure::_internal_pointer_ref:\n+      {\n+        \/\/ *src_p points to a location inside src_obj. Let's make *dst_p point to\n+        \/\/ the same location inside dst_obj.\n+        size_t off = pointer_delta(*((address*)src_p), src_obj, sizeof(u1));\n+        assert(off < s.src_obj_size_in_bytes(), \"must point to internal address\");\n+        *((address*)dst_p) = dst_obj + off;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n@@ -835,0 +852,4 @@\n+        info->set_c2i_inline_ro_entry_trampoline(\n+         (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size()));\n+        info->set_c2i_inline_entry_trampoline(\n+         (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size()));\n@@ -858,1 +879,1 @@\n-    align_up(SharedRuntime::trampoline_size(), BytesPerWord) +\n+    align_up(SharedRuntime::trampoline_size(), BytesPerWord) * 3 +\n@@ -904,0 +925,2 @@\n+        m->set_from_compiled_inline_ro_entry(info->c2i_inline_ro_entry_trampoline());\n+        m->set_from_compiled_inline_entry(info->c2i_inline_entry_trampoline());\n","filename":"src\/hotspot\/share\/memory\/archiveBuilder.cpp","additions":32,"deletions":9,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -57,0 +57,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/memory\/metaspaceShared.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -70,0 +70,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -162,0 +163,2 @@\n+bool InstanceKlass::field_is_inline_type(int index) const { return Signature::basic_type(field(index)->signature(constants())) == T_INLINE_TYPE; }\n+\n@@ -480,1 +483,3 @@\n-                                       should_store_fingerprint(is_hidden_or_anonymous));\n+                                       should_store_fingerprint(is_hidden_or_anonymous),\n+                                       parser.has_inline_fields() ? parser.java_fields_count() : 0,\n+                                       parser.is_inline_type());\n@@ -494,2 +499,1 @@\n-    }\n-    else if (is_class_loader(class_name, parser)) {\n+    } else if (is_class_loader(class_name, parser)) {\n@@ -498,0 +502,3 @@\n+    } else if (parser.is_inline_type()) {\n+      \/\/ inline type\n+      ik = new (loader_data, size, THREAD) InlineKlass(parser);\n@@ -513,0 +520,7 @@\n+#ifdef ASSERT\n+  assert(ik->size() == size, \"\");\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -516,0 +530,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = NULL;\n+  address end = NULL;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -550,1 +587,3 @@\n-  _init_thread(NULL)\n+  _init_thread(NULL),\n+  _inline_type_field_klasses(NULL),\n+  _adr_inlineklass_fixed_block(NULL)\n@@ -559,0 +598,4 @@\n+    if (parser.has_inline_fields()) {\n+      set_has_inline_type_fields();\n+    }\n+    _java_fields_count = parser.java_fields_count();\n@@ -560,3 +603,3 @@\n-  assert(NULL == _methods, \"underlying memory not zeroed?\");\n-  assert(is_instance_klass(), \"is layout incorrect?\");\n-  assert(size_helper() == parser.layout_size(), \"incorrect size_helper?\");\n+    assert(NULL == _methods, \"underlying memory not zeroed?\");\n+    assert(is_instance_klass(), \"is layout incorrect?\");\n+    assert(size_helper() == parser.layout_size(), \"incorrect size_helper?\");\n@@ -569,0 +612,3 @@\n+  if (has_inline_type_fields()) {\n+    _inline_type_field_klasses = (const Klass**) adr_inline_type_field_klasses();\n+  }\n@@ -598,1 +644,2 @@\n-    if (ti != sti && ti != NULL && !ti->is_shared()) {\n+    if (ti != sti && ti != NULL && !ti->is_shared() &&\n+        ti != Universe::the_single_IdentityObject_klass_array()) {\n@@ -605,1 +652,2 @@\n-      local_interfaces != NULL && !local_interfaces->is_shared()) {\n+      local_interfaces != NULL && !local_interfaces->is_shared() &&\n+      local_interfaces != Universe::the_single_IdentityObject_klass_array()) {\n@@ -932,0 +980,56 @@\n+\n+  \/\/ If a class declares a method that uses an inline class as an argument\n+  \/\/ type or return inline type, this inline class must be loaded during the\n+  \/\/ linking of this class because size and properties of the inline class\n+  \/\/ must be known in order to be able to perform inline type optimizations.\n+  \/\/ The implementation below is an approximation of this rule, the code\n+  \/\/ iterates over all methods of the current class (including overridden\n+  \/\/ methods), not only the methods declared by this class. This\n+  \/\/ approximation makes the code simpler, and doesn't change the semantic\n+  \/\/ because classes declaring methods overridden by the current class are\n+  \/\/ linked (and have performed their own pre-loading) before the linking\n+  \/\/ of the current class.\n+\n+\n+  \/\/ Note:\n+  \/\/ Inline class types are loaded during\n+  \/\/ the loading phase (see ClassFileParser::post_process_parsed_stream()).\n+  \/\/ Inline class types used as element types for array creation\n+  \/\/ are not pre-loaded. Their loading is triggered by either anewarray\n+  \/\/ or multianewarray bytecodes.\n+\n+  \/\/ Could it be possible to do the following processing only if the\n+  \/\/ class uses inline types?\n+  {\n+    ResourceMark rm(THREAD);\n+    for (int i = 0; i < methods()->length(); i++) {\n+      Method* m = methods()->at(i);\n+      for (SignatureStream ss(m->signature()); !ss.is_done(); ss.next()) {\n+        if (ss.is_reference()) {\n+          if (ss.is_array()) {\n+            ss.skip_array_prefix();\n+          }\n+          if (ss.type() == T_INLINE_TYPE) {\n+            Symbol* symb = ss.as_symbol();\n+\n+            oop loader = class_loader();\n+            oop protection_domain = this->protection_domain();\n+            Klass* klass = SystemDictionary::resolve_or_fail(symb,\n+                                                             Handle(THREAD, loader), Handle(THREAD, protection_domain), true,\n+                                                             CHECK_false);\n+            if (klass == NULL) {\n+              THROW_(vmSymbols::java_lang_LinkageError(), false);\n+            }\n+            if (!klass->is_inline_klass()) {\n+              Exceptions::fthrow(\n+                THREAD_AND_LOCATION,\n+                vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                \"class %s is not an inline type\",\n+                klass->external_name());\n+            }\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1003,0 +1107,1 @@\n+\n@@ -1153,0 +1258,29 @@\n+  \/\/ Step 8\n+  \/\/ Initialize classes of inline fields\n+  {\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        Klass* klass = get_inline_type_field_klass_or_null(fs.index());\n+        if (fs.access_flags().is_static() && klass == NULL) {\n+          klass = SystemDictionary::resolve_or_fail(field_signature(fs.index())->fundamental_name(THREAD),\n+              Handle(THREAD, class_loader()),\n+              Handle(THREAD, protection_domain()),\n+              true, CHECK);\n+          if (klass == NULL) {\n+            THROW(vmSymbols::java_lang_NoClassDefFoundError());\n+          }\n+          if (!klass->is_inline_klass()) {\n+            THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+          }\n+          set_inline_type_field_klass(fs.index(), klass);\n+        }\n+        InstanceKlass::cast(klass)->initialize(CHECK);\n+        if (fs.access_flags().is_static()) {\n+          if (java_mirror()->obj_field(fs.offset()) == NULL) {\n+            java_mirror()->obj_field_put(fs.offset(), InlineKlass::cast(klass)->default_value());\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1157,1 +1291,1 @@\n-  \/\/ Step 8\n+  \/\/ Step 9\n@@ -1179,1 +1313,1 @@\n-  \/\/ Step 9\n+  \/\/ Step 10\n@@ -1187,1 +1321,1 @@\n-    \/\/ Step 10 and 11\n+    \/\/ Step 11 and 12\n@@ -1459,1 +1593,1 @@\n-  ObjArrayKlass* oak = array_klasses();\n+  ArrayKlass* oak = array_klasses();\n@@ -1475,1 +1609,1 @@\n-  if (clinit != NULL && clinit->has_valid_initializer_flags()) {\n+  if (clinit != NULL && clinit->is_class_initializer()) {\n@@ -1513,1 +1647,1 @@\n-    MutexLocker x(OopMapCacheAlloc_lock);\n+    MutexLocker x(OopMapCacheAlloc_lock,  Mutex::_no_safepoint_check_flag);\n@@ -1525,5 +1659,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n-\n@@ -1600,0 +1729,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->first_field_offset() && offset < (vk->first_field_offset() + vk->get_exact_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -1987,0 +2125,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited, not even as a static factory\n+    }\n@@ -2495,0 +2636,6 @@\n+\n+  if (has_inline_type_fields()) {\n+    for (int i = 0; i < java_fields_count(); i++) {\n+      it->push(&((Klass**)adr_inline_type_field_klasses())[i]);\n+    }\n+  }\n@@ -2530,0 +2677,8 @@\n+  if (has_inline_type_fields()) {\n+    for (AllFieldStream fs(fields(), constants()); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        reset_inline_type_field_klass(fs.index());\n+      }\n+    }\n+  }\n+\n@@ -2569,0 +2724,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2600,1 +2759,1 @@\n-  if (UseBiasedLocking && BiasedLocking::enabled()) {\n+  if (UseBiasedLocking && BiasedLocking::enabled() && !is_inline_klass()) {\n@@ -2765,1 +2924,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -2767,1 +2926,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = is_inline_klass() ? JVM_SIGNATURE_INLINE_TYPE : JVM_SIGNATURE_CLASS;\n@@ -3336,1 +3495,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3340,0 +3502,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3343,0 +3510,6 @@\n+    } else if (self != NULL && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3349,1 +3522,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(NULL, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == NULL) { st->print_cr(\"NULL\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == NULL) { st->print_cr(\"NULL\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3359,0 +3553,1 @@\n+  st->print(BULLET\"misc flags:        0x%x\", _misc_flags);                        st->cr();\n@@ -3385,15 +3580,3 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);                  st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);      st->cr();\n-  if (Verbose && default_methods() != NULL) {\n-    Array<Method*>* method_array = default_methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n+  st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3401,1 +3584,1 @@\n-    st->print(BULLET\"default vtable indices:   \"); default_vtable_indices()->print_value_on(st);       st->cr();\n+    st->print(BULLET\"default vtable indices:   \"); print_array_on(st, default_vtable_indices());\n@@ -3403,2 +3586,2 @@\n-  st->print(BULLET\"local interfaces:  \"); local_interfaces()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"trans. interfaces: \"); transitive_interfaces()->print_value_on(st); st->cr();\n+  st->print(BULLET\"local interfaces:  \"); print_array_on(st, local_interfaces());\n+  st->print(BULLET\"trans. interfaces: \"); print_array_on(st, transitive_interfaces());\n@@ -3461,1 +3644,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(NULL, start_of_itable(), itable_length(), st);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":227,"deletions":44,"binary":false,"changes":271,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -57,0 +58,2 @@\n+\/\/    [EMBEDDED inline_type_field_klasses] only if has_inline_fields() == true\n+\/\/    [EMBEDDED InlineKlassFixedBlock] only if is an InlineKlass instance\n@@ -73,0 +76,1 @@\n+class BufferedInlineTypeBlob;\n@@ -135,0 +139,16 @@\n+class SigEntry;\n+\n+class InlineKlassFixedBlock {\n+  Array<SigEntry>** _extended_sig;\n+  Array<VMRegPair>** _return_regs;\n+  address* _pack_handler;\n+  address* _pack_handler_jobject;\n+  address* _unpack_handler;\n+  int* _default_value_offset;\n+  int _alignment;\n+  int _first_field_offset;\n+  int _exact_size_in_bytes;\n+\n+  friend class InlineKlass;\n+};\n+\n@@ -140,0 +160,1 @@\n+  friend class TemplateTable;\n@@ -157,1 +178,1 @@\n-    fully_initialized,                  \/\/ initialized (successfull final state)\n+    fully_initialized,                  \/\/ initialized (successful final state)\n@@ -173,1 +194,1 @@\n-  ObjArrayKlass* volatile _array_klasses;\n+  ArrayKlass* volatile _array_klasses;\n@@ -240,1 +261,1 @@\n-  \/\/ This can be used to quickly discriminate among the four kinds of\n+  \/\/ This can be used to quickly discriminate among the five kinds of\n@@ -246,0 +267,1 @@\n+  static const unsigned _kind_inline_type  = 4; \/\/ InlineKlass\n@@ -267,1 +289,8 @@\n-    _misc_has_contended_annotations           = 1 << 15  \/\/ has @Contended annotation\n+    _misc_has_contended_annotations           = 1 << 15,  \/\/ has @Contended annotation\n+    _misc_has_inline_type_fields              = 1 << 16, \/\/ has inline fields and related embedded section is not empty\n+    _misc_is_empty_inline_type                = 1 << 17, \/\/ empty inline type (*)\n+    _misc_is_naturally_atomic                 = 1 << 18, \/\/ loaded\/stored in one instruction\n+    _misc_is_declared_atomic                  = 1 << 19, \/\/ implements jl.NonTearable\n+    _misc_invalid_inline_super                = 1 << 20, \/\/ invalid super type for an inline type\n+    _misc_invalid_identity_super              = 1 << 21, \/\/ invalid super type for an identity type\n+    _misc_has_injected_identityObject         = 1 << 22  \/\/ IdentityObject has been injected by the JVM\n@@ -269,0 +298,6 @@\n+\n+  \/\/ (*) An inline type is considered empty if it contains no non-static fields or\n+  \/\/ if it contains only empty inline fields. Note that JITs have a slightly different\n+  \/\/ definition: empty inline fields must be flattened otherwise the container won't\n+  \/\/ be considered empty\n+\n@@ -272,1 +307,1 @@\n-  u2              _misc_flags;           \/\/ There is more space in access_flags for more flags.\n+  u4              _misc_flags;           \/\/ There is more space in access_flags for more flags.\n@@ -324,0 +359,3 @@\n+  const Klass**   _inline_type_field_klasses; \/\/ For \"inline class\" fields, NULL if none present\n+\n+  const InlineKlassFixedBlock* _adr_inlineklass_fixed_block;\n@@ -384,0 +422,65 @@\n+  bool has_inline_type_fields() const          {\n+    return (_misc_flags & _misc_has_inline_type_fields) != 0;\n+  }\n+  void set_has_inline_type_fields()  {\n+    _misc_flags |= _misc_has_inline_type_fields;\n+  }\n+\n+  bool is_empty_inline_type() const {\n+    return (_misc_flags & _misc_is_empty_inline_type) != 0;\n+  }\n+  void set_is_empty_inline_type() {\n+    _misc_flags |= _misc_is_empty_inline_type;\n+  }\n+\n+  \/\/ Note:  The naturally_atomic property only applies to\n+  \/\/ inline classes; it is never true on identity classes.\n+  \/\/ The bit is placed on instanceKlass for convenience.\n+\n+  \/\/ Query if h\/w provides atomic load\/store for instances.\n+  bool is_naturally_atomic() const {\n+    return (_misc_flags & _misc_is_naturally_atomic) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_is_naturally_atomic() {\n+    _misc_flags |= _misc_is_naturally_atomic;\n+  }\n+\n+  \/\/ Query if this class implements jl.NonTearable or was\n+  \/\/ mentioned in the JVM option ForceNonTearable.\n+  \/\/ This bit can occur anywhere, but is only significant\n+  \/\/ for inline classes *and* their super types.\n+  \/\/ It inherits from supers along with NonTearable.\n+  bool is_declared_atomic() const {\n+    return (_misc_flags & _misc_is_declared_atomic) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_is_declared_atomic() {\n+    _misc_flags |= _misc_is_declared_atomic;\n+  }\n+\n+  \/\/ Query if class is an invalid super class for an inline type.\n+  bool invalid_inline_super() const {\n+    return (_misc_flags & _misc_invalid_inline_super) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_invalid_inline_super() {\n+    _misc_flags |= _misc_invalid_inline_super;\n+  }\n+  \/\/ Query if class is an invalid super class for an identity type.\n+  bool invalid_identity_super() const {\n+    return (_misc_flags & _misc_invalid_identity_super) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_invalid_identity_super() {\n+    _misc_flags |= _misc_invalid_identity_super;\n+  }\n+\n+  bool has_injected_identityObject() const {\n+    return (_misc_flags & _misc_has_injected_identityObject);\n+  }\n+\n+  void set_has_injected_identityObject() {\n+    _misc_flags |= _misc_has_injected_identityObject;\n+  }\n+\n@@ -399,4 +502,4 @@\n-  ObjArrayKlass* array_klasses() const     { return _array_klasses; }\n-  inline ObjArrayKlass* array_klasses_acquire() const; \/\/ load with acquire semantics\n-  void set_array_klasses(ObjArrayKlass* k) { _array_klasses = k; }\n-  inline void release_set_array_klasses(ObjArrayKlass* k); \/\/ store with release semantics\n+  ArrayKlass* array_klasses() const     { return _array_klasses; }\n+  inline ArrayKlass* array_klasses_acquire() const; \/\/ load with acquire semantics\n+  void set_array_klasses(ArrayKlass* k) { _array_klasses = k; }\n+  inline void release_set_array_klasses(ArrayKlass* k); \/\/ store with release semantics\n@@ -446,0 +549,2 @@\n+  bool    field_is_inlined(int index) const { return field(index)->is_inlined(); }\n+  bool    field_is_inline_type(int index) const;\n@@ -573,0 +678,4 @@\n+  static ByteSize kind_offset() { return in_ByteSize(offset_of(InstanceKlass, _kind)); }\n+  static ByteSize misc_flags_offset() { return in_ByteSize(offset_of(InstanceKlass, _misc_flags)); }\n+  static u4 misc_flags_is_empty_inline_type() { return _misc_is_empty_inline_type; }\n+\n@@ -772,0 +881,1 @@\n+\n@@ -773,1 +883,1 @@\n-    return ((_misc_flags & _misc_is_being_redefined) != 0);\n+    return (_misc_flags & _misc_is_being_redefined);\n@@ -858,0 +968,1 @@\n+  bool is_inline_type_klass()           const { return is_kind(_kind_inline_type); }\n@@ -1027,0 +1138,3 @@\n+  static ByteSize inline_type_field_klasses_offset() { return in_ByteSize(offset_of(InstanceKlass, _inline_type_field_klasses)); }\n+  static ByteSize adr_inlineklass_fixed_block_offset() { return in_ByteSize(offset_of(InstanceKlass, _adr_inlineklass_fixed_block)); }\n+\n@@ -1061,2 +1175,2 @@\n-  void array_klasses_do(void f(Klass* k));\n-  void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);\n+  virtual void array_klasses_do(void f(Klass* k));\n+  virtual void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);\n@@ -1083,1 +1197,2 @@\n-                  bool is_interface, bool is_unsafe_anonymous, bool has_stored_fingerprint) {\n+                  bool is_interface, bool is_unsafe_anonymous, bool has_stored_fingerprint,\n+                  int java_fields, bool is_inline_type) {\n@@ -1090,1 +1205,3 @@\n-           (has_stored_fingerprint ? (int)sizeof(uint64_t*)\/wordSize : 0));\n+           (has_stored_fingerprint ? (int)sizeof(uint64_t*)\/wordSize : 0) +\n+           (java_fields * (int)sizeof(Klass*)\/wordSize) +\n+           (is_inline_type ? (int)sizeof(InlineKlassFixedBlock) : 0));\n@@ -1097,1 +1214,3 @@\n-                                               has_stored_fingerprint());\n+                                               has_stored_fingerprint(),\n+                                               has_inline_type_fields() ? java_fields_count() : 0,\n+                                               is_inline_klass());\n@@ -1107,0 +1226,2 @@\n+  bool bounds_check(address addr, bool edge_ok = false, intptr_t size_in_bytes = -1) const PRODUCT_RETURN0;\n+\n@@ -1155,0 +1276,54 @@\n+  address adr_inline_type_field_klasses() const {\n+    if (has_inline_type_fields()) {\n+      address adr_fing = adr_fingerprint();\n+      if (adr_fing != NULL) {\n+        return adr_fingerprint() + sizeof(u8);\n+      }\n+\n+      InstanceKlass** adr_host = adr_unsafe_anonymous_host();\n+      if (adr_host != NULL) {\n+        return (address)(adr_host + 1);\n+      }\n+\n+      Klass* volatile* adr_impl = adr_implementor();\n+      if (adr_impl != NULL) {\n+        return (address)(adr_impl + 1);\n+      }\n+\n+      return (address)end_of_nonstatic_oop_maps();\n+    } else {\n+      return NULL;\n+    }\n+  }\n+\n+  Klass* get_inline_type_field_klass(int idx) const {\n+    assert(has_inline_type_fields(), \"Sanity checking\");\n+    assert(idx < java_fields_count(), \"IOOB\");\n+    Klass* k = ((Klass**)adr_inline_type_field_klasses())[idx];\n+    assert(k != NULL, \"Should always be set before being read\");\n+    assert(k->is_inline_klass(), \"Must be an inline type\");\n+    return k;\n+  }\n+\n+  Klass* get_inline_type_field_klass_or_null(int idx) const {\n+    assert(has_inline_type_fields(), \"Sanity checking\");\n+    assert(idx < java_fields_count(), \"IOOB\");\n+    Klass* k = ((Klass**)adr_inline_type_field_klasses())[idx];\n+    assert(k == NULL || k->is_inline_klass(), \"Must be an inline type\");\n+    return k;\n+  }\n+\n+  void set_inline_type_field_klass(int idx, Klass* k) {\n+    assert(has_inline_type_fields(), \"Sanity checking\");\n+    assert(idx < java_fields_count(), \"IOOB\");\n+    assert(k != NULL, \"Should not be set to NULL\");\n+    assert(((Klass**)adr_inline_type_field_klasses())[idx] == NULL, \"Should not be set twice\");\n+    ((Klass**)adr_inline_type_field_klasses())[idx] = k;\n+  }\n+\n+  void reset_inline_type_field_klass(int idx) {\n+    assert(has_inline_type_fields(), \"Sanity checking\");\n+    assert(idx < java_fields_count(), \"IOOB\");\n+    ((Klass**)adr_inline_type_field_klasses())[idx] = NULL;\n+  }\n+\n@@ -1156,1 +1331,1 @@\n-  int size_helper() const {\n+  virtual int size_helper() const {\n@@ -1293,1 +1468,1 @@\n-\n+protected:\n@@ -1295,1 +1470,1 @@\n-  Klass* array_klass_impl(bool or_null, int n, TRAPS);\n+  virtual Klass* array_klass_impl(bool or_null, int n, TRAPS);\n@@ -1298,1 +1473,3 @@\n-  Klass* array_klass_impl(bool or_null, TRAPS);\n+  virtual Klass* array_klass_impl(bool or_null, TRAPS);\n+\n+private:\n@@ -1328,1 +1505,1 @@\n-  void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n+  virtual void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":197,"deletions":20,"binary":false,"changes":217,"status":"modified"},{"patch":"@@ -1150,3 +1150,4 @@\n-  if (m->is_static())           return false;   \/\/ e.g., Stream.empty\n-  if (m->is_initializer())      return false;   \/\/ <init> or <clinit>\n-  if (m->is_private())          return false;   \/\/ uses direct call\n+  if (m->is_static())             return false;   \/\/ e.g., Stream.empty\n+  if (m->is_private())            return false;   \/\/ uses direct call\n+  if (m->is_object_constructor()) return false;   \/\/ <init>(...)V\n+  if (m->is_class_initializer())  return false;   \/\/ <clinit>()V\n@@ -1385,0 +1386,12 @@\n+int count_interface_methods_needing_itable_index(Array<Method*>* methods) {\n+  int method_count = 0;\n+  if (methods->length() > 0) {\n+    for (int i = methods->length(); --i >= 0; ) {\n+      if (interface_method_needs_itable_index(methods->at(i))) {\n+        method_count++;\n+      }\n+    }\n+  }\n+  return method_count;\n+}\n+\n@@ -1453,1 +1466,1 @@\n-  \/\/ There's alway an extra itable entry so we can null-terminate it.\n+  \/\/ There's always an extra itable entry so we can null-terminate it.\n","filename":"src\/hotspot\/share\/oops\/klassVtable.cpp","additions":17,"deletions":4,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -292,1 +292,4 @@\n-  int size_offset_table()                { return _size_offset_table; }\n+  InstanceKlass* klass() const          { return _klass; }\n+  int table_offset() const              { return _table_offset; }\n+  int size_offset_table() const         { return _size_offset_table; }\n+  int size_method_table() const         { return _size_method_table; }\n","filename":"src\/hotspot\/share\/oops\/klassVtable.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -118,1 +119,0 @@\n-\n@@ -155,0 +155,5 @@\n+address Method::get_c2i_inline_entry() {\n+  assert(adapter() != NULL, \"must have\");\n+  return adapter()->get_c2i_inline_entry();\n+}\n+\n@@ -160,0 +165,5 @@\n+address Method::get_c2i_unverified_inline_entry() {\n+  assert(adapter() != NULL, \"must have\");\n+  return adapter()->get_c2i_unverified_inline_entry();\n+}\n+\n@@ -352,0 +362,2 @@\n+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_inline_ro_entry);\n+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_inline_entry);\n@@ -596,0 +608,18 @@\n+\/\/ InlineKlass the method is declared to return. This must not\n+\/\/ safepoint as it is called with references live on the stack at\n+\/\/ locations the GC is unaware of.\n+InlineKlass* Method::returned_inline_type(Thread* thread) const {\n+  SignatureStream ss(signature());\n+  while (!ss.at_return_type()) {\n+    ss.next();\n+  }\n+  Handle class_loader(thread, method_holder()->class_loader());\n+  Handle protection_domain(thread, method_holder()->protection_domain());\n+  Klass* k = NULL;\n+  {\n+    NoSafepointVerifier nsv;\n+    k = ss.as_klass(class_loader, protection_domain, SignatureStream::ReturnNull, thread);\n+  }\n+  assert(k != NULL && !thread->has_pending_exception(), \"can't resolve klass\");\n+  return InlineKlass::cast(k);\n+}\n@@ -606,1 +636,1 @@\n-  \/\/   aload_0\n+  \/\/   aload_0, _fast_aload_0, or _nofast_aload_0\n@@ -630,1 +660,2 @@\n-  if (cb[0] != Bytecodes::_aload_0 || cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {\n+  if ((cb[0] != Bytecodes::_aload_0 && cb[0] != Bytecodes::_fast_aload_0 && cb[0] != Bytecodes::_nofast_aload_0) ||\n+       cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {\n@@ -810,7 +841,2 @@\n-bool Method::is_initializer() const {\n-  return is_object_initializer() || is_static_initializer();\n-}\n-\n-bool Method::has_valid_initializer_flags() const {\n-  return (is_static() ||\n-          method_holder()->major_version() < 51);\n+bool Method::is_object_constructor_or_class_initializer() const {\n+  return (is_object_constructor() || is_class_initializer());\n@@ -819,1 +845,1 @@\n-bool Method::is_static_initializer() const {\n+bool Method::is_class_initializer() const {\n@@ -823,2 +849,8 @@\n-  return name() == vmSymbols::class_initializer_name() &&\n-         has_valid_initializer_flags();\n+  return (name() == vmSymbols::class_initializer_name() &&\n+          (is_static() ||\n+           method_holder()->major_version() < 51));\n+}\n+\n+\/\/ A method named <init>, if non-static, is a classic object constructor.\n+bool Method::is_object_constructor() const {\n+   return name() == vmSymbols::object_initializer_name() && !is_static();\n@@ -827,2 +859,3 @@\n-bool Method::is_object_initializer() const {\n-   return name() == vmSymbols::object_initializer_name();\n+\/\/ A static method named <init> is a factory for an inline class.\n+bool Method::is_static_init_factory() const {\n+   return name() == vmSymbols::object_initializer_name() && is_static();\n@@ -886,1 +919,1 @@\n-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {\n+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {\n@@ -902,1 +935,3 @@\n-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;\n+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {\n+      return false;\n+    }\n@@ -1071,0 +1106,2 @@\n+    _from_compiled_inline_entry = NULL;\n+    _from_compiled_inline_ro_entry = NULL;\n@@ -1073,0 +1110,2 @@\n+    _from_compiled_inline_entry = adapter()->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = adapter()->get_c2i_inline_ro_entry();\n@@ -1113,0 +1152,4 @@\n+  assert(*((int*)_from_compiled_inline_ro_entry) == 0,\n+         \"must be NULL during dump time, to be initialized at run time\");\n+  assert(*((int*)_from_compiled_inline_entry) == 0,\n+         \"must be NULL during dump time, to be initialized at run time\");\n@@ -1264,0 +1307,2 @@\n+    assert(mh->_from_compiled_inline_entry != NULL, \"must be\");\n+    assert(mh->_from_compiled_inline_ro_entry != NULL, \"must be\");\n@@ -1267,0 +1312,2 @@\n+    mh->_from_compiled_inline_entry = adapter->get_c2i_inline_entry();\n+    mh->_from_compiled_inline_ro_entry = adapter->get_c2i_inline_ro_entry();\n@@ -1274,0 +1321,12 @@\n+#if 0\n+  \/*\n+   * CDS:TODO --\n+   * \"Q\" classes in the method signature must be resolved during link_method.\n+   * However, at this point we are still inside method_holder()->restore_unshareable_info.\n+   * If we try to resolve method_holder(), or multually dependent classes, it will\n+   * cause deadlock and other ill effects.\n+   *\n+   * For now, lets do method linking inside InstanceKlass::link_class(). Optimization\n+   * may be possible if we know that resolution will never happen.\n+   *\/\n+\n@@ -1280,0 +1339,1 @@\n+#endif\n@@ -1282,1 +1342,1 @@\n-address Method::from_compiled_entry_no_trampoline() const {\n+address Method::from_compiled_entry_no_trampoline(bool caller_is_c1) const {\n@@ -1284,2 +1344,7 @@\n-  if (code) {\n-    return code->verified_entry_point();\n+  if (caller_is_c1) {\n+    \/\/ C1 - inline type arguments are passed as objects\n+    if (code) {\n+      return code->verified_inline_entry_point();\n+    } else {\n+      return adapter()->get_c2i_inline_entry();\n+    }\n@@ -1287,1 +1352,6 @@\n-    return adapter()->get_c2i_entry();\n+    \/\/ C2 - inline type arguments may be passed as fields\n+    if (code) {\n+      return code->verified_entry_point();\n+    } else {\n+      return adapter()->get_c2i_entry();\n+    }\n@@ -1304,0 +1374,12 @@\n+address Method::verified_inline_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_entry != NULL, \"must be set\");\n+  return _from_compiled_inline_entry;\n+}\n+\n+address Method::verified_inline_ro_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_ro_entry != NULL, \"must be set\");\n+  return _from_compiled_inline_ro_entry;\n+}\n+\n@@ -1335,0 +1417,2 @@\n+  mh->_from_compiled_inline_entry = code->verified_inline_entry_point();\n+  mh->_from_compiled_inline_ro_entry = code->verified_inline_ro_entry_point();\n@@ -2369,0 +2453,4 @@\n+#ifdef ASSERT\n+  if (valid_itable_index())\n+    st->print_cr(\" - itable index:      %d\",   itable_index());\n+#endif\n@@ -2446,0 +2534,1 @@\n+  if (WizardMode) access_flags().print_on(st);\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":110,"deletions":21,"binary":false,"changes":131,"status":"modified"},{"patch":"@@ -657,0 +657,6 @@\n+  if (tp->isa_aryptr()) {\n+    \/\/ In the case of a flattened inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return tp->is_aryptr()->add_field_offset_and_offset(txoffset);\n+  }\n@@ -677,0 +683,6 @@\n+  if (p1->isa_aryptr()) {\n+    \/\/ In the case of a flattened inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return p1->is_aryptr()->add_field_offset_and_offset(p2offset);\n+  }\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -116,3 +118,7 @@\n-  Node* src = in(ArrayCopyNode::Src);\n-  const Type* src_type = phase->type(src);\n-\n+    Node* src = in(ArrayCopyNode::Src);\n+    const Type* src_type = phase->type(src);\n+\n+    if (src_type == Type::TOP) {\n+      return -1;\n+    }\n+\n@@ -140,2 +146,2 @@\n-             phase->is_IterGVN() || StressReflectiveCode, \"inconsistent\");\n-\n+             (UseFlatArray && ary_src->elem()->make_oopptr() != NULL && ary_src->elem()->make_oopptr()->can_be_inline_type()) ||\n+             phase->is_IterGVN() || phase->C->inlining_incrementally() || StressReflectiveCode, \"inconsistent\");\n@@ -267,2 +273,6 @@\n-    if (is_reference_type(src_elem))   src_elem  = T_OBJECT;\n-    if (is_reference_type(dest_elem))  dest_elem = T_OBJECT;\n+    if (src_elem == T_ARRAY || (src_elem == T_INLINE_TYPE && ary_src->klass()->is_obj_array_klass())) {\n+      src_elem  = T_OBJECT;\n+    }\n+    if (dest_elem == T_ARRAY || (dest_elem == T_INLINE_TYPE && ary_dest->klass()->is_obj_array_klass())) {\n+      dest_elem = T_OBJECT;\n+    }\n@@ -276,3 +286,4 @@\n-    if (bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), dest_elem, false, BarrierSetC2::Optimization)) {\n-      \/\/ It's an object array copy but we can't emit the card marking\n-      \/\/ that is needed\n+    if (bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), dest_elem, false, BarrierSetC2::Optimization) ||\n+        (src_elem == T_INLINE_TYPE && ary_src->elem()->inline_klass()->contains_oops() &&\n+         bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), T_OBJECT, false, BarrierSetC2::Optimization))) {\n+      \/\/ It's an object array copy but we can't emit the card marking that is needed\n@@ -285,0 +296,4 @@\n+    if (dest_elem == T_INLINE_TYPE) {\n+      ciFlatArrayKlass* vak = ary_src->klass()->as_flat_array_klass();\n+      shift = vak->log2_element_size();\n+    }\n@@ -310,2 +325,4 @@\n-    adr_src  = phase->transform(new AddPNode(base_src, base_src, src_offset));\n-    adr_dest = phase->transform(new AddPNode(base_dest, base_dest, dest_offset));\n+    if (ary_src->elem()->make_oopptr() != NULL &&\n+        ary_src->elem()->make_oopptr()->can_be_inline_type()) {\n+      return false;\n+    }\n@@ -314,1 +331,1 @@\n-    if (is_reference_type(elem)) {\n+    if (elem == T_ARRAY || (elem == T_INLINE_TYPE && ary_src->klass()->is_obj_array_klass())) {\n@@ -319,1 +336,4 @@\n-    if (bs->array_copy_requires_gc_barriers(true, elem, true, BarrierSetC2::Optimization)) {\n+    if (bs->array_copy_requires_gc_barriers(true, elem, true, BarrierSetC2::Optimization) ||\n+        (elem == T_INLINE_TYPE && ary_src->elem()->inline_klass()->contains_oops() &&\n+         bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Optimization))) {\n+      \/\/ It's an object array copy but we can't emit the card marking that is needed\n@@ -323,0 +343,3 @@\n+    adr_src  = phase->transform(new AddPNode(base_src, base_src, src_offset));\n+    adr_dest = phase->transform(new AddPNode(base_dest, base_dest, dest_offset));\n+\n@@ -340,1 +363,1 @@\n-const TypePtr* ArrayCopyNode::get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n) {\n+const TypeAryPtr* ArrayCopyNode::get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n) {\n@@ -345,1 +368,1 @@\n-  return atp->add_offset(Type::OffsetBot);\n+  return atp->add_offset(Type::OffsetBot)->is_aryptr();\n@@ -348,2 +371,2 @@\n-void ArrayCopyNode::array_copy_test_overlap(PhaseGVN *phase, bool can_reshape, bool disjoint_bases, int count, Node*& forward_ctl, Node*& backward_ctl) {\n-  Node* ctl = in(TypeFunc::Control);\n+void ArrayCopyNode::array_copy_test_overlap(GraphKit& kit, bool disjoint_bases, int count, Node*& backward_ctl) {\n+  Node* ctl = kit.control();\n@@ -351,0 +374,1 @@\n+    PhaseGVN& gvn = kit.gvn();\n@@ -354,2 +378,2 @@\n-    Node* cmp = phase->transform(new CmpINode(src_offset, dest_offset));\n-    Node *bol = phase->transform(new BoolNode(cmp, BoolTest::lt));\n+    Node* cmp = gvn.transform(new CmpINode(src_offset, dest_offset));\n+    Node *bol = gvn.transform(new BoolNode(cmp, BoolTest::lt));\n@@ -358,1 +382,6 @@\n-    phase->transform(iff);\n+    gvn.transform(iff);\n+\n+    kit.set_control(gvn.transform(new IfFalseNode(iff)));\n+    backward_ctl = gvn.transform(new IfTrueNode(iff));\n+  }\n+}\n@@ -360,2 +389,33 @@\n-    forward_ctl = phase->transform(new IfFalseNode(iff));\n-    backward_ctl = phase->transform(new IfTrueNode(iff));\n+void ArrayCopyNode::copy(GraphKit& kit,\n+                         const TypeAryPtr* atp_src,\n+                         const TypeAryPtr* atp_dest,\n+                         int i,\n+                         Node* base_src,\n+                         Node* base_dest,\n+                         Node* adr_src,\n+                         Node* adr_dest,\n+                         BasicType copy_type,\n+                         const Type* value_type) {\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  Node* ctl = kit.control();\n+  if (copy_type == T_INLINE_TYPE) {\n+    ciFlatArrayKlass* vak = atp_src->klass()->as_flat_array_klass();\n+    ciInlineKlass* vk = vak->element_klass()->as_inline_klass();\n+    for (int j = 0; j < vk->nof_nonstatic_fields(); j++) {\n+      ciField* field = vk->nonstatic_field_at(j);\n+      int off_in_vt = field->offset() - vk->first_field_offset();\n+      Node* off  = kit.MakeConX(off_in_vt + i * vak->element_byte_size());\n+      ciType* ft = field->type();\n+      BasicType bt = type2field[ft->basic_type()];\n+      assert(!field->is_flattened(), \"flattened field encountered\");\n+      if (bt == T_INLINE_TYPE) {\n+        bt = T_OBJECT;\n+      }\n+      const Type* rt = Type::get_const_type(ft);\n+      const TypePtr* adr_type = atp_src->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+      assert(!bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), bt, false, BarrierSetC2::Optimization), \"GC barriers required\");\n+      Node* next_src = kit.gvn().transform(new AddPNode(base_src, adr_src, off));\n+      Node* next_dest = kit.gvn().transform(new AddPNode(base_dest, adr_dest, off));\n+      Node* v = load(bs, &kit.gvn(), ctl, kit.merged_memory(), next_src, adr_type, rt, bt);\n+      store(bs, &kit.gvn(), ctl, kit.merged_memory(), next_dest, adr_type, v, rt, bt);\n+    }\n@@ -363,1 +423,5 @@\n-    forward_ctl = ctl;\n+    Node* off = kit.MakeConX(type2aelembytes(copy_type) * i);\n+    Node* next_src = kit.gvn().transform(new AddPNode(base_src, adr_src, off));\n+    Node* next_dest = kit.gvn().transform(new AddPNode(base_dest, adr_dest, off));\n+    Node* v = load(bs, &kit.gvn(), ctl, kit.merged_memory(), next_src, atp_src, value_type, copy_type);\n+    store(bs, &kit.gvn(), ctl, kit.merged_memory(), next_dest, atp_dest, v, value_type, copy_type);\n@@ -365,0 +429,1 @@\n+  kit.set_control(ctl);\n@@ -367,16 +432,13 @@\n-Node* ArrayCopyNode::array_copy_forward(PhaseGVN *phase,\n-                                        bool can_reshape,\n-                                        Node*& forward_ctl,\n-                                        Node* mem,\n-                                        const TypePtr* atp_src,\n-                                        const TypePtr* atp_dest,\n-                                        Node* adr_src,\n-                                        Node* base_src,\n-                                        Node* adr_dest,\n-                                        Node* base_dest,\n-                                        BasicType copy_type,\n-                                        const Type* value_type,\n-                                        int count) {\n-  if (!forward_ctl->is_top()) {\n-    \/\/ copy forward\n-    MergeMemNode* mm = MergeMemNode::make(mem);\n+void ArrayCopyNode::array_copy_forward(GraphKit& kit,\n+                                       bool can_reshape,\n+                                       const TypeAryPtr* atp_src,\n+                                       const TypeAryPtr* atp_dest,\n+                                       Node* adr_src,\n+                                       Node* base_src,\n+                                       Node* adr_dest,\n+                                       Node* base_dest,\n+                                       BasicType copy_type,\n+                                       const Type* value_type,\n+                                       int count) {\n+  if (!kit.stopped()) {\n+    \/\/ copy forward\n@@ -385,9 +447,2 @@\n-      BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-      Node* v = load(bs, phase, forward_ctl, mm, adr_src, atp_src, value_type, copy_type);\n-      store(bs, phase, forward_ctl, mm, adr_dest, atp_dest, v, value_type, copy_type);\n-      for (int i = 1; i < count; i++) {\n-        Node* off  = phase->MakeConX(type2aelembytes(copy_type) * i);\n-        Node* next_src = phase->transform(new AddPNode(base_src,adr_src,off));\n-        Node* next_dest = phase->transform(new AddPNode(base_dest,adr_dest,off));\n-        v = load(bs, phase, forward_ctl, mm, next_src, atp_src, value_type, copy_type);\n-        store(bs, phase, forward_ctl, mm, next_dest, atp_dest, v, value_type, copy_type);\n+      for (int i = 0; i < count; i++) {\n+        copy(kit, atp_src, atp_dest, i, base_src, base_dest, adr_src, adr_dest, copy_type, value_type);\n@@ -396,3 +451,4 @@\n-      PhaseIterGVN* igvn = phase->is_IterGVN();\n-      igvn->_worklist.push(adr_src);\n-      igvn->_worklist.push(adr_dest);\n+      PhaseGVN& gvn = kit.gvn();\n+      assert(gvn.is_IterGVN(), \"\");\n+      gvn.record_for_igvn(adr_src);\n+      gvn.record_for_igvn(adr_dest);\n@@ -400,2 +456,0 @@\n-    return mm;\n-  return phase->C->top();\n@@ -405,14 +459,12 @@\n-Node* ArrayCopyNode::array_copy_backward(PhaseGVN *phase,\n-                                         bool can_reshape,\n-                                         Node*& backward_ctl,\n-                                         Node* mem,\n-                                         const TypePtr* atp_src,\n-                                         const TypePtr* atp_dest,\n-                                         Node* adr_src,\n-                                         Node* base_src,\n-                                         Node* adr_dest,\n-                                         Node* base_dest,\n-                                         BasicType copy_type,\n-                                         const Type* value_type,\n-                                         int count) {\n-  if (!backward_ctl->is_top()) {\n+void ArrayCopyNode::array_copy_backward(GraphKit& kit,\n+                                        bool can_reshape,\n+                                        const TypeAryPtr* atp_src,\n+                                        const TypeAryPtr* atp_dest,\n+                                        Node* adr_src,\n+                                        Node* base_src,\n+                                        Node* adr_dest,\n+                                        Node* base_dest,\n+                                        BasicType copy_type,\n+                                        const Type* value_type,\n+                                        int count) {\n+  if (!kit.stopped()) {\n@@ -420,4 +472,1 @@\n-    MergeMemNode* mm = MergeMemNode::make(mem);\n-\n-    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-    assert(copy_type != T_OBJECT || !bs->array_copy_requires_gc_barriers(false, T_OBJECT, false, BarrierSetC2::Optimization), \"only tightly coupled allocations for object arrays\");\n+    PhaseGVN& gvn = kit.gvn();\n@@ -426,6 +475,2 @@\n-      for (int i = count-1; i >= 1; i--) {\n-        Node* off  = phase->MakeConX(type2aelembytes(copy_type) * i);\n-        Node* next_src = phase->transform(new AddPNode(base_src,adr_src,off));\n-        Node* next_dest = phase->transform(new AddPNode(base_dest,adr_dest,off));\n-        Node* v = load(bs, phase, backward_ctl, mm, next_src, atp_src, value_type, copy_type);\n-        store(bs, phase, backward_ctl, mm, next_dest, atp_dest, v, value_type, copy_type);\n+      for (int i = count-1; i >= 0; i--) {\n+        copy(kit, atp_src, atp_dest, i, base_src, base_dest, adr_src, adr_dest, copy_type, value_type);\n@@ -433,6 +478,5 @@\n-      Node* v = load(bs, phase, backward_ctl, mm, adr_src, atp_src, value_type, copy_type);\n-      store(bs, phase, backward_ctl, mm, adr_dest, atp_dest, v, value_type, copy_type);\n-    } else if (can_reshape) {\n-      PhaseIterGVN* igvn = phase->is_IterGVN();\n-      igvn->_worklist.push(adr_src);\n-      igvn->_worklist.push(adr_dest);\n+    } else if(can_reshape) {\n+      PhaseGVN& gvn = kit.gvn();\n+      assert(gvn.is_IterGVN(), \"\");\n+      gvn.record_for_igvn(adr_src);\n+      gvn.record_for_igvn(adr_dest);\n@@ -440,2 +484,0 @@\n-    return phase->transform(mm);\n-  return phase->C->top();\n@@ -467,2 +509,1 @@\n-      CallProjections callprojs;\n-      extract_projections(&callprojs, true, false);\n+      CallProjections* callprojs = extract_projections(true, false);\n@@ -470,2 +511,2 @@\n-      if (callprojs.fallthrough_ioproj != NULL) {\n-        igvn->replace_node(callprojs.fallthrough_ioproj, in(TypeFunc::I_O));\n+      if (callprojs->fallthrough_ioproj != NULL) {\n+        igvn->replace_node(callprojs->fallthrough_ioproj, in(TypeFunc::I_O));\n@@ -473,2 +514,2 @@\n-      if (callprojs.fallthrough_memproj != NULL) {\n-        igvn->replace_node(callprojs.fallthrough_memproj, mem);\n+      if (callprojs->fallthrough_memproj != NULL) {\n+        igvn->replace_node(callprojs->fallthrough_memproj, mem);\n@@ -476,2 +517,2 @@\n-      if (callprojs.fallthrough_catchproj != NULL) {\n-        igvn->replace_node(callprojs.fallthrough_catchproj, ctl);\n+      if (callprojs->fallthrough_catchproj != NULL) {\n+        igvn->replace_node(callprojs->fallthrough_catchproj, ctl);\n@@ -492,0 +533,9 @@\n+#ifdef ASSERT\n+      Node* src = in(ArrayCopyNode::Src);\n+      const Type* src_type = phase->type(src);\n+      const TypeAryPtr* ary_src = src_type->isa_aryptr();\n+      BasicType elem = ary_src != NULL ? ary_src->klass()->as_array_klass()->element_type()->basic_type() : T_CONFLICT;\n+      BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+      assert(!is_clonebasic() || bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Optimization) ||\n+             (ary_src != NULL && elem == T_INLINE_TYPE && ary_src->klass()->is_obj_array_klass()), \"added control for clone?\");\n+#endif\n@@ -502,1 +552,5 @@\n-  if (remove_dead_region(phase, can_reshape))  return this;\n+  \/\/ Perform any generic optimizations first\n+  Node* result = SafePointNode::Ideal(phase, can_reshape);\n+  if (result != NULL) {\n+    return result;\n+  }\n@@ -544,0 +598,11 @@\n+  Node* src = in(ArrayCopyNode::Src);\n+  Node* dest = in(ArrayCopyNode::Dest);\n+  const Type* src_type = phase->type(src);\n+  const Type* dest_type = phase->type(dest);\n+\n+  if (src_type->isa_aryptr() && dest_type->isa_instptr()) {\n+    \/\/ clone used for load of unknown inline type can't be optimized at\n+    \/\/ this point\n+    return NULL;\n+  }\n+\n@@ -563,5 +628,21 @@\n-  Node* src = in(ArrayCopyNode::Src);\n-  Node* dest = in(ArrayCopyNode::Dest);\n-  const TypePtr* atp_src = get_address_type(phase, _src_type, src);\n-  const TypePtr* atp_dest = get_address_type(phase, _dest_type, dest);\n-  Node* in_mem = in(TypeFunc::Memory);\n+  JVMState* new_jvms = NULL;\n+  SafePointNode* new_map = NULL;\n+  if (!is_clonebasic()) {\n+    new_jvms = jvms()->clone_shallow(phase->C);\n+    new_map = new SafePointNode(req(), new_jvms);\n+    for (uint i = TypeFunc::FramePtr; i < req(); i++) {\n+      new_map->init_req(i, in(i));\n+    }\n+    new_jvms->set_map(new_map);\n+  } else {\n+    new_jvms = new (phase->C) JVMState(0);\n+    new_map = new SafePointNode(TypeFunc::Parms, new_jvms);\n+    new_jvms->set_map(new_map);\n+  }\n+  new_map->set_control(in(TypeFunc::Control));\n+  new_map->set_memory(MergeMemNode::make(in(TypeFunc::Memory)));\n+  new_map->set_i_o(in(TypeFunc::I_O));\n+  phase->record_for_igvn(new_map);\n+\n+  const TypeAryPtr* atp_src = get_address_type(phase, _src_type, src);\n+  const TypeAryPtr* atp_dest = get_address_type(phase, _dest_type, dest);\n@@ -574,0 +655,4 @@\n+  GraphKit kit(new_jvms, phase);\n+\n+  SafePointNode* backward_map = NULL;\n+  SafePointNode* forward_map = NULL;\n@@ -575,36 +660,36 @@\n-  Node* forward_ctl = phase->C->top();\n-  array_copy_test_overlap(phase, can_reshape, disjoint_bases, count, forward_ctl, backward_ctl);\n-\n-  Node* forward_mem = array_copy_forward(phase, can_reshape, forward_ctl,\n-                                         in_mem,\n-                                         atp_src, atp_dest,\n-                                         adr_src, base_src, adr_dest, base_dest,\n-                                         copy_type, value_type, count);\n-\n-  Node* backward_mem = array_copy_backward(phase, can_reshape, backward_ctl,\n-                                           in_mem,\n-                                           atp_src, atp_dest,\n-                                           adr_src, base_src, adr_dest, base_dest,\n-                                           copy_type, value_type, count);\n-\n-  Node* ctl = NULL;\n-  if (!forward_ctl->is_top() && !backward_ctl->is_top()) {\n-    ctl = new RegionNode(3);\n-    ctl->init_req(1, forward_ctl);\n-    ctl->init_req(2, backward_ctl);\n-    ctl = phase->transform(ctl);\n-    MergeMemNode* forward_mm = forward_mem->as_MergeMem();\n-    MergeMemNode* backward_mm = backward_mem->as_MergeMem();\n-    for (MergeMemStream mms(forward_mm, backward_mm); mms.next_non_empty2(); ) {\n-      if (mms.memory() != mms.memory2()) {\n-        Node* phi = new PhiNode(ctl, Type::MEMORY, phase->C->get_adr_type(mms.alias_idx()));\n-        phi->init_req(1, mms.memory());\n-        phi->init_req(2, mms.memory2());\n-        phi = phase->transform(phi);\n-        mms.set_memory(phi);\n-      }\n-    }\n-    mem = forward_mem;\n-  } else if (!forward_ctl->is_top()) {\n-    ctl = forward_ctl;\n-    mem = forward_mem;\n+\n+  array_copy_test_overlap(kit, disjoint_bases, count, backward_ctl);\n+\n+  {\n+    PreserveJVMState pjvms(&kit);\n+\n+    array_copy_forward(kit, can_reshape,\n+                       atp_src, atp_dest,\n+                       adr_src, base_src, adr_dest, base_dest,\n+                       copy_type, value_type, count);\n+\n+    forward_map = kit.stop();\n+  }\n+\n+  kit.set_control(backward_ctl);\n+  array_copy_backward(kit, can_reshape,\n+                      atp_src, atp_dest,\n+                      adr_src, base_src, adr_dest, base_dest,\n+                      copy_type, value_type, count);\n+\n+  backward_map = kit.stop();\n+\n+  if (!forward_map->control()->is_top() && !backward_map->control()->is_top()) {\n+    assert(forward_map->i_o() == backward_map->i_o(), \"need a phi on IO?\");\n+    Node* ctl = new RegionNode(3);\n+    Node* mem = new PhiNode(ctl, Type::MEMORY, TypePtr::BOTTOM);\n+    kit.set_map(forward_map);\n+    ctl->init_req(1, kit.control());\n+    mem->init_req(1, kit.reset_memory());\n+    kit.set_map(backward_map);\n+    ctl->init_req(2, kit.control());\n+    mem->init_req(2, kit.reset_memory());\n+    kit.set_control(phase->transform(ctl));\n+    kit.set_all_memory(phase->transform(mem));\n+  } else if (!forward_map->control()->is_top()) {\n+    kit.set_map(forward_map);\n@@ -612,3 +697,2 @@\n-    assert(!backward_ctl->is_top(), \"no copy?\");\n-    ctl = backward_ctl;\n-    mem = backward_mem;\n+    assert(!backward_map->control()->is_top(), \"no copy?\");\n+    kit.set_map(backward_map);\n@@ -622,1 +706,5 @@\n-  if (!finish_transform(phase, can_reshape, ctl, mem)) {\n+  mem = kit.map()->memory();\n+  if (!finish_transform(phase, can_reshape, kit.control(), mem)) {\n+    if (!can_reshape) {\n+      phase->record_for_igvn(this);\n+    }\n@@ -715,1 +803,2 @@\n-  BasicType ary_elem = ary_t->klass()->as_array_klass()->element_type()->basic_type();\n+  ciArrayKlass* klass = ary_t->klass()->as_array_klass();\n+  BasicType ary_elem = klass->element_type()->basic_type();\n@@ -718,0 +807,3 @@\n+  if (klass->is_flat_array_klass()) {\n+    elemsize = klass->as_flat_array_klass()->element_byte_size();\n+  }\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.cpp","additions":233,"deletions":141,"binary":false,"changes":374,"status":"modified"},{"patch":"@@ -93,1 +93,1 @@\n-  static const TypePtr* get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n);\n+  static const TypeAryPtr* get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n);\n@@ -99,1 +99,1 @@\n-  void array_copy_test_overlap(PhaseGVN *phase, bool can_reshape,\n+  void array_copy_test_overlap(GraphKit& kit,\n@@ -101,4 +101,7 @@\n-                               Node*& forward_ctl, Node*& backward_ctl);\n-  Node* array_copy_forward(PhaseGVN *phase, bool can_reshape, Node*& ctl,\n-                           Node* mem,\n-                           const TypePtr* atp_src, const TypePtr* atp_dest,\n+                               Node*& backward_ctl);\n+  void array_copy_forward(GraphKit& kit, bool can_reshape,\n+                          const TypeAryPtr* atp_src, const TypeAryPtr* atp_dest,\n+                          Node* adr_src, Node* base_src, Node* adr_dest, Node* base_dest,\n+                          BasicType copy_type, const Type* value_type, int count);\n+  void array_copy_backward(GraphKit& kit, bool can_reshape,\n+                           const TypeAryPtr* atp_src, const TypeAryPtr* atp_dest,\n@@ -107,5 +110,0 @@\n-  Node* array_copy_backward(PhaseGVN *phase, bool can_reshape, Node*& ctl,\n-                            Node* mem,\n-                            const TypePtr* atp_src, const TypePtr* atp_dest,\n-                            Node* adr_src, Node* base_src, Node* adr_dest, Node* base_dest,\n-                            BasicType copy_type, const Type* value_type, int count);\n@@ -114,0 +112,4 @@\n+  void copy(GraphKit& kit, const TypeAryPtr* atp_src, const TypeAryPtr* atp_dest, int i,\n+            Node* base_src, Node* base_dest, Node* adr_src, Node* adr_dest,\n+            BasicType copy_type, const Type* value_type);\n+\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.hpp","additions":13,"deletions":11,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -790,0 +790,6 @@\n+  product(bool, UseArrayLoadStoreProfile, true,                             \\\n+          \"Take advantage of profiling at array load\/store\")                \\\n+                                                                            \\\n+  product(bool, UseACmpProfile, true,                                       \\\n+          \"Take advantage of profiling at acmp\")                            \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -121,1 +122,1 @@\n-  \/\/ paths to facilitate late inlinig.\n+  \/\/ paths to facilitate late inlining.\n@@ -127,0 +128,1 @@\n+      _call_node(NULL),\n@@ -129,0 +131,8 @@\n+    if (InlineTypeReturnedAsFields && method->is_method_handle_intrinsic()) {\n+      \/\/ If that call has not been optimized by the time optimizations are over,\n+      \/\/ we'll need to add a call to create an inline type instance from the klass\n+      \/\/ returned by the call (see PhaseMacroExpand::expand_mh_intrinsic_return).\n+      \/\/ Separating memory and I\/O projections for exceptions is required to\n+      \/\/ perform that graph transformation.\n+      _separate_io_proj = true;\n+    }\n@@ -138,0 +148,1 @@\n+  PhaseGVN& gvn = kit.gvn();\n@@ -170,1 +181,4 @@\n-  kit.set_arguments_for_java_call(call);\n+  kit.set_arguments_for_java_call(call, is_late_inline());\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -196,1 +210,0 @@\n-\n@@ -208,1 +221,1 @@\n-  if (kit.gvn().type(receiver)->higher_equal(TypePtr::NULL_PTR)) {\n+  if (!receiver->is_InlineType() && kit.gvn().type(receiver)->higher_equal(TypePtr::NULL_PTR)) {\n@@ -254,0 +267,3 @@\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -344,0 +360,4 @@\n+\n+  virtual CallGenerator* inline_cg() {\n+    return _inline_cg;\n+  }\n@@ -354,3 +374,3 @@\n-  const TypeTuple *r = call->tf()->domain();\n-  for (int i1 = 0; i1 < method()->arg_size(); i1++) {\n-    if (call->in(TypeFunc::Parms + i1)->is_top() && r->field_at(TypeFunc::Parms + i1) != Type::HALF) {\n+  const TypeTuple* r = call->tf()->domain_cc();\n+  for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+    if (call->in(i1)->is_top() && r->field_at(i1) != Type::HALF) {\n@@ -368,10 +388,8 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true);\n-  if (callprojs.fallthrough_catchproj == call->in(0) ||\n-      callprojs.catchall_catchproj == call->in(0) ||\n-      callprojs.fallthrough_memproj == call->in(TypeFunc::Memory) ||\n-      callprojs.catchall_memproj == call->in(TypeFunc::Memory) ||\n-      callprojs.fallthrough_ioproj == call->in(TypeFunc::I_O) ||\n-      callprojs.catchall_ioproj == call->in(TypeFunc::I_O) ||\n-      (callprojs.resproj != NULL && call->find_edge(callprojs.resproj) != -1) ||\n-      (callprojs.exobj != NULL && call->find_edge(callprojs.exobj) != -1)) {\n+  CallProjections* callprojs = call->extract_projections(true);\n+  if (callprojs->fallthrough_catchproj == call->in(0) ||\n+      callprojs->catchall_catchproj == call->in(0) ||\n+      callprojs->fallthrough_memproj == call->in(TypeFunc::Memory) ||\n+      callprojs->catchall_memproj == call->in(TypeFunc::Memory) ||\n+      callprojs->fallthrough_ioproj == call->in(TypeFunc::I_O) ||\n+      callprojs->catchall_ioproj == call->in(TypeFunc::I_O) ||\n+      (callprojs->exobj != NULL && call->find_edge(callprojs->exobj) != -1)) {\n@@ -380,0 +398,11 @@\n+  bool result_not_used = true;\n+  for (uint i = 0; i < callprojs->nb_resproj; i++) {\n+    if (callprojs->resproj[i] != NULL) {\n+      if (callprojs->resproj[i]->outcnt() != 0) {\n+        result_not_used = false;\n+      }\n+      if (call->find_edge(callprojs->resproj[i]) != -1) {\n+        return;\n+      }\n+    }\n+  }\n@@ -387,1 +416,0 @@\n-  bool result_not_used = (callprojs.resproj == NULL || callprojs.resproj->outcnt() == 0);\n@@ -403,0 +431,1 @@\n+    PhaseGVN& gvn = *C->initial_gvn();\n@@ -406,1 +435,1 @@\n-      C->initial_gvn()->set_type_bottom(mem);\n+      gvn.set_type_bottom(mem);\n@@ -410,4 +439,2 @@\n-    uint nargs = method()->arg_size();\n-    Node* top = C->top();\n-    for (uint i1 = 0; i1 < nargs; i1++) {\n-      map->set_req(TypeFunc::Parms + i1, top);\n+    for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+      map->set_req(i1, C->top());\n@@ -421,0 +448,6 @@\n+    const TypeTuple *domain_sig = call->_tf->domain_sig();\n+    ExtendedSignature sig_cc = ExtendedSignature(method()->get_sig_cc(), SigEntryFilter());\n+    uint nargs = method()->arg_size();\n+    assert(domain_sig->cnt() - TypeFunc::Parms == nargs, \"inconsistent signature\");\n+\n+    uint j = TypeFunc::Parms;\n@@ -422,1 +455,11 @@\n-      map->set_argument(jvms, i1, call->in(TypeFunc::Parms + i1));\n+      const Type* t = domain_sig->field_at(TypeFunc::Parms + i1);\n+      if (method()->has_scalarized_args() && t->is_inlinetypeptr() && !t->maybe_null() && t->inline_klass()->can_be_passed_as_fields()) {\n+        \/\/ Inline type arguments are not passed by reference: we get an argument per\n+        \/\/ field of the inline type. Build InlineTypeNodes from the inline type arguments.\n+        GraphKit arg_kit(jvms, &gvn);\n+        InlineTypeNode* vt = InlineTypeNode::make_from_multi(&arg_kit, call, sig_cc, t->inline_klass(), j, true);\n+        map->set_control(arg_kit.control());\n+        map->set_argument(jvms, i1, vt);\n+      } else {\n+        map->set_argument(jvms, i1, call->in(j++));\n+      }\n@@ -438,0 +481,15 @@\n+    \/\/ Allocate a buffer for the returned InlineTypeNode because the caller expects an oop return.\n+    \/\/ Do this before the method handle call in case the buffer allocation triggers deoptimization.\n+    Node* buffer_oop = NULL;\n+    if (is_mh_late_inline() && _inline_cg->method()->return_type()->is_inlinetype()) {\n+      GraphKit arg_kit(jvms, &gvn);\n+      {\n+        PreserveReexecuteState preexecs(&arg_kit);\n+        arg_kit.jvms()->set_should_reexecute(true);\n+        arg_kit.inc_sp(nargs);\n+        Node* klass_node = arg_kit.makecon(TypeKlassPtr::make(_inline_cg->method()->return_type()->as_inline_klass()));\n+        buffer_oop = arg_kit.new_instance(klass_node, NULL, NULL, \/* deoptimize_on_exception *\/ true);\n+      }\n+      jvms = arg_kit.transfer_exceptions_into_jvms();\n+    }\n+\n@@ -464,0 +522,19 @@\n+\n+    \/\/ Handle inline type returns\n+    bool returned_as_fields = call->tf()->returns_inline_type_as_fields();\n+    if (result->is_InlineType()) {\n+      \/\/ Only possible if is_mh_late_inline() when the callee does not \"know\" that the caller expects an oop\n+      assert(is_mh_late_inline() && !returned_as_fields, \"sanity\");\n+      assert(buffer_oop != NULL, \"should have allocated a buffer\");\n+      InlineTypeNode* vt = result->as_InlineType();\n+      vt->store(&kit, buffer_oop, buffer_oop, vt->type()->inline_klass(), 0);\n+      \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+      \/\/ store that would make this buffer accessible by other threads.\n+      AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer_oop, &kit.gvn());\n+      assert(alloc != NULL, \"must have an allocation node\");\n+      kit.insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+      result = buffer_oop;\n+    } else if (result->is_InlineTypePtr() && returned_as_fields) {\n+      result->as_InlineTypePtr()->replace_call_results(&kit, call, C);\n+    }\n+\n@@ -505,0 +582,7 @@\n+  \/\/ AlwaysIncrementalInline causes for_method_handle_inline() to\n+  \/\/ return a LateInlineCallGenerator. Extract the\n+  \/\/ InlineCallGenerato from it.\n+  if (AlwaysIncrementalInline && cg != NULL && cg->is_late_inline()) {\n+    cg = cg->inline_cg();\n+  }\n+\n@@ -511,1 +595,1 @@\n-  if (cg != NULL && cg->is_inline()) {\n+  if (cg != NULL && (cg->is_inline() || cg->is_inlined_method_handle_intrinsic(jvms, cg->method()))) {\n@@ -803,0 +887,22 @@\n+  \/\/ Allocate inline types if they are merged with objects (similar to Parse::merge_common())\n+  uint tos = kit.jvms()->stkoff() + kit.sp();\n+  uint limit = slow_map->req();\n+  for (uint i = TypeFunc::Parms; i < limit; i++) {\n+    Node* m = kit.map()->in(i);\n+    Node* n = slow_map->in(i);\n+    const Type* t = gvn.type(m)->meet_speculative(gvn.type(n));\n+    if (m->is_InlineType() && !t->isa_inlinetype()) {\n+      \/\/ Allocate inline type in fast path\n+      m = m->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, m);\n+    }\n+    if (n->is_InlineType() && !t->isa_inlinetype()) {\n+      \/\/ Allocate inline type in slow path\n+      PreserveJVMState pjvms(&kit);\n+      kit.set_map(slow_map);\n+      n = n->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, n);\n+      slow_map = kit.stop();\n+    }\n+  }\n+\n@@ -826,2 +932,0 @@\n-  uint tos = kit.jvms()->stkoff() + kit.sp();\n-  uint limit = slow_map->req();\n@@ -863,2 +967,2 @@\n-  if (IncrementalInline && call_site_count > 0 &&\n-      (input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())) {\n+  if (IncrementalInline && (AlwaysIncrementalInline ||\n+                            (call_site_count > 0 && (input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())))) {\n@@ -872,0 +976,17 @@\n+static void cast_argument(int nargs, int arg_nb, ciType* t, GraphKit& kit) {\n+  PhaseGVN& gvn = kit.gvn();\n+  Node* arg = kit.argument(arg_nb);\n+  const Type* arg_type = arg->bottom_type();\n+  const Type* sig_type = TypeOopPtr::make_from_klass(t->as_klass());\n+  if (arg_type->isa_oopptr() && !arg_type->higher_equal(sig_type)) {\n+    const Type* narrowed_arg_type = arg_type->join_speculative(sig_type); \/\/ keep speculative part\n+    arg = gvn.transform(new CheckCastPPNode(kit.control(), arg, narrowed_arg_type));\n+    kit.set_argument(arg_nb, arg);\n+  }\n+  if (sig_type->is_inlinetypeptr() && !arg->is_InlineType() &&\n+      !kit.gvn().type(arg)->maybe_null() && t->as_inline_klass()->is_scalarizable()) {\n+    arg = InlineTypeNode::make_from_oop(&kit, arg, t->as_inline_klass());\n+    kit.set_argument(arg_nb, arg);\n+  }\n+}\n+\n@@ -924,1 +1045,3 @@\n-                                              PROB_ALWAYS);\n+                                              PROB_ALWAYS,\n+                                              NULL,\n+                                              true);\n@@ -938,0 +1061,1 @@\n+      int nargs = callee->arg_size();\n@@ -939,1 +1063,1 @@\n-      Node* member_name = kit.argument(callee->arg_size() - 1);\n+      Node* member_name = kit.argument(nargs - 1);\n@@ -959,8 +1083,1 @@\n-          Node* arg = kit.argument(0);\n-          const TypeOopPtr* arg_type = arg->bottom_type()->isa_oopptr();\n-          const Type*       sig_type = TypeOopPtr::make_from_klass(signature->accessing_klass());\n-          if (arg_type != NULL && !arg_type->higher_equal(sig_type)) {\n-            const Type* recv_type = arg_type->join_speculative(sig_type); \/\/ keep speculative part\n-            Node* cast_obj = gvn.transform(new CheckCastPPNode(kit.control(), arg, recv_type));\n-            kit.set_argument(0, cast_obj);\n-          }\n+          cast_argument(nargs, 0, signature->accessing_klass(), kit);\n@@ -972,8 +1089,1 @@\n-            Node* arg = kit.argument(receiver_skip + j);\n-            const TypeOopPtr* arg_type = arg->bottom_type()->isa_oopptr();\n-            const Type*       sig_type = TypeOopPtr::make_from_klass(t->as_klass());\n-            if (arg_type != NULL && !arg_type->higher_equal(sig_type)) {\n-              const Type* narrowed_arg_type = arg_type->join_speculative(sig_type); \/\/ keep speculative part\n-              Node* cast_obj = gvn.transform(new CheckCastPPNode(kit.control(), arg, narrowed_arg_type));\n-              kit.set_argument(receiver_skip + j, cast_obj);\n-            }\n+            cast_argument(nargs, receiver_skip + j, t, kit);\n@@ -1010,1 +1120,2 @@\n-                                              speculative_receiver_type);\n+                                              speculative_receiver_type,\n+                                              true);\n@@ -1095,1 +1206,1 @@\n-    Node* receiver = kit.null_check_receiver_before_call(method());\n+    kit.null_check_receiver_before_call(method());\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":160,"deletions":49,"binary":false,"changes":209,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -81,1 +83,1 @@\n-Node *StartNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *StartNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -105,11 +107,0 @@\n-\/\/------------------------------StartOSRNode----------------------------------\n-\/\/ The method start node for an on stack replacement adapter\n-\n-\/\/------------------------------osr_domain-----------------------------\n-const TypeTuple *StartOSRNode::osr_domain() {\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n-\n-  return TypeTuple::make(TypeFunc::Parms+1, fields);\n-}\n-\n@@ -496,0 +487,8 @@\n+      } else if (cik->is_flat_array_klass()) {\n+        ciKlass* cie = cik->as_flat_array_klass()->base_element_klass();\n+        cie->print_name_on(st);\n+        st->print(\"[%d]\", spobj->n_fields());\n+        int ndim = cik->as_array_klass()->dimension() - 1;\n+        while (ndim-- > 0) {\n+          st->print(\"[]\");\n+        }\n@@ -703,1 +702,1 @@\n-const Type *CallNode::bottom_type() const { return tf()->range(); }\n+const Type *CallNode::bottom_type() const { return tf()->range_cc(); }\n@@ -705,2 +704,4 @@\n-  if (phase->type(in(0)) == Type::TOP)  return Type::TOP;\n-  return tf()->range();\n+  if (!in(0) || phase->type(in(0)) == Type::TOP) {\n+    return Type::TOP;\n+  }\n+  return tf()->range_cc();\n@@ -711,0 +712,7 @@\n+  if (_entry_point == StubRoutines::store_inline_type_fields_to_buf()) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -719,2 +727,28 @@\n-Node *CallNode::match( const ProjNode *proj, const Matcher *match ) {\n-  switch (proj->_con) {\n+Node *CallNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n+  uint con = proj->_con;\n+  const TypeTuple *range_cc = tf()->range_cc();\n+  if (con >= TypeFunc::Parms) {\n+    if (is_CallRuntime()) {\n+      if (con == TypeFunc::Parms) {\n+        uint ideal_reg = range_cc->field_at(TypeFunc::Parms)->ideal_reg();\n+        OptoRegPair regs = match->c_return_value(ideal_reg);\n+        RegMask rm = RegMask(regs.first());\n+        if (OptoReg::is_valid(regs.second())) {\n+          rm.Insert(regs.second());\n+        }\n+        return new MachProjNode(this,con,rm,ideal_reg);\n+      } else {\n+        assert(con == TypeFunc::Parms+1, \"only one return value\");\n+        assert(range_cc->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n+        return new MachProjNode(this,con, RegMask::Empty, (uint)OptoReg::Bad);\n+      }\n+    } else {\n+      \/\/ The Call may return multiple values (inline type fields): we\n+      \/\/ create one projection per returned value.\n+      assert(con <= TypeFunc::Parms+1 || InlineTypeReturnedAsFields, \"only for multi value return\");\n+      uint ideal_reg = range_cc->field_at(con)->ideal_reg();\n+      return new MachProjNode(this, con, mask[con-TypeFunc::Parms], ideal_reg);\n+    }\n+  }\n+\n+  switch (con) {\n@@ -726,16 +760,0 @@\n-  case TypeFunc::Parms+1:       \/\/ For LONG & DOUBLE returns\n-    assert(tf()->range()->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n-    \/\/ 2nd half of doubles and longs\n-    return new MachProjNode(this,proj->_con, RegMask::Empty, (uint)OptoReg::Bad);\n-\n-  case TypeFunc::Parms: {       \/\/ Normal returns\n-    uint ideal_reg = tf()->range()->field_at(TypeFunc::Parms)->ideal_reg();\n-    OptoRegPair regs = is_CallRuntime()\n-      ? match->c_return_value(ideal_reg)  \/\/ Calls into C runtime\n-      : match->  return_value(ideal_reg); \/\/ Calls into compiled Java code\n-    RegMask rm = RegMask(regs.first());\n-    if( OptoReg::is_valid(regs.second()) )\n-      rm.Insert( regs.second() );\n-    return new MachProjNode(this,proj->_con,rm,ideal_reg);\n-  }\n-\n@@ -762,1 +780,1 @@\n-    const TypeTuple* args = _tf->domain();\n+    const TypeTuple* args = _tf->domain_sig();\n@@ -811,1 +829,1 @@\n-      const TypeTuple* d = tf()->domain();\n+      const TypeTuple* d = tf()->domain_cc();\n@@ -827,1 +845,1 @@\n-  const TypeTuple * d = tf()->domain();\n+  const TypeTuple * d = tf()->domain_cc();\n@@ -837,0 +855,11 @@\n+bool CallNode::has_debug_use(Node *n) {\n+  assert(jvms() != NULL, \"jvms should not be null\");\n+  for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+    Node *arg = in(i);\n+    if (arg == n) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -868,10 +897,15 @@\n-void CallNode::extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts) {\n-  projs->fallthrough_proj      = NULL;\n-  projs->fallthrough_catchproj = NULL;\n-  projs->fallthrough_ioproj    = NULL;\n-  projs->catchall_ioproj       = NULL;\n-  projs->catchall_catchproj    = NULL;\n-  projs->fallthrough_memproj   = NULL;\n-  projs->catchall_memproj      = NULL;\n-  projs->resproj               = NULL;\n-  projs->exobj                 = NULL;\n+CallProjections* CallNode::extract_projections(bool separate_io_proj, bool do_asserts) {\n+  uint max_res = TypeFunc::Parms-1;\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    ProjNode *pn = fast_out(i)->as_Proj();\n+    max_res = MAX2(max_res, pn->_con);\n+  }\n+\n+  assert(max_res < _tf->range_cc()->cnt(), \"result out of bounds\");\n+\n+  uint projs_size = sizeof(CallProjections);\n+  if (max_res > TypeFunc::Parms) {\n+    projs_size += (max_res-TypeFunc::Parms)*sizeof(Node*);\n+  }\n+  char* projs_storage = resource_allocate_bytes(projs_size);\n+  CallProjections* projs = new(projs_storage)CallProjections(max_res - TypeFunc::Parms + 1);\n@@ -923,1 +957,1 @@\n-      projs->resproj = pn;\n+      projs->resproj[0] = pn;\n@@ -926,1 +960,3 @@\n-      assert(false, \"unexpected projection from allocation node.\");\n+      assert(pn->_con <= max_res, \"unexpected projection from allocation node.\");\n+      projs->resproj[pn->_con-TypeFunc::Parms] = pn;\n+      break;\n@@ -933,1 +969,1 @@\n-  assert(projs->fallthrough_proj      != NULL, \"must be found\");\n+  assert(!do_asserts || projs->fallthrough_proj      != NULL, \"must be found\");\n@@ -943,0 +979,1 @@\n+  return projs;\n@@ -984,2 +1021,2 @@\n-  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain()->cnt() : (uint)TypeFunc::Parms+1;\n-  uint new_dbg_start = tf()->domain()->cnt();\n+  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain_sig()->cnt() : (uint)TypeFunc::Parms+1;\n+  uint new_dbg_start = tf()->domain_sig()->cnt();\n@@ -1026,0 +1063,4 @@\n+  Bytecodes::Code bc = jvms()->method()->java_code_at_bci(_bci);\n+  if (EnableValhalla && (bc == Bytecodes::_if_acmpeq || bc == Bytecodes::_if_acmpne)) {\n+    return true;\n+  }\n@@ -1080,0 +1121,151 @@\n+bool CallStaticJavaNode::remove_useless_allocation(PhaseGVN *phase, Node* ctl, Node* mem, Node* unc_arg) {\n+  \/\/ Split if can cause the flattened array branch of an array load to\n+  \/\/ end in an uncommon trap. In that case, the allocation of the\n+  \/\/ loaded value and its initialization is useless. Eliminate it. use\n+  \/\/ the jvm state of the allocation to create a new uncommon trap\n+  \/\/ call at the load.\n+  if (ctl == NULL || ctl->is_top() || mem == NULL || mem->is_top() || !mem->is_MergeMem()) {\n+    return false;\n+  }\n+  PhaseIterGVN* igvn = phase->is_IterGVN();\n+  if (ctl->is_Region()) {\n+    bool res = false;\n+    for (uint i = 1; i < ctl->req(); i++) {\n+      MergeMemNode* mm = mem->clone()->as_MergeMem();\n+      for (MergeMemStream mms(mm); mms.next_non_empty(); ) {\n+        Node* m = mms.memory();\n+        if (m->is_Phi() && m->in(0) == ctl) {\n+          mms.set_memory(m->in(i));\n+        }\n+      }\n+      if (remove_useless_allocation(phase, ctl->in(i), mm, unc_arg)) {\n+        res = true;\n+        if (!ctl->in(i)->is_Region()) {\n+          igvn->replace_input_of(ctl, i, phase->C->top());\n+        }\n+      }\n+      igvn->remove_dead_node(mm);\n+    }\n+    return res;\n+  }\n+  \/\/ verify the control flow is ok\n+  Node* c = ctl;\n+  Node* copy = NULL;\n+  Node* alloc = NULL;\n+  for (;;) {\n+    if (c == NULL || c->is_top()) {\n+      return false;\n+    }\n+    if (c->is_Proj() || c->is_Catch() || c->is_MemBar()) {\n+      c = c->in(0);\n+    } else if (c->Opcode() == Op_CallLeaf &&\n+               c->as_Call()->entry_point() == CAST_FROM_FN_PTR(address, OptoRuntime::load_unknown_inline)) {\n+      copy = c;\n+      c = c->in(0);\n+    } else if (c->is_Allocate()) {\n+      Node* new_obj = c->as_Allocate()->result_cast();\n+      if (copy == NULL || new_obj == NULL) {\n+        return false;\n+      }\n+      Node* copy_dest = copy->in(TypeFunc::Parms + 2);\n+      if (copy_dest != new_obj) {\n+        return false;\n+      }\n+      alloc = c;\n+      break;\n+    } else {\n+      return false;\n+    }\n+  }\n+\n+  JVMState* jvms = alloc->jvms();\n+  if (phase->C->too_many_traps(jvms->method(), jvms->bci(), Deoptimization::trap_request_reason(uncommon_trap_request()))) {\n+    return false;\n+  }\n+\n+  Node* alloc_mem = alloc->in(TypeFunc::Memory);\n+  if (alloc_mem == NULL || alloc_mem->is_top()) {\n+    return false;\n+  }\n+  if (!alloc_mem->is_MergeMem()) {\n+    alloc_mem = MergeMemNode::make(alloc_mem);\n+  }\n+\n+  \/\/ and that there's no unexpected side effect\n+  for (MergeMemStream mms2(mem->as_MergeMem(), alloc_mem->as_MergeMem()); mms2.next_non_empty2(); ) {\n+    Node* m1 = mms2.is_empty() ? mms2.base_memory() : mms2.memory();\n+    Node* m2 = mms2.memory2();\n+\n+    for (uint i = 0; i < 100; i++) {\n+      if (m1 == m2) {\n+        break;\n+      } else if (m1->is_Proj()) {\n+        m1 = m1->in(0);\n+      } else if (m1->is_MemBar()) {\n+        m1 = m1->in(TypeFunc::Memory);\n+      } else if (m1->Opcode() == Op_CallLeaf &&\n+                 m1->as_Call()->entry_point() == CAST_FROM_FN_PTR(address, OptoRuntime::load_unknown_inline)) {\n+        if (m1 != copy) {\n+          return false;\n+        }\n+        m1 = m1->in(TypeFunc::Memory);\n+      } else if (m1->is_Allocate()) {\n+        if (m1 != alloc) {\n+          return false;\n+        }\n+        break;\n+      } else if (m1->is_MergeMem()) {\n+        MergeMemNode* mm = m1->as_MergeMem();\n+        int idx = mms2.alias_idx();\n+        if (idx == Compile::AliasIdxBot) {\n+          m1 = mm->base_memory();\n+        } else {\n+          m1 = mm->memory_at(idx);\n+        }\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+  if (alloc_mem->outcnt() == 0) {\n+    igvn->remove_dead_node(alloc_mem);\n+  }\n+\n+  address call_addr = SharedRuntime::uncommon_trap_blob()->entry_point();\n+  CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\",\n+                                         jvms->bci(), NULL);\n+  unc->init_req(TypeFunc::Control, alloc->in(0));\n+  unc->init_req(TypeFunc::I_O, alloc->in(TypeFunc::I_O));\n+  unc->init_req(TypeFunc::Memory, alloc->in(TypeFunc::Memory));\n+  unc->init_req(TypeFunc::FramePtr,  alloc->in(TypeFunc::FramePtr));\n+  unc->init_req(TypeFunc::ReturnAdr, alloc->in(TypeFunc::ReturnAdr));\n+  unc->init_req(TypeFunc::Parms+0, unc_arg);\n+  unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+  unc->copy_call_debug_info(igvn, alloc->as_Allocate());\n+\n+  igvn->replace_input_of(alloc, 0, phase->C->top());\n+\n+  igvn->register_new_node_with_optimizer(unc);\n+\n+  Node* ctrl = phase->transform(new ProjNode(unc, TypeFunc::Control));\n+  Node* halt = phase->transform(new HaltNode(ctrl, alloc->in(TypeFunc::FramePtr), \"uncommon trap returned which should never happen\"));\n+  phase->C->root()->add_req(halt);\n+\n+  return true;\n+}\n+\n+\n+Node* CallStaticJavaNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  if (can_reshape && uncommon_trap_request() != 0) {\n+    if (remove_useless_allocation(phase, in(0), in(TypeFunc::Memory), in(TypeFunc::Parms))) {\n+      if (!in(0)->is_Region()) {\n+        PhaseIterGVN* igvn = phase->is_IterGVN();\n+        igvn->replace_input_of(this, 0, phase->C->top());\n+      }\n+      return this;\n+    }\n+  }\n+  return CallNode::Ideal(phase, can_reshape);\n+}\n+\n+\n@@ -1142,1 +1334,1 @@\n-Node* CallNativeNode::match(const ProjNode *proj, const Matcher *matcher) {\n+Node* CallNativeNode::match(const ProjNode *proj, const Matcher *matcher, const RegMask* mask) {\n@@ -1152,1 +1344,1 @@\n-      const Type* field_at_con = tf()->range()->field_at(proj->_con);\n+      const Type* field_at_con = tf()->range_sig()->field_at(proj->_con);\n@@ -1167,1 +1359,1 @@\n-      assert(tf()->range()->field_at(proj->_con) == Type::HALF, \"Expected HALF\");\n+      assert(tf()->range_sig()->field_at(proj->_con) == Type::HALF, \"Expected HALF\");\n@@ -1202,0 +1394,7 @@\n+  if (_entry_point == NULL) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -1206,1 +1405,1 @@\n-  assert((tf()->domain()->cnt() - TypeFunc::Parms) == argcnt, \"arg counts must match!\");\n+  assert((tf()->domain_sig()->cnt() - TypeFunc::Parms) == argcnt, \"arg counts must match!\");\n@@ -1209,1 +1408,1 @@\n-    assert(tf()->domain()->field_at(TypeFunc::Parms + i)->basic_type() == sig_bt[i], \"types must match!\");\n+    assert(tf()->domain_sig()->field_at(TypeFunc::Parms + i)->basic_type() == sig_bt[i], \"types must match!\");\n@@ -1252,0 +1451,6 @@\n+uint CallLeafNoFPNode::match_edge(uint idx) const {\n+  \/\/ Null entry point is a special case for which the target is in a\n+  \/\/ register. Need to match that edge.\n+  return entry_point() == NULL && idx == TypeFunc::Parms;\n+}\n+\n@@ -1517,1 +1722,3 @@\n-                           Node *size, Node *klass_node, Node *initial_test)\n+                           Node *size, Node *klass_node,\n+                           Node* initial_test,\n+                           InlineTypeBaseNode* inline_type_node)\n@@ -1525,0 +1732,1 @@\n+  _larval = false;\n@@ -1536,0 +1744,3 @@\n+  init_req( InlineTypeNode     , inline_type_node);\n+  \/\/ DefaultValue defaults to NULL\n+  \/\/ RawDefaultValue defaults to NULL\n@@ -1542,3 +1753,2 @@\n-         initializer->is_initializer() &&\n-         !initializer->is_static(),\n-             \"unexpected initializer method\");\n+         initializer->is_object_constructor_or_class_initializer(),\n+         \"unexpected initializer method\");\n@@ -1555,1 +1765,2 @@\n-Node *AllocateNode::make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem) {\n+\n+Node* AllocateNode::make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem) {\n@@ -1557,2 +1768,1 @@\n-  \/\/ For now only enable fast locking for non-array types\n-  if (UseBiasedLocking && Opcode() == Op_Allocate) {\n+  if (EnableValhalla) {\n@@ -1565,1 +1775,3 @@\n-  return mark_node;\n+  mark_node = phase->transform(mark_node);\n+  \/\/ Avoid returning a constant (old node) here because this method is used by LoadNode::Ideal\n+  return new OrXNode(mark_node, phase->MakeConX(_larval ? markWord::larval_bit_in_place : 0));\n@@ -1568,0 +1780,1 @@\n+\n@@ -1570,1 +1783,4 @@\n-  if (remove_dead_region(phase, can_reshape))  return this;\n+  Node* res = SafePointNode::Ideal(phase, can_reshape);\n+  if (res != NULL) {\n+    return res;\n+  }\n@@ -1995,1 +2211,3 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() &&\n+      !obj_type->isa_inlinetype() && !obj_type->is_inlinetypeptr()) {\n@@ -2163,1 +2381,3 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() &&\n+      !obj_type->isa_inlinetype() && !obj_type->is_inlinetypeptr()) {\n@@ -2245,1 +2465,2 @@\n-    dest_t = dest_t->add_offset(Type::OffsetBot)->is_oopptr();\n+    dest_t = dest_t->is_aryptr()->with_field_offset(Type::OffsetBot)->add_offset(Type::OffsetBot)->is_oopptr();\n+    t_oop = t_oop->is_aryptr()->with_field_offset(Type::OffsetBot);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":289,"deletions":68,"binary":false,"changes":357,"status":"modified"},{"patch":"@@ -84,1 +84,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n@@ -98,1 +98,0 @@\n-  static  const TypeTuple *osr_domain();\n@@ -551,1 +550,1 @@\n-class CallProjections : public StackObj {\n+class CallProjections {\n@@ -560,1 +559,19 @@\n-  Node* resproj;\n+  uint nb_resproj;\n+  Node* resproj[1]; \/\/ at least one projection\n+\n+  CallProjections(uint nbres) {\n+    fallthrough_proj      = NULL;\n+    fallthrough_catchproj = NULL;\n+    fallthrough_memproj   = NULL;\n+    fallthrough_ioproj    = NULL;\n+    catchall_catchproj    = NULL;\n+    catchall_memproj      = NULL;\n+    catchall_ioproj       = NULL;\n+    exobj                 = NULL;\n+    nb_resproj            = nbres;\n+    resproj[0]            = NULL;\n+    for (uint i = 1; i < nb_resproj; i++) {\n+      resproj[i]          = NULL;\n+    }\n+  }\n+\n@@ -583,1 +600,1 @@\n-    : SafePointNode(tf->domain()->cnt(), NULL, adr_type),\n+    : SafePointNode(tf->domain_cc()->cnt(), NULL, adr_type),\n@@ -610,1 +627,1 @@\n-  virtual Node       *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node       *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n@@ -629,0 +646,1 @@\n+  bool                has_debug_use(Node *n);\n@@ -635,2 +653,3 @@\n-    const TypeTuple *r = tf()->range();\n-    return (r->cnt() > TypeFunc::Parms &&\n+    const TypeTuple *r = tf()->range_sig();\n+    return (!tf()->returns_inline_type_as_fields() &&\n+            r->cnt() > TypeFunc::Parms &&\n@@ -643,1 +662,1 @@\n-  void extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts = true);\n+  CallProjections* extract_projections(bool separate_io_proj, bool do_asserts = true);\n@@ -714,0 +733,3 @@\n+\n+  bool remove_useless_allocation(PhaseGVN *phase, Node* ctl, Node* mem, Node* unc_arg);\n+\n@@ -722,0 +744,11 @@\n+    const TypeTuple *r = tf->range_sig();\n+    if (InlineTypeReturnedAsFields &&\n+        method != NULL &&\n+        method->is_method_handle_intrinsic() &&\n+        r->cnt() > TypeFunc::Parms &&\n+        r->field_at(TypeFunc::Parms)->isa_oopptr() &&\n+        r->field_at(TypeFunc::Parms)->is_oopptr()->can_be_inline_type()) {\n+      \/\/ Make sure this call is processed by PhaseMacroExpand::expand_mh_intrinsic_return\n+      init_flags(Flag_is_macro);\n+      C->add_macro_node(this);\n+    }\n@@ -723,0 +756,1 @@\n+\n@@ -748,0 +782,2 @@\n+  virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);\n+\n@@ -841,1 +877,1 @@\n-  virtual Node* match(const ProjNode *proj, const Matcher *m);\n+  virtual Node* match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n@@ -859,0 +895,1 @@\n+  virtual uint match_edge(uint idx) const;\n@@ -882,0 +919,3 @@\n+    InlineTypeNode,                   \/\/ InlineTypeNode if this is an inline type allocation\n+    DefaultValue,                     \/\/ default value in case of non-flattened inline type array\n+    RawDefaultValue,                  \/\/ same as above but as raw machine word\n@@ -891,0 +931,3 @@\n+    fields[InlineTypeNode] = Type::BOTTOM;\n+    fields[DefaultValue] = TypeInstPtr::NOTNULL;\n+    fields[RawDefaultValue] = TypeX_X;\n@@ -908,0 +951,1 @@\n+  bool _larval;\n@@ -911,1 +955,2 @@\n-               Node *size, Node *klass_node, Node *initial_test);\n+               Node *size, Node *klass_node, Node *initial_test,\n+               InlineTypeBaseNode* inline_type_node = NULL);\n@@ -981,1 +1026,1 @@\n-  Node* make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem);\n+  Node* make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem);\n@@ -992,4 +1037,2 @@\n-                    Node* count_val\n-                    )\n-    : AllocateNode(C, atype, ctrl, mem, abio, size, klass_node,\n-                   initial_test)\n+                    Node* count_val, Node* default_value, Node* raw_default_value)\n+    : AllocateNode(C, atype, ctrl, mem, abio, size, klass_node, initial_test)\n@@ -999,0 +1042,2 @@\n+    init_req(AllocateNode::DefaultValue,  default_value);\n+    init_req(AllocateNode::RawDefaultValue, raw_default_value);\n@@ -1118,1 +1163,1 @@\n-    return TypeFunc::make(domain,range);\n+    return TypeFunc::make(domain, range);\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":62,"deletions":17,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -166,0 +166,1 @@\n+macro(FlatArrayCheck)\n@@ -339,0 +340,2 @@\n+macro(InlineType)\n+macro(InlineTypePtr)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -389,0 +390,1 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n@@ -522,0 +524,1 @@\n+                  _inline_type_nodes (comp_arena(), 8, 0, NULL),\n@@ -626,4 +629,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -636,1 +637,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -773,0 +774,4 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n@@ -930,0 +935,3 @@\n+  _has_flattened_accesses = false;\n+  _flattened_accesses_share_alias = true;\n+\n@@ -1232,1 +1240,2 @@\n-    assert(InlineUnsafeOps, \"indeterminate pointers come only from unsafe ops\");\n+    bool default_value_load = EnableValhalla && tj->is_instptr()->klass() == ciEnv::current()->Class_klass();\n+    assert(InlineUnsafeOps || default_value_load, \"indeterminate pointers come only from unsafe ops\");\n@@ -1245,0 +1254,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1249,1 +1267,1 @@\n-      tj = ta = TypeAryPtr::make(ptr, ta->ary(), ta->klass(), true, offset, ta->instance_id());\n+      tj = ta = TypeAryPtr::make(ptr, ta->ary(), ta->klass(), true, Type::Offset(offset), ta->field_offset(), ta->instance_id());\n@@ -1255,0 +1273,2 @@\n+    \/\/ For flattened inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1258,1 +1278,1 @@\n-        tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),ta->ary(),ta->klass(),false,offset);\n+        tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1272,1 +1292,1 @@\n-        tj = ta = TypeAryPtr::make(ptr,ta->ary(),ta->klass(),false,offset);\n+        tj = ta = TypeAryPtr::make(ptr,ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1278,1 +1298,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,ta->klass(),false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1283,1 +1303,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());\n@@ -1287,1 +1307,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInlineType::BOTTOM && _flattened_accesses_share_alias) {\n+      const TypeAry *tary = TypeAry::make(TypeInlineType::BOTTOM, ta->size());\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1294,1 +1319,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1300,1 +1325,1 @@\n-      tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta->ary(),ta->klass(),false,offset);\n+      tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1314,1 +1339,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,Type::Offset(offset));\n@@ -1322,1 +1347,1 @@\n-      tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,offset);\n+      tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,Type::Offset(offset));\n@@ -1325,1 +1350,1 @@\n-      tj = to = TypeInstPtr::make(to->ptr(),to->klass(),to->klass_is_exact(),to->const_oop(),to->offset(), to->instance_id());\n+      tj = to = TypeInstPtr::make(to->ptr(),to->klass(),to->klass_is_exact(),to->const_oop(),Type::Offset(to->offset()), to->klass()->flatten_array(), to->instance_id());\n@@ -1332,1 +1357,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, Type::Offset(offset));\n@@ -1346,1 +1371,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, offset, to->instance_id());\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, Type::Offset(offset), canonical_holder->flatten_array(), to->instance_id());\n@@ -1348,1 +1373,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, offset);\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, Type::Offset(offset));\n@@ -1365,1 +1390,1 @@\n-                                   offset);\n+                                   Type::Offset(offset));\n@@ -1369,1 +1394,1 @@\n-    if( klass->is_obj_array_klass() ) {\n+    if (klass != NULL && klass->is_obj_array_klass()) {\n@@ -1373,1 +1398,1 @@\n-      tj = tk = TypeKlassPtr::make( TypePtr::NotNull, k, offset );\n+      tj = tk = TypeKlassPtr::make(TypePtr::NotNull, k, Type::Offset(offset));\n@@ -1389,1 +1414,1 @@\n-      tj = tk = TypeKlassPtr::make( TypePtr::NotNull, tk->klass(), offset );\n+      tj = tk = TypeKlassPtr::make(TypePtr::NotNull, tk->klass(), Type::Offset(offset));\n@@ -1528,1 +1553,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1532,3 +1557,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = NULL;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1584,0 +1612,1 @@\n+    ciField* field = NULL;\n@@ -1590,0 +1619,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1591,1 +1621,9 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (elemtype->isa_inlinetype() &&\n+          elemtype->inline_klass() != NULL &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->first_field_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1603,0 +1641,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1613,1 +1653,0 @@\n-      ciField* field;\n@@ -1620,0 +1659,4 @@\n+      } else if (tinst->klass()->is_inlinetype()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1621,1 +1664,1 @@\n-        ciInstanceKlass *k = tinst->klass()->as_instance_klass();\n+        ciInstanceKlass* k = tinst->klass()->as_instance_klass();\n@@ -1624,7 +1667,14 @@\n-      assert(field == NULL ||\n-             original_field == NULL ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset() == original_field->offset() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != NULL)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == NULL ||\n+           original_field == NULL ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset() == original_field->offset() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != NULL) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_aryptr()->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1635,3 +1685,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1639,6 +1690,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == NULL) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == NULL) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1800,0 +1852,349 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineTypeBase(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineTypeBase(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    assert(!n->is_InlineType(), \"chain of inline type nodes\");\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineTypePtr()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make inline types scalar in safepoints\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    InlineTypeBaseNode* vt = _inline_type_nodes.at(i)->as_InlineTypeBase();\n+    vt->make_scalar_in_safepoints(&igvn);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeBaseNode* vt = _inline_type_nodes.pop()->as_InlineTypeBase();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+      } else if (vt->is_InlineTypePtr()) {\n+        igvn.replace_node(vt, vt->get_oop());\n+      } else {\n+        igvn.replace_node(vt, igvn.C->top());\n+      }\n+    }\n+  }\n+  \/\/ TODO only check once we are removing, right?\n+  \/\/ Make sure that the return value does not keep an unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = NULL;\n+    for (uint i = 1; i < root()->req(); i++){\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == NULL, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != NULL) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flattened_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flattened_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flattened array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flattened array) correct. We're done with parsing so we\n+  \/\/ now know all flattened array accesses in this compile\n+  \/\/ unit. Let's move flattened array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flattened memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flattened array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = NULL;\n+      if (n->Opcode() == Op_StoreCM) {\n+        adr_type = get_adr_type(get_alias_index(n->in(MemNode::OopStore)->adr_type()));\n+      } else {\n+        adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      }\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != NULL) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flattened_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flattened array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != NULL &&\n+          ace->_adr_type->isa_aryptr() &&\n+          ace->_adr_type->is_aryptr()->is_flat()) {\n+        ace->_adr_type = NULL;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the NULL adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = NULL;\n+      if (m->Opcode() == Op_StoreCM) {\n+        adr_type = m->in(MemNode::OopStore)->adr_type();\n+        Node* clone = new StoreCMNode(m->in(MemNode::Control), m->in(MemNode::Memory), m->in(MemNode::Address),\n+                                      m->adr_type(), m->in(MemNode::ValueIn), m->in(MemNode::OopStore),\n+                                      get_alias_index(adr_type));\n+        igvn.register_new_node_with_optimizer(clone);\n+        igvn.replace_node(m, clone);\n+      } else {\n+        adr_type = m->adr_type();\n+#ifdef ASSERT\n+        m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      }\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flattened array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = NULL;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = NULL;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == NULL) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const Type* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat()) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flattened arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flattened array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const Type* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat()) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != NULL) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const Type* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+}\n+\n+\n@@ -2093,0 +2494,7 @@\n+  if (_inline_type_nodes.length() > 0) {\n+    \/\/ Do this once all inlining is over to avoid getting inconsistent debug info\n+    process_inline_types(igvn);\n+  }\n+\n+  adjust_flattened_array_access_aliases(igvn);\n+\n@@ -2203,0 +2611,6 @@\n+  if (_inline_type_nodes.length() > 0) {\n+    \/\/ Process inline type nodes again and remove them. From here\n+    \/\/ on we don't need to keep track of field values anymore.\n+    process_inline_types(igvn, \/* remove= *\/ true);\n+  }\n+\n@@ -2787,0 +3201,1 @@\n+\n@@ -3518,0 +3933,8 @@\n+#ifdef ASSERT\n+  case Op_InlineTypePtr:\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n+#endif\n@@ -3865,2 +4288,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -3874,1 +4297,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -3995,1 +4418,1 @@\n-  if (StressReflectiveCode) {\n+  if (StressReflectiveCode || superk == NULL || subk == NULL) {\n@@ -4004,1 +4427,2 @@\n-  if (superelem->is_array_klass())\n+  if (superelem->is_array_klass()) {\n+    ciArrayKlass* ak = superelem->as_array_klass();\n@@ -4006,0 +4430,1 @@\n+  }\n@@ -4464,0 +4889,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == NULL || tb == NULL ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are NULL.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(NULL, a));\n+    b = phase->transform(new CastP2XNode(NULL, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":498,"deletions":52,"binary":false,"changes":550,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+class CallNode;\n@@ -88,0 +89,1 @@\n+class InlineTypeBaseNode;\n@@ -304,0 +306,2 @@\n+  bool                  _has_flattened_accesses; \/\/ Any known flattened array accesses?\n+  bool                  _flattened_accesses_share_alias; \/\/ Initially all flattened array share a single slice\n@@ -318,0 +322,1 @@\n+  GrowableArray<Node*>  _inline_type_nodes;     \/\/ List of InlineType nodes\n@@ -595,0 +600,7 @@\n+  void          set_flattened_accesses()         { _has_flattened_accesses = true; }\n+  bool          flattened_accesses_share_alias() const { return _flattened_accesses_share_alias; }\n+  void          set_flattened_accesses_share_alias(bool z) { _flattened_accesses_share_alias = z; }\n+\n+  \/\/ Support for scalarized inline type calling convention\n+  bool              has_scalarized_args() const  { return _method != NULL && _method->has_scalarized_args(); }\n+  bool              needs_stack_repair()  const  { return _method != NULL && _method->get_Method()->c2_needs_stack_repair(); }\n@@ -698,0 +710,7 @@\n+  \/\/ Keep track of inline type nodes for later processing\n+  void add_inline_type(Node* n);\n+  void remove_inline_type(Node* n);\n+  void process_inline_types(PhaseIterGVN &igvn, bool remove = false);\n+\n+  void adjust_flattened_array_access_aliases(PhaseIterGVN& igvn);\n+\n@@ -838,1 +857,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = NULL) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = NULL, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }\n@@ -842,1 +861,1 @@\n-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }\n+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, NULL, uncached)->index(); }\n@@ -1075,1 +1094,1 @@\n-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);\n+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);\n@@ -1147,1 +1166,3 @@\n-  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  Node* optimize_acmp(PhaseGVN* phase, Node* a, Node* b);\n+\n+  \/\/ Auxiliary method for randomized fuzzing\/stressing\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":25,"deletions":4,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -41,0 +43,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -44,0 +47,1 @@\n+#include \"opto\/narrowptrnode.hpp\"\n@@ -57,1 +61,1 @@\n-GraphKit::GraphKit(JVMState* jvms)\n+GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)\n@@ -60,1 +64,1 @@\n-    _gvn(*C->initial_gvn()),\n+    _gvn((gvn != NULL) ? *gvn : *C->initial_gvn()),\n@@ -63,0 +67,1 @@\n+  assert(gvn == NULL || !gvn->is_IterGVN() || gvn->is_IterGVN()->delay_transform(), \"delay transform should be enabled\");\n@@ -66,0 +71,7 @@\n+#ifdef ASSERT\n+  if (_gvn.is_IterGVN() != NULL) {\n+    assert(_gvn.is_IterGVN()->delay_transform(), \"Transformation must be delayed if IterGVN is used\");\n+    \/\/ Save the initial size of _for_igvn worklist for verification (see ~GraphKit)\n+    _worklist_size = _gvn.C->for_igvn()->size();\n+  }\n+#endif\n@@ -834,1 +846,1 @@\n-           (is_anewarray && code == Bytecodes::_multianewarray);\n+           (is_anewarray && (code == Bytecodes::_multianewarray));\n@@ -842,1 +854,1 @@\n-  } else\n+  } else {\n@@ -844,0 +856,1 @@\n+  }\n@@ -1087,0 +1100,9 @@\n+  case Bytecodes::_withfield: {\n+    bool ignored_will_link;\n+    ciField* field = method()->get_field_at_bci(bci(), ignored_will_link);\n+    int      size  = field->type()->size();\n+    inputs = size+1;\n+    depth = rsize - inputs;\n+    break;\n+  }\n+\n@@ -1169,1 +1191,1 @@\n-  return _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+  return _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeKlassPtr::OBJECT));\n@@ -1212,0 +1234,1 @@\n+    case T_INLINE_TYPE : \/\/ fall through\n@@ -1383,0 +1406,16 @@\n+Node* GraphKit::null2default(Node* value, ciInlineKlass* vk) {\n+  assert(!vk->is_scalarizable(), \"Should only be used for non scalarizable inline klasses\");\n+  Node* null_ctl = top();\n+  value = null_check_oop(value, &null_ctl);\n+  if (!null_ctl->is_top()) {\n+    \/\/ Return default value if oop is null\n+    Node* region = new RegionNode(3);\n+    region->init_req(1, control());\n+    region->init_req(2, null_ctl);\n+    value = PhiNode::make(region, value, TypeInstPtr::make(TypePtr::BotPTR, vk));\n+    value->set_req(2, InlineTypeNode::default_oop(gvn(), vk));\n+    set_control(gvn().transform(region));\n+    value = gvn().transform(value);\n+  }\n+  return value;\n+}\n@@ -1387,0 +1426,3 @@\n+  if (obj->is_InlineType()) {\n+    return obj;\n+  }\n@@ -1396,0 +1438,5 @@\n+  if (t->is_inlinetypeptr() && t->inline_klass()->is_scalarizable()) {\n+    \/\/ Scalarize inline type now that we know it's non-null\n+    cast = InlineTypeNode::make_from_oop(this, cast, t->inline_klass())->as_ptr(&gvn());\n+  }\n+\n@@ -1519,1 +1566,2 @@\n-  if (((bt == T_OBJECT) && C->do_escape_analysis()) || C->eliminate_boxing()) {\n+\n+  if (((bt == T_OBJECT || bt == T_INLINE_TYPE) && C->do_escape_analysis()) || C->eliminate_boxing()) {\n@@ -1570,1 +1618,2 @@\n-                                DecoratorSet decorators) {\n+                                DecoratorSet decorators,\n+                                bool safe_for_replace) {\n@@ -1583,0 +1632,7 @@\n+  if (val->is_InlineType()) {\n+    \/\/ Store to non-flattened field. Buffer the inline type and make sure\n+    \/\/ the store is re-executed if the allocation triggers deoptimization.\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    val = val->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n@@ -1599,1 +1655,2 @@\n-                               DecoratorSet decorators) {\n+                               DecoratorSet decorators,\n+                               Node* ctl) {\n@@ -1605,1 +1662,1 @@\n-  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr, ctl);\n@@ -1703,2 +1760,2 @@\n-void GraphKit::access_clone(Node* src, Node* dst, Node* size, bool is_array) {\n-  return _barrier_set->clone(this, src, dst, size, is_array);\n+void GraphKit::access_clone(Node* src_base, Node* dst_base, Node* countx, bool is_array) {\n+  return _barrier_set->clone(this, src_base, dst_base, countx, is_array);\n@@ -1711,0 +1768,5 @@\n+  ciKlass* arytype_klass = _gvn.type(ary)->is_aryptr()->klass();\n+  if (arytype_klass != NULL && arytype_klass->is_flat_array_klass()) {\n+    ciFlatArrayKlass* vak = arytype_klass->as_flat_array_klass();\n+    shift = vak->log2_element_size();\n+  }\n@@ -1731,0 +1793,1 @@\n+  assert(elembt != T_INLINE_TYPE, \"inline types are not supported by this method\");\n@@ -1741,6 +1804,32 @@\n-void GraphKit::set_arguments_for_java_call(CallJavaNode* call) {\n-  \/\/ Add the call arguments:\n-  uint nargs = call->method()->arg_size();\n-  for (uint i = 0; i < nargs; i++) {\n-    Node* arg = argument(i);\n-    call->init_req(i + TypeFunc::Parms, arg);\n+void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {\n+  PreserveReexecuteState preexecs(this);\n+  if (EnableValhalla) {\n+    \/\/ Make sure the call is re-executed, if buffering of inline type arguments triggers deoptimization\n+    jvms()->set_should_reexecute(true);\n+    int arg_size = method()->get_declared_signature_at_bci(bci())->arg_size_for_bc(java_bc());\n+    inc_sp(arg_size);\n+  }\n+  \/\/ Add the call arguments\n+  const TypeTuple* domain = call->tf()->domain_sig();\n+  ExtendedSignature sig_cc = ExtendedSignature(call->method()->get_sig_cc(), SigEntryFilter());\n+  uint nargs = domain->cnt();\n+  for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i < nargs; i++) {\n+    Node* arg = argument(i-TypeFunc::Parms);\n+    const Type* t = domain->field_at(i);\n+    if (call->method()->has_scalarized_args() && t->is_inlinetypeptr() && !t->maybe_null() && t->inline_klass()->can_be_passed_as_fields()) {\n+      \/\/ We don't pass inline type arguments by reference but instead pass each field of the inline type\n+      InlineTypeNode* vt = arg->as_InlineType();\n+      vt->pass_fields(this, call, sig_cc, idx);\n+      \/\/ If an inline type argument is passed as fields, attach the Method* to the call site\n+      \/\/ to be able to access the extended signature later via attached_method_before_pc().\n+      \/\/ For example, see CompiledMethod::preserve_callee_argument_oops().\n+      call->set_override_symbolic_info(true);\n+      continue;\n+    } else if (arg->is_InlineType()) {\n+      \/\/ Pass inline type argument via oop to callee\n+      arg = arg->as_InlineType()->buffer(this);\n+      if (!is_late_inline) {\n+        arg = arg->as_InlineTypePtr()->get_oop();\n+      }\n+    }\n+    call->init_req(idx++, arg);\n@@ -1784,7 +1873,0 @@\n-  \/\/ Capture the return value, if any.\n-  Node* ret;\n-  if (call->method() == NULL ||\n-      call->method()->return_type()->basic_type() == T_VOID)\n-        ret = top();\n-  else  ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n-\n@@ -1803,0 +1885,19 @@\n+\n+  \/\/ Capture the return value, if any.\n+  Node* ret;\n+  if (call->method() == NULL || call->method()->return_type()->basic_type() == T_VOID) {\n+    ret = top();\n+  } else if (call->tf()->returns_inline_type_as_fields()) {\n+    \/\/ Return of multiple values (inline type fields): we create a\n+    \/\/ InlineType node, each field is a projection from the call.\n+    ciInlineKlass* vk = call->method()->return_type()->as_inline_klass();\n+    const Array<SigEntry>* sig_array = vk->extended_sig();\n+    GrowableArray<SigEntry> sig = GrowableArray<SigEntry>(sig_array->length());\n+    sig.appendAll(sig_array);\n+    ExtendedSignature sig_cc = ExtendedSignature(&sig, SigEntryFilter());\n+    uint base_input = TypeFunc::Parms + 1;\n+    ret = InlineTypeNode::make_from_multi(this, call, sig_cc, vk, base_input, false);\n+  } else {\n+    ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+  }\n+\n@@ -1893,2 +1994,1 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true);\n+  CallProjections* callprojs = call->extract_projections(true);\n@@ -1903,2 +2003,2 @@\n-  if (callprojs.fallthrough_catchproj != NULL) {\n-    C->gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);\n+  if (callprojs->fallthrough_catchproj != NULL) {\n+    C->gvn_replace_by(callprojs->fallthrough_catchproj, final_ctl);\n@@ -1906,1 +2006,1 @@\n-  if (callprojs.fallthrough_memproj != NULL) {\n+  if (callprojs->fallthrough_memproj != NULL) {\n@@ -1911,1 +2011,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);\n+    C->gvn_replace_by(callprojs->fallthrough_memproj,   final_mem);\n@@ -1914,2 +2014,2 @@\n-  if (callprojs.fallthrough_ioproj != NULL) {\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);\n+  if (callprojs->fallthrough_ioproj != NULL) {\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj,    final_io);\n@@ -1919,2 +2019,3 @@\n-  if (callprojs.resproj != NULL && result != NULL) {\n-    C->gvn_replace_by(callprojs.resproj, result);\n+  if (callprojs->resproj[0] != NULL && result != NULL) {\n+    assert(callprojs->nb_resproj == 1, \"unexpected number of results\");\n+    C->gvn_replace_by(callprojs->resproj[0], result);\n@@ -1925,2 +2026,2 @@\n-    if (callprojs.catchall_catchproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, C->top());\n+    if (callprojs->catchall_catchproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, C->top());\n@@ -1928,2 +2029,2 @@\n-    if (callprojs.catchall_memproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_memproj,   C->top());\n+    if (callprojs->catchall_memproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_memproj,   C->top());\n@@ -1931,2 +2032,2 @@\n-    if (callprojs.catchall_ioproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    C->top());\n+    if (callprojs->catchall_ioproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    C->top());\n@@ -1935,2 +2036,2 @@\n-    if (callprojs.exobj != NULL) {\n-      C->gvn_replace_by(callprojs.exobj, C->top());\n+    if (callprojs->exobj != NULL) {\n+      C->gvn_replace_by(callprojs->exobj, C->top());\n@@ -1947,2 +2048,2 @@\n-    if (callprojs.catchall_catchproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, ekit.control());\n+    if (callprojs->catchall_catchproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, ekit.control());\n@@ -1951,1 +2052,1 @@\n-    if (callprojs.catchall_memproj != NULL) {\n+    if (callprojs->catchall_memproj != NULL) {\n@@ -1953,1 +2054,1 @@\n-      C->gvn_replace_by(callprojs.catchall_memproj,   ex_mem);\n+      C->gvn_replace_by(callprojs->catchall_memproj,   ex_mem);\n@@ -1956,2 +2057,2 @@\n-    if (callprojs.catchall_ioproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());\n+    if (callprojs->catchall_ioproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    ekit.i_o());\n@@ -1961,2 +2062,2 @@\n-    if (callprojs.exobj != NULL) {\n-      C->gvn_replace_by(callprojs.exobj, ex_oop);\n+    if (callprojs->exobj != NULL) {\n+      C->gvn_replace_by(callprojs->exobj, ex_oop);\n@@ -1976,1 +2077,1 @@\n-  if (callprojs.fallthrough_catchproj != NULL && !final_ctl->is_top() && do_replaced_nodes) {\n+  if (callprojs->fallthrough_catchproj != NULL && !final_ctl->is_top() && do_replaced_nodes) {\n@@ -2174,1 +2275,1 @@\n-    const TypePtr* ptr = (ptr_kind == ProfileMaybeNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n+    const TypePtr* ptr = (ptr_kind != ProfileNeverNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n@@ -2197,1 +2298,1 @@\n-    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::OffsetBot, TypeOopPtr::InstanceBot, speculative);\n+    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, speculative);\n@@ -2231,2 +2332,9 @@\n-      if (!data->as_BitData()->null_seen()) {\n-        ptr_kind = ProfileNeverNull;\n+      if (java_bc() == Bytecodes::_aastore) {\n+        ciKlass* array_type = NULL;\n+        ciKlass* element_type = NULL;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+        exact_kls = element_type;\n+        ptr_kind = element_ptr;\n@@ -2234,7 +2342,11 @@\n-        assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n-        ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n-        uint i = 0;\n-        for (; i < call->row_limit(); i++) {\n-          ciKlass* receiver = call->receiver(i);\n-          if (receiver != NULL) {\n-            break;\n+        if (!data->as_BitData()->null_seen()) {\n+          ptr_kind = ProfileNeverNull;\n+        } else {\n+          assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n+          ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n+          uint i = 0;\n+          for (; i < call->row_limit(); i++) {\n+            ciKlass* receiver = call->receiver(i);\n+            if (receiver != NULL) {\n+              break;\n+            }\n@@ -2242,0 +2354,1 @@\n+          ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2243,1 +2356,0 @@\n-        ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2262,1 +2374,1 @@\n-  int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+  int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2265,1 +2377,1 @@\n-    const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+    const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2336,1 +2448,1 @@\n-    int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+    int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2338,1 +2450,1 @@\n-      const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+      const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2593,1 +2705,1 @@\n-      const Type* type = call_type->domain()->field_at(TypeFunc::Parms + vm_unfiltered_arg_pos);\n+      const Type* type = call_type->domain_sig()->field_at(TypeFunc::Parms + vm_unfiltered_arg_pos);\n@@ -2604,1 +2716,1 @@\n-  uint n_returns = call_type->range()->cnt() - TypeFunc::Parms;\n+  uint n_returns = call_type->range_sig()->cnt() - TypeFunc::Parms;\n@@ -2612,1 +2724,1 @@\n-      const Type* type = call_type->range()->field_at(TypeFunc::Parms + vm_ret_pos);\n+      const Type* type = call_type->range_sig()->field_at(TypeFunc::Parms + vm_ret_pos);\n@@ -2940,0 +3052,4 @@\n+  const Type* sub_t = _gvn.type(obj_or_subklass);\n+  if (sub_t->isa_inlinetype()) {\n+    obj_or_subklass = makecon(TypeKlassPtr::make(sub_t->inline_klass()));\n+  }\n@@ -2944,1 +3060,1 @@\n-    if (!_gvn.type(obj_or_subklass)->isa_klassptr()) {\n+    if (!sub_t->isa_klassptr()) {\n@@ -2947,1 +3063,0 @@\n-\n@@ -2953,1 +3068,0 @@\n-  const TypePtr* adr_type = TypeKlassPtr::make(TypePtr::NotNull, C->env()->Object_klass(), Type::OffsetBot);\n@@ -2963,2 +3077,13 @@\n-                                    float prob,\n-                                    Node* *casted_receiver) {\n+                                    float prob, Node* *casted_receiver) {\n+  Node* fail = top();\n+  const Type* rec_t = _gvn.type(receiver);\n+  if (rec_t->isa_inlinetype()) {\n+    if (klass->equals(rec_t->inline_klass())) {\n+      (*casted_receiver) = receiver; \/\/ Always passes\n+    } else {\n+      (*casted_receiver) = top();    \/\/ Always fails\n+      fail = control();\n+      set_control(top());\n+    }\n+    return fail;\n+  }\n@@ -2967,7 +3092,1 @@\n-  Node* want_klass = makecon(tklass);\n-  Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass) );\n-  Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );\n-  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n-  set_control( _gvn.transform( new IfTrueNode (iff) ));\n-  Node* fail = _gvn.transform( new IfFalseNode(iff) );\n-\n+  fail = type_check(recv_klass, tklass, prob);\n@@ -2980,1 +3099,7 @@\n-  (*casted_receiver) = _gvn.transform(cast);\n+  Node* res = _gvn.transform(cast);\n+  if (recv_xtype->is_inlinetypeptr() && recv_xtype->inline_klass()->is_scalarizable()) {\n+    assert(!gvn().type(res)->maybe_null(), \"receiver should never be null\");\n+    res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass());\n+  }\n+\n+  (*casted_receiver) = res;\n@@ -2986,0 +3111,11 @@\n+Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,\n+                           float prob) {\n+  Node* want_klass = makecon(tklass);\n+  Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass));\n+  Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );\n+  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n+  set_control(  _gvn.transform( new IfTrueNode (iff)));\n+  Node* fail = _gvn.transform( new IfFalseNode(iff));\n+  return fail;\n+}\n+\n@@ -3025,0 +3161,3 @@\n+    if (java_bc() == Bytecodes::_aastore) {\n+      return ((ciArrayLoadStoreData*)data->as_ArrayLoadStoreData())->element()->ptr_kind() == ProfileNeverNull;\n+    }\n@@ -3104,1 +3243,14 @@\n-  ciKlass* exact_kls = spec_klass == NULL ? profile_has_unique_klass() : spec_klass;\n+  ciKlass* exact_kls = spec_klass;\n+  if (exact_kls == NULL) {\n+    if (java_bc() == Bytecodes::_aastore) {\n+      ciKlass* array_type = NULL;\n+      ciKlass* element_type = NULL;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      exact_kls = element_type;\n+    } else {\n+      exact_kls = profile_has_unique_klass();\n+    }\n+  }\n@@ -3209,0 +3361,1 @@\n+  bool is_value = obj->is_InlineType();\n@@ -3212,1 +3365,1 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = is_value ? obj : null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n@@ -3230,7 +3383,9 @@\n-  bool known_statically = false;\n-  if (_gvn.type(superklass)->singleton()) {\n-    ciKlass* superk = _gvn.type(superklass)->is_klassptr()->klass();\n-    ciKlass* subk = _gvn.type(obj)->is_oopptr()->klass();\n-    if (subk != NULL && subk->is_loaded()) {\n-      int static_res = C->static_subtype_check(superk, subk);\n-      known_statically = (static_res == Compile::SSC_always_true || static_res == Compile::SSC_always_false);\n+  if (!is_value) {\n+    bool known_statically = false;\n+    if (_gvn.type(superklass)->singleton()) {\n+      ciKlass* superk = _gvn.type(superklass)->is_klassptr()->klass();\n+      ciKlass* subk = _gvn.type(obj)->is_oopptr()->klass();\n+      if (subk != NULL && subk->is_loaded()) {\n+        int static_res = C->static_subtype_check(superk, subk);\n+        known_statically = (static_res == Compile::SSC_always_true || static_res == Compile::SSC_always_false);\n+      }\n@@ -3238,14 +3393,17 @@\n-  }\n-  if (!known_statically) {\n-    const TypeOopPtr* obj_type = _gvn.type(obj)->is_oopptr();\n-    \/\/ We may not have profiling here or it may not help us. If we\n-    \/\/ have a speculative type use it to perform an exact cast.\n-    ciKlass* spec_obj_type = obj_type->speculative_type();\n-    if (spec_obj_type != NULL || (ProfileDynamicTypes && data != NULL)) {\n-      Node* cast_obj = maybe_cast_profiled_receiver(not_null_obj, NULL, spec_obj_type, safe_for_replace);\n-      if (stopped()) {            \/\/ Profile disagrees with this path.\n-        set_control(null_ctl);    \/\/ Null is the only remaining possibility.\n-        return intcon(0);\n-      }\n-      if (cast_obj != NULL) {\n-        not_null_obj = cast_obj;\n+    if (!known_statically) {\n+      const TypeOopPtr* obj_type = _gvn.type(obj)->is_oopptr();\n+      \/\/ We may not have profiling here or it may not help us. If we\n+      \/\/ have a speculative type use it to perform an exact cast.\n+      ciKlass* spec_obj_type = obj_type->speculative_type();\n+      if (spec_obj_type != NULL || (ProfileDynamicTypes && data != NULL)) {\n+        Node* cast_obj = maybe_cast_profiled_receiver(not_null_obj, NULL, spec_obj_type, safe_for_replace);\n+        if (stopped()) {            \/\/ Profile disagrees with this path.\n+          set_control(null_ctl);    \/\/ Null is the only remaining possibility.\n+          return intcon(0);\n+        }\n+        if (cast_obj != NULL &&\n+            \/\/ A value that's sometimes null is not something we can optimize well\n+            !(cast_obj->is_InlineType() && null_ctl != top())) {\n+          not_null_obj = cast_obj;\n+          is_value = not_null_obj->is_InlineType();\n+        }\n@@ -3275,1 +3433,1 @@\n-  if (safe_for_replace) {\n+  if (safe_for_replace && !is_value) {\n@@ -3290,2 +3448,1 @@\n-Node* GraphKit::gen_checkcast(Node *obj, Node* superklass,\n-                              Node* *failure_control) {\n+Node* GraphKit::gen_checkcast(Node *obj, Node* superklass, Node* *failure_control) {\n@@ -3293,2 +3450,6 @@\n-  const TypeKlassPtr *tk = _gvn.type(superklass)->is_klassptr();\n-  const Type *toop = TypeOopPtr::make_from_klass(tk->klass());\n+  const TypeKlassPtr* tk = _gvn.type(superklass)->is_klassptr();\n+  const TypeOopPtr* toop = TypeOopPtr::make_from_klass(tk->klass());\n+\n+  \/\/ Check if inline types are involved\n+  bool from_inline = obj->is_InlineType();\n+  bool to_inline = tk->klass()->is_inlinetype();\n@@ -3303,3 +3464,11 @@\n-    const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n-    if (objtp != NULL && objtp->klass() != NULL) {\n-      switch (C->static_subtype_check(tk->klass(), objtp->klass())) {\n+    ciKlass* klass = NULL;\n+    if (from_inline) {\n+      klass = _gvn.type(obj)->inline_klass();\n+    } else {\n+      const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n+      if (objtp != NULL) {\n+        klass = objtp->klass();\n+      }\n+    }\n+    if (klass != NULL) {\n+      switch (C->static_subtype_check(tk->klass(), klass)) {\n@@ -3310,1 +3479,10 @@\n-        return record_profiled_receiver_for_speculation(obj);\n+        if (!from_inline) {\n+          obj = record_profiled_receiver_for_speculation(obj);\n+          if (to_inline) {\n+            obj = null_check(obj);\n+            if (toop->inline_klass()->is_scalarizable()) {\n+              obj = InlineTypeNode::make_from_oop(this, obj, toop->inline_klass());\n+            }\n+          }\n+        }\n+        return obj;\n@@ -3312,3 +3490,11 @@\n-        \/\/ It needs a null check because a null will *pass* the cast check.\n-        \/\/ A non-null value will always produce an exception.\n-        return null_assert(obj);\n+        if (from_inline || to_inline) {\n+          if (!from_inline) {\n+            null_check(obj);\n+          }\n+          \/\/ Inline type is never null. Always throw an exception.\n+          builtin_throw(Deoptimization::Reason_class_check, makecon(TypeKlassPtr::make(klass)));\n+          return top();\n+        } else {\n+          \/\/ It needs a null check because a null will *pass* the cast check.\n+          return null_assert(obj);\n+        }\n@@ -3325,1 +3511,3 @@\n-    data = method()->method_data()->bci_to_data(bci());\n+    if (method()->method_data()->is_mature()) {\n+      data = method()->method_data()->bci_to_data(bci());\n+    }\n@@ -3333,0 +3521,3 @@\n+  _gvn.set_type(region, Type::CONTROL);\n+  _gvn.set_type(phi, toop);\n+\n@@ -3342,1 +3533,8 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = NULL;\n+  if (from_inline) {\n+    not_null_obj = obj;\n+  } else if (to_inline) {\n+    not_null_obj = null_check(obj);\n+  } else {\n+    not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  }\n@@ -3360,1 +3558,1 @@\n-  if (tk->klass_is_exact()) {\n+  if (!from_inline && tk->klass_is_exact()) {\n@@ -3371,0 +3569,7 @@\n+      if (cast_obj != NULL && cast_obj->is_InlineType()) {\n+        if (null_ctl != top()) {\n+          cast_obj = NULL; \/\/ A value that's sometimes null is not something we can optimize well\n+        } else {\n+          return cast_obj;\n+        }\n+      }\n@@ -3382,1 +3587,1 @@\n-    Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass );\n+    Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass);\n@@ -3385,1 +3590,1 @@\n-    cast_obj = _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));\n+    cast_obj = from_inline ? not_null_obj : _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));\n@@ -3391,1 +3596,7 @@\n-        builtin_throw(Deoptimization::Reason_class_check, load_object_klass(not_null_obj));\n+        Node* obj_klass = NULL;\n+        if (from_inline) {\n+          obj_klass = makecon(TypeKlassPtr::make(_gvn.type(not_null_obj)->inline_klass()));\n+        } else {\n+          obj_klass = load_object_klass(not_null_obj);\n+        }\n+        builtin_throw(Deoptimization::Reason_class_check, obj_klass);\n@@ -3418,1 +3629,114 @@\n-  return record_profiled_receiver_for_speculation(res);\n+  bool not_inline = !toop->can_be_inline_type();\n+  bool not_flattened = !UseFlatArray || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->flatten_array());\n+  if (EnableValhalla && not_flattened) {\n+    \/\/ Check if obj has been loaded from an array\n+    obj = obj->isa_DecodeN() ? obj->in(1) : obj;\n+    Node* array = NULL;\n+    if (obj->isa_Load()) {\n+      Node* address = obj->in(MemNode::Address);\n+      if (address->isa_AddP()) {\n+        array = address->as_AddP()->in(AddPNode::Base);\n+      }\n+    } else if (obj->is_Phi()) {\n+      Node* region = obj->in(0);\n+      \/\/ TODO make this more robust (see JDK-8231346)\n+      if (region->req() == 3 && region->in(2) != NULL && region->in(2)->in(0) != NULL) {\n+        IfNode* iff = region->in(2)->in(0)->isa_If();\n+        if (iff != NULL) {\n+          iff->is_flat_array_check(&_gvn, &array);\n+        }\n+      }\n+    }\n+    if (array != NULL) {\n+      const TypeAryPtr* ary_t = _gvn.type(array)->isa_aryptr();\n+      if (ary_t != NULL) {\n+        if (!ary_t->is_not_null_free() && not_inline) {\n+          \/\/ Casting array element to a non-inline-type, mark array as not null-free.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_null_free()));\n+          replace_in_map(array, cast);\n+        } else if (!ary_t->is_not_flat()) {\n+          \/\/ Casting array element to a non-flattened type, mark array as not flat.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_flat()));\n+          replace_in_map(array, cast);\n+        }\n+      }\n+    }\n+  }\n+\n+  if (!from_inline) {\n+    res = record_profiled_receiver_for_speculation(res);\n+    if (to_inline && toop->inline_klass()->is_scalarizable()) {\n+      assert(!gvn().type(res)->maybe_null(), \"Inline types are null-free\");\n+      res = InlineTypeNode::make_from_oop(this, res, toop->inline_klass());\n+    }\n+  }\n+  return res;\n+}\n+\n+Node* GraphKit::inline_type_test(Node* obj, bool is_inline) {\n+  Node* mark_adr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+  Node* mark = make_load(NULL, mark_adr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  Node* mask = MakeConX(markWord::inline_type_pattern);\n+  Node* masked = _gvn.transform(new AndXNode(mark, mask));\n+  Node* cmp = _gvn.transform(new CmpXNode(masked, mask));\n+  return _gvn.transform(new BoolNode(cmp, is_inline ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::array_lh_test(Node* klass, jint mask, jint val, bool eq) {\n+  Node* lh_adr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+  \/\/ Make sure to use immutable memory here to enable hoisting the check out of loops\n+  Node* lh_val = _gvn.transform(LoadNode::make(_gvn, NULL, immutable_memory(), lh_adr, lh_adr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+  Node* masked = _gvn.transform(new AndINode(lh_val, intcon(mask)));\n+  Node* cmp = _gvn.transform(new CmpINode(masked, intcon(val)));\n+  return _gvn.transform(new BoolNode(cmp, eq ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::flat_array_test(Node* ary, bool flat) {\n+  \/\/ We can't use immutable memory here because the mark word is mutable.\n+  \/\/ PhaseIdealLoop::move_flat_array_check_out_of_loop will make sure the\n+  \/\/ check is moved out of loops (mainly to enable loop unswitching).\n+  Node* mem = UseArrayMarkWordCheck ? memory(Compile::AliasIdxRaw) : immutable_memory();\n+  Node* cmp = _gvn.transform(new FlatArrayCheckNode(C, mem, ary));\n+  record_for_igvn(cmp); \/\/ Give it a chance to be optimized out by IGVN\n+  return _gvn.transform(new BoolNode(cmp, flat ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::null_free_array_test(Node* klass, bool null_free) {\n+  return array_lh_test(klass, Klass::_lh_null_free_bit_inplace, 0, !null_free);\n+}\n+\n+\/\/ Deoptimize if 'ary' is a null-free inline type array and 'val' is null\n+Node* GraphKit::inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {\n+  const Type* val_t = _gvn.type(val);\n+  if (val->is_InlineType() || !TypePtr::NULL_PTR->higher_equal(val_t)) {\n+    return ary; \/\/ Never null\n+  }\n+  RegionNode* region = new RegionNode(3);\n+  Node* null_ctl = top();\n+  null_check_oop(val, &null_ctl);\n+  if (null_ctl != top()) {\n+    PreserveJVMState pjvms(this);\n+    set_control(null_ctl);\n+    {\n+      \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(load_object_klass(ary), \/* null_free = *\/ false), PROB_MAX);\n+      inc_sp(nargs);\n+      uncommon_trap(Deoptimization::Reason_null_check,\n+                    Deoptimization::Action_none);\n+    }\n+    region->init_req(1, control());\n+  }\n+  region->init_req(2, control());\n+  set_control(_gvn.transform(region));\n+  record_for_igvn(region);\n+  const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+  if (val_t == TypePtr::NULL_PTR && !ary_t->is_not_null_free()) {\n+    \/\/ Since we were just successfully storing null, the array can't be null free.\n+    ary_t = ary_t->cast_to_not_null_free();\n+    Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+    if (safe_for_replace) {\n+      replace_in_map(ary, cast);\n+    }\n+    ary = cast;\n+  }\n+  return ary;\n@@ -3486,0 +3810,1 @@\n+\n@@ -3558,0 +3883,1 @@\n+  assert(!obj->is_InlineTypeBase(), \"should not unlock on inline type\");\n@@ -3598,0 +3924,1 @@\n+    assert(klass != NULL, \"klass should not be NULL\");\n@@ -3599,1 +3926,6 @@\n-    if (xklass || klass->is_array_klass()) {\n+    bool can_be_flattened = false;\n+    if (UseFlatArray && klass->is_obj_array_klass()) {\n+      ciKlass* elem = klass->as_obj_array_klass()->element_klass();\n+      can_be_flattened = elem->can_be_inline_klass() && (!elem->is_inlinetype() || elem->flatten_array());\n+    }\n+    if (xklass || (klass->is_array_klass() && !can_be_flattened)) {\n@@ -3661,0 +3993,1 @@\n+    _gvn.set_type(minit_in, Type::MEMORY);\n@@ -3668,3 +4001,26 @@\n-      const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n-      int            elemidx  = C->get_alias_index(telemref);\n-      hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      const TypeAryPtr* arytype = oop_type->is_aryptr();\n+      if (arytype->klass()->is_flat_array_klass()) {\n+        \/\/ Initially all flattened array accesses share a single slice\n+        \/\/ but that changes after parsing. Prepare the memory graph so\n+        \/\/ it can optimize flattened array accesses properly once they\n+        \/\/ don't share a single slice.\n+        assert(C->flattened_accesses_share_alias(), \"should be set at parse time\");\n+        C->set_flattened_accesses_share_alias(false);\n+        ciFlatArrayKlass* vak = arytype->klass()->as_flat_array_klass();\n+        ciInlineKlass* vk = vak->element_klass()->as_inline_klass();\n+        for (int i = 0, len = vk->nof_nonstatic_fields(); i < len; i++) {\n+          ciField* field = vk->nonstatic_field_at(i);\n+          if (field->offset() >= TrackedInitializationLimit * HeapWordSize)\n+            continue;  \/\/ do not bother to track really large numbers of fields\n+          int off_in_vt = field->offset() - vk->first_field_offset();\n+          const TypePtr* adr_type = arytype->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+          int fieldidx = C->get_alias_index(adr_type, true);\n+          hook_memory_on_init(*this, fieldidx, minit_in, minit_out);\n+        }\n+        C->set_flattened_accesses_share_alias(true);\n+        hook_memory_on_init(*this, C->get_alias_index(TypeAryPtr::INLINES), minit_in, minit_out);\n+      } else {\n+        const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n+        int            elemidx  = C->get_alias_index(telemref);\n+        hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      }\n@@ -3672,0 +4028,1 @@\n+      set_memory(minit_out, C->get_alias_index(oop_type)); \/\/ mark word\n@@ -3722,1 +4079,2 @@\n-                             bool deoptimize_on_exception) {\n+                             bool deoptimize_on_exception,\n+                             InlineTypeBaseNode* inline_type_node) {\n@@ -3729,1 +4087,1 @@\n-  int   layout_is_con = (layout_val == NULL);\n+  bool  layout_is_con = (layout_val == NULL);\n@@ -3780,1 +4138,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3787,1 +4145,1 @@\n-                                         initial_slow_test);\n+                                         initial_slow_test, inline_type_node);\n@@ -3793,1 +4151,1 @@\n-\/\/ helper for both newarray and anewarray\n+\/\/ helper for newarray and anewarray\n@@ -3803,1 +4161,1 @@\n-  int   layout_is_con = (layout_val == NULL);\n+  bool  layout_is_con = (layout_val == NULL);\n@@ -3833,1 +4191,1 @@\n-    fast_size_limit <<= (LogBytesPerLong - log2_esize);\n+    fast_size_limit <<= MAX2(LogBytesPerLong - log2_esize, 0);\n@@ -3851,1 +4209,1 @@\n-    BasicType etype  = Klass::layout_helper_element_type(layout_con);\n+    bool is_flat_array = Klass::layout_helper_is_flatArray(layout_con);\n@@ -3854,1 +4212,1 @@\n-    assert((hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n+    assert(is_flat_array || (hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n@@ -3938,1 +4296,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3947,0 +4305,63 @@\n+  const TypeKlassPtr* ary_klass = _gvn.type(klass_node)->isa_klassptr();\n+  const TypeOopPtr* ary_type = ary_klass->as_instance_type();\n+  const TypeAryPtr* ary_ptr = ary_type->isa_aryptr();\n+\n+  \/\/ Inline type array variants:\n+  \/\/ - null-ok:              MyValue.ref[] (ciObjArrayKlass \"[LMyValue$ref\")\n+  \/\/ - null-free:            MyValue.val[] (ciObjArrayKlass \"[QMyValue$val\")\n+  \/\/ - null-free, flattened: MyValue.val[] (ciFlatArrayKlass \"[QMyValue$val\")\n+  \/\/ Check if array is a null-free, non-flattened inline type array\n+  \/\/ that needs to be initialized with the default inline type.\n+  Node* default_value = NULL;\n+  Node* raw_default_value = NULL;\n+  if (ary_ptr != NULL && ary_ptr->klass_is_exact()) {\n+    \/\/ Array type is known\n+    ciKlass* elem_klass = ary_ptr->klass()->as_array_klass()->element_klass();\n+    if (elem_klass != NULL && elem_klass->is_inlinetype()) {\n+      ciInlineKlass* vk = elem_klass->as_inline_klass();\n+      if (!vk->flatten_array()) {\n+        default_value = InlineTypeNode::default_oop(gvn(), vk);\n+      }\n+    }\n+  } else if (ary_klass->klass()->can_be_inline_array_klass()) {\n+    \/\/ Array type is not known, add runtime checks\n+    assert(!ary_klass->klass_is_exact(), \"unexpected exact type\");\n+    Node* r = new RegionNode(3);\n+    default_value = new PhiNode(r, TypeInstPtr::BOTTOM);\n+\n+    Node* bol = array_lh_test(klass_node, Klass::_lh_array_tag_vt_value_bit_inplace | Klass::_lh_null_free_bit_inplace, Klass::_lh_null_free_bit_inplace);\n+    IfNode* iff = create_and_map_if(control(), bol, PROB_FAIR, COUNT_UNKNOWN);\n+\n+    \/\/ Null-free, non-flattened inline type array, initialize with the default value\n+    set_control(_gvn.transform(new IfTrueNode(iff)));\n+    Node* p = basic_plus_adr(klass_node, in_bytes(ArrayKlass::element_klass_offset()));\n+    Node* eklass = _gvn.transform(LoadKlassNode::make(_gvn, control(), immutable_memory(), p, TypeInstPtr::KLASS));\n+    Node* adr_fixed_block_addr = basic_plus_adr(eklass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    Node* adr_fixed_block = make_load(control(), adr_fixed_block_addr, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+    Node* default_value_offset_addr = basic_plus_adr(adr_fixed_block, in_bytes(InlineKlass::default_value_offset_offset()));\n+    Node* default_value_offset = make_load(control(), default_value_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+    Node* elem_mirror = load_mirror_from_klass(eklass);\n+    Node* default_value_addr = basic_plus_adr(elem_mirror, ConvI2X(default_value_offset));\n+    Node* val = access_load_at(elem_mirror, default_value_addr, _gvn.type(default_value_addr)->is_ptr(), TypeInstPtr::BOTTOM, T_OBJECT, IN_HEAP);\n+    r->init_req(1, control());\n+    default_value->init_req(1, val);\n+\n+    \/\/ Otherwise initialize with all zero\n+    r->init_req(2, _gvn.transform(new IfFalseNode(iff)));\n+    default_value->init_req(2, null());\n+\n+    set_control(_gvn.transform(r));\n+    default_value = _gvn.transform(default_value);\n+  }\n+  if (default_value != NULL) {\n+    if (UseCompressedOops) {\n+      \/\/ With compressed oops, the 64-bit init value is built from two 32-bit compressed oops\n+      default_value = _gvn.transform(new EncodePNode(default_value, default_value->bottom_type()->make_narrowoop()));\n+      Node* lower = _gvn.transform(new CastP2XNode(control(), default_value));\n+      Node* upper = _gvn.transform(new LShiftLNode(lower, intcon(32)));\n+      raw_default_value = _gvn.transform(new OrLNode(lower, upper));\n+    } else {\n+      raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));\n+    }\n+  }\n+\n@@ -3948,6 +4369,6 @@\n-  AllocateArrayNode* alloc\n-    = new AllocateArrayNode(C, AllocateArrayNode::alloc_type(TypeInt::INT),\n-                            control(), mem, i_o(),\n-                            size, klass_node,\n-                            initial_slow_test,\n-                            length);\n+  AllocateArrayNode* alloc = new AllocateArrayNode(C, AllocateArrayNode::alloc_type(TypeInt::INT),\n+                                                   control(), mem, i_o(),\n+                                                   size, klass_node,\n+                                                   initial_slow_test,\n+                                                   length, default_value,\n+                                                   raw_default_value);\n@@ -3960,1 +4381,0 @@\n-  const TypeOopPtr* ary_type = _gvn.type(klass_node)->is_klassptr()->as_instance_type();\n@@ -4118,1 +4538,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4121,2 +4541,2 @@\n-                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS),\n-                                                  ciTypeArrayKlass::make(T_BYTE), true, 0);\n+                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, true, true),\n+                                                  ciTypeArrayKlass::make(T_BYTE), true, Type::Offset(0));\n@@ -4135,1 +4555,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4147,1 +4567,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4157,1 +4577,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4268,1 +4688,8 @@\n-    return makecon(con_type);\n+    Node* con = makecon(con_type);\n+    assert(!field->type()->is_inlinetype() || (field->is_static() && !con_type->is_zero_type()), \"sanity\");\n+    \/\/ Check type of constant which might be more precise\n+    if (con_type->is_inlinetypeptr() && con_type->inline_klass()->is_scalarizable()) {\n+      \/\/ Load inline type from constant oop\n+      con = InlineTypeNode::make_from_oop(this, con, con_type->inline_klass());\n+    }\n+    return con;\n@@ -4272,0 +4699,9 @@\n+\n+\/\/---------------------------load_mirror_from_klass----------------------------\n+\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n+Node* GraphKit::load_mirror_from_klass(Node* klass) {\n+  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n+  Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+  \/\/ mirror = ((OopHandle)mirror)->resolve();\n+  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n+}\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":589,"deletions":153,"binary":false,"changes":742,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -69,0 +70,3 @@\n+#ifdef ASSERT\n+  uint              _worklist_size;\n+#endif\n@@ -81,1 +85,1 @@\n-  GraphKit(JVMState* jvms);     \/\/ the JVM state on which to operate\n+  GraphKit(JVMState* jvms, PhaseGVN* gvn = NULL);     \/\/ the JVM state on which to operate\n@@ -86,0 +90,5 @@\n+    \/\/ During incremental inlining, the Node_Array of the C->for_igvn() worklist and the IGVN\n+    \/\/ worklist are shared but the _in_worklist VectorSet is not. To avoid inconsistencies,\n+    \/\/ we should not add nodes to the _for_igvn worklist when using IGVN for the GraphKit.\n+    assert((_gvn.is_IterGVN() == NULL) || (_gvn.C->for_igvn()->size() == _worklist_size),\n+           \"GraphKit should not modify _for_igvn worklist after parsing\");\n@@ -96,1 +105,1 @@\n-  void record_for_igvn(Node* n) const { C->record_for_igvn(n); }  \/\/ delegate to Compile\n+  void record_for_igvn(Node* n) const { _gvn.record_for_igvn(n); }\n@@ -379,0 +388,2 @@\n+  Node* null2default(Node* value, ciInlineKlass* vk = NULL);\n+\n@@ -595,1 +606,2 @@\n-                        DecoratorSet decorators);\n+                        DecoratorSet decorators,\n+                        bool safe_for_replace = true);\n@@ -602,1 +614,2 @@\n-                       DecoratorSet decorators);\n+                       DecoratorSet decorators,\n+                       Node* ctl = NULL);\n@@ -647,1 +660,1 @@\n-  void access_clone(Node* src, Node* dst, Node* size, bool is_array);\n+  void access_clone(Node* src_base, Node* dst_base, Node* countx, bool is_array);\n@@ -680,1 +693,1 @@\n-  Node* null_check_receiver_before_call(ciMethod* callee) {\n+  Node* null_check_receiver_before_call(ciMethod* callee, bool replace_value = true) {\n@@ -682,0 +695,3 @@\n+    if (argument(0)->is_InlineType()) {\n+      return argument(0);\n+    }\n@@ -689,0 +705,13 @@\n+    \/\/ Scalarize inline type receiver\n+    const Type* recv_type = gvn().type(n);\n+    if (recv_type->is_inlinetypeptr() && recv_type->inline_klass()->is_scalarizable()) {\n+      assert(!recv_type->maybe_null(), \"should never be null\");\n+      InlineTypeNode* vt = InlineTypeNode::make_from_oop(this, n, recv_type->inline_klass());\n+      set_argument(0, vt);\n+      if (replace_value && !Compile::current()->inlining_incrementally()) {\n+        \/\/ Only replace in map if we are not incrementally inlining because we\n+        \/\/ share a map with the caller which might expect the inline type as oop.\n+        replace_in_map(n, vt);\n+      }\n+      n = vt;\n+    }\n@@ -694,1 +723,1 @@\n-  void  set_arguments_for_java_call(CallJavaNode* call);\n+  void  set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline = false);\n@@ -838,2 +867,8 @@\n-  Node* gen_checkcast( Node *subobj, Node* superkls,\n-                       Node* *failure_control = NULL );\n+  Node* gen_checkcast(Node *subobj, Node* superkls, Node* *failure_control = NULL);\n+\n+  \/\/ Inline types\n+  Node* inline_type_test(Node* obj, bool is_inline = true);\n+  Node* array_lh_test(Node* kls, jint mask, jint val, bool eq = true);\n+  Node* flat_array_test(Node* ary, bool flat = true);\n+  Node* null_free_array_test(Node* klass, bool null_free = true);\n+  Node* inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace = false);\n@@ -848,0 +883,1 @@\n+  Node* type_check(Node* recv_klass, const TypeKlassPtr* tklass, float prob);\n@@ -861,1 +897,2 @@\n-                     bool deoptimize_on_exception = false);\n+                     bool deoptimize_on_exception = false,\n+                     InlineTypeBaseNode* inline_type_node = NULL);\n@@ -898,0 +935,1 @@\n+  Node* load_mirror_from_klass(Node* klass);\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":48,"deletions":10,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -1183,0 +1183,17 @@\n+\/\/ Returns true if this IfNode belongs to a flat array check\n+\/\/ and returns the corresponding array in the 'array' parameter.\n+bool IfNode::is_flat_array_check(PhaseTransform* phase, Node** array) {\n+  Node* bol = in(1);\n+  if (!bol->is_Bool()) {\n+    return false;\n+  }\n+  Node* cmp = bol->in(1);\n+  if (cmp->isa_FlatArrayCheck()) {\n+    if (array != NULL) {\n+      *array = cmp->in(FlatArrayCheckNode::Array);\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -276,1 +276,1 @@\n-        if (offset == Type::OffsetBot || tptr->_offset == Type::OffsetBot)\n+        if (offset == Type::OffsetBot || tptr->offset() == Type::OffsetBot)\n@@ -278,1 +278,1 @@\n-        offset += tptr->_offset; \/\/ correct if base is offseted\n+        offset += tptr->offset(); \/\/ correct if base is offseted\n@@ -315,1 +315,5 @@\n-      Block *inb = get_block_for_node(mach->in(j));\n+      Block* inb = get_block_for_node(mach->in(j));\n+      if (mach->in(j)->is_Con() && inb == get_block_for_node(mach)) {\n+        \/\/ Ignore constant loads scheduled in the same block (we can simply hoist them as well)\n+        continue;\n+      }\n@@ -391,0 +395,20 @@\n+  } else {\n+    \/\/ Hoist constant load inputs as well.\n+    for (uint i = 1; i < best->req(); ++i) {\n+      Node* n = best->in(i);\n+      if (n->is_Con() && get_block_for_node(n) == get_block_for_node(best)) {\n+        get_block_for_node(n)->find_remove(n);\n+        block->add_inst(n);\n+        map_node_to_block(n, block);\n+        \/\/ Constant loads may kill flags (for example, when XORing a register).\n+        \/\/ Check for flag-killing projections that also need to be hoisted.\n+        for (DUIterator_Fast jmax, j = n->fast_outs(jmax); j < jmax; j++) {\n+          Node* proj = n->fast_out(j);\n+          if (proj->is_MachProj()) {\n+            get_block_for_node(proj)->find_remove(proj);\n+            block->add_inst(proj);\n+            map_node_to_block(proj, block);\n+          }\n+        }\n+      }\n+    }\n@@ -392,0 +416,1 @@\n+\n@@ -845,1 +870,1 @@\n-  uint r_cnt = mcall->tf()->range()->cnt();\n+  uint r_cnt = mcall->tf()->range_cc()->cnt();\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":29,"deletions":4,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -108,3 +109,11 @@\n-    if (!stopped() && result() != NULL) {\n-      BasicType bt = result()->bottom_type()->basic_type();\n-      push_node(bt, result());\n+    Node* res = result();\n+    if (!stopped() && res != NULL) {\n+      BasicType bt = res->bottom_type()->basic_type();\n+      if (C->inlining_incrementally() && res->is_InlineType()) {\n+        \/\/ The caller expects an oop when incrementally inlining an intrinsic that returns an\n+        \/\/ inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        res = res->as_InlineType()->buffer(this);\n+      }\n+      push_node(bt, res);\n@@ -138,1 +147,0 @@\n-  Node* load_mirror_from_klass(Node* klass);\n@@ -160,0 +168,12 @@\n+  Node* generate_value_guard(Node* kls, RegionNode* region);\n+\n+  enum ArrayKind {\n+    AnyArray,\n+    NonArray,\n+    ObjectArray,\n+    NonObjectArray,\n+    TypeArray,\n+    FlatArray,\n+    NonFlatArray\n+  };\n+\n@@ -161,0 +181,1 @@\n+\n@@ -162,1 +183,1 @@\n-    return generate_array_guard_common(kls, region, false, false);\n+    return generate_array_guard_common(kls, region, AnyArray);\n@@ -165,1 +186,1 @@\n-    return generate_array_guard_common(kls, region, false, true);\n+    return generate_array_guard_common(kls, region, NonArray);\n@@ -168,1 +189,1 @@\n-    return generate_array_guard_common(kls, region, true, false);\n+    return generate_array_guard_common(kls, region, ObjectArray);\n@@ -171,1 +192,12 @@\n-    return generate_array_guard_common(kls, region, true, true);\n+    return generate_array_guard_common(kls, region, NonObjectArray);\n+  }\n+  Node* generate_typeArray_guard(Node* kls, RegionNode* region) {\n+    return generate_array_guard_common(kls, region, TypeArray);\n+  }\n+  Node* generate_flatArray_guard(Node* kls, RegionNode* region) {\n+    assert(UseFlatArray, \"can never be flattened\");\n+    return generate_array_guard_common(kls, region, FlatArray);\n+  }\n+  Node* generate_non_flatArray_guard(Node* kls, RegionNode* region) {\n+    assert(UseFlatArray, \"can never be flattened\");\n+    return generate_array_guard_common(kls, region, NonFlatArray);\n@@ -173,2 +205,1 @@\n-  Node* generate_array_guard_common(Node* kls, RegionNode* region,\n-                                    bool obj_array, bool not_array);\n+  Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);\n@@ -232,0 +263,2 @@\n+  bool inline_unsafe_make_private_buffer();\n+  bool inline_unsafe_finish_private_buffer();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":43,"deletions":10,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -1855,1 +1855,2 @@\n-      assert(outer->outcnt() >= phis + 2 && outer->outcnt() <= phis + 2 + stores + 1, \"only phis\");\n+      \/\/ TODO disabled until JDK-8255120 is fixed\n+      \/\/ assert(outer->outcnt() >= phis + 2 && outer->outcnt() <= phis + 2 + stores + 1, \"only phis\");\n","filename":"src\/hotspot\/share\/opto\/loopnode.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -386,0 +386,16 @@\n+  if (tp->isa_aryptr()) {\n+    \/\/ In the case of a flattened inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    if (offset == Type::OffsetBot) {\n+      Node* base;\n+      Node* index;\n+      const MachOper* oper = memory_inputs(base, index);\n+      if (oper != (MachOper*)-1) {\n+        offset = oper->constant_disp();\n+        return tp->is_aryptr()->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot);\n+      }\n+    }\n+    return tp->is_aryptr()->add_field_offset_and_offset(offset);\n+  }\n+\n@@ -672,2 +688,2 @@\n-const Type *MachCallNode::bottom_type() const { return tf()->range(); }\n-const Type* MachCallNode::Value(PhaseGVN* phase) const { return tf()->range(); }\n+const Type *MachCallNode::bottom_type() const { return tf()->range_cc(); }\n+const Type* MachCallNode::Value(PhaseGVN* phase) const { return tf()->range_cc(); }\n@@ -685,1 +701,1 @@\n-  if (tf()->range()->cnt() == TypeFunc::Parms) {\n+  if (tf()->range_sig()->cnt() == TypeFunc::Parms) {\n@@ -690,0 +706,2 @@\n+  assert(tf()->returns_inline_type_as_fields(), \"multiple return values not supported\");\n+\n@@ -705,1 +723,1 @@\n-  const TypeTuple *r = tf()->range();\n+  const TypeTuple *r = tf()->range_sig();\n@@ -710,0 +728,4 @@\n+bool MachCallNode::returns_vt() const {\n+  return tf()->returns_inline_type_as_fields();\n+}\n+\n@@ -714,1 +736,6 @@\n-  if (idx < tf()->domain()->cnt()) {\n+  if (entry_point() == NULL && idx == TypeFunc::Parms) {\n+    \/\/ Null entry point is a special cast where the target of the call\n+    \/\/ is in a register.\n+    return MachNode::in_RegMask(idx);\n+  }\n+  if (idx < tf()->domain_sig()->cnt()) {\n@@ -747,1 +774,1 @@\n-  if (idx < tf()->domain()->cnt()) {\n+  if (idx < tf()->domain_cc()->cnt()) {\n","filename":"src\/hotspot\/share\/opto\/machnode.cpp","additions":33,"deletions":6,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+class MachVEPNode;\n@@ -483,0 +484,30 @@\n+\/\/------------------------------MachVEPNode-----------------------------------\n+\/\/ Machine Inline Type Entry Point Node\n+class MachVEPNode : public MachIdealNode {\n+public:\n+  Label* _verified_entry;\n+\n+  MachVEPNode(Label* verified_entry, bool verified, bool receiver_only) :\n+    _verified_entry(verified_entry),\n+    _verified(verified),\n+    _receiver_only(receiver_only) {\n+    init_class_id(Class_MachVEP);\n+  }\n+  virtual bool cmp(const Node &n) const {\n+    return (_verified_entry == ((MachVEPNode&)n)._verified_entry) &&\n+           (_verified == ((MachVEPNode&)n)._verified) &&\n+           (_receiver_only == ((MachVEPNode&)n)._receiver_only) &&\n+           MachIdealNode::cmp(n);\n+  }\n+  virtual uint size_of() const { return sizeof(*this); }\n+  virtual void emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const;\n+\n+#ifndef PRODUCT\n+  virtual const char* Name() const { return \"InlineType Entry-Point\"; }\n+  virtual void format(PhaseRegAlloc*, outputStream* st) const;\n+#endif\n+private:\n+  bool   _verified;\n+  bool   _receiver_only;\n+};\n+\n@@ -489,1 +520,0 @@\n-  virtual uint size(PhaseRegAlloc *ra_) const;\n@@ -501,1 +531,9 @@\n-  MachPrologNode( ) {}\n+  Label* _verified_entry;\n+\n+  MachPrologNode(Label* verified_entry) : _verified_entry(verified_entry) {\n+    init_class_id(Class_MachProlog);\n+  }\n+  virtual bool cmp(const Node &n) const {\n+    return (_verified_entry == ((MachPrologNode&)n)._verified_entry) && MachIdealNode::cmp(n);\n+  }\n+  virtual uint size_of() const { return sizeof(*this); }\n@@ -503,1 +541,0 @@\n-  virtual uint size(PhaseRegAlloc *ra_) const;\n@@ -518,1 +555,0 @@\n-  virtual uint size(PhaseRegAlloc *ra_) const;\n@@ -911,0 +947,1 @@\n+  bool returns_vt() const;\n","filename":"src\/hotspot\/share\/opto\/machnode.hpp","additions":41,"deletions":4,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -86,12 +88,0 @@\n-void PhaseMacroExpand::migrate_outs(Node *old, Node *target) {\n-  assert(old != NULL, \"sanity\");\n-  for (DUIterator_Fast imax, i = old->fast_outs(imax); i < imax; i++) {\n-    Node* use = old->fast_out(i);\n-    _igvn.rehash_node_delayed(use);\n-    imax -= replace_input(use, old, target);\n-    \/\/ back up iterator\n-    --i;\n-  }\n-  assert(old->outcnt() == 0, \"all uses must be deleted\");\n-}\n-\n@@ -215,1 +205,1 @@\n-  bs->eliminate_gc_barrier(this, p2x);\n+  bs->eliminate_gc_barrier(&_igvn, p2x);\n@@ -260,1 +250,1 @@\n-        int adr_offset = atype->offset();\n+        int adr_offset = atype->flattened_offset();\n@@ -303,1 +293,1 @@\n-   } else if (mem->Opcode() == Op_StrInflatedCopy) {\n+    } else if (mem->Opcode() == Op_StrInflatedCopy) {\n@@ -348,1 +338,7 @@\n-      const TypePtr* adr_type = NULL;\n+      Node* base = ac->in(ArrayCopyNode::Src);\n+      const TypePtr* adr_type = _igvn.type(base)->is_ptr();\n+      assert(adr_type->isa_aryptr(), \"only arrays here\");\n+      if (adr_type->is_aryptr()->is_flat()) {\n+        ciFlatArrayKlass* vak = adr_type->is_aryptr()->klass()->as_flat_array_klass();\n+        shift = vak->log2_element_size();\n+      }\n@@ -351,2 +347,2 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(off);\n+        adr_type = _igvn.type(adr)->is_ptr();\n+        assert(adr_type == _igvn.type(base)->is_aryptr()->add_field_offset_and_offset(off), \"incorrect address type\");\n@@ -359,0 +355,5 @@\n+        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n+          \/\/ Non constant offset in the array: we can't statically\n+          \/\/ determine the value\n+          return NULL;\n+        }\n@@ -366,7 +367,5 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(Type::OffsetBot);\n-        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n-          \/\/ Non constant offset in the array: we can't statically\n-          \/\/ determine the value\n-          return NULL;\n-        }\n+        \/\/ In the case of a flattened inline type array, each field has its\n+        \/\/ own slice so we need to extract the field being accessed from\n+        \/\/ the address computation\n+        adr_type = adr_type->is_aryptr()->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot);\n+        adr = _igvn.transform(new CastPPNode(adr, adr_type));\n@@ -383,0 +382,1 @@\n+      assert(res->isa_DecodeN(), \"should be narrow oop\");\n@@ -398,1 +398,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flattened_offset();\n@@ -437,1 +437,7 @@\n-        values.at_put(j, _igvn.zerocon(ft));\n+        Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+        if (default_value != NULL) {\n+          values.at_put(j, default_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawDefaultValue) == NULL, \"default value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -454,1 +460,7 @@\n-        values.at_put(j, _igvn.zerocon(ft));\n+        Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+        if (default_value != NULL) {\n+          values.at_put(j, default_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawDefaultValue) == NULL, \"default value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -500,1 +512,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flattened_offset();\n@@ -502,1 +514,0 @@\n-  Node *alloc_ctrl = alloc->in(TypeFunc::Control);\n@@ -518,1 +529,1 @@\n-        done = true; \/\/ Something go wrong.\n+        done = true; \/\/ Something went wrong.\n@@ -528,1 +539,1 @@\n-             atype->is_known_instance_field() && atype->offset() == offset &&\n+             atype->is_known_instance_field() && atype->flattened_offset() == offset &&\n@@ -560,0 +571,5 @@\n+      Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+      if (default_value != NULL) {\n+        return default_value;\n+      }\n+      assert(alloc->in(AllocateNode::RawDefaultValue) == NULL, \"default value may not be null\");\n@@ -591,1 +607,1 @@\n-  \/\/ Something go wrong.\n+  \/\/ Something went wrong.\n@@ -595,0 +611,37 @@\n+\/\/ Search the last value stored into the inline type's fields.\n+Node* PhaseMacroExpand::inline_type_from_mem(Node* mem, Node* ctl, ciInlineKlass* vk, const TypeAryPtr* adr_type, int offset, AllocateNode* alloc) {\n+  \/\/ Subtract the offset of the first field to account for the missing oop header\n+  offset -= vk->first_field_offset();\n+  \/\/ Create a new InlineTypeNode and retrieve the field values from memory\n+  InlineTypeNode* vt = InlineTypeNode::make_uninitialized(_igvn, vk)->as_InlineType();\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); ++i) {\n+    ciType* field_type = vt->field_type(i);\n+    int field_offset = offset + vt->field_offset(i);\n+    Node* value = NULL;\n+    if (vt->field_is_flattened(i)) {\n+      value = inline_type_from_mem(mem, ctl, field_type->as_inline_klass(), adr_type, field_offset, alloc);\n+    } else {\n+      const Type* ft = Type::get_const_type(field_type);\n+      BasicType bt = field_type->basic_type();\n+      if (UseCompressedOops && !is_java_primitive(bt)) {\n+        ft = ft->make_narrowoop();\n+        bt = T_NARROWOOP;\n+      }\n+      \/\/ Each inline type field has its own memory slice\n+      adr_type = adr_type->with_field_offset(field_offset);\n+      value = value_from_mem(mem, ctl, bt, ft, adr_type, alloc);\n+      if (value != NULL && ft->isa_narrowoop()) {\n+        assert(UseCompressedOops, \"unexpected narrow oop\");\n+        value = transform_later(new DecodeNNode(value, value->get_ptr_type()));\n+      }\n+    }\n+    if (value != NULL) {\n+      vt->set_field_value(i, value);\n+    } else {\n+      \/\/ We might have reached the TrackedInitializationLimit\n+      return NULL;\n+    }\n+  }\n+  return transform_later(vt);\n+}\n+\n@@ -647,1 +700,1 @@\n-              NOT_PRODUCT(fail_eliminate = \"Not store field referrence\";)\n+              NOT_PRODUCT(fail_eliminate = \"Not store field reference\";)\n@@ -675,0 +728,5 @@\n+      } else if (use->is_InlineType() && use->isa_InlineType()->get_oop() == res) {\n+        \/\/ ok to eliminate\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(res_type->is_inlinetypeptr(), \"Unexpected store to mark word\");\n@@ -686,1 +744,1 @@\n-          }else {\n+          } else {\n@@ -692,0 +750,3 @@\n+      } else {\n+        assert(use->Opcode() == Op_CastP2X, \"should be\");\n+        assert(!use->has_out_with(Op_OrL), \"should have been removed because oop is never null\");\n@@ -754,0 +815,4 @@\n+      if (elem_type->is_inlinetype() && !klass->is_flat_array_klass()) {\n+        assert(basic_elem_type == T_INLINE_TYPE, \"unexpected element basic type\");\n+        basic_elem_type = T_OBJECT;\n+      }\n@@ -756,0 +821,4 @@\n+      if (klass->is_flat_array_klass()) {\n+        \/\/ Flattened inline type array\n+        element_size = klass->as_flat_array_klass()->element_byte_size();\n+      }\n@@ -761,0 +830,1 @@\n+  Unique_Node_List value_worklist;\n@@ -787,0 +857,1 @@\n+        assert(!field->is_flattened(), \"flattened inline type fields should not have safepoint uses\");\n@@ -814,3 +885,9 @@\n-      const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n-\n-      Node *field_val = value_from_mem(mem, ctl, basic_elem_type, field_type, field_addr_type, alloc);\n+      Node* field_val = NULL;\n+      const TypeOopPtr* field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n+      if (klass->is_flat_array_klass()) {\n+        ciInlineKlass* vk = elem_type->as_inline_klass();\n+        assert(vk->flatten_array(), \"must be flattened\");\n+        field_val = inline_type_from_mem(mem, ctl, vk, field_addr_type->isa_aryptr(), 0, alloc);\n+      } else {\n+        field_val = value_from_mem(mem, ctl, basic_elem_type, field_type, field_addr_type, alloc);\n+      }\n@@ -874,1 +951,4 @@\n-      if (UseCompressedOops && field_type->isa_narrowoop()) {\n+      if (field_val->is_InlineType()) {\n+        \/\/ Keep track of inline types to scalarize them later\n+        value_worklist.push(field_val);\n+      } else if (UseCompressedOops && field_type->isa_narrowoop()) {\n@@ -895,0 +975,8 @@\n+  \/\/ Scalarize inline types that were added to the safepoint.\n+  \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+  \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+  bool allow_oop = (klass == NULL) || !klass->is_flat_array_klass();\n+  for (uint i = 0; i < value_worklist.size(); ++i) {\n+    Node* vt = value_worklist.at(i);\n+    vt->as_InlineType()->make_scalar_in_safepoints(&_igvn, allow_oop);\n+  }\n@@ -910,1 +998,1 @@\n-void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc) {\n+void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc, bool inline_alloc) {\n@@ -922,10 +1010,7 @@\n-#ifdef ASSERT\n-            \/\/ Verify that there is no dependent MemBarVolatile nodes,\n-            \/\/ they should be removed during IGVN, see MemBarNode::Ideal().\n-            for (DUIterator_Fast pmax, p = n->fast_outs(pmax);\n-                                       p < pmax; p++) {\n-              Node* mb = n->fast_out(p);\n-              assert(mb->is_Initialize() || !mb->is_MemBar() ||\n-                     mb->req() <= MemBarNode::Precedent ||\n-                     mb->in(MemBarNode::Precedent) != n,\n-                     \"MemBarVolatile should be eliminated for non-escaping object\");\n+            for (DUIterator_Fast pmax, p = n->fast_outs(pmax); p < pmax; p++) {\n+              MemBarNode* mb = n->fast_out(p)->isa_MemBar();\n+              if (mb != NULL && mb->req() <= MemBarNode::Precedent && mb->in(MemBarNode::Precedent) == n) {\n+                \/\/ MemBarVolatiles should have been removed by MemBarNode::Ideal() for non-inline allocations\n+                assert(inline_alloc, \"MemBarVolatile should be eliminated for non-escaping object\");\n+                mb->remove(&_igvn);\n+              }\n@@ -933,1 +1018,0 @@\n-#endif\n@@ -957,2 +1041,1 @@\n-          CallProjections callprojs;\n-          ac->extract_projections(&callprojs, true);\n+          CallProjections* callprojs = ac->extract_projections(true);\n@@ -960,3 +1043,3 @@\n-          _igvn.replace_node(callprojs.fallthrough_ioproj, ac->in(TypeFunc::I_O));\n-          _igvn.replace_node(callprojs.fallthrough_memproj, ac->in(TypeFunc::Memory));\n-          _igvn.replace_node(callprojs.fallthrough_catchproj, ac->in(TypeFunc::Control));\n+          _igvn.replace_node(callprojs->fallthrough_ioproj, ac->in(TypeFunc::I_O));\n+          _igvn.replace_node(callprojs->fallthrough_memproj, ac->in(TypeFunc::Memory));\n+          _igvn.replace_node(callprojs->fallthrough_catchproj, ac->in(TypeFunc::Control));\n@@ -979,0 +1062,8 @@\n+      } else if (use->is_InlineType()) {\n+        assert(use->isa_InlineType()->get_oop() == res, \"unexpected inline type use\");\n+        _igvn.rehash_node_delayed(use);\n+        use->isa_InlineType()->set_oop(_igvn.zerocon(T_INLINE_TYPE));\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(inline_alloc, \"Unexpected store to mark word\");\n+        _igvn.replace_node(use, use->in(MemNode::Memory));\n@@ -1012,0 +1103,5 @@\n+          \/\/ Inline type buffer allocations are followed by a membar\n+          Node* membar_after = ctrl_proj->unique_ctrl_out();\n+          if (inline_alloc && membar_after->Opcode() == Op_MemBarCPUOrder) {\n+            membar_after->as_MemBar()->remove(&_igvn);\n+          }\n@@ -1030,0 +1126,4 @@\n+      } else if (use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarStoreStore\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1062,1 +1162,1 @@\n-  if (!EliminateAllocations || !alloc->_is_non_escaping) {\n+  if (!EliminateAllocations) {\n@@ -1067,1 +1167,7 @@\n-  Node* res = alloc->result_cast();\n+\n+  \/\/ Attempt to eliminate inline type buffer allocations\n+  \/\/ regardless of usage and escape\/replaceable status.\n+  bool inline_alloc = tklass->klass()->is_inlinetype();\n+  if (!alloc->_is_non_escaping && !inline_alloc) {\n+    return false;\n+  }\n@@ -1069,3 +1175,4 @@\n-  \/\/ regardless scalar replacable status.\n-  bool boxing_alloc = C->eliminate_boxing() &&\n-                      tklass->klass()->is_instance_klass()  &&\n+  \/\/ regardless of scalar replaceable status.\n+  Node* res = alloc->result_cast();\n+  bool boxing_alloc = (res == NULL) && C->eliminate_boxing() &&\n+                      tklass->klass()->is_instance_klass() &&\n@@ -1073,1 +1180,1 @@\n-  if (!alloc->_is_scalar_replaceable && (!boxing_alloc || (res != NULL))) {\n+  if (!alloc->_is_scalar_replaceable && !boxing_alloc && !inline_alloc) {\n@@ -1085,1 +1192,1 @@\n-    assert(res == NULL, \"sanity\");\n+    assert(res == NULL || inline_alloc, \"sanity\");\n@@ -1090,0 +1197,1 @@\n+      assert(!inline_alloc, \"Inline type allocations should not have safepoint uses\");\n@@ -1110,1 +1218,1 @@\n-  process_users_of_allocation(alloc);\n+  process_users_of_allocation(alloc, inline_alloc);\n@@ -1134,1 +1242,1 @@\n-  const TypeTuple* r = boxing->tf()->range();\n+  const TypeTuple* r = boxing->tf()->range_sig();\n@@ -1335,1 +1443,1 @@\n-    IfNode *toobig_iff = new IfNode(ctrl, initial_slow_test, PROB_MIN, COUNT_UNKNOWN);\n+    IfNode* toobig_iff = new IfNode(ctrl, initial_slow_test, PROB_MIN, COUNT_UNKNOWN);\n@@ -1338,1 +1446,1 @@\n-    Node *toobig_true = new IfTrueNode( toobig_iff );\n+    Node* toobig_true = new IfTrueNode(toobig_iff);\n@@ -1341,1 +1449,1 @@\n-    toobig_false = new IfFalseNode( toobig_iff );\n+    toobig_false = new IfFalseNode(toobig_iff);\n@@ -1380,0 +1488,1 @@\n+\n@@ -1438,0 +1547,3 @@\n+  } else {\n+    \/\/ Let the runtime know if this is a larval allocation\n+    call->init_req(TypeFunc::Parms+1, _igvn.intcon(alloc->_larval));\n@@ -1470,1 +1582,1 @@\n-    migrate_outs(_memproj_fallthrough, result_phi_rawmem);\n+    _igvn.replace_in_uses(_memproj_fallthrough, result_phi_rawmem);\n@@ -1474,1 +1586,1 @@\n-  if (_memproj_catchall != NULL ) {\n+  if (_memproj_catchall != NULL) {\n@@ -1479,1 +1591,1 @@\n-    migrate_outs(_memproj_catchall, _memproj_fallthrough);\n+    _igvn.replace_in_uses(_memproj_catchall, _memproj_fallthrough);\n@@ -1489,1 +1601,1 @@\n-    migrate_outs(_ioproj_fallthrough, result_phi_i_o);\n+    _igvn.replace_in_uses(_ioproj_fallthrough, result_phi_i_o);\n@@ -1493,1 +1605,1 @@\n-  if (_ioproj_catchall != NULL ) {\n+  if (_ioproj_catchall != NULL) {\n@@ -1498,1 +1610,1 @@\n-    migrate_outs(_ioproj_catchall, _ioproj_fallthrough);\n+    _igvn.replace_in_uses(_ioproj_catchall, _ioproj_fallthrough);\n@@ -1566,1 +1678,1 @@\n-    migrate_outs(_fallthroughcatchproj, ctrl);\n+    _igvn.replace_in_uses(_fallthroughcatchproj, ctrl);\n@@ -1579,1 +1691,1 @@\n-    migrate_outs(_memproj_fallthrough, mem);\n+    _igvn.replace_in_uses(_memproj_fallthrough, mem);\n@@ -1583,1 +1695,1 @@\n-    migrate_outs(_ioproj_fallthrough, i_o);\n+    _igvn.replace_in_uses(_ioproj_fallthrough, i_o);\n@@ -1709,5 +1821,4 @@\n-Node*\n-PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n-                                    Node* control, Node* rawmem, Node* object,\n-                                    Node* klass_node, Node* length,\n-                                    Node* size_in_bytes) {\n+Node* PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n+                                          Node* control, Node* rawmem, Node* object,\n+                                          Node* klass_node, Node* length,\n+                                          Node* size_in_bytes) {\n@@ -1716,1 +1827,1 @@\n-  Node* mark_node = alloc->make_ideal_mark(&_igvn, object, control, rawmem);\n+  Node* mark_node = alloc->make_ideal_mark(&_igvn, control, rawmem);\n@@ -1748,0 +1859,2 @@\n+                                            alloc->in(AllocateNode::DefaultValue),\n+                                            alloc->in(AllocateNode::RawDefaultValue),\n@@ -2128,0 +2241,2 @@\n+  const Type* obj_type = _igvn.type(alock->obj_node());\n+  assert(!obj_type->isa_inlinetype() && !obj_type->is_inlinetypeptr(), \"Eliminating lock on inline type\");\n@@ -2409,0 +2524,42 @@\n+  const TypeOopPtr* objptr = _igvn.type(obj)->make_oopptr();\n+  if (objptr->can_be_inline_type()) {\n+    \/\/ Deoptimize and re-execute if a value\n+    assert(EnableValhalla, \"should only be used if inline types are enabled\");\n+    Node* mark = make_load(slow_path, mem, obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X->basic_type());\n+    Node* value_mask = _igvn.MakeConX(markWord::inline_type_pattern);\n+    Node* is_value = _igvn.transform(new AndXNode(mark, value_mask));\n+    Node* cmp = _igvn.transform(new CmpXNode(is_value, value_mask));\n+    Node* bol = _igvn.transform(new BoolNode(cmp, BoolTest::eq));\n+    Node* unc_ctrl = generate_slow_guard(&slow_path, bol, NULL);\n+\n+    int trap_request = Deoptimization::make_trap_request(Deoptimization::Reason_class_check, Deoptimization::Action_none);\n+    address call_addr = SharedRuntime::uncommon_trap_blob()->entry_point();\n+    const TypePtr* no_memory_effects = NULL;\n+    JVMState* jvms = lock->jvms();\n+    CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\",\n+                                           jvms->bci(), no_memory_effects);\n+\n+    unc->init_req(TypeFunc::Control, unc_ctrl);\n+    unc->init_req(TypeFunc::I_O, lock->i_o());\n+    unc->init_req(TypeFunc::Memory, mem); \/\/ may gc ptrs\n+    unc->init_req(TypeFunc::FramePtr,  lock->in(TypeFunc::FramePtr));\n+    unc->init_req(TypeFunc::ReturnAdr, lock->in(TypeFunc::ReturnAdr));\n+    unc->init_req(TypeFunc::Parms+0, _igvn.intcon(trap_request));\n+    unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+    unc->copy_call_debug_info(&_igvn, lock);\n+\n+    assert(unc->peek_monitor_box() == box, \"wrong monitor\");\n+    assert(unc->peek_monitor_obj() == obj, \"wrong monitor\");\n+\n+    \/\/ pop monitor and push obj back on stack: we trap before the monitorenter\n+    unc->pop_monitor();\n+    unc->grow_stack(unc->jvms(), 1);\n+    unc->set_stack(unc->jvms(), unc->jvms()->stk_size()-1, obj);\n+\n+    _igvn.register_new_node_with_optimizer(unc);\n+\n+    Node* ctrl = _igvn.transform(new ProjNode(unc, TypeFunc::Control));\n+    Node* halt = _igvn.transform(new HaltNode(ctrl, lock->in(TypeFunc::FramePtr), \"monitor enter on value-type\"));\n+    C->root()->add_req(halt);\n+  }\n+\n@@ -2510,0 +2667,205 @@\n+\/\/ An inline type might be returned from the call but we don't know its\n+\/\/ type. Either we get a buffered inline type (and nothing needs to be done)\n+\/\/ or one of the inlines being returned is the klass of the inline type\n+\/\/ and we need to allocate an inline type instance of that type and\n+\/\/ initialize it with other values being returned. In that case, we\n+\/\/ first try a fast path allocation and initialize the value with the\n+\/\/ inline klass's pack handler or we fall back to a runtime call.\n+void PhaseMacroExpand::expand_mh_intrinsic_return(CallStaticJavaNode* call) {\n+  assert(call->method()->is_method_handle_intrinsic(), \"must be a method handle intrinsic call\");\n+  Node* ret = call->proj_out_or_null(TypeFunc::Parms);\n+  if (ret == NULL) {\n+    return;\n+  }\n+  const TypeFunc* tf = call->_tf;\n+  const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+  const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+  call->_tf = new_tf;\n+  \/\/ Make sure the change of type is applied before projections are processed by igvn\n+  _igvn.set_type(call, call->Value(&_igvn));\n+  _igvn.set_type(ret, ret->Value(&_igvn));\n+\n+  \/\/ Before any new projection is added:\n+  CallProjections* projs = call->extract_projections(true, true);\n+\n+  Node* ctl = new Node(1);\n+  Node* mem = new Node(1);\n+  Node* io = new Node(1);\n+  Node* ex_ctl = new Node(1);\n+  Node* ex_mem = new Node(1);\n+  Node* ex_io = new Node(1);\n+  Node* res = new Node(1);\n+\n+  Node* cast = transform_later(new CastP2XNode(ctl, res));\n+  Node* mask = MakeConX(0x1);\n+  Node* masked = transform_later(new AndXNode(cast, mask));\n+  Node* cmp = transform_later(new CmpXNode(masked, mask));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* allocation_iff = new IfNode(ctl, bol, PROB_MAX, COUNT_UNKNOWN);\n+  transform_later(allocation_iff);\n+  Node* allocation_ctl = transform_later(new IfTrueNode(allocation_iff));\n+  Node* no_allocation_ctl = transform_later(new IfFalseNode(allocation_iff));\n+\n+  Node* no_allocation_res = transform_later(new CheckCastPPNode(no_allocation_ctl, res, TypeInstPtr::BOTTOM));\n+\n+  Node* mask2 = MakeConX(-2);\n+  Node* masked2 = transform_later(new AndXNode(cast, mask2));\n+  Node* rawklassptr = transform_later(new CastX2PNode(masked2));\n+  Node* klass_node = transform_later(new CheckCastPPNode(allocation_ctl, rawklassptr, TypeKlassPtr::OBJECT_OR_NULL));\n+\n+  Node* slowpath_bol = NULL;\n+  Node* top_adr = NULL;\n+  Node* old_top = NULL;\n+  Node* new_top = NULL;\n+  if (UseTLAB) {\n+    Node* end_adr = NULL;\n+    set_eden_pointers(top_adr, end_adr);\n+    Node* end = make_load(ctl, mem, end_adr, 0, TypeRawPtr::BOTTOM, T_ADDRESS);\n+    old_top = new LoadPNode(ctl, mem, top_adr, TypeRawPtr::BOTTOM, TypeRawPtr::BOTTOM, MemNode::unordered);\n+    transform_later(old_top);\n+    Node* layout_val = make_load(NULL, mem, klass_node, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);\n+    Node* size_in_bytes = ConvI2X(layout_val);\n+    new_top = new AddPNode(top(), old_top, size_in_bytes);\n+    transform_later(new_top);\n+    Node* slowpath_cmp = new CmpPNode(new_top, end);\n+    transform_later(slowpath_cmp);\n+    slowpath_bol = new BoolNode(slowpath_cmp, BoolTest::ge);\n+    transform_later(slowpath_bol);\n+  } else {\n+    slowpath_bol = intcon(1);\n+    top_adr = top();\n+    old_top = top();\n+    new_top = top();\n+  }\n+  IfNode* slowpath_iff = new IfNode(allocation_ctl, slowpath_bol, PROB_UNLIKELY_MAG(4), COUNT_UNKNOWN);\n+  transform_later(slowpath_iff);\n+\n+  Node* slowpath_true = new IfTrueNode(slowpath_iff);\n+  transform_later(slowpath_true);\n+\n+  CallStaticJavaNode* slow_call = new CallStaticJavaNode(OptoRuntime::store_inline_type_fields_Type(),\n+                                                         StubRoutines::store_inline_type_fields_to_buf(),\n+                                                         \"store_inline_type_fields\",\n+                                                         call->jvms()->bci(),\n+                                                         TypePtr::BOTTOM);\n+  slow_call->init_req(TypeFunc::Control, slowpath_true);\n+  slow_call->init_req(TypeFunc::Memory, mem);\n+  slow_call->init_req(TypeFunc::I_O, io);\n+  slow_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+  slow_call->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  slow_call->init_req(TypeFunc::Parms, res);\n+\n+  Node* slow_ctl = transform_later(new ProjNode(slow_call, TypeFunc::Control));\n+  Node* slow_mem = transform_later(new ProjNode(slow_call, TypeFunc::Memory));\n+  Node* slow_io = transform_later(new ProjNode(slow_call, TypeFunc::I_O));\n+  Node* slow_res = transform_later(new ProjNode(slow_call, TypeFunc::Parms));\n+  Node* slow_catc = transform_later(new CatchNode(slow_ctl, slow_io, 2));\n+  Node* slow_norm = transform_later(new CatchProjNode(slow_catc, CatchProjNode::fall_through_index, CatchProjNode::no_handler_bci));\n+  Node* slow_excp = transform_later(new CatchProjNode(slow_catc, CatchProjNode::catch_all_index,    CatchProjNode::no_handler_bci));\n+\n+  Node* ex_r = new RegionNode(3);\n+  Node* ex_mem_phi = new PhiNode(ex_r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* ex_io_phi = new PhiNode(ex_r, Type::ABIO);\n+  ex_r->init_req(1, slow_excp);\n+  ex_mem_phi->init_req(1, slow_mem);\n+  ex_io_phi->init_req(1, slow_io);\n+  ex_r->init_req(2, ex_ctl);\n+  ex_mem_phi->init_req(2, ex_mem);\n+  ex_io_phi->init_req(2, ex_io);\n+\n+  transform_later(ex_r);\n+  transform_later(ex_mem_phi);\n+  transform_later(ex_io_phi);\n+\n+  Node* slowpath_false = new IfFalseNode(slowpath_iff);\n+  transform_later(slowpath_false);\n+  Node* rawmem = new StorePNode(slowpath_false, mem, top_adr, TypeRawPtr::BOTTOM, new_top, MemNode::unordered);\n+  transform_later(rawmem);\n+  Node* mark_node = makecon(TypeRawPtr::make((address)markWord::inline_type_prototype().value()));\n+  rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::mark_offset_in_bytes(), mark_node, T_ADDRESS);\n+  rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  if (UseCompressedClassPointers) {\n+    rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::klass_gap_offset_in_bytes(), intcon(0), T_INT);\n+  }\n+  Node* fixed_block  = make_load(slowpath_false, rawmem, klass_node, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+  Node* pack_handler = make_load(slowpath_false, rawmem, fixed_block, in_bytes(InlineKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+\n+  CallLeafNoFPNode* handler_call = new CallLeafNoFPNode(OptoRuntime::pack_inline_type_Type(),\n+                                                        NULL,\n+                                                        \"pack handler\",\n+                                                        TypeRawPtr::BOTTOM);\n+  handler_call->init_req(TypeFunc::Control, slowpath_false);\n+  handler_call->init_req(TypeFunc::Memory, rawmem);\n+  handler_call->init_req(TypeFunc::I_O, top());\n+  handler_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+  handler_call->init_req(TypeFunc::ReturnAdr, top());\n+  handler_call->init_req(TypeFunc::Parms, pack_handler);\n+  handler_call->init_req(TypeFunc::Parms+1, old_top);\n+\n+  \/\/ We don't know how many values are returned. This assumes the\n+  \/\/ worst case, that all available registers are used.\n+  for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+    if (domain->field_at(i) == Type::HALF) {\n+      slow_call->init_req(i, top());\n+      handler_call->init_req(i+1, top());\n+      continue;\n+    }\n+    Node* proj = transform_later(new ProjNode(call, i));\n+    slow_call->init_req(i, proj);\n+    handler_call->init_req(i+1, proj);\n+  }\n+\n+  \/\/ We can safepoint at that new call\n+  slow_call->copy_call_debug_info(&_igvn, call);\n+  transform_later(slow_call);\n+  transform_later(handler_call);\n+\n+  Node* handler_ctl = transform_later(new ProjNode(handler_call, TypeFunc::Control));\n+  rawmem = transform_later(new ProjNode(handler_call, TypeFunc::Memory));\n+  Node* slowpath_false_res = transform_later(new ProjNode(handler_call, TypeFunc::Parms));\n+\n+  MergeMemNode* slowpath_false_mem = MergeMemNode::make(mem);\n+  slowpath_false_mem->set_memory_at(Compile::AliasIdxRaw, rawmem);\n+  transform_later(slowpath_false_mem);\n+\n+  Node* r = new RegionNode(4);\n+  Node* mem_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* io_phi = new PhiNode(r, Type::ABIO);\n+  Node* res_phi = new PhiNode(r, TypeInstPtr::BOTTOM);\n+\n+  r->init_req(1, no_allocation_ctl);\n+  mem_phi->init_req(1, mem);\n+  io_phi->init_req(1, io);\n+  res_phi->init_req(1, no_allocation_res);\n+  r->init_req(2, slow_norm);\n+  mem_phi->init_req(2, slow_mem);\n+  io_phi->init_req(2, slow_io);\n+  res_phi->init_req(2, slow_res);\n+  r->init_req(3, handler_ctl);\n+  mem_phi->init_req(3, slowpath_false_mem);\n+  io_phi->init_req(3, io);\n+  res_phi->init_req(3, slowpath_false_res);\n+\n+  transform_later(r);\n+  transform_later(mem_phi);\n+  transform_later(io_phi);\n+  transform_later(res_phi);\n+\n+  assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+  _igvn.replace_in_uses(projs->fallthrough_catchproj, r);\n+  _igvn.replace_in_uses(projs->fallthrough_memproj, mem_phi);\n+  _igvn.replace_in_uses(projs->fallthrough_ioproj, io_phi);\n+  _igvn.replace_in_uses(projs->resproj[0], res_phi);\n+  _igvn.replace_in_uses(projs->catchall_catchproj, ex_r);\n+  _igvn.replace_in_uses(projs->catchall_memproj, ex_mem_phi);\n+  _igvn.replace_in_uses(projs->catchall_ioproj, ex_io_phi);\n+\n+  _igvn.replace_node(ctl, projs->fallthrough_catchproj);\n+  _igvn.replace_node(mem, projs->fallthrough_memproj);\n+  _igvn.replace_node(io, projs->fallthrough_ioproj);\n+  _igvn.replace_node(res, projs->resproj[0]);\n+  _igvn.replace_node(ex_ctl, projs->catchall_catchproj);\n+  _igvn.replace_node(ex_mem, projs->catchall_memproj);\n+  _igvn.replace_node(ex_io, projs->catchall_ioproj);\n+ }\n+\n@@ -2535,1 +2897,1 @@\n-      subklass = _igvn.transform(LoadKlassNode::make(_igvn, NULL, C->immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+      subklass = _igvn.transform(LoadKlassNode::make(_igvn, NULL, C->immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeKlassPtr::OBJECT));\n@@ -2547,0 +2909,96 @@\n+\/\/ FlatArrayCheckNode (array1 array2 ...) is expanded into:\n+\/\/\n+\/\/ long mark = array1.mark | array2.mark | ...;\n+\/\/ long locked_bit = markWord::unlocked_value & array1.mark & array2.mark & ...;\n+\/\/ if (locked_bit == 0) {\n+\/\/   \/\/ One array is locked, load prototype header from the klass\n+\/\/   mark = array1.klass.proto | array2.klass.proto | ...\n+\/\/ }\n+\/\/ if ((mark & markWord::flat_array_bit_in_place) == 0) {\n+\/\/    ...\n+\/\/ }\n+void PhaseMacroExpand::expand_flatarraycheck_node(FlatArrayCheckNode* check) {\n+  if (UseArrayMarkWordCheck) {\n+    Node* mark = MakeConX(0);\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    Node* mem = check->in(FlatArrayCheckNode::Memory);\n+    for (uint i = FlatArrayCheckNode::Array; i < check->req(); ++i) {\n+      Node* ary = check->in(i);\n+      if (ary->is_top()) continue;\n+      const TypeAryPtr* t = _igvn.type(ary)->isa_aryptr();\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      Node* mark_adr = basic_plus_adr(ary, oopDesc::mark_offset_in_bytes());\n+      Node* mark_load = _igvn.transform(LoadNode::make(_igvn, NULL, mem, mark_adr, mark_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+      mark = _igvn.transform(new OrXNode(mark, mark_load));\n+      locked_bit = _igvn.transform(new AndXNode(locked_bit, mark_load));\n+    }\n+    assert(!mark->is_Con(), \"Should have been optimized out\");\n+    Node* cmp = _igvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _igvn.transform(new BoolNode(cmp, BoolTest::ne));\n+\n+    \/\/ BoolNode might be shared, replace each if user\n+    Node* old_bol = check->unique_out();\n+    assert(old_bol->is_Bool() && old_bol->as_Bool()->_test._test == BoolTest::ne, \"unexpected condition\");\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      IfNode* old_iff = old_bol->last_out(i)->as_If();\n+      Node* ctrl = old_iff->in(0);\n+      RegionNode* region = new RegionNode(3);\n+      Node* mark_phi = new PhiNode(region, TypeX_X);\n+\n+      \/\/ Check if array is unlocked\n+      IfNode* iff = _igvn.transform(new IfNode(ctrl, is_unlocked, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+\n+      \/\/ Unlocked: Use bits from mark word\n+      region->init_req(1, _igvn.transform(new IfTrueNode(iff)));\n+      mark_phi->init_req(1, mark);\n+\n+      \/\/ Locked: Load prototype header from klass\n+      ctrl = _igvn.transform(new IfFalseNode(iff));\n+      Node* proto = MakeConX(0);\n+      for (uint i = FlatArrayCheckNode::Array; i < check->req(); ++i) {\n+        Node* ary = check->in(i);\n+        if (ary->is_top()) continue;\n+        \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+        Node* klass_adr = basic_plus_adr(ary, oopDesc::klass_offset_in_bytes());\n+        Node* klass = _igvn.transform(LoadKlassNode::make(_igvn, ctrl, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeKlassPtr::OBJECT));\n+        Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+        Node* proto_load = _igvn.transform(LoadNode::make(_igvn, ctrl, C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+        proto = _igvn.transform(new OrXNode(proto, proto_load));\n+      }\n+      region->init_req(2, ctrl);\n+      mark_phi->init_req(2, proto);\n+\n+      \/\/ Check if flat array bits are set\n+      Node* mask = MakeConX(markWord::flat_array_bit_in_place);\n+      Node* masked = _igvn.transform(new AndXNode(_igvn.transform(mark_phi), mask));\n+      cmp = _igvn.transform(new CmpXNode(masked, MakeConX(0)));\n+      Node* is_not_flat = _igvn.transform(new BoolNode(cmp, BoolTest::eq));\n+\n+      ctrl = _igvn.transform(region);\n+      iff = _igvn.transform(new IfNode(ctrl, is_not_flat, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+      _igvn.replace_node(old_iff, iff);\n+    }\n+    _igvn.replace_node(check, C->top());\n+  } else {\n+    \/\/ Fall back to layout helper check\n+    Node* lhs = intcon(0);\n+    for (uint i = FlatArrayCheckNode::Array; i < check->req(); ++i) {\n+      Node* ary = check->in(i);\n+      if (ary->is_top()) continue;\n+      const TypeAryPtr* t = _igvn.type(ary)->isa_aryptr();\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      Node* klass_adr = basic_plus_adr(ary, oopDesc::klass_offset_in_bytes());\n+      Node* klass = transform_later(LoadKlassNode::make(_igvn, NULL, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeKlassPtr::OBJECT));\n+      Node* lh_addr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+      Node* lh_val = _igvn.transform(LoadNode::make(_igvn, NULL, C->immutable_memory(), lh_addr, lh_addr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+      lhs = _igvn.transform(new OrINode(lhs, lh_val));\n+    }\n+    Node* masked = transform_later(new AndINode(lhs, intcon(Klass::_lh_array_tag_vt_value_bit_inplace)));\n+    Node* cmp = transform_later(new CmpINode(masked, intcon(0)));\n+    Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+    Node* old_bol = check->unique_out();\n+    _igvn.replace_node(old_bol, bol);\n+    _igvn.replace_node(check, C->top());\n+  }\n+}\n+\n@@ -2591,2 +3049,5 @@\n-      case Node::Class_CallStaticJava:\n-        success = eliminate_boxing_node(n->as_CallStaticJava());\n+      case Node::Class_CallStaticJava: {\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          success = eliminate_boxing_node(n->as_CallStaticJava());\n+        }\n@@ -2594,0 +3055,1 @@\n+      }\n@@ -2607,0 +3069,2 @@\n+      case Node::Class_FlatArrayCheck:\n+        break;\n@@ -2640,4 +3104,7 @@\n-        \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n-        C->remove_macro_node(n);\n-        _igvn._worklist.push(n);\n-        success = true;\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n+          C->remove_macro_node(n);\n+          _igvn._worklist.push(n);\n+          success = true;\n+        }\n@@ -2733,0 +3200,9 @@\n+    case Node::Class_CallStaticJava:\n+      expand_mh_intrinsic_return(n->as_CallStaticJava());\n+      C->remove_macro_node(n);\n+      assert(C->macro_count() == (old_macro_count - 1), \"expansion must have deleted one node from macro list\");\n+      break;\n+    case Node::Class_FlatArrayCheck:\n+      expand_flatarraycheck_node(n->as_FlatArrayCheck());\n+      assert(C->macro_count() == (old_macro_count - 1), \"expansion must have deleted one node from macro list\");\n+      break;\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":566,"deletions":90,"binary":false,"changes":656,"status":"modified"},{"patch":"@@ -107,0 +107,1 @@\n+  Node* inline_type_from_mem(Node* mem, Node* ctl, ciInlineKlass* vk, const TypeAryPtr* adr_type, int offset, AllocateNode* alloc);\n@@ -112,1 +113,1 @@\n-  void process_users_of_allocation(CallNode *alloc);\n+  void process_users_of_allocation(CallNode *alloc, bool inline_alloc = false);\n@@ -120,0 +121,1 @@\n+  void expand_mh_intrinsic_return(CallStaticJavaNode* call);\n@@ -129,0 +131,1 @@\n+  Node* generate_fair_guard(Node** ctrl, Node* test, RegionNode* region);\n@@ -139,0 +142,5 @@\n+  Node* array_lh_test(Node* array, jint mask);\n+  Node* generate_flat_array_guard(Node** ctrl, Node* array, RegionNode* region);\n+  Node* generate_null_free_array_guard(Node** ctrl, Node* array, RegionNode* region);\n+  Node* generate_array_guard(Node** ctrl, Node* mem, Node* obj, RegionNode* region, jint lh_con);\n+\n@@ -148,0 +156,1 @@\n+                           Node* dest_length,\n@@ -154,0 +163,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -189,1 +200,3 @@\n-\n+  const TypePtr* adjust_for_flat_array(const TypeAryPtr* top_dest, Node*& src_offset,\n+                                       Node*& dest_offset, Node*& length, BasicType& dest_elem,\n+                                       Node*& dest_length);\n@@ -194,0 +207,2 @@\n+  void expand_flatarraycheck_node(FlatArrayCheckNode* check);\n+\n@@ -195,2 +210,0 @@\n-  void migrate_outs(Node *old, Node *target);\n-  void copy_call_debug_info(CallNode *oldcall, CallNode * newcall);\n@@ -211,0 +224,2 @@\n+  bool can_try_zeroing_elimination(AllocateArrayNode* alloc, Node* src, Node* dest) const;\n+\n","filename":"src\/hotspot\/share\/opto\/macro.hpp","additions":19,"deletions":4,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -139,1 +140,1 @@\n-inline Node* PhaseMacroExpand::generate_slow_guard(Node** ctrl, Node* test, RegionNode* region) {\n+Node* PhaseMacroExpand::generate_slow_guard(Node** ctrl, Node* test, RegionNode* region) {\n@@ -143,0 +144,4 @@\n+inline Node* PhaseMacroExpand::generate_fair_guard(Node** ctrl, Node* test, RegionNode* region) {\n+  return generate_guard(ctrl, test, region, PROB_FAIR);\n+}\n+\n@@ -280,0 +285,40 @@\n+Node* PhaseMacroExpand::array_lh_test(Node* array, jint mask) {\n+  Node* klass_adr = basic_plus_adr(array, oopDesc::klass_offset_in_bytes());\n+  Node* klass = transform_later(LoadKlassNode::make(_igvn, NULL, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeKlassPtr::OBJECT));\n+  Node* lh_addr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+  Node* lh_val = _igvn.transform(LoadNode::make(_igvn, NULL, C->immutable_memory(), lh_addr, lh_addr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+  Node* masked = transform_later(new AndINode(lh_val, intcon(mask)));\n+  Node* cmp = transform_later(new CmpINode(masked, intcon(0)));\n+  return transform_later(new BoolNode(cmp, BoolTest::ne));\n+}\n+\n+Node* PhaseMacroExpand::generate_flat_array_guard(Node** ctrl, Node* array, RegionNode* region) {\n+  assert(UseFlatArray, \"can never be flattened\");\n+  return generate_fair_guard(ctrl, array_lh_test(array, Klass::_lh_array_tag_vt_value_bit_inplace), region);\n+}\n+\n+Node* PhaseMacroExpand::generate_null_free_array_guard(Node** ctrl, Node* array, RegionNode* region) {\n+  assert(EnableValhalla, \"can never be null free\");\n+  return generate_fair_guard(ctrl, array_lh_test(array, Klass::_lh_null_free_bit_inplace), region);\n+}\n+\n+Node* PhaseMacroExpand::generate_array_guard(Node** ctrl, Node* mem, Node* obj_or_klass, RegionNode* region, jint lh_con) {\n+  if ((*ctrl)->is_top())  return NULL;\n+\n+  Node* kls = NULL;\n+  if (_igvn.type(obj_or_klass)->isa_oopptr()) {\n+    Node* k_adr = basic_plus_adr(obj_or_klass, oopDesc::klass_offset_in_bytes());\n+    kls = transform_later(LoadKlassNode::make(_igvn, NULL, C->immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeKlassPtr::OBJECT));\n+  } else {\n+    assert(_igvn.type(obj_or_klass)->isa_klassptr(), \"what else?\");\n+    kls = obj_or_klass;\n+  }\n+  Node* layout_val = make_load(NULL, mem, kls, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);\n+\n+  layout_val = transform_later(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+  Node* cmp = transform_later(new CmpINode(layout_val, intcon(lh_con)));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+\n+  return generate_fair_guard(ctrl, bol, region);\n+}\n+\n@@ -332,0 +377,19 @@\n+bool PhaseMacroExpand::can_try_zeroing_elimination(AllocateArrayNode* alloc,\n+                                                   Node* src,\n+                                                   Node* dest) const {\n+  const TypeAryPtr* top_dest = _igvn.type(dest)->isa_aryptr();\n+\n+  if (top_dest != NULL) {\n+    if (top_dest->klass() == NULL) {\n+      return false;\n+    }\n+  }\n+\n+  return ReduceBulkZeroing\n+    && !(UseTLAB && ZeroTLAB) \/\/ pointless if already zeroed\n+    && !src->eqv_uncast(dest)\n+    && alloc != NULL\n+    && _igvn.find_int_con(alloc->in(AllocateNode::ALength), 1) > 0\n+    && alloc->maybe_set_complete(&_igvn);\n+}\n+\n@@ -374,0 +438,1 @@\n+                                           Node* dest_length,\n@@ -382,1 +447,2 @@\n-  Node* original_dest      = dest;\n+  Node* default_value = NULL;\n+  Node* raw_default_value = NULL;\n@@ -390,7 +456,2 @@\n-  if (ReduceBulkZeroing\n-      && !(UseTLAB && ZeroTLAB) \/\/ pointless if already zeroed\n-      && basic_elem_type != T_CONFLICT \/\/ avoid corner case\n-      && !src->eqv_uncast(dest)\n-      && alloc != NULL\n-      && _igvn.find_int_con(alloc->in(AllocateNode::ALength), 1) > 0\n-      && alloc->maybe_set_complete(&_igvn)) {\n+  if (can_try_zeroing_elimination(alloc, src, dest) &&\n+      basic_elem_type != T_CONFLICT \/* avoid corner case *\/) {\n@@ -409,0 +470,2 @@\n+    default_value = alloc->in(AllocateNode::DefaultValue);\n+    raw_default_value = alloc->in(AllocateNode::RawDefaultValue);\n@@ -412,2 +475,0 @@\n-    \/\/original_dest   = dest;\n-    \/\/dest_uninitialized = false;\n@@ -474,1 +535,0 @@\n-      Node* dest_length = alloc->in(AllocateNode::ALength);\n@@ -481,1 +541,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             default_value, raw_default_value,\n+                             basic_elem_type,\n@@ -512,1 +574,0 @@\n-    Node* dest_length = alloc->in(AllocateNode::ALength);\n@@ -518,1 +579,3 @@\n-                           adr_type, dest, basic_elem_type,\n+                           adr_type, dest,\n+                           default_value, raw_default_value,\n+                           basic_elem_type,\n@@ -567,1 +630,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             default_value, raw_default_value,\n+                             basic_elem_type,\n@@ -577,1 +642,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             default_value, raw_default_value,\n+                             basic_elem_type,\n@@ -756,1 +823,3 @@\n-                           adr_type, dest, basic_elem_type,\n+                           adr_type, dest,\n+                           default_value, raw_default_value,\n+                           basic_elem_type,\n@@ -809,0 +878,4 @@\n+    \/\/ Do not let reads from the destination float above the arraycopy.\n+    \/\/ Since we cannot type the arrays, we don't know which slices\n+    \/\/ might be affected.  We could restrict this barrier only to those\n+    \/\/ memory slices which pertain to array elements--but don't bother.\n@@ -860,0 +933,2 @@\n+                                            Node* val,\n+                                            Node* raw_val,\n@@ -875,0 +950,1 @@\n+  assert(basic_elem_type != T_INLINE_TYPE, \"should have been converted to a basic type copy\");\n@@ -898,1 +974,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -903,1 +979,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -916,1 +992,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -945,1 +1021,7 @@\n-        mem = StoreNode::make(_igvn, ctrl, mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);\n+        if (val == NULL) {\n+          assert(raw_val == NULL, \"val may not be null\");\n+          mem = StoreNode::make(_igvn, ctrl, mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);\n+        } else {\n+          assert(_igvn.type(val)->isa_narrowoop(), \"should be narrow oop\");\n+          mem = new StoreNNode(ctrl, mem, p1, adr_type, val, MemNode::unordered);\n+        }\n@@ -950,1 +1032,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, raw_val,\n@@ -1213,0 +1295,36 @@\n+const TypePtr* PhaseMacroExpand::adjust_for_flat_array(const TypeAryPtr* top_dest, Node*& src_offset,\n+                                                       Node*& dest_offset, Node*& length, BasicType& dest_elem,\n+                                                       Node*& dest_length) {\n+#ifdef ASSERT\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  bool needs_barriers = top_dest->elem()->inline_klass()->contains_oops() &&\n+                        bs->array_copy_requires_gc_barriers(dest_length != NULL, T_OBJECT, false, BarrierSetC2::Optimization);\n+  assert(!needs_barriers, \"Flat arracopy would require GC barriers\");\n+#endif\n+  int elem_size = top_dest->klass()->as_flat_array_klass()->element_byte_size();\n+  if (elem_size >= 8) {\n+    if (elem_size > 8) {\n+      \/\/ treat as array of long but scale length, src offset and dest offset\n+      assert((elem_size % 8) == 0, \"not a power of 2?\");\n+      int factor = elem_size \/ 8;\n+      length = transform_later(new MulINode(length, intcon(factor)));\n+      src_offset = transform_later(new MulINode(src_offset, intcon(factor)));\n+      dest_offset = transform_later(new MulINode(dest_offset, intcon(factor)));\n+      if (dest_length != NULL) {\n+        dest_length = transform_later(new MulINode(dest_length, intcon(factor)));\n+      }\n+      elem_size = 8;\n+    }\n+    dest_elem = T_LONG;\n+  } else if (elem_size == 4) {\n+    dest_elem = T_INT;\n+  } else if (elem_size == 2) {\n+    dest_elem = T_CHAR;\n+  } else if (elem_size == 1) {\n+    dest_elem = T_BYTE;\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+  return TypeRawPtr::BOTTOM;\n+}\n+\n@@ -1228,0 +1346,17 @@\n+    const Type* src_type = _igvn.type(src);\n+    const Type* dest_type = _igvn.type(dest);\n+    const TypeAryPtr* top_src = src_type->isa_aryptr();\n+    const TypeAryPtr* top_dest = dest_type->isa_aryptr();\n+    BasicType dest_elem = T_OBJECT;\n+    if (top_dest != NULL && top_dest->klass() != NULL) {\n+      dest_elem = top_dest->klass()->as_array_klass()->element_type()->basic_type();\n+    }\n+    if (dest_elem == T_ARRAY || (dest_elem == T_INLINE_TYPE && top_dest->klass()->is_obj_array_klass())) {\n+      dest_elem = T_OBJECT;\n+    }\n+    if (top_src != NULL && top_src->is_flat()) {\n+      \/\/ If src is flat, dest is guaranteed to be flat as well\n+      dest_elem = T_INLINE_TYPE;\n+      top_dest = top_src;\n+    }\n+\n@@ -1233,0 +1368,1 @@\n+    Node* dest_length = NULL;\n@@ -1236,0 +1372,1 @@\n+      dest_length = alloc->in(AllocateNode::ALength);\n@@ -1238,3 +1375,15 @@\n-    const TypePtr* adr_type = _igvn.type(dest)->is_oopptr()->add_offset(Type::OffsetBot);\n-    if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n-      adr_type = ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr();\n+    const TypePtr* adr_type = NULL;\n+    if (dest_elem == T_INLINE_TYPE) {\n+      assert(dest_length != NULL, \"must be tightly coupled\");\n+      \/\/ Copy to a flat array modifies multiple memory slices. Conservatively insert a barrier\n+      \/\/ on all slices to prevent writes into the source from floating below the arraycopy.\n+      insert_mem_bar(&ctrl, &mem, Op_MemBarCPUOrder);\n+      adr_type = adjust_for_flat_array(top_dest, src_offset, dest_offset, length, dest_elem, dest_length);\n+    } else {\n+      adr_type = dest_type->is_oopptr()->add_offset(Type::OffsetBot);\n+      if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+        adr_type = ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr();\n+      }\n+      if (ac->_src_type != ac->_dest_type) {\n+        adr_type = TypeRawPtr::BOTTOM;\n+      }\n@@ -1243,1 +1392,1 @@\n-                       adr_type, T_OBJECT,\n+                       adr_type, dest_elem,\n@@ -1245,0 +1394,1 @@\n+                       dest_length,\n@@ -1246,1 +1396,0 @@\n-\n@@ -1277,2 +1426,6 @@\n-  if (is_reference_type(src_elem))  src_elem  = T_OBJECT;\n-  if (is_reference_type(dest_elem)) dest_elem = T_OBJECT;\n+  if (src_elem == T_ARRAY || (src_elem == T_INLINE_TYPE && top_src->klass()->is_obj_array_klass())) {\n+    src_elem = T_OBJECT;\n+  }\n+  if (dest_elem == T_ARRAY || (dest_elem == T_INLINE_TYPE && top_dest->klass()->is_obj_array_klass())) {\n+    dest_elem = T_OBJECT;\n+  }\n@@ -1280,3 +1433,1 @@\n-  if (ac->is_arraycopy_validated() &&\n-      dest_elem != T_CONFLICT &&\n-      src_elem == T_CONFLICT) {\n+  if (ac->is_arraycopy_validated() && dest_elem != T_CONFLICT && src_elem == T_CONFLICT) {\n@@ -1301,0 +1452,1 @@\n+                                   NULL,\n@@ -1311,1 +1463,8 @@\n-  if (src_elem != dest_elem || dest_elem == T_VOID) {\n+  \/\/\n+  \/\/ We have no stub to copy flattened inline type arrays with oop\n+  \/\/ fields if we need to emit write barriers.\n+  \/\/\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  if (src_elem != dest_elem || dest_elem == T_VOID ||\n+      (dest_elem == T_INLINE_TYPE && top_dest->elem()->inline_klass()->contains_oops() &&\n+       bs->array_copy_requires_gc_barriers(alloc != NULL, T_OBJECT, false, BarrierSetC2::Optimization))) {\n@@ -1338,5 +1497,3 @@\n-  {\n-    Node* mem = ac->in(TypeFunc::Memory);\n-    merge_mem = MergeMemNode::make(mem);\n-    transform_later(merge_mem);\n-  }\n+  Node* mem = ac->in(TypeFunc::Memory);\n+  merge_mem = MergeMemNode::make(mem);\n+  transform_later(merge_mem);\n@@ -1383,0 +1540,15 @@\n+\n+    \/\/ Handle inline type arrays\n+    if (!top_src->is_flat()) {\n+      if (UseFlatArray && !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        generate_flat_array_guard(&ctrl, src, slow_region);\n+      }\n+      if (EnableValhalla) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        generate_null_free_array_guard(&ctrl, dest, slow_region);\n+      }\n+    } else {\n+      assert(top_dest->is_flat(), \"dest array must be flat\");\n+    }\n@@ -1384,0 +1556,1 @@\n+\n@@ -1386,1 +1559,8 @@\n-  if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+  Node* dest_length = (alloc != NULL) ? alloc->in(AllocateNode::ALength) : NULL;\n+\n+  if (dest_elem == T_INLINE_TYPE) {\n+    \/\/ Copy to a flat array modifies multiple memory slices. Conservatively insert a barrier\n+    \/\/ on all slices to prevent writes into the source from floating below the arraycopy.\n+    insert_mem_bar(&ctrl, &mem, Op_MemBarCPUOrder);\n+    adr_type = adjust_for_flat_array(top_dest, src_offset, dest_offset, length, dest_elem, dest_length);\n+  } else if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n@@ -1395,0 +1575,1 @@\n+                     dest_length,\n@@ -1397,1 +1578,2 @@\n-                     false, ac->has_negative_length_guard(), slow_region);\n+                     false, ac->has_negative_length_guard(),\n+                     slow_region);\n","filename":"src\/hotspot\/share\/opto\/macroArrayCopy.cpp","additions":223,"deletions":41,"binary":false,"changes":264,"status":"modified"},{"patch":"@@ -182,0 +182,46 @@\n+\/\/ Array of RegMask, one per returned values (inline type instances can\n+\/\/ be returned as multiple return values, one per field)\n+RegMask* Matcher::return_values_mask(const TypeTuple *range) {\n+  uint cnt = range->cnt() - TypeFunc::Parms;\n+  if (cnt == 0) {\n+    return NULL;\n+  }\n+  RegMask* mask = NEW_RESOURCE_ARRAY(RegMask, cnt);\n+\n+  if (!InlineTypeReturnedAsFields) {\n+    \/\/ Get ideal-register return type\n+    uint ireg = range->field_at(TypeFunc::Parms)->ideal_reg();\n+    \/\/ Get machine return register\n+    OptoRegPair regs = return_value(ireg);\n+\n+    \/\/ And mask for same\n+    mask[0].Clear();\n+    mask[0].Insert(regs.first());\n+    if (OptoReg::is_valid(regs.second())) {\n+      mask[0].Insert(regs.second());\n+    }\n+  } else {\n+    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, cnt);\n+    VMRegPair* vm_parm_regs = NEW_RESOURCE_ARRAY(VMRegPair, cnt);\n+\n+    for (uint i = 0; i < cnt; i++) {\n+      sig_bt[i] = range->field_at(i+TypeFunc::Parms)->basic_type();\n+    }\n+\n+    int regs = SharedRuntime::java_return_convention(sig_bt, vm_parm_regs, cnt);\n+    assert(regs > 0, \"should have been tested during graph construction\");\n+    for (uint i = 0; i < cnt; i++) {\n+      mask[i].Clear();\n+\n+      OptoReg::Name reg1 = OptoReg::as_OptoReg(vm_parm_regs[i].first());\n+      if (OptoReg::is_valid(reg1)) {\n+        mask[i].Insert(reg1);\n+      }\n+      OptoReg::Name reg2 = OptoReg::as_OptoReg(vm_parm_regs[i].second());\n+      if (OptoReg::is_valid(reg2)) {\n+        mask[i].Insert(reg2);\n+      }\n+    }\n+  }\n+  return mask;\n+}\n@@ -197,15 +243,4 @@\n-  \/\/ Map a Java-signature return type into return register-value\n-  \/\/ machine registers for 0, 1 and 2 returned values.\n-  const TypeTuple *range = C->tf()->range();\n-  if( range->cnt() > TypeFunc::Parms ) { \/\/ If not a void function\n-    \/\/ Get ideal-register return type\n-    uint ireg = range->field_at(TypeFunc::Parms)->ideal_reg();\n-    \/\/ Get machine return register\n-    uint sop = C->start()->Opcode();\n-    OptoRegPair regs = return_value(ireg);\n-\n-    \/\/ And mask for same\n-    _return_value_mask = RegMask(regs.first());\n-    if( OptoReg::is_valid(regs.second()) )\n-      _return_value_mask.Insert(regs.second());\n-  }\n+  \/\/ Map Java-signature return types into return register-value\n+  \/\/ machine registers.\n+  const TypeTuple *range = C->tf()->range_cc();\n+  _return_values_mask = return_values_mask(range);\n@@ -219,1 +254,1 @@\n-  const TypeTuple *domain = C->tf()->domain();\n+  const TypeTuple *domain = C->tf()->domain_cc();\n@@ -500,0 +535,1 @@\n+\n@@ -740,1 +776,1 @@\n-  uint ret_edge_cnt = TypeFunc::Parms + ((C->tf()->range()->cnt() == TypeFunc::Parms) ? 0 : 1);\n+  uint ret_edge_cnt = C->tf()->range_cc()->cnt();\n@@ -742,4 +778,3 @@\n-  \/\/ Returns have 0 or 1 returned values depending on call signature.\n-  \/\/ Return register is specified by return_value in the AD file.\n-  if (ret_edge_cnt > TypeFunc::Parms)\n-    ret_rms[TypeFunc::Parms+0] = _return_value_mask;\n+  for (i = TypeFunc::Parms; i < ret_edge_cnt; i++) {\n+    ret_rms[i] = _return_values_mask[i-TypeFunc::Parms];\n+  }\n@@ -812,1 +847,1 @@\n-  int proj_cnt = C->tf()->domain()->cnt();\n+  int proj_cnt = C->tf()->domain_cc()->cnt();\n@@ -1083,1 +1118,5 @@\n-              m = n->in(0)->as_Multi()->match( n->as_Proj(), this );\n+              RegMask* mask = NULL;\n+              if (n->in(0)->is_Call()) {\n+                mask = return_values_mask(n->in(0)->as_Call()->tf()->range_cc());\n+              }\n+              m = n->in(0)->as_Multi()->match(n->as_Proj(), this, mask);\n@@ -1228,1 +1267,1 @@\n-    domain = call->tf()->domain();\n+    domain = call->tf()->domain_cc();\n@@ -1315,1 +1354,4 @@\n-  int argcnt = cnt - TypeFunc::Parms;\n+  \/\/ Null entry point is a special cast where the target of the call\n+  \/\/ is in a register.\n+  int adj = (call != NULL && call->entry_point() == NULL) ? 1 : 0;\n+  int argcnt = cnt - TypeFunc::Parms - adj;\n@@ -1321,1 +1363,1 @@\n-      sig_bt[i] = domain->field_at(i+TypeFunc::Parms)->basic_type();\n+      sig_bt[i] = domain->field_at(i+TypeFunc::Parms+adj)->basic_type();\n@@ -1362,1 +1404,1 @@\n-      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms];\n+      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms+adj];\n@@ -1369,1 +1411,1 @@\n-      if (OptoReg::is_valid(reg1))\n+      if (OptoReg::is_valid(reg1)) {\n@@ -1371,0 +1413,1 @@\n+      }\n@@ -1373,1 +1416,1 @@\n-      if (OptoReg::is_valid(reg2))\n+      if (OptoReg::is_valid(reg2)) {\n@@ -1375,0 +1418,1 @@\n+      }\n@@ -1390,1 +1434,1 @@\n-    uint r_cnt = mcall->tf()->range()->cnt();\n+    uint r_cnt = mcall->tf()->range_sig()->cnt();\n@@ -1411,1 +1455,1 @@\n-         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain()->cnt()), \"\");\n+         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain_cc()->cnt()), \"\");\n@@ -2379,0 +2423,7 @@\n+    case Op_ClearArray: {\n+      Node* pair = new BinaryNode(n->in(2), n->in(3));\n+      n->set_req(2, pair);\n+      n->set_req(3, n->in(4));\n+      n->del_req(4);\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":82,"deletions":31,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -39,0 +41,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -240,1 +243,1 @@\n-               tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n+        tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n@@ -858,0 +861,1 @@\n+  case T_INLINE_TYPE:\n@@ -985,1 +989,1 @@\n-      BasicType ary_elem  = ary_t->klass()->as_array_klass()->element_type()->basic_type();\n+      BasicType ary_elem = ary_t->klass()->as_array_klass()->element_type()->basic_type();\n@@ -988,0 +992,4 @@\n+      if (ary_t->klass()->is_flat_array_klass()) {\n+        ciFlatArrayKlass* vak = ary_t->klass()->as_flat_array_klass();\n+        shift = vak->log2_element_size();\n+      }\n@@ -1115,0 +1123,6 @@\n+      assert(memory_type() != T_INLINE_TYPE, \"should not be used for inline types\");\n+      Node* default_value = ld_alloc->in(AllocateNode::DefaultValue);\n+      if (default_value != NULL) {\n+        return default_value;\n+      }\n+      assert(ld_alloc->in(AllocateNode::RawDefaultValue) == NULL, \"default value may not be null\");\n@@ -1182,0 +1196,27 @@\n+  \/\/ Loading from an InlineTypePtr? The InlineTypePtr has the values of\n+  \/\/ all fields as input. Look for the field with matching offset.\n+  Node* addr = in(Address);\n+  intptr_t offset;\n+  Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);\n+  if (base != NULL && base->is_InlineTypePtr() && offset > oopDesc::klass_offset_in_bytes()) {\n+    Node* value = base->as_InlineTypePtr()->field_value_by_offset((int)offset, true);\n+    if (value->is_InlineType()) {\n+      \/\/ Non-flattened inline type field\n+      InlineTypeNode* vt = value->as_InlineType();\n+      if (vt->is_allocated(phase)) {\n+        value = vt->get_oop();\n+      } else {\n+        \/\/ Not yet allocated, bail out\n+        value = NULL;\n+      }\n+    }\n+    if (value != NULL) {\n+      if (Opcode() == Op_LoadN) {\n+        \/\/ Encode oop value if we are loading a narrow oop\n+        assert(!phase->type(value)->isa_narrowoop(), \"should already be decoded\");\n+        value = phase->transform(new EncodePNode(value, bottom_type()));\n+      }\n+      return value;\n+    }\n+  }\n+\n@@ -1792,2 +1833,6 @@\n-  AllocateNode* alloc = is_new_object_mark_load(phase);\n-  if (alloc != NULL && alloc->Opcode() == Op_Allocate && UseBiasedLocking) {\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(address, phase);\n+  if (alloc != NULL && mem->is_Proj() &&\n+      mem->in(0) != NULL &&\n+      mem->in(0) == alloc->initialization() &&\n+      Opcode() == Op_LoadX &&\n+      alloc->initialization()->proj_out_or_null(0) != NULL) {\n@@ -1796,1 +1841,1 @@\n-    return alloc->make_ideal_mark(phase, address, control, mem);\n+    return alloc->make_ideal_mark(phase, control, mem);\n@@ -1888,0 +1933,1 @@\n+        && t->isa_inlinetype() == NULL\n@@ -1922,0 +1968,1 @@\n+            tp->is_oopptr()->klass() == ciEnv::current()->Class_klass() ||\n@@ -1927,1 +1974,3 @@\n-    \/\/ Optimize loads from constant fields.\n+    BasicType bt = memory_type();\n+\n+    \/\/ Optimize loads from constant fields.\n@@ -1931,1 +1980,10 @@\n-      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), memory_type());\n+      ciType* mirror_type = const_oop->as_instance()->java_mirror_type();\n+      if (mirror_type != NULL && mirror_type->is_inlinetype()) {\n+        ciInlineKlass* vk = mirror_type->as_inline_klass();\n+        if (off == vk->default_value_offset()) {\n+          \/\/ Loading a special hidden field that contains the oop of the default inline type\n+          const Type* const_oop = TypeInstPtr::make(vk->default_instance());\n+          return (bt == T_NARROWOOP) ? const_oop->make_narrowoop() : const_oop;\n+        }\n+      }\n+      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), bt);\n@@ -1939,0 +1997,1 @@\n+            tp->is_klassptr()->klass() == NULL ||\n@@ -1945,15 +2004,31 @@\n-  } else if (tp->base() == Type::RawPtr && adr->is_Load() && off == 0) {\n-    \/* With mirrors being an indirect in the Klass*\n-     * the VM is now using two loads. LoadKlass(LoadP(LoadP(Klass, mirror_offset), zero_offset))\n-     * The LoadP from the Klass has a RawPtr type (see LibraryCallKit::load_mirror_from_klass).\n-     *\n-     * So check the type and klass of the node before the LoadP.\n-     *\/\n-    Node* adr2 = adr->in(MemNode::Address);\n-    const TypeKlassPtr* tkls = phase->type(adr2)->isa_klassptr();\n-    if (tkls != NULL && !StressReflectiveCode) {\n-      ciKlass* klass = tkls->klass();\n-      if (klass->is_loaded() && tkls->klass_is_exact() && tkls->offset() == in_bytes(Klass::java_mirror_offset())) {\n-        assert(adr->Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n-        assert(Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n-        return TypeInstPtr::make(klass->java_mirror());\n+  } else if (tp->base() == Type::RawPtr && !StressReflectiveCode) {\n+    if (adr->is_Load() && off == 0) {\n+      \/* With mirrors being an indirect in the Klass*\n+       * the VM is now using two loads. LoadKlass(LoadP(LoadP(Klass, mirror_offset), zero_offset))\n+       * The LoadP from the Klass has a RawPtr type (see LibraryCallKit::load_mirror_from_klass).\n+       *\n+       * So check the type and klass of the node before the LoadP.\n+       *\/\n+      Node* adr2 = adr->in(MemNode::Address);\n+      const TypeKlassPtr* tkls = phase->type(adr2)->isa_klassptr();\n+      if (tkls != NULL) {\n+        ciKlass* klass = tkls->klass();\n+        if (klass != NULL && klass->is_loaded() && tkls->klass_is_exact() && tkls->offset() == in_bytes(Klass::java_mirror_offset())) {\n+          assert(adr->Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n+          assert(Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n+          return TypeInstPtr::make(klass->java_mirror());\n+        }\n+      }\n+    } else {\n+      \/\/ Check for a load of the default value offset from the InlineKlassFixedBlock:\n+      \/\/ LoadI(LoadP(inline_klass, adr_inlineklass_fixed_block_offset), default_value_offset_offset)\n+      intptr_t offset = 0;\n+      Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+      if (base != NULL && base->is_Load() && offset == in_bytes(InlineKlass::default_value_offset_offset())) {\n+        const TypeKlassPtr* tkls = phase->type(base->in(MemNode::Address))->isa_klassptr();\n+        if (tkls != NULL && tkls->is_loaded() && tkls->klass_is_exact() && tkls->isa_inlinetype() &&\n+            tkls->offset() == in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())) {\n+          assert(base->Opcode() == Op_LoadP, \"must load an oop from klass\");\n+          assert(Opcode() == Op_LoadI, \"must load an int from fixed block\");\n+          return TypeInt::make(tkls->klass()->as_inline_klass()->default_value_offset());\n+        }\n@@ -1967,1 +2042,1 @@\n-    if (klass->is_loaded() && tkls->klass_is_exact()) {\n+    if (tkls->is_loaded() && tkls->klass_is_exact()) {\n@@ -1994,1 +2069,1 @@\n-    if (klass->is_loaded() ) {\n+    if (tkls->is_loaded()) {\n@@ -2057,4 +2132,5 @@\n-\n-  Node* alloc = is_new_object_mark_load(phase);\n-  if (alloc != NULL && !(alloc->Opcode() == Op_Allocate && UseBiasedLocking)) {\n-    return TypeX::make(markWord::prototype().value());\n+  if (!EnableValhalla) { \/\/ CMH: Fix JDK-8255045\n+    Node* alloc = is_new_object_mark_load(phase);\n+    if (alloc != NULL && !(alloc->Opcode() == Op_Allocate && UseBiasedLocking)) {\n+      return TypeX::make(markWord::prototype().value());\n+    }\n@@ -2199,1 +2275,2 @@\n-Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at, const TypeKlassPtr* tk) {\n+Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at,\n+                          const TypeKlassPtr* tk) {\n@@ -2286,1 +2363,1 @@\n-      return TypeKlassPtr::make(TypePtr::NotNull, ik, 0\/*offset*\/);\n+      return TypeKlassPtr::make(TypePtr::NotNull, ik, Type::Offset(0), tinst->flatten_array());\n@@ -2292,1 +2369,1 @@\n-  if( tary != NULL ) {\n+  if (tary != NULL) {\n@@ -2299,1 +2376,1 @@\n-      ciArrayKlass *ak = tary->klass()->as_array_klass();\n+      ciArrayKlass* ak = tary_klass->as_array_klass();\n@@ -2302,2 +2379,2 @@\n-      if( ak->is_obj_array_klass() ) {\n-        assert( ak->is_loaded(), \"\" );\n+      if (ak->is_obj_array_klass()) {\n+        assert(ak->is_loaded(), \"\");\n@@ -2305,2 +2382,2 @@\n-        if( base_k->is_loaded() && base_k->is_instance_klass() ) {\n-          ciInstanceKlass* ik = base_k->as_instance_klass();\n+        if (base_k->is_loaded() && base_k->is_instance_klass()) {\n+          ciInstanceKlass *ik = base_k->as_instance_klass();\n@@ -2317,3 +2394,2 @@\n-        return TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n-      } else {                  \/\/ Found a type-array?\n-        assert( ak->is_type_array_klass(), \"\" );\n+        return TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n+      } else if (ak->is_type_array_klass()) {\n@@ -2328,2 +2404,1 @@\n-    ciKlass* klass = tkls->klass();\n-    if( !klass->is_loaded() )\n+    if (!tkls->is_loaded()) {\n@@ -2331,0 +2406,2 @@\n+    }\n+    ciKlass* klass = tkls->klass();\n@@ -2340,1 +2417,5 @@\n-      return TypeKlassPtr::make(tkls->ptr(), elem, 0\/*offset*\/);\n+      return TypeKlassPtr::make(tkls->ptr(), elem, Type::Offset(0));\n+    } else if (klass->is_flat_array_klass() &&\n+               tkls->offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {\n+      ciKlass* elem = klass->as_flat_array_klass()->element_klass();\n+      return TypeKlassPtr::make(tkls->ptr(), elem, Type::Offset(0), \/* flatten_array= *\/ true);\n@@ -2548,0 +2629,1 @@\n+  case T_INLINE_TYPE:\n@@ -2609,1 +2691,1 @@\n-  {\n+  if (phase->C->get_adr_type(phase->C->get_alias_index(adr_type())) != TypeAryPtr::INLINES) {\n@@ -2629,0 +2711,1 @@\n+             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreN) ||\n@@ -2729,2 +2812,1 @@\n-  if (result == this &&\n-      ReduceFieldZeroing && phase->type(val)->is_zero_type()) {\n+  if (result == this && ReduceFieldZeroing) {\n@@ -2732,1 +2814,3 @@\n-    if (mem->is_Proj() && mem->in(0)->is_Allocate()) {\n+    if (mem->is_Proj() && mem->in(0)->is_Allocate() &&\n+        (phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::DefaultValue) == val)) {\n+      assert(!phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::DefaultValue) == NULL, \"storing null to inline type array is forbidden\");\n@@ -2745,1 +2829,9 @@\n-          result = mem;\n+          if (phase->type(val)->is_zero_type()) {\n+            result = mem;\n+          } else if (prev_mem->is_Proj() && prev_mem->in(0)->is_Initialize()) {\n+            InitializeNode* init = prev_mem->in(0)->as_Initialize();\n+            AllocateNode* alloc = init->allocation();\n+            if (alloc != NULL && alloc->in(AllocateNode::DefaultValue) == val) {\n+              result = mem;\n+            }\n+          }\n@@ -3054,1 +3146,1 @@\n-    return new ClearArrayNode(in(0), in(1), in(2), in(3), true);\n+    return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);\n@@ -3069,1 +3161,1 @@\n-  Node *zero = phase->makecon(TypeLong::ZERO);\n+  Node *val = in(4);\n@@ -3071,1 +3163,1 @@\n-  mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+  mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -3076,1 +3168,1 @@\n-    mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+    mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -3110,0 +3202,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -3120,1 +3214,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != NULL) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == NULL, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -3127,1 +3227,1 @@\n-  return clear_memory(ctl, mem, dest, phase->MakeConX(offset), end_offset, phase);\n+  return clear_memory(ctl, mem, dest, raw_val, phase->MakeConX(offset), end_offset, phase);\n@@ -3131,0 +3231,1 @@\n+                                   Node* raw_val,\n@@ -3153,1 +3254,4 @@\n-  mem = new ClearArrayNode(ctl, mem, zsize, adr, false);\n+  if (raw_val == NULL) {\n+    raw_val = phase->MakeConX(0);\n+  }\n+  mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);\n@@ -3158,0 +3262,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -3172,1 +3278,1 @@\n-    mem = clear_memory(ctl, mem, dest,\n+    mem = clear_memory(ctl, mem, dest, val, raw_val,\n@@ -3179,1 +3285,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != NULL) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == NULL, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -3318,1 +3430,1 @@\n-Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {\n+Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {\n@@ -3604,1 +3716,3 @@\n-  if (init == NULL || init->is_complete())  return false;\n+  if (init == NULL || init->is_complete()) {\n+    return false;\n+  }\n@@ -4366,0 +4480,2 @@\n+                                              allocation()->in(AllocateNode::DefaultValue),\n+                                              allocation()->in(AllocateNode::RawDefaultValue),\n@@ -4425,0 +4541,2 @@\n+                                            allocation()->in(AllocateNode::DefaultValue),\n+                                            allocation()->in(AllocateNode::RawDefaultValue),\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":176,"deletions":58,"binary":false,"changes":234,"status":"modified"},{"patch":"@@ -129,0 +129,4 @@\n+#ifdef ASSERT\n+  void set_adr_type(const TypePtr* adr_type) { _adr_type = adr_type; }\n+#endif\n+\n@@ -551,1 +555,0 @@\n-\n@@ -1133,0 +1136,1 @@\n+  bool _word_copy_only;\n@@ -1134,2 +1138,3 @@\n-  ClearArrayNode( Node *ctrl, Node *arymem, Node *word_cnt, Node *base, bool is_large)\n-    : Node(ctrl,arymem,word_cnt,base), _is_large(is_large) {\n+  ClearArrayNode( Node *ctrl, Node *arymem, Node *word_cnt, Node *base, Node* val, bool is_large)\n+    : Node(ctrl, arymem, word_cnt, base, val), _is_large(is_large),\n+      _word_copy_only(val->bottom_type()->isa_long() && (!val->bottom_type()->is_long()->is_con() || val->bottom_type()->is_long()->get_con() != 0)) {\n@@ -1147,0 +1152,1 @@\n+  bool word_copy_only() const { return _word_copy_only; }\n@@ -1153,0 +1159,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -1157,0 +1165,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -1161,0 +1171,1 @@\n+                            Node* raw_val,\n@@ -1212,1 +1223,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -160,0 +160,12 @@\n+  \/\/ Code pattern on return from a call that returns an __Value.  Can\n+  \/\/ be optimized away if the return value turns out to be an oop.\n+  if (op == Op_AndX &&\n+      in(1) != NULL &&\n+      in(1)->Opcode() == Op_CastP2X &&\n+      in(1)->in(1) != NULL &&\n+      phase->type(in(1)->in(1))->isa_oopptr() &&\n+      t2->isa_intptr_t()->_lo >= 0 &&\n+      t2->isa_intptr_t()->_hi <= MinObjAlignmentInBytesMask) {\n+    return add_id();\n+  }\n+\n@@ -593,0 +605,8 @@\n+\n+    \/\/ Check if this is part of an inline type test\n+    if (con == markWord::inline_type_pattern && in(1)->is_Load() &&\n+        phase->type(in(1)->in(MemNode::Address))->is_ptr()->offset() == oopDesc::mark_offset_in_bytes() &&\n+        phase->type(in(1)->in(MemNode::Address))->is_inlinetypeptr()) {\n+      assert(EnableValhalla, \"should only be used for inline types\");\n+      return in(2); \/\/ Obj is known to be an inline type\n+    }\n","filename":"src\/hotspot\/share\/opto\/mulnode.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -564,0 +564,3 @@\n+  if (n->is_InlineTypeBase()) {\n+    C->add_inline_type(n);\n+  }\n@@ -643,0 +646,3 @@\n+  if (is_InlineTypeBase()) {\n+    compile->remove_inline_type(this);\n+  }\n@@ -1415,0 +1421,3 @@\n+      if (dead->is_InlineTypeBase()) {\n+        igvn->C->remove_inline_type(dead);\n+      }\n@@ -2178,1 +2187,3 @@\n-      assert(i >= req() || i == 0 || is_Region() || is_Phi() || is_ArrayCopy() || (is_Unlock() && i == req()-1)\n+      assert(i >= req() || i == 0 || is_Region() || is_Phi() || is_ArrayCopy() ||\n+             (is_Allocate() && i >= AllocateNode::InlineTypeNode) ||\n+             (is_Unlock() && i == req()-1)\n@@ -2180,1 +2191,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+             \"only region, phi, arraycopy, allocate or unlock nodes have null data edges\");\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+class FlatArrayCheckNode;\n@@ -108,0 +109,1 @@\n+class MachPrologNode;\n@@ -114,0 +116,1 @@\n+class MachVEPNode;\n@@ -158,0 +161,3 @@\n+class InlineTypeBaseNode;\n+class InlineTypeNode;\n+class InlineTypePtrNode;\n@@ -672,0 +678,2 @@\n+      DEFINE_CLASS_ID(MachProlog,       Mach, 8)\n+      DEFINE_CLASS_ID(MachVEP,          Mach, 9)\n@@ -687,0 +695,3 @@\n+      DEFINE_CLASS_ID(InlineTypeBase, Type, 8)\n+        DEFINE_CLASS_ID(InlineType, InlineTypeBase, 0)\n+        DEFINE_CLASS_ID(InlineTypePtr, InlineTypeBase, 1)\n@@ -719,3 +730,4 @@\n-        DEFINE_CLASS_ID(FastLock,   Cmp, 0)\n-        DEFINE_CLASS_ID(FastUnlock, Cmp, 1)\n-        DEFINE_CLASS_ID(SubTypeCheck,Cmp, 2)\n+        DEFINE_CLASS_ID(FastLock,       Cmp, 0)\n+        DEFINE_CLASS_ID(FastUnlock,     Cmp, 1)\n+        DEFINE_CLASS_ID(SubTypeCheck,   Cmp, 2)\n+        DEFINE_CLASS_ID(FlatArrayCheck, Cmp, 3)\n@@ -845,0 +857,1 @@\n+  DEFINE_CLASS_QUERY(FlatArrayCheck)\n@@ -875,0 +888,1 @@\n+  DEFINE_CLASS_QUERY(MachProlog)\n@@ -881,0 +895,1 @@\n+  DEFINE_CLASS_QUERY(MachVEP)\n@@ -905,0 +920,3 @@\n+  DEFINE_CLASS_QUERY(InlineType)\n+  DEFINE_CLASS_QUERY(InlineTypeBase)\n+  DEFINE_CLASS_QUERY(InlineTypePtr)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":21,"deletions":3,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -312,0 +312,2 @@\n+    _sp_inc_slot(0),\n+    _sp_inc_slot_offset_in_bytes(0),\n@@ -317,1 +319,6 @@\n-    _orig_pc_slot = C->fixed_slots() - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n+    int fixed_slots = C->fixed_slots();\n+    if (C->needs_stack_repair()) {\n+      fixed_slots -= 2;\n+      _sp_inc_slot = fixed_slots;\n+    }\n+    _orig_pc_slot = fixed_slots - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n@@ -356,1 +363,2 @@\n-  MachPrologNode *prolog = new MachPrologNode();\n+  Label verified_entry;\n+  MachPrologNode* prolog = new MachPrologNode(&verified_entry);\n@@ -362,3 +370,2 @@\n-\n-  if( C->is_osr_compilation() ) {\n-    if( PoisonOSREntry ) {\n+  if (C->is_osr_compilation()) {\n+    if (PoisonOSREntry) {\n@@ -369,3 +376,14 @@\n-    if( C->method() && !C->method()->flags().is_static() ) {\n-      \/\/ Insert unvalidated entry point\n-      C->cfg()->insert( broot, 0, new MachUEPNode() );\n+    if (C->method()) {\n+      if (C->method()->has_scalarized_args()) {\n+        \/\/ Add entry point to unpack all inline type arguments\n+        C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true, \/* receiver_only *\/ false));\n+        if (!C->method()->is_static()) {\n+          \/\/ Add verified\/unverified entry points to only unpack inline type receiver at interface calls\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ false));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true,  \/* receiver_only *\/ true));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ true));\n+        }\n+      } else if (!C->method()->is_static()) {\n+        \/\/ Insert unvalidated entry point\n+        C->cfg()->insert(broot, 0, new MachUEPNode());\n+      }\n@@ -373,1 +391,0 @@\n-\n@@ -413,0 +430,25 @@\n+  if (!C->is_osr_compilation() && C->has_scalarized_args()) {\n+    \/\/ Compute the offsets of the entry points required by the inline type calling convention\n+    if (!C->method()->is_static()) {\n+      \/\/ We have entries at the beginning of the method, implemented by the first 4 nodes.\n+      \/\/ Entry                     (unverified) @ offset 0\n+      \/\/ Verified_Inline_Entry_RO\n+      \/\/ Inline_Entry              (unverified)\n+      \/\/ Verified_Inline_Entry\n+      uint offset = 0;\n+      _code_offsets.set_value(CodeOffsets::Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(0))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(1))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Inline_Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(2))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, offset);\n+    } else {\n+      _code_offsets.set_value(CodeOffsets::Entry, -1); \/\/ will be patched later\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, 0);\n+    }\n+  }\n+\n@@ -570,1 +612,3 @@\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != NULL) {\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -1008,0 +1052,1 @@\n+  bool return_vt = false;\n@@ -1030,1 +1075,1 @@\n-    if (mcall->returns_pointer()) {\n+    if (mcall->returns_pointer() || mcall->returns_vt()) {\n@@ -1033,0 +1078,3 @@\n+    if (mcall->returns_vt()) {\n+      return_vt = true;\n+    }\n@@ -1157,0 +1205,1 @@\n+      return_vt,\n@@ -1267,0 +1316,4 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Compute the byte offset of the stack increment value\n+    _sp_inc_slot_offset_in_bytes = C->regalloc()->reg2offset(OptoReg::stack2reg(_sp_inc_slot));\n+  }\n@@ -1543,2 +1596,4 @@\n-          \/\/ This destination address is NOT PC-relative\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != NULL) {\n+            \/\/ This destination address is NOT PC-relative\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -1698,1 +1753,1 @@\n-\n+#if 0 \/\/ new assert, since moved below \"if (C->failing())\", but always triggers in Valhalla\n@@ -1700,1 +1755,1 @@\n-\n+#endif\n@@ -3274,0 +3329,10 @@\n+    if (C->has_scalarized_args()) {\n+      \/\/ Inline type entry points (MachVEPNodes) require lots of space for GC barriers and oop verification\n+      \/\/ when loading object fields from the buffered argument. Increase scratch buffer size accordingly.\n+      int barrier_size = UseZGC ? 200 : (7 DEBUG_ONLY(+ 37));\n+      for (ciSignatureStream str(C->method()->signature()); !str.at_return_type(); str.next()) {\n+        if (str.type()->is_inlinetype() && str.type()->as_inline_klass()->can_be_passed_as_fields()) {\n+          size += str.type()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+      }\n+    }\n@@ -3338,0 +3403,6 @@\n+  } else if (n->is_MachProlog()) {\n+    saveL = ((MachPrologNode*)n)->_verified_entry;\n+    ((MachPrologNode*)n)->_verified_entry = &fakeL;\n+  } else if (n->is_MachVEP()) {\n+    saveL = ((MachVEPNode*)n)->_verified_entry;\n+    ((MachVEPNode*)n)->_verified_entry = &fakeL;\n@@ -3344,1 +3415,2 @@\n-  if (is_branch) \/\/ Restore label.\n+  \/\/ Restore label.\n+  if (is_branch) {\n@@ -3346,0 +3418,5 @@\n+  } else if (n->is_MachProlog()) {\n+    ((MachPrologNode*)n)->_verified_entry = saveL;\n+  } else if (n->is_MachVEP()) {\n+    ((MachVEPNode*)n)->_verified_entry = saveL;\n+  }\n@@ -3390,0 +3467,9 @@\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry_RO) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);\n+      }\n@@ -3394,13 +3480,13 @@\n-                                     entry_bci,\n-                                     &_code_offsets,\n-                                     _orig_pc_slot_offset_in_bytes,\n-                                     code_buffer(),\n-                                     frame_size_in_words(),\n-                                     oop_map_set(),\n-                                     &_handler_table,\n-                                     inc_table(),\n-                                     compiler,\n-                                     has_unsafe_access,\n-                                     SharedRuntime::is_wide_vector(C->max_vector_size()),\n-                                     C->rtm_state(),\n-                                     C->native_invokers());\n+                              entry_bci,\n+                              &_code_offsets,\n+                              _orig_pc_slot_offset_in_bytes,\n+                              code_buffer(),\n+                              frame_size_in_words(),\n+                              _oop_map_set,\n+                              &_handler_table,\n+                              &_inc_table,\n+                              compiler,\n+                              has_unsafe_access,\n+                              SharedRuntime::is_wide_vector(C->max_vector_size()),\n+                              C->rtm_state(),\n+                              C->native_invokers());\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":115,"deletions":29,"binary":false,"changes":144,"status":"modified"},{"patch":"@@ -1077,1 +1077,1 @@\n-    if (!n->is_Con() && !_worklist.member(n)) {\n+    if (n->outcnt() != 0 && !n->is_Con() && !_worklist.member(n)) {\n@@ -1091,1 +1091,1 @@\n-    if (!n->is_Con()) { \/\/ skip Con nodes\n+    if (n->outcnt() != 0 && !n->is_Con()) { \/\/ skip dead and Con nodes\n@@ -1192,6 +1192,0 @@\n-  if (_delay_transform) {\n-    \/\/ Register the node but don't optimize for now\n-    register_new_node_with_optimizer(n);\n-    return n;\n-  }\n-\n@@ -1204,0 +1198,6 @@\n+  if (_delay_transform) {\n+    \/\/ Add the node to the worklist but don't optimize for now\n+    _worklist.push(n);\n+    return n;\n+  }\n+\n@@ -1420,0 +1420,3 @@\n+      if (dead->is_InlineTypeBase()) {\n+        C->remove_inline_type(dead);\n+      }\n@@ -1483,0 +1486,13 @@\n+void PhaseIterGVN::replace_in_uses(Node* n, Node* m) {\n+  assert(n != NULL, \"sanity\");\n+  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+    Node* u = n->fast_out(i);\n+    if (u != n) {\n+      rehash_node_delayed(u);\n+      int nb = u->replace_edge(n, m);\n+      --i, imax -= nb;\n+    }\n+  }\n+  assert(n->outcnt() == 0, \"all uses must be deleted\");\n+}\n+\n@@ -1583,0 +1599,9 @@\n+    \/\/ Inline type nodes can have other inline types as users. If an input gets\n+    \/\/ updated, make sure that inline type users get a chance for optimization.\n+    if (use->is_InlineTypeBase()) {\n+      for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+        Node* u = use->fast_out(i2);\n+        if (u->is_InlineTypeBase())\n+          _worklist.push(u);\n+      }\n+    }\n@@ -1628,0 +1653,8 @@\n+    if (use_op == Op_CastP2X) {\n+      for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+        Node* u = use->fast_out(i2);\n+        if (u->Opcode() == Op_AndX) {\n+          _worklist.push(u);\n+        }\n+      }\n+    }\n@@ -1652,0 +1685,11 @@\n+\n+    \/\/ Give CallStaticJavaNode::remove_useless_allocation a chance to run\n+    if (use->is_Region()) {\n+      Node* c = use;\n+      do {\n+        c = c->unique_ctrl_out();\n+      } while (c != NULL && c->is_Region());\n+      if (c != NULL && c->is_CallStaticJava() && c->as_CallStaticJava()->uncommon_trap_request() != 0) {\n+        _worklist.push(c);\n+      }\n+    }\n@@ -1790,0 +1834,8 @@\n+        if (m_op == Op_CastP2X) {\n+          for (DUIterator_Fast i2max, i2 = m->fast_outs(i2max); i2 < i2max; i2++) {\n+            Node* u = m->fast_out(i2);\n+            if (u->Opcode() == Op_AndX) {\n+              worklist.push(u);\n+            }\n+          }\n+        }\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":60,"deletions":8,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -468,1 +468,1 @@\n-  virtual void record_for_igvn(Node *n) { }\n+  virtual void record_for_igvn(Node *n) { _worklist.push(n); }\n@@ -518,0 +518,2 @@\n+  void replace_in_uses(Node* n, Node* m);\n+\n","filename":"src\/hotspot\/share\/opto\/phaseX.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -188,1 +188,1 @@\n-      for (uint i = TypeFunc::Parms; i < call->tf()->domain()->cnt(); i++) {\n+      for (uint i = TypeFunc::Parms; i < call->tf()->domain_sig()->cnt(); i++) {\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -57,0 +57,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -667,1 +668,22 @@\n-  return handle == NULL ? 0 : ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)) ;\n+  if (handle == NULL) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, Universe::inline_type_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(SystemDictionary::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return ObjectSynchronizer::FastHashCode(THREAD, obj);\n+  }\n@@ -723,0 +745,1 @@\n+       klass->is_inline_klass() ||\n@@ -1268,1 +1291,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1271,1 +1295,1 @@\n-    size = 2;\n+    size = 3;\n@@ -1281,1 +1305,2 @@\n-      Klass* k = InstanceKlass::cast(klass)->local_interfaces()->at(index);\n+      InstanceKlass* ik = InstanceKlass::cast(klass);\n+      Klass* k = ik->local_interfaces()->at(index);\n@@ -1285,1 +1310,1 @@\n-    \/\/ All arrays implement java.lang.Cloneable and java.io.Serializable\n+    \/\/ All arrays implement java.lang.Cloneable, java.io.Serializable and java.lang.IdentityObject\n@@ -1288,0 +1313,1 @@\n+    result->obj_at_put(2, SystemDictionary::IdentityObject_klass()->java_mirror());\n@@ -1902,0 +1928,2 @@\n+  bool is_ctor = (method->is_object_constructor() ||\n+                  method->is_static_init_factory());\n@@ -1903,1 +1931,1 @@\n-    return (method->is_initializer() && !method->is_static());\n+    return is_ctor;\n@@ -1905,1 +1933,3 @@\n-    return  (!method->is_initializer() && !method->is_overpass());\n+    return (!is_ctor &&\n+            !method->is_class_initializer() &&\n+            !method->is_overpass());\n@@ -1968,0 +1998,2 @@\n+        assert(method->is_object_constructor() ||\n+               method->is_static_init_factory(), \"must be\");\n@@ -2227,3 +2259,1 @@\n-  if (!m->is_initializer() || m->is_static()) {\n-    method = Reflection::new_method(m, true, CHECK_NULL);\n-  } else {\n+  if (m->is_object_constructor() || m->is_static_init_factory()) {\n@@ -2231,0 +2261,2 @@\n+  } else {\n+    method = Reflection::new_method(m, true, CHECK_NULL);\n@@ -2518,0 +2550,39 @@\n+\/\/ Arrays support \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+JVM_ENTRY(jboolean, JVM_ArrayIsAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  JVMWrapper(\"JVM_ArrayIsAccessAtomic\");\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == NULL) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  return ArrayKlass::cast(k)->element_access_is_atomic();\n+JVM_END\n+\n+JVM_ENTRY(jobject, JVM_ArrayEnsureAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  JVMWrapper(\"JVM_ArrayEnsureAccessAtomic\");\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == NULL) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vk = FlatArrayKlass::cast(k);\n+    if (!vk->element_access_is_atomic()) {\n+      \/**\n+       * Need to decide how to implement:\n+       *\n+       * 1) Change to objArrayOop layout, therefore oop->klass() differs so\n+       * then \"<atomic>[Qfoo;\" klass needs to subclass \"[Qfoo;\" to pass through\n+       * \"checkcast\" & \"instanceof\"\n+       *\n+       * 2) Use extra header in the flatArrayOop to flag atomicity required and\n+       * possibly per instance lock structure. Said info, could be placed in\n+       * \"trailer\" rather than disturb the current arrayOop\n+       *\/\n+      Unimplemented();\n+    }\n+  }\n+  return array;\n+JVM_END\n+\n@@ -2697,1 +2768,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3728,1 +3799,1 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3749,0 +3820,1 @@\n+  objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3750,1 +3822,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":84,"deletions":13,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -125,5 +125,5 @@\n-  IS_METHOD            = java_lang_invoke_MemberName::MN_IS_METHOD,\n-  IS_CONSTRUCTOR       = java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR,\n-  IS_FIELD             = java_lang_invoke_MemberName::MN_IS_FIELD,\n-  IS_TYPE              = java_lang_invoke_MemberName::MN_IS_TYPE,\n-  CALLER_SENSITIVE     = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,\n+  IS_METHOD             = java_lang_invoke_MemberName::MN_IS_METHOD,\n+  IS_OBJECT_CONSTRUCTOR = java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR,\n+  IS_FIELD              = java_lang_invoke_MemberName::MN_IS_FIELD,\n+  IS_TYPE               = java_lang_invoke_MemberName::MN_IS_TYPE,\n+  CALLER_SENSITIVE      = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,\n@@ -131,8 +131,8 @@\n-  REFERENCE_KIND_SHIFT = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,\n-  REFERENCE_KIND_MASK  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,\n-  SEARCH_SUPERCLASSES  = java_lang_invoke_MemberName::MN_SEARCH_SUPERCLASSES,\n-  SEARCH_INTERFACES    = java_lang_invoke_MemberName::MN_SEARCH_INTERFACES,\n-  LM_UNCONDITIONAL     = java_lang_invoke_MemberName::MN_UNCONDITIONAL_MODE,\n-  LM_MODULE            = java_lang_invoke_MemberName::MN_MODULE_MODE,\n-  LM_TRUSTED           = java_lang_invoke_MemberName::MN_TRUSTED_MODE,\n-  ALL_KINDS      = IS_METHOD | IS_CONSTRUCTOR | IS_FIELD | IS_TYPE\n+  REFERENCE_KIND_SHIFT  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,\n+  REFERENCE_KIND_MASK   = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,\n+  SEARCH_SUPERCLASSES   = java_lang_invoke_MemberName::MN_SEARCH_SUPERCLASSES,\n+  SEARCH_INTERFACES     = java_lang_invoke_MemberName::MN_SEARCH_INTERFACES,\n+  LM_UNCONDITIONAL      = java_lang_invoke_MemberName::MN_UNCONDITIONAL_MODE,\n+  LM_MODULE             = java_lang_invoke_MemberName::MN_MODULE_MODE,\n+  LM_TRUSTED            = java_lang_invoke_MemberName::MN_TRUSTED_MODE,\n+  ALL_KINDS      = IS_METHOD | IS_OBJECT_CONSTRUCTOR | IS_FIELD | IS_TYPE\n@@ -149,1 +149,1 @@\n-    flags |= IS_CONSTRUCTOR;\n+    flags |= IS_OBJECT_CONSTRUCTOR;\n@@ -168,1 +168,1 @@\n-    case IS_CONSTRUCTOR:\n+    case IS_OBJECT_CONSTRUCTOR:\n@@ -310,2 +310,2 @@\n-    } else if (m->is_initializer()) {\n-      flags |= IS_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);\n+    } else if (m->is_object_constructor()) {\n+      flags |= IS_OBJECT_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);\n@@ -349,0 +349,3 @@\n+  if (fd.is_inlined()) {\n+    flags |= JVM_ACC_FIELD_INLINED;\n+  }\n@@ -805,1 +808,1 @@\n-  case IS_CONSTRUCTOR:\n+  case IS_OBJECT_CONSTRUCTOR:\n@@ -811,1 +814,3 @@\n-        if (name == vmSymbols::object_initializer_name()) {\n+        if (name != vmSymbols::object_initializer_name()) {\n+          break;                \/\/ will throw after end of switch\n+        } else if (type->is_void_method_signature()) {\n@@ -814,1 +819,2 @@\n-          break;                \/\/ will throw after end of switch\n+          \/\/ LinkageError unless it returns something reasonable\n+          LinkResolver::resolve_static_call(result, link_info, false, THREAD);\n@@ -874,1 +880,1 @@\n-  case IS_CONSTRUCTOR:\n+  case IS_OBJECT_CONSTRUCTOR:\n@@ -953,1 +959,1 @@\n-      match_flags &= ~(IS_CONSTRUCTOR | IS_METHOD);\n+      match_flags &= ~(IS_OBJECT_CONSTRUCTOR | IS_METHOD);\n@@ -983,1 +989,1 @@\n-  if ((match_flags & (IS_METHOD | IS_CONSTRUCTOR)) != 0) {\n+  if ((match_flags & (IS_METHOD | IS_OBJECT_CONSTRUCTOR)) != 0) {\n@@ -988,2 +994,2 @@\n-    bool negate_name_test = false;\n-    \/\/ fix name so that it captures the intention of IS_CONSTRUCTOR\n+    bool ctor_ok = true, sfac_ok = true;\n+    \/\/ fix name so that it captures the intention of IS_OBJECT_CONSTRUCTOR\n@@ -997,1 +1003,2 @@\n-    } else if (!(match_flags & IS_CONSTRUCTOR)) {\n+      sfac_ok = false;\n+    } else if (!(match_flags & IS_OBJECT_CONSTRUCTOR)) {\n@@ -999,6 +1006,1 @@\n-      if (name == NULL) {\n-        name = init_name;\n-        negate_name_test = true; \/\/ if we see the name, we *omit* the entry\n-      } else if (name == init_name) {\n-        return 0;               \/\/ no methods of this constructor name\n-      }\n+      ctor_ok = false;  \/\/ but sfac_ok is true, so we might find <init>\n@@ -1014,1 +1016,1 @@\n-      if (name != NULL && ((m_name != name) ^ negate_name_test))\n+      if (name != NULL && m_name != name)\n@@ -1018,0 +1020,4 @@\n+      if (m_name == init_name) {  \/\/ might be either ctor or sfac\n+        if (m->is_object_constructor()  && !ctor_ok)  continue;\n+        if (m->is_static_init_factory() && !sfac_ok)  continue;\n+      }\n@@ -1117,1 +1123,1 @@\n-    template(java_lang_invoke_MemberName,MN_IS_CONSTRUCTOR) \\\n+    template(java_lang_invoke_MemberName,MN_IS_OBJECT_CONSTRUCTOR) \\\n@@ -1261,1 +1267,1 @@\n-               (flags & ALL_KINDS) == IS_CONSTRUCTOR) {\n+               (flags & ALL_KINDS) == IS_OBJECT_CONSTRUCTOR) {\n","filename":"src\/hotspot\/share\/prims\/methodHandles.cpp","additions":41,"deletions":35,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -56,0 +57,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -61,0 +63,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -1795,0 +1798,92 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return NULL;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(aoop->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return NULL;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayOop result_array =\n+      oopFactory::new_objArray(SystemDictionary::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  instanceOop ioop = ih();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ioop->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array);\n+WB_END\n+\n+class CollectOops : public BasicOopIterateClosure {\n+ public:\n+  GrowableArray<Handle>* array;\n+\n+  objArrayOop create_results(TRAPS) {\n+    objArrayOop result_array =\n+        oopFactory::new_objArray(SystemDictionary::Object_klass(), array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < array->length(); i++) {\n+      result_array->obj_at_put(i, array->at(i)());\n+    }\n+    return result_array;\n+  }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return (jobjectArray)JNIHandles::make_local(THREAD, create_results(THREAD));\n+  }\n+\n+  void add_oop(oop o) {\n+    \/\/ Value might be oop, but JLS can't see as Object, just iterate through it...\n+    if (o != NULL && o->is_inline_type()) {\n+      o->oop_iterate(this);\n+    } else {\n+      array->append(Handle(Thread::current(), o));\n+    }\n+  }\n+\n+  void do_oop(oop* o) { add_oop(*o); }\n+  void do_oop(narrowOop* v) { add_oop(CompressedOops::decode(*v)); }\n+};\n+\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+\n+  JNIHandles::resolve(thing)->oop_iterate(&collectOops);\n+\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  StackFrameStream sfs(thread, false \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, NULL, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\n@@ -2478,0 +2573,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":101,"deletions":0,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -2148,0 +2148,10 @@\n+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypePassFieldsAsArgs)) {\n+    FLAG_SET_CMDLINE(InlineTypePassFieldsAsArgs, false);\n+    warning(\"InlineTypePassFieldsAsArgs is not supported on this platform\");\n+  }\n+\n+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypeReturnedAsFields)) {\n+    FLAG_SET_CMDLINE(InlineTypeReturnedAsFields, false);\n+    warning(\"InlineTypeReturnedAsFields is not supported on this platform\");\n+  }\n+\n@@ -3070,0 +3080,18 @@\n+  if (EnableValhalla) {\n+    \/\/ create_property(\"valhalla.enableValhalla\", \"true\", InternalProperty)\n+    const char* prop_name = \"valhalla.enableValhalla\";\n+    const char* prop_value = \"true\";\n+    const size_t prop_len = strlen(prop_name) + strlen(prop_value) + 2;\n+    char* property = AllocateHeap(prop_len, mtArguments);\n+    int ret = jio_snprintf(property, prop_len, \"%s=%s\", prop_name, prop_value);\n+    if (ret < 0 || ret >= (int)prop_len) {\n+      FreeHeap(property);\n+      return JNI_ENOMEM;\n+    }\n+    bool added = add_property(property, UnwriteableProperty, InternalProperty);\n+    FreeHeap(property);\n+    if (!added) {\n+      return JNI_ENOMEM;\n+    }\n+  }\n+\n@@ -3184,0 +3212,5 @@\n+  if (UseBiasedLocking) {\n+    jio_fprintf(defaultStream::error_stream(), \"Valhalla does not support use with UseBiasedLocking\");\n+    return JNI_ERR;\n+  }\n+\n@@ -4198,0 +4231,5 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !is_dumping_archive())) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported\n+    InlineTypePassFieldsAsArgs = false;\n+    InlineTypeReturnedAsFields = false;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -55,0 +56,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -289,0 +293,19 @@\n+\n+#ifdef COMPILER1\n+  if (cm->is_compiled_by_c1() && cm->method()->has_scalarized_args() &&\n+      pc() < cm->verified_inline_entry_point()) {\n+    \/\/ The VEP and VIEP(RO) of C1-compiled methods call into the runtime to buffer scalarized value\n+    \/\/ type args. We can't deoptimize at that point because the buffers have not yet been initialized.\n+    \/\/ Also, if the method is synchronized, we first need to acquire the lock.\n+    \/\/ Don't patch the return pc to delay deoptimization until we enter the method body (the check\n+    \/\/ addedin LIRGenerator::do_Base will detect the pending deoptimization by checking the original_pc).\n+#ifdef ASSERT\n+    NativeCall* call = nativeCall_before(this->pc());\n+    address dest = call->destination();\n+    assert(dest == Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id) ||\n+           dest == Runtime1::entry_for(Runtime1::buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+    return;\n+  }\n+#endif\n+\n@@ -684,1 +707,1 @@\n-                          OopClosure* f) {\n+                          OopClosure* f, BufferedValueClosure* bvt_f) {\n@@ -696,1 +719,3 @@\n-      _f->do_oop(addr);\n+      if (_f != NULL) {\n+        _f->do_oop(addr);\n+      }\n@@ -708,1 +733,3 @@\n-        _f->do_oop(addr);\n+        if (_f != NULL) {\n+          _f->do_oop(addr);\n+        }\n@@ -883,1 +910,1 @@\n-  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f);\n+  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f, NULL);\n@@ -895,0 +922,17 @@\n+void frame::buffered_values_interpreted_do(BufferedValueClosure* f) {\n+  assert(is_interpreted_frame(), \"Not an interpreted frame\");\n+  Thread *thread = Thread::current();\n+  methodHandle m (thread, interpreter_frame_method());\n+  jint      bci = interpreter_frame_bci();\n+\n+  assert(m->is_method(), \"checking frame value\");\n+  assert(!m->is_native() && bci >= 0 && bci < m->code_size(),\n+         \"invalid bci value\");\n+\n+  InterpreterFrameClosure blk(this, m->max_locals(), m->max_stack(), NULL, f);\n+\n+  \/\/ process locals & expression stack\n+  InterpreterOopMap mask;\n+  m->mask_for(bci, &mask);\n+  mask.iterate_oop(&blk);\n+}\n@@ -942,0 +986,1 @@\n+    assert(_offset < _arg_size, \"out of bounds\");\n@@ -959,5 +1004,1 @@\n-    _arg_size  = ArgumentSizeComputer(signature).size() + (has_receiver ? 1 : 0) + (has_appendix ? 1 : 0);\n-\n-    int arg_size;\n-    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &arg_size);\n-    assert(arg_size == _arg_size, \"wrong arg size\");\n+    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &_arg_size);\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":50,"deletions":9,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -808,0 +808,18 @@\n+  notproduct(bool, PrintInlineLayout, false,                                \\\n+          \"Print field layout for each inline type\")                        \\\n+                                                                            \\\n+  notproduct(bool, PrintFlatArrayLayout, false,                             \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxSize, -1,                                \\\n+          \"Max size for flattening inline array elements, <0 no limit\")     \\\n+                                                                            \\\n+  product(intx, InlineFieldMaxFlatSize, 128,                                \\\n+          \"Max size for flattening inline type fields, <0 no limit\")        \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  product(bool, InlineArrayAtomicAccess, false,                             \\\n+          \"Atomic inline array accesses by-default, for all inline arrays\") \\\n+                                                                            \\\n@@ -823,2 +841,2 @@\n-  product(bool, UseBiasedLocking, false,                                    \\\n-          \"(Deprecated) Enable biased locking in JVM\")                      \\\n+  product(bool, UseBiasedLocking, false,                                     \\\n+          \"(Deprecated) Enable biased locking in JVM (completely disabled by Valhalla)\") \\\n@@ -2489,0 +2507,23 @@\n+  product(bool, EnableValhalla, true,                                       \\\n+          \"Enable experimental Valhalla features\")                          \\\n+                                                                            \\\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressInlineTypeReturnedAsFields, false,                    \\\n+          \"Stress return of fields instead of an inline type reference\")    \\\n+                                                                            \\\n+  develop(bool, ScalarizeInlineTypes, true,                                 \\\n+          \"Scalarize inline types in compiled code\")                        \\\n+                                                                            \\\n+  product(bool, UseArrayMarkWordCheck, NOT_LP64(false) LP64_ONLY(true),     \\\n+          \"Use bits in the mark word to check for flat\/null-free arrays\")   \\\n+                                                                            \\\n+  product(ccstrlist, ForceNonTearable, \"\", DIAGNOSTIC,                      \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n@@ -2502,0 +2543,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":44,"deletions":2,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -118,0 +118,1 @@\n+  VMRegImpl::set_regName();  \/\/ need this before generate_stubs (for printing oop maps).\n@@ -128,1 +129,0 @@\n-  VMRegImpl::set_regName(); \/\/ need this before generate_stubs (for printing oop maps).\n","filename":"src\/hotspot\/share\/runtime\/init.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -924,0 +925,1 @@\n+    ResourceMark rm;\n@@ -925,2 +927,25 @@\n-    bool return_oop = nm->method()->is_returning_oop();\n-    Handle return_value;\n+    Method* method = nm->method();\n+    bool return_oop = method->is_returning_oop();\n+\n+    GrowableArray<Handle> return_values;\n+    InlineKlass* vk = NULL;\n+\n+    if (return_oop && InlineTypeReturnedAsFields) {\n+      SignatureStream ss(method->signature());\n+      while (!ss.at_return_type()) {\n+        ss.next();\n+      }\n+      if (ss.type() == T_INLINE_TYPE) {\n+        \/\/ Check if inline type is returned as fields\n+        vk = InlineKlass::returned_inline_klass(map);\n+        if (vk != NULL) {\n+          \/\/ We're at a safepoint at the return of a method that returns\n+          \/\/ multiple values. We must make sure we preserve the oop values\n+          \/\/ across the safepoint.\n+          assert(vk == method->returned_inline_type(thread()), \"bad inline klass\");\n+          vk->save_oop_fields(map, return_values);\n+          return_oop = false;\n+        }\n+      }\n+    }\n+\n@@ -933,1 +958,1 @@\n-      return_value = Handle(self, result);\n+      return_values.push(Handle(self, result));\n@@ -947,1 +972,4 @@\n-      caller_fr.set_saved_oop_result(&map, return_value());\n+      assert(return_values.length() == 1, \"only one return value\");\n+      caller_fr.set_saved_oop_result(&map, return_values.pop()());\n+    } else if (vk != NULL) {\n+      vk->restore_oop_results(map, return_values);\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"asm\/codeBuffer.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -40,0 +42,1 @@\n+class SigEntry;\n@@ -56,1 +59,2 @@\n-                                         bool is_optimized, TRAPS);\n+                                         bool is_optimized,\n+                                         bool* caller_is_c1, TRAPS);\n@@ -66,1 +70,0 @@\n-  static address             _resolve_static_call_entry;\n@@ -87,1 +90,0 @@\n-\n@@ -320,1 +322,2 @@\n-                                     bool is_optimized, TRAPS);\n+                                     bool is_optimized,\n+                                     bool* caller_is_c1, TRAPS);\n@@ -328,1 +331,1 @@\n-                                             bool& needs_ic_stub_refill, TRAPS);\n+                                             bool& needs_ic_stub_refill, bool& is_optimized, bool caller_is_c1, TRAPS);\n@@ -334,1 +337,1 @@\n-  static methodHandle reresolve_call_site(JavaThread *thread, TRAPS);\n+  static methodHandle reresolve_call_site(JavaThread *thread, bool& is_static_call, bool& is_optimized, bool& caller_is_c1, TRAPS);\n@@ -338,1 +341,1 @@\n-  static methodHandle handle_ic_miss_helper(JavaThread* thread, TRAPS);\n+  static methodHandle handle_ic_miss_helper(JavaThread* thread, bool& is_optimized, bool& caller_is_c1, TRAPS);\n@@ -347,0 +350,13 @@\n+  static address entry_for_handle_wrong_method(methodHandle callee_method, bool is_static_call, bool is_optimized, bool caller_is_c1) {\n+    assert(callee_method->verified_code_entry() != NULL, \"Jump to zero!\");\n+    assert(callee_method->verified_inline_code_entry() != NULL, \"Jump to zero!\");\n+    assert(callee_method->verified_inline_ro_code_entry() != NULL, \"Jump to zero!\");\n+    if (caller_is_c1) {\n+      return callee_method->verified_inline_code_entry();\n+    } else if (is_static_call || is_optimized) {\n+      return callee_method->verified_code_entry();\n+    } else {\n+      return callee_method->verified_inline_ro_code_entry();\n+    }\n+  }\n+\n@@ -374,0 +390,8 @@\n+  static int java_calling_convention(const GrowableArray<SigEntry>* sig, VMRegPair* regs) {\n+    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig->length());\n+    int total_args_passed = SigEntry::fill_sig_bt(sig, sig_bt);\n+    return java_calling_convention(sig_bt, regs, total_args_passed);\n+  }\n+  static int java_return_convention(const BasicType* sig_bt, VMRegPair* regs, int total_args_passed);\n+  static const uint java_return_convention_max_int;\n+  static const uint java_return_convention_max_float;\n@@ -421,6 +445,10 @@\n-  static AdapterHandlerEntry* generate_i2c2i_adapters(MacroAssembler *_masm,\n-                                                      int total_args_passed,\n-                                                      int max_arg,\n-                                                      const BasicType *sig_bt,\n-                                                      const VMRegPair *regs,\n-                                                      AdapterFingerPrint* fingerprint);\n+  static AdapterHandlerEntry* generate_i2c2i_adapters(MacroAssembler *masm,\n+                                                      int comp_args_on_stack,\n+                                                      const GrowableArray<SigEntry>* sig,\n+                                                      const VMRegPair* regs,\n+                                                      const GrowableArray<SigEntry>* sig_cc,\n+                                                      const VMRegPair* regs_cc,\n+                                                      const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                      const VMRegPair* regs_cc_ro,\n+                                                      AdapterFingerPrint* fingerprint,\n+                                                      AdapterBlob*& new_adapter);\n@@ -429,2 +457,1 @@\n-                              int total_args_passed,\n-                              const BasicType *sig_bt,\n+                              const GrowableArray<SigEntry>* sig,\n@@ -506,0 +533,3 @@\n+  static void load_inline_type_fields_in_regs(JavaThread *thread, oopDesc* res);\n+  static void store_inline_type_fields_to_buf(JavaThread *thread, intptr_t res);\n+\n@@ -516,0 +546,3 @@\n+  static void allocate_inline_types(JavaThread* thread, Method* callee, bool allocate_receiver);\n+  static oop allocate_inline_types_impl(JavaThread* thread, methodHandle callee, bool allocate_receiver, TRAPS);\n+  static void apply_post_barriers(JavaThread* thread, objArrayOopDesc* array);\n@@ -519,0 +552,1 @@\n+  static BufferedInlineTypeBlob* generate_buffered_inline_type_adapter(const InlineKlass* vk);\n@@ -638,0 +672,2 @@\n+  address _c2i_inline_entry;\n+  address _c2i_inline_ro_entry;\n@@ -639,0 +675,1 @@\n+  address _c2i_unverified_inline_entry;\n@@ -641,0 +678,3 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  const GrowableArray<SigEntry>* _sig_cc;\n+\n@@ -648,1 +688,2 @@\n-  void init(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry, address c2i_unverified_entry, address c2i_no_clinit_check_entry) {\n+  void init(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry, address c2i_inline_entry,\n+            address c2i_inline_ro_entry, address c2i_unverified_entry, address c2i_unverified_inline_entry, address c2i_no_clinit_check_entry) {\n@@ -652,0 +693,2 @@\n+    _c2i_inline_entry = c2i_inline_entry;\n+    _c2i_inline_ro_entry = c2i_inline_ro_entry;\n@@ -653,0 +696,1 @@\n+    _c2i_unverified_inline_entry = c2i_unverified_inline_entry;\n@@ -654,0 +698,1 @@\n+    _sig_cc = NULL;\n@@ -668,0 +713,2 @@\n+  address get_c2i_inline_entry()            const { return _c2i_inline_entry; }\n+  address get_c2i_inline_ro_entry()         const { return _c2i_inline_ro_entry; }\n@@ -669,0 +716,1 @@\n+  address get_c2i_unverified_inline_entry() const { return _c2i_unverified_inline_entry; }\n@@ -674,0 +722,4 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  void set_sig_cc(const GrowableArray<SigEntry>* sig)  { _sig_cc = sig; }\n+  const GrowableArray<SigEntry>* get_sig_cc()    const { return _sig_cc; }\n+\n@@ -702,4 +754,2 @@\n-                                        address i2c_entry,\n-                                        address c2i_entry,\n-                                        address c2i_unverified_entry,\n-                                        address c2i_no_clinit_check_entry = NULL);\n+                                        address i2c_entry, address c2i_entry, address c2i_inline_entry, address c2i_inline_ro_entry,\n+                                        address c2i_unverified_entry, address c2i_unverified_inline_entry, address c2i_no_clinit_check_entry = NULL);\n@@ -718,0 +768,60 @@\n+\/\/ Utility class for computing the calling convention of the 3 types\n+\/\/ of compiled method entries:\n+\/\/     Method::_from_compiled_entry               - sig_cc\n+\/\/     Method::_from_compiled_inline_ro_entry     - sig_cc_ro\n+\/\/     Method::_from_compiled_inline_entry        - sig\n+class CompiledEntrySignature : public StackObj {\n+  Method* _method;\n+  int  _num_inline_args;\n+  bool _has_inline_recv;\n+  GrowableArray<SigEntry> *_sig;\n+  GrowableArray<SigEntry> *_sig_cc;\n+  GrowableArray<SigEntry> *_sig_cc_ro;\n+  VMRegPair* _regs;\n+  VMRegPair* _regs_cc;\n+  VMRegPair* _regs_cc_ro;\n+\n+  int _args_on_stack;\n+  int _args_on_stack_cc;\n+  int _args_on_stack_cc_ro;\n+\n+  bool _c1_needs_stack_repair;\n+  bool _c2_needs_stack_repair;\n+  bool _has_scalarized_args;\n+\n+public:\n+  Method* method()                     const { return _method; }\n+\n+  \/\/ Used by Method::_from_compiled_inline_entry\n+  GrowableArray<SigEntry>& sig()       const { return *_sig; }\n+\n+  \/\/ Used by Method::_from_compiled_entry\n+  GrowableArray<SigEntry>& sig_cc()    const { return *_sig_cc; }\n+\n+  \/\/ Used by Method::_from_compiled_inline_ro_entry\n+  GrowableArray<SigEntry>& sig_cc_ro() const { return *_sig_cc_ro; }\n+\n+  VMRegPair* regs()                    const { return _regs; }\n+  VMRegPair* regs_cc()                 const { return _regs_cc; }\n+  VMRegPair* regs_cc_ro()              const { return _regs_cc_ro; }\n+\n+  int args_on_stack()                  const { return _args_on_stack; }\n+  int args_on_stack_cc()               const { return _args_on_stack_cc; }\n+  int args_on_stack_cc_ro()            const { return _args_on_stack_cc_ro; }\n+\n+  int  num_inline_args()               const { return _num_inline_args; }\n+  bool has_inline_arg()                const { return _num_inline_args > 0; }\n+  bool has_inline_recv()               const { return _has_inline_recv; }\n+\n+  bool has_scalarized_args()           const { return _has_scalarized_args; }\n+  bool c1_needs_stack_repair()         const { return _c1_needs_stack_repair; }\n+  bool c2_needs_stack_repair()         const { return _c2_needs_stack_repair; }\n+  CodeOffsets::Entries c1_inline_ro_entry_type() const;\n+\n+  CompiledEntrySignature(Method* method);\n+  void compute_calling_conventions();\n+\n+private:\n+  int compute_scalarized_cc(GrowableArray<SigEntry>*& sig_cc, VMRegPair*& regs_cc, bool scalar_receiver);\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":130,"deletions":20,"binary":false,"changes":150,"status":"modified"},{"patch":"@@ -259,0 +259,12 @@\n+#define CHECK_THROW_NOSYNC_IMSE(obj)  \\\n+  if (EnableValhalla && (obj)->mark().is_inline_type()) {  \\\n+    ResourceMark rm(THREAD);                \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n+#define CHECK_THROW_NOSYNC_IMSE_0(obj)  \\\n+    if (EnableValhalla && (obj)->mark().is_inline_type()) {  \\\n+    ResourceMark rm(THREAD);                  \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n@@ -286,0 +298,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -335,0 +348,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -442,0 +456,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -455,1 +470,1 @@\n-  assert(!mark.has_bias_pattern(), \"should not see bias pattern here\");\n+  assert(!UseBiasedLocking || !mark.has_bias_pattern(), \"should not see bias pattern here\");\n@@ -491,0 +506,4 @@\n+  if (EnableValhalla && mark.is_inline_type()) {\n+    return;\n+  }\n+  assert(!EnableValhalla || !object->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -493,0 +512,1 @@\n+         !UseBiasedLocking ||\n@@ -554,0 +574,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -568,0 +589,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -594,0 +616,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -613,0 +636,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -617,0 +641,1 @@\n+    assert(!obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n@@ -618,1 +643,0 @@\n-  assert(!obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n@@ -656,0 +680,1 @@\n+  CHECK_THROW_NOSYNC_IMSE_0(obj);\n@@ -680,0 +705,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -695,0 +721,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -712,0 +739,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -861,0 +889,4 @@\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+    \/\/ VM should be calling bootstrap method\n+    ShouldNotReachHere();\n+  }\n@@ -888,1 +920,1 @@\n-    assert(!mark.has_bias_pattern(), \"invariant\");\n+    assert(!UseBiasedLocking || !mark.has_bias_pattern(), \"invariant\");\n@@ -991,6 +1023,0 @@\n-\/\/ Deprecated -- use FastHashCode() instead.\n-\n-intptr_t ObjectSynchronizer::identity_hash_value_for(Handle obj) {\n-  return FastHashCode(Thread::current(), obj());\n-}\n-\n@@ -1000,0 +1026,3 @@\n+  if (EnableValhalla && h_obj->mark().is_inline_type()) {\n+    return false;\n+  }\n@@ -1251,0 +1280,4 @@\n+  if (EnableValhalla) {\n+    guarantee(!object->klass()->is_inline_klass(), \"Attempt to inflate inline type\");\n+  }\n+\n@@ -1255,1 +1288,1 @@\n-    assert(!mark.has_bias_pattern(), \"invariant\");\n+    assert(!UseBiasedLocking || !mark.has_bias_pattern(), \"invariant\");\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":43,"deletions":10,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -105,1 +105,1 @@\n-  static intptr_t identity_hash_value_for(Handle obj);\n+  static intptr_t identity_hash_value_for(Handle obj);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -64,0 +64,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -1515,0 +1516,1 @@\n+  _return_buffered_value(nullptr),\n@@ -1571,1 +1573,0 @@\n-\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -444,0 +444,1 @@\n+ public:\n@@ -1014,0 +1015,1 @@\n+  friend class VTBuffer;\n@@ -1071,0 +1073,1 @@\n+  oop           _return_buffered_value; \/\/ buffered value being returned\n@@ -1507,0 +1510,3 @@\n+  oop return_buffered_value() const              { return _return_buffered_value; }\n+  void set_return_buffered_value(oop val)        { _return_buffered_value = val; }\n+\n@@ -1571,0 +1577,1 @@\n+  static ByteSize return_buffered_value_offset() { return byte_offset_of(JavaThread, _return_buffered_value); }\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -114,0 +114,1 @@\n+  template(ClassPrintLayout)                      \\\n@@ -419,0 +420,10 @@\n+class VM_PrintClassLayout: public VM_Operation {\n+ private:\n+  outputStream* _out;\n+  char* _class_name;\n+ public:\n+  VM_PrintClassLayout(outputStream* st, char* class_name): _out(st), _class_name(class_name) {}\n+  VMOp_Type type() const { return VMOp_PrintClassHierarchy; }\n+  void doit();\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -222,1 +222,1 @@\n-  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ObjArrayKlass*)                        \\\n+  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ArrayKlass*)                        \\\n@@ -237,1 +237,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags,                                   u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags,                                   u4)                                    \\\n@@ -1626,0 +1626,1 @@\n+  declare_c2_type(MachVEPNode, MachIdealNode)                             \\\n@@ -2308,0 +2309,2 @@\n+  declare_constant(InstanceKlass::_misc_invalid_inline_super)             \\\n+  declare_constant(InstanceKlass::_misc_invalid_identity_super)           \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"oops\/array.hpp\"\n+#include \"oops\/oop.hpp\"\n@@ -440,0 +442,6 @@\n+  void appendAll(const Array<E>* l) {\n+    for (int i = 0; i < l->length(); i++) {\n+      this->at_put_grow(this->_len, l->at(i), E());\n+    }\n+  }\n+\n@@ -786,2 +794,2 @@\n-  GrowableArrayFilterIterator(const GrowableArrayIterator<E>& begin, UnaryPredicate filter_predicate) :\n-      _array(begin._array), _position(begin._position), _predicate(filter_predicate) {\n+  GrowableArrayFilterIterator(const GrowableArray<E>* array, UnaryPredicate filter_predicate) :\n+      _array(array), _position(0), _predicate(filter_predicate) {\n@@ -789,1 +797,1 @@\n-    while(_position != _array->length() && !_predicate(_array->at(_position))) {\n+    while(!at_end() && !_predicate(_array->at(_position))) {\n@@ -798,1 +806,1 @@\n-    } while(_position != _array->length() && !_predicate(_array->at(_position)));\n+    } while(!at_end() && !_predicate(_array->at(_position)));\n@@ -823,0 +831,4 @@\n+\n+  bool at_end() const {\n+    return _array == NULL || _position == _array->end()._position;\n+  }\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1182,0 +1182,2 @@\n+            } else if (cl.isInlineClass()) {\n+                throw new NotSerializableException(cl.getName());\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectOutputStream.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+import java.lang.reflect.InaccessibleObjectException;\n@@ -506,0 +507,1 @@\n+        boolean isInlineClass = cl.isInlineClass();\n@@ -577,0 +579,2 @@\n+            } else if (isInlineClass && writeReplaceMethod == null) {\n+                deserializeEx = new ExceptionInfo(name, \"inline class\");\n@@ -1574,1 +1578,1 @@\n-        } catch (NoSuchMethodException ex) {\n+        } catch (NoSuchMethodException | InaccessibleObjectException ex) {\n@@ -1905,0 +1909,1 @@\n+                \/\/ Skip IdentityObject to keep the computed SVUID the same.\n@@ -1906,1 +1911,2 @@\n-                    dout.writeUTF(ifaceNames[i]);\n+                    if (!\"java.lang.IdentityObject\".equals(ifaceNames[i]))\n+                        dout.writeUTF(ifaceNames[i]);\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectStreamClass.java","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -199,3 +199,4 @@\n-    private static final int ANNOTATION= 0x00002000;\n-    private static final int ENUM      = 0x00004000;\n-    private static final int SYNTHETIC = 0x00001000;\n+    private static final int ANNOTATION = 0x00002000;\n+    private static final int ENUM       = 0x00004000;\n+    private static final int SYNTHETIC  = 0x00001000;\n+    private static final int INLINE     = 0x00000100;\n@@ -235,2 +236,3 @@\n-        return (isInterface() ? \"interface \" : (isPrimitive() ? \"\" : \"class \"))\n-            + getName();\n+        return (isInlineClass() ? \"inline \" : \"\")\n+               + (isInterface() ? \"interface \" : (isPrimitive() ? \"\" : \"class \"))\n+               + getName();\n@@ -298,0 +300,4 @@\n+                if (isInlineClass()) {\n+                    sb.append(\"inline\");\n+                    sb.append(' ');\n+                }\n@@ -472,2 +478,2 @@\n-                                            ClassLoader loader,\n-                                            Class<?> caller)\n+                                    ClassLoader loader,\n+                                    Class<?> caller)\n@@ -550,0 +556,178 @@\n+    \/**\n+     * Returns {@code true} if this class is an inline class.\n+     *\n+     * @return {@code true} if this class is an inline class\n+     * @since Valhalla\n+     *\/\n+    public boolean isInlineClass() {\n+        return (this.getModifiers() & INLINE) != 0;\n+    }\n+\n+    \/**\n+     * Returns a {@code Class} object representing the <em>value projection<\/em>\n+     * type of this class if this {@code Class} represents the reference projection\n+     * type of an {@linkplain #isInlineClass() inline class}.\n+     * If this {@code Class} represents the value projection type\n+     * of an inline class, then this method returns this class.\n+     * Otherwise an empty {@link Optional} is returned.\n+     *\n+     * @return the {@code Class} object representing the value projection type of\n+     *         this class if this class is the value projection type\n+     *         or the reference projection type of an inline class;\n+     *         an empty {@link Optional} otherwise\n+     * @since Valhalla\n+     *\/\n+    public Optional<Class<?>> valueType() {\n+        if (isPrimitive() || isInterface() || isArray())\n+            return Optional.empty();\n+\n+        Class<?>[] valRefTypes = getProjectionTypes();\n+        return valRefTypes.length > 0 ? Optional.of(valRefTypes[0]) : Optional.empty();\n+    }\n+\n+    \/**\n+     * Returns a {@code Class} object representing the reference type\n+     * of this class.\n+     * <p>\n+     * If this {@code Class} represents an {@linkplain #isInlineClass()\n+     * inline class} with a reference projection type, then this method\n+     * returns the <em>reference projection<\/em> type of this inline class.\n+     * <p>\n+     * If this {@code Class} represents the reference projection type\n+     * of an inline class, then this method returns this class.\n+     * <p>\n+     * If this class is an {@linkplain #isInlineClass() inline class}\n+     * without a reference projection, then this method returns an empty\n+     * {@code Optional}.\n+     * <p>\n+     * If this class is an identity class, then this method returns this\n+     * class.\n+     * <p>\n+     * Otherwise this method returns an empty {@code Optional}.\n+     *\n+     * @return the {@code Class} object representing a reference type for\n+     *         this class if present; an empty {@link Optional} otherwise.\n+     * @since Valhalla\n+     *\/\n+    public Optional<Class<?>> referenceType() {\n+        if (isPrimitive()) return Optional.empty();\n+        if (isInterface() || isArray()) return Optional.of(this);\n+\n+        Class<?>[] valRefTypes = getProjectionTypes();\n+        return valRefTypes.length == 2 ? Optional.of(valRefTypes[1]) : Optional.empty();\n+    }\n+\n+    \/*\n+     * Returns true if this Class object represents a reference projection\n+     * type for an inline class.\n+     *\n+     * A reference projection type must be a sealed abstract class that\n+     * permits the inline projection type to extend.  The inline projection\n+     * type and reference projection type for an inline type must be of\n+     * the same package.\n+     *\/\n+    private boolean isReferenceProjectionType() {\n+        if (isPrimitive() || isArray() || isInterface() || isInlineClass())\n+            return false;\n+\n+        int mods = getModifiers();\n+        if (!Modifier.isAbstract(mods)) {\n+            return false;\n+        }\n+\n+        Class<?>[] valRefTypes = getProjectionTypes();\n+        return valRefTypes.length == 2 && valRefTypes[1] == this;\n+    }\n+\n+    private transient Class<?>[] projectionTypes;\n+    private Class<?>[] getProjectionTypes() {\n+        if (isPrimitive() || isArray() || isInterface())\n+            return null;\n+\n+        Class<?>[] valRefTypes = projectionTypes;\n+        if (valRefTypes == null) {\n+            \/\/ C.ensureProjectionTypesInited calls initProjectionTypes that may\n+            \/\/ call D.ensureProjectionTypesInited where D is its superclass.\n+            \/\/ So initProjectionTypes is called without holding any lock to\n+            \/\/ avoid potential deadlock when multiple threads attempt to\n+            \/\/ initialize the projection types for C and E where D is\n+            \/\/ the superclass of both C and E (which is an error case)\n+            valRefTypes = newProjectionTypeArray();\n+        }\n+        synchronized (this) {\n+            \/\/ set the projection types if not set\n+            if (projectionTypes == null) {\n+                projectionTypes = valRefTypes;\n+            }\n+        }\n+        return projectionTypes;\n+    }\n+\n+    \/*\n+     * Returns an array of Class object whose element at index 0 represents the\n+     * value projection type and element at index 1 represents the reference\n+     * projection type if present.\n+     *\n+     * If this Class object is neither a value projection type nor\n+     * a reference projection type for an inline class, then an empty array\n+     * is returned.\n+     *\/\n+    private Class<?>[] newProjectionTypeArray() {\n+        if (isPrimitive() || isArray() || isInterface())\n+            return null;\n+\n+        if (isInlineClass()) {\n+            Class<?> superClass = getSuperclass();\n+            if (superClass != Object.class && superClass.isReferenceProjectionType()) {\n+                return new Class<?>[] { this, superClass };\n+            } else {\n+                return new Class<?>[] { this };\n+            }\n+        } else {\n+            Class<?> valType = valueProjectionType();\n+            if (valType != null) {\n+                return new Class<?>[] { valType, this};\n+            } else {\n+                return EMPTY_CLASS_ARRAY;\n+            }\n+        }\n+    }\n+\n+    \/*\n+     * Returns the value projection type if this Class represents\n+     * a reference projection type.  If this class is an inline class\n+     * then this method returns this class.  Otherwise, returns null.\n+     *\/\n+    private Class<?> valueProjectionType() {\n+        if (isPrimitive() || isArray() || isInterface())\n+            return null;\n+\n+        if (isInlineClass())\n+            return this;\n+\n+        int mods = getModifiers();\n+        if (!Modifier.isAbstract(mods)) {\n+            return null;\n+        }\n+\n+        \/\/ A reference projection type must be a sealed abstract class\n+        \/\/ that permits the inline projection type to extend.\n+        \/\/ The inline projection type and reference projection type for\n+        \/\/ an inline type must be of the same package.\n+        String[] subclassNames = getPermittedSubclasses0();\n+        if (subclassNames.length == 1) {\n+            String cn = subclassNames[0].replace('\/', '.');\n+            int index = cn.lastIndexOf('.');\n+            String pn = index > 0 ? cn.substring(0, index) : \"\";\n+            if (pn.equals(getPackageName())) {\n+                try {\n+                    Class<?> valType = Class.forName(cn, false, getClassLoader());\n+                    if (valType.isInlineClass()) {\n+                        return valType;\n+                    }\n+                } catch (ClassNotFoundException e) {}\n+            }\n+        }\n+        return null;\n+    }\n+\n@@ -829,0 +1013,2 @@\n+     * <tr><th scope=\"row\"> {@linkplain #isInlineClass() inline class} with <a href=\"ClassLoader.html#binary-name\">binary name<\/a> <i>N<\/i>\n+     *                                      <td style=\"text-align:center\"> {@code Q}<em>N<\/em>{@code ;}\n@@ -847,0 +1033,2 @@\n+     * Point.class.getName()\n+     *     returns \"Point\"\n@@ -849,0 +1037,4 @@\n+     * (new Point[3]).getClass().getName()\n+     *     returns \"[QPoint;\"\n+     * (new Point.ref[3][4]).getClass().getName()\n+     *     returns \"[[LPoint$ref;\"\n@@ -1276,1 +1468,0 @@\n-\n@@ -1287,1 +1478,0 @@\n-\n@@ -1668,1 +1858,1 @@\n-                return cl.getName() + \"[]\".repeat(dimensions);\n+                return cl.getTypeName() + \"[]\".repeat(dimensions);\n@@ -3772,1 +3962,3 @@\n-     * null and is not assignable to the type T.\n+     * {@code null} and is not assignable to the type T.\n+     * @throws NullPointerException if this is an {@linkplain #isInlineClass()\n+     * inline type} and the object is {@code null}\n@@ -3779,0 +3971,3 @@\n+        if (isInlineClass() && obj == null)\n+            throw new NullPointerException(getName() + \" is an inline class\");\n+\n@@ -4074,1 +4269,1 @@\n-         return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n+        return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n@@ -4287,1 +4482,3 @@\n-        } else if (isHidden()) {\n+        }\n+        String typeDesc = isInlineClass() ? \"Q\" : \"L\";\n+        if (isHidden()) {\n@@ -4290,1 +4487,1 @@\n-            return \"L\" + name.substring(0, index).replace('.', '\/')\n+            return typeDesc + name.substring(0, index).replace('.', '\/')\n@@ -4293,1 +4490,1 @@\n-            return \"L\" + getName().replace('.', '\/') + \";\";\n+            return typeDesc + getName().replace('.', '\/') + \";\";\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Class.java","additions":212,"deletions":15,"binary":false,"changes":227,"status":"modified"},{"patch":"@@ -1812,0 +1812,4 @@\n+            @Override\n+            public String inlineObjectToString(Object o) {\n+                return ValueBootstrapMethods.inlineObjectToString(o);\n+            }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandleImpl.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -462,0 +462,4 @@\n+        if (referent != null && referent.getClass().isInlineClass()) {\n+            throw new IllegalArgumentException(\"cannot reference an inline value of type: \" +\n+                    referent.getClass().getName());\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/Reference.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -125,0 +125,2 @@\n+    String inlineObjectToString(Object o);\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangInvokeAccess.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -132,1 +132,0 @@\n-\n@@ -358,1 +357,0 @@\n-\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -101,0 +101,3 @@\n+    \/** Marks a type as a value-type *\/\n+    public static final int VALUE        = 1<<16;\n+\n@@ -110,0 +113,1 @@\n+    public static final int ACC_INLINE   = 0x0100;\n@@ -125,0 +129,13 @@\n+    \/** Flag is set for a class symbol if it defines one or more non-empty\n+     *  instance initializer block(s). This is relevenat only for class symbols\n+     *  that originate from source types. For binary types the instance initializer\n+     *  blocks are \"normalized\" into the constructors.\n+     *\/\n+    public static final int HASINITBLOCK         = 1<<18;\n+\n+    \/** Flag is set for a method symbol if it is an empty no-arg ctor.\n+     *  i.e one that simply returns (jlO) or merely chains to a super's\n+     *  EMPTYNOARGCONSTR\n+     *\/\n+    public static final int EMPTYNOARGCONSTR         = 1<<18;\n+\n@@ -387,1 +404,1 @@\n-        LocalClassFlags                   = FINAL | ABSTRACT | STRICTFP | ENUM | SYNTHETIC,\n+        LocalClassFlags                   = FINAL | ABSTRACT | STRICTFP | ENUM | SYNTHETIC  | VALUE,\n@@ -402,1 +419,1 @@\n-        ExtendedStandardFlags             = (long)StandardFlags | DEFAULT | SEALED | NON_SEALED,\n+        ExtendedStandardFlags             = (long)StandardFlags | DEFAULT | SEALED | NON_SEALED | VALUE,\n@@ -433,0 +450,1 @@\n+            if (0 != (flags & VALUE))     modifiers.add(Modifier.VALUE);\n@@ -474,0 +492,2 @@\n+        HASINITBLOCK(Flags.HASINITBLOCK),\n+        EMPTYNOARGCONSTR(Flags.EMPTYNOARGCONSTR),\n@@ -477,0 +497,1 @@\n+        INLINE(Flags.VALUE),\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Flags.java","additions":23,"deletions":2,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-import java.util.stream.Collectors;\n@@ -171,0 +170,1 @@\n+        allowInlineTypes = Feature.INLINE_TYPES.allowedInSource(source);\n@@ -177,0 +177,1 @@\n+        allowValueMemberCycles = options.isSet(\"allowValueMemberCycles\");\n@@ -203,0 +204,4 @@\n+    \/** Switch: allow inline types?\n+     *\/\n+    boolean allowInlineTypes;\n+\n@@ -221,0 +226,5 @@\n+    \/**\n+     * Switch: Allow value type member cycles?\n+     *\/\n+    boolean allowValueMemberCycles;\n+\n@@ -317,1 +327,11 @@\n-                log.error(pos, Errors.CantAssignValToFinalVar(v));\n+                boolean complain = true;\n+                \/* Allow updates to instance fields of value classes by any method in the same nest via the\n+                   withfield operator -This does not result in mutation of final fields; the code generator\n+                   would implement `copy on write' semantics via the opcode `withfield'.\n+                *\/\n+                if (env.info.inWithField && v.getKind() == ElementKind.FIELD && (v.flags() & STATIC) == 0 && types.isValue(v.owner.type)) {\n+                    if (env.enclClass.sym.outermostClass() == v.owner.outermostClass())\n+                        complain = false;\n+                }\n+                if (complain)\n+                    log.error(pos, Errors.CantAssignValToFinalVar(v));\n@@ -811,1 +831,1 @@\n-                List<Type> bounds = List.of(attribType(tvar.bounds.head, env));\n+                List<Type> bounds = List.of(chk.checkRefType(tvar.bounds.head, attribType(tvar.bounds.head, env), false));\n@@ -813,1 +833,1 @@\n-                    bounds = bounds.prepend(attribType(bound, env));\n+                    bounds = bounds.prepend(chk.checkRefType(bound, attribType(bound, env), false));\n@@ -974,0 +994,3 @@\n+                if (env.tree.hasTag(NEWCLASS) && types.isValue(c.getSuperclass())) {\n+                    c.flags_field |= VALUE; \/\/ avoid further secondary errors.\n+                }\n@@ -1195,1 +1218,1 @@\n-                            TreeInfo.getConstructorInvocationName(body.stats, names) == names.empty) {\n+                            TreeInfo.getConstructorInvocationName(body.stats, names, true) == names.empty) {\n@@ -1226,0 +1249,6 @@\n+                if (m.isConstructor() && m.type.getParameterTypes().size() == 0) {\n+                    if ((owner.type == syms.objectType) ||\n+                            (tree.body.stats.size() == 1 && TreeInfo.getConstructorInvocationName(tree.body.stats, names, false) == names._super)) {\n+                        m.flags_field |= EMPTYNOARGCONSTR;\n+                    }\n+                }\n@@ -1295,0 +1324,3 @@\n+            \/* Don't want constant propagation\/folding for instance fields of value classes,\n+               as these can undergo updates via copy on write.\n+            *\/\n@@ -1296,1 +1328,1 @@\n-                if ((v.flags_field & FINAL) == 0 ||\n+                if ((v.flags_field & FINAL) == 0 || ((v.flags_field & STATIC) == 0 && types.isValue(v.owner.type)) ||\n@@ -1420,1 +1452,5 @@\n-            if ((tree.flags & STATIC) != 0) localEnv.info.staticLevel++;\n+            if ((tree.flags & STATIC) != 0)\n+                localEnv.info.staticLevel++;\n+            else if (tree.stats.size() > 0)\n+                env.info.scope.owner.flags_field |= HASINITBLOCK;\n+\n@@ -1485,0 +1521,33 @@\n+    public void visitWithField(JCWithField tree) {\n+        boolean inWithField = env.info.inWithField;\n+        try {\n+            env.info.inWithField = true;\n+            Type fieldtype = attribTree(tree.field, env.dup(tree), varAssignmentInfo);\n+            attribExpr(tree.value, env, fieldtype);\n+            Type capturedType = syms.errType;\n+            if (tree.field.type != null && !tree.field.type.isErroneous()) {\n+                final Symbol sym = TreeInfo.symbol(tree.field);\n+                if (sym == null || sym.kind != VAR || sym.owner.kind != TYP ||\n+                        (sym.flags() & STATIC) != 0 || !types.isValue(sym.owner.type)) {\n+                    log.error(tree.field.pos(), Errors.ValueInstanceFieldExpectedHere);\n+                } else {\n+                    Type ownType = sym.owner.type;\n+                    switch(tree.field.getTag()) {\n+                        case IDENT:\n+                            JCIdent ident = (JCIdent) tree.field;\n+                            ownType = ident.sym.owner.type;\n+                            break;\n+                        case SELECT:\n+                            JCFieldAccess fieldAccess = (JCFieldAccess) tree.field;\n+                            ownType = fieldAccess.selected.type;\n+                            break;\n+                    }\n+                    capturedType = capture(ownType);\n+                }\n+            }\n+            result = check(tree, capturedType, KindSelector.VAL, resultInfo);\n+        } finally {\n+            env.info.inWithField = inWithField;\n+        }\n+    }\n+\n@@ -1528,1 +1597,1 @@\n-                Type base = types.asSuper(exprType, syms.iterableType.tsym);\n+                Type base = types.asSuper(exprType, syms.iterableType.tsym, true);\n@@ -1742,1 +1811,1 @@\n-        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env));\n+        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env), false);\n@@ -1823,1 +1892,1 @@\n-            types.asSuper(resource, syms.autoCloseableType.tsym) != null &&\n+            types.asSuper(resource, syms.autoCloseableType.tsym, true) != null &&\n@@ -2013,1 +2082,2 @@\n-            \/\/ Those were all the cases that could result in a primitive\n+            \/\/ Those were all the cases that could result in a primitive. See if primitive boxing and inline\n+            \/\/ narrowing conversions bring about a convergence.\n@@ -2015,1 +2085,2 @@\n-                                 .map(t -> t.isPrimitive() ? types.boxedClass(t).type : t)\n+                                 .map(t -> t.isPrimitive() ? types.boxedClass(t).type\n+                                         : t.isReferenceProjection() ? t.valueProjection() : t)\n@@ -2026,1 +2097,1 @@\n-                                 .map(t -> chk.checkNonVoid(posIt.next(), t))\n+                                 .map(t -> chk.checkNonVoid(posIt.next(), t.isValue() ? t.referenceProjection() : t))\n@@ -2029,1 +2100,1 @@\n-            \/\/ both are known to be reference types.  The result is\n+            \/\/ both are known to be reference types (or projections).  The result is\n@@ -2447,0 +2518,36 @@\n+            final Symbol symbol = TreeInfo.symbol(tree.meth);\n+            if (symbol != null) {\n+                \/* Is this an ill conceived attempt to invoke jlO methods not available on value types ??\n+                 *\/\n+                boolean superCallOnValueReceiver = types.isValue(env.enclClass.sym.type)\n+                        && (tree.meth.hasTag(SELECT))\n+                        && ((JCFieldAccess)tree.meth).selected.hasTag(IDENT)\n+                        && TreeInfo.name(((JCFieldAccess)tree.meth).selected) == names._super;\n+                if (types.isValue(qualifier) || superCallOnValueReceiver) {\n+                    int argSize = argtypes.size();\n+                    Name name = symbol.name;\n+                    switch (name.toString()) {\n+                        case \"wait\":\n+                            if (argSize == 0\n+                                    || (types.isConvertible(argtypes.head, syms.longType) &&\n+                                    (argSize == 1 || (argSize == 2 && types.isConvertible(argtypes.tail.head, syms.intType))))) {\n+                                log.error(tree.pos(), Errors.ValueDoesNotSupport(name));\n+                            }\n+                            break;\n+                        case \"notify\":\n+                        case \"notifyAll\":\n+                        case \"clone\":\n+                        case \"finalize\":\n+                            if (argSize == 0)\n+                                log.error(tree.pos(), Errors.ValueDoesNotSupport(name));\n+                            break;\n+                        case \"hashCode\":\n+                        case \"equals\":\n+                        case \"toString\":\n+                            if (superCallOnValueReceiver)\n+                                log.error(tree.pos(), Errors.ValueDoesNotSupport(names.fromString(\"invocation of super.\" + name)));\n+                            break;\n+                    }\n+                }\n+            }\n+\n@@ -2461,0 +2568,9 @@\n+                \/\/ Temporary treatment for inline class: Given an inline class V that implements\n+                \/\/ I1, I2, ... In, v.getClass() is typed to be Class<? extends Object & I1 & I2 .. & In>\n+                Type wcb;\n+                if (qualifierType.isValue()) {\n+                    List<Type> bounds = List.of(syms.objectType).appendList(((ClassSymbol) qualifierType.tsym).getInterfaces());\n+                    wcb = bounds.size() > 1 ? types.makeIntersectionType(bounds) : syms.objectType;\n+                } else {\n+                    wcb = types.erasure(qualifierType);\n+                }\n@@ -2462,1 +2578,1 @@\n-                        List.of(new WildcardType(types.erasure(qualifierType),\n+                        List.of(new WildcardType(wcb,\n@@ -2635,0 +2751,8 @@\n+            \/\/ Check that it is an instantiation of a class and not a projection type\n+            if (clazz.hasTag(SELECT)) {\n+                JCFieldAccess fieldAccess = (JCFieldAccess) clazz;\n+                if (fieldAccess.selected.type.isValue() &&\n+                        (fieldAccess.name == names.ref || fieldAccess.name == names.val)) {\n+                    log.error(tree.pos(), Errors.ProjectionCantBeInstantiated);\n+                }\n+            }\n@@ -2806,0 +2930,1 @@\n+                    chk.checkParameterizationWithValues(tree, clazztype);\n@@ -2878,0 +3003,3 @@\n+        \/\/ Likewise arg can't be null if it is a value.\n+        if (types.isValue(arg.type))\n+            return arg;\n@@ -3892,0 +4020,1 @@\n+                chk.checkForSuspectClassLiteralComparison(tree, left, right);\n@@ -4097,1 +4226,1 @@\n-                tree.name == names._class)\n+                tree.name == names._class || tree.name == names._default)\n@@ -4099,0 +4228,4 @@\n+            if (tree.name == names._default && !allowInlineTypes) {\n+                log.error(DiagnosticFlag.SOURCE_LEVEL, tree.pos(),\n+                        Feature.INLINE_TYPES.error(sourceName));\n+            }\n@@ -4120,4 +4253,9 @@\n-                log.error(tree.pos(), Errors.TypeVarCantBeDeref);\n-                result = tree.type = types.createErrorType(tree.name, site.tsym, site);\n-                tree.sym = tree.type.tsym;\n-                return ;\n+                if (tree.name == names._default) {\n+                    result = check(tree, litType(BOT).constType(null),\n+                            KindSelector.VAL, resultInfo);\n+                } else {\n+                    log.error(tree.pos(), Errors.TypeVarCantBeDeref);\n+                    result = tree.type = types.createErrorType(tree.name, site.tsym, site);\n+                    tree.sym = tree.type.tsym;\n+                    return;\n+                }\n@@ -4131,0 +4269,1 @@\n+\n@@ -4266,0 +4405,6 @@\n+                } else if (name == names._default) {\n+                    return new VarSymbol(STATIC, names._default, site, site.tsym);\n+                } else if (name == names.ref && site.isValue() && resultInfo.pkind.contains(KindSelector.TYP)) {\n+                    return site.tsym.referenceProjection();\n+                } else if (name == names.val && site.isValue() && resultInfo.pkind.contains(KindSelector.TYP)) {\n+                    return site.tsym;\n@@ -4275,0 +4420,4 @@\n+                if (name == names._default) {\n+                    \/\/ Be sure to return the default value before examining bounds\n+                    return new VarSymbol(STATIC, names._default, site, site.tsym);\n+                }\n@@ -4299,1 +4448,1 @@\n-                \/\/ .class is allowed for these.\n+                \/\/ .class and .default is allowed for these.\n@@ -4304,0 +4453,2 @@\n+                } else if (name == names._default) {\n+                    return new VarSymbol(STATIC, names._default, site, site.tsym);\n@@ -4912,1 +5063,1 @@\n-                make.Modifiers(PUBLIC | ABSTRACT),\n+                make.Modifiers(PUBLIC | ABSTRACT | (extending != null && TreeInfo.symbol(extending).isValue() ? VALUE : 0)),\n@@ -4935,1 +5086,1 @@\n-        result = check(tree, new WildcardType(chk.checkRefType(tree.pos(), type),\n+        result = check(tree, new WildcardType(chk.checkRefType(tree.pos(), type, false),\n@@ -5042,0 +5193,7 @@\n+            if (types.isValue(c.type)) {\n+                final Env<AttrContext> env = typeEnvs.get(c);\n+                if (!allowValueMemberCycles) {\n+                    if (env != null && env.tree != null && env.tree.hasTag(CLASSDEF))\n+                        chk.checkNonCyclicMembership((JCClassDecl)env.tree);\n+                }\n+            }\n@@ -5152,1 +5310,1 @@\n-            } else {\n+            } else if ((c.flags_field & Flags.COMPOUND) == 0) {\n@@ -5207,0 +5365,8 @@\n+                if ((c.flags() & (VALUE | ABSTRACT)) == VALUE) { \/\/ for non-intersection, concrete values.\n+                    Assert.check(env.tree.hasTag(CLASSDEF));\n+                    JCClassDecl classDecl = (JCClassDecl) env.tree;\n+                    if (classDecl.extending != null) {\n+                        chk.checkConstraintsOfInlineSuper(env.tree.pos(), c);\n+                    }\n+                }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":190,"deletions":24,"binary":false,"changes":214,"status":"modified"},{"patch":"@@ -139,1 +139,0 @@\n-\n@@ -499,1 +498,1 @@\n-\/* *************************************************************************\n+    \/* *************************************************************************\n@@ -605,0 +604,5 @@\n+        } else {\n+            if (found.hasTag(CLASS)) {\n+                if (inferenceContext != infer.emptyContext)\n+                    checkParameterizationWithValues(pos, found);\n+            }\n@@ -735,0 +739,43 @@\n+    void checkConstraintsOfInlineSuper(DiagnosticPosition pos, ClassSymbol c) {\n+        for(Type st = types.supertype(c.type); st != Type.noType; st = types.supertype(st)) {\n+            if (st == null || st.tsym == null || st.tsym.kind == ERR)\n+                return;\n+            if  (st.tsym == syms.objectType.tsym)\n+                return;\n+            if (!st.tsym.isAbstract()) {\n+                log.error(pos, Errors.ConcreteSupertypeForInlineClass(c, st));\n+            }\n+            if ((st.tsym.flags() & HASINITBLOCK) != 0) {\n+                log.error(pos, Errors.SuperClassDeclaresInitBlock(c, st));\n+            }\n+            \/\/ No instance fields and no arged constructors both mean inner classes cannot be inline supers.\n+            Type encl = st.getEnclosingType();\n+            if (encl != null && encl.hasTag(CLASS)) {\n+                log.error(pos, Errors.SuperClassCannotBeInner(c, st));\n+            }\n+            for (Symbol s : st.tsym.members().getSymbols(NON_RECURSIVE)) {\n+                switch (s.kind) {\n+                case VAR:\n+                    if ((s.flags() & STATIC) == 0) {\n+                        log.error(pos, Errors.SuperFieldNotAllowed(s, c, st));\n+                    }\n+                    break;\n+                case MTH:\n+                    if ((s.flags() & SYNCHRONIZED) != 0) {\n+                        log.error(pos, Errors.SuperMethodCannotBeSynchronized(s, c, st));\n+                    } else if (s.isConstructor()) {\n+                        MethodSymbol m = (MethodSymbol)s;\n+                        if (m.getParameters().size() > 0) {\n+                            log.error(pos, Errors.SuperConstructorCannotTakeArguments(m, c, st));\n+                        } else {\n+                            if ((m.flags() & (GENERATEDCONSTR | EMPTYNOARGCONSTR)) == 0) {\n+                                log.error(pos, Errors.SuperNoArgConstructorMustBeEmpty(m, c, st));\n+                            }\n+                        }\n+                    }\n+                    break;\n+                }\n+            }\n+        }\n+    }\n+\n@@ -737,2 +784,2 @@\n-    Type checkConstructorRefType(DiagnosticPosition pos, Type t) {\n-        t = checkClassOrArrayType(pos, t);\n+    Type checkConstructorRefType(JCExpression expr, Type t) {\n+        t = checkClassOrArrayType(expr, t);\n@@ -741,1 +788,1 @@\n-                log.error(pos, Errors.AbstractCantBeInstantiated(t.tsym));\n+                log.error(expr, Errors.AbstractCantBeInstantiated(t.tsym));\n@@ -744,1 +791,1 @@\n-                log.error(pos, Errors.EnumCantBeInstantiated);\n+                log.error(expr, Errors.EnumCantBeInstantiated);\n@@ -747,1 +794,10 @@\n-                t = checkClassType(pos, t, true);\n+                \/\/ Projection types may not be mentioned in constructor references\n+                if (expr.hasTag(SELECT)) {\n+                    JCFieldAccess fieldAccess = (JCFieldAccess) expr;\n+                    if (fieldAccess.selected.type.isValue() &&\n+                            (fieldAccess.name == names.ref || fieldAccess.name == names.val)) {\n+                        log.error(expr, Errors.ProjectionCantBeInstantiated);\n+                        t = types.createErrorType(t);\n+                    }\n+                }\n+                t = checkClassType(expr, t, true);\n@@ -751,1 +807,1 @@\n-                log.error(pos, Errors.GenericArrayCreation);\n+                log.error(expr, Errors.GenericArrayCreation);\n@@ -782,0 +838,1 @@\n+     *  @param valueOK       If false, a value class does not qualify\n@@ -783,2 +840,2 @@\n-    Type checkRefType(DiagnosticPosition pos, Type t) {\n-        if (t.isReference())\n+    Type checkRefType(DiagnosticPosition pos, Type t, boolean valueOK) {\n+        if (t.isReference() && (valueOK || !types.isValue(t)))\n@@ -792,0 +849,9 @@\n+    \/** Check that type is a reference type, i.e. a class, interface or array type\n+     *  or a type variable.\n+     *  @param pos           Position to be used for error reporting.\n+     *  @param t             The type to be checked.\n+     *\/\n+    Type checkRefType(DiagnosticPosition pos, Type t) {\n+        return checkRefType(pos, t, true);\n+    }\n+\n@@ -800,1 +866,1 @@\n-            l.head = checkRefType(tl.head.pos(), l.head);\n+            l.head = checkRefType(tl.head.pos(), l.head, false);\n@@ -836,0 +902,48 @@\n+    void checkParameterizationWithValues(DiagnosticPosition pos, Type t) {\n+        valueParameterizationChecker.visit(t, pos);\n+    }\n+\n+    \/** valueParameterizationChecker: A type visitor that descends down the given type looking for instances of value types\n+     *  being used as type arguments and issues error against those usages.\n+     *\/\n+    private final Types.SimpleVisitor<Void, DiagnosticPosition> valueParameterizationChecker = new Types.SimpleVisitor<Void, DiagnosticPosition>() {\n+\n+        @Override\n+        public Void visitType(Type t, DiagnosticPosition pos) {\n+            return null;\n+        }\n+\n+        @Override\n+        public Void visitClassType(ClassType t, DiagnosticPosition pos) {\n+            for (Type targ : t.allparams()) {\n+                if (types.isValue(targ)) {\n+                    log.error(pos, Errors.GenericParameterizationWithValueType(t));\n+                }\n+                visit(targ, pos);\n+            }\n+            return null;\n+        }\n+\n+        @Override\n+        public Void visitTypeVar(TypeVar t, DiagnosticPosition pos) {\n+             return null;\n+        }\n+\n+        @Override\n+        public Void visitCapturedType(CapturedType t, DiagnosticPosition pos) {\n+            return null;\n+        }\n+\n+        @Override\n+        public Void visitArrayType(ArrayType t, DiagnosticPosition pos) {\n+            return visit(t.elemtype, pos);\n+        }\n+\n+        @Override\n+        public Void visitWildcardType(WildcardType t, DiagnosticPosition pos) {\n+            return visit(t.type, pos);\n+        }\n+    };\n+\n+\n+\n@@ -984,1 +1098,38 @@\n-        return types.upward(t, types.captures(t));\n+        Type varType = types.upward(t, types.captures(t));\n+        if (varType.hasTag(CLASS)) {\n+            checkParameterizationWithValues(pos, varType);\n+        }\n+        return varType;\n+    }\n+\n+    public void checkForSuspectClassLiteralComparison(\n+            final JCBinary tree,\n+            final Type leftType,\n+            final Type rightType) {\n+\n+        if (lint.isEnabled(LintCategory.MIGRATION)) {\n+            if (isInvocationOfGetClass(tree.lhs) && isClassOfSomeInterface(rightType) ||\n+                    isInvocationOfGetClass(tree.rhs) && isClassOfSomeInterface(leftType)) {\n+                log.warning(LintCategory.MIGRATION, tree.pos(), Warnings.GetClassComparedWithInterface);\n+            }\n+        }\n+    }\n+    \/\/where\n+    private boolean isClassOfSomeInterface(Type someClass) {\n+        if (someClass.tsym.flatName() == names.java_lang_Class) {\n+            List<Type> arguments = someClass.getTypeArguments();\n+            if (arguments.length() == 1) {\n+                return arguments.head.isInterface();\n+            }\n+        }\n+        return false;\n+    }\n+    \/\/where\n+    private boolean isInvocationOfGetClass(JCExpression tree) {\n+        tree = TreeInfo.skipParens(tree);\n+        if (tree.hasTag(APPLY)) {\n+            JCMethodInvocation apply = (JCMethodInvocation)tree;\n+            MethodSymbol msym = (MethodSymbol)TreeInfo.symbol(apply.meth);\n+            return msym.name == names.getClass && msym.implementedIn(syms.objectType.tsym, types) != null;\n+        }\n+        return false;\n@@ -1182,1 +1333,1 @@\n-            else\n+            else {\n@@ -1184,0 +1335,4 @@\n+                if (types.isValue(sym.owner.type) && (flags & STATIC) == 0) {\n+                    implicit |= FINAL;\n+                }\n+            }\n@@ -1211,1 +1366,3 @@\n-                mask = MethodFlags;\n+                \/\/ instance methods of value types do not have a monitor associated with their `this'\n+                mask = ((sym.owner.flags_field & VALUE) != 0 && (flags & Flags.STATIC) == 0) ?\n+                        MethodFlags & ~SYNCHRONIZED : MethodFlags;\n@@ -1244,2 +1401,2 @@\n-                \/\/ enums can't be declared abstract, final, sealed or non-sealed\n-                mask &= ~(ABSTRACT | FINAL | SEALED | NON_SEALED);\n+                \/\/ enums can't be declared abstract, final, sealed or non-sealed or value type\n+                mask &= ~(ABSTRACT | FINAL | SEALED | NON_SEALED | VALUE);\n@@ -1283,1 +1440,1 @@\n-                               FINAL | NATIVE | SYNCHRONIZED)\n+                               FINAL | NATIVE | SYNCHRONIZED | VALUE)\n@@ -1293,1 +1450,1 @@\n-                 checkDisjoint(pos, flags,\n+                 checkDisjoint(pos, (flags | implicit), \/\/ complain against volatile & implcitly final entities too.\n@@ -1467,1 +1624,2 @@\n-                tree.selected.type.isParameterized()) {\n+                tree.selected.type.isParameterized() &&\n+                    (tree.name != names.ref || !tree.type.isReferenceProjection())) {\n@@ -1471,0 +1629,2 @@\n+                \/\/ Tolerate the pseudo-select V.ref: V<T>.ref will be static if V<T> is and\n+                \/\/ should not be confused as selecting a static member of a parameterized type.\n@@ -1797,0 +1957,9 @@\n+        if (origin.isValue() && other.owner == syms.objectType.tsym && m.type.getParameterTypes().size() == 0) {\n+            if (m.name == names.clone || m.name == names.finalize) {\n+                log.error(TreeInfo.diagnosticPositionFor(m, tree),\n+                        Errors.InlineClassMayNotOverride(m.name));\n+                m.flags_field |= BAD_OVERRIDE;\n+                return;\n+            }\n+        }\n+\n@@ -2109,1 +2278,2 @@\n-                (env.info.isAnonymousDiamond && !m.isConstructor() && !m.isPrivate());\n+                (env.info.isAnonymousDiamond && !m.isConstructor() && !m.isPrivate() &&\n+                        (!m.owner.isValue() || (tree.body.flags & SYNTHETIC) == 0));\n@@ -2235,0 +2405,39 @@\n+    \/\/ A value class cannot contain a field of its own type either or indirectly.\n+    void checkNonCyclicMembership(JCClassDecl tree) {\n+        Assert.check((tree.sym.flags_field & LOCKED) == 0);\n+        try {\n+            tree.sym.flags_field |= LOCKED;\n+            for (List<? extends JCTree> l = tree.defs; l.nonEmpty(); l = l.tail) {\n+                if (l.head.hasTag(VARDEF)) {\n+                    JCVariableDecl field = (JCVariableDecl) l.head;\n+                    if (cyclePossible(field.sym)) {\n+                        Type fieldType = field.sym.type;\n+                        checkNonCyclicMembership((ClassSymbol) fieldType.tsym, field.pos());\n+                    }\n+                }\n+            }\n+        } finally {\n+            tree.sym.flags_field &= ~LOCKED;\n+        }\n+\n+    }\n+    \/\/ where\n+    private void checkNonCyclicMembership(ClassSymbol c, DiagnosticPosition pos) {\n+        if ((c.flags_field & LOCKED) != 0) {\n+            log.error(pos, Errors.CyclicValueTypeMembership(c));\n+            return;\n+        }\n+        try {\n+            c.flags_field |= LOCKED;\n+            for (Symbol fld : c.members().getSymbols(s -> s.kind == VAR && cyclePossible((VarSymbol) s), NON_RECURSIVE)) {\n+                checkNonCyclicMembership((ClassSymbol) fld.type.tsym, pos);\n+            }\n+        } finally {\n+            c.flags_field &= ~LOCKED;\n+        }\n+    }\n+        \/\/ where\n+        private boolean cyclePossible(VarSymbol symbol) {\n+            return (symbol.flags() & STATIC) == 0 && types.isValue(symbol.type);\n+        }\n+\n@@ -2483,0 +2692,4 @@\n+\n+        if (c.isValue() && types.asSuper(c, syms.identityObjectType.tsym, true) != null) {\n+            log.error(pos, Errors.InlineTypeMustNotImplementIdentityObject(c));\n+        }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Check.java","additions":233,"deletions":20,"binary":false,"changes":253,"status":"modified"},{"patch":"@@ -429,0 +429,2 @@\n+                            \"java\/lang\/Class.asIndirectType()Ljava\/lang\/Class;\",\n+                            \"java\/lang\/Class.asPrimaryType()Ljava\/lang\/Class;\",\n@@ -430,1 +432,5 @@\n-                            \"java\/math\/BigInteger.shiftRightImplWorker([I[IIII)V\");\n+                            \"java\/math\/BigInteger.shiftRightImplWorker([I[IIII)V\",\n+                            \"jdk\/internal\/misc\/Unsafe.finishPrivateBuffer(Ljava\/lang\/Object;)Ljava\/lang\/Object;\",\n+                            \"jdk\/internal\/misc\/Unsafe.getValue(Ljava\/lang\/Object;JLjava\/lang\/Class;)Ljava\/lang\/Object;\",\n+                            \"jdk\/internal\/misc\/Unsafe.makePrivateBuffer(Ljava\/lang\/Object;)Ljava\/lang\/Object;\",\n+                            \"jdk\/internal\/misc\/Unsafe.putValue(Ljava\/lang\/Object;JLjava\/lang\/Class;Ljava\/lang\/Object;)V\");\n","filename":"src\/jdk.internal.vm.compiler\/share\/classes\/org.graalvm.compiler.hotspot.test\/src\/org\/graalvm\/compiler\/hotspot\/test\/CheckGraalIntrinsics.java","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -65,0 +65,77 @@\n+# Valhalla\n+compiler\/arguments\/CheckCICompilerCount.java                        8205030 generic-all\n+compiler\/arguments\/CheckCompileThresholdScaling.java                8205030 generic-all\n+compiler\/codecache\/CheckSegmentedCodeCache.java                     8205030 generic-all\n+compiler\/codecache\/cli\/TestSegmentedCodeCacheOption.java            8205030 generic-all\n+compiler\/codecache\/cli\/codeheapsize\/TestCodeHeapSizeOptions.java    8205030 generic-all\n+compiler\/codecache\/cli\/printcodecache\/TestPrintCodeCacheOption.java 8205030 generic-all\n+compiler\/whitebox\/OSRFailureLevel4Test.java                         8205030 generic-all\n+\n+compiler\/aot\/cli\/DisabledAOTWithLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/SingleAOTOptionTest.java 8226295 generic-all\n+compiler\/aot\/cli\/MultipleAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileClassWithDebugTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileModuleTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/AtFileTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionWrongFileTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ClasspathOptionUnknownClassTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileDirectoryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionNotExistingTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileClassTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileJarTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/IgnoreErrorsTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileAbsoluteDirectoryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/NonExistingAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/SingleAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/IncorrectAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/RecompilationTest.java 8226295 generic-all\n+compiler\/aot\/SharedUsageTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/ClassSearchTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/SearchPathTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/module\/ModuleSourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/ClassSourceTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/directory\/DirectorySourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/jar\/JarSourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/NativeOrderOutputStreamTest.java 8226295 generic-all\n+compiler\/aot\/verification\/vmflags\/TrackedFlagTest.java 8226295 generic-all\n+compiler\/aot\/verification\/vmflags\/NotTrackedFlagTest.java 8226295 generic-all\n+compiler\/aot\/verification\/ClassAndLibraryNotMatchTest.java 8226295 generic-all\n+compiler\/aot\/DeoptimizationTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SelfChanged.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SelfChangedCDS.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SuperChanged.java 8226295 generic-all\n+\n@@ -92,0 +169,22 @@\n+# Valhalla TODO:\n+runtime\/CompressedOops\/CompressedClassPointers.java 8210258 generic-all\n+runtime\/RedefineTests\/RedefineLeak.java 8205032 generic-all\n+runtime\/SharedArchiveFile\/BootAppendTests.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/CdsDifferentCompactStrings.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/CdsDifferentObjectAlignment.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/NonBootLoaderClasses.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/PrintSharedArchiveAndExit.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedArchiveFile.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedStringsDedup.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedStringsRunAuto.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedSymbolTableBucketSize.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SpaceUtilizationCheck.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/TestInterpreterMethodEntries.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformInterfaceAndImplementor.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformSuperAndSubClasses.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformSuperSubTwoPckgs.java 8210258 generic-all\n+runtime\/appcds\/ClassLoaderTest.java 8210258 generic-all\n+runtime\/appcds\/HelloTest.java 8210258 generic-all\n+runtime\/appcds\/sharedStrings\/SharedStringsBasic.java 8210258 generic-all\n+\n+\n@@ -103,0 +202,27 @@\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":126,"deletions":0,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -151,0 +151,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n","filename":"test\/lib\/sun\/hotspot\/WhiteBox.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"}]}