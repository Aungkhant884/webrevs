{"files":[{"patch":"@@ -1438,0 +1438,1 @@\n+        args = concat(args, \"--with-version-pre=\" + version_numbers.get(\"DEFAULT_PROMOTED_VERSION_PRE\"));\n","filename":"make\/conf\/jib-profiles.js","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -765,0 +765,1 @@\n+BUILD_HOTSPOT_JTREG_LIBRARIES_CFLAGS_libobjmonusage007 := $(NSK_JVMTI_AGENT_INCLUDES)\n@@ -1473,0 +1474,1 @@\n+  BUILD_HOTSPOT_JTREG_LIBRARIES_LIBS_libobjmonusage007 += -lpthread\n","filename":"make\/test\/JtregNativeHotspot.gmk","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1648,0 +1648,3 @@\n+  } else if (_entry_point == NULL) {\n+    \/\/ See CallLeafNoFPIndirect\n+    return 1 * NativeInstruction::instruction_size;\n@@ -1759,3 +1762,0 @@\n-  \/\/ n.b. frame size includes space for return pc and rfp\n-  const int framesize = C->output()->frame_size_in_bytes();\n-\n@@ -1766,4 +1766,1 @@\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n+  __ verified_entry(C, 0);\n@@ -1771,8 +1768,2 @@\n-    __ mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n-    __ bind(L_skip_barrier);\n-  }\n-\n-  if (C->max_vector_size() > 0) {\n-    __ reinitialize_ptrue();\n+  if (C->stub_function() == NULL) {\n+    __ entry_barrier();\n@@ -1781,27 +1772,2 @@\n-  int bangsize = C->output()->bang_size_in_bytes();\n-  if (C->output()->need_stack_bang(bangsize))\n-    __ generate_stack_overflow_check(bangsize);\n-\n-  __ build_frame(framesize);\n-\n-  if (C->stub_function() == NULL) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n-      \/\/ Dummy labels for just measuring the code size\n-      Label dummy_slow_path;\n-      Label dummy_continuation;\n-      Label dummy_guard;\n-      Label* slow_path = &dummy_slow_path;\n-      Label* continuation = &dummy_continuation;\n-      Label* guard = &dummy_guard;\n-      if (!Compile::current()->output()->in_scratch_emit_size()) {\n-        \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-        C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-        Compile::current()->output()->add_stub(stub);\n-        slow_path = &stub->entry();\n-        continuation = &stub->continuation();\n-        guard = &stub->guard();\n-      }\n-      \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n-      bs->nmethod_entry_barrier(&_masm, slow_path, continuation, guard);\n-    }\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n@@ -1824,6 +1790,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1873,1 +1833,1 @@\n-  __ remove_frame(framesize);\n+  __ remove_frame(framesize, C->needs_stack_repair());\n@@ -1892,5 +1852,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {\n-  \/\/ Variable size. Determine dynamically.\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2202,1 +2157,49 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n+\n+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+{\n+  C2_MacroAssembler _masm(&cbuf);\n+\n+  if (!_verified) {\n+    Label skip;\n+    __ cmp_klass(j_rarg0, rscratch2, rscratch1);\n+    __ br(Assembler::EQ, skip);\n+      __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+    __ bind(skip);\n+\n+  } else {\n+    \/\/ insert a nop at the start of the prolog so we can patch in a\n+    \/\/ branch if we need to invalidate the method later\n+    __ nop();\n+\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == NULL) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int framesize = ra_->C->output()->frame_slots() << LogBytesPerInt;\n+      __ remove_frame(framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ b(dummy_verified_entry);\n+    } else {\n+      __ b(*_verified_entry);\n+    }\n+  }\n+}\n@@ -2204,0 +2207,1 @@\n+\/\/=============================================================================\n@@ -2225,0 +2229,1 @@\n+  Label skip;\n@@ -2226,0 +2231,1 @@\n+  \/\/ UseCompressedClassPointers logic are inside cmp_klass\n@@ -2227,1 +2233,1 @@\n-  Label skip;\n+\n@@ -2235,5 +2241,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_);\n-}\n-\n@@ -3700,0 +3701,33 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic()) {\n+      if (!_method->signature()->returns_null_free_inline_type()) {\n+        \/\/ The last return value is not set by the callee but used to pass IsInit information to compiled code.\n+        \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+        uint con = (tf()->range_cc()->cnt() - 1);\n+        for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+          ProjNode* proj = fast_out(i)->as_Proj();\n+          if (proj->_con == con) {\n+            \/\/ Set IsInit if r0 is non-null (a non-null value is returned buffered or scalarized)\n+            OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+            VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+            Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+            __ cmp(r0, zr);\n+            __ cset(toReg, Assembler::NE);\n+            if (reg->is_stack()) {\n+              int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+              __ str(toReg, Address(sp, st_off));\n+            }\n+            break;\n+          }\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ R0 either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero r0\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ r0 &= (r0 & 1) - 1\n+        __ andr(rscratch1, r0, 0x1);\n+        __ sub(rscratch1, rscratch1, 0x1);\n+        __ andr(r0, r0, rscratch1);\n+      }\n+    }\n@@ -3794,0 +3828,5 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        __ andr(tmp, tmp, ~((int) markWord::inline_type_bit_in_place));\n+      }\n+\n@@ -7267,1 +7306,1 @@\n-    \"mov  $dst, $con\\t# ptr\\n\\t\"\n+    \"mov  $dst, $con\\t# ptr\"\n@@ -8470,0 +8509,15 @@\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -15340,1 +15394,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg_immL0(iRegL_R11 cnt, iRegP_R10 base, immL0 zero, Universe dummy, rFlagsReg cr)\n@@ -15342,1 +15396,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) zero));\n@@ -15359,0 +15413,16 @@\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, KILL cr);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ClearArray $cnt, $base, $val\" %}\n+\n+  ins_encode %{\n+    __ fill_words($base$$Register, $cnt$$Register, $val$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -15362,1 +15432,2 @@\n-            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord));\n+            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord)\n+            && !((ClearArrayNode*)n)->word_copy_only());\n@@ -16676,0 +16747,18 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPIndirect(iRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == NULL);\n+\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(CALL_COST);\n+\n+  format %{ \"CALL, runtime leaf nofp indirect $target\" %}\n+\n+  ins_encode %{\n+    __ blr($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n@@ -16678,0 +16767,2 @@\n+  predicate(n->as_Call()->entry_point() != NULL);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":156,"deletions":65,"binary":false,"changes":221,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -42,0 +43,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -454,1 +456,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -498,0 +500,28 @@\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != NULL, \"must be\");\n+        __ far_call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      __ test_oop_is_not_inline_type(r0, rscratch2, skip);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+      __ ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ cbz(rscratch1, skip);\n+      __ blr(rscratch1);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, r0 points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n+\n@@ -499,1 +529,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -511,0 +541,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -556,0 +590,1 @@\n+    case T_PRIMITIVE_OBJECT:\n@@ -557,3 +592,1 @@\n-        if (patch_code == lir_patch_none) {\n-          jobject2reg(c->as_jobject(), dest->as_register());\n-        } else {\n+        if (patch_code != lir_patch_none) {\n@@ -561,0 +594,2 @@\n+        } else {\n+          jobject2reg(c->as_jobject(), dest->as_register());\n@@ -602,0 +637,1 @@\n+  case T_PRIMITIVE_OBJECT:\n@@ -668,0 +704,1 @@\n+  case T_PRIMITIVE_OBJECT:\n@@ -670,0 +707,2 @@\n+    \/\/ Non-null case is not handled on aarch64 but handled on x86\n+    \/\/ FIXME: do we need to add it here?\n@@ -708,1 +747,1 @@\n-    if (src->type() == T_OBJECT) {\n+    if (src->type() == T_OBJECT || src->type() == T_PRIMITIVE_OBJECT) {\n@@ -808,0 +847,1 @@\n+    case T_PRIMITIVE_OBJECT: \/\/ fall through\n@@ -937,1 +977,1 @@\n-  if (addr->base()->type() == T_OBJECT) {\n+  if (addr->base()->type() == T_OBJECT || addr->base()->type() == T_PRIMITIVE_OBJECT) {\n@@ -961,0 +1001,1 @@\n+    case T_PRIMITIVE_OBJECT: \/\/ fall through\n@@ -1020,0 +1061,14 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, NULL);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n@@ -1211,1 +1266,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->type() == T_PRIMITIVE_OBJECT ||\n@@ -1317,0 +1372,1 @@\n+  if (op->need_null_check()) {\n@@ -1335,0 +1391,1 @@\n+  }\n@@ -1523,0 +1580,127 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be flattened (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is flattened, take the slow path.\n+\n+  Register klass = op->tmp()->as_register();\n+  if (UseArrayMarkWordCheck) {\n+    __ test_flattened_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  } else {\n+    __ load_klass(klass, op->array()->as_register());\n+    __ ldrw(klass, Address(klass, Klass::layout_helper_offset()));\n+    __ tst(klass, Klass::_lh_array_tag_flat_value_bit_inplace);\n+    __ br(Assembler::NE, *op->stub()->entry());\n+  }\n+  if (!op->value()->is_illegal()) {\n+    \/\/ The array is not flattened, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cbnz(op->value()->as_register(), skip);\n+    if (UseArrayMarkWordCheck) {\n+      __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    } else {\n+      __ tst(klass, Klass::_lh_null_free_array_bit_inplace);\n+      __ br(Assembler::NE, *op->stub()->entry());\n+    }\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  if (UseArrayMarkWordCheck) {\n+    Label test_mark_word;\n+    Register tmp = op->tmp()->as_register();\n+    __ ldr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+    __ tst(tmp, markWord::unlocked_value);\n+    __ br(Assembler::NE, test_mark_word);\n+    __ load_prototype_header(tmp, op->array()->as_register());\n+    __ bind(test_mark_word);\n+    __ tst(tmp, markWord::null_free_array_bit_in_place);\n+  } else {\n+    Register klass = op->tmp()->as_register();\n+    __ load_klass(klass, op->array()->as_register());\n+    __ ldr(klass, Address(klass, Klass::layout_helper_offset()));\n+    __ tst(klass, Klass::_lh_null_free_array_bit_inplace);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmp(left, right);\n+  __ br(Assembler::EQ, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  {\n+    __ cbz(left, L_oops_not_equal);\n+    __ cbz(right, L_oops_not_equal);\n+  }\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == NULL || right_klass == NULL) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ mov(tmp1, markWord::inline_type_pattern);\n+    __ ldr(rscratch1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ ldr(rscratch1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ cmp(tmp1, (u1)markWord::inline_type_pattern);\n+    __ br(Assembler::NE, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != NULL && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ b(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ ldrw(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldrw(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpw(left_klass_op, right_klass_op);\n+    } else {\n+      __ ldr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmp(left_klass_op, right_klass_op);\n+    }\n+\n+    __ br(Assembler::EQ, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  \/\/ We've returned from the stub. R0 contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cbz(r0, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n+\n+\n@@ -1964,0 +2148,1 @@\n+      case T_PRIMITIVE_OBJECT:\n@@ -2038,1 +2223,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2049,1 +2234,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2132,0 +2317,1 @@\n+    case T_PRIMITIVE_OBJECT:\n@@ -2168,0 +2354,1 @@\n+    case T_PRIMITIVE_OBJECT:\n@@ -2212,0 +2399,22 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ cbz(obj, *slow_path->entry());\n+  }\n+  if (UseArrayMarkWordCheck) {\n+    if (is_dest) {\n+      __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    } else {\n+      __ test_flattened_array_oop(obj, tmp, *slow_path->entry());\n+    }\n+  } else {\n+    __ load_klass(tmp, obj);\n+    __ ldr(tmp, Address(tmp, Klass::layout_helper_offset()));\n+    if (is_dest) {\n+      \/\/ Take the slow path if it's a null_free destination array, in case the source array contains NULLs.\n+      __ tst(tmp, Klass::_lh_null_free_array_bit_inplace);\n+    } else {\n+      __ tst(tmp, Klass::_lh_array_tag_flat_value_bit_inplace);\n+    }\n+    __ br(Assembler::NE, *slow_path->entry());\n+  }\n+}\n@@ -2230,0 +2439,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ b(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -2283,0 +2498,9 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -2857,0 +3081,20 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ cbz(obj, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr(), rscratch2);\n+  __ ldrb(rscratch1, mdo_addr);\n+  __ orr(rscratch1, rscratch1, flag);\n+  __ strb(rscratch1, mdo_addr);\n+\n+  __ bind(not_inline_type);\n+}\n@@ -2997,0 +3241,4 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ ldr(rscratch2, frame_map()->address_for_orig_pc_addr());\n+  __ cmp(rscratch2, (u1)NULL_WORD);\n+}\n@@ -3144,0 +3392,1 @@\n+  case T_PRIMITIVE_OBJECT:\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":259,"deletions":10,"binary":false,"changes":269,"status":"modified"},{"patch":"@@ -31,0 +31,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -88,0 +90,6 @@\n+\n+  if (EnableValhalla) {\n+    \/\/ Mask always_locked bit such that we go to the slow path if object is an inline type\n+    andr(hdr, hdr, ~markWord::inline_type_bit_in_place);\n+  }\n+\n@@ -166,2 +174,8 @@\n-  \/\/ This assumes that all prototype bits fit in an int32_t\n-  mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n+  if (EnableValhalla) {\n+    \/\/ Need to copy markWord::prototype header for klass\n+    assert_different_registers(obj, klass, len, t1, t2);\n+    ldr(t1, Address(klass, Klass::prototype_header_offset()));\n+  } else {\n+    \/\/ This assumes that all prototype bits fit in an int32_t\n+    mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n+  }\n@@ -257,0 +271,1 @@\n+\n@@ -304,0 +319,11 @@\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_offset_for_orig_pc, int sp_inc, bool reset_orig_pc, bool needs_stack_repair) {\n+  MacroAssembler::build_frame(frame_size_in_bytes);\n+\n+  if (needs_stack_repair) {\n+    save_stack_increment(sp_inc, frame_size_in_bytes);\n+  }\n+  if (reset_orig_pc) {\n+    \/\/ Zero orig_pc to detect deoptimization during buffering in the entry points\n+    str(zr, Address(sp, sp_offset_for_orig_pc));\n+  }\n+}\n@@ -305,2 +331,1 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n-  assert(bang_size_in_bytes >= framesize, \"stack bang size incorrect\");\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n@@ -309,0 +334,1 @@\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n@@ -310,1 +336,2 @@\n-  MacroAssembler::build_frame(framesize);\n+\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, has_scalarized_args, needs_stack_repair);\n@@ -315,3 +342,4 @@\n-}\n-void C1_MacroAssembler::remove_frame(int framesize) {\n-  MacroAssembler::remove_frame(framesize);\n+  if (verified_inline_entry_label != NULL) {\n+    \/\/ Jump here from the scalarized entry points that already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -321,1 +349,0 @@\n-\n@@ -328,0 +355,62 @@\n+  if (C1Breakpoint) brk(1);\n+}\n+\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  GrowableArray<SigEntry>* sig    = ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? ces->sig_cc_ro() : ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, true, ces->c1_needs_stack_repair());\n+\n+  \/\/ The runtime call might safepoint, make sure nmethod entry barrier is executed\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  mov(r19, (intptr_t) ces->method());\n+  if (is_inline_ro_entry) {\n+    far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id)));\n+  } else {\n+    far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ The runtime call returns the new array in r20 instead of the usual r0\n+  \/\/ because r0 is also j_rarg7 which may be holding a live argument here.\n+  Register val_array = r20;\n+\n+  \/\/ Remove the temp frame\n+  MacroAssembler::remove_frame(frame_size_in_bytes);\n+\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    sp_inc = extend_stack_for_inline_args(args_on_stack);\n+  }\n+\n+  shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs,          \/\/ to\n+                      sp_inc, val_array);\n+\n+  \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+  \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, sp_inc, false, ces->c1_needs_stack_repair());\n+\n+  b(verified_inline_entry_label);\n+  return rt_call_offset;\n@@ -330,0 +419,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":99,"deletions":9,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -48,0 +48,23 @@\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+    \/\/ Dummy labels for just measuring the code size\n+    Label dummy_slow_path;\n+    Label dummy_continuation;\n+    Label dummy_guard;\n+    Label* slow_path = &dummy_slow_path;\n+    Label* continuation = &dummy_continuation;\n+    Label* guard = &dummy_guard;\n+    if (!Compile::current()->output()->in_scratch_emit_size()) {\n+      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+      Compile::current()->output()->add_stub(stub);\n+      slow_path = &stub->entry();\n+      continuation = &stub->continuation();\n+      guard = &stub->guard();\n+    }\n+    \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n+    bs->nmethod_entry_barrier(this, slow_path, continuation, guard);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+  void entry_barrier();\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -158,2 +158,2 @@\n-      sender_unextended_sp = sender_sp;\n-      saved_fp = (intptr_t*) *(sender_sp - frame::sender_sp_offset);\n+      intptr_t **saved_fp_addr = (intptr_t**) (sender_sp - frame::sender_sp_offset);\n+      saved_fp = *saved_fp_addr;\n@@ -162,1 +162,4 @@\n-    }\n+      \/\/ Repair the sender sp if this is a method with scalarized inline type args\n+      sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+      sender_unextended_sp = sender_sp;\n+    }\n@@ -564,0 +567,1 @@\n+    case T_PRIMITIVE_OBJECT :\n@@ -783,0 +787,16 @@\n+\/\/ Check for a method with scalarized inline type arguments that needs\n+\/\/ a stack repair and return the repaired sender stack pointer.\n+intptr_t* frame::repair_sender_sp(intptr_t* sender_sp, intptr_t** saved_fp_addr) const {\n+  CompiledMethod* cm = _cb->as_compiled_method_or_null();\n+  if (cm != NULL && cm->needs_stack_repair()) {\n+    \/\/ The stack increment resides just below the saved FP on the stack and\n+    \/\/ records the total frame size excluding the two words for saving FP and LR.\n+    intptr_t* sp_inc_addr = (intptr_t*) (saved_fp_addr - 1);\n+    assert(*sp_inc_addr % StackAlignmentInBytes == 0, \"sp_inc not aligned\");\n+    int real_frame_size = (*sp_inc_addr \/ wordSize) + 2;\n+    assert(real_frame_size >= _cb->frame_size() && real_frame_size <= 1000000, \"invalid frame size\");\n+    sender_sp = unextended_sp() + real_frame_size;\n+  }\n+  return sender_sp;\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":23,"deletions":3,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -36,0 +36,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -417,0 +420,4 @@\n+#ifdef ASSERT\n+   address sender_pc_copy = pauth_strip_verifiable((address) *(l_sender_sp-1), (address) *(l_sender_sp-2));\n+#endif\n+\n@@ -419,0 +426,5 @@\n+  intptr_t** saved_fp_addr = (intptr_t**) (l_sender_sp - frame::sender_sp_offset);\n+\n+  \/\/ Repair the sender sp if the frame has been extended\n+  l_sender_sp = repair_sender_sp(l_sender_sp, saved_fp_addr);\n+\n@@ -423,1 +435,9 @@\n-  intptr_t** saved_fp_addr = (intptr_t**) (l_sender_sp - frame::sender_sp_offset);\n+#ifdef ASSERT\n+  if (sender_pc != sender_pc_copy) {\n+    \/\/ When extending the stack in the callee method entry to make room for unpacking of value\n+    \/\/ type args, we keep a copy of the sender pc at the expected location in the callee frame.\n+    \/\/ If the sender pc is patched due to deoptimization, the copy is not consistent anymore.\n+    nmethod* nm = CodeCache::find_blob(sender_pc)->as_nmethod();\n+    assert(sender_pc == nm->deopt_mh_handler_begin() || sender_pc == nm->deopt_handler_begin(), \"unexpected sender pc\");\n+  }\n+#endif\n@@ -429,2 +449,14 @@\n-    if (!_cb->is_compiled()) { \/\/ compiled frames do not use callee-saved registers\n-      map->set_include_argument_oops(_cb->caller_must_gc_arguments(map->thread()));\n+    bool c1_buffering = false;\n+#ifdef COMPILER1\n+    nmethod* nm = _cb->as_nmethod_or_null();\n+    if (nm != NULL && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+        pc() < nm->verified_inline_entry_point()) {\n+      \/\/ The VEP and VIEP(RO) of C1-compiled methods call buffer_inline_args_xxx\n+      \/\/ before doing any argument shuffling, so we need to scan the oops\n+      \/\/ as the caller passes them.\n+      c1_buffering = true;\n+    }\n+#endif\n+    if (!_cb->is_compiled() || c1_buffering) { \/\/ compiled frames do not use callee-saved registers\n+      bool caller_args = _cb->caller_must_gc_arguments(map->thread()) || c1_buffering;\n+      map->set_include_argument_oops(caller_args);\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.inline.hpp","additions":35,"deletions":3,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/barrierSetRuntime.hpp\"\n@@ -49,0 +50,2 @@\n+\n+  assert(type != T_PRIMITIVE_OBJECT, \"Not supported yet\");\n@@ -86,0 +89,3 @@\n+  bool is_not_null = (decorators & IS_NOT_NULL) != 0;\n+\n+  assert(type != T_PRIMITIVE_OBJECT, \"Not supported yet\");\n@@ -89,5 +95,6 @@\n-    val = val == noreg ? zr : val;\n-      if (UseCompressedOops) {\n-        assert(!dst.uses(val), \"not enough registers\");\n-        if (val != zr) {\n-          __ encode_heap_oop(val);\n+      if (val == noreg) {\n+        assert(!is_not_null, \"inconsistent access\");\n+        if (UseCompressedOops) {\n+          __ strw(zr, dst);\n+        } else {\n+          __ str(zr, dst);\n@@ -96,2 +103,11 @@\n-        __ strw(val, dst);\n-        __ str(val, dst);\n+        if (UseCompressedOops) {\n+          assert(!dst.uses(val), \"not enough registers\");\n+          if (is_not_null) {\n+            __ encode_heap_oop_not_null(val);\n+          } else {\n+            __ encode_heap_oop(val);\n+          }\n+          __ strw(val, dst);\n+        } else {\n+          __ str(val, dst);\n+        }\n@@ -102,0 +118,1 @@\n+      assert(val != noreg, \"not supported\");\n@@ -122,0 +139,13 @@\n+void BarrierSetAssembler::value_copy(MacroAssembler* masm, DecoratorSet decorators,\n+                                     Register src, Register dst, Register value_klass) {\n+  \/\/ value_copy implementation is fairly complex, and there are not any\n+  \/\/ \"short-cuts\" to be made from asm. What there is, appears to have the same\n+  \/\/ cost in C++, so just \"call_VM_leaf\" for now rather than maintain hundreds\n+  \/\/ of hand-rolled instructions...\n+  if (decorators & IS_DEST_UNINITIALIZED) {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetRuntime::value_copy_is_dest_uninitialized), src, dst, value_klass);\n+  } else {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetRuntime::value_copy), src, dst, value_klass);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.cpp","additions":37,"deletions":7,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -56,0 +56,3 @@\n+  virtual void value_copy(MacroAssembler* masm, DecoratorSet decorators,\n+                          Register src, Register dst, Register value_klass);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -272,0 +273,63 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  {\n+    SkipIfEqual skip_if(this, &DTraceAllocProbes, 0);\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::read_inlined_field(Register holder_klass,\n+                                                   Register field_index, Register field_offset,\n+                                                   Register temp, Register obj) {\n+  Label alloc_failed, empty_value, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = rscratch1;\n+  const Register dst_temp   = temp;\n+  assert_different_registers(obj, holder_klass, field_index, field_offset, dst_temp);\n+\n+  \/\/ Grab the inline field klass\n+  push(holder_klass);\n+  const Register field_klass = holder_klass;\n+  get_inline_type_field_klass(holder_klass, field_index, field_klass);\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(field_klass, dst_temp, empty_value);\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  data_for_oop(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, field_klass);\n+  pop(obj);\n+  pop(holder_klass);\n+  b(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(field_klass, dst_temp, obj);\n+  pop(holder_klass);\n+  b(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  pop(holder_klass);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_inlined_field),\n+          obj, field_index, holder_klass);\n+\n+  bind(done);\n+\n+  \/\/ Ensure the stores to copy the inline field contents are visible\n+  \/\/ before any subsequent store that publishes this reference.\n+  membar(Assembler::StoreStore);\n+}\n+\n@@ -318,1 +382,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -324,1 +389,3 @@\n-  profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  if (profile) {\n+    profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  }\n@@ -330,1 +397,3 @@\n-  profile_typecheck_failed(r2); \/\/ blows r2\n+  if (profile) {\n+    profile_typecheck_failed(r2); \/\/ blows r2\n+  }\n@@ -691,0 +760,1 @@\n+\n@@ -710,0 +780,31 @@\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    Label skip;\n+    test_oop_is_not_inline_type(r0, rscratch2, skip);\n+\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    cbz(rscratch1, skip);\n+\n+    blr(rscratch1);\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      ldr(rscratch1, Address(rfp, frame::interpreter_frame_method_offset * wordSize));\n+      ldrw(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      tstw(rscratch1, Method::scalarized_return_flag());\n+      br(Assembler::EQ, skip_stress);\n+      load_klass(r0, r0);\n+      orr(r0, r0, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    bind(skip);\n+    \/\/ Check above kills sender esp in rscratch2. Reload it.\n+    ldr(rscratch2, Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+  }\n+\n@@ -764,0 +865,4 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+    }\n@@ -1108,1 +1213,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1120,1 +1225,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()) : in_bytes(BranchData::branch_data_size()));\n@@ -1484,0 +1589,79 @@\n+void InterpreterMacroAssembler::profile_array(Register mdp,\n+                                              Register array,\n+                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flattened_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element(Register mdp,\n+                                                Register element,\n+                                                Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadStoreData::array_load_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n@@ -1734,1 +1918,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1780,1 +1964,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":191,"deletions":7,"binary":false,"changes":198,"status":"modified"},{"patch":"@@ -147,0 +147,22 @@\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n+  \/\/ Allocate instance in \"obj\" and read in the content of the inline field\n+  \/\/ NOTES:\n+  \/\/   - input holder object via \"obj\", which must be r0,\n+  \/\/     will return new instance via the same reg\n+  \/\/   - assumes holder_klass and valueKlass field klass have both been resolved\n+  void read_inlined_field(Register holder_klass,\n+                          Register field_index, Register field_offset,\n+                          Register temp,  Register obj = r0);\n+\n+  \/\/ Allocate value buffer in \"obj\" and read in flattened element at the given index\n+  \/\/ NOTES:\n+  \/\/   - Return via \"obj\" must be r0\n+  \/\/   - kills all given regs\n+  void read_flattened_element(Register array, Register index,\n+                              Register t1, Register t2,\n+                              Register obj = r0);\n+\n@@ -192,1 +214,1 @@\n-  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype );\n+  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype, bool profile = true);\n@@ -285,1 +307,1 @@\n-  void profile_not_taken_branch(Register mdp);\n+  void profile_not_taken_branch(Register mdp, bool acmp = false);\n@@ -298,0 +320,3 @@\n+  void profile_array(Register mdp, Register array, Register tmp);\n+  void profile_element(Register mdp, Register element, Register tmp);\n+  void profile_acmp(Register mdp, Register left, Register right, Register tmp);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.hpp","additions":27,"deletions":2,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -56,0 +57,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -58,0 +60,1 @@\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -1125,0 +1128,35 @@\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  ldr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  ldr(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  ldr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass, temp_reg);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset);\n+  load_heap_oop(obj, field, temp_reg, rscratch2);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n@@ -1475,1 +1513,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1508,1 +1550,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1590,0 +1636,1 @@\n+  assert_different_registers(arg_1, c_rarg0);\n@@ -1597,0 +1644,2 @@\n+  assert_different_registers(arg_1, c_rarg0);\n+  assert_different_registers(arg_2, c_rarg0, c_rarg1);\n@@ -1603,0 +1652,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -1652,0 +1705,110 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  assert_different_registers(markword, rscratch2);\n+  andr(markword, markword, markWord::inline_type_mask_in_place);\n+  mov(rscratch2, markWord::inline_type_pattern);\n+  cmp(markword, rscratch2);\n+  br(Assembler::EQ, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  andr(temp_reg, temp_reg, JVM_ACC_VALUE);\n+  cbnz(temp_reg, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  assert_different_registers(tmp, rscratch1);\n+  cbz(object, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  ldr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  mov(rscratch1, is_inline_type_mask);\n+  andr(tmp, tmp, rscratch1);\n+  cmp(tmp, rscratch1);\n+  br(Assembler::NE, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  ldrw(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  andr(temp_reg, temp_reg, InstanceKlassFlags::is_empty_inline_type_value());\n+  cbnz(temp_reg, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_null_free_inline_type_shift, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbz(flags, ConstantPoolCacheEntry::is_null_free_inline_type_shift, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_inlined_shift, is_flattened);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  ldr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  tst(temp_reg, markWord::unlocked_value);\n+  br(Assembler::NE, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  load_prototype_header(temp_reg, oop);\n+\n+  bind(test_mark_word);\n+  andr(temp_reg, temp_reg, test_bit);\n+  if (jmp_set) {\n+    cbnz(temp_reg, jmp_label);\n+  } else {\n+    cbz(temp_reg, jmp_label);\n+  }\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label& is_flattened_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flattened_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::NE, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::EQ, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {\n+  tst(lh, Klass::_lh_null_free_array_bit_inplace);\n+  br(Assembler::NE, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {\n+  tst(lh, Klass::_lh_null_free_array_bit_inplace);\n+  br(Assembler::EQ, is_non_null_free_array);\n+}\n+\n@@ -4074,0 +4237,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -4132,0 +4303,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src) {\n+  load_klass(dst, src);\n+  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -4456,0 +4632,40 @@\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  ldr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  ldrw(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    add(data, data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rscratch1, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  ldrw(rscratch1, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  lsr(rscratch1, rscratch1, Klass::_lh_log2_element_size_shift);\n+  andr(rscratch1, rscratch1, Klass::_lh_log2_element_size_mask);\n+  lslv(index, index, rscratch1);\n+\n+  add(data, array, index);\n+  add(data, data, arrayOopDesc::base_offset_in_bytes(T_PRIMITIVE_OBJECT));\n+}\n+\n@@ -4532,0 +4748,96 @@\n+\/\/ Object \/ value buffer allocation...\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == r0, \"needs to be r0\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  ldrw(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  tst(layout_size, Klass::_lh_instance_slow_path_bit);\n+  br(Assembler::NE, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  if (UseTLAB) {\n+    push(klass);\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      b(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      b(initialize_object);\n+    }\n+\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      subs(layout_size, layout_size, sizeof(oopDesc));\n+      br(Assembler::EQ, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      tst(layout_size, 7);\n+      br(Assembler::EQ, L);\n+      stop(\"object size is not multiple of 8 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      lsr(layout_size, layout_size, LogBytesPerLong);\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        Register base = t2;\n+\n+        bind(loop);\n+        add(rscratch1, new_obj, layout_size, Assembler::LSL, LogBytesPerLong);\n+        str(zr, Address(rscratch1, sizeof(oopDesc) - 1*oopSize));\n+        subs(layout_size, layout_size, 1);\n+        br(Assembler::NE, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    ldr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes ()));\n+    store_klass_gap(new_obj, zr);  \/\/ zero klass gap for compressed oops\n+    mov(t2, klass);         \/\/ preserve klass\n+    store_klass(new_obj, t2);  \/\/ src klass reg is potentially compressed\n+\n+    \/\/ TODO: Valhalla removed SharedRuntime::dtrace_object_alloc from here ?\n+\n+    b(done);\n+  }\n+\n+  if (UseTLAB) {\n+    bind(slow_case);\n+    pop(klass);\n+  }\n+  bind(slow_case_no_pop);\n+  b(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -4571,0 +4883,13 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  ldr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cbnz(inline_klass, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  ldr(inline_klass, Address(inline_klass, index, Address::lsl(3)));\n+}\n+\n@@ -4696,0 +5021,51 @@\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  if (needs_stack_repair) {\n+    \/\/ Remove the extension of the caller's frame used for inline type unpacking\n+    \/\/\n+    \/\/ Right now the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Saved LR #1               |\n+    \/\/ | Saved FP #1               |\n+    \/\/ |---------------------------|\n+    \/\/ | Extension space for       |\n+    \/\/ |   inline arg (un)packing  |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | Saved LR #2               |\n+    \/\/ | Saved FP #2               |\n+    \/\/ |---------------------------|  <-- FP\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ There are two copies of FP and LR on the stack. They will be identical\n+    \/\/ unless the caller has been deoptimized, in which case LR #1 will be patched\n+    \/\/ to point at the deopt blob, and LR #2 will still point into the old method.\n+    \/\/\n+    \/\/ The sp_inc stack slot holds the total size of the frame including the\n+    \/\/ extension space minus two words for the saved FP and LR.\n+\n+    int sp_inc_offset = initial_framesize - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+    ldr(rscratch1, Address(sp, sp_inc_offset));\n+    add(sp, sp, rscratch1);\n+    ldp(rfp, lr, Address(post(sp, 2 * wordSize)));\n+  } else {\n+    remove_frame(initial_framesize);\n+  }\n+}\n+\n+void MacroAssembler::save_stack_increment(int sp_inc, int frame_size) {\n+  int real_frame_size = frame_size + sp_inc;\n+  assert(sp_inc == 0 || sp_inc > 2*wordSize, \"invalid sp_inc value\");\n+  assert(real_frame_size >= 2*wordSize, \"frame size must include FP\/LR space\");\n+  assert((real_frame_size & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+\n+  int sp_inc_offset = frame_size - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+  \/\/ Subtract two words for the saved FP and LR as these will be popped\n+  \/\/ separately. See remove_frame above.\n+  mov(rscratch1, real_frame_size - 2*wordSize);\n+  str(rscratch1, Address(sp, sp_inc_offset));\n+}\n@@ -5604,0 +5980,441 @@\n+#ifdef COMPILER2\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+\n+    mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n+    clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    bind(L_skip_barrier);\n+  }\n+\n+  if (C->max_vector_size() > 0) {\n+    reinitialize_ptrue();\n+  }\n+\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  if (C->output()->need_stack_bang(bangsize))\n+    generate_stack_overflow_check(bangsize);\n+\n+  \/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->output()->frame_size_in_bytes();\n+  build_frame(framesize);\n+\n+  if (C->needs_stack_repair()) {\n+    save_stack_increment(sp_inc, framesize);\n+  }\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+#endif \/\/ COMPILER2\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  tbz(r0, 0, skip);\n+  int call_offset = -1;\n+\n+  \/\/ Be careful not to clobber r1-7 which hold returned fields\n+  \/\/ Also do not use callee-saved registers as these may be live in the interpreter\n+  Register tmp1 = r13, tmp2 = r14, klass = r15, r0_preserved = r12;\n+\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != NULL, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(r0_preserved, r0); \/\/ save r0 for slow_case since *_allocate may corrupt it when allocation failed\n+\n+  if (vk != NULL) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(klass, (intptr_t)vk->get_InlineKlass());\n+    jint obj_size = vk->layout_helper();\n+    assert(obj_size != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB) {\n+      tlab_allocate(r0, noreg, obj_size, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+    andr(klass, r0, -2);\n+    ldrw(tmp2, Address(klass, Klass::layout_helper_offset()));\n+    if (UseTLAB) {\n+      tlab_allocate(r0, tmp2, 0, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = r0;\n+    mov(rscratch1, (intptr_t)markWord::inline_type_prototype().value());\n+    str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+    store_klass_gap(buffer_obj, zr);\n+    if (vk == NULL) {\n+      \/\/ store_klass corrupts klass, so save it for later use (interpreter case only).\n+      mov(tmp1, klass);\n+    }\n+    store_klass(buffer_obj, klass);\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != NULL) {\n+      far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      \/\/ tmp1 holds klass preserved above\n+      ldr(tmp1, Address(tmp1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      ldr(tmp1, Address(tmp1, InlineKlass::pack_handler_offset()));\n+      blr(tmp1);\n+    }\n+\n+    membar(Assembler::StoreStore);\n+    b(skip);\n+  } else {\n+    \/\/ Must have already branched to slow_case above.\n+    DEBUG_ONLY(should_not_reach_here());\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(r0, r0_preserved);\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    far_call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+  membar(Assembler::StoreStore);\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_Register() && to->is_Register()) {\n+          mov(to->as_Register(), from->as_Register());\n+        } else if (from->is_FloatRegister() && to->is_FloatRegister()) {\n+          fmovd(to->as_FloatRegister(), from->as_FloatRegister());\n+        } else {\n+          ShouldNotReachHere();\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+            ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  int sp_inc = args_on_stack * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  assert(sp_inc > 0, \"sanity\");\n+\n+  \/\/ Save a copy of the FP and LR here for deoptimization patching and frame walking\n+  stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+\n+  \/\/ Adjust the stack pointer. This will be repaired on return by MacroAssembler::remove_frame\n+  if (sp_inc < (1 << 9)) {\n+    sub(sp, sp, sp_inc);   \/\/ Fits in an immediate\n+  } else {\n+    mov(rscratch1, sp_inc);\n+    sub(sp, sp, rscratch1);\n+  }\n+\n+  return sp_inc + 2 * wordSize;  \/\/ Account for the FP\/LR space\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r11;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(tmp1, Address(sp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        cbz(fromReg, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        mov(tmp2, 1);\n+        str(tmp2, Address(sp, st_off));\n+      } else {\n+        mov(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_FloatRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr, rscratch1, rscratch2);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        str(dst, Address(sp, st_off));\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      ldrd(toReg->as_FloatRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      ldrs(toReg->as_FloatRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      b(L_notNull);\n+      bind(L_null);\n+      \/\/ Set IsInit field to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+            str(zr, Address(sp, st_off));\n+          } else {\n+            mov(toReg->as_Register(), zr);\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_PRIMITIVE_OBJECT, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ The GC barrier expanded by store_heap_oop below may call into the\n+  \/\/ runtime so use callee-saved registers for any values that need to be\n+  \/\/ preserved. The GC barrier assembler should take care of saving the\n+  \/\/ Java argument registers.\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r21;\n+  Register from_reg_tmp = r22;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r12;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_PRIMITIVE_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index), tmp1, tmp2);\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldrb(tmp2, Address(sp, ld_off));\n+        cbnz(tmp2, L_notNull);\n+      } else {\n+        cbnz(fromReg->as_Register(), L_notNull);\n+      }\n+      mov(val_obj, 0);\n+      b(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+\n+    if (!fromReg->is_FloatRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        load_sized_value(src, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      strd(fromReg->as_FloatRegister(), dst);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      strs(fromReg->as_FloatRegister(), dst);\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v8->as_VMReg() : r14->as_VMReg();\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":819,"deletions":2,"binary":false,"changes":821,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -35,0 +36,4 @@\n+#include \"runtime\/signature.hpp\"\n+\n+\n+class ciInlineKlass;\n@@ -609,0 +614,31 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type);\n+  void test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type);\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type);\n+\n+  \/\/ Get the default value oop for the given InlineKlass\n+  void get_default_value_oop(Register inline_klass, Register temp_reg, Register obj);\n+  \/\/ The empty value oop, for the given InlineKlass (\"empty\" as in no instance fields)\n+  \/\/ get_default_value_oop with extra assertion for empty inline klass\n+  void get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj);\n+\n+  void test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free);\n+  void test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free);\n+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened);\n+\n+  \/\/ Check oops for special arrays, i.e. flattened and\/or null-free\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flattened_array_oop(Register klass, Register temp_reg, Label& is_flattened_array);\n+  void test_non_flattened_array_oop(Register oop, Register temp_reg, Label&is_non_flattened_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flatten or null-free arrays...\n+  void test_flattened_array_layout(Register lh, Label& is_flattened_array);\n+  void test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array);\n+  void test_null_free_array_layout(Register lh, Label& is_null_free_array);\n+  void test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array);\n+\n@@ -845,0 +881,2 @@\n+  void load_metadata(Register dst, Register src);\n+\n@@ -859,0 +897,9 @@\n+  void access_value_copy(DecoratorSet decorators, Register src, Register dst, Register inline_klass);\n+\n+  \/\/ inline type data payload offsets...\n+  void first_field_offset(Register inline_klass, Register offset);\n+  void data_for_oop(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -872,0 +919,2 @@\n+  void load_prototype_header(Register dst, Register src);\n+\n@@ -919,0 +968,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -929,0 +987,3 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n@@ -1312,0 +1373,18 @@\n+  void verified_entry(Compile* C, int sp_inc);\n+\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[], Register val_array);\n+  int extend_stack_for_inline_args(int args_on_stack);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair);\n+  VMReg spill_reg_for(VMReg reg);\n+  void save_stack_increment(int sp_inc, int frame_size);\n+\n@@ -1376,0 +1455,2 @@\n+  void fill_words(Register base, uint64_t cnt, Register value);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":81,"deletions":0,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -340,0 +341,1 @@\n+    case T_PRIMITIVE_OBJECT:\n@@ -373,0 +375,80 @@\n+\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt, VMRegPair *regs, int total_args_passed) {\n+\n+  \/\/ Create the mapping between argument positions and registers.\n+\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    r0 \/* j_rarg7 *\/, j_rarg6, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+\n+  static const FloatRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3, j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+      \/\/ Should T_METADATA be added to java_calling_convention as well ?\n+    case T_METADATA:\n+    case T_PRIMITIVE_OBJECT:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -407,0 +489,109 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+     for (int i = 0; i < sig_extended->length(); i++) {\n+       BasicType bt = sig_extended->at(i)._bt;\n+       if (bt == T_PRIMITIVE_OBJECT) {\n+         \/\/ In sig_extended, an inline type argument starts with:\n+         \/\/ T_PRIMITIVE_OBJECT, followed by the types of the fields of the\n+         \/\/ inline type and T_VOID to mark the end of the value\n+         \/\/ type. Inline types are flattened so, for instance, in the\n+         \/\/ case of an inline type with an int field and an inline type\n+         \/\/ field that itself has 2 fields, an int and a long:\n+         \/\/ T_PRIMITIVE_OBJECT T_INT T_PRIMITIVE_OBJECT T_INT T_LONG T_VOID (second\n+         \/\/ slot for the T_LONG) T_VOID (inner T_PRIMITIVE_OBJECT) T_VOID\n+         \/\/ (outer T_PRIMITIVE_OBJECT)\n+         total_args_passed++;\n+         int vt = 1;\n+         do {\n+           i++;\n+           BasicType bt = sig_extended->at(i)._bt;\n+           BasicType prev_bt = sig_extended->at(i-1)._bt;\n+           if (bt == T_PRIMITIVE_OBJECT) {\n+             vt++;\n+           } else if (bt == T_VOID &&\n+                      prev_bt != T_LONG &&\n+                      prev_bt != T_DOUBLE) {\n+             vt--;\n+           }\n+         } while (vt != 0);\n+       } else {\n+         total_args_passed++;\n+       }\n+     }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   Register tmp1,\n+                                   Register tmp2,\n+                                   Register tmp3,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  assert(bt != T_PRIMITIVE_OBJECT || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"\");\n+    return;\n+  }\n+\n+  if (!r_1->is_FloatRegister()) {\n+    Register val = r25;\n+    if (r_1->is_stack()) {\n+      \/\/ memory to memory use r25 (scratch registers is used by store_heap_oop)\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, tmp1, tmp2, tmp3);\n+    if (is_oop) {\n+      __ store_heap_oop(to, val, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else {\n+      \/\/ only a float use just part of the slot\n+      __ strs(r_1->as_FloatRegister(), to);\n+    }\n+  }\n+}\n+\n@@ -408,3 +599,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -412,1 +601,28 @@\n-                            Label& skip_fixup) {\n+                            bool requires_clinit_barrier,\n+                            address& c2i_no_clinit_check_entry,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+  if (requires_clinit_barrier && VM_Version::supports_fast_class_init_checks()) {\n+    Label L_skip_barrier;\n+\n+    { \/\/ Bypass the barrier for non-static methods\n+      __ ldrw(rscratch1, Address(rmethod, Method::access_flags_offset()));\n+      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n+      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n+    }\n+\n+    __ load_method_holder(rscratch2, rmethod);\n+    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+\n+    __ bind(L_skip_barrier);\n+    c2i_no_clinit_check_entry = __ pc();\n+  }\n+\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->c2i_entry_barrier(masm);\n+\n@@ -422,1 +638,22 @@\n-  int words_pushed = 0;\n+  \/\/ Name some registers to be used in the following code. We can use\n+  \/\/ anything except r0-r7 which are arguments in the Java calling\n+  \/\/ convention, rmethod (r12), and r13 which holds the outgoing sender\n+  \/\/ SP for the interpreter.\n+  Register buf_array = r10;   \/\/ Array of buffered inline types\n+  Register buf_oop = r11;     \/\/ Buffered inline type oop\n+  Register tmp1 = r15;\n+  Register tmp2 = r16;\n+  Register tmp3 = r17;\n+\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_PRIMITIVE_OBJECT);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types\n+      \/\/ Allocate the buffers here with a runtime call.\n+      RegisterSaver reg_save(false \/* save_vectors *\/);\n+      OopMap* map = reg_save.save_live_registers(masm, 0, &frame_size_in_words);\n@@ -424,2 +661,2 @@\n-  \/\/ Since all args are passed on the stack, total_args_passed *\n-  \/\/ Interpreter::stackElementSize is the space we need.\n+      frame_complete = __ offset();\n+      address the_pc = __ pc();\n@@ -427,1 +664,2 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+      Label retaddr;\n+      __ set_last_Java_frame(sp, noreg, retaddr, rscratch1);\n@@ -429,1 +667,3 @@\n-  __ mov(r19_sender_sp, sp);\n+      __ mov(c_rarg0, rthread);\n+      __ mov(c_rarg1, rmethod);\n+      __ mov(c_rarg2, (int64_t)alloc_inline_receiver);\n@@ -431,2 +671,3 @@\n-  \/\/ stack is aligned, keep it that way\n-  extraspace = align_up(extraspace, 2*wordSize);\n+      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+      __ blr(rscratch1);\n+      __ bind(retaddr);\n@@ -434,2 +675,2 @@\n-  if (extraspace)\n-    __ sub(sp, sp, extraspace);\n+      oop_maps->add_gc_map(__ pc() - start, map);\n+      __ reset_last_Java_frame(false);\n@@ -437,6 +678,1 @@\n-  \/\/ Now write the args into the outgoing interpreter space\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n+      reg_save.restore_live_registers(masm);\n@@ -444,16 +680,3 @@\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+      Label no_exception;\n+      __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));\n+      __ cbz(rscratch1, no_exception);\n@@ -461,5 +684,9 @@\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n+      __ str(zr, Address(rthread, JavaThread::vm_result_offset()));\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ b(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(buf_array, rthread);\n+      __ get_vm_result_2(rmethod, rthread); \/\/ TODO: required to keep the callee Method live?\n@@ -467,9 +694,1 @@\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rscratch1\n-      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size\n-                    + extraspace\n-                    + words_pushed * wordSize);\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ ldrw(rscratch1, Address(sp, ld_off));\n-        __ str(rscratch1, Address(sp, st_off));\n+  }\n@@ -477,1 +696,8 @@\n-      } else {\n+  \/\/ Since all args are passed on the stack, total_args_passed *\n+  \/\/ Interpreter::stackElementSize is the space we need.\n+\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n+  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+\n+  \/\/ stack is aligned, keep it that way\n+  extraspace = align_up(extraspace, StackAlignmentInBytes);\n@@ -479,1 +705,2 @@\n-        __ ldr(rscratch1, Address(sp, ld_off));\n+  \/\/ set senderSP value\n+  __ mov(r19_sender_sp, sp);\n@@ -481,6 +708,28 @@\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ str(rscratch1, Address(sp, next_off));\n+  __ sub(sp, sp, extraspace);\n+\n+  \/\/ Now write the args into the outgoing interpreter space\n+\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_PRIMITIVE_OBJECT,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_PRIMITIVE_OBJECT\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_PRIMITIVE_OBJECT) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(sp, offset), tmp1, tmp2, tmp3, extraspace, false);\n+      next_arg_int++;\n@@ -488,7 +737,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaaaull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ str(rscratch1, Address(sp, st_off));\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov(rscratch1, CONST64(0xdeadffffdeadaaaa));\n+        __ str(rscratch1, Address(sp, st_off));\n@@ -496,16 +742,24 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ str(r, Address(sp, st_off));\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ jlong\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaabull);\n-          __ str(rscratch1, Address(sp, st_off));\n-          __ str(r, Address(sp, next_off));\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_PRIMITIVE_OBJECT);\n+      __ load_heap_oop(buf_oop, Address(buf_array, index), tmp1, tmp2);\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      Label L_null;\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp - 1)._bt;\n+        if (bt == T_PRIMITIVE_OBJECT) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -514,1 +768,22 @@\n-          __ str(r, Address(sp, st_off));\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          if (off == -1) {\n+            \/\/ Nullable inline type argument, emit null check\n+            VMReg reg = regs[next_arg_comp-ignored].first();\n+            Label L_notNull;\n+            if (reg->is_stack()) {\n+              int ld_off = reg->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+              __ ldrb(tmp1, Address(sp, ld_off));\n+              __ cbnz(tmp1, L_notNull);\n+            } else {\n+              __ cbnz(reg->as_Register(), L_notNull);\n+            }\n+            __ str(zr, Address(sp, st_off));\n+            __ b(L_null);\n+            __ bind(L_notNull);\n+            continue;\n+          }\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(buf_oop, off), tmp1, tmp2, tmp3, extraspace, is_oop);\n@@ -516,14 +791,4 @@\n-      }\n-    } else {\n-      assert(r_1->is_FloatRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ strs(r_1->as_FloatRegister(), Address(sp, st_off));\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaacull);\n-        __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        __ strd(r_1->as_FloatRegister(), Address(sp, next_off));\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ str(buf_oop, Address(sp, st_off));\n+      __ bind(L_null);\n@@ -539,0 +804,1 @@\n+void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm, int comp_args_on_stack, const GrowableArray<SigEntry>* sig, const VMRegPair *regs) {\n@@ -540,5 +806,0 @@\n-void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,\n-                                    int total_args_passed,\n-                                    int comp_args_on_stack,\n-                                    const BasicType *sig_bt,\n-                                    const VMRegPair *regs) {\n@@ -602,1 +863,1 @@\n-  int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)>>LogBytesPerWord;\n+  int comp_words_on_stack = 0;\n@@ -604,2 +865,3 @@\n-    __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n-    __ andr(sp, rscratch1, -16);\n+     comp_words_on_stack = align_up(comp_args_on_stack * VMRegImpl::stack_slot_size, wordSize) >> LogBytesPerWord;\n+     __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n+     __ andr(sp, rscratch1, -16);\n@@ -610,1 +872,1 @@\n-  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_offset())));\n+  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_inline_offset())));\n@@ -624,0 +886,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -626,2 +890,5 @@\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+    BasicType bt = sig->at(i)._bt;\n+\n+    assert(bt != T_PRIMITIVE_OBJECT, \"i2c adapter doesn't unpack inline typ args\");\n+    if (bt == T_VOID) {\n+      assert(i > 0 && (sig->at(i - 1)._bt == T_LONG || sig->at(i - 1)._bt == T_DOUBLE), \"missing half\");\n@@ -632,0 +899,1 @@\n+    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(), \"scrambled load targets?\");\n@@ -633,3 +901,1 @@\n-    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),\n-            \"scrambled load targets?\");\n-    int ld_off = (total_args_passed - i - 1)*Interpreter::stackElementSize;\n+    int ld_off = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n@@ -650,1 +916,1 @@\n-      int st_off = regs[i].first()->reg2stack()*VMRegImpl::stack_slot_size;\n+      int st_off = regs[i].first()->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -667,2 +933,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n@@ -671,11 +936,28 @@\n-        __ str(rscratch2, Address(sp, st_off));\n-      }\n-    } else if (r_1->is_Register()) {  \/\/ Register argument\n-      Register r = r_1->as_Register();\n-      if (r_2->is_valid()) {\n-        \/\/\n-        \/\/ We are using two VMRegs. This can be either T_OBJECT,\n-        \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n-        \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n-        \/\/ So we must adjust where to pick up the data to match the\n-        \/\/ interpreter.\n+         __ str(rscratch2, Address(sp, st_off));\n+       }\n+     } else if (r_1->is_Register()) {  \/\/ Register argument\n+       Register r = r_1->as_Register();\n+       if (r_2->is_valid()) {\n+         \/\/\n+         \/\/ We are using two VMRegs. This can be either T_OBJECT,\n+         \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n+         \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n+         \/\/ So we must adjust where to pick up the data to match the\n+         \/\/ interpreter.\n+\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n+\n+         \/\/ this can be a misaligned move\n+         __ ldr(r, Address(esp, offset));\n+       } else {\n+         \/\/ sign extend and use a full word?\n+         __ ldrw(r, Address(esp, ld_off));\n+       }\n+     } else {\n+       if (!r_2->is_valid()) {\n+         __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n+       } else {\n+         __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n+       }\n+     }\n+   }\n@@ -683,17 +965,0 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n-\n-        \/\/ this can be a misaligned move\n-        __ ldr(r, Address(esp, offset));\n-      } else {\n-        \/\/ sign extend and use a full word?\n-        __ ldrw(r, Address(esp, ld_off));\n-      }\n-    } else {\n-      if (!r_2->is_valid()) {\n-        __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n-      } else {\n-        __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n-      }\n-    }\n-  }\n@@ -716,1 +981,0 @@\n-\n@@ -720,13 +984,1 @@\n-\/\/ ---------------------------------------------------------------\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                                            int total_args_passed,\n-                                                            int comp_args_on_stack,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n-  address i2c_entry = __ pc();\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n-\n-  address c2i_unverified_entry = __ pc();\n-  Label skip_fixup;\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n@@ -767,0 +1019,1 @@\n+}\n@@ -768,5 +1021,12 @@\n-  address c2i_entry = __ pc();\n-  \/\/ Class initialization barrier for static methods\n-  address c2i_no_clinit_check_entry = NULL;\n-  if (VM_Version::supports_fast_class_init_checks()) {\n-    Label L_skip_barrier;\n+\/\/ ---------------------------------------------------------------\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler* masm,\n+                                                            int comp_args_on_stack,\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter,\n+                                                            bool allocate_code_blob) {\n@@ -775,5 +1035,2 @@\n-    { \/\/ Bypass the barrier for non-static methods\n-      __ ldrw(rscratch1, Address(rmethod, Method::access_flags_offset()));\n-      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n-      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n-    }\n+  address i2c_entry = __ pc();\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -781,3 +1038,3 @@\n-    __ load_method_holder(rscratch2, rmethod);\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+  address c2i_unverified_entry        = __ pc();\n+  address c2i_unverified_inline_entry = __ pc();\n+  Label skip_fixup;\n@@ -785,3 +1042,1 @@\n-    __ bind(L_skip_barrier);\n-    c2i_no_clinit_check_entry = __ pc();\n-  }\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -789,2 +1044,3 @@\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->c2i_entry_barrier(masm);\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n@@ -792,1 +1048,26 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n+  address c2i_no_clinit_check_entry = NULL;\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    \/\/ No class init barrier needed because method is guaranteed to be non-static\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, \/* requires_clinit_barrier = *\/ false, c2i_no_clinit_check_entry,\n+                    skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+    skip_fixup.reset();\n+  }\n+\n+  \/\/ Scalarized c2i adapter\n+  address c2i_entry        = __ pc();\n+  address c2i_inline_entry = __ pc();\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, \/* requires_clinit_barrier = *\/ true, c2i_no_clinit_check_entry,\n+                  skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ true);\n+\n+  \/\/ Non-scalarized c2i adapter\n+  if (regs != regs_cc) {\n+    c2i_unverified_inline_entry = __ pc();\n+    Label inline_entry_skip_fixup;\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, \/* requires_clinit_barrier = *\/ true, c2i_no_clinit_check_entry,\n+                    inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+  }\n@@ -795,1 +1076,9 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+\n+  \/\/ The c2i adapter might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  if (allocate_code_blob) {\n+    bool caller_must_gc_arguments = (regs != regs_cc);\n+    new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n+  }\n+\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -843,0 +1132,1 @@\n+      case T_PRIMITIVE_OBJECT:\n@@ -1667,0 +1957,1 @@\n+      case T_PRIMITIVE_OBJECT:\n@@ -1781,0 +2072,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        __ andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -1846,0 +2141,1 @@\n+  case T_PRIMITIVE_OBJECT:           \/\/ Really a handle\n@@ -3096,0 +3392,113 @@\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler _masm(&buffer);\n+  MacroAssembler* masm = &_masm;\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  Register Rresult = r14;  \/\/ See StubGenerator::generate_call_stub().\n+  __ ldr(r0, Address(Rresult));\n+  __ resolve_jobject(r0 \/* value *\/,\n+                     rthread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ str(r0, Address(Rresult));\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_PRIMITIVE_OBJECT) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ strs(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_DOUBLE) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r15, r16, r17);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r15, r16, r17, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(lr);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  Label skip;\n+  __ cbz(r0, skip);\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_PRIMITIVE_OBJECT) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ ldrs(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ ldrd(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(r0, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from, rscratch1, rscratch2);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ bind(skip);\n+\n+  __ ret(lr);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":583,"deletions":174,"binary":false,"changes":757,"status":"modified"},{"patch":"@@ -315,1 +315,1 @@\n-    \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+    \/\/ T_OBJECT, T_PRIMITIVE_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n@@ -318,4 +318,13 @@\n-    __ ldr(j_rarg2, result);\n-    Label is_long, is_float, is_double, exit;\n-    __ ldr(j_rarg1, result_type);\n-    __ cmp(j_rarg1, (u1)T_OBJECT);\n+    \/\/ All of j_rargN may be used to return inline type fields so be careful\n+    \/\/ not to clobber those.\n+    \/\/ SharedRuntime::generate_buffered_inline_type_adapter() knows the register\n+    \/\/ assignment of Rresult below.\n+    Register Rresult = r14, Rresult_type = r15;\n+    __ ldr(Rresult, result);\n+    Label is_long, is_float, is_double, check_prim, exit;\n+    __ ldr(Rresult_type, result_type);\n+    __ cmp(Rresult_type, (u1)T_OBJECT);\n+    __ br(Assembler::EQ, check_prim);\n+    __ cmp(Rresult_type, (u1)T_PRIMITIVE_OBJECT);\n+    __ br(Assembler::EQ, check_prim);\n+    __ cmp(Rresult_type, (u1)T_LONG);\n@@ -323,3 +332,1 @@\n-    __ cmp(j_rarg1, (u1)T_LONG);\n-    __ br(Assembler::EQ, is_long);\n-    __ cmp(j_rarg1, (u1)T_FLOAT);\n+    __ cmp(Rresult_type, (u1)T_FLOAT);\n@@ -327,1 +334,1 @@\n-    __ cmp(j_rarg1, (u1)T_DOUBLE);\n+    __ cmp(Rresult_type, (u1)T_DOUBLE);\n@@ -331,1 +338,1 @@\n-    __ strw(r0, Address(j_rarg2));\n+    __ strw(r0, Address(Rresult));\n@@ -379,0 +386,11 @@\n+    __ BIND(check_prim);\n+    if (InlineTypeReturnedAsFields) {\n+      \/\/ Check for scalarized return value\n+      __ tbz(r0, 0, is_long);\n+      \/\/ Load pack handler address\n+      __ andr(rscratch1, r0, -2);\n+      __ ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(rscratch1, Address(rscratch1, InlineKlass::pack_handler_jobject_offset()));\n+      __ blr(rscratch1);\n+      __ b(exit);\n+    }\n@@ -381,1 +399,1 @@\n-    __ str(r0, Address(j_rarg2, 0));\n+    __ str(r0, Address(Rresult, 0));\n@@ -385,1 +403,1 @@\n-    __ strs(j_farg0, Address(j_rarg2, 0));\n+    __ strs(j_farg0, Address(Rresult, 0));\n@@ -389,1 +407,1 @@\n-    __ strd(j_farg0, Address(j_rarg2, 0));\n+    __ strd(j_farg0, Address(Rresult, 0));\n@@ -2091,0 +2109,8 @@\n+    \/\/ Check for flat inline type array -> return -1\n+    __ tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+    __ br(Assembler::NE, L_failed);\n+\n+    \/\/ Check for null-free (non-flat) inline type array -> handle as object array\n+    __ tst(lh, Klass::_lh_null_free_array_bit_inplace);\n+    __ br(Assembler::NE, L_failed);\n+\n@@ -7882,0 +7908,128 @@\n+  \/\/ Call here from the interpreter or compiled code to either load\n+  \/\/ multiple returned values from the inline type instance being\n+  \/\/ returned to registers or to store returned values to a newly\n+  \/\/ allocated inline type instance.\n+  address generate_return_value_stub(address destination, const char* name, bool has_res) {\n+    \/\/ We need to save all registers the calling convention may use so\n+    \/\/ the runtime calls read or update those registers. This needs to\n+    \/\/ be in sync with SharedRuntime::java_return_convention().\n+    \/\/ n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0\n+    enum layout {\n+      j_rarg7_off = 0, j_rarg7_2,    \/\/ j_rarg7 is r0\n+      j_rarg6_off, j_rarg6_2,\n+      j_rarg5_off, j_rarg5_2,\n+      j_rarg4_off, j_rarg4_2,\n+      j_rarg3_off, j_rarg3_2,\n+      j_rarg2_off, j_rarg2_2,\n+      j_rarg1_off, j_rarg1_2,\n+      j_rarg0_off, j_rarg0_2,\n+\n+      j_farg7_off, j_farg7_2,\n+      j_farg6_off, j_farg6_2,\n+      j_farg5_off, j_farg5_2,\n+      j_farg4_off, j_farg4_2,\n+      j_farg3_off, j_farg3_2,\n+      j_farg2_off, j_farg2_2,\n+      j_farg1_off, j_farg1_2,\n+      j_farg0_off, j_farg0_2,\n+\n+      rfp_off, rfp_off2,\n+      return_off, return_off2,\n+\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    CodeBuffer code(name, 512, 64);\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+\n+    int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+    assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+    int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+    int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+    OopMapSet* oop_maps = new OopMapSet();\n+    OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg7_off), j_rarg7->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg6_off), j_rarg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+    address start = __ pc();\n+\n+    __ enter(); \/\/ Save FP and LR before call\n+\n+    __ stpd(j_farg1, j_farg0, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg3, j_farg2, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg5, j_farg4, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg7, j_farg6, Address(__ pre(sp, -2 * wordSize)));\n+\n+    __ stp(j_rarg1, j_rarg0, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg3, j_rarg2, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg5, j_rarg4, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg7, j_rarg6, Address(__ pre(sp, -2 * wordSize)));\n+\n+    int frame_complete = __ offset();\n+\n+    \/\/ Set up last_Java_sp and last_Java_fp\n+    address the_pc = __ pc();\n+    __ set_last_Java_frame(sp, noreg, the_pc, rscratch1);\n+\n+    \/\/ Call runtime\n+    __ mov(c_rarg1, r0);\n+    __ mov(c_rarg0, rthread);\n+\n+    __ mov(rscratch1, destination);\n+    __ blr(rscratch1);\n+\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    __ reset_last_Java_frame(false);\n+\n+    __ ldp(j_rarg7, j_rarg6, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg5, j_rarg4, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg3, j_rarg2, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg1, j_rarg0, Address(__ post(sp, 2 * wordSize)));\n+\n+    __ ldpd(j_farg7, j_farg6, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg5, j_farg4, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg3, j_farg2, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg1, j_farg0, Address(__ post(sp, 2 * wordSize)));\n+\n+    __ leave();\n+\n+    \/\/ check for pending exceptions\n+    Label pending;\n+    __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ cbnz(rscratch1, pending);\n+\n+    if (has_res) {\n+      __ get_vm_result(r0, rthread);\n+    }\n+\n+    __ ret(lr);\n+\n+    __ bind(pending);\n+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+    \/\/ -------------\n+    \/\/ make sure all code is generated\n+    masm->flush();\n+\n+    RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &code, frame_complete, frame_size_in_words, oop_maps, false);\n+    return stub->entry_point();\n+  }\n+\n@@ -7931,0 +8085,7 @@\n+\n+    if (InlineTypeReturnedAsFields) {\n+      StubRoutines::_load_inline_type_fields_in_regs =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), \"load_inline_type_fields_in_regs\", false);\n+      StubRoutines::_store_inline_type_fields_to_buf =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), \"store_inline_type_fields_to_buf\", true);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":174,"deletions":13,"binary":false,"changes":187,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -438,0 +439,5 @@\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(NULL, true);\n+  }\n+\n@@ -543,0 +549,1 @@\n+  case T_PRIMITIVE_OBJECT: \/\/ fall through (value types are handled with oops)\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -3224,0 +3224,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1818,1 +1818,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n@@ -1859,1 +1859,1 @@\n-    profile_obj_type(ret, R28_mdx, -in_bytes(ReturnTypeEntry::size()), tmp1, tmp2);\n+    profile_obj_type(ret, R28_mdx, -in_bytes(SingleTypeEntry::size()), tmp1, tmp2);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/frame.inline.hpp\"\n@@ -67,0 +68,4 @@\n+  if (EnableValhalla) {\n+    \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+    andptr(hdr, ~((int) markWord::inline_type_bit_in_place));\n+  }\n@@ -148,1 +153,9 @@\n-  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n+  if (EnableValhalla) {\n+    \/\/ Need to copy markWord::prototype header for klass\n+    assert_different_registers(obj, klass, len, t1, t2);\n+    movptr(t1, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);\n+  } else {\n+    \/\/ This assumes that all prototype bits fit in an int32_t\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n+  }\n@@ -301,0 +314,12 @@\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_offset_for_orig_pc, int sp_inc, bool reset_orig_pc, bool needs_stack_repair) {\n+  push(rbp);\n+  if (PreserveFramePointer) {\n+    mov(rbp, rsp);\n+  }\n+#if !defined(_LP64) && defined(COMPILER2)\n+  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_jvmci()) {\n+      \/\/ c2 leaves fpu stack dirty. Clean it on entry\n+      empty_FPU_stack();\n+    }\n+#endif \/\/ !_LP64 && COMPILER2\n+  decrement(rsp, frame_size_in_bytes);\n@@ -302,2 +327,13 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n-  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  if (needs_stack_repair) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    int real_frame_size = sp_inc + frame_size_in_bytes + wordSize;\n+    movptr(Address(rsp, frame_size_in_bytes - wordSize), real_frame_size);\n+  }\n+  if (reset_orig_pc) {\n+    \/\/ Zero orig_pc to detect deoptimization during buffering in the entry points\n+    movptr(Address(rsp, sp_offset_for_orig_pc), 0);\n+  }\n+}\n+\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n@@ -309,0 +345,1 @@\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n@@ -311,11 +348,1 @@\n-  push(rbp);\n-  if (PreserveFramePointer) {\n-    mov(rbp, rsp);\n-  }\n-#if !defined(_LP64) && defined(COMPILER2)\n-  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_jvmci()) {\n-    \/\/ c2 leaves fpu stack dirty. Clean it on entry\n-    empty_FPU_stack();\n-  }\n-#endif \/\/ !_LP64 && COMPILER2\n-  decrement(rsp, frame_size_in_bytes); \/\/ does not emit code for frame_size == 0\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, has_scalarized_args, needs_stack_repair);\n@@ -326,5 +353,4 @@\n-}\n-\n-void C1_MacroAssembler::remove_frame(int frame_size_in_bytes) {\n-  increment(rsp, frame_size_in_bytes);  \/\/ Does not emit code for frame_size == 0\n-  pop(rbp);\n+  if (verified_inline_entry_label != NULL) {\n+    \/\/ Jump here from the scalarized entry points that already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -334,1 +360,0 @@\n-\n@@ -351,0 +376,58 @@\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  GrowableArray<SigEntry>* sig    = ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? ces->sig_cc_ro() : ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, true, ces->c1_needs_stack_repair());\n+\n+  \/\/ The runtime call might safepoint, make sure nmethod entry barrier is executed\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  movptr(rbx, (intptr_t)(ces->method()));\n+  if (is_inline_ro_entry) {\n+    call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id)));\n+  } else {\n+    call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ Remove the temp frame\n+  addptr(rsp, frame_size_in_bytes);\n+  pop(rbp);\n+\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    sp_inc = extend_stack_for_inline_args(args_on_stack);\n+  }\n+\n+  shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs,          \/\/ to\n+                      sp_inc, rax);\n+\n+  \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+  \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, sp_inc, false, ces->c1_needs_stack_repair());\n+\n+  jmp(verified_inline_entry_label);\n+  return rt_call_offset;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":103,"deletions":20,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -48,1 +48,20 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void C2_MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+    Register klass = rscratch1;\n+\n+    mov_metadata(klass, C->method()->holder()->constant_encoding());\n+    clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    bind(L_skip_barrier);\n+  }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n@@ -101,0 +120,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment just below the saved rbp (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, framesize - wordSize), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -129,0 +154,1 @@\n+}\n@@ -130,17 +156,14 @@\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n- #ifdef _LP64\n-    if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n-      \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n-      Label dummy_slow_path;\n-      Label dummy_continuation;\n-      Label* slow_path = &dummy_slow_path;\n-      Label* continuation = &dummy_continuation;\n-      if (!Compile::current()->output()->in_scratch_emit_size()) {\n-        \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-        C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-        Compile::current()->output()->add_stub(stub);\n-        slow_path = &stub->entry();\n-        continuation = &stub->continuation();\n-      }\n-      bs->nmethod_entry_barrier(this, slow_path, continuation);\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+#ifdef _LP64\n+  if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+    \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n+    Label dummy_slow_path;\n+    Label dummy_continuation;\n+    Label* slow_path = &dummy_slow_path;\n+    Label* continuation = &dummy_continuation;\n+    if (!Compile::current()->output()->in_scratch_emit_size()) {\n+      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+      Compile::current()->output()->add_stub(stub);\n+      slow_path = &stub->entry();                                                                                                                                                                                                                                                                                                                                                              continuation = &stub->continuation();\n@@ -148,0 +171,2 @@\n+    bs->nmethod_entry_barrier(this, slow_path, continuation);\n+  }\n@@ -149,2 +174,2 @@\n-    \/\/ Don't bother with out-of-line nmethod entry barrier stub for x86_32.\n-    bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n+  \/\/ Don't bother with out-of-line nmethod entry barrier stub for x86_32.\n+  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n@@ -152,1 +177,0 @@\n-  }\n@@ -607,0 +631,4 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andptr(tmpReg, ~((int) markWord::inline_type_bit_in_place));\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":49,"deletions":21,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(Compile* C, int sp_inc = 0);\n@@ -34,0 +34,1 @@\n+  void entry_barrier();\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -146,1 +146,0 @@\n-      sender_unextended_sp = sender_sp;\n@@ -150,2 +149,2 @@\n-      saved_fp = (intptr_t*) *(sender_sp - frame::sender_sp_offset);\n-    }\n+      intptr_t** saved_fp_addr = (intptr_t**) (sender_sp - frame::sender_sp_offset);\n+      saved_fp = *saved_fp_addr;\n@@ -153,0 +152,4 @@\n+      \/\/ Repair the sender sp if this is a method with scalarized inline type args\n+      sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+      sender_unextended_sp = sender_sp;\n+    }\n@@ -554,0 +557,1 @@\n+    case T_PRIMITIVE_OBJECT:\n@@ -657,0 +661,15 @@\n+\/\/ Check for a method with scalarized inline type arguments that needs\n+\/\/ a stack repair and return the repaired sender stack pointer.\n+intptr_t* frame::repair_sender_sp(intptr_t* sender_sp, intptr_t** saved_fp_addr) const {\n+  CompiledMethod* cm = _cb->as_compiled_method_or_null();\n+  if (cm != NULL && cm->needs_stack_repair()) {\n+    \/\/ The stack increment resides just below the saved rbp on the stack\n+    \/\/ and does not account for the return address.\n+    intptr_t* real_frame_size_addr = (intptr_t*) (saved_fp_addr - 1);\n+    int real_frame_size = ((*real_frame_size_addr) + wordSize) \/ wordSize;\n+    assert(real_frame_size >= _cb->frame_size() && real_frame_size <= 1000000, \"invalid frame size\");\n+    sender_sp = unextended_sp() + real_frame_size;\n+  }\n+  return sender_sp;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":22,"deletions":3,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -147,0 +147,3 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  intptr_t* repair_sender_sp(intptr_t* sender_sp, intptr_t** saved_fp_addr) const;\n+\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -36,0 +36,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -401,2 +404,3 @@\n-  \/\/ On Intel the return_address is always the word on the stack\n-  address sender_pc = (address) *(sender_sp-1);\n+#ifdef ASSERT\n+  address sender_pc_copy = (address) *(sender_sp-1);\n+#endif\n@@ -409,0 +413,16 @@\n+  \/\/ Repair the sender sp if the frame has been extended\n+  sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+\n+  \/\/ On Intel the return_address is always the word on the stack\n+  address sender_pc = (address) *(sender_sp-1);\n+\n+#ifdef ASSERT\n+  if (sender_pc != sender_pc_copy) {\n+    \/\/ When extending the stack in the callee method entry to make room for unpacking of value\n+    \/\/ type args, we keep a copy of the sender pc at the expected location in the callee frame.\n+    \/\/ If the sender pc is patched due to deoptimization, the copy is not consistent anymore.\n+    nmethod* nm = CodeCache::find_blob(sender_pc)->as_nmethod();\n+    assert(sender_pc == nm->deopt_mh_handler_begin() || sender_pc == nm->deopt_handler_begin(), \"unexpected sender pc\");\n+  }\n+#endif\n+\n@@ -413,2 +433,20 @@\n-    if (!_cb->is_compiled()) { \/\/ compiled frames do not use callee-saved registers\n-      map->set_include_argument_oops(_cb->caller_must_gc_arguments(map->thread()));\n+    bool c1_buffering = false;\n+#ifdef COMPILER1\n+    nmethod* nm = _cb->as_nmethod_or_null();\n+    if (nm != NULL && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+        pc() < nm->verified_inline_entry_point()) {\n+      \/\/ The VEP and VIEP(RO) of C1-compiled methods call buffer_inline_args_xxx\n+      \/\/ before doing any argument shuffling, so we need to scan the oops\n+      \/\/ as the caller passes them.\n+      c1_buffering = true;\n+#ifdef ASSERT\n+      NativeCall* call = nativeCall_before(pc());\n+      address dest = call->destination();\n+      assert(dest == Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id) ||\n+             dest == Runtime1::entry_for(Runtime1::buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+    }\n+#endif\n+    if (!_cb->is_compiled() || c1_buffering) { \/\/ compiled frames do not use callee-saved registers\n+      bool caller_args = _cb->caller_must_gc_arguments(map->thread()) || c1_buffering;\n+      map->set_include_argument_oops(caller_args);\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.inline.hpp","additions":42,"deletions":4,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"asm\/macroAssembler.inline.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"gc\/shared\/barrierSetRuntime.hpp\"\n@@ -47,0 +49,1 @@\n+  assert(type != T_PRIMITIVE_OBJECT, \"Not supported yet\");\n@@ -112,0 +115,1 @@\n+  assert(type != T_PRIMITIVE_OBJECT, \"Not supported yet\");\n@@ -198,0 +202,13 @@\n+void BarrierSetAssembler::value_copy(MacroAssembler* masm, DecoratorSet decorators,\n+                                     Register src, Register dst, Register value_klass) {\n+  \/\/ value_copy implementation is fairly complex, and there are not any\n+  \/\/ \"short-cuts\" to be made from asm. What there is, appears to have the same\n+  \/\/ cost in C++, so just \"call_VM_leaf\" for now rather than maintain hundreds\n+  \/\/ of hand-rolled instructions...\n+  if (decorators & IS_DEST_UNINITIALIZED) {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetRuntime::value_copy_is_dest_uninitialized), src, dst, value_klass);\n+  } else {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetRuntime::value_copy), src, dst, value_klass);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,0 +52,3 @@\n+  virtual void value_copy(MacroAssembler* masm, DecoratorSet decorators,\n+                          Register src, Register dst, Register value_klass);\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -201,0 +201,1 @@\n+  assert(type != T_PRIMITIVE_OBJECT, \"Not supported yet\");\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zBarrierSetAssembler_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -153,1 +154,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -198,1 +199,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n@@ -558,1 +559,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -566,1 +568,3 @@\n-  profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  if (profile) {\n+    profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  }\n@@ -572,1 +576,3 @@\n-  profile_typecheck_failed(rcx); \/\/ blows rcx\n+  if (profile) {\n+    profile_typecheck_failed(rcx); \/\/ blows rcx\n+  }\n@@ -1018,1 +1024,1 @@\n- \/\/ get method access flags\n+  \/\/ get method access flags\n@@ -1142,4 +1148,2 @@\n-  \/\/ remove activation\n-  \/\/ get sender sp\n-  movptr(rbx,\n-         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    movptr(rbx,\n+               Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n@@ -1167,0 +1171,40 @@\n+\n+  \/\/ remove activation\n+  \/\/ get sender sp\n+  movptr(rbx,\n+         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    Label skip;\n+    test_oop_is_not_inline_type(rax, rscratch1, skip);\n+\n+#ifndef _LP64\n+    super_call_VM_leaf(StubRoutines::load_inline_type_fields_in_regs());\n+#else\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rdi, rax, rscratch1);\n+    movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    testptr(rdi, rdi);\n+    jcc(Assembler::zero, skip);\n+    call(rdi);\n+#endif\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      movptr(rscratch1, Address(rbp, frame::interpreter_frame_method_offset * wordSize));\n+      movl(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      testl(rcx, Method::scalarized_return_flag());\n+      jcc(Assembler::zero, skip_stress);\n+      load_klass(rax, rax, rscratch1);\n+      orptr(rax, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    \/\/ call above kills the value in rbx. Reload it.\n+    movptr(rbx, Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n@@ -1187,0 +1231,106 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  {\n+    SkipIfEqual skip_if(this, &DTraceAllocProbes, 0, rscratch1);\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::read_inlined_field(Register holder_klass,\n+                                                     Register field_index, Register field_offset,\n+                                                     Register obj) {\n+  Label alloc_failed, empty_value, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+  assert_different_registers(obj, holder_klass, field_index, field_offset, dst_temp);\n+\n+  \/\/ Grap the inline field klass\n+  push(holder_klass);\n+  const Register field_klass = holder_klass;\n+  get_inline_type_field_klass(holder_klass, field_index, field_klass);\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(field_klass, dst_temp, empty_value);\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  data_for_oop(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, field_klass);\n+  pop(obj);\n+  pop(holder_klass);\n+  jmp(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(field_klass, dst_temp, obj);\n+  pop(holder_klass);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  pop(holder_klass);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_inlined_field),\n+          obj, field_index, holder_klass);\n+\n+  bind(done);\n+}\n+\n+void InterpreterMacroAssembler::read_flattened_element(Register array, Register index,\n+                                                       Register t1, Register t2,\n+                                                       Register obj) {\n+  assert_different_registers(array, index, t1, t2);\n+  Label alloc_failed, empty_value, done;\n+  const Register array_klass = t2;\n+  const Register elem_klass = t1;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+\n+  \/\/ load in array->klass()->element_klass()\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(array_klass, array, tmp_load_klass);\n+  movptr(elem_klass, Address(array_klass, ArrayKlass::element_klass_offset()));\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(elem_klass, dst_temp, empty_value);\n+\n+  \/\/ calc source into \"array_klass\" and free up some regs\n+  const Register src = array_klass;\n+  push(index); \/\/ preserve index reg in case alloc_failed\n+  data_for_value_array_index(array, array_klass, index, src);\n+\n+  allocate_instance(elem_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+  \/\/ Have an oop instance buffer, copy into it\n+  store_ptr(0, obj); \/\/ preserve obj (overwrite index, no longer needed)\n+  data_for_oop(obj, dst_temp, elem_klass);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, elem_klass);\n+  pop(obj);\n+  jmp(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(elem_klass, dst_temp, obj);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(index);\n+  if (array == c_rarg2) {\n+    mov(elem_klass, array);\n+    array = elem_klass;\n+  }\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_load), array, index);\n+\n+  bind(done);\n+}\n+\n@@ -1231,0 +1381,4 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+    }\n@@ -1577,1 +1731,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1589,1 +1743,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()): in_bytes(BranchData::branch_data_size()));\n@@ -1964,0 +2118,78 @@\n+void InterpreterMacroAssembler::profile_array(Register mdp,\n+                                              Register array,\n+                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flattened_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element(Register mdp,\n+                                                Register element,\n+                                                Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadStoreData::array_load_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":244,"deletions":12,"binary":false,"changes":256,"status":"modified"},{"patch":"@@ -193,1 +193,1 @@\n-  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype );\n+  void gen_subtype_check(Register sub_klass, Label &ok_is_subtype, bool profile = true);\n@@ -233,0 +233,23 @@\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+  \/\/ Allocate instance in \"obj\" and read in the content of the inline field\n+  \/\/ NOTES:\n+  \/\/   - input holder object via \"obj\", which must be rax,\n+  \/\/     will return new instance via the same reg\n+  \/\/   - assumes holder_klass and valueKlass field klass have both been resolved\n+  \/\/   - 32 bits: kills rdi and rsi\n+  void read_inlined_field(Register holder_klass,\n+                            Register field_index, Register field_offset,\n+                            Register obj = rax);\n+\n+  \/\/ Allocate value buffer in \"obj\" and read in flattened element at the given index\n+  \/\/ NOTES:\n+  \/\/   - Return via \"obj\" must be rax\n+  \/\/   - kills all given regs\n+  \/\/   - 32 bits: kills rdi and rsi\n+  void read_flattened_element(Register array, Register index,\n+                              Register t1, Register t2,\n+                              Register obj = rax);\n+\n@@ -271,1 +294,1 @@\n-  void profile_not_taken_branch(Register mdp);\n+  void profile_not_taken_branch(Register mdp, bool acmp = false);\n@@ -284,0 +307,3 @@\n+  void profile_array(Register mdp, Register array, Register tmp);\n+  void profile_element(Register mdp, Register element, Register tmp);\n+  void profile_acmp(Register mdp, Register left, Register right, Register tmp);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.hpp","additions":28,"deletions":2,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/jfieldIDWorkaround.hpp\"\n@@ -84,1 +85,1 @@\n-  __ shrptr(roffset, 2);                         \/\/ offset\n+  __ shrptr(roffset, jfieldIDWorkaround::offset_shift);                         \/\/ offset\n@@ -186,1 +187,1 @@\n-  __ shrptr(roffset, 2);                         \/\/ offset\n+  __ shrptr(roffset, jfieldIDWorkaround::offset_shift);                         \/\/ offset\n","filename":"src\/hotspot\/cpu\/x86\/jniFastGetField_x86_64.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -54,0 +55,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -56,0 +58,4 @@\n+#include \"vmreg_x86.inline.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1691,0 +1697,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2870,0 +2880,140 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  testl(temp_reg, JVM_ACC_VALUE);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  testptr(object, object);\n+  jcc(Assembler::zero, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  movl(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  testl(temp_reg, InstanceKlassFlags::is_empty_inline_type_value());\n+  jcc(Assembler::notZero, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_null_free_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_null_free_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::zero, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inlined_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inlined);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,\n+                                              Label&is_flattened_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flattened_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flattened_array_layout(temp_reg, is_flattened_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flattened_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flattened_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flattened_array_layout(temp_reg, is_non_flattened_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_null_free_array_layout(temp_reg, is_null_free_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_null_free_array_layout(temp_reg, is_non_null_free_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_array_bit_inplace);\n+  jcc(Assembler::notZero, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_array_bit_inplace);\n+  jcc(Assembler::zero, is_non_null_free_array);\n+}\n+\n+\n@@ -3974,0 +4124,114 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  push(klass);\n+  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(klass);\n+#ifndef _LP64\n+  if (UseTLAB) {\n+    get_thread(thread);\n+  }\n+#endif \/\/ _LP64\n+\n+  if (UseTLAB) {\n+    tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    jmp(slow_case);\n+  }\n+\n+  \/\/ If UseTLAB is true, the object is created above and there is an initialize need.\n+  \/\/ Otherwise, skip and go to the slow path.\n+  if (UseTLAB) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      decrement(layout_size, sizeof(oopDesc));\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 1*oopSize), zero);\n+        NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 2*oopSize), zero));\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+#ifdef _LP64\n+    xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+#endif\n+    movptr(t2, klass);         \/\/ preserve klass\n+    store_klass(new_obj, t2, rscratch1);  \/\/ src klass reg is potentially compressed\n+\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -4226,0 +4490,50 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  movptr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(inline_klass, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  movptr(inline_klass, Address(inline_klass, index, Address::times_ptr));\n+}\n+\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  movptr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  movl(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  movptr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset, Address::times_1);\n+  load_heap_oop(obj, field);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n+\n@@ -4574,1 +4888,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4636,1 +4954,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -5123,0 +5445,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5132,1 +5462,6 @@\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+}\n+\n+void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {\n+  load_klass(dst, src, tmp);\n+  movptr(dst, Address(dst, Klass::prototype_header_offset()));\n@@ -5171,0 +5506,40 @@\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_PRIMITIVE_OBJECT)));\n+}\n+\n@@ -5510,1 +5885,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, KRegister mask) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, KRegister mask) {\n@@ -5516,1 +5891,1 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_512bit);\n+    evpbroadcastq(xtmp, val, AVX_512bit);\n@@ -5518,1 +5893,3 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -5520,1 +5897,2 @@\n-    pxor(xtmp, xtmp);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n@@ -5543,1 +5921,1 @@\n-    fill64_masked(3, base, 0, xtmp, mask, cnt, rtmp, true);\n+    fill64_masked(3, base, 0, xtmp, mask, cnt, val, true);\n@@ -5562,1 +5940,1 @@\n-    fill32_masked(3, base, 0, xtmp, mask, cnt, rtmp);\n+    fill32_masked(3, base, 0, xtmp, mask, cnt, val);\n@@ -5575,0 +5953,398 @@\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != NULL, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(rscratch1, rax); \/\/ save rax for slow_case since *_allocate may corrupt it when allocation failed\n+  if (vk != NULL) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+    jint obj_size = vk->layout_helper();\n+    assert(obj_size != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB) {\n+      tlab_allocate(r15_thread, rax, noreg, obj_size, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+    mov(rbx, rax);\n+    andptr(rbx, -2);\n+    movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+    if (UseTLAB) {\n+      tlab_allocate(r15_thread, rax, r14, 0, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = rax;\n+    movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+    xorl(r13, r13);\n+    store_klass_gap(buffer_obj, r13);\n+    if (vk == NULL) {\n+      \/\/ store_klass corrupts rbx(klass), so save it in r13 for later use (interpreter case only).\n+      mov(r13, rbx);\n+    }\n+    store_klass(buffer_obj, rbx, rscratch1);\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != NULL) {\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(r13, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(rax, rscratch1);\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  \/\/ Two additional slots to account for return address\n+  int sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  \/\/ Save the return address, adjust the stack (make sure it is properly\n+  \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+  \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n+  assert(sp_inc > 0, \"sanity\");\n+  pop(r13);\n+  subptr(rsp, sp_inc);\n+  push(r13);\n+  return sp_inc;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(tmp1, Address(rsp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        testptr(fromReg, fromReg);\n+        jcc(Assembler::zero, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), 1);\n+      } else {\n+        movq(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      jmp(L_notNull);\n+      bind(L_null);\n+      \/\/ Set IsInit field to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+            movq(Address(rsp, st_off), 0);\n+          } else {\n+            xorq(toReg->as_Register(), toReg->as_Register());\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_PRIMITIVE_OBJECT, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14;\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_PRIMITIVE_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        testb(Address(rsp, ld_off), 1);\n+      } else {\n+        testb(fromReg->as_Register(), 1);\n+      }\n+      jcc(Assembler::notZero, L_notNull);\n+      movptr(val_obj, 0);\n+      jmp(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    movq(rbp, Address(rsp, initial_framesize));\n+    \/\/ The stack increment resides just below the saved rbp\n+    addq(rsp, Address(rsp, initial_framesize - wordSize));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n@@ -5664,2 +6440,2 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp,\n-                               bool is_large, KRegister mask) {\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp,\n+                               bool is_large, bool word_copy_only, KRegister mask) {\n@@ -5670,1 +6446,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"val register must be eax for rep stos\");\n@@ -5676,3 +6452,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n@@ -5692,1 +6465,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -5701,1 +6474,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -5705,1 +6478,1 @@\n-    xmm_clear_mem(base, cnt, tmp, xtmp, mask);\n+    xmm_clear_mem(base, cnt, val, xtmp, mask);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":791,"deletions":18,"binary":false,"changes":809,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -36,0 +37,2 @@\n+class ciInlineKlass;\n+\n@@ -105,0 +108,31 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type);\n+  void test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type);\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type);\n+\n+  \/\/ Get the default value oop for the given InlineKlass\n+  void get_default_value_oop(Register inline_klass, Register temp_reg, Register obj);\n+  \/\/ The empty value oop, for the given InlineKlass (\"empty\" as in no instance fields)\n+  \/\/ get_default_value_oop with extra assertion for empty inline klass\n+  void get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj);\n+\n+  void test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free);\n+  void test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free);\n+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined);\n+\n+  \/\/ Check oops for special arrays, i.e. flattened and\/or null-free\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flattened_array_oop(Register oop, Register temp_reg, Label&is_flattened_array);\n+  void test_non_flattened_array_oop(Register oop, Register temp_reg, Label&is_non_flattened_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flatten or null-free arrays...\n+  void test_flattened_array_layout(Register lh, Label& is_flattened_array);\n+  void test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array);\n+  void test_null_free_array_layout(Register lh, Label& is_null_free_array);\n+  void test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array);\n+\n@@ -351,0 +385,1 @@\n+  void load_metadata(Register dst, Register src);\n@@ -359,0 +394,10 @@\n+  void access_value_copy(DecoratorSet decorators, Register src, Register dst, Register inline_klass);\n+\n+  \/\/ inline type data payload offsets...\n+  void first_field_offset(Register inline_klass, Register offset);\n+  void data_for_oop(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n+\n@@ -370,0 +415,2 @@\n+  void load_prototype_header(Register dst, Register src, Register tmp);\n+\n@@ -571,0 +618,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -582,0 +638,3 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n@@ -732,1 +791,2 @@\n-  void andptr(Register src1, Register src2) { LP64_ONLY(andq(src1, src2)) NOT_LP64(andl(src1, src2)) ; }\n+  void andptr(Register dst, Register src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n+  void andptr(Register dst, Address src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n@@ -1829,0 +1889,15 @@\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[], Register val_array);\n+  int extend_stack_for_inline_args(int args_on_stack);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair);\n+  VMReg spill_reg_for(VMReg reg);\n+\n@@ -1831,1 +1906,1 @@\n-  void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large, KRegister mask=knoreg);\n+  void clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only, KRegister mask=knoreg);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":77,"deletions":2,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -485,0 +485,1 @@\n+    case T_PRIMITIVE_OBJECT:\n@@ -535,0 +536,9 @@\n+const uint SharedRuntime::java_return_convention_max_int = 1;\n+const uint SharedRuntime::java_return_convention_max_float = 1;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  Unimplemented();\n+  return 0;\n+}\n+\n@@ -596,3 +606,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>& sig_extended,\n@@ -600,1 +608,5 @@\n-                            Label& skip_fixup) {\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet*& oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words) {\n@@ -622,1 +634,1 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+  int extraspace = sig_extended.length() * Interpreter::stackElementSize;\n@@ -633,3 +645,3 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+  for (int i = 0; i < sig_extended.length(); i++) {\n+    if (sig_extended.at(i)._bt == T_VOID) {\n+      assert(i > 0 && (sig_extended.at(i-1)._bt == T_LONG || sig_extended.at(i-1)._bt == T_DOUBLE), \"missing half\");\n@@ -640,1 +652,1 @@\n-    int st_off = ((total_args_passed - 1) - i) * Interpreter::stackElementSize;\n+    int st_off = ((sig_extended.length() - 1) - i) * Interpreter::stackElementSize;\n@@ -686,1 +698,1 @@\n-        assert(sig_bt[i] == T_DOUBLE || sig_bt[i] == T_LONG, \"wrong type\");\n+        assert(sig_extended.at(i)._bt == T_DOUBLE || sig_extended.at(i)._bt == T_LONG, \"wrong type\");\n@@ -719,2 +731,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>& sig_extended,\n@@ -723,0 +734,1 @@\n+\n@@ -811,2 +823,2 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n+  for (int i = 0; i < sig_extended.length(); i++) {\n+    if (sig_extended.at(i)._bt == T_VOID) {\n@@ -815,1 +827,1 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      assert(i > 0 && (sig_extended.at(i-1)._bt == T_LONG || sig_extended.at(i-1)._bt == T_DOUBLE), \"missing half\");\n@@ -824,1 +836,1 @@\n-    int ld_off = (total_args_passed - i) * Interpreter::stackElementSize;\n+    int ld_off = (sig_extended.length() - i) * Interpreter::stackElementSize;\n@@ -922,2 +934,1 @@\n-                                                            int total_args_passed,\n-                                                            const BasicType *sig_bt,\n+                                                            const GrowableArray<SigEntry>& sig_extended,\n@@ -926,1 +937,2 @@\n-                                                            AdapterFingerPrint* fingerprint) {\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n@@ -929,1 +941,1 @@\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig_extended, regs);\n@@ -969,1 +981,4 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  OopMapSet* oop_maps = NULL;\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+  gen_c2i_adapter(masm, sig_extended, regs, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words);\n@@ -972,0 +987,1 @@\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps);\n@@ -995,0 +1011,1 @@\n+    case T_PRIMITIVE_OBJECT:\n@@ -1578,0 +1595,1 @@\n+      case T_PRIMITIVE_OBJECT:\n@@ -1752,0 +1770,1 @@\n+  case T_PRIMITIVE_OBJECT:           \/\/ Really a handle\n@@ -2839,0 +2858,5 @@\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  Unimplemented();\n+  return NULL;\n+}\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":45,"deletions":21,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -527,0 +528,1 @@\n+    case T_PRIMITIVE_OBJECT:\n@@ -560,0 +562,82 @@\n+\/\/ Same as java_calling_convention() but for multiple return\n+\/\/ values. There's no way to store them on the stack so if we don't\n+\/\/ have enough registers, multiple values can't be returned.\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    rax, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+  static const XMMRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3,\n+    j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_PRIMITIVE_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+    case T_METADATA:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -602,0 +686,106 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+    for (int i = 0; i < sig_extended->length(); i++) {\n+      BasicType bt = sig_extended->at(i)._bt;\n+      if (bt == T_PRIMITIVE_OBJECT) {\n+        \/\/ In sig_extended, an inline type argument starts with:\n+        \/\/ T_PRIMITIVE_OBJECT, followed by the types of the fields of the\n+        \/\/ inline type and T_VOID to mark the end of the value\n+        \/\/ type. Inline types are flattened so, for instance, in the\n+        \/\/ case of an inline type with an int field and an inline type\n+        \/\/ field that itself has 2 fields, an int and a long:\n+        \/\/ T_PRIMITIVE_OBJECT T_INT T_PRIMITIVE_OBJECT T_INT T_LONG T_VOID (second\n+        \/\/ slot for the T_LONG) T_VOID (inner T_PRIMITIVE_OBJECT) T_VOID\n+        \/\/ (outer T_PRIMITIVE_OBJECT)\n+        total_args_passed++;\n+        int vt = 1;\n+        do {\n+          i++;\n+          BasicType bt = sig_extended->at(i)._bt;\n+          BasicType prev_bt = sig_extended->at(i-1)._bt;\n+          if (bt == T_PRIMITIVE_OBJECT) {\n+            vt++;\n+          } else if (bt == T_VOID &&\n+                     prev_bt != T_LONG &&\n+                     prev_bt != T_DOUBLE) {\n+            vt--;\n+          }\n+        } while (vt != 0);\n+      } else {\n+        total_args_passed++;\n+      }\n+    }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  assert(bt != T_PRIMITIVE_OBJECT || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"must be invalid\");\n+    return;\n+  }\n+\n+  if (!r_1->is_XMMRegister()) {\n+    Register val = rax;\n+    if (r_1->is_stack()) {\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, rscratch1);\n+    if (is_oop) {\n+      __ push(r13);\n+      __ push(rbx);\n+      __ store_heap_oop(to, val, rscratch1, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      __ pop(rbx);\n+      __ pop(r13);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    }\n+  }\n+}\n@@ -604,3 +794,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -608,1 +796,32 @@\n-                            Label& skip_fixup) {\n+                            bool requires_clinit_barrier,\n+                            address& c2i_no_clinit_check_entry,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+  if (requires_clinit_barrier && VM_Version::supports_fast_class_init_checks()) {\n+    Label L_skip_barrier;\n+    Register method = rbx;\n+\n+    { \/\/ Bypass the barrier for non-static methods\n+      Register flags = rscratch1;\n+      __ movl(flags, Address(method, Method::access_flags_offset()));\n+      __ testl(flags, JVM_ACC_STATIC);\n+      __ jcc(Assembler::zero, L_skip_barrier); \/\/ non-static\n+    }\n+\n+    Register klass = rscratch1;\n+    __ load_method_holder(klass, method);\n+    __ clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    __ bind(L_skip_barrier);\n+    c2i_no_clinit_check_entry = __ pc();\n+  }\n+\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->c2i_entry_barrier(masm);\n+\n@@ -618,0 +837,42 @@\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_PRIMITIVE_OBJECT);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types.\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ false);\n+\n+      frame_complete = __ offset();\n+\n+      __ set_last_Java_frame(noreg, noreg, NULL, rscratch1);\n+\n+      __ mov(c_rarg0, r15_thread);\n+      __ mov(c_rarg1, rbx);\n+      __ mov64(c_rarg2, (int64_t)alloc_inline_receiver);\n+      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+      __ jcc(Assembler::equal, no_exception);\n+\n+      __ movptr(Address(r15_thread, JavaThread::vm_result_offset()), NULL_WORD);\n+      __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+      __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(rscratch2, r15_thread); \/\/ Use rscratch2 (r11) as temporary because rscratch1 (r10) is trashed by movptr()\n+      __ get_vm_result_2(rbx, r15_thread); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n@@ -620,1 +881,1 @@\n-\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n@@ -655,46 +916,24 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n-    }\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rax\n-      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ movl(rax, Address(rsp, ld_off));\n-        __ movptr(Address(rsp, st_off), rax);\n-\n-      } else {\n-\n-        __ movq(rax, Address(rsp, ld_off));\n-\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ movq(Address(rsp, next_off), rax);\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_PRIMITIVE_OBJECT,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_PRIMITIVE_OBJECT\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_PRIMITIVE_OBJECT) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(rsp, offset), extraspace, false);\n+      next_arg_int++;\n@@ -703,7 +942,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n-          __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ movq(Address(rsp, st_off), rax);\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n+        __ movptr(Address(rsp, st_off), rax);\n@@ -711,16 +947,26 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ movl(Address(rsp, st_off), r);\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ long\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaab));\n-          __ movptr(Address(rsp, st_off), rax);\n-          __ movq(Address(rsp, next_off), r);\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_PRIMITIVE_OBJECT);\n+      __ load_heap_oop(r14, Address(rscratch2, index));\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      Label L_null;\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp-1)._bt;\n+        if (bt == T_PRIMITIVE_OBJECT) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID &&\n+                   prev_bt != T_LONG &&\n+                   prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -729,1 +975,22 @@\n-          __ movptr(Address(rsp, st_off), r);\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          if (off == -1) {\n+            \/\/ Nullable inline type argument, emit null check\n+            VMReg reg = regs[next_arg_comp-ignored].first();\n+            Label L_notNull;\n+            if (reg->is_stack()) {\n+              int ld_off = reg->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+              __ testb(Address(rsp, ld_off), 1);\n+            } else {\n+              __ testb(reg->as_Register(), 1);\n+            }\n+            __ jcc(Assembler::notZero, L_notNull);\n+            __ movptr(Address(rsp, st_off), 0);\n+            __ jmp(L_null);\n+            __ bind(L_notNull);\n+            continue;\n+          }\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(r14, off), extraspace, is_oop);\n@@ -731,14 +998,4 @@\n-      }\n-    } else {\n-      assert(r_1->is_XMMRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ movflt(Address(rsp, st_off), r_1->as_XMMRegister());\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov64(rax, CONST64(0xdeadffffdeadaaac));\n-        __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        __ movdbl(Address(rsp, next_off), r_1->as_XMMRegister());\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ movptr(Address(rsp, st_off), r14);\n+      __ bind(L_null);\n@@ -767,2 +1024,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>* sig,\n@@ -854,1 +1110,1 @@\n-  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_offset())));\n+  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_inline_offset())));\n@@ -868,0 +1124,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -871,1 +1129,3 @@\n-    if (sig_bt[i] == T_VOID) {\n+    BasicType bt = sig->at(i)._bt;\n+    assert(bt != T_PRIMITIVE_OBJECT, \"i2c adapter doesn't unpack inline type args\");\n+    if (bt == T_VOID) {\n@@ -874,1 +1134,2 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      BasicType prev_bt = (i > 0) ? sig->at(i-1)._bt : T_ILLEGAL;\n+      assert(i > 0 && (prev_bt == T_LONG || prev_bt == T_DOUBLE), \"missing half\");\n@@ -916,1 +1177,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -931,1 +1192,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -964,1 +1225,1 @@\n-  \/\/ only needed because eof c2 resolve stubs return Method* as a result in\n+  \/\/ only needed because of c2 resolve stubs return Method* as a result in\n@@ -970,0 +1231,22 @@\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Label ok;\n+\n+  Register holder = rax;\n+  Register receiver = j_rarg0;\n+  Register temp = rbx;\n+\n+  __ load_klass(temp, receiver, rscratch1);\n+  __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n+  __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n+  __ jcc(Assembler::equal, ok);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+\n+  __ bind(ok);\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), NULL_WORD);\n+  __ jcc(Assembler::equal, skip_fixup);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n+\n@@ -971,2 +1254,1 @@\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                                            int total_args_passed,\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler* masm,\n@@ -974,3 +1256,9 @@\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter,\n+                                                            bool allocate_code_blob) {\n@@ -978,2 +1266,1 @@\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -990,1 +1277,2 @@\n-  address c2i_unverified_entry = __ pc();\n+  address c2i_unverified_entry        = __ pc();\n+  address c2i_unverified_inline_entry = __ pc();\n@@ -992,11 +1280,1 @@\n-  Label ok;\n-\n-  Register holder = rax;\n-  Register receiver = j_rarg0;\n-  Register temp = rbx;\n-  {\n-    __ load_klass(temp, receiver, rscratch1);\n-    __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ jcc(Assembler::equal, ok);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -1005,10 +1283,3 @@\n-    __ bind(ok);\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), NULL_WORD);\n-    __ jcc(Assembler::equal, skip_fixup);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  }\n-\n-  address c2i_entry = __ pc();\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n@@ -1016,1 +1287,1 @@\n-  \/\/ Class initialization barrier for static methods\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n@@ -1018,16 +1289,7 @@\n-  if (VM_Version::supports_fast_class_init_checks()) {\n-    Label L_skip_barrier;\n-    Register method = rbx;\n-\n-    { \/\/ Bypass the barrier for non-static methods\n-      Register flags = rscratch1;\n-      __ movl(flags, Address(method, Method::access_flags_offset()));\n-      __ testl(flags, JVM_ACC_STATIC);\n-      __ jcc(Assembler::zero, L_skip_barrier); \/\/ non-static\n-    }\n-\n-    Register klass = rscratch1;\n-    __ load_method_holder(klass, method);\n-    __ clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    \/\/ No class init barrier needed because method is guaranteed to be non-static\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, \/* requires_clinit_barrier = *\/ false, c2i_no_clinit_check_entry,\n+                    skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+    skip_fixup.reset();\n+  }\n@@ -1035,2 +1297,15 @@\n-    __ bind(L_skip_barrier);\n-    c2i_no_clinit_check_entry = __ pc();\n+  \/\/ Scalarized c2i adapter\n+  address c2i_entry        = __ pc();\n+  address c2i_inline_entry = __ pc();\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, \/* requires_clinit_barrier = *\/ true, c2i_no_clinit_check_entry,\n+                  skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ true);\n+\n+  \/\/ Non-scalarized c2i adapter\n+  if (regs != regs_cc) {\n+    c2i_unverified_inline_entry = __ pc();\n+    Label inline_entry_skip_fixup;\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, \/* requires_clinit_barrier = *\/ true, c2i_no_clinit_check_entry,\n+                    inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n@@ -1039,2 +1314,1 @@\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->c2i_entry_barrier(masm);\n+  __ flush();\n@@ -1042,1 +1316,6 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  \/\/ The c2i adapters might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  if (allocate_code_blob) {\n+    bool caller_must_gc_arguments = (regs != regs_cc);\n+    new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n+  }\n@@ -1044,2 +1323,1 @@\n-  __ flush();\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -1103,0 +1381,1 @@\n+      case T_PRIMITIVE_OBJECT:\n@@ -2029,0 +2308,1 @@\n+      case T_PRIMITIVE_OBJECT:\n@@ -2154,0 +2434,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        __ andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -2215,0 +2499,1 @@\n+  case T_PRIMITIVE_OBJECT:           \/\/ Really a handle\n@@ -3706,0 +3991,110 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  __ movptr(rax, Address(r13, 0));\n+  __ resolve_jobject(rax \/* value *\/,\n+                     r15_thread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ movptr(Address(r13, 0), rax);\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_PRIMITIVE_OBJECT) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r14, r13, rbx, rscratch1);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r14, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(0);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  Label skip;\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::zero, skip);\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_PRIMITIVE_OBJECT) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(rax, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(rax, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ bind(skip);\n+  __ ret(0);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":545,"deletions":150,"binary":false,"changes":695,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"asm\/assembler.hpp\"\n@@ -39,0 +40,2 @@\n+#include \"utilities\/macros.hpp\"\n+#include \"vmreg_x86.inline.hpp\"\n@@ -320,5 +323,9 @@\n-  \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n-  __ movptr(c_rarg0, result);\n-  Label is_long, is_float, is_double, exit;\n-  __ movl(c_rarg1, result_type);\n-  __ cmpl(c_rarg1, T_OBJECT);\n+  \/\/ T_OBJECT, T_PRIMITIVE_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+  __ movptr(r13, result);\n+  Label is_long, is_float, is_double, check_prim, exit;\n+  __ movl(rbx, result_type);\n+  __ cmpl(rbx, T_OBJECT);\n+  __ jcc(Assembler::equal, check_prim);\n+  __ cmpl(rbx, T_PRIMITIVE_OBJECT);\n+  __ jcc(Assembler::equal, check_prim);\n+  __ cmpl(rbx, T_LONG);\n@@ -326,3 +333,1 @@\n-  __ cmpl(c_rarg1, T_LONG);\n-  __ jcc(Assembler::equal, is_long);\n-  __ cmpl(c_rarg1, T_FLOAT);\n+  __ cmpl(rbx, T_FLOAT);\n@@ -330,1 +335,1 @@\n-  __ cmpl(c_rarg1, T_DOUBLE);\n+  __ cmpl(rbx, T_DOUBLE);\n@@ -334,1 +339,1 @@\n-  __ movl(Address(c_rarg0, 0), rax);\n+  __ movl(Address(r13, 0), rax);\n@@ -398,0 +403,13 @@\n+  __ BIND(check_prim);\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check for scalarized return value\n+    __ testptr(rax, 1);\n+    __ jcc(Assembler::zero, is_long);\n+    \/\/ Load pack handler address\n+    __ andptr(rax, -2);\n+    __ movptr(rax, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    __ movptr(rbx, Address(rax, InlineKlass::pack_handler_jobject_offset()));\n+    \/\/ Call pack handler to initialize the buffer\n+    __ call(rbx);\n+    __ jmp(exit);\n+  }\n@@ -399,1 +417,1 @@\n-  __ movq(Address(c_rarg0, 0), rax);\n+  __ movq(Address(r13, 0), rax);\n@@ -403,1 +421,1 @@\n-  __ movflt(Address(c_rarg0, 0), xmm0);\n+  __ movflt(Address(r13, 0), xmm0);\n@@ -407,1 +425,1 @@\n-  __ movdbl(Address(c_rarg0, 0), xmm0);\n+  __ movdbl(Address(r13, 0), xmm0);\n@@ -3821,0 +3839,10 @@\n+  \/\/ Generate these first because they are called from other stubs\n+  if (InlineTypeReturnedAsFields) {\n+    StubRoutines::_load_inline_type_fields_in_regs =\n+      generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs),\n+                                 \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf =\n+      generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf),\n+                                 \"store_inline_type_fields_to_buf\", true);\n+  }\n+\n@@ -3880,0 +3908,144 @@\n+\/\/ Call here from the interpreter or compiled code to either load\n+\/\/ multiple returned values from the inline type instance being\n+\/\/ returned to registers or to store returned values to a newly\n+\/\/ allocated inline type instance.\n+\/\/ Register is a class, but it would be assigned numerical value.\n+\/\/ \"0\" is assigned for xmm0. Thus we need to ignore -Wnonnull.\n+PRAGMA_DIAG_PUSH\n+PRAGMA_NONNULL_IGNORED\n+address StubGenerator::generate_return_value_stub(address destination, const char* name, bool has_res) {\n+  \/\/ We need to save all registers the calling convention may use so\n+  \/\/ the runtime calls read or update those registers. This needs to\n+  \/\/ be in sync with SharedRuntime::java_return_convention().\n+  enum layout {\n+    pad_off = frame::arg_reg_save_area_bytes\/BytesPerInt, pad_off_2,\n+    rax_off, rax_off_2,\n+    j_rarg5_off, j_rarg5_2,\n+    j_rarg4_off, j_rarg4_2,\n+    j_rarg3_off, j_rarg3_2,\n+    j_rarg2_off, j_rarg2_2,\n+    j_rarg1_off, j_rarg1_2,\n+    j_rarg0_off, j_rarg0_2,\n+    j_farg0_off, j_farg0_2,\n+    j_farg1_off, j_farg1_2,\n+    j_farg2_off, j_farg2_2,\n+    j_farg3_off, j_farg3_2,\n+    j_farg4_off, j_farg4_2,\n+    j_farg5_off, j_farg5_2,\n+    j_farg6_off, j_farg6_2,\n+    j_farg7_off, j_farg7_2,\n+    rbp_off, rbp_off_2,\n+    return_off, return_off_2,\n+\n+    framesize\n+  };\n+\n+  CodeBuffer buffer(name, 1000, 512);\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+\n+  int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+  assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+  int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+  int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+  OopMapSet *oop_maps = new OopMapSet();\n+  OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+  map->set_callee_saved(VMRegImpl::stack2reg(rax_off), rax->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+  int start = __ offset();\n+\n+  __ subptr(rsp, frame_size_in_bytes - 8 \/* return address*\/);\n+\n+  __ movptr(Address(rsp, rbp_off * BytesPerInt), rbp);\n+  __ movdbl(Address(rsp, j_farg7_off * BytesPerInt), j_farg7);\n+  __ movdbl(Address(rsp, j_farg6_off * BytesPerInt), j_farg6);\n+  __ movdbl(Address(rsp, j_farg5_off * BytesPerInt), j_farg5);\n+  __ movdbl(Address(rsp, j_farg4_off * BytesPerInt), j_farg4);\n+  __ movdbl(Address(rsp, j_farg3_off * BytesPerInt), j_farg3);\n+  __ movdbl(Address(rsp, j_farg2_off * BytesPerInt), j_farg2);\n+  __ movdbl(Address(rsp, j_farg1_off * BytesPerInt), j_farg1);\n+  __ movdbl(Address(rsp, j_farg0_off * BytesPerInt), j_farg0);\n+\n+  __ movptr(Address(rsp, j_rarg0_off * BytesPerInt), j_rarg0);\n+  __ movptr(Address(rsp, j_rarg1_off * BytesPerInt), j_rarg1);\n+  __ movptr(Address(rsp, j_rarg2_off * BytesPerInt), j_rarg2);\n+  __ movptr(Address(rsp, j_rarg3_off * BytesPerInt), j_rarg3);\n+  __ movptr(Address(rsp, j_rarg4_off * BytesPerInt), j_rarg4);\n+  __ movptr(Address(rsp, j_rarg5_off * BytesPerInt), j_rarg5);\n+  __ movptr(Address(rsp, rax_off * BytesPerInt), rax);\n+\n+  int frame_complete = __ offset();\n+\n+  __ set_last_Java_frame(noreg, noreg, NULL, rscratch1);\n+\n+  __ mov(c_rarg0, r15_thread);\n+  __ mov(c_rarg1, rax);\n+\n+  __ call(RuntimeAddress(destination));\n+\n+  \/\/ Set an oopmap for the call site.\n+\n+  oop_maps->add_gc_map( __ offset() - start, map);\n+\n+  \/\/ clear last_Java_sp\n+  __ reset_last_Java_frame(false);\n+\n+  __ movptr(rbp, Address(rsp, rbp_off * BytesPerInt));\n+  __ movdbl(j_farg7, Address(rsp, j_farg7_off * BytesPerInt));\n+  __ movdbl(j_farg6, Address(rsp, j_farg6_off * BytesPerInt));\n+  __ movdbl(j_farg5, Address(rsp, j_farg5_off * BytesPerInt));\n+  __ movdbl(j_farg4, Address(rsp, j_farg4_off * BytesPerInt));\n+  __ movdbl(j_farg3, Address(rsp, j_farg3_off * BytesPerInt));\n+  __ movdbl(j_farg2, Address(rsp, j_farg2_off * BytesPerInt));\n+  __ movdbl(j_farg1, Address(rsp, j_farg1_off * BytesPerInt));\n+  __ movdbl(j_farg0, Address(rsp, j_farg0_off * BytesPerInt));\n+\n+  __ movptr(j_rarg0, Address(rsp, j_rarg0_off * BytesPerInt));\n+  __ movptr(j_rarg1, Address(rsp, j_rarg1_off * BytesPerInt));\n+  __ movptr(j_rarg2, Address(rsp, j_rarg2_off * BytesPerInt));\n+  __ movptr(j_rarg3, Address(rsp, j_rarg3_off * BytesPerInt));\n+  __ movptr(j_rarg4, Address(rsp, j_rarg4_off * BytesPerInt));\n+  __ movptr(j_rarg5, Address(rsp, j_rarg5_off * BytesPerInt));\n+  __ movptr(rax, Address(rsp, rax_off * BytesPerInt));\n+\n+  __ addptr(rsp, frame_size_in_bytes-8);\n+\n+  \/\/ check for pending exceptions\n+  Label pending;\n+  __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+  __ jcc(Assembler::notEqual, pending);\n+\n+  if (has_res) {\n+    __ get_vm_result(rax, r15_thread);\n+  }\n+\n+  __ ret(0);\n+\n+  __ bind(pending);\n+\n+  __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+  \/\/ -------------\n+  \/\/ make sure all code is generated\n+  _masm->flush();\n+\n+  RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_in_words, oop_maps, false);\n+  return stub->entry_point();\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":185,"deletions":13,"binary":false,"changes":198,"status":"modified"},{"patch":"@@ -544,0 +544,3 @@\n+  \/\/ interpreter or compiled code marshalling registers to\/from inline type instance\n+  address generate_return_value_stub(address destination, const char* name, bool has_res);\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -64,1 +65,1 @@\n-int TemplateInterpreter::InterpreterCodeSize = JVMCI_ONLY(268) NOT_JVMCI(256) * 1024;\n+int TemplateInterpreter::InterpreterCodeSize = JVMCI_ONLY(280) NOT_JVMCI(268) * 1024;\n@@ -212,0 +213,4 @@\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(NULL);\n+  }\n+\n@@ -354,0 +359,1 @@\n+  case T_PRIMITIVE_OBJECT: \/\/ fall through (inline types are handled with oops)\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -181,0 +182,1 @@\n+  case Bytecodes::_fast_qputfield:\n@@ -373,0 +375,1 @@\n+  __ andl(rdx, ~JVM_CONSTANT_QDescBit);\n@@ -824,9 +827,27 @@\n-  \/\/ rax: index\n-  \/\/ rdx: array\n-  index_check(rdx, rax); \/\/ kills rbx\n-  do_oop_load(_masm,\n-              Address(rdx, rax,\n-                      UseCompressedOops ? Address::times_4 : Address::times_ptr,\n-                      arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n-              rax,\n-              IS_ARRAY);\n+  Register array = rdx;\n+  Register index = rax;\n+\n+  index_check(array, index); \/\/ kills rbx\n+  __ profile_array(rbx, array, rcx);\n+  if (UseFlatArray) {\n+    Label is_flat_array, done;\n+    __ test_flattened_array_oop(array, rbx, is_flat_array);\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+    __ jmp(done);\n+    __ bind(is_flat_array);\n+    __ read_flattened_element(array, index, rbx, rcx, rax);\n+    __ bind(done);\n+  } else {\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+  }\n+  __ profile_element(rbx, rax, rcx);\n@@ -1118,1 +1139,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1130,0 +1151,4 @@\n+\n+  __ profile_array(rdi, rdx, rbx);\n+  __ profile_element(rdi, rax, rbx);\n+\n@@ -1133,0 +1158,7 @@\n+  \/\/ Move array class to rdi\n+  __ load_klass(rdi, rdx, rscratch1);\n+  if (UseFlatArray) {\n+    __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+    __ test_flattened_array_layout(rbx, is_flat_array);\n+  }\n+\n@@ -1135,3 +1167,2 @@\n-  \/\/ Move superklass into rax\n-  __ load_klass(rax, rdx, rscratch1);\n-  __ movptr(rax, Address(rax,\n+  \/\/ Move array element superklass into rax\n+  __ movptr(rax, Address(rdi,\n@@ -1142,1 +1173,2 @@\n-  __ gen_subtype_check(rbx, ok_is_subtype);\n+  \/\/ is \"rbx <: rax\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(rbx, ok_is_subtype, false);\n@@ -1160,1 +1192,2 @@\n-  __ profile_null_seen(rbx);\n+  if (EnablePrimitiveClasses) {\n+    Label is_null_into_value_array_npe, store_null;\n@@ -1162,0 +1195,9 @@\n+    \/\/ No way to store null in null-free array\n+    __ test_null_free_array_oop(rdx, rbx, is_null_into_value_array_npe);\n+    __ jmp(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ jump(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n@@ -1164,0 +1206,7 @@\n+  __ jmp(done);\n+\n+  if (UseFlatArray) {\n+    Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n+\n+    \/\/ Simplistic type check...\n@@ -1165,0 +1214,27 @@\n+    \/\/ Profile the not-null value's klass.\n+    __ load_klass(rbx, rax, rscratch1);\n+    \/\/ Move element klass into rax\n+    __ movptr(rax, Address(rdi, ArrayKlass::element_klass_offset()));\n+    \/\/ flat value array needs exact type match\n+    \/\/ is \"rax == rbx\" (value subclass == array element superclass)\n+    __ cmpptr(rax, rbx);\n+    __ jccb(Assembler::equal, is_type_ok);\n+\n+    __ jump(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));\n+\n+    __ bind(is_type_ok);\n+    \/\/ rbx: value's klass\n+    \/\/ rdx: array\n+    \/\/ rdi: array klass\n+    __ test_klass_is_empty_inline_type(rbx, rax, done);\n+\n+    \/\/ calc dst for copy\n+    __ movl(rax, at_tos_p1()); \/\/ index\n+    __ data_for_value_array_index(rdx, rdi, rax, rax);\n+\n+    \/\/ ...and src for copy\n+    __ movptr(rcx, at_tos());  \/\/ value\n+    __ data_for_oop(rcx, rcx, rbx);\n+\n+    __ access_value_copy(IN_HEAP, rcx, rax, rbx);\n+  }\n@@ -2333,1 +2409,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -2335,0 +2411,36 @@\n+\n+  __ profile_acmp(rbx, rdx, rax, rcx);\n+\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  if (EnableValhalla) {\n+    __ cmpoop(rdx, rax);\n+    __ jcc(Assembler::equal, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either rax or rdx is null\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+    __ testptr(rdx, rdx);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ movptr(rbx, Address(rdx, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, is_inline_type_mask);\n+    __ cmpptr(rbx, is_inline_type_mask);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(rbx, rdx);\n+    __ load_metadata(rcx, rax);\n+    __ cmpptr(rbx, rcx);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(rax, rdx, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(rax, rdx, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -2337,0 +2449,1 @@\n+  __ bind(taken);\n@@ -2339,1 +2452,10 @@\n-  __ profile_not_taken_branch(rax);\n+  __ profile_not_taken_branch(rax, true);\n+}\n+\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored...rax answer, jmp to outcome...\n+  __ testl(rax, rax);\n+  __ jcc(Assembler::zero, not_subst);\n+  __ jmp(is_subst);\n@@ -2609,1 +2731,2 @@\n-  __ remove_activation(state, rbcp);\n+\n+  __ remove_activation(state, rbcp, true, true, true);\n@@ -2807,0 +2930,1 @@\n+  const Register flags2 = rdx;\n@@ -2812,2 +2936,0 @@\n-  if (!is_static) pop_and_check_object(obj);\n-\n@@ -2816,1 +2938,9 @@\n-  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj;\n+  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj, notInlineType;\n+\n+  if (!is_static) {\n+    __ movptr(rcx, Address(cache, index, Address::times_ptr,\n+                           in_bytes(ConstantPoolCache::base_offset() +\n+                                    ConstantPoolCacheEntry::f1_offset())));\n+  }\n+\n+  __ movl(flags2, flags);\n@@ -2826,0 +2956,1 @@\n+  if (!is_static) pop_and_check_object(obj);\n@@ -2835,0 +2966,1 @@\n+\n@@ -2837,1 +2969,1 @@\n-\n+   if (!is_static) pop_and_check_object(obj);\n@@ -2852,4 +2984,83 @@\n-  do_oop_load(_masm, field, rax);\n-  __ push(atos);\n-  if (!is_static && rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+  if (!EnablePrimitiveClasses) {\n+    if (!is_static) pop_and_check_object(obj);\n+    do_oop_load(_masm, field, rax);\n+    __ push(atos);\n+    if (!is_static && rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+    }\n+    __ jmp(Done);\n+  } else {\n+    if (is_static) {\n+      __ load_heap_oop(rax, field);\n+      Label is_null_free_inline_type, uninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_null_free_inline_type(flags2, rscratch1, is_null_free_inline_type);\n+        \/\/ field is not a null free inline type\n+        __ push(atos);\n+        __ jmp(Done);\n+      \/\/ field is a null free inline type, must not return null even if uninitialized\n+      __ bind(is_null_free_inline_type);\n+          __ testptr(rax, rax);\n+        __ jcc(Assembler::zero, uninitialized);\n+          __ push(atos);\n+          __ jmp(Done);\n+        __ bind(uninitialized);\n+          __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+#ifdef _LP64\n+          Label slow_case, finish;\n+          __ movptr(rbx, Address(obj, java_lang_Class::klass_offset()));\n+          __ cmpb(Address(rbx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+          __ jcc(Assembler::notEqual, slow_case);\n+        __ get_default_value_oop(rbx, rscratch1, rax);\n+        __ jmp(finish);\n+        __ bind(slow_case);\n+#endif \/\/ LP64\n+          __ call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field),\n+                obj, flags2);\n+#ifdef _LP64\n+          __ bind(finish);\n+  #endif \/\/ _LP64\n+        __ verify_oop(rax);\n+        __ push(atos);\n+        __ jmp(Done);\n+    } else {\n+      Label is_inlined, nonnull, is_inline_type, rewrite_inline;\n+      __ test_field_is_null_free_inline_type(flags2, rscratch1, is_inline_type);\n+      \/\/ field is not a null free inline type\n+      pop_and_check_object(obj);\n+      __ load_heap_oop(rax, field);\n+      __ push(atos);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+      }\n+      __ jmp(Done);\n+      __ bind(is_inline_type);\n+        __ test_field_is_inlined(flags2, rscratch1, is_inlined);\n+          \/\/ field is not inlined\n+          __ movptr(rax, rcx);  \/\/ small dance required to preserve the klass_holder somewhere\n+          pop_and_check_object(obj);\n+          __ push(rax);\n+          __ load_heap_oop(rax, field);\n+          __ pop(rcx);\n+          __ testptr(rax, rax);\n+          __ jcc(Assembler::notZero, nonnull);\n+            __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+            __ get_inline_type_field_klass(rcx, flags2, rbx);\n+            __ get_default_value_oop(rbx, rcx, rax);\n+          __ bind(nonnull);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(rewrite_inline);\n+        __ bind(is_inlined);\n+        \/\/ field is inlined\n+          __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+          pop_and_check_object(rax);\n+          __ read_inlined_field(rcx, flags2, rbx, rax);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_qgetfield, bc, rbx);\n+      }\n+        __ jmp(Done);\n+    }\n@@ -2857,1 +3068,0 @@\n-  __ jmp(Done);\n@@ -2860,0 +3070,3 @@\n+\n+  if (!is_static) pop_and_check_object(obj);\n+\n@@ -2959,0 +3172,22 @@\n+void TemplateTable::withfield() {\n+  transition(vtos, atos);\n+\n+  Register cache = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n+  Register index = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n+\n+  resolve_cache_and_index(f2_byte, cache, index, sizeof(u2));\n+\n+  Register cpentry = rbx;\n+\n+  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n+\n+  __ lea(cpentry, Address(cache, index, Address::times_ptr,\n+                         in_bytes(cp_base_offset)));\n+  __ lea(rax, at_tos());\n+  __ call_VM(rbx, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), cpentry, rax);\n+  \/\/ new value type is returned in rbx\n+  \/\/ stack adjustment is returned in rax\n+  __ verify_oop(rbx);\n+  __ addptr(rsp, rax);\n+  __ movptr(rax, rbx);\n+}\n@@ -3054,0 +3289,1 @@\n+  const Register flags2 = rdx;\n@@ -3070,0 +3306,1 @@\n+  __ movl(flags2, flags);\n@@ -3072,1 +3309,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags, flags2);\n@@ -3078,1 +3315,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags, flags2);\n@@ -3084,1 +3321,1 @@\n-                                              Register obj, Register off, Register flags) {\n+                                              Register obj, Register off, Register flags, Register flags2) {\n@@ -3091,1 +3328,1 @@\n-        notLong, notFloat, notObj;\n+        notLong, notFloat, notObj, notInlineType;\n@@ -3134,6 +3371,53 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, rax);\n-    if (!is_static && rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+    if (!EnablePrimitiveClasses) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, rax);\n+      if (!is_static && rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+      }\n+      __ jmp(Done);\n+    } else {\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_inline_type;\n+        __ test_field_is_not_null_free_inline_type(flags2, rscratch1, is_inline_type);\n+        __ null_check(rax);\n+        __ bind(is_inline_type);\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(Done);\n+      } else {\n+        Label is_inline_type, is_inlined, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_null_free_inline_type(flags2, rscratch1, is_inline_type);\n+        \/\/ Not an inline type\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        __ bind(rewrite_not_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(is_inline_type);\n+        __ null_check(rax);\n+        __ test_field_is_inlined(flags2, rscratch1, is_inlined);\n+        \/\/ field is not inlined\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(rewrite_inline);\n+        __ bind(is_inlined);\n+        \/\/ field is inlined\n+        pop_and_check_object(obj);\n+        assert_different_registers(rax, rdx, obj, off);\n+        __ load_klass(rdx, rax, rscratch1);\n+        __ data_for_oop(rax, rax, rdx);\n+        __ addptr(obj, off);\n+        __ access_value_copy(IN_HEAP, rax, obj, rdx);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_qputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+      }\n@@ -3141,1 +3425,0 @@\n-    __ jmp(Done);\n@@ -3280,0 +3563,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -3305,0 +3589,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/ fall through\n@@ -3344,0 +3629,4 @@\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rscratch2, rdx);  \/\/ saving flags for is_inlined test\n+  }\n+\n@@ -3357,1 +3646,4 @@\n-  fast_storefield_helper(field, rax);\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rdx, rscratch2);  \/\/ restoring flags for is_inlined test\n+  }\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3363,1 +3655,4 @@\n-  fast_storefield_helper(field, rax);\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rdx, rscratch2);  \/\/ restoring flags for is_inlined test\n+  }\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3368,1 +3663,1 @@\n-void TemplateTable::fast_storefield_helper(Address field, Register rax) {\n+void TemplateTable::fast_storefield_helper(Address field, Register rax, Register flags) {\n@@ -3372,0 +3667,17 @@\n+  case Bytecodes::_fast_qputfield:\n+    {\n+      Label is_inlined, done;\n+      __ null_check(rax);\n+      __ test_field_is_inlined(flags, rscratch1, is_inlined);\n+      \/\/ field is not inlined\n+      do_oop_store(_masm, field, rax);\n+      __ jmp(done);\n+      __ bind(is_inlined);\n+      \/\/ field is inlined\n+      __ load_klass(rdx, rax, rscratch1);\n+      __ data_for_oop(rax, rax, rdx);\n+      __ lea(rcx, field);\n+      __ access_value_copy(IN_HEAP, rax, rcx, rdx);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3373,1 +3685,3 @@\n-    do_oop_store(_masm, field, rax);\n+    {\n+      do_oop_store(_masm, field, rax);\n+    }\n@@ -3443,1 +3757,1 @@\n-  __ movptr(rbx, Address(rcx, rbx, Address::times_ptr,\n+  __ movptr(rdx, Address(rcx, rbx, Address::times_ptr,\n@@ -3450,1 +3764,1 @@\n-  Address field(rax, rbx, Address::times_1);\n+  Address field(rax, rdx, Address::times_1);\n@@ -3454,0 +3768,39 @@\n+  case Bytecodes::_fast_qgetfield:\n+    {\n+      Label is_inlined, nonnull, Done;\n+      __ movptr(rscratch1, Address(rcx, rbx, Address::times_ptr,\n+                                   in_bytes(ConstantPoolCache::base_offset() +\n+                                            ConstantPoolCacheEntry::flags_offset())));\n+      __ test_field_is_inlined(rscratch1, rscratch2, is_inlined);\n+        \/\/ field is not inlined\n+        __ load_heap_oop(rax, field);\n+        __ testptr(rax, rax);\n+        __ jcc(Assembler::notZero, nonnull);\n+          __ movl(rdx, Address(rcx, rbx, Address::times_ptr,\n+                             in_bytes(ConstantPoolCache::base_offset() +\n+                                      ConstantPoolCacheEntry::flags_offset())));\n+          __ andl(rdx, ConstantPoolCacheEntry::field_index_mask);\n+          __ movptr(rcx, Address(rcx, rbx, Address::times_ptr,\n+                                       in_bytes(ConstantPoolCache::base_offset() +\n+                                                ConstantPoolCacheEntry::f1_offset())));\n+          __ get_inline_type_field_klass(rcx, rdx, rbx);\n+          __ get_default_value_oop(rbx, rcx, rax);\n+        __ bind(nonnull);\n+        __ verify_oop(rax);\n+        __ jmp(Done);\n+      __ bind(is_inlined);\n+      \/\/ field is inlined\n+        __ push(rdx); \/\/ save offset\n+        __ movl(rdx, Address(rcx, rbx, Address::times_ptr,\n+                           in_bytes(ConstantPoolCache::base_offset() +\n+                                    ConstantPoolCacheEntry::flags_offset())));\n+        __ andl(rdx, ConstantPoolCacheEntry::field_index_mask);\n+        __ movptr(rcx, Address(rcx, rbx, Address::times_ptr,\n+                                     in_bytes(ConstantPoolCache::base_offset() +\n+                                              ConstantPoolCacheEntry::f1_offset())));\n+        __ pop(rbx); \/\/ restore offset\n+        __ read_inlined_field(rcx, rdx, rbx, rax);\n+      __ bind(Done);\n+      __ verify_oop(rax);\n+    }\n+    break;\n@@ -3923,2 +4276,1 @@\n-  Label slow_case_no_pop;\n-  Label initialize_header;\n+  Label is_not_value;\n@@ -3934,1 +4286,1 @@\n-  __ jcc(Assembler::notEqual, slow_case_no_pop);\n+  __ jcc(Assembler::notEqual, slow_case);\n@@ -3938,1 +4290,7 @@\n-  __ push(rcx);  \/\/ save the contexts of klass for initializing the header\n+\n+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InlineKlassKind);\n+  __ jcc(Assembler::notEqual, is_not_value);\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_InstantiationError));\n+\n+  __ bind(is_not_value);\n@@ -3941,1 +4299,0 @@\n-  \/\/ make sure klass is fully initialized\n@@ -3945,14 +4302,2 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ movl(rdx, Address(rcx, Klass::layout_helper_offset()));\n-  \/\/ test to see if it has a finalizer or is malformed in some way\n-  __ testl(rdx, Klass::_lh_instance_slow_path_bit);\n-  __ jcc(Assembler::notZero, slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n+  __ allocate_instance(rcx, rax, rdx, rbx, true, slow_case);\n+  __ jmp(done);\n@@ -3960,1 +4305,2 @@\n-  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n+  \/\/ slow case\n+  __ bind(slow_case);\n@@ -3962,7 +4308,2 @@\n-  if (UseTLAB) {\n-    NOT_LP64(__ get_thread(thread);)\n-    __ tlab_allocate(thread, rax, rdx, 0, rcx, rbx, slow_case);\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ jmp(initialize_header);\n-    }\n+  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n+  Register rarg2 = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n@@ -3970,4 +4311,4 @@\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    __ decrement(rdx, sizeof(oopDesc));\n-    __ jcc(Assembler::zero, initialize_header);\n+  __ get_constant_pool(rarg1);\n+  __ get_unsigned_2_byte_index_at_bcp(rarg2, 1);\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);\n+   __ verify_oop(rax);\n@@ -3975,4 +4316,3 @@\n-    \/\/ Initialize topmost object field, divide rdx by 8, check if odd and\n-    \/\/ test if zero.\n-    __ xorl(rcx, rcx);    \/\/ use zero reg to clear memory (shorter code)\n-    __ shrl(rdx, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+  \/\/ continue\n+  __ bind(done);\n+}\n@@ -3980,10 +4320,2 @@\n-    \/\/ rdx must have been multiple of 8\n-#ifdef ASSERT\n-    \/\/ make sure rdx was multiple of 8\n-    Label L;\n-    \/\/ Ignore partial flag stall after shrl() since it is debug VM\n-    __ jcc(Assembler::carryClear, L);\n-    __ stop(\"object size is not multiple of 2 - adjust this code\");\n-    __ bind(L);\n-    \/\/ rdx must be > 0, no extra check needed here\n-#endif\n+void TemplateTable::aconst_init() {\n+  transition(vtos, atos);\n@@ -3991,8 +4323,3 @@\n-    \/\/ initialize remaining object fields: rdx was a multiple of 8\n-    { Label loop;\n-    __ bind(loop);\n-    __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n-    __ decrement(rdx);\n-    __ jcc(Assembler::notZero, loop);\n-    }\n+  Label slow_case;\n+  Label done;\n+  Label is_value;\n@@ -4000,10 +4327,2 @@\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-              (intptr_t)markWord::prototype().value()); \/\/ header\n-    __ pop(rcx);   \/\/ get saved klass back in the register.\n-#ifdef _LP64\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n-#endif\n-    __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n+  __ get_unsigned_2_byte_index_at_bcp(rdx, 1);\n+  __ get_cpool_and_tags(rcx, rax);\n@@ -4011,8 +4330,6 @@\n-    {\n-      SkipIfEqual skip_if(_masm, &DTraceAllocProbes, 0, rscratch1);\n-      \/\/ Trigger dtrace event for fastpath\n-      __ push(atos);\n-      __ call_VM_leaf(\n-           CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), rax);\n-      __ pop(atos);\n-    }\n+  \/\/ Make sure the class we're about to instantiate has been resolved.\n+  \/\/ This is done before loading InstanceKlass to be consistent with the order\n+  \/\/ how Constant Pool is updated (see ConstantPool::klass_at_put)\n+  const int tags_offset = Array<u1>::base_offset_in_bytes();\n+  __ cmpb(Address(rax, rdx, Address::times_1, tags_offset), JVM_CONSTANT_Class);\n+  __ jcc(Assembler::notEqual, slow_case);\n@@ -4020,2 +4337,18 @@\n-    __ jmp(done);\n-  }\n+  \/\/ get InstanceKlass\n+  __ load_resolved_klass_at_index(rcx, rcx, rdx);\n+\n+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InlineKlassKind);\n+  __ jcc(Assembler::equal, is_value);\n+\n+  \/\/ in the future, aconst_init will just return null instead of throwing an exception\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_IncompatibleClassChangeError));\n+\n+  __ bind(is_value);\n+\n+  \/\/ make sure klass is fully initialized\n+  __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+  __ jcc(Assembler::notEqual, slow_case);\n+\n+  \/\/ have a resolved InlineKlass in rcx, return the default value oop from it\n+  __ get_default_value_oop(rcx, rdx, rax);\n+  __ jmp(done);\n@@ -4023,4 +4356,1 @@\n-  \/\/ slow case\n-  __ pop(rcx);   \/\/ restore stack pointer to what it was when we came in.\n-  __ bind(slow_case_no_pop);\n-  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n+  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n@@ -4031,3 +4361,4 @@\n-  __ get_constant_pool(rarg1);\n-  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);\n-   __ verify_oop(rax);\n+  __ get_constant_pool(rarg1);\n+\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::aconst_init),\n+      rarg1, rarg2);\n@@ -4036,1 +4367,1 @@\n-  \/\/ continue\n+  __ verify_oop(rax);\n@@ -4076,4 +4407,5 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+      Address::times_1,\n+      Array<u1>::base_offset_in_bytes()));\n+  __ andl (rdx, ~JVM_CONSTANT_QDescBit);\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4117,0 +4449,3 @@\n+  __ jmp(done);\n+\n+  __ bind(is_null);\n@@ -4120,4 +4455,15 @@\n-    __ jmp(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n+  if (EnablePrimitiveClasses) {\n+    \/\/ Get cpool & tags index\n+    __ get_cpool_and_tags(rcx, rdx); \/\/ rcx=cpool, rdx=tags array\n+    __ get_unsigned_2_byte_index_at_bcp(rbx, 1); \/\/ rbx=index\n+    \/\/ See if CP entry is a Q-descriptor\n+    __ movzbl(rcx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+    __ andl (rcx, JVM_CONSTANT_QDescBit);\n+    __ cmpl(rcx, JVM_CONSTANT_QDescBit);\n+    __ jcc(Assembler::notEqual, done);\n+    __ jump(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+  }\n+\n@@ -4139,4 +4485,5 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+  __ andl (rdx, ~JVM_CONSTANT_QDescBit);\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4194,1 +4541,0 @@\n-\n@@ -4256,0 +4602,4 @@\n+  Label is_inline_type;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ test_markword_is_inline_type(rbx, is_inline_type);\n+\n@@ -4345,0 +4695,5 @@\n+\n+  __ bind(is_inline_type);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                    InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n@@ -4353,0 +4708,11 @@\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  Label has_identity;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ andptr(rbx, is_inline_type_mask);\n+  __ cmpl(rbx, is_inline_type_mask);\n+  __ jcc(Assembler::notEqual, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":504,"deletions":138,"binary":false,"changes":642,"status":"modified"},{"patch":"@@ -1803,1 +1803,1 @@\n-  if (!UseFastStosb && UseSSE >= 2 && UseUnalignedLoadStores) {\n+  if (UseSSE >= 2 && UseUnalignedLoadStores) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -493,0 +493,4 @@\n+  if (_entry_point == NULL) {\n+    \/\/ CallLeafNoFPInDirect\n+    return 3; \/\/ callq (register)\n+  }\n@@ -499,0 +503,1 @@\n+\n@@ -907,14 +912,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n-    Register klass = rscratch1;\n-\n-    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+  __ verified_entry(C);\n@@ -922,1 +914,2 @@\n-    __ bind(L_skip_barrier);\n+  if (ra_->C->stub_function() == NULL) {\n+    __ entry_barrier();\n@@ -925,1 +918,3 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n+  }\n@@ -937,6 +932,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -990,23 +979,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    emit_opcode(cbuf, Assembler::REX_W);\n-    if (framesize < 0x80) {\n-      emit_opcode(cbuf, 0x83); \/\/ addq rsp, #framesize\n-      emit_rm(cbuf, 0x3, 0x00, RSP_enc);\n-      emit_d8(cbuf, framesize);\n-    } else {\n-      emit_opcode(cbuf, 0x81); \/\/ addq rsp, #framesize\n-      emit_rm(cbuf, 0x3, 0x00, RSP_enc);\n-      emit_d32(cbuf, framesize);\n-    }\n-  }\n-\n-  \/\/ popq rbp\n-  emit_opcode(cbuf, 0x58 | RBP_enc);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair());\n@@ -1032,6 +1001,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1670,0 +1633,49 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+{\n+  C2_MacroAssembler _masm(&cbuf);\n+  uint insts_size = cbuf.insts_size();\n+  if (!_verified) {\n+    if (UseCompressedClassPointers) {\n+      __ load_klass(rscratch1, j_rarg0, rscratch2);\n+      __ cmpptr(rax, rscratch1);\n+    } else {\n+      __ cmpptr(rax, Address(j_rarg0, oopDesc::klass_offset_in_bytes()));\n+    }\n+    __ jump_cc(Assembler::notEqual, RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == NULL) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int initial_framesize = ra_->C->output()->frame_size_in_bytes() - 2*wordSize;\n+      __ remove_frame(initial_framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ jmp(dummy_verified_entry);\n+    } else {\n+      __ jmp(*_verified_entry);\n+    }\n+  }\n+  \/* WARNING these NOPs are critical so that verified entry point is properly\n+     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n+  int nops_cnt = 4 - ((cbuf.insts_size() - insts_size) & 0x3);\n+  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n+  if (nops_cnt > 0) {\n+    __ nop(nops_cnt);\n+  }\n+}\n+\n@@ -1712,7 +1724,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n-\n@@ -3997,0 +4002,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -4343,1 +4364,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -6931,0 +6952,13 @@\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -11446,0 +11480,1 @@\n+\n@@ -11448,1 +11483,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -11451,3 +11486,120 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                            Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small ClearArray AVX512 non-constant length.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -11501,2 +11653,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false, $ktmp$$KRegister);\n@@ -11507,3 +11659,2 @@\n-\/\/ Small ClearArray AVX512 non-constant length.\n-instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                       Universe dummy, rFlagsReg cr)\n+instruct rep_stos_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                 Universe dummy, rFlagsReg cr)\n@@ -11511,2 +11662,2 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -11514,1 +11665,1 @@\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -11562,2 +11713,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true, $ktmp$$KRegister);\n@@ -11569,1 +11720,1 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -11572,3 +11723,99 @@\n-  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large ClearArray AVX512.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -11613,2 +11860,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false, $ktmp$$KRegister);\n@@ -11619,3 +11866,2 @@\n-\/\/ Large ClearArray AVX512.\n-instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                             Universe dummy, rFlagsReg cr)\n+instruct rep_stos_large_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                       Universe dummy, rFlagsReg cr)\n@@ -11623,3 +11869,3 @@\n-  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -11664,2 +11910,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true, $ktmp$$KRegister);\n@@ -11671,1 +11917,1 @@\n-instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rax_RegL val, kReg ktmp, Universe dummy, rFlagsReg cr)\n@@ -11673,3 +11919,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() &&\n-              ((UseAVX > 2) && VM_Version::supports_avx512vlbw()));\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() &&\n+            ((UseAVX > 2) && VM_Version::supports_avx512vlbw()));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -11677,1 +11923,1 @@\n-  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n+  effect(TEMP tmp, USE_KILL val, TEMP ktmp, KILL cr);\n@@ -11680,1 +11926,1 @@\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$constant, $val$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n@@ -13510,0 +13756,15 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == NULL);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -13512,0 +13773,1 @@\n+  predicate(n->as_Call()->entry_point() != NULL);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":355,"deletions":93,"binary":false,"changes":448,"status":"modified"},{"patch":"@@ -56,7 +56,26 @@\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(\n-                        MacroAssembler *masm,\n-                        int total_args_passed,\n-                        int comp_args_on_stack,\n-                        const BasicType *sig_bt,\n-                        const VMRegPair *regs,\n-                        AdapterFingerPrint *fingerprint) {\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                           VMRegPair *regs,\n+                                           int total_args_passed) {\n+  Unimplemented();\n+  return 0;\n+}\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  Unimplemented();\n+  return NULL;\n+}\n+\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler* masm,\n+                                                            int comp_args_on_stack,\n+                                                            const GrowableArray <SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray <SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray <SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter,\n+                                                            bool allocate_code_blob) {\n+  if (allocate_code_blob) {\n+    new_adapter = AdapterBlob::create(masm->code(), 0, 0, NULL);\n+  }\n@@ -67,0 +86,4 @@\n+    CAST_FROM_FN_PTR(address,zero_null_code_stub),\n+    CAST_FROM_FN_PTR(address,zero_null_code_stub),\n+    CAST_FROM_FN_PTR(address,zero_null_code_stub),\n+    CAST_FROM_FN_PTR(address,zero_null_code_stub),\n","filename":"src\/hotspot\/cpu\/zero\/sharedRuntime_zero.cpp","additions":30,"deletions":7,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-VtableStub* VtableStubs::create_vtable_stub(int vtable_index) {\n+VtableStub* VtableStubs::create_vtable_stub(int vtable_index, bool caller_is_c1) {\n@@ -35,1 +35,1 @@\n-VtableStub* VtableStubs::create_itable_stub(int vtable_index) {\n+VtableStub* VtableStubs::create_itable_stub(int vtable_index, bool caller_is_c1) {\n","filename":"src\/hotspot\/cpu\/zero\/vtableStubs_zero.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -805,1 +805,1 @@\n-  return  false;\n+  return false;\n@@ -895,1 +895,2 @@\n-      strcmp(_matrule->_opType,\"Halt\"      )==0 )\n+      strcmp(_matrule->_opType,\"Halt\"      )==0 ||\n+      strcmp(_matrule->_opType,\"CallLeafNoFP\")==0)\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -50,0 +50,3 @@\n+                 Inline_Entry,\n+                 Verified_Inline_Entry,\n+                 Verified_Inline_Entry_RO,\n@@ -65,0 +68,1 @@\n+  void check(int e) const { assert(0 <= e && e < max_Entries, \"must be\"); }\n@@ -70,0 +74,3 @@\n+    _values[Inline_Entry  ] = 0;\n+    _values[Verified_Inline_Entry] = -1;\n+    _values[Verified_Inline_Entry_RO] = -1;\n@@ -78,2 +85,2 @@\n-  int value(Entries e) { return _values[e]; }\n-  void set_value(Entries e, int val) { _values[e] = val; }\n+  int value(Entries e) const { check(e); return _values[e]; }\n+  void set_value(Entries e, int val) { check(e); _values[e] = val; }\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.hpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -214,0 +216,2 @@\n+  assert(!_gen->in_conditional_code(), \"LIRItem cannot be loaded in conditional code\");\n+\n@@ -609,1 +613,2 @@\n-void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info) {\n+void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no,\n+                                 CodeEmitInfo* info_for_exception, CodeEmitInfo* info, CodeStub* throw_imse_stub) {\n@@ -612,1 +617,1 @@\n-  CodeStub* slow_path = new MonitorEnterStub(object, lock, info);\n+  CodeStub* slow_path = new MonitorEnterStub(object, lock, info, throw_imse_stub, scratch);\n@@ -615,1 +620,1 @@\n-  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception);\n+  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception, throw_imse_stub);\n@@ -639,4 +644,9 @@\n-void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n-  klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n-  \/\/ If klass is not loaded we do not know if the klass has finalizers:\n-  if (UseFastNewInstance && klass->is_loaded()\n+void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, bool allow_inline, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n+  if (allow_inline) {\n+    assert(!is_unresolved && klass->is_loaded(), \"inline type klass should be resolved\");\n+    __ metadata2reg(klass->constant_encoding(), klass_reg);\n+  } else {\n+    klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n+  }\n+  \/\/ If klass is not loaded we do not know if the klass has finalizers or is an unexpected inline klass\n+  if (UseFastNewInstance && klass->is_loaded() && (allow_inline || !klass->is_inlinetype())\n@@ -656,2 +666,2 @@\n-    CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, Runtime1::new_instance_id);\n-    __ branch(lir_cond_always, slow_path);\n+    CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, allow_inline ? Runtime1::new_instance_id : Runtime1::new_instance_no_inline_id);\n+    __ jump(slow_path);\n@@ -757,0 +767,10 @@\n+  if (!src->is_loaded_flattened_array() && !dst->is_loaded_flattened_array()) {\n+    flags &= ~LIR_OpArrayCopy::always_slow_path;\n+  }\n+  if (!src->maybe_flattened_array()) {\n+    flags &= ~LIR_OpArrayCopy::src_inlinetype_check;\n+  }\n+  if (!dst->maybe_flattened_array() && !dst->maybe_null_free_array()) {\n+    flags &= ~LIR_OpArrayCopy::dst_inlinetype_check;\n+  }\n+\n@@ -1561,2 +1581,4 @@\n-  _constants.append(c);\n-  _reg_for_constants.append(result);\n+  if (!in_conditional_code()) {\n+    _constants.append(c);\n+    _reg_for_constants.append(result);\n+  }\n@@ -1566,0 +1588,6 @@\n+void LIRGenerator::set_in_conditional_code(bool v) {\n+  assert(v != _in_conditional_code, \"must change state\");\n+  _in_conditional_code = v;\n+}\n+\n+\n@@ -1657,0 +1685,5 @@\n+  if (!inline_type_field_access_prolog(x)) {\n+    \/\/ Field store will always deopt due to unloaded field or holder klass\n+    return;\n+  }\n+\n@@ -1678,0 +1711,173 @@\n+\/\/ FIXME -- I can't find any other way to pass an address to access_load_at().\n+class TempResolvedAddress: public Instruction {\n+ public:\n+  TempResolvedAddress(ValueType* type, LIR_Opr addr) : Instruction(type) {\n+    set_operand(addr);\n+  }\n+  virtual void input_values_do(ValueVisitor*) {}\n+  virtual void visit(InstructionVisitor* v)   {}\n+  virtual const char* name() const  { return \"TempResolvedAddress\"; }\n+};\n+\n+LIR_Opr LIRGenerator::get_and_load_element_address(LIRItem& array, LIRItem& index) {\n+  ciType* array_type = array.value()->declared_type();\n+  ciFlatArrayKlass* flat_array_klass = array_type->as_flat_array_klass();\n+  assert(flat_array_klass->is_loaded(), \"must be\");\n+\n+  int array_header_size = flat_array_klass->array_header_in_bytes();\n+  int shift = flat_array_klass->log2_element_size();\n+\n+#ifndef _LP64\n+  LIR_Opr index_op = new_register(T_INT);\n+  \/\/ FIXME -- on 32-bit, the shift below can overflow, so we need to check that\n+  \/\/ the top (shift+1) bits of index_op must be zero, or\n+  \/\/ else throw ArrayIndexOutOfBoundsException\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::intConst(const_index << shift), index_op);\n+  } else {\n+    __ shift_left(index_op, shift, index.result());\n+  }\n+#else\n+  LIR_Opr index_op = new_register(T_LONG);\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::longConst(const_index << shift), index_op);\n+  } else {\n+    __ convert(Bytecodes::_i2l, index.result(), index_op);\n+    \/\/ Need to shift manually, as LIR_Address can scale only up to 3.\n+    __ shift_left(index_op, shift, index_op);\n+  }\n+#endif\n+\n+  LIR_Opr elm_op = new_pointer_register();\n+  LIR_Address* elm_address = generate_address(array.result(), index_op, 0, array_header_size, T_ADDRESS);\n+  __ leal(LIR_OprFact::address(elm_address), elm_op);\n+  return elm_op;\n+}\n+\n+void LIRGenerator::access_sub_element(LIRItem& array, LIRItem& index, LIR_Opr& result, ciField* field, int sub_offset) {\n+  assert(field != NULL, \"Need a subelement type specified\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  BasicType subelt_type = field->type()->basic_type();\n+  TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(subelt_type), elm_op);\n+  LIRItem elm_item(elm_resolved_addr, this);\n+\n+  DecoratorSet decorators = IN_HEAP;\n+  access_load_at(decorators, subelt_type,\n+                     elm_item, LIR_OprFact::intConst(sub_offset), result,\n+                     NULL, NULL);\n+\n+  if (field->is_null_free()) {\n+    assert(field->type()->is_loaded(), \"Must be\");\n+    assert(field->type()->is_inlinetype(), \"Must be if loaded\");\n+    assert(field->type()->as_inline_klass()->is_initialized(), \"Must be\");\n+    LabelObj* L_end = new LabelObj();\n+    __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(NULL));\n+    __ branch(lir_cond_notEqual, L_end->label());\n+    set_in_conditional_code(true);\n+    Constant* default_value = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+    if (default_value->is_pinned()) {\n+      __ move(LIR_OprFact::value_type(default_value->type()), result);\n+    } else {\n+      __ move(load_constant(default_value), result);\n+    }\n+    __ branch_destination(L_end->label());\n+    set_in_conditional_code(false);\n+  }\n+}\n+\n+void LIRGenerator::access_flattened_array(bool is_load, LIRItem& array, LIRItem& index, LIRItem& obj_item,\n+                                          ciField* field, int sub_offset) {\n+  assert(sub_offset == 0 || field != NULL, \"Sanity check\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  ciInlineKlass* elem_klass = NULL;\n+  if (field != NULL) {\n+    elem_klass = field->type()->as_inline_klass();\n+  } else {\n+    elem_klass = array.value()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+  }\n+  for (int i = 0; i < elem_klass->nof_nonstatic_fields(); i++) {\n+    ciField* inner_field = elem_klass->nonstatic_field_at(i);\n+    assert(!inner_field->is_flattened(), \"flattened fields must have been expanded\");\n+    int obj_offset = inner_field->offset();\n+    int elm_offset = obj_offset - elem_klass->first_field_offset() + sub_offset; \/\/ object header is not stored in array.\n+    BasicType field_type = inner_field->type()->basic_type();\n+\n+    \/\/ Types which are smaller than int are still passed in an int register.\n+    BasicType reg_type = field_type;\n+    switch (reg_type) {\n+    case T_BYTE:\n+    case T_BOOLEAN:\n+    case T_SHORT:\n+    case T_CHAR:\n+      reg_type = T_INT;\n+      break;\n+    default:\n+      break;\n+    }\n+\n+    LIR_Opr temp = new_register(reg_type);\n+    TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(field_type), elm_op);\n+    LIRItem elm_item(elm_resolved_addr, this);\n+\n+    DecoratorSet decorators = IN_HEAP;\n+    if (is_load) {\n+      access_load_at(decorators, field_type,\n+                     elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                     NULL, NULL);\n+      access_store_at(decorators, field_type,\n+                      obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                      NULL, NULL);\n+    } else {\n+      access_load_at(decorators, field_type,\n+                     obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                     NULL, NULL);\n+      access_store_at(decorators, field_type,\n+                      elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                      NULL, NULL);\n+    }\n+  }\n+}\n+\n+void LIRGenerator::check_flattened_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path) {\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_flattened_array(array, value, tmp, slow_path);\n+}\n+\n+void LIRGenerator::check_null_free_array(LIRItem& array, LIRItem& value, CodeEmitInfo* info) {\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+  __ branch(lir_cond_equal, L_end->label());\n+  __ null_check(value.result(), info);\n+  __ branch_destination(L_end->label());\n+}\n+\n+bool LIRGenerator::needs_flattened_array_store_check(StoreIndexed* x) {\n+  if (x->elt_type() == T_OBJECT && x->array()->maybe_flattened_array()) {\n+    ciType* type = x->value()->declared_type();\n+    if (type != NULL && type->is_klass()) {\n+      ciKlass* klass = type->as_klass();\n+      if (!klass->can_be_inline_klass() || (klass->is_inlinetype() && !klass->as_inline_klass()->flatten_array())) {\n+        \/\/ This is known to be a non-flattened object. If the array is flattened,\n+        \/\/ it will be caught by the code generated by array_store_check().\n+        return false;\n+      }\n+    }\n+    \/\/ We're not 100% sure, so let's do the flattened_array_store_check.\n+    return true;\n+  }\n+  return false;\n+}\n+\n+bool LIRGenerator::needs_null_free_array_store_check(StoreIndexed* x) {\n+  return x->elt_type() == T_OBJECT && x->array()->maybe_null_free_array();\n+}\n+\n@@ -1680,0 +1886,2 @@\n+  assert(x->elt_type() != T_ARRAY, \"never used\");\n+  bool is_loaded_flattened_array = x->array()->is_loaded_flattened_array();\n@@ -1683,3 +1891,3 @@\n-  bool needs_store_check = obj_store && (x->value()->as_Constant() == NULL ||\n-                                         !get_jobject_constant(x->value())->is_null_object() ||\n-                                         x->should_profile());\n+  bool needs_store_check = obj_store && !(is_loaded_flattened_array && x->is_exact_flattened_array_store()) &&\n+                                        (x->value()->as_Constant() == NULL ||\n+                                         !get_jobject_constant(x->value())->is_null_object());\n@@ -1698,2 +1906,3 @@\n-\n-  if (needs_store_check || x->check_boolean()) {\n+\n+  if (needs_store_check || x->check_boolean()\n+      || is_loaded_flattened_array || needs_flattened_array_store_check(x) || needs_null_free_array_store_check(x)) {\n@@ -1728,0 +1937,16 @@\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flattened_array()) {\n+      \/\/ No need to profile a store to a flattened array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      ciMethodData* md = NULL;\n+      ciArrayLoadStoreData* load_store = NULL;\n+      profile_array_type(x, md, load_store);\n+      if (x->array()->maybe_null_free_array()) {\n+        profile_null_free_array(array, md, load_store);\n+      }\n+      profile_element_type(x->value(), md, load_store);\n+    }\n+  }\n+\n@@ -1730,1 +1955,1 @@\n-    array_store_check(value.result(), array.result(), store_check_info, x->profiled_method(), x->profiled_bci());\n+    array_store_check(value.result(), array.result(), store_check_info, NULL, -1);\n@@ -1733,4 +1958,21 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-  if (x->check_boolean()) {\n-    decorators |= C1_MASK_BOOLEAN;\n-  }\n+  if (is_loaded_flattened_array) {\n+    if (!x->value()->is_null_free()) {\n+      __ null_check(value.result(), new CodeEmitInfo(range_check_info));\n+    }\n+    \/\/ If array element is an empty inline type, no need to copy anything\n+    if (!x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+      access_flattened_array(false, array, index, value);\n+    }\n+  } else {\n+    StoreFlattenedArrayStub* slow_path = NULL;\n+\n+    if (needs_flattened_array_store_check(x)) {\n+      \/\/ Check if we indeed have a flattened array\n+      index.load_item();\n+      slow_path = new StoreFlattenedArrayStub(array.result(), index.result(), value.result(), state_for(x, x->state_before()));\n+      check_flattened_array(array.result(), value.result(), slow_path);\n+      set_in_conditional_code(true);\n+    } else if (needs_null_free_array_store_check(x)) {\n+      CodeEmitInfo* info = new CodeEmitInfo(range_check_info);\n+      check_null_free_array(array, value, info);\n+    }\n@@ -1738,2 +1980,12 @@\n-  access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n-                  NULL, null_check_info);\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    if (x->check_boolean()) {\n+      decorators |= C1_MASK_BOOLEAN;\n+    }\n+\n+    access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n+                    NULL, null_check_info);\n+    if (slow_path != NULL) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+  }\n@@ -1820,0 +2072,25 @@\n+bool LIRGenerator::inline_type_field_access_prolog(AccessField* x) {\n+  ciField* field = x->field();\n+  assert(!field->is_flattened(), \"Flattened field access should have been expanded\");\n+  if (!field->is_null_free()) {\n+    return true; \/\/ Not an inline type field\n+  }\n+  \/\/ Deoptimize if the access is non-static and requires patching (holder not loaded\n+  \/\/ or not accessible) because then we only have partial field information and the\n+  \/\/ field could be flattened (see ciField constructor).\n+  bool could_be_flat = !x->is_static() && x->needs_patching();\n+  \/\/ Deoptimize if we load from a static field with an uninitialized type because we\n+  \/\/ need to throw an exception if initialization of the type failed.\n+  bool not_initialized = x->is_static() && x->as_LoadField() != NULL &&\n+      !field->type()->as_instance_klass()->is_initialized();\n+  if (could_be_flat || not_initialized) {\n+    CodeEmitInfo* info = state_for(x, x->state_before());\n+    CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),\n+                                        Deoptimization::Reason_unloaded,\n+                                        Deoptimization::Action_make_not_entrant);\n+    __ jump(stub);\n+    return false;\n+  }\n+  return true;\n+}\n+\n@@ -1849,0 +2126,7 @@\n+  if (!inline_type_field_access_prolog(x)) {\n+    \/\/ Field load will always deopt due to unloaded field or holder klass\n+    LIR_Opr result = rlock_result(x, field_type);\n+    __ move(LIR_OprFact::oopConst(NULL), result);\n+    return;\n+  }\n+\n@@ -1877,0 +2161,34 @@\n+\n+  ciField* field = x->field();\n+  if (field->is_null_free()) {\n+    \/\/ Load from non-flattened inline type field requires\n+    \/\/ a null check to replace null with the default value.\n+    ciInstanceKlass* holder = field->holder();\n+    if (field->is_static() && holder->is_loaded()) {\n+      ciObject* val = holder->java_mirror()->field_value(field).as_object();\n+      if (!val->is_null_object()) {\n+        \/\/ Static field is initialized, we don't need to perform a null check.\n+        return;\n+      }\n+    }\n+    ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+    if (inline_klass->is_initialized()) {\n+      LabelObj* L_end = new LabelObj();\n+      __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(NULL));\n+      __ branch(lir_cond_notEqual, L_end->label());\n+      set_in_conditional_code(true);\n+      Constant* default_value = new Constant(new InstanceConstant(inline_klass->default_instance()));\n+      if (default_value->is_pinned()) {\n+        __ move(LIR_OprFact::value_type(default_value->type()), result);\n+      } else {\n+        __ move(load_constant(default_value), result);\n+      }\n+      __ branch_destination(L_end->label());\n+      set_in_conditional_code(false);\n+    } else {\n+      info = state_for(x, x->state_before());\n+      __ cmp(lir_cond_equal, result, LIR_OprFact::oopConst(NULL));\n+      __ branch(lir_cond_equal, new DeoptimizeStub(info, Deoptimization::Reason_uninitialized,\n+                                                         Deoptimization::Action_make_not_entrant));\n+    }\n+  }\n@@ -2021,1 +2339,68 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  ciMethodData* md = NULL;\n+  ciArrayLoadStoreData* load_store = NULL;\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flattened_array()) {\n+      \/\/ No need to profile a load from a flattened array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      profile_array_type(x, md, load_store);\n+    }\n+  }\n+\n+  Value element;\n+  if (x->vt() != NULL) {\n+    assert(x->array()->is_loaded_flattened_array(), \"must be\");\n+    \/\/ Find the destination address (of the NewInlineTypeInstance).\n+    LIRItem obj_item(x->vt(), this);\n+\n+    access_flattened_array(true, array, index, obj_item,\n+                           x->delayed() == NULL ? 0 : x->delayed()->field(),\n+                           x->delayed() == NULL ? 0 : x->delayed()->offset());\n+    set_no_result(x);\n+  } else if (x->delayed() != NULL) {\n+    assert(x->array()->is_loaded_flattened_array(), \"must be\");\n+    LIR_Opr result = rlock_result(x, x->delayed()->field()->type()->basic_type());\n+    access_sub_element(array, index, result, x->delayed()->field(), x->delayed()->offset());\n+  } else if (x->array() != NULL && x->array()->is_loaded_flattened_array() &&\n+             x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_initialized() &&\n+             x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+    \/\/ Load the default instance instead of reading the element\n+    ciInlineKlass* elem_klass = x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    assert(elem_klass->is_initialized(), \"Must be\");\n+    Constant* default_value = new Constant(new InstanceConstant(elem_klass->default_instance()));\n+    if (default_value->is_pinned()) {\n+      __ move(LIR_OprFact::value_type(default_value->type()), result);\n+    } else {\n+      __ move(load_constant(default_value), result);\n+    }\n+  } else {\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    LoadFlattenedArrayStub* slow_path = NULL;\n+\n+    if (x->should_profile() && x->array()->maybe_null_free_array()) {\n+      profile_null_free_array(array, md, load_store);\n+    }\n+\n+    if (x->elt_type() == T_OBJECT && x->array()->maybe_flattened_array()) {\n+      assert(x->delayed() == NULL, \"Delayed LoadIndexed only apply to loaded_flattened_arrays\");\n+      index.load_item();\n+      \/\/ if we are loading from flattened array, load it using a runtime call\n+      slow_path = new LoadFlattenedArrayStub(array.result(), index.result(), result, state_for(x, x->state_before()));\n+      check_flattened_array(array.result(), LIR_OprFact::illegalOpr, slow_path);\n+      set_in_conditional_code(true);\n+    }\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    access_load_at(decorators, x->elt_type(),\n+                   array, index.result(), result,\n+                   NULL, null_check_info);\n+\n+    if (slow_path != NULL) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+\n+    element = x;\n+  }\n@@ -2023,4 +2408,3 @@\n-  LIR_Opr result = rlock_result(x, x->elt_type());\n-  access_load_at(decorators, x->elt_type(),\n-                 array, index.result(), result,\n-                 NULL, null_check_info);\n+  if (x->should_profile()) {\n+    profile_element_type(element, md, load_store);\n+  }\n@@ -2029,0 +2413,12 @@\n+void LIRGenerator::do_Deoptimize(Deoptimize* x) {\n+  \/\/ This happens only when a class X uses the withfield\/aconst_init bytecode\n+  \/\/ to refer to an inline class V, where V has not yet been loaded\/resolved.\n+  \/\/ This is not a common case. Let's just deoptimize.\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+  CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),\n+                                      Deoptimization::Reason_unloaded,\n+                                      Deoptimization::Action_make_not_entrant);\n+  __ jump(stub);\n+  LIR_Opr reg = rlock_result(x, T_OBJECT);\n+  __ move(LIR_OprFact::oopConst(NULL), reg);\n+}\n@@ -2527,1 +2923,1 @@\n-  if (do_update) {\n+  if (do_update && signature_at_call_k != NULL) {\n@@ -2612,0 +3008,46 @@\n+void LIRGenerator::profile_flags(ciMethodData* md, ciProfileData* data, int flag, LIR_Condition condition) {\n+  assert(md != NULL && data != NULL, \"should have been initialized\");\n+  LIR_Opr mdp = new_register(T_METADATA);\n+  __ metadata2reg(md->constant_encoding(), mdp);\n+  LIR_Address* addr = new LIR_Address(mdp, md->byte_offset_of_slot(data, DataLayout::flags_offset()), T_BYTE);\n+  LIR_Opr flags = new_register(T_INT);\n+  __ move(addr, flags);\n+  if (condition != lir_cond_always) {\n+    LIR_Opr update = new_register(T_INT);\n+    __ cmove(condition, LIR_OprFact::intConst(0), LIR_OprFact::intConst(flag), update, T_INT);\n+  } else {\n+    __ logical_or(flags, LIR_OprFact::intConst(flag), flags);\n+  }\n+  __ store(flags, addr);\n+}\n+\n+void LIRGenerator::profile_null_free_array(LIRItem array, ciMethodData* md, ciArrayLoadStoreData* load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+\n+  profile_flags(md, load_store, ArrayLoadStoreData::null_free_array_byte_constant(), lir_cond_equal);\n+}\n+\n+void LIRGenerator::profile_array_type(AccessIndexed* x, ciMethodData*& md, ciArrayLoadStoreData*& load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  int bci = x->profiled_bci();\n+  md = x->profiled_method()->method_data();\n+  assert(md != NULL, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(bci);\n+  assert(data != NULL && data->is_ArrayLoadStoreData(), \"incorrect profiling entry\");\n+  load_store = (ciArrayLoadStoreData*)data;\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_store, ArrayLoadStoreData::array_offset()), 0,\n+               load_store->array()->type(), x->array(), mdp, true, NULL, NULL);\n+}\n+\n+void LIRGenerator::profile_element_type(Value element, ciMethodData* md, ciArrayLoadStoreData* load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  assert(md != NULL && load_store != NULL, \"should have been initialized\");\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_store, ArrayLoadStoreData::element_offset()), 0,\n+               load_store->element()->type(), element, mdp, false, NULL, NULL);\n+}\n+\n@@ -2692,0 +3134,8 @@\n+  if (method()->has_scalarized_args()) {\n+    \/\/ Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized inline type arguments\n+    \/\/ in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.\n+    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, 0), NULL, false);\n+    CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);\n+    __ append(new LIR_Op0(lir_check_orig_pc));\n+    __ branch(lir_cond_notEqual, deopt_stub);\n+  }\n@@ -2707,0 +3157,14 @@\n+void LIRGenerator::invoke_load_one_argument(LIRItem* param, LIR_Opr loc) {\n+  if (loc->is_register()) {\n+    param->load_item_force(loc);\n+  } else {\n+    LIR_Address* addr = loc->as_address_ptr();\n+    param->load_for_store(addr->type());\n+    assert(addr->type() != T_PRIMITIVE_OBJECT, \"not supported yet\");\n+    if (addr->type() == T_OBJECT) {\n+      __ move_wide(param->result(), addr);\n+    } else {\n+      __ move(param->result(), addr);\n+    }\n+  }\n+}\n@@ -2714,10 +3178,1 @@\n-    if (loc->is_register()) {\n-      param->load_item_force(loc);\n-    } else {\n-      LIR_Address* addr = loc->as_address_ptr();\n-      param->load_for_store(addr->type());\n-      if (addr->type() == T_OBJECT) {\n-        __ move_wide(param->result(), addr);\n-      } else\n-        __ move(param->result(), addr);\n-    }\n+    invoke_load_one_argument(param, loc);\n@@ -2889,1 +3344,1 @@\n-  if (can_inline_as_constant(right.value())) {\n+  if (can_inline_as_constant(right.value()) && !x->substitutability_check()) {\n@@ -2892,0 +3347,1 @@\n+    \/\/ substitutability_check() needs to use right as a base register.\n@@ -2899,3 +3355,60 @@\n-  LIR_Opr reg = rlock_result(x);\n-  __ cmp(lir_cond(x->cond()), left.result(), right.result());\n-  __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  if (x->substitutability_check()) {\n+    substitutability_check(x, left, right, t_val, f_val);\n+  } else {\n+    LIR_Opr reg = rlock_result(x);\n+    __ cmp(lir_cond(x->cond()), left.result(), right.result());\n+    __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  }\n+}\n+\n+void LIRGenerator::substitutability_check(IfOp* x, LIRItem& left, LIRItem& right, LIRItem& t_val, LIRItem& f_val) {\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  bool is_acmpeq = (x->cond() == If::eql);\n+  LIR_Opr equal_result     = is_acmpeq ? t_val.result() : f_val.result();\n+  LIR_Opr not_equal_result = is_acmpeq ? f_val.result() : t_val.result();\n+  LIR_Opr result = rlock_result(x);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+}\n+\n+void LIRGenerator::substitutability_check(If* x, LIRItem& left, LIRItem& right) {\n+  LIR_Opr equal_result     = LIR_OprFact::intConst(1);\n+  LIR_Opr not_equal_result = LIR_OprFact::intConst(0);\n+  LIR_Opr result = new_register(T_INT);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  __ cmp(lir_cond(x->cond()), result, equal_result);\n+}\n+\n+void LIRGenerator::substitutability_check_common(Value left_val, Value right_val, LIRItem& left, LIRItem& right,\n+                                                 LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result,\n+                                                 CodeEmitInfo* info) {\n+  LIR_Opr tmp1 = LIR_OprFact::illegalOpr;\n+  LIR_Opr tmp2 = LIR_OprFact::illegalOpr;\n+  LIR_Opr left_klass_op = LIR_OprFact::illegalOpr;\n+  LIR_Opr right_klass_op = LIR_OprFact::illegalOpr;\n+\n+  ciKlass* left_klass  = left_val ->as_loaded_klass_or_null();\n+  ciKlass* right_klass = right_val->as_loaded_klass_or_null();\n+\n+  if ((left_klass == NULL || right_klass == NULL) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    init_temps_for_substitutability_check(tmp1, tmp2);\n+  }\n+\n+  if (left_klass != NULL && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+  } else {\n+    BasicType t_klass = UseCompressedOops ? T_INT : T_METADATA;\n+    left_klass_op = new_register(t_klass);\n+    right_klass_op = new_register(t_klass);\n+  }\n+\n+  CodeStub* slow_path = new SubstitutabilityCheckStub(left.result(), right.result(), info);\n+  __ substitutability_check(result, left.result(), right.result(), equal_result, not_equal_result,\n+                            tmp1, tmp2,\n+                            left_klass, right_klass, left_klass_op, right_klass_op, info, slow_path);\n@@ -3172,1 +3685,1 @@\n-    ciReturnTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n+    ciSingleTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n@@ -3193,0 +3706,47 @@\n+bool LIRGenerator::profile_inline_klass(ciMethodData* md, ciProfileData* data, Value value, int flag) {\n+  ciKlass* klass = value->as_loaded_klass_or_null();\n+  if (klass != NULL) {\n+    if (klass->is_inlinetype()) {\n+      profile_flags(md, data, flag, lir_cond_always);\n+    } else if (klass->can_be_inline_klass()) {\n+      return false;\n+    }\n+  } else {\n+    return false;\n+  }\n+  return true;\n+}\n+\n+\n+void LIRGenerator::do_ProfileACmpTypes(ProfileACmpTypes* x) {\n+  ciMethod* method = x->method();\n+  assert(method != NULL, \"method should be set if branch is profiled\");\n+  ciMethodData* md = method->method_data_or_null();\n+  assert(md != NULL, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(x->bci());\n+  assert(data != NULL, \"must have profiling data\");\n+  assert(data->is_ACmpData(), \"need BranchData for two-way branches\");\n+  ciACmpData* acmp = (ciACmpData*)data;\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()), 0,\n+               acmp->left()->type(), x->left(), mdp, !x->left_maybe_null(), NULL, NULL);\n+  int flags_offset = md->byte_offset_of_slot(data, DataLayout::flags_offset());\n+  if (!profile_inline_klass(md, acmp, x->left(), ACmpData::left_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->left(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::left_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()),\n+               in_bytes(ACmpData::right_offset()) - in_bytes(ACmpData::left_offset()),\n+               acmp->right()->type(), x->right(), mdp, !x->right_maybe_null(), NULL, NULL);\n+  if (!profile_inline_klass(md, acmp, x->right(), ACmpData::right_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->right(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::right_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":604,"deletions":44,"binary":false,"changes":648,"status":"modified"},{"patch":"@@ -427,1 +427,0 @@\n-    assert(type == _method_entry_ref, \"only special type allowed for now\");\n@@ -430,1 +429,1 @@\n-    _builder->add_special_ref(type, src_obj, field_offset);\n+    _builder->add_special_ref(type, src_obj, field_offset, ref->size() * BytesPerWord);\n@@ -474,4 +473,0 @@\n-void ArchiveBuilder::add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset) {\n-  _special_refs->append(SpecialRefInfo(type, src_obj, field_offset));\n-}\n-\n@@ -684,2 +679,18 @@\n-    assert(s.type() == MetaspaceClosure::_method_entry_ref, \"only special type allowed for now\");\n-    assert(*src_p == *dst_p, \"must be a copy\");\n+\n+    MetaspaceClosure::assert_valid(s.type());\n+    switch (s.type()) {\n+    case MetaspaceClosure::_method_entry_ref:\n+      assert(*src_p == *dst_p, \"must be a copy\");\n+      break;\n+    case MetaspaceClosure::_internal_pointer_ref:\n+      {\n+        \/\/ *src_p points to a location inside src_obj. Let's make *dst_p point to\n+        \/\/ the same location inside dst_obj.\n+        size_t off = pointer_delta(*((address*)src_p), src_obj, sizeof(u1));\n+        assert(off < s.src_obj_size_in_bytes(), \"must point to internal address\");\n+        *((address*)dst_p) = dst_obj + off;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":19,"deletions":8,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -131,0 +131,1 @@\n+    DEBUG_ONLY(size_t _src_obj_size_in_bytes;)\n@@ -134,2 +135,4 @@\n-    SpecialRefInfo(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset)\n-      : _type(type), _src_obj(src_obj), _field_offset(field_offset) {}\n+    SpecialRefInfo(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset, size_t src_obj_size_in_bytes)\n+      : _type(type), _src_obj(src_obj), _field_offset(field_offset) {\n+      DEBUG_ONLY(_src_obj_size_in_bytes = src_obj_size_in_bytes);\n+    }\n@@ -140,0 +143,2 @@\n+\n+    DEBUG_ONLY(size_t src_obj_size_in_bytes() const { return _src_obj_size_in_bytes; })\n@@ -364,1 +369,3 @@\n-  void add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset);\n+  void add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset, size_t src_obj_size_in_bytes) {\n+    _special_refs->append(SpecialRefInfo(type, src_obj, field_offset, src_obj_size_in_bytes));\n+  }\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -493,1 +493,1 @@\n-  if (src_obj != nullptr) {\n+  if (src_obj != nullptr && (!(EnableValhalla && src_obj->mark().is_inline_type()))) {\n@@ -495,1 +495,1 @@\n-    fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    fake_oop->set_mark(src_klass->prototype_header().copy_set_hash(src_hash));\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -476,1 +476,5 @@\n-  if (k->local_interfaces()->length() != _interfaces->length()) {\n+  const int actual_num_interfaces = k->local_interfaces()->length();\n+  const int specified_num_interfaces = _interfaces->length(); \/\/ specified in classlist\n+  int expected_num_interfaces = actual_num_interfaces;\n+\n+  if (specified_num_interfaces != expected_num_interfaces) {\n@@ -480,1 +484,1 @@\n-          _interfaces->length(), k->local_interfaces()->length());\n+          specified_num_interfaces, expected_num_interfaces);\n","filename":"src\/hotspot\/share\/cds\/classListParser.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -31,0 +31,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -55,0 +57,1 @@\n+\/\/ NOTE: this table must be in-sync with sun.jvm.hotspot.memory.FileMapInfo::populateMetadataTypeArray().\n@@ -64,1 +67,3 @@\n-  f(TypeArrayKlass)\n+  f(TypeArrayKlass) \\\n+  f(FlatArrayKlass) \\\n+  f(InlineKlass)\n","filename":"src\/hotspot\/share\/cds\/cppVtables.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -144,0 +144,65 @@\n+inline void CDSMustMatchFlags::do_print(outputStream* st, bool v) {\n+  st->print(\"%s\", v ? \"true\" : \"false\");\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, intx v) {\n+  st->print(INTX_FORMAT, v);\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, uintx v) {\n+  st->print(UINTX_FORMAT, v);\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, double v) {\n+  st->print(\"%f\", v);\n+}\n+\n+void CDSMustMatchFlags::init() {\n+  Arguments::assert_is_dumping_archive();\n+  _max_name_width = 0;\n+\n+#define INIT_CDS_MUST_MATCH_FLAG(n) \\\n+  _v_##n = n; \\\n+  _max_name_width = MAX2(_max_name_width,strlen(#n));\n+  CDS_MUST_MATCH_FLAGS_DO(INIT_CDS_MUST_MATCH_FLAG);\n+#undef INIT_CDS_MUST_MATCH_FLAG\n+}\n+\n+bool CDSMustMatchFlags::runtime_check() const {\n+#define CHECK_CDS_MUST_MATCH_FLAG(n) \\\n+  if (_v_##n != n) { \\\n+    ResourceMark rm; \\\n+    stringStream ss; \\\n+    ss.print(\"VM option %s is different between dumptime (\", #n);  \\\n+    do_print(&ss, _v_ ## n); \\\n+    ss.print(\") and runtime (\"); \\\n+    do_print(&ss, n); \\\n+    ss.print(\")\"); \\\n+    FileMapInfo::fail_continue(\"%s\", ss.as_string()); \\\n+    return false; \\\n+  }\n+  CDS_MUST_MATCH_FLAGS_DO(CHECK_CDS_MUST_MATCH_FLAG);\n+#undef CHECK_CDS_MUST_MATCH_FLAG\n+\n+  return true;\n+}\n+\n+void CDSMustMatchFlags::print_info() const {\n+  LogTarget(Info, cds) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"Recorded VM flags during dumptime:\");\n+    print(&ls);\n+  }\n+}\n+\n+void CDSMustMatchFlags::print(outputStream* st) const {\n+#define PRINT_CDS_MUST_MATCH_FLAG(n) \\\n+  st->print(\"- %-s \", #n);                   \\\n+  st->sp(int(_max_name_width - strlen(#n))); \\\n+  do_print(st, _v_##n);                      \\\n+  st->cr();\n+  CDS_MUST_MATCH_FLAGS_DO(PRINT_CDS_MUST_MATCH_FLAG);\n+#undef PRINT_CDS_MUST_MATCH_FLAG\n+}\n+\n@@ -310,0 +375,1 @@\n+  _must_match.init();\n@@ -372,0 +438,1 @@\n+  _must_match.print(st);\n@@ -1480,0 +1547,4 @@\n+  if (!header()->check_must_match_flags()) {\n+    return false;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":71,"deletions":0,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -184,0 +185,28 @@\n+#define CDS_MUST_MATCH_FLAGS_DO(f) \\\n+  f(EnableValhalla) \\\n+  f(FlatArrayElementMaxOops) \\\n+  f(FlatArrayElementMaxSize) \\\n+  f(InlineFieldMaxFlatSize) \\\n+  f(InlineTypePassFieldsAsArgs) \\\n+  f(InlineTypeReturnedAsFields)\n+\n+class CDSMustMatchFlags {\n+private:\n+  size_t _max_name_width;\n+#define DECLARE_CDS_MUST_MATCH_FLAG(n) \\\n+  decltype(n) _v_##n;\n+  CDS_MUST_MATCH_FLAGS_DO(DECLARE_CDS_MUST_MATCH_FLAG);\n+#undef DECLARE_CDS_MUST_MATCH_FLAG\n+\n+  inline static void do_print(outputStream* st, bool v);\n+  inline static void do_print(outputStream* st, intx v);\n+  inline static void do_print(outputStream* st, uintx v);\n+  inline static void do_print(outputStream* st, double v);\n+  void print_info() const;\n+\n+public:\n+  void init();\n+  bool runtime_check() const;\n+  void print(outputStream* st) const;\n+};\n+\n@@ -239,0 +268,1 @@\n+  CDSMustMatchFlags _must_match;        \/\/ These flags must be the same between dumptime and runtime\n@@ -325,0 +355,4 @@\n+  bool check_must_match_flags() const {\n+    return _must_match.runtime_check();\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":34,"deletions":0,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -65,0 +65,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -558,0 +558,3 @@\n+        \/\/ If the array is flattened, a larger part of it is modified than\n+        \/\/ the size of a reference. However, if OFFSET_ANY is given as\n+        \/\/ parameter to set_modified(), size is not taken into account.\n@@ -942,0 +945,1 @@\n+      case Bytecodes::_aconst_init:\n@@ -944,0 +948,15 @@\n+      case Bytecodes::_withfield: {\n+        bool will_link;\n+        ciField* field = s.get_field(will_link);\n+        BasicType field_type = field->type()->basic_type();\n+        if (field_type == T_OBJECT || field_type == T_ARRAY) {\n+          set_global_escape(state.apop());\n+        } else if (type2size[field_type] == 1) {\n+          state.spop();\n+        } else {\n+          state.lpop();\n+        }\n+        set_method_escape(state.apop());\n+        state.apush(allocated_obj);\n+        break;\n+      }\n","filename":"src\/hotspot\/share\/ci\/bcEscapeAnalyzer.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -538,1 +539,3 @@\n-      (sym->char_at(1) == JVM_SIGNATURE_ARRAY || sym->char_at(1) == JVM_SIGNATURE_CLASS)) {\n+      (sym->char_at(1) == JVM_SIGNATURE_ARRAY ||\n+       sym->char_at(1) == JVM_SIGNATURE_CLASS ||\n+       sym->char_at(1) == JVM_SIGNATURE_PRIMITIVE_OBJECT )) {\n@@ -551,1 +554,2 @@\n-      return ciObjArrayKlass::make_impl(elem_klass);\n+      bool null_free_array = sym->is_Q_array_signature() && sym->char_at(1) == JVM_SIGNATURE_PRIMITIVE_OBJECT;\n+      return ciArrayKlass::make(elem_klass, null_free_array);\n@@ -577,0 +581,15 @@\n+  int i = 0;\n+  while (sym->char_at(i) == JVM_SIGNATURE_ARRAY) {\n+    i++;\n+  }\n+  if (i > 0 && sym->char_at(i) == JVM_SIGNATURE_PRIMITIVE_OBJECT) {\n+    \/\/ An unloaded array class of inline types is an ObjArrayKlass, an\n+    \/\/ unloaded inline type class is an InstanceKlass. For consistency,\n+    \/\/ make the signature of the unloaded array of inline type use L\n+    \/\/ rather than Q.\n+    char* new_name = name_buffer(sym->utf8_length()+1);\n+    strncpy(new_name, (char*)sym->base(), sym->utf8_length());\n+    new_name[i] = JVM_SIGNATURE_CLASS;\n+    new_name[sym->utf8_length()] = '\\0';\n+    return get_unloaded_klass(accessing_klass, ciSymbol::make(new_name));\n+  }\n@@ -663,0 +682,8 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciEnv::is_inline_klass\n+\/\/\n+\/\/ Check if the klass is an inline klass.\n+bool ciEnv::has_Q_signature(const constantPoolHandle& cpool, int index) {\n+  GUARDED_VM_ENTRY(return cpool->klass_name_at(index)->is_Q_signature();)\n+}\n+\n@@ -759,1 +786,5 @@\n-    return ciConstant(T_OBJECT, mirror);\n+    if (klass->is_loaded() && tag.is_Qdescriptor_klass()) {\n+      return ciConstant(T_OBJECT, klass->as_inline_klass()->val_mirror());\n+    } else {\n+      return ciConstant(T_OBJECT, mirror);\n+    }\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":34,"deletions":3,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -138,0 +138,2 @@\n+  bool       has_Q_signature(const constantPoolHandle& cpool,\n+                             int klass_index);\n@@ -205,0 +207,4 @@\n+  ciFlatArrayKlass* get_flat_array_klass(Klass* o) {\n+    if (o == NULL) return NULL;\n+    return get_metadata(o)->as_flat_array_klass();\n+  }\n@@ -500,0 +506,4 @@\n+  ciWrapper* make_null_free_wrapper(ciType* type) {\n+    return _factory->make_null_free_wrapper(type);\n+  }\n+\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-    _known_to_link_with_put(nullptr), _known_to_link_with_get(nullptr) {\n+  _original_holder(nullptr), _is_flattened(false), _known_to_link_with_put(nullptr), _known_to_link_with_get(nullptr) {\n@@ -108,0 +108,3 @@\n+  \/\/ this is needed if the field class is not yet loaded.\n+  _is_null_free = _signature->is_Q_signature();\n+\n@@ -219,0 +222,25 @@\n+\/\/ Special copy constructor used to flatten inline type fields by\n+\/\/ copying the fields of the inline type to a new holder klass.\n+ciField::ciField(ciField* field, ciInstanceKlass* holder, int offset, bool is_final) {\n+  assert(field->holder()->is_inlinetype(), \"should only be used for inline type field flattening\");\n+  \/\/ Set the is_final flag\n+  jint final = is_final ? JVM_ACC_FINAL : ~JVM_ACC_FINAL;\n+  AccessFlags flags(field->flags().as_int() & final);\n+  _flags = ciFlags(flags);\n+  _holder = holder;\n+  _offset = offset;\n+  \/\/ Copy remaining fields\n+  _name = field->_name;\n+  _signature = field->_signature;\n+  _type = field->_type;\n+  \/\/ Trust final flattened fields\n+  _is_constant = is_final;\n+  _known_to_link_with_put = field->_known_to_link_with_put;\n+  _known_to_link_with_get = field->_known_to_link_with_get;\n+  _constant_value = field->_constant_value;\n+  assert(!field->is_flattened(), \"field must not be flattened\");\n+  _is_flattened = false;\n+  _is_null_free = field->_is_null_free;\n+  _original_holder = (field->_original_holder != nullptr) ? field->_original_holder : field->_holder;\n+}\n+\n@@ -236,0 +264,3 @@\n+  \/\/ Trust final fields in inline type buffers\n+  if (holder->is_inlinetype())\n+    return true;\n@@ -263,0 +294,3 @@\n+  _is_flattened = fd->is_inlined();\n+  _is_null_free = fd->signature()->is_Q_signature();\n+  _original_holder = nullptr;\n@@ -345,1 +379,3 @@\n-  ciKlass* type = CURRENT_ENV->get_klass_by_name_impl(_holder, constantPoolHandle(), _signature, false);\n+  \/\/ Use original holder for fields that came in through flattening\n+  ciKlass* accessing_klass = (_original_holder != nullptr) ? _original_holder : _holder;\n+  ciKlass* type = CURRENT_ENV->get_klass_by_name_impl(accessing_klass, constantPoolHandle(), _signature, false);\n@@ -374,2 +410,2 @@\n-         bc == Bytecodes::_getfield  || bc == Bytecodes::_putfield,\n-         \"unexpected bytecode\");\n+         bc == Bytecodes::_getfield  || bc == Bytecodes::_putfield  ||\n+         bc == Bytecodes::_withfield, \"unexpected bytecode\");\n@@ -458,0 +494,2 @@\n+  tty->print(\" is_flattened=%s\", bool_to_str(_is_flattened));\n+  tty->print(\" is_null_free=%s\", bool_to_str(_is_null_free));\n","filename":"src\/hotspot\/share\/ci\/ciField.cpp","additions":42,"deletions":4,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+  ciInstanceKlass* _original_holder; \/\/ For flattened fields\n@@ -52,0 +53,2 @@\n+  bool             _is_flattened;\n+  bool             _is_null_free;\n@@ -61,0 +64,1 @@\n+  ciField(ciField* field, ciInstanceKlass* holder, int offset, bool is_final);\n@@ -105,1 +109,1 @@\n-  BasicType layout_type() { return type2field[(_type == nullptr) ? T_OBJECT : _type->basic_type()]; }\n+  BasicType layout_type() { return type2field[type()->basic_type()]; }\n@@ -177,0 +181,3 @@\n+  bool is_flattened            () const { return _is_flattened; }\n+  bool is_null_free            () const { return _is_null_free; }\n+\n","filename":"src\/hotspot\/share\/ci\/ciField.hpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -43,1 +43,1 @@\n-ciType* ciInstance::java_mirror_type() {\n+ciType* ciInstance::java_mirror_type(bool* is_val_mirror) {\n@@ -56,0 +56,3 @@\n+    if (is_val_mirror != nullptr) {\n+      *is_val_mirror = java_lang_Class::is_secondary_mirror(m);\n+    }\n@@ -79,0 +82,1 @@\n+    case T_PRIMITIVE_OBJECT:  \/\/ fall through\n@@ -111,1 +115,3 @@\n-  assert(field->is_static() || klass()->is_subclass_of(field->holder()), \"invalid access - must be subclass\");\n+  assert(field->is_static() || field->holder()->is_inlinetype() || klass()->is_subclass_of(field->holder()),\n+         \"invalid access - must be subclass\");\n+\n","filename":"src\/hotspot\/share\/ci\/ciInstance.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-  ciType* java_mirror_type();\n+  ciType* java_mirror_type(bool* is_val_mirror = NULL);\n","filename":"src\/hotspot\/share\/ci\/ciInstance.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -39,0 +41,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -122,2 +125,3 @@\n-                                 jobject loader, jobject protection_domain)\n-  : ciKlass(name, T_OBJECT)\n+                                 jobject loader, jobject protection_domain,\n+                                 BasicType bt)\n+  : ciKlass(name, bt)\n@@ -128,1 +132,1 @@\n-  _nonstatic_fields = nullptr;\n+  _nonstatic_fields = nullptr;         \/\/ initialized lazily by compute_nonstatic_fields\n@@ -340,1 +344,1 @@\n-    _flags.print_klass_flags();\n+    _flags.print_klass_flags(st);\n@@ -344,1 +348,1 @@\n-      _super->print_name();\n+      _super->print_name_on(st);\n@@ -438,0 +442,23 @@\n+ciField* ciInstanceKlass::get_non_flattened_field_by_offset(int field_offset) {\n+  if (super() != nullptr && super()->has_nonstatic_fields()) {\n+    ciField* f = super()->get_non_flattened_field_by_offset(field_offset);\n+    if (f != nullptr) {\n+      return f;\n+    }\n+  }\n+\n+  VM_ENTRY_MARK;\n+  InstanceKlass* k = get_instanceKlass();\n+  Arena* arena = CURRENT_ENV->arena();\n+  for (JavaFieldStream fs(k); !fs.done(); fs.next()) {\n+    if (fs.access_flags().is_static())  continue;\n+    fieldDescriptor& fd = fs.field_descriptor();\n+    if (fd.offset() == field_offset) {\n+      ciField* f = new (arena) ciField(&fd);\n+      return f;\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n@@ -496,6 +523,1 @@\n-  int flen = fields->length();\n-\n-  \/\/ Now sort them by offset, ascending.\n-  \/\/ (In principle, they could mix with superclass fields.)\n-  fields->sort(sort_field_by_offset);\n-  return flen;\n+  return fields->length();\n@@ -505,3 +527,1 @@\n-GrowableArray<ciField*>*\n-ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>*\n-                                               super_fields) {\n+GrowableArray<ciField*>* ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields, bool flatten) {\n@@ -533,2 +553,22 @@\n-    ciField* field = new (arena) ciField(&fd);\n-    fields->append(field);\n+    if (fd.is_inlined() && flatten) {\n+      \/\/ Inline type fields are embedded\n+      int field_offset = fd.offset();\n+      \/\/ Get InlineKlass and adjust number of fields\n+      Klass* k = get_instanceKlass()->get_inline_type_field_klass(fd.index());\n+      ciInlineKlass* vk = CURRENT_ENV->get_klass(k)->as_inline_klass();\n+      flen += vk->nof_nonstatic_fields() - 1;\n+      \/\/ Iterate over fields of the flattened inline type and copy them to 'this'\n+      for (int i = 0; i < vk->nof_nonstatic_fields(); ++i) {\n+        ciField* flattened_field = vk->nonstatic_field_at(i);\n+        \/\/ Adjust offset to account for missing oop header\n+        int offset = field_offset + (flattened_field->offset() - vk->first_field_offset());\n+        \/\/ A flattened field can be treated as final if the non-flattened\n+        \/\/ field is declared final or the holder klass is an inline type itself.\n+        bool is_final = fd.is_final() || is_inlinetype();\n+        ciField* field = new (arena) ciField(flattened_field, this, offset, is_final);\n+        fields->append(field);\n+      }\n+    } else {\n+      ciField* field = new (arena) ciField(&fd);\n+      fields->append(field);\n+    }\n@@ -537,0 +577,3 @@\n+  \/\/ Now sort them by offset, ascending.\n+  \/\/ (In principle, they could mix with superclass fields.)\n+  fields->sort(sort_field_by_offset);\n@@ -643,0 +686,16 @@\n+bool ciInstanceKlass::can_be_inline_klass(bool is_exact) {\n+  if (!EnableValhalla) {\n+    return false;\n+  }\n+  if (!is_loaded() || is_inlinetype()) {\n+    \/\/ Not loaded or known to be an inline klass\n+    return true;\n+  }\n+  if (!is_exact) {\n+    \/\/ Not exact, check if this is a valid super for an inline klass\n+    VM_ENTRY_MARK;\n+    return !get_instanceKlass()->carries_identity_modifier();\n+  }\n+  return false;\n+}\n+\n@@ -651,1 +710,2 @@\n-class StaticFinalFieldPrinter : public FieldClosure {\n+class StaticFieldPrinter : public FieldClosure {\n+protected:\n@@ -653,0 +713,8 @@\n+public:\n+  StaticFieldPrinter(outputStream* out) :\n+    _out(out) {\n+  }\n+  void do_field_helper(fieldDescriptor* fd, oop obj, bool flattened);\n+};\n+\n+class StaticFinalFieldPrinter : public StaticFieldPrinter {\n@@ -656,2 +724,1 @@\n-    _out(out),\n-    _holder(holder) {\n+    StaticFieldPrinter(out), _holder(holder) {\n@@ -662,46 +729,58 @@\n-      oop mirror = fd->field_holder()->java_mirror();\n-      _out->print(\"staticfield %s %s %s \", _holder, fd->name()->as_quoted_ascii(), fd->signature()->as_quoted_ascii());\n-      switch (fd->field_type()) {\n-        case T_BYTE:    _out->print_cr(\"%d\", mirror->byte_field(fd->offset()));   break;\n-        case T_BOOLEAN: _out->print_cr(\"%d\", mirror->bool_field(fd->offset()));   break;\n-        case T_SHORT:   _out->print_cr(\"%d\", mirror->short_field(fd->offset()));  break;\n-        case T_CHAR:    _out->print_cr(\"%d\", mirror->char_field(fd->offset()));   break;\n-        case T_INT:     _out->print_cr(\"%d\", mirror->int_field(fd->offset()));    break;\n-        case T_LONG:    _out->print_cr(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n-        case T_FLOAT: {\n-          float f = mirror->float_field(fd->offset());\n-          _out->print_cr(\"%d\", *(int*)&f);\n-          break;\n-        }\n-        case T_DOUBLE: {\n-          double d = mirror->double_field(fd->offset());\n-          _out->print_cr(INT64_FORMAT, *(int64_t*)&d);\n-          break;\n-        }\n-        case T_ARRAY:  \/\/ fall-through\n-        case T_OBJECT: {\n-          oop value =  mirror->obj_field_acquire(fd->offset());\n-          if (value == nullptr) {\n-            _out->print_cr(\"null\");\n-          } else if (value->is_instance()) {\n-            assert(fd->field_type() == T_OBJECT, \"\");\n-            if (value->is_a(vmClasses::String_klass())) {\n-              const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n-              _out->print_cr(\"\\\"%s\\\"\", (ascii_value != nullptr) ? ascii_value : \"\");\n-            } else {\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print_cr(\"%s\", klass_name);\n-            }\n-          } else if (value->is_array()) {\n-            typeArrayOop ta = (typeArrayOop)value;\n-            _out->print(\"%d\", ta->length());\n-            if (value->is_objArray()) {\n-              objArrayOop oa = (objArrayOop)value;\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print(\" %s\", klass_name);\n-            }\n-            _out->cr();\n-          } else {\n-            ShouldNotReachHere();\n-          }\n-          break;\n+      InstanceKlass* holder = fd->field_holder();\n+      oop mirror = holder->java_mirror();\n+      _out->print(\"staticfield %s %s \", _holder, fd->name()->as_quoted_ascii());\n+      BasicType bt = fd->field_type();\n+      if (bt != T_OBJECT && bt != T_ARRAY) {\n+        _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+      }\n+      do_field_helper(fd, mirror, false);\n+      _out->cr();\n+    }\n+  }\n+};\n+\n+class InlineTypeFieldPrinter : public StaticFieldPrinter {\n+  oop _obj;\n+public:\n+  InlineTypeFieldPrinter(outputStream* out, oop obj) :\n+    StaticFieldPrinter(out), _obj(obj) {\n+  }\n+  void do_field(fieldDescriptor* fd) {\n+    do_field_helper(fd, _obj, true);\n+    _out->print(\" \");\n+  }\n+};\n+\n+void StaticFieldPrinter::do_field_helper(fieldDescriptor* fd, oop mirror, bool flattened) {\n+  BasicType bt = fd->field_type();\n+  switch (bt) {\n+    case T_BYTE:    _out->print(\"%d\", mirror->byte_field(fd->offset()));   break;\n+    case T_BOOLEAN: _out->print(\"%d\", mirror->bool_field(fd->offset()));   break;\n+    case T_SHORT:   _out->print(\"%d\", mirror->short_field(fd->offset()));  break;\n+    case T_CHAR:    _out->print(\"%d\", mirror->char_field(fd->offset()));   break;\n+    case T_INT:     _out->print(\"%d\", mirror->int_field(fd->offset()));    break;\n+    case T_LONG:    _out->print(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n+    case T_FLOAT: {\n+      float f = mirror->float_field(fd->offset());\n+      _out->print(\"%d\", *(int*)&f);\n+      break;\n+    }\n+    case T_DOUBLE: {\n+      double d = mirror->double_field(fd->offset());\n+      _out->print(INT64_FORMAT, *(int64_t*)&d);\n+      break;\n+    }\n+    case T_ARRAY:  \/\/ fall-through\n+    case T_OBJECT: {\n+      _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+      oop value =  mirror->obj_field_acquire(fd->offset());\n+      if (value == nullptr) {\n+        _out->print_cr(\"null\");\n+      } else if (value->is_instance()) {\n+        assert(fd->field_type() == T_OBJECT, \"\");\n+        if (value->is_a(vmClasses::String_klass())) {\n+          const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n+          _out->print(\"\\\"%s\\\"\", (ascii_value != nullptr) ? ascii_value : \"\");\n+         } else {\n+          const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+          _out->print(\"%s\", klass_name);\n@@ -709,2 +788,7 @@\n-        default:\n-          ShouldNotReachHere();\n+      } else if (value->is_array()) {\n+        typeArrayOop ta = (typeArrayOop)value;\n+        _out->print(\"%d\", ta->length());\n+        if (value->is_objArray() || value->is_flatArray()) {\n+          objArrayOop oa = (objArrayOop)value;\n+          const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+          _out->print(\" %s\", klass_name);\n@@ -712,0 +796,4 @@\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+      break;\n@@ -713,0 +801,25 @@\n+    case T_PRIMITIVE_OBJECT: {\n+      ResetNoHandleMark rnhm;\n+      Thread* THREAD = Thread::current();\n+      SignatureStream ss(fd->signature(), false);\n+      Symbol* name = ss.as_symbol();\n+      assert(!HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+      InstanceKlass* holder = fd->field_holder();\n+      InstanceKlass* k = SystemDictionary::find_instance_klass(THREAD, name,\n+                                                               Handle(THREAD, holder->class_loader()),\n+                                                               Handle(THREAD, holder->protection_domain()));\n+      assert(k != nullptr && !HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+      InlineKlass* vk = InlineKlass::cast(k);\n+      oop obj;\n+      if (flattened) {\n+        int field_offset = fd->offset() - vk->first_field_offset();\n+        obj = cast_to_oop(cast_from_oop<address>(mirror) + field_offset);\n+      } else {\n+        obj = mirror->obj_field_acquire(fd->offset());\n+      }\n+      InlineTypeFieldPrinter print_field(_out, obj);\n+      vk->do_nonstatic_fields(&print_field);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n@@ -714,1 +827,1 @@\n-};\n+}\n@@ -734,1 +847,1 @@\n-  if (_transitive_interfaces == NULL) {\n+  if (_transitive_interfaces == nullptr) {\n@@ -748,1 +861,1 @@\n-                                                                                                             0, NULL);\n+                                                                                                             0, nullptr);\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.cpp","additions":183,"deletions":70,"binary":false,"changes":253,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+\n@@ -87,1 +88,1 @@\n-  ciInstanceKlass(ciSymbol* name, jobject loader, jobject protection_domain);\n+  ciInstanceKlass(ciSymbol* name, jobject loader, jobject protection_domain, BasicType bt = T_OBJECT); \/\/ for unloaded klasses\n@@ -111,2 +112,2 @@\n-  int  compute_nonstatic_fields();\n-  GrowableArray<ciField*>* compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields);\n+  virtual int compute_nonstatic_fields();\n+  GrowableArray<ciField*>* compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields, bool flatten = true);\n@@ -210,0 +211,2 @@\n+  \/\/ get field descriptor at field_offset ignoring flattening\n+  ciField* get_non_flattened_field_by_offset(int field_offset);\n@@ -213,1 +216,1 @@\n-    if (_nonstatic_fields == nullptr)\n+    if (_nonstatic_fields == nullptr) {\n@@ -215,1 +218,1 @@\n-    else\n+    } else {\n@@ -217,0 +220,1 @@\n+    }\n@@ -247,1 +251,0 @@\n-  bool is_super       () { return flags().is_super(); }\n@@ -264,0 +267,2 @@\n+  virtual bool can_be_inline_klass(bool is_exact = false);\n+\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.hpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -229,0 +229,9 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciKlass::prototype_header\n+markWord ciKlass::prototype_header() const {\n+  assert(is_loaded(), \"not loaded\");\n+  GUARDED_VM_ENTRY(\n+    return get_Klass()->prototype_header();\n+  )\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciKlass.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -47,1 +47,3 @@\n-  friend class ciSignature;\n+  friend class ciSignature;\n+  friend class ciFlatArrayKlass;\n+  friend class ciArrayKlass;\n@@ -110,0 +112,8 @@\n+  virtual bool can_be_inline_klass(bool is_exact = false) {\n+    return false;\n+  }\n+\n+  virtual bool can_be_inline_array_klass() {\n+    return EnableValhalla && is_java_lang_Object();\n+  }\n+\n@@ -124,0 +134,2 @@\n+  markWord prototype_header() const;\n+\n","filename":"src\/hotspot\/share\/ci\/ciKlass.hpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+  virtual bool is_inlinetype() const        { return false; }\n@@ -59,0 +60,1 @@\n+  virtual bool is_flat_array_klass() const  { return false; }\n@@ -61,0 +63,2 @@\n+  virtual bool is_wrapper() const           { return false; }\n+  virtual bool flatten_array() const        { return false; }\n@@ -95,0 +99,4 @@\n+  ciFlatArrayKlass*        as_flat_array_klass() {\n+    assert(is_flat_array_klass(), \"bad cast\");\n+    return (ciFlatArrayKlass*)this;\n+  }\n@@ -103,0 +111,8 @@\n+  ciInlineKlass*           as_inline_klass() {\n+    assert(is_inlinetype(), \"bad cast\");\n+    return (ciInlineKlass*)this;\n+  }\n+  ciWrapper*               as_wrapper() {\n+    assert(is_wrapper(), \"bad cast\");\n+    return (ciWrapper*)this;\n+  }\n","filename":"src\/hotspot\/share\/ci\/ciMetadata.hpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -657,0 +658,33 @@\n+bool ciMethod::array_access_profiled_type(int bci, ciKlass*& array_type, ciKlass*& element_type, ProfilePtrKind& element_ptr, bool &flat_array, bool &null_free_array) {\n+  if (method_data() != NULL && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != NULL && data->is_ArrayLoadStoreData()) {\n+      ciArrayLoadStoreData* array_access = (ciArrayLoadStoreData*)data->as_ArrayLoadStoreData();\n+      array_type = array_access->array()->valid_type();\n+      element_type = array_access->element()->valid_type();\n+      element_ptr = array_access->element()->ptr_kind();\n+      flat_array = array_access->flat_array();\n+      null_free_array = array_access->null_free_array();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ciMethod::acmp_profiled_type(int bci, ciKlass*& left_type, ciKlass*& right_type, ProfilePtrKind& left_ptr, ProfilePtrKind& right_ptr, bool &left_inline_type, bool &right_inline_type) {\n+  if (method_data() != NULL && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != NULL && data->is_ACmpData()) {\n+      ciACmpData* acmp = (ciACmpData*)data->as_ACmpData();\n+      left_type = acmp->left()->valid_type();\n+      right_type = acmp->right()->valid_type();\n+      left_ptr = acmp->left()->ptr_kind();\n+      right_ptr = acmp->right()->ptr_kind();\n+      left_inline_type = acmp->left_inline_type();\n+      right_inline_type = acmp->right_inline_type();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -955,1 +989,1 @@\n-\/\/ ciMethod::is_object_initializer\n+\/\/ ciMethod::is_object_constructor\n@@ -957,2 +991,15 @@\n-bool ciMethod::is_object_initializer() const {\n-   return name() == ciSymbols::object_initializer_name();\n+bool ciMethod::is_object_constructor() const {\n+   return (name() == ciSymbols::object_initializer_name()\n+           && signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciMethod::is_static_vnew_factory\n+\/\/\n+bool ciMethod::is_static_vnew_factory() const {\n+   return (name() == ciSymbols::inline_factory_name()\n+           && !signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n@@ -1226,1 +1273,1 @@\n-bool ciMethod::is_initializer () const {         FETCH_FLAG_FROM_VM(is_initializer); }\n+bool ciMethod::is_object_constructor_or_class_initializer() const { FETCH_FLAG_FROM_VM(is_object_constructor_or_class_initializer); }\n@@ -1386,0 +1433,1 @@\n+  if (bt == T_PRIMITIVE_OBJECT)   return T_OBJECT;\n@@ -1473,0 +1521,18 @@\n+\n+bool ciMethod::is_scalarized_arg(int idx) const {\n+  VM_ENTRY_MARK;\n+  return get_Method()->is_scalarized_arg(idx);\n+}\n+\n+bool ciMethod::has_scalarized_args() const {\n+  VM_ENTRY_MARK;\n+  return get_Method()->has_scalarized_args();\n+}\n+\n+const GrowableArray<SigEntry>* ciMethod::get_sig_cc() const {\n+  VM_ENTRY_MARK;\n+  if (get_Method()->adapter() == NULL) {\n+    return NULL;\n+  }\n+  return get_Method()->adapter()->get_sig_cc();\n+}\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":70,"deletions":4,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+  ProfileUnknownNull,\n@@ -201,1 +202,1 @@\n-  bool is_static_initializer()  const { return get_Method()->is_static_initializer();  }\n+  bool is_class_initializer()   const { return get_Method()->is_class_initializer();   }\n@@ -271,1 +272,4 @@\n-\n+  bool          array_access_profiled_type(int bci, ciKlass*& array_type, ciKlass*& element_type, ProfilePtrKind& element_ptr, bool &flat_array, bool &null_free);\n+  bool          acmp_profiled_type(int bci, ciKlass*& left_type, ciKlass*& right_type,\n+                                   ProfilePtrKind& left_ptr, ProfilePtrKind& right_ptr,\n+                                   bool &left_inline_type, bool &right_inline_type);\n@@ -343,0 +347,1 @@\n+  bool has_vararg     () const                   { return flags().has_vararg(); }\n@@ -354,1 +359,0 @@\n-  bool is_initializer () const;\n@@ -360,0 +364,3 @@\n+  bool is_object_constructor() const;\n+  bool is_static_vnew_factory() const;\n+  bool is_object_constructor_or_class_initializer() const;\n@@ -361,1 +368,0 @@\n-  bool is_object_initializer() const;\n@@ -384,0 +390,5 @@\n+\n+  \/\/ Support for the inline type calling convention\n+  bool is_scalarized_arg(int idx) const;\n+  bool has_scalarized_args() const;\n+  const GrowableArray<SigEntry>* get_sig_cc() const;\n","filename":"src\/hotspot\/share\/ci\/ciMethod.hpp","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -303,1 +303,1 @@\n-void ciReturnTypeEntry::translate_type_data_from(const ReturnTypeEntry* ret) {\n+void ciSingleTypeEntry::translate_type_data_from(const SingleTypeEntry* ret) {\n@@ -308,1 +308,1 @@\n-    set_type(ReturnTypeEntry::with_status((Klass*)nullptr, k));\n+    set_type(SingleTypeEntry::with_status((Klass*)nullptr, k));\n@@ -359,0 +359,4 @@\n+  case DataLayout::array_load_store_data_tag:\n+    return new ciArrayLoadStoreData(data_layout);\n+  case DataLayout::acmp_data_tag:\n+    return new ciACmpData(data_layout);\n@@ -754,0 +758,13 @@\n+      } else if (pdata->is_ArrayLoadStoreData()) {\n+        ciArrayLoadStoreData* array_load_store_data = (ciArrayLoadStoreData*)pdata;\n+        dump_replay_data_type_helper(out, round, count, array_load_store_data, ciArrayLoadStoreData::array_offset(),\n+                                     array_load_store_data->array()->valid_type());\n+        dump_replay_data_type_helper(out, round, count, array_load_store_data, ciArrayLoadStoreData::element_offset(),\n+                                     array_load_store_data->element()->valid_type());\n+      } else if (pdata->is_ACmpData()) {\n+        ciACmpData* acmp_data = (ciACmpData*)pdata;\n+        dump_replay_data_type_helper(out, round, count, acmp_data, ciACmpData::left_offset(),\n+                                     acmp_data->left()->valid_type());\n+        dump_replay_data_type_helper(out, round, count, acmp_data, ciACmpData::right_offset(),\n+                                     acmp_data->right()->valid_type());\n+\n@@ -836,1 +853,1 @@\n-void ciReturnTypeEntry::print_data_on(outputStream* st) const {\n+void ciSingleTypeEntry::print_data_on(outputStream* st) const {\n@@ -909,0 +926,21 @@\n+\n+void ciArrayLoadStoreData::print_data_on(outputStream* st, const char* extra) const {\n+  print_shared(st, \"ciArrayLoadStoreData\", extra);\n+  tab(st, true);\n+  st->print(\"array\");\n+  array()->print_data_on(st);\n+  tab(st, true);\n+  st->print(\"element\");\n+  element()->print_data_on(st);\n+}\n+\n+void ciACmpData::print_data_on(outputStream* st, const char* extra) const {\n+  BranchData::print_data_on(st, extra);\n+  st->cr();\n+  tab(st, true);\n+  st->print(\"left\");\n+  left()->print_data_on(st);\n+  tab(st, true);\n+  st->print(\"right\");\n+  right()->print_data_on(st);\n+}\n","filename":"src\/hotspot\/share\/ci\/ciMethodData.cpp","additions":41,"deletions":3,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -127,1 +127,1 @@\n-class ciReturnTypeEntry : public ReturnTypeEntry, ciTypeEntries {\n+class ciSingleTypeEntry : public SingleTypeEntry, ciTypeEntries {\n@@ -129,1 +129,1 @@\n-  void translate_type_data_from(const ReturnTypeEntry* ret);\n+  void translate_type_data_from(const SingleTypeEntry* ret);\n@@ -149,1 +149,1 @@\n-  ciReturnTypeEntry* ret() const { return (ciReturnTypeEntry*)CallTypeData::ret(); }\n+  ciSingleTypeEntry* ret() const { return (ciSingleTypeEntry*)CallTypeData::ret(); }\n@@ -261,1 +261,1 @@\n-  ciReturnTypeEntry* ret() const { return (ciReturnTypeEntry*)VirtualCallTypeData::ret(); }\n+  ciSingleTypeEntry* ret() const { return (ciSingleTypeEntry*)VirtualCallTypeData::ret(); }\n@@ -365,0 +365,34 @@\n+class ciArrayLoadStoreData : public ArrayLoadStoreData {\n+public:\n+  ciArrayLoadStoreData(DataLayout* layout) : ArrayLoadStoreData(layout) {}\n+\n+  ciSingleTypeEntry* array() const { return (ciSingleTypeEntry*)ArrayLoadStoreData::array(); }\n+  ciSingleTypeEntry* element() const { return (ciSingleTypeEntry*)ArrayLoadStoreData::element(); }\n+\n+  virtual void translate_from(const ProfileData* data) {\n+    array()->translate_type_data_from(data->as_ArrayLoadStoreData()->array());\n+    element()->translate_type_data_from(data->as_ArrayLoadStoreData()->element());\n+  }\n+\n+#ifndef PRODUCT\n+  void print_data_on(outputStream* st, const char* extra = NULL) const;\n+#endif\n+};\n+\n+class ciACmpData : public ACmpData {\n+public:\n+  ciACmpData(DataLayout* layout) : ACmpData(layout) {}\n+\n+  ciSingleTypeEntry* left() const { return (ciSingleTypeEntry*)ACmpData::left(); }\n+  ciSingleTypeEntry* right() const { return (ciSingleTypeEntry*)ACmpData::right(); }\n+\n+  virtual void translate_from(const ProfileData* data) {\n+    left()->translate_type_data_from(data->as_ACmpData()->left());\n+    right()->translate_type_data_from(data->as_ACmpData()->right());\n+  }\n+\n+#ifndef PRODUCT\n+  void print_data_on(outputStream* st, const char* extra = NULL) const;\n+#endif\n+};\n+\n","filename":"src\/hotspot\/share\/ci\/ciMethodData.hpp","additions":38,"deletions":4,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -56,0 +57,1 @@\n+  _null_free = k->name()->is_Q_array_signature() && k->name()->char_at(1) == JVM_SIGNATURE_PRIMITIVE_OBJECT;\n@@ -67,8 +69,10 @@\n-    _base_element_klass = base_element_klass;\n-    assert(_base_element_klass->is_instance_klass() ||\n-           _base_element_klass->is_type_array_klass(), \"bad base klass\");\n-    if (dimension == 1) {\n-      _element_klass = base_element_klass;\n-    } else {\n-      _element_klass = nullptr;\n-    }\n+  _base_element_klass = base_element_klass;\n+  assert(_base_element_klass->is_instance_klass() ||\n+         _base_element_klass->is_type_array_klass() ||\n+         _base_element_klass->is_flat_array_klass(), \"bad base klass\");\n+  if (dimension == 1) {\n+    _element_klass = base_element_klass;\n+  } else {\n+    _element_klass = nullptr;\n+  }\n+  _null_free = array_name->is_Q_array_signature() && array_name->char_at(1) == JVM_SIGNATURE_PRIMITIVE_OBJECT;\n@@ -119,1 +123,1 @@\n-\n+  assert(base_name_sym->char_at(0) != JVM_SIGNATURE_PRIMITIVE_OBJECT, \"unloaded array klass element should not have Q-type\");\n@@ -138,1 +142,0 @@\n-\n@@ -151,2 +154,1 @@\n-  \/\/ The array klass was unable to be made or the element klass was\n-  \/\/ not loaded.\n+  \/\/ The array klass was unable to be made or the element klass was not loaded.\n@@ -179,0 +181,4 @@\n+  \/\/ Even if MyValue is exact, [LMyValue is not exact due to [QMyValue <: [LMyValue.\n+  if (!is_elem_null_free() && (!is_loaded() || element_klass()->is_inlinetype())) {\n+    return nullptr;\n+  }\n","filename":"src\/hotspot\/share\/ci\/ciObjArrayKlass.cpp","additions":18,"deletions":12,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+#include \"ci\/ciFlatArray.hpp\"\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -45,0 +47,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -368,0 +371,3 @@\n+  } else if (o->is_flatArray()) {\n+    flatArrayHandle h_ta(THREAD, (flatArrayOop)o);\n+    return new (arena()) ciFlatArray(h_ta);\n@@ -387,1 +393,3 @@\n-    if (k->is_instance_klass()) {\n+    if (k->is_inline_klass()) {\n+      return new (arena()) ciInlineKlass(k);\n+    } else if (k->is_instance_klass()) {\n@@ -390,0 +398,2 @@\n+    } else if (k->is_flatArray_klass()) {\n+      return new (arena()) ciFlatArrayKlass(k);\n@@ -499,1 +509,1 @@\n-    if (element_type == T_OBJECT) {\n+    if (element_type == T_OBJECT || element_type == T_PRIMITIVE_OBJECT) {\n@@ -630,0 +640,6 @@\n+ciWrapper* ciObjectFactory::make_null_free_wrapper(ciType* type) {\n+  ciWrapper* wrapper = new (arena()) ciWrapper(type);\n+  init_ident_of(wrapper);\n+  return wrapper;\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciObjectFactory.cpp","additions":18,"deletions":2,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -973,0 +974,1 @@\n+\n@@ -1019,27 +1021,79 @@\n-  \/\/ staticfield <klass> <name> <signature> <value>\n-  \/\/\n-  \/\/ Initialize a class and fill in the value for a static field.\n-  \/\/ This is useful when the compile was dependent on the value of\n-  \/\/ static fields but it's impossible to properly rerun the static\n-  \/\/ initializer.\n-  void process_staticfield(TRAPS) {\n-    InstanceKlass* k = (InstanceKlass *)parse_klass(CHECK);\n-\n-    if (k == nullptr || ReplaySuppressInitializers == 0 ||\n-        (ReplaySuppressInitializers == 2 && k->class_loader() == nullptr)) {\n-      skip_remaining();\n-      return;\n-    }\n-\n-    assert(k->is_initialized(), \"must be\");\n-\n-    const char* field_name = parse_escaped_string();\n-    const char* field_signature = parse_string();\n-    fieldDescriptor fd;\n-    Symbol* name = SymbolTable::new_symbol(field_name);\n-    Symbol* sig = SymbolTable::new_symbol(field_signature);\n-    if (!k->find_local_field(name, sig, &fd) ||\n-        !fd.is_static() ||\n-        fd.has_initial_value()) {\n-      report_error(field_name);\n-      return;\n+  class InlineTypeFieldInitializer : public FieldClosure {\n+    oop _vt;\n+    CompileReplay* _replay;\n+  public:\n+    InlineTypeFieldInitializer(oop vt, CompileReplay* replay)\n+  : _vt(vt), _replay(replay) {}\n+\n+    void do_field(fieldDescriptor* fd) {\n+      BasicType bt = fd->field_type();\n+      const char* string_value = bt != T_PRIMITIVE_OBJECT ? _replay->parse_escaped_string() : nullptr;\n+      switch (bt) {\n+      case T_BYTE: {\n+        int value = atoi(string_value);\n+        _vt->byte_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_BOOLEAN: {\n+        int value = atoi(string_value);\n+        _vt->bool_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_SHORT: {\n+        int value = atoi(string_value);\n+        _vt->short_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_CHAR: {\n+        int value = atoi(string_value);\n+        _vt->char_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_INT: {\n+        int value = atoi(string_value);\n+        _vt->int_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_LONG: {\n+        jlong value;\n+        if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n+          fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n+          break;\n+        }\n+        _vt->long_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_FLOAT: {\n+        float value = atof(string_value);\n+        _vt->float_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        double value = atof(string_value);\n+        _vt->double_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_ARRAY:\n+      case T_OBJECT: {\n+        JavaThread* THREAD = JavaThread::current();\n+        bool res = _replay->process_staticfield_reference(string_value, _vt, fd, THREAD);\n+        assert(res, \"should succeed for arrays & objects\");\n+        break;\n+      }\n+      case T_PRIMITIVE_OBJECT: {\n+        InlineKlass* vk = InlineKlass::cast(fd->field_holder()->get_inline_type_field_klass(fd->index()));\n+        if (fd->is_inlined()) {\n+          int field_offset = fd->offset() - vk->first_field_offset();\n+          oop obj = cast_to_oop(cast_from_oop<address>(_vt) + field_offset);\n+          InlineTypeFieldInitializer init_fields(obj, _replay);\n+          vk->do_nonstatic_fields(&init_fields);\n+        } else {\n+          oop value = vk->allocate_instance(JavaThread::current());\n+          _vt->obj_field_put(fd->offset(), value);\n+        }\n+        break;\n+      }\n+      default: {\n+        fatal(\"Unhandled type: %s\", type2name(bt));\n+      }\n+      }\n@@ -1047,0 +1101,1 @@\n+  };\n@@ -1048,1 +1103,1 @@\n-    oop java_mirror = k->java_mirror();\n+  bool process_staticfield_reference(const char* field_signature, oop java_mirror, fieldDescriptor* fd, TRAPS) {\n@@ -1055,4 +1110,2 @@\n-        ArrayKlass* kelem = (ArrayKlass *)parse_klass(CHECK);\n-        if (kelem == nullptr) {\n-          return;\n-        }\n+        Klass* k = resolve_klass(field_signature, CHECK_(true));\n+        ArrayKlass* kelem = (ArrayKlass *)k;\n@@ -1068,1 +1121,1 @@\n-        value = kelem->multi_allocate(rank, dims, CHECK);\n+        value = kelem->multi_allocate(rank, dims, CHECK_(true));\n@@ -1071,1 +1124,1 @@\n-          value = oopFactory::new_byteArray(length, CHECK);\n+          value = oopFactory::new_byteArray(length, CHECK_(true));\n@@ -1073,1 +1126,1 @@\n-          value = oopFactory::new_boolArray(length, CHECK);\n+          value = oopFactory::new_boolArray(length, CHECK_(true));\n@@ -1075,1 +1128,1 @@\n-          value = oopFactory::new_charArray(length, CHECK);\n+          value = oopFactory::new_charArray(length, CHECK_(true));\n@@ -1077,1 +1130,1 @@\n-          value = oopFactory::new_shortArray(length, CHECK);\n+          value = oopFactory::new_shortArray(length, CHECK_(true));\n@@ -1079,1 +1132,1 @@\n-          value = oopFactory::new_floatArray(length, CHECK);\n+          value = oopFactory::new_floatArray(length, CHECK_(true));\n@@ -1081,1 +1134,1 @@\n-          value = oopFactory::new_doubleArray(length, CHECK);\n+          value = oopFactory::new_doubleArray(length, CHECK_(true));\n@@ -1083,1 +1136,1 @@\n-          value = oopFactory::new_intArray(length, CHECK);\n+          value = oopFactory::new_intArray(length, CHECK_(true));\n@@ -1085,1 +1138,1 @@\n-          value = oopFactory::new_longArray(length, CHECK);\n+          value = oopFactory::new_longArray(length, CHECK_(true));\n@@ -1088,2 +1141,6 @@\n-          Klass* kelem = resolve_klass(field_signature + 1, CHECK);\n-          value = oopFactory::new_objArray(kelem, length, CHECK);\n+          Klass* kelem = resolve_klass(field_signature + 1, CHECK_(true));\n+          value = oopFactory::new_objArray(kelem, length, CHECK_(true));\n+        } else if (field_signature[0] == JVM_SIGNATURE_ARRAY &&\n+                   field_signature[1] == JVM_SIGNATURE_PRIMITIVE_OBJECT) {\n+          Klass* kelem = resolve_klass(field_signature + 1, CHECK_(true));\n+          value = oopFactory::new_valueArray(kelem, length, CHECK_(true));\n@@ -1094,0 +1151,87 @@\n+      java_mirror->obj_field_put(fd->offset(), value);\n+      return true;\n+    } else if (strcmp(field_signature, \"Ljava\/lang\/String;\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      Handle value = java_lang_String::create_from_str(string_value, CHECK_(true));\n+      java_mirror->obj_field_put(fd->offset(), value());\n+      return true;\n+    } else if (field_signature[0] == 'L') {\n+      const char* instance = parse_escaped_string();\n+      Klass* k = resolve_klass(instance, CHECK_(true));\n+      oop value = InstanceKlass::cast(k)->allocate_instance(CHECK_(true));\n+      java_mirror->obj_field_put(fd->offset(), value);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  \/\/ Initialize a class and fill in the value for a static field.\n+  \/\/ This is useful when the compile was dependent on the value of\n+  \/\/ static fields but it's impossible to properly rerun the static\n+  \/\/ initializer.\n+  void process_staticfield(TRAPS) {\n+    InstanceKlass* k = (InstanceKlass *)parse_klass(CHECK);\n+\n+    if (k == nullptr || ReplaySuppressInitializers == 0 ||\n+        (ReplaySuppressInitializers == 2 && k->class_loader() == nullptr)) {\n+        skip_remaining();\n+      return;\n+    }\n+\n+    assert(k->is_initialized(), \"must be\");\n+\n+    const char* field_name = parse_escaped_string();\n+    const char* field_signature = parse_string();\n+    fieldDescriptor fd;\n+    Symbol* name = SymbolTable::new_symbol(field_name);\n+    Symbol* sig = SymbolTable::new_symbol(field_signature);\n+    if (!k->find_local_field(name, sig, &fd) ||\n+        !fd.is_static() ||\n+        fd.has_initial_value()) {\n+      report_error(field_name);\n+      return;\n+    }\n+\n+    oop java_mirror = k->java_mirror();\n+    if (strcmp(field_signature, \"I\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->int_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"B\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->byte_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"C\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->char_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"S\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->short_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"Z\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->bool_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"J\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      jlong value;\n+      if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n+        fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n+        return;\n+      }\n+      java_mirror->long_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"F\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      float value = atof(string_value);\n+      java_mirror->float_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"D\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      double value = atof(string_value);\n+      java_mirror->double_field_put(fd.offset(), value);\n+    } else if (field_signature[0] == JVM_SIGNATURE_PRIMITIVE_OBJECT) {\n+      Klass* kelem = resolve_klass(field_signature, CHECK);\n+      InlineKlass* vk = InlineKlass::cast(kelem);\n+      oop value = vk->allocate_instance(CHECK);\n+      InlineTypeFieldInitializer init_fields(value, this);\n+      vk->do_nonstatic_fields(&init_fields);\n@@ -1096,37 +1240,2 @@\n-      const char* string_value = parse_escaped_string();\n-      if (strcmp(field_signature, \"I\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->int_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"B\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->byte_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"C\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->char_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"S\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->short_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"Z\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->bool_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"J\") == 0) {\n-        jlong value;\n-        if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n-          fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n-          return;\n-        }\n-        java_mirror->long_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"F\") == 0) {\n-        float value = atof(string_value);\n-        java_mirror->float_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"D\") == 0) {\n-        double value = atof(string_value);\n-        java_mirror->double_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"Ljava\/lang\/String;\") == 0) {\n-        Handle value = java_lang_String::create_from_str(string_value, CHECK);\n-        java_mirror->obj_field_put(fd.offset(), value());\n-      } else if (field_signature[0] == JVM_SIGNATURE_CLASS) {\n-        Klass* k = resolve_klass(string_value, CHECK);\n-        oop value = InstanceKlass::cast(k)->allocate_instance(CHECK);\n-        java_mirror->obj_field_put(fd.offset(), value);\n-      } else {\n+      bool res = process_staticfield_reference(field_signature, java_mirror, &fd, CHECK);\n+      if (!res)  {\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":189,"deletions":80,"binary":false,"changes":269,"status":"modified"},{"patch":"@@ -64,0 +64,3 @@\n+      if (type->is_inlinetype() && ss.has_Q_descriptor()) {\n+        type = env->make_null_free_wrapper(type);\n+      }\n@@ -71,0 +74,14 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciSignature::returns_null_free_inline_type\n+bool ciSignature::returns_null_free_inline_type() const {\n+  GUARDED_VM_ENTRY(return get_symbol()->is_Q_method_signature();)\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciSignature::is_null_free_at\n+\/\/\n+\/\/ True if we know that the argument at 'index' is null-free.\n+bool ciSignature::is_null_free_at(int index) const {\n+  return _types.at(index)->is_null_free();\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciSignature.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -225,0 +225,1 @@\n+  bool has_Q_signature() const;\n@@ -331,0 +332,8 @@\n+  bool is_null_free() {\n+    if (at_return_type()) {\n+      return _sig->returns_null_free_inline_type();\n+    } else {\n+      return _sig->is_null_free_at(_pos);\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/ci\/ciStreams.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -83,0 +83,14 @@\n+bool ciSymbol::starts_with(char prefix_char) const {\n+  GUARDED_VM_ENTRY(return get_symbol()->starts_with(prefix_char);)\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciSymbol::ends_with\n+\/\/\n+\/\/ Tests if the symbol ends with the given suffix.\n+bool ciSymbol::ends_with(const char* suffix, int len) const {\n+  GUARDED_VM_ENTRY(return get_symbol()->ends_with(suffix, len);)\n+}\n+bool ciSymbol::ends_with(char suffix_char) const {\n+  GUARDED_VM_ENTRY(return get_symbol()->ends_with(suffix_char);)\n+}\n@@ -102,0 +116,12 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciSymbol::is_Q_signature\n+bool ciSymbol::is_Q_signature() const {\n+  GUARDED_VM_ENTRY(return get_symbol()->is_Q_signature();)\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciSymbol::is_Q_array_signature\n+bool ciSymbol::is_Q_array_signature() const {\n+  GUARDED_VM_ENTRY(return get_symbol()->is_Q_array_signature();)\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciSymbol.cpp","additions":26,"deletions":0,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-\/\/ This class represents a Java reference or primitive type.\n+\/\/ This class represents a Java reference, inline type or primitive type.\n@@ -48,1 +48,1 @@\n-  _basic_type = k->is_array_klass() ? T_ARRAY : T_OBJECT;\n+  _basic_type = k->is_array_klass() ? T_ARRAY : (k->is_inline_klass() ? T_PRIMITIVE_OBJECT : T_OBJECT);\n","filename":"src\/hotspot\/share\/ci\/ciType.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -277,1 +278,11 @@\n-  } else if (t1->is_primitive_type() || t2->is_primitive_type()) {\n+  }\n+  \/\/ Unwrap after saving nullness information and handling top meets\n+  bool null_free1 = t1->is_null_free();\n+  bool null_free2 = t2->is_null_free();\n+  if (t1->unwrap() == t2->unwrap() && null_free1 == null_free2) {\n+    return t1;\n+  }\n+  t1 = t1->unwrap();\n+  t2 = t2->unwrap();\n+\n+  if (t1->is_primitive_type() || t2->is_primitive_type()) {\n@@ -279,1 +290,1 @@\n-    \/\/ is T.  null_type meet null_type is null_type.\n+    \/\/ is T. null_type meet null_type is null_type.\n@@ -293,35 +304,38 @@\n-  } else {\n-    \/\/ Both types are non-top non-primitive types.  That is,\n-    \/\/ both types are either instanceKlasses or arrayKlasses.\n-    ciKlass* object_klass = analyzer->env()->Object_klass();\n-    ciKlass* k1 = t1->as_klass();\n-    ciKlass* k2 = t2->as_klass();\n-    if (k1->equals(object_klass) || k2->equals(object_klass)) {\n-      return object_klass;\n-    } else if (!k1->is_loaded() || !k2->is_loaded()) {\n-      \/\/ Unloaded classes fall to java.lang.Object at a merge.\n-      return object_klass;\n-    } else if (k1->is_interface() != k2->is_interface()) {\n-      \/\/ When an interface meets a non-interface, we get Object;\n-      \/\/ This is what the verifier does.\n-      return object_klass;\n-    } else if (k1->is_array_klass() || k2->is_array_klass()) {\n-      \/\/ When an array meets a non-array, we get Object.\n-      \/\/ When objArray meets typeArray, we also get Object.\n-      \/\/ And when typeArray meets different typeArray, we again get Object.\n-      \/\/ But when objArray meets objArray, we look carefully at element types.\n-      if (k1->is_obj_array_klass() && k2->is_obj_array_klass()) {\n-        \/\/ Meet the element types, then construct the corresponding array type.\n-        ciKlass* elem1 = k1->as_obj_array_klass()->element_klass();\n-        ciKlass* elem2 = k2->as_obj_array_klass()->element_klass();\n-        ciKlass* elem  = type_meet_internal(elem1, elem2, analyzer)->as_klass();\n-        \/\/ Do an easy shortcut if one type is a super of the other.\n-        if (elem == elem1) {\n-          assert(k1 == ciObjArrayKlass::make(elem), \"shortcut is OK\");\n-          return k1;\n-        } else if (elem == elem2) {\n-          assert(k2 == ciObjArrayKlass::make(elem), \"shortcut is OK\");\n-          return k2;\n-        } else {\n-          return ciObjArrayKlass::make(elem);\n-        }\n+  }\n+\n+  \/\/ Both types are non-top non-primitive types.  That is,\n+  \/\/ both types are either instanceKlasses or arrayKlasses.\n+  ciKlass* object_klass = analyzer->env()->Object_klass();\n+  ciKlass* k1 = t1->as_klass();\n+  ciKlass* k2 = t2->as_klass();\n+  if (k1->equals(object_klass) || k2->equals(object_klass)) {\n+    return object_klass;\n+  } else if (!k1->is_loaded() || !k2->is_loaded()) {\n+    \/\/ Unloaded classes fall to java.lang.Object at a merge.\n+    return object_klass;\n+  } else if (k1->is_interface() != k2->is_interface()) {\n+    \/\/ When an interface meets a non-interface, we get Object;\n+    \/\/ This is what the verifier does.\n+    return object_klass;\n+  } else if (k1->is_array_klass() || k2->is_array_klass()) {\n+    \/\/ When an array meets a non-array, we get Object.\n+    \/\/ When (obj\/flat)Array meets typeArray, we also get Object.\n+    \/\/ And when typeArray meets different typeArray, we again get Object.\n+    \/\/ But when (obj\/flat)Array meets (obj\/flat)Array, we look carefully at element types.\n+    if ((k1->is_obj_array_klass() || k1->is_flat_array_klass()) &&\n+        (k2->is_obj_array_klass() || k2->is_flat_array_klass())) {\n+      bool null_free = k1->as_array_klass()->is_elem_null_free() &&\n+                       k2->as_array_klass()->is_elem_null_free();\n+      ciType* elem1 = k1->as_array_klass()->element_klass();\n+      ciType* elem2 = k2->as_array_klass()->element_klass();\n+      ciType* elem = elem1;\n+      if (elem1 != elem2) {\n+        elem = type_meet_internal(elem1, elem2, analyzer)->as_klass();\n+      }\n+      \/\/ Do an easy shortcut if one type is a super of the other.\n+      if (elem == elem1 && !elem->is_inlinetype()) {\n+        assert(k1 == ciArrayKlass::make(elem, null_free), \"shortcut is OK\");\n+        return k1;\n+      } else if (elem == elem2 && !elem->is_inlinetype()) {\n+        assert(k2 == ciArrayKlass::make(elem, null_free), \"shortcut is OK\");\n+        return k2;\n@@ -329,1 +343,1 @@\n-        return object_klass;\n+        return ciArrayKlass::make(elem, null_free);\n@@ -332,4 +346,9 @@\n-      \/\/ Must be two plain old instance klasses.\n-      assert(k1->is_instance_klass(), \"previous cases handle non-instances\");\n-      assert(k2->is_instance_klass(), \"previous cases handle non-instances\");\n-      return k1->least_common_ancestor(k2);\n+      return object_klass;\n+    }\n+  } else {\n+    \/\/ Must be two plain old instance klasses.\n+    assert(k1->is_instance_klass(), \"previous cases handle non-instances\");\n+    assert(k2->is_instance_klass(), \"previous cases handle non-instances\");\n+    ciType* result = k1->least_common_ancestor(k2);\n+    if (null_free1 && null_free2 && result->is_inlinetype()) {\n+      result = analyzer->mark_as_null_free(result);\n@@ -337,0 +356,1 @@\n+    return result;\n@@ -398,1 +418,6 @@\n-    state->push(method()->holder());\n+    ciType* holder = method()->holder();\n+    if (holder->is_inlinetype()) {\n+      \/\/ The receiver is null-free\n+      holder = mark_as_null_free(holder);\n+    }\n+    state->push(holder);\n@@ -404,1 +429,5 @@\n-    state->push_translate(str.type());\n+    ciType* arg = str.type();\n+    if (str.is_null_free()) {\n+      arg = mark_as_null_free(arg);\n+    }\n+    state->push_translate(arg);\n@@ -550,2 +579,2 @@\n-\/\/ ciTypeFlow::StateVector::do_aaload\n-void ciTypeFlow::StateVector::do_aaload(ciBytecodeStream* str) {\n+\/\/ ciTypeFlow::StateVector::do_aload\n+void ciTypeFlow::StateVector::do_aload(ciBytecodeStream* str) {\n@@ -553,1 +582,1 @@\n-  ciObjArrayKlass* array_klass = pop_objArray();\n+  ciArrayKlass* array_klass = pop_objOrFlatArray();\n@@ -555,1 +584,1 @@\n-    \/\/ Did aaload on a null reference; push a null and ignore the exception.\n+    \/\/ Did aload on a null reference; push a null and ignore the exception.\n@@ -580,1 +609,5 @@\n-    push_object(element_klass);\n+    if (array_klass->is_elem_null_free()) {\n+      push(outer()->mark_as_null_free(element_klass));\n+    } else {\n+      push_object(element_klass);\n+    }\n@@ -590,0 +623,1 @@\n+  bool null_free = str->has_Q_signature();\n@@ -591,6 +625,13 @@\n-    \/\/ VM's interpreter will not load 'klass' if object is null.\n-    \/\/ Type flow after this block may still be needed in two situations:\n-    \/\/ 1) C2 uses do_null_assert() and continues compilation for later blocks\n-    \/\/ 2) C2 does an OSR compile in a later block (see bug 4778368).\n-    pop_object();\n-    do_null_assert(klass);\n+    if (null_free) {\n+      trap(str, klass,\n+           Deoptimization::make_trap_request\n+           (Deoptimization::Reason_unloaded,\n+            Deoptimization::Action_reinterpret));\n+    } else {\n+      \/\/ VM's interpreter will not load 'klass' if object is NULL.\n+      \/\/ Type flow after this block may still be needed in two situations:\n+      \/\/ 1) C2 uses do_null_assert() and continues compilation for later blocks\n+      \/\/ 2) C2 does an OSR compile in a later block (see bug 4778368).\n+      pop_object();\n+      do_null_assert(klass);\n+    }\n@@ -598,2 +639,13 @@\n-    pop_object();\n-    push_object(klass);\n+    ciType* type = pop_value();\n+    null_free |= type->is_null_free();\n+    type = type->unwrap();\n+    if (type->is_loaded() && klass->is_loaded() &&\n+        type != klass && type->is_subtype_of(klass)) {\n+      \/\/ Useless cast, propagate more precise type of object\n+      klass = type->as_klass();\n+    }\n+    if (klass->is_inlinetype() && null_free) {\n+      push(outer()->mark_as_null_free(klass));\n+    } else {\n+      push_object(klass);\n+    }\n@@ -620,1 +672,10 @@\n-    if (!field_type->is_loaded()) {\n+    if (field->is_static() && field->is_null_free() &&\n+        !field_type->as_instance_klass()->is_initialized()) {\n+      \/\/ Deoptimize if we load from a static field with an uninitialized inline type\n+      \/\/ because we need to throw an exception if initialization of the type failed.\n+      trap(str, field_type->as_klass(),\n+           Deoptimization::make_trap_request\n+           (Deoptimization::Reason_unloaded,\n+            Deoptimization::Action_reinterpret));\n+      return;\n+    } else if (!field_type->is_loaded()) {\n@@ -641,0 +702,3 @@\n+      if (field->is_null_free()) {\n+        field_type = outer()->mark_as_null_free(field_type);\n+      }\n@@ -706,1 +770,11 @@\n-        do_null_assert(return_type->as_klass());\n+        if (InlineTypeReturnedAsFields) {\n+          \/\/ Return might be in scalarized form but we can't handle it because we\n+          \/\/ don't know the type. This can happen due to a missing preload attribute.\n+          \/\/ TODO 8284443 Use PhaseMacroExpand::expand_mh_intrinsic_return for this\n+          trap(str, nullptr,\n+               Deoptimization::make_trap_request\n+               (Deoptimization::Reason_uninitialized,\n+                Deoptimization::Action_reinterpret));\n+        } else {\n+          do_null_assert(return_type->as_klass());\n+        }\n@@ -708,0 +782,3 @@\n+        if (sigstr.is_null_free()) {\n+          return_type = outer()->mark_as_null_free(return_type);\n+        }\n@@ -738,1 +815,5 @@\n-        push_object(obj->klass());\n+        ciType* type = obj->klass();\n+        if (type->is_inlinetype()) {\n+          type = outer()->mark_as_null_free(type);\n+        }\n+        push(type);\n@@ -773,1 +854,1 @@\n-  if (!will_link || str->is_unresolved_klass()) {\n+  if (!will_link || str->is_unresolved_klass() || klass->is_inlinetype()) {\n@@ -780,0 +861,33 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciTypeFlow::StateVector::do_aconst_init\n+void ciTypeFlow::StateVector::do_aconst_init(ciBytecodeStream* str) {\n+  bool will_link;\n+  ciKlass* klass = str->get_klass(will_link);\n+  if (!will_link || str->is_unresolved_klass() || !klass->is_inlinetype()) {\n+    trap(str, klass, str->get_klass_index());\n+  } else {\n+    push(outer()->mark_as_null_free(klass));\n+  }\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciTypeFlow::StateVector::do_withfield\n+void ciTypeFlow::StateVector::do_withfield(ciBytecodeStream* str) {\n+  bool will_link;\n+  ciField* field = str->get_field(will_link);\n+  ciKlass* klass = field->holder();\n+  if (!will_link) {\n+    trap(str, klass, str->get_field_holder_index());\n+  } else {\n+    ciType* type = pop_value();\n+    ciType* field_type = field->type();\n+    if (field_type->is_two_word()) {\n+      ciType* type2 = pop_value();\n+      assert(type2->is_two_word(), \"must be 2nd half\");\n+      assert(type == half_type(type2), \"must be 2nd half\");\n+    }\n+    pop_object();\n+    push(outer()->mark_as_null_free(klass));\n+  }\n+}\n+\n@@ -885,1 +999,1 @@\n-  case Bytecodes::_aaload: do_aaload(str);                       break;\n+  case Bytecodes::_aaload: do_aload(str);                           break;\n@@ -891,1 +1005,1 @@\n-      pop_objArray();\n+      pop_objOrFlatArray();\n@@ -913,1 +1027,2 @@\n-        push_object(ciObjArrayKlass::make(element_klass));\n+        bool null_free = str->has_Q_signature();\n+        push_object(ciArrayKlass::make(element_klass, null_free));\n@@ -1445,0 +1560,3 @@\n+  case Bytecodes::_aconst_init: do_aconst_init(str);              break;\n+  case Bytecodes::_withfield: do_withfield(str);                    break;\n+\n@@ -1472,0 +1590,1 @@\n+\n@@ -1492,1 +1611,1 @@\n-  ciType* type = type_at(c);\n+  ciType* type = type_at(c)->unwrap();\n@@ -1755,3 +1874,6 @@\n-      case Bytecodes::_athrow:     case Bytecodes::_ireturn:\n-      case Bytecodes::_lreturn:    case Bytecodes::_freturn:\n-      case Bytecodes::_dreturn:    case Bytecodes::_areturn:\n+      case Bytecodes::_athrow:\n+      case Bytecodes::_ireturn:\n+      case Bytecodes::_lreturn:\n+      case Bytecodes::_freturn:\n+      case Bytecodes::_dreturn:\n+      case Bytecodes::_areturn:\n@@ -3151,0 +3273,5 @@\n+ciType* ciTypeFlow::mark_as_null_free(ciType* type) {\n+  \/\/ Wrap the type to carry the information that it is null-free\n+  return env()->make_null_free_wrapper(type);\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciTypeFlow.cpp","additions":195,"deletions":68,"binary":false,"changes":263,"status":"modified"},{"patch":"@@ -334,2 +334,2 @@\n-    \/\/ pop_objArray and pop_typeArray narrow the tos to ciObjArrayKlass\n-    \/\/ or ciTypeArrayKlass (resp.).  In the rare case that an explicit\n+    \/\/ pop_objOrFlatArray and pop_typeArray narrow the tos to ciObjArrayKlass,\n+    \/\/ ciFlatArrayKlass or ciTypeArrayKlass (resp.). In the rare case that an explicit\n@@ -337,1 +337,1 @@\n-    ciObjArrayKlass* pop_objArray() {\n+    ciArrayKlass* pop_objOrFlatArray() {\n@@ -340,2 +340,3 @@\n-      assert(array->is_obj_array_klass(), \"must be object array type\");\n-      return array->as_obj_array_klass();\n+      assert(array->is_obj_array_klass() || array->is_flat_array_klass(),\n+             \"must be a flat or an object array type\");\n+      return array->as_array_klass();\n@@ -355,1 +356,1 @@\n-    void do_aaload(ciBytecodeStream* str);\n+    void do_aload(ciBytecodeStream* str);\n@@ -364,0 +365,2 @@\n+    void do_aconst_init(ciBytecodeStream* str);\n+    void do_withfield(ciBytecodeStream* str);\n@@ -852,0 +855,2 @@\n+  ciType* mark_as_null_free(ciType* type);\n+\n","filename":"src\/hotspot\/share\/ci\/ciTypeFlow.hpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+\n@@ -52,0 +53,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -85,0 +87,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -147,0 +150,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        65\n+\n@@ -185,1 +190,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -506,1 +511,8 @@\n-        cp->unresolved_klass_at_put(index, class_index, num_klasses++);\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n+        if (name->is_Q_signature()) {\n+          cp->unresolved_qdescriptor_at_put(index, class_index, num_klasses++);\n+        } else {\n+          cp->unresolved_klass_at_put(index, class_index, num_klasses++);\n+        }\n@@ -706,1 +718,2 @@\n-          \/\/ If a class method name begins with '<', it must be \"<init>\" and have void signature.\n+          \/\/ If a class method name begins with '<', it must be \"<init>\" and have void signature,\n+          \/\/ or if it is an inline type, <vnew> with return.\n@@ -710,1 +723,2 @@\n-            if (name != vmSymbols::object_initializer_name()) {\n+            if (name != vmSymbols::object_initializer_name() &&\n+                name != vmSymbols::inline_factory_name()) {\n@@ -715,2 +729,7 @@\n-            } else if (!Signature::is_void_method(signature)) { \/\/ must have void signature.\n-              throwIllegalSignature(\"Method\", name, signature, CHECK);\n+            } else if (!Signature::is_void_method(signature)) {\n+              \/\/ if return type is non-void then it must be an inline type\n+              if (name == vmSymbols::object_initializer_name() ||\n+                  !EnableValhalla || !supports_inline_types() ||\n+                  !signature->ends_with(JVM_SIGNATURE_ENDCLASS)) {\n+                throwIllegalSignature(\"Method\", name, signature, CHECK);\n+              }\n@@ -735,2 +754,13 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+\n+            if (EnableValhalla && supports_inline_types() && name == vmSymbols::inline_factory_name()) { \/\/ <vnew>\n+              \/\/ <vnew> factory methods must be non-void return and invokeStatic.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature() || ref_kind != JVM_REF_invokeStatic) {\n+                classfile_parse_error(\n+                  \"Bad factory method name at constant pool index %u in class file %s\",\n+                  name_ref_index, CHECK);\n+              }\n+            } else if (name != vmSymbols::object_initializer_name()) { \/\/ !<init>\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -742,2 +772,10 @@\n-            } else {\n-              if (name == vmSymbols::object_initializer_name()) {\n+            } else { \/\/ <init>\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else {\n@@ -820,4 +858,31 @@\n-\/\/ Side-effects: populates the _local_interfaces field\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+static void check_identity_and_value_modifiers(ClassFileParser* current, const InstanceKlass* super_type, TRAPS) {\n+  assert(super_type != nullptr,\"Method doesn't support null super type\");\n+  if (super_type->carries_identity_modifier()) {\n+    if (current->carries_value_modifier()) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(\n+          THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_IncompatibleClassChangeError(),\n+          \"Value type %s has an identity type as supertype\",\n+          current->class_name()->as_klass_external_name());\n+        return;\n+      }\n+    current->set_carries_identity_modifier();\n+  }\n+  if (super_type->carries_value_modifier()) {\n+    if (current->carries_identity_modifier()) {\n+      ResourceMark rm(THREAD);\n+      Exceptions::fthrow(\n+        THREAD_AND_LOCATION,\n+        vmSymbols::java_lang_IncompatibleClassChangeError(),\n+        \"Identity type %s has a value type as supertype\",\n+        current->class_name()->as_klass_external_name());\n+      return;\n+    }\n+    current->set_carries_value_modifier();\n+  }\n+}\n+\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n@@ -825,0 +890,7 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n+                                       bool* const is_declared_atomic,\n@@ -832,0 +904,1 @@\n+\n@@ -834,3 +907,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n-\n-    int index;\n+    _local_interface_indexes = new GrowableArray<u2>(itfs_len);\n+    int index = 0;\n@@ -839,1 +911,0 @@\n-      Klass* interf;\n@@ -844,32 +915,1 @@\n-      if (cp->tag_at(interface_index).is_klass()) {\n-        interf = cp->resolved_klass_at(interface_index);\n-      } else {\n-        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n-\n-        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n-        \/\/ But need to make sure it's not an array type.\n-        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n-                           \"Bad interface name in class file %s\", CHECK);\n-\n-        \/\/ Call resolve_super so class circularity is checked\n-        interf = SystemDictionary::resolve_super_or_fail(\n-                                                  _class_name,\n-                                                  unresolved_klass,\n-                                                  Handle(THREAD, _loader_data->class_loader()),\n-                                                  _protection_domain,\n-                                                  false,\n-                                                  CHECK);\n-      }\n-\n-      if (!interf->is_interface()) {\n-        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n-                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n-                          _class_name->as_klass_external_name(),\n-                          interf->external_name(),\n-                          interf->class_in_module_of_loader()));\n-      }\n-\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n-        *has_nonstatic_concrete_methods = true;\n-      }\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      _local_interface_indexes->at_put_grow(index, interface_index);\n@@ -893,2 +933,1 @@\n-        const InstanceKlass* const k = _local_interfaces->at(index);\n-        name = k->name();\n+        name = cp->klass_name_at(_local_interface_indexes->at(index));\n@@ -1380,0 +1419,1 @@\n+  STATIC_INLINE,        \/\/ inline type field\n@@ -1385,0 +1425,1 @@\n+  NONSTATIC_INLINE,\n@@ -1404,6 +1445,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  NONSTATIC_OOP,       \/\/ T_PRIMITIVE_OBJECT = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20,\n@@ -1424,6 +1466,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  STATIC_OOP,          \/\/ T_PRIMITIVE_OBJECT = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20\n@@ -1432,1 +1475,1 @@\n-static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type) {\n+static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type, bool is_inline_type) {\n@@ -1436,0 +1479,3 @@\n+  if (is_inline_type) {\n+    result = is_static ? STATIC_INLINE : NONSTATIC_INLINE;\n+  }\n@@ -1449,2 +1495,2 @@\n-  void update(bool is_static, BasicType type) {\n-    FieldAllocationType atype = basic_type_to_atype(is_static, type);\n+  void update(bool is_static, BasicType type, bool is_inline_type) {\n+    FieldAllocationType atype = basic_type_to_atype(is_static, type, is_inline_type);\n@@ -1462,1 +1508,1 @@\n-                                   bool is_interface,\n+                                   AccessFlags class_access_flags,\n@@ -1478,0 +1524,1 @@\n+  bool is_inline_type = class_access_flags.is_value_class() && !class_access_flags.is_abstract();\n@@ -1485,1 +1532,5 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);\n@@ -1515,0 +1566,1 @@\n+  int instance_fields_count = 0;\n@@ -1519,0 +1571,4 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, class_access_flags, CHECK);\n@@ -1520,2 +1576,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1537,0 +1591,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1596,1 +1651,1 @@\n-    fac->update(is_static, type);\n+    fac->update(is_static, type, type == T_PRIMITIVE_OBJECT);\n@@ -1640,1 +1695,1 @@\n-      fac->update(false, type);\n+      fac->update(false, type, false);\n@@ -1645,0 +1700,27 @@\n+  if (is_inline_type) {\n+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);\n+    field->initialize(JVM_ACC_FIELD_INTERNAL | JVM_ACC_STATIC,\n+                      (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(default_value_name)),\n+                      (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(object_signature)),\n+                      0);\n+    const BasicType type = Signature::basic_type(vmSymbols::object_signature());\n+    fac->update(true, type, false);\n+    index++;\n+  }\n+\n+  if (is_inline_type && instance_fields_count == 0) {\n+    _is_empty_inline_type = true;\n+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);\n+    field->initialize(JVM_ACC_FIELD_INTERNAL,\n+        (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(empty_marker_name)),\n+        (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(byte_signature)),\n+        0);\n+    const BasicType type = Signature::basic_type(vmSymbols::byte_signature());\n+    fac->update(false, type, false);\n+    index++;\n+  }\n+\n+  if (instance_fields_count > 0) {\n+    _has_nonstatic_fields = true;\n+  }\n+\n@@ -1958,0 +2040,5 @@\n+  const char* class_note = \"\";\n+  if (is_inline_type() && name == vmSymbols::object_initializer_name()) {\n+    class_note = \" (an inline class)\";\n+  }\n+\n@@ -1961,2 +2048,2 @@\n-      \"%s \\\"%s\\\" in class %s has illegal signature \\\"%s\\\"\", type,\n-      name->as_C_string(), _class_name->as_C_string(), sig->as_C_string());\n+      \"%s \\\"%s\\\" in class %s%s has illegal signature \\\"%s\\\"\", type,\n+      name->as_C_string(), _class_name->as_C_string(), class_note, sig->as_C_string());\n@@ -2263,0 +2350,2 @@\n+                                      bool is_value_class,\n+                                      bool is_abstract_class,\n@@ -2304,1 +2393,34 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n+    verify_legal_method_modifiers(flags, access_flags() , name, CHECK_NULL);\n+  }\n+\n+  if (EnableValhalla && supports_inline_types() && name == vmSymbols::inline_factory_name()) {\n+    if (is_interface) {\n+      classfile_parse_error(\"Interface cannot have a method named <vnew>, class file %s\", CHECK_NULL);\n+    } else if (!is_value_class) {\n+       classfile_parse_error(\"Identity class cannot have a method <vnew>, class file %s\", CHECK_NULL);\n+    } else if (signature->is_void_method_signature()) {\n+       classfile_parse_error(\"Factory method <vnew> must have a non-void return type, class file %s\", CHECK_NULL);\n+    } else { \/\/ also OK, a static factory, as long as the return value is good\n+      bool ok = false;\n+      SignatureStream ss((Symbol*) signature, true);\n+      while (!ss.at_return_type())  ss.next();\n+      if (ss.is_reference()) {\n+        Symbol* ret = ss.as_symbol();\n+        const Symbol* required = class_name();\n+        if (is_hidden()) {\n+          \/\/ The original class name for hidden classes changed.\n+          \/\/\/ So using the original name in the return type is no longer valid.\n+          required = vmSymbols::java_lang_Object();\n+        }\n+        ok = (ret == required);\n+      }\n+      if (!ok) {\n+        throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+      }\n+      \/\/ factory method, with a non-void return.  No other\n+      \/\/ definition of <vnew> is possible.\n+      \/\/\n+      \/\/ The verifier (in verify_invoke_instructions) will inspect the\n+      \/\/ signature of any attempt to invoke <vnew>, and ensure that it\n+      \/\/ returns non-void.\n+    }\n@@ -2307,3 +2429,24 @@\n-  if (name == vmSymbols::object_initializer_name() && is_interface) {\n-    classfile_parse_error(\"Interface cannot have a method named <init>, class file %s\", THREAD);\n-    return nullptr;\n+  if (name == vmSymbols::object_initializer_name()) {\n+    if (is_interface) {\n+      classfile_parse_error(\"Interface cannot have a method named <init>, class file %s\", CHECK_NULL);\n+    } else if ((!is_value_class || is_abstract_class) && signature->is_void_method_signature()) {\n+      \/\/ OK, a constructor\n+    } else {\n+      \/\/ not OK, so throw the same error as in verify_legal_method_signature.\n+      throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+    }\n+    \/\/ A declared <init> method must always be a non-static\n+    \/\/ object constructor, with a void return.\n+    \/\/\n+    \/\/ The verifier (in verify_invoke_instructions) will inspect the\n+    \/\/ signature of any attempt to invoke <init>, and ensure that it\n+    \/\/ returns void.\n+  }\n+\n+  if (EnableValhalla) {\n+    if (((flags & JVM_ACC_SYNCHRONIZED) == JVM_ACC_SYNCHRONIZED)\n+        && ((flags & JVM_ACC_STATIC) == 0 )\n+        && !carries_identity_modifier()) {\n+      classfile_parse_error(\"Invalid synchronized method in non-identity class %s\", THREAD);\n+        return nullptr;\n+    }\n@@ -2876,0 +3019,2 @@\n+                                    bool is_value_class,\n+                                    bool is_abstract_type,\n@@ -2900,0 +3045,2 @@\n+                                    is_value_class,\n+                                    is_abstract_type,\n@@ -3176,2 +3323,2 @@\n-    \/\/ Access flags\n-    jint flags;\n+\n+    jint recognized_modifiers = RECOGNIZED_INNER_CLASS_MODIFIERS;\n@@ -3180,3 +3327,1 @@\n-      flags = cfs->get_u2_fast() & (RECOGNIZED_INNER_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-    } else {\n-      flags = cfs->get_u2_fast() & RECOGNIZED_INNER_CLASS_MODIFIERS;\n+      recognized_modifiers |= JVM_ACC_MODULE;\n@@ -3184,0 +3329,7 @@\n+    if (supports_inline_types()) {\n+      recognized_modifiers |= JVM_ACC_PRIMITIVE | JVM_ACC_VALUE | JVM_ACC_IDENTITY;\n+    }\n+\n+    \/\/ Access flags\n+    jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+\n@@ -3188,1 +3340,13 @@\n-    verify_legal_class_modifiers(flags, CHECK_0);\n+\n+    if (EnableValhalla) {\n+      if (!supports_inline_types()) {\n+        const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+        const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+        if (!is_module && !is_interface) {\n+          flags |= JVM_ACC_IDENTITY;\n+        }\n+      }\n+    }\n+\n+    const char* name = inner_name_index == 0 ? \"unnamed\" : cp->symbol_at(inner_name_index)->as_utf8();\n+    verify_legal_class_modifiers(flags, name, false, CHECK_0);\n@@ -3292,0 +3456,33 @@\n+u2 ClassFileParser::parse_classfile_preload_attribute(const ClassFileStream* const cfs,\n+                                                                   const u1* const preload_attribute_start,\n+                                                                   TRAPS) {\n+  const u1* const current_mark = cfs->current();\n+  u2 length = 0;\n+  if (preload_attribute_start != nullptr) {\n+    cfs->set_current(preload_attribute_start);\n+    cfs->guarantee_more(2, CHECK_0);  \/\/ length\n+    length = cfs->get_u2_fast();\n+  }\n+  const int size = length;\n+  Array<u2>* const preload_classes = MetadataFactory::new_array<u2>(_loader_data, size, CHECK_0);\n+  _preload_classes = preload_classes;\n+  if (length > 0) {\n+    int index = 0;\n+    cfs->guarantee_more(2 * length, CHECK_0);\n+    for (int n = 0; n < length; n++) {\n+      const u2 class_info_index = cfs->get_u2_fast();\n+      check_property(\n+        valid_klass_reference_at(class_info_index),\n+        \"Preload class_info_index %u has bad constant type in class file %s\",\n+        class_info_index, CHECK_0);\n+      preload_classes->at_put(index++, class_info_index);\n+    }\n+    assert(index == size, \"wrong size\");\n+  }\n+\n+  \/\/ Restore buffer's current position.\n+  cfs->set_current(current_mark);\n+\n+  return length;\n+}\n+\n@@ -3576,0 +3773,2 @@\n+  \/\/ Set _preload_classes attribute to default sentinel\n+  _preload_classes = Universe::the_empty_short_array();\n@@ -3582,0 +3781,1 @@\n+  bool parsed_preload_attribute = false;\n@@ -3607,0 +3807,2 @@\n+  const u1* preload_attribute_start = nullptr;\n+  u4  preload_attribute_length = 0;\n@@ -3833,0 +4035,9 @@\n+            if (EnableValhalla && tag == vmSymbols::tag_preload()) {\n+              if (parsed_preload_attribute) {\n+                classfile_parse_error(\"Multiple Preload attributes in class file %s\", CHECK);\n+                return;\n+              }\n+              parsed_preload_attribute = true;\n+              preload_attribute_start = cfs->current();\n+              preload_attribute_length = attribute_length;\n+            }\n@@ -3913,0 +4124,12 @@\n+  if (parsed_preload_attribute) {\n+    const u2 num_classes = parse_classfile_preload_attribute(\n+                            cfs,\n+                            preload_attribute_start,\n+                            CHECK);\n+    if (_need_verify) {\n+      guarantee_property(\n+        preload_attribute_length == sizeof(num_classes) + sizeof(u2) * num_classes,\n+        \"Wrong Preload attribute length in class file %s\", CHECK);\n+    }\n+  }\n+\n@@ -3977,0 +4200,1 @@\n+  this_klass->set_preload_classes(_preload_classes);\n@@ -4028,2 +4252,1 @@\n-                   \"Invalid superclass index %u in class file %s\",\n-                   super_class_index,\n+                   \"Invalid superclass index 0 in class file %s\",\n@@ -4038,1 +4261,0 @@\n-    bool is_array = false;\n@@ -4041,4 +4263,0 @@\n-      if (need_verify)\n-        is_array = super_klass->is_array_klass();\n-    } else if (need_verify) {\n-      is_array = (cp->klass_name_at(super_class_index)->char_at(0) == JVM_SIGNATURE_ARRAY);\n@@ -4047,0 +4265,1 @@\n+      bool is_array = (cp->klass_name_at(super_class_index)->char_at(0) == JVM_SIGNATURE_ARRAY);\n@@ -4170,0 +4389,19 @@\n+void ClassFileParser::throwInlineTypeLimitation(THREAD_AND_LOCATION_DECL,\n+                                                const char* msg,\n+                                                const Symbol* name,\n+                                                const Symbol* sig) const {\n+\n+  ResourceMark rm(THREAD);\n+  if (name == nullptr || sig == nullptr) {\n+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"class: %s - %s\", _class_name->as_C_string(), msg);\n+  }\n+  else {\n+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"\\\"%s\\\" sig: \\\"%s\\\" class: %s - %s\", name->as_C_string(), sig->as_C_string(),\n+        _class_name->as_C_string(), msg);\n+  }\n+}\n+\n@@ -4204,0 +4442,5 @@\n+      if (ik->is_inline_klass()) {\n+        JavaThread *THREAD = JavaThread::current();\n+        throwInlineTypeLimitation(THREAD_AND_LOCATION, \"Inline Types do not support Cloneable\");\n+        return;\n+      }\n@@ -4244,0 +4487,6 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 61.65535 and later\n+  return _major_version > JAVA_21_VERSION ||\n+         (_major_version == JAVA_21_VERSION \/*&& _minor_version == JAVA_PREVIEW_MINOR_VERSION*\/); \/\/ JAVA_PREVIEW_MINOR_VERSION not yet implemented by javac, check JVMS draft\n+}\n+\n@@ -4287,3 +4536,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4310,0 +4560,1 @@\n+\n@@ -4338,0 +4589,10 @@\n+    \/\/ The JVMS says that super classes for value types must not have the ACC_IDENTITY\n+    \/\/ flag set. But, java.lang.Object must still be allowed to be a direct super class\n+    \/\/ for a value classes.  So, it is treated as a special case for now.\n+    if (this_klass->access_flags().is_value_class() &&\n+        super_ik->name() != vmSymbols::java_lang_Object() &&\n+        super_ik->is_identity_class()) {\n+      classfile_icce_error(\"value class %s cannot inherit from class %s\", super_ik, THREAD);\n+      return;\n+    }\n+\n@@ -4521,1 +4782,1 @@\n-void ClassFileParser::verify_legal_class_modifiers(jint flags, TRAPS) const {\n+void ClassFileParser::verify_legal_class_modifiers(jint flags, const char* name, bool is_Object, TRAPS) const {\n@@ -4523,0 +4784,4 @@\n+  const bool is_value_class = (flags & JVM_ACC_VALUE) != 0;\n+  const bool is_primitive_class = (flags & JVM_ACC_PRIMITIVE) != 0;\n+  const bool is_identity_class = (flags & JVM_ACC_IDENTITY) != 0;\n+  const bool is_inner_class = name != nullptr;\n@@ -4534,1 +4799,23 @@\n-  if (!_need_verify) { return; }\n+  if (is_value_class && !EnableValhalla) {\n+      ResourceMark rm(THREAD);\n+      Exceptions::fthrow(\n+        THREAD_AND_LOCATION,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"Class modifier ACC_VALUE in class %s requires option -XX:+EnableValhalla\",\n+        _class_name->as_C_string()\n+      );\n+    return;\n+  }\n+\n+  if (is_primitive_class && !EnablePrimitiveClasses) {\n+      ResourceMark rm(THREAD);\n+      Exceptions::fthrow(\n+        THREAD_AND_LOCATION,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"Class modifier ACC_PRIMITIVE in class %s requires option -XX:+EnablePrimitiveClasses\",\n+        _class_name->as_C_string()\n+      );\n+    return;\n+  }\n+\n+  \/\/ if (!_need_verify) { return; }\n@@ -4546,2 +4833,6 @@\n-      (is_interface && major_gte_1_5 && (is_super || is_enum)) ||\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (is_interface && major_gte_1_5 && ((is_super && (!EnableValhalla || !supports_inline_types())) || is_enum)) ||   \/\/  ACC_SUPER (now ACC_IDENTITY) was illegal for interfaces\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (is_value_class && is_enum) ||\n+      (is_identity_class && is_value_class) ||\n+      (EnableValhalla && supports_inline_types() && !is_module && !is_abstract && !is_Object && !(is_identity_class || is_value_class) && !is_inner_class) ||\n+      (EnablePrimitiveClasses && supports_inline_types() && is_primitive_class && (!is_value_class || !is_final || is_interface || is_abstract))) {\n@@ -4549,7 +4840,21 @@\n-    Exceptions::fthrow(\n-      THREAD_AND_LOCATION,\n-      vmSymbols::java_lang_ClassFormatError(),\n-      \"Illegal class modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags\n-    );\n-    return;\n+    const char* class_note = \"\";\n+    if (is_value_class)  class_note = \" (a value class)\";\n+    if (is_primitive_class)  class_note = \" (a primitive class)\";\n+    if (is_value_class && is_identity_class) class_note = \" (a value and identity class)\";\n+    if (name == nullptr) { \/\/ Not an inner class\n+      Exceptions::fthrow(\n+        THREAD_AND_LOCATION,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"Illegal class modifiers in class %s%s: 0x%X\",\n+        _class_name->as_C_string(), class_note, flags\n+      );\n+      return;\n+    } else {\n+      Exceptions::fthrow(\n+        THREAD_AND_LOCATION,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"Illegal class modifiers in declaration of inner class %s%s of class %s: 0x%X\",\n+        name, class_note, _class_name->as_C_string(), flags\n+      );\n+      return;\n+    }\n@@ -4621,2 +4926,2 @@\n-void ClassFileParser::verify_legal_field_modifiers(jint flags,\n-                                                   bool is_interface,\n+void ClassFileParser:: verify_legal_field_modifiers(jint flags,\n+                                                   AccessFlags class_access_flags,\n@@ -4636,0 +4941,5 @@\n+  const bool is_interface = class_access_flags.is_interface();\n+  const bool is_abstract = class_access_flags.is_abstract();\n+  const bool is_value_class = class_access_flags.is_value_class();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n+\n@@ -4647,0 +4957,6 @@\n+    } else {\n+      if (is_value_class && !is_abstract && !is_static && !is_final) {\n+        is_illegal = true;\n+      } else if (is_abstract && !is_identity_class && !is_static) {\n+        is_illegal = true;\n+      }\n@@ -4662,1 +4978,1 @@\n-                                                    bool is_interface,\n+                                                    AccessFlags class_access_flags,\n@@ -4681,0 +4997,5 @@\n+  const bool is_factory      = (name == vmSymbols::inline_factory_name() && supports_inline_types());\n+  const bool is_interface    = class_access_flags.is_interface();\n+  const bool is_value_class  = class_access_flags.is_value_class();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n+  const bool is_abstract_class = class_access_flags.is_abstract();\n@@ -4684,0 +5005,1 @@\n+  const char* class_note = \"\";\n@@ -4717,1 +5039,7 @@\n-      if (is_initializer) {\n+      if (is_factory) { \/\/ <vnew> factory method\n+        if (is_final || is_synchronized || is_native || !is_static ||\n+            is_abstract || is_bridge) {\n+          is_illegal = true;\n+          class_note = (is_value_class ? \" (a value class)\" : \" (not a value class)\");\n+        }\n+      } else if (is_initializer) {\n@@ -4723,4 +5051,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n-            is_illegal = true;\n+        if (!is_identity_class && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (not an identity class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n+              is_illegal = true;\n+            }\n@@ -4735,5 +5068,14 @@\n-    Exceptions::fthrow(\n-      THREAD_AND_LOCATION,\n-      vmSymbols::java_lang_ClassFormatError(),\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+    if (is_value_class && is_initializer) {\n+      Exceptions::fthrow(\n+        THREAD_AND_LOCATION,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"Method <init> is not allowed in value class %s\",\n+        _class_name->as_C_string());\n+    } else {\n+      Exceptions::fthrow(\n+        THREAD_AND_LOCATION,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+        name->as_C_string(), _class_name->as_C_string(),\n+        class_note, flags);\n+    }\n@@ -4897,1 +5239,12 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_PRIMITIVE_OBJECT:\n+      \/\/ Can't enable this check fully until JDK upgrades the bytecode generators (TODO: JDK-8270852).\n+      \/\/ For now, compare to class file version 51 so old verifier doesn't see Q signatures.\n+      if ( (_major_version < 51 \/* CONSTANT_CLASS_DESCRIPTORS *\/ ) || (!EnablePrimitiveClasses)) {\n+        classfile_parse_error(\"Class name contains illegal Q-signature \"\n+                              \"in descriptor in class file %s, requires option -XX:+EnablePrimitiveClasses\",\n+                              CHECK_0);\n+        return nullptr;\n+      }\n+      \/\/ fall through\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -4908,1 +5261,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -4964,0 +5317,4 @@\n+    } else if ((_major_version >= CONSTANT_CLASS_DESCRIPTORS || _class_name->starts_with(\"jdk\/internal\/reflect\/\"))\n+                   && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; and Q...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -5029,1 +5386,4 @@\n-      if (name == vmSymbols::object_initializer_name() || name == vmSymbols::class_initializer_name()) {\n+      if (name == vmSymbols::object_initializer_name() ||\n+          name == vmSymbols::class_initializer_name()  ||\n+          (EnableValhalla && supports_inline_types() &&\n+          name == vmSymbols::inline_factory_name())) {\n@@ -5061,0 +5421,3 @@\n+  if ((!supports_inline_types() || !EnablePrimitiveClasses) && (signature->is_Q_signature() || signature->is_Q_array_signature())) {\n+    throwIllegalSignature(\"Field\", name, signature, CHECK);\n+  }\n@@ -5088,2 +5451,3 @@\n-  int sig_length = signature->utf8_length();\n-  if (name->utf8_length() > 0 &&\n+  if (!is_value_class()) {\n+    int sig_length = signature->utf8_length();\n+    if (name->utf8_length() > 0 &&\n@@ -5093,1 +5457,2 @@\n-    throwIllegalSignature(\"Method\", name, signature, THREAD);\n+      throwIllegalSignature(\"Method\", name, signature, THREAD);\n+    }\n@@ -5269,1 +5634,0 @@\n-\n@@ -5305,0 +5669,10 @@\n+  if (_field_info->_is_naturally_atomic && ik->is_inline_klass()) {\n+    ik->set_is_naturally_atomic();\n+  }\n+\n+  if (carries_identity_modifier()) {\n+    ik->set_carries_identity_modifier();\n+  } else if (carries_value_modifier()) {\n+    ik->set_carries_value_modifier();\n+  }\n+\n@@ -5306,1 +5680,1 @@\n-  ik->set_static_oop_field_count(_fac->count[STATIC_OOP]);\n+  ik->set_static_oop_field_count(_fac->count[STATIC_OOP] + _fac->count[STATIC_INLINE]);\n@@ -5323,0 +5697,1 @@\n+  assert(nullptr == _preload_classes, \"invariant\");\n@@ -5360,0 +5735,3 @@\n+  if (_is_declared_atomic) {\n+    ik->set_is_declared_atomic();\n+  }\n@@ -5467,0 +5845,27 @@\n+  bool all_fields_empty = true;\n+  for (AllFieldStream fs(ik->fields(), ik->constants()); !fs.done(); fs.next()) {\n+    if (!fs.access_flags().is_static()) {\n+      if (fs.field_descriptor().is_inline_type()) {\n+        Klass* k = _inline_type_field_klasses->at(fs.index());\n+        ik->set_inline_type_field_klass(fs.index(), k);\n+        if (!InlineKlass::cast(k)->is_empty_inline_type()) { all_fields_empty = false; }\n+      } else {\n+        all_fields_empty = false;\n+      }\n+    } else if (is_inline_type() && (fs.name() == vmSymbols::default_value_name())) {\n+      InlineKlass::cast(ik)->set_default_value_offset(ik->field_offset(fs.index()));\n+    }\n+  }\n+\n+  if (_is_empty_inline_type || (is_inline_type() && all_fields_empty)) {\n+    ik->set_is_empty_inline_type();\n+  }\n+\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_alignment(_alignment);\n+    vk->set_first_field_offset(_first_field_offset);\n+    vk->set_exact_size_in_bytes(_exact_size_in_bytes);\n+    InlineKlass::cast(ik)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -5557,0 +5962,1 @@\n+  _preload_classes(nullptr),\n@@ -5559,0 +5965,1 @@\n+  _local_interface_indexes(nullptr),\n@@ -5570,0 +5977,1 @@\n+  _inline_type_field_klasses(nullptr),\n@@ -5597,0 +6005,7 @@\n+  _has_inline_type_fields(false),\n+  _has_nonstatic_fields(false),\n+  _is_empty_inline_type(false),\n+  _is_naturally_atomic(false),\n+  _is_declared_atomic(false),\n+  _carries_value_modifier(false),\n+  _carries_identity_modifier(false),\n@@ -5646,0 +6061,1 @@\n+  _preload_classes = nullptr;\n@@ -5663,0 +6079,4 @@\n+  if (_inline_type_field_klasses != nullptr) {\n+     MetadataFactory::free_array<InlineKlass*>(_loader_data, _inline_type_field_klasses);\n+  }\n+\n@@ -5685,0 +6105,4 @@\n+  if (_preload_classes != nullptr && _preload_classes != Universe::the_empty_short_array()) {\n+    MetadataFactory::free_array<u2>(_loader_data, _preload_classes);\n+  }\n+\n@@ -5769,2 +6193,1 @@\n-  \/\/ Access flags\n-  jint flags;\n+  jint recognized_modifiers = JVM_RECOGNIZED_CLASS_MODIFIERS;\n@@ -5773,3 +6196,1 @@\n-    flags = stream->get_u2_fast() & (JVM_RECOGNIZED_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-  } else {\n-    flags = stream->get_u2_fast() & JVM_RECOGNIZED_CLASS_MODIFIERS;\n+    recognized_modifiers |= JVM_ACC_MODULE;\n@@ -5777,0 +6198,7 @@\n+  \/\/ JVM_ACC_VALUE and JVM_ACC_PRIMITIVE supported version\n+  if (supports_inline_types()) {\n+    recognized_modifiers |= JVM_ACC_PRIMITIVE | JVM_ACC_VALUE;\n+  }\n+\n+  \/\/ Access flags\n+  jint flags = stream->get_u2_fast() & recognized_modifiers;\n@@ -5783,12 +6211,0 @@\n-  verify_legal_class_modifiers(flags, CHECK);\n-\n-  short bad_constant = class_bad_constant_seen();\n-  if (bad_constant != 0) {\n-    \/\/ Do not throw CFE until after the access_flags are checked because if\n-    \/\/ ACC_MODULE is set in the access flags, then NCDFE must be thrown, not CFE.\n-    classfile_parse_error(\"Unknown constant tag %u in class file %s\", bad_constant, THREAD);\n-    return;\n-  }\n-\n-  _access_flags.set_flags(flags);\n-\n@@ -5806,0 +6222,32 @@\n+  bool is_java_lang_Object = class_name_in_cp == vmSymbols::java_lang_Object();\n+\n+  verify_legal_class_modifiers(flags, nullptr, is_java_lang_Object, CHECK);\n+\n+  if (EnableValhalla) {\n+    if(!supports_inline_types()) {\n+      const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+      const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+      if (!is_module && !is_interface && !is_java_lang_Object) {\n+        flags |= JVM_ACC_IDENTITY;\n+      }\n+    }\n+  }\n+\n+  _access_flags.set_flags(flags);\n+\n+  if (EnableValhalla) {\n+    if (_access_flags.is_identity_class()) set_carries_identity_modifier();\n+    if (_access_flags.is_value_class()) set_carries_value_modifier();\n+    if (carries_identity_modifier() && carries_value_modifier()) {\n+      classfile_parse_error(\"Class %s has both ACC_IDENTITY and ACC_VALUE modifiers\", THREAD);\n+    }\n+  }\n+\n+  short bad_constant = class_bad_constant_seen();\n+  if (bad_constant != 0) {\n+    \/\/ Do not throw CFE until after the access_flags are checked because if\n+    \/\/ ACC_MODULE is set in the access flags, then NCDFE must be thrown, not CFE.\n+    classfile_parse_error(\"Unknown constant tag %u in class file %s\", bad_constant, THREAD);\n+    return;\n+  }\n+\n@@ -5883,0 +6331,1 @@\n+                   &_is_declared_atomic,\n@@ -5885,2 +6334,0 @@\n-  assert(_local_interfaces != nullptr, \"invariant\");\n-\n@@ -5890,1 +6337,1 @@\n-               _access_flags.is_interface(),\n+               _access_flags,\n@@ -5901,1 +6348,3 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                is_value_class(),\n+                is_abstract_class(),\n@@ -5981,2 +6430,2 @@\n-                   \"java.lang.Object cannot implement an interface in class file %s\",\n-                   CHECK);\n+        \"java.lang.Object cannot implement an interface in class file %s\",\n+        CHECK);\n@@ -5987,1 +6436,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -6009,0 +6458,9 @@\n+    if (_super_klass->is_interface()) {\n+      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (EnableValhalla) {\n+      check_identity_and_value_modifiers(this, _super_klass, CHECK);\n+    }\n+\n@@ -6012,0 +6470,4 @@\n+    if (_super_klass->is_declared_atomic()) {\n+      _is_declared_atomic = true;\n+    }\n+  }\n@@ -6013,3 +6475,54 @@\n-    if (_super_klass->is_interface()) {\n-      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n-      return;\n+  if (*ForceNonTearable != '\\0') {\n+    \/\/ Allow a command line switch to force the same atomicity property:\n+    const char* class_name_str = _class_name->as_C_string();\n+    if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+      _is_declared_atomic = true;\n+    }\n+  }\n+\n+  int itfs_len = _local_interface_indexes == nullptr ? 0 : _local_interface_indexes->length();\n+  _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n+  if (_local_interface_indexes != nullptr) {\n+    for (int i = 0; i < _local_interface_indexes->length(); i++) {\n+      u2 interface_index = _local_interface_indexes->at(i);\n+      Klass* interf;\n+      if (cp->tag_at(interface_index).is_klass()) {\n+        interf = cp->resolved_klass_at(interface_index);\n+      } else {\n+        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n+\n+        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n+        \/\/ But need to make sure it's not an array type.\n+        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n+                            \"Bad interface name in class file %s\", CHECK);\n+\n+        \/\/ Call resolve_super so class circularity is checked\n+        interf = SystemDictionary::resolve_super_or_fail(\n+                                                  _class_name,\n+                                                  unresolved_klass,\n+                                                  Handle(THREAD, _loader_data->class_loader()),\n+                                                  _protection_domain,\n+                                                  false,\n+                                                  CHECK);\n+      }\n+\n+      if (!interf->is_interface()) {\n+        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n+                          _class_name->as_klass_external_name(),\n+                          interf->external_name(),\n+                          interf->class_in_module_of_loader()));\n+      }\n+\n+      if (EnableValhalla) {\n+        \/\/ Check modifiers and set carries_identity_modifier\/carries_value_modifier flags\n+        check_identity_and_value_modifiers(this, InstanceKlass::cast(interf), CHECK);\n+      }\n+\n+      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+        _has_nonstatic_concrete_methods = true;\n+      }\n+      if (InstanceKlass::cast(interf)->is_declared_atomic()) {\n+        _is_declared_atomic = true;\n+      }\n+      _local_interfaces->at_put(i, InstanceKlass::cast(interf));\n@@ -6018,0 +6531,1 @@\n+  assert(_local_interfaces != nullptr, \"invariant\");\n@@ -6046,1 +6560,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -6052,0 +6566,26 @@\n+\n+  if (EnablePrimitiveClasses) {\n+    _inline_type_field_klasses = MetadataFactory::new_array<InlineKlass*>(_loader_data,\n+                                                   java_fields_count(),\n+                                                   nullptr,\n+                                                   CHECK);\n+    for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_PRIMITIVE_OBJECT && !fs.access_flags().is_static()) {\n+        \/\/ Pre-load inline class\n+        Klass* klass = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+            Handle(THREAD, _loader_data->class_loader()),\n+            _protection_domain, true, CHECK);\n+        assert(klass != nullptr, \"Sanity check\");\n+        if (!klass->access_flags().is_value_class()) {\n+          assert(klass->is_instance_klass(), \"Sanity check\");\n+          ResourceMark rm(THREAD);\n+            THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                      err_msg(\"Class %s expects class %s to be an inline type, but it is not\",\n+                      _class_name->as_C_string(),\n+                      InstanceKlass::cast(klass)->external_name()));\n+        }\n+        _inline_type_field_klasses->at_put(fs.index(), InlineKlass::cast(klass));\n+      }\n+    }\n+  }\n+\n@@ -6054,2 +6594,9 @@\n-                        _parsed_annotations->is_contended(), _field_info);\n-  lb.build_layout();\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      _field_info, _inline_type_field_klasses);\n+  lb.build_layout(CHECK);\n+  if (is_inline_type()) {\n+    _alignment = lb.get_alignment();\n+    _first_field_offset = lb.get_first_field_offset();\n+    _exact_size_in_bytes = lb.get_exact_size_in_byte();\n+  }\n+  _has_inline_type_fields = _field_info->_has_inline_fields;\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":704,"deletions":157,"binary":false,"changes":861,"status":"modified"},{"patch":"@@ -77,0 +77,2 @@\n+  bool  _is_naturally_atomic;\n+  bool _has_inline_fields;\n@@ -130,0 +132,1 @@\n+  Array<u2>* _preload_classes;\n@@ -132,0 +135,1 @@\n+  GrowableArray<u2>* _local_interface_indexes;\n@@ -144,0 +148,1 @@\n+  Array<InlineKlass*>* _inline_type_field_klasses;\n@@ -156,0 +161,4 @@\n+  int _alignment;\n+  int _first_field_offset;\n+  int _exact_size_in_bytes;\n+\n@@ -194,0 +203,8 @@\n+  bool _has_inline_type_fields;\n+  bool _has_nonstatic_fields;\n+  bool _is_empty_inline_type;\n+  bool _is_naturally_atomic;\n+  bool _is_declared_atomic;\n+  bool _carries_value_modifier;      \/\/ Has ACC_VALUE mddifier or one of its super types has\n+  bool _carries_identity_modifier;   \/\/ Has ACC_IDENTITY modifier or one of its super types has\n+\n@@ -241,0 +258,1 @@\n+                        bool* is_declared_atomic,\n@@ -260,1 +278,1 @@\n-                    bool is_interface,\n+                    AccessFlags class_access_flags,\n@@ -270,0 +288,2 @@\n+                       bool is_value_class,\n+                       bool is_abstract_class,\n@@ -276,0 +296,2 @@\n+                     bool is_value_class,\n+                     bool is_abstract_class,\n@@ -330,0 +352,4 @@\n+  u2 parse_classfile_preload_attribute(const ClassFileStream* const cfs,\n+                                                    const u1* const preload_attribute_start,\n+                                                    TRAPS);\n+\n@@ -448,0 +474,5 @@\n+  void throwInlineTypeLimitation(THREAD_AND_LOCATION_DECL,\n+                                 const char* msg,\n+                                 const Symbol* name = NULL,\n+                                 const Symbol* sig  = NULL) const;\n+\n@@ -470,2 +501,4 @@\n-  void verify_legal_class_modifiers(jint flags, TRAPS) const;\n-  void verify_legal_field_modifiers(jint flags, bool is_interface, TRAPS) const;\n+  void verify_legal_class_modifiers(jint flags, const char* name, bool is_Object, TRAPS) const;\n+  void verify_legal_field_modifiers(jint flags,\n+                                    AccessFlags class_access_flags,\n+                                    TRAPS) const;\n@@ -473,1 +506,1 @@\n-                                     bool is_interface,\n+                                     AccessFlags class_access_flags,\n@@ -531,0 +564,3 @@\n+  \/\/ Check if the class file supports inline types\n+  bool supports_inline_types() const;\n+\n@@ -558,0 +594,12 @@\n+  bool is_inline_type() const { return _access_flags.is_value_class() && !_access_flags.is_interface() && !_access_flags.is_abstract(); }\n+  bool is_value_class() const { return _access_flags.is_value_class(); }\n+  bool is_abstract_class() const { return _access_flags.is_abstract(); }\n+  bool is_identity_class() const { return _access_flags.is_identity_class(); }\n+  bool is_value_capable_class() const;\n+  bool has_inline_fields() const { return _has_inline_type_fields; }\n+  bool carries_identity_modifier() const { return _carries_identity_modifier; }\n+  void set_carries_identity_modifier() { _carries_identity_modifier = true; }\n+  bool carries_value_modifier() const { return _carries_value_modifier; }\n+  void set_carries_value_modifier() { _carries_value_modifier = true; }\n+\n+  u2 java_fields_count() const { return _java_fields_count; }\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":52,"deletions":4,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -205,1 +205,1 @@\n-    if (*start == JVM_SIGNATURE_CLASS) {\n+    if (*start == JVM_SIGNATURE_CLASS || *start == JVM_SIGNATURE_PRIMITIVE_OBJECT) {\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -401,0 +402,10 @@\n+void ClassLoaderData::inline_classes_do(void f(InlineKlass*)) {\n+  \/\/ Lock-free access requires load_acquire\n+  for (Klass* k = Atomic::load_acquire(&_klasses); k != NULL; k = k->next_link()) {\n+    if (k->is_inline_klass()) {\n+      f(InlineKlass::cast(k));\n+    }\n+    assert(k != k->next_link(), \"no loops!\");\n+  }\n+}\n+\n@@ -555,0 +566,2 @@\n+  inline_classes_do(InlineKlass::cleanup);\n+\n@@ -852,1 +865,5 @@\n-        MetadataFactory::free_metadata(this, (InstanceKlass*)m);\n+        if (!((Klass*)m)->is_inline_klass()) {\n+          MetadataFactory::free_metadata(this, (InstanceKlass*)m);\n+        } else {\n+          MetadataFactory::free_metadata(this, (InlineKlass*)m);\n+        }\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.cpp","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -187,0 +187,1 @@\n+  void inline_classes_do(void f(InlineKlass*));\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n@@ -35,0 +37,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -37,1 +40,0 @@\n-\n@@ -40,0 +42,1 @@\n+  _inline_klass(nullptr),\n@@ -56,0 +59,1 @@\n+ _inline_klass(nullptr),\n@@ -62,1 +66,1 @@\n-  assert(kind == REGULAR || kind == FLATTENED || kind == INHERITED,\n+  assert(kind == REGULAR || kind == INLINED || kind == INHERITED,\n@@ -78,1 +82,2 @@\n-  _primitive_fields(nullptr),\n+  _small_primitive_fields(nullptr),\n+  _big_primitive_fields(nullptr),\n@@ -86,2 +91,4 @@\n-  if (_primitive_fields == nullptr) {\n-    _primitive_fields = new GrowableArray<LayoutRawBlock*>(INITIAL_LIST_SIZE);\n+  if (size >= oopSize) {\n+    add_to_big_primitive_list(block);\n+  } else {\n+    add_to_small_primitive_list(block);\n@@ -89,1 +96,0 @@\n-  _primitive_fields->append(block);\n@@ -102,0 +108,10 @@\n+void FieldGroup::add_inlined_field(AllFieldStream fs, InlineKlass* vk) {\n+  LayoutRawBlock* block = new LayoutRawBlock(fs.index(), LayoutRawBlock::INLINED, vk->get_exact_size_in_bytes(), vk->get_alignment(), false);\n+  block->set_inline_klass(vk);\n+  if (block->size() >= oopSize) {\n+    add_to_big_primitive_list(block);\n+  } else {\n+    add_to_small_primitive_list(block);\n+  }\n+}\n+\n@@ -103,2 +119,5 @@\n-  if (_primitive_fields != nullptr) {\n-    _primitive_fields->sort(LayoutRawBlock::compare_size_inverted);\n+  if (_small_primitive_fields != nullptr) {\n+    _small_primitive_fields->sort(LayoutRawBlock::compare_size_inverted);\n+  }\n+  if (_big_primitive_fields != nullptr) {\n+    _big_primitive_fields->sort(LayoutRawBlock::compare_size_inverted);\n@@ -108,0 +127,14 @@\n+void FieldGroup::add_to_small_primitive_list(LayoutRawBlock* block) {\n+  if (_small_primitive_fields == nullptr) {\n+    _small_primitive_fields = new GrowableArray<LayoutRawBlock*>(INITIAL_LIST_SIZE);\n+  }\n+  _small_primitive_fields->append(block);\n+}\n+\n+void FieldGroup::add_to_big_primitive_list(LayoutRawBlock* block) {\n+  if (_big_primitive_fields == nullptr) {\n+    _big_primitive_fields = new GrowableArray<LayoutRawBlock*>(INITIAL_LIST_SIZE);\n+  }\n+  _big_primitive_fields->append(block);\n+}\n+\n@@ -141,1 +174,2 @@\n-      _start = _blocks;  \/\/ start allocating fields from the first empty block\n+      _start = _blocks; \/\/ Setting _start to _blocks instead of _last would allow subclasses\n+      \/\/ to allocate fields in empty slots of their super classes\n@@ -149,3 +183,5 @@\n-  LayoutRawBlock* block = _start;\n-  while (block->kind() != LayoutRawBlock::INHERITED && block->kind() != LayoutRawBlock::REGULAR\n-      && block->kind() != LayoutRawBlock::FLATTENED && block->kind() != LayoutRawBlock::PADDING) {\n+  LayoutRawBlock* block = _blocks;\n+  while (block != nullptr\n+         && block->kind() != LayoutRawBlock::INHERITED\n+         && block->kind() != LayoutRawBlock::REGULAR\n+         && block->kind() != LayoutRawBlock::INLINED) {\n@@ -157,3 +193,2 @@\n-\n-\/\/ Insert a set of fields into a layout using a best-fit strategy.\n-\/\/ For each field, search for the smallest empty slot able to fit the field\n+\/\/ Insert a set of fields into a layout.\n+\/\/ For each field, search for an empty slot able to fit the field\n@@ -173,1 +208,0 @@\n-\n@@ -191,0 +225,1 @@\n+\n@@ -207,1 +242,0 @@\n-\n@@ -306,3 +340,11 @@\n-      int size = type2aelembytes(type);\n-      \/\/ INHERITED blocks are marked as non-reference because oop_maps are handled by their holder class\n-      LayoutRawBlock* block = new LayoutRawBlock(fs.index(), LayoutRawBlock::INHERITED, size, size, false);\n+      LayoutRawBlock* block;\n+      if (type == T_PRIMITIVE_OBJECT) {\n+        InlineKlass* vk = InlineKlass::cast(ik->get_inline_type_field_klass(fs.index()));\n+        block = new LayoutRawBlock(fs.index(), LayoutRawBlock::INHERITED, vk->get_exact_size_in_bytes(),\n+                                   vk->get_alignment(), false);\n+\n+      } else {\n+        int size = type2aelembytes(type);\n+        \/\/ INHERITED blocks are marked as non-reference because oop_maps are handled by their holder class\n+        block = new LayoutRawBlock(fs.index(), LayoutRawBlock::INHERITED, size, size, false);\n+      }\n@@ -314,1 +356,0 @@\n-\n@@ -319,1 +360,0 @@\n-\n@@ -355,1 +395,0 @@\n-\n@@ -366,1 +405,0 @@\n-\n@@ -380,1 +418,0 @@\n-\n@@ -432,47 +469,46 @@\n-      case LayoutRawBlock::REGULAR: {\n-        FieldInfo* fi = FieldInfo::from_field_array(_fields, b->field_index());\n-        output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n-                         b->offset(),\n-                         fi->name(_cp)->as_C_string(),\n-                         fi->signature(_cp)->as_C_string(),\n-                         b->size(),\n-                         b->alignment(),\n-                         \"REGULAR\");\n-        break;\n-      }\n-      case LayoutRawBlock::FLATTENED: {\n-        FieldInfo* fi = FieldInfo::from_field_array(_fields, b->field_index());\n-        output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n-                         b->offset(),\n-                         fi->name(_cp)->as_C_string(),\n-                         fi->signature(_cp)->as_C_string(),\n-                         b->size(),\n-                         b->alignment(),\n-                         \"FLATTENED\");\n-        break;\n-      }\n-      case LayoutRawBlock::RESERVED: {\n-        output->print_cr(\" @%d %d\/- %s\",\n-                         b->offset(),\n-                         b->size(),\n-                         \"RESERVED\");\n-        break;\n-      }\n-      case LayoutRawBlock::INHERITED: {\n-        assert(!is_static, \"Static fields are not inherited in layouts\");\n-        assert(super != nullptr, \"super klass must be provided to retrieve inherited fields info\");\n-        bool found = false;\n-        const InstanceKlass* ik = super;\n-        while (!found && ik != nullptr) {\n-          for (AllFieldStream fs(ik->fields(), ik->constants()); !fs.done(); fs.next()) {\n-            if (fs.offset() == b->offset()) {\n-              output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n-                  b->offset(),\n-                  fs.name()->as_C_string(),\n-                  fs.signature()->as_C_string(),\n-                  b->size(),\n-                  b->size(), \/\/ so far, alignment constraint == size, will change with Valhalla\n-                  \"INHERITED\");\n-              found = true;\n-              break;\n-            }\n+    case LayoutRawBlock::REGULAR: {\n+      FieldInfo* fi = FieldInfo::from_field_array(_fields, b->field_index());\n+      output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n+                       b->offset(),\n+                       fi->name(_cp)->as_C_string(),\n+                       fi->signature(_cp)->as_C_string(),\n+                       b->size(),\n+                       b->alignment(),\n+                       \"REGULAR\");\n+      break;\n+    }\n+    case LayoutRawBlock::INLINED: {\n+      FieldInfo* fi = FieldInfo::from_field_array(_fields, b->field_index());\n+      output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n+                       b->offset(),\n+                       fi->name(_cp)->as_C_string(),\n+                       fi->signature(_cp)->as_C_string(),\n+                       b->size(),\n+                       b->alignment(),\n+                       \"INLINED\");\n+      break;\n+    }\n+    case LayoutRawBlock::RESERVED: {\n+      output->print_cr(\" @%d %d\/- %s\",\n+                       b->offset(),\n+                       b->size(),\n+                       \"RESERVED\");\n+      break;\n+    }\n+    case LayoutRawBlock::INHERITED: {\n+      assert(!is_static, \"Static fields are not inherited in layouts\");\n+      assert(super != nullptr, \"super klass must be provided to retrieve inherited fields info\");\n+      bool found = false;\n+      const InstanceKlass* ik = super;\n+      while (!found && ik != nullptr) {\n+        for (AllFieldStream fs(ik->fields(), ik->constants()); !fs.done(); fs.next()) {\n+          if (fs.offset() == b->offset()) {\n+            output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n+                b->offset(),\n+                fs.name()->as_C_string(),\n+                fs.signature()->as_C_string(),\n+                b->size(),\n+                b->size(), \/\/ so far, alignment constraint == size, will change with Valhalla\n+                \"INHERITED\");\n+            found = true;\n+            break;\n@@ -480,2 +516,1 @@\n-          ik = ik->java_super();\n-        break;\n+        ik = ik->java_super();\n@@ -484,12 +519,14 @@\n-      case LayoutRawBlock::EMPTY:\n-        output->print_cr(\" @%d %d\/1 %s\",\n-                         b->offset(),\n-                         b->size(),\n-                        \"EMPTY\");\n-        break;\n-      case LayoutRawBlock::PADDING:\n-        output->print_cr(\" @%d %d\/1 %s\",\n-                         b->offset(),\n-                         b->size(),\n-                        \"PADDING\");\n-        break;\n+      break;\n+    }\n+    case LayoutRawBlock::EMPTY:\n+      output->print_cr(\" @%d %d\/1 %s\",\n+                       b->offset(),\n+                       b->size(),\n+                       \"EMPTY\");\n+      break;\n+    case LayoutRawBlock::PADDING:\n+      output->print_cr(\" @%d %d\/1 %s\",\n+                       b->offset(),\n+                       b->size(),\n+                       \"PADDING\");\n+      break;\n@@ -502,1 +539,2 @@\n-      Array<u2>* fields, bool is_contended, FieldLayoutInfo* info) :\n+                                       Array<u2>* fields, bool is_contended, bool is_inline_type,\n+                                       FieldLayoutInfo* info, Array<InlineKlass*>* inline_type_field_klasses) :\n@@ -508,0 +546,1 @@\n+  _inline_type_field_klasses(inline_type_field_klasses),\n@@ -515,0 +554,2 @@\n+  _first_field_offset(-1),\n+  _exact_size_in_bytes(-1),\n@@ -516,2 +557,7 @@\n-  _is_contended(is_contended) {}\n-\n+  _has_inline_type_fields(false),\n+  _is_contended(is_contended),\n+  _is_inline_type(is_inline_type),\n+  _has_flattening_information(is_inline_type),\n+  _has_nonatomic_values(false),\n+  _atomic_field_count(0)\n+ {}\n@@ -544,1 +590,1 @@\n-\/\/ Field sorting for regular classes:\n+\/\/ Field sorting for regular (non-inline) classes:\n@@ -549,0 +595,1 @@\n+\/\/   - field flattening decisions are taken in this method\n@@ -556,0 +603,1 @@\n+      _atomic_field_count++;  \/\/ we might decrement this\n@@ -571,13 +619,19 @@\n-      case T_BYTE:\n-      case T_CHAR:\n-      case T_DOUBLE:\n-      case T_FLOAT:\n-      case T_INT:\n-      case T_LONG:\n-      case T_SHORT:\n-      case T_BOOLEAN:\n-        group->add_primitive_field(fs, type);\n-        break;\n-      case T_OBJECT:\n-      case T_ARRAY:\n-        if (group != _static_fields) _nonstatic_oopmap_count++;\n+    case T_BYTE:\n+    case T_CHAR:\n+    case T_DOUBLE:\n+    case T_FLOAT:\n+    case T_INT:\n+    case T_LONG:\n+    case T_SHORT:\n+    case T_BOOLEAN:\n+      group->add_primitive_field(fs, type);\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY:\n+      if (group != _static_fields) _nonstatic_oopmap_count++;\n+      group->add_oop_field(fs);\n+      break;\n+    case T_PRIMITIVE_OBJECT:\n+      _has_inline_type_fields = true;\n+      if (group == _static_fields) {\n+        \/\/ static fields are never inlined\n@@ -585,3 +639,34 @@\n-        break;\n-      default:\n-        fatal(\"Something wrong?\");\n+      } else {\n+        _has_flattening_information = true;\n+        \/\/ Flattening decision to be taken here\n+        \/\/ This code assumes all verification already have been performed\n+        \/\/ (field's type has been loaded and it is an inline klass)\n+        JavaThread* THREAD = JavaThread::current();\n+        Klass* klass =  _inline_type_field_klasses->at(fs.index());\n+        assert(klass != nullptr, \"Sanity check\");\n+        InlineKlass* vk = InlineKlass::cast(klass);\n+        bool too_big_to_flatten = (InlineFieldMaxFlatSize >= 0 &&\n+                                   (vk->size_helper() * HeapWordSize) > InlineFieldMaxFlatSize);\n+        bool too_atomic_to_flatten = vk->is_declared_atomic() || AlwaysAtomicAccesses;\n+        bool too_volatile_to_flatten = fs.access_flags().is_volatile();\n+        if (vk->is_naturally_atomic()) {\n+          too_atomic_to_flatten = false;\n+          \/\/too_volatile_to_flatten = false; \/\/FIXME\n+          \/\/ volatile fields are currently never inlined, this could change in the future\n+        }\n+        if (!(too_big_to_flatten | too_atomic_to_flatten | too_volatile_to_flatten) || fs.access_flags().is_final()) {\n+          group->add_inlined_field(fs, vk);\n+          _nonstatic_oopmap_count += vk->nonstatic_oop_map_count();\n+          fs.set_inlined(true);\n+          if (!vk->is_atomic()) {  \/\/ flat and non-atomic: take note\n+            _has_nonatomic_values = true;\n+            _atomic_field_count--;  \/\/ every other field is atomic but this one\n+          }\n+        } else {\n+          _nonstatic_oopmap_count++;\n+          group->add_oop_field(fs);\n+        }\n+      }\n+      break;\n+    default:\n+      fatal(\"Something wrong?\");\n@@ -599,0 +684,103 @@\n+\/* Field sorting for inline classes:\n+ *   - because inline classes are immutable, the @Contended annotation is ignored\n+ *     when computing their layout (with only read operation, there's no false\n+ *     sharing issue)\n+ *   - this method also records the alignment of the field with the most\n+ *     constraining alignment, this value is then used as the alignment\n+ *     constraint when flattening this inline type into another container\n+ *   - field flattening decisions are taken in this method (those decisions are\n+ *     currently only based in the size of the fields to be inlined, the size\n+ *     of the resulting instance is not considered)\n+ *\/\n+void FieldLayoutBuilder::inline_class_field_sorting(TRAPS) {\n+  assert(_is_inline_type, \"Should only be used for inline classes\");\n+  int alignment = 1;\n+  for (AllFieldStream fs(_fields, _constant_pool); !fs.done(); fs.next()) {\n+    FieldGroup* group = nullptr;\n+    int field_alignment = 1;\n+    if (fs.access_flags().is_static()) {\n+      group = _static_fields;\n+    } else {\n+      _has_nonstatic_fields = true;\n+      _atomic_field_count++;  \/\/ we might decrement this\n+      group = _root_group;\n+    }\n+    assert(group != nullptr, \"invariant\");\n+    BasicType type = Signature::basic_type(fs.signature());\n+    switch(type) {\n+    case T_BYTE:\n+    case T_CHAR:\n+    case T_DOUBLE:\n+    case T_FLOAT:\n+    case T_INT:\n+    case T_LONG:\n+    case T_SHORT:\n+    case T_BOOLEAN:\n+      if (group != _static_fields) {\n+        field_alignment = type2aelembytes(type); \/\/ alignment == size for primitive types\n+      }\n+      group->add_primitive_field(fs, type);\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY:\n+      if (group != _static_fields) {\n+        _nonstatic_oopmap_count++;\n+        field_alignment = type2aelembytes(type); \/\/ alignment == size for oops\n+      }\n+      group->add_oop_field(fs);\n+      break;\n+    case T_PRIMITIVE_OBJECT: {\n+\/\/      fs.set_inline(true);\n+      _has_inline_type_fields = true;\n+      if (group == _static_fields) {\n+        \/\/ static fields are never inlined\n+        group->add_oop_field(fs);\n+      } else {\n+        \/\/ Flattening decision to be taken here\n+        \/\/ This code assumes all verifications have already been performed\n+        \/\/ (field's type has been loaded and it is an inline klass)\n+        JavaThread* THREAD = JavaThread::current();\n+        Klass* klass =  _inline_type_field_klasses->at(fs.index());\n+        assert(klass != nullptr, \"Sanity check\");\n+        InlineKlass* vk = InlineKlass::cast(klass);\n+        bool too_big_to_flatten = (InlineFieldMaxFlatSize >= 0 &&\n+                                   (vk->size_helper() * HeapWordSize) > InlineFieldMaxFlatSize);\n+        bool too_atomic_to_flatten = vk->is_declared_atomic() || AlwaysAtomicAccesses;\n+        bool too_volatile_to_flatten = fs.access_flags().is_volatile();\n+        if (vk->is_naturally_atomic()) {\n+          too_atomic_to_flatten = false;\n+          \/\/too_volatile_to_flatten = false; \/\/FIXME\n+          \/\/ volatile fields are currently never inlined, this could change in the future\n+        }\n+        if (!(too_big_to_flatten | too_atomic_to_flatten | too_volatile_to_flatten) || fs.access_flags().is_final()) {\n+          group->add_inlined_field(fs, vk);\n+          _nonstatic_oopmap_count += vk->nonstatic_oop_map_count();\n+          field_alignment = vk->get_alignment();\n+          fs.set_inlined(true);\n+          if (!vk->is_atomic()) {  \/\/ flat and non-atomic: take note\n+            _has_nonatomic_values = true;\n+            _atomic_field_count--;  \/\/ every other field is atomic but this one\n+          }\n+        } else {\n+          _nonstatic_oopmap_count++;\n+          field_alignment = type2aelembytes(T_OBJECT);\n+          group->add_oop_field(fs);\n+        }\n+      }\n+      break;\n+    }\n+    default:\n+      fatal(\"Unexpected BasicType\");\n+    }\n+    if (!fs.access_flags().is_static() && field_alignment > alignment) alignment = field_alignment;\n+  }\n+  _alignment = alignment;\n+  if (!_has_nonstatic_fields) {\n+    \/\/ There are a number of fixes required throughout the type system and JIT\n+    Exceptions::fthrow(THREAD_AND_LOCATION,\n+                       vmSymbols::java_lang_ClassFormatError(),\n+                       \"Value Types do not support zero instance size yet\");\n+    return;\n+  }\n+}\n+\n@@ -606,5 +794,7 @@\n-\/\/ Computation of regular classes layout is an evolution of the previous default layout\n-\/\/ (FieldAllocationStyle 1):\n-\/\/   - primitive fields are allocated first (from the biggest to the smallest)\n-\/\/   - then oop fields are allocated, either in existing gaps or at the end of\n-\/\/     the layout\n+\/* Computation of regular classes layout is an evolution of the previous default layout\n+ * (FieldAllocationStyle 1):\n+ *   - primitive fields (both primitive types and flattened inline types) are allocated\n+ *     first, from the biggest to the smallest\n+ *   - then oop fields are allocated (to increase chances to have contiguous oops and\n+ *     a simpler oopmap).\n+ *\/\n@@ -615,1 +805,0 @@\n-\n@@ -623,1 +812,2 @@\n-  _layout->add(_root_group->primitive_fields());\n+  _layout->add(_root_group->big_primitive_fields());\n+  _layout->add(_root_group->small_primitive_fields());\n@@ -631,1 +821,2 @@\n-      _layout->add(cg->primitive_fields(), start);\n+      _layout->add(cg->big_primitive_fields());\n+      _layout->add(cg->small_primitive_fields(), start);\n@@ -640,0 +831,4 @@\n+  \/\/ Warning: IntanceMirrorKlass expects static oops to be allocated first\n+  _static_layout->add_contiguously(_static_fields->oop_fields());\n+  _static_layout->add(_static_fields->big_primitive_fields());\n+  _static_layout->add(_static_fields->small_primitive_fields());\n@@ -641,2 +836,49 @@\n-  _static_layout->add_contiguously(this->_static_fields->oop_fields());\n-  _static_layout->add(this->_static_fields->primitive_fields());\n+  epilogue();\n+}\n+\n+\/* Computation of inline classes has a slightly different strategy than for\n+ * regular classes. Regular classes have their oop fields allocated at the end\n+ * of the layout to increase GC performances. Unfortunately, this strategy\n+ * increases the number of empty slots inside an instance. Because the purpose\n+ * of inline classes is to be embedded into other containers, it is critical\n+ * to keep their size as small as possible. For this reason, the allocation\n+ * strategy is:\n+ *   - big primitive fields (primitive types and flattened inline type smaller\n+ *     than an oop) are allocated first (from the biggest to the smallest)\n+ *   - then oop fields\n+ *   - then small primitive fields (from the biggest to the smallest)\n+ *\/\n+void FieldLayoutBuilder::compute_inline_class_layout(TRAPS) {\n+  prologue();\n+  inline_class_field_sorting(CHECK);\n+  \/\/ Inline types are not polymorphic, so they cannot inherit fields.\n+  \/\/ By consequence, at this stage, the layout must be composed of a RESERVED\n+  \/\/ block, followed by an EMPTY block.\n+  assert(_layout->start()->kind() == LayoutRawBlock::RESERVED, \"Unexpected\");\n+  assert(_layout->start()->next_block()->kind() == LayoutRawBlock::EMPTY, \"Unexpected\");\n+  LayoutRawBlock* first_empty = _layout->start()->next_block();\n+  if (first_empty->offset() % _alignment != 0) {\n+    LayoutRawBlock* padding = new LayoutRawBlock(LayoutRawBlock::PADDING, _alignment - (first_empty->offset() % _alignment));\n+    _layout->insert(first_empty, padding);\n+    _layout->set_start(padding->next_block());\n+  }\n+\n+  _layout->add(_root_group->big_primitive_fields());\n+  _layout->add(_root_group->oop_fields());\n+  _layout->add(_root_group->small_primitive_fields());\n+\n+  LayoutRawBlock* first_field = _layout->first_field_block();\n+   if (first_field != nullptr) {\n+     _first_field_offset = _layout->first_field_block()->offset();\n+     _exact_size_in_bytes = _layout->last_block()->offset() - _layout->first_field_block()->offset();\n+   } else {\n+     \/\/ special case for empty value types\n+     _first_field_offset = _layout->blocks()->size();\n+     _exact_size_in_bytes = 0;\n+   }\n+  _exact_size_in_bytes = _layout->last_block()->offset() - _layout->first_field_block()->offset();\n+\n+  \/\/ Warning:: InstanceMirrorKlass expects static oops to be allocated first\n+  _static_layout->add_contiguously(_static_fields->oop_fields());\n+  _static_layout->add(_static_fields->big_primitive_fields());\n+  _static_layout->add(_static_fields->small_primitive_fields());\n@@ -647,0 +889,37 @@\n+void FieldLayoutBuilder::add_inlined_field_oopmap(OopMapBlocksBuilder* nonstatic_oop_maps,\n+                InlineKlass* vklass, int offset) {\n+  int diff = offset - vklass->first_field_offset();\n+  const OopMapBlock* map = vklass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* last_map = map + vklass->nonstatic_oop_map_count();\n+  while (map < last_map) {\n+    nonstatic_oop_maps->add(map->offset() + diff, map->count());\n+    map++;\n+  }\n+}\n+\n+void FieldLayoutBuilder::register_embedded_oops_from_list(OopMapBlocksBuilder* nonstatic_oop_maps, GrowableArray<LayoutRawBlock*>* list) {\n+  if (list != nullptr) {\n+    for (int i = 0; i < list->length(); i++) {\n+      LayoutRawBlock* f = list->at(i);\n+      if (f->kind() == LayoutRawBlock::INLINED) {\n+        InlineKlass* vk = f->inline_klass();\n+        assert(vk != nullptr, \"Should have been initialized\");\n+        if (vk->contains_oops()) {\n+          add_inlined_field_oopmap(nonstatic_oop_maps, vk, f->offset());\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+void FieldLayoutBuilder::register_embedded_oops(OopMapBlocksBuilder* nonstatic_oop_maps, FieldGroup* group) {\n+  if (group->oop_fields() != nullptr) {\n+    for (int i = 0; i < group->oop_fields()->length(); i++) {\n+      LayoutRawBlock* b = group->oop_fields()->at(i);\n+      nonstatic_oop_maps->add(b->offset(), 1);\n+    }\n+  }\n+  register_embedded_oops_from_list(nonstatic_oop_maps, group->big_primitive_fields());\n+  register_embedded_oops_from_list(nonstatic_oop_maps, group->small_primitive_fields());\n+}\n+\n@@ -651,1 +930,0 @@\n-\n@@ -658,8 +936,1 @@\n-\n-  if (_root_group->oop_fields() != nullptr) {\n-    for (int i = 0; i < _root_group->oop_fields()->length(); i++) {\n-      LayoutRawBlock* b = _root_group->oop_fields()->at(i);\n-      nonstatic_oop_maps->add(b->offset(), 1);\n-    }\n-  }\n-\n+  register_embedded_oops(nonstatic_oop_maps, _root_group);\n@@ -671,1 +942,1 @@\n-        nonstatic_oop_maps->add(cg->oop_fields()->at(0)->offset(), cg->oop_count());\n+        register_embedded_oops(nonstatic_oop_maps, cg);\n@@ -675,1 +946,0 @@\n-\n@@ -691,2 +961,16 @@\n-\n-  if (PrintFieldLayout) {\n+  _info->_has_inline_fields = _has_inline_type_fields;\n+\n+  \/\/ An inline type is naturally atomic if it has just one field, and\n+  \/\/ that field is simple enough.\n+  _info->_is_naturally_atomic = (_is_inline_type &&\n+                                 (_atomic_field_count <= 1) &&\n+                                 !_has_nonatomic_values &&\n+                                 _contended_groups.is_empty());\n+  \/\/ This may be too restrictive, since if all the fields fit in 64\n+  \/\/ bits we could make the decision to align instances of this class\n+  \/\/ to 64-bit boundaries, and load and store them as single words.\n+  \/\/ And on machines which supported larger atomics we could similarly\n+  \/\/ allow larger values to be atomic, if properly aligned.\n+\n+\n+  if (PrintFieldLayout || (PrintInlineLayout && _has_flattening_information)) {\n@@ -700,0 +984,5 @@\n+    if (_is_inline_type) {\n+      tty->print_cr(\"First field offset = %d\", _first_field_offset);\n+      tty->print_cr(\"Alignment = %d bytes\", _alignment);\n+      tty->print_cr(\"Exact size = %d bytes\", _exact_size_in_bytes);\n+    }\n@@ -704,2 +993,6 @@\n-void FieldLayoutBuilder::build_layout() {\n-  compute_regular_layout();\n+void FieldLayoutBuilder::build_layout(TRAPS) {\n+  if (_is_inline_type) {\n+    compute_inline_class_layout(CHECK);\n+  } else {\n+    compute_regular_layout();\n+  }\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.cpp","additions":424,"deletions":131,"binary":false,"changes":555,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-\n@@ -57,6 +56,6 @@\n-    EMPTY,         \/\/ empty slot, space is taken from this to allocate fields\n-    RESERVED,      \/\/ reserved for JVM usage (for instance object header)\n-    PADDING,       \/\/ padding (because of alignment constraints or @Contended)\n-    REGULAR,       \/\/ primitive or oop field (including non-flattened inline fields)\n-    FLATTENED,     \/\/ flattened field\n-    INHERITED      \/\/ field(s) inherited from super classes\n+    EMPTY,            \/\/ empty slot, space is taken from this to allocate fields\n+    RESERVED,         \/\/ reserved for JVM usage (for instance object header)\n+    PADDING,          \/\/ padding (because of alignment constraints or @Contended)\n+    REGULAR,          \/\/ primitive or oop field (including inline type fields not inlined)\n+    INLINED,          \/\/ field inlined\n+    INHERITED         \/\/ field(s) inherited from super classes\n@@ -68,0 +67,1 @@\n+  InlineKlass* _inline_klass;\n@@ -96,0 +96,5 @@\n+  InlineKlass* inline_klass() const {\n+    assert(_inline_klass != nullptr, \"Must be initialized\");\n+    return _inline_klass;\n+  }\n+  void set_inline_klass(InlineKlass* inline_klass) { _inline_klass = inline_klass; }\n@@ -116,1 +121,0 @@\n-\n@@ -122,1 +126,1 @@\n-\/\/ oop, or flattened.\n+\/\/ oop, or inlined.\n@@ -128,1 +132,3 @@\n-  GrowableArray<LayoutRawBlock*>* _primitive_fields;\n+\n+  GrowableArray<LayoutRawBlock*>* _small_primitive_fields;\n+  GrowableArray<LayoutRawBlock*>* _big_primitive_fields;\n@@ -139,1 +145,2 @@\n-  GrowableArray<LayoutRawBlock*>* primitive_fields() const { return _primitive_fields; }\n+  GrowableArray<LayoutRawBlock*>* small_primitive_fields() const { return _small_primitive_fields; }\n+  GrowableArray<LayoutRawBlock*>* big_primitive_fields() const { return _big_primitive_fields; }\n@@ -146,0 +153,2 @@\n+  void add_inlined_field(AllFieldStream fs, InlineKlass* vk);\n+  void add_block(LayoutRawBlock** list, LayoutRawBlock* block);\n@@ -147,0 +156,3 @@\n+ private:\n+  void add_to_small_primitive_list(LayoutRawBlock* block);\n+  void add_to_big_primitive_list(LayoutRawBlock* block);\n@@ -186,0 +198,2 @@\n+  LayoutRawBlock* blocks() { return _blocks; }\n+\n@@ -204,5 +218,4 @@\n-\/\/ This class has three methods to generate layout: one for regular classes\n-\/\/ and two for classes with hard coded offsets (java,lang.ref.Reference\n-\/\/ and the boxing classes). The rationale for having multiple methods\n-\/\/ is that each kind of class has a different set goals regarding\n-\/\/ its layout, so instead of mixing several layout strategies into a\n+\/\/ This class has two methods to generate layout: one for identity classes\n+\/\/ and one for inline classes. The rational for having two methods\n+\/\/ is that each kind of classes has a different set goals regarding\n+\/\/ its layout, so instead of mixing two layout strategies into a\n@@ -225,1 +238,1 @@\n-\/\/  can vary with the allocation strategy.\n+\/\/  differ for inline classes and identity classes.\n@@ -229,1 +242,0 @@\n-\n@@ -235,0 +247,1 @@\n+  Array<InlineKlass*>* _inline_type_field_klasses;\n@@ -242,0 +255,2 @@\n+  int _first_field_offset;\n+  int _exact_size_in_bytes;\n@@ -243,1 +258,8 @@\n-  bool _is_contended; \/\/ is a contended class?\n+  bool _has_inline_type_fields;\n+  bool _is_contended;\n+  bool _is_inline_type;\n+  bool _has_flattening_information;\n+  bool _has_nonatomic_values;\n+  int _atomic_field_count;\n+\n+  FieldGroup* get_or_create_contended_group(int g);\n@@ -247,1 +269,1 @@\n-                     Array<u2>* fields, bool is_contended, FieldLayoutInfo* info);\n+      Array<u2>* fields, bool is_contended, bool is_inline_type, FieldLayoutInfo* info, Array<InlineKlass*>* inline_type_field_klasses);\n@@ -254,1 +276,11 @@\n-  void build_layout();\n+  int get_first_field_offset() {\n+    assert(_first_field_offset != -1, \"Uninitialized\");\n+    return _first_field_offset;\n+  }\n+\n+  int get_exact_size_in_byte() {\n+    assert(_exact_size_in_bytes != -1, \"Uninitialized\");\n+    return _exact_size_in_bytes;\n+  }\n+\n+  void build_layout(TRAPS);\n@@ -256,0 +288,1 @@\n+  void compute_inline_class_layout(TRAPS);\n@@ -258,1 +291,1 @@\n- private:\n+ protected:\n@@ -262,1 +295,4 @@\n-  FieldGroup* get_or_create_contended_group(int g);\n+  void inline_class_field_sorting(TRAPS);\n+  void add_inlined_field_oopmap(OopMapBlocksBuilder* nonstatic_oop_map, InlineKlass* vk, int offset);\n+  void register_embedded_oops_from_list(OopMapBlocksBuilder* nonstatic_oop_maps, GrowableArray<LayoutRawBlock*>* list);\n+  void register_embedded_oops(OopMapBlocksBuilder* nonstatic_oop_maps, FieldGroup* group);\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.hpp","additions":59,"deletions":23,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -54,0 +54,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -55,1 +57,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -783,0 +785,2 @@\n+int java_lang_Class::_primary_mirror_offset;\n+int java_lang_Class::_secondary_mirror_offset;\n@@ -967,1 +971,10 @@\n-    if (k->is_typeArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+      assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+      if (is_scratch) {\n+        comp_mirror = Handle(THREAD, HeapShared::scratch_java_mirror(element_klass));\n+      } else {\n+        InlineKlass* vk = InlineKlass::cast(element_klass);\n+        comp_mirror = Handle(THREAD, vk->val_mirror());\n+      }\n+    } else if (k->is_typeArray_klass()) {\n@@ -978,0 +991,5 @@\n+      oop comp_oop = element_klass->java_mirror();\n+      if (element_klass->is_inline_klass()) {\n+        InlineKlass* ik = InlineKlass::cast(element_klass);\n+        comp_oop = k->name()->is_Q_array_signature() ? ik->val_mirror() : ik->ref_mirror();\n+      }\n@@ -981,1 +999,1 @@\n-        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+        comp_mirror = Handle(THREAD, comp_oop);\n@@ -1043,0 +1061,6 @@\n+\n+    if (k->is_inline_klass()) {\n+      oop secondary_mirror = create_secondary_mirror(k, mirror, CHECK);\n+      set_primary_mirror(mirror(), mirror());\n+      set_secondary_mirror(mirror(), secondary_mirror);\n+    }\n@@ -1051,0 +1075,19 @@\n+\/\/ Create the secondary mirror for inline class. Sets all the fields of this java.lang.Class\n+\/\/ instance with the same value as the primary mirror\n+oop java_lang_Class::create_secondary_mirror(Klass* k, Handle mirror, TRAPS) {\n+  assert(k->is_inline_klass(), \"primitive class\");\n+  \/\/ Allocate mirror (java.lang.Class instance)\n+  oop mirror_oop = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(k, CHECK_0);\n+  Handle secondary_mirror(THREAD, mirror_oop);\n+\n+  java_lang_Class::set_klass(secondary_mirror(), k);\n+  java_lang_Class::set_static_oop_field_count(secondary_mirror(), static_oop_field_count(mirror()));\n+\n+  set_protection_domain(secondary_mirror(), protection_domain(mirror()));\n+  set_class_loader(secondary_mirror(), class_loader(mirror()));\n+  \/\/ ## handle if java.base is not yet defined\n+  set_module(secondary_mirror(), module(mirror()));\n+  set_primary_mirror(secondary_mirror(), mirror());\n+  set_secondary_mirror(secondary_mirror(), secondary_mirror());\n+  return secondary_mirror();\n+}\n@@ -1179,0 +1222,20 @@\n+oop java_lang_Class::primary_mirror(oop java_class) {\n+  assert(_primary_mirror_offset != 0, \"must be set\");\n+  return java_class->obj_field(_primary_mirror_offset);\n+}\n+\n+void java_lang_Class::set_primary_mirror(oop java_class, oop mirror) {\n+  assert(_primary_mirror_offset != 0, \"must be set\");\n+  java_class->obj_field_put(_primary_mirror_offset, mirror);\n+}\n+\n+oop java_lang_Class::secondary_mirror(oop java_class) {\n+  assert(_secondary_mirror_offset != 0, \"must be set\");\n+  return java_class->obj_field(_secondary_mirror_offset);\n+}\n+\n+void java_lang_Class::set_secondary_mirror(oop java_class, oop mirror) {\n+  assert(_secondary_mirror_offset != 0, \"must be set\");\n+  java_class->obj_field_put(_secondary_mirror_offset, mirror);\n+}\n+\n@@ -1263,0 +1326,1 @@\n+  bool is_Q_descriptor = false;\n@@ -1268,0 +1332,1 @@\n+    is_Q_descriptor = k->is_inline_klass() && is_secondary_mirror(java_class);\n@@ -1274,1 +1339,3 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    st->print(is_Q_descriptor ? \"Q\" : \"L\");\n+  }\n@@ -1295,2 +1362,7 @@\n-      const char* sigstr = k->signature_name();\n-      int         siglen = (int) strlen(sigstr);\n+      const char* sigstr;\n+      if (k->is_inline_klass() && is_secondary_mirror(java_class)) {\n+        sigstr = InlineKlass::cast(k)->val_signature_name();\n+      } else {\n+        sigstr = k->signature_name();\n+      }\n+      int siglen = (int) strlen(sigstr);\n@@ -1376,0 +1448,2 @@\n+  macro(_primary_mirror_offset,      k, \"primaryType\",         class_signature,       false); \\\n+  macro(_secondary_mirror_offset,    k, \"secondaryType\",       class_signature,       false); \\\n@@ -2603,1 +2677,1 @@\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      if (method->is_object_constructor() &&\n@@ -4197,1 +4271,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":82,"deletions":8,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -231,0 +231,3 @@\n+  static int _primary_mirror_offset;\n+  static int _secondary_mirror_offset;\n+\n@@ -244,0 +247,3 @@\n+  static void set_primary_mirror(oop java_class, oop comp_mirror);\n+  static void set_secondary_mirror(oop java_class, oop comp_mirror);\n+\n@@ -258,0 +264,1 @@\n+  static oop  create_secondary_mirror(Klass* k, Handle mirror, TRAPS);\n@@ -287,0 +294,3 @@\n+  static int component_mirror_offset()     { CHECK_INIT(_component_mirror_offset); }\n+  static int primary_mirror_offset()       { CHECK_INIT(_primary_mirror_offset); }\n+  static int secondary_mirror_offset()     { CHECK_INIT(_secondary_mirror_offset); }\n@@ -294,0 +304,5 @@\n+  static oop  primary_mirror(oop java_class);\n+  static oop  secondary_mirror(oop java_class);\n+  static bool is_primary_mirror(oop java_class);\n+  static bool is_secondary_mirror(oop java_class);\n+\n@@ -299,2 +314,0 @@\n-  static int component_mirror_offset() { return _component_mirror_offset; }\n-\n@@ -1298,1 +1311,1 @@\n-    MN_IS_CONSTRUCTOR        = 0x00020000, \/\/ constructor\n+    MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, \/\/ constructor\n@@ -1303,0 +1316,1 @@\n+    MN_FLATTENED             = 0x00400000, \/\/ flattened field\n@@ -1839,1 +1853,0 @@\n-\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":17,"deletions":4,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -310,0 +310,18 @@\n+inline bool java_lang_Class::is_primary_mirror(oop java_class) {\n+  Klass* k = as_Klass(java_class);\n+  if (k->is_inline_klass()) {\n+    return java_class == primary_mirror(java_class);\n+  } else {\n+    return true;\n+  }\n+}\n+\n+inline bool java_lang_Class::is_secondary_mirror(oop java_class) {\n+  Klass* k = as_Klass(java_class);\n+  if (k->is_inline_klass()) {\n+    return java_class == secondary_mirror(java_class);\n+  } else {\n+    return false;\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.inline.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -64,0 +64,2 @@\n+\/\/ For INLINE_FIELD, set when loading inline type fields for\n+\/\/ class circularity checking.\n@@ -104,0 +106,3 @@\n+    case PlaceholderTable::PRIMITIVE_OBJECT_FIELD:\n+       queuehead = _inlineTypeFieldQ;\n+       break;\n@@ -120,0 +125,3 @@\n+    case PlaceholderTable::PRIMITIVE_OBJECT_FIELD:\n+       _inlineTypeFieldQ = seenthread;\n+       break;\n@@ -238,0 +246,1 @@\n+  case PlaceholderTable::PRIMITIVE_OBJECT_FIELD: return \"PRIMITIVE_OBJECT_FIELD\";\n@@ -302,1 +311,2 @@\n-      && (probe->defineThreadQ() == nullptr) && (probe->definer() == nullptr)) {\n+      && (probe->defineThreadQ() == nullptr) && (probe->definer() == nullptr)\n+      && (probe->inlineTypeFieldQ() == nullptr)) {\n@@ -336,0 +346,3 @@\n+  st->print(\"inlineTypeFieldQ threads:\");\n+  inlineTypeFieldQ()->print_action_queue(st);\n+  st->cr();\n","filename":"src\/hotspot\/share\/classfile\/placeholders.cpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+  \/\/ PRIMITIVE_OBJECT_FIELD: needed to check for inline type fields circularity\n@@ -53,2 +54,3 @@\n-    DEFINE_CLASS = 3               \/\/ find_or_define class\n-  };\n+    DEFINE_CLASS = 3,              \/\/ find_or_define class\n+    PRIMITIVE_OBJECT_FIELD = 4     \/\/ primitive object fields\n+ };\n@@ -96,0 +98,1 @@\n+  SeenThread*       _inlineTypeFieldQ;  \/\/ queue of inline types for circularity checking\n@@ -113,1 +116,2 @@\n-     _superThreadQ(nullptr), _loadInstanceThreadQ(nullptr), _defineThreadQ(nullptr) { }\n+     _superThreadQ(nullptr), _loadInstanceThreadQ(nullptr), _defineThreadQ(nullptr),\n+     _inlineTypeFieldQ(nullptr) { }\n@@ -124,0 +128,3 @@\n+  SeenThread*        inlineTypeFieldQ()    const { return _inlineTypeFieldQ; }\n+  void               set_inlineTypeFieldQ(SeenThread* SeenThread) { _inlineTypeFieldQ = SeenThread; }\n+\n@@ -136,0 +143,4 @@\n+  bool inline_type_field_in_progress() {\n+    return (_inlineTypeFieldQ != nullptr);\n+  }\n+\n","filename":"src\/hotspot\/share\/classfile\/placeholders.hpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -192,1 +192,11 @@\n-    return VerificationType::reference_type(_cp->klass_name_at(class_index));\n+    Symbol* klass_name = _cp->klass_name_at(class_index);\n+    if (klass_name->is_Q_signature()) {\n+      Symbol* fund_name = klass_name->fundamental_name(THREAD);\n+      if (fund_name == NULL) {\n+        _stream->stackmap_format_error(\"TBD something bad happened\", THREAD);\n+        return VerificationType::bogus_type();\n+      }\n+      return VerificationType::inline_type(fund_name);\n+    } else {\n+      return VerificationType::reference_type(klass_name);\n+    }\n","filename":"src\/hotspot\/share\/classfile\/stackMapTable.cpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -70,0 +71,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -77,0 +79,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -335,1 +338,1 @@\n-      \/\/ Ignore wrapping L and ;.\n+      \/\/ Ignore wrapping L and ; (and Q and ; for value types).\n@@ -364,1 +367,8 @@\n-      k = k->array_klass(ndims, CHECK_NULL);\n+      if (class_name->is_Q_array_signature()) {\n+        if (!k->is_inline_klass()) {\n+          THROW_MSG_NULL(vmSymbols::java_lang_IncompatibleClassChangeError(), \"L\/Q mismatch on bottom type\");\n+        }\n+        k = InlineKlass::cast(k)->value_array_klass(ndims, CHECK_NULL);\n+      } else {\n+        k = k->array_klass(ndims, CHECK_NULL);\n+      }\n@@ -491,0 +501,43 @@\n+Klass* SystemDictionary::resolve_inline_type_field_or_fail(AllFieldStream* fs,\n+                                                           Handle class_loader,\n+                                                           Handle protection_domain,\n+                                                           bool throw_error,\n+                                                           TRAPS) {\n+  Symbol* class_name = fs->signature()->fundamental_name(THREAD);\n+  class_loader = Handle(THREAD, java_lang_ClassLoader::non_reflection_class_loader(class_loader()));\n+  ClassLoaderData* loader_data = class_loader_data(class_loader);\n+  bool throw_circularity_error = false;\n+  PlaceholderEntry* oldprobe;\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    oldprobe = PlaceholderTable::get_entry(class_name, loader_data);\n+    if (oldprobe != nullptr &&\n+      oldprobe->check_seen_thread(THREAD, PlaceholderTable::PRIMITIVE_OBJECT_FIELD)) {\n+      throw_circularity_error = true;\n+\n+    } else {\n+      PlaceholderTable::find_and_add(class_name, loader_data,\n+                                   PlaceholderTable::PRIMITIVE_OBJECT_FIELD, nullptr, THREAD);\n+    }\n+  }\n+\n+  Klass* klass = nullptr;\n+  if (!throw_circularity_error) {\n+    klass = SystemDictionary::resolve_or_fail(class_name, class_loader,\n+                                               protection_domain, true, THREAD);\n+  } else {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG_NULL(vmSymbols::java_lang_ClassCircularityError(), class_name->as_C_string());\n+  }\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    PlaceholderTable::find_and_remove(class_name, loader_data,\n+                                      PlaceholderTable::PRIMITIVE_OBJECT_FIELD, THREAD);\n+  }\n+\n+  class_name->decrement_refcount();\n+  return klass;\n+}\n+\n@@ -640,1 +693,1 @@\n-         !Signature::has_envelope(name), \"invalid class name\");\n+         !Signature::has_envelope(name), \"invalid class name: %s\", name == nullptr ? \"nullptr\" : name->as_C_string());\n@@ -851,1 +904,1 @@\n-    if (t != T_OBJECT) {\n+    if (t != T_OBJECT && t != T_PRIMITIVE_OBJECT) {\n@@ -857,1 +910,5 @@\n-      k = k->array_klass_or_null(ndims);\n+      if (class_name->is_Q_array_signature()) {\n+        k = InlineKlass::cast(k)->value_array_klass_or_null(ndims);\n+      } else {\n+        k = k->array_klass_or_null(ndims);\n+      }\n@@ -1213,0 +1270,18 @@\n+\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik->fields(), ik->constants()); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_PRIMITIVE_OBJECT) {\n+        if (!fs.access_flags().is_static()) {\n+          \/\/ Pre-load inline class\n+          Klass* real_k = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+            class_loader, protection_domain, true, CHECK_NULL);\n+          Klass* k = ik->get_inline_type_field_klass_or_null(fs.index());\n+          if (real_k != k) {\n+            \/\/ oops, the app has substituted a different version of k!\n+            return nullptr;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1248,0 +1323,1 @@\n+\n@@ -1829,1 +1905,1 @@\n-    if (t != T_OBJECT) {\n+    if (t != T_OBJECT && t != T_PRIMITIVE_OBJECT) {\n@@ -1837,1 +1913,5 @@\n-      klass = klass->array_klass_or_null(ndims);\n+      if (class_name->is_Q_array_signature()) {\n+        klass = InlineKlass::cast(klass)->value_array_klass_or_null(ndims);\n+      } else {\n+        klass = klass->array_klass_or_null(ndims);\n+      }\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":87,"deletions":7,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+class AllFieldStream;\n@@ -135,0 +136,6 @@\n+  static Klass* resolve_inline_type_field_or_fail(AllFieldStream* fs,\n+                                                  Handle class_loader,\n+                                                  Handle protection_domain,\n+                                                  bool throw_error,\n+                                                  TRAPS);\n+\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -67,0 +67,7 @@\n+  \/\/ Need to do this check when called from CDS.\n+  if (this_class->access_flags().is_primitive_class()) {\n+    Klass* from_class = SystemDictionary::resolve_or_fail(\n+      from_name, Handle(THREAD, klass->class_loader()),\n+      Handle(THREAD, klass->protection_domain()), true, CHECK_false);\n+    return from_class == this_class;\n+  }\n@@ -71,1 +78,1 @@\n-    \/\/ to interfaces java.lang.Cloneable and java.io.Serializable.\n+    \/\/ to interfaces java.lang.Cloneable and java.io.Serializable\n@@ -127,0 +134,29 @@\n+\n+\/*\n+    \/\/ This code implements non-covariance between inline type arrays and both\n+    \/\/ arrays of objects and arrays of interface types.  If covariance is\n+    \/\/ supported for inline type arrays then this code should be removed.\n+    if (comp_from.is_inline_type() && !comp_this.is_null() && comp_this.is_reference()) {\n+      \/\/ An array of inline types is not assignable to an array of java.lang.Objects.\n+      if (comp_this.name() == vmSymbols::java_lang_Object()) {\n+        return false;\n+      }\n+\n+      \/\/ Need to load 'comp_this' to see if it is an interface.\n+      InstanceKlass* klass = context->current_class();\n+      {\n+        HandleMark hm(THREAD);\n+        Klass* comp_this_class = SystemDictionary::resolve_or_fail(\n+            comp_this.name(), Handle(THREAD, klass->class_loader()),\n+            Handle(THREAD, klass->protection_domain()), true, CHECK_false);\n+        klass->class_loader_data()->record_dependency(comp_this_class);\n+        if (log_is_enabled(Debug, class, resolve)) {\n+          Verifier::trace_class_resolution(comp_this_class, klass);\n+        }\n+        \/\/ An array of inline types is not assignable to an array of interface types.\n+        if (comp_this_class->is_interface()) {\n+          return false;\n+        }\n+      }\n+    }\n+*\/\n@@ -135,0 +171,39 @@\n+bool VerificationType::is_inline_type_assignable_from(const VerificationType& from) const {\n+  \/\/ Check that 'from' is not null, is an inline type, and is the same inline type.\n+  assert(is_inline_type(), \"called with a non-inline type\");\n+  assert(!is_null(), \"inline type is not null\");\n+  return (!from.is_null() && from.is_inline_type() && name() == from.name());\n+}\n+\n+bool VerificationType::is_ref_assignable_from_inline_type(const VerificationType& from, ClassVerifier* context, TRAPS) const {\n+  assert(!from.is_null(), \"Inline type should not be null\");\n+  if (!is_null() && (name()->is_same_fundamental_type(from.name()) ||\n+      name() == vmSymbols::java_lang_Object())) {\n+    return true;\n+  }\n+\n+  \/\/ Need to load 'this' to see if it is an interface or supertype.\n+  InstanceKlass* klass = context->current_class();\n+  {\n+    HandleMark hm(THREAD);\n+    Klass* this_class = SystemDictionary::resolve_or_fail(\n+        name(), Handle(THREAD, klass->class_loader()),\n+        Handle(THREAD, klass->protection_domain()), true, CHECK_false);\n+    klass->class_loader_data()->record_dependency(this_class);\n+    if (log_is_enabled(Debug, class, resolve)) {\n+      Verifier::trace_class_resolution(this_class, klass);\n+    }\n+    if (this_class->is_interface()) {\n+      return true;\n+    } else {\n+      Klass* from_class = SystemDictionary::resolve_or_fail(\n+        from.name(), Handle(THREAD, klass->class_loader()),\n+        Handle(THREAD, klass->protection_domain()), true, CHECK_false);\n+      if (log_is_enabled(Debug, class, resolve)) {\n+        Verifier::trace_class_resolution(from_class, klass);\n+      }\n+      return from_class->is_subclass_of(this_class);\n+    }\n+  }\n+}\n+\n@@ -149,1 +224,2 @@\n-    case T_OBJECT: {\n+    case T_OBJECT:\n+    case T_PRIMITIVE_OBJECT: {\n@@ -155,1 +231,3 @@\n-      return VerificationType::reference_type(component_copy);\n+      return (ss.type() == T_PRIMITIVE_OBJECT) ?\n+        VerificationType::inline_type(component_copy) :\n+        VerificationType::reference_type(component_copy);\n@@ -181,0 +259,2 @@\n+    case InlineTypeQuery:  st->print(\"inline type\"); break;\n+    case NonScalarQuery:   st->print(\"reference or inline type\"); break;\n@@ -189,0 +269,2 @@\n+      } else if (is_inline_type()) {\n+        name()->print_Qvalue_on(st);\n","filename":"src\/hotspot\/share\/classfile\/verificationType.cpp","additions":85,"deletions":3,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+#define INLINE_TYPE_MAJOR_VERSION                       56\n@@ -278,1 +279,1 @@\n-    \/\/ We need to skip the following four for bootstraping\n+    \/\/ We need to skip the following four for bootstrapping\n@@ -501,0 +502,7 @@\n+    case WRONG_INLINE_TYPE:\n+      ss->print(\"Type \");\n+      _type.details(ss);\n+      ss->print(\" and type \");\n+      _expected.details(ss);\n+      ss->print(\" must be identical inline types.\");\n+      break;\n@@ -595,0 +603,8 @@\n+VerificationType reference_or_inline_type(InstanceKlass* klass) {\n+  if (klass->is_inline_klass()) {\n+    return VerificationType::inline_type(klass->name());\n+  } else {\n+    return VerificationType::reference_type(klass->name());\n+  }\n+}\n+\n@@ -598,1 +614,1 @@\n-  _this_type = VerificationType::reference_type(klass->name());\n+  _this_type = reference_or_inline_type(klass);\n@@ -1038,1 +1054,1 @@\n-          if (!atype.is_reference_array()) {\n+          if (!atype.is_nonscalar_array()) {\n@@ -1211,1 +1227,1 @@\n-          if (!atype.is_reference_array()) {\n+          if (!atype.is_nonscalar_array()) {\n@@ -1611,1 +1627,1 @@\n-            VerificationType::reference_check(), CHECK_VERIFY(this));\n+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -1616,1 +1632,1 @@\n-            VerificationType::reference_check(), CHECK_VERIFY(this));\n+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -1667,1 +1683,1 @@\n-            VerificationType::reference_check(), CHECK_VERIFY(this));\n+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -1679,1 +1695,1 @@\n-          if (_method->name() == vmSymbols::object_initializer_name() &&\n+          if (_method->is_object_constructor() &&\n@@ -1699,0 +1715,11 @@\n+        case Bytecodes::_withfield :\n+          if (_klass->major_version() < INLINE_TYPE_MAJOR_VERSION) {\n+            class_format_error(\n+              \"withfield not supported by this class file version (%d.%d), class %s\",\n+              _klass->major_version(), _klass->minor_version(), _klass->external_name());\n+            return;\n+          }\n+          \/\/ pass FALSE, operand can't be an array type for withfield.\n+          verify_field_instructions(\n+            &bcs, &current_frame, cp, false, CHECK_VERIFY(this));\n+          no_control_flow = false; break;\n@@ -1702,4 +1729,0 @@\n-          verify_invoke_instructions(\n-            &bcs, code_length, &current_frame, (bci >= ex_min && bci < ex_max),\n-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));\n-          no_control_flow = false; break;\n@@ -1710,1 +1733,1 @@\n-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));\n+            &this_uninit, cp, &stackmap_table, CHECK_VERIFY(this));\n@@ -1728,0 +1751,22 @@\n+        case Bytecodes::_aconst_init :\n+        {\n+          if (_klass->major_version() < INLINE_TYPE_MAJOR_VERSION) {\n+            class_format_error(\n+              \"aconst_init not supported by this class file version (%d.%d), class %s\",\n+              _klass->major_version(), _klass->minor_version(), _klass->external_name());\n+            return;\n+          }\n+          index = bcs.get_index_u2();\n+          verify_cp_class_type(bci, index, cp, CHECK_VERIFY(this));\n+          VerificationType ref_type = cp_index_to_type(index, cp, CHECK_VERIFY(this));\n+          if (!ref_type.is_object()) {\n+            verify_error(ErrorContext::bad_type(bci,\n+                TypeOrigin::cp(index, ref_type)),\n+                \"Illegal aconst_init instruction\");\n+            return;\n+          }\n+          VerificationType inline_type =\n+            VerificationType::change_ref_to_inline_type(ref_type);\n+          current_frame.push_stack(inline_type, CHECK_VERIFY(this));\n+          no_control_flow = false; break;\n+        }\n@@ -1768,3 +1813,3 @@\n-        case Bytecodes::_monitorexit :\n-          current_frame.pop_stack(\n-            VerificationType::reference_check(), CHECK_VERIFY(this));\n+        case Bytecodes::_monitorexit : {\n+          VerificationType ref = current_frame.pop_stack(\n+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -1772,0 +1817,1 @@\n+        }\n@@ -2036,0 +2082,1 @@\n+\n@@ -2150,1 +2197,1 @@\n-            | (1 << JVM_CONSTANT_String)  | (1 << JVM_CONSTANT_Class)\n+            | (1 << JVM_CONSTANT_String) | (1 << JVM_CONSTANT_Class)\n@@ -2325,1 +2372,1 @@\n-    (!allow_arrays || !ref_class_type.is_array())) {\n+      (!allow_arrays || !ref_class_type.is_array())) {\n@@ -2332,0 +2379,1 @@\n+\n@@ -2361,0 +2409,11 @@\n+    case Bytecodes::_withfield: {\n+      for (int i = n - 1; i >= 0; i--) {\n+        current_frame->pop_stack(field_type[i], CHECK_VERIFY(this));\n+      }\n+      \/\/ Check that the receiver is a subtype of the referenced class.\n+      current_frame->pop_stack(target_class_type, CHECK_VERIFY(this));\n+      VerificationType target_inline_type =\n+        VerificationType::change_ref_to_inline_type(target_class_type);\n+      current_frame->push_stack(target_inline_type, CHECK_VERIFY(this));\n+      break;\n+    }\n@@ -2782,1 +2841,1 @@\n-    bool in_try_block, bool *this_uninit, VerificationType return_type,\n+    bool in_try_block, bool *this_uninit,\n@@ -2814,1 +2873,1 @@\n-  \/\/ Get referenced class type\n+  \/\/ Get referenced class\n@@ -2880,3 +2939,7 @@\n-    \/\/ Make sure <init> can only be invoked by invokespecial\n-    if (opcode != Bytecodes::_invokespecial ||\n-        method_name != vmSymbols::object_initializer_name()) {\n+    \/\/ Make sure:\n+    \/\/   <init> can only be invoked by invokespecial.\n+    \/\/   <vnew> can only be invoked by invokestatic.\n+    if (!((opcode == Bytecodes::_invokestatic &&\n+           method_name == vmSymbols::inline_factory_name()) ||\n+         (opcode == Bytecodes::_invokespecial &&\n+          method_name == vmSymbols::object_initializer_name()))) {\n@@ -2890,1 +2953,1 @@\n-                  current_class()->super()->name()))) {\n+                  current_class()->super()->name()))) { \/\/ super() can never be an inline_type.\n@@ -2975,3 +3038,1 @@\n-      \/\/ <init> method must have a void return type\n-      \/* Unreachable?  Class file parser verifies that methods with '<' have\n-       * void return *\/\n+      \/\/ an <init> method must have a void return type\n@@ -2990,0 +3051,7 @@\n+  } else { \/\/ no return type\n+    \/\/ <vnew> method may not have a void return type\n+    if (method_name == vmSymbols::inline_factory_name()) {\n+      verify_error(ErrorContext::bad_code(bci),\n+          \"Return type must be non-void in <vnew> static factory method\");\n+      return;\n+    }\n@@ -3037,1 +3105,2 @@\n-    \/\/ add one dimension to component with 'L' prepended and ';' postpended.\n+    char Q_or_L = component_type.is_inline_type() ? JVM_SIGNATURE_PRIMITIVE_OBJECT : JVM_SIGNATURE_CLASS;\n+    \/\/ add one dimension to component with 'L' or 'Q' prepended and ';' appended.\n@@ -3041,1 +3110,1 @@\n-                         JVM_SIGNATURE_ARRAY, JVM_SIGNATURE_CLASS, component_name);\n+                         JVM_SIGNATURE_ARRAY, Q_or_L, component_name);\n@@ -3083,1 +3152,1 @@\n-    index, VerificationType::reference_check(), CHECK_VERIFY(this));\n+    index, VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -3120,1 +3189,1 @@\n-    VerificationType::reference_check(), CHECK_VERIFY(this));\n+    VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n","filename":"src\/hotspot\/share\/classfile\/verifier.cpp","additions":100,"deletions":31,"binary":false,"changes":131,"status":"modified"},{"patch":"@@ -43,1 +43,1 @@\n-    DYNAMICCONSTANT_MAJOR_VERSION       = 55\n+    DYNAMICCONSTANT_MAJOR_VERSION       = 55,\n@@ -158,0 +158,1 @@\n+    WRONG_INLINE_TYPE,    \/\/ Mismatched inline type\n@@ -221,0 +222,3 @@\n+  static ErrorContext bad_inline_type(u2 bci, TypeOrigin type, TypeOrigin exp) {\n+    return ErrorContext(bci, WRONG_INLINE_TYPE, type, exp);\n+  }\n@@ -351,1 +355,1 @@\n-    bool in_try_block, bool* this_uninit, VerificationType return_type,\n+    bool in_try_block, bool* this_uninit,\n@@ -448,1 +452,8 @@\n-    return VerificationType::reference_type(cp->klass_name_at(index));\n+    Symbol* name = cp->klass_name_at(index);\n+    if (name->is_Q_signature()) {\n+      \/\/ Remove the Q and ;\n+      \/\/ TBD need error msg if fundamental_name() returns NULL?\n+      Symbol* fund_name = name->fundamental_name(CHECK_(VerificationType::bogus_type()));\n+      return VerificationType::inline_type(fund_name);\n+    }\n+    return VerificationType::reference_type(name);\n@@ -486,2 +497,10 @@\n-        *inference_type =\n-          VerificationType::reference_type(name_copy);\n+        *inference_type = VerificationType::reference_type(name_copy);\n+        return 1;\n+      }\n+    case T_PRIMITIVE_OBJECT:\n+      {\n+        Symbol* vname = sig_type->as_symbol();\n+        \/\/ Create another symbol to save as signature stream unreferences this symbol.\n+        Symbol* vname_copy = create_temporary_symbol(vname);\n+        assert(vname_copy == vname, \"symbols don't match\");\n+        *inference_type = VerificationType::inline_type(vname_copy);\n","filename":"src\/hotspot\/share\/classfile\/verifier.hpp","additions":24,"deletions":5,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -250,0 +250,4 @@\n+  case vmIntrinsics::_asPrimaryType:\n+  case vmIntrinsics::_asPrimaryTypeArg:\n+  case vmIntrinsics::_asValueType:\n+  case vmIntrinsics::_asValueTypeArg:\n@@ -320,0 +324,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -329,0 +335,1 @@\n+  case vmIntrinsics::_getValue:\n@@ -338,0 +345,1 @@\n+  case vmIntrinsics::_putValue:\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -302,0 +302,6 @@\n+  do_intrinsic(_asPrimaryType,            java_lang_Class,        asPrimaryType_name, void_class_signature,      F_R)   \\\n+  do_intrinsic(_asPrimaryTypeArg,         jdk_internal_value_PrimitiveClass, asPrimaryType_name, class_class_signature, F_S) \\\n+   do_name(     asPrimaryType_name,                              \"asPrimaryType\")                                       \\\n+  do_intrinsic(_asValueType,              java_lang_Class,        asValueType_name, void_class_signature,        F_R)   \\\n+  do_intrinsic(_asValueTypeArg,           jdk_internal_value_PrimitiveClass, asValueType_name,   class_class_signature, F_S) \\\n+   do_name(     asValueType_name,                                \"asValueType\")                                         \\\n@@ -656,0 +662,2 @@\n+  do_signature(getValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;)Ljava\/lang\/Object;\")                   \\\n+  do_signature(putValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;Ljava\/lang\/Object;)V\")                  \\\n@@ -666,0 +674,3 @@\n+  do_name(getValue_name,\"getValue\")             do_name(putValue_name,\"putValue\")                                       \\\n+  do_name(makePrivateBuffer_name,\"makePrivateBuffer\")                                                                   \\\n+  do_name(finishPrivateBuffer_name,\"finishPrivateBuffer\")                                                               \\\n@@ -676,0 +687,1 @@\n+  do_intrinsic(_getValue,           jdk_internal_misc_Unsafe,     getValue_name, getValue_signature,             F_RN)  \\\n@@ -685,0 +697,4 @@\n+  do_intrinsic(_putValue,           jdk_internal_misc_Unsafe,     putValue_name, putValue_signature,             F_RN)  \\\n+                                                                                                                        \\\n+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \\\n+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -181,0 +181,1 @@\n+  template(tag_preload,                               \"Preload\")                                  \\\n@@ -392,0 +393,1 @@\n+  template(inline_factory_name,                       \"<vnew>\")                                   \\\n@@ -528,0 +530,2 @@\n+  template(default_value_name,                        \".default\")                                 \\\n+  template(empty_marker_name,                         \".empty\")                                   \\\n@@ -603,0 +607,1 @@\n+  template(class_class_signature,                     \"(Ljava\/lang\/Class;)Ljava\/lang\/Class;\")     \\\n@@ -616,0 +621,1 @@\n+  template(object_object_boolean_signature,           \"(Ljava\/lang\/Object;Ljava\/lang\/Object;)Z\") \\\n@@ -757,0 +763,2 @@\n+  template(primaryType_name,                           \"primaryType\")                                             \\\n+  template(secondaryType_name,                         \"secondaryType\")                                           \\\n@@ -783,0 +791,5 @@\n+  template(java_lang_runtime_ValueObjectMethods,            \"java\/lang\/runtime\/ValueObjectMethods\")               \\\n+  template(isSubstitutable_name,                            \"isSubstitutable\")                                    \\\n+  template(valueObjectHashCode_name,                        \"valueObjectHashCode\")                                \\\n+  template(jdk_internal_value_PrimitiveClass,               \"jdk\/internal\/value\/PrimitiveClass\")                  \\\n+                                                                                                                  \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -270,2 +270,2 @@\n-BufferBlob::BufferBlob(const char* name, int size, CodeBuffer* cb)\n-  : RuntimeBlob(name, cb, sizeof(BufferBlob), size, CodeOffsets::frame_never_safe, 0, nullptr)\n+BufferBlob::BufferBlob(const char* name, int header_size, int size, CodeBuffer* cb)\n+  : RuntimeBlob(name, cb, header_size, size, CodeOffsets::frame_never_safe, 0, nullptr)\n@@ -282,1 +282,1 @@\n-    blob = new (size) BufferBlob(name, size, cb);\n+    blob = new (size) BufferBlob(name, sizeof(BufferBlob), size, cb);\n@@ -298,0 +298,4 @@\n+BufferBlob::BufferBlob(const char* name, int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments)\n+  : RuntimeBlob(name, cb, sizeof(BufferBlob), size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n+{}\n+\n@@ -302,2 +306,2 @@\n-AdapterBlob::AdapterBlob(int size, CodeBuffer* cb) :\n-  BufferBlob(\"I2C\/C2I adapters\", size, cb) {\n+AdapterBlob::AdapterBlob(int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n+  BufferBlob(\"I2C\/C2I adapters\", size, cb, frame_complete, frame_size, oop_maps, caller_must_gc_arguments) {\n@@ -307,1 +311,1 @@\n-AdapterBlob* AdapterBlob::create(CodeBuffer* cb) {\n+AdapterBlob* AdapterBlob::create(CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) {\n@@ -316,1 +320,1 @@\n-    blob = new (size) AdapterBlob(size, cb);\n+    blob = new (size) AdapterBlob(size, cb, frame_complete, frame_size, oop_maps, caller_must_gc_arguments);\n@@ -394,0 +398,25 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Implementation of BufferedInlineTypeBlob\n+BufferedInlineTypeBlob::BufferedInlineTypeBlob(int size, CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) :\n+  BufferBlob(\"buffered inline type\", sizeof(BufferedInlineTypeBlob), size, cb),\n+  _pack_fields_off(pack_fields_off),\n+  _pack_fields_jobject_off(pack_fields_jobject_off),\n+  _unpack_fields_off(unpack_fields_off) {\n+  CodeCache::commit(this);\n+}\n+\n+BufferedInlineTypeBlob* BufferedInlineTypeBlob::create(CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) {\n+  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n+\n+  BufferedInlineTypeBlob* blob = nullptr;\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(BufferedInlineTypeBlob));\n+  {\n+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+    blob = new (size) BufferedInlineTypeBlob(size, cb, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+  }\n+  \/\/ Track memory usage statistic after releasing CodeCache_lock\n+  MemoryService::track_code_cache_memory_usage();\n+\n+  return blob;\n+}\n+\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":36,"deletions":7,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -160,0 +160,1 @@\n+  virtual bool is_buffered_inline_type_blob() const   { return false; }\n@@ -399,0 +400,1 @@\n+  friend class BufferedInlineTypeBlob;\n@@ -405,1 +407,2 @@\n-  BufferBlob(const char* name, int size, CodeBuffer* cb);\n+  BufferBlob(const char* name, int header_size, int size, CodeBuffer* cb);\n+  BufferBlob(const char* name, int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n@@ -437,1 +440,1 @@\n-  AdapterBlob(int size, CodeBuffer* cb);\n+  AdapterBlob(int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n@@ -441,1 +444,5 @@\n-  static AdapterBlob* create(CodeBuffer* cb);\n+  static AdapterBlob* create(CodeBuffer* cb,\n+                             int frame_complete,\n+                             int frame_size,\n+                             OopMapSet* oop_maps,\n+                             bool caller_must_gc_arguments = false);\n@@ -445,0 +452,2 @@\n+\n+  bool caller_must_gc_arguments(JavaThread* thread) const { return true; }\n@@ -477,0 +486,22 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ BufferedInlineTypeBlob : used for pack\/unpack handlers\n+\n+class BufferedInlineTypeBlob: public BufferBlob {\n+private:\n+  const int _pack_fields_off;\n+  const int _pack_fields_jobject_off;\n+  const int _unpack_fields_off;\n+\n+  BufferedInlineTypeBlob(int size, CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off);\n+\n+public:\n+  \/\/ Creation\n+  static BufferedInlineTypeBlob* create(CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off);\n+\n+  address pack_fields() const { return code_begin() + _pack_fields_off; }\n+  address pack_fields_jobject() const { return code_begin() + _pack_fields_jobject_off; }\n+  address unpack_fields() const { return code_begin() + _unpack_fields_off; }\n+\n+  \/\/ Typing\n+  virtual bool is_buffered_inline_type_blob() const { return true; }\n+};\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":34,"deletions":3,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -249,1 +249,1 @@\n-                                    bool& needs_ic_stub_refill, TRAPS) {\n+                                    bool& needs_ic_stub_refill, bool caller_is_c1, TRAPS) {\n@@ -258,1 +258,1 @@\n-    entry = VtableStubs::find_itable_stub(itable_index);\n+    entry = VtableStubs::find_itable_stub(itable_index, caller_is_c1);\n@@ -284,1 +284,1 @@\n-    entry = VtableStubs::find_vtable_stub(vtable_index);\n+    entry = VtableStubs::find_vtable_stub(vtable_index, caller_is_c1);\n@@ -512,0 +512,1 @@\n+                                           bool caller_is_c1,\n@@ -537,1 +538,1 @@\n-      entry      = method_code->verified_entry_point();\n+      entry      = caller_is_c1 ? method_code->verified_inline_entry_point() : method_code->verified_entry_point();\n@@ -539,1 +540,1 @@\n-      entry      = method_code->entry_point();\n+      entry      = caller_is_c1 ? method_code->inline_entry_point() : method_code->entry_point();\n@@ -548,1 +549,2 @@\n-      info.set_interpreter_entry(method()->get_c2i_entry(), method());\n+      address entry = caller_is_c1 ? method()->get_c2i_inline_entry() : method()->get_c2i_entry();\n+      info.set_interpreter_entry(entry, method());\n@@ -553,1 +555,2 @@\n-      info.set_icholder_entry(method()->get_c2i_unverified_entry(), holder);\n+      entry = (caller_is_c1)? method()->get_c2i_unverified_inline_entry() : method()->get_c2i_unverified_entry();\n+      info.set_icholder_entry(entry, holder);\n@@ -640,1 +643,3 @@\n-void CompiledStaticCall::compute_entry(const methodHandle& m, bool caller_is_nmethod, StaticCallInfo& info) {\n+void CompiledStaticCall::compute_entry(const methodHandle& m, CompiledMethod* caller_nm, StaticCallInfo& info) {\n+  assert(!m->mismatch(), \"Mismatch for static call\");\n+  bool caller_is_nmethod = caller_nm->is_nmethod();\n@@ -645,1 +650,5 @@\n-    info._entry  = m_code->verified_entry_point();\n+    if (caller_nm->is_compiled_by_c1()) {\n+      info._entry = m_code->verified_inline_entry_point();\n+    } else {\n+      info._entry = m_code->verified_entry_point();\n+    }\n@@ -651,1 +660,7 @@\n-    info._entry      = m()->get_c2i_entry();\n+    if (caller_nm->is_compiled_by_c1()) {\n+      \/\/ C1 -> interp: values passed as oops\n+      info._entry = m()->get_c2i_inline_entry();\n+    } else {\n+      \/\/ C2 -> interp: values passed as fields\n+      info._entry = m()->get_c2i_entry();\n+    }\n","filename":"src\/hotspot\/share\/code\/compiledIC.cpp","additions":25,"deletions":10,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -264,1 +264,1 @@\n-  bool set_to_megamorphic(CallInfo* call_info, Bytecodes::Code bytecode, bool& needs_ic_stub_refill, TRAPS);\n+  bool set_to_megamorphic(CallInfo* call_info, Bytecodes::Code bytecode, bool& needs_ic_stub_refill, bool caller_is_c1, TRAPS);\n@@ -268,1 +268,1 @@\n-                                        CompiledICInfo& info, TRAPS);\n+                                        bool caller_is_c1, CompiledICInfo& info, TRAPS);\n@@ -316,1 +316,1 @@\n-\/\/    compilled code <------------> interpreted code\n+\/\/    compiled code <------------> interpreted code\n@@ -347,1 +347,1 @@\n-  static void compute_entry(const methodHandle& m, bool caller_is_nmethod, StaticCallInfo& info);\n+  static void compute_entry(const methodHandle& m, CompiledMethod* caller_nm, StaticCallInfo& info);\n","filename":"src\/hotspot\/share\/code\/compiledIC.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -384,1 +384,12 @@\n-      signature    = callee->signature();\n+      signature = callee->signature();\n+\n+      \/\/ If inline types are passed as fields, use the extended signature\n+      \/\/ which contains the types of all (oop) fields of the inline type.\n+      if (is_compiled_by_c2() && callee->has_scalarized_args()) {\n+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();\n+        assert(sig != NULL, \"sig should never be null\");\n+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);\n+        has_receiver = false; \/\/ The extended signature contains the receiver type\n+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);\n+        return;\n+      }\n","filename":"src\/hotspot\/share\/code\/compiledMethod.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -202,0 +202,10 @@\n+  bool  needs_stack_repair() const {\n+    if (is_compiled_by_c1()) {\n+      return method()->c1_needs_stack_repair();\n+    } else if (is_compiled_by_c2()) {\n+      return method()->c2_needs_stack_repair();\n+    } else {\n+      return false;\n+    }\n+  }\n+\n@@ -214,0 +224,2 @@\n+  virtual address verified_inline_entry_point() const = 0;\n+  virtual address verified_inline_ro_entry_point() const = 0;\n@@ -220,0 +232,1 @@\n+  virtual address inline_entry_point() const = 0;\n","filename":"src\/hotspot\/share\/code\/compiledMethod.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -153,0 +153,1 @@\n+  _is_init = read_from(stream);\n@@ -170,0 +171,5 @@\n+    if (_is_init == NULL) {\n+      \/\/ MarkerValue is used for null-free objects\n+      _is_init = new MarkerValue();\n+    }\n+    _is_init->write_on(stream);\n","filename":"src\/hotspot\/share\/code\/debugInfo.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -126,0 +126,1 @@\n+  ScopeValue*                _is_init;\n@@ -130,1 +131,1 @@\n-  ObjectValue(int id, ScopeValue* klass)\n+  ObjectValue(int id, ScopeValue* klass, ScopeValue* is_init = nullptr)\n@@ -133,0 +134,1 @@\n+     , _is_init(is_init)\n@@ -142,0 +144,1 @@\n+     , _is_init(nullptr)\n@@ -150,0 +153,1 @@\n+  ScopeValue*                 is_init() const           { return _is_init; }\n@@ -155,0 +159,1 @@\n+  bool                        maybe_null() const        { return !_is_init->is_marker(); }\n","filename":"src\/hotspot\/share\/code\/debugInfo.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -292,0 +292,1 @@\n+                                              bool        return_scalarized,\n@@ -310,0 +311,1 @@\n+  last_pd->set_return_scalarized(return_scalarized);\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -110,0 +110,1 @@\n+                      bool        return_scalarized = false,\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1705,1 +1705,2 @@\n-      || m->number_of_breakpoints() > 0) {\n+      || m->number_of_breakpoints() > 0\n+      || m->mismatch()) {\n","filename":"src\/hotspot\/share\/code\/dependencies.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -691,0 +691,6 @@\n+\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\"); \/\/ for the next 3 fields\n+    _inline_entry_point       = _entry_point;\n+    _verified_inline_entry_point = _verified_entry_point;\n+    _verified_inline_ro_entry_point = _verified_entry_point;\n+\n@@ -875,0 +881,3 @@\n+    _inline_entry_point       = code_begin()         + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point = code_begin()      + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin()   + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n@@ -3004,0 +3013,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -3005,0 +3015,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -3014,0 +3026,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -3016,33 +3038,12 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != nullptr) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n-    }\n-  }\n-\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != nullptr) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n-    }\n-    if (m != nullptr && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != nullptr) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n@@ -3050,54 +3051,63 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      intptr_t out_preserve = SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n-        }\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n-        }\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+    }\n+  }\n+\n+  Method* m = method();\n+  if (m == nullptr || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN4(entry_point(), verified_entry_point(), verified_inline_entry_point(), verified_inline_ro_entry_point());\n+  low = MIN2(low, inline_entry_point());\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  CompiledEntrySignature ces(m);\n+  ces.compute_calling_conventions(false);\n+  const GrowableArray<SigEntry>* sig_cc;\n+  const VMRegPair* regs;\n+  if (block_begin == verified_entry_point()) {\n+    sig_cc = ces.sig_cc();\n+    regs = ces.regs_cc();\n+  } else if (block_begin == verified_inline_entry_point()) {\n+    sig_cc = ces.sig();\n+    regs = ces.regs();\n+  } else if (block_begin == verified_inline_ro_entry_point()) {\n+    sig_cc = ces.sig_cc_ro();\n+    regs = ces.regs_cc_ro();\n+  } else {\n+    return;\n+  }\n+\n+  bool has_this = !m->is_static();\n+  if (ces.has_inline_recv() && block_begin == verified_entry_point()) {\n+    \/\/ <this> argument is scalarized for verified_entry_point()\n+    has_this = false;\n+  }\n+  const char* spname = \"sp\"; \/\/ make arch-specific?\n+  int stack_slot_offset = this->frame_size() * wordSize;\n+  int tab1 = 14, tab2 = 24;\n+  int sig_index = 0;\n+  int arg_index = has_this ? -1 : 0;\n+  bool did_old_sp = false;\n+  for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n+    bool at_this = (arg_index == -1);\n+    bool at_old_sp = false;\n+    BasicType t = (*sig)._bt;\n+    if (at_this) {\n+      stream->print(\"  # this: \");\n+    } else {\n+      stream->print(\"  # parm%d: \", arg_index);\n+    }\n+    stream->move_to(tab1);\n+    VMReg fst = regs[sig_index].first();\n+    VMReg snd = regs[sig_index].second();\n+    if (fst->is_reg()) {\n+      stream->print(\"%s\", fst->name());\n+      if (snd->is_valid())  {\n+        stream->print(\":%s\", snd->name());\n@@ -3105,6 +3115,18 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n-        stream->print(\"  (%s of caller)\", spname);\n-        stream->cr();\n+    } else if (fst->is_stack()) {\n+      int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+      if (offset == stack_slot_offset)  at_old_sp = true;\n+      stream->print(\"[%s+0x%x]\", spname, offset);\n+    } else {\n+      stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+    }\n+    stream->print(\" \");\n+    stream->move_to(tab2);\n+    stream->print(\"= \");\n+    if (at_this) {\n+      m->method_holder()->print_value_on(stream);\n+    } else {\n+      bool did_name = false;\n+      if (is_reference_type(t)) {\n+        Symbol* name = (*sig)._symbol;\n+        name->print_value_on(stream);\n+        did_name = true;\n@@ -3112,0 +3134,2 @@\n+      if (!did_name)\n+        stream->print(\"%s\", type2name(t));\n@@ -3113,0 +3137,14 @@\n+    if (at_old_sp) {\n+      stream->print(\"  (%s of caller)\", spname);\n+      did_old_sp = true;\n+    }\n+    stream->cr();\n+    sig_index += type2size[t];\n+    arg_index += 1;\n+  }\n+  if (!did_old_sp) {\n+    stream->print(\"  # \");\n+    stream->move_to(tab1);\n+    stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+    stream->print(\"  (%s of caller)\", spname);\n+    stream->cr();\n@@ -3236,1 +3274,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_scalarized=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_scalarized());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":132,"deletions":94,"binary":false,"changes":226,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"compiler\/compilerDefinitions.hpp\"\n@@ -197,0 +198,3 @@\n+  address _inline_entry_point;               \/\/ inline type entry point (unpack all inline type args) with class check\n+  address _verified_inline_entry_point;      \/\/ inline type entry point (unpack all inline type args) without class check\n+  address _verified_inline_ro_entry_point;   \/\/ inline type entry point (unpack receiver only) without class check\n@@ -432,2 +436,5 @@\n-  address entry_point() const                     { return _entry_point;             } \/\/ normal entry point\n-  address verified_entry_point() const            { return _verified_entry_point;    } \/\/ if klass is correct\n+  address entry_point() const                     { return _entry_point;             }        \/\/ normal entry point\n+  address verified_entry_point() const            { return _verified_entry_point;    }        \/\/ normal entry point without class check\n+  address inline_entry_point() const              { return _inline_entry_point; }             \/\/ inline type entry point (unpack all inline type args)\n+  address verified_inline_entry_point() const     { return _verified_inline_entry_point; }    \/\/ inline type entry point (unpack all inline type args) without class check\n+  address verified_inline_ro_entry_point() const  { return _verified_inline_ro_entry_point; } \/\/ inline type entry point (only unpack receiver) without class check\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+  _return_scalarized = pd->return_scalarized();\n@@ -56,0 +57,1 @@\n+  _return_scalarized = false;\n","filename":"src\/hotspot\/share\/code\/scopeDesc.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+  bool return_scalarized() const { return _return_scalarized; }\n@@ -108,0 +109,1 @@\n+  bool          _return_scalarized;\n@@ -111,1 +113,0 @@\n-\n","filename":"src\/hotspot\/share\/code\/scopeDesc.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -212,1 +212,1 @@\n-address VtableStubs::find_stub(bool is_vtable_stub, int vtable_index) {\n+address VtableStubs::find_stub(bool is_vtable_stub, int vtable_index, bool caller_is_c1) {\n@@ -218,1 +218,1 @@\n-    s = lookup(is_vtable_stub, vtable_index);\n+    s = lookup(is_vtable_stub, vtable_index, caller_is_c1);\n@@ -221,1 +221,1 @@\n-        s = create_vtable_stub(vtable_index);\n+        s = create_vtable_stub(vtable_index, caller_is_c1);\n@@ -223,1 +223,1 @@\n-        s = create_itable_stub(vtable_index);\n+        s = create_itable_stub(vtable_index, caller_is_c1);\n@@ -231,1 +231,1 @@\n-      enter(is_vtable_stub, vtable_index, s);\n+      enter(is_vtable_stub, vtable_index, caller_is_c1, s);\n@@ -233,1 +233,1 @@\n-        tty->print_cr(\"Decoding VtableStub %s[%d]@\" INTX_FORMAT,\n+        tty->print_cr(\"Decoding VtableStub (%s) %s[%d]@\" INTX_FORMAT, caller_is_c1 ? \"c1\" : \"full opt\",\n@@ -242,1 +242,1 @@\n-        JvmtiExport::post_dynamic_code_generated_while_holding_locks(is_vtable_stub? \"vtable stub\": \"itable stub\",\n+        JvmtiExport::post_dynamic_code_generated_while_holding_locks(is_vtable_stub? \"vtable stub\": \"itable stub\",  \/\/ FIXME: need to pass caller_is_c1??\n@@ -251,1 +251,1 @@\n-inline uint VtableStubs::hash(bool is_vtable_stub, int vtable_index){\n+inline uint VtableStubs::hash(bool is_vtable_stub, int vtable_index, bool caller_is_c1) {\n@@ -254,0 +254,3 @@\n+  if (caller_is_c1) {\n+    hash = 7 - hash;\n+  }\n@@ -258,1 +261,1 @@\n-VtableStub* VtableStubs::lookup(bool is_vtable_stub, int vtable_index) {\n+VtableStub* VtableStubs::lookup(bool is_vtable_stub, int vtable_index, bool caller_is_c1) {\n@@ -260,1 +263,1 @@\n-  unsigned hash = VtableStubs::hash(is_vtable_stub, vtable_index);\n+  unsigned hash = VtableStubs::hash(is_vtable_stub, vtable_index, caller_is_c1);\n@@ -262,1 +265,1 @@\n-  while( s && !s->matches(is_vtable_stub, vtable_index)) s = s->next();\n+  while( s && !s->matches(is_vtable_stub, vtable_index, caller_is_c1)) s = s->next();\n@@ -267,1 +270,1 @@\n-void VtableStubs::enter(bool is_vtable_stub, int vtable_index, VtableStub* s) {\n+void VtableStubs::enter(bool is_vtable_stub, int vtable_index, bool caller_is_c1, VtableStub* s) {\n@@ -269,2 +272,2 @@\n-  assert(s->matches(is_vtable_stub, vtable_index), \"bad vtable stub\");\n-  unsigned int h = VtableStubs::hash(is_vtable_stub, vtable_index);\n+  assert(s->matches(is_vtable_stub, vtable_index, caller_is_c1), \"bad vtable stub\");\n+  unsigned int h = VtableStubs::hash(is_vtable_stub, vtable_index, caller_is_c1);\n@@ -280,1 +283,1 @@\n-  uint hash = VtableStubs::hash(stub->is_vtable_stub(), stub->index());\n+  uint hash = VtableStubs::hash(stub->is_vtable_stub(), stub->index(), stub->caller_is_c1());\n","filename":"src\/hotspot\/share\/code\/vtableStubs.cpp","additions":18,"deletions":15,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -89,6 +89,6 @@\n-  static VtableStub* create_vtable_stub(int vtable_index);\n-  static VtableStub* create_itable_stub(int vtable_index);\n-  static VtableStub* lookup            (bool is_vtable_stub, int vtable_index);\n-  static void        enter             (bool is_vtable_stub, int vtable_index, VtableStub* s);\n-  static inline uint hash              (bool is_vtable_stub, int vtable_index);\n-  static address     find_stub         (bool is_vtable_stub, int vtable_index);\n+  static VtableStub* create_vtable_stub(int vtable_index, bool caller_is_c1);\n+  static VtableStub* create_itable_stub(int vtable_index, bool caller_is_c1);\n+  static VtableStub* lookup            (bool is_vtable_stub, int vtable_index, bool caller_is_c1);\n+  static void        enter             (bool is_vtable_stub, int vtable_index, bool caller_is_c1, VtableStub* s);\n+  static inline uint hash              (bool is_vtable_stub, int vtable_index, bool caller_is_c1);\n+  static address     find_stub         (bool is_vtable_stub, int vtable_index, bool caller_is_c1);\n@@ -104,2 +104,2 @@\n-  static address     find_vtable_stub(int vtable_index) { return find_stub(true,  vtable_index); }\n-  static address     find_itable_stub(int itable_index) { return find_stub(false, itable_index); }\n+  static address     find_vtable_stub(int vtable_index, bool caller_is_c1) { return find_stub(true,  vtable_index, caller_is_c1); }\n+  static address     find_itable_stub(int itable_index, bool caller_is_c1) { return find_stub(false, itable_index, caller_is_c1); }\n@@ -129,0 +129,2 @@\n+  bool           _caller_is_c1;      \/\/ True if this is for a caller compiled by C1,\n+                                     \/\/ which doesn't scalarize parameters.\n@@ -133,1 +135,1 @@\n-  VtableStub(bool is_vtable_stub, int index)\n+  VtableStub(bool is_vtable_stub, int index, bool caller_is_c1)\n@@ -135,1 +137,1 @@\n-          _is_vtable_stub(is_vtable_stub) {}\n+          _is_vtable_stub(is_vtable_stub), _caller_is_c1(caller_is_c1) {}\n@@ -147,2 +149,2 @@\n-  bool matches(bool is_vtable_stub, int index) const {\n-    return _index == index && _is_vtable_stub == is_vtable_stub;\n+  bool matches(bool is_vtable_stub, int index, bool caller_is_c1) const {\n+    return _index == index && _is_vtable_stub == is_vtable_stub && _caller_is_c1 == caller_is_c1;\n@@ -176,0 +178,1 @@\n+  bool caller_is_c1()                            { return  _caller_is_c1;   }\n","filename":"src\/hotspot\/share\/code\/vtableStubs.hpp","additions":15,"deletions":12,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -1211,1 +1211,1 @@\n-          if (vfst.method()->is_static_initializer() ||\n+        if (vfst.method()->is_class_initializer() ||\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -475,0 +475,2 @@\n+    case Bytecodes::_aconst_init:\n+    case Bytecodes::_withfield:\n","filename":"src\/hotspot\/share\/compiler\/methodLiveness.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -306,2 +306,3 @@\n-          !vmSymbols::class_initializer_name()->equals(method_name)) {\n-        error_msg = \"Chars '<' and '>' only allowed in <init> and <clinit>\";\n+          !vmSymbols::class_initializer_name()->equals(method_name) &&\n+          !vmSymbols::inline_factory_name()->equals(method_name)) {\n+        error_msg = \"Chars '<' and '>' only allowed in <init>, <clinit> and <vnew>\";\n","filename":"src\/hotspot\/share\/compiler\/methodMatcher.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/compiler\/oopMap.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManager.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"classfile\/vmSymbols.hpp\"\n@@ -30,0 +31,2 @@\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/objArrayKlass.inline.hpp\"\n@@ -55,0 +58,27 @@\n+void BarrierSet::throw_array_null_pointer_store_exception(arrayOop src, arrayOop dst, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  Klass* bound = ObjArrayKlass::cast(dst->klass())->element_klass();\n+  stringStream ss;\n+  ss.print(\"arraycopy: can not copy null values into %s[]\",\n+           bound->external_name());\n+  THROW_MSG(vmSymbols::java_lang_NullPointerException(), ss.as_string());\n+}\n+\n+void BarrierSet::throw_array_store_exception(arrayOop src, arrayOop dst, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  Klass* bound = ObjArrayKlass::cast(dst->klass())->element_klass();\n+  Klass* stype = ObjArrayKlass::cast(src->klass())->element_klass();\n+  stringStream ss;\n+  if (!bound->is_subtype_of(stype)) {\n+    ss.print(\"arraycopy: type mismatch: can not copy %s[] into %s[]\",\n+             stype->external_name(), bound->external_name());\n+  } else {\n+    \/\/ oop_arraycopy should return the index in the source array that\n+    \/\/ contains the problematic oop.\n+    ss.print(\"arraycopy: element type mismatch: can not cast one of the elements\"\n+             \" of %s[] to the type of the destination array, %s\",\n+             stype->external_name(), bound->external_name());\n+  }\n+  THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSet.cpp","additions":30,"deletions":0,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"utilities\/exceptions.hpp\"\n@@ -122,0 +123,3 @@\n+  static void throw_array_null_pointer_store_exception(arrayOop src, arrayOop dst, TRAPS);\n+  static void throw_array_store_exception(arrayOop src, arrayOop dst, TRAPS);\n+\n@@ -285,1 +289,1 @@\n-    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -316,0 +320,5 @@\n+\n+    static void value_copy_in_heap(void* src, void* dst, InlineKlass* md) {\n+      Raw::value_copy(src, dst, md);\n+    }\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSet.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/thread.hpp\"\n@@ -37,1 +39,1 @@\n-inline bool BarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+inline void BarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -43,1 +45,2 @@\n-  if (!HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value) {\n+  if ((!HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value) &&\n+      (!HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value)) {\n@@ -45,1 +48,2 @@\n-    return Raw::oop_arraycopy(nullptr, 0, src, nullptr, 0, dst, length);\n+    Raw::oop_arraycopy(nullptr, 0, src, nullptr, 0, dst, length);\n+    return;\n@@ -52,2 +56,8 @@\n-    if (!oopDesc::is_instanceof_or_null(CompressedOops::decode(elem), dst_klass)) {\n-      return false;\n+    if (HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value && CompressedOops::is_null(elem)) {\n+      throw_array_null_pointer_store_exception(src_obj, dst_obj, JavaThread::current());\n+      return;\n+    }\n+    if (HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value &&\n+        (!oopDesc::is_instanceof_or_null(CompressedOops::decode(elem), dst_klass))) {\n+      throw_array_store_exception(src_obj, dst_obj, JavaThread::current());\n+      return;\n@@ -57,2 +67,0 @@\n-\n-  return true;\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSet.inline.hpp","additions":15,"deletions":7,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -46,0 +46,4 @@\n+Node* C2ParseAccess::control() const {\n+  return _ctl == nullptr ? _kit->control() : _ctl;\n+}\n+\n@@ -153,1 +157,1 @@\n-    Node* control = control_dependent ? kit->control() : nullptr;\n+    Node* control = control_dependent ? parse_access.control() : nullptr;\n@@ -679,1 +683,1 @@\n-  ArrayCopyNode* ac = ArrayCopyNode::make(kit, false, src_base, offset,  dst_base, offset, payload_size, true, false);\n+  ArrayCopyNode* ac = ArrayCopyNode::make(kit, false, src_base, offset, dst_base, offset, payload_size, true, false);\n@@ -780,1 +784,1 @@\n-  phase->igvn().replace_node(ac, call);\n+  phase->replace_node(ac, call);\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -149,0 +149,1 @@\n+  Node* _ctl;\n@@ -154,1 +155,2 @@\n-                BasicType type, Node* base, C2AccessValuePtr& addr) :\n+                BasicType type, Node* base, C2AccessValuePtr& addr,\n+                Node* ctl = NULL) :\n@@ -156,1 +158,2 @@\n-    _kit(kit) {\n+    _kit(kit),\n+    _ctl(ctl) {\n@@ -161,0 +164,1 @@\n+  Node* control() const;\n@@ -238,1 +242,1 @@\n-  virtual void clone(GraphKit* kit, Node* src, Node* dst, Node* size, bool is_array) const;\n+  virtual void clone(GraphKit* kit, Node* src_base, Node* dst_base, Node* size, bool is_array) const;\n@@ -266,1 +270,1 @@\n-  virtual void eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const { }\n+  virtual void eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const { }\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.hpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -161,1 +161,1 @@\n-void CardTableBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+void CardTableBarrierSetC2::eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const {\n@@ -163,10 +163,16 @@\n-  Node *shift = node->unique_out();\n-  Node *addp = shift->unique_out();\n-  for (DUIterator_Last jmin, j = addp->last_outs(jmin); j >= jmin; --j) {\n-    Node *mem = addp->last_out(j);\n-    if (UseCondCardMark && mem->is_Load()) {\n-      assert(mem->Opcode() == Op_LoadB, \"unexpected code shape\");\n-      \/\/ The load is checking if the card has been written so\n-      \/\/ replace it with zero to fold the test.\n-      macro->replace_node(mem, macro->intcon(0));\n-      continue;\n+  for (DUIterator_Last imin, i = node->last_outs(imin); i >= imin; --i) {\n+    Node* shift = node->last_out(i);\n+    for (DUIterator_Last jmin, j = shift->last_outs(jmin); j >= jmin; --j) {\n+      Node* addp = shift->last_out(j);\n+      for (DUIterator_Last kmin, k = addp->last_outs(kmin); k >= kmin; --k) {\n+        Node* mem = addp->last_out(k);\n+        if (UseCondCardMark && mem->is_Load()) {\n+          assert(mem->Opcode() == Op_LoadB, \"unexpected code shape\");\n+          \/\/ The load is checking if the card has been written so\n+          \/\/ replace it with zero to fold the test.\n+          igvn->replace_node(mem, igvn->intcon(0));\n+          continue;\n+        }\n+        assert(mem->is_Store(), \"store required\");\n+        igvn->replace_node(mem, mem->in(MemNode::Memory));\n+      }\n@@ -174,2 +180,0 @@\n-    assert(mem->is_Store(), \"store required\");\n-    macro->replace_node(mem, mem->in(MemNode::Memory));\n@@ -180,1 +184,1 @@\n-  bool is_oop = is_reference_type(type);\n+  bool is_oop = type == T_OBJECT || type == T_ARRAY;\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.cpp","additions":18,"deletions":14,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -287,0 +287,1 @@\n+  oop obj_buffer_allocate(Klass* klass, size_t size, TRAPS); \/\/ doesn't clear memory\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -408,2 +408,1 @@\n-  \/\/ May be bootstrapping\n-  oopDesc::set_mark(mem, markWord::prototype());\n+  oopDesc::set_mark(mem, Klass::default_prototype_header(_klass));\n@@ -422,0 +421,6 @@\n+oop ObjBufferAllocator::initialize(HeapWord* mem) const {\n+  oopDesc::set_klass_gap(mem, 0);\n+  return finish(mem);\n+}\n+\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -72,1 +72,0 @@\n-\n@@ -88,1 +87,1 @@\n-    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -91,1 +90,5 @@\n-\n+  private:\n+    \/\/ Failing checkcast or check null during copy, still needs barrier\n+    template <typename T>\n+    static inline void oop_arraycopy_partial_barrier(BarrierSetT *bs, T* dst_raw, T* p);\n+  public:\n@@ -105,0 +108,2 @@\n+\n+    static void value_copy_in_heap(void* src, void* dst, InlineKlass* md);\n","filename":"src\/hotspot\/share\/gc\/shared\/modRefBarrierSet.hpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -95,1 +96,12 @@\n-inline bool ModRefBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n+inline void ModRefBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n+oop_arraycopy_partial_barrier(BarrierSetT *bs, T* dst_raw, T* p) {\n+  const size_t pd = pointer_delta(p, dst_raw, (size_t)heapOopSize);\n+  \/\/ pointer delta is scaled to number of elements (length field in\n+  \/\/ objArrayOop) which we assume is 32 bit.\n+  assert(pd == (size_t)(int)pd, \"length field overflow\");\n+  bs->write_ref_array((HeapWord*)dst_raw, pd);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline void ModRefBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n@@ -104,1 +116,2 @@\n-  if (!HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value) {\n+  if ((!HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value) &&\n+      (!HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value)) {\n@@ -117,11 +130,11 @@\n-      if (oopDesc::is_instanceof_or_null(CompressedOops::decode(element), bound)) {\n-        bs->template write_ref_field_pre<decorators>(p);\n-        *p = element;\n-      } else {\n-        \/\/ We must do a barrier to cover the partial copy.\n-        const size_t pd = pointer_delta(p, dst_raw, (size_t)heapOopSize);\n-        \/\/ pointer delta is scaled to number of elements (length field in\n-        \/\/ objArrayOop) which we assume is 32 bit.\n-        assert(pd == (size_t)(int)pd, \"length field overflow\");\n-        bs->write_ref_array((HeapWord*)dst_raw, pd);\n-        return false;\n+      \/\/ Apply any required checks\n+      if (HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value && CompressedOops::is_null(element)) {\n+        oop_arraycopy_partial_barrier(bs, dst_raw, p);\n+        throw_array_null_pointer_store_exception(src_obj, dst_obj, JavaThread::current());\n+        return;\n+      }\n+      if (HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value &&\n+          (!oopDesc::is_instanceof_or_null(CompressedOops::decode(element), bound))) {\n+        oop_arraycopy_partial_barrier(bs, dst_raw, p);\n+        throw_array_store_exception(src_obj, dst_obj, JavaThread::current());\n+        return;\n@@ -129,0 +142,3 @@\n+      \/\/ write\n+      bs->template write_ref_field_pre<decorators>(p);\n+      *p = element;\n@@ -132,1 +148,0 @@\n-  return true;\n@@ -143,0 +158,32 @@\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ModRefBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n+value_copy_in_heap(void* src, void* dst, InlineKlass* md) {\n+  if (HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value || (!md->contains_oops())) {\n+    Raw::value_copy(src, dst, md);\n+  } else {\n+    BarrierSetT* bs = barrier_set_cast<BarrierSetT>(BarrierSet::barrier_set());\n+    \/\/ src\/dst aren't oops, need offset to adjust oop map offset\n+    const address dst_oop_addr_offset = ((address) dst) - md->first_field_offset();\n+    typedef typename ValueOopType<decorators>::type OopType;\n+\n+    \/\/ Pre-barriers...\n+    OopMapBlock* map = md->start_of_nonstatic_oop_maps();\n+    OopMapBlock* const end = map + md->nonstatic_oop_map_count();\n+    while (map != end) {\n+      address doop_address = dst_oop_addr_offset + map->offset();\n+      bs->write_ref_array_pre((OopType*) doop_address, map->count(), false);\n+      map++;\n+    }\n+\n+    Raw::value_copy(src, dst, md);\n+\n+    \/\/ Post-barriers...\n+    map = md->start_of_nonstatic_oop_maps();\n+    while (map != end) {\n+      address doop_address = dst_oop_addr_offset + map->offset();\n+      bs->write_ref_array((HeapWord*) doop_address, map->count());\n+      map++;\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/modRefBarrierSet.inline.hpp","additions":61,"deletions":14,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -831,1 +831,1 @@\n-    Node* offset = phase->igvn().MakeConX(in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n+    Node* offset = phase->MakeConX(in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n@@ -883,1 +883,1 @@\n-    phase->igvn().replace_node(ac, call);\n+    phase->replace_node(ac, call);\n@@ -909,1 +909,1 @@\n-void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* n) const {\n+void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseIterGVN* igvn, Node* n) const {\n@@ -911,1 +911,1 @@\n-    shenandoah_eliminate_wb_pre(n, &macro->igvn());\n+    shenandoah_eliminate_wb_pre(n, igvn);\n@@ -1044,1 +1044,1 @@\n-    uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain()->cnt();\n+    uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain_sig()->cnt();\n@@ -1130,1 +1130,1 @@\n-        uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain()->cnt();\n+        uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain_sig()->cnt();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -466,1 +466,1 @@\n-        const TypeTuple* args = n->as_Call()->_tf->domain();\n+        const TypeTuple* args = n->as_Call()->_tf->domain_sig();\n@@ -587,1 +587,1 @@\n-      uint stop = n->is_Call() ? n->as_Call()->tf()->domain()->cnt() : n->req();\n+      uint stop = n->is_Call() ? n->as_Call()->tf()->domain_sig()->cnt() : n->req();\n@@ -807,6 +807,5 @@\n-        CallProjections projs;\n-        c->as_Call()->extract_projections(&projs, true, false);\n-        if (projs.fallthrough_memproj != nullptr) {\n-          if (projs.fallthrough_memproj->adr_type() == TypePtr::BOTTOM) {\n-            if (projs.catchall_memproj == nullptr) {\n-              mem = projs.fallthrough_memproj;\n+        CallProjections* projs = c->as_Call()->extract_projections(true, false);\n+        if (projs->fallthrough_memproj != nullptr) {\n+          if (projs->fallthrough_memproj->adr_type() == TypePtr::BOTTOM) {\n+            if (projs->catchall_memproj == nullptr) {\n+              mem = projs->fallthrough_memproj;\n@@ -814,2 +813,2 @@\n-              if (phase->is_dominator(projs.fallthrough_catchproj, ctrl)) {\n-                mem = projs.fallthrough_memproj;\n+              if (phase->is_dominator(projs->fallthrough_catchproj, ctrl)) {\n+                mem = projs->fallthrough_memproj;\n@@ -817,2 +816,2 @@\n-                assert(phase->is_dominator(projs.catchall_catchproj, ctrl), \"one proj must dominate barrier\");\n-                mem = projs.catchall_memproj;\n+                assert(phase->is_dominator(projs->catchall_catchproj, ctrl), \"one proj must dominate barrier\");\n+                mem = projs->catchall_memproj;\n@@ -1078,1 +1077,1 @@\n-static Node* create_phis_on_call_return(Node* ctrl, Node* c, Node* n, Node* n_clone, const CallProjections& projs, PhaseIdealLoop* phase) {\n+static Node* create_phis_on_call_return(Node* ctrl, Node* c, Node* n, Node* n_clone, const CallProjections* projs, PhaseIdealLoop* phase) {\n@@ -1090,1 +1089,1 @@\n-    if (phase->is_dominator(projs.fallthrough_catchproj, in)) {\n+    if (phase->is_dominator(projs->fallthrough_catchproj, in)) {\n@@ -1092,1 +1091,1 @@\n-    } else if (phase->is_dominator(projs.catchall_catchproj, in)) {\n+    } else if (phase->is_dominator(projs->catchall_catchproj, in)) {\n@@ -1208,3 +1207,1 @@\n-      CallProjections projs;\n-      call->extract_projections(&projs, false, false);\n-\n+      CallProjections* projs = call->extract_projections(false, false);\n@@ -1215,2 +1212,2 @@\n-      phase->register_new_node(lrb_clone, projs.catchall_catchproj);\n-      phase->set_ctrl(lrb, projs.fallthrough_catchproj);\n+      phase->register_new_node(lrb_clone, projs->catchall_catchproj);\n+      phase->set_ctrl(lrb, projs->fallthrough_catchproj);\n@@ -1238,1 +1235,1 @@\n-          if (phase->is_dominator(call, c) && phase->is_dominator(c, projs.fallthrough_proj)) {\n+          if (phase->is_dominator(call, c) && phase->is_dominator(c, projs->fallthrough_proj)) {\n@@ -1246,1 +1243,1 @@\n-            phase->register_new_node(u_clone, projs.catchall_catchproj);\n+            phase->register_new_node(u_clone, projs->catchall_catchproj);\n@@ -1248,1 +1245,1 @@\n-            phase->set_ctrl(u, projs.fallthrough_catchproj);\n+            phase->set_ctrl(u, projs->fallthrough_catchproj);\n@@ -1254,1 +1251,1 @@\n-                  if (phase->is_dominator(projs.catchall_catchproj, u->in(0)->in(k))) {\n+                  if (phase->is_dominator(projs->catchall_catchproj, u->in(0)->in(k))) {\n@@ -1257,1 +1254,1 @@\n-                  } else if (!phase->is_dominator(projs.fallthrough_catchproj, u->in(0)->in(k))) {\n+                  } else if (!phase->is_dominator(projs->fallthrough_catchproj, u->in(0)->in(k))) {\n@@ -1264,1 +1261,1 @@\n-              if (phase->is_dominator(projs.catchall_catchproj, c)) {\n+              if (phase->is_dominator(projs->catchall_catchproj, c)) {\n@@ -1269,1 +1266,1 @@\n-              } else if (!phase->is_dominator(projs.fallthrough_catchproj, c)) {\n+              } else if (!phase->is_dominator(projs->fallthrough_catchproj, c)) {\n@@ -2342,5 +2339,4 @@\n-    CallProjections projs;\n-    call->extract_projections(&projs, true, false);\n-    if (projs.catchall_memproj != nullptr) {\n-      if (projs.fallthrough_memproj == n) {\n-        c = projs.fallthrough_catchproj;\n+    CallProjections* projs = call->extract_projections(true, false);\n+    if (projs->catchall_memproj != nullptr) {\n+      if (projs->fallthrough_memproj == n) {\n+        c = projs->fallthrough_catchproj;\n@@ -2348,2 +2344,2 @@\n-        assert(projs.catchall_memproj == n, \"\");\n-        c = projs.catchall_catchproj;\n+        assert(projs->catchall_memproj == n, \"\");\n+        c = projs->catchall_catchproj;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":30,"deletions":34,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -332,1 +332,1 @@\n-bool ShenandoahBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+void ShenandoahBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -339,1 +339,1 @@\n-  return Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n+  Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n-    \/\/ will re-resovle the call and update the compiled IC.\n+    \/\/ will re-resolve the call and update the compiled IC.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSetNMethod.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -107,1 +107,1 @@\n-    number_of_result_handlers = 10                              \/\/ number of result handlers for native calls\n+    number_of_result_handlers = 11                              \/\/ number of result handlers for native calls\n","filename":"src\/hotspot\/share\/interpreter\/abstractInterpreter.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -257,0 +257,1 @@\n+  bool is_withfield() const                      { return java_code() == Bytecodes::_withfield; }\n@@ -264,1 +265,2 @@\n-                                                          is_putstatic(); }\n+                                                          is_putstatic()  ||\n+                                                          is_withfield(); }\n@@ -297,0 +299,9 @@\n+class Bytecode_aconst_init: public Bytecode {\n+ public:\n+  Bytecode_aconst_init(Method* method, address bcp): Bytecode(method, bcp) { verify(); }\n+  void verify() const { assert(java_code() == Bytecodes::_aconst_init, \"check aconst_init\"); }\n+\n+  \/\/ Returns index\n+  long index() const   { return get_index_u2(Bytecodes::_aconst_init); };\n+};\n+\n","filename":"src\/hotspot\/share\/interpreter\/bytecode.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -601,0 +601,1 @@\n+    case Bytecodes::_withfield:\n@@ -632,0 +633,1 @@\n+    case Bytecodes::_aconst_init:\n","filename":"src\/hotspot\/share\/interpreter\/bytecodeTracer.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -983,0 +983,1 @@\n+    case Bytecodes::_withfield:\n@@ -1114,0 +1115,1 @@\n+    case Bytecodes::_checkcast:\n@@ -1134,0 +1136,1 @@\n+    case Bytecodes::_withfield:\n@@ -1174,1 +1177,7 @@\n-    os->print(\"\\\" is null\");\n+    address code_base = _method->constMethod()->code_base();\n+    Bytecodes::Code code = Bytecodes::java_code_at(_method, code_base + bci);\n+    if (code == Bytecodes::_aastore) {\n+      os->print(\"\\\" is null or is a null-free array and there's an attempt to store null in it\");\n+    } else {\n+      os->print(\"\\\" is null\");\n+    }\n@@ -1424,0 +1433,1 @@\n+    case Bytecodes::_withfield:\n@@ -1436,0 +1446,5 @@\n+    case Bytecodes::_checkcast: {\n+        int cp_index = Bytes::get_Java_u2(code_base + pos);\n+        ConstantPool* cp = _method->constants();\n+        os->print(\"Cannot cast to null-free type \\\"%s\\\"\", cp->klass_at_noresolve(cp_index)->as_C_string());\n+      } break;\n","filename":"src\/hotspot\/share\/interpreter\/bytecodeUtils.cpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -477,0 +477,2 @@\n+  def(_aconst_init         , \"aconst_ init\"        , \"bkk\"  , nullptr    , T_OBJECT ,  1, true);\n+  def(_withfield           , \"withfield\"           , \"bJJ\"  , nullptr    , T_OBJECT , -1, true);\n@@ -482,0 +484,1 @@\n+  def(_fast_qgetfield      , \"fast_qgetfield\"      , \"bJJ\"  , nullptr    , T_OBJECT ,  0, true , _getfield       );\n@@ -491,0 +494,1 @@\n+  def(_fast_qputfield      , \"fast_qputfield\"      , \"bJJ\"  , nullptr    , T_OBJECT ,  0, true , _putfield       );\n","filename":"src\/hotspot\/share\/interpreter\/bytecodes.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -247,0 +247,4 @@\n+    \/\/ value-type bytecodes\n+    _aconst_init          = 203, \/\/ 0xcb\n+    _withfield            = 204, \/\/ 0xcc\n+\n@@ -251,0 +255,1 @@\n+    _fast_qgetfield       ,\n@@ -260,0 +265,1 @@\n+    _fast_qputfield       ,\n","filename":"src\/hotspot\/share\/interpreter\/bytecodes.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -48,0 +49,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -78,0 +82,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -159,1 +164,3 @@\n-  oop java_class = klass->java_mirror();\n+  oop java_class = tag.is_Qdescriptor_klass()\n+                      ? InlineKlass::cast(klass)->val_mirror()\n+                      : klass->java_mirror();\n@@ -223,0 +230,4 @@\n+  if (klass->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_InstantiationError());\n+  }\n+\n@@ -247,0 +258,194 @@\n+JRT_ENTRY(void, InterpreterRuntime::aconst_init(JavaThread* current, ConstantPool* pool, int index))\n+  \/\/ Getting the InlineKlass\n+  Klass* k = pool->klass_at(index, CHECK);\n+  if (!k->is_inline_klass()) {\n+    \/\/ inconsistency with 'new' which throws an InstantiationError\n+    \/\/ in the future, aconst_init will just return null instead of throwing an exception\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+  assert(k->is_inline_klass(), \"aconst_init argument must be the inline type class\");\n+  InlineKlass* vklass = InlineKlass::cast(k);\n+\n+  vklass->initialize(CHECK);\n+  oop res = vklass->default_value();\n+  current->set_vm_result(res);\n+JRT_END\n+\n+JRT_ENTRY(int, InterpreterRuntime::withfield(JavaThread* current, ConstantPoolCacheEntry* cpe, uintptr_t ptr))\n+  oop obj = nullptr;\n+  int recv_offset = type2size[as_BasicType(cpe->flag_state())];\n+  assert(frame::interpreter_frame_expression_stack_direction() == -1, \"currently is -1 on all platforms\");\n+  int ret_adj = (recv_offset + type2size[T_OBJECT] )* AbstractInterpreter::stackElementSize;\n+  int offset = cpe->f2_as_offset();\n+  obj = (oopDesc*)(((uintptr_t*)ptr)[recv_offset * Interpreter::stackElementWords]);\n+  if (obj == nullptr) {\n+    THROW_(vmSymbols::java_lang_NullPointerException(), ret_adj);\n+  }\n+  assert(oopDesc::is_oop(obj), \"Verifying receiver\");\n+  assert(obj->klass()->is_inline_klass(), \"Must have been checked during resolution\");\n+  instanceHandle old_value_h(THREAD, (instanceOop)obj);\n+  oop ref = nullptr;\n+  if (cpe->flag_state() == atos) {\n+    ref = *(oopDesc**)ptr;\n+  }\n+  Handle ref_h(THREAD, ref);\n+  InlineKlass* ik = InlineKlass::cast(old_value_h()->klass());\n+  \/\/ Ensure that the class is initialized or being initialized\n+  \/\/ If the class is in error state, the creation of a new value should not be allowed\n+  ik->initialize(CHECK_(ret_adj));\n+\n+  bool can_skip = false;\n+  switch(cpe->flag_state()) {\n+    case ztos:\n+      if (old_value_h()->bool_field(offset) == (jboolean)(*(jint*)ptr)) can_skip = true;\n+      break;\n+    case btos:\n+      if (old_value_h()->byte_field(offset) == (jbyte)(*(jint*)ptr)) can_skip = true;\n+      break;\n+    case ctos:\n+      if (old_value_h()->char_field(offset) == (jchar)(*(jint*)ptr)) can_skip = true;\n+      break;\n+    case stos:\n+      if (old_value_h()->short_field(offset) == (jshort)(*(jint*)ptr)) can_skip = true;\n+      break;\n+    case itos:\n+      if (old_value_h()->int_field(offset) == *(jint*)ptr) can_skip = true;\n+      break;\n+    case ltos:\n+      if (old_value_h()->long_field(offset) == *(jlong*)ptr) can_skip = true;\n+      break;\n+    case ftos:\n+      if (memcmp(old_value_h()->field_addr<jfloat>(offset), (jfloat*)ptr, sizeof(jfloat)) == 0) can_skip = true;\n+      break;\n+    case dtos:\n+      if (memcmp(old_value_h()->field_addr<jdouble>(offset), (jdouble*)ptr, sizeof(jdouble)) == 0) can_skip = true;\n+      break;\n+    case atos:\n+      if (!cpe->is_inlined() && old_value_h()->obj_field(offset) == ref_h()) can_skip = true;\n+      break;\n+    default:\n+      break;\n+  }\n+  if (can_skip) {\n+    current->set_vm_result(old_value_h());\n+    return ret_adj;\n+  }\n+\n+  instanceOop new_value = ik->allocate_instance_buffer(CHECK_(ret_adj));\n+  Handle new_value_h = Handle(THREAD, new_value);\n+  ik->inline_copy_oop_to_new_oop(old_value_h(), new_value_h());\n+  switch(cpe->flag_state()) {\n+    case ztos:\n+      new_value_h()->bool_field_put(offset, (jboolean)(*(jint*)ptr));\n+      break;\n+    case btos:\n+      new_value_h()->byte_field_put(offset, (jbyte)(*(jint*)ptr));\n+      break;\n+    case ctos:\n+      new_value_h()->char_field_put(offset, (jchar)(*(jint*)ptr));\n+      break;\n+    case stos:\n+      new_value_h()->short_field_put(offset, (jshort)(*(jint*)ptr));\n+      break;\n+    case itos:\n+      new_value_h()->int_field_put(offset, (*(jint*)ptr));\n+      break;\n+    case ltos:\n+      new_value_h()->long_field_put(offset, *(jlong*)ptr);\n+      break;\n+    case ftos:\n+      new_value_h()->float_field_put(offset, *(jfloat*)ptr);\n+      break;\n+    case dtos:\n+      new_value_h()->double_field_put(offset, *(jdouble*)ptr);\n+      break;\n+    case atos:\n+      {\n+        if (cpe->is_null_free_inline_type())  {\n+          if (!cpe->is_inlined()) {\n+              if (ref_h() == nullptr) {\n+                THROW_(vmSymbols::java_lang_NullPointerException(), ret_adj);\n+              }\n+              new_value_h()->obj_field_put(offset, ref_h());\n+            } else {\n+              int field_index = cpe->field_index();\n+              InlineKlass* field_ik = InlineKlass::cast(ik->get_inline_type_field_klass(field_index));\n+              field_ik->write_inlined_field(new_value_h(), offset, ref_h(), CHECK_(ret_adj));\n+            }\n+        } else {\n+          new_value_h()->obj_field_put(offset, ref_h());\n+        }\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  current->set_vm_result(new_value_h());\n+  return ret_adj;\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::uninitialized_static_inline_type_field(JavaThread* current, oopDesc* mirror, int index))\n+  \/\/ The interpreter tries to access an inline static field that has not been initialized.\n+  \/\/ This situation can happen in different scenarios:\n+  \/\/   1 - if the load or initialization of the field failed during step 8 of\n+  \/\/       the initialization of the holder of the field, in this case the access to the field\n+  \/\/       must fail\n+  \/\/   2 - it can also happen when the initialization of the holder class triggered the initialization of\n+  \/\/       another class which accesses this field in its static initializer, in this case the\n+  \/\/       access must succeed to allow circularity\n+  \/\/ The code below tries to load and initialize the field's class again before returning the default value.\n+  \/\/ If the field was not initialized because of an error, an exception should be thrown.\n+  \/\/ If the class is being initialized, the default value is returned.\n+  instanceHandle mirror_h(THREAD, (instanceOop)mirror);\n+  InstanceKlass* klass = InstanceKlass::cast(java_lang_Class::as_Klass(mirror));\n+  assert(klass->field_signature(index)->is_Q_signature(), \"Sanity check\");\n+  if (klass->is_being_initialized() && klass->is_init_thread(THREAD)) {\n+    int offset = klass->field_offset(index);\n+    Klass* field_k = klass->get_inline_type_field_klass_or_null(index);\n+    if (field_k == nullptr) {\n+      field_k = SystemDictionary::resolve_or_fail(klass->field_signature(index)->fundamental_name(THREAD),\n+          Handle(THREAD, klass->class_loader()),\n+          Handle(THREAD, klass->protection_domain()),\n+          true, CHECK);\n+      assert(field_k != nullptr, \"Should have been loaded or an exception thrown above\");\n+      klass->set_inline_type_field_klass(index, field_k);\n+    }\n+    field_k->initialize(CHECK);\n+    oop defaultvalue = InlineKlass::cast(field_k)->default_value();\n+    \/\/ It is safe to initialize the static field because 1) the current thread is the initializing thread\n+    \/\/ and is the only one that can access it, and 2) the field is actually not initialized (i.e. null)\n+    \/\/ otherwise the JVM should not be executing this code.\n+    mirror_h()->obj_field_put(offset, defaultvalue);\n+    current->set_vm_result(defaultvalue);\n+  } else {\n+    assert(klass->is_in_error_state(), \"If not initializing, initialization must have failed to get there\");\n+    ResourceMark rm(THREAD);\n+    const char* desc = \"Could not initialize class \";\n+    const char* className = klass->external_name();\n+    size_t msglen = strlen(desc) + strlen(className) + 1;\n+    char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+    if (nullptr == message) {\n+      \/\/ Out of memory: can't create detailed error message\n+      THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), className);\n+    } else {\n+      jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+      THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), message);\n+    }\n+  }\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::read_inlined_field(JavaThread* current, oopDesc* obj, int index, Klass* field_holder))\n+  Handle obj_h(THREAD, obj);\n+\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+\n+  assert(field_holder->is_instance_klass(), \"Sanity check\");\n+  InstanceKlass* klass = InstanceKlass::cast(field_holder);\n+\n+  assert(klass->field_is_inlined(index), \"Sanity check\");\n+\n+  InlineKlass* field_vklass = InlineKlass::cast(klass->get_inline_type_field_klass(index));\n+\n+  oop res = field_vklass->read_inlined_field(obj_h(), klass->field_offset(index), CHECK);\n+  current->set_vm_result(res);\n+JRT_END\n@@ -256,1 +461,8 @@\n-  objArrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n+  bool      is_qtype_desc = pool->tag_at(index).is_Qdescriptor_klass();\n+  arrayOop obj;\n+  if ((!klass->is_array_klass()) && is_qtype_desc) { \/\/ Logically creates elements, ensure klass init\n+    klass->initialize(CHECK);\n+    obj = oopFactory::new_valueArray(klass, size, CHECK);\n+  } else {\n+    obj = oopFactory::new_objArray(klass, size, CHECK);\n+  }\n@@ -260,0 +472,10 @@\n+JRT_ENTRY(void, InterpreterRuntime::value_array_load(JavaThread* current, arrayOopDesc* array, int index))\n+  flatArrayHandle vah(current, (flatArrayOop)array);\n+  oop value_holder = flatArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK);\n+  current->set_vm_result(value_holder);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::value_array_store(JavaThread* current, void* val, arrayOopDesc* array, int index))\n+  assert(val != nullptr, \"can't store null into flat array\");\n+  ((flatArrayOop)array)->value_copy_to_index(cast_to_oop(val), index);\n+JRT_END\n@@ -265,2 +487,3 @@\n-  int          i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n-  Klass* klass   = constants->klass_at(i, CHECK);\n+  int i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n+  Klass* klass = constants->klass_at(i, CHECK);\n+  bool is_qtype = klass->name()->is_Q_array_signature();\n@@ -271,0 +494,4 @@\n+  if (is_qtype) { \/\/ Logically creates elements, ensure klass init\n+    klass->initialize(CHECK);\n+  }\n+\n@@ -295,0 +522,23 @@\n+JRT_ENTRY(jboolean, InterpreterRuntime::is_substitutable(JavaThread* current, oopDesc* aobj, oopDesc* bobj))\n+  assert(oopDesc::is_oop(aobj) && oopDesc::is_oop(bobj), \"must be valid oops\");\n+\n+  Handle ha(THREAD, aobj);\n+  Handle hb(THREAD, bobj);\n+  JavaValue result(T_BOOLEAN);\n+  JavaCallArguments args;\n+  args.push_oop(ha);\n+  args.push_oop(hb);\n+  methodHandle method(current, Universe::is_substitutable_method());\n+  JavaCalls::call(&result, method, &args, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    \/\/ Something really bad happened because isSubstitutable() should not throw exceptions\n+    \/\/ If it is an error, just let it propagate\n+    \/\/ If it is an exception, wrap it into an InternalError\n+    if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+      Handle e(THREAD, PENDING_EXCEPTION);\n+      CLEAR_PENDING_EXCEPTION;\n+      THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in substitutability test\", e, false);\n+    }\n+  }\n+  return result.get_jboolean();\n+JRT_END\n@@ -631,0 +881,4 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_InstantiationError(JavaThread* current))\n+  THROW(vmSymbols::java_lang_InstantiationError());\n+JRT_END\n+\n@@ -664,1 +918,1 @@\n-                    bytecode == Bytecodes::_putstatic);\n+                    bytecode == Bytecodes::_putstatic || bytecode == Bytecodes::_withfield);\n@@ -666,0 +920,1 @@\n+  bool is_inline_type  = bytecode == Bytecodes::_withfield;\n@@ -710,3 +965,9 @@\n-    get_code = ((is_static) ? Bytecodes::_getstatic : Bytecodes::_getfield);\n-    if ((is_put && !has_initialized_final_update) || !info.access_flags().is_final()) {\n-      put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_putfield);\n+    if (is_static) {\n+      get_code = Bytecodes::_getstatic;\n+    } else {\n+      get_code = Bytecodes::_getfield;\n+    }\n+    if (is_put && is_inline_type) {\n+        put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_withfield);\n+    } else if ((is_put && !has_initialized_final_update) || !info.access_flags().is_final()) {\n+        put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_putfield);\n@@ -724,1 +985,3 @@\n-    info.access_flags().is_volatile()\n+    info.access_flags().is_volatile(),\n+    info.is_inlined(),\n+    info.signature()->is_Q_signature() && info.is_inline_type()\n@@ -963,0 +1226,1 @@\n+  case Bytecodes::_withfield:\n@@ -1165,0 +1429,1 @@\n+  bool is_inlined = cp_entry->is_inlined();\n@@ -1173,1 +1438,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(cp_entry_f1, cp_entry->f2_as_index(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(cp_entry_f1, cp_entry->f2_as_index(), is_static, is_inlined);\n@@ -1203,0 +1468,6 @@\n+\n+  \/\/ Both Q-signatures and L-signatures are mapped to atos\n+  if (cp_entry->flag_state() == atos && ik->field_signature(index)->is_Q_signature()) {\n+    sig_type = JVM_SIGNATURE_PRIMITIVE_OBJECT;\n+  }\n+\n@@ -1204,0 +1475,1 @@\n+  bool is_inlined = cp_entry->is_inlined();\n@@ -1206,1 +1478,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, cp_entry->f2_as_index(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, cp_entry->f2_as_index(), is_static, is_inlined);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":283,"deletions":11,"binary":false,"changes":294,"status":"modified"},{"patch":"@@ -963,0 +963,1 @@\n+         byte == Bytecodes::_withfield ||\n@@ -967,1 +968,2 @@\n-  bool is_put    = (byte == Bytecodes::_putfield  || byte == Bytecodes::_putstatic || byte == Bytecodes::_nofast_putfield);\n+  bool is_put    = (byte == Bytecodes::_putfield  || byte == Bytecodes::_putstatic ||\n+                    byte == Bytecodes::_nofast_putfield || byte == Bytecodes::_withfield);\n@@ -973,0 +975,15 @@\n+\n+  if (byte == Bytecodes::_withfield && !resolved_klass->is_inline_klass()) {\n+    ResourceMark rm(THREAD);\n+    char msg[200];\n+    jio_snprintf(msg, sizeof(msg), \"Bytecode withfield cannot be used on identity class %s\", resolved_klass->external_name());\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(), msg);\n+  }\n+\n+  if (is_put && !is_static && byte != Bytecodes::_withfield && resolved_klass->is_inline_klass()) {\n+    ResourceMark rm(THREAD);\n+    char msg[200];\n+    jio_snprintf(msg, sizeof(msg), \"Bytecode putfield cannot be used on primitive class %s\", resolved_klass->external_name());\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(), msg);\n+  }\n+\n@@ -1004,0 +1021,2 @@\n+    \/\/ (3) by withfield when field is in a value type and the\n+    \/\/     selected class and current class are nest mates.\n@@ -1007,6 +1026,15 @@\n-        ResourceMark rm(THREAD);\n-        stringStream ss;\n-        ss.print(\"Update to %s final field %s.%s attempted from a different class (%s) than the field's declaring class\",\n-                 is_static ? \"static\" : \"non-static\", resolved_klass->external_name(), fd.name()->as_C_string(),\n-                current_klass->external_name());\n-        THROW_MSG(vmSymbols::java_lang_IllegalAccessError(), ss.as_string());\n+        \/\/ If byte code is a withfield check if they are nestmates.\n+        bool are_nestmates = false;\n+        if (sel_klass->is_instance_klass() &&\n+            InstanceKlass::cast(sel_klass)->is_inline_klass() &&\n+            current_klass->is_instance_klass()) {\n+          are_nestmates = InstanceKlass::cast(current_klass)->has_nestmate_access_to(InstanceKlass::cast(sel_klass), THREAD);\n+        }\n+        if (!are_nestmates) {\n+          ResourceMark rm(THREAD);\n+          stringStream ss;\n+          ss.print(\"Update to %s final field %s.%s attempted from a different class (%s) than the field's declaring class\",\n+                   is_static ? \"static\" : \"non-static\", resolved_klass->external_name(), fd.name()->as_C_string(),\n+                    current_klass->external_name());\n+          THROW_MSG(vmSymbols::java_lang_IllegalAccessError(), ss.as_string());\n+        }\n@@ -1020,1 +1048,1 @@\n-                                                   !m->is_static_initializer());\n+                                                   !m->is_class_initializer());\n@@ -1023,1 +1051,1 @@\n-                                                     !m->is_object_initializer());\n+                                                     !m->is_object_constructor());\n@@ -1151,0 +1179,2 @@\n+  \/\/ Since this method is never inherited from a super, any appearance here under\n+  \/\/ the wrong class would be an error.\n@@ -1221,1 +1251,1 @@\n-      \/\/ check if the method is not <init>\n+      \/\/ check if the method is not <init>, which is never inherited\n@@ -1643,2 +1673,2 @@\n-                             const methodHandle& attached_method,\n-                             Bytecodes::Code byte, TRAPS) {\n+                                  const methodHandle& attached_method,\n+                                  Bytecodes::Code byte, bool check_null_and_abstract, TRAPS) {\n@@ -1649,0 +1679,1 @@\n+  Klass* recv_klass = recv.is_null() ? defc : recv->klass();\n@@ -1651,2 +1682,2 @@\n-      resolve_virtual_call(result, recv, recv->klass(), link_info,\n-                           \/*check_null_and_abstract=*\/true, CHECK);\n+      resolve_virtual_call(result, recv, recv_klass, link_info,\n+                           check_null_and_abstract, CHECK);\n@@ -1655,2 +1686,2 @@\n-      resolve_interface_call(result, recv, recv->klass(), link_info,\n-                             \/*check_null_and_abstract=*\/true, CHECK);\n+      resolve_interface_call(result, recv, recv_klass, link_info,\n+                             check_null_and_abstract, CHECK);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":47,"deletions":16,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -346,1 +346,1 @@\n-                             Bytecodes::Code byte, TRAPS);\n+                             Bytecodes::Code byte, bool check_null_and_abstract, TRAPS);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -286,1 +286,1 @@\n-    bool v2 = vars[i].is_reference()  ? true : false;\n+    bool v2 = vars[i].is_reference();\n@@ -295,1 +295,1 @@\n-    bool v2 = stack[j].is_reference() ? true : false;\n+    bool v2 = stack[j].is_reference();\n@@ -378,1 +378,1 @@\n-    if ( cell->is_reference()) {\n+    if (cell->is_reference()) {\n","filename":"src\/hotspot\/share\/interpreter\/oopMapCache.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -182,1 +182,1 @@\n-    int  cp_index    = Bytes::get_Java_u2(p);\n+    int cp_index    = Bytes::get_Java_u2(p);\n@@ -218,1 +218,0 @@\n-\n@@ -463,1 +462,1 @@\n-                  if (!method->is_static_initializer()) {\n+                  if (!method->is_class_initializer()) {\n@@ -467,1 +466,1 @@\n-                  if (!method->is_object_initializer()) {\n+                  if (!method->is_object_constructor()) {\n@@ -479,0 +478,1 @@\n+      case Bytecodes::_withfield     : \/\/ fall through but may require more checks for correctness\n","filename":"src\/hotspot\/share\/interpreter\/rewriter.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -53,1 +53,2 @@\n-  T_OBJECT\n+  T_OBJECT ,\n+  T_PRIMITIVE_OBJECT\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreterGenerator.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -301,0 +301,1 @@\n+  static void withfield();\n@@ -303,0 +304,1 @@\n+  static void aconst_init();\n","filename":"src\/hotspot\/share\/interpreter\/templateTable.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1101,0 +1101,1 @@\n+      const bool return_scalarized     = false;\n@@ -1104,1 +1105,1 @@\n-                                      has_ea_local_in_scope, arg_escape,\n+                                      return_scalarized, has_ea_local_in_scope, arg_escape,\n@@ -1241,0 +1242,2 @@\n+      _offsets.set_value(CodeOffsets::Verified_Inline_Entry, pc_offset);\n+      _offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, pc_offset);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -85,1 +85,4 @@\n-    if (!mh->is_native() && !mh->is_static() && !mh->is_initializer()) {\n+    if (!mh->is_native() &&\n+        !mh->is_static() &&\n+        !mh->is_object_constructor() &&\n+        !mh->is_class_initializer()) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompiler.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1375,1 +1375,1 @@\n-              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false);\n+              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false, CHECK_NULL);\n@@ -1607,1 +1607,1 @@\n-  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false);\n+  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false, THREAD);\n@@ -1907,1 +1907,1 @@\n-    if (m->is_initializer() && !m->is_static()) {\n+    if (m->is_object_constructor()) {\n@@ -1934,1 +1934,1 @@\n-    if (!m->is_initializer() && !m->is_overpass()) {\n+    if (!(m->is_object_constructor() || m->is_class_initializer()) && !m->is_overpass()) {\n@@ -2613,2 +2613,1 @@\n-  if (m->is_initializer()) {\n-    if (m->is_static_initializer()) {\n+  if (m->is_class_initializer()) {\n@@ -2617,1 +2616,2 @@\n-    }\n+  }\n+  else if (m->is_object_constructor() || m->is_static_vnew_factory()) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -164,1 +164,1 @@\n-      _jca->push_oop(next_arg(T_OBJECT));\n+      (type == T_PRIMITIVE_OBJECT) ? _jca->push_oop(next_arg(T_PRIMITIVE_OBJECT)) : _jca->push_oop(next_arg(T_OBJECT));\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -163,1 +163,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u4)                                    \\\n@@ -600,0 +600,2 @@\n+  declare_constant(DataLayout::array_load_store_data_tag)                 \\\n+  declare_constant(DataLayout::acmp_data_tag)                             \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n+#include \"runtime\/reflectionUtils.hpp\"\n@@ -39,0 +41,1 @@\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n@@ -512,0 +515,132 @@\n+\n+class FindClassByNameClosure : public KlassInfoClosure {\n+ private:\n+  GrowableArray<Klass*>* _klasses;\n+  Symbol* _classname;\n+ public:\n+  FindClassByNameClosure(GrowableArray<Klass*>* klasses, Symbol* classname) :\n+    _klasses(klasses), _classname(classname) { }\n+\n+  void do_cinfo(KlassInfoEntry* cie) {\n+    if (cie->klass()->name() == _classname) {\n+      _klasses->append(cie->klass());\n+    }\n+  }\n+};\n+\n+class FieldDesc {\n+private:\n+  Symbol* _name;\n+  Symbol* _signature;\n+  int _offset;\n+  int _index;\n+  InstanceKlass* _holder;\n+  AccessFlags _access_flags;\n+ public:\n+  FieldDesc() {\n+    _name = NULL;\n+    _signature = NULL;\n+    _offset = -1;\n+    _index = -1;\n+    _holder = NULL;\n+    _access_flags = AccessFlags();\n+  }\n+  FieldDesc(fieldDescriptor& fd) {\n+    _name = fd.name();\n+    _signature = fd.signature();\n+    _offset = fd.offset();\n+    _index = fd.index();\n+    _holder = fd.field_holder();\n+    _access_flags = fd.access_flags();\n+  }\n+  const Symbol* name() { return _name;}\n+  const Symbol* signature() { return _signature; }\n+  const int offset() { return _offset; }\n+  const int index() { return _index; }\n+  const InstanceKlass* holder() { return _holder; }\n+  const AccessFlags& access_flags() { return _access_flags; }\n+  const bool is_inline_type() { return Signature::basic_type(_signature) == T_PRIMITIVE_OBJECT; }\n+};\n+\n+static int compare_offset(FieldDesc* f1, FieldDesc* f2) {\n+   return f1->offset() > f2->offset() ? 1 : -1;\n+}\n+\n+static void print_field(outputStream* st, int level, int offset, FieldDesc& fd, bool is_inline_type, bool is_inlined ) {\n+  const char* inlined_msg = \"\";\n+  if (is_inline_type) {\n+    inlined_msg = is_inlined ? \"inlined\" : \"not inlined\";\n+  }\n+  st->print_cr(\"  @ %d %*s \\\"%s\\\" %s %s %s\",\n+      offset, level * 3, \"\",\n+      fd.name()->as_C_string(),\n+      fd.signature()->as_C_string(),\n+      is_inline_type ? \" \/\/ inline type \" : \"\",\n+      inlined_msg);\n+}\n+\n+static void print_inlined_field(outputStream* st, int level, int offset, InstanceKlass* klass) {\n+  assert(klass->is_inline_klass(), \"Only inline types can be inlined\");\n+  InlineKlass* vklass = InlineKlass::cast(klass);\n+  GrowableArray<FieldDesc>* fields = new (mtServiceability) GrowableArray<FieldDesc>(100, mtServiceability);\n+  for (FieldStream fd(klass, false, false); !fd.eos(); fd.next()) {\n+    if (!fd.access_flags().is_static()) {\n+      fields->append(FieldDesc(fd.field_descriptor()));\n+    }\n+  }\n+  fields->sort(compare_offset);\n+  for(int i = 0; i < fields->length(); i++) {\n+    FieldDesc fd = fields->at(i);\n+    int offset2 = offset + fd.offset() - vklass->first_field_offset();\n+    print_field(st, level, offset2, fd,\n+        fd.is_inline_type(), fd.holder()->field_is_inlined(fd.index()));\n+    if (fd.holder()->field_is_inlined(fd.index())) {\n+      print_inlined_field(st, level + 1, offset2 ,\n+          InstanceKlass::cast(fd.holder()->get_inline_type_field_klass(fd.index())));\n+    }\n+  }\n+}\n+\n+void PrintClassLayout::print_class_layout(outputStream* st, char* class_name) {\n+  KlassInfoTable cit(true);\n+  if (cit.allocation_failed()) {\n+    st->print_cr(\"ERROR: Ran out of C-heap; hierarchy not generated\");\n+    return;\n+  }\n+\n+  Thread* THREAD = Thread::current();\n+\n+  Symbol* classname = SymbolTable::probe(class_name, (int)strlen(class_name));\n+\n+  GrowableArray<Klass*>* klasses = new (mtServiceability) GrowableArray<Klass*>(100, mtServiceability);\n+\n+  FindClassByNameClosure fbnc(klasses, classname);\n+  cit.iterate(&fbnc);\n+\n+  for(int i = 0; i < klasses->length(); i++) {\n+    Klass* klass = klasses->at(i);\n+    if (!klass->is_instance_klass()) continue;  \/\/ Skip\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    int tab = 1;\n+    st->print_cr(\"Class %s [@%s]:\", klass->name()->as_C_string(),\n+        klass->class_loader_data()->loader_name());\n+    ResourceMark rm;\n+    GrowableArray<FieldDesc>* fields = new (mtServiceability) GrowableArray<FieldDesc>(100, mtServiceability);\n+    for (FieldStream fd(ik, false, false); !fd.eos(); fd.next()) {\n+      if (!fd.access_flags().is_static()) {\n+        fields->append(FieldDesc(fd.field_descriptor()));\n+      }\n+    }\n+    fields->sort(compare_offset);\n+    for(int i = 0; i < fields->length(); i++) {\n+      FieldDesc fd = fields->at(i);\n+      print_field(st, 0, fd.offset(), fd, fd.is_inline_type(), fd.holder()->field_is_inlined(fd.index()));\n+      if (fd.holder()->field_is_inlined(fd.index())) {\n+        print_inlined_field(st, 1, fd.offset(),\n+            InstanceKlass::cast(fd.holder()->get_inline_type_field_klass(fd.index())));\n+      }\n+    }\n+  }\n+  st->cr();\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/heapInspection.cpp","additions":135,"deletions":0,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -194,0 +194,5 @@\n+class PrintClassLayout : AllStatic {\n+ public:\n+  static void print_class_layout(outputStream* st, char* classname);\n+};\n+\n","filename":"src\/hotspot\/share\/memory\/heapInspection.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -57,0 +57,2 @@\n+  virtual void do_oop_no_buffering(oop* o) { do_oop(o); }\n+  virtual void do_oop_no_buffering(narrowOop* o) { do_oop(o); }\n@@ -139,0 +141,5 @@\n+class BufferedValueClosure : public Closure {\n+public:\n+  virtual void do_buffered_value(oop* p) = 0;\n+};\n+\n","filename":"src\/hotspot\/share\/memory\/iterator.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -82,1 +82,5 @@\n-    _method_entry_ref\n+    \/\/ A field that points to a method entry. E.g., Method::_i2i_entry\n+    _method_entry_ref,\n+\n+    \/\/ A field that points to a location inside the current object.\n+    _internal_pointer_ref,\n@@ -361,1 +365,9 @@\n-    push_special(_method_entry_ref, ref, (intptr_t*)p);\n+    push_special(_method_entry_ref, ref, p);\n+    if (!ref->keep_after_pushing()) {\n+      delete ref;\n+    }\n+  }\n+\n+  template <class T> void push_internal_pointer(T** mpp, intptr_t* p) {\n+    Ref* ref = new MSORef<T>(mpp, _default);\n+    push_special(_internal_pointer_ref, ref, p);\n@@ -370,1 +382,5 @@\n-    assert(type == _method_entry_ref, \"only special type allowed for now\");\n+    assert_valid(type);\n+  }\n+\n+  static void assert_valid(SpecialRef type) {\n+    assert(type == _method_entry_ref || type == _internal_pointer_ref, \"only special types allowed for now\");\n","filename":"src\/hotspot\/share\/memory\/metaspaceClosure.hpp","additions":19,"deletions":3,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -33,0 +33,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n@@ -36,0 +39,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -126,0 +130,36 @@\n+arrayOop oopFactory::new_valueArray(Klass* k, int length, TRAPS) {\n+  InlineKlass* klass = InlineKlass::cast(k);\n+  \/\/ Request flattened, but we might not actually get it...either way \"null-free\" are the aaload\/aastore semantics\n+  Klass* array_klass = klass->value_array_klass(CHECK_NULL);\n+  assert(array_klass->is_null_free_array_klass(), \"Expect a null-free array class here\");\n+\n+  arrayOop oop;\n+  if (array_klass->is_flatArray_klass()) {\n+    oop = (arrayOop) FlatArrayKlass::cast(array_klass)->allocate(length, CHECK_NULL);\n+    assert(oop == NULL || oop->is_flatArray(), \"sanity\");\n+    assert(oop == NULL || oop->klass()->is_flatArray_klass(), \"sanity\");\n+  } else {\n+    oop = (arrayOop) ObjArrayKlass::cast(array_klass)->allocate(length, CHECK_NULL);\n+  }\n+  assert(oop == NULL || oop->klass()->is_null_free_array_klass(), \"sanity\");\n+  assert(oop == NULL || oop->is_null_free_array(), \"sanity\");\n+  return oop;\n+}\n+\n+objArrayHandle oopFactory::copy_flatArray_to_objArray(flatArrayHandle array, TRAPS) {\n+  int len = array->length();\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(array->klass());\n+  objArrayOop oarray = new_objectArray(array->length(), CHECK_(objArrayHandle()));\n+  objArrayHandle oarrayh(THREAD, oarray);\n+  vak->copy_array(array(), 0, oarrayh(), 0, len, CHECK_(objArrayHandle()));\n+  return oarrayh;\n+}\n+\n+objArrayHandle  oopFactory::ensure_objArray(oop array, TRAPS) {\n+  if (array != NULL && array->is_flatArray()) {\n+    return copy_flatArray_to_objArray(flatArrayHandle(THREAD, flatArrayOop(array)), THREAD);\n+  } else {\n+    return objArrayHandle(THREAD, objArrayOop(array));\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/oopFactory.cpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -138,0 +138,2 @@\n+LatestMethodCache* Universe::_is_substitutable_cache  = nullptr;\n+LatestMethodCache* Universe::_value_object_hash_code_cache = nullptr;\n@@ -237,0 +239,2 @@\n+  _is_substitutable_cache->metaspace_pointers_do(it);\n+  _value_object_hash_code_cache->metaspace_pointers_do(it);\n@@ -288,0 +292,2 @@\n+  _is_substitutable_cache->serialize(f);\n+  _value_object_hash_code_cache->serialize(f);\n@@ -368,0 +374,1 @@\n+\n@@ -790,1 +797,0 @@\n-\n@@ -810,0 +816,2 @@\n+  Universe::_is_substitutable_cache = new LatestMethodCache();\n+  Universe::_value_object_hash_code_cache = new LatestMethodCache();\n@@ -977,0 +985,11 @@\n+\n+  \/\/ Set up substitutability testing\n+  ResourceMark rm;\n+  initialize_known_method(_is_substitutable_cache,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::isSubstitutable_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true, CHECK);\n+  initialize_known_method(_value_object_hash_code_cache,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::valueObjectHashCode_name()->as_C_string(),\n+                          vmSymbols::object_int_signature(), true, CHECK);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":20,"deletions":1,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -123,0 +123,2 @@\n+  static LatestMethodCache* _is_substitutable_cache;   \/\/ ValueObjectMethods.isSubstitutable() method\n+  static LatestMethodCache* _value_object_hash_code_cache;  \/\/ ValueObjectMethods.valueObjectHashCode() method\n@@ -154,0 +156,1 @@\n+\n@@ -276,0 +279,3 @@\n+  static Method*      is_substitutable_method()       { return _is_substitutable_cache->get_method(); }\n+  static Method*      value_object_hash_code_method() { return _value_object_hash_code_cache->get_method(); }\n+\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+\/\/ * value_copy: Copy the contents of a value type from one heap address to another\n@@ -93,0 +94,2 @@\n+class InlineKlass;\n+\n@@ -125,0 +128,6 @@\n+  template <DecoratorSet expected_mo_decorators>\n+  static void verify_heap_value_decorators() {\n+    const DecoratorSet heap_value_decorators = IN_HEAP | IS_DEST_UNINITIALIZED;\n+    verify_decorators<expected_mo_decorators | heap_value_decorators>();\n+  }\n+\n@@ -132,1 +141,1 @@\n-  static inline bool oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, const T* src_raw,\n+  static inline void oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, const T* src_raw,\n@@ -137,3 +146,3 @@\n-    return AccessInternal::arraycopy<decorators | INTERNAL_VALUE_IS_OOP>(src_obj, src_offset_in_bytes, src_raw,\n-                                                                         dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                                         length);\n+    AccessInternal::arraycopy<decorators | INTERNAL_VALUE_IS_OOP>(src_obj, src_offset_in_bytes, src_raw,\n+                                                                  dst_obj, dst_offset_in_bytes, dst_raw,\n+                                                                  length);\n@@ -215,0 +224,8 @@\n+  \/\/ inline type heap access (when inlined)...\n+\n+  \/\/ Copy value type data from src to dst\n+  static inline void value_copy(void* src, void* dst, InlineKlass* md) {\n+    verify_heap_value_decorators<IN_HEAP>();\n+    AccessInternal::value_copy<decorators>(src, dst, md);\n+  }\n+\n@@ -325,1 +342,1 @@\n-  static inline bool oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes,\n+  static inline void oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes,\n@@ -328,3 +345,3 @@\n-    return AccessT::oop_arraycopy(src_obj, src_offset_in_bytes, static_cast<const HeapWord*>(nullptr),\n-                                  dst_obj, dst_offset_in_bytes, static_cast<HeapWord*>(nullptr),\n-                                  length);\n+    AccessT::oop_arraycopy(src_obj, src_offset_in_bytes, static_cast<const HeapWord*>(nullptr),\n+                           dst_obj, dst_offset_in_bytes, static_cast<HeapWord*>(nullptr),\n+                           length);\n@@ -334,4 +351,4 @@\n-  static inline bool oop_arraycopy_raw(T* src, T* dst, size_t length) {\n-    return AccessT::oop_arraycopy(nullptr, 0, src,\n-                                  nullptr, 0, dst,\n-                                  length);\n+  static inline void oop_arraycopy_raw(T* src, T* dst, size_t length) {\n+    AccessT::oop_arraycopy(nullptr, 0, src,\n+                           nullptr, 0, dst,\n+                           length);\n","filename":"src\/hotspot\/share\/oops\/access.hpp","additions":29,"deletions":12,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-    static bool access_barrier(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static void access_barrier(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -134,1 +134,0 @@\n-      return true;\n@@ -138,1 +137,1 @@\n-    static bool oop_access_barrier(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static void oop_access_barrier(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -142,3 +141,3 @@\n-      return GCBarrierType::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, reinterpret_cast<OopType*>(src_raw),\n-                                                  dst_obj, dst_offset_in_bytes, reinterpret_cast<OopType*>(dst_raw),\n-                                                  length);\n+      GCBarrierType::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, reinterpret_cast<OopType*>(src_raw),\n+                                           dst_obj, dst_offset_in_bytes, reinterpret_cast<OopType*>(dst_raw),\n+                                           length);\n@@ -203,0 +202,7 @@\n+  template <class GCBarrierType, DecoratorSet decorators>\n+  struct PostRuntimeDispatch<GCBarrierType, BARRIER_VALUE_COPY, decorators>: public AllStatic {\n+    static void access_barrier(void* src, void* dst, InlineKlass* md) {\n+      GCBarrierType::value_copy_in_heap(src, dst, md);\n+    }\n+  };\n+\n@@ -334,1 +340,1 @@\n-  bool RuntimeDispatch<decorators, T, BARRIER_ARRAYCOPY>::arraycopy_init(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+  void RuntimeDispatch<decorators, T, BARRIER_ARRAYCOPY>::arraycopy_init(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -339,3 +345,3 @@\n-    return function(src_obj, src_offset_in_bytes, src_raw,\n-                    dst_obj, dst_offset_in_bytes, dst_raw,\n-                    length);\n+    function(src_obj, src_offset_in_bytes, src_raw,\n+             dst_obj, dst_offset_in_bytes, dst_raw,\n+             length);\n@@ -350,0 +356,7 @@\n+\n+  template <DecoratorSet decorators, typename T>\n+  void RuntimeDispatch<decorators, T, BARRIER_VALUE_COPY>::value_copy_init(void* src, void* dst, InlineKlass* md) {\n+    func_t function = BarrierResolver<decorators, func_t, BARRIER_VALUE_COPY>::resolve_barrier();\n+    _value_copy_func = function;\n+    function(src, dst, md);\n+  }\n","filename":"src\/hotspot\/share\/oops\/access.inline.hpp","additions":23,"deletions":10,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -48,0 +48,8 @@\n+\/\/ This meta-function returns either oop or narrowOop depending on whether\n+\/\/ a back-end needs to consider compressed oops types or not.\n+template <DecoratorSet decorators>\n+struct ValueOopType: AllStatic {\n+  static const bool needs_oop_compress = HasDecorator<decorators, INTERNAL_RT_USE_COMPRESSED_OOPS>::value;\n+  using type = std::conditional_t<needs_oop_compress, narrowOop, oop>;\n+};\n+\n@@ -59,1 +67,2 @@\n-    BARRIER_CLONE\n+    BARRIER_CLONE,\n+    BARRIER_VALUE_COPY\n@@ -105,1 +114,1 @@\n-    typedef bool (*arraycopy_func_t)(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    typedef void (*arraycopy_func_t)(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -109,0 +118,1 @@\n+    typedef void (*value_copy_func_t)(void* src, void* dst, InlineKlass* md);\n@@ -113,1 +123,1 @@\n-    typedef bool (*arraycopy_func_t)(arrayOop src_obj, size_t src_offset_in_bytes, void* src,\n+    typedef void (*arraycopy_func_t)(arrayOop src_obj, size_t src_offset_in_bytes, void* src,\n@@ -135,0 +145,1 @@\n+  ACCESS_GENERATE_ACCESS_FUNCTION(BARRIER_VALUE_COPY, value_copy_func_t);\n@@ -334,1 +345,1 @@\n-  static bool arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+  static void arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -379,1 +390,1 @@\n-  static bool oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+  static void oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -384,0 +395,2 @@\n+  static void value_copy(void* src, void* dst, InlineKlass* md);\n+\n@@ -553,1 +566,1 @@\n-    static bool arraycopy_init(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static void arraycopy_init(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -557,1 +570,1 @@\n-    static inline bool arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static inline void arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -580,0 +593,12 @@\n+  template <DecoratorSet decorators, typename T>\n+  struct RuntimeDispatch<decorators, T, BARRIER_VALUE_COPY>: AllStatic {\n+    typedef typename AccessFunction<decorators, T, BARRIER_VALUE_COPY>::type func_t;\n+    static func_t _value_copy_func;\n+\n+    static void value_copy_init(void* src, void* dst, InlineKlass* md);\n+\n+    static inline void value_copy(void* src, void* dst, InlineKlass* md) {\n+      _value_copy_func(src, dst, md);\n+    }\n+  };\n+\n@@ -621,0 +646,4 @@\n+  template <DecoratorSet decorators, typename T>\n+  typename AccessFunction<decorators, T, BARRIER_VALUE_COPY>::type\n+  RuntimeDispatch<decorators, T, BARRIER_VALUE_COPY>::_value_copy_func = &value_copy_init;\n+\n@@ -868,1 +897,1 @@\n-      HasDecorator<decorators, AS_RAW>::value && CanHardwireRaw<decorators>::value, bool>::type\n+      HasDecorator<decorators, AS_RAW>::value && CanHardwireRaw<decorators>::value, void>::type\n@@ -874,3 +903,3 @@\n-        return Raw::oop_arraycopy(src_obj, src_offset_in_bytes, src_raw,\n-                                  dst_obj, dst_offset_in_bytes, dst_raw,\n-                                  length);\n+        Raw::oop_arraycopy(src_obj, src_offset_in_bytes, src_raw,\n+                           dst_obj, dst_offset_in_bytes, dst_raw,\n+                           length);\n@@ -878,3 +907,3 @@\n-        return Raw::arraycopy(src_obj, src_offset_in_bytes, src_raw,\n-                              dst_obj, dst_offset_in_bytes, dst_raw,\n-                              length);\n+        Raw::arraycopy(src_obj, src_offset_in_bytes, src_raw,\n+                       dst_obj, dst_offset_in_bytes, dst_raw,\n+                       length);\n@@ -886,1 +915,1 @@\n-      HasDecorator<decorators, AS_RAW>::value && !CanHardwireRaw<decorators>::value, bool>::type\n+      HasDecorator<decorators, AS_RAW>::value && !CanHardwireRaw<decorators>::value, void>::type\n@@ -892,3 +921,3 @@\n-        return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n-                                                                  dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                                  length);\n+        PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n+                                                           dst_obj, dst_offset_in_bytes, dst_raw,\n+                                                           length);\n@@ -897,3 +926,3 @@\n-        return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n-                                                                  dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                                  length);\n+        PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n+                                                           dst_obj, dst_offset_in_bytes, dst_raw,\n+                                                           length);\n@@ -905,1 +934,1 @@\n-      !HasDecorator<decorators, AS_RAW>::value, bool>::type\n+      !HasDecorator<decorators, AS_RAW>::value, void>::type\n@@ -911,3 +940,3 @@\n-        return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n-                                                                  dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                                  length);\n+        PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n+                                                           dst_obj, dst_offset_in_bytes, dst_raw,\n+                                                           length);\n@@ -915,3 +944,3 @@\n-        return RuntimeDispatch<decorators, T, BARRIER_ARRAYCOPY>::arraycopy(src_obj, src_offset_in_bytes, src_raw,\n-                                                                            dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                                            length);\n+        RuntimeDispatch<decorators, T, BARRIER_ARRAYCOPY>::arraycopy(src_obj, src_offset_in_bytes, src_raw,\n+                                                                     dst_obj, dst_offset_in_bytes, dst_raw,\n+                                                                     length);\n@@ -935,0 +964,16 @@\n+\n+    template <DecoratorSet decorators>\n+    inline static typename EnableIf<\n+      HasDecorator<decorators, AS_RAW>::value>::type\n+    value_copy(void* src, void* dst, InlineKlass* md) {\n+      typedef RawAccessBarrier<decorators & RAW_DECORATOR_MASK> Raw;\n+      Raw::value_copy(src, dst, md);\n+    }\n+\n+    template <DecoratorSet decorators>\n+    inline static typename EnableIf<\n+      !HasDecorator<decorators, AS_RAW>::value>::type\n+      value_copy(void* src, void* dst, InlineKlass* md) {\n+      const DecoratorSet expanded_decorators = decorators;\n+      RuntimeDispatch<expanded_decorators, void*, BARRIER_VALUE_COPY>::value_copy(src, dst, md);\n+    }\n@@ -1049,1 +1094,1 @@\n-  inline bool arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+  inline void arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -1052,3 +1097,3 @@\n-    return PreRuntimeDispatch::arraycopy<decorators>(src_obj, src_offset_in_bytes, src_raw,\n-                                                     dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                     length);\n+    PreRuntimeDispatch::arraycopy<decorators>(src_obj, src_offset_in_bytes, src_raw,\n+                                              dst_obj, dst_offset_in_bytes, dst_raw,\n+                                              length);\n@@ -1058,1 +1103,1 @@\n-  inline bool arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, HeapWord* src_raw,\n+  inline void arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, HeapWord* src_raw,\n@@ -1062,3 +1107,3 @@\n-    return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n-                                                              dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                              length);\n+    PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n+                                                       dst_obj, dst_offset_in_bytes, dst_raw,\n+                                                       length);\n@@ -1068,1 +1113,1 @@\n-  inline bool arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, narrowOop* src_raw,\n+  inline void arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, narrowOop* src_raw,\n@@ -1073,3 +1118,3 @@\n-    return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n-                                                              dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                              length);\n+    PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n+                                                       dst_obj, dst_offset_in_bytes, dst_raw,\n+                                                       length);\n@@ -1208,1 +1253,1 @@\n-  inline bool arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, const T* src_raw,\n+  inline void arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, const T* src_raw,\n@@ -1216,3 +1261,3 @@\n-    return arraycopy_reduce_types<expanded_decorators>(src_obj, src_offset_in_bytes, const_cast<DecayedT*>(src_raw),\n-                                                       dst_obj, dst_offset_in_bytes, const_cast<DecayedT*>(dst_raw),\n-                                                       length);\n+    arraycopy_reduce_types<expanded_decorators>(src_obj, src_offset_in_bytes, const_cast<DecayedT*>(src_raw),\n+                                                dst_obj, dst_offset_in_bytes, const_cast<DecayedT*>(dst_raw),\n+                                                length);\n@@ -1227,0 +1272,6 @@\n+  template <DecoratorSet decorators>\n+  inline void value_copy(void* src, void* dst, InlineKlass* md) {\n+    const DecoratorSet expanded_decorators = DecoratorFixup<decorators>::value;\n+    PreRuntimeDispatch::value_copy<expanded_decorators>(src, dst, md);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/accessBackend.hpp","additions":95,"deletions":44,"binary":false,"changes":139,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -127,1 +128,1 @@\n-inline bool RawAccessBarrier<decorators>::oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+inline void RawAccessBarrier<decorators>::oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -130,3 +131,3 @@\n-  return arraycopy(src_obj, src_offset_in_bytes, src_raw,\n-                   dst_obj, dst_offset_in_bytes, dst_raw,\n-                   length);\n+  arraycopy(src_obj, src_offset_in_bytes, src_raw,\n+            dst_obj, dst_offset_in_bytes, dst_raw,\n+            length);\n@@ -343,1 +344,1 @@\n-inline bool RawAccessBarrier<decorators>::arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+inline void RawAccessBarrier<decorators>::arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -349,1 +350,0 @@\n-  return true;\n@@ -372,0 +372,5 @@\n+template <DecoratorSet decorators>\n+inline void RawAccessBarrier<decorators>::value_copy(void* src, void* dst, InlineKlass* md) {\n+  assert(is_aligned(src, md->get_alignment()) && is_aligned(dst, md->get_alignment()), \"Unalign value_copy\");\n+  AccessInternal::arraycopy_conjoint_atomic(src, dst, static_cast<size_t>(md->get_exact_size_in_bytes()));\n+}\n","filename":"src\/hotspot\/share\/oops\/accessBackend.inline.hpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -197,0 +197,2 @@\n+\/\/ * ARRAYCOPY_NOTNULL: This property means that the source array may contain null elements\n+\/\/   but the destination does not allow null elements (i.e. throw NPE)\n@@ -203,5 +205,6 @@\n-const DecoratorSet ARRAYCOPY_DISJOINT             = UCONST64(1) << 25;\n-const DecoratorSet ARRAYCOPY_ARRAYOF              = UCONST64(1) << 26;\n-const DecoratorSet ARRAYCOPY_ATOMIC               = UCONST64(1) << 27;\n-const DecoratorSet ARRAYCOPY_ALIGNED              = UCONST64(1) << 28;\n-const DecoratorSet ARRAYCOPY_DECORATOR_MASK       = ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT |\n+const DecoratorSet ARRAYCOPY_NOTNULL              = UCONST64(1) << 25;\n+const DecoratorSet ARRAYCOPY_DISJOINT             = UCONST64(1) << 26;\n+const DecoratorSet ARRAYCOPY_ARRAYOF              = UCONST64(1) << 27;\n+const DecoratorSet ARRAYCOPY_ATOMIC               = UCONST64(1) << 28;\n+const DecoratorSet ARRAYCOPY_ALIGNED              = UCONST64(1) << 29;\n+const DecoratorSet ARRAYCOPY_DECORATOR_MASK       = ARRAYCOPY_CHECKCAST | ARRAYCOPY_NOTNULL |\n@@ -215,2 +218,2 @@\n-const DecoratorSet ACCESS_READ                    = UCONST64(1) << 29;\n-const DecoratorSet ACCESS_WRITE                   = UCONST64(1) << 30;\n+const DecoratorSet ACCESS_READ                    = UCONST64(1) << 30;\n+const DecoratorSet ACCESS_WRITE                   = UCONST64(1) << 31;\n@@ -219,1 +222,1 @@\n-const DecoratorSet DECORATOR_LAST = UCONST64(1) << 30;\n+const DecoratorSet DECORATOR_LAST = UCONST64(1) << 31;\n","filename":"src\/hotspot\/share\/oops\/accessDecorators.hpp","additions":11,"deletions":8,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"oops\/objArrayKlass.hpp\"\n@@ -101,0 +103,23 @@\n+Symbol* ArrayKlass::create_element_klass_array_name(Klass* element_klass, bool qdesc, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  Symbol* name = nullptr;\n+  char *name_str = element_klass->name()->as_C_string();\n+  int len = element_klass->name()->utf8_length();\n+  char *new_str = NEW_RESOURCE_ARRAY(char, len + 4);\n+  int idx = 0;\n+  new_str[idx++] = JVM_SIGNATURE_ARRAY;\n+  if (element_klass->is_instance_klass()) { \/\/ it could be an array or simple type\n+    if (qdesc) {\n+      new_str[idx++] = JVM_SIGNATURE_PRIMITIVE_OBJECT;\n+    } else {\n+      new_str[idx++] = JVM_SIGNATURE_CLASS;\n+    }\n+  }\n+  memcpy(&new_str[idx], name_str, len * sizeof(char));\n+  idx += len;\n+  if (element_klass->is_instance_klass()) {\n+    new_str[idx++] = JVM_SIGNATURE_ENDCLASS;\n+  }\n+  new_str[idx++] = '\\0';\n+  return SymbolTable::new_symbol(new_str);\n+}\n@@ -138,0 +163,4 @@\n+oop ArrayKlass::component_mirror() const {\n+  return java_lang_Class::component_mirror(java_mirror());\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.cpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-\n+protected:\n@@ -74,1 +74,1 @@\n-    return type == T_DOUBLE || type == T_LONG;\n+    return type == T_DOUBLE || type == T_LONG || type == T_PRIMITIVE_OBJECT;\n","filename":"src\/hotspot\/share\/oops\/arrayOop.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -248,1 +249,1 @@\n-      \/\/ All of these should have been reverted back to ClassIndex before calling\n+      \/\/ All of these should have been reverted back to Unresolved before calling\n@@ -267,0 +268,1 @@\n+  assert(!k->name()->is_Q_signature(), \"Q-type without JVM_CONSTANT_QDescBit\");\n@@ -385,1 +387,4 @@\n-      tag_at_put(index, JVM_CONSTANT_UnresolvedClass);\n+      {\n+        jbyte qdesc_bit = tag_at(index).is_Qdescriptor_klass() ? (jbyte) JVM_CONSTANT_QDescBit : 0;\n+        tag_at_put(index, JVM_CONSTANT_UnresolvedClass | qdesc_bit);\n+      }\n@@ -441,1 +446,2 @@\n-  tag_at_put(cp_index, JVM_CONSTANT_UnresolvedClass);\n+  jbyte qdesc_bit = tag_at(cp_index).is_Qdescriptor_klass() ? (jbyte) JVM_CONSTANT_QDescBit : 0;\n+  tag_at_put(cp_index, JVM_CONSTANT_UnresolvedClass | qdesc_bit);\n@@ -487,0 +493,6 @@\n+void check_is_inline_type(Klass* k, TRAPS) {\n+  if (!k->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+}\n+\n@@ -524,0 +536,5 @@\n+  bool inline_type_signature = false;\n+  if (name->is_Q_signature()) {\n+    name = name->fundamental_name(THREAD);\n+    inline_type_signature = true;\n+  }\n@@ -533,0 +550,3 @@\n+  if (inline_type_signature) {\n+    name->decrement_refcount();\n+  }\n@@ -541,0 +561,16 @@\n+  if (!HAS_PENDING_EXCEPTION && inline_type_signature) {\n+    check_is_inline_type(k, THREAD);\n+  }\n+\n+  if (!HAS_PENDING_EXCEPTION) {\n+    Klass* bottom_klass = nullptr;\n+    if (k->is_objArray_klass()) {\n+      bottom_klass = ObjArrayKlass::cast(k)->bottom_klass();\n+      assert(bottom_klass != nullptr, \"Should be set\");\n+      assert(bottom_klass->is_instance_klass() || bottom_klass->is_typeArray_klass(), \"Sanity check\");\n+    } else if (k->is_flatArray_klass()) {\n+      bottom_klass = FlatArrayKlass::cast(k)->element_klass();\n+      assert(bottom_klass != nullptr, \"Should be set\");\n+    }\n+  }\n+\n@@ -544,1 +580,5 @@\n-    save_and_throw_exception(this_cp, which, constantTag(JVM_CONSTANT_UnresolvedClass), CHECK_NULL);\n+    jbyte tag = JVM_CONSTANT_UnresolvedClass;\n+    if (this_cp->tag_at(which).is_Qdescriptor_klass()) {\n+      tag |= JVM_CONSTANT_QDescBit;\n+    }\n+    save_and_throw_exception(this_cp, which, constantTag(tag), CHECK_NULL);\n@@ -563,0 +603,4 @@\n+  jbyte tag = JVM_CONSTANT_Class;\n+  if (this_cp->tag_at(which).is_Qdescriptor_klass()) {\n+    tag |= JVM_CONSTANT_QDescBit;\n+  }\n@@ -567,1 +611,1 @@\n-                                  (jbyte)JVM_CONSTANT_Class);\n+                                  tag);\n@@ -1000,1 +1044,3 @@\n-      result_oop = resolved->java_mirror();\n+      result_oop = tag.is_Qdescriptor_klass()\n+                      ? InlineKlass::cast(resolved)->val_mirror()\n+                      : resolved->java_mirror();\n@@ -1917,0 +1963,6 @@\n+      case (JVM_CONSTANT_Class | JVM_CONSTANT_QDescBit): {\n+        idx1 = Bytes::get_Java_u2(bytes);\n+        printf(\"qclass        #%03d\", idx1);\n+        ent_size = 2;\n+        break;\n+      }\n@@ -1959,0 +2011,4 @@\n+      case (JVM_CONSTANT_UnresolvedClass | JVM_CONSTANT_QDescBit): {\n+        printf(\"UnresolvedQClass: %s\", WARN_MSG);\n+        break;\n+      }\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":62,"deletions":6,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -288,1 +288,1 @@\n-  \/\/ For temporary use while constructing constant pool\n+  \/\/ For temporary use while constructing constant pool. Used during a retransform\/class redefinition as well.\n@@ -297,0 +297,9 @@\n+  void unresolved_qdescriptor_at_put(int which, int name_index, int resolved_klass_index) {\n+      release_tag_at_put(which, JVM_CONSTANT_UnresolvedClass | (jbyte) JVM_CONSTANT_QDescBit);\n+\n+      assert((name_index & 0xffff0000) == 0, \"must be\");\n+      assert((resolved_klass_index & 0xffff0000) == 0, \"must be\");\n+      *int_at_addr(which) =\n+        build_int_from_shorts((jushort)resolved_klass_index, (jushort)name_index);\n+    }\n+\n","filename":"src\/hotspot\/share\/oops\/constantPool.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -124,1 +124,3 @@\n-                                       bool is_volatile) {\n+                                       bool is_volatile,\n+                                       bool is_inlined,\n+                                       bool is_null_free_inline_type) {\n@@ -129,0 +131,1 @@\n+  assert(!is_inlined || is_null_free_inline_type, \"Sanity check\");\n@@ -131,1 +134,3 @@\n-                  ((is_final    ? 1 : 0) << is_final_shift),\n+                  ((is_final    ? 1 : 0) << is_final_shift) |\n+                  ((is_inlined  ? 1 : 0) << is_inlined_shift) |\n+                  ((is_null_free_inline_type ? 1 : 0) << is_null_free_inline_type_shift),\n@@ -289,0 +294,1 @@\n+      invoke_code = Bytecodes::_invokevirtual;\n@@ -304,1 +310,1 @@\n-    set_bytecode_2(Bytecodes::_invokevirtual);\n+    set_bytecode_2(invoke_code);\n","filename":"src\/hotspot\/share\/oops\/cpCache.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -52,1 +52,1 @@\n-\/\/ _flags     [tos|0|F=1|0|0|0|f|v|0 |0000|field_index] (for field entries)\n+\/\/ _flags     [tos|0|F=1|0|I|i|f|v|0 |0000|field_index] (for field entries)\n@@ -78,0 +78,2 @@\n+\/\/ I  flag true if field is a null free inline type (must never be null)\n+\/\/ i  flag true if field is inlined\n@@ -184,0 +186,1 @@\n+    is_null_free_inline_type_shift = 24,  \/\/ (I) is the field a null free inline type (must never be null)\n@@ -185,0 +188,1 @@\n+    is_inlined_shift           = 23,  \/\/ (i) is the field inlined?\n@@ -223,1 +227,3 @@\n-    bool            is_volatile                  \/\/ the field is volatile\n+    bool            is_volatile,                 \/\/ the field is volatile\n+    bool            is_inlined,                  \/\/ the field is inlined\n+    bool            is_null_free_inline_type     \/\/ the field is an inline type (must never be null)\n@@ -312,0 +318,1 @@\n+      case Bytecodes::_withfield       :    \/\/ fall through\n@@ -339,0 +346,1 @@\n+  int       f2_as_offset() const                 { assert(is_field_entry(),  \"\"); return (int)_f2; }\n@@ -344,0 +352,1 @@\n+  bool is_inlined() const                        { return (_flags & (1 << is_inlined_shift))        != 0; }\n@@ -353,0 +362,1 @@\n+  bool is_null_free_inline_type() const          { return (_flags & (1 << is_null_free_inline_type_shift)) != 0; }\n","filename":"src\/hotspot\/share\/oops\/cpCache.hpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+\n@@ -137,0 +138,8 @@\n+  bool is_inlined() {\n+    return field()->is_inlined();\n+  }\n+\n+  void set_inlined(bool b) {\n+    field()->set_inlined(b);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/fieldStreams.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -142,1 +142,1 @@\n-    if (!is_static)\n+    if (!is_static) {\n@@ -144,0 +144,1 @@\n+    }\n@@ -846,1 +847,1 @@\n-  assert(cts.is_reference() || cts.is_value() || cts.is_address(),\n+  assert(cts.is_reference() || cts.is_inline_type() || cts.is_address(),\n@@ -1387,0 +1388,3 @@\n+    case Bytecodes::_aconst_init:      ppush1(CellTypeState::make_line_ref(itr->bci())); break;\n+    case Bytecodes::_withfield:         do_withfield(itr->get_index_u2_cpcache(), itr->bci()); break;\n+\n@@ -1600,2 +1604,2 @@\n-    case Bytecodes::_getstatic:         do_field(true,  true,  itr->get_index_u2_cpcache(), itr->bci()); break;\n-    case Bytecodes::_putstatic:         do_field(false, true,  itr->get_index_u2_cpcache(), itr->bci()); break;\n+    case Bytecodes::_getstatic:         do_field(true,  true, itr->get_index_u2_cpcache(), itr->bci()); break;\n+    case Bytecodes::_putstatic:         do_field(false, true, itr->get_index_u2_cpcache(), itr->bci()); break;\n@@ -1605,0 +1609,1 @@\n+    case Bytecodes::_invokeinterface:\n@@ -1606,4 +1611,3 @@\n-    case Bytecodes::_invokespecial:     do_method(false, false, itr->get_index_u2_cpcache(), itr->bci()); break;\n-    case Bytecodes::_invokestatic:      do_method(true,  false, itr->get_index_u2_cpcache(), itr->bci()); break;\n-    case Bytecodes::_invokedynamic:     do_method(true,  false, itr->get_index_u4(),         itr->bci()); break;\n-    case Bytecodes::_invokeinterface:   do_method(false, true,  itr->get_index_u2_cpcache(), itr->bci()); break;\n+    case Bytecodes::_invokespecial:     do_method(false, itr->get_index_u2_cpcache(), itr->bci()); break;\n+    case Bytecodes::_invokestatic:      do_method(true , itr->get_index_u2_cpcache(), itr->bci()); break;\n+    case Bytecodes::_invokedynamic:     do_method(true , itr->get_index_u4(),         itr->bci()); break;\n@@ -1629,0 +1633,1 @@\n+\n@@ -1735,1 +1740,1 @@\n-  assert(in.is_reference() || in.is_value(), \"sanity check\");\n+  assert(in.is_reference() || in.is_inline_type(), \"sanity check\");\n@@ -1954,1 +1959,3 @@\n-  if (!is_static) in[i++] = CellTypeState::ref;\n+  if (!is_static) {\n+    in[i++] = CellTypeState::ref;\n+  }\n@@ -1960,1 +1967,1 @@\n-void GenerateOopMap::do_method(int is_static, int is_interface, int idx, int bci) {\n+void GenerateOopMap::do_method(int is_static, int idx, int bci) {\n@@ -1997,0 +2004,27 @@\n+void GenerateOopMap::do_withfield(int idx, int bci) {\n+  \/\/ Dig up signature for field in constant pool\n+  ConstantPool* cp = method()->constants();\n+  int nameAndTypeIdx = cp->name_and_type_ref_index_at(idx);\n+  int signatureIdx = cp->signature_ref_index_at(nameAndTypeIdx);\n+  Symbol* signature = cp->symbol_at(signatureIdx);\n+\n+  \/\/ Parse signature (especially simple for fields)\n+  assert(signature->utf8_length() > 0,\n+      \"field signatures cannot have zero length\");\n+  \/\/ The signature is UFT8 encoded, but the first char is always ASCII for signatures.\n+  CellTypeState temp[4];\n+  CellTypeState *eff = signature_to_effect(signature, bci, temp);\n+\n+  CellTypeState in[4];\n+  int i = copy_cts(in, eff);\n+  in[i++] = CellTypeState::ref;\n+  in[i] = CellTypeState::bottom;\n+  assert(i <= 3, \"sanity check\");\n+\n+  CellTypeState out[2];\n+  out[0] = CellTypeState::ref;\n+  out[1] = CellTypeState::bottom;\n+\n+  pp(in, out);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.cpp","additions":45,"deletions":11,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -94,1 +94,1 @@\n-  enum { info_mask            = right_n_bits(28),\n+  enum { info_mask            = right_n_bits(27),\n@@ -107,3 +107,3 @@\n-  enum { top_info_bit         = nth_bit(27),\n-         not_bottom_info_bit  = nth_bit(26),\n-         info_data_mask       = right_n_bits(26),\n+  enum { top_info_bit         = nth_bit(26),\n+         not_bottom_info_bit  = nth_bit(25),\n+         info_data_mask       = right_n_bits(25),\n@@ -114,2 +114,2 @@\n-  enum { ref_not_lock_bit     = nth_bit(25),  \/\/ 0 if this reference is locked as a monitor\n-         ref_slot_bit         = nth_bit(24),  \/\/ 1 if this reference is a \"slot\" reference,\n+  enum { ref_not_lock_bit     = nth_bit(24),  \/\/ 0 if this reference is locked as a monitor\n+         ref_slot_bit         = nth_bit(23),  \/\/ 1 if this reference is a \"slot\" reference,\n@@ -117,1 +117,1 @@\n-         ref_data_mask        = right_n_bits(24) };\n+         ref_data_mask        = right_n_bits(23) };\n@@ -119,0 +119,5 @@\n+  \/\/ Within the INFO data, these values are used to distinguish different\n+  \/\/ kinds of value types.\n+  enum { valuetype_slot_bit   = nth_bit(24),  \/\/ 1 if this reference is a \"slot\" value type,\n+    \/\/ 0 if it is a \"line\" value type.\n+    valuetype_data_mask  = right_n_bits(24) };\n@@ -199,1 +204,1 @@\n-  bool is_value() const                 { return ((_state & bits_mask) == val_bit); }\n+  bool is_inline_type() const           { return ((_state & bits_mask) == val_bit); }\n@@ -400,1 +405,2 @@\n-  void  do_method                           (int is_static, int is_interface, int idx, int bci);\n+  void  do_method                           (int is_static, int idx, int bci);\n+  void  do_withfield                       (int idx, int bci);\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.hpp","additions":15,"deletions":9,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -166,0 +167,2 @@\n+bool InstanceKlass::field_is_null_free_inline_type(int index) const { return Signature::basic_type(field(index)->signature(constants())) == T_PRIMITIVE_OBJECT; }\n+\n@@ -438,1 +441,3 @@\n-                                       parser.is_interface());\n+                                       parser.is_interface(),\n+                                       parser.has_inline_fields() ? parser.java_fields_count() : 0,\n+                                       parser.is_inline_type());\n@@ -460,0 +465,3 @@\n+  } else if (parser.is_inline_type()) {\n+    \/\/ inline type\n+    ik = new (loader_data, size, THREAD) InlineKlass(parser);\n@@ -471,0 +479,7 @@\n+#ifdef ASSERT\n+  assert(ik->size() == size, \"\");\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -474,0 +489,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = nullptr;\n+  address end = nullptr;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -513,1 +551,4 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _inline_type_field_klasses(nullptr),\n+  _preload_classes(nullptr),\n+  _adr_inlineklass_fixed_block(nullptr)\n@@ -520,0 +561,4 @@\n+    if (parser.has_inline_fields()) {\n+      set_has_inline_type_fields();\n+    }\n+    _java_fields_count = parser.java_fields_count();\n@@ -524,0 +569,4 @@\n+\n+  if (has_inline_type_fields()) {\n+    _inline_type_field_klasses = (const Klass**) adr_inline_type_field_klasses();\n+  }\n@@ -688,0 +737,6 @@\n+  if (preload_classes() != nullptr &&\n+      preload_classes() != Universe::the_empty_short_array() &&\n+      !preload_classes()->is_shared()) {\n+    MetadataFactory::free_array<jushort>(loader_data, preload_classes());\n+  }\n+\n@@ -844,0 +899,78 @@\n+\n+  \/\/ If a class declares a method that uses an inline class as an argument\n+  \/\/ type or return inline type, this inline class must be loaded during the\n+  \/\/ linking of this class because size and properties of the inline class\n+  \/\/ must be known in order to be able to perform inline type optimizations.\n+  \/\/ The implementation below is an approximation of this rule, the code\n+  \/\/ iterates over all methods of the current class (including overridden\n+  \/\/ methods), not only the methods declared by this class. This\n+  \/\/ approximation makes the code simpler, and doesn't change the semantic\n+  \/\/ because classes declaring methods overridden by the current class are\n+  \/\/ linked (and have performed their own pre-loading) before the linking\n+  \/\/ of the current class.\n+\n+\n+  \/\/ Note:\n+  \/\/ Inline class types are loaded during\n+  \/\/ the loading phase (see ClassFileParser::post_process_parsed_stream()).\n+  \/\/ Inline class types used as element types for array creation\n+  \/\/ are not pre-loaded. Their loading is triggered by either anewarray\n+  \/\/ or multianewarray bytecodes.\n+\n+  \/\/ Could it be possible to do the following processing only if the\n+  \/\/ class uses inline types?\n+  if (EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    if (EnablePrimitiveClasses) {\n+      for (int i = 0; i < methods()->length(); i++) {\n+        Method* m = methods()->at(i);\n+        for (SignatureStream ss(m->signature()); !ss.is_done(); ss.next()) {\n+          if (ss.is_reference()) {\n+            if (ss.is_array()) {\n+              continue;\n+            }\n+            if (ss.type() == T_PRIMITIVE_OBJECT) {\n+              Symbol* symb = ss.as_symbol();\n+              if (symb == name()) continue;\n+              oop loader = class_loader();\n+              oop protection_domain = this->protection_domain();\n+              Klass* klass = SystemDictionary::resolve_or_fail(symb,\n+                                                              Handle(THREAD, loader), Handle(THREAD, protection_domain), true,\n+                                                              CHECK_false);\n+              if (klass == nullptr) {\n+                THROW_(vmSymbols::java_lang_LinkageError(), false);\n+              }\n+              if (!klass->is_inline_klass()) {\n+                Exceptions::fthrow(\n+                  THREAD_AND_LOCATION,\n+                  vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                  \"class %s is not an inline type\",\n+                  klass->external_name());\n+              }\n+            }\n+          }\n+        }\n+      }\n+    }\n+    \/\/ Aggressively preloading all classes from the Preload attribute\n+    if (preload_classes() != nullptr) {\n+      for (int i = 0; i < preload_classes()->length(); i++) {\n+        if (constants()->tag_at(preload_classes()->at(i)).is_klass()) continue;\n+        Symbol* class_name = constants()->klass_at_noresolve(preload_classes()->at(i));\n+        if (class_name == name()) continue;\n+        oop loader = class_loader();\n+        oop protection_domain = this->protection_domain();\n+        Klass* klass = SystemDictionary::resolve_or_null(class_name,\n+                                                          Handle(THREAD, loader), Handle(THREAD, protection_domain), THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          CLEAR_PENDING_EXCEPTION;\n+        }\n+        if (klass != nullptr) {\n+          log_info(class, preload)(\"Preloading class %s during linking of class %s because of its Preload attribute\", class_name->as_C_string(), name()->as_C_string());\n+        } else {\n+          log_warning(class, preload)(\"Preloading of class %s during linking of class %s (Preload attribute) failed\", class_name->as_C_string(), name()->as_C_string());\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1088,0 +1221,19 @@\n+  \/\/ Pre-allocating an instance of the default value\n+  if (is_inline_klass()) {\n+      InlineKlass* vk = InlineKlass::cast(this);\n+      oop val = vk->allocate_instance(THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          {\n+              EXCEPTION_MARK;\n+              add_initialization_error(THREAD, e);\n+              \/\/ Locks object, set state, and notify all waiting threads\n+              set_initialization_state_and_notify(initialization_error, THREAD);\n+              CLEAR_PENDING_EXCEPTION;\n+          }\n+          THROW_OOP(e());\n+      }\n+      vk->set_default_value(val);\n+  }\n+\n@@ -1120,1 +1272,41 @@\n-\n+  \/\/ Initialize classes of inline fields\n+  if (EnablePrimitiveClasses) {\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_PRIMITIVE_OBJECT) {\n+        Klass* klass = get_inline_type_field_klass_or_null(fs.index());\n+        if (fs.access_flags().is_static() && klass == nullptr) {\n+          klass = SystemDictionary::resolve_or_fail(field_signature(fs.index())->fundamental_name(THREAD),\n+              Handle(THREAD, class_loader()),\n+              Handle(THREAD, protection_domain()),\n+              true, THREAD);\n+          set_inline_type_field_klass(fs.index(), klass);\n+        }\n+\n+        if (!HAS_PENDING_EXCEPTION) {\n+          assert(klass != nullptr, \"Must  be\");\n+          InstanceKlass::cast(klass)->initialize(THREAD);\n+          if (fs.access_flags().is_static()) {\n+            if (java_mirror()->obj_field(fs.offset()) == nullptr) {\n+              java_mirror()->obj_field_put(fs.offset(), InlineKlass::cast(klass)->default_value());\n+            }\n+          }\n+        }\n+\n+        if (HAS_PENDING_EXCEPTION) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          {\n+            EXCEPTION_MARK;\n+            add_initialization_error(THREAD, e);\n+            \/\/ Locks object, set state, and notify all waiting threads\n+            set_initialization_state_and_notify(initialization_error, THREAD);\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+          THROW_OOP(e());\n+        }\n+      }\n+    }\n+  }\n+\n+\n+  \/\/ Step 9\n@@ -1143,1 +1335,1 @@\n-  \/\/ Step 9\n+  \/\/ Step 10\n@@ -1149,1 +1341,1 @@\n-    \/\/ Step 10 and 11\n+    \/\/ Step 11 and 12\n@@ -1418,1 +1610,2 @@\n-        ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, CHECK_NULL);\n+        ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this,\n+                                                                  false, false, CHECK_NULL);\n@@ -1425,2 +1618,2 @@\n-  ObjArrayKlass* oak = array_klasses();\n-  return oak->array_klass(n, THREAD);\n+  ArrayKlass* ak = array_klasses();\n+  return ak->array_klass(n, THREAD);\n@@ -1431,2 +1624,2 @@\n-  ObjArrayKlass* oak = array_klasses_acquire();\n-  if (oak == nullptr) {\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == nullptr) {\n@@ -1435,1 +1628,1 @@\n-    return oak->array_klass_or_null(n);\n+    return ak->array_klass_or_null(n);\n@@ -1452,1 +1645,1 @@\n-  if (clinit != nullptr && clinit->has_valid_initializer_flags()) {\n+  if (clinit != nullptr && clinit->is_class_initializer()) {\n@@ -1501,1 +1694,1 @@\n-    MutexLocker x(OopMapCacheAlloc_lock);\n+    MutexLocker x(OopMapCacheAlloc_lock,  Mutex::_no_safepoint_check_flag);\n@@ -1513,5 +1706,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n-\n@@ -1588,0 +1776,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->first_field_offset() && offset < (vk->first_field_offset() + vk->get_exact_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -1980,0 +2177,4 @@\n+    if (name == vmSymbols::object_initializer_name() ||\n+        name == vmSymbols::inline_factory_name()) {\n+      break;  \/\/ <init> and <vnew> is never inherited\n+    }\n@@ -2443,0 +2644,1 @@\n+  it->push(&_preload_classes);\n@@ -2444,0 +2646,6 @@\n+\n+  if (has_inline_type_fields()) {\n+    for (int i = 0; i < java_fields_count(); i++) {\n+      it->push(&((Klass**)adr_inline_type_field_klasses())[i]);\n+    }\n+  }\n@@ -2487,1 +2695,9 @@\n-  \/\/ These are not allocated from metaspace. They are safe to set to null.\n+  if (has_inline_type_fields()) {\n+    for (AllFieldStream fs(fields(), constants()); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_PRIMITIVE_OBJECT) {\n+        reset_inline_type_field_klass(fs.index());\n+      }\n+    }\n+  }\n+\n+  \/\/ These are not allocated from metaspace. They are safe to set to nullptr.\n@@ -2549,0 +2765,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2731,0 +2951,2 @@\n+  return signature_name_of_carrier(JVM_SIGNATURE_CLASS);\n+}\n@@ -2732,0 +2954,1 @@\n+const char* InstanceKlass::signature_name_of_carrier(char c) const {\n@@ -2738,1 +2961,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -2740,1 +2963,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = c;\n@@ -3082,2 +3305,1 @@\n-  \/\/ Remember to strip ACC_SUPER bit\n-  return (access & (~JVM_ACC_SUPER)) & JVM_ACC_WRITTEN_FLAGS;\n+  return (access & JVM_ACC_WRITTEN_FLAGS);\n@@ -3340,1 +3562,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3344,0 +3569,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3347,0 +3577,6 @@\n+    } else if (self != nullptr && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3353,1 +3589,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(nullptr, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3367,0 +3624,1 @@\n+  st->print(BULLET\"misc flags:        0x%x\", _misc_flags.flags());               st->cr();\n@@ -3393,15 +3651,3 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);                  st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);      st->cr();\n-  if (Verbose && default_methods() != nullptr) {\n-    Array<Method*>* method_array = default_methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n+  st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3409,1 +3655,1 @@\n-    st->print(BULLET\"default vtable indices:   \"); default_vtable_indices()->print_value_on(st);       st->cr();\n+    st->print(BULLET\"default vtable indices:   \"); print_array_on(st, default_vtable_indices());\n@@ -3411,2 +3657,2 @@\n-  st->print(BULLET\"local interfaces:  \"); local_interfaces()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"trans. interfaces: \"); transitive_interfaces()->print_value_on(st); st->cr();\n+  st->print(BULLET\"local interfaces:  \"); print_array_on(st, local_interfaces());\n+  st->print(BULLET\"trans. interfaces: \"); print_array_on(st, transitive_interfaces());\n@@ -3458,0 +3704,1 @@\n+  st->print(BULLET\"preload classes:     \"); preload_classes()->print_value_on(st); st->cr();\n@@ -3468,1 +3715,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(nullptr, start_of_itable(), itable_length(), st);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":291,"deletions":44,"binary":false,"changes":335,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -54,0 +55,2 @@\n+\/\/    [EMBEDDED inline_type_field_klasses] only if has_inline_fields() == true\n+\/\/    [EMBEDDED InlineKlassFixedBlock] only if is an InlineKlass instance\n@@ -70,0 +73,1 @@\n+class BufferedInlineTypeBlob;\n@@ -130,0 +134,17 @@\n+class SigEntry;\n+\n+class InlineKlassFixedBlock {\n+  Array<SigEntry>** _extended_sig;\n+  Array<VMRegPair>** _return_regs;\n+  address* _pack_handler;\n+  address* _pack_handler_jobject;\n+  address* _unpack_handler;\n+  int* _default_value_offset;\n+  ArrayKlass** _null_free_inline_array_klasses;\n+  int _alignment;\n+  int _first_field_offset;\n+  int _exact_size_in_bytes;\n+\n+  friend class InlineKlass;\n+};\n+\n@@ -135,0 +156,1 @@\n+  friend class TemplateTable;\n@@ -169,1 +191,1 @@\n-  ObjArrayKlass* volatile _array_klasses;\n+  ArrayKlass* volatile _array_klasses;\n@@ -288,0 +310,3 @@\n+  const Klass**   _inline_type_field_klasses; \/\/ For \"inline class\" fields, NULL if none present\n+  Array<u2>* _preload_classes;\n+  const InlineKlassFixedBlock* _adr_inlineklass_fixed_block;\n@@ -342,0 +367,28 @@\n+  bool has_inline_type_fields() const { return _misc_flags.has_inline_type_fields(); }\n+  void set_has_inline_type_fields()   { _misc_flags.set_has_inline_type_fields(true); }\n+\n+  bool is_empty_inline_type() const   { return _misc_flags.is_empty_inline_type(); }\n+  void set_is_empty_inline_type()     { _misc_flags.set_is_empty_inline_type(true); }\n+\n+  \/\/ Note:  The naturally_atomic property only applies to\n+  \/\/ inline classes; it is never true on identity classes.\n+  \/\/ The bit is placed on instanceKlass for convenience.\n+\n+  \/\/ Query if h\/w provides atomic load\/store for instances.\n+  bool is_naturally_atomic() const  { return _misc_flags.is_naturally_atomic(); }\n+  void set_is_naturally_atomic()    { _misc_flags.set_is_naturally_atomic(true); }\n+\n+  \/\/ Query if this class implements jl.NonTearable or was\n+  \/\/ mentioned in the JVM option ForceNonTearable.\n+  \/\/ This bit can occur anywhere, but is only significant\n+  \/\/ for inline classes *and* their super types.\n+  \/\/ It inherits from supers along with NonTearable.\n+  bool is_declared_atomic() const { return _misc_flags.is_declared_atomic(); }\n+  void set_is_declared_atomic()   { _misc_flags.set_is_declared_atomic(true); }\n+\n+  bool carries_value_modifier() const { return _misc_flags.carries_value_modifier(); }\n+  void set_carries_value_modifier()   { _misc_flags.set_carries_value_modifier(true); }\n+\n+  bool carries_identity_modifier() const  { return _misc_flags.carries_identity_modifier(); }\n+  void set_carries_identity_modifier()    { _misc_flags.set_carries_identity_modifier(true); }\n+\n@@ -357,3 +410,3 @@\n-  ObjArrayKlass* array_klasses() const     { return _array_klasses; }\n-  inline ObjArrayKlass* array_klasses_acquire() const; \/\/ load with acquire semantics\n-  inline void release_set_array_klasses(ObjArrayKlass* k); \/\/ store with release semantics\n+  ArrayKlass* array_klasses() const     { return _array_klasses; }\n+  inline ArrayKlass* array_klasses_acquire() const; \/\/ load with acquire semantics\n+  inline void release_set_array_klasses(ArrayKlass* k); \/\/ store with release semantics\n@@ -403,0 +456,2 @@\n+  bool    field_is_inlined(int index) const { return field(index)->is_inlined(); }\n+  bool    field_is_null_free_inline_type(int index) const;\n@@ -414,0 +469,3 @@\n+  Array<u2>* preload_classes() const { return _preload_classes; }\n+  void set_preload_classes(Array<u2>* c) { _preload_classes = c; }\n+\n@@ -545,0 +603,3 @@\n+  static ByteSize kind_offset() { return in_ByteSize(offset_of(InstanceKlass, _kind)); }\n+  static ByteSize misc_flags_offset() { return in_ByteSize(offset_of(InstanceKlass, _misc_flags)); }\n+\n@@ -887,0 +948,3 @@\n+  static ByteSize inline_type_field_klasses_offset() { return in_ByteSize(offset_of(InstanceKlass, _inline_type_field_klasses)); }\n+  static ByteSize adr_inlineklass_fixed_block_offset() { return in_ByteSize(offset_of(InstanceKlass, _adr_inlineklass_fixed_block)); }\n+\n@@ -942,1 +1006,2 @@\n-                  bool is_interface) {\n+                  bool is_interface,\n+                  int java_fields, bool is_inline_type) {\n@@ -947,1 +1012,3 @@\n-           (is_interface ? (int)sizeof(Klass*)\/wordSize : 0));\n+           (is_interface ? (int)sizeof(Klass*)\/wordSize : 0) +\n+           (java_fields * (int)sizeof(Klass*)\/wordSize) +\n+           (is_inline_type ? (int)sizeof(InlineKlassFixedBlock) : 0));\n@@ -953,1 +1020,3 @@\n-                                               is_interface());\n+                                               is_interface(),\n+                                               has_inline_type_fields() ? java_fields_count() : 0,\n+                                               is_inline_klass());\n@@ -961,0 +1030,1 @@\n+  bool bounds_check(address addr, bool edge_ok = false, intptr_t size_in_bytes = -1) const PRODUCT_RETURN0;\n@@ -967,0 +1037,6 @@\n+  inline address adr_inline_type_field_klasses() const;\n+  inline Klass* get_inline_type_field_klass(int idx) const;\n+  inline Klass* get_inline_type_field_klass_or_null(int idx) const;\n+  inline void set_inline_type_field_klass(int idx, Klass* k);\n+  inline void reset_inline_type_field_klass(int idx);\n+\n@@ -968,1 +1044,1 @@\n-  int size_helper() const {\n+  virtual int size_helper() const {\n@@ -1022,0 +1098,1 @@\n+  const char* signature_name_of_carrier(char c) const;\n@@ -1144,1 +1221,1 @@\n-  void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n+  virtual void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":86,"deletions":9,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -65,1 +65,46 @@\n-inline ObjArrayKlass* InstanceKlass::array_klasses_acquire() const {\n+inline address InstanceKlass::adr_inline_type_field_klasses() const {\n+  if (has_inline_type_fields()) {\n+    InstanceKlass* volatile* adr_impl = adr_implementor();\n+    if (adr_impl != NULL) {\n+      return (address)(adr_impl + 1);\n+    }\n+\n+    return (address)end_of_nonstatic_oop_maps();\n+  } else {\n+    return NULL;\n+  }\n+}\n+\n+inline Klass* InstanceKlass::get_inline_type_field_klass(int idx) const {\n+  assert(has_inline_type_fields(), \"Sanity checking\");\n+  assert(idx < java_fields_count(), \"IOOB\");\n+  Klass* k = ((Klass**)adr_inline_type_field_klasses())[idx];\n+  assert(k != NULL, \"Should always be set before being read\");\n+  assert(k->is_inline_klass(), \"Must be an inline type\");\n+  return k;\n+}\n+\n+inline Klass* InstanceKlass::get_inline_type_field_klass_or_null(int idx) const {\n+  assert(has_inline_type_fields(), \"Sanity checking\");\n+  assert(idx < java_fields_count(), \"IOOB\");\n+  Klass* k = ((Klass**)adr_inline_type_field_klasses())[idx];\n+  assert(k == NULL || k->is_inline_klass(), \"Must be an inline type\");\n+  return k;\n+}\n+\n+inline void InstanceKlass::set_inline_type_field_klass(int idx, Klass* k) {\n+  assert(has_inline_type_fields(), \"Sanity checking\");\n+  assert(idx < java_fields_count(), \"IOOB\");\n+  assert(k != NULL, \"Should not be set to NULL\");\n+  assert(((Klass**)adr_inline_type_field_klasses())[idx] == NULL, \"Should not be set twice\");\n+  ((Klass**)adr_inline_type_field_klasses())[idx] = k;\n+}\n+\n+inline void InstanceKlass::reset_inline_type_field_klass(int idx) {\n+  assert(has_inline_type_fields(), \"Sanity checking\");\n+  assert(idx < java_fields_count(), \"IOOB\");\n+  ((Klass**)adr_inline_type_field_klasses())[idx] = NULL;\n+}\n+\n+\n+inline ArrayKlass* InstanceKlass::array_klasses_acquire() const {\n@@ -69,1 +114,1 @@\n-inline void InstanceKlass::release_set_array_klasses(ObjArrayKlass* k) {\n+inline void InstanceKlass::release_set_array_klasses(ArrayKlass* k) {\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.inline.hpp","additions":47,"deletions":2,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -49,1 +49,12 @@\n-    flag(has_localvariable_table            , 1 << 14) \/* has localvariable information *\/\n+    flag(has_localvariable_table            , 1 << 14) \/* has localvariable information *\/ \\\n+    flag(has_inline_type_fields             , 1 << 15) \/* has inline fields and related embedded section is not empty *\/ \\\n+    flag(is_empty_inline_type               , 1 << 16) \/* empty inline type (*) *\/ \\\n+    flag(is_naturally_atomic                , 1 << 17) \/* loaded\/stored in one instruction *\/ \\\n+    flag(is_declared_atomic                 , 1 << 18) \/* Listed -XX:ForceNonTearable=clist option *\/ \\\n+    flag(carries_value_modifier             , 1 << 19) \/* the class or one of its super types has the ACC_VALUE modifier *\/ \\\n+    flag(carries_identity_modifier          , 1 << 20) \/* the class or one of its super types has the ACC_IDENTITY modifier *\/\n+\n+  \/* (*) An inline type is considered empty if it contains no non-static fields or\n+     if it contains only empty inline fields. Note that JITs have a slightly different\n+     definition: empty inline fields must be flattened otherwise the container won't\n+     be considered empty *\/\n@@ -62,1 +73,1 @@\n-  u2 _flags;\n+  u4 _flags;\n@@ -89,0 +100,7 @@\n+\n+  u4 flags() const { return _flags; }\n+\n+  static u4 is_empty_inline_type_value() {\n+    return _misc_is_empty_inline_type;\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/instanceKlassFlags.hpp","additions":20,"deletions":2,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -203,1 +203,2 @@\n-                           _shared_class_path_index(-1) {\n+                               _prototype_header(markWord::prototype()),\n+                               _shared_class_path_index(-1) {\n@@ -217,1 +218,1 @@\n-  int lh = array_layout_helper(tag, hsize, etype, exact_log2(esize));\n+  int lh = array_layout_helper(tag, false, hsize, etype, exact_log2(esize));\n@@ -747,0 +748,2 @@\n+     st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+     st->cr();\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+  InlineKlassKind,\n@@ -48,0 +49,1 @@\n+  FlatArrayKlassKind,\n@@ -102,1 +104,1 @@\n-  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops\n+  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops, 0xA0 if value types\n@@ -168,0 +170,1 @@\n+  markWord _prototype_header;  \/\/ inline type and inline array mark patterns\n@@ -397,1 +400,1 @@\n-  static const int _lh_array_tag_bits          = 2;\n+  static const int _lh_array_tag_bits          = 3;\n@@ -399,2 +402,10 @@\n-  static const int _lh_array_tag_obj_value     = ~0x01;   \/\/ 0x80000000 >> 30\n-  static const unsigned int _lh_array_tag_type_value = 0Xffffffff; \/\/ ~0x00,  \/\/ 0xC0000000 >> 30\n+  static const unsigned int _lh_array_tag_type_value = 0Xfffffffc;\n+  static const unsigned int _lh_array_tag_vt_value   = 0Xfffffffd;\n+  static const unsigned int _lh_array_tag_obj_value  = 0Xfffffffe;\n+\n+  \/\/ null-free array flag bit under the array tag bits, shift one more to get array tag value\n+  static const int _lh_null_free_shift = _lh_array_tag_shift - 1;\n+  static const int _lh_null_free_mask  = 1;\n+\n+  static const jint _lh_array_tag_flat_value_bit_inplace = (jint) (1 << _lh_array_tag_shift);\n+  static const jint _lh_null_free_array_bit_inplace = (jint) (_lh_null_free_mask << _lh_null_free_shift);\n@@ -418,2 +429,1 @@\n-    \/\/ _lh_array_tag_type_value == (lh >> _lh_array_tag_shift);\n-    return (juint)lh >= (juint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+    return (juint) _lh_array_tag_type_value == (juint)(lh >> _lh_array_tag_shift);\n@@ -422,2 +432,13 @@\n-    \/\/ _lh_array_tag_obj_value == (lh >> _lh_array_tag_shift);\n-    return (jint)lh < (jint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+    return (juint)_lh_array_tag_obj_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_flatArray(jint lh) {\n+    return (juint)_lh_array_tag_vt_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_null_free(jint lh) {\n+    assert(layout_helper_is_flatArray(lh) || layout_helper_is_objArray(lh), \"must be array of inline types\");\n+    return ((lh >> _lh_null_free_shift) & _lh_null_free_mask);\n+  }\n+  static jint layout_helper_set_null_free(jint lh) {\n+    lh |= (_lh_null_free_mask << _lh_null_free_shift);\n+    assert(layout_helper_is_null_free(lh), \"Bad encoding\");\n+    return lh;\n@@ -434,1 +455,1 @@\n-    assert(btvalue >= T_BOOLEAN && btvalue <= T_OBJECT, \"sanity\");\n+    assert((btvalue >= T_BOOLEAN && btvalue <= T_OBJECT) || btvalue == T_PRIMITIVE_OBJECT, \"sanity\");\n@@ -455,1 +476,1 @@\n-    assert(l2esz <= LogBytesPerLong,\n+    assert(layout_helper_element_type(lh) == T_PRIMITIVE_OBJECT || l2esz <= LogBytesPerLong,\n@@ -459,1 +480,1 @@\n-  static jint array_layout_helper(jint tag, int hsize, BasicType etype, int log2_esize) {\n+  static jint array_layout_helper(jint tag, bool null_free, int hsize, BasicType etype, int log2_esize) {\n@@ -461,0 +482,1 @@\n+      |    ((null_free ? 1 : 0) <<  _lh_null_free_shift)\n@@ -613,0 +635,1 @@\n+  virtual bool is_flatArray_klass_slow()    const { return false; }\n@@ -614,0 +637,2 @@\n+  \/\/ current implementation uses this method even in non debug builds\n+  virtual bool is_inline_klass_slow()       const { return false; }\n@@ -629,0 +654,1 @@\n+  bool is_inline_klass()                const { return is_inline_klass_slow(); } \/\/temporary hack\n@@ -630,1 +656,1 @@\n-  bool is_other_instance_klass()        const { return _kind == InstanceKlassKind; }\n+  bool is_other_instance_klass()        const { return _kind <= InlineKlassKind; }\n@@ -636,0 +662,1 @@\n+  bool is_flatArray_klass()             const { return assert_same_query( _kind == FlatArrayKlassKind, is_flatArray_klass_slow()); }\n@@ -640,0 +667,2 @@\n+  inline bool is_null_free_array_klass()      const { return layout_helper_is_null_free(layout_helper()); }\n+\n@@ -648,1 +677,2 @@\n-  bool is_super() const                 { return _access_flags.is_super(); }\n+  bool is_value_class() const           { return _access_flags.is_value_class(); }\n+  bool is_identity_class() const        { return _access_flags.is_identity_class(); }\n@@ -671,0 +701,11 @@\n+  \/\/ inline types and inline type array patterns\n+  markWord prototype_header() const {\n+    return _prototype_header;\n+  }\n+  static inline markWord default_prototype_header(Klass* k) {\n+    return (k == NULL) ? markWord::prototype() : k->prototype_header();\n+  }\n+\n+  inline void set_prototype_header(markWord header);\n+  static ByteSize prototype_header_offset() { return in_ByteSize(offset_of(Klass, _prototype_header)); }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":54,"deletions":13,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -1232,3 +1232,4 @@\n-  if (m->is_static())           return false;   \/\/ e.g., Stream.empty\n-  if (m->is_initializer())      return false;   \/\/ <init> or <clinit>\n-  if (m->is_private())          return false;   \/\/ uses direct call\n+  if (m->is_static())             return false;   \/\/ e.g., Stream.empty\n+  if (m->is_private())            return false;   \/\/ uses direct call\n+  if (m->is_object_constructor()) return false;   \/\/ <init>(...)V\n+  if (m->is_class_initializer())  return false;   \/\/ <clinit>()V\n@@ -1443,0 +1444,12 @@\n+int count_interface_methods_needing_itable_index(Array<Method*>* methods) {\n+  int method_count = 0;\n+  if (methods->length() > 0) {\n+    for (int i = methods->length(); --i >= 0; ) {\n+      if (interface_method_needs_itable_index(methods->at(i))) {\n+        method_count++;\n+      }\n+    }\n+  }\n+  return method_count;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klassVtable.cpp","additions":16,"deletions":3,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -297,1 +297,4 @@\n-  int size_offset_table()                { return _size_offset_table; }\n+  InstanceKlass* klass() const          { return _klass; }\n+  int table_offset() const              { return _table_offset; }\n+  int size_offset_table() const         { return _size_offset_table; }\n+  int size_method_table() const         { return _size_method_table; }\n","filename":"src\/hotspot\/share\/oops\/klassVtable.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+\/\/\n@@ -64,0 +65,64 @@\n+\/\/\n+\/\/\n+\/\/\n+\/\/  Valhalla\n+\/\/\n+\/\/  <CMH: merge this doc into the text above>\n+\/\/\n+\/\/  Project Valhalla has mark word encoding requirements for the following oops:\n+\/\/\n+\/\/  * inline types: have alternative bytecode behavior, e.g. can not be locked\n+\/\/    - \"larval state\": mutable state, but only during object init, observable\n+\/\/      by only by a single thread (generally do not mutate markWord)\n+\/\/\n+\/\/  * flat arrays: load\/decode of klass layout helper is expensive for aaload\n+\/\/\n+\/\/  * \"null free\" arrays: load\/decode of klass layout helper again for aaload\n+\/\/\n+\/\/  EnableValhalla\n+\/\/\n+\/\/  Formerly known as \"biased lock bit\", \"unused_gap\" is free to use: using this\n+\/\/  bit to indicate inline type, combined with \"unlocked\" lock bits, means we\n+\/\/  will not interfere with lock encodings (displaced, inflating, and monitor),\n+\/\/  since inline types can't be locked.\n+\/\/\n+\/\/  Further state encoding\n+\/\/\n+\/\/  32 bit plaforms currently have no further room for encoding. No room for\n+\/\/  \"denormalized layout helper bits\", these fast mark word tests can only be made on\n+\/\/  64 bit platforms. 32-bit platforms need to load the klass->_layout_helper. This\n+\/\/  said, the larval state bit is still required for operation, stealing from the hash\n+\/\/  code is simplest mechanism.\n+\/\/\n+\/\/  Valhalla specific encodings\n+\/\/\n+\/\/  Revised Bit-format of an object header (most significant first, big endian layout below):\n+\/\/\n+\/\/  32 bits:\n+\/\/  --------\n+\/\/  hash:24 ------------>| larval:1 age:4 inline_type:1 lock:2\n+\/\/\n+\/\/  64 bits:\n+\/\/  --------\n+\/\/  unused:1 | <-- hash:31 -->| unused:22 larval:1 age:4 flat_array:1 null_free_array:1 inline_type:1 lock:2\n+\/\/\n+\/\/  The \"fast\" static type bits (flat_array, null_free_array, and inline_type)\n+\/\/  are placed lowest next to lock bits to more easily decode forwarding pointers.\n+\/\/  G1 for example, implicitly clears age bits (\"G1FullGCCompactionPoint::forward()\")\n+\/\/  using \"oopDesc->forwardee()\", so it necessary for \"markWord::decode_pointer()\"\n+\/\/  to return a non-NULL for this case, but not confuse the static type bits for\n+\/\/  a pointer.\n+\/\/\n+\/\/  Static types bits are recorded in the \"klass->prototype_header()\", displaced\n+\/\/  mark should simply use the prototype header as \"slow path\", rather chasing\n+\/\/  monitor or stack lock races.\n+\/\/\n+\/\/  Lock patterns (note inline types can't be locked\/monitor\/inflating)...\n+\/\/\n+\/\/  [ptr            | 000]  locked             ptr points to real header on stack\n+\/\/  [header         | ?01]  unlocked           regular object header\n+\/\/  [ptr            | 010]  monitor            inflated lock (header is wapped out)\n+\/\/  [ptr            | ?11]  marked             used to mark an object\n+\/\/  [0 ............ | 000]  inflating          inflation in progress\n+\/\/\n+\/\/\n@@ -102,2 +167,1 @@\n-  \/\/ Constants\n-  static const int age_bits                       = 4;\n+  \/\/ Constants, in least significant bit order\n@@ -105,2 +169,9 @@\n-  static const int first_unused_gap_bits          = 1;\n-  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - first_unused_gap_bits;\n+  static const int first_unused_gap_bits          = 1; \/\/ When !EnableValhalla\n+  \/\/ EnableValhalla: static prototype header bits (fast path instead of klass layout_helper)\n+  static const int inline_type_bits               = 1;\n+  static const int null_free_array_bits           = LP64_ONLY(1) NOT_LP64(0);\n+  static const int flat_array_bits                = LP64_ONLY(1) NOT_LP64(0);\n+  \/\/ instance state\n+  static const int age_bits                       = 4;\n+  static const int larval_bits                    = 1;\n+  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - inline_type_bits - larval_bits - flat_array_bits - null_free_array_bits;\n@@ -108,1 +179,1 @@\n-  static const int second_unused_gap_bits         = LP64_ONLY(1) NOT_LP64(0);\n+  static const int second_unused_gap_bits         = LP64_ONLY(1) NOT_LP64(0); \/\/ !EnableValhalla: unused\n@@ -111,2 +182,7 @@\n-  static const int age_shift                      = lock_bits + first_unused_gap_bits;\n-  static const int hash_shift                     = age_shift + age_bits + second_unused_gap_bits;\n+  static const int inline_type_shift              = lock_bits;\n+  static const int null_free_array_shift          = inline_type_shift + inline_type_bits;\n+  static const int flat_array_shift               = null_free_array_shift + null_free_array_bits;\n+  static const int age_shift                      = flat_array_shift + flat_array_bits;\n+  static const int unused_gap_shift               = age_shift + age_bits; \/\/ !EnableValhalla: unused\n+  static const int larval_shift                   = age_shift + age_bits;\n+  static const int hash_shift                     = LP64_ONLY(32) NOT_LP64(larval_shift + larval_bits);\n@@ -116,0 +192,10 @@\n+  static const uintptr_t inline_type_mask         = right_n_bits(lock_bits + inline_type_bits);\n+  static const uintptr_t inline_type_mask_in_place = inline_type_mask << lock_shift;\n+  static const uintptr_t inline_type_bit_in_place = 1 << inline_type_shift;\n+  static const uintptr_t null_free_array_mask     = right_n_bits(null_free_array_bits);\n+  static const uintptr_t null_free_array_mask_in_place = (null_free_array_mask << null_free_array_shift) | lock_mask_in_place;\n+  static const uintptr_t null_free_array_bit_in_place  = (1 << null_free_array_shift);\n+  static const uintptr_t flat_array_mask          = right_n_bits(flat_array_bits);\n+  static const uintptr_t flat_array_mask_in_place = (flat_array_mask << flat_array_shift) | null_free_array_mask_in_place | lock_mask_in_place;\n+  static const uintptr_t flat_array_bit_in_place  = (1 << flat_array_shift);\n+\n@@ -118,0 +204,5 @@\n+\n+  static const uintptr_t larval_mask              = right_n_bits(larval_bits);\n+  static const uintptr_t larval_mask_in_place     = (larval_mask << larval_shift) | inline_type_mask_in_place;\n+  static const uintptr_t larval_bit_in_place      = (1 << larval_shift);\n+\n@@ -126,0 +217,10 @@\n+  static const uintptr_t inline_type_pattern      = inline_type_bit_in_place | unlocked_value;\n+  static const uintptr_t null_free_array_pattern  = null_free_array_bit_in_place | unlocked_value;\n+  static const uintptr_t flat_array_pattern       = flat_array_bit_in_place | null_free_array_pattern;\n+  \/\/ Has static klass prototype, used for decode\/encode pointer\n+  static const uintptr_t static_prototype_mask    = LP64_ONLY(right_n_bits(inline_type_bits + flat_array_bits + null_free_array_bits)) NOT_LP64(right_n_bits(inline_type_bits));\n+  static const uintptr_t static_prototype_mask_in_place = static_prototype_mask << lock_bits;\n+  static const uintptr_t static_prototype_value_max = (1 << age_shift) - 1;\n+\n+  static const uintptr_t larval_pattern           = larval_bit_in_place | inline_type_pattern;\n+\n@@ -135,0 +236,4 @@\n+  bool is_inline_type() const {\n+    return (mask_bits(value(), inline_type_mask_in_place) == inline_type_pattern);\n+  }\n+\n@@ -145,0 +250,3 @@\n+\n+  \/\/ is unlocked and not an inline type (which cannot be involved in locking, displacement or inflation)\n+  \/\/ i.e. test both lock bits and the inline type bit together\n@@ -146,1 +254,1 @@\n-    return (mask_bits(value(), lock_mask_in_place) == unlocked_value);\n+    return (mask_bits(value(), inline_type_mask_in_place) == unlocked_value);\n@@ -163,1 +271,1 @@\n-    return (!is_unlocked() || !has_no_hash());\n+    return (!is_unlocked() || !has_no_hash() || (EnableValhalla && is_larval_state()));\n@@ -235,0 +343,30 @@\n+  \/\/ private buffered value operations\n+  markWord enter_larval_state() const {\n+    return markWord(value() | larval_bit_in_place);\n+  }\n+  markWord exit_larval_state() const {\n+    return markWord(value() & ~larval_bit_in_place);\n+  }\n+  bool is_larval_state() const {\n+    return (mask_bits(value(), larval_mask_in_place) == larval_pattern);\n+  }\n+\n+#ifdef _LP64 \/\/ 64 bit encodings only\n+  bool is_flat_array() const {\n+    return (mask_bits(value(), flat_array_mask_in_place) == flat_array_pattern);\n+  }\n+\n+  bool is_null_free_array() const {\n+    return (mask_bits(value(), null_free_array_mask_in_place) == null_free_array_pattern);\n+  }\n+#else\n+  bool is_flat_array() const {\n+    fatal(\"Should not ask this for mark word, ask oopDesc\");\n+    return false;\n+  }\n+\n+  bool is_null_free_array() const {\n+    fatal(\"Should not ask this for mark word, ask oopDesc\");\n+    return false;\n+  }\n+#endif\n@@ -240,0 +378,14 @@\n+  static markWord inline_type_prototype() {\n+    return markWord(inline_type_pattern);\n+  }\n+\n+#ifdef _LP64 \/\/ 64 bit encodings only\n+  static markWord flat_array_prototype() {\n+    return markWord(flat_array_pattern);\n+  }\n+\n+  static markWord null_free_array_prototype() {\n+    return markWord(null_free_array_pattern);\n+  }\n+#endif\n+\n@@ -247,1 +399,4 @@\n-  inline void* decode_pointer() { return (void*)clear_lock_bits().value(); }\n+  inline void* decode_pointer() {\n+    return (EnableValhalla && _value < static_prototype_value_max) ? NULL :\n+      (void*) (clear_lock_bits().value());\n+  }\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":165,"deletions":10,"binary":false,"changes":175,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -123,1 +124,0 @@\n-\n@@ -161,0 +161,5 @@\n+address Method::get_c2i_inline_entry() {\n+  assert(adapter() != nullptr, \"must have\");\n+  return adapter()->get_c2i_inline_entry();\n+}\n+\n@@ -166,0 +171,5 @@\n+address Method::get_c2i_unverified_inline_entry() {\n+  assert(adapter() != nullptr, \"must have\");\n+  return adapter()->get_c2i_unverified_inline_entry();\n+}\n+\n@@ -672,0 +682,13 @@\n+\/\/ InlineKlass the method is declared to return. This must not\n+\/\/ safepoint as it is called with references live on the stack at\n+\/\/ locations the GC is unaware of.\n+InlineKlass* Method::returns_inline_type(Thread* thread) const {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  NoSafepointVerifier nsv;\n+  SignatureStream ss(signature());\n+  while (!ss.at_return_type()) {\n+    ss.next();\n+  }\n+  return ss.as_inline_klass(method_holder());\n+}\n+\n@@ -677,1 +700,1 @@\n-  \/\/   aload_0\n+  \/\/   aload_0, _fast_aload_0, or _nofast_aload_0\n@@ -701,1 +724,2 @@\n-  if (cb[0] != Bytecodes::_aload_0 || cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {\n+  if ((cb[0] != Bytecodes::_aload_0 && cb[0] != Bytecodes::_fast_aload_0 && cb[0] != Bytecodes::_nofast_aload_0) ||\n+       cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {\n@@ -859,0 +883,5 @@\n+  if (has_scalarized_return()) {\n+    \/\/ Don't treat this as (trivial) getter method because the\n+    \/\/ inline type should be returned in a scalarized form.\n+    return false;\n+  }\n@@ -880,0 +909,5 @@\n+  if (has_scalarized_args()) {\n+    \/\/ Don't treat this as (trivial) setter method because the\n+    \/\/ inline type argument should be passed in a scalarized form.\n+    return false;\n+  }\n@@ -890,1 +924,2 @@\n-          Bytecodes::is_return(java_code_at(last_index)));\n+          Bytecodes::is_return(java_code_at(last_index)) &&\n+          !has_scalarized_args());\n@@ -893,2 +928,2 @@\n-bool Method::is_initializer() const {\n-  return is_object_initializer() || is_static_initializer();\n+bool Method::is_object_constructor_or_class_initializer() const {\n+  return (is_object_constructor() || is_class_initializer());\n@@ -897,6 +932,1 @@\n-bool Method::has_valid_initializer_flags() const {\n-  return (is_static() ||\n-          method_holder()->major_version() < 51);\n-}\n-\n-bool Method::is_static_initializer() const {\n+bool Method::is_class_initializer() const {\n@@ -906,2 +936,3 @@\n-  return name() == vmSymbols::class_initializer_name() &&\n-         has_valid_initializer_flags();\n+  return (name() == vmSymbols::class_initializer_name() &&\n+          (is_static() ||\n+           method_holder()->major_version() < 51));\n@@ -910,2 +941,8 @@\n-bool Method::is_object_initializer() const {\n-   return name() == vmSymbols::object_initializer_name();\n+\/\/ A method named <init>, is a classic object constructor.\n+bool Method::is_object_constructor() const {\n+  return name() == vmSymbols::object_initializer_name();\n+}\n+\n+\/\/ A method named <vnew> is a factory for an inline class.\n+bool Method::is_static_vnew_factory() const {\n+  return name() == vmSymbols::inline_factory_name();\n@@ -969,1 +1006,1 @@\n-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {\n+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {\n@@ -985,1 +1022,3 @@\n-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;\n+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {\n+      return false;\n+    }\n@@ -1154,0 +1193,2 @@\n+    _from_compiled_inline_entry = nullptr;\n+    _from_compiled_inline_ro_entry = nullptr;\n@@ -1156,0 +1197,2 @@\n+    _from_compiled_inline_entry = adapter()->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = adapter()->get_c2i_inline_ro_entry();\n@@ -1187,0 +1230,2 @@\n+  _from_compiled_inline_entry = nullptr;\n+  _from_compiled_inline_ro_entry = nullptr;\n@@ -1225,0 +1270,3 @@\n+  if (InlineTypeReturnedAsFields && returns_inline_type(THREAD)) {\n+    set_has_scalarized_return(true);\n+  }\n@@ -1264,0 +1312,2 @@\n+  mh->_from_compiled_inline_entry = adapter->get_c2i_inline_entry();\n+  mh->_from_compiled_inline_ro_entry = adapter->get_c2i_inline_ro_entry();\n@@ -1280,0 +1330,12 @@\n+address Method::verified_inline_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_entry != nullptr, \"must be set\");\n+  return _from_compiled_inline_entry;\n+}\n+\n+address Method::verified_inline_ro_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_ro_entry != nullptr, \"must be set\");\n+  return _from_compiled_inline_ro_entry;\n+}\n+\n@@ -1311,0 +1373,2 @@\n+  mh->_from_compiled_inline_entry = code->verified_inline_entry_point();\n+  mh->_from_compiled_inline_ro_entry = code->verified_inline_ro_entry_point();\n@@ -2287,0 +2351,25 @@\n+bool Method::is_scalarized_arg(int idx) const {\n+  if (!has_scalarized_args()) {\n+    return false;\n+  }\n+  \/\/ Search through signature and check if argument is wrapped in T_PRIMITIVE_OBJECT\/T_VOID\n+  int depth = 0;\n+  const GrowableArray<SigEntry>* sig = adapter()->get_sig_cc();\n+  for (int i = 0; i < sig->length(); i++) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_PRIMITIVE_OBJECT) {\n+      depth++;\n+    }\n+    if (idx == 0) {\n+      break; \/\/ Argument found\n+    }\n+    if (bt == T_VOID && (sig->at(i-1)._bt != T_LONG && sig->at(i-1)._bt != T_DOUBLE)) {\n+      depth--;\n+    }\n+    if (depth == 0 && bt != T_LONG && bt != T_DOUBLE) {\n+      idx--; \/\/ Advance to next argument\n+    }\n+  }\n+  return depth != 0;\n+}\n+\n@@ -2318,0 +2407,4 @@\n+#ifdef ASSERT\n+  if (valid_itable_index())\n+    st->print_cr(\" - itable index:      %d\",   itable_index());\n+#endif\n@@ -2325,1 +2418,3 @@\n-  st->print_cr(\" - compiled entry     \" PTR_FORMAT, p2i(from_compiled_entry()));\n+  st->print_cr(\" - compiled entry           \" PTR_FORMAT, p2i(from_compiled_entry()));\n+  st->print_cr(\" - compiled inline entry    \" PTR_FORMAT, p2i(from_compiled_inline_entry()));\n+  st->print_cr(\" - compiled inline ro entry \" PTR_FORMAT, p2i(from_compiled_inline_ro_entry()));\n@@ -2395,0 +2490,1 @@\n+  if (WizardMode) access_flags().print_on(st);\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":115,"deletions":19,"binary":false,"changes":134,"status":"modified"},{"patch":"@@ -94,3 +94,8 @@\n-    _scoped                 = 1 << 7,\n-    _changes_current_thread = 1 << 8,\n-    _jvmti_mount_transition = 1 << 9,\n+    _scalarized_args        = 1 << 7,\n+    _scalarized_return      = 1 << 8,\n+    _c1_needs_stack_repair  = 1 << 9,\n+    _c2_needs_stack_repair  = 1 << 10,\n+    _scoped                 = 1 << 11,\n+    _changes_current_thread = 1 << 12,\n+    _jvmti_mount_transition = 1 << 13,\n+    _mismatch               = 1 << 14\n@@ -111,1 +116,3 @@\n-  volatile address _from_compiled_entry;        \/\/ Cache of: _code ? _code->entry_point() : _adapter->c2i_entry()\n+  volatile address _from_compiled_entry;           \/\/ Cache of: _code ? _code->verified_entry_point()           : _adapter->c2i_entry()\n+  volatile address _from_compiled_inline_ro_entry; \/\/ Cache of: _code ? _code->verified_inline_ro_entry_point() : _adapter->c2i_inline_ro_entry()\n+  volatile address _from_compiled_inline_entry;    \/\/ Cache of: _code ? _code->verified_inline_entry_point()    : _adapter->c2i_inline_entry()\n@@ -150,0 +157,2 @@\n+  address from_compiled_inline_ro_entry() const;\n+  address from_compiled_inline_entry() const;\n@@ -431,0 +440,2 @@\n+  address verified_inline_code_entry();\n+  address verified_inline_ro_code_entry();\n@@ -449,1 +460,7 @@\n-    _from_compiled_entry =  entry;\n+    _from_compiled_entry = entry;\n+  }\n+  void set_from_compiled_inline_ro_entry(address entry) {\n+    _from_compiled_inline_ro_entry = entry;\n+  }\n+  void set_from_compiled_inline_entry(address entry) {\n+    _from_compiled_inline_entry = entry;\n@@ -454,0 +471,1 @@\n+  address get_c2i_inline_entry();\n@@ -455,0 +473,1 @@\n+  address get_c2i_unverified_inline_entry();\n@@ -568,1 +587,1 @@\n-  bool is_returning_fp() const                   { BasicType r = result_type(); return (r == T_FLOAT || r == T_DOUBLE); }\n+  InlineKlass* returns_inline_type(Thread* thread) const;\n@@ -643,6 +662,0 @@\n-  \/\/ returns true if the method is an initializer (<init> or <clinit>).\n-  bool is_initializer() const;\n-\n-  \/\/ returns true if the method is static OR if the classfile version < 51\n-  bool has_valid_initializer_flags() const;\n-\n@@ -651,1 +664,1 @@\n-  bool is_static_initializer() const;\n+  bool is_class_initializer() const;\n@@ -653,2 +666,9 @@\n-  \/\/ returns true if the method name is <init>\n-  bool is_object_initializer() const;\n+  \/\/ returns true if the method name is <init> and the method is not a static factory\n+  bool is_object_constructor() const;\n+\n+  \/\/ returns true if the method is an object constructor or class initializer\n+  \/\/ (non-static <init> or <clinit>), but false for factories (static <vnew>).\n+  bool is_object_constructor_or_class_initializer() const;\n+\n+  \/\/ returns true if the method name is <vnew> and the method is static\n+  bool is_static_vnew_factory() const;\n@@ -676,0 +696,2 @@\n+  static ByteSize from_compiled_inline_offset()  { return byte_offset_of(Method, _from_compiled_inline_entry); }\n+  static ByteSize from_compiled_inline_ro_offset(){ return byte_offset_of(Method, _from_compiled_inline_ro_entry); }\n@@ -677,0 +699,1 @@\n+  static ByteSize flags_offset()                 { return byte_offset_of(Method, _flags); }\n@@ -896,0 +919,46 @@\n+  bool has_scalarized_args() const {\n+    return (_flags & _scalarized_args) != 0;\n+  }\n+\n+  void set_has_scalarized_args(bool x) {\n+    _flags = x ? (_flags | _scalarized_args) : (_flags & ~_scalarized_args);\n+  }\n+\n+  bool has_scalarized_return() const {\n+    return (_flags & _scalarized_return) != 0;\n+  }\n+\n+  void set_has_scalarized_return(bool x) {\n+    _flags = x ? (_flags | _scalarized_return) : (_flags & ~_scalarized_return);\n+  }\n+\n+  static u2 scalarized_return_flag() {\n+    return _scalarized_return;\n+  }\n+\n+  bool is_scalarized_arg(int idx) const;\n+\n+  bool c1_needs_stack_repair() const {\n+    return (_flags & _c1_needs_stack_repair) != 0;\n+  }\n+\n+  bool c2_needs_stack_repair() const {\n+    return (_flags & _c2_needs_stack_repair) != 0;\n+  }\n+\n+  void set_c1_needs_stack_repair(bool x) {\n+    _flags = x ? (_flags | _c1_needs_stack_repair) : (_flags & ~_c1_needs_stack_repair);\n+  }\n+\n+  void set_c2_needs_stack_repair(bool x) {\n+    _flags = x ? (_flags | _c2_needs_stack_repair) : (_flags & ~_c2_needs_stack_repair);\n+  }\n+\n+  bool mismatch() const {\n+    return (_flags & _mismatch) != 0;\n+  }\n+\n+  void set_mismatch(bool x) {\n+    _flags = x ? (_flags | _mismatch) : (_flags & ~_mismatch);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":84,"deletions":15,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -37,0 +37,8 @@\n+inline address Method::from_compiled_inline_ro_entry() const {\n+  return Atomic::load_acquire(&_from_compiled_inline_ro_entry);\n+}\n+\n+inline address Method::from_compiled_inline_entry() const {\n+  return Atomic::load_acquire(&_from_compiled_inline_entry);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/method.inline.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -143,1 +143,1 @@\n-    st->print(\"flags(%d) \", flags);\n+    st->print(\"flags(%d) %p\/%d\", flags, data(), in_bytes(DataLayout::flags_offset()));\n@@ -213,1 +213,1 @@\n-  assert(TypeStackSlotEntries::per_arg_count() > ReturnTypeEntry::static_cell_count(), \"code to test for arguments\/results broken\");\n+  assert(TypeStackSlotEntries::per_arg_count() > SingleTypeEntry::static_cell_count(), \"code to test for arguments\/results broken\");\n@@ -223,1 +223,1 @@\n-    ret_cell = ReturnTypeEntry::static_cell_count();\n+    ret_cell = SingleTypeEntry::static_cell_count();\n@@ -326,1 +326,1 @@\n-void ReturnTypeEntry::clean_weak_klass_links(bool always_clean) {\n+void SingleTypeEntry::clean_weak_klass_links(bool always_clean) {\n@@ -364,1 +364,1 @@\n-void ReturnTypeEntry::print_data_on(outputStream* st) const {\n+void SingleTypeEntry::print_data_on(outputStream* st) const {\n@@ -529,0 +529,4 @@\n+  if (data()->flags()) {\n+    tty->cr();\n+    tab(st);\n+  }\n@@ -654,0 +658,21 @@\n+void ArrayLoadStoreData::print_data_on(outputStream* st, const char* extra) const {\n+  print_shared(st, \"ArrayLoadStore\", extra);\n+  st->cr();\n+  tab(st, true);\n+  st->print(\"array\");\n+  _array.print_data_on(st);\n+  tab(st, true);\n+  st->print(\"element\");\n+  _element.print_data_on(st);\n+}\n+\n+void ACmpData::print_data_on(outputStream* st, const char* extra) const {\n+  BranchData::print_data_on(st, extra);\n+  tab(st, true);\n+  st->print(\"left\");\n+  _left.print_data_on(st);\n+  tab(st, true);\n+  st->print(\"right\");\n+  _right.print_data_on(st);\n+}\n+\n@@ -675,1 +700,0 @@\n-  case Bytecodes::_aastore:\n@@ -681,0 +705,3 @@\n+  case Bytecodes::_aaload:\n+  case Bytecodes::_aastore:\n+    return ArrayLoadStoreData::static_cell_count();\n@@ -720,2 +747,0 @@\n-  case Bytecodes::_if_acmpeq:\n-  case Bytecodes::_if_acmpne:\n@@ -725,0 +750,3 @@\n+  case Bytecodes::_if_acmpne:\n+  case Bytecodes::_if_acmpeq:\n+    return ACmpData::static_cell_count();\n@@ -783,0 +811,1 @@\n+  case Bytecodes::_aaload:\n@@ -986,1 +1015,0 @@\n-  case Bytecodes::_aastore:\n@@ -995,0 +1023,5 @@\n+  case Bytecodes::_aaload:\n+  case Bytecodes::_aastore:\n+    cell_count = ArrayLoadStoreData::static_cell_count();\n+    tag = DataLayout::array_load_store_data_tag;\n+    break;\n@@ -1066,2 +1099,0 @@\n-  case Bytecodes::_if_acmpeq:\n-  case Bytecodes::_if_acmpne:\n@@ -1073,0 +1104,5 @@\n+  case Bytecodes::_if_acmpeq:\n+  case Bytecodes::_if_acmpne:\n+    cell_count = ACmpData::static_cell_count();\n+    tag = DataLayout::acmp_data_tag;\n+    break;\n@@ -1140,0 +1176,4 @@\n+  case DataLayout::array_load_store_data_tag:\n+    return ((new ArrayLoadStoreData(this))->cell_count());\n+  case DataLayout::acmp_data_tag:\n+    return ((new ACmpData(this))->cell_count());\n@@ -1174,0 +1214,4 @@\n+  case DataLayout::array_load_store_data_tag:\n+    return new ArrayLoadStoreData(this);\n+  case DataLayout::acmp_data_tag:\n+    return new ACmpData(this);\n","filename":"src\/hotspot\/share\/oops\/methodData.cpp","additions":55,"deletions":11,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -131,1 +131,3 @@\n-    speculative_trap_data_tag\n+    speculative_trap_data_tag,\n+    array_load_store_data_tag,\n+    acmp_data_tag\n@@ -267,0 +269,1 @@\n+class       ACmpData;\n@@ -272,0 +275,1 @@\n+class   ArrayLoadStoreData;\n@@ -279,1 +283,1 @@\n-  friend class ReturnTypeEntry;\n+  friend class SingleTypeEntry;\n@@ -400,0 +404,2 @@\n+  virtual bool is_ArrayLoadStoreData() const { return false; }\n+  virtual bool is_ACmpData()           const { return false; }\n@@ -458,0 +464,8 @@\n+  ArrayLoadStoreData* as_ArrayLoadStoreData() const {\n+    assert(is_ArrayLoadStoreData(), \"wrong type\");\n+    return is_ArrayLoadStoreData() ? (ArrayLoadStoreData*)this : NULL;\n+  }\n+  ACmpData* as_ACmpData() const {\n+    assert(is_ACmpData(), \"wrong type\");\n+    return is_ACmpData() ? (ACmpData*)this : NULL;\n+  }\n@@ -612,1 +626,2 @@\n-      layout->tag() == DataLayout::branch_data_tag, \"wrong type\");\n+      layout->tag() == DataLayout::branch_data_tag ||\n+      layout->tag() == DataLayout::acmp_data_tag, \"wrong type\");\n@@ -848,1 +863,1 @@\n-class ReturnTypeEntry : public TypeEntries {\n+class SingleTypeEntry : public TypeEntries {\n@@ -856,1 +871,1 @@\n-  ReturnTypeEntry(int base_off)\n+  SingleTypeEntry(int base_off)\n@@ -890,1 +905,1 @@\n-\/\/ (TypeStackSlotEntries), a return type (ReturnTypeEntry) and a\n+\/\/ (TypeStackSlotEntries), a return type (SingleTypeEntry) and a\n@@ -944,1 +959,1 @@\n-    return ReturnTypeEntry::size() + in_ByteSize(header_cell_count() * DataLayout::cell_size);\n+    return SingleTypeEntry::size() + in_ByteSize(header_cell_count() * DataLayout::cell_size);\n@@ -959,1 +974,1 @@\n-  ReturnTypeEntry _ret;\n+  SingleTypeEntry _ret;\n@@ -978,1 +993,1 @@\n-    _ret(cell_count() - ReturnTypeEntry::static_cell_count())\n+    _ret(cell_count() - SingleTypeEntry::static_cell_count())\n@@ -991,1 +1006,1 @@\n-  const ReturnTypeEntry* ret() const {\n+  const SingleTypeEntry* ret() const {\n@@ -1262,1 +1277,1 @@\n-  ReturnTypeEntry _ret;\n+  SingleTypeEntry _ret;\n@@ -1281,1 +1296,1 @@\n-    _ret(cell_count() - ReturnTypeEntry::static_cell_count())\n+    _ret(cell_count() - SingleTypeEntry::static_cell_count())\n@@ -1294,1 +1309,1 @@\n-  const ReturnTypeEntry* ret() const {\n+  const SingleTypeEntry* ret() const {\n@@ -1496,1 +1511,1 @@\n-    assert(layout->tag() == DataLayout::branch_data_tag, \"wrong type\");\n+    assert(layout->tag() == DataLayout::branch_data_tag || layout->tag() == DataLayout::acmp_data_tag, \"wrong type\");\n@@ -1853,0 +1868,146 @@\n+class ArrayLoadStoreData : public ProfileData {\n+private:\n+  enum {\n+    flat_array_flag = DataLayout::first_flag,\n+    null_free_array_flag = flat_array_flag + 1,\n+  };\n+\n+  SingleTypeEntry _array;\n+  SingleTypeEntry _element;\n+\n+public:\n+  ArrayLoadStoreData(DataLayout* layout) :\n+    ProfileData(layout),\n+    _array(0),\n+    _element(SingleTypeEntry::static_cell_count()) {\n+    assert(layout->tag() == DataLayout::array_load_store_data_tag, \"wrong type\");\n+    _array.set_profile_data(this);\n+    _element.set_profile_data(this);\n+  }\n+\n+  const SingleTypeEntry* array() const {\n+    return &_array;\n+  }\n+\n+  const SingleTypeEntry* element() const {\n+    return &_element;\n+  }\n+\n+  virtual bool is_ArrayLoadStoreData() const { return true; }\n+\n+  static int static_cell_count() {\n+    return SingleTypeEntry::static_cell_count() * 2;\n+  }\n+\n+  virtual int cell_count() const {\n+    return static_cell_count();\n+  }\n+\n+  void set_flat_array() { set_flag_at(flat_array_flag); }\n+  bool flat_array() const { return flag_at(flat_array_flag); }\n+\n+  void set_null_free_array() { set_flag_at(null_free_array_flag); }\n+  bool null_free_array() const { return flag_at(null_free_array_flag); }\n+\n+  \/\/ Code generation support\n+  static int flat_array_byte_constant() {\n+    return flag_number_to_constant(flat_array_flag);\n+  }\n+\n+  static int null_free_array_byte_constant() {\n+    return flag_number_to_constant(null_free_array_flag);\n+  }\n+\n+  static ByteSize array_offset() {\n+    return cell_offset(0);\n+  }\n+\n+  static ByteSize element_offset() {\n+    return cell_offset(SingleTypeEntry::static_cell_count());\n+  }\n+\n+  virtual void clean_weak_klass_links(bool always_clean) {\n+    _array.clean_weak_klass_links(always_clean);\n+    _element.clean_weak_klass_links(always_clean);\n+  }\n+\n+  static ByteSize array_load_store_data_size() {\n+    return cell_offset(static_cell_count());\n+  }\n+\n+  virtual void print_data_on(outputStream* st, const char* extra = NULL) const;\n+};\n+\n+class ACmpData : public BranchData {\n+private:\n+  enum {\n+    left_inline_type_flag = DataLayout::first_flag,\n+    right_inline_type_flag\n+  };\n+\n+  SingleTypeEntry _left;\n+  SingleTypeEntry _right;\n+\n+public:\n+  ACmpData(DataLayout* layout) :\n+    BranchData(layout),\n+    _left(BranchData::static_cell_count()),\n+    _right(BranchData::static_cell_count() + SingleTypeEntry::static_cell_count()) {\n+    assert(layout->tag() == DataLayout::acmp_data_tag, \"wrong type\");\n+    _left.set_profile_data(this);\n+    _right.set_profile_data(this);\n+  }\n+\n+  const SingleTypeEntry* left() const {\n+    return &_left;\n+  }\n+\n+  const SingleTypeEntry* right() const {\n+    return &_right;\n+  }\n+\n+  virtual bool is_ACmpData() const { return true; }\n+\n+  static int static_cell_count() {\n+    return BranchData::static_cell_count() + SingleTypeEntry::static_cell_count() * 2;\n+  }\n+\n+  virtual int cell_count() const {\n+    return static_cell_count();\n+  }\n+\n+  void set_left_inline_type() { set_flag_at(left_inline_type_flag); }\n+  bool left_inline_type() const { return flag_at(left_inline_type_flag); }\n+\n+  void set_right_inline_type() { set_flag_at(right_inline_type_flag); }\n+  bool right_inline_type() const { return flag_at(right_inline_type_flag); }\n+\n+  \/\/ Code generation support\n+  static int left_inline_type_byte_constant() {\n+    return flag_number_to_constant(left_inline_type_flag);\n+  }\n+\n+  static int right_inline_type_byte_constant() {\n+    return flag_number_to_constant(right_inline_type_flag);\n+  }\n+\n+  static ByteSize left_offset() {\n+    return cell_offset(BranchData::static_cell_count());\n+  }\n+\n+  static ByteSize right_offset() {\n+    return cell_offset(BranchData::static_cell_count() + SingleTypeEntry::static_cell_count());\n+  }\n+\n+  virtual void clean_weak_klass_links(bool always_clean) {\n+    _left.clean_weak_klass_links(always_clean);\n+    _right.clean_weak_klass_links(always_clean);\n+  }\n+\n+  static ByteSize acmp_data_size() {\n+    return cell_offset(static_cell_count());\n+  }\n+\n+  virtual void print_data_on(outputStream* st, const char* extra = NULL) const;\n+};\n+\n","filename":"src\/hotspot\/share\/oops\/methodData.hpp","additions":175,"deletions":14,"binary":false,"changes":189,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -48,1 +49,3 @@\n-ObjArrayKlass* ObjArrayKlass::allocate(ClassLoaderData* loader_data, int n, Klass* k, Symbol* name, TRAPS) {\n+ObjArrayKlass* ObjArrayKlass::allocate(ClassLoaderData* loader_data, int n,\n+                                       Klass* k, Symbol* name, bool null_free,\n+                                       TRAPS) {\n@@ -54,1 +57,1 @@\n-  return new (loader_data, size, THREAD) ObjArrayKlass(n, k, name);\n+  return new (loader_data, size, THREAD) ObjArrayKlass(n, k, name, null_free);\n@@ -58,1 +61,3 @@\n-                                                      int n, Klass* element_klass, TRAPS) {\n+                                                      int n, Klass* element_klass,\n+                                                      bool null_free, bool qdesc, TRAPS) {\n+  assert(!null_free || (n == 1 && element_klass->is_inline_klass() && qdesc), \"null-free unsupported\");\n@@ -66,1 +71,5 @@\n-      super_klass = element_super->array_klass_or_null();\n+      if (null_free) {\n+        super_klass = element_klass->array_klass_or_null();\n+      } else {\n+        super_klass = element_super->array_klass_or_null();\n+      }\n@@ -78,0 +87,5 @@\n+      if (null_free) {\n+        if (element_klass->array_klass_or_null() == nullptr) {\n+          supers_exist = false;\n+        }\n+      }\n@@ -83,1 +97,5 @@\n-          super_klass = element_super->array_klass(CHECK_NULL);\n+          if (null_free) {\n+            element_klass->array_klass(CHECK_NULL);\n+          } else {\n+            element_super->array_klass(CHECK_NULL);\n+          }\n@@ -89,1 +107,5 @@\n-          ek = element_klass->array_klass(n, CHECK_NULL);\n+          if (null_free) {\n+            ek = InlineKlass::cast(element_klass)->value_array_klass(CHECK_NULL);\n+          } else {\n+            ek = element_klass->array_klass(n, CHECK_NULL);\n+          }\n@@ -100,19 +122,1 @@\n-  Symbol* name = nullptr;\n-  {\n-    ResourceMark rm(THREAD);\n-    char *name_str = element_klass->name()->as_C_string();\n-    int len = element_klass->name()->utf8_length();\n-    char *new_str = NEW_RESOURCE_ARRAY(char, len + 4);\n-    int idx = 0;\n-    new_str[idx++] = JVM_SIGNATURE_ARRAY;\n-    if (element_klass->is_instance_klass()) { \/\/ it could be an array or simple type\n-      new_str[idx++] = JVM_SIGNATURE_CLASS;\n-    }\n-    memcpy(&new_str[idx], name_str, len * sizeof(char));\n-    idx += len;\n-    if (element_klass->is_instance_klass()) {\n-      new_str[idx++] = JVM_SIGNATURE_ENDCLASS;\n-    }\n-    new_str[idx++] = '\\0';\n-    name = SymbolTable::new_symbol(new_str);\n-  }\n+  Symbol* name = ArrayKlass::create_element_klass_array_name(element_klass, qdesc, CHECK_NULL);\n@@ -121,1 +125,1 @@\n-  ObjArrayKlass* oak = ObjArrayKlass::allocate(loader_data, n, element_klass, name, CHECK_NULL);\n+  ObjArrayKlass* oak = ObjArrayKlass::allocate(loader_data, n, element_klass, name, null_free, CHECK_NULL);\n@@ -139,1 +143,1 @@\n-ObjArrayKlass::ObjArrayKlass(int n, Klass* element_klass, Symbol* name) : ArrayKlass(name, Kind) {\n+ObjArrayKlass::ObjArrayKlass(int n, Klass* element_klass, Symbol* name, bool null_free) : ArrayKlass(name, Kind) {\n@@ -143,0 +147,2 @@\n+  assert(!null_free || name->is_Q_array_signature(), \"sanity check\");\n+\n@@ -146,0 +152,2 @@\n+  } else if (element_klass->is_flatArray_klass()) {\n+    bk = FlatArrayKlass::cast(element_klass)->element_klass();\n@@ -153,1 +161,12 @@\n-  set_layout_helper(array_layout_helper(T_OBJECT));\n+  int lh = array_layout_helper(T_OBJECT);\n+  if (null_free) {\n+    assert(n == 1, \"Bytecode does not support null-free multi-dim\");\n+    lh = layout_helper_set_null_free(lh);\n+#ifdef _LP64\n+    set_prototype_header(markWord::null_free_array_prototype());\n+    assert(prototype_header().is_null_free_array(), \"sanity\");\n+#else\n+    set_prototype_header(markWord::inline_type_prototype());\n+#endif\n+  }\n+  set_layout_helper(lh);\n@@ -166,2 +185,17 @@\n-  return (objArrayOop)Universe::heap()->array_allocate(this, size, length,\n-                                                       \/* do_zero *\/ true, THREAD);\n+  bool populate_null_free = is_null_free_array_klass();\n+  objArrayOop array =  (objArrayOop)Universe::heap()->array_allocate(this, size, length,\n+                                                       \/* do_zero *\/ true, CHECK_NULL);\n+  objArrayHandle array_h(THREAD, array);\n+  if (populate_null_free) {\n+    assert(dimension() == 1, \"Can only populate the final dimension\");\n+    assert(element_klass()->is_inline_klass(), \"Unexpected\");\n+    assert(!element_klass()->is_array_klass(), \"ArrayKlass unexpected here\");\n+    assert(!InlineKlass::cast(element_klass())->flatten_array(), \"Expected flatArrayOop allocation\");\n+    element_klass()->initialize(CHECK_NULL);\n+    \/\/ Populate default values...\n+    instanceOop value = (instanceOop) InlineKlass::cast(element_klass())->default_value();\n+    for (int i = 0; i < length; i++) {\n+      array_h->obj_at_put(i, value);\n+    }\n+  }\n+  return array_h();\n@@ -172,0 +206,8 @@\n+  if (rank == 1) { \/\/ last dim may be flatArray, check if we have any special storage requirements\n+    if (name()->char_at(1) != JVM_SIGNATURE_ARRAY &&  name()->is_Q_array_signature()) {\n+      return oopFactory::new_valueArray(element_klass(), length, CHECK_NULL);\n+    } else {\n+      return oopFactory::new_objArray(element_klass(), length, CHECK_NULL);\n+    }\n+  }\n+  guarantee(rank > 1, \"Rank below 1\");\n@@ -178,16 +220,14 @@\n-  if (rank > 1) {\n-    if (length != 0) {\n-      for (int index = 0; index < length; index++) {\n-        ArrayKlass* ak = ArrayKlass::cast(ld_klass);\n-        oop sub_array = ak->multi_allocate(rank-1, &sizes[1], CHECK_NULL);\n-        h_array->obj_at_put(index, sub_array);\n-      }\n-    } else {\n-      \/\/ Since this array dimension has zero length, nothing will be\n-      \/\/ allocated, however the lower dimension values must be checked\n-      \/\/ for illegal values.\n-      for (int i = 0; i < rank - 1; ++i) {\n-        sizes += 1;\n-        if (*sizes < 0) {\n-          THROW_MSG_0(vmSymbols::java_lang_NegativeArraySizeException(), err_msg(\"%d\", *sizes));\n-        }\n+  if (length != 0) {\n+    for (int index = 0; index < length; index++) {\n+      ArrayKlass* ak = ArrayKlass::cast(ld_klass);\n+      oop sub_array = ak->multi_allocate(rank-1, &sizes[1], CHECK_NULL);\n+      h_array->obj_at_put(index, sub_array);\n+    }\n+  } else {\n+    \/\/ Since this array dimension has zero length, nothing will be\n+    \/\/ allocated, however the lower dimension values must be checked\n+    \/\/ for illegal values.\n+    for (int i = 0; i < rank - 1; ++i) {\n+      sizes += 1;\n+      if (*sizes < 0) {\n+        THROW_MSG_0(vmSymbols::java_lang_NegativeArraySizeException(), err_msg(\"%d\", *sizes));\n@@ -211,0 +251,3 @@\n+    \/\/ Perform null check if dst is null-free but src has no such guarantee\n+    bool null_check = ((!s->klass()->is_null_free_array_klass()) &&\n+        d->klass()->is_null_free_array_klass());\n@@ -212,2 +255,5 @@\n-      \/\/ elements are guaranteed to be subtypes, so no check necessary\n-      ArrayAccess<ARRAYCOPY_DISJOINT>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n+      if (null_check) {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_NOTNULL>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n+      } else {\n+        ArrayAccess<ARRAYCOPY_DISJOINT>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n+      }\n@@ -215,16 +261,4 @@\n-      \/\/ slow case: need individual subtype checks\n-      \/\/ note: don't use obj_at_put below because it includes a redundant store check\n-      if (!ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST>::oop_arraycopy(s, src_offset, d, dst_offset, length)) {\n-        ResourceMark rm(THREAD);\n-        stringStream ss;\n-        if (!bound->is_subtype_of(stype)) {\n-          ss.print(\"arraycopy: type mismatch: can not copy %s[] into %s[]\",\n-                   stype->external_name(), bound->external_name());\n-        } else {\n-          \/\/ oop_arraycopy should return the index in the source array that\n-          \/\/ contains the problematic oop.\n-          ss.print(\"arraycopy: element type mismatch: can not cast one of the elements\"\n-                   \" of %s[] to the type of the destination array, %s\",\n-                   stype->external_name(), bound->external_name());\n-        }\n-        THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+      if (null_check) {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST | ARRAYCOPY_NOTNULL>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n+      } else {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n@@ -240,0 +274,7 @@\n+  if (UseFlatArray) {\n+    if (d->is_flatArray()) {\n+      FlatArrayKlass::cast(d->klass())->copy_array(s, src_pos, d, dst_pos, length, THREAD);\n+      return;\n+    }\n+  }\n+\n@@ -312,1 +353,0 @@\n-\n@@ -329,2 +369,2 @@\n-        Klass* k =\n-          ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, CHECK_NULL);\n+        Klass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this,\n+                                                          false, this->name()->is_Q_array_signature(), CHECK_NULL);\n@@ -440,1 +480,1 @@\n-  st->print(\" - instance klass: \");\n+  st->print(\" - element klass: \");\n@@ -502,1 +542,2 @@\n-  guarantee(bk->is_instance_klass() || bk->is_typeArray_klass(),  \"invalid bottom klass\");\n+  guarantee(bk->is_instance_klass() || bk->is_typeArray_klass() || bk->is_flatArray_klass(),\n+            \"invalid bottom klass\");\n@@ -508,0 +549,1 @@\n+  guarantee(obj->is_null_free_array() || (!is_null_free_array_klass()), \"null-free klass but not object\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":111,"deletions":69,"binary":false,"changes":180,"status":"modified"},{"patch":"@@ -121,1 +121,1 @@\n-  return !SafepointSynchronize::is_at_safepoint();\n+  return !SafepointSynchronize::is_at_safepoint() ;\n@@ -140,6 +140,8 @@\n-bool oopDesc::is_instance_noinline()    const { return is_instance();    }\n-bool oopDesc::is_instanceRef_noinline() const { return is_instanceRef(); }\n-bool oopDesc::is_stackChunk_noinline()  const { return is_stackChunk();  }\n-bool oopDesc::is_array_noinline()       const { return is_array();       }\n-bool oopDesc::is_objArray_noinline()    const { return is_objArray();    }\n-bool oopDesc::is_typeArray_noinline()   const { return is_typeArray();   }\n+bool oopDesc::is_instance_noinline()        const { return is_instance();         }\n+bool oopDesc::is_instanceRef_noinline()     const { return is_instanceRef();      }\n+bool oopDesc::is_stackChunk_noinline()      const { return is_stackChunk();       }\n+bool oopDesc::is_array_noinline()           const { return is_array();            }\n+bool oopDesc::is_objArray_noinline()        const { return is_objArray();         }\n+bool oopDesc::is_typeArray_noinline()       const { return is_typeArray();        }\n+bool oopDesc::is_flatArray_noinline()       const { return is_flatArray();        }\n+bool oopDesc::is_null_free_array_noinline() const { return is_null_free_array();  }\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":9,"deletions":7,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -44,0 +44,10 @@\n+\/\/\n+\/\/ oopDesc::_mark - the \"oop mark word\" encoding to be found separately in markWord.hpp\n+\/\/\n+\/\/ oopDesc::_metadata - encodes the object's klass pointer, as a raw pointer in \"_klass\"\n+\/\/                      or compressed pointer in \"_compressed_klass\"\n+\/\/\n+\/\/ The overall size of the _metadata field is dependent on \"UseCompressedClassPointers\",\n+\/\/ hence the terms \"narrow\" (32 bits) vs \"wide\" (64 bits).\n+\/\/\n+\n@@ -112,6 +122,9 @@\n-  inline bool is_instance()    const;\n-  inline bool is_instanceRef() const;\n-  inline bool is_stackChunk()  const;\n-  inline bool is_array()       const;\n-  inline bool is_objArray()    const;\n-  inline bool is_typeArray()   const;\n+  inline bool is_instance()         const;\n+  inline bool is_inline_type()      const;\n+  inline bool is_instanceRef()      const;\n+  inline bool is_stackChunk()       const;\n+  inline bool is_array()            const;\n+  inline bool is_objArray()         const;\n+  inline bool is_typeArray()        const;\n+  inline bool is_flatArray()        const;\n+  inline bool is_null_free_array()  const;\n@@ -120,6 +133,8 @@\n-  bool is_instance_noinline()    const;\n-  bool is_instanceRef_noinline() const;\n-  bool is_stackChunk_noinline()  const;\n-  bool is_array_noinline()       const;\n-  bool is_objArray_noinline()    const;\n-  bool is_typeArray_noinline()   const;\n+  bool is_instance_noinline()         const;\n+  bool is_instanceRef_noinline()      const;\n+  bool is_stackChunk_noinline()       const;\n+  bool is_array_noinline()            const;\n+  bool is_objArray_noinline()         const;\n+  bool is_typeArray_noinline()        const;\n+  bool is_flatArray_noinline()        const;\n+  bool is_null_free_array_noinline()  const;\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":27,"deletions":12,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -82,1 +82,1 @@\n-  set_mark(markWord::prototype());\n+  set_mark(Klass::default_prototype_header(klass()));\n@@ -208,0 +208,15 @@\n+bool oopDesc::is_inline_type() const { return mark().is_inline_type(); }\n+#ifdef _LP64\n+bool oopDesc::is_flatArray() const {\n+  markWord mrk = mark();\n+  return (mrk.is_unlocked()) ? mrk.is_flat_array() : klass()->is_flatArray_klass();\n+}\n+bool oopDesc::is_null_free_array() const {\n+  markWord mrk = mark();\n+  return (mrk.is_unlocked()) ? mrk.is_null_free_array() : klass()->is_null_free_array_klass();\n+}\n+#else\n+bool oopDesc::is_flatArray()       const { return klass()->is_flatArray_klass(); }\n+bool oopDesc::is_null_free_array() const { return klass()->is_null_free_array_klass(); }\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+typedef class     flatArrayOopDesc*           flatArrayOop;\n@@ -152,0 +153,1 @@\n+DEF_OOP(flatArray);\n@@ -180,0 +182,1 @@\n+class     InlineKlass;\n@@ -187,0 +190,1 @@\n+class     FlatArrayKlass;\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -118,0 +118,75 @@\n+bool Symbol::is_Q_signature() const {\n+  int len = utf8_length();\n+  return len > 2 && char_at(0) == JVM_SIGNATURE_PRIMITIVE_OBJECT && char_at(len - 1) == JVM_SIGNATURE_ENDCLASS;\n+}\n+\n+bool Symbol::is_Q_array_signature() const {\n+  int l = utf8_length();\n+  if (l < 2 || char_at(0) != JVM_SIGNATURE_ARRAY || char_at(l - 1) != JVM_SIGNATURE_ENDCLASS) {\n+    return false;\n+  }\n+  for (int i = 1; i < (l - 2); i++) {\n+    char c = char_at(i);\n+    if (c == JVM_SIGNATURE_PRIMITIVE_OBJECT) {\n+      return true;\n+    }\n+    if (c != JVM_SIGNATURE_ARRAY) {\n+      return false;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool Symbol::is_Q_method_signature() const {\n+  assert(SignatureVerifier::is_valid_method_signature(this), \"must be\");\n+  int len = utf8_length();\n+  if (len > 4 && char_at(0) == JVM_SIGNATURE_FUNC) {\n+    for (int i=1; i<len-3; i++) { \/\/ Must end with \")Qx;\", where x is at least one character or more.\n+      if (char_at(i) == JVM_SIGNATURE_ENDFUNC && char_at(i+1) == JVM_SIGNATURE_PRIMITIVE_OBJECT) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+Symbol* Symbol::fundamental_name(TRAPS) {\n+  if ((char_at(0) == JVM_SIGNATURE_PRIMITIVE_OBJECT || char_at(0) == JVM_SIGNATURE_CLASS) && ends_with(JVM_SIGNATURE_ENDCLASS)) {\n+    return SymbolTable::new_symbol(this, 1, utf8_length() - 1);\n+  } else {\n+    \/\/ reference count is incremented to be consistent with the behavior with\n+    \/\/ the SymbolTable::new_symbol() call above\n+    this->increment_refcount();\n+    return this;\n+  }\n+}\n+\n+bool Symbol::is_same_fundamental_type(Symbol* s) const {\n+  if (this == s) return true;\n+  if (utf8_length() < 3) return false;\n+  int offset1, offset2, len;\n+  if (ends_with(JVM_SIGNATURE_ENDCLASS)) {\n+    if (char_at(0) != JVM_SIGNATURE_PRIMITIVE_OBJECT && char_at(0) != JVM_SIGNATURE_CLASS) return false;\n+    offset1 = 1;\n+    len = utf8_length() - 2;\n+  } else {\n+    offset1 = 0;\n+    len = utf8_length();\n+  }\n+  if (ends_with(JVM_SIGNATURE_ENDCLASS)) {\n+    if (s->char_at(0) != JVM_SIGNATURE_PRIMITIVE_OBJECT && s->char_at(0) != JVM_SIGNATURE_CLASS) return false;\n+    offset2 = 1;\n+  } else {\n+    offset2 = 0;\n+  }\n+  if ((offset2 + len) > s->utf8_length()) return false;\n+  if ((utf8_length() - offset1 * 2) != (s->utf8_length() - offset2 * 2))\n+    return false;\n+  int l = len;\n+  while (l-- > 0) {\n+    if (char_at(offset1 + l) != s->char_at(offset2 + l))\n+      return false;\n+  }\n+  return true;\n+}\n+\n@@ -445,0 +520,8 @@\n+void Symbol::print_Qvalue_on(outputStream* st) const {\n+  st->print(\"'Q\");\n+  for (int i = 0; i < utf8_length(); i++) {\n+    st->print(\"%c\", char_at(i));\n+  }\n+  st->print(\";'\");\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/symbol.cpp","additions":83,"deletions":0,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -214,1 +214,1 @@\n-  bool starts_with(int prefix_char) const {\n+  bool starts_with(char prefix_char) const {\n@@ -244,0 +244,12 @@\n+  \/\/ True if this is a descriptor for a method with void return.\n+  \/\/ (Assumes it is a valid descriptor.)\n+  bool is_void_method_signature() const {\n+    return starts_with('(') && ends_with('V');\n+  }\n+\n+  bool is_Q_signature() const;\n+  bool is_Q_array_signature() const;\n+  bool is_Q_method_signature() const;\n+  Symbol* fundamental_name(TRAPS);\n+  bool is_same_fundamental_type(Symbol*) const;\n+\n@@ -288,0 +300,1 @@\n+  void print_Qvalue_on(outputStream* st) const;  \/\/ Second level print for Q-types.\n","filename":"src\/hotspot\/share\/oops\/symbol.hpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -98,1 +98,0 @@\n-  \/\/ For typeArrays this is only called for the last dimension\n@@ -191,1 +190,1 @@\n-              class_loader_data(), dim + 1, this, CHECK_NULL);\n+              class_loader_data(), dim + 1, this, false, false, CHECK_NULL);\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -89,1 +89,1 @@\n-  if (callee_method->is_initializer()) {\n+  if (callee_method->is_object_constructor()) {\n@@ -92,1 +92,1 @@\n-  if (caller_method->is_initializer() &&\n+  if (caller_method->is_object_constructor_or_class_initializer() &&\n","filename":"src\/hotspot\/share\/opto\/bytecodeInfo.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -760,0 +760,6 @@\n+  product(bool, UseArrayLoadStoreProfile, true,                             \\\n+          \"Take advantage of profiling at array load\/store\")                \\\n+                                                                            \\\n+  product(bool, UseACmpProfile, true,                                       \\\n+          \"Take advantage of profiling at acmp\")                            \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -599,0 +599,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -608,0 +610,1 @@\n+  case vmIntrinsics::_getValue:\n@@ -617,0 +620,1 @@\n+  case vmIntrinsics::_putValue:\n@@ -701,0 +705,4 @@\n+  case vmIntrinsics::_asPrimaryType:\n+  case vmIntrinsics::_asPrimaryTypeArg:\n+  case vmIntrinsics::_asValueType:\n+  case vmIntrinsics::_asValueTypeArg:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -46,0 +48,1 @@\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -81,1 +84,1 @@\n-Node *StartNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *StartNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -105,11 +108,0 @@\n-\/\/------------------------------StartOSRNode----------------------------------\n-\/\/ The method start node for an on stack replacement adapter\n-\n-\/\/------------------------------osr_domain-----------------------------\n-const TypeTuple *StartOSRNode::osr_domain() {\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n-\n-  return TypeTuple::make(TypeFunc::Parms+1, fields);\n-}\n-\n@@ -501,0 +493,8 @@\n+      } else if (cik->is_flat_array_klass()) {\n+        ciKlass* cie = cik->as_flat_array_klass()->base_element_klass();\n+        cie->print_name_on(st);\n+        st->print(\"[%d]\", spobj->n_fields());\n+        int ndim = cik->as_array_klass()->dimension() - 1;\n+        while (ndim-- > 0) {\n+          st->print(\"[]\");\n+        }\n@@ -506,0 +506,7 @@\n+        if (iklass != NULL && iklass->is_inlinetype()) {\n+          Node* init_node = mcall->in(first_ind++);\n+          if (!init_node->is_top()) {\n+            st->print(\" [is_init\");\n+            format_helper(regalloc, st, init_node, \":\", -1, NULL);\n+          }\n+        }\n@@ -719,1 +726,1 @@\n-const Type *CallNode::bottom_type() const { return tf()->range(); }\n+const Type *CallNode::bottom_type() const { return tf()->range_cc(); }\n@@ -721,2 +728,4 @@\n-  if (phase->type(in(0)) == Type::TOP)  return Type::TOP;\n-  return tf()->range();\n+  if (!in(0) || phase->type(in(0)) == Type::TOP) {\n+    return Type::TOP;\n+  }\n+  return tf()->range_cc();\n@@ -727,0 +736,7 @@\n+  if (_entry_point == StubRoutines::store_inline_type_fields_to_buf()) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -735,27 +751,26 @@\n-Node *CallNode::match( const ProjNode *proj, const Matcher *match ) {\n-  switch (proj->_con) {\n-  case TypeFunc::Control:\n-  case TypeFunc::I_O:\n-  case TypeFunc::Memory:\n-    return new MachProjNode(this,proj->_con,RegMask::Empty,MachProjNode::unmatched_proj);\n-\n-  case TypeFunc::Parms+1:       \/\/ For LONG & DOUBLE returns\n-    assert(tf()->range()->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n-    \/\/ 2nd half of doubles and longs\n-    return new MachProjNode(this,proj->_con, RegMask::Empty, (uint)OptoReg::Bad);\n-\n-  case TypeFunc::Parms: {       \/\/ Normal returns\n-    uint ideal_reg = tf()->range()->field_at(TypeFunc::Parms)->ideal_reg();\n-    OptoRegPair regs = Opcode() == Op_CallLeafVector\n-      ? match->vector_return_value(ideal_reg)      \/\/ Calls into assembly vector routine\n-      : is_CallRuntime()\n-        ? match->c_return_value(ideal_reg)  \/\/ Calls into C runtime\n-        : match->  return_value(ideal_reg); \/\/ Calls into compiled Java code\n-    RegMask rm = RegMask(regs.first());\n-\n-    if (Opcode() == Op_CallLeafVector) {\n-      \/\/ If the return is in vector, compute appropriate regmask taking into account the whole range\n-      if(ideal_reg >= Op_VecS && ideal_reg <= Op_VecZ) {\n-        if(OptoReg::is_valid(regs.second())) {\n-          for (OptoReg::Name r = regs.first(); r <= regs.second(); r = OptoReg::add(r, 1)) {\n-            rm.Insert(r);\n+Node *CallNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n+  uint con = proj->_con;\n+  const TypeTuple* range_cc = tf()->range_cc();\n+  if (con >= TypeFunc::Parms) {\n+    if (tf()->returns_inline_type_as_fields()) {\n+      \/\/ The call returns multiple values (inline type fields): we\n+      \/\/ create one projection per returned value.\n+      assert(con <= TypeFunc::Parms+1 || InlineTypeReturnedAsFields, \"only for multi value return\");\n+      uint ideal_reg = range_cc->field_at(con)->ideal_reg();\n+      return new MachProjNode(this, con, mask[con-TypeFunc::Parms], ideal_reg);\n+    } else {\n+      if (con == TypeFunc::Parms) {\n+        uint ideal_reg = range_cc->field_at(TypeFunc::Parms)->ideal_reg();\n+        OptoRegPair regs = Opcode() == Op_CallLeafVector\n+          ? match->vector_return_value(ideal_reg)      \/\/ Calls into assembly vector routine\n+          : match->c_return_value(ideal_reg);\n+        RegMask rm = RegMask(regs.first());\n+\n+        if (Opcode() == Op_CallLeafVector) {\n+          \/\/ If the return is in vector, compute appropriate regmask taking into account the whole range\n+          if(ideal_reg >= Op_VecS && ideal_reg <= Op_VecZ) {\n+            if(OptoReg::is_valid(regs.second())) {\n+              for (OptoReg::Name r = regs.first(); r <= regs.second(); r = OptoReg::add(r, 1)) {\n+                rm.Insert(r);\n+              }\n+            }\n@@ -764,0 +779,9 @@\n+\n+        if (OptoReg::is_valid(regs.second())) {\n+          rm.Insert(regs.second());\n+        }\n+        return new MachProjNode(this,con,rm,ideal_reg);\n+      } else {\n+        assert(con == TypeFunc::Parms+1, \"only one return value\");\n+        assert(range_cc->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n+        return new MachProjNode(this,con, RegMask::Empty, (uint)OptoReg::Bad);\n@@ -766,4 +790,0 @@\n-\n-    if( OptoReg::is_valid(regs.second()) )\n-      rm.Insert( regs.second() );\n-    return new MachProjNode(this,proj->_con,rm,ideal_reg);\n@@ -772,0 +792,6 @@\n+  switch (con) {\n+  case TypeFunc::Control:\n+  case TypeFunc::I_O:\n+  case TypeFunc::Memory:\n+    return new MachProjNode(this,proj->_con,RegMask::Empty,MachProjNode::unmatched_proj);\n+\n@@ -792,1 +818,1 @@\n-    const TypeTuple* args = _tf->domain();\n+    const TypeTuple* args = _tf->domain_sig();\n@@ -841,1 +867,1 @@\n-      const TypeTuple* d = tf()->domain();\n+      const TypeTuple* d = tf()->domain_cc();\n@@ -856,2 +882,2 @@\n-bool CallNode::has_non_debug_use(Node *n) {\n-  const TypeTuple * d = tf()->domain();\n+bool CallNode::has_non_debug_use(Node* n) {\n+  const TypeTuple* d = tf()->domain_cc();\n@@ -859,2 +885,1 @@\n-    Node *arg = in(i);\n-    if (arg == n) {\n+    if (in(i) == n) {\n@@ -867,0 +892,11 @@\n+bool CallNode::has_debug_use(Node* n) {\n+  if (jvms() != NULL) {\n+    for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+      if (in(i) == n) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -898,10 +934,15 @@\n-void CallNode::extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts) {\n-  projs->fallthrough_proj      = NULL;\n-  projs->fallthrough_catchproj = NULL;\n-  projs->fallthrough_ioproj    = NULL;\n-  projs->catchall_ioproj       = NULL;\n-  projs->catchall_catchproj    = NULL;\n-  projs->fallthrough_memproj   = NULL;\n-  projs->catchall_memproj      = NULL;\n-  projs->resproj               = NULL;\n-  projs->exobj                 = NULL;\n+CallProjections* CallNode::extract_projections(bool separate_io_proj, bool do_asserts) {\n+  uint max_res = TypeFunc::Parms-1;\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    ProjNode *pn = fast_out(i)->as_Proj();\n+    max_res = MAX2(max_res, pn->_con);\n+  }\n+\n+  assert(max_res < _tf->range_cc()->cnt(), \"result out of bounds\");\n+\n+  uint projs_size = sizeof(CallProjections);\n+  if (max_res > TypeFunc::Parms) {\n+    projs_size += (max_res-TypeFunc::Parms)*sizeof(Node*);\n+  }\n+  char* projs_storage = resource_allocate_bytes(projs_size);\n+  CallProjections* projs = new(projs_storage)CallProjections(max_res - TypeFunc::Parms + 1);\n@@ -953,1 +994,1 @@\n-      projs->resproj = pn;\n+      projs->resproj[0] = pn;\n@@ -956,1 +997,3 @@\n-      assert(false, \"unexpected projection from allocation node.\");\n+      assert(pn->_con <= max_res, \"unexpected projection from allocation node.\");\n+      projs->resproj[pn->_con-TypeFunc::Parms] = pn;\n+      break;\n@@ -963,1 +1006,1 @@\n-  assert(projs->fallthrough_proj      != NULL, \"must be found\");\n+  assert(!do_asserts || projs->fallthrough_proj      != NULL, \"must be found\");\n@@ -973,0 +1016,1 @@\n+  return projs;\n@@ -1004,2 +1048,2 @@\n-  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain()->cnt() : (uint)TypeFunc::Parms+1;\n-  uint new_dbg_start = tf()->domain()->cnt();\n+  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain_sig()->cnt() : (uint)TypeFunc::Parms+1;\n+  uint new_dbg_start = tf()->domain_sig()->cnt();\n@@ -1046,0 +1090,4 @@\n+  Bytecodes::Code bc = jvms()->method()->java_code_at_bci(jvms()->bci());\n+  if (EnableValhalla && (bc == Bytecodes::_if_acmpeq || bc == Bytecodes::_if_acmpne)) {\n+    return true;\n+  }\n@@ -1079,0 +1127,10 @@\n+  if (can_reshape && uncommon_trap_request() != 0) {\n+    if (remove_useless_allocation(phase, in(0), in(TypeFunc::Memory), in(TypeFunc::Parms))) {\n+      if (!in(0)->is_Region()) {\n+        PhaseIterGVN* igvn = phase->is_IterGVN();\n+        igvn->replace_input_of(this, 0, phase->C->top());\n+      }\n+      return this;\n+    }\n+  }\n+\n@@ -1128,0 +1186,124 @@\n+bool CallStaticJavaNode::remove_useless_allocation(PhaseGVN *phase, Node* ctl, Node* mem, Node* unc_arg) {\n+  \/\/ Split if can cause the flattened array branch of an array load to\n+  \/\/ end in an uncommon trap. In that case, the allocation of the\n+  \/\/ loaded value and its initialization is useless. Eliminate it. use\n+  \/\/ the jvm state of the allocation to create a new uncommon trap\n+  \/\/ call at the load.\n+  if (ctl == NULL || ctl->is_top() || mem == NULL || mem->is_top() || !mem->is_MergeMem()) {\n+    return false;\n+  }\n+  PhaseIterGVN* igvn = phase->is_IterGVN();\n+  if (ctl->is_Region()) {\n+    bool res = false;\n+    for (uint i = 1; i < ctl->req(); i++) {\n+      MergeMemNode* mm = mem->clone()->as_MergeMem();\n+      for (MergeMemStream mms(mm); mms.next_non_empty(); ) {\n+        Node* m = mms.memory();\n+        if (m->is_Phi() && m->in(0) == ctl) {\n+          mms.set_memory(m->in(i));\n+        }\n+      }\n+      if (remove_useless_allocation(phase, ctl->in(i), mm, unc_arg)) {\n+        res = true;\n+        if (!ctl->in(i)->is_Region()) {\n+          igvn->replace_input_of(ctl, i, phase->C->top());\n+        }\n+      }\n+      igvn->remove_dead_node(mm);\n+    }\n+    return res;\n+  }\n+  \/\/ verify the control flow is ok\n+  Node* call = ctl;\n+  MemBarNode* membar = NULL;\n+  for (;;) {\n+    if (call == NULL || call->is_top()) {\n+      return false;\n+    }\n+    if (call->is_Proj() || call->is_Catch() || call->is_MemBar()) {\n+      call = call->in(0);\n+    } else if (call->Opcode() == Op_CallStaticJava &&\n+               call->as_Call()->entry_point() == OptoRuntime::load_unknown_inline_Java()) {\n+      assert(call->in(0)->is_Proj() && call->in(0)->in(0)->is_MemBar(), \"missing membar\");\n+      membar = call->in(0)->in(0)->as_MemBar();\n+      break;\n+    } else {\n+      return false;\n+    }\n+  }\n+\n+  JVMState* jvms = call->jvms();\n+  if (phase->C->too_many_traps(jvms->method(), jvms->bci(), Deoptimization::trap_request_reason(uncommon_trap_request()))) {\n+    return false;\n+  }\n+\n+  Node* alloc_mem = call->in(TypeFunc::Memory);\n+  if (alloc_mem == NULL || alloc_mem->is_top()) {\n+    return false;\n+  }\n+  if (!alloc_mem->is_MergeMem()) {\n+    alloc_mem = MergeMemNode::make(alloc_mem);\n+    igvn->register_new_node_with_optimizer(alloc_mem);\n+  }\n+\n+  \/\/ and that there's no unexpected side effect\n+  for (MergeMemStream mms2(mem->as_MergeMem(), alloc_mem->as_MergeMem()); mms2.next_non_empty2(); ) {\n+    Node* m1 = mms2.is_empty() ? mms2.base_memory() : mms2.memory();\n+    Node* m2 = mms2.memory2();\n+\n+    for (uint i = 0; i < 100; i++) {\n+      if (m1 == m2) {\n+        break;\n+      } else if (m1->is_Proj()) {\n+        m1 = m1->in(0);\n+      } else if (m1->is_MemBar()) {\n+        m1 = m1->in(TypeFunc::Memory);\n+      } else if (m1->Opcode() == Op_CallStaticJava &&\n+                 m1->as_Call()->entry_point() == OptoRuntime::load_unknown_inline_Java()) {\n+        if (m1 != call) {\n+          return false;\n+        }\n+        break;\n+      } else if (m1->is_MergeMem()) {\n+        MergeMemNode* mm = m1->as_MergeMem();\n+        int idx = mms2.alias_idx();\n+        if (idx == Compile::AliasIdxBot) {\n+          m1 = mm->base_memory();\n+        } else {\n+          m1 = mm->memory_at(idx);\n+        }\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+  if (alloc_mem->outcnt() == 0) {\n+    igvn->remove_dead_node(alloc_mem);\n+  }\n+\n+  \/\/ Remove membar preceding the call\n+  membar->remove(igvn);\n+\n+  address call_addr = SharedRuntime::uncommon_trap_blob()->entry_point();\n+  CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\", NULL);\n+  unc->init_req(TypeFunc::Control, call->in(0));\n+  unc->init_req(TypeFunc::I_O, call->in(TypeFunc::I_O));\n+  unc->init_req(TypeFunc::Memory, call->in(TypeFunc::Memory));\n+  unc->init_req(TypeFunc::FramePtr,  call->in(TypeFunc::FramePtr));\n+  unc->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  unc->init_req(TypeFunc::Parms+0, unc_arg);\n+  unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+  unc->copy_call_debug_info(igvn, call->as_CallStaticJava());\n+\n+  igvn->replace_input_of(call, 0, phase->C->top());\n+\n+  igvn->register_new_node_with_optimizer(unc);\n+\n+  Node* ctrl = phase->transform(new ProjNode(unc, TypeFunc::Control));\n+  Node* halt = phase->transform(new HaltNode(ctrl, call->in(TypeFunc::FramePtr), \"uncommon trap returned which should never happen\"));\n+  igvn->add_input_to(phase->C->root(), halt);\n+\n+  return true;\n+}\n+\n+\n@@ -1232,0 +1414,7 @@\n+  if (_entry_point == NULL) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -1237,1 +1426,1 @@\n-  assert(tf()->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte == _num_bits,\n+  assert(tf()->range_sig()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte == _num_bits,\n@@ -1239,1 +1428,1 @@\n-  const TypeTuple* d = tf()->domain();\n+  const TypeTuple* d = tf()->domain_sig();\n@@ -1263,0 +1452,6 @@\n+uint CallLeafNoFPNode::match_edge(uint idx) const {\n+  \/\/ Null entry point is a special case for which the target is in a\n+  \/\/ register. Need to match that edge.\n+  return entry_point() == NULL && idx == TypeFunc::Parms;\n+}\n+\n@@ -1313,1 +1508,14 @@\n-  return remove_dead_region(phase, can_reshape) ? this : NULL;\n+  if (remove_dead_region(phase, can_reshape)) {\n+    return this;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  if (phase->C->scalarize_in_safepoints() && can_reshape && jvms() != NULL) {\n+    for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+      Node* n = in(i)->uncast();\n+      if (n->is_InlineType()) {\n+        n->as_InlineType()->make_scalar_in_safepoints(phase->is_IterGVN());\n+      }\n+    }\n+  }\n+  return NULL;\n@@ -1477,1 +1685,1 @@\n-  if (!alloc->is_Allocate()\n+  if (alloc != NULL && !alloc->is_Allocate()\n@@ -1535,1 +1743,3 @@\n-                           Node *size, Node *klass_node, Node *initial_test)\n+                           Node *size, Node *klass_node,\n+                           Node* initial_test,\n+                           InlineTypeNode* inline_type_node)\n@@ -1543,0 +1753,1 @@\n+  _larval = false;\n@@ -1555,0 +1766,3 @@\n+  init_req( InlineType     , inline_type_node);\n+  \/\/ DefaultValue defaults to NULL\n+  \/\/ RawDefaultValue defaults to NULL\n@@ -1561,3 +1775,2 @@\n-         initializer->is_initializer() &&\n-         !initializer->is_static(),\n-             \"unexpected initializer method\");\n+         initializer->is_object_constructor_or_class_initializer(),\n+         \"unexpected initializer method\");\n@@ -1574,1 +1787,2 @@\n-Node *AllocateNode::make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem) {\n+\n+Node* AllocateNode::make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem) {\n@@ -1576,3 +1790,10 @@\n-  \/\/ For now only enable fast locking for non-array types\n-  mark_node = phase->MakeConX(markWord::prototype().value());\n-  return mark_node;\n+  if (EnableValhalla) {\n+    Node* klass_node = in(AllocateNode::KlassNode);\n+    Node* proto_adr = phase->transform(new AddPNode(klass_node, klass_node, phase->MakeConX(in_bytes(Klass::prototype_header_offset()))));\n+    mark_node = LoadNode::make(*phase, control, mem, proto_adr, TypeRawPtr::BOTTOM, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  } else {\n+    mark_node = phase->MakeConX(markWord::prototype().value());\n+  }\n+  mark_node = phase->transform(mark_node);\n+  \/\/ Avoid returning a constant (old node) here because this method is used by LoadNode::Ideal\n+  return new OrXNode(mark_node, phase->MakeConX(_larval ? markWord::larval_bit_in_place : 0));\n@@ -1957,1 +2178,2 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() && !obj_type->is_inlinetypeptr()) {\n@@ -2153,1 +2375,2 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() && !obj_type->is_inlinetypeptr()) {\n@@ -2233,1 +2456,2 @@\n-    dest_t = dest_t->add_offset(Type::OffsetBot)->is_oopptr();\n+    dest_t = dest_t->is_aryptr()->with_field_offset(Type::OffsetBot)->add_offset(Type::OffsetBot)->is_oopptr();\n+    t_oop = t_oop->is_aryptr()->with_field_offset(Type::OffsetBot);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":306,"deletions":82,"binary":false,"changes":388,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"opto\/graphKit.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -32,0 +34,1 @@\n+#include \"opto\/rootnode.hpp\"\n@@ -100,1 +103,15 @@\n-  return (in(0) && remove_dead_region(phase, can_reshape)) ? this : NULL;\n+  if (in(0) && remove_dead_region(phase, can_reshape)) {\n+    return this;\n+  }\n+\n+  \/\/ Push cast through InlineTypeNode\n+  InlineTypeNode* vt = in(1)->isa_InlineType();\n+  if (vt != NULL && phase->type(vt)->filter_speculative(_type) != Type::TOP) {\n+    Node* cast = clone();\n+    cast->set_req(1, vt->get_oop());\n+    vt = vt->clone()->as_InlineType();\n+    vt->set_oop(phase->transform(cast));\n+    return vt;\n+  }\n+\n+  return NULL;\n@@ -406,0 +423,10 @@\n+\/\/=============================================================================\n+\/\/------------------------------Identity---------------------------------------\n+\/\/ If input is already higher or equal to cast type, then this is an identity.\n+Node* CheckCastPPNode::Identity(PhaseGVN* phase) {\n+  if (in(1)->is_InlineType() && _type->isa_instptr() && phase->type(in(1))->inline_klass()->is_subtype_of(_type->is_instptr()->instance_klass())) {\n+    return in(1);\n+  }\n+  return ConstraintCastNode::Identity(phase);\n+}\n+\n@@ -422,0 +449,10 @@\n+    \/\/ TODO 8302672\n+    if (!StressReflectiveCode && my_type->isa_aryptr() && in_type->isa_aryptr()) {\n+      \/\/ Propagate array properties (not flat\/null-free)\n+      \/\/ Don't do this when StressReflectiveCode is enabled because it might lead to\n+      \/\/ a dying data path while the corresponding flat\/null-free check is not folded.\n+      my_type = my_type->is_aryptr()->update_properties(in_type->is_aryptr());\n+      if (my_type == NULL) {\n+        return Type::TOP; \/\/ Inconsistent properties\n+      }\n+    }\n@@ -426,1 +463,1 @@\n-      result =  my_type->cast_to_ptr_type(my_type->join_ptr(in_ptr));\n+      result = my_type->cast_to_ptr_type(my_type->join_ptr(in_ptr));\n@@ -513,0 +550,16 @@\n+\n+  if (t->is_zero_type() || !t->maybe_null()) {\n+    for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+      Node* u = fast_out(i);\n+      if (u->Opcode() == Op_OrL) {\n+        for (DUIterator_Fast jmax, j = u->fast_outs(jmax); j < jmax; j++) {\n+          Node* cmp = u->fast_out(j);\n+          if (cmp->Opcode() == Op_CmpL) {\n+            \/\/ Give CmpL a chance to get optimized\n+            phase->record_for_igvn(cmp);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/castnode.cpp","additions":55,"deletions":2,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -451,1 +452,1 @@\n-bool RegionNode::try_clean_mem_phi(PhaseGVN *phase) {\n+Node* PhiNode::try_clean_mem_phi(PhaseGVN *phase) {\n@@ -471,2 +472,1 @@\n-  PhiNode* phi = has_unique_phi();\n-  if (phi && phi->type() == Type::MEMORY && req() == 3 && phi->is_diamond_phi(true)) {\n+  if (type() == Type::MEMORY && is_diamond_phi(true)) {\n@@ -474,1 +474,2 @@\n-    assert(phi->req() == 3, \"same as region\");\n+    assert(req() == 3, \"same as region\");\n+    Node* r = in(0);\n@@ -476,2 +477,2 @@\n-      Node *mem = phi->in(i);\n-      if (mem && mem->is_MergeMem() && in(i)->outcnt() == 1) {\n+      Node *mem = in(i);\n+      if (mem && mem->is_MergeMem() && r->in(i)->outcnt() == 1) {\n@@ -481,1 +482,1 @@\n-        Node* other = phi->in(j);\n+        Node* other = in(j);\n@@ -485,2 +486,1 @@\n-          phase->is_IterGVN()->replace_node(phi, m);\n-          return true;\n+          return m;\n@@ -491,1 +491,1 @@\n-  return false;\n+  return NULL;\n@@ -506,2 +506,9 @@\n-    if (has_phis && try_clean_mem_phi(phase)) {\n-      has_phis = false;\n+    if (has_phis) {\n+      PhiNode* phi = has_unique_phi();\n+      if (phi != NULL) {\n+        Node* m = phi->try_clean_mem_phi(phase);\n+        if (m != NULL) {\n+          phase->is_IterGVN()->replace_node(phi, m);\n+          has_phis = false;\n+        }\n+      }\n@@ -945,1 +952,2 @@\n-             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck()) {\n+             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck() ||\n+             cmp1->is_FlatArrayCheck() || cmp2->is_FlatArrayCheck()) {\n@@ -1042,1 +1050,1 @@\n-  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at), \"flatten at\");\n+  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at) || (flatten_phi_adr_type(at) == TypeAryPtr::INLINES && Compile::current()->flattened_accesses_share_alias()), \"flatten at\");\n@@ -1171,0 +1179,8 @@\n+  \/\/ Flat array element shouldn't get their own memory slice until flattened_accesses_share_alias is cleared.\n+  \/\/ It could be the graph has no loads\/stores and flattened_accesses_share_alias is never cleared. EA could still\n+  \/\/ creates per element Phis but that wouldn't be a problem as there are no memory accesses for that array.\n+  assert(_adr_type == NULL || _adr_type->isa_aryptr() == NULL ||\n+         _adr_type->is_aryptr()->is_known_instance() ||\n+         !_adr_type->is_aryptr()->is_flat() ||\n+         !Compile::current()->flattened_accesses_share_alias() ||\n+         _adr_type == TypeAryPtr::INLINES, \"flat array element shouldn't get its own slice yet\");\n@@ -1435,0 +1451,8 @@\n+  if (phase->is_IterGVN()) {\n+    Node* m = try_clean_mem_phi(phase);\n+    if (m != NULL) {\n+      return m;\n+    }\n+  }\n+\n+\n@@ -1969,0 +1993,44 @@\n+\/\/ Push inline type input nodes (and null) down through the phi recursively (can handle data loops).\n+InlineTypeNode* PhiNode::push_inline_types_through(PhaseGVN* phase, bool can_reshape, ciInlineKlass* vk, bool is_init) {\n+  InlineTypeNode* vt = InlineTypeNode::make_null(*phase, vk)->clone_with_phis(phase, in(0), is_init);\n+  if (can_reshape) {\n+    \/\/ Replace phi right away to be able to use the inline\n+    \/\/ type node when reaching the phi again through data loops.\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+    for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+      Node* u = fast_out(i);\n+      igvn->rehash_node_delayed(u);\n+      imax -= u->replace_edge(this, vt);\n+      --i;\n+    }\n+    igvn->rehash_node_delayed(this);\n+    assert(outcnt() == 0, \"should be dead now\");\n+  }\n+  ResourceMark rm;\n+  Node_List casts;\n+  for (uint i = 1; i < req(); ++i) {\n+    Node* n = in(i);\n+    while (n->is_ConstraintCast()) {\n+      casts.push(n);\n+      n = n->in(1);\n+    }\n+    if (phase->type(n)->is_zero_type()) {\n+      n = InlineTypeNode::make_null(*phase, vk);\n+    } else if (n->is_Phi()) {\n+      assert(can_reshape, \"can only handle phis during IGVN\");\n+      n = phase->transform(n->as_Phi()->push_inline_types_through(phase, can_reshape, vk, is_init));\n+    }\n+    while (casts.size() != 0) {\n+      \/\/ Push the cast(s) through the InlineTypeNode\n+      Node* cast = casts.pop()->clone();\n+      cast->set_req_X(1, n->as_InlineType()->get_oop(), phase);\n+      n = n->clone();\n+      n->as_InlineType()->set_oop(phase->transform(cast));\n+      n = phase->transform(n);\n+    }\n+    bool transform = !can_reshape && (i == (req()-1)); \/\/ Transform phis on last merge\n+    vt->merge_with(phase, n->as_InlineType(), i, transform);\n+  }\n+  return vt;\n+}\n+\n@@ -2300,0 +2368,2 @@\n+    \/\/ TODO revisit this with JDK-8247216\n+    bool mergemem_only = true;\n@@ -2312,0 +2382,2 @@\n+      } else {\n+        mergemem_only = false;\n@@ -2316,1 +2388,1 @@\n-    if (!saw_self && adr_type() == TypePtr::BOTTOM)  merge_width = 0;\n+    if (!mergemem_only && !saw_self && adr_type() == TypePtr::BOTTOM)  merge_width = 0;\n@@ -2387,0 +2459,5 @@\n+            if (igvn) {\n+              \/\/ TODO revisit this with JDK-8247216\n+              \/\/ Put 'n' on the worklist because it might be modified by MergeMemStream::iteration_setup\n+              igvn->_worklist.push(n);\n+            }\n@@ -2505,0 +2582,75 @@\n+  \/\/ Check recursively if inputs are either an inline type, constant null\n+  \/\/ or another Phi (including self references through data loops). If so,\n+  \/\/ push the inline types down through the phis to enable folding of loads.\n+  if (EnableValhalla && _type->isa_ptr() && req() > 2) {\n+    ResourceMark rm;\n+    Unique_Node_List worklist;\n+    worklist.push(this);\n+    bool can_optimize = true;\n+    ciInlineKlass* vk = NULL;\n+    \/\/ true if all IsInit inputs of all InlineType* nodes are true\n+    bool is_init = true;\n+    Node_List casts;\n+\n+    \/\/ TODO 8302217 We need to prevent endless pushing through\n+    bool only_phi = (outcnt() != 0);\n+    for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+      Node* n = fast_out(i);\n+      if (n->is_InlineType() && n->in(1) == this) {\n+        can_optimize = false;\n+        break;\n+      }\n+      if (!n->is_Phi()) {\n+        only_phi = false;\n+      }\n+    }\n+    if (only_phi) {\n+      can_optimize = false;\n+    }\n+    for (uint next = 0; next < worklist.size() && can_optimize; next++) {\n+      Node* phi = worklist.at(next);\n+      for (uint i = 1; i < phi->req() && can_optimize; i++) {\n+        Node* n = phi->in(i);\n+        if (n == NULL) {\n+          can_optimize = false;\n+          break;\n+        }\n+        while (n->is_ConstraintCast()) {\n+          if (n->in(0) != NULL && n->in(0)->is_top()) {\n+            \/\/ Will die, don't optimize\n+            can_optimize = false;\n+            break;\n+          }\n+          casts.push(n);\n+          n = n->in(1);\n+        }\n+        const Type* t = phase->type(n);\n+        if (n->is_InlineType() && (vk == NULL || vk == t->inline_klass())) {\n+          vk = (vk == NULL) ? t->inline_klass() : vk;\n+          if (phase->find_int_con(n->as_InlineType()->get_is_init(), 0) != 1) {\n+            is_init = false;\n+          }\n+        } else if (n->is_Phi() && can_reshape && n->bottom_type()->isa_ptr()) {\n+          worklist.push(n);\n+        } else if (t->is_zero_type()) {\n+          is_init = false;\n+        } else {\n+          can_optimize = false;\n+        }\n+      }\n+    }\n+    \/\/ Check if cast nodes can be pushed through\n+    const Type* t = Type::get_const_type(vk);\n+    while (casts.size() != 0 && can_optimize && t != NULL) {\n+      Node* cast = casts.pop();\n+      if (t->filter(cast->bottom_type()) == Type::TOP) {\n+        can_optimize = false;\n+      }\n+    }\n+    if (can_optimize && vk != NULL) {\n+\/\/ TODO 8302217\n+\/\/      assert(!_type->isa_ptr() || _type->maybe_null() || is_init, \"Phi not null but a possible null was seen\");\n+      return push_inline_types_through(phase, can_reshape, vk, is_init);\n+    }\n+  }\n+\n@@ -2849,0 +3001,6 @@\n+\n+  \/\/ CheckCastPPNode::Ideal() for inline types reuses the exception\n+  \/\/ paths of a call to perform an allocation: we can see a Phi here.\n+  if (in(1)->is_Phi()) {\n+    return this;\n+  }\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":173,"deletions":15,"binary":false,"changes":188,"status":"modified"},{"patch":"@@ -131,1 +131,0 @@\n-  bool try_clean_mem_phi(PhaseGVN* phase);\n@@ -250,0 +249,3 @@\n+  Node* try_clean_mem_phi(PhaseGVN *phase);\n+\n+  InlineTypeNode* push_inline_types_through(PhaseGVN* phase, bool can_reshape, ciInlineKlass* vk, bool is_init);\n@@ -432,0 +434,2 @@\n+  bool is_flat_array_check(PhaseTransform* phase, Node** array = NULL);\n+\n@@ -647,0 +651,1 @@\n+    init_class_id(Class_Blackhole);\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1784,1 +1784,1 @@\n-          derived->bottom_type()->make_ptr()->is_ptr()->_offset == 0, \"sanity\");\n+         derived->bottom_type()->make_ptr()->is_ptr()->offset() == 0, \"sanity\");\n@@ -1787,1 +1787,1 @@\n-  if( tj == NULL || tj->_offset == 0 ) {\n+  if (tj == NULL || tj->offset() == 0) {\n@@ -1953,1 +1953,1 @@\n-                  derived->bottom_type()->make_ptr()->is_ptr()->_offset == 0, \"sanity\");\n+                 derived->bottom_type()->make_ptr()->is_ptr()->offset() == 0, \"sanity\");\n@@ -1955,1 +1955,1 @@\n-          if( tj && tj->_offset != 0 && tj->isa_oop_ptr() ) {\n+          if (tj && tj->offset() != 0 && tj->isa_oop_ptr()) {\n@@ -2245,1 +2245,1 @@\n-  const TypeTuple *domain = C->tf()->domain();\n+  const TypeTuple *domain = C->tf()->domain_cc();\n@@ -2454,1 +2454,1 @@\n-                  if (is_derived && check->bottom_type()->is_ptr()->_offset != 0) {\n+                  if (is_derived && check->bottom_type()->is_ptr()->offset() != 0) {\n@@ -2458,1 +2458,1 @@\n-                    assert(check->bottom_type()->is_ptr()->_offset == 0, \"Bad base pointer\");\n+                    assert(check->bottom_type()->is_ptr()->offset() == 0, \"Bad base pointer\");\n@@ -2467,1 +2467,1 @@\n-                } else if (check->bottom_type()->is_ptr()->_offset == 0) {\n+                } else if (check->bottom_type()->is_ptr()->offset() == 0) {\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -185,0 +185,1 @@\n+macro(FlatArrayCheck)\n@@ -371,0 +372,1 @@\n+macro(InlineType)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -395,0 +396,3 @@\n+  if (dead->is_InlineType()) {\n+    remove_inline_type(dead);\n+  }\n@@ -435,0 +439,3 @@\n+    if (n->outcnt() == 0) {\n+      worklist->push(n);\n+    }\n@@ -442,0 +449,6 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n+#ifdef ASSERT\n+  if (_modified_nodes != NULL) {\n+    _modified_nodes->remove_useless_nodes(useful.member_set());\n+  }\n+#endif\n@@ -604,0 +617,1 @@\n+                  _has_circular_inline_type(false),\n@@ -623,0 +637,1 @@\n+                  _inline_type_nodes (comp_arena(), 8, 0, NULL),\n@@ -727,4 +742,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -737,1 +750,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -859,0 +872,10 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n+  \/\/ TODO 8284443 Only reserve extra slot if needed\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ One extra slot to hold the IsInit information for a nullable\n+    \/\/ inline type return if we run out of registers.\n+    next_slot += 2;\n+  }\n@@ -892,0 +915,1 @@\n+    _has_circular_inline_type(false),\n@@ -1017,0 +1041,4 @@\n+  _has_flattened_accesses = false;\n+  _flattened_accesses_share_alias = true;\n+  _scalarize_in_safepoints = false;\n+\n@@ -1305,1 +1333,2 @@\n-    assert(InlineUnsafeOps || StressReflectiveCode, \"indeterminate pointers come only from unsafe ops\");\n+    bool default_value_load = EnableValhalla && tj->is_instptr()->instance_klass() == ciEnv::current()->Class_klass();\n+    assert(InlineUnsafeOps || StressReflectiveCode || default_value_load, \"indeterminate pointers come only from unsafe ops\");\n@@ -1318,0 +1347,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1331,0 +1369,2 @@\n+    \/\/ For flattened inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1371,1 +1411,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());\n@@ -1375,1 +1415,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInstPtr::BOTTOM && _flattened_accesses_share_alias) {\n+      const TypeAry* tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size(), \/* stable= *\/ false, \/* flat= *\/ true);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1382,1 +1427,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1432,1 +1477,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, Type::Offset(offset));\n@@ -1447,1 +1492,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, offset, to->instance_id());\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, Type::Offset(offset), to->instance_id());\n@@ -1449,1 +1494,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, offset);\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, Type::Offset(offset));\n@@ -1465,1 +1510,1 @@\n-                                       offset);\n+                                       Type::Offset(offset));\n@@ -1471,1 +1516,1 @@\n-        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), offset);\n+        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), Type::Offset(offset));\n@@ -1473,1 +1518,1 @@\n-        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, offset);\n+        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, Type::Offset(offset), tk->is_not_flat(), tk->is_not_null_free(), tk->is_null_free());\n@@ -1476,1 +1521,0 @@\n-\n@@ -1606,1 +1650,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1611,3 +1655,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = NULL;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1663,0 +1710,1 @@\n+    ciField* field = NULL;\n@@ -1669,0 +1717,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1670,1 +1719,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (flat->is_flat() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->first_field_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1682,0 +1738,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1692,1 +1750,0 @@\n-      ciField* field;\n@@ -1699,0 +1756,4 @@\n+      } else if (tinst->is_inlinetypeptr()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1703,7 +1764,14 @@\n-      assert(field == NULL ||\n-             original_field == NULL ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset() == original_field->offset() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != NULL)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == NULL ||\n+           original_field == NULL ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset() == original_field->offset() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != NULL) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1714,3 +1782,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1718,6 +1787,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == NULL) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == NULL) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1840,0 +1910,418 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineType()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    } else if (n->is_CheckCastPP()) {\n+      wq.push(n->in(1));\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make sure that the return value does not keep an otherwise unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = NULL;\n+    for (uint i = 1; i < root()->req(); i++) {\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == NULL, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != NULL) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  if (_inline_type_nodes.length() == 0) {\n+    return;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  set_scalarize_in_safepoints(true);\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    _inline_type_nodes.at(i)->as_InlineType()->make_scalar_in_safepoints(&igvn);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes by replacing them with their oop input\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeNode* vt = _inline_type_nodes.pop()->as_InlineType();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+        continue;\n+      }\n+      for (DUIterator i = vt->outs(); vt->has_out(i); i++) {\n+        DEBUG_ONLY(bool must_be_buffered = false);\n+        Node* u = vt->out(i);\n+        \/\/ Check if any users are blackholes. If so, rewrite them to use either the\n+        \/\/ allocated buffer, or individual components, instead of the inline type node\n+        \/\/ that goes away.\n+        if (u->is_Blackhole()) {\n+          BlackholeNode* bh = u->as_Blackhole();\n+\n+          \/\/ Unlink the old input\n+          int idx = bh->find_edge(vt);\n+          assert(idx != -1, \"The edge should be there\");\n+          bh->del_req(idx);\n+          --i;\n+\n+          if (vt->is_allocated(&igvn)) {\n+            \/\/ Already has the allocated instance, blackhole that\n+            bh->add_req(vt->get_oop());\n+          } else {\n+            \/\/ Not allocated yet, blackhole the components\n+            for (uint c = 0; c < vt->field_count(); c++) {\n+              bh->add_req(vt->field_value(c));\n+            }\n+          }\n+\n+          \/\/ Node modified, record for IGVN\n+          igvn.record_for_igvn(bh);\n+        }\n+#ifdef ASSERT\n+        \/\/ Verify that inline type is buffered when replacing by oop\n+        else if (u->is_InlineType()) {\n+          InlineTypeNode* vt2 = u->as_InlineType();\n+          for (uint i = 0; i < vt2->field_count(); ++i) {\n+            if (vt2->field_value(i) == vt && !vt2->field_is_flattened(i)) {\n+              \/\/ Use in non-flat field\n+              must_be_buffered = true;\n+            }\n+          }\n+        } else if (u->is_Phi()) {\n+          \/\/ TODO 8302217 Remove this once InlineTypeNodes are reliably pushed through\n+        } else if (u->Opcode() != Op_Return || !tf()->returns_inline_type_as_fields()) {\n+          must_be_buffered = true;\n+        }\n+        if (must_be_buffered && !vt->is_allocated(&igvn)) {\n+          vt->dump(0);\n+          u->dump(0);\n+          assert(false, \"Should have been buffered\");\n+        }\n+#endif\n+      }\n+      igvn.replace_node(vt, vt->get_oop());\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flattened_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flattened_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flattened array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flattened array) correct. We're done with parsing so we\n+  \/\/ now know all flattened array accesses in this compile\n+  \/\/ unit. Let's move flattened array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flattened memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flattened array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = NULL;\n+      if (n->Opcode() == Op_StoreCM) {\n+        adr_type = get_adr_type(get_alias_index(n->in(MemNode::OopStore)->adr_type()));\n+      } else {\n+        adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      }\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != NULL) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flattened_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flattened array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != NULL &&\n+          ace->_adr_type->is_flat()) {\n+        ace->_adr_type = NULL;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the NULL adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = NULL;\n+      if (m->Opcode() == Op_StoreCM) {\n+        adr_type = m->in(MemNode::OopStore)->adr_type();\n+        if (adr_type != TypeAryPtr::INLINES) {\n+          \/\/ store was optimized out and we lost track of the adr_type\n+          Node* clone = new StoreCMNode(m->in(MemNode::Control), m->in(MemNode::Memory), m->in(MemNode::Address),\n+                                        m->adr_type(), m->in(MemNode::ValueIn), m->in(MemNode::OopStore),\n+                                        get_alias_index(adr_type));\n+          igvn.register_new_node_with_optimizer(clone);\n+          igvn.replace_node(m, clone);\n+        }\n+      } else {\n+        adr_type = m->adr_type();\n+#ifdef ASSERT\n+        m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      }\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flattened array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = NULL;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = NULL;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == NULL) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const TypePtr* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flattened arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flattened array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const TypePtr* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != NULL) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const TypePtr* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flattened_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != NULL) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n@@ -2129,1 +2617,4 @@\n-  assert(_modified_nodes == NULL, \"not allowed\");\n+#ifdef ASSERT\n+  Unique_Node_List* modified_nodes = _modified_nodes;\n+  _modified_nodes = NULL;\n+#endif\n@@ -2143,0 +2634,1 @@\n+  DEBUG_ONLY( _modified_nodes = modified_nodes; )\n@@ -2286,0 +2778,5 @@\n+  \/\/ Process inline type nodes now that all inlining is over\n+  process_inline_types(igvn);\n+\n+  adjust_flattened_array_access_aliases(igvn);\n+\n@@ -2399,0 +2896,8 @@\n+  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n+\n+  if (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual and MH calls.\n+    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n+    process_late_inline_calls_no_inline(igvn);\n+  }\n+\n@@ -2409,0 +2914,4 @@\n+  \/\/ Process inline type nodes again and remove them. From here\n+  \/\/ on we don't need to keep track of field values anymore.\n+  process_inline_types(igvn, \/* remove= *\/ true);\n+\n@@ -2424,0 +2933,1 @@\n+  DEBUG_ONLY( _late_inlines.clear(); )\n@@ -2426,8 +2936,0 @@\n-\n-  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n-\n-  if (_late_inlines.length() > 0) {\n-    \/\/ More opportunities to optimize virtual and MH calls.\n-    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n-    process_late_inline_calls_no_inline(igvn);\n-  }\n@@ -3058,0 +3560,1 @@\n+\n@@ -3210,1 +3713,16 @@\n-      n->add_prec(prec);\n+      if (prec->is_MergeMem()) {\n+        MergeMemNode* mm = prec->as_MergeMem();\n+        Node* base = mm->base_memory();\n+        for (int i = AliasIdxRaw + 1; i < num_alias_types(); i++) {\n+          const TypePtr* adr_type = get_adr_type(i);\n+          if (adr_type->is_flat()) {\n+            Node* m = mm->memory_at(i);\n+            n->add_prec(m);\n+          }\n+        }\n+        if (mm->outcnt() == 0) {\n+          mm->disconnect_inputs(this);\n+        }\n+      } else {\n+        n->add_prec(prec);\n+      }\n@@ -3806,0 +4324,7 @@\n+#ifdef ASSERT\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n+#endif\n@@ -4182,2 +4707,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -4191,1 +4716,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4247,0 +4772,1 @@\n+               (n->is_Allocate() && i >= AllocateNode::InlineType) ||\n@@ -4249,1 +4775,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+              \"only region, phi, arraycopy, allocate, unlock or membar nodes have null data edges\");\n@@ -4383,0 +4909,8 @@\n+\n+    \/\/ Do not fold the subtype check to an array klass pointer comparison for [V? arrays.\n+    \/\/ [QMyValue is a subtype of [LMyValue but the klass for [QMyValue is not equal to\n+    \/\/ the klass for [LMyValue. Perform a full test.\n+    if (!superk->is_aryklassptr()->is_null_free() && superk->is_aryklassptr()->elem()->isa_instklassptr() &&\n+        superk->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->is_inlinetype()) {\n+      return SSC_full_test;\n+    }\n@@ -4934,0 +5468,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == NULL || tb == NULL ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are NULL.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(NULL, a));\n+    b = phase->transform(new CastP2XNode(NULL, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":607,"deletions":52,"binary":false,"changes":659,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+class CallNode;\n@@ -92,0 +93,1 @@\n+class InlineTypeNode;\n@@ -317,0 +319,1 @@\n+  bool                  _has_circular_inline_type; \/\/ True if method loads an inline type with a circular, non-flattened field\n@@ -343,0 +346,3 @@\n+  bool                  _has_flattened_accesses; \/\/ Any known flattened array accesses?\n+  bool                  _flattened_accesses_share_alias; \/\/ Initially all flattened array share a single slice\n+  bool                  _scalarize_in_safepoints; \/\/ Scalarize inline types in safepoint debug info\n@@ -358,0 +364,1 @@\n+  GrowableArray<Node*>  _inline_type_nodes;     \/\/ List of InlineType nodes\n@@ -595,0 +602,2 @@\n+  bool              has_circular_inline_type() const { return _has_circular_inline_type; }\n+  void          set_has_circular_inline_type(bool z) { _has_circular_inline_type = z; }\n@@ -631,0 +640,10 @@\n+  void          set_flattened_accesses()         { _has_flattened_accesses = true; }\n+  bool          flattened_accesses_share_alias() const { return _flattened_accesses_share_alias; }\n+  void          set_flattened_accesses_share_alias(bool z) { _flattened_accesses_share_alias = z; }\n+  bool          scalarize_in_safepoints() const { return _scalarize_in_safepoints; }\n+  void          set_scalarize_in_safepoints(bool z) { _scalarize_in_safepoints = z; }\n+\n+  \/\/ Support for scalarized inline type calling convention\n+  bool              has_scalarized_args() const  { return _method != NULL && _method->has_scalarized_args(); }\n+  bool              needs_stack_repair()  const  { return _method != NULL && _method->get_Method()->c2_needs_stack_repair(); }\n+\n@@ -732,0 +751,7 @@\n+  \/\/ Keep track of inline type nodes for later processing\n+  void add_inline_type(Node* n);\n+  void remove_inline_type(Node* n);\n+  void process_inline_types(PhaseIterGVN &igvn, bool remove = false);\n+\n+  void adjust_flattened_array_access_aliases(PhaseIterGVN& igvn);\n+\n@@ -876,1 +902,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = NULL) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = NULL, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }\n@@ -880,1 +906,1 @@\n-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }\n+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, NULL, uncached)->index(); }\n@@ -1111,1 +1137,1 @@\n-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);\n+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);\n@@ -1187,1 +1213,3 @@\n-  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  Node* optimize_acmp(PhaseGVN* phase, Node* a, Node* b);\n+\n+  \/\/ Auxiliary method for randomized fuzzing\/stressing\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -587,1 +588,1 @@\n-  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_initializer()) {\n+  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_constructor()) {\n@@ -717,1 +718,1 @@\n-          \/\/ It's OK for a method  to return a value that is discarded.\n+          \/\/ It's OK for a method to return a value that is discarded.\n@@ -729,0 +730,3 @@\n+            if (declared_signature->returns_null_free_inline_type()) {\n+              sig_type = sig_type->join_speculative(TypePtr::NOTNULL);\n+            }\n@@ -775,0 +779,5 @@\n+    if (rtype->is_inlinetype() && !peek()->is_InlineType()) {\n+      Node* retnode = pop();\n+      retnode = InlineTypeNode::make_from_oop(this, retnode, rtype->as_inline_klass(), !gvn().type(retnode)->maybe_null());\n+      push_node(T_PRIMITIVE_OBJECT, retnode);\n+    }\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -155,0 +156,10 @@\n+    if ((n->Opcode() == Op_LoadX || n->Opcode() == Op_StoreX) &&\n+        !n->in(MemNode::Address)->is_AddP() &&\n+        _igvn->type(n->in(MemNode::Address))->isa_oopptr()) {\n+      \/\/ Load\/Store at mark work address is at offset 0 so has no AddP which confuses EA\n+      Node* addp = new AddPNode(n->in(MemNode::Address), n->in(MemNode::Address), _igvn->MakeConX(0));\n+      _igvn->register_new_node_with_optimizer(addp);\n+      _igvn->replace_input_of(n, MemNode::Address, addp);\n+      ideal_nodes.push(addp);\n+      _nodes.at_put_grow(addp->_idx, NULL, NULL);\n+    }\n@@ -451,1 +462,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_sig();\n@@ -525,0 +536,11 @@\n+      } else if (n->as_Call()->tf()->returns_inline_type_as_fields()) {\n+        bool returns_oop = false;\n+        for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax && !returns_oop; i++) {\n+          ProjNode* pn = n->fast_out(i)->as_Proj();\n+          if (pn->_con >= TypeFunc::Parms && pn->bottom_type()->isa_ptr()) {\n+            returns_oop = true;\n+          }\n+        }\n+        if (returns_oop) {\n+          add_call_node(n->as_Call());\n+        }\n@@ -556,0 +578,1 @@\n+    case Op_InlineType:\n@@ -626,2 +649,4 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_Call() &&\n+          (n->in(0)->as_Call()->returns_pointer() || n->bottom_type()->isa_ptr())) {\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+               n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -727,0 +752,1 @@\n+    case Op_InlineType:\n@@ -781,2 +807,2 @@\n-      assert(n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-             n->in(0)->as_Call()->returns_pointer(), \"Unexpected node type\");\n+      assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+             n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -958,1 +984,1 @@\n-  assert(call->returns_pointer(), \"only for call which returns pointer\");\n+  assert(call->returns_pointer() || call->tf()->returns_inline_type_as_fields(), \"only for call which returns pointer\");\n@@ -1034,1 +1060,2 @@\n-      assert(strncmp(name, \"_multianewarray\", 15) == 0, \"TODO: add failed case check\");\n+      assert(strncmp(name, \"_multianewarray\", 15) == 0 ||\n+             strncmp(name, \"_load_unknown_inline\", 20) == 0, \"TODO: add failed case check\");\n@@ -1062,1 +1089,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -1110,1 +1137,1 @@\n-      const TypeTuple * d = call->tf()->domain();\n+      const TypeTuple * d = call->tf()->domain_sig();\n@@ -1141,1 +1168,4 @@\n-                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != NULL)));\n+                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != NULL)) ||\n+                               (aat->isa_aryptr() && aat->isa_aryptr()->elem() != NULL &&\n+                                                               aat->isa_aryptr()->is_flat() &&\n+                                                               aat->isa_aryptr()->elem()->inline_klass()->contains_oops()));\n@@ -1190,0 +1220,3 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"vectorizedMismatch\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"load_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_unknown_inline\") == 0 ||\n@@ -1252,1 +1285,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -1296,1 +1329,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_cc();\n@@ -1709,0 +1742,1 @@\n+  PointsToNode* init_val = phantom_obj;\n@@ -1714,1 +1748,8 @@\n-    return 0;\n+    if (alloc->as_Allocate()->in(AllocateNode::DefaultValue) != NULL) {\n+      \/\/ Non-flattened inline type arrays are initialized with\n+      \/\/ the default value instead of null. Handle them here.\n+      init_val = ptnode_adr(alloc->as_Allocate()->in(AllocateNode::DefaultValue)->_idx);\n+      assert(init_val != NULL, \"default value should be registered\");\n+    } else {\n+      return 0;\n+    }\n@@ -1716,1 +1757,2 @@\n-  assert(pta->arraycopy_dst() || alloc->as_CallStaticJava(), \"sanity\");\n+  \/\/ Non-escaped allocation returned from Java or runtime call has unknown values in fields.\n+  assert(pta->arraycopy_dst() || alloc->is_CallStaticJava() || init_val != phantom_obj, \"sanity\");\n@@ -1718,1 +1760,1 @@\n-  if (!pta->arraycopy_dst() && alloc->as_CallStaticJava()->method() == NULL) {\n+  if (alloc->is_CallStaticJava() && alloc->as_CallStaticJava()->method() == NULL) {\n@@ -1720,1 +1762,2 @@\n-    assert(strncmp(name, \"_multianewarray\", 15) == 0, \"sanity\");\n+    assert(strncmp(name, \"_multianewarray\", 15) == 0 ||\n+           strncmp(name, \"_load_unknown_inline\", 20) == 0, \"sanity\");\n@@ -1728,1 +1771,1 @@\n-      if (add_edge(field, phantom_obj)) {\n+      if (add_edge(field, init_val)) {\n@@ -1743,1 +1786,1 @@\n-  if (!alloc->is_Allocate()) {\n+  if (!alloc->is_Allocate() || alloc->as_Allocate()->in(AllocateNode::DefaultValue) != NULL) {\n@@ -1829,1 +1872,1 @@\n-                tty->print_cr(\"----------missed referernce to object-----------\");\n+                tty->print_cr(\"----------missed reference to object------------\");\n@@ -1831,1 +1874,1 @@\n-                tty->print_cr(\"----------object referernced by init store -----\");\n+                tty->print_cr(\"----------object referenced by init store-------\");\n@@ -2097,1 +2140,2 @@\n-          if (not_global_escape(alock->obj_node())) {\n+          const Type* obj_type = igvn->type(alock->obj_node());\n+          if (not_global_escape(alock->obj_node()) && !obj_type->is_inlinetypeptr()) {\n@@ -2138,5 +2182,10 @@\n-      MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n-      mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n-      mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n-      igvn->register_new_node_with_optimizer(mb);\n-      igvn->replace_node(storestore, mb);\n+      if (alloc->in(AllocateNode::InlineType) != NULL) {\n+        \/\/ Non-escaping inline type buffer allocations don't require a membar\n+        storestore->as_MemBar()->remove(_igvn);\n+      } else {\n+        MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n+        mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n+        mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n+        igvn->register_new_node_with_optimizer(mb);\n+        igvn->replace_node(storestore, mb);\n+      }\n@@ -2295,0 +2344,1 @@\n+  int field_offset = adr_type->isa_aryptr() ? adr_type->isa_aryptr()->field_offset().get() : Type::OffsetBot;\n@@ -2296,1 +2346,1 @@\n-  if (offset == Type::OffsetBot) {\n+  if (offset == Type::OffsetBot && field_offset == Type::OffsetBot) {\n@@ -2308,1 +2358,1 @@\n-      ciField* field = _compile->alias_type(adr_type->isa_instptr())->field();\n+      ciField* field = _compile->alias_type(adr_type->is_ptr())->field();\n@@ -2327,2 +2377,8 @@\n-        const Type* elemtype = adr_type->isa_aryptr()->elem();\n-        bt = elemtype->array_element_basic_type();\n+        const Type* elemtype = adr_type->is_aryptr()->elem();\n+        if (adr_type->is_aryptr()->is_flat() && field_offset != Type::OffsetBot) {\n+          ciInlineKlass* vk = elemtype->inline_klass();\n+          field_offset += vk->first_field_offset();\n+          bt = vk->get_field_by_offset(field_offset, false)->layout_type();\n+        } else {\n+          bt = elemtype->array_element_basic_type();\n+        }\n@@ -2512,3 +2568,1 @@\n-  const TypePtr *t_ptr = adr_type->isa_ptr();\n-  assert(t_ptr != NULL, \"must be a pointer type\");\n-  return t_ptr->offset();\n+  return adr_type->is_ptr()->flattened_offset();\n@@ -2668,1 +2722,8 @@\n-    t = base_t->add_offset(offs)->is_oopptr();\n+    if (base_t->isa_aryptr() != NULL) {\n+      \/\/ In the case of a flattened inline type array, each field has its\n+      \/\/ own slice so we need to extract the field being accessed from\n+      \/\/ the address computation\n+      t = base_t->isa_aryptr()->add_field_offset_and_offset(offs)->is_oopptr();\n+    } else {\n+      t = base_t->add_offset(offs)->is_oopptr();\n+    }\n@@ -2670,1 +2731,1 @@\n-  int inst_id =  base_t->instance_id();\n+  int inst_id = base_t->instance_id();\n@@ -2684,1 +2745,1 @@\n-  \/\/ It could happened when CHA type is different from MDO type on a dead path\n+  \/\/ It could happen when CHA type is different from MDO type on a dead path\n@@ -2694,1 +2755,12 @@\n-  const TypeOopPtr *tinst = base_t->add_offset(t->offset())->is_oopptr();\n+  const TypePtr* tinst = base_t->add_offset(t->offset());\n+  if (tinst->isa_aryptr() && t->isa_aryptr()) {\n+    \/\/ In the case of a flattened inline type array, each field has its\n+    \/\/ own slice so we need to keep track of the field being accessed.\n+    tinst = tinst->is_aryptr()->with_field_offset(t->is_aryptr()->field_offset().get());\n+    \/\/ Keep array properties (not flat\/null-free)\n+    tinst = tinst->is_aryptr()->update_properties(t->is_aryptr());\n+    if (tinst == NULL) {\n+      return false; \/\/ Skip dead path with inconsistent properties\n+    }\n+  }\n+\n@@ -3376,0 +3448,7 @@\n+          if (tn_t->isa_aryptr()) {\n+            \/\/ Keep array properties (not flat\/null-free)\n+            tinst = tinst->is_aryptr()->update_properties(tn_t->is_aryptr());\n+            if (tinst == NULL) {\n+              continue; \/\/ Skip dead path with inconsistent properties\n+            }\n+          }\n@@ -3401,1 +3480,1 @@\n-      if(use->is_Mem() && use->in(MemNode::Address) == n) {\n+      if (use->is_Mem() && use->in(MemNode::Address) == n) {\n@@ -3437,0 +3516,3 @@\n+      } else if (use->Opcode() == Op_Return) {\n+        \/\/ Allocation is referenced by field of returned inline type\n+        assert(_compile->tf()->returns_inline_type_as_fields(), \"EA: unexpected reference by ReturnNode\");\n@@ -3450,1 +3532,1 @@\n-              op == Op_SubTypeCheck ||\n+              op == Op_SubTypeCheck || op == Op_InlineType || op == Op_FlatArrayCheck ||\n@@ -3520,0 +3602,3 @@\n+    } else if (n->is_CallLeaf() && n->as_CallLeaf()->_name != NULL &&\n+               strcmp(n->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+      n = n->as_CallLeaf()->proj_out(TypeFunc::Memory);\n@@ -3562,1 +3647,1 @@\n-      } else if(use->is_Mem()) {\n+      } else if (use->is_Mem()) {\n@@ -3571,0 +3656,4 @@\n+      } else if (use->is_CallLeaf() && use->as_CallLeaf()->_name != NULL &&\n+                 strcmp(use->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+        \/\/ store_unknown_inline overwrites destination array\n+        memnode_worklist.append_if_missing(use);\n@@ -3580,1 +3669,1 @@\n-              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {\n+              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar || op == Op_FlatArrayCheck)) {\n@@ -3667,1 +3756,1 @@\n-  \/\/ chains as is done in split_memory_phi() since they  will\n+  \/\/ chains as is done in split_memory_phi() since they will\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":131,"deletions":42,"binary":false,"changes":173,"status":"modified"},{"patch":"@@ -279,1 +279,1 @@\n-        if (offset == Type::OffsetBot || tptr->_offset == Type::OffsetBot)\n+        if (offset == Type::OffsetBot || tptr->offset() == Type::OffsetBot)\n@@ -281,1 +281,1 @@\n-        offset += tptr->_offset; \/\/ correct if base is offsetted\n+        offset += tptr->offset(); \/\/ correct if base is offsetted\n@@ -318,1 +318,5 @@\n-      Block *inb = get_block_for_node(mach->in(j));\n+      Block* inb = get_block_for_node(mach->in(j));\n+      if (mach->in(j)->is_Con() && mach->in(j)->req() == 1 && inb == get_block_for_node(mach)) {\n+        \/\/ Ignore constant loads scheduled in the same block (we can simply hoist them as well)\n+        continue;\n+      }\n@@ -411,0 +415,21 @@\n+\n+  \/\/ Hoist constant load inputs as well.\n+  for (uint i = 1; i < best->req(); ++i) {\n+    Node* n = best->in(i);\n+    if (n->is_Con() && get_block_for_node(n) == get_block_for_node(best)) {\n+      get_block_for_node(n)->find_remove(n);\n+      block->add_inst(n);\n+      map_node_to_block(n, block);\n+      \/\/ Constant loads may kill flags (for example, when XORing a register).\n+      \/\/ Check for flag-killing projections that also need to be hoisted.\n+      for (DUIterator_Fast jmax, j = n->fast_outs(jmax); j < jmax; j++) {\n+        Node* proj = n->fast_out(j);\n+        if (proj->is_MachProj()) {\n+          get_block_for_node(proj)->find_remove(proj);\n+          block->add_inst(proj);\n+          map_node_to_block(proj, block);\n+        }\n+      }\n+    }\n+  }\n+\n@@ -882,1 +907,1 @@\n-  uint r_cnt = mcall->tf()->range()->cnt();\n+  uint r_cnt = mcall->tf()->range_cc()->cnt();\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":29,"deletions":4,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -323,0 +324,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -332,0 +335,1 @@\n+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_PRIMITIVE_OBJECT,Relaxed, false);\n@@ -342,0 +346,1 @@\n+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_PRIMITIVE_OBJECT,Relaxed, false);\n@@ -515,0 +520,5 @@\n+  case vmIntrinsics::_asPrimaryType:\n+  case vmIntrinsics::_asPrimaryTypeArg:\n+  case vmIntrinsics::_asValueType:\n+  case vmIntrinsics::_asValueTypeArg:           return inline_primitive_Class_conversion(intrinsic_id());\n+\n@@ -2001,1 +2011,1 @@\n-    } else if (type == T_OBJECT) {\n+    } else if (type == T_OBJECT || type == T_PRIMITIVE_OBJECT) {\n@@ -2180,0 +2190,1 @@\n+  bool null_free = false;\n@@ -2185,0 +2196,1 @@\n+      null_free = alias_type->field()->is_null_free();\n@@ -2193,0 +2205,1 @@\n+      null_free = adr_type->is_aryptr()->is_null_free();\n@@ -2205,0 +2218,3 @@\n+    if (null_free) {\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n@@ -2259,2 +2275,2 @@\n-      assert(rtype == type, \"getter must return the expected value\");\n-      assert(sig->count() == 2, \"oop getter has 2 arguments\");\n+      assert(rtype == type || (rtype == T_OBJECT && type == T_PRIMITIVE_OBJECT), \"getter must return the expected value\");\n+      assert(sig->count() == 2 || (type == T_PRIMITIVE_OBJECT && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n@@ -2266,1 +2282,1 @@\n-      assert(sig->count() == 3, \"oop putter has 3 arguments\");\n+      assert(sig->count() == 3 || (type == T_PRIMITIVE_OBJECT && sig->count() == 4), \"oop putter has 3 arguments\");\n@@ -2270,1 +2286,1 @@\n-      assert(vtype == type, \"putter must accept the expected value\");\n+      assert(vtype == type || (type == T_PRIMITIVE_OBJECT && vtype == T_OBJECT), \"putter must accept the expected value\");\n@@ -2292,0 +2308,55 @@\n+\n+  ciInlineKlass* inline_klass = NULL;\n+  if (type == T_PRIMITIVE_OBJECT) {\n+    const TypeInstPtr* cls = _gvn.type(argument(4))->isa_instptr();\n+    if (cls == NULL || cls->const_oop() == NULL) {\n+      return false;\n+    }\n+    ciType* mirror_type = cls->const_oop()->as_instance()->java_mirror_type();\n+    if (!mirror_type->is_inlinetype()) {\n+      return false;\n+    }\n+    inline_klass = mirror_type->as_inline_klass();\n+  }\n+\n+  if (base->is_InlineType()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (is_store) {\n+      if (!vt->is_allocated(&_gvn)) {\n+        return false;\n+      }\n+      base = vt->get_oop();\n+    } else {\n+      if (offset->is_Con()) {\n+        long off = find_long_con(offset, 0);\n+        ciInlineKlass* vk = vt->type()->inline_klass();\n+        if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+          return false;\n+        }\n+\n+        ciField* field = vk->get_non_flattened_field_by_offset(off);\n+        if (field != NULL) {\n+          BasicType bt = field->layout_type();\n+          if (bt == T_ARRAY || bt == T_NARROWOOP || (bt == T_PRIMITIVE_OBJECT && !field->is_flattened())) {\n+            bt = T_OBJECT;\n+          }\n+          if (bt == type && (bt != T_PRIMITIVE_OBJECT || field->type() == inline_klass)) {\n+            Node* value = vt->field_value_by_offset(off, false);\n+            if (value->is_InlineType()) {\n+              value = value->as_InlineType()->adjust_scalarization_depth(this);\n+            }\n+            set_result(value);\n+            return true;\n+          }\n+        }\n+      }\n+      {\n+        \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        vt = vt->buffer(this);\n+      }\n+      base = vt->get_oop();\n+    }\n+  }\n+\n@@ -2302,1 +2373,1 @@\n-    if (type != T_OBJECT) {\n+    if (type != T_OBJECT && (inline_klass == NULL || !inline_klass->has_object_fields())) {\n@@ -2320,1 +2391,1 @@\n-  Node* val = is_store ? argument(4) : NULL;\n+  Node* val = is_store ? argument(4 + (type == T_PRIMITIVE_OBJECT ? 1 : 0)) : NULL;\n@@ -2341,1 +2412,25 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = NULL;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != NULL &&\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flattened_field_by_offset(off);\n+    }\n+    if (field != NULL) {\n+      bt = field->layout_type();\n+    }\n+    assert(bt == alias_type->basic_type() || bt == T_PRIMITIVE_OBJECT, \"should match\");\n+    if (field != NULL && bt == T_PRIMITIVE_OBJECT && !field->is_flattened()) {\n+      bt = T_OBJECT;\n+    }\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2344,0 +2439,3 @@\n+    if (adr_type->is_flat()) {\n+      bt = T_PRIMITIVE_OBJECT;\n+    }\n@@ -2349,1 +2447,1 @@\n-    if (is_reference_type(bt, true)) {\n+    if (bt != T_PRIMITIVE_OBJECT && is_reference_type(bt, true)) {\n@@ -2364,0 +2462,23 @@\n+  if (type == T_PRIMITIVE_OBJECT) {\n+    if (adr_type->isa_instptr()) {\n+      if (field == NULL || field->type() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else if (adr_type->isa_aryptr()) {\n+      const Type* elem = adr_type->is_aryptr()->elem();\n+      if (!adr_type->is_flat() || elem->inline_klass() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else {\n+      mismatched = true;\n+    }\n+    if (is_store) {\n+      const Type* val_t = _gvn.type(val);\n+      if (!val_t->is_inlinetypeptr() || val_t->inline_klass() != inline_klass) {\n+        set_map(old_map);\n+        set_sp(old_sp);\n+        return false;\n+      }\n+    }\n+  }\n+\n@@ -2365,1 +2486,1 @@\n-  assert(!mismatched || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n+  assert(!mismatched || type == T_PRIMITIVE_OBJECT || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n@@ -2377,4 +2498,8 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != NULL) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != NULL) {\n+        value_type = tjp;\n+      }\n+    } else if (type == T_PRIMITIVE_OBJECT) {\n+      value_type = NULL;\n@@ -2396,2 +2521,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != NULL && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != NULL && field->is_constant() && !field->is_flattened() && !mismatched) {\n@@ -2403,1 +2528,16 @@\n-      p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      if (type == T_PRIMITIVE_OBJECT) {\n+        if (adr_type->isa_instptr() && !mismatched) {\n+          ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+          int offset = adr_type->is_instptr()->offset();\n+          p = InlineTypeNode::make_from_flattened(this, inline_klass, base, base, holder, offset, decorators);\n+        } else {\n+          p = InlineTypeNode::make_from_flattened(this, inline_klass, base, adr, NULL, 0, decorators);\n+        }\n+      } else {\n+        p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+        const TypeOopPtr* ptr = value_type->make_oopptr();\n+        if (ptr != NULL && ptr->is_inlinetypeptr()) {\n+          \/\/ Load a non-flattened inline type from memory\n+          p = InlineTypeNode::make_from_oop(this, p, ptr->inline_klass(), !ptr->maybe_null());\n+        }\n+      }\n@@ -2441,1 +2581,17 @@\n-    access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    if (type == T_PRIMITIVE_OBJECT) {\n+      if (adr_type->isa_instptr() && !mismatched) {\n+        ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+        int offset = adr_type->is_instptr()->offset();\n+        val->as_InlineType()->store_flattened(this, base, base, holder, offset, decorators);\n+      } else {\n+        val->as_InlineType()->store_flattened(this, base, adr, NULL, 0, decorators);\n+      }\n+    } else {\n+      access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    }\n+  }\n+\n+  if (argument(1)->is_InlineType() && is_store) {\n+    InlineTypeNode* value = InlineTypeNode::make_from_oop(this, base, _gvn.type(argument(1))->inline_klass());\n+    value = value->make_larval(this, false);\n+    replace_in_map(argument(1), value);\n@@ -2447,0 +2603,40 @@\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+  if (!value->is_InlineType()) {\n+    return false;\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  set_result(value->as_InlineType()->make_larval(this, true));\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+  if (!buffer->is_InlineType()) {\n+    return false;\n+  }\n+  InlineTypeNode* vt = buffer->as_InlineType();\n+  if (!vt->is_allocated(&_gvn)) {\n+    return false;\n+  }\n+  \/\/ TODO 8239003 Why is this needed?\n+  if (AllocateNode::Ideal_allocation(vt->get_oop(), &_gvn) == NULL) {\n+    return false;\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  set_result(vt->finish_larval(this));\n+  return true;\n+}\n+\n@@ -2652,0 +2848,13 @@\n+    if (oldval != NULL && oldval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      oldval = oldval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+    if (newval != NULL && newval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      newval = newval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+\n@@ -2813,2 +3022,7 @@\n-\n-  Node* obj = new_instance(kls, test);\n+  Node* obj = NULL;\n+  const TypeInstKlassPtr* tkls = _gvn.type(kls)->isa_instklassptr();\n+  if (tkls != NULL && tkls->instance_klass()->is_inlinetype()) {\n+    obj = InlineTypeNode::make_default(_gvn, tkls->instance_klass()->as_inline_klass())->buffer(this);\n+  } else {\n+    obj = new_instance(kls, test);\n+  }\n@@ -3393,1 +3607,1 @@\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n@@ -3412,9 +3626,0 @@\n-\/\/---------------------------load_mirror_from_klass----------------------------\n-\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {\n-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n-  Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n-  \/\/ mirror = ((OopHandle)mirror)->resolve();\n-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n-}\n-\n@@ -3463,0 +3668,1 @@\n+\n@@ -3656,0 +3862,31 @@\n+\/\/-------------------------inline_primitive_Class_conversion-------------------\n+\/\/               Class<T> java.lang.Class                  .asPrimaryType()\n+\/\/ public static Class<T> jdk.internal.value.PrimitiveClass.asPrimaryType(Class<T>)\n+\/\/               Class<T> java.lang.Class                  .asValueType()\n+\/\/ public static Class<T> jdk.internal.value.PrimitiveClass.asValueType(Class<T>)\n+bool LibraryCallKit::inline_primitive_Class_conversion(vmIntrinsics::ID id) {\n+  Node* mirror = argument(0); \/\/ Receiver\/argument Class\n+  const TypeInstPtr* mirror_con = _gvn.type(mirror)->isa_instptr();\n+  if (mirror_con == NULL) {\n+    return false;\n+  }\n+\n+  bool is_val_mirror = true;\n+  ciType* tm = mirror_con->java_mirror_type(&is_val_mirror);\n+  if (tm != NULL) {\n+    Node* result = mirror;\n+    if ((id == vmIntrinsics::_asPrimaryType || id == vmIntrinsics::_asPrimaryTypeArg) && is_val_mirror) {\n+      result = _gvn.makecon(TypeInstPtr::make(tm->as_inline_klass()->ref_mirror()));\n+    } else if (id == vmIntrinsics::_asValueType || id == vmIntrinsics::_asValueTypeArg) {\n+      if (!tm->is_inlinetype()) {\n+        return false; \/\/ Throw UnsupportedOperationException\n+      } else if (!is_val_mirror) {\n+        result = _gvn.makecon(TypeInstPtr::make(tm->as_inline_klass()->val_mirror()));\n+      }\n+    }\n+    set_result(result);\n+    return true;\n+  }\n+  return false;\n+}\n+\n@@ -3671,1 +3908,2 @@\n-  ciType* tm = mirror_con->java_mirror_type();\n+  bool requires_null_check = false;\n+  ciType* tm = mirror_con->java_mirror_type(&requires_null_check);\n@@ -3681,0 +3919,3 @@\n+        if (requires_null_check) {\n+          obj = null_check(obj);\n+        }\n@@ -3701,0 +3942,3 @@\n+  if (requires_null_check) {\n+    obj = null_check(obj);\n+  }\n@@ -3708,1 +3952,1 @@\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -3718,0 +3962,2 @@\n+  Node* io = i_o();\n+  Node* mem = merged_memory();\n@@ -3719,0 +3965,21 @@\n+    if (EnableValhalla && !requires_null_check) {\n+      \/\/ Check if we are casting to QMyValue\n+      Node* ctrl_val_mirror = generate_fair_guard(is_val_mirror(mirror), NULL);\n+      if (ctrl_val_mirror != NULL) {\n+        RegionNode* r = new RegionNode(3);\n+        record_for_igvn(r);\n+        r->init_req(1, control());\n+\n+        \/\/ Casting to QMyValue, check for null\n+        set_control(ctrl_val_mirror);\n+        { \/\/ PreserveJVMState because null check replaces obj in map\n+          PreserveJVMState pjvms(this);\n+          Node* null_ctr = top();\n+          null_check_oop(obj, &null_ctr);\n+          region->init_req(_npe_path, null_ctr);\n+          r->init_req(2, control());\n+        }\n+        set_control(_gvn.transform(r));\n+      }\n+    }\n+\n@@ -3725,1 +3992,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -3729,0 +3997,3 @@\n+    \/\/ Set IO and memory because gen_checkcast may override them when buffering inline types\n+    set_i_o(io);\n+    set_all_memory(mem);\n@@ -3762,0 +4033,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -3764,0 +4036,1 @@\n+  record_for_igvn(prim_region);\n@@ -3788,2 +4061,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -3799,0 +4075,3 @@\n+    \/\/ If superc is an inline mirror, we also need to check if superc == subc because LMyValue\n+    \/\/ is not a subtype of QMyValue but due to subk == superk the subtype check will pass.\n+    generate_fair_guard(is_val_mirror(args[0]), prim_region);\n@@ -3806,1 +4085,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -3811,1 +4091,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -3842,2 +4122,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind) {\n@@ -3849,9 +4128,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -3862,4 +4132,11 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;\n+      case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -3875,0 +4152,21 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case ObjectArray:\n+    case NonObjectArray: {\n+      value = Klass::_lh_array_tag_obj_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == ObjectArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -3876,4 +4174,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -3881,3 +4176,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -3890,1 +4182,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -4035,1 +4327,13 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    const TypeKlassPtr* tklass = _gvn.type(klass_node)->is_klassptr();\n+    bool exclude_flat = UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == NULL || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        tklass->can_be_inline_array() && (!tklass->is_flat() || tklass->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_objArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n@@ -4039,1 +4343,1 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n@@ -4060,0 +4364,32 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != NULL && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_fair_guard(flat_array_test(klass_node, \/* flat = *\/ false), bailout);\n+        }\n+      } else if (UseFlatArray && (orig_t == NULL || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!tklass->is_flat() && tklass->can_be_inline_array()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_fair_guard(flat_array_test(load_object_klass(original)), bailout);\n+        if (orig_t != NULL) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(null_free_array_test(klass_node), bailout);\n+      }\n+    }\n+\n@@ -4102,1 +4438,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -4241,1 +4577,6 @@\n-  Node* obj = NULL;\n+  Node* obj = argument(0);\n+\n+  if (gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -4251,1 +4592,0 @@\n-    obj = argument(0);\n@@ -4291,1 +4631,2 @@\n-  Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+  \/\/ This also serves as guard against inline types\n+  Node *lock_mask      = _gvn.MakeConX(markWord::inline_type_mask_in_place);\n@@ -4357,1 +4698,10 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    const Type* t = _gvn.type(obj);\n+    if (t->maybe_null()) {\n+      null_check(obj);\n+    }\n+    set_result(makecon(TypeInstPtr::make(t->inline_klass()->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -4722,1 +5072,2 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    obj = null_check_receiver();\n@@ -4732,1 +5083,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -4762,0 +5114,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -4767,3 +5124,0 @@\n-      Node* obj_length = load_array_length(obj);\n-      Node* obj_size  = NULL;\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size, \/*deoptimize_on_exception=*\/true);\n@@ -4772,20 +5126,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);\n-        if (is_obja != NULL) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          \/\/ Clones are always tightly coupled.\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Expansion) &&\n+          obj_type->can_be_inline_array() &&\n+          (ary_ptr == NULL || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flattened inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_fair_guard(flat_array_test(obj_klass), slow_region);\n@@ -4793,7 +5134,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -4802,7 +5136,43 @@\n-        copy_to_clone(obj, alloc_obj, obj_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(obj);\n+        Node* obj_size  = NULL;\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);\n+          if (is_obja != NULL) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            \/\/ Clones are always tightly coupled.\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, obj_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -4812,4 +5182,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -4985,2 +5351,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -4989,1 +5354,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -5031,1 +5396,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -5232,1 +5597,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -5259,0 +5624,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -5262,0 +5629,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -5277,2 +5646,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -5321,0 +5689,2 @@\n+      slow_region->add_req(not_subtype_ctrl);\n+    }\n@@ -5322,6 +5692,28 @@\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n+    src_type = _gvn.type(src);\n+    top_src  = src_type->isa_aryptr();\n+\n+    \/\/ Handle flat inline type arrays (null-free arrays are handled by the subtype check above)\n+    if (!stopped() && UseFlatArray) {\n+      \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check). Handle flat src here.\n+      assert(top_dest == NULL || !top_dest->is_flat() || top_src->is_flat(), \"src array must be flat\");\n+      if (top_src != NULL && top_src->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (top_dest != NULL && !top_dest->is_flat()) {\n+          generate_fair_guard(flat_array_test(dest_klass, \/* flat = *\/ false), slow_region);\n+          \/\/ Since dest is flat and src <: dest, dest must have the same type as src.\n+          top_dest = top_src->cast_to_exactness(false);\n+          assert(top_dest->is_flat(), \"dest must be flat\");\n+          dest = _gvn.transform(new CheckCastPPNode(control(), dest, top_dest));\n+        }\n+      } else if (top_src == NULL || !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        assert(top_dest == NULL || !top_dest->is_flat(), \"dest array must not be flat\");\n+        generate_fair_guard(flat_array_test(src), slow_region);\n+        if (top_src != NULL) {\n+          top_src = top_src->cast_to_not_flat();\n+          src = _gvn.transform(new CheckCastPPNode(control(), src, top_src));\n+        }\n@@ -5330,0 +5722,1 @@\n+\n@@ -5337,4 +5730,0 @@\n-\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n-    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":516,"deletions":127,"binary":false,"changes":643,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -108,3 +109,11 @@\n-    if (!stopped() && result() != NULL) {\n-      BasicType bt = result()->bottom_type()->basic_type();\n-      push_node(bt, result());\n+    Node* res = result();\n+    if (!stopped() && res != NULL) {\n+      BasicType bt = res->bottom_type()->basic_type();\n+      if (C->inlining_incrementally() && res->is_InlineType()) {\n+        \/\/ The caller expects an oop when incrementally inlining an intrinsic that returns an\n+        \/\/ inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        res = res->as_InlineType()->buffer(this);\n+      }\n+      push_node(bt, res);\n@@ -141,1 +150,0 @@\n-  Node* load_mirror_from_klass(Node* klass);\n@@ -163,0 +171,9 @@\n+\n+  enum ArrayKind {\n+    AnyArray,\n+    NonArray,\n+    ObjectArray,\n+    NonObjectArray,\n+    TypeArray\n+  };\n+\n@@ -164,0 +181,1 @@\n+\n@@ -165,1 +183,1 @@\n-    return generate_array_guard_common(kls, region, false, false);\n+    return generate_array_guard_common(kls, region, AnyArray);\n@@ -168,1 +186,1 @@\n-    return generate_array_guard_common(kls, region, false, true);\n+    return generate_array_guard_common(kls, region, NonArray);\n@@ -171,1 +189,1 @@\n-    return generate_array_guard_common(kls, region, true, false);\n+    return generate_array_guard_common(kls, region, ObjectArray);\n@@ -174,1 +192,4 @@\n-    return generate_array_guard_common(kls, region, true, true);\n+    return generate_array_guard_common(kls, region, NonObjectArray);\n+  }\n+  Node* generate_typeArray_guard(Node* kls, RegionNode* region) {\n+    return generate_array_guard_common(kls, region, TypeArray);\n@@ -176,2 +197,1 @@\n-  Node* generate_array_guard_common(Node* kls, RegionNode* region,\n-                                    bool obj_array, bool not_array);\n+  Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);\n@@ -238,0 +258,2 @@\n+  bool inline_unsafe_make_private_buffer();\n+  bool inline_unsafe_finish_private_buffer();\n@@ -254,0 +276,1 @@\n+  bool inline_primitive_Class_conversion(vmIntrinsics::ID id);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":33,"deletions":10,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -79,2 +79,4 @@\n-         LoopNestInnerLoop = 1 << 17,\n-         LoopNestLongOuterLoop = 1 << 18};\n+         LoopNestInnerLoop   = 1<< 17,\n+         LoopNestLongOuterLoop = 1<< 18,\n+         FlattenedArrays     = 1<<19};\n+\n@@ -106,0 +108,1 @@\n+  bool is_flattened_arrays() const { return _loop_flags & FlattenedArrays; }\n@@ -121,0 +124,1 @@\n+  void mark_flattened_arrays() { _loop_flags |= FlattenedArrays; }\n@@ -1454,3 +1458,3 @@\n-                                        Node_List &old_new,\n-                                        IfNode* unswitch_iff,\n-                                        CloneLoopMode mode);\n+                                      Node_List &old_new,\n+                                      Node_List &unswitch_iffs,\n+                                      CloneLoopMode mode);\n@@ -1474,1 +1478,1 @@\n-  IfNode* find_unswitching_candidate(const IdealLoopTree *loop) const;\n+  IfNode* find_unswitching_candidate(const IdealLoopTree *loop, Node_List& unswitch_iffs) const;\n@@ -1591,0 +1595,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1592,0 +1597,1 @@\n+  bool flatten_array_element_type_check(Node *n);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":12,"deletions":6,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -64,0 +65,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return NULL;\n+  }\n+\n@@ -699,0 +706,4 @@\n+      if (inp->isa_InlineType()) {\n+        \/\/ TODO 8302217 This prevents PhiNode::push_inline_types_through\n+        return NULL;\n+      }\n@@ -1024,0 +1035,49 @@\n+\/\/ If UseArrayMarkWordCheck is enabled, we can't use immutable memory for the flat array check\n+\/\/ because we are loading the mark word which is mutable. Although the bits we are interested in\n+\/\/ are immutable (we check for markWord::unlocked_value), we need to use raw memory to not break\n+\/\/ anti dependency analysis. Below code will attempt to still move flat array checks out of loops,\n+\/\/ mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::ArrayOrKlass)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -1036,0 +1096,6 @@\n+\n+  if (UseArrayMarkWordCheck && n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1313,0 +1379,98 @@\n+bool PhaseIdealLoop::flatten_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flattened array and the load of the value\n+  \/\/ happens with a flattened array check then: push the type check\n+  \/\/ through the phi of the flattened array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == NULL) {\n+    return false;\n+  }\n+\n+  assert(obj != NULL && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != NULL, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      const Type* tcast = cast_clone->Value(&_igvn);\n+      _igvn.set_type(cast_clone, tcast);\n+      cast_clone->as_Type()->set_type(tcast);\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1319,0 +1483,4 @@\n+  if (flatten_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1453,0 +1621,5 @@\n+\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(this);\n+  }\n@@ -1880,1 +2053,9 @@\n-  Node *sample_cmp = sample_bool->in(1);\n+  Node* sample_cmp = sample_bool->in(1);\n+  const Type* t = Type::TOP;\n+  const TypePtr* at = NULL;\n+  if (sample_cmp->is_FlatArrayCheck()) {\n+    \/\/ Left input of a FlatArrayCheckNode is memory, set the (adr) type of the phi accordingly\n+    assert(sample_cmp->in(1)->bottom_type() == Type::MEMORY, \"unexpected input type\");\n+    t = Type::MEMORY;\n+    at = TypeRawPtr::BOTTOM;\n+  }\n@@ -1883,1 +2064,1 @@\n-  PhiNode *phi1 = new PhiNode(phi->in(0), Type::TOP);\n+  PhiNode *phi1 = new PhiNode(phi->in(0), t, at);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":183,"deletions":2,"binary":false,"changes":185,"status":"modified"},{"patch":"@@ -186,0 +186,44 @@\n+\/\/ Array of RegMask, one per returned values (inline type instances can\n+\/\/ be returned as multiple return values, one per field)\n+RegMask* Matcher::return_values_mask(const TypeFunc* tf) {\n+  const TypeTuple* range = tf->range_cc();\n+  uint cnt = range->cnt() - TypeFunc::Parms;\n+  if (cnt == 0) {\n+    return NULL;\n+  }\n+  RegMask* mask = NEW_RESOURCE_ARRAY(RegMask, cnt);\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, cnt);\n+  VMRegPair* vm_parm_regs = NEW_RESOURCE_ARRAY(VMRegPair, cnt);\n+  for (uint i = 0; i < cnt; i++) {\n+    sig_bt[i] = range->field_at(i+TypeFunc::Parms)->basic_type();\n+  }\n+\n+  int regs = SharedRuntime::java_return_convention(sig_bt, vm_parm_regs, cnt);\n+  if (regs <= 0) {\n+    \/\/ We ran out of registers to store the IsInit information for a nullable inline type return.\n+    \/\/ Since it is only set in the 'call_epilog', we can simply put it on the stack.\n+    assert(tf->returns_inline_type_as_fields(), \"should have been tested during graph construction\");\n+    \/\/ TODO 8284443 Can we teach the register allocator to reserve a stack slot instead?\n+    \/\/ mask[--cnt] = STACK_ONLY_mask does not work (test with -XX:+StressGCM)\n+    int slot = C->fixed_slots() - 2;\n+    if (C->needs_stack_repair()) {\n+      slot -= 2; \/\/ Account for stack increment value\n+    }\n+    mask[--cnt].Clear();\n+    mask[cnt].Insert(OptoReg::stack2reg(slot));\n+  }\n+  for (uint i = 0; i < cnt; i++) {\n+    mask[i].Clear();\n+\n+    OptoReg::Name reg1 = OptoReg::as_OptoReg(vm_parm_regs[i].first());\n+    if (OptoReg::is_valid(reg1)) {\n+      mask[i].Insert(reg1);\n+    }\n+    OptoReg::Name reg2 = OptoReg::as_OptoReg(vm_parm_regs[i].second());\n+    if (OptoReg::is_valid(reg2)) {\n+      mask[i].Insert(reg2);\n+    }\n+  }\n+\n+  return mask;\n+}\n@@ -201,15 +245,3 @@\n-  \/\/ Map a Java-signature return type into return register-value\n-  \/\/ machine registers for 0, 1 and 2 returned values.\n-  const TypeTuple *range = C->tf()->range();\n-  if( range->cnt() > TypeFunc::Parms ) { \/\/ If not a void function\n-    \/\/ Get ideal-register return type\n-    uint ireg = range->field_at(TypeFunc::Parms)->ideal_reg();\n-    \/\/ Get machine return register\n-    uint sop = C->start()->Opcode();\n-    OptoRegPair regs = return_value(ireg);\n-\n-    \/\/ And mask for same\n-    _return_value_mask = RegMask(regs.first());\n-    if( OptoReg::is_valid(regs.second()) )\n-      _return_value_mask.Insert(regs.second());\n-  }\n+  \/\/ Map Java-signature return types into return register-value\n+  \/\/ machine registers.\n+  _return_values_mask = return_values_mask(C->tf());\n@@ -223,1 +255,1 @@\n-  const TypeTuple *domain = C->tf()->domain();\n+  const TypeTuple *domain = C->tf()->domain_cc();\n@@ -527,0 +559,1 @@\n+\n@@ -785,1 +818,1 @@\n-  uint ret_edge_cnt = TypeFunc::Parms + ((C->tf()->range()->cnt() == TypeFunc::Parms) ? 0 : 1);\n+  uint ret_edge_cnt = C->tf()->range_cc()->cnt();\n@@ -787,4 +820,3 @@\n-  \/\/ Returns have 0 or 1 returned values depending on call signature.\n-  \/\/ Return register is specified by return_value in the AD file.\n-  if (ret_edge_cnt > TypeFunc::Parms)\n-    ret_rms[TypeFunc::Parms+0] = _return_value_mask;\n+  for (i = TypeFunc::Parms; i < ret_edge_cnt; i++) {\n+    ret_rms[i] = _return_values_mask[i-TypeFunc::Parms];\n+  }\n@@ -857,1 +889,1 @@\n-  int proj_cnt = C->tf()->domain()->cnt();\n+  int proj_cnt = C->tf()->domain_cc()->cnt();\n@@ -1130,1 +1162,5 @@\n-              m = n->in(0)->as_Multi()->match( n->as_Proj(), this );\n+              RegMask* mask = NULL;\n+              if (n->in(0)->is_Call() && n->in(0)->as_Call()->tf()->returns_inline_type_as_fields()) {\n+                mask = return_values_mask(n->in(0)->as_Call()->tf());\n+              }\n+              m = n->in(0)->as_Multi()->match(n->as_Proj(), this, mask);\n@@ -1269,1 +1305,1 @@\n-    domain = call->tf()->domain();\n+    domain = call->tf()->domain_cc();\n@@ -1348,1 +1384,4 @@\n-  int argcnt = cnt - TypeFunc::Parms;\n+  \/\/ Null entry point is a special cast where the target of the call\n+  \/\/ is in a register.\n+  int adj = (call != NULL && call->entry_point() == NULL) ? 1 : 0;\n+  int argcnt = cnt - TypeFunc::Parms - adj;\n@@ -1354,1 +1393,1 @@\n-      sig_bt[i] = domain->field_at(i+TypeFunc::Parms)->basic_type();\n+      sig_bt[i] = domain->field_at(i+TypeFunc::Parms+adj)->basic_type();\n@@ -1395,1 +1434,1 @@\n-      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms];\n+      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms+adj];\n@@ -1413,1 +1452,1 @@\n-      if (OptoReg::is_valid(reg1))\n+      if (OptoReg::is_valid(reg1)) {\n@@ -1415,0 +1454,1 @@\n+      }\n@@ -1417,1 +1457,1 @@\n-      if (OptoReg::is_valid(reg2))\n+      if (OptoReg::is_valid(reg2)) {\n@@ -1419,0 +1459,1 @@\n+      }\n@@ -1434,1 +1475,1 @@\n-    uint r_cnt = mcall->tf()->range()->cnt();\n+    uint r_cnt = mcall->tf()->range_sig()->cnt();\n@@ -1455,1 +1496,1 @@\n-         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain()->cnt()), \"\");\n+         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain_cc()->cnt()), \"\");\n@@ -2138,1 +2179,1 @@\n-      for (int i = n->req() - 1; i >= 0; --i) { \/\/ For my children\n+      for (int i = n->len() - 1; i >= 0; --i) { \/\/ For my children\n@@ -2458,0 +2499,7 @@\n+    case Op_ClearArray: {\n+      Node* pair = new BinaryNode(n->in(2), n->in(3));\n+      n->set_req(2, pair);\n+      n->set_req(3, n->in(4));\n+      n->del_req(4);\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":80,"deletions":32,"binary":false,"changes":112,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -41,0 +43,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -234,0 +237,2 @@\n+                     ->cast_to_not_flat(t_oop->is_aryptr()->is_not_flat())\n+                     ->cast_to_not_null_free(t_oop->is_aryptr()->is_not_null_free())\n@@ -260,1 +265,1 @@\n-               tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n+        tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n@@ -928,0 +933,1 @@\n+  case T_PRIMITIVE_OBJECT:\n@@ -1023,1 +1029,1 @@\n-      uint shift  = exact_log2(type2aelembytes(ary_elem));\n+      uint shift  = ary_t->is_flat() ? ary_t->flat_log_elem_size() : exact_log2(type2aelembytes(ary_elem));\n@@ -1144,1 +1150,1 @@\n-        const TypeVect* out_vt = as_LoadVector()->vect_type();\n+        const TypeVect* out_vt = is_Load() ? as_LoadVector()->vect_type() : as_StoreVector()->vect_type();\n@@ -1162,0 +1168,6 @@\n+      assert(memory_type() != T_PRIMITIVE_OBJECT, \"should not be used for inline types\");\n+      Node* default_value = ld_alloc->in(AllocateNode::DefaultValue);\n+      if (default_value != NULL) {\n+        return default_value;\n+      }\n+      assert(ld_alloc->in(AllocateNode::RawDefaultValue) == NULL, \"default value may not be null\");\n@@ -1229,0 +1241,17 @@\n+  \/\/ Loading from an InlineType? The InlineType has the values of\n+  \/\/ all fields as input. Look for the field with matching offset.\n+  Node* addr = in(Address);\n+  intptr_t offset;\n+  Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);\n+  if (base != NULL && base->is_InlineType() && offset > oopDesc::klass_offset_in_bytes()) {\n+    Node* value = base->as_InlineType()->field_value_by_offset((int)offset, true);\n+    if (value != NULL) {\n+      if (Opcode() == Op_LoadN) {\n+        \/\/ Encode oop value if we are loading a narrow oop\n+        assert(!phase->type(value)->isa_narrowoop(), \"should already be decoded\");\n+        value = phase->transform(new EncodePNode(value, bottom_type()));\n+      }\n+      return value;\n+    }\n+  }\n+\n@@ -1953,0 +1982,1 @@\n+        && !ary->is_flat()\n@@ -1988,0 +2018,2 @@\n+            \/\/ Default value load\n+            tp->is_instptr()->instance_klass() == ciEnv::current()->Class_klass() ||\n@@ -1993,1 +2025,3 @@\n-    \/\/ Optimize loads from constant fields.\n+    BasicType bt = memory_type();\n+\n+    \/\/ Optimize loads from constant fields.\n@@ -1997,1 +2031,19 @@\n-      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), memory_type());\n+      ciType* mirror_type = const_oop->as_instance()->java_mirror_type();\n+      if (mirror_type != NULL) {\n+        const Type* const_oop = NULL;\n+        ciInlineKlass* vk = mirror_type->is_inlinetype() ? mirror_type->as_inline_klass() : NULL;\n+        \/\/ Fold default value loads\n+        if (vk != NULL && off == vk->default_value_offset()) {\n+          const_oop = TypeInstPtr::make(vk->default_instance());\n+        }\n+        \/\/ Fold class mirror loads\n+        if (off == java_lang_Class::primary_mirror_offset()) {\n+          const_oop = (vk == NULL) ? TypePtr::NULL_PTR : TypeInstPtr::make(vk->ref_instance());\n+        } else if (off == java_lang_Class::secondary_mirror_offset()) {\n+          const_oop = (vk == NULL) ? TypePtr::NULL_PTR : TypeInstPtr::make(vk->val_instance());\n+        }\n+        if (const_oop != NULL) {\n+          return (bt == T_NARROWOOP) ? const_oop->make_narrowoop() : const_oop;\n+        }\n+      }\n+      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), bt);\n@@ -2012,15 +2064,31 @@\n-  } else if (tp->base() == Type::RawPtr && adr->is_Load() && off == 0) {\n-    \/* With mirrors being an indirect in the Klass*\n-     * the VM is now using two loads. LoadKlass(LoadP(LoadP(Klass, mirror_offset), zero_offset))\n-     * The LoadP from the Klass has a RawPtr type (see LibraryCallKit::load_mirror_from_klass).\n-     *\n-     * So check the type and klass of the node before the LoadP.\n-     *\/\n-    Node* adr2 = adr->in(MemNode::Address);\n-    const TypeKlassPtr* tkls = phase->type(adr2)->isa_klassptr();\n-    if (tkls != NULL && !StressReflectiveCode) {\n-      if (tkls->is_loaded() && tkls->klass_is_exact() && tkls->offset() == in_bytes(Klass::java_mirror_offset())) {\n-        ciKlass* klass = tkls->exact_klass();\n-        assert(adr->Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n-        assert(Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n-        return TypeInstPtr::make(klass->java_mirror());\n+  } else if (tp->base() == Type::RawPtr && !StressReflectiveCode) {\n+    if (adr->is_Load() && off == 0) {\n+      \/* With mirrors being an indirect in the Klass*\n+       * the VM is now using two loads. LoadKlass(LoadP(LoadP(Klass, mirror_offset), zero_offset))\n+       * The LoadP from the Klass has a RawPtr type (see LibraryCallKit::load_mirror_from_klass).\n+       *\n+       * So check the type and klass of the node before the LoadP.\n+       *\/\n+      Node* adr2 = adr->in(MemNode::Address);\n+      const TypeKlassPtr* tkls = phase->type(adr2)->isa_klassptr();\n+      if (tkls != NULL) {\n+        if (tkls->is_loaded() && tkls->klass_is_exact() && tkls->offset() == in_bytes(Klass::java_mirror_offset())) {\n+          ciKlass* klass = tkls->exact_klass();\n+          assert(adr->Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n+          assert(Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n+          return TypeInstPtr::make(klass->java_mirror());\n+        }\n+      }\n+    } else {\n+      \/\/ Check for a load of the default value offset from the InlineKlassFixedBlock:\n+      \/\/ LoadI(LoadP(inline_klass, adr_inlineklass_fixed_block_offset), default_value_offset_offset)\n+      intptr_t offset = 0;\n+      Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+      if (base != NULL && base->is_Load() && offset == in_bytes(InlineKlass::default_value_offset_offset())) {\n+        const TypeKlassPtr* tkls = phase->type(base->in(MemNode::Address))->isa_klassptr();\n+        if (tkls != NULL && tkls->is_loaded() && tkls->klass_is_exact() && tkls->exact_klass()->is_inlinetype() &&\n+            tkls->offset() == in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())) {\n+          assert(base->Opcode() == Op_LoadP, \"must load an oop from klass\");\n+          assert(Opcode() == Op_LoadI, \"must load an int from fixed block\");\n+          return TypeInt::make(tkls->exact_klass()->as_inline_klass()->default_value_offset());\n+        }\n@@ -2132,1 +2200,0 @@\n-\n@@ -2135,1 +2202,10 @@\n-    return TypeX::make(markWord::prototype().value());\n+    if (EnableValhalla) {\n+      \/\/ The mark word may contain property bits (inline, flat, null-free)\n+      Node* klass_node = alloc->in(AllocateNode::KlassNode);\n+      const TypeKlassPtr* tkls = phase->type(klass_node)->isa_klassptr();\n+      if (tkls != NULL && tkls->is_loaded() && tkls->klass_is_exact()) {\n+        return TypeX::make(tkls->exact_klass()->prototype_header().value());\n+      }\n+    } else {\n+      return TypeX::make(markWord::prototype().value());\n+    }\n@@ -2286,1 +2362,2 @@\n-Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at, const TypeKlassPtr* tk) {\n+Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at,\n+                          const TypeKlassPtr* tk) {\n@@ -2333,1 +2410,2 @@\n-      ciType* t = tinst->java_mirror_type();\n+      bool null_free = false;\n+      ciType* t = tinst->java_mirror_type(&null_free);\n@@ -2343,1 +2421,1 @@\n-          return TypeKlassPtr::make(ciArrayKlass::make(t), Type::trust_interfaces);\n+          return TypeKlassPtr::make(ciArrayKlass::make(t, null_free), Type::trust_interfaces);\n@@ -2362,1 +2440,1 @@\n-  const TypeAryPtr *tary = tp->isa_aryptr();\n+  const TypeAryPtr* tary = tp->isa_aryptr();\n@@ -2588,0 +2666,1 @@\n+  case T_PRIMITIVE_OBJECT:\n@@ -2638,1 +2717,1 @@\n-  {\n+  if (phase->C->get_adr_type(phase->C->get_alias_index(adr_type())) != TypeAryPtr::INLINES) {\n@@ -2658,0 +2737,1 @@\n+             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreN) ||\n@@ -2754,2 +2834,1 @@\n-  if (result == this &&\n-      ReduceFieldZeroing && phase->type(val)->is_zero_type()) {\n+  if (result == this && ReduceFieldZeroing) {\n@@ -2757,1 +2836,2 @@\n-    if (mem->is_Proj() && mem->in(0)->is_Allocate()) {\n+    if (mem->is_Proj() && mem->in(0)->is_Allocate() &&\n+        (phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::DefaultValue) == val)) {\n@@ -2761,1 +2841,1 @@\n-    if (result == this) {\n+    if (result == this && phase->type(val)->is_zero_type()) {\n@@ -2946,3 +3026,7 @@\n-    Node* mem = my_store->as_MergeMem()->memory_at(oop_alias_idx());\n-    set_req_X(MemNode::OopStore, mem, phase);\n-    return this;\n+    if (oop_alias_idx() != phase->C->get_alias_index(TypeAryPtr::INLINES) ||\n+        phase->C->flattened_accesses_share_alias()) {\n+      \/\/ The alias that was recorded is no longer accurate enough.\n+      Node* mem = my_store->as_MergeMem()->memory_at(oop_alias_idx());\n+      set_req_X(MemNode::OopStore, mem, phase);\n+      return this;\n+    }\n@@ -3107,1 +3191,1 @@\n-    return new ClearArrayNode(in(0), in(1), in(2), in(3), true);\n+    return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);\n@@ -3125,1 +3209,1 @@\n-  Node *zero = phase->makecon(TypeLong::ZERO);\n+  Node *val = in(4);\n@@ -3127,1 +3211,1 @@\n-  mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+  mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -3132,1 +3216,1 @@\n-    mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+    mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -3166,0 +3250,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -3176,1 +3262,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != NULL) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == NULL, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -3183,1 +3275,1 @@\n-  return clear_memory(ctl, mem, dest, phase->MakeConX(offset), end_offset, phase);\n+  return clear_memory(ctl, mem, dest, raw_val, phase->MakeConX(offset), end_offset, phase);\n@@ -3187,0 +3279,1 @@\n+                                   Node* raw_val,\n@@ -3209,1 +3302,4 @@\n-  mem = new ClearArrayNode(ctl, mem, zsize, adr, false);\n+  if (raw_val == NULL) {\n+    raw_val = phase->MakeConX(0);\n+  }\n+  mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);\n@@ -3214,0 +3310,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -3228,1 +3326,1 @@\n-    mem = clear_memory(ctl, mem, dest,\n+    mem = clear_memory(ctl, mem, dest, val, raw_val,\n@@ -3235,1 +3333,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != NULL) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == NULL, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -3380,1 +3484,1 @@\n-Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {\n+Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {\n@@ -3667,1 +3771,3 @@\n-  if (init == NULL || init->is_complete())  return false;\n+  if (init == NULL || init->is_complete()) {\n+    return false;\n+  }\n@@ -3845,0 +3951,6 @@\n+                if (base->is_Phi()) {\n+                  \/\/ In rare case, base may be a PhiNode and it may read\n+                  \/\/ the same memory slice between InitializeNode and store.\n+                  failed = true;\n+                  break;\n+                }\n@@ -4431,0 +4543,2 @@\n+                                              allocation()->in(AllocateNode::DefaultValue),\n+                                              allocation()->in(AllocateNode::RawDefaultValue),\n@@ -4490,0 +4604,2 @@\n+                                            allocation()->in(AllocateNode::DefaultValue),\n+                                            allocation()->in(AllocateNode::RawDefaultValue),\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":161,"deletions":45,"binary":false,"changes":206,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -575,0 +576,3 @@\n+  if (n->is_InlineType()) {\n+    C->add_inline_type(n);\n+  }\n@@ -658,0 +662,3 @@\n+  if (is_InlineType()) {\n+    compile->remove_inline_type(this);\n+  }\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+class FlatArrayCheckNode;\n@@ -117,0 +118,1 @@\n+class MachPrologNode;\n@@ -123,0 +125,1 @@\n+class MachVEPNode;\n@@ -167,0 +170,1 @@\n+class InlineTypeNode;\n@@ -668,0 +672,1 @@\n+        DEFINE_CLASS_ID(Blackhole,        MemBar, 2)\n@@ -689,0 +694,2 @@\n+      DEFINE_CLASS_ID(MachProlog,       Mach, 8)\n+      DEFINE_CLASS_ID(MachVEP,          Mach, 9)\n@@ -715,1 +722,2 @@\n-      DEFINE_CLASS_ID(Con, Type, 8)\n+      DEFINE_CLASS_ID(InlineType, Type, 8)\n+      DEFINE_CLASS_ID(Con, Type, 9)\n@@ -754,3 +762,4 @@\n-        DEFINE_CLASS_ID(FastLock,   Cmp, 0)\n-        DEFINE_CLASS_ID(FastUnlock, Cmp, 1)\n-        DEFINE_CLASS_ID(SubTypeCheck,Cmp, 2)\n+        DEFINE_CLASS_ID(FastLock,       Cmp, 0)\n+        DEFINE_CLASS_ID(FastUnlock,     Cmp, 1)\n+        DEFINE_CLASS_ID(SubTypeCheck,   Cmp, 2)\n+        DEFINE_CLASS_ID(FlatArrayCheck, Cmp, 3)\n@@ -855,0 +864,1 @@\n+  DEFINE_CLASS_QUERY(Blackhole)\n@@ -884,0 +894,1 @@\n+  DEFINE_CLASS_QUERY(FlatArrayCheck)\n@@ -916,0 +927,1 @@\n+  DEFINE_CLASS_QUERY(MachProlog)\n@@ -922,0 +934,1 @@\n+  DEFINE_CLASS_QUERY(MachVEP)\n@@ -947,0 +960,1 @@\n+  DEFINE_CLASS_QUERY(InlineType)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":18,"deletions":4,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -244,1 +245,9 @@\n-    _orig_pc_slot = C->fixed_slots() - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n+    int fixed_slots = C->fixed_slots();\n+    if (C->needs_stack_repair()) {\n+      fixed_slots -= 2;\n+    }\n+    \/\/ TODO 8284443 Only reserve extra slot if needed\n+    if (InlineTypeReturnedAsFields) {\n+      fixed_slots -= 2;\n+    }\n+    _orig_pc_slot = fixed_slots - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n@@ -285,1 +294,2 @@\n-  MachPrologNode *prolog = new MachPrologNode();\n+  Label verified_entry;\n+  MachPrologNode* prolog = new MachPrologNode(&verified_entry);\n@@ -291,3 +301,2 @@\n-\n-  if( C->is_osr_compilation() ) {\n-    if( PoisonOSREntry ) {\n+  if (C->is_osr_compilation()) {\n+    if (PoisonOSREntry) {\n@@ -298,3 +307,14 @@\n-    if( C->method() && !C->method()->flags().is_static() ) {\n-      \/\/ Insert unvalidated entry point\n-      C->cfg()->insert( broot, 0, new MachUEPNode() );\n+    if (C->method()) {\n+      if (C->method()->has_scalarized_args()) {\n+        \/\/ Add entry point to unpack all inline type arguments\n+        C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true, \/* receiver_only *\/ false));\n+        if (!C->method()->is_static()) {\n+          \/\/ Add verified\/unverified entry points to only unpack inline type receiver at interface calls\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ false));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true,  \/* receiver_only *\/ true));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ true));\n+        }\n+      } else if (!C->method()->is_static()) {\n+        \/\/ Insert unvalidated entry point\n+        C->cfg()->insert(broot, 0, new MachUEPNode());\n+      }\n@@ -302,1 +322,0 @@\n-\n@@ -342,0 +361,25 @@\n+  if (!C->is_osr_compilation() && C->has_scalarized_args()) {\n+    \/\/ Compute the offsets of the entry points required by the inline type calling convention\n+    if (!C->method()->is_static()) {\n+      \/\/ We have entries at the beginning of the method, implemented by the first 4 nodes.\n+      \/\/ Entry                     (unverified) @ offset 0\n+      \/\/ Verified_Inline_Entry_RO\n+      \/\/ Inline_Entry              (unverified)\n+      \/\/ Verified_Inline_Entry\n+      uint offset = 0;\n+      _code_offsets.set_value(CodeOffsets::Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(0))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(1))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Inline_Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(2))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, offset);\n+    } else {\n+      _code_offsets.set_value(CodeOffsets::Entry, -1); \/\/ will be patched later\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, 0);\n+    }\n+  }\n+\n@@ -502,1 +546,3 @@\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != NULL) {\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -757,0 +803,17 @@\n+      uint first_ind = spobj->first_index(sfpt->jvms());\n+      \/\/ Nullable, scalarized inline types have an is_init input\n+      \/\/ that needs to be checked before using the field values.\n+      ScopeValue* is_init = NULL;\n+      if (cik->is_inlinetype()) {\n+        Node* init_node = sfpt->in(first_ind++);\n+        assert(init_node != NULL, \"is_init node not found\");\n+        if (!init_node->is_top()) {\n+          const TypeInt* init_type = init_node->bottom_type()->is_int();\n+          if (init_node->is_Con()) {\n+            is_init = new ConstantIntValue(init_type->get_con());\n+          } else {\n+            OptoReg::Name init_reg = C->regalloc()->get_reg_first(init_node);\n+            is_init = new_loc_value(C->regalloc(), init_reg, Location::normal);\n+          }\n+        }\n+      }\n@@ -758,1 +821,1 @@\n-                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()));\n+                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), is_init);\n@@ -761,1 +824,0 @@\n-      uint first_ind = spobj->first_index(sfpt->jvms());\n@@ -943,0 +1005,1 @@\n+  bool return_scalarized = false;\n@@ -963,1 +1026,1 @@\n-    if (mcall->returns_pointer()) {\n+    if (mcall->returns_pointer() || mcall->returns_scalarized()) {\n@@ -966,0 +1029,3 @@\n+    if (mcall->returns_scalarized()) {\n+      return_scalarized = true;\n+    }\n@@ -1089,0 +1155,1 @@\n+      return_scalarized,\n@@ -1464,2 +1531,4 @@\n-          \/\/ This destination address is NOT PC-relative\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != NULL) {\n+            \/\/ This destination address is NOT PC-relative\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -1629,1 +1698,0 @@\n-\n@@ -3015,0 +3083,13 @@\n+\n+      \/\/ Do not allow a CheckCastPP node whose input is a raw pointer to\n+      \/\/ float past a safepoint.  This can occur when a buffered inline\n+      \/\/ type is allocated in a loop and the CheckCastPP from that\n+      \/\/ allocation is reused outside the loop.  If the use inside the\n+      \/\/ loop is scalarized the CheckCastPP will no longer be connected\n+      \/\/ to the loop safepoint.  See JDK-8264340.\n+      if (m->is_Mach() && m->as_Mach()->ideal_Opcode() == Op_CheckCastPP) {\n+        Node *def = m->in(1);\n+        if (def != NULL && def->bottom_type()->base() == Type::RawPtr) {\n+          last_safept_node->add_prec(m);\n+        }\n+      }\n@@ -3173,0 +3254,19 @@\n+    if (C->has_scalarized_args()) {\n+      \/\/ Inline type entry points (MachVEPNodes) require lots of space for GC barriers and oop verification\n+      \/\/ when loading object fields from the buffered argument. Increase scratch buffer size accordingly.\n+      ciMethod* method = C->method();\n+      int barrier_size = UseZGC ? 200 : (7 DEBUG_ONLY(+ 37));\n+      int arg_num = 0;\n+      if (!method->is_static()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += method->holder()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+      for (ciSignatureStream str(method->signature()); !str.at_return_type(); str.next()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += str.type()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+    }\n@@ -3243,1 +3343,2 @@\n-  if (is_branch) \/\/ Restore label.\n+  \/\/ Restore label.\n+  if (is_branch) {\n@@ -3245,0 +3346,1 @@\n+  }\n@@ -3288,0 +3390,9 @@\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry_RO) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);\n+      }\n@@ -3292,14 +3403,14 @@\n-                                     entry_bci,\n-                                     &_code_offsets,\n-                                     _orig_pc_slot_offset_in_bytes,\n-                                     code_buffer(),\n-                                     frame_size_in_words(),\n-                                     oop_map_set(),\n-                                     &_handler_table,\n-                                     inc_table(),\n-                                     compiler,\n-                                     has_unsafe_access,\n-                                     SharedRuntime::is_wide_vector(C->max_vector_size()),\n-                                     C->has_monitors(),\n-                                     0,\n-                                     C->rtm_state());\n+                              entry_bci,\n+                              &_code_offsets,\n+                              _orig_pc_slot_offset_in_bytes,\n+                              code_buffer(),\n+                              frame_size_in_words(),\n+                              _oop_map_set,\n+                              &_handler_table,\n+                              inc_table(),\n+                              compiler,\n+                              has_unsafe_access,\n+                              SharedRuntime::is_wide_vector(C->max_vector_size()),\n+                              C->has_monitors(),\n+                              0,\n+                              C->rtm_state());\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":142,"deletions":31,"binary":false,"changes":173,"status":"modified"},{"patch":"@@ -445,1 +445,1 @@\n-  Node *fetch_interpreter_state(int index, BasicType bt, Node *local_addrs, Node *local_addrs_base);\n+  Node* fetch_interpreter_state(int index, const Type* type, Node* local_addrs, Node* local_addrs_base);\n@@ -491,1 +491,1 @@\n-  void array_store_check();\n+  Node* array_store_check(Node*& adr, const Type*& elemtype);\n@@ -498,0 +498,1 @@\n+  Node* record_profile_for_speculation_at_array_load(Node* ld);\n@@ -546,1 +547,1 @@\n-  void do_get_xxx(Node* obj, ciField* field, bool is_field);\n+  void do_get_xxx(Node* obj, ciField* field);\n@@ -551,0 +552,2 @@\n+  void do_aconst_init();\n+  void do_withfield();\n@@ -552,1 +555,1 @@\n-  void do_anewarray();\n+  void do_newarray();\n@@ -567,1 +570,6 @@\n-  void    do_if(BoolTest::mask btest, Node* c);\n+  void    do_if(BoolTest::mask btest, Node* c, bool new_path = false, Node** ctrl_taken = NULL);\n+  void    do_acmp(BoolTest::mask btest, Node* left, Node* right);\n+  void    acmp_always_null_input(Node* input, const TypeOopPtr* tinput, BoolTest::mask btest, Node* eq_region);\n+  void    acmp_known_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, ciKlass* input_type, BoolTest::mask btest, Node* eq_region);\n+  Node*   acmp_null_check(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, Node*& null_ctl);\n+  void    acmp_unknown_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, BoolTest::mask btest, Node* eq_region);\n","filename":"src\/hotspot\/share\/opto\/parse.hpp","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"opto\/convertnode.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -105,4 +107,10 @@\n-Node *Parse::fetch_interpreter_state(int index,\n-                                     BasicType bt,\n-                                     Node *local_addrs,\n-                                     Node *local_addrs_base) {\n+Node* Parse::fetch_interpreter_state(int index,\n+                                     const Type* type,\n+                                     Node* local_addrs,\n+                                     Node* local_addrs_base) {\n+  BasicType bt = type->basic_type();\n+  if (type == TypePtr::NULL_PTR) {\n+    \/\/ Ptr types are mixed together with T_ADDRESS but NULL is\n+    \/\/ really for T_OBJECT types so correct it.\n+    bt = T_OBJECT;\n+  }\n@@ -120,0 +128,1 @@\n+  case T_PRIMITIVE_OBJECT:\n@@ -150,1 +159,0 @@\n-\n@@ -174,0 +182,6 @@\n+    if (tp->is_inlinetypeptr() && !tp->maybe_null()) {\n+      \/\/ Check inline types for null here to prevent checkcast from adding an\n+      \/\/ exception state before the bytecode entry (use 'bad_type_ctrl' instead).\n+      l = null_check_oop(l, &bad_type_ctrl);\n+      bad_type_exit->control()->add_req(bad_type_ctrl);\n+    }\n@@ -190,1 +204,0 @@\n-\n@@ -228,1 +241,0 @@\n-\n@@ -232,1 +244,1 @@\n-    Node *lock_object = fetch_interpreter_state(index*2, T_OBJECT, monitors_addr, osr_buf);\n+    Node* lock_object = fetch_interpreter_state(index*2, Type::get_const_basic_type(T_OBJECT), monitors_addr, osr_buf);\n@@ -234,2 +246,1 @@\n-    Node *displaced_hdr = fetch_interpreter_state((index*2) + 1, T_ADDRESS, monitors_addr, osr_buf);\n-\n+    Node* displaced_hdr = fetch_interpreter_state((index*2) + 1, Type::get_const_basic_type(T_ADDRESS), monitors_addr, osr_buf);\n@@ -302,7 +313,1 @@\n-    BasicType bt = type->basic_type();\n-    if (type == TypePtr::NULL_PTR) {\n-      \/\/ Ptr types are mixed together with T_ADDRESS but NULL is\n-      \/\/ really for T_OBJECT types so correct it.\n-      bt = T_OBJECT;\n-    }\n-    Node *value = fetch_interpreter_state(index, bt, locals_addr, osr_buf);\n+    Node* value = fetch_interpreter_state(index, type, locals_addr, osr_buf);\n@@ -597,0 +602,21 @@\n+  \/\/ Handle inline type arguments\n+  int arg_size = method()->arg_size();\n+  for (int i = 0; i < arg_size; i++) {\n+    Node* parm = local(i);\n+    const Type* t = _gvn.type(parm);\n+    if (t->is_inlinetypeptr()) {\n+      \/\/ Create InlineTypeNode from the oop and replace the parameter\n+      Node* vt = InlineTypeNode::make_from_oop(this, parm, t->inline_klass(), !t->maybe_null());\n+      set_local(i, vt);\n+    } else if (UseTypeSpeculation && (i == (arg_size - 1)) && !is_osr_parse() && method()->has_vararg() &&\n+               t->isa_aryptr() != NULL && !t->is_aryptr()->is_null_free() && !t->is_aryptr()->is_not_null_free()) {\n+      \/\/ Speculate on varargs Object array being not null-free (and therefore also not flattened)\n+      const TypePtr* spec_type = t->speculative();\n+      spec_type = (spec_type != NULL && spec_type->isa_aryptr() != NULL) ? spec_type : t->is_aryptr();\n+      spec_type = spec_type->remove_speculative()->is_aryptr()->cast_to_not_null_free();\n+      spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, spec_type);\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), parm, t->join_speculative(spec_type)));\n+      set_local(i, cast);\n+    }\n+  }\n+\n@@ -780,2 +806,2 @@\n-  if (tf()->range()->cnt() > TypeFunc::Parms) {\n-    const Type* ret_type = tf()->range()->field_at(TypeFunc::Parms);\n+  if (tf()->range_sig()->cnt() > TypeFunc::Parms) {\n+    const Type* ret_type = tf()->range_sig()->field_at(TypeFunc::Parms);\n@@ -803,1 +829,1 @@\n-    assert((int)(tf()->range()->cnt() - TypeFunc::Parms) == ret_size, \"good tf range\");\n+    assert((int)(tf()->range_sig()->cnt() - TypeFunc::Parms) == ret_size, \"good tf range\");\n@@ -810,1 +836,0 @@\n-\n@@ -815,2 +840,2 @@\n-  int        arg_size = tf->domain()->cnt();\n-  int        max_size = MAX2(arg_size, (int)tf->range()->cnt());\n+  int        arg_size = tf->domain_sig()->cnt();\n+  int        max_size = MAX2(arg_size, (int)tf->range_cc()->cnt());\n@@ -819,0 +844,1 @@\n+  jvms->set_map(map);\n@@ -830,3 +856,20 @@\n-  uint i;\n-  for (i = 0; i < (uint)arg_size; i++) {\n-    Node* parm = initial_gvn()->transform(new ParmNode(start, i));\n+  PhaseGVN& gvn = *initial_gvn();\n+  uint i = 0;\n+  int arg_num = 0;\n+  for (uint j = 0; i < (uint)arg_size; i++) {\n+    const Type* t = tf->domain_sig()->field_at(i);\n+    Node* parm = NULL;\n+    if (t->is_inlinetypeptr() && method()->is_scalarized_arg(arg_num)) {\n+      \/\/ Inline type arguments are not passed by reference: we get an argument per\n+      \/\/ field of the inline type. Build InlineTypeNodes from the inline type arguments.\n+      GraphKit kit(jvms, &gvn);\n+      kit.set_control(map->control());\n+      Node* old_mem = map->memory();\n+      \/\/ Use immutable memory for inline type loads and restore it below\n+      kit.set_all_memory(C->immutable_memory());\n+      parm = InlineTypeNode::make_from_multi(&kit, start, t->inline_klass(), j, \/* in= *\/ true, \/* null_free= *\/ !t->maybe_null());\n+      map->set_control(kit.control());\n+      map->set_memory(old_mem);\n+    } else {\n+      parm = gvn.transform(new ParmNode(start, j++));\n+    }\n@@ -836,0 +879,3 @@\n+    if (i >= TypeFunc::Parms && t != Type::HALF) {\n+      arg_num++;\n+    }\n@@ -842,1 +888,0 @@\n-  jvms->set_map(map);\n@@ -869,1 +914,1 @@\n-  int ret_size = tf()->range()->cnt() - TypeFunc::Parms;\n+  int ret_size = tf()->range_sig()->cnt() - TypeFunc::Parms;\n@@ -873,2 +918,26 @@\n-    ret->add_req(kit.argument(0));\n-    \/\/ Note:  The second dummy edge is not needed by a ReturnNode.\n+    Node* res = kit.argument(0);\n+    if (tf()->returns_inline_type_as_fields()) {\n+      \/\/ Multiple return values (inline type fields): add as many edges\n+      \/\/ to the Return node as returned values.\n+      InlineTypeNode* vt = res->as_InlineType();\n+      ret->add_req_batch(NULL, tf()->range_cc()->cnt() - TypeFunc::Parms);\n+      if (vt->is_allocated(&kit.gvn()) && !StressCallingConvention) {\n+        ret->init_req(TypeFunc::Parms, vt->get_oop());\n+      } else {\n+        \/\/ Return the tagged klass pointer to signal scalarization to the caller\n+        Node* tagged_klass = vt->tagged_klass(kit.gvn());\n+        if (!method()->signature()->returns_null_free_inline_type()) {\n+          \/\/ Return null if the inline type is null (IsInit field is not set)\n+          Node* conv   = kit.gvn().transform(new ConvI2LNode(vt->get_is_init()));\n+          Node* shl    = kit.gvn().transform(new LShiftLNode(conv, kit.intcon(63)));\n+          Node* shr    = kit.gvn().transform(new RShiftLNode(shl, kit.intcon(63)));\n+          tagged_klass = kit.gvn().transform(new AndLNode(tagged_klass, shr));\n+        }\n+        ret->init_req(TypeFunc::Parms, tagged_klass);\n+      }\n+      uint idx = TypeFunc::Parms + 1;\n+      vt->pass_fields(&kit, ret, idx, false, method()->signature()->returns_null_free_inline_type());\n+    } else {\n+      ret->add_req(res);\n+      \/\/ Note:  The second dummy edge is not needed by a ReturnNode.\n+    }\n@@ -998,1 +1067,1 @@\n-  if (method()->is_initializer() &&\n+  if (method()->is_object_constructor_or_class_initializer() &&\n@@ -1036,2 +1105,2 @@\n-  if (tf()->range()->cnt() > TypeFunc::Parms) {\n-    const Type* ret_type = tf()->range()->field_at(TypeFunc::Parms);\n+  if (tf()->range_sig()->cnt() > TypeFunc::Parms) {\n+    const Type* ret_type = tf()->range_sig()->field_at(TypeFunc::Parms);\n@@ -1130,1 +1199,1 @@\n-    kit.null_check_receiver_before_call(method());\n+    kit.null_check_receiver_before_call(method(), false);\n@@ -1168,1 +1237,1 @@\n-  uint arg_size = tf()->domain()->cnt();\n+  uint arg_size = tf()->domain_sig()->cnt();\n@@ -1242,0 +1311,1 @@\n+      assert(!_gvn.type(lock_obj)->make_oopptr()->can_be_inline_type(), \"can't be an inline type\");\n@@ -1658,0 +1728,36 @@\n+  \/\/ Check for merge conflicts involving inline types\n+  JVMState* old_jvms = map()->jvms();\n+  int old_bci = bci();\n+  JVMState* tmp_jvms = old_jvms->clone_shallow(C);\n+  tmp_jvms->set_should_reexecute(true);\n+  tmp_jvms->bind_map(map());\n+  \/\/ Execution needs to restart a the next bytecode (entry of next\n+  \/\/ block)\n+  if (target->is_merged() ||\n+      pnum > PhiNode::Input ||\n+      target->is_handler() ||\n+      target->is_loop_head()) {\n+    set_parse_bci(target->start());\n+    for (uint j = TypeFunc::Parms; j < map()->req(); j++) {\n+      Node* n = map()->in(j);                 \/\/ Incoming change to target state.\n+      const Type* t = NULL;\n+      if (tmp_jvms->is_loc(j)) {\n+        t = target->local_type_at(j - tmp_jvms->locoff());\n+      } else if (tmp_jvms->is_stk(j) && j < (uint)sp() + tmp_jvms->stkoff()) {\n+        t = target->stack_type_at(j - tmp_jvms->stkoff());\n+      }\n+      if (t != NULL && t != Type::BOTTOM) {\n+        if (n->is_InlineType() && !t->is_inlinetypeptr()) {\n+          \/\/ Allocate inline type in src block to be able to merge it with oop in target block\n+          map()->set_req(j, n->as_InlineType()->buffer(this));\n+        } else if (!n->is_InlineType() && t->is_inlinetypeptr()) {\n+          \/\/ Scalarize null in src block to be able to merge it with inline type in target block\n+          assert(gvn().type(n)->is_zero_type(), \"Should have been scalarized\");\n+          map()->set_req(j, InlineTypeNode::make_null(gvn(), t->inline_klass()));\n+        }\n+      }\n+    }\n+  }\n+  old_jvms->bind_map(map());\n+  set_parse_bci(old_bci);\n+\n@@ -1712,0 +1818,1 @@\n+\n@@ -1747,0 +1854,1 @@\n+    bool last_merge = (pnum == PhiNode::Input);\n@@ -1751,1 +1859,1 @@\n-      if (m->is_Phi() && m->as_Phi()->region() == r)\n+      if (m->is_Phi() && m->as_Phi()->region() == r) {\n@@ -1753,1 +1861,3 @@\n-      else\n+      } else if (m->is_InlineType() && m->as_InlineType()->has_phi_inputs(r)) {\n+        phi = m->as_InlineType()->get_oop()->as_Phi();\n+      } else {\n@@ -1755,0 +1865,1 @@\n+      }\n@@ -1788,1 +1899,24 @@\n-      if (phi != NULL) {\n+      \/\/ Merging two inline types?\n+      if (phi != NULL && phi->bottom_type()->is_inlinetypeptr()) {\n+        \/\/ Reload current state because it may have been updated by ensure_phi\n+        m = map()->in(j);\n+        InlineTypeNode* vtm = m->as_InlineType(); \/\/ Current inline type\n+        InlineTypeNode* vtn = n->as_InlineType(); \/\/ Incoming inline type\n+        assert(vtm->get_oop() == phi, \"Inline type should have Phi input\");\n+        if (TraceOptoParse) {\n+#ifdef ASSERT\n+          tty->print_cr(\"\\nMerging inline types\");\n+          tty->print_cr(\"Current:\");\n+          vtm->dump(2);\n+          tty->print_cr(\"Incoming:\");\n+          vtn->dump(2);\n+          tty->cr();\n+#endif\n+        }\n+        \/\/ Do the merge\n+        vtm->merge_with(&_gvn, vtn, pnum, last_merge);\n+        if (last_merge) {\n+          map()->set_req(j, _gvn.transform_no_reclaim(vtm));\n+          record_for_igvn(vtm);\n+        }\n+      } else if (phi != NULL) {\n@@ -1792,1 +1926,1 @@\n-        if (pnum == PhiNode::Input) {\n+        if (last_merge) {\n@@ -1808,2 +1942,1 @@\n-    if (pnum == PhiNode::Input &&\n-        !r->in(0)) {         \/\/ The occasional useless Region\n+    if (last_merge && !r->in(0)) {         \/\/ The occasional useless Region\n@@ -1961,0 +2094,2 @@\n+      } else if (n->is_InlineType() && n->as_InlineType()->has_phi_inputs(r)) {\n+        n->as_InlineType()->add_new_path(r);\n@@ -1983,0 +2118,4 @@\n+  InlineTypeNode* vt = o->isa_InlineType();\n+  if (vt != NULL && vt->has_phi_inputs(region)) {\n+    return vt->get_oop()->as_Phi();\n+  }\n@@ -2002,2 +2141,2 @@\n-  \/\/ is mixing ints and oops or some such.  Forcing it to top\n-  \/\/ makes it go dead.\n+  \/\/ is already dead or is mixing ints and oops or some such.\n+  \/\/ Forcing it to top makes it go dead.\n@@ -2016,5 +2155,14 @@\n-  PhiNode* phi = PhiNode::make(region, o, t);\n-  gvn().set_type(phi, t);\n-  if (C->do_escape_analysis()) record_for_igvn(phi);\n-  map->set_req(idx, phi);\n-  return phi;\n+  if (vt != NULL && t->is_inlinetypeptr()) {\n+    \/\/ Inline types are merged by merging their field values.\n+    \/\/ Create a cloned InlineTypeNode with phi inputs that\n+    \/\/ represents the merged inline type and update the map.\n+    vt = vt->clone_with_phis(&_gvn, region);\n+    map->set_req(idx, vt);\n+    return vt->get_oop()->as_Phi();\n+  } else {\n+    PhiNode* phi = PhiNode::make(region, o, t);\n+    gvn().set_type(phi, t);\n+    if (C->do_escape_analysis()) record_for_igvn(phi);\n+    map->set_req(idx, phi);\n+    return phi;\n+  }\n@@ -2188,1 +2336,4 @@\n-  set_bci(InvocationEntryBci);\n+  \/\/ vreturn can trigger an allocation so vreturn can throw. Setting\n+  \/\/ the bci here breaks exception handling. Commenting this out\n+  \/\/ doesn't seem to break anything.\n+  \/\/  set_bci(InvocationEntryBci);\n@@ -2195,0 +2346,34 @@\n+  \/\/ frame pointer is always same, already captured\n+  if (value != NULL) {\n+    Node* phi = _exits.argument(0);\n+    const Type* return_type = phi->bottom_type();\n+    const TypeInstPtr* tr = return_type->isa_instptr();\n+    if ((tf()->returns_inline_type_as_fields() || (_caller->has_method() && !Compile::current()->inlining_incrementally())) &&\n+        return_type->is_inlinetypeptr()) {\n+      \/\/ Inline type is returned as fields, make sure it is scalarized\n+      if (!value->is_InlineType()) {\n+        value = InlineTypeNode::make_from_oop(this, value, return_type->inline_klass(), method()->signature()->returns_null_free_inline_type());\n+      }\n+      if (!_caller->has_method() || Compile::current()->inlining_incrementally()) {\n+        \/\/ Returning from root or an incrementally inlined method. Make sure all non-flattened\n+        \/\/ fields are buffered and re-execute if allocation triggers deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        assert(tf()->returns_inline_type_as_fields(), \"must be returned as fields\");\n+        jvms()->set_should_reexecute(true);\n+        inc_sp(1);\n+        value = value->as_InlineType()->allocate_fields(this);\n+      }\n+    } else if (value->is_InlineType()) {\n+      \/\/ Inline type is returned as oop, make sure it is buffered and re-execute\n+      \/\/ if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      inc_sp(1);\n+      value = value->as_InlineType()->buffer(this);\n+    }\n+    \/\/ ...else\n+    \/\/ If returning oops to an interface-return, there is a silent free\n+    \/\/ cast from oop to interface allowed by the Verifier. Make it explicit here.\n+    phi->add_req(value);\n+  }\n+\n@@ -2212,9 +2397,0 @@\n-  \/\/ frame pointer is always same, already captured\n-  if (value != NULL) {\n-    \/\/ If returning oops to an interface-return, there is a silent free\n-    \/\/ cast from oop to interface allowed by the Verifier.  Make it explicit\n-    \/\/ here.\n-    Node* phi = _exits.argument(0);\n-    phi->add_req(value);\n-  }\n-\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":234,"deletions":58,"binary":false,"changes":292,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciSymbols.hpp\"\n@@ -39,0 +40,2 @@\n+#include \"opto\/idealKit.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -53,0 +56,17 @@\n+Node* Parse::record_profile_for_speculation_at_array_load(Node* ld) {\n+  \/\/ Feed unused profile data to type speculation\n+  if (UseTypeSpeculation && UseArrayLoadStoreProfile) {\n+    ciKlass* array_type = NULL;\n+    ciKlass* element_type = NULL;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool flat_array = true;\n+    bool null_free_array = true;\n+    method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+    if (element_type != NULL || element_ptr != ProfileMaybeNull) {\n+      ld = record_profile_for_speculation(ld, element_type, element_ptr);\n+    }\n+  }\n+  return ld;\n+}\n+\n+\n@@ -56,1 +76,0 @@\n-  bool big_val = bt == T_DOUBLE || bt == T_LONG;\n@@ -60,2 +79,92 @@\n-  pop();                      \/\/ index (already used)\n-  Node* array = pop();        \/\/ the array itself\n+  Node* idx = pop();\n+  Node* ary = pop();\n+\n+  \/\/ Handle inline type arrays\n+  const TypeOopPtr* elemptr = elemtype->make_oopptr();\n+  const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+  if (ary_t->is_flat()) {\n+    \/\/ Load from flattened inline type array\n+    Node* vt = InlineTypeNode::make_from_flattened(this, elemtype->inline_klass(), ary, adr);\n+    push(vt);\n+    return;\n+  } else if (ary_t->is_null_free()) {\n+    \/\/ Load from non-flattened inline type array (elements can never be null)\n+    bt = T_PRIMITIVE_OBJECT;\n+  } else if (!ary_t->is_not_flat()) {\n+    \/\/ Cannot statically determine if array is flattened, emit runtime check\n+    assert(UseFlatArray && is_reference_type(bt) && elemptr->can_be_inline_type() && !ary_t->klass_is_exact() && !ary_t->is_not_null_free() &&\n+           (!elemptr->is_inlinetypeptr() || elemptr->inline_klass()->flatten_array()), \"array can't be flattened\");\n+    IdealKit ideal(this);\n+    IdealVariable res(ideal);\n+    ideal.declarations_done();\n+    ideal.if_then(flat_array_test(ary, \/* flat = *\/ false)); {\n+      \/\/ non-flattened\n+      assert(ideal.ctrl()->in(0)->as_If()->is_flat_array_check(&_gvn), \"Should be found\");\n+      sync_kit(ideal);\n+      const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n+      Node* ld = access_load_at(ary, adr, adr_type, elemptr, bt,\n+                                IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD);\n+      if (elemptr->is_inlinetypeptr()) {\n+        assert(elemptr->maybe_null(), \"null free array should be handled above\");\n+        ld = InlineTypeNode::make_from_oop(this, ld, elemptr->inline_klass(), false);\n+      }\n+      ideal.sync_kit(this);\n+      ideal.set(res, ld);\n+    } ideal.else_(); {\n+      \/\/ flattened\n+      sync_kit(ideal);\n+      if (elemptr->is_inlinetypeptr()) {\n+        \/\/ Element type is known, cast and load from flattened representation\n+        ciInlineKlass* vk = elemptr->inline_klass();\n+        assert(vk->flatten_array() && elemptr->maybe_null(), \"never\/always flat - should be optimized\");\n+        ciArrayKlass* array_klass = ciArrayKlass::make(vk, \/* null_free *\/ true);\n+        const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+        Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, arytype));\n+        Node* casted_adr = array_element_address(cast, idx, T_PRIMITIVE_OBJECT, ary_t->size(), control());\n+        \/\/ Re-execute flattened array load if buffering triggers deoptimization\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        inc_sp(2);\n+        Node* vt = InlineTypeNode::make_from_flattened(this, vk, cast, casted_adr)->buffer(this, false);\n+        ideal.set(res, vt);\n+        ideal.sync_kit(this);\n+      } else {\n+        \/\/ Element type is unknown, emit runtime call\n+\n+        \/\/ Below membars keep this access to an unknown flattened array correctly\n+        \/\/ ordered with other unknown and known flattened array accesses.\n+        insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+        Node* call = NULL;\n+        {\n+          \/\/ Re-execute flattened array load if runtime call triggers deoptimization\n+          PreserveReexecuteState preexecs(this);\n+          jvms()->set_bci(_bci);\n+          jvms()->set_should_reexecute(true);\n+          inc_sp(2);\n+          kill_dead_locals();\n+          call = make_runtime_call(RC_NO_LEAF | RC_NO_IO,\n+                                   OptoRuntime::load_unknown_inline_type(),\n+                                   OptoRuntime::load_unknown_inline_Java(),\n+                                   NULL, TypeRawPtr::BOTTOM,\n+                                   ary, idx);\n+        }\n+        make_slow_call_ex(call, env()->Throwable_klass(), false);\n+        Node* buffer = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+\n+        insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+        \/\/ Keep track of the information that the inline type is flattened in arrays\n+        const Type* unknown_value = elemptr->is_instptr()->cast_to_flatten_array();\n+        buffer = _gvn.transform(new CheckCastPPNode(control(), buffer, unknown_value));\n+\n+        ideal.sync_kit(this);\n+        ideal.set(res, buffer);\n+      }\n+    } ideal.end_if();\n+    sync_kit(ideal);\n+    Node* ld = _gvn.transform(ideal.value(res));\n+    ld = record_profile_for_speculation_at_array_load(ld);\n+    push_node(bt, ld);\n+    return;\n+  }\n@@ -67,2 +176,1 @@\n-\n-  Node* ld = access_load_at(array, adr, adr_type, elemtype, bt,\n+  Node* ld = access_load_at(ary, adr, adr_type, elemtype, bt,\n@@ -70,4 +178,5 @@\n-  if (big_val) {\n-    push_pair(ld);\n-  } else {\n-    push(ld);\n+  ld = record_profile_for_speculation_at_array_load(ld);\n+  \/\/ Loading a non-flattened inline type\n+  if (elemptr != NULL && elemptr->is_inlinetypeptr()) {\n+    assert(!ary_t->is_null_free() || !elemptr->maybe_null(), \"inline type array elements should never be null\");\n+    ld = InlineTypeNode::make_from_oop(this, ld, elemptr->inline_klass(), !elemptr->maybe_null());\n@@ -75,0 +184,1 @@\n+  push_node(bt, ld);\n@@ -81,2 +191,1 @@\n-  bool big_val = bt == T_DOUBLE || bt == T_LONG;\n-  Node* adr = array_addressing(bt, big_val ? 2 : 1, elemtype);\n+  Node* adr = array_addressing(bt, type2size[bt], elemtype);\n@@ -84,0 +193,1 @@\n+  Node* cast_val = NULL;\n@@ -85,10 +195,2 @@\n-    array_store_check();\n-    if (stopped()) {\n-      return;\n-    }\n-  }\n-  Node* val;                  \/\/ Oop to store\n-  if (big_val) {\n-    val = pop_pair();\n-  } else {\n-    val = pop();\n+    cast_val = array_store_check(adr, elemtype);\n+    if (stopped()) return;\n@@ -96,2 +198,6 @@\n-  pop();                      \/\/ index (already used)\n-  Node* array = pop();        \/\/ the array itself\n+  Node* val = pop_node(bt); \/\/ Value to store\n+  Node* idx = pop();        \/\/ Index in the array\n+  Node* ary = pop();        \/\/ The array itself\n+\n+  const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+  const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n@@ -101,0 +207,113 @@\n+  } else if (bt == T_OBJECT) {\n+    elemtype = elemtype->make_oopptr();\n+    const Type* tval = _gvn.type(cast_val);\n+    \/\/ Based on the value to be stored, try to determine if the array is not null-free and\/or not flat.\n+    \/\/ This is only legal for non-null stores because the array_store_check always passes for null, even\n+    \/\/ if the array is null-free. Null stores are handled in GraphKit::gen_inline_array_null_guard().\n+    bool not_null_free = !tval->maybe_null() && !tval->is_oopptr()->can_be_inline_type();\n+    bool not_flattened = not_null_free || (tval->is_inlinetypeptr() && !tval->inline_klass()->flatten_array());\n+    if (!ary_t->is_not_null_free() && not_null_free) {\n+      \/\/ Storing a non-inline type, mark array as not null-free (-> not flat).\n+      ary_t = ary_t->cast_to_not_null_free();\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+      replace_in_map(ary, cast);\n+      ary = cast;\n+    } else if (!ary_t->is_not_flat() && not_flattened) {\n+      \/\/ Storing a non-flattened value, mark array as not flat.\n+      ary_t = ary_t->cast_to_not_flat();\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+      replace_in_map(ary, cast);\n+      ary = cast;\n+    }\n+\n+    if (ary_t->is_flat()) {\n+      \/\/ Store to flattened inline type array\n+      assert(!tval->maybe_null(), \"should be guaranteed by array store check\");\n+      \/\/ Re-execute flattened array store if buffering triggers deoptimization\n+      PreserveReexecuteState preexecs(this);\n+      inc_sp(3);\n+      jvms()->set_should_reexecute(true);\n+      cast_val->as_InlineType()->store_flattened(this, ary, adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+      return;\n+    } else if (ary_t->is_null_free()) {\n+      \/\/ Store to non-flattened inline type array (elements can never be null)\n+      assert(!tval->maybe_null(), \"should be guaranteed by array store check\");\n+      if (elemtype->inline_klass()->is_empty()) {\n+        \/\/ Ignore empty inline stores, array is already initialized.\n+        return;\n+      }\n+    } else if (!ary_t->is_not_flat() && (tval != TypePtr::NULL_PTR || StressReflectiveCode)) {\n+      \/\/ Array might be flattened, emit runtime checks (for NULL, a simple inline_array_null_guard is sufficient).\n+      assert(UseFlatArray && !not_flattened && elemtype->is_oopptr()->can_be_inline_type() &&\n+             !ary_t->klass_is_exact() && !ary_t->is_not_null_free(), \"array can't be flattened\");\n+      IdealKit ideal(this);\n+      ideal.if_then(flat_array_test(ary, \/* flat = *\/ false)); {\n+        \/\/ non-flattened\n+        assert(ideal.ctrl()->in(0)->as_If()->is_flat_array_check(&_gvn), \"Should be found\");\n+        sync_kit(ideal);\n+        Node* cast_ary = inline_array_null_guard(ary, cast_val, 3);\n+        inc_sp(3);\n+        access_store_at(cast_ary, adr, adr_type, cast_val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY, false);\n+        dec_sp(3);\n+        ideal.sync_kit(this);\n+      } ideal.else_(); {\n+        sync_kit(ideal);\n+        \/\/ flattened\n+        Node* null_ctl = top();\n+        Node* val = null_check_oop(cast_val, &null_ctl);\n+        if (null_ctl != top()) {\n+          PreserveJVMState pjvms(this);\n+          inc_sp(3);\n+          set_control(null_ctl);\n+          uncommon_trap(Deoptimization::Reason_null_check, Deoptimization::Action_none);\n+          dec_sp(3);\n+        }\n+        \/\/ Try to determine the inline klass\n+        ciInlineKlass* vk = NULL;\n+        if (tval->is_inlinetypeptr()) {\n+          vk = tval->inline_klass();\n+        } else if (elemtype->is_inlinetypeptr()) {\n+          vk = elemtype->inline_klass();\n+        }\n+        Node* casted_ary = ary;\n+        if (vk != NULL && !stopped()) {\n+          \/\/ Element type is known, cast and store to flattened representation\n+          assert(vk->flatten_array() && elemtype->maybe_null(), \"never\/always flat - should be optimized\");\n+          ciArrayKlass* array_klass = ciArrayKlass::make(vk, \/* null_free *\/ true);\n+          const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+          casted_ary = _gvn.transform(new CheckCastPPNode(control(), casted_ary, arytype));\n+          Node* casted_adr = array_element_address(casted_ary, idx, T_OBJECT, arytype->size(), control());\n+          if (!val->is_InlineType()) {\n+            assert(!gvn().type(val)->maybe_null(), \"inline type array elements should never be null\");\n+            val = InlineTypeNode::make_from_oop(this, val, vk);\n+          }\n+          \/\/ Re-execute flattened array store if buffering triggers deoptimization\n+          PreserveReexecuteState preexecs(this);\n+          inc_sp(3);\n+          jvms()->set_should_reexecute(true);\n+          val->as_InlineType()->store_flattened(this, casted_ary, casted_adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+        } else if (!stopped()) {\n+          \/\/ Element type is unknown, emit runtime call\n+\n+          \/\/ Below membars keep this access to an unknown flattened array correctly\n+          \/\/ ordered with other unknown and known flattened array accesses.\n+          insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+          make_runtime_call(RC_LEAF,\n+                            OptoRuntime::store_unknown_inline_type(),\n+                            CAST_FROM_FN_PTR(address, OptoRuntime::store_unknown_inline),\n+                            \"store_unknown_inline\", TypeRawPtr::BOTTOM,\n+                            val, casted_ary, idx);\n+\n+          insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+        }\n+        ideal.sync_kit(this);\n+      }\n+      ideal.end_if();\n+      sync_kit(ideal);\n+      return;\n+    } else if (!ary_t->is_not_null_free()) {\n+      \/\/ Array is not flattened but may be null free\n+      assert(elemtype->is_oopptr()->can_be_inline_type() && !ary_t->klass_is_exact(), \"array can't be null-free\");\n+      ary = inline_array_null_guard(ary, cast_val, 3, true);\n+    }\n@@ -102,3 +321,3 @@\n-  const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n-\n-  access_store_at(array, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+  inc_sp(3);\n+  access_store_at(ary, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+  dec_sp(3);\n@@ -205,0 +424,116 @@\n+  \/\/ This could be an access to an inline type array. We can't tell if it's\n+  \/\/ flat or not. Knowing the exact type avoids runtime checks and leads to\n+  \/\/ a much simpler graph shape. Check profile information.\n+  if (!arytype->is_flat() && !arytype->is_not_flat()) {\n+    \/\/ First check the speculative type\n+    Deoptimization::DeoptReason reason = Deoptimization::Reason_speculate_class_check;\n+    ciKlass* array_type = arytype->speculative_type();\n+    if (too_many_traps_or_recompiles(reason) || array_type == NULL) {\n+      \/\/ No speculative type, check profile data at this bci\n+      array_type = NULL;\n+      reason = Deoptimization::Reason_class_check;\n+      if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(reason)) {\n+        ciKlass* element_type = NULL;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      }\n+    }\n+    if (array_type != NULL) {\n+      \/\/ Speculate that this array has the exact type reported by profile data\n+      Node* better_ary = NULL;\n+      DEBUG_ONLY(Node* old_control = control();)\n+      Node* slow_ctl = type_check_receiver(ary, array_type, 1.0, &better_ary);\n+      if (stopped()) {\n+        \/\/ The check always fails and therefore profile information is incorrect. Don't use it.\n+        assert(old_control == slow_ctl, \"type check should have been removed\");\n+        set_control(slow_ctl);\n+      } else if (!slow_ctl->is_top()) {\n+        { PreserveJVMState pjvms(this);\n+          set_control(slow_ctl);\n+          uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+        }\n+        replace_in_map(ary, better_ary);\n+        ary = better_ary;\n+        arytype  = _gvn.type(ary)->is_aryptr();\n+        elemtype = arytype->elem();\n+      }\n+    }\n+  } else if (UseTypeSpeculation && UseArrayLoadStoreProfile) {\n+    \/\/ No need to speculate: feed profile data at this bci for the\n+    \/\/ array to type speculation\n+    ciKlass* array_type = NULL;\n+    ciKlass* element_type = NULL;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool flat_array = true;\n+    bool null_free_array = true;\n+    method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+    if (array_type != NULL) {\n+      ary = record_profile_for_speculation(ary, array_type, ProfileMaybeNull);\n+    }\n+  }\n+\n+  \/\/ We have no exact array type from profile data. Check profile data\n+  \/\/ for a non null-free or non flat array. Non null-free implies non\n+  \/\/ flat so check this one first. Speculating on a non null-free\n+  \/\/ array doesn't help aaload but could be profitable for a\n+  \/\/ subsequent aastore.\n+  if (!arytype->is_null_free() && !arytype->is_not_null_free()) {\n+    bool null_free_array = true;\n+    Deoptimization::DeoptReason reason = Deoptimization::Reason_none;\n+    if (arytype->speculative() != NULL &&\n+        arytype->speculative()->is_aryptr()->is_not_null_free() &&\n+        !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+      null_free_array = false;\n+      reason = Deoptimization::Reason_speculate_class_check;\n+    } else if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {\n+      ciKlass* array_type = NULL;\n+      ciKlass* element_type = NULL;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      reason = Deoptimization::Reason_class_check;\n+    }\n+    if (!null_free_array) {\n+      { \/\/ Deoptimize if null-free array\n+        BuildCutout unless(this, null_free_array_test(load_object_klass(ary), \/* null_free = *\/ false), PROB_MAX);\n+        uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+      }\n+      assert(!stopped(), \"null-free array should have been caught earlier\");\n+      Node* better_ary = _gvn.transform(new CheckCastPPNode(control(), ary, arytype->cast_to_not_null_free()));\n+      replace_in_map(ary, better_ary);\n+      ary = better_ary;\n+      arytype = _gvn.type(ary)->is_aryptr();\n+    }\n+  }\n+\n+  if (!arytype->is_flat() && !arytype->is_not_flat()) {\n+    bool flat_array = true;\n+    Deoptimization::DeoptReason reason = Deoptimization::Reason_none;\n+    if (arytype->speculative() != NULL &&\n+        arytype->speculative()->is_aryptr()->is_not_flat() &&\n+        !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+      flat_array = false;\n+      reason = Deoptimization::Reason_speculate_class_check;\n+    } else if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(reason)) {\n+      ciKlass* array_type = NULL;\n+      ciKlass* element_type = NULL;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      reason = Deoptimization::Reason_class_check;\n+    }\n+    if (!flat_array) {\n+      { \/\/ Deoptimize if flat array\n+        BuildCutout unless(this, flat_array_test(ary, \/* flat = *\/ false), PROB_MAX);\n+        uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+      }\n+      assert(!stopped(), \"flat array should have been caught earlier\");\n+      Node* better_ary = _gvn.transform(new CheckCastPPNode(control(), ary, arytype->cast_to_not_flat()));\n+      replace_in_map(ary, better_ary);\n+      ary = better_ary;\n+      arytype = _gvn.type(ary)->is_aryptr();\n+    }\n+  }\n+\n@@ -1436,1 +1771,1 @@\n-void Parse::do_if(BoolTest::mask btest, Node* c) {\n+void Parse::do_if(BoolTest::mask btest, Node* c, bool new_path, Node** ctrl_taken) {\n@@ -1520,2 +1855,2 @@\n-      if (C->eliminate_boxing()) {\n-        \/\/ Mark the successor block as parsed\n+      if (C->eliminate_boxing() && !new_path) {\n+        \/\/ Mark the successor block as parsed (if we haven't created a new path)\n@@ -1527,1 +1862,9 @@\n-        merge(target_bci);\n+        if (new_path) {\n+          \/\/ Merge by using a new path\n+          merge_new_path(target_bci);\n+        } else if (ctrl_taken != NULL) {\n+          \/\/ Don't merge but save taken branch to be wired by caller\n+          *ctrl_taken = control();\n+        } else {\n+          merge(target_bci);\n+        }\n@@ -1536,1 +1879,1 @@\n-  if (stopped()) {\n+  if (stopped() && ctrl_taken == NULL) {\n@@ -1538,1 +1881,1 @@\n-      \/\/ Mark the successor block as parsed\n+      \/\/ Mark the successor block as parsed (if caller does not re-wire control flow)\n@@ -1546,0 +1889,392 @@\n+\n+static ProfilePtrKind speculative_ptr_kind(const TypeOopPtr* t) {\n+  if (t->speculative() == NULL) {\n+    return ProfileUnknownNull;\n+  }\n+  if (t->speculative_always_null()) {\n+    return ProfileAlwaysNull;\n+  }\n+  if (t->speculative_maybe_null()) {\n+    return ProfileMaybeNull;\n+  }\n+  return ProfileNeverNull;\n+}\n+\n+void Parse::acmp_always_null_input(Node* input, const TypeOopPtr* tinput, BoolTest::mask btest, Node* eq_region) {\n+  inc_sp(2);\n+  Node* cast = null_check_common(input, T_OBJECT, true, NULL,\n+                                 !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check) &&\n+                                 speculative_ptr_kind(tinput) == ProfileAlwaysNull);\n+  dec_sp(2);\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      replace_in_map(input, cast);\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    replace_in_map(input, cast);\n+  }\n+}\n+\n+Node* Parse::acmp_null_check(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, Node*& null_ctl) {\n+  inc_sp(2);\n+  null_ctl = top();\n+  Node* cast = null_check_oop(input, &null_ctl,\n+                              input_ptr == ProfileNeverNull || (input_ptr == ProfileUnknownNull && !too_many_traps_or_recompiles(Deoptimization::Reason_null_check)),\n+                              false,\n+                              speculative_ptr_kind(tinput) == ProfileNeverNull &&\n+                              !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check));\n+  dec_sp(2);\n+  assert(!stopped(), \"null input should have been caught earlier\");\n+  return cast;\n+}\n+\n+void Parse::acmp_known_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, ciKlass* input_type, BoolTest::mask btest, Node* eq_region) {\n+  Node* ne_region = new RegionNode(1);\n+  Node* null_ctl;\n+  Node* cast = acmp_null_check(input, tinput, input_ptr, null_ctl);\n+  ne_region->add_req(null_ctl);\n+\n+  Node* slow_ctl = type_check_receiver(cast, input_type, 1.0, &cast);\n+  {\n+    PreserveJVMState pjvms(this);\n+    inc_sp(2);\n+    set_control(slow_ctl);\n+    Deoptimization::DeoptReason reason;\n+    if (tinput->speculative_type() != NULL && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+      reason = Deoptimization::Reason_speculate_class_check;\n+    } else {\n+      reason = Deoptimization::Reason_class_check;\n+    }\n+    uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+  }\n+  ne_region->add_req(control());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      if (null_ctl == top()) {\n+        replace_in_map(input, cast);\n+      }\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    if (null_ctl == top()) {\n+      replace_in_map(input, cast);\n+    }\n+    set_control(_gvn.transform(ne_region));\n+  }\n+}\n+\n+void Parse::acmp_unknown_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, BoolTest::mask btest, Node* eq_region) {\n+  Node* ne_region = new RegionNode(1);\n+  Node* null_ctl;\n+  Node* cast = acmp_null_check(input, tinput, input_ptr, null_ctl);\n+  ne_region->add_req(null_ctl);\n+\n+  {\n+    BuildCutout unless(this, inline_type_test(cast, \/* is_inline = *\/ false), PROB_MAX);\n+    inc_sp(2);\n+    uncommon_trap_exact(Deoptimization::Reason_class_check, Deoptimization::Action_maybe_recompile);\n+  }\n+\n+  ne_region->add_req(control());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      if (null_ctl == top()) {\n+        replace_in_map(input, cast);\n+      }\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    if (null_ctl == top()) {\n+      replace_in_map(input, cast);\n+    }\n+    set_control(_gvn.transform(ne_region));\n+  }\n+}\n+\n+void Parse::do_acmp(BoolTest::mask btest, Node* left, Node* right) {\n+  ciKlass* left_type = NULL;\n+  ciKlass* right_type = NULL;\n+  ProfilePtrKind left_ptr = ProfileUnknownNull;\n+  ProfilePtrKind right_ptr = ProfileUnknownNull;\n+  bool left_inline_type = true;\n+  bool right_inline_type = true;\n+\n+  \/\/ Leverage profiling at acmp\n+  if (UseACmpProfile) {\n+    method()->acmp_profiled_type(bci(), left_type, right_type, left_ptr, right_ptr, left_inline_type, right_inline_type);\n+    if (too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {\n+      left_type = NULL;\n+      right_type = NULL;\n+      left_inline_type = true;\n+      right_inline_type = true;\n+    }\n+    if (too_many_traps_or_recompiles(Deoptimization::Reason_null_check)) {\n+      left_ptr = ProfileUnknownNull;\n+      right_ptr = ProfileUnknownNull;\n+    }\n+  }\n+\n+  if (UseTypeSpeculation) {\n+    record_profile_for_speculation(left, left_type, left_ptr);\n+    record_profile_for_speculation(right, right_type, right_ptr);\n+  }\n+\n+  if (!EnableValhalla) {\n+    Node* cmp = CmpP(left, right);\n+    cmp = optimize_cmp_with_klass(cmp);\n+    do_if(btest, cmp);\n+    return;\n+  }\n+\n+  \/\/ Check for equality before potentially allocating\n+  if (left == right) {\n+    do_if(btest, makecon(TypeInt::CC_EQ));\n+    return;\n+  }\n+\n+  \/\/ Allocate inline type operands and re-execute on deoptimization\n+  if (left->is_InlineType()) {\n+    if (_gvn.type(right)->is_zero_type() ||\n+        (right->is_InlineType() && _gvn.type(right->as_InlineType()->get_is_init())->is_zero_type())) {\n+      \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+      \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+      Node* cmp = CmpI(left->as_InlineType()->get_is_init(), intcon(0));\n+      do_if(btest, cmp);\n+      return;\n+    } else {\n+      PreserveReexecuteState preexecs(this);\n+      inc_sp(2);\n+      jvms()->set_should_reexecute(true);\n+      left = left->as_InlineType()->buffer(this)->get_oop();\n+    }\n+  }\n+  if (right->is_InlineType()) {\n+    PreserveReexecuteState preexecs(this);\n+    inc_sp(2);\n+    jvms()->set_should_reexecute(true);\n+    right = right->as_InlineType()->buffer(this)->get_oop();\n+  }\n+\n+  \/\/ First, do a normal pointer comparison\n+  const TypeOopPtr* tleft = _gvn.type(left)->isa_oopptr();\n+  const TypeOopPtr* tright = _gvn.type(right)->isa_oopptr();\n+  Node* cmp = CmpP(left, right);\n+  cmp = optimize_cmp_with_klass(cmp);\n+  if (tleft == NULL || !tleft->can_be_inline_type() ||\n+      tright == NULL || !tright->can_be_inline_type()) {\n+    \/\/ This is sufficient, if one of the operands can't be an inline type\n+    do_if(btest, cmp);\n+    return;\n+  }\n+  Node* eq_region = NULL;\n+  if (btest == BoolTest::eq) {\n+    do_if(btest, cmp, true);\n+    if (stopped()) {\n+      return;\n+    }\n+  } else {\n+    assert(btest == BoolTest::ne, \"only eq or ne\");\n+    Node* is_not_equal = NULL;\n+    eq_region = new RegionNode(3);\n+    {\n+      PreserveJVMState pjvms(this);\n+      do_if(btest, cmp, false, &is_not_equal);\n+      if (!stopped()) {\n+        eq_region->init_req(1, control());\n+      }\n+    }\n+    if (is_not_equal == NULL || is_not_equal->is_top()) {\n+      record_for_igvn(eq_region);\n+      set_control(_gvn.transform(eq_region));\n+      return;\n+    }\n+    set_control(is_not_equal);\n+  }\n+\n+  \/\/ Prefer speculative types if available\n+  if (!too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+    if (tleft->speculative_type() != NULL) {\n+      left_type = tleft->speculative_type();\n+    }\n+    if (tright->speculative_type() != NULL) {\n+      right_type = tright->speculative_type();\n+    }\n+  }\n+\n+  if (speculative_ptr_kind(tleft) != ProfileMaybeNull && speculative_ptr_kind(tleft) != ProfileUnknownNull) {\n+    ProfilePtrKind speculative_left_ptr = speculative_ptr_kind(tleft);\n+    if (speculative_left_ptr == ProfileAlwaysNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_assert)) {\n+      left_ptr = speculative_left_ptr;\n+    } else if (speculative_left_ptr == ProfileNeverNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check)) {\n+      left_ptr = speculative_left_ptr;\n+    }\n+  }\n+  if (speculative_ptr_kind(tright) != ProfileMaybeNull && speculative_ptr_kind(tright) != ProfileUnknownNull) {\n+    ProfilePtrKind speculative_right_ptr = speculative_ptr_kind(tright);\n+    if (speculative_right_ptr == ProfileAlwaysNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_assert)) {\n+      right_ptr = speculative_right_ptr;\n+    } else if (speculative_right_ptr == ProfileNeverNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check)) {\n+      right_ptr = speculative_right_ptr;\n+    }\n+  }\n+\n+  if (left_ptr == ProfileAlwaysNull) {\n+    \/\/ Comparison with null. Assert the input is indeed null and we're done.\n+    acmp_always_null_input(left, tleft, btest, eq_region);\n+    return;\n+  }\n+  if (right_ptr == ProfileAlwaysNull) {\n+    \/\/ Comparison with null. Assert the input is indeed null and we're done.\n+    acmp_always_null_input(right, tright, btest, eq_region);\n+    return;\n+  }\n+  if (left_type != NULL && !left_type->is_inlinetype()) {\n+    \/\/ Comparison with an object of known type\n+    acmp_known_non_inline_type_input(left, tleft, left_ptr, left_type, btest, eq_region);\n+    return;\n+  }\n+  if (right_type != NULL && !right_type->is_inlinetype()) {\n+    \/\/ Comparison with an object of known type\n+    acmp_known_non_inline_type_input(right, tright, right_ptr, right_type, btest, eq_region);\n+    return;\n+  }\n+  if (!left_inline_type) {\n+    \/\/ Comparison with an object known not to be an inline type\n+    acmp_unknown_non_inline_type_input(left, tleft, left_ptr, btest, eq_region);\n+    return;\n+  }\n+  if (!right_inline_type) {\n+    \/\/ Comparison with an object known not to be an inline type\n+    acmp_unknown_non_inline_type_input(right, tright, right_ptr, btest, eq_region);\n+    return;\n+  }\n+\n+  \/\/ Pointers are not equal, check if first operand is non-null\n+  Node* ne_region = new RegionNode(6);\n+  Node* null_ctl;\n+  Node* not_null_right = acmp_null_check(right, tright, right_ptr, null_ctl);\n+  ne_region->init_req(1, null_ctl);\n+\n+  \/\/ First operand is non-null, check if it is an inline type\n+  Node* is_value = inline_type_test(not_null_right);\n+  IfNode* is_value_iff = create_and_map_if(control(), is_value, PROB_FAIR, COUNT_UNKNOWN);\n+  Node* not_value = _gvn.transform(new IfFalseNode(is_value_iff));\n+  ne_region->init_req(2, not_value);\n+  set_control(_gvn.transform(new IfTrueNode(is_value_iff)));\n+\n+  \/\/ The first operand is an inline type, check if the second operand is non-null\n+  Node* not_null_left = acmp_null_check(left, tleft, left_ptr, null_ctl);\n+  ne_region->init_req(3, null_ctl);\n+\n+  \/\/ Check if both operands are of the same class.\n+  Node* kls_left = load_object_klass(not_null_left);\n+  Node* kls_right = load_object_klass(not_null_right);\n+  Node* kls_cmp = CmpP(kls_left, kls_right);\n+  Node* kls_bol = _gvn.transform(new BoolNode(kls_cmp, BoolTest::ne));\n+  IfNode* kls_iff = create_and_map_if(control(), kls_bol, PROB_FAIR, COUNT_UNKNOWN);\n+  Node* kls_ne = _gvn.transform(new IfTrueNode(kls_iff));\n+  set_control(_gvn.transform(new IfFalseNode(kls_iff)));\n+  ne_region->init_req(4, kls_ne);\n+\n+  if (stopped()) {\n+    record_for_igvn(ne_region);\n+    set_control(_gvn.transform(ne_region));\n+    if (btest == BoolTest::ne) {\n+      {\n+        PreserveJVMState pjvms(this);\n+        int target_bci = iter().get_dest();\n+        merge(target_bci);\n+      }\n+      record_for_igvn(eq_region);\n+      set_control(_gvn.transform(eq_region));\n+    }\n+    return;\n+  }\n+\n+  \/\/ Both operands are values types of the same class, we need to perform a\n+  \/\/ substitutability test. Delegate to ValueObjectMethods::isSubstitutable().\n+  Node* ne_io_phi = PhiNode::make(ne_region, i_o());\n+  Node* mem = reset_memory();\n+  Node* ne_mem_phi = PhiNode::make(ne_region, mem);\n+\n+  Node* eq_io_phi = NULL;\n+  Node* eq_mem_phi = NULL;\n+  if (eq_region != NULL) {\n+    eq_io_phi = PhiNode::make(eq_region, i_o());\n+    eq_mem_phi = PhiNode::make(eq_region, mem);\n+  }\n+\n+  set_all_memory(mem);\n+\n+  kill_dead_locals();\n+  ciMethod* subst_method = ciEnv::current()->ValueObjectMethods_klass()->find_method(ciSymbols::isSubstitutable_name(), ciSymbols::object_object_boolean_signature());\n+  CallStaticJavaNode *call = new CallStaticJavaNode(C, TypeFunc::make(subst_method), SharedRuntime::get_resolve_static_call_stub(), subst_method);\n+  call->set_override_symbolic_info(true);\n+  call->init_req(TypeFunc::Parms, not_null_left);\n+  call->init_req(TypeFunc::Parms+1, not_null_right);\n+  inc_sp(2);\n+  set_edges_for_java_call(call, false, false);\n+  Node* ret = set_results_for_java_call(call, false, true);\n+  dec_sp(2);\n+\n+  \/\/ Test the return value of ValueObjectMethods::isSubstitutable()\n+  Node* subst_cmp = _gvn.transform(new CmpINode(ret, intcon(1)));\n+  Node* ctl = C->top();\n+  if (btest == BoolTest::eq) {\n+    PreserveJVMState pjvms(this);\n+    do_if(btest, subst_cmp);\n+    if (!stopped()) {\n+      ctl = control();\n+    }\n+  } else {\n+    assert(btest == BoolTest::ne, \"only eq or ne\");\n+    PreserveJVMState pjvms(this);\n+    do_if(btest, subst_cmp, false, &ctl);\n+    if (!stopped()) {\n+      eq_region->init_req(2, control());\n+      eq_io_phi->init_req(2, i_o());\n+      eq_mem_phi->init_req(2, reset_memory());\n+    }\n+  }\n+  ne_region->init_req(5, ctl);\n+  ne_io_phi->init_req(5, i_o());\n+  ne_mem_phi->init_req(5, reset_memory());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  set_i_o(_gvn.transform(ne_io_phi));\n+  set_all_memory(_gvn.transform(ne_mem_phi));\n+\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+    set_i_o(_gvn.transform(eq_io_phi));\n+    set_all_memory(_gvn.transform(eq_mem_phi));\n+  }\n+}\n+\n@@ -1786,0 +2521,4 @@\n+        if (obj->is_InlineType()) {\n+          assert(obj->as_InlineType()->is_allocated(&_gvn), \"must be allocated\");\n+          obj = obj->as_InlineType()->get_oop();\n+        }\n@@ -2632,14 +3371,20 @@\n-    if (!_gvn.type(b)->speculative_maybe_null() &&\n-        !too_many_traps(Deoptimization::Reason_speculate_null_check)) {\n-      inc_sp(1);\n-      Node* null_ctl = top();\n-      b = null_check_oop(b, &null_ctl, true, true, true);\n-      assert(null_ctl->is_top(), \"no null control here\");\n-      dec_sp(1);\n-    } else if (_gvn.type(b)->speculative_always_null() &&\n-               !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {\n-      inc_sp(1);\n-      b = null_assert(b);\n-      dec_sp(1);\n-    }\n-    c = _gvn.transform( new CmpPNode(b, a) );\n+    if (b->is_InlineType()) {\n+      \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+      \/\/ input instead of the oop input to avoid keeping buffer allocations alive\n+      c = _gvn.transform(new CmpINode(b->as_InlineType()->get_is_init(), zerocon(T_INT)));\n+    } else {\n+      if (!_gvn.type(b)->speculative_maybe_null() &&\n+          !too_many_traps(Deoptimization::Reason_speculate_null_check)) {\n+        inc_sp(1);\n+        Node* null_ctl = top();\n+        b = null_check_oop(b, &null_ctl, true, true, true);\n+        assert(null_ctl->is_top(), \"no null control here\");\n+        dec_sp(1);\n+      } else if (_gvn.type(b)->speculative_always_null() &&\n+                 !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {\n+        inc_sp(1);\n+        b = null_assert(b);\n+        dec_sp(1);\n+      }\n+      c = _gvn.transform( new CmpPNode(b, a) );\n+    }\n@@ -2656,3 +3401,1 @@\n-    c = _gvn.transform( new CmpPNode(b, a) );\n-    c = optimize_cmp_with_klass(c);\n-    do_if(btest, c);\n+    do_acmp(btest, b, a);\n@@ -2713,1 +3456,1 @@\n-    do_anewarray();\n+    do_newarray();\n@@ -2724,0 +3467,6 @@\n+  case Bytecodes::_aconst_init:\n+    do_aconst_init();\n+    break;\n+  case Bytecodes::_withfield:\n+    do_withfield();\n+    break;\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":799,"deletions":50,"binary":false,"changes":849,"status":"modified"},{"patch":"@@ -1321,6 +1321,0 @@\n-  if (_delay_transform) {\n-    \/\/ Register the node but don't optimize for now\n-    register_new_node_with_optimizer(n);\n-    return n;\n-  }\n-\n@@ -1333,0 +1327,6 @@\n+  if (_delay_transform) {\n+    \/\/ Add the node to the worklist but don't optimize for now\n+    _worklist.push(n);\n+    return n;\n+  }\n+\n@@ -1602,0 +1602,13 @@\n+void PhaseIterGVN::replace_in_uses(Node* n, Node* m) {\n+  assert(n != NULL, \"sanity\");\n+  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+    Node* u = n->fast_out(i);\n+    if (u != n) {\n+      rehash_node_delayed(u);\n+      int nb = u->replace_edge(n, m);\n+      --i, imax -= nb;\n+    }\n+  }\n+  assert(n->outcnt() == 0, \"all uses must be deleted\");\n+}\n+\n@@ -1749,0 +1762,9 @@\n+    \/\/ Inline type nodes can have other inline types as users. If an input gets\n+    \/\/ updated, make sure that inline type users get a chance for optimization.\n+    if (use->is_InlineType()) {\n+      for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+        Node* u = use->fast_out(i2);\n+        if (u->is_InlineType())\n+          _worklist.push(u);\n+      }\n+    }\n@@ -1832,0 +1854,8 @@\n+    if (use_op == Op_CastP2X) {\n+      for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+        Node* u = use->fast_out(i2);\n+        if (u->Opcode() == Op_AndX) {\n+          _worklist.push(u);\n+        }\n+      }\n+    }\n@@ -1856,0 +1886,11 @@\n+\n+    \/\/ Give CallStaticJavaNode::remove_useless_allocation a chance to run\n+    if (use->is_Region()) {\n+      Node* c = use;\n+      do {\n+        c = c->unique_ctrl_out_or_null();\n+      } while (c != NULL && c->is_Region());\n+      if (c != NULL && c->is_CallStaticJava() && c->as_CallStaticJava()->uncommon_trap_request() != 0) {\n+        _worklist.push(c);\n+      }\n+    }\n@@ -2046,0 +2087,1 @@\n+  push_cast(worklist, use);\n@@ -2107,0 +2149,12 @@\n+void PhaseCCP::push_cast(Unique_Node_List& worklist, const Node* use) {\n+  uint use_op = use->Opcode();\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":60,"deletions":6,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -474,1 +474,1 @@\n-  virtual void record_for_igvn(Node *n) { }\n+  virtual void record_for_igvn(Node *n) { _worklist.push(n); }\n@@ -528,0 +528,2 @@\n+  void replace_in_uses(Node* n, Node* m);\n+\n@@ -610,0 +612,1 @@\n+  static void push_cast(Unique_Node_List& worklist, const Node* use);\n","filename":"src\/hotspot\/share\/opto\/phaseX.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -877,1 +878,8 @@\n-Node *CmpLNode::Ideal( PhaseGVN *phase, bool can_reshape ) {\n+\/\/------------------------------Ideal------------------------------------------\n+Node* CmpLNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* a = NULL;\n+  Node* b = NULL;\n+  if (is_double_null_check(phase, a, b) && (phase->type(a)->is_zero_type() || phase->type(b)->is_zero_type())) {\n+    \/\/ Degraded to a simple null check, use old acmp\n+    return new CmpPNode(a, b);\n+  }\n@@ -888,0 +896,25 @@\n+\/\/ Match double null check emitted by Compile::optimize_acmp()\n+bool CmpLNode::is_double_null_check(PhaseGVN* phase, Node*& a, Node*& b) const {\n+  if (in(1)->Opcode() == Op_OrL &&\n+      in(1)->in(1)->Opcode() == Op_CastP2X &&\n+      in(1)->in(2)->Opcode() == Op_CastP2X &&\n+      in(2)->bottom_type()->is_zero_type()) {\n+    assert(EnableValhalla, \"unexpected double null check\");\n+    a = in(1)->in(1)->in(1);\n+    b = in(1)->in(2)->in(1);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\/\/------------------------------Value------------------------------------------\n+const Type* CmpLNode::Value(PhaseGVN* phase) const {\n+  Node* a = NULL;\n+  Node* b = NULL;\n+  if (is_double_null_check(phase, a, b) && (!phase->type(a)->maybe_null() || !phase->type(b)->maybe_null())) {\n+    \/\/ One operand is never NULL, emit constant false\n+    return TypeInt::CC_GT;\n+  }\n+  return SubNode::Value(phase);\n+}\n+\n@@ -1013,1 +1046,16 @@\n-\n+    if (!unrelated_classes) {\n+      \/\/ Handle inline type arrays\n+      if ((r0->flatten_array() && r1->not_flatten_array()) ||\n+          (r1->flatten_array() && r0->not_flatten_array())) {\n+        \/\/ One type is flattened in arrays but the other type is not. Must be unrelated.\n+        unrelated_classes = true;\n+      } else if ((r0->is_not_flat() && r1->is_flat()) ||\n+                 (r1->is_not_flat() && r0->is_flat())) {\n+        \/\/ One type is a non-flattened array and the other type is a flattened array. Must be unrelated.\n+        unrelated_classes = true;\n+      } else if ((r0->is_not_null_free() && r1->is_null_free()) ||\n+                 (r1->is_not_null_free() && r0->is_null_free())) {\n+        \/\/ One type is a nullable array and the other type is a null-free array. Must be unrelated.\n+        unrelated_classes = true;\n+      }\n+    }\n@@ -1098,1 +1146,8 @@\n-Node *CmpPNode::Ideal( PhaseGVN *phase, bool can_reshape ) {\n+Node* CmpPNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  \/\/ TODO 8284443 in(1) could be cast?\n+  if (in(1)->is_InlineType() && phase->type(in(2))->is_zero_type()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    return new CmpINode(in(1)->as_InlineType()->get_is_init(), phase->intcon(0));\n+  }\n+\n@@ -1170,0 +1225,8 @@\n+  \/\/ Do not fold the subtype check to an array klass pointer comparison for [V? arrays.\n+  \/\/ [QMyValue is a subtype of [LMyValue but the klass for [QMyValue is not equal to\n+  \/\/ the klass for [LMyValue. Do not bypass the klass load from the primary supertype array.\n+  if (superklass->is_obj_array_klass() && !superklass->as_array_klass()->is_elem_null_free() &&\n+      superklass->as_array_klass()->element_klass()->is_inlinetype()) {\n+    return NULL;\n+  }\n+\n@@ -1313,0 +1376,37 @@\n+\/\/=============================================================================\n+\/\/------------------------------Value------------------------------------------\n+const Type* FlatArrayCheckNode::Value(PhaseGVN* phase) const {\n+  bool all_not_flat = true;\n+  for (uint i = ArrayOrKlass; i < req(); ++i) {\n+    const Type* t = phase->type(in(i));\n+    if (t == Type::TOP) {\n+      return Type::TOP;\n+    }\n+    if (t->is_ptr()->is_flat()) {\n+      \/\/ One of the input arrays is flat, check always passes\n+      return TypeInt::CC_EQ;\n+    } else if (!t->is_ptr()->is_not_flat()) {\n+      \/\/ One of the input arrays might be flat\n+      all_not_flat = false;\n+    }\n+  }\n+  if (all_not_flat) {\n+    \/\/ None of the input arrays can be flat, check always fails\n+    return TypeInt::CC_GT;\n+  }\n+  return TypeInt::CC;\n+}\n+\n+\/\/------------------------------Ideal------------------------------------------\n+Node* FlatArrayCheckNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  bool changed = false;\n+  \/\/ Remove inputs that are known to be non-flat\n+  for (uint i = ArrayOrKlass; i < req(); ++i) {\n+    const Type* t = phase->type(in(i));\n+    if (t->isa_ptr() && t->is_ptr()->is_not_flat()) {\n+      del_req(i--);\n+      changed = true;\n+    }\n+  }\n+  return changed ? this : NULL;\n+}\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":103,"deletions":3,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -49,0 +49,21 @@\n+  \/\/ FIXME: shouldn't this be encoded in helper methods of the type system (maybe_java_subtype_of() etc.?)\n+  \/\/ Similar to logic in CmpPNode::sub()\n+  bool unrelated_classes = false;\n+  \/\/ Handle inline type arrays\n+  if (subk->flatten_array() && superk->not_flatten_array()) {\n+    \/\/ The subtype is flattened in arrays and the supertype is not flattened in arrays. Must be unrelated.\n+    unrelated_classes = true;\n+  } else if (subk->is_not_flat() && superk->is_flat()) {\n+    \/\/ The subtype is a non-flattened array and the supertype is a flattened array. Must be unrelated.\n+    unrelated_classes = true;\n+  } else if (subk->is_not_null_free() && superk->is_null_free()) {\n+    \/\/ The subtype is a nullable array and the supertype is null-free array. Must be unrelated.\n+    unrelated_classes = true;\n+  }\n+  if (unrelated_classes) {\n+    TypePtr::PTR jp = sub_t->is_ptr()->join_ptr(super_t->is_ptr()->_ptr);\n+    if (jp != TypePtr::Null && jp != TypePtr::BotPTR) {\n+      return TypeInt::CC_GT;\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/subtypenode.cpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -26,0 +26,3 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciField.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -51,0 +54,46 @@\n+const Type::Offset Type::Offset::top(Type::OffsetTop);\n+const Type::Offset Type::Offset::bottom(Type::OffsetBot);\n+\n+const Type::Offset Type::Offset::meet(const Type::Offset other) const {\n+  \/\/ Either is 'TOP' offset?  Return the other offset!\n+  int offset = other._offset;\n+  if (_offset == OffsetTop) return Offset(offset);\n+  if (offset == OffsetTop) return Offset(_offset);\n+  \/\/ If either is different, return 'BOTTOM' offset\n+  if (_offset != offset) return bottom;\n+  return Offset(_offset);\n+}\n+\n+const Type::Offset Type::Offset::dual() const {\n+  if (_offset == OffsetTop) return bottom;\/\/ Map 'TOP' into 'BOTTOM'\n+  if (_offset == OffsetBot) return top;\/\/ Map 'BOTTOM' into 'TOP'\n+  return Offset(_offset);               \/\/ Map everything else into self\n+}\n+\n+const Type::Offset Type::Offset::add(intptr_t offset) const {\n+  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n+  if (_offset == OffsetTop || offset == OffsetTop) return top;\n+  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n+  if (_offset == OffsetBot || offset == OffsetBot) return bottom;\n+  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n+  offset += (intptr_t)_offset;\n+  if (offset != (int)offset || offset == OffsetTop) return bottom;\n+\n+  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n+  \/\/ It is possible to construct a negative offset during PhaseCCP\n+\n+  return Offset((int)offset);        \/\/ Sum valid offsets\n+}\n+\n+void Type::Offset::dump2(outputStream *st) const {\n+  if (_offset == 0) {\n+    return;\n+  } else if (_offset == OffsetTop) {\n+    st->print(\"+top\");\n+  }\n+  else if (_offset == OffsetBot) {\n+    st->print(\"+bot\");\n+  } else if (_offset) {\n+    st->print(\"+%d\", _offset);\n+  }\n+}\n@@ -224,0 +273,5 @@\n+  case T_PRIMITIVE_OBJECT: {\n+    ciInlineKlass* vk = type->unwrap()->as_inline_klass();\n+    return TypeOopPtr::make_from_klass(vk)->join_speculative(type->is_null_free() ? TypePtr::NOTNULL : TypePtr::BOTTOM);\n+  }\n+\n@@ -252,0 +306,1 @@\n+    case T_PRIMITIVE_OBJECT:\n@@ -289,0 +344,1 @@\n+    case T_PRIMITIVE_OBJECT: conbt = T_OBJECT; break;\n@@ -295,0 +351,1 @@\n+    case T_PRIMITIVE_OBJECT: loadbt = T_OBJECT; break;\n@@ -530,3 +587,3 @@\n-  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, 0);\n-  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, OffsetBot);\n-  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, OffsetBot);\n+  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, Offset(0));\n+  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, Offset::bottom);\n+  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, Offset::bottom);\n@@ -549,1 +606,1 @@\n-                                           false, 0, oopDesc::mark_offset_in_bytes());\n+                                           false, 0, Offset(oopDesc::mark_offset_in_bytes()));\n@@ -551,2 +608,2 @@\n-                                           false, 0, oopDesc::klass_offset_in_bytes());\n-  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, OffsetBot, TypeOopPtr::InstanceBot);\n+                                           false, 0, Offset(oopDesc::klass_offset_in_bytes()));\n+  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, Offset::bottom, TypeOopPtr::InstanceBot);\n@@ -554,1 +611,1 @@\n-  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, NULL, OffsetBot);\n+  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, NULL, Offset::bottom);\n@@ -577,1 +634,1 @@\n-  TypeAryPtr::RANGE   = TypeAryPtr::make( TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), NULL \/* current->env()->Object_klass() *\/, false, arrayOopDesc::length_offset_in_bytes());\n+  TypeAryPtr::RANGE   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), NULL \/* current->env()->Object_klass() *\/, false, Offset(arrayOopDesc::length_offset_in_bytes()));\n@@ -579,1 +636,1 @@\n-  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), NULL \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), NULL \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -589,1 +646,1 @@\n-    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), NULL \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), NULL \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -591,7 +648,8 @@\n-  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Type::OffsetBot);\n-  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Type::OffsetBot);\n-  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Type::OffsetBot);\n-  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Type::OffsetBot);\n-  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Type::OffsetBot);\n-  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Type::OffsetBot);\n-  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Type::OffsetBot);\n+  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Offset::bottom);\n+  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Offset::bottom);\n+  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Offset::bottom);\n+  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Offset::bottom);\n+  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Offset::bottom);\n+  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Offset::bottom);\n+  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Offset::bottom);\n+  TypeAryPtr::INLINES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ true), NULL, false, Offset::bottom);\n@@ -602,0 +660,1 @@\n+  TypeAryPtr::_array_body_type[T_PRIMITIVE_OBJECT] = TypeAryPtr::OOPS;\n@@ -612,2 +671,2 @@\n-  TypeInstKlassPtr::OBJECT = TypeInstKlassPtr::make(TypePtr::NotNull, current->env()->Object_klass(), 0);\n-  TypeInstKlassPtr::OBJECT_OR_NULL = TypeInstKlassPtr::make(TypePtr::BotPTR, current->env()->Object_klass(), 0);\n+  TypeInstKlassPtr::OBJECT = TypeInstKlassPtr::make(TypePtr::NotNull, current->env()->Object_klass(), Offset(0));\n+  TypeInstKlassPtr::OBJECT_OR_NULL = TypeInstKlassPtr::make(TypePtr::BotPTR, current->env()->Object_klass(), Offset(0));\n@@ -652,0 +711,1 @@\n+  _const_basic_type[T_PRIMITIVE_OBJECT] = TypeInstPtr::BOTTOM;\n@@ -668,0 +728,1 @@\n+  _zero_type[T_PRIMITIVE_OBJECT] = TypePtr::NULL_PTR;\n@@ -2124,0 +2185,12 @@\n+static void collect_inline_fields(ciInlineKlass* vk, const Type** field_array, uint& pos) {\n+  for (int j = 0; j < vk->nof_nonstatic_fields(); j++) {\n+    ciField* field = vk->nonstatic_field_at(j);\n+    BasicType bt = field->type()->basic_type();\n+    const Type* ft = Type::get_const_type(field->type());\n+    field_array[pos++] = ft;\n+    if (type2size[bt] == 2) {\n+      field_array[pos++] = Type::HALF;\n+    }\n+  }\n+}\n+\n@@ -2126,1 +2199,1 @@\n-const TypeTuple *TypeTuple::make_range(ciSignature* sig, InterfaceHandling interface_handling) {\n+const TypeTuple *TypeTuple::make_range(ciSignature* sig, InterfaceHandling interface_handling, bool ret_vt_fields) {\n@@ -2129,0 +2202,8 @@\n+  if (ret_vt_fields) {\n+    arg_cnt = return_type->as_inline_klass()->inline_arg_slots() + 1;\n+    if (!sig->returns_null_free_inline_type()) {\n+      \/\/ InlineTypeNode::IsInit field used for null checking\n+      arg_cnt++;\n+    }\n+  }\n+\n@@ -2149,0 +2230,13 @@\n+  case T_PRIMITIVE_OBJECT:\n+    if (ret_vt_fields) {\n+      uint pos = TypeFunc::Parms;\n+      field_array[pos++] = get_const_type(return_type); \/\/ Oop might be null when returning as fields\n+      collect_inline_fields(return_type->as_inline_klass(), field_array, pos);\n+      if (!sig->returns_null_free_inline_type()) {\n+        \/\/ InlineTypeNode::IsInit field used for null checking\n+        field_array[pos++] = get_const_basic_type(T_BOOLEAN);\n+      }\n+    } else {\n+      field_array[TypeFunc::Parms] = get_const_type(return_type)->join_speculative(sig->returns_null_free_inline_type() ? TypePtr::NOTNULL : TypePtr::BOTTOM);\n+    }\n+    break;\n@@ -2158,2 +2252,10 @@\n-const TypeTuple *TypeTuple::make_domain(ciInstanceKlass* recv, ciSignature* sig, InterfaceHandling interface_handling) {\n-  uint arg_cnt = sig->size();\n+const TypeTuple *TypeTuple::make_domain(ciMethod* method, InterfaceHandling interface_handling, bool vt_fields_as_args) {\n+  ciSignature* sig = method->signature();\n+  uint arg_cnt = sig->size() + (method->is_static() ? 0 : 1);\n+  if (vt_fields_as_args) {\n+    arg_cnt = 0;\n+    assert(method->get_sig_cc() != NULL, \"Should have scalarized signature\");\n+    for (ExtendedSignature sig_cc = ExtendedSignature(method->get_sig_cc(), SigEntryFilter()); !sig_cc.at_end(); ++sig_cc) {\n+      arg_cnt += type2size[(*sig_cc)._bt];\n+    }\n+  }\n@@ -2162,8 +2264,8 @@\n-  const Type **field_array;\n-  if (recv != NULL) {\n-    arg_cnt++;\n-    field_array = fields(arg_cnt);\n-    \/\/ Use get_const_type here because it respects UseUniqueSubclasses:\n-    field_array[pos++] = get_const_type(recv, interface_handling)->join_speculative(TypePtr::NOTNULL);\n-  } else {\n-    field_array = fields(arg_cnt);\n+  const Type** field_array = fields(arg_cnt);\n+  if (!method->is_static()) {\n+    ciInstanceKlass* recv = method->holder();\n+    if (vt_fields_as_args && recv->is_inlinetype() && recv->as_inline_klass()->can_be_passed_as_fields()) {\n+      collect_inline_fields(recv->as_inline_klass(), field_array, pos);\n+    } else {\n+      field_array[pos++] = get_const_type(recv, interface_handling)->join_speculative(TypePtr::NOTNULL);\n+    }\n@@ -2175,0 +2277,1 @@\n+    BasicType bt = type->basic_type();\n@@ -2176,1 +2279,1 @@\n-    switch (type->basic_type()) {\n+    switch (bt) {\n@@ -2197,0 +2300,12 @@\n+    case T_PRIMITIVE_OBJECT: {\n+      if (vt_fields_as_args && method->is_scalarized_arg(i + (method->is_static() ? 0 : 1))) {\n+        if (!sig->is_null_free_at(i)) {\n+          \/\/ InlineTypeNode::IsInit field used for null checking\n+          field_array[pos++] = get_const_basic_type(T_BOOLEAN);\n+        }\n+        collect_inline_fields(type->as_inline_klass(), field_array, pos);\n+      } else {\n+        field_array[pos++] = get_const_type(type)->join_speculative(sig->is_null_free_at(i) ? TypePtr::NOTNULL : TypePtr::BOTTOM);\n+      }\n+      break;\n+    }\n@@ -2202,0 +2317,1 @@\n+  assert(pos == TypeFunc::Parms + arg_cnt, \"wrong number of arguments\");\n@@ -2336,1 +2452,2 @@\n-const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable) {\n+const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable,\n+                             bool flat, bool not_flat, bool not_null_free) {\n@@ -2341,1 +2458,1 @@\n-  return (TypeAry*)(new TypeAry(elem,size,stable))->hashcons();\n+  return (TypeAry*)(new TypeAry(elem, size, stable, flat, not_flat, not_null_free))->hashcons();\n@@ -2363,1 +2480,4 @@\n-                         _stable && a->_stable);\n+                         _stable && a->_stable,\n+                         _flat && a->_flat,\n+                         _not_flat && a->_not_flat,\n+                         _not_null_free && a->_not_null_free);\n@@ -2376,1 +2496,1 @@\n-  return new TypeAry(_elem->dual(), size_dual, !_stable);\n+  return new TypeAry(_elem->dual(), size_dual, !_stable, !_flat, !_not_flat, !_not_null_free);\n@@ -2385,1 +2505,5 @@\n-    _size == a->_size;\n+    _size == a->_size &&\n+    _flat == a->_flat &&\n+    _not_flat == a->_not_flat &&\n+    _not_null_free == a->_not_null_free;\n+\n@@ -2391,1 +2515,2 @@\n-  return (intptr_t)_elem + (intptr_t)_size + (_stable ? 43 : 0);\n+  return (intptr_t)_elem + (intptr_t)_size + (_stable ? 43 : 0) +\n+      (_flat ? 44 : 0) + (_not_flat ? 45 : 0) + (_not_null_free ? 46 : 0);\n@@ -2398,1 +2523,1 @@\n-  return make(_elem->remove_speculative(), _size, _stable);\n+  return make(_elem->remove_speculative(), _size, _stable, _flat, _not_flat, _not_null_free);\n@@ -2405,1 +2530,1 @@\n-  return make(_elem->cleanup_speculative(), _size, _stable);\n+  return make(_elem->cleanup_speculative(), _size, _stable, _flat, _not_flat, _not_null_free);\n@@ -2424,0 +2549,5 @@\n+  if (_flat) st->print(\"flat:\");\n+  if (Verbose) {\n+    if (_not_flat) st->print(\"not flat:\");\n+    if (_not_null_free) st->print(\"not null free:\");\n+  }\n@@ -2463,2 +2593,10 @@\n-  if (tinst)\n-    return tinst->instance_klass()->is_final();\n+  if (tinst) {\n+    if (tinst->instance_klass()->is_final()) {\n+      \/\/ Even if MyValue is exact, [LMyValue is not exact due to [QMyValue <: [LMyValue.\n+      if (tinst->is_inlinetypeptr() && (tinst->ptr() == TypePtr::BotPTR || tinst->ptr() == TypePtr::TopPTR)) {\n+        return false;\n+      }\n+      return true;\n+    }\n+    return false;\n+  }\n@@ -2660,1 +2798,1 @@\n-const TypePtr *TypePtr::make(TYPES t, enum PTR ptr, int offset, const TypePtr* speculative, int inline_depth) {\n+const TypePtr* TypePtr::make(TYPES t, enum PTR ptr, Offset offset, const TypePtr* speculative, int inline_depth) {\n@@ -2674,1 +2812,1 @@\n-  return _offset;\n+  return offset();\n@@ -2745,7 +2883,2 @@\n-int TypePtr::meet_offset( int offset ) const {\n-  \/\/ Either is 'TOP' offset?  Return the other offset!\n-  if( _offset == OffsetTop ) return offset;\n-  if( offset == OffsetTop ) return _offset;\n-  \/\/ If either is different, return 'BOTTOM' offset\n-  if( _offset != offset ) return OffsetBot;\n-  return _offset;\n+Type::Offset TypePtr::meet_offset(int offset) const {\n+  return _offset.meet(Offset(offset));\n@@ -2755,4 +2888,2 @@\n-int TypePtr::dual_offset( ) const {\n-  if( _offset == OffsetTop ) return OffsetBot;\/\/ Map 'TOP' into 'BOTTOM'\n-  if( _offset == OffsetBot ) return OffsetTop;\/\/ Map 'BOTTOM' into 'TOP'\n-  return _offset;               \/\/ Map everything else into self\n+Type::Offset TypePtr::dual_offset() const {\n+  return _offset.dual();\n@@ -2771,13 +2902,2 @@\n-int TypePtr::xadd_offset( intptr_t offset ) const {\n-  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n-  if( _offset == OffsetTop || offset == OffsetTop ) return OffsetTop;\n-  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n-  if( _offset == OffsetBot || offset == OffsetBot ) return OffsetBot;\n-  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n-  offset += (intptr_t)_offset;\n-  if (offset != (int)offset || offset == OffsetTop) return OffsetBot;\n-\n-  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n-  \/\/ It is possible to construct a negative offset during PhaseCCP\n-\n-  return (int)offset;        \/\/ Sum valid offsets\n+Type::Offset TypePtr::xadd_offset(intptr_t offset) const {\n+  return _offset.add(offset);\n@@ -2792,1 +2912,1 @@\n-  return make(AnyPtr, _ptr, offset, _speculative, _inline_depth);\n+  return make(AnyPtr, _ptr, Offset(offset), _speculative, _inline_depth);\n@@ -2799,1 +2919,1 @@\n-  return _ptr == a->ptr() && _offset == a->offset() && eq_speculative(a) && _inline_depth == a->_inline_depth;\n+  return _ptr == a->ptr() && _offset == a->_offset && eq_speculative(a) && _inline_depth == a->_inline_depth;\n@@ -2805,1 +2925,1 @@\n-  return java_add(java_add((jint)_ptr, (jint)_offset), java_add((jint)hash_speculative(), (jint)_inline_depth));\n+  return java_add(java_add((jint)_ptr, (jint)offset()), java_add((jint)hash_speculative(), (jint)_inline_depth));\n@@ -2961,1 +3081,1 @@\n-      return speculative->klass();\n+      return speculative->exact_klass();\n@@ -3072,3 +3192,1 @@\n-  if( _offset == OffsetTop ) st->print(\"+top\");\n-  else if( _offset == OffsetBot ) st->print(\"+bot\");\n-  else if( _offset ) st->print(\"+%d\", _offset);\n+  _offset.dump2(st);\n@@ -3109,1 +3227,1 @@\n-  return (_offset != OffsetBot) && !below_centerline(_ptr);\n+  return (_offset != Offset::bottom) && !below_centerline(_ptr);\n@@ -3113,1 +3231,1 @@\n-  return (_offset == OffsetTop) || above_centerline(_ptr);\n+  return (_offset == Offset::top) || above_centerline(_ptr);\n@@ -3486,1 +3604,1 @@\n-TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, int offset,\n+TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, Offset offset, Offset field_offset,\n@@ -3497,2 +3615,2 @@\n-      (offset > 0) && xk && (k != 0) && k->is_instance_klass()) {\n-    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset);\n+      (offset.get() > 0) && xk && (k != 0) && k->is_instance_klass()) {\n+    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset.get());\n@@ -3501,2 +3619,2 @@\n-  if (_offset > 0 || _offset == Type::OffsetTop || _offset == Type::OffsetBot) {\n-    if (_offset == oopDesc::klass_offset_in_bytes()) {\n+  if (this->offset() > 0 || this->offset() == Type::OffsetTop || this->offset() == Type::OffsetBot) {\n+    if (this->offset() == oopDesc::klass_offset_in_bytes()) {\n@@ -3508,3 +3626,12 @@\n-    } else if (this->isa_aryptr()) {\n-      _is_ptr_to_narrowoop = (UseCompressedOops && klass()->is_obj_array_klass() &&\n-                             _offset != arrayOopDesc::length_offset_in_bytes());\n+    } else if (UseCompressedOops && this->isa_aryptr() && this->offset() != arrayOopDesc::length_offset_in_bytes()) {\n+      if (klass()->is_obj_array_klass()) {\n+        _is_ptr_to_narrowoop = true;\n+      } else if (klass()->is_flat_array_klass() && field_offset != Offset::top && field_offset != Offset::bottom) {\n+        \/\/ Check if the field of the inline type array element contains oops\n+        ciInlineKlass* vk = klass()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+        int foffset = field_offset.get() + vk->first_field_offset();\n+        ciField* field = vk->get_field_by_offset(foffset, false);\n+        assert(field != NULL, \"missing field\");\n+        BasicType bt = field->layout_type();\n+        _is_ptr_to_narrowoop = UseCompressedOops && ::is_reference_type(bt);\n+      }\n@@ -3512,1 +3639,0 @@\n-      ciInstanceKlass* ik = klass()->as_instance_klass();\n@@ -3515,1 +3641,1 @@\n-      } else if (_offset == OffsetBot || _offset == OffsetTop) {\n+      } else if (_offset == Offset::bottom || _offset == Offset::top) {\n@@ -3520,3 +3646,2 @@\n-\n-            (_offset == java_lang_Class::klass_offset() ||\n-             _offset == java_lang_Class::array_klass_offset())) {\n+            (this->offset() == java_lang_Class::klass_offset() ||\n+             this->offset() == java_lang_Class::array_klass_offset())) {\n@@ -3528,1 +3653,1 @@\n-                   _offset >= InstanceMirrorKlass::offset_of_static_fields()) {\n+                   this->offset() >= InstanceMirrorKlass::offset_of_static_fields()) {\n@@ -3533,8 +3658,14 @@\n-            field = k->get_field_by_offset(_offset, true);\n-          }\n-          if (field != NULL) {\n-            BasicType basic_elem_type = field->layout_type();\n-            _is_ptr_to_narrowoop = UseCompressedOops && ::is_reference_type(basic_elem_type);\n-          } else {\n-            \/\/ unsafe access\n-            _is_ptr_to_narrowoop = UseCompressedOops;\n+            if (k->is_inlinetype() && this->offset() == k->as_inline_klass()->default_value_offset()) {\n+              \/\/ Special hidden field that contains the oop of the default inline type\n+              \/\/ basic_elem_type = T_PRIMITIVE_OBJECT;\n+             _is_ptr_to_narrowoop = UseCompressedOops;\n+            } else {\n+              field = k->get_field_by_offset(this->offset(), true);\n+              if (field != NULL) {\n+                BasicType basic_elem_type = field->layout_type();\n+                _is_ptr_to_narrowoop = UseCompressedOops && ::is_reference_type(basic_elem_type);\n+              } else {\n+                \/\/ unsafe access\n+                _is_ptr_to_narrowoop = UseCompressedOops;\n+              }\n+            }\n@@ -3544,1 +3675,2 @@\n-          ciField* field = ik->get_field_by_offset(_offset, false);\n+          ciInstanceKlass* ik = klass()->as_instance_klass();\n+          ciField* field = ik->get_field_by_offset(this->offset(), false);\n@@ -3564,2 +3696,2 @@\n-const TypeOopPtr *TypeOopPtr::make(PTR ptr, int offset, int instance_id,\n-                                     const TypePtr* speculative, int inline_depth) {\n+const TypeOopPtr *TypeOopPtr::make(PTR ptr, Offset offset, int instance_id,\n+                                   const TypePtr* speculative, int inline_depth) {\n@@ -3570,1 +3702,1 @@\n-  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, InterfaceSet(), xk, o, offset, instance_id, speculative, inline_depth))->hashcons();\n+  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, InterfaceSet(), xk, o, offset, Offset::bottom, instance_id, speculative, inline_depth))->hashcons();\n@@ -3595,1 +3727,0 @@\n-\n@@ -3641,1 +3772,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -3683,1 +3814,1 @@\n-  return new TypeOopPtr(_base, dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeOopPtr(_base, dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), Offset::bottom, dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -3688,2 +3819,2 @@\n-const TypeOopPtr* TypeOopPtr::make_from_klass_common(ciKlass* klass, bool klass_change, bool try_for_exact, InterfaceHandling interface_handling) {\n-  if (klass->is_instance_klass()) {\n+const TypeOopPtr* TypeOopPtr::make_from_klass_common(ciKlass *klass, bool klass_change, bool try_for_exact, InterfaceHandling interface_handling) {\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n@@ -3716,1 +3847,1 @@\n-    return TypeInstPtr::make(TypePtr::BotPTR, klass, interfaces, klass_is_exact, NULL, 0);\n+    return TypeInstPtr::make(TypePtr::BotPTR, klass, interfaces, klass_is_exact, NULL, Offset(0));\n@@ -3718,5 +3849,18 @@\n-    \/\/ Element is an object array. Recursively call ourself.\n-    ciKlass* eklass = klass->as_obj_array_klass()->element_klass();\n-    const TypeOopPtr *etype = TypeOopPtr::make_from_klass_common(eklass, false, try_for_exact, interface_handling);\n-    bool xk = etype->klass_is_exact();\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    \/\/ Element is an object or inline type array. Recursively call ourself.\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ false, try_for_exact, interface_handling);\n+    bool null_free = klass->as_array_klass()->is_elem_null_free();\n+    if (null_free) {\n+      etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n+    \/\/ Determine null-free\/flattened properties\n+    const TypeOopPtr* exact_etype = etype;\n+    if (etype->can_be_inline_type()) {\n+      \/\/ Use exact type if element can be an inline type\n+      exact_etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ true, \/* try_for_exact= *\/ true, interface_handling);\n+    }\n+    bool not_null_free = !exact_etype->can_be_inline_type();\n+    bool not_flat = !UseFlatArray || not_null_free || (exact_etype->is_inlinetypeptr() && !exact_etype->inline_klass()->flatten_array());\n+\n+    \/\/ Even if MyValue is exact, [LMyValue is not exact due to [QMyValue <: [LMyValue.\n+    bool xk = etype->klass_is_exact() && (!etype->is_inlinetypeptr() || null_free);\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, not_flat, not_null_free);\n@@ -3726,1 +3870,1 @@\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, NULL, xk, 0);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, NULL, xk, Offset(0));\n@@ -3731,1 +3875,2 @@\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS,\n+                                        \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3734,1 +3879,7 @@\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, 0);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, Offset(0));\n+    return arr;\n+  } else if (klass->is_flat_array_klass()) {\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ true);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, Offset(0));\n@@ -3750,2 +3901,2 @@\n-  if (klass->is_instance_klass()) {\n-    \/\/ Element is an instance\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n+    \/\/ Element is an instance or inline type\n@@ -3755,1 +3906,1 @@\n-      return TypeInstPtr::make(TypePtr::NotNull, klass, true, NULL, 0);\n+      return TypeInstPtr::make(TypePtr::NotNull, klass, true, NULL, Offset(0));\n@@ -3759,3 +3910,8 @@\n-    const TypeOopPtr *etype =\n-      TypeOopPtr::make_from_klass_raw(klass->as_obj_array_klass()->element_klass(), trust_interfaces);\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    bool null_free = false;\n+    if (klass->as_array_klass()->is_elem_null_free()) {\n+      null_free = true;\n+      etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()),\n+                                        \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ !null_free);\n@@ -3766,1 +3922,1 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3768,1 +3924,1 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3772,3 +3928,3 @@\n-    const Type* etype =\n-      (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const Type* etype = (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()),\n+                                        \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3778,1 +3934,13 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n+    } else {\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n+    }\n+  } else if (klass->is_flat_array_klass()) {\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()), \/* stable= *\/ false, \/* flat= *\/ true);\n+    \/\/ We used to pass NotNull in here, asserting that the sub-arrays\n+    \/\/ are all not-null.  This is not true in generally, as code can\n+    \/\/ slam NULLs down in the subarrays.\n+    if (make_constant) {\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3780,1 +3948,1 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3791,1 +3959,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -3793,1 +3961,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -3853,6 +4021,1 @@\n-  switch( _offset ) {\n-  case OffsetTop: st->print(\"+top\"); break;\n-  case OffsetBot: st->print(\"+any\"); break;\n-  case         0: break;\n-  default:        st->print(\"+%d\",_offset); break;\n-  }\n+  _offset.dump2(st);\n@@ -3875,1 +4038,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -3884,1 +4047,1 @@\n-  return make(_ptr, offset, _instance_id, with_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, Offset(offset), _instance_id, with_offset_speculative(offset), _inline_depth);\n@@ -3999,3 +4162,4 @@\n-TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, int off,\n-                         int instance_id, const TypePtr* speculative, int inline_depth)\n-  : TypeOopPtr(InstPtr, ptr, k, interfaces, xk, o, off, instance_id, speculative, inline_depth) {\n+TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, Offset off,\n+                         bool flatten_array, int instance_id, const TypePtr* speculative, int inline_depth)\n+  : TypeOopPtr(InstPtr, ptr, k, interfaces, xk, o, off, Offset::bottom, instance_id, speculative, inline_depth),\n+    _flatten_array(flatten_array) {\n@@ -4006,0 +4170,2 @@\n+  assert(!klass()->flatten_array() || flatten_array, \"Should be flat in array\");\n+  assert(!flatten_array || can_be_inline_type(), \"Only inline types can be flat in array\");\n@@ -4014,1 +4180,2 @@\n-                                     int offset,\n+                                     Offset offset,\n+                                     bool flatten_array,\n@@ -4036,0 +4203,3 @@\n+  \/\/ Check if this type is known to be flat in arrays\n+  flatten_array = flatten_array || k->flatten_array();\n+\n@@ -4038,1 +4208,1 @@\n-    (TypeInstPtr*)(new TypeInstPtr(ptr, k, interfaces, xk, o ,offset, instance_id, speculative, inline_depth))->hashcons();\n+    (TypeInstPtr*)(new TypeInstPtr(ptr, k, interfaces, xk, o, offset, flatten_array, instance_id, speculative, inline_depth))->hashcons();\n@@ -4104,1 +4274,1 @@\n-  return make(ptr, klass(), _interfaces, klass_is_exact(), ptr == Constant ? const_oop() : NULL, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, klass(), _interfaces, klass_is_exact(), ptr == Constant ? const_oop() : NULL, _offset, _flatten_array, _instance_id, _speculative, _inline_depth);\n@@ -4115,1 +4285,1 @@\n-  return make(ptr(), klass(), _interfaces, klass_is_exact, const_oop(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), klass(), _interfaces, klass_is_exact, const_oop(), _offset, _flatten_array, _instance_id, _speculative, _inline_depth);\n@@ -4121,1 +4291,1 @@\n-  return make(_ptr, klass(),  _interfaces, _klass_is_exact, const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, _klass_is_exact, const_oop(), _offset, _flatten_array, instance_id, _speculative, _inline_depth);\n@@ -4128,1 +4298,1 @@\n-  int off = meet_offset(tinst->offset());\n+  Offset off = meet_offset(tinst->offset());\n@@ -4153,1 +4323,1 @@\n-    else if (loaded->ptr() == TypePtr::AnyNull)  { return make(ptr, unloaded->klass(), interfaces, false, NULL, off, instance_id, speculative, depth); }\n+    else if (loaded->ptr() == TypePtr::AnyNull)  { return make(ptr, unloaded->klass(), interfaces, false, NULL, off, false, instance_id, speculative, depth); }\n@@ -4214,1 +4384,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4223,1 +4393,1 @@\n-                  (ptr == Constant ? const_oop() : NULL), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : NULL), offset, flatten_array(), instance_id, speculative, depth);\n@@ -4239,1 +4409,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4251,1 +4421,1 @@\n-                  (ptr == Constant ? const_oop() : NULL), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : NULL), offset, flatten_array(), instance_id, speculative, depth);\n@@ -4279,1 +4449,1 @@\n-    int off = meet_offset(tinst->offset());\n+    Offset off = meet_offset(tinst->offset());\n@@ -4291,0 +4461,1 @@\n+    bool res_flatten_array = false;\n@@ -4292,1 +4463,1 @@\n-    MeetResult kind = meet_instptr(ptr, interfaces, this, tinst, res_klass, res_xk);\n+    MeetResult kind = meet_instptr(ptr, interfaces, this, tinst, res_klass, res_xk, res_flatten_array);\n@@ -4333,1 +4504,1 @@\n-      res = make(ptr, res_klass, interfaces, res_xk, o, off, instance_id, speculative, depth);\n+      res = make(ptr, res_klass, interfaces, res_xk, o, off, res_flatten_array, instance_id, speculative, depth);\n@@ -4345,1 +4516,1 @@\n-                      ciKlass*& res_klass, bool& res_xk) {\n+                                                            ciKlass*& res_klass, bool& res_xk, bool& res_flatten_array) {\n@@ -4348,0 +4519,4 @@\n+  bool this_flatten_array = this_type->flatten_array();\n+  bool other_flatten_array = other_type->flatten_array();\n+  bool this_flatten_array_orig = this_flatten_array;\n+  bool other_flatten_array_orig = other_flatten_array;\n@@ -4358,1 +4533,1 @@\n-  if (ptr != Constant && this_klass->equals(other_klass) && this_xk == other_xk) {\n+  if (ptr != Constant && this_klass->equals(other_klass) && this_xk == other_xk && this_flatten_array == other_flatten_array) {\n@@ -4361,0 +4536,1 @@\n+    res_flatten_array = this_flatten_array;\n@@ -4396,0 +4572,1 @@\n+  bool flat_array = false;\n@@ -4397,1 +4574,0 @@\n-\n@@ -4401,1 +4577,2 @@\n-  } else if (!other_xk && this_type->is_meet_subtype_of(other_type)) {\n+    flat_array = below_centerline(ptr) ? (this_flatten_array && other_flatten_array) : (this_flatten_array || other_flatten_array);\n+  } else if (!other_xk && this_type->is_meet_subtype_of(other_type) && (!other_flatten_array || this_flatten_array)) {\n@@ -4404,1 +4581,2 @@\n-  } else if(!this_xk && other_type->is_meet_subtype_of(this_type)) {\n+    flat_array = this_flatten_array;\n+  } else if (!this_xk && other_type->is_meet_subtype_of(this_type) && (!this_flatten_array || other_flatten_array)) {\n@@ -4407,0 +4585,1 @@\n+    flat_array = other_flatten_array;\n@@ -4413,0 +4592,1 @@\n+      this_flatten_array = other_flatten_array = flat_array;\n@@ -4416,0 +4596,1 @@\n+      this_flatten_array = other_flatten_array;\n@@ -4419,0 +4600,1 @@\n+      other_flatten_array = this_flatten_array;\n@@ -4421,0 +4603,1 @@\n+      this_flatten_array = flat_array;\n@@ -4431,0 +4614,1 @@\n+    res_flatten_array = this_flatten_array;\n@@ -4447,0 +4631,1 @@\n+  res_flatten_array = this_flatten_array_orig && other_flatten_array_orig;\n@@ -4452,1 +4637,1 @@\n-ciType* TypeInstPtr::java_mirror_type() const {\n+ciType* TypeInstPtr::java_mirror_type(bool* is_val_mirror) const {\n@@ -4458,2 +4643,1 @@\n-\n-  return const_oop()->as_instance()->java_mirror_type();\n+  return const_oop()->as_instance()->java_mirror_type(is_val_mirror);\n@@ -4467,1 +4651,1 @@\n-  return new TypeInstPtr(dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeInstPtr(dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), flatten_array(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -4476,0 +4660,1 @@\n+    flatten_array() == p->flatten_array() &&\n@@ -4483,1 +4668,1 @@\n-  int hash = java_add(java_add((jint)klass()->hash(), (jint)TypeOopPtr::hash()), _interfaces.hash());\n+  int hash = java_add(java_add(java_add((jint)klass()->hash(), (jint)TypeOopPtr::hash()), _interfaces.hash()), (jint)flatten_array());\n@@ -4538,5 +4723,1 @@\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      st->print(\"+any\");\n-    else if( _offset == OffsetTop ) st->print(\"+unknown\");\n-    else st->print(\"+%d\", _offset);\n-  }\n+  _offset.dump2(st);\n@@ -4545,0 +4726,5 @@\n+\n+  if (flatten_array() && !klass()->is_inlinetype()) {\n+    st->print(\" (flatten array)\");\n+  }\n+\n@@ -4557,1 +4743,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), xadd_offset(offset),\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), xadd_offset(offset), flatten_array(),\n@@ -4562,1 +4748,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), offset,\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), Offset(offset), flatten_array(),\n@@ -4571,1 +4757,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset,\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flatten_array(),\n@@ -4579,1 +4765,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flatten_array(), _instance_id, _speculative, depth);\n@@ -4584,1 +4770,5 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flatten_array(), instance_id, _speculative, _inline_depth);\n+}\n+\n+const TypeInstPtr *TypeInstPtr::cast_to_flatten_array() const {\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, true, _instance_id, _speculative, _inline_depth);\n@@ -4601,1 +4791,1 @@\n-  return TypeInstKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, klass(), _interfaces, 0);\n+  return TypeInstKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, klass(), _interfaces, Offset(0), flatten_array());\n@@ -4646,1 +4836,0 @@\n-\n@@ -4678,0 +4867,1 @@\n+const TypeAryPtr *TypeAryPtr::INLINES;\n@@ -4680,1 +4870,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4690,1 +4880,4 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, NULL, ary, k, xk, offset, instance_id, false, speculative, inline_depth))->hashcons();\n+  if (k != NULL && k->is_flat_array_klass() && !ary->_flat) {\n+    k = NULL;\n+  }\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, NULL, ary, k, xk, offset, field_offset, instance_id, false, speculative, inline_depth))->hashcons();\n@@ -4694,1 +4887,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4706,1 +4899,4 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n+  if (k != NULL && k->is_flat_array_klass() && !ary->_flat) {\n+    k = NULL;\n+  }\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, field_offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n@@ -4712,1 +4908,1 @@\n-  return make(ptr, ptr == Constant ? const_oop() : NULL, _ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, ptr == Constant ? const_oop() : NULL, _ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4720,1 +4916,1 @@\n-  return make(ptr(), const_oop(), _ary, klass(), klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), const_oop(), _ary, klass(), klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4726,1 +4922,1 @@\n-  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4782,2 +4978,54 @@\n-  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable(), is_flat(), is_not_flat(), is_not_null_free());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+\/\/-------------------------------cast_to_not_flat------------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_flat(bool not_flat) const {\n+  if (not_flat == is_not_flat()) {\n+    return this;\n+  }\n+  assert(!not_flat || !is_flat(), \"inconsistency\");\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), is_flat(), not_flat, is_not_null_free());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+\/\/-------------------------------cast_to_not_null_free-------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_null_free(bool not_null_free) const {\n+  if (not_null_free == is_not_null_free()) {\n+    return this;\n+  }\n+  assert(!not_null_free || !is_flat(), \"inconsistency\");\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), is_flat(), \/* not_flat= *\/ not_null_free ? true : is_not_flat(), not_null_free);\n+  const TypeAryPtr* res = make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset,\n+                               _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+  if (res->speculative() == res->remove_speculative()) {\n+    return res->remove_speculative();\n+  }\n+  return res;\n+}\n+\n+\/\/---------------------------------update_properties---------------------------\n+const TypeAryPtr* TypeAryPtr::update_properties(const TypeAryPtr* from) const {\n+  if ((from->is_flat()          && is_not_flat()) ||\n+      (from->is_not_flat()      && is_flat()) ||\n+      (from->is_null_free()     && is_not_null_free()) ||\n+      (from->is_not_null_free() && is_null_free())) {\n+    return NULL; \/\/ Inconsistent properties\n+  } else if (from->is_not_null_free()) {\n+    return cast_to_not_null_free(); \/\/ Implies not flat\n+  } else if (from->is_not_flat()) {\n+    return cast_to_not_flat();\n+  }\n+  return this;\n+}\n+\n+jint TypeAryPtr::flat_layout_helper() const {\n+  return klass()->as_flat_array_klass()->layout_helper();\n+}\n+\n+int TypeAryPtr::flat_elem_size() const {\n+  return klass()->as_flat_array_klass()->element_byte_size();\n+}\n+\n+int TypeAryPtr::flat_log_elem_size() const {\n+  return klass()->as_flat_array_klass()->log2_element_size();\n@@ -4799,1 +5047,1 @@\n-  const TypeAry* new_ary = TypeAry::make(elem, size(), stable);\n+  const TypeAry* new_ary = TypeAry::make(elem, size(), stable, is_flat(), is_not_flat(), is_not_null_free());\n@@ -4801,1 +5049,1 @@\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4821,2 +5069,2 @@\n-  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n+  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable(), is_flat(), is_not_flat(), is_not_null_free());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n@@ -4831,1 +5079,2 @@\n-    TypeOopPtr::eq(p);  \/\/ Check sub-parts\n+    TypeOopPtr::eq(p) &&\/\/ Check sub-parts\n+    _field_offset == p->_field_offset;\n@@ -4837,1 +5086,1 @@\n-  return (intptr_t)_ary + TypeOopPtr::hash();\n+  return (intptr_t)_ary + TypeOopPtr::hash() + _field_offset.get();\n@@ -4881,1 +5130,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4890,1 +5139,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4904,1 +5153,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4920,1 +5169,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4934,1 +5183,2 @@\n-    int off = meet_offset(tap->offset());\n+    Offset off = meet_offset(tap->offset());\n+    Offset field_off = meet_field_offset(tap->field_offset());\n@@ -4943,0 +5193,3 @@\n+    bool res_flat = false;\n+    bool res_not_flat = false;\n+    bool res_not_null_free = false;\n@@ -4944,1 +5197,1 @@\n-    if (meet_aryptr(ptr, elem, this, tap, res_klass, res_xk) == NOT_SUBTYPE) {\n+    if (meet_aryptr(ptr, elem, this, tap, res_klass, res_xk, res_flat, res_not_flat, res_not_null_free) == NOT_SUBTYPE) {\n@@ -4946,0 +5199,14 @@\n+    } else if (this->is_flat() != tap->is_flat()) {\n+      \/\/ Meeting flattened inline type array with non-flattened array. Adjust (field) offset accordingly.\n+      if (tary->_flat) {\n+        \/\/ Result is flattened\n+        off = Offset(is_flat() ? offset() : tap->offset());\n+        field_off = is_flat() ? field_offset() : tap->field_offset();\n+      } else if (below_centerline(ptr)) {\n+        \/\/ Result is non-flattened\n+        off = Offset(flattened_offset()).meet(Offset(tap->flattened_offset()));\n+        field_off = Offset::bottom;\n+      } else if (flattened_offset() == tap->flattened_offset()) {\n+        off = Offset(!is_flat() ? offset() : tap->offset());\n+        field_off = !is_flat() ? field_offset() : tap->field_offset();\n+      }\n@@ -4963,1 +5230,1 @@\n-    return make(ptr, o, TypeAry::make(elem, tary->_size, tary->_stable), res_klass, res_xk, off, instance_id, speculative, depth);\n+    return make(ptr, o, TypeAry::make(elem, tary->_size, tary->_stable, res_flat, res_not_flat, res_not_null_free), res_klass, res_xk, off, field_off, instance_id, speculative, depth);\n@@ -4969,1 +5236,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4984,2 +5251,2 @@\n-      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces.contains(tp_interfaces) && !tp->klass_is_exact()) {\n-        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces.contains(tp_interfaces) && !tp->klass_is_exact() && !tp->flatten_array()) {\n+        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4991,1 +5258,1 @@\n-        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, NULL,offset, instance_id, speculative, depth);\n+        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, NULL, offset, false, instance_id, speculative, depth);\n@@ -5003,1 +5270,1 @@\n-        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces.contains(tp_interfaces) && !tp->klass_is_exact()) {\n+        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces.contains(tp_interfaces) && !tp->klass_is_exact() && !tp->flatten_array()) {\n@@ -5006,1 +5273,1 @@\n-                      _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                      _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -5018,1 +5285,1 @@\n-      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, NULL, offset, instance_id, speculative, depth);\n+      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, NULL, offset, false, instance_id, speculative, depth);\n@@ -5027,2 +5294,2 @@\n-template<class T> TypePtr::MeetResult TypePtr::meet_aryptr(PTR& ptr, const Type*& elem, const T* this_ary,\n-                                                           const T* other_ary, ciKlass*& res_klass, bool& res_xk) {\n+template<class T> TypePtr::MeetResult TypePtr::meet_aryptr(PTR& ptr, const Type*& elem, const T* this_ary, const T* other_ary,\n+                                                           ciKlass*& res_klass, bool& res_xk, bool &res_flat, bool& res_not_flat, bool& res_not_null_free) {\n@@ -5038,0 +5305,6 @@\n+  bool this_flat = this_ary->is_flat();\n+  bool this_not_flat = this_ary->is_not_flat();\n+  bool other_flat = other_ary->is_flat();\n+  bool other_not_flat = other_ary->is_not_flat();\n+  bool this_not_null_free = this_ary->is_not_null_free();\n+  bool other_not_null_free = other_ary->is_not_null_free();\n@@ -5040,0 +5313,4 @@\n+  res_flat = this_flat && other_flat;\n+  res_not_flat = this_not_flat && other_not_flat;\n+  res_not_null_free = this_not_null_free && other_not_null_free;\n+\n@@ -5043,3 +5320,3 @@\n-    if (this_top_or_bottom)\n-      res_klass = other_klass;\n-    else if (other_top_or_bottom || other_klass == this_klass) {\n+      if (this_top_or_bottom) {\n+        res_klass = other_klass;\n+      } else if (other_top_or_bottom || other_klass == this_klass) {\n@@ -5082,0 +5359,3 @@\n+        if (this_ary->is_flat()) {\n+          elem = this_ary->elem();\n+        }\n@@ -5085,1 +5365,1 @@\n-      return result;\n+      break;\n@@ -5088,2 +5368,2 @@\n-          res_xk = true;\n-      } else if(above_centerline(this_ptr)) {\n+        res_xk = true;\n+      } else if (above_centerline(this_ptr)) {\n@@ -5094,0 +5374,4 @@\n+        \/\/ Even if MyValue is exact, [LMyValue is not exact due to [QMyValue <: [LMyValue.\n+        if (res_xk && !res_not_null_free) {\n+          res_xk = false;\n+        }\n@@ -5095,1 +5379,1 @@\n-      return result;\n+      break;\n@@ -5102,0 +5386,3 @@\n+        if (other_ary->is_flat()) {\n+          elem = other_ary->elem();\n+        }\n@@ -5105,0 +5392,4 @@\n+        \/\/ Even if MyValue is exact, [LMyValue is not exact due to [QMyValue <: [LMyValue.\n+        if (res_xk && !res_not_null_free) {\n+          res_xk = false;\n+        }\n@@ -5106,1 +5397,1 @@\n-      return result;\n+      break;\n@@ -5119,1 +5410,10 @@\n-  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(),_klass, _klass_is_exact, dual_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(), _klass, _klass_is_exact, dual_offset(), dual_field_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+}\n+\n+Type::Offset TypeAryPtr::meet_field_offset(const Type::Offset offset) const {\n+  return _field_offset.meet(offset);\n+}\n+\n+\/\/------------------------------dual_offset------------------------------------\n+Type::Offset TypeAryPtr::dual_field_offset() const {\n+  return _field_offset.dual();\n@@ -5147,1 +5447,10 @@\n-  if( _offset != 0 ) {\n+  if (is_flat()) {\n+    st->print(\":flat\");\n+    st->print(\"(\");\n+    _field_offset.dump2(st);\n+    st->print(\")\");\n+  }\n+  if (is_null_free()) {\n+    st->print(\":null_free\");\n+  }\n+  if (offset() != 0) {\n@@ -5149,3 +5458,3 @@\n-    if( _offset == OffsetTop )       st->print(\"+undefined\");\n-    else if( _offset == OffsetBot )  st->print(\"+any\");\n-    else if( _offset < header_size ) st->print(\"+%d\", _offset);\n+    if( _offset == Offset::top )       st->print(\"+undefined\");\n+    else if( _offset == Offset::bottom )  st->print(\"+any\");\n+    else if( offset() < header_size ) st->print(\"+%d\", offset());\n@@ -5159,1 +5468,1 @@\n-        st->print(\"[%d]\", (_offset - array_base)\/elem_size);\n+        st->print(\"[%d]\", (offset() - array_base)\/elem_size);\n@@ -5176,0 +5485,4 @@\n+  \/\/ FIXME: Does this belong here? Or in the meet code itself?\n+  if (is_flat() && is_not_flat()) {\n+    return true;\n+  }\n@@ -5181,1 +5494,1 @@\n-  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _instance_id, add_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _field_offset, _instance_id, add_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n@@ -5185,1 +5498,1 @@\n-  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, offset, _instance_id, with_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, Offset(offset), _field_offset, _instance_id, with_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n@@ -5189,1 +5502,1 @@\n-  return make(_ptr, _const_oop, ary, _klass, _klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(_ptr, _const_oop, ary, _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -5197,1 +5510,14 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, NULL, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, NULL, _inline_depth, _is_autobox_cache);\n+}\n+\n+const Type* TypeAryPtr::cleanup_speculative() const {\n+  if (speculative() == NULL) {\n+    return this;\n+  }\n+  \/\/ Keep speculative part if it contains information about flat-\/nullability\n+  const TypeAryPtr* spec_aryptr = speculative()->isa_aryptr();\n+  if (spec_aryptr != NULL && !above_centerline(spec_aryptr->ptr()) &&\n+      (spec_aryptr->is_not_flat() || spec_aryptr->is_not_null_free())) {\n+    return this;\n+  }\n+  return TypeOopPtr::cleanup_speculative();\n@@ -5204,1 +5530,44 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, depth, _is_autobox_cache);\n+}\n+\n+const TypeAryPtr* TypeAryPtr::with_field_offset(int offset) const {\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, Offset(offset), _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+const TypePtr* TypeAryPtr::add_field_offset_and_offset(intptr_t offset) const {\n+  int adj = 0;\n+  if (is_flat() && offset != Type::OffsetBot && offset != Type::OffsetTop) {\n+    if (_offset.get() != OffsetBot && _offset.get() != OffsetTop) {\n+      adj = _offset.get();\n+      offset += _offset.get();\n+    }\n+    uint header = arrayOopDesc::base_offset_in_bytes(T_OBJECT);\n+    if (_field_offset.get() != OffsetBot && _field_offset.get() != OffsetTop) {\n+      offset += _field_offset.get();\n+      if (_offset.get() == OffsetBot || _offset.get() == OffsetTop) {\n+        offset += header;\n+      }\n+    }\n+    if (elem()->make_oopptr()->is_inlinetypeptr() && (offset >= (intptr_t)header || offset < 0)) {\n+      \/\/ Try to get the field of the inline type array element we are pointing to\n+      ciInlineKlass* vk = elem()->inline_klass();\n+      int shift = flat_log_elem_size();\n+      int mask = (1 << shift) - 1;\n+      intptr_t field_offset = ((offset - header) & mask);\n+      ciField* field = vk->get_field_by_offset(field_offset + vk->first_field_offset(), false);\n+      if (field != NULL) {\n+        return with_field_offset(field_offset)->add_offset(offset - field_offset - adj);\n+      }\n+    }\n+  }\n+  return add_offset(offset - adj);\n+}\n+\n+\/\/ Return offset incremented by field_offset for flattened inline type arrays\n+const int TypeAryPtr::flattened_offset() const {\n+  int offset = _offset.get();\n+  if (offset != Type::OffsetBot && offset != Type::OffsetTop &&\n+      _field_offset != Offset::bottom && _field_offset != Offset::top) {\n+    offset += _field_offset.get();\n+  }\n+  return offset;\n@@ -5209,1 +5578,1 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth);\n@@ -5214,0 +5583,1 @@\n+\n@@ -5304,1 +5674,0 @@\n-\n@@ -5388,1 +5757,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -5408,1 +5777,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -5410,1 +5779,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -5461,1 +5830,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5489,1 +5858,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5522,1 +5891,1 @@\n-  switch( _offset ) {\n+  switch (offset()) {\n@@ -5526,1 +5895,1 @@\n-  default:        st->print(\"+%d\",_offset); break;\n+  default:        st->print(\"+%d\",offset()); break;\n@@ -5536,1 +5905,1 @@\n-TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, int offset):\n+TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, Offset offset):\n@@ -5541,1 +5910,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -5544,1 +5913,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -5549,1 +5918,1 @@\n-const TypeMetadataPtr *TypeMetadataPtr::make(PTR ptr, ciMetadata* m, int offset) {\n+const TypeMetadataPtr* TypeMetadataPtr::make(PTR ptr, ciMetadata* m, Offset offset) {\n@@ -5560,1 +5929,3 @@\n-    if (elem->is_klassptr()->klass_is_exact()) {\n+    if (elem->is_klassptr()->klass_is_exact() &&\n+        \/\/ Even if MyValue is exact, [LMyValue is not exact due to [QMyValue <: [LMyValue.\n+        (is_null_free() || is_flat() || !_ary->_elem->make_oopptr()->is_inlinetypeptr())) {\n@@ -5564,1 +5935,1 @@\n-  return TypeAryKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, elem, klass(), 0);\n+  return TypeAryKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, elem, klass(), Offset(0), is_not_flat(), is_not_null_free(), is_null_free());\n@@ -5567,1 +5938,1 @@\n-const TypeKlassPtr* TypeKlassPtr::make(ciKlass *klass, InterfaceHandling interface_handling) {\n+const TypeKlassPtr* TypeKlassPtr::make(ciKlass* klass, InterfaceHandling interface_handling) {\n@@ -5574,1 +5945,1 @@\n-const TypeKlassPtr* TypeKlassPtr::make(PTR ptr, ciKlass* klass, int offset, InterfaceHandling interface_handling) {\n+const TypeKlassPtr* TypeKlassPtr::make(PTR ptr, ciKlass* klass, Offset offset, InterfaceHandling interface_handling) {\n@@ -5582,3 +5953,1 @@\n-\n-\/\/------------------------------TypeKlassPtr-----------------------------------\n-TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const InterfaceSet& interfaces, int offset)\n+TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const InterfaceSet& interfaces, Offset offset)\n@@ -5587,1 +5956,1 @@\n-         klass->is_type_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"no interface here\");\n+         klass->is_type_array_klass() || klass->is_flat_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"no interface here\");\n@@ -5627,1 +5996,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -5659,1 +6028,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert( offset() >= 0, \"\" );\n@@ -5661,1 +6030,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -5705,5 +6074,2 @@\n-\n-  if (_offset) {               \/\/ Dump offset, if any\n-    if (_offset == OffsetBot)      { st->print(\"+any\"); }\n-    else if (_offset == OffsetTop) { st->print(\"+unknown\"); }\n-    else                            { st->print(\"+%d\", _offset); }\n+  if (Verbose) {\n+    if (isa_instklassptr() && is_instklassptr()->flatten_array()) st->print(\":flatten array\");\n@@ -5711,1 +6077,1 @@\n-\n+  _offset.dump2(st);\n@@ -5727,0 +6093,1 @@\n+    flatten_array() == p->flatten_array() &&\n@@ -5731,1 +6098,1 @@\n-  return java_add((jint)klass()->hash(), TypeKlassPtr::hash());\n+  return java_add(java_add((jint)klass()->hash(), TypeKlassPtr::hash()), (jint)flatten_array());\n@@ -5734,1 +6101,3 @@\n-const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, int offset) {\n+const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, Offset offset, bool flatten_array) {\n+  flatten_array = flatten_array || k->flatten_array();\n+\n@@ -5736,1 +6105,1 @@\n-    (TypeInstKlassPtr*)(new TypeInstKlassPtr(ptr, k, interfaces, offset))->hashcons();\n+    (TypeInstKlassPtr*)(new TypeInstKlassPtr(ptr, k, interfaces, offset, flatten_array))->hashcons();\n@@ -5743,2 +6112,2 @@\n-const TypePtr* TypeInstKlassPtr::add_offset( intptr_t offset ) const {\n-  return make( _ptr, klass(), _interfaces, xadd_offset(offset) );\n+const TypePtr *TypeInstKlassPtr::add_offset( intptr_t offset ) const {\n+  return make(_ptr, klass(), _interfaces, xadd_offset(offset), flatten_array());\n@@ -5748,1 +6117,1 @@\n-  return make(_ptr, klass(), _interfaces, offset);\n+  return make(_ptr, klass(), _interfaces, Offset(offset), flatten_array());\n@@ -5755,1 +6124,1 @@\n-  return make(ptr, _klass, _interfaces, _offset);\n+  return make(ptr, _klass, _interfaces, _offset, flatten_array());\n@@ -5771,1 +6140,1 @@\n-  return make(klass_is_exact ? Constant : NotNull, k, _interfaces, _offset);\n+  return make(klass_is_exact ? Constant : NotNull, k, _interfaces, _offset, flatten_array());\n@@ -5806,1 +6175,1 @@\n-  return TypeInstPtr::make(TypePtr::BotPTR, k, interfaces, xk, NULL, 0);\n+  return TypeInstPtr::make(TypePtr::BotPTR, k, interfaces, xk, NULL, Offset(0), flatten_array() && !klass()->is_inlinetype());\n@@ -5839,1 +6208,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5847,1 +6216,1 @@\n-      return make( ptr, klass(), _interfaces, offset );\n+      return make(ptr, klass(), _interfaces, offset, flatten_array());\n@@ -5860,1 +6229,1 @@\n-    return TypePtr::BOTTOM;\n+      return TypePtr::BOTTOM;\n@@ -5880,1 +6249,1 @@\n-    int  off     = meet_offset(tkls->offset());\n+    Offset  off     = meet_offset(tkls->offset());\n@@ -5886,1 +6255,2 @@\n-    switch(meet_instptr(ptr, interfaces, this, tkls, res_klass, res_xk)) {\n+    bool res_flatten_array = false;\n+    switch(meet_instptr(ptr, interfaces, this, tkls, res_klass, res_xk, res_flatten_array)) {\n@@ -5894,1 +6264,1 @@\n-        const Type* res = make(ptr, res_klass, interfaces, off);\n+        const Type* res = make(ptr, res_klass, interfaces, off, res_flatten_array);\n@@ -5903,1 +6273,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5916,1 +6286,1 @@\n-        return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset);\n+        return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset, tp->is_not_flat(), tp->is_not_null_free(), tp->is_null_free());\n@@ -5921,1 +6291,1 @@\n-        return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+        return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -5935,2 +6305,1 @@\n-          return TypeAryKlassPtr::make(ptr,\n-                                       tp->elem(), tp->klass(), offset);\n+          return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset, tp->is_not_flat(), tp->is_not_null_free(), tp->is_null_free());\n@@ -5944,1 +6313,1 @@\n-      return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+      return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -5956,1 +6325,1 @@\n-  return new TypeInstKlassPtr(dual_ptr(), klass(), _interfaces, dual_offset());\n+  return new TypeInstKlassPtr(dual_ptr(), klass(), _interfaces, dual_offset(), flatten_array());\n@@ -6060,0 +6429,15 @@\n+bool TypeInstKlassPtr::can_be_inline_array() const {\n+  return _klass->equals(ciEnv::current()->Object_klass()) && TypeAryKlassPtr::_array_interfaces->contains(_interfaces);\n+}\n+\n+bool TypeAryKlassPtr::can_be_inline_array() const {\n+  return _elem->isa_instklassptr() && _elem->is_instklassptr()->_klass->can_be_inline_klass();\n+}\n+\n+bool TypeInstPtr::can_be_inline_array() const {\n+  return _klass->equals(ciEnv::current()->Object_klass()) && TypeAryPtr::_array_interfaces->contains(_interfaces);\n+}\n+\n+bool TypeAryPtr::can_be_inline_array() const {\n+  return elem()->make_ptr() && elem()->make_ptr()->isa_instptr() && elem()->make_ptr()->is_instptr()->_klass->can_be_inline_klass();\n+}\n@@ -6061,2 +6445,2 @@\n-const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, const Type* elem, ciKlass* k, int offset) {\n-  return (TypeAryKlassPtr*)(new TypeAryKlassPtr(ptr, elem, k, offset))->hashcons();\n+const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, const Type* elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool null_free) {\n+  return (TypeAryKlassPtr*)(new TypeAryKlassPtr(ptr, elem, k, offset, not_flat, not_null_free, null_free))->hashcons();\n@@ -6065,1 +6449,1 @@\n-const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, ciKlass* k, int offset, InterfaceHandling interface_handling) {\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool not_flat, bool not_null_free, bool null_free) {\n@@ -6069,2 +6453,6 @@\n-    const TypeKlassPtr *etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n-    return TypeAryKlassPtr::make(ptr, etype, NULL, offset);\n+    const TypeKlassPtr* etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n+    \/\/ Even if MyValue is exact, [LMyValue is not exact due to [QMyValue <: [LMyValue.\n+    if (etype->klass_is_exact() && etype->isa_instklassptr() && etype->is_instklassptr()->klass()->is_inlinetype() && !null_free) {\n+      etype = TypeInstKlassPtr::make(NotNull, etype->is_instklassptr()->klass(), Offset(etype->is_instklassptr()->offset()));\n+    }\n+    return TypeAryKlassPtr::make(ptr, etype, NULL, offset, not_flat, not_null_free, null_free);\n@@ -6074,1 +6462,5 @@\n-    return TypeAryKlassPtr::make(ptr, etype, k, offset);\n+    return TypeAryKlassPtr::make(ptr, etype, k, offset, not_flat, not_null_free, null_free);\n+  } else if (k->is_flat_array_klass()) {\n+    ciKlass* eklass = k->as_flat_array_klass()->element_klass();\n+    const TypeKlassPtr* etype = TypeKlassPtr::make(eklass);\n+    return TypeAryKlassPtr::make(ptr, etype, k, offset, not_flat, not_null_free, null_free);\n@@ -6081,0 +6473,11 @@\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling) {\n+  bool null_free = k->as_array_klass()->is_elem_null_free();\n+  bool not_null_free = (ptr == Constant) ? !null_free : !k->is_flat_array_klass() && (k->is_type_array_klass() || !k->as_array_klass()->element_klass()->can_be_inline_klass(false));\n+\n+  bool not_flat = !UseFlatArray || not_null_free || (k->as_array_klass()->element_klass() != NULL &&\n+                                                     k->as_array_klass()->element_klass()->is_inlinetype() &&\n+                                                     !k->as_array_klass()->element_klass()->flatten_array());\n+\n+  return TypeAryKlassPtr::make(ptr, k, offset, interface_handling, not_flat, not_null_free, null_free);\n+}\n+\n@@ -6082,1 +6485,1 @@\n-  return TypeAryKlassPtr::make(Constant, klass, 0, interface_handling);\n+  return TypeAryKlassPtr::make(Constant, klass, Offset(0), interface_handling);\n@@ -6091,0 +6494,3 @@\n+    _not_flat == p->_not_flat &&\n+    _not_null_free == p->_not_null_free &&\n+    _null_free == p->_null_free &&\n@@ -6097,1 +6503,2 @@\n-  return (intptr_t)_elem + TypeKlassPtr::hash();\n+  return (intptr_t)_elem + TypeKlassPtr::hash() + (_not_flat ? 43 : 0) +\n+      (_not_null_free ? 44 : 0) + (_null_free ? 45 : 0);\n@@ -6113,1 +6520,6 @@\n-  if ((tinst = el->isa_instptr()) != NULL) {\n+  if (is_flat() && el->is_inlinetypeptr()) {\n+    \/\/ Klass is required by TypeAryPtr::flat_layout_helper() and others\n+    if (el->inline_klass() != NULL) {\n+      k_ary = ciArrayKlass::make(el->inline_klass(), \/* null_free *\/ true);\n+    }\n+  } else if ((tinst = el->isa_instptr()) != NULL) {\n@@ -6185,1 +6597,1 @@\n-    k = ciObjArrayKlass::make(k);\n+    k = ciArrayKlass::make(k, is_null_free());\n@@ -6205,1 +6617,1 @@\n-  return make(_ptr, elem(), klass(), xadd_offset(offset));\n+  return make(_ptr, elem(), klass(), xadd_offset(offset), is_not_flat(), is_not_null_free(), _null_free);\n@@ -6209,1 +6621,1 @@\n-  return make(_ptr, elem(), klass(), offset);\n+  return make(_ptr, elem(), klass(), Offset(offset), is_not_flat(), is_not_null_free(), _null_free);\n@@ -6216,1 +6628,1 @@\n-  return make(ptr, elem(), _klass, _offset);\n+  return make(ptr, elem(), _klass, _offset, is_not_flat(), is_not_null_free(), _null_free);\n@@ -6224,0 +6636,4 @@\n+  \/\/ Even if MyValue is exact, [LMyValue is not exact due to [QMyValue <: [LMyValue.\n+  if (tk->isa_instklassptr() && tk->klass()->is_inlinetype() && !is_null_free()) {\n+    return false;\n+  }\n@@ -6230,1 +6646,4 @@\n-  if (must_be_exact()) return this;  \/\/ cannot clear xk\n+  if (must_be_exact() && !klass_is_exact) return this;  \/\/ cannot clear xk\n+  if (klass_is_exact == this->klass_is_exact()) {\n+    return this;\n+  }\n@@ -6236,1 +6655,16 @@\n-  return make(klass_is_exact ? Constant : NotNull, elem, k, _offset);\n+  bool not_flat = is_not_flat();\n+  bool not_null_free = is_not_null_free();\n+  if (_elem->isa_klassptr()) {\n+    if (klass_is_exact || _elem->isa_aryklassptr()) {\n+      assert(!is_null_free() && !is_flat(), \"null-free (or flat) inline type arrays should always be exact\");\n+      \/\/ An array can't be null-free (or flat) if the klass is exact\n+      not_null_free = true;\n+      not_flat = true;\n+    } else {\n+      \/\/ Klass is not exact (anymore), re-compute null-free\/flat properties\n+      const TypeOopPtr* exact_etype = TypeOopPtr::make_from_klass_unique(_elem->is_instklassptr()->instance_klass());\n+      not_null_free = !exact_etype->can_be_inline_type();\n+      not_flat = !UseFlatArray || not_null_free || (exact_etype->is_inlinetypeptr() && !exact_etype->inline_klass()->flatten_array());\n+    }\n+  }\n+  return make(klass_is_exact ? Constant : NotNull, elem, k, _offset, not_flat, not_null_free, _null_free);\n@@ -6253,1 +6687,5 @@\n-  return TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(el, TypeInt::POS), k, xk, 0);\n+  bool null_free = _null_free;\n+  if (null_free && el->isa_ptr()) {\n+    el = el->is_ptr()->join_speculative(TypePtr::NOTNULL);\n+  }\n+  return TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(el, TypeInt::POS, false, is_flat(), is_not_flat(), is_not_null_free()), k, xk, Offset(0));\n@@ -6287,1 +6725,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -6295,1 +6733,1 @@\n-      return make( ptr, _elem, klass(), offset );\n+      return make(ptr, _elem, klass(), offset, is_not_flat(), is_not_null_free(), is_null_free());\n@@ -6328,1 +6766,1 @@\n-    int off = meet_offset(tap->offset());\n+    Offset off = meet_offset(tap->offset());\n@@ -6330,1 +6768,0 @@\n-\n@@ -6334,1 +6771,5 @@\n-    meet_aryptr(ptr, elem, this, tap, res_klass, res_xk);\n+    bool res_flat = false;\n+    bool res_not_flat = false;\n+    bool res_not_null_free = false;\n+    MeetResult res = meet_aryptr(ptr, elem, this, tap,\n+                                 res_klass, res_xk, res_flat, res_not_flat, res_not_null_free);\n@@ -6336,1 +6777,13 @@\n-    return make(ptr, elem, res_klass, off);\n+    bool null_free = meet_null_free(tap->_null_free);\n+    if (res == NOT_SUBTYPE) {\n+      null_free = false;\n+    } else if (res == SUBTYPE) {\n+      if (above_centerline(tap->ptr()) && !above_centerline(this->ptr())) {\n+        null_free = _null_free;\n+      } else if (above_centerline(this->ptr()) && !above_centerline(tap->ptr())) {\n+        null_free = tap->_null_free;\n+      } else if (above_centerline(this->ptr()) && above_centerline(tap->ptr())) {\n+        null_free = _null_free || tap->_null_free;\n+      }\n+    }\n+    return make(ptr, elem, res_klass, off, res_not_flat, res_not_null_free, null_free);\n@@ -6340,1 +6793,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -6353,1 +6806,1 @@\n-        return TypeAryKlassPtr::make(ptr, _elem, _klass, offset);\n+        return TypeAryKlassPtr::make(ptr, _elem, _klass, offset, is_not_flat(), is_not_null_free(), is_null_free());\n@@ -6358,1 +6811,1 @@\n-        return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+        return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -6372,1 +6825,1 @@\n-          return make(ptr, _elem, _klass, offset);\n+          return make(ptr, _elem, _klass, offset, is_not_flat(), is_not_null_free(), is_null_free());\n@@ -6380,1 +6833,1 @@\n-      return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+      return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -6417,0 +6870,3 @@\n+    if (other->is_null_free() && !this_one->is_null_free()) {\n+      return false; \/\/ [LMyValue is not a subtype of [QMyValue\n+    }\n@@ -6504,1 +6960,1 @@\n-  return new TypeAryKlassPtr(dual_ptr(), elem()->dual(), klass(), dual_offset());\n+  return new TypeAryKlassPtr(dual_ptr(), elem()->dual(), klass(), dual_offset(), !is_not_flat(), !is_not_null_free(), dual_null_free());\n@@ -6514,1 +6970,1 @@\n-    k = ciObjArrayKlass::make(k);\n+    k = ciArrayKlass::make(k, _null_free);\n@@ -6561,5 +7017,5 @@\n-\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      { st->print(\"+any\"); }\n-    else if( _offset == OffsetTop ) { st->print(\"+unknown\"); }\n-    else                            { st->print(\"+%d\", _offset); }\n+  if (is_flat()) st->print(\":flat\");\n+  if (_null_free) st->print(\":null free\");\n+  if (Verbose) {\n+    if (_not_flat) st->print(\":not flat\");\n+    if (_not_null_free) st->print(\":not null free\");\n@@ -6568,0 +7024,2 @@\n+  _offset.dump2(st);\n+\n@@ -6586,2 +7044,14 @@\n-const TypeFunc *TypeFunc::make( const TypeTuple *domain, const TypeTuple *range ) {\n-  return (TypeFunc*)(new TypeFunc(domain,range))->hashcons();\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain_sig, const TypeTuple* domain_cc,\n+                               const TypeTuple *range_sig, const TypeTuple *range_cc) {\n+  return (TypeFunc*)(new TypeFunc(domain_sig, domain_cc, range_sig, range_cc))->hashcons();\n+}\n+\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain, const TypeTuple *range) {\n+  return make(domain, domain, range, range);\n+}\n+\n+\/\/------------------------------osr_domain-----------------------------\n+const TypeTuple* osr_domain() {\n+  const Type **fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n+  return TypeTuple::make(TypeFunc::Parms+1, fields);\n@@ -6591,1 +7061,1 @@\n-const TypeFunc *TypeFunc::make(ciMethod* method) {\n+const TypeFunc* TypeFunc::make(ciMethod* method, bool is_osr_compilation) {\n@@ -6593,7 +7063,24 @@\n-  const TypeFunc* tf = C->last_tf(method); \/\/ check cache\n-  if (tf != NULL)  return tf;  \/\/ The hit rate here is almost 50%.\n-  const TypeTuple *domain;\n-  if (method->is_static()) {\n-    domain = TypeTuple::make_domain(NULL, method->signature(), ignore_interfaces);\n-  } else {\n-    domain = TypeTuple::make_domain(method->holder(), method->signature(), ignore_interfaces);\n+  const TypeFunc* tf = NULL;\n+  if (!is_osr_compilation) {\n+    tf = C->last_tf(method); \/\/ check cache\n+    if (tf != NULL)  return tf;  \/\/ The hit rate here is almost 50%.\n+  }\n+  \/\/ Inline types are not passed\/returned by reference, instead each field of\n+  \/\/ the inline type is passed\/returned as an argument. We maintain two views of\n+  \/\/ the argument\/return list here: one based on the signature (with an inline\n+  \/\/ type argument\/return as a single slot), one based on the actual calling\n+  \/\/ convention (with an inline type argument\/return as a list of its fields).\n+  bool has_scalar_args = method->has_scalarized_args() && !is_osr_compilation;\n+  \/\/ Fall back to the non-scalarized calling convention when compiling a call via a mismatching method\n+  if (method != C->method() && method->get_Method()->mismatch()) {\n+    has_scalar_args = false;\n+  }\n+  const TypeTuple* domain_sig = is_osr_compilation ? osr_domain() : TypeTuple::make_domain(method, ignore_interfaces, false);\n+  const TypeTuple* domain_cc = has_scalar_args ? TypeTuple::make_domain(method, ignore_interfaces, true) : domain_sig;\n+  ciSignature* sig = method->signature();\n+  bool has_scalar_ret = sig->return_type()->is_inlinetype() && sig->return_type()->as_inline_klass()->can_be_returned_as_fields();\n+  const TypeTuple* range_sig = TypeTuple::make_range(sig, ignore_interfaces, false);\n+  const TypeTuple* range_cc = has_scalar_ret ? TypeTuple::make_range(sig, ignore_interfaces, true) : range_sig;\n+  tf = TypeFunc::make(domain_sig, domain_cc, range_sig, range_cc);\n+  if (!is_osr_compilation) {\n+    C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -6601,3 +7088,0 @@\n-  const TypeTuple *range  = TypeTuple::make_range(method->signature(), ignore_interfaces);\n-  tf = TypeFunc::make(domain, range);\n-  C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -6638,2 +7122,4 @@\n-  return _domain == a->_domain &&\n-    _range == a->_range;\n+  return _domain_sig == a->_domain_sig &&\n+    _domain_cc == a->_domain_cc &&\n+    _range_sig == a->_range_sig &&\n+    _range_cc == a->_range_cc;\n@@ -6645,1 +7131,1 @@\n-  return (intptr_t)_domain + (intptr_t)_range;\n+  return (intptr_t)_domain_sig + (intptr_t)_domain_cc + (intptr_t)_range_sig + (intptr_t)_range_cc;\n@@ -6652,1 +7138,1 @@\n-  if( _range->cnt() <= Parms )\n+  if( _range_sig->cnt() <= Parms )\n@@ -6656,2 +7142,2 @@\n-    for (i = Parms; i < _range->cnt()-1; i++) {\n-      _range->field_at(i)->dump2(d,depth,st);\n+    for (i = Parms; i < _range_sig->cnt()-1; i++) {\n+      _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -6660,1 +7146,1 @@\n-    _range->field_at(i)->dump2(d,depth,st);\n+    _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -6669,3 +7155,3 @@\n-  if (Parms < _domain->cnt())\n-    _domain->field_at(Parms)->dump2(d,depth-1,st);\n-  for (uint i = Parms+1; i < _domain->cnt(); i++) {\n+  if (Parms < _domain_sig->cnt())\n+    _domain_sig->field_at(Parms)->dump2(d,depth-1,st);\n+  for (uint i = Parms+1; i < _domain_sig->cnt(); i++) {\n@@ -6673,1 +7159,1 @@\n-    _domain->field_at(i)->dump2(d,depth-1,st);\n+    _domain_sig->field_at(i)->dump2(d,depth-1,st);\n@@ -6693,1 +7179,1 @@\n-  if (range()->cnt() == TypeFunc::Parms) {\n+  if (range_sig()->cnt() == TypeFunc::Parms) {\n@@ -6696,1 +7182,1 @@\n-  return range()->field_at(TypeFunc::Parms)->basic_type();\n+  return range_sig()->field_at(TypeFunc::Parms)->basic_type();\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":826,"deletions":340,"binary":false,"changes":1166,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -137,0 +139,24 @@\n+  class Offset {\n+  private:\n+    int _offset;\n+\n+  public:\n+    explicit Offset(int offset) : _offset(offset) {}\n+\n+    const Offset meet(const Offset other) const;\n+    const Offset dual() const;\n+    const Offset add(intptr_t offset) const;\n+    bool operator==(const Offset& other) const {\n+      return _offset == other._offset;\n+    }\n+    bool operator!=(const Offset& other) const {\n+      return _offset != other._offset;\n+    }\n+    int get() const { return _offset; }\n+\n+    void dump2(outputStream *st) const;\n+\n+    static const Offset top;\n+    static const Offset bottom;\n+  };\n+\n@@ -327,0 +353,3 @@\n+  bool is_inlinetypeptr() const;\n+  virtual ciInlineKlass* inline_klass() const;\n+\n@@ -725,2 +754,2 @@\n-  static const TypeTuple *make_range(ciSignature *sig, InterfaceHandling interface_handling = ignore_interfaces);\n-  static const TypeTuple *make_domain(ciInstanceKlass* recv, ciSignature *sig, InterfaceHandling interface_handling);\n+  static const TypeTuple *make_range(ciSignature* sig, InterfaceHandling interface_handling = ignore_interfaces, bool ret_vt_fields = false);\n+  static const TypeTuple *make_domain(ciMethod* method, InterfaceHandling interface_handling, bool vt_fields_as_args = false);\n@@ -755,2 +784,2 @@\n-  TypeAry(const Type* elem, const TypeInt* size, bool stable) : Type(Array),\n-      _elem(elem), _size(size), _stable(stable) {}\n+  TypeAry(const Type* elem, const TypeInt* size, bool stable, bool flat, bool not_flat, bool not_null_free) : Type(Array),\n+      _elem(elem), _size(size), _stable(stable), _flat(flat), _not_flat(not_flat), _not_null_free(not_null_free) {}\n@@ -767,0 +796,6 @@\n+\n+  \/\/ Inline type array properties\n+  const bool _flat;             \/\/ Array is flattened\n+  const bool _not_flat;         \/\/ Array is never flattened\n+  const bool _not_null_free;    \/\/ Array is never null-free\n+\n@@ -770,1 +805,2 @@\n-  static const TypeAry* make(const Type* elem, const TypeInt* size, bool stable = false);\n+  static const TypeAry* make(const Type* elem, const TypeInt* size, bool stable = false,\n+                             bool flat = false, bool not_flat = false, bool not_null_free = false);\n@@ -931,1 +967,1 @@\n-  TypePtr(TYPES t, PTR ptr, int offset,\n+  TypePtr(TYPES t, PTR ptr, Offset offset,\n@@ -986,2 +1022,2 @@\n-  template<class T> static TypePtr::MeetResult meet_instptr(PTR& ptr, InterfaceSet& interfaces, const T* this_type,\n-                                                            const T* other_type, ciKlass*& res_klass, bool& res_xk);\n+  template<class T> static TypePtr::MeetResult meet_instptr(PTR& ptr, InterfaceSet& interfaces, const T* this_type, const T* other_type,\n+                                                            ciKlass*& res_klass, bool& res_xk, bool& res_flatten_array);\n@@ -990,1 +1026,1 @@\n-                                                  ciKlass*& res_klass, bool& res_xk);\n+                                                  ciKlass*& res_klass, bool& res_xk, bool &res_flat, bool &res_not_flat, bool &res_not_null_free);\n@@ -1001,1 +1037,1 @@\n-  const int _offset;            \/\/ Offset into oop, with TOP & BOT\n+  const Offset _offset;         \/\/ Offset into oop, with TOP & BOT\n@@ -1004,1 +1040,1 @@\n-  const int offset() const { return _offset; }\n+  const int offset() const { return _offset.get(); }\n@@ -1007,1 +1043,1 @@\n-  static const TypePtr *make(TYPES t, PTR ptr, int offset,\n+  static const TypePtr* make(TYPES t, PTR ptr, Offset offset,\n@@ -1016,1 +1052,1 @@\n-  int xadd_offset( intptr_t offset ) const;\n+  Type::Offset xadd_offset(intptr_t offset) const;\n@@ -1019,0 +1055,1 @@\n+  virtual const int flattened_offset() const { return offset(); }\n@@ -1026,2 +1063,2 @@\n-  int meet_offset( int offset ) const;\n-  int dual_offset( ) const;\n+  Offset meet_offset(int offset) const;\n+  Offset dual_offset() const;\n@@ -1055,0 +1092,8 @@\n+  virtual bool can_be_inline_type() const { return false; }\n+  virtual bool flatten_array()      const { return false; }\n+  virtual bool not_flatten_array()  const { return false; }\n+  virtual bool is_flat()            const { return false; }\n+  virtual bool is_not_flat()        const { return false; }\n+  virtual bool is_null_free()       const { return false; }\n+  virtual bool is_not_null_free()   const { return false; }\n+\n@@ -1072,1 +1117,1 @@\n-  TypeRawPtr( PTR ptr, address bits ) : TypePtr(RawPtr,ptr,0), _bits(bits){}\n+  TypeRawPtr(PTR ptr, address bits) : TypePtr(RawPtr,ptr,Offset(0)), _bits(bits){}\n@@ -1108,2 +1153,2 @@\n- TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, int offset, int instance_id,\n-             const TypePtr* speculative, int inline_depth);\n+  TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const InterfaceSet& interfaces,bool xk, ciObject* o, Offset offset, Offset field_offset,\n+             int instance_id, const TypePtr* speculative, int inline_depth);\n@@ -1149,1 +1194,1 @@\n-  virtual ciKlass* klass() const { return _klass;     }\n+  virtual ciKlass* klass() const { return _klass; }\n@@ -1196,1 +1241,1 @@\n-  static const TypeOopPtr* make(PTR ptr, int offset, int instance_id,\n+  static const TypeOopPtr* make(PTR ptr, Offset offset, int instance_id,\n@@ -1215,1 +1260,4 @@\n-  bool is_known_instance_field() const { return is_known_instance() && _offset >= 0; }\n+  bool is_known_instance_field() const { return is_known_instance() && _offset.get() >= 0; }\n+\n+  virtual bool can_be_inline_type() const { return (_klass == NULL || _klass->can_be_inline_klass(_klass_is_exact)); }\n+  virtual bool can_be_inline_array() const { ShouldNotReachHere(); return false; }\n@@ -1278,2 +1326,3 @@\n-  TypeInstPtr(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, int offset, int instance_id,\n-              const TypePtr* speculative, int inline_depth);\n+  TypeInstPtr(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, Offset offset,\n+              bool flatten_array, int instance_id, const TypePtr* speculative,\n+              int inline_depth);\n@@ -1283,0 +1332,1 @@\n+  bool _flatten_array;     \/\/ Type is flat in arrays\n@@ -1301,1 +1351,1 @@\n-    return make(TypePtr::Constant, k, interfaces, true, o, 0, InstanceBot);\n+    return make(TypePtr::Constant, k, interfaces, true, o, Offset(0));\n@@ -1304,1 +1354,1 @@\n-  static const TypeInstPtr *make(ciObject* o, int offset) {\n+  static const TypeInstPtr *make(ciObject* o, Offset offset) {\n@@ -1307,1 +1357,1 @@\n-    return make(TypePtr::Constant, k, interfaces, true, o, offset, InstanceBot);\n+    return make(TypePtr::Constant, k, interfaces, true, o, offset);\n@@ -1313,1 +1363,1 @@\n-    return make(ptr, klass, interfaces, false, NULL, 0, InstanceBot);\n+    return make(ptr, klass, interfaces, false, NULL, Offset(0));\n@@ -1319,1 +1369,1 @@\n-    return make(ptr, klass, interfaces, true, NULL, 0, InstanceBot);\n+    return make(ptr, klass, interfaces, true, NULL, Offset(0));\n@@ -1323,1 +1373,1 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* klass, int offset) {\n+  static const TypeInstPtr *make(PTR ptr, ciKlass* klass, Offset offset) {\n@@ -1325,1 +1375,1 @@\n-    return make(ptr, klass, interfaces, false, NULL, offset, InstanceBot);\n+    return make(ptr, klass, interfaces, false, NULL, offset);\n@@ -1328,1 +1378,3 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, int offset,\n+  \/\/ Make a pointer to an oop.\n+  static const TypeInstPtr* make(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, Offset offset,\n+                                 bool flatten_array = false,\n@@ -1333,1 +1385,1 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* k, bool xk, ciObject* o, int offset, int instance_id = InstanceBot) {\n+  static const TypeInstPtr *make(PTR ptr, ciKlass* k, bool xk, ciObject* o, Offset offset, int instance_id = InstanceBot) {\n@@ -1335,1 +1387,1 @@\n-    return make(ptr, k, interfaces, xk, o, offset, instance_id);\n+    return make(ptr, k, interfaces, xk, o, offset, false, instance_id);\n@@ -1344,1 +1396,1 @@\n-  ciType* java_mirror_type() const;\n+  ciType* java_mirror_type(bool* is_val_mirror = NULL) const;\n@@ -1360,0 +1412,4 @@\n+  virtual const TypeInstPtr* cast_to_flatten_array() const;\n+  virtual bool flatten_array() const { return _flatten_array; }\n+  virtual bool not_flatten_array() const { return !can_be_inline_type() || (_klass->is_inlinetype() && !flatten_array()); }\n+\n@@ -1367,0 +1423,2 @@\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1391,0 +1449,1 @@\n+  friend class TypeInstPtr;\n@@ -1392,4 +1451,4 @@\n-  TypeAryPtr( PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk,\n-              int offset, int instance_id, bool is_autobox_cache,\n-              const TypePtr* speculative, int inline_depth)\n-    : TypeOopPtr(AryPtr,ptr,k,*_array_interfaces,xk,o,offset, instance_id, speculative, inline_depth),\n+  TypeAryPtr(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk,\n+             Offset offset, Offset field_offset, int instance_id, bool is_autobox_cache,\n+             const TypePtr* speculative, int inline_depth)\n+    : TypeOopPtr(AryPtr, ptr, k, *_array_interfaces, xk, o, offset, field_offset, instance_id, speculative, inline_depth),\n@@ -1397,1 +1456,2 @@\n-    _is_autobox_cache(is_autobox_cache)\n+    _is_autobox_cache(is_autobox_cache),\n+    _field_offset(field_offset)\n@@ -1403,2 +1463,2 @@\n-        _offset != 0 && _offset != arrayOopDesc::length_offset_in_bytes() &&\n-        _offset != arrayOopDesc::klass_offset_in_bytes()) {\n+        _offset.get() != 0 && _offset.get() != arrayOopDesc::length_offset_in_bytes() &&\n+        _offset.get() != arrayOopDesc::klass_offset_in_bytes()) {\n@@ -1413,0 +1473,6 @@\n+  \/\/ For flattened inline type arrays, each field of the inline type in\n+  \/\/ the array has its own memory slice so we need to keep track of\n+  \/\/ which field is accessed\n+  const Offset _field_offset;\n+  Offset meet_field_offset(const Type::Offset offset) const;\n+  Offset dual_field_offset() const;\n@@ -1440,0 +1506,6 @@\n+  \/\/ Inline type array properties\n+  bool is_flat()          const { return _ary->_flat; }\n+  bool is_not_flat()      const { return _ary->_not_flat; }\n+  bool is_null_free()     const { return is_flat() || (_ary->_elem->make_ptr() != NULL && _ary->_elem->make_ptr()->is_inlinetypeptr() && (_ary->_elem->make_ptr()->ptr() == NotNull || _ary->_elem->make_ptr()->ptr() == AnyNull)); }\n+  bool is_not_null_free() const { return _ary->_not_null_free; }\n+\n@@ -1442,1 +1514,2 @@\n-  static const TypeAryPtr *make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+  static const TypeAryPtr* make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, Offset offset,\n+                                Offset field_offset = Offset::bottom,\n@@ -1447,1 +1520,2 @@\n-  static const TypeAryPtr *make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+  static const TypeAryPtr* make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, Offset offset,\n+                                Offset field_offset = Offset::bottom,\n@@ -1450,1 +1524,2 @@\n-                                int inline_depth = InlineDepthBottom, bool is_autobox_cache = false);\n+                                int inline_depth = InlineDepthBottom,\n+                                bool is_autobox_cache = false);\n@@ -1469,0 +1544,1 @@\n+  virtual const Type* cleanup_speculative() const;\n@@ -1476,0 +1552,8 @@\n+  \/\/ Inline type array properties\n+  const TypeAryPtr* cast_to_not_flat(bool not_flat = true) const;\n+  const TypeAryPtr* cast_to_not_null_free(bool not_null_free = true) const;\n+  const TypeAryPtr* update_properties(const TypeAryPtr* new_type) const;\n+  jint flat_layout_helper() const;\n+  int flat_elem_size() const;\n+  int flat_log_elem_size() const;\n+\n@@ -1481,1 +1565,8 @@\n-  static jint max_array_length(BasicType etype) ;\n+  static jint max_array_length(BasicType etype);\n+\n+  const int flattened_offset() const;\n+  const Offset field_offset() const { return _field_offset; }\n+  const TypeAryPtr* with_field_offset(int offset) const;\n+  const TypePtr* add_field_offset_and_offset(intptr_t offset) const;\n+\n+  virtual bool can_be_inline_type() const { return false; }\n@@ -1484,0 +1575,2 @@\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1495,0 +1588,1 @@\n+  static const TypeAryPtr *INLINES;\n@@ -1513,1 +1607,1 @@\n-  TypeMetadataPtr(PTR ptr, ciMetadata* metadata, int offset);\n+  TypeMetadataPtr(PTR ptr, ciMetadata* metadata, Offset offset);\n@@ -1525,1 +1619,1 @@\n-  static const TypeMetadataPtr* make(PTR ptr, ciMetadata* m, int offset);\n+  static const TypeMetadataPtr* make(PTR ptr, ciMetadata* m, Offset offset);\n@@ -1556,1 +1650,1 @@\n-  TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const InterfaceSet& interfaces, int offset);\n+  TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const InterfaceSet& interfaces, Offset offset);\n@@ -1595,1 +1689,1 @@\n-  static const TypeKlassPtr *make(PTR ptr, ciKlass* klass, int offset, InterfaceHandling interface_handling = ignore_interfaces);\n+  static const TypeKlassPtr *make(PTR ptr, ciKlass* klass, Offset offset, InterfaceHandling interface_handling = ignore_interfaces);\n@@ -1614,0 +1708,1 @@\n+  virtual bool can_be_inline_array() const { ShouldNotReachHere(); return false; }\n@@ -1648,2 +1743,2 @@\n-  TypeInstKlassPtr(PTR ptr, ciKlass* klass, const InterfaceSet& interfaces, int offset)\n-    : TypeKlassPtr(InstKlassPtr, ptr, klass, interfaces, offset) {\n+  TypeInstKlassPtr(PTR ptr, ciKlass* klass, const InterfaceSet& interfaces, Offset offset, bool flatten_array)\n+    : TypeKlassPtr(InstKlassPtr, ptr, klass, interfaces, offset), _flatten_array(flatten_array) {\n@@ -1655,0 +1750,2 @@\n+  const bool _flatten_array; \/\/ Type is flat in arrays\n+\n@@ -1666,0 +1763,2 @@\n+  virtual bool can_be_inline_type() const { return (_klass == NULL || _klass->can_be_inline_klass(klass_is_exact())); }\n+\n@@ -1668,1 +1767,1 @@\n-    return make(TypePtr::Constant, k, interfaces, 0);\n+    return make(TypePtr::Constant, k, interfaces, Offset(0));\n@@ -1670,1 +1769,1 @@\n-  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, int offset);\n+  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, Offset offset, bool flatten_array = false);\n@@ -1672,1 +1771,1 @@\n-  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, int offset) {\n+  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, Offset offset) {\n@@ -1693,0 +1792,5 @@\n+  virtual bool flatten_array() const { return _flatten_array; }\n+  virtual bool not_flatten_array() const { return !_klass->can_be_inline_klass() || (_klass->is_inlinetype() && !flatten_array()); }\n+\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1707,0 +1811,3 @@\n+  const bool _not_flat;      \/\/ Array is never flattened\n+  const bool _not_null_free; \/\/ Array is never null-free\n+  const bool _null_free;\n@@ -1709,3 +1816,3 @@\n-  TypeAryKlassPtr(PTR ptr, const Type *elem, ciKlass* klass, int offset)\n-    : TypeKlassPtr(AryKlassPtr, ptr, klass, *_array_interfaces, offset), _elem(elem) {\n-    assert(klass == NULL || klass->is_type_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"\");\n+  TypeAryKlassPtr(PTR ptr, const Type *elem, ciKlass* klass, Offset offset, bool not_flat, int not_null_free, bool null_free)\n+    : TypeKlassPtr(AryKlassPtr, ptr, klass, *_array_interfaces, offset), _elem(elem), _not_flat(not_flat), _not_null_free(not_null_free), _null_free(null_free) {\n+    assert(klass == NULL || klass->is_type_array_klass() || klass->is_flat_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"\");\n@@ -1720,0 +1827,8 @@\n+  bool dual_null_free() const {\n+    return _null_free;\n+  }\n+\n+  bool meet_null_free(bool other) const {\n+    return _null_free && other;\n+  }\n+\n@@ -1725,1 +1840,1 @@\n-  static const TypeAryKlassPtr *make(PTR ptr, ciKlass* k, int offset, InterfaceHandling interface_handling);\n+  static const TypeAryKlassPtr* make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool not_flat, bool not_null_free, bool null_free);\n@@ -1733,1 +1848,2 @@\n-  static const TypeAryKlassPtr *make(PTR ptr, const Type *elem, ciKlass* k, int offset);\n+  static const TypeAryKlassPtr* make(PTR ptr, const Type* elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool null_free);\n+  static const TypeAryKlassPtr* make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling);\n@@ -1758,0 +1874,6 @@\n+  bool is_flat()          const { return klass() != NULL && klass()->is_flat_array_klass(); }\n+  bool is_not_flat()      const { return _not_flat; }\n+  bool is_null_free()     const { return _null_free; }\n+  bool is_not_null_free() const { return _not_null_free; }\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1893,1 +2015,2 @@\n-  TypeFunc( const TypeTuple *domain, const TypeTuple *range ) : Type(Function),  _domain(domain), _range(range) {}\n+  TypeFunc(const TypeTuple *domain_sig, const TypeTuple *domain_cc, const TypeTuple *range_sig, const TypeTuple *range_cc)\n+    : Type(Function), _domain_sig(domain_sig), _domain_cc(domain_cc), _range_sig(range_sig), _range_cc(range_cc) {}\n@@ -1899,2 +2022,13 @@\n-  const TypeTuple* const _domain;     \/\/ Domain of inputs\n-  const TypeTuple* const _range;      \/\/ Range of results\n+  \/\/ Domains of inputs: inline type arguments are not passed by\n+  \/\/ reference, instead each field of the inline type is passed as an\n+  \/\/ argument. We maintain 2 views of the argument list here: one\n+  \/\/ based on the signature (with an inline type argument as a single\n+  \/\/ slot), one based on the actual calling convention (with a value\n+  \/\/ type argument as a list of its fields).\n+  const TypeTuple* const _domain_sig;\n+  const TypeTuple* const _domain_cc;\n+  \/\/ Range of results. Similar to domains: an inline type result can be\n+  \/\/ returned in registers in which case range_cc lists all fields and\n+  \/\/ is the actual calling convention.\n+  const TypeTuple* const _range_sig;\n+  const TypeTuple* const _range_cc;\n@@ -1914,5 +2048,8 @@\n-  const TypeTuple* domain() const { return _domain; }\n-  const TypeTuple* range()  const { return _range; }\n-\n-  static const TypeFunc *make(ciMethod* method);\n-  static const TypeFunc *make(ciSignature signature, const Type* extra);\n+  const TypeTuple* domain_sig() const { return _domain_sig; }\n+  const TypeTuple* domain_cc()  const { return _domain_cc; }\n+  const TypeTuple* range_sig()  const { return _range_sig; }\n+  const TypeTuple* range_cc()   const { return _range_cc; }\n+\n+  static const TypeFunc* make(ciMethod* method, bool is_osr_compilation = false);\n+  static const TypeFunc *make(const TypeTuple* domain_sig, const TypeTuple* domain_cc,\n+                              const TypeTuple* range_sig, const TypeTuple* range_cc);\n@@ -1926,0 +2063,2 @@\n+  bool returns_inline_type_as_fields() const { return range_sig() != range_cc(); }\n+\n@@ -2181,0 +2320,8 @@\n+inline bool Type::is_inlinetypeptr() const {\n+  return isa_instptr() != NULL && is_instptr()->instance_klass()->is_inlinetype();\n+}\n+\n+inline ciInlineKlass* Type::inline_klass() const {\n+  return make_ptr()->is_instptr()->instance_klass()->as_inline_klass();\n+}\n+\n@@ -2207,0 +2354,1 @@\n+#define CmpUXNode    CmpULNode\n@@ -2225,0 +2373,1 @@\n+#define Op_StoreX    Op_StoreL\n@@ -2253,0 +2402,1 @@\n+#define CmpUXNode    CmpUNode\n@@ -2271,0 +2421,1 @@\n+#define Op_StoreX    Op_StoreI\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":218,"deletions":67,"binary":false,"changes":285,"status":"modified"},{"patch":"@@ -59,0 +59,2 @@\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -428,0 +430,1 @@\n+  bool is_inlined = InstanceKlass::cast(k1)->field_is_inlined(slot);\n@@ -429,1 +432,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset);\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset, is_inlined);\n@@ -446,1 +449,1 @@\n-  if (m->is_initializer()) {\n+  if (m->is_object_constructor() || m->is_static_vnew_factory()) {\n@@ -502,3 +505,12 @@\n-  jboolean ret = sub_klass->is_subtype_of(super_klass) ?\n-                   JNI_TRUE : JNI_FALSE;\n-\n+  jboolean ret;\n+  if (sub_klass == super_klass && sub_klass->is_inline_klass()) {\n+    \/\/ val type is a subtype of ref type\n+    InlineKlass* ik = InlineKlass::cast(sub_klass);\n+    if (sub_mirror == super_mirror || (ik->val_mirror() == sub_mirror && ik->ref_mirror() == super_mirror)) {\n+      ret = JNI_TRUE;\n+    } else {\n+      ret = JNI_FALSE;\n+    }\n+  } else {\n+    ret = sub_klass->is_subtype_of(super_klass) ? JNI_TRUE : JNI_FALSE;\n+  }\n@@ -804,1 +816,2 @@\n-    case T_OBJECT:      push_object(va_arg(_ap, jobject)); break;\n+    case T_OBJECT:\n+    case T_PRIMITIVE_OBJECT: push_object(va_arg(_ap, jobject)); break;\n@@ -844,1 +857,2 @@\n-    case T_OBJECT:      push_object((_ap++)->l); break;\n+    case T_OBJECT:\n+    case T_PRIMITIVE_OBJECT: push_object((_ap++)->l); break;\n@@ -980,5 +994,19 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherArray ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherArray ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  } else {\n+    JavaValue jvalue(T_PRIMITIVE_OBJECT);\n+    JNI_ArgumentPusherArray ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, nullptr, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -986,1 +1014,1 @@\n-JNI_END\n+  JNI_END\n@@ -998,5 +1026,19 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherVaArg ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  } else {\n+    JavaValue jvalue(T_PRIMITIVE_OBJECT);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, nullptr, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -1016,8 +1058,25 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  va_list args;\n-  va_start(args, methodID);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherVaArg ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n-  va_end(args);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    va_list args;\n+    va_start(args, methodID);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+    va_end(args);\n+  } else {\n+    va_list args;\n+    va_start(args, methodID);\n+    JavaValue jvalue(T_PRIMITIVE_OBJECT);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, nullptr, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    va_end(args);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -1774,1 +1833,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset());\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset(), fd.is_inlined());\n@@ -1784,0 +1843,1 @@\n+  oop res = nullptr;\n@@ -1789,2 +1849,12 @@\n-  oop loaded_obj = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n-  jobject ret = JNIHandles::make_local(THREAD, loaded_obj);\n+  if (!jfieldIDWorkaround::is_inlined_jfieldID(fieldID)) {\n+    res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instance can have inlined fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);  \/\/ performance bottleneck\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineKlass* field_vklass = InlineKlass::cast(holder->get_inline_type_field_klass(fd.index()));\n+    res = field_vklass->read_inlined_field(o, ik->field_offset(fd.index()), CHECK_NULL);\n+  }\n+  jobject ret = JNIHandles::make_local(THREAD, res);\n@@ -1882,1 +1952,12 @@\n-  HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  if (!jfieldIDWorkaround::is_inlined_jfieldID(fieldID)) {\n+    HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instances can have inlined fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineKlass* vklass = InlineKlass::cast(holder->get_inline_type_field_klass(fd.index()));\n+    oop v = JNIHandles::resolve_non_null(value);\n+    vklass->write_inlined_field(o, offset, v, CHECK);\n+  }\n@@ -2299,4 +2380,13 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  if (a->is_within_bounds(index)) {\n-    ret = JNIHandles::make_local(THREAD, a->obj_at(index));\n-    return ret;\n+  oop res = nullptr;\n+  arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+  if (arr->is_within_bounds(index)) {\n+    if (arr->is_flatArray()) {\n+      flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+      flatArrayHandle vah(thread, a);\n+      res = flatArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK_NULL);\n+      assert(res != nullptr, \"Must be set in one of two paths above\");\n+    } else {\n+      assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+      objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+      res = a->obj_at(index);\n+    }\n@@ -2306,1 +2396,1 @@\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n+    ss.print(\"Index %d out of bounds for length %d\", index,arr->length());\n@@ -2309,0 +2399,2 @@\n+  ret = JNIHandles::make_local(THREAD, res);\n+  return ret;\n@@ -2318,24 +2410,51 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  oop v = JNIHandles::resolve(value);\n-  if (a->is_within_bounds(index)) {\n-    if (v == nullptr || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n-      a->obj_at_put(index, v);\n-    } else {\n-      ResourceMark rm(THREAD);\n-      stringStream ss;\n-      Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n-      ss.print(\"type mismatch: can not store %s to %s[%d]\",\n-               v->klass()->external_name(),\n-               bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n-               index);\n-      for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n-        ss.print(\"[]\");\n-      }\n-      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n-    }\n-  } else {\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n-    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n-  }\n+   bool oob = false;\n+   int length = -1;\n+   oop res = nullptr;\n+   arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+   if (arr->is_within_bounds(index)) {\n+     if (arr->is_flatArray()) {\n+       flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       FlatArrayKlass* vaklass = FlatArrayKlass::cast(a->klass());\n+       InlineKlass* element_vklass = vaklass->element_klass();\n+       if (v != nullptr && v->is_a(element_vklass)) {\n+         a->value_copy_to_index(v, index);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *kl = FlatArrayKlass::cast(a->klass());\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             kl->external_name(),\n+             index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     } else {\n+       assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+       objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       if (v == nullptr || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n+         a->obj_at_put(index, v);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n+                 index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     }\n+   } else {\n+     ResourceMark rm(THREAD);\n+     stringStream ss;\n+     ss.print(\"Index %d out of bounds for length %d\", index, arr->length());\n+     THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n+   }\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":178,"deletions":59,"binary":false,"changes":237,"status":"modified"},{"patch":"@@ -259,1 +259,2 @@\n-      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT)) {\n+      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT) &&\n+      !(fd.field_type() == T_PRIMITIVE_OBJECT && ftype == T_OBJECT)) {\n@@ -296,1 +297,2 @@\n-      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT)) {\n+      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT) &&\n+      !(fd.field_type() == T_PRIMITIVE_OBJECT && ftype == T_OBJECT)) {\n@@ -347,1 +349,1 @@\n-check_is_obj_array(JavaThread* thr, jarray jArray) {\n+check_is_obj_or_inline_array(JavaThread* thr, jarray jArray) {\n@@ -349,1 +351,1 @@\n-  if (!aOop->is_objArray()) {\n+  if (!aOop->is_objArray() && !aOop->is_flatArray()) {\n@@ -469,1 +471,1 @@\n-      name[0] == JVM_SIGNATURE_CLASS &&            \/\/ 'L'\n+      (name[0] == JVM_SIGNATURE_CLASS || name[0] == JVM_SIGNATURE_PRIMITIVE_OBJECT) && \/\/ 'L' or 'Q'\n@@ -1609,1 +1611,1 @@\n-      check_is_obj_array(thr, array);\n+      check_is_obj_or_inline_array(thr, array);\n@@ -1623,1 +1625,1 @@\n-      check_is_obj_array(thr, array);\n+      check_is_obj_or_inline_array(thr, array);\n","filename":"src\/hotspot\/share\/prims\/jniCheck.cpp","additions":9,"deletions":7,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -615,2 +616,23 @@\n-  \/\/ as implemented in the classic virtual machine; return 0 if object is null\n-  return handle == nullptr ? 0 : ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)) ;\n+  \/\/ as implemented in the classic virtual machine; return 0 if object is nullptr\n+  if (handle == nullptr) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, Universe::value_object_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return ObjectSynchronizer::FastHashCode(THREAD, obj);\n+  }\n@@ -668,0 +690,1 @@\n+       klass->is_inline_klass() ||\n@@ -1181,1 +1204,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1183,1 +1207,1 @@\n-    assert(klass->is_objArray_klass() || klass->is_typeArray_klass(), \"Illegal mirror klass\");\n+    assert(klass->is_objArray_klass() || klass->is_typeArray_klass() || klass->is_flatArray_klass(), \"Illegal mirror klass\");\n@@ -1194,1 +1218,2 @@\n-      Klass* k = InstanceKlass::cast(klass)->local_interfaces()->at(index);\n+      InstanceKlass* ik = InstanceKlass::cast(klass);\n+      Klass* k = ik->local_interfaces()->at(index);\n@@ -1229,0 +1254,13 @@\n+JVM_ENTRY(jboolean, JVM_IsIdentityClass(JNIEnv *env, jclass cls))\n+  oop mirror = JNIHandles::resolve_non_null(cls);\n+  if (java_lang_Class::is_primitive(mirror)) {\n+    return JNI_FALSE;\n+  }\n+  Klass* k = java_lang_Class::as_Klass(mirror);\n+  if (EnableValhalla) {\n+    return k->is_array_klass() || k->is_identity_class();\n+  } else {\n+    return k->is_interface() ? JNI_FALSE : JNI_TRUE;\n+  }\n+JVM_END\n+\n@@ -1844,0 +1882,2 @@\n+  bool is_ctor = (method->is_object_constructor() ||\n+                  method->is_static_vnew_factory());\n@@ -1845,1 +1885,1 @@\n-    return (method->is_initializer() && !method->is_static());\n+    return is_ctor;\n@@ -1847,1 +1887,3 @@\n-    return  (!method->is_initializer() && !method->is_overpass());\n+    return (!is_ctor &&\n+            !method->is_class_initializer() &&\n+            !method->is_overpass());\n@@ -1910,0 +1952,2 @@\n+        assert(method->is_object_constructor() ||\n+               method->is_static_vnew_factory(), \"must be\");\n@@ -2192,3 +2236,1 @@\n-  if (!m->is_initializer() || m->is_static()) {\n-    method = Reflection::new_method(m, true, CHECK_NULL);\n-  } else {\n+  if (m->is_object_constructor() || m->is_static_vnew_factory()) {\n@@ -2196,0 +2238,2 @@\n+  } else {\n+    method = Reflection::new_method(m, true, CHECK_NULL);\n@@ -2466,0 +2510,37 @@\n+\/\/ Arrays support \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+JVM_ENTRY(jboolean, JVM_ArrayIsAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == nullptr) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  return ArrayKlass::cast(k)->element_access_is_atomic();\n+JVM_END\n+\n+JVM_ENTRY(jobject, JVM_ArrayEnsureAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == nullptr) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vk = FlatArrayKlass::cast(k);\n+    if (!vk->element_access_is_atomic()) {\n+      \/**\n+       * Need to decide how to implement:\n+       *\n+       * 1) Change to objArrayOop layout, therefore oop->klass() differs so\n+       * then \"<atomic>[Qfoo;\" klass needs to subclass \"[Qfoo;\" to pass through\n+       * \"checkcast\" & \"instanceof\"\n+       *\n+       * 2) Use extra header in the flatArrayOop to flag atomicity required and\n+       * possibly per instance lock structure. Said info, could be placed in\n+       * \"trailer\" rather than disturb the current arrayOop\n+       *\/\n+      Unimplemented();\n+    }\n+  }\n+  return array;\n+JVM_END\n+\n@@ -2628,1 +2709,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3473,0 +3554,4 @@\n+JVM_LEAF(jboolean, JVM_IsValhallaEnabled(void))\n+  return EnableValhalla ? JNI_TRUE : JNI_FALSE;\n+JVM_END\n+\n@@ -3550,1 +3635,1 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3570,0 +3655,1 @@\n+  objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3571,1 +3657,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":98,"deletions":13,"binary":false,"changes":111,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"classfile\/vmClasses.hpp\"\n@@ -916,2 +917,1 @@\n-  write_u2(ik()->access_flags().get_flags() & JVM_RECOGNIZED_CLASS_MODIFIERS);\n-\n+  write_u2(ik()->access_flags().get_flags() & (JVM_RECOGNIZED_CLASS_MODIFIERS | JVM_ACC_PRIMITIVE | JVM_ACC_VALUE | JVM_ACC_IDENTITY));\n@@ -930,0 +930,1 @@\n+\n@@ -1028,0 +1029,1 @@\n+      case Bytecodes::_withfield       :  \/\/ fall through\n","filename":"src\/hotspot\/share\/prims\/jvmtiClassFileReconstituter.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2893,3 +2893,3 @@\n-    if (k->is_super()) {\n-      result |= JVM_ACC_SUPER;\n-    }\n+    \/\/ if (k->is_super()) {\n+    \/\/   result |= JVM_ACC_SUPER;\n+    \/\/ }\n@@ -3033,1 +3033,2 @@\n-                                            src_st.access_flags().is_static());\n+                                            src_st.access_flags().is_static(),\n+                                            src_st.field_descriptor().is_inlined());\n@@ -3070,2 +3071,3 @@\n-    Array<InstanceKlass*>* interface_list = InstanceKlass::cast(k)->local_interfaces();\n-    const int result_length = (interface_list == nullptr ? 0 : interface_list->length());\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    Array<InstanceKlass*>* interface_list = ik->local_interfaces();\n+    int result_length = (interface_list == nullptr ? 0 : interface_list->length());\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2314,1 +2314,1 @@\n-  if (sig_type == JVM_SIGNATURE_CLASS) {\n+  if (sig_type == JVM_SIGNATURE_CLASS || sig_type == JVM_SIGNATURE_PRIMITIVE_OBJECT) {\n","filename":"src\/hotspot\/share\/prims\/jvmtiExport.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include <oops\/inlineKlass.hpp>\n@@ -81,0 +82,1 @@\n+      \/\/ CMH flat arrays (InlineKlass)\n","filename":"src\/hotspot\/share\/prims\/jvmtiGetLoadedClasses.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -459,1 +459,2 @@\n-  if (ty_sign[0] == JVM_SIGNATURE_CLASS &&\n+  if ((ty_sign[0] == JVM_SIGNATURE_CLASS ||\n+       ty_sign[0] == JVM_SIGNATURE_PRIMITIVE_OBJECT) &&\n@@ -537,0 +538,1 @@\n+  case T_PRIMITIVE_OBJECT:\n@@ -684,1 +686,1 @@\n-      if (_type == T_OBJECT) {\n+      if (_type == T_OBJECT || _type == T_PRIMITIVE_OBJECT) {\n@@ -702,1 +704,2 @@\n-      case T_OBJECT: {\n+      case T_OBJECT:\n+      case T_PRIMITIVE_OBJECT: {\n@@ -723,1 +726,2 @@\n-        case T_OBJECT: {\n+        case T_OBJECT:\n+        case T_PRIMITIVE_OBJECT: {\n","filename":"src\/hotspot\/share\/prims\/jvmtiImpl.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -607,2 +607,1 @@\n-    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be\n-    \/\/ here\n+    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be here\n@@ -918,0 +917,12 @@\n+static jvmtiError check_preload_attribute(InstanceKlass* the_class,\n+                                          InstanceKlass* scratch_class) {\n+  Thread* thread = Thread::current();\n+  ResourceMark rm(thread);\n+\n+  \/\/ Check whether the class Preload attribute has been changed.\n+  return check_attribute_arrays(\"Preload\",\n+                                the_class, scratch_class,\n+                                the_class->preload_classes(),\n+                                scratch_class->preload_classes());\n+}\n+\n@@ -997,0 +1008,6 @@\n+  \/\/ Check whether the Preload attribute has been changed.\n+  err = check_preload_attribute(the_class, scratch_class);\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n+\n@@ -1966,0 +1983,6 @@\n+  \/\/ rewrite constant pool references in the Preload attribute:\n+  if (!rewrite_cp_refs_in_preload_attribute(scratch_class)) {\n+    \/\/ propagate failure back to caller\n+    return false;\n+  }\n+\n@@ -2114,0 +2137,13 @@\n+\/\/ Rewrite constant pool references in the Preload attribute.\n+bool VM_RedefineClasses::rewrite_cp_refs_in_preload_attribute(\n+       InstanceKlass* scratch_class) {\n+\n+  Array<u2>* preload_classes = scratch_class->preload_classes();\n+  assert(preload_classes != NULL, \"unexpected null preload_classes\");\n+  for (int i = 0; i < preload_classes->length(); i++) {\n+    u2 cp_index = preload_classes->at(i);\n+    preload_classes->at_put(i, find_new_index(cp_index));\n+  }\n+  return true;\n+}\n+\n@@ -2246,0 +2282,2 @@\n+      case Bytecodes::_aconst_init   : \/\/ fall through\n+      case Bytecodes::_withfield      : \/\/ fall through\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":40,"deletions":2,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -476,0 +476,1 @@\n+  bool rewrite_cp_refs_in_preload_attribute(InstanceKlass* scratch_class);\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -134,12 +134,13 @@\n-  IS_METHOD            = java_lang_invoke_MemberName::MN_IS_METHOD,\n-  IS_CONSTRUCTOR       = java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR,\n-  IS_FIELD             = java_lang_invoke_MemberName::MN_IS_FIELD,\n-  IS_TYPE              = java_lang_invoke_MemberName::MN_IS_TYPE,\n-  CALLER_SENSITIVE     = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,\n-  TRUSTED_FINAL        = java_lang_invoke_MemberName::MN_TRUSTED_FINAL,\n-  REFERENCE_KIND_SHIFT = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,\n-  REFERENCE_KIND_MASK  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,\n-  LM_UNCONDITIONAL     = java_lang_invoke_MemberName::MN_UNCONDITIONAL_MODE,\n-  LM_MODULE            = java_lang_invoke_MemberName::MN_MODULE_MODE,\n-  LM_TRUSTED           = java_lang_invoke_MemberName::MN_TRUSTED_MODE,\n-  ALL_KINDS      = IS_METHOD | IS_CONSTRUCTOR | IS_FIELD | IS_TYPE\n+  IS_METHOD             = java_lang_invoke_MemberName::MN_IS_METHOD,\n+  IS_OBJECT_CONSTRUCTOR = java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR,\n+  IS_FIELD              = java_lang_invoke_MemberName::MN_IS_FIELD,\n+  IS_TYPE               = java_lang_invoke_MemberName::MN_IS_TYPE,\n+  CALLER_SENSITIVE      = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,\n+  TRUSTED_FINAL         = java_lang_invoke_MemberName::MN_TRUSTED_FINAL,\n+  FLATTENED             = java_lang_invoke_MemberName::MN_FLATTENED,\n+  REFERENCE_KIND_SHIFT  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,\n+  REFERENCE_KIND_MASK   = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,\n+  LM_UNCONDITIONAL      = java_lang_invoke_MemberName::MN_UNCONDITIONAL_MODE,\n+  LM_MODULE             = java_lang_invoke_MemberName::MN_MODULE_MODE,\n+  LM_TRUSTED            = java_lang_invoke_MemberName::MN_TRUSTED_MODE,\n+  ALL_KINDS      = IS_METHOD | IS_OBJECT_CONSTRUCTOR | IS_FIELD | IS_TYPE\n@@ -156,1 +157,1 @@\n-    flags |= IS_CONSTRUCTOR;\n+    flags |= IS_OBJECT_CONSTRUCTOR;\n@@ -175,1 +176,1 @@\n-    case IS_CONSTRUCTOR:\n+    case IS_OBJECT_CONSTRUCTOR:\n@@ -317,2 +318,2 @@\n-    } else if (m->is_initializer()) {\n-      flags |= IS_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);\n+    } else if (m->is_object_constructor()) {\n+      flags |= IS_OBJECT_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);\n@@ -357,0 +358,1 @@\n+  if (fd.is_inlined()) flags |= FLATTENED;;\n@@ -810,1 +812,1 @@\n-  case IS_CONSTRUCTOR:\n+  case IS_OBJECT_CONSTRUCTOR:\n@@ -816,1 +818,1 @@\n-        if (name == vmSymbols::object_initializer_name()) {\n+        if (name == vmSymbols::object_initializer_name() && type->is_void_method_signature()) {\n@@ -818,0 +820,2 @@\n+        } else if (name == vmSymbols::inline_factory_name()) {\n+          LinkResolver::resolve_static_call(result, link_info, false, THREAD);\n@@ -879,1 +883,1 @@\n-  case IS_CONSTRUCTOR:\n+  case IS_OBJECT_CONSTRUCTOR:\n@@ -1006,1 +1010,1 @@\n-    template(java_lang_invoke_MemberName,MN_IS_CONSTRUCTOR) \\\n+    template(java_lang_invoke_MemberName,MN_IS_OBJECT_CONSTRUCTOR) \\\n@@ -1011,0 +1015,1 @@\n+    template(java_lang_invoke_MemberName,MN_FLATTENED) \\\n@@ -1148,1 +1153,1 @@\n-               (flags & ALL_KINDS) == IS_CONSTRUCTOR) {\n+               (flags & ALL_KINDS) == IS_OBJECT_CONSTRUCTOR) {\n","filename":"src\/hotspot\/share\/prims\/methodHandles.cpp","additions":26,"deletions":21,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logStream.hpp\"\n@@ -39,0 +41,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -46,0 +51,1 @@\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n@@ -152,1 +158,0 @@\n-\n@@ -228,0 +233,1 @@\n+    assert(_obj == NULL || !_obj->is_inline_type() || _obj->mark().is_larval_state(), \"must be an object instance or a larval inline type\");\n@@ -231,1 +237,0 @@\n-\n@@ -244,0 +249,62 @@\n+#ifdef ASSERT\n+\/*\n+ * Get the field descriptor of the field of the given object at the given offset.\n+ *\/\n+static bool get_field_descriptor(oop p, jlong offset, fieldDescriptor* fd) {\n+  bool found = false;\n+  Klass* k = p->klass();\n+  if (k->is_instance_klass()) {\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    found = ik->find_field_from_offset((int)offset, false, fd);\n+    if (!found && ik->is_mirror_instance_klass()) {\n+      Klass* k2 = java_lang_Class::as_Klass(p);\n+      if (k2->is_instance_klass()) {\n+        ik = InstanceKlass::cast(k2);\n+        found = ik->find_field_from_offset((int)offset, true, fd);\n+      }\n+    }\n+  }\n+  return found;\n+}\n+#endif \/\/ ASSERT\n+\n+static void assert_and_log_unsafe_value_access(oop p, jlong offset, InlineKlass* vk) {\n+  Klass* k = p->klass();\n+#ifdef ASSERT\n+  if (k->is_instance_klass()) {\n+    assert_field_offset_sane(p, offset);\n+    fieldDescriptor fd;\n+    bool found = get_field_descriptor(p, offset, &fd);\n+    if (found) {\n+      assert(found, \"value field not found\");\n+      assert(fd.is_inlined(), \"field not flat\");\n+    } else {\n+      if (log_is_enabled(Trace, valuetypes)) {\n+        log_trace(valuetypes)(\"not a field in %s at offset \" UINT64_FORMAT_X,\n+                              p->klass()->external_name(), (uint64_t)offset);\n+      }\n+    }\n+  } else if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+    int index = (offset - vak->array_header_in_bytes()) \/ vak->element_byte_size();\n+    address dest = (address)((flatArrayOop)p)->value_at_addr(index, vak->layout_helper());\n+    assert(dest == (cast_from_oop<address>(p) + offset), \"invalid offset\");\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+#endif \/\/ ASSERT\n+  if (log_is_enabled(Trace, valuetypes)) {\n+    if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      int index = (offset - vak->array_header_in_bytes()) \/ vak->element_byte_size();\n+      address dest = (address)((flatArrayOop)p)->value_at_addr(index, vak->layout_helper());\n+      log_trace(valuetypes)(\"%s array type %s index %d element size %d offset \" UINT64_FORMAT_X \" at \" INTPTR_FORMAT,\n+                            p->klass()->external_name(), vak->external_name(),\n+                            index, vak->element_byte_size(), (uint64_t)offset, p2i(dest));\n+    } else {\n+      log_trace(valuetypes)(\"%s field type %s at offset \" UINT64_FORMAT_X,\n+                            p->klass()->external_name(), vk->external_name(), (uint64_t)offset);\n+    }\n+  }\n+}\n+\n@@ -258,0 +325,1 @@\n+  assert(!p->is_inline_type() || p->mark().is_larval_state(), \"must be an object instance or a larval inline type\");\n@@ -261,0 +329,65 @@\n+UNSAFE_ENTRY(jlong, Unsafe_ValueHeaderSize(JNIEnv *env, jobject unsafe, jclass c)) {\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  return vk->first_field_offset();\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jboolean, Unsafe_IsFlattenedField(JNIEnv *env, jobject unsafe, jobject o)) {\n+  oop f = JNIHandles::resolve_non_null(o);\n+  Klass* k = java_lang_Class::as_Klass(java_lang_reflect_Field::clazz(f));\n+  int slot = java_lang_reflect_Field::slot(f);\n+  return InstanceKlass::cast(k)->field_is_inlined(slot);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jboolean, Unsafe_IsFlattenedArray(JNIEnv *env, jobject unsafe, jclass c)) {\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));\n+  return k->is_flatArray_klass();\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_UninitializedDefaultValue(JNIEnv *env, jobject unsafe, jclass vc)) {\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  oop v = vk->default_value();\n+  return JNIHandles::make_local(THREAD, v);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_GetValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc)) {\n+  oop base = JNIHandles::resolve(obj);\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  assert_and_log_unsafe_value_access(base, offset, vk);\n+  Handle base_h(THREAD, base);\n+  oop v = vk->read_inlined_field(base_h(), offset, CHECK_NULL);\n+  return JNIHandles::make_local(THREAD, v);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(void, Unsafe_PutValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc, jobject value)) {\n+  oop base = JNIHandles::resolve(obj);\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  assert(!base->is_inline_type() || base->mark().is_larval_state(), \"must be an object instance or a larval inline type\");\n+  assert_and_log_unsafe_value_access(base, offset, vk);\n+  oop v = JNIHandles::resolve(value);\n+  vk->write_inlined_field(base, offset, v, CHECK);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_MakePrivateBuffer(JNIEnv *env, jobject unsafe, jobject value)) {\n+  oop v = JNIHandles::resolve_non_null(value);\n+  assert(v->is_inline_type(), \"must be an inline type instance\");\n+  Handle vh(THREAD, v);\n+  InlineKlass* vk = InlineKlass::cast(v->klass());\n+  instanceOop new_value = vk->allocate_instance_buffer(CHECK_NULL);\n+  vk->inline_copy_oop_to_new_oop(vh(),  new_value);\n+  markWord mark = new_value->mark();\n+  new_value->set_mark(mark.enter_larval_state());\n+  return JNIHandles::make_local(THREAD, new_value);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_FinishPrivateBuffer(JNIEnv *env, jobject unsafe, jobject value)) {\n+  oop v = JNIHandles::resolve(value);\n+  assert(v->mark().is_larval_state(), \"must be a larval value\");\n+  markWord mark = v->mark();\n+  v->set_mark(mark.exit_larval_state());\n+  return JNIHandles::make_local(THREAD, v);\n+} UNSAFE_END\n+\n@@ -593,0 +726,5 @@\n+  } else if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+    InlineKlass* vklass = vak->element_klass();\n+    base = vak->array_header_in_bytes();\n+    scale = vak->element_byte_size();\n@@ -628,0 +766,6 @@\n+UNSAFE_ENTRY(jlong, Unsafe_GetObjectSize0(JNIEnv* env, jobject o, jobject obj))\n+  oop p = JNIHandles::resolve(obj);\n+  return p->size() * HeapWordSize;\n+UNSAFE_END\n+\n+\n@@ -846,4 +990,4 @@\n-    {CC \"get\" #Type,      CC \"(\" OBJ \"J)\" #Desc,       FN_PTR(Unsafe_Get##Type)}, \\\n-    {CC \"put\" #Type,      CC \"(\" OBJ \"J\" #Desc \")V\",   FN_PTR(Unsafe_Put##Type)}, \\\n-    {CC \"get\" #Type \"Volatile\",      CC \"(\" OBJ \"J)\" #Desc,       FN_PTR(Unsafe_Get##Type##Volatile)}, \\\n-    {CC \"put\" #Type \"Volatile\",      CC \"(\" OBJ \"J\" #Desc \")V\",   FN_PTR(Unsafe_Put##Type##Volatile)}\n+    {CC \"get\"  #Type,      CC \"(\" OBJ \"J)\" #Desc,                 FN_PTR(Unsafe_Get##Type)}, \\\n+    {CC \"put\"  #Type,      CC \"(\" OBJ \"J\" #Desc \")V\",             FN_PTR(Unsafe_Put##Type)}, \\\n+    {CC \"get\"  #Type \"Volatile\",      CC \"(\" OBJ \"J)\" #Desc,      FN_PTR(Unsafe_Get##Type##Volatile)}, \\\n+    {CC \"put\"  #Type \"Volatile\",      CC \"(\" OBJ \"J\" #Desc \")V\",  FN_PTR(Unsafe_Put##Type##Volatile)}\n@@ -858,0 +1002,9 @@\n+    {CC \"isFlattenedArray\", CC \"(\" CLS \")Z\",                     FN_PTR(Unsafe_IsFlattenedArray)},\n+    {CC \"isFlattenedField0\", CC \"(\" OBJ \")Z\",                    FN_PTR(Unsafe_IsFlattenedField)},\n+    {CC \"getValue\",         CC \"(\" OBJ \"J\" CLS \")\" OBJ,          FN_PTR(Unsafe_GetValue)},\n+    {CC \"putValue\",         CC \"(\" OBJ \"J\" CLS OBJ \")V\",         FN_PTR(Unsafe_PutValue)},\n+    {CC \"uninitializedDefaultValue\", CC \"(\" CLS \")\" OBJ,         FN_PTR(Unsafe_UninitializedDefaultValue)},\n+    {CC \"makePrivateBuffer\",     CC \"(\" OBJ \")\" OBJ,             FN_PTR(Unsafe_MakePrivateBuffer)},\n+    {CC \"finishPrivateBuffer\",   CC \"(\" OBJ \")\" OBJ,             FN_PTR(Unsafe_FinishPrivateBuffer)},\n+    {CC \"valueHeaderSize\",       CC \"(\" CLS \")J\",                FN_PTR(Unsafe_ValueHeaderSize)},\n+\n@@ -880,0 +1033,1 @@\n+    {CC \"getObjectSize0\",     CC \"(Ljava\/lang\/Object;)J\", FN_PTR(Unsafe_GetObjectSize0)},\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":160,"deletions":6,"binary":false,"changes":166,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -61,0 +62,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -67,0 +69,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -1868,0 +1871,87 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return NULL;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return NULL;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+class CollectOops : public BasicOopIterateClosure {\n+ public:\n+  GrowableArray<Handle>* array;\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < array->length(); i++) {\n+      result_array->obj_at_put(i, array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    \/\/ Value might be oop, but JLS can't see as Object, just iterate through it...\n+    if (oh != NULL && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      array->append(oh);\n+    }\n+  }\n+\n+  void do_oop(oop* o) { add_oop(HeapAccess<>::oop_load(o)); }\n+  void do_oop(narrowOop* v) { add_oop(HeapAccess<>::oop_load(v)); }\n+};\n+\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  StackFrameStream sfs(thread, false \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, NULL, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\n@@ -2707,0 +2797,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":96,"deletions":0,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+#include <string.h>\n@@ -1929,1 +1930,0 @@\n-unsigned int patch_mod_count = 0;\n@@ -1976,0 +1976,10 @@\n+  if (AMD64_ONLY(false &&) AARCH64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypePassFieldsAsArgs)) {\n+    FLAG_SET_CMDLINE(InlineTypePassFieldsAsArgs, false);\n+    warning(\"InlineTypePassFieldsAsArgs is not supported on this platform\");\n+  }\n+\n+  if (AMD64_ONLY(false &&) AARCH64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypeReturnedAsFields)) {\n+    FLAG_SET_CMDLINE(InlineTypeReturnedAsFields, false);\n+    warning(\"InlineTypeReturnedAsFields is not supported on this platform\");\n+  }\n+\n@@ -2096,2 +2106,0 @@\n-  bool patch_mod_javabase = false;\n-\n@@ -2111,1 +2119,1 @@\n-  jint result = parse_each_vm_init_arg(vm_options_args, &patch_mod_javabase, JVMFlagOrigin::JIMAGE_RESOURCE);\n+  jint result = parse_each_vm_init_arg(vm_options_args, JVMFlagOrigin::JIMAGE_RESOURCE);\n@@ -2118,1 +2126,1 @@\n-  result = parse_each_vm_init_arg(java_tool_options_args, &patch_mod_javabase, JVMFlagOrigin::ENVIRON_VAR);\n+  result = parse_each_vm_init_arg(java_tool_options_args, JVMFlagOrigin::ENVIRON_VAR);\n@@ -2124,1 +2132,1 @@\n-  result = parse_each_vm_init_arg(cmd_line_args, &patch_mod_javabase, JVMFlagOrigin::COMMAND_LINE);\n+  result = parse_each_vm_init_arg(cmd_line_args, JVMFlagOrigin::COMMAND_LINE);\n@@ -2131,1 +2139,1 @@\n-  result = parse_each_vm_init_arg(java_options_args, &patch_mod_javabase, JVMFlagOrigin::ENVIRON_VAR);\n+  result = parse_each_vm_init_arg(java_options_args, JVMFlagOrigin::ENVIRON_VAR);\n@@ -2145,1 +2153,1 @@\n-  result = finalize_vm_init_args(patch_mod_javabase);\n+  result = finalize_vm_init_args();\n@@ -2198,1 +2206,1 @@\n-int Arguments::process_patch_mod_option(const char* patch_mod_tail, bool* patch_mod_javabase) {\n+int Arguments::process_patch_mod_option(const char* patch_mod_tail) {\n@@ -2214,1 +2222,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1, patch_mod_javabase);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/);\n@@ -2216,3 +2224,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2226,0 +2231,64 @@\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, each module may have value classes that\n+  \/\/ are to be patched into the module.\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (enable_preview() && EnableValhalla) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module or --enable-preview\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2280,1 +2349,1 @@\n-jint Arguments::parse_each_vm_init_arg(const JavaVMInitArgs* args, bool* patch_mod_javabase, JVMFlagOrigin origin) {\n+jint Arguments::parse_each_vm_init_arg(const JavaVMInitArgs* args, JVMFlagOrigin origin) {\n@@ -2405,1 +2474,1 @@\n-      int res = process_patch_mod_option(tail, patch_mod_javabase);\n+      int res = process_patch_mod_option(tail);\n@@ -2914,0 +2983,6 @@\n+  if (!EnableValhalla && EnablePrimitiveClasses) {\n+    jio_fprintf(defaultStream::error_stream(),\n+                \"Cannot specify -XX:+EnablePrimitiveClasses without -XX:+EnableValhalla\");\n+    return JNI_EINVAL;\n+  }\n+\n@@ -2928,12 +3003,3 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool* patch_mod_javabase) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (*patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      *patch_mod_javabase = true;\n-    }\n-  }\n+bool match_module(void *module_name, ModulePatchPath *patch) {\n+  return (strcmp((char *)module_name, patch->module_name()) == 0);\n+}\n@@ -2941,0 +3007,5 @@\n+bool Arguments::patch_mod_javabase() {\n+    return _patch_mod_prefix != nullptr && _patch_mod_prefix->find((void*)JAVA_BASE_NAME, match_module) >= 0;\n+}\n+\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append) {\n@@ -2946,1 +3017,16 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find((void*)module_name, match_module);\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -2989,1 +3075,1 @@\n-jint Arguments::finalize_vm_init_args(bool patch_mod_javabase) {\n+jint Arguments::finalize_vm_init_args() {\n@@ -3072,0 +3158,5 @@\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n+    return JNI_ERR;\n+  }\n+\n@@ -3108,1 +3199,1 @@\n-  if (UseSharedSpaces && patch_mod_javabase) {\n+  if (UseSharedSpaces && patch_mod_javabase()) {\n@@ -4058,0 +4149,7 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    InlineTypePassFieldsAsArgs = false;\n+    InlineTypeReturnedAsFields = false;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":128,"deletions":30,"binary":false,"changes":158,"status":"modified"},{"patch":"@@ -91,0 +91,1 @@\n+  inline void append_path(const char* path) { _path->append_value(path); }\n@@ -398,1 +399,1 @@\n-  static int process_patch_mod_option(const char* patch_mod_tail, bool* patch_mod_javabase);\n+  static int process_patch_mod_option(const char* patch_mod_tail);\n@@ -434,2 +435,2 @@\n-  static jint parse_each_vm_init_arg(const JavaVMInitArgs* args, bool* patch_mod_javabase, JVMFlagOrigin origin);\n-  static jint finalize_vm_init_args(bool patch_mod_javabase);\n+  static jint parse_each_vm_init_arg(const JavaVMInitArgs* args, JVMFlagOrigin origin);\n+  static jint finalize_vm_init_args();\n@@ -603,1 +604,3 @@\n-  static void add_patch_mod_prefix(const char *module_name, const char *path, bool* patch_mod_javabase);\n+  static void add_patch_mod_prefix(const char *module_name, const char *path, bool allow_append);\n+  static bool patch_mod_javabase();\n+  static int finalize_patch_module();\n","filename":"src\/hotspot\/share\/runtime\/arguments.hpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -51,0 +51,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n@@ -56,0 +58,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -192,1 +195,1 @@\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+    assert(obj.not_null() || k->is_inline_klass() || realloc_failures, \"reallocation was missed\");\n@@ -194,1 +197,5 @@\n-      st.print(\" allocation failed\");\n+      if (k->is_inline_klass()) {\n+        st.print(\" is null\");\n+      } else {\n+        st.print(\" allocation failed\");\n+      }\n@@ -228,2 +235,13 @@\n-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n-  Handle return_value;\n+  ScopeDesc* scope = chunk->at(0)->scope();\n+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n+  \/\/ In case of the return of multiple values, we must take care\n+  \/\/ of all oop return values.\n+  GrowableArray<Handle> return_oops;\n+  InlineKlass* vk = nullptr;\n+  if (save_oop_result && scope->return_scalarized()) {\n+    vk = InlineKlass::returned_inline_klass(map);\n+    if (vk != nullptr) {\n+      vk->save_oop_fields(map, return_oops);\n+      save_oop_result = false;\n+    }\n+  }\n@@ -235,1 +253,1 @@\n-    return_value = Handle(thread, result);\n+    return_oops.push(Handle(thread, result));\n@@ -242,1 +260,1 @@\n-  if (objects != nullptr) {\n+  if (objects != nullptr || vk != nullptr) {\n@@ -247,1 +265,8 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, CHECK_AND_CLEAR_(true));\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+        bool skip_internal = (compiled_method != nullptr) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, CHECK_AND_CLEAR_(true));\n+      }\n@@ -252,1 +277,8 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+        bool skip_internal = (compiled_method != nullptr) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, THREAD);\n+      }\n@@ -255,2 +287,0 @@\n-    bool skip_internal = (compiled_method != nullptr) && !compiled_method->is_compiled_by_jvmci();\n-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal);\n@@ -261,1 +291,1 @@\n-  if (save_oop_result) {\n+  if (save_oop_result || vk != nullptr) {\n@@ -263,1 +293,2 @@\n-    deoptee.set_saved_oop_result(&map, return_value());\n+    assert(return_oops.length() == 1, \"no inline type\");\n+    deoptee.set_saved_oop_result(&map, return_oops.pop()());\n@@ -598,1 +629,1 @@\n-  \/\/ If the sender is deoptimized the we must retrieve the address of the handler\n+  \/\/ If the sender is deoptimized we must retrieve the address of the handler\n@@ -1083,2 +1114,12 @@\n-\n-    oop obj = nullptr;\n+    \/\/ Check if the object may be null and has an additional is_init input that needs\n+    \/\/ to be checked before using the field values. Skip re-allocation if it is null.\n+    if (sv->maybe_null()) {\n+      assert(k->is_inline_klass(), \"must be an inline klass\");\n+      intptr_t init_value = StackValue::create_stack_value(fr, reg_map, sv->is_init())->get_int();\n+      jint is_init = (jint)*((jint*)&init_value);\n+      if (is_init == 0) {\n+        continue;\n+      }\n+    }\n+\n+    oop obj = nullptr;\n@@ -1112,0 +1153,4 @@\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* ak = FlatArrayKlass::cast(k);\n+      \/\/ Inline type array must be zeroed because not all memory is reassigned\n+      obj = ak->allocate(sv->field_size(), THREAD);\n@@ -1141,0 +1186,15 @@\n+\/\/ We're deoptimizing at the return of a call, inline type fields are\n+\/\/ in registers. When we go back to the interpreter, it will expect a\n+\/\/ reference to an inline type instance. Allocate and initialize it from\n+\/\/ the register values here.\n+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {\n+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);\n+  if (new_vt == nullptr) {\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);\n+  }\n+  return_oops.clear();\n+  return_oops.push(Handle(THREAD, new_vt));\n+  return false;\n+}\n+\n@@ -1313,0 +1373,1 @@\n+  InstanceKlass* _klass;\n@@ -1317,0 +1378,1 @@\n+    _klass = nullptr;\n@@ -1326,1 +1388,1 @@\n-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal) {\n+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal, int base_offset, TRAPS) {\n@@ -1335,0 +1397,8 @@\n+        if (fs.signature()->is_Q_signature()) {\n+          if (fs.is_inlined()) {\n+            \/\/ Resolve klass of flattened inline type field\n+            field._klass = InlineKlass::cast(klass->get_inline_type_field_klass(fs.index()));\n+          } else {\n+            field._type = T_OBJECT;\n+          }\n+        }\n@@ -1342,0 +1412,11 @@\n+    BasicType type = fields->at(i)._type;\n+    int offset = base_offset + fields->at(i)._offset;\n+    \/\/ Check for flattened inline type field before accessing the ScopeValue because it might not have any fields\n+    if (type == T_PRIMITIVE_OBJECT) {\n+      \/\/ Recursively re-assign flattened inline type fields\n+      InstanceKlass* vk = fields->at(i)._klass;\n+      assert(vk != nullptr, \"must be resolved\");\n+      offset -= InlineKlass::cast(vk)->first_field_offset(); \/\/ Adjust offset to omit oop header\n+      svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, skip_internal, offset, CHECK_0);\n+      continue; \/\/ Continue because we don't need to increment svIndex\n+    }\n@@ -1345,3 +1426,2 @@\n-    int offset = fields->at(i)._offset;\n-    BasicType type = fields->at(i)._type;\n-      case T_OBJECT: case T_ARRAY:\n+      case T_OBJECT:\n+      case T_ARRAY:\n@@ -1428,0 +1508,14 @@\n+\/\/ restore fields of an eliminated inline type array\n+void Deoptimization::reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool skip_internal, TRAPS) {\n+  InlineKlass* vk = vak->element_klass();\n+  assert(vk->flatten_array(), \"should only be used for flattened inline type arrays\");\n+  \/\/ Adjust offset to omit oop header\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_PRIMITIVE_OBJECT) - InlineKlass::cast(vk)->first_field_offset();\n+  \/\/ Initialize all elements of the flattened inline type array\n+  for (int i = 0; i < sv->field_size(); i++) {\n+    ScopeValue* val = sv->field_at(i);\n+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));\n+    reassign_fields_by_klass(vk, fr, reg_map, val->as_ObjectValue(), 0, (oop)obj, skip_internal, offset, CHECK);\n+  }\n+}\n+\n@@ -1429,1 +1523,1 @@\n-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal) {\n+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal, TRAPS) {\n@@ -1434,1 +1528,1 @@\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+    assert(obj.not_null() || realloc_failures || sv->maybe_null(), \"reallocation was missed\");\n@@ -1474,1 +1568,4 @@\n-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal);\n+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal, 0, CHECK);\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      reassign_flat_array_elements(fr, reg_map, sv, (flatArrayOop) obj(), vak, skip_internal, CHECK);\n@@ -1634,1 +1731,1 @@\n-  \/\/ Deoptimize only if the frame comes from compile code.\n+  \/\/ Deoptimize only if the frame comes from compiled code.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":120,"deletions":23,"binary":false,"changes":143,"status":"modified"},{"patch":"@@ -180,0 +180,1 @@\n+  static bool realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS);\n@@ -182,1 +183,2 @@\n-  static void reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal);\n+  static void reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool skip_internal, TRAPS);\n+  static void reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal, TRAPS);\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -58,1 +59,1 @@\n-  return is_final() && (is_static() || ik->is_hidden() || ik->is_record());\n+  return is_final() && (is_static() || ik->is_hidden() || ik->is_record() || ik->is_inline_klass());\n@@ -155,3 +156,5 @@\n-  print_on(st);\n-  st->print(\" \");\n-\n+  if (ft != T_PRIMITIVE_OBJECT) {\n+    print_on(st);\n+    st->print(\" \");\n+  }\n+  jint as_int = 0;\n@@ -187,5 +190,10 @@\n-    case T_ARRAY:\n-      if (obj->obj_field(offset()) != nullptr) {\n-        obj->obj_field(offset())->print_value_on(st);\n-      } else {\n-        st->print(\"nullptr\");\n+    case T_PRIMITIVE_OBJECT:\n+      if (is_inlined()) {\n+        \/\/ Print fields of inlined fields (recursively)\n+        InlineKlass* vk = InlineKlass::cast(field_holder()->get_inline_type_field_klass(index()));\n+        int field_offset = offset() - vk->first_field_offset();\n+        obj = cast_to_oop(cast_from_oop<address>(obj) + field_offset);\n+        st->print_cr(\"Inline type field inlined '%s':\", vk->name()->as_C_string());\n+        FieldPrinter print_field(st, obj);\n+        vk->do_nonstatic_fields(&print_field);\n+        return; \/\/ Do not print underlying representation\n@@ -193,1 +201,2 @@\n-      break;\n+      \/\/ inline type field not inlined, fall through\n+    case T_ARRAY:\n@@ -232,0 +241,1 @@\n+  st->cr();\n","filename":"src\/hotspot\/share\/runtime\/fieldDescriptor.cpp","additions":20,"deletions":10,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -62,0 +63,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -350,0 +354,19 @@\n+\n+#ifdef COMPILER1\n+  if (cm->is_compiled_by_c1() && cm->method()->has_scalarized_args() &&\n+      pc() < cm->verified_inline_entry_point()) {\n+    \/\/ The VEP and VIEP(RO) of C1-compiled methods call into the runtime to buffer scalarized value\n+    \/\/ type args. We can't deoptimize at that point because the buffers have not yet been initialized.\n+    \/\/ Also, if the method is synchronized, we first need to acquire the lock.\n+    \/\/ Don't patch the return pc to delay deoptimization until we enter the method body (the check\n+    \/\/ added in LIRGenerator::do_Base will detect the pending deoptimization by checking the original_pc).\n+#if defined ASSERT && !defined AARCH64   \/\/ Stub call site does not look like NativeCall on AArch64\n+    NativeCall* call = nativeCall_before(this->pc());\n+    address dest = call->destination();\n+    assert(dest == Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id) ||\n+           dest == Runtime1::entry_for(Runtime1::buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+    return;\n+  }\n+#endif\n+\n@@ -742,1 +765,1 @@\n-                          OopClosure* f) {\n+                          OopClosure* f, BufferedValueClosure* bvt_f) {\n@@ -754,1 +777,3 @@\n-      _f->do_oop(addr);\n+      if (_f != NULL) {\n+        _f->do_oop(addr);\n+      }\n@@ -766,1 +791,3 @@\n-        _f->do_oop(addr);\n+        if (_f != NULL) {\n+          _f->do_oop(addr);\n+        }\n@@ -941,1 +968,1 @@\n-  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f);\n+  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f, NULL);\n@@ -953,0 +980,17 @@\n+void frame::buffered_values_interpreted_do(BufferedValueClosure* f) {\n+  assert(is_interpreted_frame(), \"Not an interpreted frame\");\n+  Thread *thread = Thread::current();\n+  methodHandle m (thread, interpreter_frame_method());\n+  jint      bci = interpreter_frame_bci();\n+\n+  assert(m->is_method(), \"checking frame value\");\n+  assert(!m->is_native() && bci >= 0 && bci < m->code_size(),\n+         \"invalid bci value\");\n+\n+  InterpreterFrameClosure blk(this, m->max_locals(), m->max_stack(), NULL, f);\n+\n+  \/\/ process locals & expression stack\n+  InterpreterOopMap mask;\n+  m->mask_for(bci, &mask);\n+  mask.iterate_oop(&blk);\n+}\n@@ -1004,0 +1048,1 @@\n+    assert(_offset < _arg_size, \"out of bounds\");\n@@ -1030,5 +1075,1 @@\n-    _arg_size  = ArgumentSizeComputer(signature).size() + (has_receiver ? 1 : 0) + (has_appendix ? 1 : 0);\n-\n-    int arg_size;\n-    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &arg_size);\n-    assert(arg_size == _arg_size, \"wrong arg size\");\n+    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &_arg_size);\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":50,"deletions":9,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -437,0 +437,1 @@\n+  void buffered_values_interpreted_do(BufferedValueClosure* f);\n","filename":"src\/hotspot\/share\/runtime\/frame.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -797,0 +797,18 @@\n+  notproduct(bool, PrintInlineLayout, false,                                \\\n+          \"Print field layout for each inline type\")                        \\\n+                                                                            \\\n+  notproduct(bool, PrintFlatArrayLayout, false,                             \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxSize, -1,                                \\\n+          \"Max size for flattening inline array elements, <0 no limit\")     \\\n+                                                                            \\\n+  product(intx, InlineFieldMaxFlatSize, 128,                                \\\n+          \"Max size for flattening inline type fields, <0 no limit\")        \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  product(bool, InlineArrayAtomicAccess, false,                             \\\n+          \"Atomic inline array accesses by-default, for all inline arrays\") \\\n+                                                                            \\\n@@ -1972,0 +1990,23 @@\n+  product(bool, EnableValhalla, true,                                       \\\n+          \"Enable experimental Valhalla features\")                          \\\n+                                                                            \\\n+  product(bool, EnablePrimitiveClasses, false,                              \\\n+          \"Enable experimental Valhalla primitive classes\")                 \\\n+                                                                            \\\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressCallingConvention, false,                             \\\n+          \"Stress the scalarized calling convention.\")                      \\\n+                                                                            \\\n+  product(bool, UseArrayMarkWordCheck, NOT_LP64(false) LP64_ONLY(true),     \\\n+          \"Use bits in the mark word to check for flat\/null-free arrays\")   \\\n+                                                                            \\\n+  product(ccstrlist, ForceNonTearable, \"\", DIAGNOSTIC,                      \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"oops\/inlineKlass.hpp\"\n+#include \"runtime\/atomic.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/handles.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+class InlineKlass;\n@@ -131,0 +132,1 @@\n+DEF_HANDLE(flatArray        , is_flatArray_noinline        )\n","filename":"src\/hotspot\/share\/runtime\/handles.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -64,0 +64,1 @@\n+DEF_HANDLE_CONSTR(flatArray, is_flatArray_noinline)\n","filename":"src\/hotspot\/share\/runtime\/handles.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -122,0 +122,1 @@\n+  VMRegImpl::set_regName();       \/\/ need this before generate_stubs (for printing oop maps).\n@@ -143,1 +144,0 @@\n-  VMRegImpl::set_regName(); \/\/ need this before generate_stubs (for printing oop maps).\n","filename":"src\/hotspot\/share\/runtime\/init.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -149,4 +150,4 @@\n-    case T_BOOLEAN: \/\/ fall through\n-    case T_CHAR   : \/\/ fall through\n-    case T_SHORT  : \/\/ fall through\n-    case T_INT    : \/\/ fall through\n+    case T_BOOLEAN  : \/\/ fall through\n+    case T_CHAR     : \/\/ fall through\n+    case T_SHORT    : \/\/ fall through\n+    case T_INT      : \/\/ fall through\n@@ -154,2 +155,3 @@\n-    case T_OBJECT : \/\/ fall through\n-    case T_ARRAY  : \/\/ fall through\n+    case T_OBJECT   : \/\/ fall through\n+    case T_ARRAY    : \/\/ fall through\n+    case T_PRIMITIVE_OBJECT: \/\/ fall through\n@@ -157,5 +159,5 @@\n-    case T_BYTE   : \/\/ fall through\n-    case T_VOID   : return T_INT;\n-    case T_LONG   : return T_LONG;\n-    case T_FLOAT  : return T_FLOAT;\n-    case T_DOUBLE : return T_DOUBLE;\n+    case T_BYTE     : \/\/ fall through\n+    case T_VOID     : return T_INT;\n+    case T_LONG     : return T_LONG;\n+    case T_FLOAT    : return T_FLOAT;\n+    case T_DOUBLE   : return T_DOUBLE;\n@@ -163,2 +165,3 @@\n-    case T_ARRAY  : \/\/ fall through\n-    case T_OBJECT:  return T_OBJECT;\n+    case T_ARRAY    : \/\/ fall through\n+    case T_OBJECT   : return T_OBJECT;\n+    case T_PRIMITIVE_OBJECT: return T_PRIMITIVE_OBJECT;\n@@ -292,0 +295,13 @@\n+\n+  \/\/ Special case for factory methods\n+  if (EnableValhalla && !constructor_signature->is_void_method_signature()) {\n+    guarantee(klass->is_inline_klass(), \"inline classes must use factory methods\");\n+    JavaValue factory_result(T_OBJECT);\n+    JavaCalls::call_static(&factory_result, klass,\n+                           vmSymbols::inline_factory_name(),\n+                           constructor_signature, args, CHECK_NH);\n+    return Handle(THREAD, factory_result.get_oop());\n+  }\n+\n+  \/\/ main branch of code creates a non-inline object:\n+  assert(!klass->is_inline_klass(), \"classic constructors are only for non-inline classes\");\n@@ -393,0 +409,12 @@\n+  jobject value_buffer = nullptr;\n+  if (InlineTypeReturnedAsFields && (result->get_type() == T_PRIMITIVE_OBJECT || result->get_type() == T_OBJECT)) {\n+    \/\/ Pre allocate a buffered inline type in case the result is returned\n+    \/\/ flattened by compiled code\n+    InlineKlass* vk = method->returns_inline_type(thread);\n+    if (vk != nullptr && vk->can_be_returned_as_fields()) {\n+      oop instance = vk->allocate_instance(CHECK);\n+      value_buffer = JNIHandles::make_local(thread, instance);\n+      result->set_jobject(value_buffer);\n+    }\n+  }\n+\n@@ -443,0 +471,1 @@\n+    JNIHandles::destroy_local(value_buffer);\n@@ -579,0 +608,1 @@\n+    case T_PRIMITIVE_OBJECT:\n@@ -591,1 +621,1 @@\n-  if (is_reference_type(return_type)) return_type = T_OBJECT;\n+  if (return_type == T_ARRAY) return_type = T_OBJECT;\n","filename":"src\/hotspot\/share\/runtime\/javaCalls.cpp","additions":44,"deletions":14,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -143,0 +143,1 @@\n+  oop           _return_buffered_value; \/\/ buffered value being returned\n@@ -700,0 +701,3 @@\n+  oop return_buffered_value() const              { return _return_buffered_value; }\n+  void set_return_buffered_value(oop val)        { _return_buffered_value = val; }\n+\n@@ -765,0 +769,1 @@\n+  static ByteSize return_buffered_value_offset() { return byte_offset_of(JavaThread, _return_buffered_value); }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"classfile\/vmSymbols.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"runtime\/javaCalls.hpp\"\n@@ -290,0 +292,38 @@\n+bool JNIHandles::is_same_object(jobject handle1, jobject handle2) {\n+  oop obj1 = resolve_no_keepalive(handle1);\n+  oop obj2 = resolve_no_keepalive(handle2);\n+\n+  bool ret = obj1 == obj2;\n+\n+  if (EnableValhalla) {\n+    if (!ret && obj1 != NULL && obj2 != NULL && obj1->klass() == obj2->klass() && obj1->klass()->is_inline_klass()) {\n+      \/\/ The two references are different, they are not null and they are both inline types,\n+      \/\/ a full substitutability test is required, calling ValueObjectMethods.isSubstitutable()\n+      \/\/ (similarly to InterpreterRuntime::is_substitutable)\n+      JavaThread* THREAD = JavaThread::current();\n+      Handle ha(THREAD, obj1);\n+      Handle hb(THREAD, obj2);\n+      JavaValue result(T_BOOLEAN);\n+      JavaCallArguments args;\n+      args.push_oop(ha);\n+      args.push_oop(hb);\n+      methodHandle method(THREAD, Universe::is_substitutable_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        \/\/ Something really bad happened because isSubstitutable() should not throw exceptions\n+        \/\/ If it is an error, just let it propagate\n+        \/\/ If it is an exception, wrap it into an InternalError\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in substitutability test\", e, false);\n+        }\n+      }\n+      ret = result.get_jboolean();\n+    }\n+  }\n+\n+  return ret;\n+}\n+\n+\n","filename":"src\/hotspot\/share\/runtime\/jniHandles.cpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -107,6 +107,0 @@\n-inline bool JNIHandles::is_same_object(jobject handle1, jobject handle2) {\n-  oop obj1 = resolve_no_keepalive(handle1);\n-  oop obj2 = resolve_no_keepalive(handle2);\n-  return obj1 == obj2;\n-}\n-\n","filename":"src\/hotspot\/share\/runtime\/jniHandles.inline.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -86,1 +86,1 @@\n-                          \/\/ 'B','Z','J','I','S','C','D','F','V','L','['\n+                          \/\/ 'B','Z','J','I','S','C','D','F','V','L','Q','['\n","filename":"src\/hotspot\/share\/runtime\/perfMemory.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -55,0 +56,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -350,1 +352,5 @@\n-    return oopFactory::new_objArray(k, length, THREAD);\n+    if (k->is_inline_klass() && java_lang_Class::is_secondary_mirror(element_mirror)) {\n+      return oopFactory::new_valueArray(k, length, THREAD);\n+    } else {\n+      return oopFactory::new_objArray(k, length, THREAD);\n+    }\n@@ -391,1 +397,5 @@\n-  klass = klass->array_klass(dim, CHECK_NULL);\n+  if (klass->is_inline_klass() && java_lang_Class::is_secondary_mirror(element_mirror)) {\n+    klass = InlineKlass::cast(klass)->value_array_klass(dim, CHECK_NULL);\n+  } else {\n+    klass = klass->array_klass(dim, CHECK_NULL);\n+  }\n@@ -768,3 +778,0 @@\n-  if (log_is_enabled(Debug, class, resolve)) {\n-    trace_class_resolution(nt);\n-  }\n@@ -776,2 +783,1 @@\n-  assert(!method()->is_initializer() ||\n-         (for_constant_pool_access && method()->is_static()),\n+  assert(!method()->name()->starts_with('<') || for_constant_pool_access,\n@@ -826,1 +832,3 @@\n-  assert(method()->is_initializer(), \"should call new_method instead\");\n+  assert(method()->is_object_constructor() ||\n+         method()->is_static_vnew_factory(),\n+         \"should call new_method instead\");\n@@ -879,1 +887,2 @@\n-  java_lang_reflect_Field::set_modifiers(rh(), fd->access_flags().as_int() & JVM_RECOGNIZED_FIELD_MODIFIERS);\n+  int modifiers = fd->access_flags().as_int() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n+  java_lang_reflect_Field::set_modifiers(rh(), modifiers);\n@@ -994,2 +1003,4 @@\n-    \/\/ no need to resolve if method is private or <init>\n-    if (reflected_method->is_private() || reflected_method->name() == vmSymbols::object_initializer_name()) {\n+    \/\/ no need to resolve if method is private, <init> or <vnew>\n+    if (reflected_method->is_private() ||\n+        reflected_method->name() == vmSymbols::object_initializer_name() ||\n+        reflected_method->name() == vmSymbols::inline_factory_name()) {\n@@ -1150,0 +1161,2 @@\n+  } else if (java_lang_Class::as_Klass(return_type_mirror)->is_inline_klass()) {\n+    rtype = java_lang_Class::is_primary_mirror(return_type_mirror) ? T_OBJECT : T_PRIMITIVE_OBJECT;\n@@ -1177,1 +1190,0 @@\n-  assert(method->name() == vmSymbols::object_initializer_name(), \"invalid constructor\");\n@@ -1184,0 +1196,18 @@\n+\n+  \/\/ Special case for factory methods\n+  if (!method->signature()->is_void_method_signature()) {\n+    assert(klass->is_inline_klass(), \"inline classes must use factory methods\");\n+    assert(method->name() == vmSymbols::inline_factory_name(), \"wrong factory method name\");\n+    Handle no_receiver; \/\/ null instead of receiver\n+    BasicType rtype;\n+    if (klass->is_hidden()) {\n+      rtype = T_OBJECT;\n+    } else {\n+      rtype = T_PRIMITIVE_OBJECT;\n+    }\n+    return invoke(klass, method, no_receiver, override, ptypes, rtype, args, false, CHECK_NULL);\n+  }\n+\n+  \/\/ main branch of code creates a non-inline object:\n+  assert(!klass->is_inline_klass(), \"classic constructors are only for non-inline classes\");\n+  assert(method->name() == vmSymbols::object_initializer_name(), \"wrong constructor name\");\n","filename":"src\/hotspot\/share\/runtime\/reflection.cpp","additions":42,"deletions":12,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -918,0 +919,1 @@\n+    ResourceMark rm;\n@@ -919,1 +921,2 @@\n-    bool return_oop = nm->method()->is_returning_oop();\n+    Method* method = nm->method();\n+    bool return_oop = method->is_returning_oop();\n@@ -921,1 +924,17 @@\n-    Handle return_value;\n+\n+    GrowableArray<Handle> return_values;\n+    InlineKlass* vk = NULL;\n+    if (return_oop && InlineTypeReturnedAsFields &&\n+        (method->result_type() == T_PRIMITIVE_OBJECT || method->result_type() == T_OBJECT)) {\n+      \/\/ Check if an inline type is returned as fields\n+      vk = InlineKlass::returned_inline_klass(map);\n+      if (vk != NULL) {\n+        \/\/ We're at a safepoint at the return of a method that returns\n+        \/\/ multiple values. We must make sure we preserve the oop values\n+        \/\/ across the safepoint.\n+        assert(vk == method->returns_inline_type(thread()), \"bad inline klass\");\n+        vk->save_oop_fields(map, return_values);\n+        return_oop = false;\n+      }\n+    }\n+\n@@ -928,1 +947,1 @@\n-      return_value = Handle(self, result);\n+      return_values.push(Handle(self, result));\n@@ -942,1 +961,4 @@\n-      caller_fr.set_saved_oop_result(&map, return_value());\n+      assert(return_values.length() == 1, \"only one return value\");\n+      caller_fr.set_saved_oop_result(&map, return_values.pop()());\n+    } else if (vk != NULL) {\n+      vk->restore_oop_results(map, return_values);\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":26,"deletions":4,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -49,0 +50,2 @@\n+#include \"oops\/access.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -53,0 +56,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -54,0 +58,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -89,1 +94,0 @@\n-address             SharedRuntime::_resolve_static_call_entry;\n@@ -110,1 +114,0 @@\n-  _resolve_static_call_entry           = _resolve_static_call_blob->entry_point();\n@@ -1190,0 +1193,15 @@\n+  \/\/ Substitutability test implementation piggy backs on static call resolution\n+  Bytecodes::Code code = caller->java_code_at(bci);\n+  if (code == Bytecodes::_if_acmpeq || code == Bytecodes::_if_acmpne) {\n+    bc = Bytecodes::_invokestatic;\n+    methodHandle attached_method(THREAD, extract_attached_method(vfst));\n+    assert(attached_method.not_null(), \"must have attached method\");\n+    vmClasses::ValueObjectMethods_klass()->initialize(CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, false, CHECK_NH);\n+#ifdef ASSERT\n+    Method* is_subst = vmClasses::ValueObjectMethods_klass()->find_method(vmSymbols::isSubstitutable_name(), vmSymbols::object_object_boolean_signature());\n+    assert(callinfo.selected_method() == is_subst, \"must be isSubstitutable method\");\n+#endif\n+    return receiver;\n+  }\n+\n@@ -1225,0 +1243,6 @@\n+    } else {\n+      assert(attached_method->has_scalarized_args(), \"invalid use of attached method\");\n+      if (!attached_method->method_holder()->is_inline_klass()) {\n+        \/\/ Ignore the attached method in this case to not confuse below code\n+        attached_method = methodHandle(current, nullptr);\n+      }\n@@ -1233,0 +1257,1 @@\n+  bool check_null_and_abstract = true;\n@@ -1246,2 +1271,3 @@\n-    if (attached_method.is_null()) {\n-      Method* callee = bytecode.static_target(CHECK_NH);\n+    Method* callee = attached_method();\n+    if (callee == nullptr) {\n+      callee = bytecode.static_target(CHECK_NH);\n@@ -1252,7 +1278,17 @@\n-\n-    \/\/ Retrieve from a compiled argument list\n-    receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n-    assert(oopDesc::is_oop_or_null(receiver()), \"\");\n-\n-    if (receiver.is_null()) {\n-      THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+    bool caller_is_c1 = callerFrame.is_compiled_frame() && callerFrame.cb()->is_compiled_by_c1();\n+    if (!caller_is_c1 && callee->is_scalarized_arg(0)) {\n+      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n+      \/\/ Resolve the call without receiver null checking.\n+      assert(!callee->mismatch(), \"calls with inline type receivers should never mismatch\");\n+      assert(attached_method.not_null() && !attached_method->is_abstract(), \"must have non-abstract attached method\");\n+      if (bc == Bytecodes::_invokeinterface) {\n+        bc = Bytecodes::_invokevirtual; \/\/ C2 optimistically replaces interface calls by virtual calls\n+      }\n+      check_null_and_abstract = false;\n+    } else {\n+      \/\/ Retrieve from a compiled argument list\n+      receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n+      assert(oopDesc::is_oop_or_null(receiver()), \"\");\n+      if (receiver.is_null()) {\n+        THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+      }\n@@ -1265,1 +1301,1 @@\n-    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);\n@@ -1274,1 +1310,1 @@\n-  if (has_receiver) {\n+  if (has_receiver && check_null_and_abstract) {\n@@ -1302,1 +1338,1 @@\n-methodHandle SharedRuntime::find_callee_method(TRAPS) {\n+methodHandle SharedRuntime::find_callee_method(bool is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1328,0 +1364,4 @@\n+    \/\/ Calls via mismatching methods are always non-scalarized\n+    if (callinfo.resolved_method()->mismatch() && !is_optimized) {\n+      caller_is_c1 = true;\n+    }\n@@ -1335,1 +1375,1 @@\n-methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, TRAPS) {\n+methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1337,1 +1377,1 @@\n-  callee_method = resolve_sub_helper(is_virtual, is_optimized, THREAD);\n+  callee_method = resolve_sub_helper(is_virtual, is_optimized, caller_is_c1, THREAD);\n@@ -1354,1 +1394,1 @@\n-      callee_method = resolve_sub_helper(is_virtual, is_optimized, THREAD);\n+      callee_method = resolve_sub_helper(is_virtual, is_optimized, caller_is_c1, THREAD);\n@@ -1362,1 +1402,1 @@\n-                                                CompiledMethod* caller_nm, bool is_virtual, bool is_optimized,\n+                                                CompiledMethod* caller_nm, bool is_virtual, bool is_optimized, bool& caller_is_c1,\n@@ -1386,1 +1426,8 @@\n-    assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, \"sanity check\");\n+    Klass* receiver_klass = nullptr;\n+    if (!caller_is_c1 && callee_method->is_scalarized_arg(0)) {\n+      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n+      receiver_klass = callee_method->method_holder();\n+    } else {\n+      assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, \"sanity check\");\n+      receiver_klass = invoke_code == Bytecodes::_invokehandle ? nullptr : receiver->klass();\n+    }\n@@ -1388,3 +1435,2 @@\n-    Klass* klass = invoke_code == Bytecodes::_invokehandle ? nullptr : receiver->klass();\n-    CompiledIC::compute_monomorphic_entry(callee_method, klass,\n-                     is_optimized, static_bound, is_nmethod, virtual_call_info,\n+    CompiledIC::compute_monomorphic_entry(callee_method, receiver_klass,\n+                     is_optimized, static_bound, is_nmethod, caller_is_c1, virtual_call_info,\n@@ -1394,1 +1440,1 @@\n-    CompiledStaticCall::compute_entry(callee_method, is_nmethod, static_call_info);\n+    CompiledStaticCall::compute_entry(callee_method, caller_nm, static_call_info);\n@@ -1447,1 +1493,1 @@\n-methodHandle SharedRuntime::resolve_sub_helper(bool is_virtual, bool is_optimized, TRAPS) {\n+methodHandle SharedRuntime::resolve_sub_helper(bool is_virtual, bool is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1467,0 +1513,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || (call_info.resolved_method()->mismatch() && !is_optimized)) {\n+    caller_is_c1 = true;\n+  }\n@@ -1531,1 +1581,1 @@\n-                                                  is_virtual, is_optimized, receiver,\n+                                                  is_virtual, is_optimized, caller_is_c1, receiver,\n@@ -1557,0 +1607,2 @@\n+  bool is_optimized = false;\n+  bool caller_is_c1 = false;\n@@ -1558,1 +1610,1 @@\n-    callee_method = SharedRuntime::handle_ic_miss_helper(CHECK_NULL);\n+    callee_method = SharedRuntime::handle_ic_miss_helper(is_optimized, caller_is_c1, CHECK_NULL);\n@@ -1563,2 +1615,1 @@\n-  assert(callee_method->verified_code_entry() != nullptr, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  return entry_for_handle_wrong_method(callee_method, false, is_optimized, caller_is_c1);\n@@ -1605,1 +1656,5 @@\n-      return callee->get_c2i_entry();\n+      if (caller_frame.is_interpreted_frame()) {\n+        return callee->get_c2i_inline_entry();\n+      } else {\n+        return callee->get_c2i_entry();\n+      }\n@@ -1611,0 +1666,3 @@\n+  bool is_static_call = false;\n+  bool is_optimized = false;\n+  bool caller_is_c1 = false;\n@@ -1613,1 +1671,1 @@\n-    callee_method = SharedRuntime::reresolve_call_site(CHECK_NULL);\n+    callee_method = SharedRuntime::reresolve_call_site(is_static_call, is_optimized, caller_is_c1, CHECK_NULL);\n@@ -1617,2 +1675,1 @@\n-  assert(callee_method->verified_code_entry() != nullptr, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  return entry_for_handle_wrong_method(callee_method, is_static_call, is_optimized, caller_is_c1);\n@@ -1659,0 +1716,1 @@\n+  bool caller_is_c1 = false;\n@@ -1661,1 +1719,1 @@\n-    callee_method = SharedRuntime::resolve_helper(false, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(false, false, caller_is_c1, CHECK_NULL);\n@@ -1689,2 +1747,4 @@\n-  assert(callee_method->verified_code_entry() != nullptr, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  address entry = caller_is_c1 ?\n+    callee_method->verified_inline_code_entry() : callee_method->verified_code_entry();\n+  assert(entry != nullptr, \"Jump to zero!\");\n+  return entry;\n@@ -1697,0 +1757,1 @@\n+  bool caller_is_c1 = false;\n@@ -1698,1 +1759,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, false, caller_is_c1, CHECK_NULL);\n@@ -1702,2 +1763,4 @@\n-  assert(callee_method->verified_code_entry() != nullptr, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  address entry = caller_is_c1 ?\n+    callee_method->verified_inline_code_entry() : callee_method->verified_inline_ro_code_entry();\n+  assert(entry != nullptr, \"Jump to zero!\");\n+  return entry;\n@@ -1711,0 +1774,1 @@\n+  bool caller_is_c1 = false;\n@@ -1712,1 +1776,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, true, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, true, caller_is_c1, CHECK_NULL);\n@@ -1716,2 +1780,4 @@\n-  assert(callee_method->verified_code_entry() != nullptr, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  address entry = caller_is_c1 ?\n+    callee_method->verified_inline_code_entry() : callee_method->verified_code_entry();\n+  assert(entry != nullptr, \"Jump to zero!\");\n+  return entry;\n@@ -1728,1 +1794,1 @@\n-                                                   bool& needs_ic_stub_refill, TRAPS) {\n+                                                   bool& needs_ic_stub_refill, bool& is_optimized, bool caller_is_c1, TRAPS) {\n@@ -1739,0 +1805,1 @@\n+    is_optimized = true;\n@@ -1776,0 +1843,1 @@\n+                                            caller_is_c1,\n@@ -1784,1 +1852,1 @@\n-    bool successful = inline_cache->set_to_megamorphic(&call_info, bc, needs_ic_stub_refill, CHECK_false);\n+    bool successful = inline_cache->set_to_megamorphic(&call_info, bc, needs_ic_stub_refill, caller_is_c1, CHECK_false);\n@@ -1800,1 +1868,1 @@\n-methodHandle SharedRuntime::handle_ic_miss_helper(TRAPS) {\n+methodHandle SharedRuntime::handle_ic_miss_helper(bool& is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1820,1 +1888,3 @@\n-    methodHandle callee_method = SharedRuntime::reresolve_call_site(CHECK_(methodHandle()));\n+    bool is_static_call = false;\n+    methodHandle callee_method = SharedRuntime::reresolve_call_site(is_static_call, is_optimized, caller_is_c1, CHECK_(methodHandle()));\n+    assert(!is_static_call, \"IC miss at static call?\");\n@@ -1879,0 +1949,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_is_c1 = true;\n+  }\n@@ -1884,1 +1958,1 @@\n-                                                     bc, call_info, needs_ic_stub_refill, CHECK_(methodHandle()));\n+                                                     bc, call_info, needs_ic_stub_refill, is_optimized, caller_is_c1, CHECK_(methodHandle()));\n@@ -1916,1 +1990,1 @@\n-methodHandle SharedRuntime::reresolve_call_site(TRAPS) {\n+methodHandle SharedRuntime::reresolve_call_site(bool& is_static_call, bool& is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1926,0 +2000,3 @@\n+  if (caller.is_compiled_frame()) {\n+    caller_is_c1 = caller.cb()->is_compiled_by_c1();\n+  }\n@@ -1936,1 +2013,0 @@\n-    bool is_static_call = false;\n@@ -1974,1 +2050,2 @@\n-        bool is_static_call = false;\n+        is_static_call = false;\n+        is_optimized = false;\n@@ -1981,0 +2058,1 @@\n+            is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);\n@@ -2004,2 +2082,1 @@\n-  methodHandle callee_method = find_callee_method(CHECK_(methodHandle()));\n-\n+  methodHandle callee_method = find_callee_method(is_optimized, caller_is_c1, CHECK_(methodHandle()));\n@@ -2179,1 +2256,1 @@\n-      address entry_point = callee->verified_entry_point();\n+      address entry_point = cb->is_compiled_by_c1() ? callee->verified_inline_entry_point() : callee->verified_entry_point();\n@@ -2509,1 +2586,1 @@\n-  static int adapter_encoding(BasicType in) {\n+  static BasicType adapter_encoding(BasicType in) {\n@@ -2515,1 +2592,1 @@\n-        \/\/ There are all promoted to T_INT in the calling convention\n+        \/\/ They are all promoted to T_INT in the calling convention\n@@ -2542,1 +2619,1 @@\n-  AdapterFingerPrint(int total_args_passed, BasicType* sig_bt) {\n+  AdapterFingerPrint(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n@@ -2545,0 +2622,1 @@\n+    int total_args_passed = (sig != nullptr) ? sig->length() : 0;\n@@ -2562,0 +2640,2 @@\n+    BasicType prev_bt = T_ILLEGAL;\n+    int vt_count = 0;\n@@ -2564,4 +2644,27 @@\n-      for (int byte = 0; sig_index < total_args_passed && byte < _basic_types_per_int; byte++) {\n-        int bt = adapter_encoding(sig_bt[sig_index++]);\n-        assert((bt & _basic_type_mask) == bt, \"must fit in 4 bits\");\n-        value = (value << _basic_type_bits) | bt;\n+      for (int byte = 0; byte < _basic_types_per_int; byte++) {\n+        BasicType bt = T_ILLEGAL;\n+        if (sig_index < total_args_passed) {\n+          bt = sig->at(sig_index++)._bt;\n+          if (bt == T_PRIMITIVE_OBJECT) {\n+            \/\/ Found start of inline type in signature\n+            assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+            if (sig_index == 1 && has_ro_adapter) {\n+              \/\/ With a ro_adapter, replace receiver inline type delimiter by T_VOID to prevent matching\n+              \/\/ with other adapters that have the same inline type as first argument and no receiver.\n+              bt = T_VOID;\n+            }\n+            vt_count++;\n+          } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+            \/\/ Found end of inline type in signature\n+            assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+            vt_count--;\n+            assert(vt_count >= 0, \"invalid vt_count\");\n+          } else if (vt_count == 0) {\n+            \/\/ Widen fields that are not part of a scalarized inline type argument\n+            bt = adapter_encoding(bt);\n+          }\n+          prev_bt = bt;\n+        }\n+        int bt_val = (bt == T_ILLEGAL) ? 0 : bt;\n+        assert((bt_val & _basic_type_mask) == bt_val, \"must fit in 4 bits\");\n+        value = (value << _basic_type_bits) | bt_val;\n@@ -2571,0 +2674,1 @@\n+    assert(vt_count == 0, \"invalid vt_count\");\n@@ -2635,8 +2739,4 @@\n-        }\n-        switch (v) {\n-          case T_INT:    st.print(\"I\");    break;\n-          case T_LONG:   long_prev = true; break;\n-          case T_FLOAT:  st.print(\"F\");    break;\n-          case T_DOUBLE: st.print(\"D\");    break;\n-          case T_VOID:   break;\n-          default: ShouldNotReachHere();\n+        } else if (v == T_LONG) {\n+          long_prev = true;\n+        } else if (v != T_VOID){\n+          st.print(\"%c\", type2char((BasicType)v));\n@@ -2689,1 +2789,1 @@\n-static AdapterHandlerEntry* lookup(int total_args_passed, BasicType* sig_bt) {\n+static AdapterHandlerEntry* lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n@@ -2692,1 +2792,1 @@\n-  AdapterFingerPrint fp(total_args_passed, sig_bt);\n+  AdapterFingerPrint fp(sig, has_ro_adapter);\n@@ -2726,1 +2826,1 @@\n-const int AdapterHandlerLibrary_size = 16*K;\n+const int AdapterHandlerLibrary_size = 48*K;\n@@ -2768,1 +2868,1 @@\n-    _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(0, nullptr),\n+    _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(nullptr),\n@@ -2770,0 +2870,1 @@\n+                                                                wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,\n@@ -2771,4 +2872,8 @@\n-\n-    _no_arg_handler = create_adapter(no_arg_blob, 0, nullptr, true);\n-    BasicType obj_args[] = { T_OBJECT };\n-    _obj_arg_handler = create_adapter(obj_arg_blob, 1, obj_args, true);\n+    CompiledEntrySignature no_args;\n+    no_args.compute_calling_conventions();\n+    _no_arg_handler = create_adapter(no_arg_blob, no_args, true);\n+\n+    CompiledEntrySignature obj_args;\n+    SigEntry::add_entry(obj_args.sig(), T_OBJECT, nullptr);\n+    obj_args.compute_calling_conventions();\n+    _obj_arg_handler = create_adapter(obj_arg_blob, obj_args, true);\n@@ -2778,2 +2883,4 @@\n-    BasicType int_args[] = { T_INT };\n-    _int_arg_handler = create_adapter(int_arg_blob, 1, int_args, true);\n+    CompiledEntrySignature int_args;\n+    SigEntry::add_entry(int_args.sig(), T_INT, nullptr);\n+    int_args.compute_calling_conventions();\n+    _int_arg_handler = create_adapter(int_arg_blob, int_args, true);\n@@ -2781,2 +2888,5 @@\n-    BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-    _obj_int_arg_handler = create_adapter(obj_int_arg_blob, 2, obj_int_args, true);\n+    CompiledEntrySignature obj_int_args;\n+    SigEntry::add_entry(obj_int_args.sig(), T_OBJECT, nullptr);\n+    SigEntry::add_entry(obj_int_args.sig(), T_INT, nullptr);\n+    obj_int_args.compute_calling_conventions();\n+    _obj_int_arg_handler = create_adapter(obj_int_arg_blob, obj_int_args, true);\n@@ -2784,2 +2894,5 @@\n-    BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-    _obj_obj_arg_handler = create_adapter(obj_obj_arg_blob, 2, obj_obj_args, true);\n+    CompiledEntrySignature obj_obj_args;\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT, nullptr);\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT, nullptr);\n+    obj_obj_args.compute_calling_conventions();\n+    _obj_obj_arg_handler = create_adapter(obj_obj_arg_blob, obj_obj_args, true);\n@@ -2793,0 +2906,1 @@\n+  return;\n@@ -2805,0 +2919,2 @@\n+                                                      address c2i_inline_entry,\n+                                                      address c2i_inline_ro_entry,\n@@ -2806,0 +2922,1 @@\n+                                                      address c2i_unverified_inline_entry,\n@@ -2807,3 +2924,2 @@\n-  \/\/ Insert an entry into the table\n-  return new AdapterHandlerEntry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry,\n-                                 c2i_no_clinit_check_entry);\n+  return new AdapterHandlerEntry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry,\n+                              c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -2814,1 +2930,1 @@\n-    return _abstract_method_handler;\n+    return nullptr;\n@@ -2821,0 +2937,3 @@\n+      if (InlineTypePassFieldsAsArgs && method->method_holder()->is_inline_klass()) {\n+        return nullptr;\n+      }\n@@ -2824,1 +2943,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_arg_handler;\n+      }\n@@ -2835,1 +2963,1 @@\n-             !method->is_static()) {\n+             !method->is_static() && (!InlineTypePassFieldsAsArgs || !method->method_holder()->is_inline_klass())) {\n@@ -2837,1 +2965,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_obj_arg_handler;\n+      }\n@@ -2851,5 +2988,9 @@\n-class AdapterSignatureIterator : public SignatureIterator {\n- private:\n-  BasicType stack_sig_bt[16];\n-  BasicType* sig_bt;\n-  int index;\n+CompiledEntrySignature::CompiledEntrySignature(Method* method) :\n+  _method(method), _num_inline_args(0), _has_inline_recv(false),\n+  _regs(nullptr), _regs_cc(nullptr), _regs_cc_ro(nullptr),\n+  _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),\n+  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _supers(nullptr) {\n+  _sig = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc_ro = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+}\n@@ -2857,13 +2998,10 @@\n- public:\n-  AdapterSignatureIterator(Symbol* signature,\n-                           fingerprint_t fingerprint,\n-                           bool is_static,\n-                           int total_args_passed) :\n-    SignatureIterator(signature, fingerprint),\n-    index(0)\n-  {\n-    sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n-    if (!is_static) { \/\/ Pass in receiver first\n-      sig_bt[index++] = T_OBJECT;\n-    }\n-    do_parameters_on(this);\n+\/\/ See if we can save space by sharing the same entry for VIEP and VIEP(RO),\n+\/\/ or the same entry for VEP and VIEP(RO).\n+CodeOffsets::Entries CompiledEntrySignature::c1_inline_ro_entry_type() const {\n+  if (!has_scalarized_args()) {\n+    \/\/ VEP\/VIEP\/VIEP(RO) all share the same entry. There's no packing.\n+    return CodeOffsets::Verified_Entry;\n+  }\n+  if (_method->is_static()) {\n+    \/\/ Static methods don't need VIEP(RO)\n+    return CodeOffsets::Verified_Entry;\n@@ -2872,2 +3010,13 @@\n-  BasicType* basic_types() {\n-    return sig_bt;\n+  if (has_inline_recv()) {\n+    if (num_inline_args() == 1) {\n+      \/\/ Share same entry for VIEP and VIEP(RO).\n+      \/\/ This is quite common: we have an instance method in an InlineKlass that has\n+      \/\/ no inline type args other than <this>.\n+      return CodeOffsets::Verified_Inline_Entry;\n+    } else {\n+      assert(num_inline_args() > 1, \"must be\");\n+      \/\/ No sharing:\n+      \/\/   VIEP(RO) -- <this> is passed as object\n+      \/\/   VEP      -- <this> is passed as fields\n+      return CodeOffsets::Verified_Inline_Entry_RO;\n+    }\n@@ -2876,0 +3025,92 @@\n+  \/\/ Either a static method, or <this> is not an inline type\n+  if (args_on_stack_cc() != args_on_stack_cc_ro()) {\n+    \/\/ No sharing:\n+    \/\/ Some arguments are passed on the stack, and we have inserted reserved entries\n+    \/\/ into the VEP, but we never insert reserved entries into the VIEP(RO).\n+    return CodeOffsets::Verified_Inline_Entry_RO;\n+  } else {\n+    \/\/ Share same entry for VEP and VIEP(RO).\n+    return CodeOffsets::Verified_Entry;\n+  }\n+}\n+\n+\/\/ Returns all super methods (transitive) in classes and interfaces that are overridden by the current method.\n+GrowableArray<Method*>* CompiledEntrySignature::get_supers() {\n+  if (_supers != nullptr) {\n+    return _supers;\n+  }\n+  _supers = new GrowableArray<Method*>();\n+  \/\/ Skip private, static, and <init> methods\n+  if (_method->is_private() || _method->is_static() || _method->is_object_constructor()) {\n+    return _supers;\n+  }\n+  Symbol* name = _method->name();\n+  Symbol* signature = _method->signature();\n+  const Klass* holder = _method->method_holder()->super();\n+  Symbol* holder_name = holder->name();\n+  ThreadInVMfromUnknown tiv;\n+  JavaThread* current = JavaThread::current();\n+  HandleMark hm(current);\n+  Handle loader(current, _method->method_holder()->class_loader());\n+\n+  \/\/ Walk up the class hierarchy and search for super methods\n+  while (holder != nullptr) {\n+    Method* super_method = holder->lookup_method(name, signature);\n+    if (super_method == nullptr) {\n+      break;\n+    }\n+    if (!super_method->is_static() && !super_method->is_private() &&\n+        (!super_method->is_package_private() ||\n+         super_method->method_holder()->is_same_class_package(loader(), holder_name))) {\n+      _supers->push(super_method);\n+    }\n+    holder = super_method->method_holder()->super();\n+  }\n+  \/\/ Search interfaces for super methods\n+  Array<InstanceKlass*>* interfaces = _method->method_holder()->transitive_interfaces();\n+  for (int i = 0; i < interfaces->length(); ++i) {\n+    Method* m = interfaces->at(i)->lookup_method(name, signature);\n+    if (m != nullptr && !m->is_static() && m->is_public()) {\n+      _supers->push(m);\n+    }\n+  }\n+  return _supers;\n+}\n+\n+\/\/ Iterate over arguments and compute scalarized and non-scalarized signatures\n+void CompiledEntrySignature::compute_calling_conventions(bool init) {\n+  bool has_scalarized = false;\n+  if (_method != nullptr) {\n+    InstanceKlass* holder = _method->method_holder();\n+    int arg_num = 0;\n+    if (!_method->is_static()) {\n+      if (holder->is_inline_klass() && InlineKlass::cast(holder)->can_be_passed_as_fields() &&\n+          (init || _method->is_scalarized_arg(arg_num))) {\n+        _sig_cc->appendAll(InlineKlass::cast(holder)->extended_sig());\n+        has_scalarized = true;\n+        _has_inline_recv = true;\n+        _num_inline_args++;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, T_OBJECT, holder->name());\n+      }\n+      SigEntry::add_entry(_sig, T_OBJECT, holder->name());\n+      SigEntry::add_entry(_sig_cc_ro, T_OBJECT, holder->name());\n+      arg_num++;\n+    }\n+    for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {\n+      BasicType bt = ss.type();\n+      if (bt == T_OBJECT || bt == T_PRIMITIVE_OBJECT) {\n+        InlineKlass* vk = ss.as_inline_klass(holder);\n+        if (vk != nullptr && vk->can_be_passed_as_fields() && (init || _method->is_scalarized_arg(arg_num))) {\n+          \/\/ Check for a calling convention mismatch with super method(s)\n+          bool scalar_super = false;\n+          bool non_scalar_super = false;\n+          GrowableArray<Method*>* supers = get_supers();\n+          for (int i = 0; i < supers->length(); ++i) {\n+            Method* super_method = supers->at(i);\n+            if (super_method->is_scalarized_arg(arg_num)) {\n+              scalar_super = true;\n+            } else {\n+              non_scalar_super = true;\n+            }\n+          }\n@@ -2877,3 +3118,8 @@\n-  int slots() {\n-    return index;\n-  }\n+          \/\/ Randomly enable below code paths for stress testing\n+          bool stress = init && StressCallingConvention;\n+          if (stress && (os::random() & 1) == 1) {\n+            non_scalar_super = true;\n+            if ((os::random() & 1) == 1) {\n+              scalar_super = true;\n+            }\n+          }\n@@ -2881,0 +3127,48 @@\n+          if (non_scalar_super) {\n+            \/\/ Found a super method with a non-scalarized argument. Fall back to the non-scalarized calling convention.\n+            if (scalar_super) {\n+              \/\/ Found non-scalar *and* scalar super methods. We can't handle both.\n+              \/\/ Mark the scalar method as mismatch and re-compile call sites to use non-scalarized calling convention.\n+              for (int i = 0; i < supers->length(); ++i) {\n+                Method* super_method = supers->at(i);\n+                if (super_method->is_scalarized_arg(arg_num) debug_only(|| (stress && (os::random() & 1) == 1))) {\n+                  super_method->set_mismatch(true);\n+                  MutexLocker ml(Compile_lock, Mutex::_safepoint_check_flag);\n+                  JavaThread* thread = JavaThread::current();\n+                  HandleMark hm(thread);\n+                  methodHandle mh(thread, super_method);\n+                  CodeCache::flush_dependents_on_method(mh);\n+                }\n+              }\n+            }\n+            \/\/ Fall back to non-scalarized calling convention\n+            SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+            SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+          } else {\n+            _num_inline_args++;\n+            has_scalarized = true;\n+            int last = _sig_cc->length();\n+            int last_ro = _sig_cc_ro->length();\n+            _sig_cc->appendAll(vk->extended_sig());\n+            _sig_cc_ro->appendAll(vk->extended_sig());\n+            if (bt == T_OBJECT) {\n+              \/\/ Nullable inline type argument, insert InlineTypeNode::IsInit field right after T_PRIMITIVE_OBJECT\n+              _sig_cc->insert_before(last+1, SigEntry(T_BOOLEAN, -1, nullptr));\n+              _sig_cc_ro->insert_before(last_ro+1, SigEntry(T_BOOLEAN, -1, nullptr));\n+            }\n+          }\n+        } else {\n+          SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+          SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+        }\n+        bt = T_OBJECT;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, ss.type(), ss.as_symbol());\n+        SigEntry::add_entry(_sig_cc_ro, ss.type(), ss.as_symbol());\n+      }\n+      SigEntry::add_entry(_sig, bt, ss.as_symbol());\n+      if (bt != T_VOID) {\n+        arg_num++;\n+      }\n+    }\n+  }\n@@ -2882,1 +3176,3 @@\n- private:\n+  \/\/ Compute the non-scalarized calling convention\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n@@ -2884,5 +3180,16 @@\n-  friend class SignatureIterator;  \/\/ so do_parameters_on can call do_type\n-  void do_type(BasicType type) {\n-    sig_bt[index++] = type;\n-    if (type == T_LONG || type == T_DOUBLE) {\n-      sig_bt[index++] = T_VOID; \/\/ Longs & doubles take 2 Java slots\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized && !_method->is_native()) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n+\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+\n+    \/\/ Upper bound on stack arguments to avoid hitting the argument limit and\n+    \/\/ bailing out of compilation (\"unsupported incoming calling sequence\").\n+    \/\/ TODO we need a reasonable limit (flag?) here\n+    if (MAX2(_args_on_stack_cc, _args_on_stack_cc_ro) <= 60) {\n+      return; \/\/ Success\n@@ -2891,1 +3198,10 @@\n-};\n+\n+  \/\/ No scalarized args\n+  _sig_cc = _sig;\n+  _regs_cc = _regs;\n+  _args_on_stack_cc = _args_on_stack;\n+\n+  _sig_cc_ro = _sig;\n+  _regs_cc_ro = _regs;\n+  _args_on_stack_cc_ro = _args_on_stack;\n+}\n@@ -2908,2 +3224,9 @@\n-  \/\/ Fill in the signature array, for the calling-convention call.\n-  int total_args_passed = method->size_of_parameters(); \/\/ All args on stack\n+  CompiledEntrySignature ces(method());\n+  ces.compute_calling_conventions();\n+  if (ces.has_scalarized_args()) {\n+    method->set_has_scalarized_args(true);\n+    method->set_c1_needs_stack_repair(ces.c1_needs_stack_repair());\n+    method->set_c2_needs_stack_repair(ces.c2_needs_stack_repair());\n+  } else if (method->is_abstract()) {\n+    return _abstract_method_handler;\n+  }\n@@ -2911,4 +3234,0 @@\n-  AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-  assert(si.slots() == total_args_passed, \"\");\n-  BasicType* sig_bt = si.basic_types();\n@@ -2918,0 +3237,13 @@\n+    if (ces.has_scalarized_args() && method->is_abstract()) {\n+      \/\/ Save a C heap allocated version of the signature for abstract methods with scalarized inline type arguments\n+      address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();\n+      entry = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(nullptr),\n+                                               StubRoutines::throw_AbstractMethodError_entry(),\n+                                               wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,\n+                                               wrong_method_abstract, wrong_method_abstract);\n+      GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc_ro()->length(), mtInternal);\n+      heap_sig->appendAll(ces.sig_cc_ro());\n+      entry->set_sig_cc(heap_sig);\n+      return entry;\n+    }\n+\n@@ -2919,1 +3251,1 @@\n-    entry = lookup(total_args_passed, sig_bt);\n+    entry = lookup(ces.sig_cc(), ces.has_inline_recv());\n@@ -2925,1 +3257,1 @@\n-        AdapterHandlerEntry* comparison_entry = create_adapter(comparison_blob, total_args_passed, sig_bt, false);\n+        AdapterHandlerEntry* comparison_entry = create_adapter(comparison_blob, ces, false);\n@@ -2935,1 +3267,1 @@\n-    entry = create_adapter(new_adapter, total_args_passed, sig_bt, \/* allocate_code_blob *\/ true);\n+    entry = create_adapter(new_adapter, ces, \/* allocate_code_blob *\/ true);\n@@ -2946,2 +3278,1 @@\n-                                                           int total_args_passed,\n-                                                           BasicType* sig_bt,\n+                                                           CompiledEntrySignature& ces,\n@@ -2956,5 +3287,0 @@\n-  VMRegPair stack_regs[16];\n-  VMRegPair* regs = (total_args_passed <= 16) ? stack_regs : NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);\n-\n-  \/\/ Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage\n-  int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed);\n@@ -2968,1 +3294,1 @@\n-  AdapterFingerPrint* fingerprint = new AdapterFingerPrint(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fingerprint = new AdapterFingerPrint(ces.sig_cc(), ces.has_inline_recv());\n@@ -2971,5 +3297,17 @@\n-                                                total_args_passed,\n-                                                comp_args_on_stack,\n-                                                sig_bt,\n-                                                regs,\n-                                                fingerprint);\n+                                                ces.args_on_stack(),\n+                                                ces.sig(),\n+                                                ces.regs(),\n+                                                ces.sig_cc(),\n+                                                ces.regs_cc(),\n+                                                ces.sig_cc_ro(),\n+                                                ces.regs_cc_ro(),\n+                                                fingerprint,\n+                                                new_adapter,\n+                                                allocate_code_blob);\n+\n+  if (ces.has_scalarized_args()) {\n+    \/\/ Save a C heap allocated version of the scalarized signature and store it in the adapter\n+    GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc()->length(), mtInternal);\n+    heap_sig->appendAll(ces.sig_cc());\n+    entry->set_sig_cc(heap_sig);\n+  }\n@@ -2986,1 +3324,0 @@\n-  new_adapter = AdapterBlob::create(&buffer);\n@@ -3028,0 +3365,2 @@\n+  assert(base <= _c2i_inline_entry || _c2i_inline_entry == nullptr, \"\");\n+  assert(base <= _c2i_inline_ro_entry || _c2i_inline_ro_entry == nullptr, \"\");\n@@ -3029,0 +3368,1 @@\n+  assert(base <= _c2i_unverified_inline_entry || _c2i_unverified_inline_entry == nullptr, \"\");\n@@ -3041,0 +3381,4 @@\n+  if (_c2i_inline_entry != nullptr)\n+    _c2i_inline_entry += delta;\n+  if (_c2i_inline_ro_entry != nullptr)\n+    _c2i_inline_ro_entry += delta;\n@@ -3043,0 +3387,2 @@\n+  if (_c2i_unverified_inline_entry != nullptr)\n+    _c2i_unverified_inline_entry += delta;\n@@ -3051,0 +3397,3 @@\n+  if (_sig_cc != nullptr) {\n+    delete _sig_cc;\n+  }\n@@ -3137,0 +3486,1 @@\n+      BasicType stack_sig_bt[16];\n@@ -3138,0 +3488,1 @@\n+      BasicType* sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n@@ -3140,5 +3491,13 @@\n-      AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-      BasicType* sig_bt = si.basic_types();\n-      assert(si.slots() == total_args_passed, \"\");\n-      BasicType ret_type = si.return_type();\n+      int i = 0;\n+      if (!method->is_static()) {  \/\/ Pass in receiver first\n+        sig_bt[i++] = T_OBJECT;\n+      }\n+      SignatureStream ss(method->signature());\n+      for (; !ss.at_return_type(); ss.next()) {\n+        sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n+        if (ss.type() == T_LONG || ss.type() == T_DOUBLE) {\n+          sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n+        }\n+      }\n+      assert(i == total_args_passed, \"\");\n+      BasicType ret_type = ss.type();\n@@ -3381,0 +3740,6 @@\n+  if (get_c2i_entry() != nullptr) {\n+    st->print(\" c2iVE: \" INTPTR_FORMAT, p2i(get_c2i_inline_entry()));\n+  }\n+  if (get_c2i_entry() != nullptr) {\n+    st->print(\" c2iVROE: \" INTPTR_FORMAT, p2i(get_c2i_inline_ro_entry()));\n+  }\n@@ -3382,1 +3747,4 @@\n-    st->print(\" c2iUV: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iUE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+  }\n+  if (get_c2i_unverified_entry() != nullptr) {\n+    st->print(\" c2iUVE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_inline_entry()));\n@@ -3471,0 +3839,196 @@\n+\n+\/\/ We are at a compiled code to interpreter call. We need backing\n+\/\/ buffers for all inline type arguments. Allocate an object array to\n+\/\/ hold them (convenient because once we're done with it we don't have\n+\/\/ to worry about freeing it).\n+oop SharedRuntime::allocate_inline_types_impl(JavaThread* current, methodHandle callee, bool allocate_receiver, TRAPS) {\n+  assert(InlineTypePassFieldsAsArgs, \"no reason to call this\");\n+  ResourceMark rm;\n+\n+  int nb_slots = 0;\n+  InstanceKlass* holder = callee->method_holder();\n+  allocate_receiver &= !callee->is_static() && holder->is_inline_klass() && callee->is_scalarized_arg(0);\n+  if (allocate_receiver) {\n+    nb_slots++;\n+  }\n+  int arg_num = callee->is_static() ? 0 : 1;\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if ((bt == T_OBJECT || bt == T_PRIMITIVE_OBJECT) && callee->is_scalarized_arg(arg_num)) {\n+      nb_slots++;\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);\n+  objArrayHandle array(THREAD, array_oop);\n+  arg_num = callee->is_static() ? 0 : 1;\n+  int i = 0;\n+  if (allocate_receiver) {\n+    InlineKlass* vk = InlineKlass::cast(holder);\n+    oop res = vk->allocate_instance(CHECK_NULL);\n+    array->obj_at_put(i++, res);\n+  }\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if ((bt == T_OBJECT || bt == T_PRIMITIVE_OBJECT) && callee->is_scalarized_arg(arg_num)) {\n+      InlineKlass* vk = ss.as_inline_klass(holder);\n+      assert(vk != nullptr, \"Unexpected klass\");\n+      oop res = vk->allocate_instance(CHECK_NULL);\n+      array->obj_at_put(i++, res);\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  return array();\n+}\n+\n+JRT_ENTRY(void, SharedRuntime::allocate_inline_types(JavaThread* current, Method* callee_method, bool allocate_receiver))\n+  methodHandle callee(current, callee_method);\n+  oop array = SharedRuntime::allocate_inline_types_impl(current, callee, allocate_receiver, CHECK);\n+  current->set_vm_result(array);\n+  current->set_vm_result_2(callee()); \/\/ TODO: required to keep callee live?\n+JRT_END\n+\n+\/\/ We're returning from an interpreted method: load each field into a\n+\/\/ register following the calling convention\n+JRT_LEAF(void, SharedRuntime::load_inline_type_fields_in_regs(JavaThread* current, oopDesc* res))\n+{\n+  assert(res->klass()->is_inline_klass(), \"only inline types here\");\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+  assert(callerFrame.is_interpreted_frame(), \"should be coming from interpreter\");\n+\n+  InlineKlass* vk = InlineKlass::cast(res->klass());\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  if (regs == nullptr) {\n+    \/\/ The fields of the inline klass don't fit in registers, bail out\n+    return;\n+  }\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_PRIMITIVE_OBJECT) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    address loc = reg_map.location(pair.first(), nullptr);\n+    switch(bt) {\n+    case T_BOOLEAN:\n+      *(jboolean*)loc = res->bool_field(off);\n+      break;\n+    case T_CHAR:\n+      *(jchar*)loc = res->char_field(off);\n+      break;\n+    case T_BYTE:\n+      *(jbyte*)loc = res->byte_field(off);\n+      break;\n+    case T_SHORT:\n+      *(jshort*)loc = res->short_field(off);\n+      break;\n+    case T_INT: {\n+      *(jint*)loc = res->int_field(off);\n+      break;\n+    }\n+    case T_LONG:\n+#ifdef _LP64\n+      *(intptr_t*)loc = res->long_field(off);\n+#else\n+      Unimplemented();\n+#endif\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY: {\n+      *(oop*)loc = res->obj_field(off);\n+      break;\n+    }\n+    case T_FLOAT:\n+      *(jfloat*)loc = res->float_field(off);\n+      break;\n+    case T_DOUBLE:\n+      *(jdouble*)loc = res->double_field(off);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+#ifdef ASSERT\n+  VMRegPair pair = regs->at(0);\n+  address loc = reg_map.location(pair.first(), nullptr);\n+  assert(*(oopDesc**)loc == res, \"overwritten object\");\n+#endif\n+\n+  current->set_vm_result(res);\n+}\n+JRT_END\n+\n+\/\/ We've returned to an interpreted method, the interpreter needs a\n+\/\/ reference to an inline type instance. Allocate it and initialize it\n+\/\/ from field's values in registers.\n+JRT_BLOCK_ENTRY(void, SharedRuntime::store_inline_type_fields_to_buf(JavaThread* current, intptr_t res))\n+{\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+\n+#ifdef ASSERT\n+  InlineKlass* verif_vk = InlineKlass::returned_inline_klass(reg_map);\n+#endif\n+\n+  if (!is_set_nth_bit(res, 0)) {\n+    \/\/ We're not returning with inline type fields in registers (the\n+    \/\/ calling convention didn't allow it for this inline klass)\n+    assert(!Metaspace::contains((void*)res), \"should be oop or pointer in buffer area\");\n+    current->set_vm_result((oopDesc*)res);\n+    assert(verif_vk == nullptr, \"broken calling convention\");\n+    return;\n+  }\n+\n+  clear_nth_bit(res, 0);\n+  InlineKlass* vk = (InlineKlass*)res;\n+  assert(verif_vk == vk, \"broken calling convention\");\n+  assert(Metaspace::contains((void*)res), \"should be klass\");\n+\n+  \/\/ Allocate handles for every oop field so they are safe in case of\n+  \/\/ a safepoint when allocating\n+  GrowableArray<Handle> handles;\n+  vk->save_oop_fields(reg_map, handles);\n+\n+  \/\/ It's unsafe to safepoint until we are here\n+  JRT_BLOCK;\n+  {\n+    JavaThread* THREAD = current;\n+    oop vt = vk->realloc_result(reg_map, handles, CHECK);\n+    current->set_vm_result(vt);\n+  }\n+  JRT_BLOCK_END;\n+}\n+JRT_END\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":711,"deletions":147,"binary":false,"changes":858,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"asm\/codeBuffer.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -38,0 +40,1 @@\n+class SigEntry;\n@@ -50,1 +53,1 @@\n-                                          CompiledMethod* caller_nm, bool is_virtual, bool is_optimized,\n+                                          CompiledMethod* caller_nm, bool is_virtual, bool is_optimized, bool& caller_is_c1,\n@@ -52,1 +55,1 @@\n-  static methodHandle resolve_sub_helper(bool is_virtual, bool is_optimized, TRAPS);\n+  static methodHandle resolve_sub_helper(bool is_virtual, bool is_optimized, bool& caller_is_c1, TRAPS);\n@@ -62,1 +65,0 @@\n-  static address             _resolve_static_call_entry;\n@@ -85,1 +87,0 @@\n-\n@@ -322,1 +323,1 @@\n-  static methodHandle resolve_helper(bool is_virtual, bool is_optimized, TRAPS);\n+  static methodHandle resolve_helper(bool is_virtual, bool is_optimized, bool& caller_is_c1, TRAPS);\n@@ -330,1 +331,1 @@\n-                                             bool& needs_ic_stub_refill, TRAPS);\n+                                             bool& needs_ic_stub_refill, bool& is_optimized, bool caller_is_c1, TRAPS);\n@@ -336,1 +337,1 @@\n-  static methodHandle reresolve_call_site(TRAPS);\n+  static methodHandle reresolve_call_site(bool& is_static_call, bool& is_optimized, bool& caller_is_c1, TRAPS);\n@@ -340,1 +341,1 @@\n-  static methodHandle handle_ic_miss_helper(TRAPS);\n+  static methodHandle handle_ic_miss_helper(bool& is_optimized, bool& caller_is_c1, TRAPS);\n@@ -343,1 +344,1 @@\n-  static methodHandle find_callee_method(TRAPS);\n+  static methodHandle find_callee_method(bool is_optimized, bool& caller_is_c1, TRAPS);\n@@ -349,0 +350,13 @@\n+  static address entry_for_handle_wrong_method(methodHandle callee_method, bool is_static_call, bool is_optimized, bool caller_is_c1) {\n+    assert(callee_method->verified_code_entry() != nullptr, \"Jump to zero!\");\n+    assert(callee_method->verified_inline_code_entry() != nullptr, \"Jump to zero!\");\n+    assert(callee_method->verified_inline_ro_code_entry() != nullptr, \"Jump to zero!\");\n+    if (caller_is_c1) {\n+      return callee_method->verified_inline_code_entry();\n+    } else if (is_static_call || is_optimized) {\n+      return callee_method->verified_code_entry();\n+    } else {\n+      return callee_method->verified_inline_ro_code_entry();\n+    }\n+  }\n+\n@@ -371,0 +385,8 @@\n+  static int java_calling_convention(const GrowableArray<SigEntry>* sig, VMRegPair* regs) {\n+    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig->length());\n+    int total_args_passed = SigEntry::fill_sig_bt(sig, sig_bt);\n+    return java_calling_convention(sig_bt, regs, total_args_passed);\n+  }\n+  static int java_return_convention(const BasicType* sig_bt, VMRegPair* regs, int total_args_passed);\n+  static const uint java_return_convention_max_int;\n+  static const uint java_return_convention_max_float;\n@@ -418,6 +440,11 @@\n-  static AdapterHandlerEntry* generate_i2c2i_adapters(MacroAssembler *_masm,\n-                                                      int total_args_passed,\n-                                                      int max_arg,\n-                                                      const BasicType *sig_bt,\n-                                                      const VMRegPair *regs,\n-                                                      AdapterFingerPrint* fingerprint);\n+  static AdapterHandlerEntry* generate_i2c2i_adapters(MacroAssembler *masm,\n+                                                      int comp_args_on_stack,\n+                                                      const GrowableArray<SigEntry>* sig,\n+                                                      const VMRegPair* regs,\n+                                                      const GrowableArray<SigEntry>* sig_cc,\n+                                                      const VMRegPair* regs_cc,\n+                                                      const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                      const VMRegPair* regs_cc_ro,\n+                                                      AdapterFingerPrint* fingerprint,\n+                                                      AdapterBlob*& new_adapter,\n+                                                      bool allocate_code_blob);\n@@ -426,2 +453,1 @@\n-                              int total_args_passed,\n-                              const BasicType *sig_bt,\n+                              const GrowableArray<SigEntry>* sig,\n@@ -501,0 +527,3 @@\n+  static void load_inline_type_fields_in_regs(JavaThread* current, oopDesc* res);\n+  static void store_inline_type_fields_to_buf(JavaThread* current, intptr_t res);\n+\n@@ -511,0 +540,2 @@\n+  static void allocate_inline_types(JavaThread* current, Method* callee, bool allocate_receiver);\n+  static oop allocate_inline_types_impl(JavaThread* current, methodHandle callee, bool allocate_receiver, TRAPS);\n@@ -514,0 +545,1 @@\n+  static BufferedInlineTypeBlob* generate_buffered_inline_type_adapter(const InlineKlass* vk);\n@@ -619,0 +651,2 @@\n+  address _c2i_inline_entry;\n+  address _c2i_inline_ro_entry;\n@@ -620,0 +654,1 @@\n+  address _c2i_unverified_inline_entry;\n@@ -622,0 +657,3 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  const GrowableArray<SigEntry>* _sig_cc;\n+\n@@ -630,1 +668,2 @@\n-                      address c2i_unverified_entry,\n+                      address c2i_inline_entry, address c2i_inline_ro_entry,\n+                      address c2i_unverified_entry, address c2i_unverified_inline_entry,\n@@ -635,0 +674,2 @@\n+    _c2i_inline_entry(c2i_inline_entry),\n+    _c2i_inline_ro_entry(c2i_inline_ro_entry),\n@@ -636,1 +677,3 @@\n-    _c2i_no_clinit_check_entry(c2i_no_clinit_check_entry)\n+    _c2i_unverified_inline_entry(c2i_unverified_inline_entry),\n+    _c2i_no_clinit_check_entry(c2i_no_clinit_check_entry),\n+    _sig_cc(nullptr)\n@@ -645,4 +688,7 @@\n-  address get_i2c_entry()                  const { return _i2c_entry; }\n-  address get_c2i_entry()                  const { return _c2i_entry; }\n-  address get_c2i_unverified_entry()       const { return _c2i_unverified_entry; }\n-  address get_c2i_no_clinit_check_entry()  const { return _c2i_no_clinit_check_entry; }\n+  address get_i2c_entry()                   const { return _i2c_entry; }\n+  address get_c2i_entry()                   const { return _c2i_entry; }\n+  address get_c2i_inline_entry()            const { return _c2i_inline_entry; }\n+  address get_c2i_inline_ro_entry()         const { return _c2i_inline_ro_entry; }\n+  address get_c2i_unverified_entry()        const { return _c2i_unverified_entry; }\n+  address get_c2i_unverified_inline_entry() const { return _c2i_unverified_inline_entry; }\n+  address get_c2i_no_clinit_check_entry()   const { return _c2i_no_clinit_check_entry; }\n@@ -653,0 +699,4 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  void set_sig_cc(const GrowableArray<SigEntry>* sig)  { _sig_cc = sig; }\n+  const GrowableArray<SigEntry>* get_sig_cc()    const { return _sig_cc; }\n+\n@@ -665,0 +715,2 @@\n+class CompiledEntrySignature;\n+\n@@ -679,2 +731,1 @@\n-                                             int total_args_passed,\n-                                             BasicType* sig_bt,\n+                                             CompiledEntrySignature& ces,\n@@ -686,4 +737,2 @@\n-                                        address i2c_entry,\n-                                        address c2i_entry,\n-                                        address c2i_unverified_entry,\n-                                        address c2i_no_clinit_check_entry = nullptr);\n+                                        address i2c_entry, address c2i_entry, address c2i_inline_entry, address c2i_inline_ro_entry,\n+                                        address c2i_unverified_entry, address c2i_unverified_inline_entry, address c2i_no_clinit_check_entry = nullptr);\n@@ -702,0 +751,59 @@\n+\/\/ Utility class for computing the calling convention of the 3 types\n+\/\/ of compiled method entries:\n+\/\/     Method::_from_compiled_entry               - sig_cc\n+\/\/     Method::_from_compiled_inline_ro_entry     - sig_cc_ro\n+\/\/     Method::_from_compiled_inline_entry        - sig\n+class CompiledEntrySignature : public StackObj {\n+  Method* _method;\n+  int  _num_inline_args;\n+  bool _has_inline_recv;\n+  GrowableArray<SigEntry>* _sig;\n+  GrowableArray<SigEntry>* _sig_cc;\n+  GrowableArray<SigEntry>* _sig_cc_ro;\n+  VMRegPair* _regs;\n+  VMRegPair* _regs_cc;\n+  VMRegPair* _regs_cc_ro;\n+\n+  int _args_on_stack;\n+  int _args_on_stack_cc;\n+  int _args_on_stack_cc_ro;\n+\n+  bool _c1_needs_stack_repair;\n+  bool _c2_needs_stack_repair;\n+\n+  GrowableArray<Method*>* _supers;\n+\n+public:\n+  Method* method()                     const { return _method; }\n+\n+  \/\/ Used by Method::_from_compiled_inline_entry\n+  GrowableArray<SigEntry>* sig()       const { return _sig; }\n+\n+  \/\/ Used by Method::_from_compiled_entry\n+  GrowableArray<SigEntry>* sig_cc()    const { return _sig_cc; }\n+\n+  \/\/ Used by Method::_from_compiled_inline_ro_entry\n+  GrowableArray<SigEntry>* sig_cc_ro() const { return _sig_cc_ro; }\n+\n+  VMRegPair* regs()                    const { return _regs; }\n+  VMRegPair* regs_cc()                 const { return _regs_cc; }\n+  VMRegPair* regs_cc_ro()              const { return _regs_cc_ro; }\n+\n+  int args_on_stack()                  const { return _args_on_stack; }\n+  int args_on_stack_cc()               const { return _args_on_stack_cc; }\n+  int args_on_stack_cc_ro()            const { return _args_on_stack_cc_ro; }\n+\n+  int  num_inline_args()               const { return _num_inline_args; }\n+  bool has_inline_recv()               const { return _has_inline_recv; }\n+\n+  bool has_scalarized_args()           const { return _sig != _sig_cc; }\n+  bool c1_needs_stack_repair()         const { return _c1_needs_stack_repair; }\n+  bool c2_needs_stack_repair()         const { return _c2_needs_stack_repair; }\n+  CodeOffsets::Entries c1_inline_ro_entry_type() const;\n+\n+  GrowableArray<Method*>* get_supers();\n+\n+  CompiledEntrySignature(Method* method = nullptr);\n+  void compute_calling_conventions(bool init = true);\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":137,"deletions":29,"binary":false,"changes":166,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -40,0 +41,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -52,1 +54,1 @@\n-\/\/ FieldType  = \"B\" | \"C\" | \"D\" | \"F\" | \"I\" | \"J\" | \"S\" | \"Z\" | \"L\" ClassName \";\" | \"[\" FieldType.\n+\/\/ FieldType  = \"B\" | \"C\" | \"D\" | \"F\" | \"I\" | \"J\" | \"S\" | \"Z\" | \"L\" ClassName \";\" | \"Q\" ValueClassName \";\" | \"[\" FieldType.\n@@ -250,0 +252,1 @@\n+  case T_PRIMITIVE_OBJECT:\n@@ -336,0 +339,1 @@\n+  case T_PRIMITIVE_OBJECT:\n@@ -420,0 +424,1 @@\n+  case JVM_SIGNATURE_PRIMITIVE_OBJECT:\n@@ -501,0 +506,15 @@\n+InlineKlass* SignatureStream::as_inline_klass(InstanceKlass* holder) {\n+  ThreadInVMfromUnknown tiv;\n+  JavaThread* THREAD = JavaThread::current();\n+  HandleMark hm(THREAD);\n+  Handle class_loader(THREAD, holder->class_loader());\n+  Handle protection_domain(THREAD, holder->protection_domain());\n+  Klass* k = as_klass(class_loader, protection_domain, SignatureStream::CachedOrNull, THREAD);\n+  assert(!HAS_PENDING_EXCEPTION, \"Should never throw\");\n+  if (k != NULL && k->is_inline_klass()) {\n+    return InlineKlass::cast(k);\n+  } else {\n+    return NULL;\n+  }\n+}\n+\n@@ -540,1 +560,2 @@\n-  return klass->java_mirror();\n+  return has_Q_descriptor() ? InlineKlass::cast(klass)->val_mirror()\n+                            : klass->java_mirror();\n@@ -580,1 +601,0 @@\n-\n@@ -599,1 +619,1 @@\n-bool SignatureVerifier::is_valid_method_signature(Symbol* sig) {\n+bool SignatureVerifier::is_valid_method_signature(const Symbol* sig) {\n@@ -622,1 +642,1 @@\n-bool SignatureVerifier::is_valid_type_signature(Symbol* sig) {\n+bool SignatureVerifier::is_valid_type_signature(const Symbol* sig) {\n@@ -651,0 +671,1 @@\n+    case JVM_SIGNATURE_PRIMITIVE_OBJECT: \/\/ fall through\n@@ -669,0 +690,53 @@\n+\n+\/\/ Adds an argument to the signature\n+void SigEntry::add_entry(GrowableArray<SigEntry>* sig, BasicType bt, Symbol* symbol, int offset) {\n+  sig->append(SigEntry(bt, offset, symbol));\n+  if (bt == T_LONG || bt == T_DOUBLE) {\n+    sig->append(SigEntry(T_VOID, offset, symbol)); \/\/ Longs and doubles take two stack slots\n+  }\n+}\n+\n+\/\/ Returns true if the argument at index 'i' is not an inline type delimiter\n+bool SigEntry::skip_value_delimiters(const GrowableArray<SigEntry>* sig, int i) {\n+  return (sig->at(i)._bt != T_PRIMITIVE_OBJECT &&\n+          (sig->at(i)._bt != T_VOID || sig->at(i-1)._bt == T_LONG || sig->at(i-1)._bt == T_DOUBLE));\n+}\n+\n+\/\/ Fill basic type array from signature array\n+int SigEntry::fill_sig_bt(const GrowableArray<SigEntry>* sig, BasicType* sig_bt) {\n+  int count = 0;\n+  for (int i = 0; i < sig->length(); i++) {\n+    if (skip_value_delimiters(sig, i)) {\n+      sig_bt[count++] = sig->at(i)._bt;\n+    }\n+  }\n+  return count;\n+}\n+\n+\/\/ Create a temporary symbol from the signature array\n+TempNewSymbol SigEntry::create_symbol(const GrowableArray<SigEntry>* sig) {\n+  ResourceMark rm;\n+  int length = sig->length();\n+  char* sig_str = NEW_RESOURCE_ARRAY(char, 2*length + 3);\n+  int idx = 0;\n+  sig_str[idx++] = '(';\n+  for (int i = 0; i < length; i++) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_PRIMITIVE_OBJECT || bt == T_VOID) {\n+      \/\/ Ignore\n+    } else {\n+      if (bt == T_ARRAY) {\n+        bt = T_OBJECT; \/\/ We don't know the element type, treat as Object\n+      }\n+      sig_str[idx++] = type2char(bt);\n+      if (bt == T_OBJECT) {\n+        sig_str[idx++] = ';';\n+      }\n+    }\n+  }\n+  sig_str[idx++] = ')';\n+  \/\/ Add a dummy return type. It won't be used but SignatureStream needs it.\n+  sig_str[idx++] = 'V';\n+  sig_str[idx++] = '\\0';\n+  return SymbolTable::new_symbol(sig_str);\n+}\n","filename":"src\/hotspot\/share\/runtime\/signature.cpp","additions":79,"deletions":5,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -119,1 +120,1 @@\n-    return (signature_char == JVM_SIGNATURE_CLASS);\n+    return (signature_char == JVM_SIGNATURE_CLASS) || (signature_char == JVM_SIGNATURE_PRIMITIVE_OBJECT);\n@@ -260,1 +261,2 @@\n-    case T_OBJECT:  type_name(\"jobject\" ); break;\n+    case T_OBJECT:\n+    case T_PRIMITIVE_OBJECT:  type_name(\"jobject\" ); break;\n@@ -409,0 +411,1 @@\n+    case T_PRIMITIVE_OBJECT:\n@@ -521,0 +524,4 @@\n+  bool has_Q_descriptor() const {\n+    return has_envelope() && (_signature->char_at(_begin) == JVM_SIGNATURE_PRIMITIVE_OBJECT);\n+  }\n+\n@@ -563,0 +570,1 @@\n+\n@@ -564,0 +572,1 @@\n+  InlineKlass* as_inline_klass(InstanceKlass* holder);\n@@ -567,0 +576,51 @@\n+class SigEntryFilter;\n+typedef GrowableArrayFilterIterator<SigEntry, SigEntryFilter> ExtendedSignature;\n+\n+\/\/ Used for adapter generation. One SigEntry is used per element of\n+\/\/ the signature of the method. Inline type arguments are treated\n+\/\/ specially. See comment for InlineKlass::collect_fields().\n+class SigEntry {\n+ public:\n+  BasicType _bt;\n+  int _offset;\n+  Symbol* _symbol;\n+\n+  SigEntry()\n+    : _bt(T_ILLEGAL), _offset(-1), _symbol(NULL) {}\n+\n+  SigEntry(BasicType bt, int offset, Symbol* symbol)\n+    : _bt(bt), _offset(offset), _symbol(symbol) {}\n+\n+  static int compare(SigEntry* e1, SigEntry* e2) {\n+    if (e1->_offset != e2->_offset) {\n+      return e1->_offset - e2->_offset;\n+    }\n+    assert((e1->_bt == T_LONG && (e2->_bt == T_LONG || e2->_bt == T_VOID)) ||\n+           (e1->_bt == T_DOUBLE && (e2->_bt == T_DOUBLE || e2->_bt == T_VOID)) ||\n+           e1->_bt == T_PRIMITIVE_OBJECT || e2->_bt == T_PRIMITIVE_OBJECT || e1->_bt == T_VOID || e2->_bt == T_VOID, \"bad bt\");\n+    if (e1->_bt == e2->_bt) {\n+      assert(e1->_bt == T_PRIMITIVE_OBJECT || e1->_bt == T_VOID, \"only ones with duplicate offsets\");\n+      return 0;\n+    }\n+    if (e1->_bt == T_VOID ||\n+        e2->_bt == T_PRIMITIVE_OBJECT) {\n+      return 1;\n+    }\n+    if (e1->_bt == T_PRIMITIVE_OBJECT ||\n+        e2->_bt == T_VOID) {\n+      return -1;\n+    }\n+    ShouldNotReachHere();\n+    return 0;\n+  }\n+  static void add_entry(GrowableArray<SigEntry>* sig, BasicType bt, Symbol* symbol, int offset = -1);\n+  static bool skip_value_delimiters(const GrowableArray<SigEntry>* sig, int i);\n+  static int fill_sig_bt(const GrowableArray<SigEntry>* sig, BasicType* sig_bt);\n+  static TempNewSymbol create_symbol(const GrowableArray<SigEntry>* sig);\n+};\n+\n+class SigEntryFilter {\n+public:\n+  bool operator()(const SigEntry& entry) { return entry._bt != T_PRIMITIVE_OBJECT && entry._bt != T_VOID; }\n+};\n+\n@@ -632,1 +692,1 @@\n- #ifdef ASSERT\n+#ifdef ASSERT\n@@ -635,2 +695,2 @@\n-    static bool is_valid_method_signature(Symbol* sig);\n-    static bool is_valid_type_signature(Symbol* sig);\n+    static bool is_valid_method_signature(const Symbol* sig);\n+    static bool is_valid_type_signature(const Symbol* sig);\n","filename":"src\/hotspot\/share\/runtime\/signature.hpp","additions":65,"deletions":5,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -44,7 +44,0 @@\n-template StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMap* reg_map, ScopeValue* sv);\n-template StackValue* StackValue::create_stack_value(const frame* fr, const SmallRegisterMap* reg_map, ScopeValue* sv);\n-\n-template<typename RegisterMapT>\n-StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMapT* reg_map, ScopeValue* sv) {\n-  return create_stack_value(sv, stack_value_address(fr, reg_map, sv), reg_map);\n-}\n@@ -127,0 +120,4 @@\n+\n+template StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMap* reg_map, ScopeValue* sv);\n+template StackValue* StackValue::create_stack_value(const frame* fr, const SmallRegisterMap* reg_map, ScopeValue* sv);\n+\n@@ -128,1 +125,2 @@\n-StackValue* StackValue::create_stack_value(ScopeValue* sv, address value_addr, const RegisterMapT* reg_map) {\n+StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMapT* reg_map, ScopeValue* sv) {\n+  address value_addr = stack_value_address(fr, reg_map, sv);\n@@ -227,2 +225,9 @@\n-    Handle ov = ((ObjectValue *)sv)->value();\n-    return new StackValue(ov, (ov.is_null()) ? 1 : 0);\n+    ObjectValue* ov = ((ObjectValue *)sv);\n+    bool scalar_replaced = ov->value().is_null();\n+    if (ov->maybe_null()) {\n+      \/\/ Don't treat inline type as scalar replaced if it is null\n+      intptr_t is_init_value = StackValue::create_stack_value(fr, reg_map, ov->is_init())->get_int();\n+      jint is_init = (jint)*((jint*)&is_init_value);\n+      scalar_replaced &= (is_init != 0);\n+    }\n+    return new StackValue(ov->value(), scalar_replaced ? 1 : 0);\n","filename":"src\/hotspot\/share\/runtime\/stackValue.cpp","additions":15,"deletions":10,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -171,0 +171,3 @@\n+address StubRoutines::_load_inline_type_fields_in_regs = nullptr;\n+address StubRoutines::_store_inline_type_fields_to_buf = nullptr;\n+\n@@ -504,0 +507,1 @@\n+  case T_PRIMITIVE_OBJECT:\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -259,0 +259,3 @@\n+  static address _load_inline_type_fields_in_regs;\n+  static address _store_inline_type_fields_to_buf;\n+\n@@ -452,0 +455,3 @@\n+\n+  static address load_inline_type_fields_in_regs() { return _load_inline_type_fields_in_regs; }\n+  static address store_inline_type_fields_to_buf() { return _store_inline_type_fields_to_buf; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -286,0 +286,14 @@\n+#define CHECK_THROW_NOSYNC_IMSE(obj)  \\\n+  if (EnableValhalla && (obj)->mark().is_inline_type()) {  \\\n+    JavaThread* THREAD = current;           \\\n+    ResourceMark rm(THREAD);                \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n+#define CHECK_THROW_NOSYNC_IMSE_0(obj)  \\\n+  if (EnableValhalla && (obj)->mark().is_inline_type()) {  \\\n+    JavaThread* THREAD = current;             \\\n+    ResourceMark rm(THREAD);                  \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n@@ -312,0 +326,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -360,0 +375,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -479,0 +495,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -528,0 +545,4 @@\n+    if (EnableValhalla && mark.is_inline_type()) {\n+      return;\n+    }\n+    assert(!EnableValhalla || !object->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -591,0 +612,2 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n+\n@@ -601,0 +624,2 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n+\n@@ -621,0 +646,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -640,0 +666,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -678,0 +705,1 @@\n+  CHECK_THROW_NOSYNC_IMSE_0(obj);\n@@ -699,0 +727,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -714,0 +743,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -860,0 +890,4 @@\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+    \/\/ VM should be calling bootstrap method\n+    ShouldNotReachHere();\n+  }\n@@ -973,0 +1007,3 @@\n+  if (EnableValhalla && h_obj->mark().is_inline_type()) {\n+    return false;\n+  }\n@@ -1205,0 +1242,4 @@\n+  if (EnableValhalla) {\n+    guarantee(!object->klass()->is_inline_klass(), \"Attempt to inflate inline type\");\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -476,0 +476,1 @@\n+    case T_PRIMITIVE_OBJECT:\n","filename":"src\/hotspot\/share\/runtime\/vframe_hp.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -108,0 +108,1 @@\n+  template(ClassPrintLayout)                      \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -511,0 +511,4 @@\n+\n+void VM_PrintClassLayout::doit() {\n+  PrintClassLayout::print_class_layout(_out, _class_name);\n+}\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -262,0 +262,10 @@\n+class VM_PrintClassLayout: public VM_Operation {\n+ private:\n+  outputStream* _out;\n+  char* _class_name;\n+ public:\n+  VM_PrintClassLayout(outputStream* st, char* class_name): _out(st), _class_name(class_name) {}\n+  VMOp_Type type() const { return VMOp_PrintClassHierarchy; }\n+  void doit();\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -69,0 +69,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -223,1 +225,1 @@\n-  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ObjArrayKlass*)                        \\\n+  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ArrayKlass*)                        \\\n@@ -1221,0 +1223,1 @@\n+           declare_type(FlatArrayKlass, ArrayKlass)                       \\\n@@ -1224,0 +1227,1 @@\n+        declare_type(InlineKlass, InstanceKlass)                          \\\n@@ -1597,0 +1601,1 @@\n+  declare_c2_type(MachVEPNode, MachIdealNode)                             \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -111,0 +111,1 @@\n+  DCmdFactory::register_DCmdFactory(new DCmdFactoryImpl<PrintClassLayoutDCmd>(full_export, true, false));\n@@ -145,1 +146,0 @@\n-\n@@ -934,1 +934,25 @@\n-#endif\n+\n+PrintClassLayoutDCmd::PrintClassLayoutDCmd(outputStream* output, bool heap) :\n+                                       DCmdWithParser(output, heap),\n+  _classname(\"classname\", \"Name of class whose layout should be printed. \",\n+             \"STRING\", true) {\n+  _dcmdparser.add_dcmd_argument(&_classname);\n+}\n+\n+void PrintClassLayoutDCmd::execute(DCmdSource source, TRAPS) {\n+  VM_PrintClassLayout printClassLayoutOp(output(), _classname.value());\n+  VMThread::execute(&printClassLayoutOp);\n+}\n+\n+int PrintClassLayoutDCmd::num_arguments() {\n+  ResourceMark rm;\n+  PrintClassLayoutDCmd* dcmd = new PrintClassLayoutDCmd(NULL, false);\n+  if (dcmd != NULL) {\n+    DCmdMark mark(dcmd);\n+    return dcmd->_dcmdparser.num_arguments();\n+  } else {\n+    return 0;\n+  }\n+}\n+\n+#endif \/\/ INCLUDE_SERVICES\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":26,"deletions":2,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -389,0 +389,25 @@\n+class PrintClassLayoutDCmd : public DCmdWithParser {\n+protected:\n+  DCmdArgument<char*> _classname; \/\/ lass name whose layout should be printed.\n+public:\n+  PrintClassLayoutDCmd(outputStream* output, bool heap);\n+  static const char* name() {\n+    return \"VM.class_print_layout\";\n+  }\n+  static const char* description() {\n+    return \"Print the layout of an instance of a class, including inlined fields. \"\n+           \"The name of each class is followed by the ClassLoaderData* of its ClassLoader, \"\n+           \"or \\\"null\\\" if loaded by the bootstrap class loader.\";\n+  }\n+  static const char* impact() {\n+      return \"Medium: Depends on number of loaded classes.\";\n+  }\n+  static const JavaPermission permission() {\n+    JavaPermission p = {\"java.lang.management.ManagementPermission\",\n+                        \"monitor\", NULL};\n+    return p;\n+  }\n+  static int num_arguments();\n+  virtual void execute(DCmdSource source, TRAPS);\n+};\n+\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -43,0 +43,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -45,0 +47,1 @@\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n@@ -312,0 +315,22 @@\n+ * HPROF_FLAT_ARRAYS        list of flat arrays\n+ *\n+ *               [flat array sub-records]*\n+ *\n+ *               HPROF_FLAT_ARRAY      flat array\n+ *\n+ *                          id         array object ID (dumped as HPROF_GC_PRIM_ARRAY_DUMP)\n+ *                          id         element class ID (dumped by HPROF_GC_CLASS_DUMP)\n+ *\n+ * HPROF_INLINED_FIELDS     decribes inlined fields\n+ *\n+ *               [class with inlined fields sub-records]*\n+ *\n+ *               HPROF_CLASS_WITH_INLINED_FIELDS\n+ *\n+ *                          id         class ID (dumped as HPROF_GC_CLASS_DUMP)\n+ *\n+ *                          u2         number of instance inlined fields (not including super)\n+ *                          [u2,       inlined field index,\n+ *                           u2,       synthetic field count,\n+ *                           id,       original field name,\n+ *                           id]*      inlined field class ID (dumped by HPROF_GC_CLASS_DUMP)\n@@ -349,0 +374,7 @@\n+  \/\/ inlined object support\n+  HPROF_FLAT_ARRAYS             = 0x12,\n+  HPROF_INLINED_FIELDS          = 0x13,\n+  \/\/ inlined object subrecords\n+  HPROF_FLAT_ARRAY                  = 0x01,\n+  HPROF_CLASS_WITH_INLINED_FIELDS   = 0x01,\n+\n@@ -383,0 +415,65 @@\n+\n+class AbstractDumpWriter;\n+\n+class InlinedObjects {\n+\n+  struct ClassInlinedFields {\n+    const Klass *klass;\n+    uintx base_index;   \/\/ base index of the inlined field names (1st field has index base_index+1).\n+    ClassInlinedFields(const Klass *klass = nullptr, uintx base_index = 0) : klass(klass), base_index(base_index) {}\n+\n+    \/\/ For GrowableArray::find_sorted().\n+    static int compare(const ClassInlinedFields& a, const ClassInlinedFields& b) {\n+      return a.klass - b.klass;\n+    }\n+    \/\/ For GrowableArray::sort().\n+    static int compare(ClassInlinedFields* a, ClassInlinedFields* b) {\n+      return compare(*a, *b);\n+    }\n+  };\n+\n+  uintx _min_string_id;\n+  uintx _max_string_id;\n+\n+  GrowableArray<ClassInlinedFields> *_inlined_field_map;\n+\n+  \/\/ counters for classes with inlined fields and for the fields\n+  int _classes_count;\n+  int _inlined_fields_count;\n+\n+  static InlinedObjects *_instance;\n+\n+  static void inlined_field_names_callback(InlinedObjects* _this, const Klass *klass, uintx base_index, int count);\n+\n+  GrowableArray<oop> *_flat_arrays;\n+\n+public:\n+  InlinedObjects()\n+    : _min_string_id(0), _max_string_id(0),\n+    _inlined_field_map(nullptr),\n+    _classes_count(0), _inlined_fields_count(0),\n+    _flat_arrays(nullptr) {\n+  }\n+\n+  static InlinedObjects* get_instance() {\n+    return _instance;\n+  }\n+\n+  void init();\n+  void release();\n+\n+  void dump_inlined_field_names(AbstractDumpWriter *writer);\n+\n+  uintx get_base_index_for(Klass* k);\n+  uintx get_next_string_id(uintx id);\n+\n+  void dump_classed_with_inlined_fields(AbstractDumpWriter* writer);\n+\n+  void add_flat_array(oop array);\n+  void dump_flat_arrays(AbstractDumpWriter* writer);\n+\n+};\n+\n+InlinedObjects *InlinedObjects::_instance = nullptr;\n+\n+\n@@ -901,2 +998,2 @@\n-  \/\/ returns the size of the instance of the given class\n-  static u4 instance_size(Klass* k);\n+  \/\/ calculates the total size of the all fields of the given class.\n+  static u4 instance_size(InstanceKlass* ik);\n@@ -914,2 +1011,8 @@\n-  \/\/ dump the raw values of the instance fields of the given object\n-  static void dump_instance_fields(AbstractDumpWriter* writer, oop o);\n+  \/\/ dump the raw values of the instance fields of the given identity or inlined object;\n+  \/\/ for identity objects offset is 0 and 'klass' is o->klass(),\n+  \/\/ for inlined objects offset is the offset in the holder object, 'klass' is inlined object class\n+  static void dump_instance_fields(AbstractDumpWriter* writer, oop o, int offset, InstanceKlass* klass);\n+  \/\/ dump the raw values of the instance fields of the given inlined object;\n+  \/\/ dump_instance_fields wrapper for inlined objects\n+  static void dump_inlined_object_fields(AbstractDumpWriter* writer, oop o, int offset, InlineKlass* klass);\n+\n@@ -919,1 +1022,1 @@\n-  static void dump_instance_field_descriptors(AbstractDumpWriter* writer, Klass* k);\n+  static void dump_instance_field_descriptors(AbstractDumpWriter* writer, InstanceKlass* k, uintx *inlined_fields_index = nullptr);\n@@ -929,0 +1032,2 @@\n+  \/\/ creates HPROF_GC_PRIM_ARRAY_DUMP record for the given flat array\n+  static void dump_flat_array(AbstractDumpWriter* writer, flatArrayOop array);\n@@ -936,0 +1041,3 @@\n+  \/\/ extended version to dump flat arrays as primitive arrays;\n+  \/\/ type_size specifies size of the inlined objects.\n+  static int calculate_array_max_length(AbstractDumpWriter* writer, arrayOop array, int type_size, short header_size);\n@@ -949,0 +1057,10 @@\n+\n+  \/\/ helper methods for inlined fields.\n+  static bool is_inlined_field(const FieldStream& fld) {\n+    return fld.field_descriptor().is_inlined();\n+  }\n+  static InlineKlass* get_inlined_field_klass(const FieldStream &fld) {\n+    assert(is_inlined_field(fld), \"must be inlined field\");\n+    InstanceKlass* holder_klass = fld.field_descriptor().field_holder();\n+    return InlineKlass::cast(holder_klass->get_inline_type_field_klass(fld.index()));\n+  }\n@@ -962,0 +1080,1 @@\n+    case JVM_SIGNATURE_PRIMITIVE_OBJECT: return HPROF_NORMAL_OBJECT; \/\/ not inlined Q-object, i.e. identity object.\n@@ -992,0 +1111,1 @@\n+    case JVM_SIGNATURE_PRIMITIVE_OBJECT:\n@@ -1030,0 +1150,1 @@\n+\n@@ -1034,0 +1155,1 @@\n+    case JVM_SIGNATURE_PRIMITIVE_OBJECT: \/\/ not inlined Q-object, i.e. identity object.\n@@ -1094,3 +1216,2 @@\n-\/\/ returns the size of the instance of the given class\n-u4 DumperSupport::instance_size(Klass* k) {\n-  InstanceKlass* ik = InstanceKlass::cast(k);\n+\/\/ calculates the total size of the all fields of the given class.\n+u4 DumperSupport::instance_size(InstanceKlass *ik) {\n@@ -1101,1 +1222,5 @@\n-      size += sig2size(fld.signature());\n+      if (is_inlined_field(fld)) {\n+        size += instance_size(get_inlined_field_klass(fld));\n+      } else {\n+        size += sig2size(fld.signature());\n+      }\n@@ -1113,0 +1238,2 @@\n+      assert(!is_inlined_field(fldc), \"static fields cannot be inlined\");\n+\n@@ -1147,0 +1274,2 @@\n+      assert(!is_inlined_field(fld), \"static fields cannot be inlined\");\n+\n@@ -1175,5 +1304,5 @@\n-\/\/ dump the raw values of the instance fields of the given object\n-void DumperSupport::dump_instance_fields(AbstractDumpWriter* writer, oop o) {\n-  InstanceKlass* ik = InstanceKlass::cast(o->klass());\n-\n-  for (FieldStream fld(ik, false, false); !fld.eos(); fld.next()) {\n+\/\/ dump the raw values of the instance fields of the given identity or inlined object;\n+\/\/ for identity objects offset is 0 and 'klass' is o->klass(),\n+\/\/ for inlined objects offset is the offset in the holder object, 'klass' is inlined object class.\n+void DumperSupport::dump_instance_fields(AbstractDumpWriter* writer, oop o, int offset, InstanceKlass *klass) {\n+  for (FieldStream fld(klass, false, false); !fld.eos(); fld.next()) {\n@@ -1181,2 +1310,9 @@\n-      Symbol* sig = fld.signature();\n-      dump_field_value(writer, sig->char_at(0), o, fld.offset());\n+      if (is_inlined_field(fld)) {\n+        InlineKlass* field_klass = get_inlined_field_klass(fld);\n+        \/\/ the field is inlined, so all its fields are stored without headers.\n+        int fields_offset = offset + fld.offset() - field_klass->first_field_offset();\n+        dump_inlined_object_fields(writer, o, offset + fld.offset(), field_klass);\n+      } else {\n+        Symbol* sig = fld.signature();\n+        dump_field_value(writer, sig->char_at(0), o, offset + fld.offset());\n+      }\n@@ -1187,1 +1323,6 @@\n-\/\/ dumps the definition of the instance fields for a given class\n+void DumperSupport::dump_inlined_object_fields(AbstractDumpWriter* writer, oop o, int offset, InlineKlass* klass) {\n+  \/\/ the object is inlined, so all its fields are stored without headers.\n+  dump_instance_fields(writer, o, offset - klass->first_field_offset(), klass);\n+}\n+\n+\/\/ gets the count of the instance fields for a given class\n@@ -1192,1 +1333,8 @@\n-    if (!fldc.access_flags().is_static()) field_count++;\n+    if (!fldc.access_flags().is_static()) {\n+      if (is_inlined_field(fldc)) {\n+        \/\/ add \"synthetic\" fields for inlined fields.\n+        field_count += get_instance_fields_count(get_inlined_field_klass(fldc));\n+      } else {\n+        field_count++;\n+      }\n+    }\n@@ -1199,2 +1347,8 @@\n-void DumperSupport::dump_instance_field_descriptors(AbstractDumpWriter* writer, Klass* k) {\n-  InstanceKlass* ik = InstanceKlass::cast(k);\n+\/\/ inlined_fields_id is not-NULL for inlined fields (to get synthetic field name IDs\n+\/\/ by using InlinedObjects::get_next_string_id()).\n+void DumperSupport::dump_instance_field_descriptors(AbstractDumpWriter* writer, InstanceKlass* ik, uintx* inlined_fields_id) {\n+  \/\/ inlined_fields_id != NULL means ik is a class of inlined field.\n+  \/\/ Inlined field id pointer for this class; lazyly initialized\n+  \/\/ if the class has inlined field(s) and the caller didn't provide inlined_fields_id.\n+  uintx *this_klass_inlined_fields_id = inlined_fields_id;\n+  uintx inlined_id = 0;\n@@ -1205,1 +1359,23 @@\n-      Symbol* sig = fld.signature();\n+      if (is_inlined_field(fld)) {\n+        \/\/ dump \"synthetic\" fields for inlined fields.\n+        if (this_klass_inlined_fields_id == nullptr) {\n+          inlined_id = InlinedObjects::get_instance()->get_base_index_for(ik);\n+          this_klass_inlined_fields_id = &inlined_id;\n+        }\n+        dump_instance_field_descriptors(writer, get_inlined_field_klass(fld), this_klass_inlined_fields_id);\n+      } else {\n+        Symbol* sig = fld.signature();\n+        Symbol* name = nullptr;\n+        \/\/ Use inlined_fields_id provided by caller.\n+        if (inlined_fields_id != nullptr) {\n+          uintx name_id = InlinedObjects::get_instance()->get_next_string_id(*inlined_fields_id);\n+\n+          \/\/ name_id == 0 is returned on error. use original field signature.\n+          if (name_id != 0) {\n+            *inlined_fields_id = name_id;\n+            name = reinterpret_cast<Symbol*>(name_id);\n+          }\n+        }\n+        if (name == nullptr) {\n+          name = fld.name();\n+        }\n@@ -1207,2 +1383,3 @@\n-      writer->write_symbolID(fld.name());   \/\/ name\n-      writer->write_u1(sig2tag(sig));       \/\/ type\n+        writer->write_symbolID(name);         \/\/ name\n+        writer->write_u1(sig2tag(sig));       \/\/ type\n+      }\n@@ -1230,1 +1407,1 @@\n-  dump_instance_fields(writer, o);\n+  dump_instance_fields(writer, o, 0, ik);\n@@ -1275,1 +1452,1 @@\n-  writer->write_u4(DumperSupport::instance_size(ik));\n+  writer->write_u4(HeapWordSize * ik->size_helper());\n@@ -1329,4 +1506,1 @@\n-int DumperSupport::calculate_array_max_length(AbstractDumpWriter* writer, arrayOop array, short header_size) {\n-  BasicType type = ArrayKlass::cast(array->klass())->element_type();\n-  assert(type >= T_BOOLEAN && type <= T_OBJECT, \"invalid array element type\");\n-\n+int DumperSupport::calculate_array_max_length(AbstractDumpWriter* writer, arrayOop array, int type_size, short header_size) {\n@@ -1335,7 +1509,0 @@\n-  int type_size;\n-  if (type == T_OBJECT) {\n-    type_size = sizeof(address);\n-  } else {\n-    type_size = type2aelembytes(type);\n-  }\n-\n@@ -1349,0 +1516,1 @@\n+    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n@@ -1355,0 +1523,13 @@\n+int DumperSupport::calculate_array_max_length(AbstractDumpWriter* writer, arrayOop array, short header_size) {\n+  BasicType type = ArrayKlass::cast(array->klass())->element_type();\n+  assert((type >= T_BOOLEAN && type <= T_OBJECT) || type == T_PRIMITIVE_OBJECT, \"invalid array element type\");\n+  int type_size;\n+  if (type == T_OBJECT || type == T_PRIMITIVE_OBJECT) {  \/\/ TODO: FIXME\n+    type_size = sizeof(address);\n+  } else {\n+    type_size = type2aelembytes(type);\n+  }\n+\n+  return calculate_array_max_length(writer, array, type_size, header_size);\n+}\n+\n@@ -1386,0 +1567,40 @@\n+\/\/ creates HPROF_GC_PRIM_ARRAY_DUMP record for the given flat array\n+void DumperSupport::dump_flat_array(AbstractDumpWriter* writer, flatArrayOop array) {\n+  FlatArrayKlass* array_klass = FlatArrayKlass::cast(array->klass());\n+  InlineKlass* element_klass = array_klass->element_klass();\n+  int element_size = instance_size(element_klass);\n+  \/*                          id         array object ID\n+   *                          u4         stack trace serial number\n+   *                          u4         number of elements\n+   *                          u1         element type\n+   *\/\n+  short header_size = 1 + sizeof(address) + 2 * 4 + 1;\n+\n+  \/\/ TODO: use T_SHORT\/T_INT\/T_LONG if needed to avoid truncation\n+  BasicType type = T_BYTE;\n+  int type_size = type2aelembytes(type);\n+  int length = calculate_array_max_length(writer, array, element_size, header_size);\n+  u4 length_in_bytes = (u4)(length * element_size);\n+  u4 size = header_size + length_in_bytes;\n+\n+  writer->start_sub_record(HPROF_GC_PRIM_ARRAY_DUMP, size);\n+  writer->write_objectID(array);\n+  writer->write_u4(STACK_TRACE_ID);\n+  \/\/ TODO: round up array length for T_SHORT\/T_INT\/T_LONG\n+  writer->write_u4(length * element_size);\n+  writer->write_u1(type2tag(type));\n+\n+  for (int index = 0; index < length; index++) {\n+    \/\/ need offset in the holder to read inlined object. calculate it from flatArrayOop::value_at_addr()\n+    int offset = (int)((address)array->value_at_addr(index, array_klass->layout_helper())\n+                  - cast_from_oop<address>(array));\n+    dump_inlined_object_fields(writer, array, offset, element_klass);\n+  }\n+\n+  \/\/ TODO: write padding bytes for T_SHORT\/T_INT\/T_LONG\n+\n+  InlinedObjects::get_instance()->add_flat_array(array);\n+\n+  writer->end_sub_record();\n+}\n+\n@@ -1507,0 +1728,264 @@\n+class InlinedFieldNameDumper : public LockedClassesDo {\n+public:\n+  typedef void (*Callback)(InlinedObjects *owner, const Klass *klass, uintx base_index, int count);\n+\n+private:\n+  AbstractDumpWriter* _writer;\n+  InlinedObjects *_owner;\n+  Callback       _callback;\n+  uintx _index;\n+\n+  void dump_inlined_field_names(GrowableArray<Symbol*>* super_names, Symbol* field_name, InlineKlass* klass) {\n+    super_names->push(field_name);\n+    for (FieldStream fld(klass, false, false); !fld.eos(); fld.next()) {\n+      if (!fld.access_flags().is_static()) {\n+        if (DumperSupport::is_inlined_field(fld)) {\n+          dump_inlined_field_names(super_names, fld.name(), DumperSupport::get_inlined_field_klass(fld));\n+        } else {\n+          \/\/ get next string ID.\n+          uintx next_index = _owner->get_next_string_id(_index);\n+          if (next_index == 0) {\n+            \/\/ something went wrong (overflow?)\n+            \/\/ stop generation; the rest of inlined objects will have original field names.\n+            return;\n+          }\n+          _index = next_index;\n+\n+          \/\/ Calculate length.\n+          int len = fld.name()->utf8_length();\n+          for (GrowableArrayIterator<Symbol*> it = super_names->begin(); it != super_names->end(); ++it) {\n+            len += (*it)->utf8_length() + 1;    \/\/ +1 for \".\".\n+          }\n+\n+          DumperSupport::write_header(_writer, HPROF_UTF8, oopSize + len);\n+          _writer->write_symbolID(reinterpret_cast<Symbol*>(_index));\n+          \/\/ Write the string value.\n+          \/\/ 1) super_names.\n+          for (GrowableArrayIterator<Symbol*> it = super_names->begin(); it != super_names->end(); ++it) {\n+            _writer->write_raw((*it)->bytes(), (*it)->utf8_length());\n+            _writer->write_u1('.');\n+          }\n+          \/\/ 2) field name.\n+          _writer->write_raw(fld.name()->bytes(), fld.name()->utf8_length());\n+        }\n+      }\n+    }\n+    super_names->pop();\n+  }\n+\n+  void dump_inlined_field_names(Symbol* field_name, InlineKlass* field_klass) {\n+    GrowableArray<Symbol*> super_names(4, mtServiceability);\n+    dump_inlined_field_names(&super_names, field_name, field_klass);\n+  }\n+\n+public:\n+  InlinedFieldNameDumper(AbstractDumpWriter* writer, InlinedObjects* owner, Callback callback)\n+    : _writer(writer), _owner(owner), _callback(callback), _index(0)  {\n+  }\n+\n+  void do_klass(Klass* k) {\n+    if (!k->is_instance_klass()) {\n+      return;\n+    }\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    \/\/ if (ik->has_inline_type_fields()) {\n+    \/\/   return;\n+    \/\/ }\n+\n+    uintx base_index = _index;\n+    int count = 0;\n+\n+    for (FieldStream fld(ik, false, false); !fld.eos(); fld.next()) {\n+      if (!fld.access_flags().is_static()) {\n+        if (DumperSupport::is_inlined_field(fld)) {\n+          dump_inlined_field_names(fld.name(), DumperSupport::get_inlined_field_klass(fld));\n+          count++;\n+        }\n+      }\n+    }\n+\n+    if (count != 0) {\n+      _callback(_owner, k, base_index, count);\n+    }\n+  }\n+};\n+\n+class InlinedFieldsDumper : public LockedClassesDo {\n+private:\n+  AbstractDumpWriter* _writer;\n+\n+public:\n+  InlinedFieldsDumper(AbstractDumpWriter* writer) : _writer(writer) {}\n+\n+  void do_klass(Klass* k) {\n+    if (!k->is_instance_klass()) {\n+      return;\n+    }\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    \/\/ if (ik->has_inline_type_fields()) {\n+    \/\/   return;\n+    \/\/ }\n+\n+    \/\/ We can be at a point where java mirror does not exist yet.\n+    \/\/ So we need to check that the class is at least loaded, to avoid crash from a null mirror.\n+    if (!ik->is_loaded()) {\n+      return;\n+    }\n+\n+    u2 inlined_count = 0;\n+    for (FieldStream fld(ik, false, false); !fld.eos(); fld.next()) {\n+      if (!fld.access_flags().is_static()) {\n+        if (DumperSupport::is_inlined_field(fld)) {\n+          inlined_count++;\n+        }\n+      }\n+    }\n+    if (inlined_count != 0) {\n+      _writer->write_u1(HPROF_CLASS_WITH_INLINED_FIELDS);\n+\n+      \/\/ class ID\n+      _writer->write_classID(ik);\n+      \/\/ number of inlined fields\n+      _writer->write_u2(inlined_count);\n+      u2 index = 0;\n+      for (FieldStream fld(ik, false, false); !fld.eos(); fld.next()) {\n+        if (!fld.access_flags().is_static()) {\n+          if (DumperSupport::is_inlined_field(fld)) {\n+            \/\/ inlined field index\n+            _writer->write_u2(index);\n+            \/\/ synthetic field count\n+            u2 field_count = DumperSupport::get_instance_fields_count(DumperSupport::get_inlined_field_klass(fld));\n+            _writer->write_u2(field_count);\n+            \/\/ original field name\n+            _writer->write_symbolID(fld.name());\n+            \/\/ inlined field class ID\n+            _writer->write_classID(DumperSupport::get_inlined_field_klass(fld));\n+\n+            index += field_count;\n+          } else {\n+            index++;\n+          }\n+        }\n+      }\n+    }\n+  }\n+};\n+\n+\n+void InlinedObjects::init() {\n+  _instance = this;\n+\n+  struct Closure : public SymbolClosure {\n+    uintx _min_id = max_uintx;\n+    uintx _max_id = 0;\n+    Closure() : _min_id(max_uintx), _max_id(0) {}\n+\n+    void do_symbol(Symbol** p) {\n+      uintx val = reinterpret_cast<uintx>(*p);\n+      if (val < _min_id) {\n+        _min_id = val;\n+      }\n+      if (val > _max_id) {\n+        _max_id = val;\n+      }\n+    }\n+  } closure;\n+\n+  SymbolTable::symbols_do(&closure);\n+\n+  _min_string_id = closure._min_id;\n+  _max_string_id = closure._max_id;\n+}\n+\n+void InlinedObjects::release() {\n+  _instance = nullptr;\n+\n+  if (_inlined_field_map != nullptr) {\n+    delete _inlined_field_map;\n+    _inlined_field_map = nullptr;\n+  }\n+  if (_flat_arrays != nullptr) {\n+    delete _flat_arrays;\n+    _flat_arrays = nullptr;\n+  }\n+}\n+\n+void InlinedObjects::inlined_field_names_callback(InlinedObjects* _this, const Klass* klass, uintx base_index, int count) {\n+  if (_this->_inlined_field_map == nullptr) {\n+    _this->_inlined_field_map = new (mtServiceability) GrowableArray<ClassInlinedFields>(100, mtServiceability);\n+  }\n+  _this->_inlined_field_map->append(ClassInlinedFields(klass, base_index));\n+\n+  \/\/ counters for dumping classes with inlined fields\n+  _this->_classes_count++;\n+  _this->_inlined_fields_count += count;\n+}\n+\n+void InlinedObjects::dump_inlined_field_names(AbstractDumpWriter* writer) {\n+  InlinedFieldNameDumper nameDumper(writer, this, inlined_field_names_callback);\n+  ClassLoaderDataGraph::classes_do(&nameDumper);\n+\n+  if (_inlined_field_map != nullptr) {\n+    \/\/ prepare the map for  get_base_index_for().\n+    _inlined_field_map->sort(ClassInlinedFields::compare);\n+  }\n+}\n+\n+uintx InlinedObjects::get_base_index_for(Klass* k) {\n+  if (_inlined_field_map != nullptr) {\n+    bool found = false;\n+    int idx = _inlined_field_map->find_sorted<ClassInlinedFields, ClassInlinedFields::compare>(ClassInlinedFields(k, 0), found);\n+    if (found) {\n+        return _inlined_field_map->at(idx).base_index;\n+    }\n+  }\n+\n+  \/\/ return max_uintx, so get_next_string_id returns 0.\n+  return max_uintx;\n+}\n+\n+uintx InlinedObjects::get_next_string_id(uintx id) {\n+  if (++id == _min_string_id) {\n+    return _max_string_id + 1;\n+  }\n+  return id;\n+}\n+\n+void InlinedObjects::dump_classed_with_inlined_fields(AbstractDumpWriter* writer) {\n+  if (_classes_count != 0) {\n+    \/\/ Record for each class contains tag(u1), class ID and count(u2)\n+    \/\/ for each inlined field index(u2), synthetic fields count(u2), original field name and class ID\n+    int size = _classes_count * (1 + sizeof(address) + 2)\n+             + _inlined_fields_count * (2 + 2 + sizeof(address) + sizeof(address));\n+    DumperSupport::write_header(writer, HPROF_INLINED_FIELDS, (u4)size);\n+\n+    InlinedFieldsDumper dumper(writer);\n+    ClassLoaderDataGraph::classes_do(&dumper);\n+  }\n+}\n+\n+void InlinedObjects::add_flat_array(oop array) {\n+  if (_flat_arrays == nullptr) {\n+    _flat_arrays = new (mtServiceability) GrowableArray<oop>(100, mtServiceability);\n+  }\n+  _flat_arrays->append(array);\n+}\n+\n+void InlinedObjects::dump_flat_arrays(AbstractDumpWriter* writer) {\n+  if (_flat_arrays != nullptr) {\n+    \/\/ For each flat array the record contains tag (u1), object ID and class ID.\n+    int size = _flat_arrays->length() * (1 + sizeof(address) + sizeof(address));\n+\n+    DumperSupport::write_header(writer, HPROF_FLAT_ARRAYS, (u4)size);\n+    for (GrowableArrayIterator<oop> it = _flat_arrays->begin(); it != _flat_arrays->end(); ++it) {\n+      flatArrayOop array = flatArrayOop(*it);\n+      FlatArrayKlass* array_klass = FlatArrayKlass::cast(array->klass());\n+      InlineKlass* element_klass = array_klass->element_klass();\n+      writer->write_u1(HPROF_FLAT_ARRAY);\n+      writer->write_objectID(array);\n+      writer->write_classID(element_klass);\n+    }\n+  }\n+}\n+\n+\n@@ -1729,0 +2214,2 @@\n+  } else if (o->is_flatArray()) {\n+    DumperSupport::dump_flat_array(writer(), flatArrayOop(o));\n@@ -1747,0 +2234,8 @@\n+  } else if (o->is_typeArray()) {\n+    flatArrayOop array = flatArrayOop(o);\n+    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n+    assert(type == T_PRIMITIVE_OBJECT, \"invalid array element type\");\n+    int length = array->length();\n+    \/\/TODO: FIXME\n+    \/\/int type_size = type2aelembytes(type);\n+    \/\/size = (size_t)length * type_size;\n@@ -1819,0 +2314,5 @@\n+\n+  \/\/ Inlined object support.\n+  InlinedObjects          _inlined_objects;\n+  InlinedObjects* inlined_objects() { return &_inlined_objects; }\n+\n@@ -2231,0 +2731,7 @@\n+    \/\/ HPROF_UTF8 records for inlined field names.\n+    inlined_objects()->init();\n+    inlined_objects()->dump_inlined_field_names(writer());\n+\n+    \/\/ HPROF_INLINED_FIELDS\n+    inlined_objects()->dump_classed_with_inlined_fields(writer());\n+\n@@ -2309,0 +2816,3 @@\n+\n+  inlined_objects()->dump_flat_arrays(writer());\n+\n@@ -2311,0 +2821,2 @@\n+\n+  inlined_objects()->release();\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":548,"deletions":36,"binary":false,"changes":584,"status":"modified"},{"patch":"@@ -255,0 +255,2 @@\n+#define THREAD_AND_LOCATION_DECL                 TRAPS, const char* file, int line\n+#define THREAD_AND_LOCATION_ARGS                 THREAD, file, line\n","filename":"src\/hotspot\/share\/utilities\/exceptions.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -121,1 +121,1 @@\n-  assert(num_type_chars == 11, \"must have tested the right number of mappings\");\n+  assert(num_type_chars == 12, \"must have tested the right number of mappings\");\n@@ -139,0 +139,1 @@\n+      case T_PRIMITIVE_OBJECT:\n@@ -204,0 +205,1 @@\n+  _type2aelembytes[T_PRIMITIVE_OBJECT]  = heapOopSize;\n@@ -215,2 +217,2 @@\n-  JVM_SIGNATURE_VOID,    0,\n-  0, 0, 0, 0\n+  JVM_SIGNATURE_PRIMITIVE_OBJECT, JVM_SIGNATURE_VOID,\n+  0, 0, 0, 0, 0\n@@ -232,0 +234,1 @@\n+  \"inline_type\",\n@@ -262,1 +265,1 @@\n-int type2size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, -1};\n+int type2size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, -1};\n@@ -279,6 +282,7 @@\n-  T_VOID,                  \/\/ T_VOID     = 14,\n-  T_ADDRESS,               \/\/ T_ADDRESS  = 15,\n-  T_NARROWOOP,             \/\/ T_NARROWOOP= 16,\n-  T_METADATA,              \/\/ T_METADATA = 17,\n-  T_NARROWKLASS,           \/\/ T_NARROWKLASS = 18,\n-  T_CONFLICT               \/\/ T_CONFLICT = 19,\n+  T_PRIMITIVE_OBJECT,      \/\/ T_PRIMITIVE_OBJECT = 14,\n+  T_VOID,                  \/\/ T_VOID     = 15,\n+  T_ADDRESS,               \/\/ T_ADDRESS  = 16,\n+  T_NARROWOOP,             \/\/ T_NARROWOOP= 17,\n+  T_METADATA,              \/\/ T_METADATA = 18,\n+  T_NARROWKLASS,           \/\/ T_NARROWKLASS = 19,\n+  T_CONFLICT               \/\/ T_CONFLICT = 20\n@@ -303,6 +307,7 @@\n-  T_VOID,    \/\/ T_VOID     = 14,\n-  T_ADDRESS, \/\/ T_ADDRESS  = 15,\n-  T_NARROWOOP, \/\/ T_NARROWOOP  = 16,\n-  T_METADATA,  \/\/ T_METADATA   = 17,\n-  T_NARROWKLASS, \/\/ T_NARROWKLASS  = 18,\n-  T_CONFLICT \/\/ T_CONFLICT = 19,\n+  T_OBJECT,  \/\/ T_PRIMITIVE_OBJECT = 14,\n+  T_VOID,    \/\/ T_VOID     = 15,\n+  T_ADDRESS, \/\/ T_ADDRESS  = 16,\n+  T_NARROWOOP, \/\/ T_NARROWOOP  = 17,\n+  T_METADATA,  \/\/ T_METADATA   = 18,\n+  T_NARROWKLASS, \/\/ T_NARROWKLASS  = 19,\n+  T_CONFLICT \/\/ T_CONFLICT = 20\n@@ -327,6 +332,7 @@\n-  0,                         \/\/ T_VOID     = 14,\n-  T_OBJECT_aelem_bytes,      \/\/ T_ADDRESS  = 15,\n-  T_NARROWOOP_aelem_bytes,   \/\/ T_NARROWOOP= 16,\n-  T_OBJECT_aelem_bytes,      \/\/ T_METADATA = 17,\n-  T_NARROWKLASS_aelem_bytes, \/\/ T_NARROWKLASS= 18,\n-  0                          \/\/ T_CONFLICT = 19,\n+  T_PRIMITIVE_OBJECT_aelem_bytes, \/\/ T_PRIMITIVE_OBJECT = 14,\n+  0,                         \/\/ T_VOID     = 15,\n+  T_OBJECT_aelem_bytes,      \/\/ T_ADDRESS  = 16,\n+  T_NARROWOOP_aelem_bytes,   \/\/ T_NARROWOOP= 17,\n+  T_OBJECT_aelem_bytes,      \/\/ T_METADATA = 18,\n+  T_NARROWKLASS_aelem_bytes, \/\/ T_NARROWKLASS= 19,\n+  0                          \/\/ T_CONFLICT = 20\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.cpp","additions":28,"deletions":22,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -651,0 +651,9 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Prototyping\n+\/\/ \"Code Missing Here\" macro, un-define when integrating back from prototyping stage and break\n+\/\/ compilation on purpose (i.e. \"forget me not\")\n+#define PROTOTYPE\n+#ifdef PROTOTYPE\n+#define CMH(m)\n+#endif\n+\n@@ -736,6 +745,7 @@\n-  T_VOID        = 14,\n-  T_ADDRESS     = 15,\n-  T_NARROWOOP   = 16,\n-  T_METADATA    = 17,\n-  T_NARROWKLASS = 18,\n-  T_CONFLICT    = 19, \/\/ for stack value type with conflicting contents\n+  T_PRIMITIVE_OBJECT = 14,\n+  T_VOID        = 15,\n+  T_ADDRESS     = 16,\n+  T_NARROWOOP   = 17,\n+  T_METADATA    = 18,\n+  T_NARROWKLASS = 19,\n+  T_CONFLICT    = 20, \/\/ for stack value type with conflicting contents\n@@ -756,0 +766,1 @@\n+    F(JVM_SIGNATURE_PRIMITIVE_OBJECT, T_PRIMITIVE_OBJECT, N) \\\n@@ -785,1 +796,1 @@\n-  return (t == T_OBJECT || t == T_ARRAY || (include_narrow_oop && t == T_NARROWOOP));\n+  return (t == T_OBJECT || t == T_ARRAY || t == T_PRIMITIVE_OBJECT || (include_narrow_oop && t == T_NARROWOOP));\n@@ -843,1 +854,2 @@\n-  T_VOID_size        = 0\n+  T_VOID_size        = 0,\n+  T_PRIMITIVE_OBJECT_size = 1\n@@ -873,0 +885,1 @@\n+  T_PRIMITIVE_OBJECT_aelem_bytes = 8,\n@@ -876,0 +889,1 @@\n+  T_PRIMITIVE_OBJECT_aelem_bytes = 4,\n@@ -968,1 +982,1 @@\n-  vtos = 9,             \/\/ tos not cached\n+  vtos = 9,             \/\/ tos not cached,\n@@ -985,1 +999,2 @@\n-    case T_ARRAY  : \/\/ fall through\n+    case T_PRIMITIVE_OBJECT: \/\/ fall through\n+    case T_ARRAY  :   \/\/ fall through\n@@ -1296,0 +1311,6 @@\n+\/\/ TEMP!!!!\n+\/\/ This should be removed after LW2 arrays are implemented (JDK-8220790).\n+\/\/ It's an alias to (EnableValhalla && (FlatArrayElementMaxSize != 0)),\n+\/\/ which is actually not 100% correct, but works for the current set of C1\/C2\n+\/\/ implementation and test cases.\n+#define UseFlatArray (EnablePrimitiveClasses && (FlatArrayElementMaxSize != 0))\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":31,"deletions":10,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"oops\/array.hpp\"\n+#include \"oops\/oop.hpp\"\n@@ -445,0 +447,6 @@\n+  void appendAll(const Array<E>* l) {\n+    for (int i = 0; i < l->length(); i++) {\n+      this->at_put_grow(this->_len, l->at(i), E());\n+    }\n+  }\n+\n@@ -856,2 +864,2 @@\n-  GrowableArrayFilterIterator(const GrowableArrayIterator<E>& begin, UnaryPredicate filter_predicate) :\n-      _array(begin._array), _position(begin._position), _predicate(filter_predicate) {\n+  GrowableArrayFilterIterator(const GrowableArray<E>* array, UnaryPredicate filter_predicate) :\n+      _array(array), _position(0), _predicate(filter_predicate) {\n@@ -859,1 +867,1 @@\n-    while(_position != _array->length() && !_predicate(_array->at(_position))) {\n+    while(!at_end() && !_predicate(_array->at(_position))) {\n@@ -868,1 +876,1 @@\n-    } while(_position != _array->length() && !_predicate(_array->at(_position)));\n+    } while(!at_end() && !_predicate(_array->at(_position)));\n@@ -893,0 +901,4 @@\n+\n+  bool at_end() const {\n+    return _array == NULL || _position == _array->end()._position;\n+  }\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"utilities\/ostream.hpp\"\n@@ -73,0 +74,215 @@\n+class StringMatcher {\n+ public:\n+  typedef int getc_function_t(const char* &source, const char* limit);\n+\n+ private:\n+  \/\/ These do not get properly inlined.\n+  \/\/ For full performance, this should be a template class\n+  \/\/ parameterized by two function arguments.\n+  getc_function_t* _pattern_getc;\n+  getc_function_t* _string_getc;\n+\n+ public:\n+  StringMatcher(getc_function_t pattern_getc,\n+                getc_function_t string_getc)\n+    : _pattern_getc(pattern_getc),\n+      _string_getc(string_getc)\n+  { }\n+\n+  enum {  \/\/ special results from _pattern_getc\n+    string_match_comma  = -0x100 + ',',\n+    string_match_star   = -0x100 + '*',\n+    string_match_eos    = -0x100 + '\\0'\n+  };\n+\n+ private:\n+  const char*\n+  skip_anchor_word(const char* match,\n+                   const char* match_end,\n+                   int anchor_length,\n+                   const char* pattern,\n+                   const char* pattern_end) {\n+    assert(pattern < pattern_end && anchor_length > 0, \"\");\n+    const char* begp = pattern;\n+    int ch1 = _pattern_getc(begp, pattern_end);\n+    \/\/ note that begp is now advanced over ch1\n+    assert(ch1 > 0, \"regular char only\");\n+    const char* matchp = match;\n+    const char* limitp = match_end - anchor_length;\n+    while (matchp <= limitp) {\n+      int mch = _string_getc(matchp, match_end);\n+      if (mch == ch1) {\n+        const char* patp = begp;\n+        const char* anchorp = matchp;\n+        while (patp < pattern_end) {\n+          char ch = _pattern_getc(patp, pattern_end);\n+          char mch = _string_getc(anchorp, match_end);\n+          if (mch != ch) {\n+            anchorp = NULL;\n+            break;\n+          }\n+        }\n+        if (anchorp != NULL) {\n+          return anchorp;  \/\/ Found a full copy of the anchor.\n+        }\n+        \/\/ That did not work, so restart the search for ch1.\n+      }\n+    }\n+    return NULL;\n+  }\n+\n+ public:\n+  bool string_match(const char* pattern,\n+                    const char* string) {\n+    return string_match(pattern, pattern + strlen(pattern),\n+                        string, string + strlen(string));\n+  }\n+  bool string_match(const char* pattern, const char* pattern_end,\n+                    const char* string, const char* string_end) {\n+    const char* patp = pattern;\n+    switch (_pattern_getc(patp, pattern_end)) {\n+    case string_match_eos:\n+      return false;  \/\/ Empty pattern is always false.\n+    case string_match_star:\n+      if (patp == pattern_end) {\n+        return true;   \/\/ Lone star pattern is always true.\n+      }\n+      break;\n+    }\n+    patp = pattern;  \/\/ Reset after lookahead.\n+    const char* matchp = string;  \/\/ NULL if failing\n+    for (;;) {\n+      int ch = _pattern_getc(patp, pattern_end);\n+      switch (ch) {\n+      case string_match_eos:\n+      case string_match_comma:\n+        \/\/ End of a list item; see if it's a match.\n+        if (matchp == string_end) {\n+          return true;\n+        }\n+        if (ch == string_match_comma) {\n+          \/\/ Get ready to match the next item.\n+          matchp = string;\n+          continue;\n+        }\n+        return false;  \/\/ End of all items.\n+\n+      case string_match_star:\n+        if (matchp != NULL) {\n+          \/\/ Wildcard:  Parse out following anchor word and look for it.\n+          const char* begp = patp;\n+          const char* endp = patp;\n+          int anchor_len = 0;\n+          for (;;) {\n+            \/\/ get as many following regular characters as possible\n+            endp = patp;\n+            ch = _pattern_getc(patp, pattern_end);\n+            if (ch <= 0) {\n+              break;\n+            }\n+            anchor_len += 1;\n+          }\n+          \/\/ Anchor word [begp..endp) does not contain ch, so back up.\n+          \/\/ Now do an eager match to the anchor word, and commit to it.\n+          patp = endp;\n+          if (ch == string_match_eos ||\n+              ch == string_match_comma) {\n+            \/\/ Anchor word is at end of pattern, so treat it as a fixed pattern.\n+            const char* limitp = string_end - anchor_len;\n+            matchp = limitp;\n+            patp = begp;\n+            \/\/ Resume normal scanning at the only possible match position.\n+            continue;\n+          }\n+          \/\/ Find a floating occurrence of the anchor and continue matching.\n+          \/\/ Note:  This is greedy; there is no backtrack here.  Good enough.\n+          matchp = skip_anchor_word(matchp, string_end, anchor_len, begp, endp);\n+        }\n+        continue;\n+      }\n+      \/\/ Normal character.\n+      if (matchp != NULL) {\n+        int mch = _string_getc(matchp, string_end);\n+        if (mch != ch) {\n+          matchp = NULL;\n+        }\n+      }\n+    }\n+  }\n+};\n+\n+\/\/ Match a wildcarded class list to a proposed class name (in internal form).\n+\/\/ Commas or newlines separate multiple possible matches; stars are shell-style wildcards.\n+class ClassListMatcher : public StringMatcher {\n+ public:\n+  ClassListMatcher()\n+    : StringMatcher(pattern_list_getc, class_name_getc)\n+  { }\n+\n+ private:\n+  static int pattern_list_getc(const char* &pattern_ptr,\n+                               const char* pattern_end) {\n+    if (pattern_ptr == pattern_end) {\n+      return string_match_eos;\n+    }\n+    int ch = (unsigned char) *pattern_ptr++;\n+    switch (ch) {\n+    case ' ': case '\\t': case '\\n': case '\\r':\n+    case ',':\n+      \/\/ End of list item.\n+      for (;;) {\n+        switch (*pattern_ptr) {\n+        case ' ': case '\\t': case '\\n': case '\\r':\n+        case ',':\n+          pattern_ptr += 1;  \/\/ Collapse multiple commas or spaces.\n+          continue;\n+        }\n+        break;\n+      }\n+      return string_match_comma;\n+\n+    case '*':\n+      \/\/ Wildcard, matching any number of chars.\n+      while (*pattern_ptr == '*') {\n+        pattern_ptr += 1;  \/\/ Collapse multiple stars.\n+      }\n+      return string_match_star;\n+\n+    case '.':\n+      ch = '\/';   \/\/ Look for internal form of package separator\n+      break;\n+\n+    case '\\\\':\n+      \/\/ Superquote in pattern escapes * , whitespace, and itself.\n+      if (pattern_ptr < pattern_end) {\n+        ch = (unsigned char) *pattern_ptr++;\n+      }\n+      break;\n+    }\n+\n+    assert(ch > 0, \"regular char only\");\n+    return ch;\n+  }\n+\n+  static int class_name_getc(const char* &name_ptr,\n+                             const char* name_end) {\n+    if (name_ptr == name_end) {\n+      return string_match_eos;\n+    }\n+    int ch = (unsigned char) *name_ptr++;\n+    if (ch == '.') {\n+      ch = '\/';   \/\/ Normalize to internal form of package separator\n+    }\n+    return ch;  \/\/ plain character\n+  }\n+};\n+\n+bool StringUtils::class_list_match(const char* class_pattern_list,\n+                                   const char* class_name) {\n+  if (class_pattern_list == NULL || class_name == NULL || class_name[0] == '\\0')\n+    return false;\n+  ClassListMatcher clm;\n+  return clm.string_match(class_pattern_list, class_name);\n+}\n+\n+\n","filename":"src\/hotspot\/share\/utilities\/stringUtils.cpp","additions":216,"deletions":0,"binary":false,"changes":216,"status":"modified"},{"patch":"@@ -220,0 +220,2 @@\n+ * Value objects cannot be `java.io.Externalizable` because value objects are\n+ * immutable and `Externalizable.readExternal` is unable to modify the fields of the value.\n@@ -245,0 +247,5 @@\n+ * <p>Value objects are deserialized differently than ordinary serializable objects or records.\n+ * See <a href=\"{@docRoot}\/..\/specs\/serialization\/serial-arch.html#serialization-of-value-objects\">\n+ * <cite>Java Object Serialization Specification,<\/cite> Section 1.14,\n+ * \"Serialization of Value Objects\"<\/a> for additional information.\n+ *\n@@ -2266,1 +2273,2 @@\n-        passHandle = handles.assign(unshared ? unsharedMarker : obj);\n+        \/\/ Assign the handle and initially set to null or the unsharedMarker\n+        passHandle = handles.assign(unshared ? unsharedMarker : null);\n@@ -2279,0 +2287,6 @@\n+            if (desc.isValue()) {\n+                throw new NotSerializableException(\"Externalizable not valid for value class \"\n+                        + cl.getName());\n+            }\n+            if (!unshared)\n+                handles.setObject(passHandle, obj);\n@@ -2280,0 +2294,7 @@\n+        } else if (desc.isValue()) {\n+            \/\/ For value objects, read the fields and finish the buffer before publishing the ref\n+            assert obj != null : \"obj == null: \" + desc;\n+            readSerialData(obj, desc);\n+            obj = desc.finishValue(obj);\n+            if (!unshared)\n+                handles.setObject(passHandle, obj);\n@@ -2281,0 +2302,3 @@\n+            \/\/ For all other objects, publish the ref and then read the data\n+            if (!unshared)\n+                handles.setObject(passHandle, obj);\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectInputStream.java","additions":25,"deletions":1,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -130,0 +130,6 @@\n+ * Value classes implementing {@link Externalizable} cannot be serialized\n+ * or deserialized, the value object is immutable and the state cannot be restored.\n+ * Use {@link Serializable} {@code writeReplace} to delegate to another serializable\n+ * object such as a record.\n+ *\n+ * Value objects cannot be {@code java.io.Externalizable}.\n@@ -1447,0 +1453,3 @@\n+                if (desc.forClass().isValue())\n+                    throw new NotSerializableException(\"Externalizable not valid for value class \"\n+                            + desc.forClass().getName());\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectOutputStream.java","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+import java.lang.reflect.InaccessibleObjectException;\n@@ -74,1 +75,1 @@\n- *    <cite>Java Object Serialization Specification,<\/cite> Section 4.6, \"Stream Unique Identifiers\"<\/a>.\n+ *    <cite>Java Object Serialization Specification<\/cite>, Section 4.6, \"Stream Unique Identifiers\"<\/a>.\n@@ -137,0 +138,2 @@\n+    \/** true if represents a value class *\/\n+    private boolean isValue;\n@@ -376,0 +379,1 @@\n+        isValue = cl.isValue();\n@@ -409,0 +413,3 @@\n+                    } else if (isValue) {\n+                        \/\/ Value objects are created using Unsafe.\n+                        cons = null;\n@@ -446,1 +453,1 @@\n-            } else if (cons == null && !isRecord) {\n+            } else if (cons == null && !(isRecord | isValue)) {\n@@ -640,0 +647,1 @@\n+            isValue = localDesc.isValue;\n@@ -921,0 +929,8 @@\n+    \/**\n+     * {@return {code true} if the class is a value class, {@code false} otherwise}\n+     *\/\n+    boolean isValue() {\n+        requireInitialized();\n+        return isValue;\n+    }\n+\n@@ -943,1 +959,1 @@\n-     * externalizable and defines a public no-arg constructor, or if it is\n+     * externalizable and defines a public no-arg constructor, if it is\n@@ -945,1 +961,2 @@\n-     * accessible no-arg constructor.  Otherwise, returns false.\n+     * accessible no-arg constructor, or if the class is a value class.\n+     * Otherwise, returns false.\n@@ -949,1 +966,1 @@\n-        return (cons != null);\n+        return (cons != null | isValue);\n@@ -1054,1 +1071,4 @@\n-        } else {\n+        } else if (isValue) {\n+            \/\/ Start with a buffered default value.\n+            return FieldReflector.newValueInstance(cl);\n+        }  else {\n@@ -1059,0 +1079,9 @@\n+    \/**\n+     * Finish the initialization of a value object.\n+     * @param obj an object (larval if a value object)\n+     * @return the finished object\n+     *\/\n+    Object finishValue(Object obj) {\n+        return (isValue) ? FieldReflector.finishValueInstance(obj) : obj;\n+    }\n+\n@@ -1433,1 +1462,1 @@\n-        } catch (NoSuchMethodException ex) {\n+        } catch (NoSuchMethodException | InaccessibleObjectException ex) {\n@@ -1926,0 +1955,22 @@\n+        \/**\n+         * Return a new instance of the class using Unsafe.uninitializedDefaultValue\n+         * and buffer it.\n+         * @param clazz The value class\n+         * @return a buffered default value\n+         *\/\n+        static Object newValueInstance(Class<?> clazz) throws InstantiationException{\n+            assert clazz.isValue() : \"Should be a value class\";\n+            Object obj = UNSAFE.uninitializedDefaultValue(clazz);\n+            return UNSAFE.makePrivateBuffer(obj);\n+        }\n+\n+        \/**\n+         * Finish a value object, clear the larval state and returning the value object.\n+         * @param obj a buffered value object in a larval state\n+         * @return the finished value object\n+         *\/\n+        static Object finishValueInstance(Object obj) {\n+            assert (obj.getClass().isValue()) : \"Should be a value class\";\n+            return UNSAFE.finishPrivateBuffer(obj);\n+        }\n+\n@@ -2046,0 +2097,1 @@\n+                Field f = fields[i].getField();\n@@ -2047,1 +2099,4 @@\n-                    case 'L', '[' -> UNSAFE.getReference(obj, readKeys[i]);\n+                    case 'L', '[' ->\n+                            UNSAFE.isFlattened(f)\n+                                    ? UNSAFE.getValue(obj, readKeys[i], f.getType())\n+                                    : UNSAFE.getReference(obj, readKeys[i]);\n@@ -2084,0 +2139,1 @@\n+                        Field f = fields[i].getField();\n@@ -2088,1 +2144,0 @@\n-                            Field f = fields[i].getField();\n@@ -2097,2 +2152,7 @@\n-                        if (!dryRun)\n-                            UNSAFE.putReference(obj, key, val);\n+                        if (!dryRun) {\n+                            if (UNSAFE.isFlattened(f)) {\n+                                UNSAFE.putValue(obj, key, f.getType(), val);\n+                            } else {\n+                                UNSAFE.putReference(obj, key, val);\n+                            }\n+                        }\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectStreamClass.java","additions":71,"deletions":11,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -2663,0 +2664,22 @@\n+\n+            @Override\n+            public Class<?> asPrimaryType(Class<?> clazz) {\n+                return clazz.asPrimaryType();\n+            }\n+            public Class<?> asValueType(Class<?> clazz) {\n+                return clazz.asValueType();\n+            }\n+\n+            public boolean isPrimaryType(Class<?> clazz) {\n+                return clazz.isPrimaryType();\n+            }\n+            public boolean isPrimitiveValueType(Class<?> clazz) {\n+                return clazz.isPrimitiveValueType();\n+            }\n+            public boolean isPrimitiveClass(Class<?> clazz) {\n+                return clazz.isPrimitiveClass();\n+            }\n+\n+            public int classFileFormatVersion(Class<?> clazz) {\n+                return clazz.getClassFileVersion();\n+            }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/System.java","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-                ReferenceClassDescImpl {\n+                ClassDescImpl {\n@@ -173,1 +173,1 @@\n-               : new ReferenceClassDescImpl(descriptor);\n+               : new ClassDescImpl(descriptor);\n@@ -299,1 +299,1 @@\n-        return descriptorString().startsWith(\"L\");\n+        return descriptorString().startsWith(\"L\") || descriptorString().startsWith(\"Q\");\n","filename":"src\/java.base\/share\/classes\/java\/lang\/constant\/ClassDesc.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -264,0 +264,4 @@\n+        GET_VALUE(\"getValue\"),\n+        PUT_VALUE(\"putValue\"),\n+        GET_VALUE_VOLATILE(\"getValueVolatile\"),\n+        PUT_VALUE_VOLATILE(\"putValueVolatile\"),\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/LambdaForm.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+import java.lang.invoke.MethodHandles.Lookup;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/LambdaFormEditor.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -113,8 +113,9 @@\n-            MN_IS_METHOD           = 0x00010000, \/\/ method (not constructor)\n-            MN_IS_CONSTRUCTOR      = 0x00020000, \/\/ constructor\n-            MN_IS_FIELD            = 0x00040000, \/\/ field\n-            MN_IS_TYPE             = 0x00080000, \/\/ nested type\n-            MN_CALLER_SENSITIVE    = 0x00100000, \/\/ @CallerSensitive annotation detected\n-            MN_TRUSTED_FINAL       = 0x00200000, \/\/ trusted final field\n-            MN_REFERENCE_KIND_SHIFT = 24, \/\/ refKind\n-            MN_REFERENCE_KIND_MASK = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT;\n+            MN_IS_METHOD             = 0x00010000, \/\/ method (not object constructor)\n+            MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, \/\/ object constructor\n+            MN_IS_FIELD              = 0x00040000, \/\/ field\n+            MN_IS_TYPE               = 0x00080000, \/\/ nested type\n+            MN_CALLER_SENSITIVE      = 0x00100000, \/\/ @CallerSensitive annotation detected\n+            MN_TRUSTED_FINAL         = 0x00200000, \/\/ trusted final field\n+            MN_FLATTENED             = 0x00400000, \/\/ flattened field\n+            MN_REFERENCE_KIND_SHIFT  = 24, \/\/ refKind\n+            MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT;\n@@ -174,1 +175,1 @@\n-    static boolean refKindIsConstructor(byte refKind) {\n+    static boolean refKindIsObjectConstructor(byte refKind) {\n@@ -602,1 +603,1 @@\n-            sb.append(getCharType(pt));\n+            sb.append(getCharErasedType(pt));\n@@ -604,1 +605,1 @@\n-        sb.append('_').append(getCharType(guardType.returnType()));\n+        sb.append('_').append(getCharErasedType(guardType.returnType()));\n@@ -607,1 +608,1 @@\n-    static char getCharType(Class<?> pt) {\n+    static char getCharErasedType(Class<?> pt) {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandleNatives.java","additions":13,"deletions":12,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import jdk.internal.value.PrimitiveClass;\n@@ -715,1 +716,1 @@\n-     * with special names ({@code \"<init>\"} and {@code \"<clinit>\"}).\n+     * with special names ({@code \"<init>\"}, {@code \"<vnew>\"} and {@code \"<clinit>\"}).\n@@ -1639,0 +1640,1 @@\n+            assert PrimitiveClass.isPrimaryType(lookupClass);\n@@ -2756,0 +2758,2 @@\n+         *\n+         *\n@@ -2771,0 +2775,3 @@\n+            if (type.returnType() != void.class) {\n+                throw new NoSuchMethodException(\"Constructors must have void return type: \" + refc.getName());\n+            }\n@@ -3457,1 +3464,1 @@\n-            assert(ctor.isConstructor());\n+            assert(ctor.isObjectConstructor() || ctor.isStaticValueFactoryMethod());\n@@ -3460,1 +3467,10 @@\n-            return lookup.getDirectConstructorNoSecurityManager(ctor.getDeclaringClass(), ctor);\n+            Class<?> defc = c.getDeclaringClass();\n+            if (ctor.isObjectConstructor()) {\n+                assert(ctor.getMethodType().returnType() == void.class);\n+                return lookup.getDirectConstructorNoSecurityManager(defc, ctor);\n+            } else {\n+                \/\/ static init factory is a static method\n+                assert(ctor.isMethod() && ctor.getMethodType().returnType() == defc && ctor.getReferenceKind() == REF_invokeStatic) : ctor.toString();\n+                assert(!MethodHandleNatives.isCallerSensitive(ctor));  \/\/ must not be caller-sensitive\n+                return lookup.getDirectMethodNoSecurityManager(ctor.getReferenceKind(), defc, ctor, lookup);\n+            }\n@@ -3705,1 +3721,1 @@\n-            if (name.startsWith(\"<\") && refKind != REF_newInvokeSpecial) {\n+            if (isIllegalMethodName(refKind, name)) {\n@@ -3708,0 +3724,1 @@\n+\n@@ -3727,0 +3744,12 @@\n+        \/*\n+         * \"<init>\" can only be invoked via invokespecial\n+         * \"<vnew>\" factory can only invoked via invokestatic\n+         *\/\n+        boolean isIllegalMethodName(byte refKind, String name) {\n+            if (name.startsWith(\"<\")) {\n+                return MemberName.VALUE_FACTORY_NAME.equals(name) ? refKind != REF_invokeStatic\n+                                                                  : refKind != REF_newInvokeSpecial;\n+            }\n+            return false;\n+        }\n+\n@@ -3729,2 +3758,3 @@\n-            if (name.startsWith(\"<\") && refKind != REF_newInvokeSpecial)\n-                throw new NoSuchMethodException(\"illegal method name: \"+name);\n+            if (isIllegalMethodName(refKind, name)) {\n+                throw new NoSuchMethodException(\"illegal method name: \" + name + \" \" + refKind);\n+            }\n@@ -3834,1 +3864,1 @@\n-            if (!fullPrivilegeLookup && defc != refc) {\n+            if (!fullPrivilegeLookup && PrimitiveClass.asPrimaryType(defc) != PrimitiveClass.asPrimaryType(refc)) {\n@@ -3842,1 +3872,1 @@\n-            if (m.isConstructor())\n+            if (m.isObjectConstructor())\n@@ -3917,1 +3947,1 @@\n-                               (defc == refc ||\n+                               (PrimitiveClass.asPrimaryType(defc) == PrimitiveClass.asPrimaryType(refc) ||\n@@ -3922,1 +3952,1 @@\n-                           (defc == refc ||\n+                           (PrimitiveClass.asPrimaryType(defc) == PrimitiveClass.asPrimaryType(refc) ||\n@@ -3999,1 +4029,0 @@\n-\n@@ -4143,1 +4172,1 @@\n-            assert(ctor.isConstructor());\n+            assert(ctor.isObjectConstructor());\n@@ -5049,1 +5078,1 @@\n-            if (value == null)\n+            if (!PrimitiveClass.isPrimitiveValueType(type) && value == null)\n@@ -5094,1 +5123,9 @@\n-        return type.isPrimitive() ?  zero(Wrapper.forPrimitiveType(type), type) : zero(Wrapper.OBJECT, type);\n+        if (type.isPrimitive()) {\n+            return zero(Wrapper.forPrimitiveType(type), type);\n+        } else if (PrimitiveClass.isPrimitiveValueType(type)) {\n+            \/\/ singleton default value\n+            Object value = UNSAFE.uninitializedDefaultValue(type);\n+            return identity(type).bindTo(value);\n+        } else {\n+            return zero(Wrapper.OBJECT, type);\n+        }\n@@ -5124,1 +5161,1 @@\n-        MethodType mtype = methodType(ptype, ptype);\n+        MethodType mtype = MethodType.methodType(ptype, ptype);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandles.java","additions":52,"deletions":15,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import jdk.internal.value.PrimitiveClass;\n@@ -59,1 +60,6 @@\n-                return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                if (f.isFlattened()) {\n+                    return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                        ? new VarHandleValues.FieldInstanceReadOnly(refc, foffset, type)\n+                        : new VarHandleValues.FieldInstanceReadWrite(refc, foffset, type));\n+                } else {\n+                    return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n@@ -62,0 +68,1 @@\n+                }\n@@ -121,3 +128,9 @@\n-                return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n-                       ? new VarHandleReferences.FieldStaticReadOnly(decl, base, foffset, type)\n-                       : new VarHandleReferences.FieldStaticReadWrite(decl, base, foffset, type));\n+                if (f.isFlattened()) {\n+                    return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                            ? new VarHandleValues.FieldStaticReadOnly(decl, refc, foffset, type)\n+                            : new VarHandleValues.FieldStaticReadWrite(decl, refc, foffset, type));\n+                } else {\n+                    return f.isFinal() && !isWriteAllowedOnFinalFields\n+                            ? new VarHandleReferences.FieldStaticReadOnly(decl, base, foffset, type)\n+                            : new VarHandleReferences.FieldStaticReadWrite(decl, base, foffset, type);\n+                }\n@@ -212,1 +225,6 @@\n-            return maybeAdapt(new VarHandleReferences.Array(aoffset, ashift, arrayClass));\n+            \/\/ the redundant componentType.isPrimitiveValueType() check is\n+            \/\/ there to minimize the performance impact to non-value array.\n+            \/\/ It should be removed when Unsafe::isFlattenedArray is intrinsified.\n+            return maybeAdapt(PrimitiveClass.isPrimitiveValueType(componentType) && UNSAFE.isFlattenedArray(arrayClass)\n+                ? new VarHandleValues.Array(aoffset, ashift, arrayClass)\n+                : new VarHandleReferences.Array(aoffset, ashift, arrayClass));\n@@ -631,1 +649,1 @@\n-            } else if (MethodHandleNatives.refKindIsConstructor(refKind)) {\n+            } else if (MethodHandleNatives.refKindIsObjectConstructor(refKind)) {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/VarHandles.java","additions":24,"deletions":6,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -97,1 +97,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -104,1 +104,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -111,1 +111,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -118,1 +118,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -125,1 +125,0 @@\n-\n@@ -149,0 +148,7 @@\n+#if[Object]\n+        @ForceInline\n+        static Object checkCast(FieldInstanceReadWrite handle, $type$ value) {\n+            return handle.fieldType.cast(value);\n+        }\n+#end[Object]\n+\n@@ -153,2 +159,2 @@\n-                             handle.fieldOffset,\n-                             {#if[Object]?handle.fieldType.cast(value):value});\n+                             handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                             {#if[Object]?checkCast(handle, value):value});\n@@ -161,2 +167,2 @@\n-                                     handle.fieldOffset,\n-                                     {#if[Object]?handle.fieldType.cast(value):value});\n+                                     handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                     {#if[Object]?checkCast(handle, value):value});\n@@ -169,2 +175,2 @@\n-                                   handle.fieldOffset,\n-                                   {#if[Object]?handle.fieldType.cast(value):value});\n+                                   handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                   {#if[Object]?checkCast(handle, value):value});\n@@ -177,2 +183,2 @@\n-                                    handle.fieldOffset,\n-                                    {#if[Object]?handle.fieldType.cast(value):value});\n+                                    handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                    {#if[Object]?checkCast(handle, value):value});\n@@ -186,3 +192,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -195,3 +201,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -204,3 +210,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -213,3 +219,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -222,3 +228,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -231,3 +237,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -240,3 +246,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -249,3 +255,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -258,2 +264,2 @@\n-                                          handle.fieldOffset,\n-                                          {#if[Object]?handle.fieldType.cast(value):value});\n+                                          handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                          {#if[Object]?checkCast(handle, value):value});\n@@ -266,2 +272,2 @@\n-                                          handle.fieldOffset,\n-                                          {#if[Object]?handle.fieldType.cast(value):value});\n+                                          handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                          {#if[Object]?checkCast(handle, value):value});\n@@ -274,2 +280,2 @@\n-                                          handle.fieldOffset,\n-                                          {#if[Object]?handle.fieldType.cast(value):value});\n+                                          handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                          {#if[Object]?checkCast(handle, value):value});\n@@ -445,1 +451,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -452,1 +458,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -459,1 +465,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -466,1 +472,1 @@\n-                                 handle.fieldOffset);\n+                                 handle.fieldOffset{#if[Value]?, handle.fieldType});\n@@ -497,0 +503,6 @@\n+#if[Object]\n+        static Object checkCast(FieldStaticReadWrite handle, $type$ value) {\n+            return handle.fieldType.cast(value);\n+        }\n+#end[Object]\n+\n@@ -501,2 +513,2 @@\n-                             handle.fieldOffset,\n-                             {#if[Object]?handle.fieldType.cast(value):value});\n+                             handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                             {#if[Object]?checkCast(handle, value):value});\n@@ -509,2 +521,2 @@\n-                                     handle.fieldOffset,\n-                                     {#if[Object]?handle.fieldType.cast(value):value});\n+                                     handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                     {#if[Object]?checkCast(handle, value):value});\n@@ -517,2 +529,2 @@\n-                                   handle.fieldOffset,\n-                                   {#if[Object]?handle.fieldType.cast(value):value});\n+                                   handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                   {#if[Object]?checkCast(handle, value):value});\n@@ -525,2 +537,2 @@\n-                                    handle.fieldOffset,\n-                                    {#if[Object]?handle.fieldType.cast(value):value});\n+                                    handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                    {#if[Object]?checkCast(handle, value):value});\n@@ -534,3 +546,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -544,3 +556,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -553,3 +565,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -562,3 +574,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -571,3 +583,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -580,3 +592,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -589,3 +601,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -598,3 +610,3 @@\n-                                               handle.fieldOffset,\n-                                               {#if[Object]?handle.fieldType.cast(expected):expected},\n-                                               {#if[Object]?handle.fieldType.cast(value):value});\n+                                               handle.fieldOffset{#if[Object]?, handle.fieldType},\n+                                               {#if[Object]?checkCast(handle, expected):expected},\n+                                               {#if[Object]?checkCast(handle, value):value});\n@@ -607,2 +619,2 @@\n-                                          handle.fieldOffset,\n-                                          {#if[Object]?handle.fieldType.cast(value):value});\n+                                          handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                          {#if[Object]?checkCast(handle, value):value});\n@@ -615,2 +627,2 @@\n-                                          handle.fieldOffset,\n-                                          {#if[Object]?handle.fieldType.cast(value):value});\n+                                          handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                          {#if[Object]?checkCast(handle, value):value});\n@@ -623,2 +635,2 @@\n-                                          handle.fieldOffset,\n-                                          {#if[Object]?handle.fieldType.cast(value):value});\n+                                          handle.fieldOffset{#if[Value]?, handle.fieldType},\n+                                          {#if[Object]?checkCast(handle, value):value});\n@@ -731,0 +743,8 @@\n+#if[Reference]\n+    static VarHandle makeVarHandleValuesArray(Class<?> arrayClass) {\n+        Class<?> componentType = arrayClass.getComponentType();\n+        assert UNSAFE.isFlattenedArray(arrayClass);\n+        \/\/ should cache these VarHandle for performance\n+        return VarHandles.makeArrayElementHandle(arrayClass);\n+    }\n+#end[Reference]\n@@ -823,1 +843,9 @@\n-            array[index] = {#if[Object]?handle.componentType.cast(value):value};\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                vh.set(oarray, index, reflectiveTypeCheck(array, value));\n+                return;\n+            }\n+#end[Reference]\n+            array[index] = {#if[Object]?runtimeTypeCheck(handle, array, value):value};\n@@ -834,0 +862,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.getVolatile(oarray, index);\n+            }\n+#end[Reference]\n@@ -835,1 +870,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase);\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType});\n@@ -846,0 +881,8 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                vh.setVolatile(oarray, index, reflectiveTypeCheck(array, value));\n+                return;\n+            }\n+#end[Reference]\n@@ -847,1 +890,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -859,0 +902,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.getOpaque(oarray, index);\n+            }\n+#end[Reference]\n@@ -860,1 +910,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase);\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType});\n@@ -871,0 +921,8 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                vh.setOpaque(oarray, index, reflectiveTypeCheck(array, value));\n+                return;\n+            }\n+#end[Reference]\n@@ -872,1 +930,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -884,0 +942,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.getAcquire(oarray, index);\n+            }\n+#end[Reference]\n@@ -885,1 +950,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase);\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType});\n@@ -896,0 +961,8 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                vh.setRelease(oarray, index, reflectiveTypeCheck(array, value));\n+                return;\n+            }\n+#end[Reference]\n@@ -897,1 +970,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -910,0 +983,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.compareAndSet(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -911,1 +991,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -924,0 +1004,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.compareAndExchange(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -925,1 +1012,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -938,0 +1025,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.compareAndExchangeAcquire(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -939,1 +1033,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -952,0 +1046,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.compareAndExchangeRelease(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -953,1 +1054,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -966,0 +1067,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.weakCompareAndSetPlain(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -967,1 +1075,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -980,0 +1088,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.weakCompareAndSet(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -981,1 +1096,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -994,0 +1109,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.weakCompareAndSetAcquire(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -995,1 +1117,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -1008,0 +1130,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.weakCompareAndSetRelease(oarray, index, expected, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -1009,1 +1138,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -1022,0 +1151,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.getAndSet(oarray, index, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -1023,1 +1159,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -1035,0 +1171,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.getAndSetAcquire(oarray, index, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -1036,1 +1179,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -1048,0 +1191,7 @@\n+#if[Reference]\n+            if (UNSAFE.isFlattenedArray(oarray.getClass())) {\n+                \/\/ for flattened array, delegate to VarHandle of the inline type array\n+                VarHandle vh = makeVarHandleValuesArray(oarray.getClass());\n+                return vh.getAndSetRelease(oarray, index, reflectiveTypeCheck(array, value));\n+            }\n+#end[Reference]\n@@ -1049,1 +1199,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/X-VarHandle.java.template","additions":253,"deletions":103,"binary":false,"changes":356,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -567,0 +568,39 @@\n+\n+    \/**\n+     * {@return the primary class for a primitive class}\n+     *\n+     * @param klass a class\n+     *\/\n+    Class<?> asPrimaryType(Class<?> klass);\n+\n+    \/**\n+     * {@return the value type of a primitive class}\n+     *\n+     * @param klass a class\n+     *\/\n+    Class<?> asValueType(Class<?> klass);\n+\n+    \/**\n+     * {@return true if the class is the primary type of a primitive class}\n+     *\n+     * @param klass a class\n+     *\/\n+    boolean isPrimaryType(Class<?> klass);\n+\n+    \/**\n+     * {@return true if the class is the primary type of a primitive class}\n+     *\n+     * @param klass a class\n+     *\/\n+    boolean isPrimitiveValueType(Class<?> klass);\n+\n+    \/**\n+     * Returns {@code true} if this class is a primitive class.\n+     *\/\n+    boolean isPrimitiveClass(Class<?> klass);\n+\n+    \/**\n+     * Returns the class file format version of the class.\n+     *\/\n+    int classFileFormatVersion(Class<?> klass);\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangAccess.java","additions":40,"deletions":0,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -134,1 +134,0 @@\n-\n@@ -275,0 +274,2 @@\n+    exports jdk.internal.value to  \/\/ Needed by Unsafe\n+        jdk.unsupported;\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -142,4 +142,6 @@\n-     * initializer}  For a constructor, the name {@code \"<init>\"} is\n-     * returned, for a static initializer, the name {@code \"<clinit>\"}\n-     * is returned, and for an anonymous class or instance\n-     * initializer, an {@linkplain Name##empty_name empty name} is\n+     * initializer}  For a constructor, the name {@code \"<init>\"} or\n+     * initializer} For a constructor, the name {@code \"<init>\"} is\n+     * returned, for a value class static factory method, the name\n+     * {@code \"<vnew>\"} is returned, for a static initializer, the\n+     * name {@code \"<clinit>\"} is returned, and for an anonymous class\n+     * or instance initializer, an {@linkplain Name##empty_name empty name} is\n","filename":"src\/java.compiler\/share\/classes\/javax\/lang\/model\/element\/ExecutableElement.java","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -102,0 +102,7 @@\n+    \/** Marks a type as a primitive class. We can't reuse the class file encoding (ACC_PRIMITIVE)\n+     * since the latter shares its value (0x800) with ACC_STRICT (javac speak: STRICT_FP) and while\n+     * STRICT_FP is not a valid flag for a class in the class file level, javac's ASTs flag a class\n+     * as being STRICT_FP so as to propagate the FP strictness to methods of the class thereby causing\n+     * a clash *\/\n+    public static final int PRIMITIVE_CLASS  = 1<<16;\n+\n@@ -108,1 +115,2 @@\n-    public static final int ACC_SUPER    = 0x0020;\n+    public static final int ACC_IDENTITY = 0x0020;\n+    public static final int ACC_VALUE    = 0x0040;\n@@ -111,0 +119,1 @@\n+    public static final int ACC_PRIMITIVE = 0x0800;\n@@ -126,0 +135,21 @@\n+    \/** Flag is set for a class symbol if it defines one or more non-empty\n+     *  instance initializer block(s). This is relevenat only for class symbols\n+     *  that originate from source types. For binary types the instance initializer\n+     *  blocks are \"normalized\" into the constructors.\n+     *\/\n+    public static final int HASINITBLOCK         = 1<<18;\n+\n+    \/** Flag is set for a method symbol if it is an empty no-arg ctor.\n+     *  i.e. one that simply returns (jlO) or merely chains to a super's\n+     *  no-arg ctor\n+     *\/\n+    public static final int EMPTYNOARGCONSTR         = 1<<18;\n+\n+    \/** Flag is set for a class or interface whose instances have identity\n+     * i.e. class\/interface declarations that are expressly declared with\n+     * the modifier `identity' or (b) any concrete class not declared with the\n+     * modifier `value' (c) abstract class not declared `value' but meets various\n+     * stipulations (d) older class files with ACC_SUPER bit set\n+     *\/\n+    public static final int IDENTITY_TYPE            = 1<<19;\n+\n@@ -131,0 +161,3 @@\n+    \/** Marks a type as a value class *\/\n+    public static final int VALUE_CLASS      = 1<<20;\n+\n@@ -408,2 +441,2 @@\n-        LocalClassFlags                   = FINAL | ABSTRACT | STRICTFP | ENUM | SYNTHETIC,\n-        StaticLocalFlags                  = LocalClassFlags | STATIC | INTERFACE,\n+        LocalClassFlags                   = FINAL | ABSTRACT | STRICTFP | ENUM | SYNTHETIC | ACC_IDENTITY,\n+        StaticLocalClassFlags             = LocalClassFlags | STATIC | INTERFACE,\n@@ -421,1 +454,2 @@\n-                                            SYNCHRONIZED | FINAL | STRICTFP;\n+                                            SYNCHRONIZED | FINAL | STRICTFP,\n+        AdjustedClassFlags                = ClassFlags | ACC_PRIMITIVE | ACC_VALUE;\n@@ -423,5 +457,7 @@\n-        ExtendedStandardFlags             = (long)StandardFlags | DEFAULT | SEALED | NON_SEALED,\n-        ExtendedMemberClassFlags          = (long)MemberClassFlags | SEALED | NON_SEALED,\n-        ExtendedMemberStaticClassFlags    = (long) MemberStaticClassFlags | SEALED | NON_SEALED,\n-        ExtendedClassFlags                = (long)ClassFlags | SEALED | NON_SEALED,\n-        ModifierFlags                     = ((long)StandardFlags & ~INTERFACE) | DEFAULT | SEALED | NON_SEALED,\n+        ExtendedStandardFlags             = (long)StandardFlags | DEFAULT | SEALED | NON_SEALED | PRIMITIVE_CLASS | VALUE_CLASS,\n+        ExtendedMemberClassFlags          = (long)MemberClassFlags | SEALED | NON_SEALED | PRIMITIVE_CLASS | VALUE_CLASS,\n+        ExtendedMemberStaticClassFlags    = (long) MemberStaticClassFlags | SEALED | NON_SEALED | PRIMITIVE_CLASS | VALUE_CLASS,\n+        ExtendedClassFlags                = (long)ClassFlags | SEALED | NON_SEALED | PRIMITIVE_CLASS | VALUE_CLASS,\n+        ExtendedLocalClassFlags           = (long) LocalClassFlags | PRIMITIVE_CLASS | VALUE_CLASS,\n+        ExtendedStaticLocalClassFlags     = (long) StaticLocalClassFlags | PRIMITIVE_CLASS | VALUE_CLASS,\n+        ModifierFlags                     = ((long)StandardFlags & ~INTERFACE) | DEFAULT | SEALED | NON_SEALED | PRIMITIVE_CLASS | VALUE_CLASS,\n@@ -453,0 +489,2 @@\n+            if (0 != (flags & PRIMITIVE_CLASS))     modifiers.add(Modifier.PRIMITIVE);\n+            if (0 != (flags & VALUE_CLASS))     modifiers.add(Modifier.VALUE);\n@@ -494,0 +532,8 @@\n+        HASINITBLOCK(Flags.HASINITBLOCK),\n+        EMPTYNOARGCONSTR(Flags.EMPTYNOARGCONSTR),\n+        IDENTITY_TYPE(Flags.IDENTITY_TYPE) {\n+            @Override\n+            public String toString() {\n+                return \"identity\";\n+            }\n+        },\n@@ -498,0 +544,2 @@\n+        PRIMITIVE(Flags.PRIMITIVE_CLASS),\n+        VALUE(Flags.VALUE_CLASS),\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Flags.java","additions":57,"deletions":9,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -219,1 +219,1 @@\n-                    !s.isConstructor())\n+                    !s.isInitOrVNew())\n@@ -245,1 +245,1 @@\n-                    (s.kind == MTH && !s.isConstructor() &&\n+                    (s.kind == MTH && !s.isInitOrVNew() &&\n@@ -247,1 +247,1 @@\n-                    (s.kind == MTH && s.isConstructor()))\n+                    (s.kind == MTH && s.isInitOrVNew()))\n@@ -640,1 +640,1 @@\n-                                                      t.getMetadata());\n+                                                      t.getMetadata(), t.getFlavor());\n@@ -1058,1 +1058,1 @@\n-                    } else if (exsym.isConstructor()) {\n+                    } else if (exsym.isInitOrVNew()) {\n@@ -1165,1 +1165,1 @@\n-                    if (tree.sym.isConstructor()) {\n+                    if (tree.sym.isInitOrVNew()) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/TypeAnnotations.java","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -52,1 +52,0 @@\n-import com.sun.tools.javac.comp.LambdaToMethod;\n@@ -95,0 +94,1 @@\n+    final boolean allowPrimitiveClasses;\n@@ -122,0 +122,2 @@\n+        Options options = Options.instance(context);\n+        allowPrimitiveClasses = Feature.PRIMITIVE_CLASSES.allowedInSource(source) && options.isSet(\"enablePrimitiveClasses\");\n@@ -270,1 +272,1 @@\n-                else return new ClassType(outer1, typarams1.toList(), t.tsym, t.getMetadata()) {\n+                else return new ClassType(outer1, typarams1.toList(), t.tsym, t.getMetadata(), t.getFlavor()) {\n@@ -601,0 +603,11 @@\n+\n+        if (allowPrimitiveClasses) {\n+            boolean tValue = t.isPrimitiveClass();\n+            boolean sValue = s.isPrimitiveClass();\n+            if (tValue != sValue) {\n+                return tValue ?\n+                        isSubtype(t.referenceProjection(), s) :\n+                        !t.hasTag(BOT) && isSubtype(t, s.referenceProjection());\n+            }\n+        }\n+\n@@ -763,2 +776,4 @@\n-            } else if (abstracts.size() == 1) {\n-                return new FunctionDescriptor(abstracts.first());\n+            }\n+            FunctionDescriptor descRes;\n+            if (abstracts.size() == 1) {\n+                descRes = new FunctionDescriptor(abstracts.first());\n@@ -766,1 +781,1 @@\n-                FunctionDescriptor descRes = mergeDescriptors(origin, abstracts.toList());\n+                descRes = mergeDescriptors(origin, abstracts.toList());\n@@ -785,1 +800,11 @@\n-                return descRes;\n+            \/\/ an interface must be neither an identity interface nor a value interface to be functional.\n+            List<Type> allInterfaces = closure(origin.type);\n+            for (Type iface : allInterfaces) {\n+                if (iface.isValueInterface()) {\n+                    throw failure(\"not.a.functional.intf.1\", origin, diags.fragment(Fragments.ValueInterfaceNonfunctional));\n+                }\n+                if (iface.isIdentityInterface()) {\n+                    throw failure(\"not.a.functional.intf.1\", origin, diags.fragment(Fragments.IdentityInterfaceNonfunctional));\n+                }\n+            }\n+            return descRes;\n@@ -955,1 +980,1 @@\n-                        t.name != names.init &&\n+                        !names.isInitOrVNew(t.name) &&\n@@ -1026,1 +1051,13 @@\n-                    return isSubtypeUncheckedInternal(elemtype(t), elemtype(s), false, warn);\n+                    \/\/ if T.ref <: S, then T[] <: S[]\n+                    Type es = elemtype(s);\n+                    Type et = elemtype(t);\n+                    if (allowPrimitiveClasses) {\n+                        if (et.isPrimitiveClass()) {\n+                            et = et.referenceProjection();\n+                            if (es.isPrimitiveClass())\n+                                es = es.referenceProjection();  \/\/ V <: V, surely\n+                        }\n+                    }\n+                    if (!isSubtypeUncheckedInternal(et, es, false, warn))\n+                        return false;\n+                    return true;\n@@ -1123,1 +1160,1 @@\n-                         s.hasTag(BOT) || s.hasTag(CLASS) ||\n+                         s.hasTag(BOT) || (s.hasTag(CLASS) && (!allowPrimitiveClasses || !s.isPrimitiveClass())) ||\n@@ -1190,0 +1227,1 @@\n+                    && (t.tsym != s.tsym || t.isReferenceProjection() == s.isReferenceProjection())\n@@ -1201,2 +1239,11 @@\n-                    else\n-                        return isSubtypeNoCapture(t.elemtype, elemtype(s));\n+                    else {\n+                        \/\/ if T.ref <: S, then T[] <: S[]\n+                        Type es = elemtype(s);\n+                        Type et = elemtype(t);\n+                        if (allowPrimitiveClasses && et.isPrimitiveClass()) {\n+                            et = et.referenceProjection();\n+                            if (es.isPrimitiveClass())\n+                                es = es.referenceProjection();  \/\/ V <: V, surely\n+                        }\n+                        return isSubtypeNoCapture(et, es);\n+                    }\n@@ -1422,1 +1469,2 @@\n-                    && visit(t.getEnclosingType(), s.getEnclosingType())\n+                    && t.isReferenceProjection() == s.isReferenceProjection()\n+                    && visit(getEnclosingType(t), getEnclosingType(s))\n@@ -1425,0 +1473,8 @@\n+                \/\/ where\n+                private Type getEnclosingType(Type t) {\n+                    Type et = t.getEnclosingType();\n+                    if (et.isReferenceProjection()) {\n+                        et = et.valueProjection();\n+                    }\n+                    return et;\n+                }\n@@ -1584,0 +1640,9 @@\n+\n+                    \/\/ -----------------------------------  Unspecified behavior ----------------\n+\n+                    \/* If a primitive class V implements an interface I, then does \"? extends I\" contain V?\n+                       It seems widening must be applied here to answer yes to compile some common code\n+                       patterns.\n+                    *\/\n+\n+                    \/\/ ---------------------------------------------------------------------------\n@@ -1673,1 +1738,1 @@\n-            if (isSubtype(erasure(ts.type), erasure(ss.type))) {\n+            if (isSubtype(erasure(ts.type.referenceProjectionOrSelf()), erasure(ss.type))) {\n@@ -1728,1 +1793,1 @@\n-                if (s.hasTag(ERROR) || s.hasTag(BOT))\n+                if (s.hasTag(ERROR) || (s.hasTag(BOT) && (!allowPrimitiveClasses || !t.isPrimitiveClass())))\n@@ -1747,0 +1812,10 @@\n+                    if (allowPrimitiveClasses) {\n+                        if (t.isPrimitiveClass()) {\n+                            \/\/ (s) Value ? == (s) Value.ref\n+                            t = t.referenceProjection();\n+                        }\n+                        if (s.isPrimitiveClass()) {\n+                            \/\/ (Value) t ? == (Value.ref) t\n+                            s = s.referenceProjection();\n+                        }\n+                    }\n@@ -2140,0 +2215,29 @@\n+     * Further caveats in Valhalla: There are two \"hazards\" we need to watch out for when using\n+     * this method.\n+     *\n+     * 1. Since Foo.ref and Foo.val share the same symbol, that of Foo.class, a call to\n+     *    asSuper(Foo.ref.type, Foo.val.type.tsym) would return non-null. This MAY NOT BE correct\n+     *    depending on the call site. Foo.val is NOT a super type of Foo.ref either in the language\n+     *    model or in the VM's world view. An example of such an hazardous call used to exist in\n+     *    Gen.visitTypeCast. When we emit code for  (Foo) Foo.ref.instance a check for whether we\n+     *    really need the cast cannot\/shouldn't be gated on\n+     *\n+     *        asSuper(tree.expr.type, tree.clazz.type.tsym) == null)\n+     *\n+     *    but use !types.isSubtype(tree.expr.type, tree.clazz.type) which operates in terms of\n+     *    types. When we operate in terms of symbols, there is a loss of type information leading\n+     *    to a hazard. Whether a call to asSuper should be transformed into a isSubtype call is\n+     *    tricky. isSubtype returns just a boolean while asSuper returns richer information which\n+     *    may be required at the call site. Also where the concerned symbol corresponds to a\n+     *    generic class, an asSuper call cannot be conveniently rewritten as an isSubtype call\n+     *    (see that asSuper(ArrayList<String>.type, List<T>.tsym) != null while\n+     *    isSubType(ArrayList<String>.type, List<T>.type) is false;) So care needs to be exercised.\n+     *\n+     * 2. Given a primitive class Foo, a call to asSuper(Foo.type, SuperclassOfFoo.tsym) and\/or\n+     *    a call to asSuper(Foo.type, SuperinterfaceOfFoo.tsym) would answer null. In many places\n+     *    that is NOT what we want. An example of such a hazardous call used to occur in\n+     *    Attr.visitForeachLoop when checking to make sure the for loop's control variable of a type\n+     *    that implements Iterable: viz: types.asSuper(exprType, syms.iterableType.tsym);\n+     *    These hazardous calls should be rewritten as\n+     *    types.asSuper(exprType.referenceProjectionOrSelf(), syms.iterableType.tsym); instead.\n+     *\n@@ -2152,0 +2256,6 @@\n+\n+        if (allowPrimitiveClasses && t.isPrimitiveClass()) {\n+            \/\/ No man may be an island, but the bell tolls for a value.\n+            return t.tsym == sym ? t : null;\n+        }\n+\n@@ -2282,3 +2392,12 @@\n-        return (sym.flags() & STATIC) != 0\n-            ? sym.type\n-            : memberType.visit(t, sym);\n+\n+        if ((sym.flags() & STATIC) != 0)\n+            return sym.type;\n+\n+        \/* If any primitive class types are involved, switch over to the reference universe,\n+           where the hierarchy is navigable. V and V.ref have identical membership\n+           with no bridging needs.\n+        *\/\n+        if (allowPrimitiveClasses && t.isPrimitiveClass())\n+            t = t.referenceProjection();\n+\n+        return memberType.visit(t, sym);\n@@ -2437,7 +2556,19 @@\n-                Type erased = t.tsym.erasure(Types.this);\n-                if (recurse) {\n-                    erased = new ErasedClassType(erased.getEnclosingType(),erased.tsym,\n-                            t.getMetadata().without(Kind.ANNOTATIONS));\n-                    return erased;\n-                } else {\n-                    return combineMetadata(erased, t);\n+                \/\/ erasure(projection(primitive)) = projection(erasure(primitive))\n+                Type erased = eraseClassType(t, recurse);\n+                if (erased.hasTag(CLASS) && t.flavor != erased.getFlavor()) {\n+                    erased = new ClassType(erased.getEnclosingType(),\n+                            List.nil(), erased.tsym,\n+                            erased.getMetadata(), t.flavor);\n+                }\n+                return erased;\n+            }\n+                \/\/ where\n+                private Type eraseClassType(ClassType t, Boolean recurse) {\n+                    Type erased = t.tsym.erasure(Types.this);\n+                    if (recurse) {\n+                        erased = new ErasedClassType(erased.getEnclosingType(), erased.tsym,\n+                                t.getMetadata().without(Kind.ANNOTATIONS));\n+                        return erased;\n+                    } else {\n+                        return combineMetadata(erased, t);\n+                    }\n@@ -2445,1 +2576,0 @@\n-            }\n@@ -2765,1 +2895,1 @@\n-                                         t.getMetadata());\n+                                         t.getMetadata(), t.getFlavor());\n@@ -3886,1 +4016,1 @@\n-                                 class1.tsym);\n+                                 class1.tsym, TypeMetadata.EMPTY, class1.getFlavor());\n@@ -4446,1 +4576,1 @@\n-                                 cls.getMetadata());\n+                                 cls.getMetadata(), cls.getFlavor());\n@@ -4857,0 +4987,1 @@\n+        private boolean encodeTypeSig;\n@@ -4858,1 +4989,1 @@\n-        public UniqueType(Type type, Types types) {\n+        public UniqueType(Type type, Types types, boolean encodeTypeSig) {\n@@ -4861,0 +4992,5 @@\n+            this.encodeTypeSig = encodeTypeSig;\n+        }\n+\n+        public UniqueType(Type type, Types types) {\n+            this(type, types, true);\n@@ -4872,0 +5008,4 @@\n+        public boolean encodeTypeSig() {\n+            return encodeTypeSig;\n+        }\n+\n@@ -5110,1 +5250,4 @@\n-                    append('L');\n+                    if (types.allowPrimitiveClasses && type.isPrimitiveClass())\n+                        append('Q');\n+                    else\n+                        append('L');\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Types.java","additions":173,"deletions":30,"binary":false,"changes":203,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+import com.sun.tools.javac.code.Type.ClassType.Flavor;\n@@ -170,0 +171,1 @@\n+        allowPrimitiveClasses = Feature.PRIMITIVE_CLASSES.allowedInSource(source) && options.isSet(\"enablePrimitiveClasses\");\n@@ -188,0 +190,4 @@\n+    \/** Switch: allow primitive classes ?\n+     *\/\n+    boolean allowPrimitiveClasses;\n+\n@@ -276,1 +282,1 @@\n-            ((owner.name == names.init ||    \/\/ i.e. we are in a constructor\n+            ((names.isInitOrVNew(owner.name) ||    \/\/ i.e. we are in a constructor\n@@ -803,1 +809,1 @@\n-                List<Type> bounds = List.of(attribType(tvar.bounds.head, env));\n+                List<Type> bounds = List.of(chk.checkRefType(tvar.bounds.head, attribType(tvar.bounds.head, env), false));\n@@ -805,1 +811,1 @@\n-                    bounds = bounds.prepend(attribType(bound, env));\n+                    bounds = bounds.prepend(chk.checkRefType(bound, attribType(bound, env), false));\n@@ -1075,1 +1081,1 @@\n-                if (tree.name == names.init) {\n+                if (names.isInitOrVNew(tree.name)) {\n@@ -1190,1 +1196,1 @@\n-                if (tree.name == names.init && owner.type != syms.objectType) {\n+                if (names.isInitOrVNew(tree.name) && owner.type != syms.objectType) {\n@@ -1193,1 +1199,1 @@\n-                            TreeInfo.getConstructorInvocationName(body.stats, names) == names.empty) {\n+                            TreeInfo.getConstructorInvocationName(body.stats, names, true) == names.empty) {\n@@ -1206,0 +1212,6 @@\n+                    } else if ((env.enclClass.sym.flags() & VALUE_CLASS) != 0 &&\n+                        (tree.mods.flags & GENERATEDCONSTR) == 0 &&\n+                        TreeInfo.isSuperCall(body.stats.head)) {\n+                        \/\/ value constructors are not allowed to call super directly,\n+                        \/\/ but tolerate compiler generated ones, these are ignored during code generation\n+                        log.error(tree.body.stats.head.pos(), Errors.CallToSuperNotAllowedInValueCtor);\n@@ -1293,0 +1305,3 @@\n+            \/* Don't want constant propagation\/folding for instance fields of primitive classes,\n+               as these can undergo updates via copy on write.\n+            *\/\n@@ -1294,1 +1309,1 @@\n-                if ((v.flags_field & FINAL) == 0 ||\n+                if ((v.flags_field & FINAL) == 0 || ((v.flags_field & STATIC) == 0 && v.owner.isValueClass()) ||\n@@ -1334,1 +1349,2 @@\n-        return false;\n+        \/\/ isValueObject is not included in Object yet so we need a work around\n+        return name == names.isValueObject;\n@@ -1527,1 +1543,1 @@\n-                Type base = types.asSuper(exprType, syms.iterableType.tsym);\n+                Type base = types.asSuper(exprType.referenceProjectionOrSelf(), syms.iterableType.tsym);\n@@ -1543,1 +1559,1 @@\n-                    if (types.asSuper(iterSymbol.type.getReturnType(), syms.iteratorType.tsym) == null) {\n+                    if (types.asSuper(iterSymbol.type.getReturnType().referenceProjectionOrSelf(), syms.iteratorType.tsym) == null) {\n@@ -1885,1 +1901,1 @@\n-        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env));\n+        chk.checkIdentityType(tree.pos(), attribExpr(tree.lock, env));\n@@ -1976,1 +1992,1 @@\n-            types.asSuper(resource, syms.autoCloseableType.tsym) != null &&\n+            types.asSuper(resource.referenceProjectionOrSelf(), syms.autoCloseableType.tsym) != null &&\n@@ -2165,1 +2181,2 @@\n-            \/\/ Those were all the cases that could result in a primitive\n+            \/\/ Those were all the cases that could result in a primitive. See if primitive boxing and primitive\n+            \/\/ value conversions bring about a convergence.\n@@ -2167,1 +2184,2 @@\n-                                 .map(t -> t.isPrimitive() ? types.boxedClass(t).type : t)\n+                                 .map(t -> t.isPrimitive() ? types.boxedClass(t).type\n+                                         : t.isReferenceProjection() ? t.valueProjection() : t)\n@@ -2178,1 +2196,1 @@\n-                                 .map(t -> chk.checkNonVoid(posIt.next(), t))\n+                                 .map(t -> chk.checkNonVoid(posIt.next(), allowPrimitiveClasses && t.isPrimitiveClass() ? t.referenceProjection() : t))\n@@ -2181,1 +2199,1 @@\n-            \/\/ both are known to be reference types.  The result is\n+            \/\/ both are known to be reference types (or projections).  The result is\n@@ -2620,0 +2638,4 @@\n+                \/\/ Special treatment for primitive classes: Given an expression v of type V where\n+                \/\/ V is a primitive class, v.getClass() is typed to be Class<? extends |V.ref|>\n+                Type wcb = types.erasure(allowPrimitiveClasses && qualifierType.isPrimitiveClass() ?\n+                                         qualifierType.referenceProjection() : qualifierType.baseType());\n@@ -2621,1 +2643,1 @@\n-                        List.of(new WildcardType(types.erasure(qualifierType.baseType()),\n+                        List.of(new WildcardType(wcb,\n@@ -2625,1 +2647,2 @@\n-                        restype.getMetadata());\n+                        restype.getMetadata(),\n+                        restype.getFlavor());\n@@ -2645,1 +2668,1 @@\n-            if (enclMethod != null && enclMethod.name == names.init) {\n+            if (enclMethod != null && names.isInitOrVNew(enclMethod.name)) {\n@@ -2794,0 +2817,10 @@\n+            \/\/ Check that it is an instantiation of a class and not a projection type\n+            if (allowPrimitiveClasses) {\n+                if (clazz.hasTag(SELECT)) {\n+                    JCFieldAccess fieldAccess = (JCFieldAccess) clazz;\n+                    if (fieldAccess.selected.type.isPrimitiveClass() &&\n+                            (fieldAccess.name == names.ref || fieldAccess.name == names.val)) {\n+                        log.error(tree.pos(), Errors.ProjectionCantBeInstantiated);\n+                    }\n+                }\n+            }\n@@ -2818,1 +2851,2 @@\n-                                               clazztype.getMetadata());\n+                                               clazztype.getMetadata(),\n+                                               clazztype.getFlavor());\n@@ -2972,0 +3006,3 @@\n+                    if (allowPrimitiveClasses) {\n+                        chk.checkParameterizationByPrimitiveClass(tree, clazztype);\n+                    }\n@@ -3044,0 +3081,3 @@\n+        \/\/ Likewise arg can't be null if it is a primitive class instance.\n+        if (allowPrimitiveClasses && arg.type.isPrimitiveClass())\n+            return arg;\n@@ -3533,1 +3573,2 @@\n-                    for (Symbol s : enclClass.members_field.getSymbolsByName(names.init)) {\n+                    Name constructorName = owner.isConcreteValueClass() ? names.vnew : names.init;\n+                    for (Symbol s : enclClass.members_field.getSymbolsByName(constructorName)) {\n@@ -3596,0 +3637,1 @@\n+            Symbol lhsSym = TreeInfo.symbol(that.expr);\n@@ -3597,0 +3639,4 @@\n+                \/\/ TODO - a bit hacky but...\n+                if (lhsSym != null && lhsSym.isConcreteValueClass() && that.name == names.init) {\n+                    that.name = names.vnew;\n+                }\n@@ -3602,1 +3648,0 @@\n-                Symbol lhsSym = TreeInfo.symbol(that.expr);\n@@ -3686,1 +3731,1 @@\n-            that.sym = refSym.isConstructor() ? refSym.baseSymbol() : refSym;\n+            that.sym = refSym.isInitOrVNew() ? refSym.baseSymbol() : refSym;\n@@ -4364,0 +4409,10 @@\n+        Assert.check(site == tree.selected.type);\n+        if (allowPrimitiveClasses && tree.name == names._class && site.isPrimitiveClass()) {\n+            \/* JDK-8269956: Where a reflective (class) literal is needed, the unqualified Point.class is\n+             * always the \"primary\" mirror - representing the primitive reference runtime type - thereby\n+             * always matching the behavior of Object::getClass\n+             *\/\n+             if (!tree.selected.hasTag(SELECT) || ((JCFieldAccess) tree.selected).name != names.val) {\n+                 tree.selected.setType(site = site.referenceProjection());\n+             }\n+        }\n@@ -4376,1 +4431,1 @@\n-                return ;\n+                return;\n@@ -4480,1 +4535,1 @@\n-                Type site1 = types.asSuper(env.enclClass.sym.type, site.tsym);\n+                Type site1 = types.asSuper(env.enclClass.sym.type.referenceProjectionOrSelf(), site.tsym);\n@@ -4523,0 +4578,2 @@\n+                } else if (allowPrimitiveClasses && site.isPrimitiveClass() && isType(location) && resultInfo.pkind.contains(KindSelector.TYP) && (name == names.ref || name == names.val)) {\n+                    return site.tsym;\n@@ -4626,1 +4683,1 @@\n-                \/\/ except for two situations:\n+                \/\/ except for three situations:\n@@ -4629,0 +4686,3 @@\n+                    if (allowPrimitiveClasses) {\n+                        Assert.check(owntype.getFlavor() != Flavor.X_Typeof_X);\n+                    }\n@@ -4632,1 +4692,8 @@\n-                    \/\/ (a) If the symbol's type is parameterized, erase it\n+                    \/\/ (a) If symbol is a primitive class and its reference projection\n+                    \/\/ is requested via the .ref notation, then adjust the computed type to\n+                    \/\/ reflect this.\n+                    if (allowPrimitiveClasses && owntype.isPrimitiveClass() && tree.hasTag(SELECT) && ((JCFieldAccess) tree).name == names.ref) {\n+                        owntype = new ClassType(owntype.getEnclosingType(), owntype.getTypeArguments(), (TypeSymbol)sym, owntype.getMetadata(), Flavor.L_TypeOf_Q);\n+                    }\n+\n+                    \/\/ (b) If the symbol's type is parameterized, erase it\n@@ -4639,1 +4706,1 @@\n-                    \/\/ (b) If the symbol's type is an inner class, then\n+                    \/\/ (c) If the symbol's type is an inner class, then\n@@ -4659,1 +4726,1 @@\n-                                owntype.getMetadata());\n+                                owntype.getMetadata(), owntype.getFlavor());\n@@ -4722,1 +4789,1 @@\n-            if (sym.name != names.init || tree.hasTag(REFERENCE)) {\n+            if (!names.isInitOrVNew(sym.name) || tree.hasTag(REFERENCE)) {\n@@ -4976,0 +5043,31 @@\n+    public void visitDefaultValue(JCDefaultValue tree) {\n+        if (!allowPrimitiveClasses) {\n+            log.error(DiagnosticFlag.SOURCE_LEVEL, tree.pos(),\n+                    Feature.PRIMITIVE_CLASSES.error(sourceName));\n+        }\n+\n+        \/\/ Attribute the qualifier expression, and determine its symbol (if any).\n+        Type site = attribTree(tree.clazz, env, new ResultInfo(KindSelector.TYP_PCK, Type.noType));\n+        if (!pkind().contains(KindSelector.TYP_PCK))\n+            site = capture(site); \/\/ Capture field access\n+        if (!allowPrimitiveClasses) {\n+            result = types.createErrorType(names._default, site.tsym, site);\n+        } else {\n+            Symbol sym = switch (site.getTag()) {\n+                case WILDCARD -> throw new AssertionError(tree);\n+                case PACKAGE -> {\n+                    log.error(tree.pos, Errors.CantResolveLocation(Kinds.KindName.CLASS, site.tsym.getQualifiedName(), null, null,\n+                            Fragments.Location(Kinds.typeKindName(env.enclClass.type), env.enclClass.type, null)));\n+                    yield syms.errSymbol;\n+                }\n+                case ERROR -> types.createErrorType(names._default, site.tsym, site).tsym;\n+                default -> new VarSymbol(STATIC, names._default, site, site.tsym);\n+            };\n+\n+            if (site.hasTag(TYPEVAR) && sym.kind != ERR) {\n+                site = types.skipTypeVars(site, true);\n+            }\n+            result = checkId(tree, site, sym, env, resultInfo);\n+        }\n+    }\n+\n@@ -5042,1 +5140,1 @@\n-                                        clazztype.getMetadata());\n+                                        clazztype.getMetadata(), clazztype.getFlavor());\n@@ -5169,1 +5267,1 @@\n-                make.Modifiers(PUBLIC | ABSTRACT),\n+                make.Modifiers(PUBLIC | ABSTRACT | (extending != null && TreeInfo.symbol(extending).isPrimitiveClass() ? PRIMITIVE_CLASS : 0)),\n@@ -5192,1 +5290,1 @@\n-        result = check(tree, new WildcardType(chk.checkRefType(tree.pos(), type),\n+        result = check(tree, new WildcardType(chk.checkRefType(tree.pos(), type, false),\n@@ -5310,0 +5408,5 @@\n+            if (allowPrimitiveClasses && c.type.isPrimitiveClass()) {\n+                final Env<AttrContext> env = typeEnvs.get(c);\n+                if (env != null && env.tree != null && env.tree.hasTag(CLASSDEF))\n+                    chk.checkNonCyclicMembership((JCClassDecl)env.tree);\n+            }\n@@ -5430,1 +5533,1 @@\n-            } else {\n+            } else if ((c.flags_field & Flags.COMPOUND) == 0) {\n@@ -5485,0 +5588,5 @@\n+                if (c.isValueClass()) {\n+                    Assert.check(env.tree.hasTag(CLASSDEF));\n+                    chk.checkConstraintsOfValueClass(env.tree.pos(), c);\n+                }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":142,"deletions":34,"binary":false,"changes":176,"status":"modified"},{"patch":"@@ -172,0 +172,1 @@\n+        allowPrimitiveClasses = Feature.PRIMITIVE_CLASSES.allowedInSource(source) && options.isSet(\"enablePrimitiveClasses\");\n@@ -215,0 +216,4 @@\n+    \/** Are primitive classes allowed\n+     *\/\n+    private final boolean allowPrimitiveClasses;\n+\n@@ -617,0 +622,5 @@\n+        } else {\n+            if (allowPrimitiveClasses && found.hasTag(CLASS)) {\n+                if (inferenceContext != infer.emptyContext)\n+                    checkParameterizationByPrimitiveClass(pos, found);\n+            }\n@@ -747,0 +757,51 @@\n+    void checkConstraintsOfValueClass(DiagnosticPosition pos, ClassSymbol c) {\n+        for (Type st : types.closure(c.type)) {\n+            if (st == null || st.tsym == null || st.tsym.kind == ERR)\n+                continue;\n+            if  (st.tsym == syms.objectType.tsym || st.tsym == syms.recordType.tsym || st.isInterface())\n+                continue;\n+            if (!st.tsym.isAbstract()) {\n+                if (c != st.tsym) {\n+                    log.error(pos, Errors.ConcreteSupertypeForValueClass(c, st));\n+                }\n+                continue;\n+            }\n+            \/\/ dealing with an abstract value or value super class below.\n+            Fragment fragment = c.isAbstract() && c.isValueClass() && c == st.tsym ? Fragments.AbstractValueClass(c) : Fragments.SuperclassOfValueClass(c, st);\n+            if ((st.tsym.flags() & HASINITBLOCK) != 0) {\n+                log.error(pos, Errors.AbstractValueClassDeclaresInitBlock(fragment));\n+            }\n+            Type encl = st.getEnclosingType();\n+            if (encl != null && encl.hasTag(CLASS)) {\n+                log.error(pos, Errors.AbstractValueClassCannotBeInner(fragment));\n+            }\n+            for (Symbol s : st.tsym.members().getSymbols(NON_RECURSIVE)) {\n+                switch (s.kind) {\n+                case VAR:\n+                    if ((s.flags() & STATIC) == 0) {\n+                        log.error(pos, Errors.InstanceFieldNotAllowed(s, fragment));\n+                    }\n+                    break;\n+                case MTH:\n+                    if ((s.flags() & (SYNCHRONIZED | STATIC)) == SYNCHRONIZED) {\n+                        log.error(pos, Errors.SuperClassMethodCannotBeSynchronized(s, c, st));\n+                    } else if (s.isInitOrVNew()) {\n+                        MethodSymbol m = (MethodSymbol)s;\n+                        if (m.getParameters().size() > 0) {\n+                            log.error(pos, Errors.AbstractValueClassConstructorCannotTakeArguments(m, fragment));\n+                        } else if (m.getTypeParameters().size() > 0) {\n+                            log.error(pos, Errors.AbstractValueClassConstructorCannotBeGeneric(m, fragment));\n+                        } else if (m.type.getThrownTypes().size() > 0) {\n+                            log.error(pos, Errors.AbstractValueClassConstructorCannotThrow(m, fragment));\n+                        } else if (protection(m.flags()) > protection(m.owner.flags())) {\n+                            log.error(pos, Errors.AbstractValueClassConstructorHasWeakerAccess(m, fragment));\n+                        } else if ((m.flags() & EMPTYNOARGCONSTR) == 0) {\n+                                log.error(pos, Errors.AbstractValueClassNoArgConstructorMustBeEmpty(m, fragment));\n+                        }\n+                    }\n+                    break;\n+                }\n+            }\n+        }\n+    }\n+\n@@ -749,2 +810,2 @@\n-    Type checkConstructorRefType(DiagnosticPosition pos, Type t) {\n-        t = checkClassOrArrayType(pos, t);\n+    Type checkConstructorRefType(JCExpression expr, Type t) {\n+        t = checkClassOrArrayType(expr, t);\n@@ -753,1 +814,1 @@\n-                log.error(pos, Errors.AbstractCantBeInstantiated(t.tsym));\n+                log.error(expr, Errors.AbstractCantBeInstantiated(t.tsym));\n@@ -756,1 +817,1 @@\n-                log.error(pos, Errors.EnumCantBeInstantiated);\n+                log.error(expr, Errors.EnumCantBeInstantiated);\n@@ -759,1 +820,10 @@\n-                t = checkClassType(pos, t, true);\n+                \/\/ Projection types may not be mentioned in constructor references\n+                if (expr.hasTag(SELECT)) {\n+                    JCFieldAccess fieldAccess = (JCFieldAccess) expr;\n+                    if (allowPrimitiveClasses && fieldAccess.selected.type.isPrimitiveClass() &&\n+                            (fieldAccess.name == names.ref || fieldAccess.name == names.val)) {\n+                        log.error(expr, Errors.ProjectionCantBeInstantiated);\n+                        t = types.createErrorType(t);\n+                    }\n+                }\n+                t = checkClassType(expr, t, true);\n@@ -763,1 +833,1 @@\n-                log.error(pos, Errors.GenericArrayCreation);\n+                log.error(expr, Errors.GenericArrayCreation);\n@@ -794,0 +864,1 @@\n+     *  @param primitiveClassOK       If false, a primitive class does not qualify\n@@ -795,2 +866,2 @@\n-    Type checkRefType(DiagnosticPosition pos, Type t) {\n-        if (t.isReference())\n+    Type checkRefType(DiagnosticPosition pos, Type t, boolean primitiveClassOK) {\n+        if (t.isReference() && (!allowPrimitiveClasses || primitiveClassOK || !t.isPrimitiveClass()))\n@@ -804,0 +875,31 @@\n+    \/** Check that type is an identity type, i.e. not a primitive\/value type\n+     *  nor its reference projection. When not discernible statically,\n+     *  give it the benefit of doubt and defer to runtime.\n+     *\n+     *  @param pos           Position to be used for error reporting.\n+     *  @param t             The type to be checked.\n+     *\/\n+    void checkIdentityType(DiagnosticPosition pos, Type t) {\n+        if (t.hasTag(TYPEVAR)) {\n+            t = types.skipTypeVars(t, false);\n+        }\n+        if (t.isIntersection()) {\n+            IntersectionClassType ict = (IntersectionClassType)t;\n+            for (Type component : ict.getExplicitComponents()) {\n+                checkIdentityType(pos, component);\n+            }\n+            return;\n+        }\n+        if (t.isPrimitive() || t.isValueClass() || t.isValueInterface() || t.isReferenceProjection())\n+            typeTagError(pos, diags.fragment(Fragments.TypeReqIdentity), t);\n+    }\n+\n+    \/** Check that type is a reference type, i.e. a class, interface or array type\n+     *  or a type variable.\n+     *  @param pos           Position to be used for error reporting.\n+     *  @param t             The type to be checked.\n+     *\/\n+    Type checkRefType(DiagnosticPosition pos, Type t) {\n+        return checkRefType(pos, t, true);\n+    }\n+\n@@ -812,1 +914,1 @@\n-            l.head = checkRefType(tl.head.pos(), l.head);\n+            l.head = checkRefType(tl.head.pos(), l.head, false);\n@@ -848,0 +950,49 @@\n+    void checkParameterizationByPrimitiveClass(DiagnosticPosition pos, Type t) {\n+        parameterizationByPrimitiveClassChecker.visit(t, pos);\n+    }\n+\n+    \/** parameterizationByPrimitiveClassChecker: A type visitor that descends down the given type looking for instances of primitive classes\n+     *  being used as type arguments and issues error against those usages.\n+     *\/\n+    private final Types.SimpleVisitor<Void, DiagnosticPosition> parameterizationByPrimitiveClassChecker =\n+            new Types.SimpleVisitor<Void, DiagnosticPosition>() {\n+\n+        @Override\n+        public Void visitType(Type t, DiagnosticPosition pos) {\n+            return null;\n+        }\n+\n+        @Override\n+        public Void visitClassType(ClassType t, DiagnosticPosition pos) {\n+            for (Type targ : t.allparams()) {\n+                if (allowPrimitiveClasses && targ.isPrimitiveClass()) {\n+                    log.error(pos, Errors.GenericParameterizationWithPrimitiveClass(t));\n+                }\n+                visit(targ, pos);\n+            }\n+            return null;\n+        }\n+\n+        @Override\n+        public Void visitTypeVar(TypeVar t, DiagnosticPosition pos) {\n+             return null;\n+        }\n+\n+        @Override\n+        public Void visitCapturedType(CapturedType t, DiagnosticPosition pos) {\n+            return null;\n+        }\n+\n+        @Override\n+        public Void visitArrayType(ArrayType t, DiagnosticPosition pos) {\n+            return visit(t.elemtype, pos);\n+        }\n+\n+        @Override\n+        public Void visitWildcardType(WildcardType t, DiagnosticPosition pos) {\n+            return visit(t.type, pos);\n+        }\n+    };\n+\n+\n+\n@@ -980,1 +1131,1 @@\n-                (s.isConstructor() ||\n+                (s.isInitOrVNew() ||\n@@ -996,1 +1147,5 @@\n-        return types.upward(t, types.captures(t)).baseType();\n+        Type varType = types.upward(t, types.captures(t)).baseType();\n+        if (allowPrimitiveClasses && varType.hasTag(CLASS)) {\n+            checkParameterizationByPrimitiveClass(pos, varType);\n+        }\n+        return varType;\n@@ -1019,0 +1174,1 @@\n+        \/\/ TODO - is enum so <init>\n@@ -1194,1 +1350,1 @@\n-            else\n+            else {\n@@ -1196,0 +1352,4 @@\n+                if (sym.owner.type.isValueClass() && (flags & STATIC) == 0) {\n+                    implicit |= FINAL;\n+                }\n+            }\n@@ -1198,1 +1358,1 @@\n-            if (sym.name == names.init) {\n+            if (names.isInitOrVNew(sym.name)) {\n@@ -1221,1 +1381,2 @@\n-                mask = RecordMethodFlags;\n+                mask = ((sym.owner.flags_field & VALUE_CLASS) != 0 && (flags & Flags.STATIC) == 0) ?\n+                        RecordMethodFlags & ~SYNCHRONIZED : RecordMethodFlags;\n@@ -1223,1 +1384,3 @@\n-                mask = MethodFlags;\n+                \/\/ value objects do not have an associated monitor\/lock\n+                mask = ((sym.owner.flags_field & VALUE_CLASS) != 0 && (flags & Flags.STATIC) == 0) ?\n+                        MethodFlags & ~SYNCHRONIZED : MethodFlags;\n@@ -1240,1 +1403,1 @@\n-                mask = staticOrImplicitlyStatic && allowRecords && (flags & ANNOTATION) == 0 ? StaticLocalFlags : LocalClassFlags;\n+                mask = staticOrImplicitlyStatic && allowRecords && (flags & ANNOTATION) == 0 ? ExtendedStaticLocalClassFlags : ExtendedLocalClassFlags;\n@@ -1260,2 +1423,2 @@\n-                \/\/ enums can't be declared abstract, final, sealed or non-sealed\n-                mask &= ~(ABSTRACT | FINAL | SEALED | NON_SEALED);\n+                \/\/ enums can't be declared abstract, final, sealed or non-sealed or primitive\/value\n+                mask &= ~(ABSTRACT | FINAL | SEALED | NON_SEALED | PRIMITIVE_CLASS | VALUE_CLASS);\n@@ -1274,0 +1437,17 @@\n+\n+            \/\/ primitive classes are implicitly final value classes.\n+            if ((flags & PRIMITIVE_CLASS) != 0)\n+                implicit |= VALUE_CLASS | FINAL;\n+\n+            \/\/ concrete value classes are implicitly final\n+            if ((flags & (ABSTRACT | INTERFACE | VALUE_CLASS)) == VALUE_CLASS) {\n+                implicit |= FINAL;\n+                if ((flags & NON_SEALED) != 0) {\n+                    \/\/ cant declare a final value class non-sealed\n+                    log.error(pos,\n+                            Errors.ModNotAllowedHere(asFlagSet(NON_SEALED)));\n+                }\n+            }\n+\n+            \/\/ TYPs can't be declared synchronized\n+            mask &= ~SYNCHRONIZED;\n@@ -1302,1 +1482,5 @@\n-                               FINAL | NATIVE | SYNCHRONIZED)\n+                               FINAL | NATIVE | SYNCHRONIZED | PRIMITIVE_CLASS)\n+                 &&\n+                 checkDisjoint(pos, flags,\n+                        IDENTITY_TYPE,\n+                        PRIMITIVE_CLASS | VALUE_CLASS)\n@@ -1312,1 +1496,1 @@\n-                 checkDisjoint(pos, flags,\n+                 checkDisjoint(pos, (flags | implicit), \/\/ complain against volatile & implcitly final entities too.\n@@ -1328,1 +1512,7 @@\n-                                ANNOTATION)) {\n+                                ANNOTATION)\n+                 && checkDisjoint(pos, flags,\n+                                IDENTITY_TYPE,\n+                                ANNOTATION)\n+                && checkDisjoint(pos, flags,\n+                                VALUE_CLASS,\n+                                ANNOTATION) ) {\n@@ -1501,1 +1691,2 @@\n-                tree.selected.type.isParameterized()) {\n+                tree.selected.type.isParameterized() &&\n+                    (tree.name != names.ref || !tree.type.isReferenceProjection())) {\n@@ -1505,0 +1696,2 @@\n+                \/\/ Tolerate the pseudo-select V.ref: V<T>.ref will be static if V<T> is and\n+                \/\/ should not be confused as selecting a static member of a parameterized type.\n@@ -1568,1 +1761,1 @@\n-                    env.enclMethod != null && env.enclMethod.name == names.init;\n+                    env.enclMethod != null && names.isInitOrVNew(env.enclMethod.name);\n@@ -2169,1 +2362,1 @@\n-                (env.info.isAnonymousDiamond && !m.isConstructor() && !m.isPrivate());\n+                (env.info.isAnonymousDiamond && !m.isInitOrVNew() && !m.isPrivate());\n@@ -2295,0 +2488,39 @@\n+    \/\/ A primitive class cannot contain a field of its own type either or indirectly.\n+    void checkNonCyclicMembership(JCClassDecl tree) {\n+        if (allowPrimitiveClasses) {\n+            Assert.check((tree.sym.flags_field & LOCKED) == 0);\n+            try {\n+                tree.sym.flags_field |= LOCKED;\n+                for (List<? extends JCTree> l = tree.defs; l.nonEmpty(); l = l.tail) {\n+                    if (l.head.hasTag(VARDEF)) {\n+                        JCVariableDecl field = (JCVariableDecl) l.head;\n+                        if (cyclePossible(field.sym)) {\n+                            checkNonCyclicMembership((ClassSymbol) field.type.tsym, field.pos());\n+                        }\n+                    }\n+                }\n+            } finally {\n+                tree.sym.flags_field &= ~LOCKED;\n+            }\n+        }\n+    }\n+    \/\/ where\n+    private void checkNonCyclicMembership(ClassSymbol c, DiagnosticPosition pos) {\n+        if ((c.flags_field & LOCKED) != 0) {\n+            log.error(pos, Errors.CyclicPrimitiveClassMembership(c));\n+            return;\n+        }\n+        try {\n+            c.flags_field |= LOCKED;\n+            for (Symbol fld : c.members().getSymbols(s -> s.kind == VAR && cyclePossible((VarSymbol) s), NON_RECURSIVE)) {\n+                checkNonCyclicMembership((ClassSymbol) fld.type.tsym, pos);\n+            }\n+        } finally {\n+            c.flags_field &= ~LOCKED;\n+        }\n+    }\n+        \/\/ where\n+        private boolean cyclePossible(VarSymbol symbol) {\n+            return (symbol.flags() & STATIC) == 0 && allowPrimitiveClasses && symbol.type.isPrimitiveClass();\n+        }\n+\n@@ -2543,0 +2775,22 @@\n+\n+        boolean cIsValue = (c.tsym.flags() & VALUE_CLASS) != 0;\n+        boolean cHasIdentity = (c.tsym.flags() & IDENTITY_TYPE) != 0;\n+        Type identitySuper = null, valueSuper = null;\n+        for (Type t : types.closure(c)) {\n+            if (t != c) {\n+                if ((t.tsym.flags() & IDENTITY_TYPE) != 0)\n+                    identitySuper = t;\n+                else if ((t.tsym.flags() & VALUE_CLASS) != 0)\n+                    valueSuper = t;\n+                if (cIsValue &&  identitySuper != null) {\n+                    log.error(pos, Errors.ValueTypeHasIdentitySuperType(c, identitySuper));\n+                    break;\n+                } else if (cHasIdentity &&  valueSuper != null) {\n+                    log.error(pos, Errors.IdentityTypeHasValueSuperType(c, valueSuper));\n+                    break;\n+                } else if (identitySuper != null && valueSuper != null) {\n+                    log.error(pos, Errors.MutuallyIncompatibleSupers(c, identitySuper, valueSuper));\n+                    break;\n+                }\n+            }\n+        }\n@@ -2660,1 +2914,1 @@\n-                     !s.isConstructor();\n+                     !s.isInitOrVNew();\n@@ -2719,1 +2973,1 @@\n-                     !s.isConstructor();\n+                     !s.isInitOrVNew();\n@@ -3422,1 +3676,1 @@\n-                if (s.kind == MTH && !s.isConstructor())\n+                if (s.kind == MTH && !s.isInitOrVNew())\n@@ -3430,1 +3684,1 @@\n-                if (s.kind == MTH && s.isConstructor())\n+                if (s.kind == MTH && s.isInitOrVNew())\n@@ -3449,1 +3703,1 @@\n-                        (s.kind == MTH && !s.isConstructor() &&\n+                        (s.kind == MTH && !s.isInitOrVNew() &&\n@@ -3451,1 +3705,1 @@\n-                        (s.kind == MTH && s.isConstructor())) {\n+                        (s.kind == MTH && s.isInitOrVNew())) {\n@@ -4749,1 +5003,1 @@\n-                    if (sym.isConstructor() &&\n+                    if (sym.isInitOrVNew() &&\n@@ -4777,1 +5031,1 @@\n-                        if (sym.isConstructor()) {\n+                        if (sym.isInitOrVNew()) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Check.java","additions":286,"deletions":32,"binary":false,"changes":318,"status":"modified"},{"patch":"@@ -356,1 +356,2 @@\n-            if ((init = (owner.name == names.init)) || owner.name == names.clinit) {\n+            \/\/ TODO - can <vnew> exist in this context?\n+            if ((init = names.isInitOrVNew(owner.name)) || owner.name == names.clinit) {\n@@ -375,1 +376,1 @@\n-                make.QualIdent(lambdaType.getReturnType().tsym),\n+                make.QualIdent(lambdaType.getReturnType().tsym).setType(lambdaType.getReturnType()),\n@@ -1664,1 +1665,2 @@\n-                for (Symbol s : csym.members_field.getSymbolsByName(names.init)) {\n+                Name constructorName = csym.isConcreteValueClass() ? names.vnew : names.init;\n+                for (Symbol s : csym.members_field.getSymbolsByName(constructorName)) {\n@@ -1768,1 +1770,1 @@\n-                    && sym.name != names.init;\n+                    && !names.isInitOrVNew(sym.name);\n@@ -1866,1 +1868,1 @@\n-                return types.asSuper(tree.target, syms.serializableType.tsym) != null;\n+                return types.asSuper(tree.target.referenceProjectionOrSelf(), syms.serializableType.tsym) != null;\n@@ -1890,0 +1892,2 @@\n+                } else if (methodName.equals(\"<vnew>\")) {\n+                    methodName = \"vnew\";\n@@ -2344,1 +2348,1 @@\n-                          (tree.sym.owner.isDirectlyOrIndirectlyLocal() || tree.sym.owner.isInner()));\n+                          (tree.sym.owner.isDirectlyOrIndirectlyLocal() || tree.sym.owner.isInner() || tree.sym.owner.isValueClass()));\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/LambdaToMethod.java","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+import com.sun.tools.javac.jvm.Target;\n@@ -55,0 +56,1 @@\n+import static com.sun.tools.javac.code.Flags.SYNCHRONIZED;\n@@ -1110,0 +1112,11 @@\n+\n+            if (tree.sym != syms.objectType.tsym && tree.sym != syms.recordType.tsym) {\n+                if ((tree.sym.flags() & (ABSTRACT | INTERFACE | VALUE_CLASS)) == 0) {\n+                    tree.sym.flags_field |= IDENTITY_TYPE;\n+                }\n+                if ((tree.sym.flags() & (ABSTRACT | IDENTITY_TYPE | INTERFACE)) == ABSTRACT) {\n+                    if (abstractClassHasImplicitIdentity(tree)) {\n+                        tree.sym.flags_field |= IDENTITY_TYPE;\n+                    }\n+                }\n+            }\n@@ -1112,0 +1125,44 @@\n+            \/\/ where\n+            private boolean abstractClassHasImplicitIdentity(JCClassDecl tree) {\n+\n+                Type t = tree.sym.type;\n+\n+                if (t == null || t.tsym == null || t.tsym.kind == ERR)\n+                    return false;\n+\n+                if ((t.tsym.flags() & HASINITBLOCK) != 0) {\n+                    return true;\n+                }\n+\n+                \/\/ No instance fields and no arged constructors both mean inner classes cannot be value class supers.\n+                Type encl = t.getEnclosingType();\n+                if (encl != null && encl.hasTag(CLASS)) {\n+                    return true;\n+                }\n+                for (Symbol s : t.tsym.members().getSymbols(NON_RECURSIVE)) {\n+                    switch (s.kind) {\n+                        case VAR:\n+                            if ((s.flags() & STATIC) == 0) {\n+                                return true;\n+                            }\n+                            break;\n+                        case MTH:\n+                            if ((s.flags() & (SYNCHRONIZED | STATIC)) == SYNCHRONIZED) {\n+                                return true;\n+                            } else if (s.isInitOrVNew()) {\n+                                MethodSymbol m = (MethodSymbol)s;\n+                                if (m.getParameters().size() > 0\n+                                        || m.getTypeParameters().size() > 0\n+                                        || m.type.getThrownTypes().nonEmpty()\n+                                        || (m.flags() & EMPTYNOARGCONSTR) == 0\n+                                        || (Check.protection(m.flags()) > Check.protection(m.owner.flags()))) {\n+                                    return true;\n+                                }\n+                            }\n+                            break;\n+                    }\n+                }\n+                return false;\n+            }\n+\n+\n@@ -1305,1 +1362,2 @@\n-                constructorSymbol = new MethodSymbol(flags, names.init,\n+                Name constructorName = owner().isConcreteValueClass() ? names.vnew : names.init;\n+                constructorSymbol = new MethodSymbol(flags, constructorName,\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/TypeEnter.java","additions":59,"deletions":1,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+    private static final Object[] NO_STATIC_ARGS = new Object[0];\n@@ -81,0 +82,1 @@\n+    private final TransValues transValues;\n@@ -117,0 +119,1 @@\n+        transValues = TransValues.instance(context);\n@@ -134,0 +137,2 @@\n+        Source source = Source.instance(context);\n+        allowPrimitiveClasses = Source.Feature.PRIMITIVE_CLASSES.allowedInSource(source) && options.isSet(\"enablePrimitiveClasses\");\n@@ -178,0 +183,2 @@\n+    boolean allowPrimitiveClasses;\n+\n@@ -267,0 +274,14 @@\n+    \/** Insert a reference to given type in the constant pool,\n+     *  checking for an array with too many dimensions;\n+     *  return the reference's index.\n+     *  @param type   The type for which a reference is inserted.\n+     *\/\n+    int makeRef(DiagnosticPosition pos, Type type, boolean emitQtype) {\n+        checkDimension(pos, type);\n+        if (emitQtype) {\n+            return poolWriter.putClass(new ConstantPoolQType(type, types));\n+        } else {\n+            return poolWriter.putClass(type);\n+        }\n+    }\n+\n@@ -273,1 +294,1 @@\n-        return poolWriter.putClass(checkDimension(pos, type));\n+        return makeRef(pos, type, false);\n@@ -327,1 +348,1 @@\n-        else items.makeMemberItem(msym, name == names.init).invoke();\n+        else items.makeMemberItem(msym, names.isInitOrVNew(name)).invoke();\n@@ -550,1 +571,1 @@\n-        if (md.name == names.init && TreeInfo.isInitialConstructor(md)) {\n+        if (names.isInitOrVNew(md.name) && TreeInfo.isInitialConstructor(md)) {\n@@ -953,1 +974,1 @@\n-            if (meth.isConstructor()) {\n+            if (meth.isInitOrVNew()) {\n@@ -993,0 +1014,3 @@\n+                    } else if (env.enclMethod.sym.isValueObjectFactory()) {\n+                        items.makeLocalItem(env.enclMethod.factoryProduct).load();\n+                        code.emitop0(areturn);\n@@ -1047,1 +1071,2 @@\n-                                        poolWriter);\n+                                        poolWriter,\n+                                        allowPrimitiveClasses);\n@@ -1057,1 +1082,1 @@\n-                if (meth.isConstructor() && selfType != syms.objectType)\n+                if (meth.isInitOrVNew() && selfType != syms.objectType)\n@@ -1096,0 +1121,4 @@\n+        Type localType = v.erasure(types);\n+        if (localType.requiresPreload(env.enclClass.sym)) {\n+            poolWriter.enterPreloadClass((ClassSymbol) localType.tsym);\n+        }\n@@ -1144,0 +1173,31 @@\n+    public void visitWithField(JCWithField tree) {\n+        switch(tree.field.getTag()) {\n+            case IDENT:\n+                Symbol sym = ((JCIdent) tree.field).sym;\n+                items.makeThisItem().load();\n+                genExpr(tree.value, tree.field.type).load();\n+                sym = binaryQualifier(sym, env.enclClass.type);\n+                code.emitop2(withfield, sym, PoolWriter::putMember);\n+                result = items.makeStackItem(tree.type);\n+                break;\n+            case SELECT:\n+                JCFieldAccess fieldAccess = (JCFieldAccess) tree.field;\n+                sym = TreeInfo.symbol(fieldAccess);\n+                \/\/ JDK-8207332: To maintain the order of side effects, must compute value ahead of field\n+                genExpr(tree.value, tree.field.type).load();\n+                genExpr(fieldAccess.selected, fieldAccess.selected.type).load();\n+                if (Code.width(tree.field.type) == 2) {\n+                    code.emitop0(dup_x2);\n+                    code.emitop0(pop);\n+                } else {\n+                    code.emitop0(swap);\n+                }\n+                sym = binaryQualifier(sym, fieldAccess.selected.type);\n+                code.emitop2(withfield, sym, PoolWriter::putMember);\n+                result = items.makeStackItem(tree.type);\n+                break;\n+            default:\n+                Assert.check(false);\n+        }\n+    }\n+\n@@ -2059,1 +2119,1 @@\n-                code.emitAnewarray(makeRef(pos, elemtype), type);\n+                code.emitAnewarray(makeRef(pos, elemtype, elemtype.isPrimitiveClass()), type);\n@@ -2285,0 +2345,1 @@\n+        \/\/ primitive reference conversion is a nop when we bifurcate the primitive class, as the VM sees a subtyping relationship.\n@@ -2287,2 +2348,9 @@\n-           types.asSuper(tree.expr.type, tree.clazz.type.tsym) == null) {\n-            code.emitop2(checkcast, checkDimension(tree.pos(), tree.clazz.type), PoolWriter::putClass);\n+            (!tree.clazz.type.isReferenceProjection() || !types.isSameType(tree.clazz.type.valueProjection(), tree.expr.type) || true) &&\n+           !types.isSubtype(tree.expr.type, tree.clazz.type)) {\n+            checkDimension(tree.pos(), tree.clazz.type);\n+            if (tree.clazz.type.isPrimitiveClass()) {\n+                code.emitop2(checkcast, new ConstantPoolQType(tree.clazz.type, types), PoolWriter::putClass);\n+            } else {\n+                code.emitop2(checkcast, tree.clazz.type, PoolWriter::putClass);\n+            }\n+\n@@ -2350,1 +2418,1 @@\n-            code.emitLdc((LoadableConstant)checkDimension(tree.pos(), tree.selected.type));\n+            code.emitLdc((LoadableConstant) tree.selected.type, makeRef(tree.pos(), tree.selected.type, tree.selected.type.isPrimitiveClass()));\n@@ -2353,1 +2421,1 @@\n-       }\n+        }\n@@ -2409,0 +2477,12 @@\n+    public void visitDefaultValue(JCDefaultValue tree) {\n+        if (tree.type.isValueClass()) {\n+            code.emitop2(aconst_init, checkDimension(tree.pos(), tree.type), PoolWriter::putClass);\n+        } else if (tree.type.isReference()) {\n+            code.emitop0(aconst_null);\n+        } else {\n+            code.emitop0(zero(Code.typecode(tree.type)));\n+        }\n+        result = items.makeStackItem(tree.type);\n+        return;\n+    }\n+\n@@ -2465,0 +2545,1 @@\n+            cdef = transValues.translateTopLevelClass(cdef, make);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/Gen.java","additions":92,"deletions":11,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -279,1 +279,1 @@\n-            return (DeclaredType) new Type.ClassType(outer, targs.toList(), sym);\n+            return (DeclaredType) new Type.ClassType(outer, targs.toList(), sym, TypeMetadata.EMPTY, sym.type.getFlavor());\n@@ -300,1 +300,1 @@\n-        if (types.asSuper(site, sym.getEnclosingElement()) == null)\n+        if (types.asSuper(site.referenceProjectionOrSelf(), sym.getEnclosingElement()) == null)\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/model\/JavacTypes.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+import com.sun.tools.javac.code.Flags.Flag;\n@@ -52,0 +53,1 @@\n+import static com.sun.tools.javac.code.Flags.asFlagSet;\n@@ -60,0 +62,1 @@\n+import static com.sun.tools.javac.parser.Tokens.TokenKind.SYNCHRONIZED;\n@@ -192,0 +195,2 @@\n+        this.allowPrimitiveClasses = Feature.PRIMITIVE_CLASSES.allowedInSource(source) && fac.options.isSet(\"enablePrimitiveClasses\");\n+        this.allowValueClasses = Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -229,0 +234,8 @@\n+    \/** Switch: are primitive classes allowed in this source level?\n+     *\/\n+     boolean allowPrimitiveClasses;\n+\n+    \/** Switch: are value classes allowed in this source level?\n+     *\/\n+    boolean allowValueClasses;\n+\n@@ -492,0 +505,16 @@\n+    \/** If next input token matches one of the two given tokens, skip it, otherwise report\n+     *  an error.\n+     *\n+     * @return The actual token kind.\n+     *\/\n+    public TokenKind accept2(TokenKind tk1, TokenKind tk2) {\n+        TokenKind returnValue = token.kind;\n+        if (token.kind == tk1 || token.kind == tk2) {\n+            nextToken();\n+        } else {\n+            setErrorEndPos(token.pos);\n+            reportSyntaxError(S.prevToken().endPos, Errors.Expected2(tk1, tk2));\n+        }\n+        return returnValue;\n+    }\n+\n@@ -1384,0 +1413,6 @@\n+                            case DEFAULT:\n+                                if (typeArgs != null) return illegal();\n+                                selectExprMode();\n+                                t = to(F.at(pos).DefaultValue(t));\n+                                nextToken();\n+                                break loop;\n@@ -1441,3 +1476,4 @@\n-                        if (!isMode(TYPE) && isUnboundMemberRef()) {\n-                            \/\/this is an unbound method reference whose qualifier\n-                            \/\/is a generic type i.e. A<S>::m\n+                        if (!isMode(TYPE) && isParameterizedTypePrefix()) {\n+                            \/\/this is either an unbound method reference whose qualifier\n+                            \/\/is a generic type i.e. A<S>::m or a default value creation of\n+                            \/\/the form ValueType<S>.default\n@@ -1456,0 +1492,6 @@\n+                                if (token.kind == DEFAULT) {\n+                                    t =  toP(F.at(token.pos).DefaultValue(t));\n+                                    nextToken();\n+                                    selectExprMode();\n+                                    return term3Rest(t, typeArgs);\n+                                }\n@@ -1679,1 +1721,2 @@\n-     * method reference or a binary expression. To disambiguate, look for a\n+     * method reference or a default value creation that uses a parameterized type\n+     * or a binary expression. To disambiguate, look for a\n@@ -1683,1 +1726,1 @@\n-    boolean isUnboundMemberRef() {\n+    boolean isParameterizedTypePrefix() {\n@@ -2281,1 +2324,1 @@\n-            accept(CLASS);\n+            TokenKind selector = accept2(CLASS, DEFAULT);\n@@ -2299,1 +2342,5 @@\n-                t = toP(F.at(pos).Select(t, names._class));\n+                if (selector == CLASS) {\n+                    t = toP(F.at(pos).Select(t, names._class));\n+                } else {\n+                    t = toP(F.at(pos).DefaultValue(t));\n+                }\n@@ -2331,0 +2378,1 @@\n+            \/\/ TODO - will be converted in Attr\n@@ -2343,2 +2391,2 @@\n-        List<JCAnnotation> newAnnotations = typeAnnotationsOpt();\n-\n+        final JCModifiers mods = modifiersOpt();\n+        List<JCAnnotation> newAnnotations = mods.annotations;\n@@ -2348,0 +2396,3 @@\n+            if (mods.flags != 0) {\n+                log.error(token.pos, Errors.ModNotAllowedHere(asFlagSet(mods.flags)));\n+            }\n@@ -2416,0 +2467,3 @@\n+            long badModifiers = mods.flags & ~(Flags.PRIMITIVE_CLASS | Flags.VALUE_CLASS | Flags.FINAL);\n+            if (badModifiers != 0)\n+                log.error(token.pos, Errors.ModNotAllowedHere(asFlagSet(badModifiers)));\n@@ -2420,1 +2474,5 @@\n-            return classCreatorRest(newpos, null, typeArgs, t);\n+            JCNewClass newClass = classCreatorRest(newpos, null, typeArgs, t, mods.flags);\n+            if ((newClass.def == null) && (mods.flags != 0)) {\n+                log.error(newClass.pos, Errors.ModNotAllowedHere(asFlagSet(mods.flags)));\n+            }\n+            return newClass;\n@@ -2445,1 +2503,1 @@\n-        return classCreatorRest(newpos, encl, typeArgs, t);\n+        return classCreatorRest(newpos, encl, typeArgs, t, 0);\n@@ -2523,1 +2581,2 @@\n-                                  JCExpression t)\n+                                  JCExpression t,\n+                                  long flags)\n@@ -2530,1 +2589,1 @@\n-            JCModifiers mods = F.at(Position.NOPOS).Modifiers(0);\n+            JCModifiers mods = F.at(Position.NOPOS).Modifiers(flags);\n@@ -2533,1 +2592,2 @@\n-        return toP(F.at(newpos).NewClass(encl, typeArgs, t, args, body));\n+        JCNewClass newClass = toP(F.at(newpos).NewClass(encl, typeArgs, t, args, body));\n+        return newClass;\n@@ -2766,0 +2826,4 @@\n+        if ((isPrimitiveModifier() && allowPrimitiveClasses) || (isValueModifier() || isIdentityModifier()) && allowValueClasses) {\n+            dc = token.comment(CommentStyle.JAVADOC);\n+            return List.of(classOrRecordOrInterfaceOrEnumDeclaration(modifiersOpt(), dc));\n+        }\n@@ -3339,1 +3403,4 @@\n-                return variableDeclarators(modifiersOpt(), t, stats, true).toList();\n+                pos = token.pos;\n+                JCModifiers mods = F.at(Position.NOPOS).Modifiers(0);\n+                F.at(pos);\n+                return variableDeclarators(mods, t, stats, true).toList();\n@@ -3436,0 +3503,12 @@\n+                if (isPrimitiveModifier()) {\n+                    flag = Flags.PRIMITIVE_CLASS;\n+                    break;\n+                }\n+                if (isValueModifier()) {\n+                    flag = Flags.VALUE_CLASS;\n+                    break;\n+                }\n+                if (isIdentityModifier()) {\n+                    flag = Flags.IDENTITY_TYPE;\n+                    break;\n+                }\n@@ -3687,0 +3766,19 @@\n+        if (name == names.primitive) {\n+            if (allowPrimitiveClasses) {\n+                return Source.JDK18;\n+            }\n+        }\n+        if (name == names.value) {\n+            if (allowValueClasses) {\n+                return Source.JDK18;\n+            } else if (shouldWarn) {\n+                log.warning(pos, Warnings.RestrictedTypeNotAllowedPreview(name, Source.JDK18));\n+            }\n+        }\n+        if (name == names.identity) {\n+            if (allowPrimitiveClasses) {\n+                return Source.JDK18;\n+            } else if (shouldWarn) {\n+                log.warning(pos, Warnings.RestrictedTypeNotAllowedPreview(name, Source.JDK18));\n+            }\n+        }\n@@ -4129,1 +4227,2 @@\n-                if (methDef.name == names.init && methDef.params.isEmpty() && (methDef.mods.flags & Flags.COMPACT_RECORD_CONSTRUCTOR) != 0) {\n+                \/\/ TODO - specifically for record.\n+                if (names.isInitOrVNew(methDef.name) && methDef.params.isEmpty() && (methDef.mods.flags & Flags.COMPACT_RECORD_CONSTRUCTOR) != 0) {\n@@ -4581,0 +4680,78 @@\n+    protected boolean isPrimitiveModifier() {\n+        if (token.kind == IDENTIFIER && token.name() == names.primitive) {\n+            boolean isPrimitiveModifier = false;\n+            Token next = S.token(1);\n+            switch (next.kind) {\n+                case PRIVATE: case PROTECTED: case PUBLIC: case STATIC: case TRANSIENT:\n+                case FINAL: case ABSTRACT: case NATIVE: case VOLATILE: case SYNCHRONIZED:\n+                case STRICTFP: case MONKEYS_AT: case DEFAULT: case BYTE: case SHORT:\n+                case CHAR: case INT: case LONG: case FLOAT: case DOUBLE: case BOOLEAN: case VOID:\n+                case CLASS: case INTERFACE: case ENUM:\n+                    isPrimitiveModifier = true;\n+                    break;\n+                case IDENTIFIER: \/\/ primitive record R || primitive primitive || primitive identity || primitive value || new primitive Comparable() {}\n+                    if (next.name() == names.record || next.name() == names.primitive || next.name() == names.identity\n+                            || next.name() == names.value || (mode & EXPR) != 0)\n+                        isPrimitiveModifier = true;\n+                    break;\n+            }\n+            if (isPrimitiveModifier) {\n+                checkSourceLevel(Feature.PRIMITIVE_CLASSES);\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n+    protected boolean isValueModifier() {\n+        if (token.kind == IDENTIFIER && token.name() == names.value) {\n+            boolean isValueModifier = false;\n+            Token next = S.token(1);\n+            switch (next.kind) {\n+                case PRIVATE: case PROTECTED: case PUBLIC: case STATIC: case TRANSIENT:\n+                case FINAL: case ABSTRACT: case NATIVE: case VOLATILE: case SYNCHRONIZED:\n+                case STRICTFP: case MONKEYS_AT: case DEFAULT: case BYTE: case SHORT:\n+                case CHAR: case INT: case LONG: case FLOAT: case DOUBLE: case BOOLEAN: case VOID:\n+                case CLASS: case INTERFACE: case ENUM:\n+                    isValueModifier = true;\n+                    break;\n+                case IDENTIFIER: \/\/ value record R || value value || value identity || value primitive || new value Comparable() {} ??\n+                    if (next.name() == names.record || next.name() == names.value || next.name() == names.identity\n+                            || next.name() == names.primitive || (mode & EXPR) != 0)\n+                        isValueModifier = true;\n+                    break;\n+            }\n+            if (isValueModifier) {\n+                checkSourceLevel(Feature.VALUE_CLASSES);\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n+    protected boolean isIdentityModifier() {\n+        if (token.kind == IDENTIFIER && token.name() == names.identity) {\n+            boolean isIdentityModifier = false;\n+            Token next = S.token(1);\n+            switch (next.kind) {\n+                case PRIVATE: case PROTECTED: case PUBLIC: case STATIC: case TRANSIENT:\n+                case FINAL: case ABSTRACT: case NATIVE: case VOLATILE: case SYNCHRONIZED:\n+                case STRICTFP: case MONKEYS_AT: case DEFAULT: case BYTE: case SHORT:\n+                case CHAR: case INT: case LONG: case FLOAT: case DOUBLE: case BOOLEAN: case VOID:\n+                case CLASS: case INTERFACE: case ENUM:\n+                    isIdentityModifier = true;\n+                    break;\n+                case IDENTIFIER: \/\/ identity record R || identity primitive || || identity identity || identity value || new identity Comparable() {}\n+                    if (next.name() == names.record || next.name() == names.primitive || next.name() == names.identity\n+                            || next.name() == names.value || (mode & EXPR) != 0)\n+                        isIdentityModifier = true;\n+                    break;\n+            }\n+            if (isIdentityModifier) {\n+                checkSourceLevel(Feature.VALUE_CLASSES);\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n@@ -4608,1 +4785,4 @@\n-                case IDENTIFIER -> isNonSealedIdentifier(next, currentIsNonSealed ? 3 : 1) || next.name() == names.sealed;\n+                case IDENTIFIER -> isNonSealedIdentifier(next, currentIsNonSealed ? 3 : 1) ||\n+                        next.name() == names.sealed ||\n+                        next.name() == names.value ||\n+                        next.name() == names.identity;\n@@ -4639,1 +4819,1 @@\n-            if (!isRecord || name != names.init || token.kind == LPAREN) {\n+            if (!isRecord || !names.isInitOrVNew(name) || token.kind == LPAREN) {\n@@ -5095,1 +5275,4 @@\n-        if (preview.isPreview(feature) && !preview.isEnabled()) {\n+        if (feature == Feature.PRIMITIVE_CLASSES && !allowPrimitiveClasses) {\n+            \/\/ primitive classes are special\n+            log.error(DiagnosticFlag.SOURCE_LEVEL, pos, feature.error(source.name));\n+        } else if (preview.isPreview(feature) && !preview.isEnabled()) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":202,"deletions":19,"binary":false,"changes":221,"status":"modified"},{"patch":"@@ -271,0 +271,6 @@\n+compiler.misc.value.interface.nonfunctional=\\\n+    since it is a value interface\n+\n+compiler.misc.identity.interface.nonfunctional=\\\n+    since it is an identity interface\n+\n@@ -717,1 +723,1 @@\n-    improperly formed type, some parameters are missing\n+    improperly formed type, some parameters are missing or misplaced\n@@ -2604,0 +2610,3 @@\n+compiler.misc.type.req.identity=\\\n+    a type with identity\n+\n@@ -3596,0 +3605,3 @@\n+compiler.misc.bad.access.flags=\\\n+    bad access flags combination: {0}\n+\n@@ -3924,0 +3936,88 @@\n+compiler.misc.feature.primitive.classes=\\\n+    primitive classes\n+\n+compiler.misc.feature.value.classes=\\\n+    value classes\n+\n+# 0: symbol\n+compiler.err.cyclic.primitive.class.membership=\\\n+    cyclic primitive class membership involving {0}\n+\n+# 0: string (expected version)\n+compiler.err.primitive.classes.not.supported=\\\n+    primitive classes are not supported\\n\\\n+     (use -source {0} or higher to enable primitive classes and pass compiler option: -XDenablePrimitiveClasses)\n+\n+compiler.err.this.exposed.prematurely=\\\n+    value class instance should not be passed around before being fully initialized\n+\n+# 0: type\n+compiler.err.generic.parameterization.with.primitive.class=\\\n+    Inferred type {0} involves generic parameterization by a primitive class\n+\n+# 0: type, 1: type\n+compiler.err.value.type.has.identity.super.type=\\\n+    The identity type {1} cannot be a supertype of the value type {0}\n+\n+# 0: type, 1: type\n+compiler.err.identity.type.has.value.super.type=\\\n+    The value type {1} cannot be a supertype of the identity type {0}\n+\n+# 0: type, 1: type, 2: type\n+compiler.err.mutually.incompatible.supers=\\\n+    The type {0} has mutually incompatible supertypes: the identity type {1} and the value type {2}\n+\n+# 0: symbol, 1: type\n+compiler.err.concrete.supertype.for.value.class=\\\n+    The concrete class {1} is not allowed to be a super class of the value class {0} either directly or indirectly\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.class.method.cannot.be.synchronized=\\\n+    The method {0} in the super class {2} of the value class {1} is synchronized. This is disallowed\n+\n+# 0: symbol, 1: message segment\n+compiler.err.abstract.value.class.constructor.cannot.take.arguments=\\\n+    {1} defines a constructor {0} that takes arguments. This is disallowed\n+\n+# 0: symbol, 1: message segment\n+compiler.err.abstract.value.class.constructor.cannot.be.generic=\\\n+    {1} defines a generic constructor {0}. This is disallowed\n+\n+# 0: symbol, 1: message segment\n+compiler.err.abstract.value.class.constructor.cannot.throw=\\\n+    {1} defines a constructor {0} that throws an exception. This is disallowed\n+\n+# 0: symbol, 1: message segment\n+compiler.err.abstract.value.class.constructor.has.weaker.access=\\\n+    {1} defines a constructor {0} with a weaker access privilege than the declaring class. This is disallowed\n+\n+# 0: symbol, 1: message segment\n+compiler.err.instance.field.not.allowed=\\\n+    {1} defines an instance field {0}. This is disallowed\n+\n+# 0: symbol, 1: message segment\n+compiler.err.abstract.value.class.no.arg.constructor.must.be.empty=\\\n+    {1} defines a nonempty no-arg constructor {0}. This is disallowed\n+\n+# 0: message segment\n+compiler.err.abstract.value.class.declares.init.block=\\\n+    {0} declares one or more non-empty instance initializer blocks. This is disallowed.\n+\n+# 0: message segment\n+compiler.err.abstract.value.class.cannot.be.inner=\\\n+    {0} is an inner class. This is disallowed.\n+\n+# 0: symbol, 1: type\n+compiler.misc.superclass.of.value.class=\\\n+    The super class {1} of the value class {0}\n+\n+# 0: symbol\n+compiler.misc.abstract.value.class=\\\n+    The abstract value class {0}\n+\n+compiler.err.projection.cant.be.instantiated=\\\n+    Illegal attempt to instantiate a projection type\n+\n+compiler.err.call.to.super.not.allowed.in.value.ctor=\\\n+    call to super not allowed in value class constructor\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":101,"deletions":1,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -104,1 +104,1 @@\n-    final int instanceKlassMiscFlagsOffset = getFieldOffset(\"InstanceKlass::_misc_flags._flags\", Integer.class, \"u2\");\n+    final int instanceKlassMiscFlagsOffset = getFieldOffset(\"InstanceKlass::_misc_flags._flags\", Integer.class, \"u4\");\n@@ -319,0 +319,2 @@\n+    final int dataLayoutArrayLoadStoreDataTag = getConstant(\"DataLayout::array_load_store_data_tag\", Integer.class);\n+    final int dataLayoutACmpDataTag = getConstant(\"DataLayout::acmp_data_tag\", Integer.class);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -87,0 +87,2 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n+\n@@ -104,0 +106,3 @@\n+# Valhalla\n+runtime\/AccModule\/ConstModule.java 8294051 generic-all\n+\n@@ -125,0 +130,27 @@\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":32,"deletions":0,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-  runtime\n+  runtime \\\n@@ -59,0 +59,8 @@\n+hotspot_valhalla = \\\n+  runtime\/valhalla \\\n+  compiler\/valhalla \\\n+  serviceability\/jvmti\/Valhalla\n+\n+hotspot_valhalla_runtime = \\\n+  runtime\/valhalla\n+\n@@ -147,1 +155,1 @@\n-  compiler\/codegen\/aes \\\n+  compiler\/codegen\/aes \\\n@@ -201,0 +209,1 @@\n+  compiler\/valhalla\/ \\\n@@ -257,0 +266,7 @@\n+tier1_compiler_no_valhalla = \\\n+  :tier1_compiler_1 \\\n+  :tier1_compiler_2 \\\n+  :tier1_compiler_3 \\\n+  :tier1_compiler_not_xcomp \\\n+  -compiler\/valhalla\n+\n@@ -405,0 +421,4 @@\n+tier1_runtime_no_valhalla = \\\n+  :tier1_runtime \\\n+  -runtime\/valhalla\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":22,"deletions":2,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -1354,1 +1354,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -1386,1 +1386,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -552,0 +552,12 @@\n+    \/**\n+     * Checks if deopt of {@code m} is stable at the specified {@code compLevel}.\n+     *\n+     * @param m the method to be checked.\n+     * @param compLevel the compilation level.\n+     * @return {@code true} if deopt of {@code m} is stable at {@code compLevel};\n+     *         {@code false} otherwise.\n+     *\/\n+    public static boolean isStableDeopt(Method m, CompLevel compLevel) {\n+        return TestVM.isStableDeopt(m, compLevel);\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/TestFramework.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -698,0 +698,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n","filename":"test\/jdk\/ProblemList.txt","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -114,0 +114,1 @@\n+    jdk\/modules \\\n@@ -116,1 +117,9 @@\n-    jni\/nullCaller\n+    jni\/nullCaller \\\n+    valhalla\n+\n+# valhalla lworld tests\n+jdk_valhalla = \\\n+    java\/lang\/invoke \\\n+    valhalla \\\n+    java\/lang\/instrument\/valhalla\n+\n","filename":"test\/jdk\/TEST.groups","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -49,0 +51,2 @@\n+    static final Class<?> type = boolean.class;\n+\n@@ -83,1 +87,1 @@\n-                    VarHandleTestAccessBoolean.class, \"final_v\" + postfix, boolean.class);\n+                    VarHandleTestAccessBoolean.class, \"final_v\" + postfix, type);\n@@ -87,1 +91,1 @@\n-                    VarHandleTestAccessBoolean.class, \"v\" + postfix, boolean.class);\n+                    VarHandleTestAccessBoolean.class, \"v\" + postfix, type);\n@@ -91,1 +95,1 @@\n-                VarHandleTestAccessBoolean.class, \"static_final_v\" + postfix, boolean.class);\n+                VarHandleTestAccessBoolean.class, \"static_final_v\" + postfix, type);\n@@ -95,1 +99,1 @@\n-                VarHandleTestAccessBoolean.class, \"static_v\" + postfix, boolean.class);\n+                VarHandleTestAccessBoolean.class, \"static_v\" + postfix, type);\n@@ -114,1 +118,1 @@\n-                VarHandleTestAccessBoolean.class, \"final_v\", boolean.class);\n+                VarHandleTestAccessBoolean.class, \"final_v\", type);\n@@ -117,1 +121,1 @@\n-                VarHandleTestAccessBoolean.class, \"v\", boolean.class);\n+                VarHandleTestAccessBoolean.class, \"v\", type);\n@@ -120,1 +124,1 @@\n-            VarHandleTestAccessBoolean.class, \"static_final_v\", boolean.class);\n+            VarHandleTestAccessBoolean.class, \"static_final_v\", type);\n@@ -123,1 +127,1 @@\n-            VarHandleTestAccessBoolean.class, \"static_v\", boolean.class);\n+            VarHandleTestAccessBoolean.class, \"static_v\", type);\n@@ -210,1 +214,1 @@\n-        assertEquals(vh.varType(), boolean.class);\n+        assertEquals(vh.varType(), type);\n@@ -222,1 +226,1 @@\n-                    VarHandleTestAccessBoolean.class, \"final_v\", boolean.class);\n+                    VarHandleTestAccessBoolean.class, \"final_v\", type);\n@@ -227,1 +231,1 @@\n-                    VarHandleTestAccessBoolean.class, \"v\", boolean.class);\n+                    VarHandleTestAccessBoolean.class, \"v\", type);\n@@ -235,1 +239,1 @@\n-                VarHandleTestAccessBoolean.class, \"static_final_v\", boolean.class);\n+                VarHandleTestAccessBoolean.class, \"static_final_v\", type);\n@@ -240,1 +244,1 @@\n-                VarHandleTestAccessBoolean.class, \"static_v\", boolean.class);\n+                VarHandleTestAccessBoolean.class, \"static_v\", type);\n@@ -355,1 +359,0 @@\n-\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessBoolean.java","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -49,0 +51,2 @@\n+    static final Class<?> type = byte.class;\n+\n@@ -83,1 +87,1 @@\n-                    VarHandleTestAccessByte.class, \"final_v\" + postfix, byte.class);\n+                    VarHandleTestAccessByte.class, \"final_v\" + postfix, type);\n@@ -87,1 +91,1 @@\n-                    VarHandleTestAccessByte.class, \"v\" + postfix, byte.class);\n+                    VarHandleTestAccessByte.class, \"v\" + postfix, type);\n@@ -91,1 +95,1 @@\n-                VarHandleTestAccessByte.class, \"static_final_v\" + postfix, byte.class);\n+                VarHandleTestAccessByte.class, \"static_final_v\" + postfix, type);\n@@ -95,1 +99,1 @@\n-                VarHandleTestAccessByte.class, \"static_v\" + postfix, byte.class);\n+                VarHandleTestAccessByte.class, \"static_v\" + postfix, type);\n@@ -114,1 +118,1 @@\n-                VarHandleTestAccessByte.class, \"final_v\", byte.class);\n+                VarHandleTestAccessByte.class, \"final_v\", type);\n@@ -117,1 +121,1 @@\n-                VarHandleTestAccessByte.class, \"v\", byte.class);\n+                VarHandleTestAccessByte.class, \"v\", type);\n@@ -120,1 +124,1 @@\n-            VarHandleTestAccessByte.class, \"static_final_v\", byte.class);\n+            VarHandleTestAccessByte.class, \"static_final_v\", type);\n@@ -123,1 +127,1 @@\n-            VarHandleTestAccessByte.class, \"static_v\", byte.class);\n+            VarHandleTestAccessByte.class, \"static_v\", type);\n@@ -210,1 +214,1 @@\n-        assertEquals(vh.varType(), byte.class);\n+        assertEquals(vh.varType(), type);\n@@ -222,1 +226,1 @@\n-                    VarHandleTestAccessByte.class, \"final_v\", byte.class);\n+                    VarHandleTestAccessByte.class, \"final_v\", type);\n@@ -227,1 +231,1 @@\n-                    VarHandleTestAccessByte.class, \"v\", byte.class);\n+                    VarHandleTestAccessByte.class, \"v\", type);\n@@ -235,1 +239,1 @@\n-                VarHandleTestAccessByte.class, \"static_final_v\", byte.class);\n+                VarHandleTestAccessByte.class, \"static_final_v\", type);\n@@ -240,1 +244,1 @@\n-                VarHandleTestAccessByte.class, \"static_v\", byte.class);\n+                VarHandleTestAccessByte.class, \"static_v\", type);\n@@ -344,1 +348,0 @@\n-\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessByte.java","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -49,0 +51,2 @@\n+    static final Class<?> type = char.class;\n+\n@@ -83,1 +87,1 @@\n-                    VarHandleTestAccessChar.class, \"final_v\" + postfix, char.class);\n+                    VarHandleTestAccessChar.class, \"final_v\" + postfix, type);\n@@ -87,1 +91,1 @@\n-                    VarHandleTestAccessChar.class, \"v\" + postfix, char.class);\n+                    VarHandleTestAccessChar.class, \"v\" + postfix, type);\n@@ -91,1 +95,1 @@\n-                VarHandleTestAccessChar.class, \"static_final_v\" + postfix, char.class);\n+                VarHandleTestAccessChar.class, \"static_final_v\" + postfix, type);\n@@ -95,1 +99,1 @@\n-                VarHandleTestAccessChar.class, \"static_v\" + postfix, char.class);\n+                VarHandleTestAccessChar.class, \"static_v\" + postfix, type);\n@@ -114,1 +118,1 @@\n-                VarHandleTestAccessChar.class, \"final_v\", char.class);\n+                VarHandleTestAccessChar.class, \"final_v\", type);\n@@ -117,1 +121,1 @@\n-                VarHandleTestAccessChar.class, \"v\", char.class);\n+                VarHandleTestAccessChar.class, \"v\", type);\n@@ -120,1 +124,1 @@\n-            VarHandleTestAccessChar.class, \"static_final_v\", char.class);\n+            VarHandleTestAccessChar.class, \"static_final_v\", type);\n@@ -123,1 +127,1 @@\n-            VarHandleTestAccessChar.class, \"static_v\", char.class);\n+            VarHandleTestAccessChar.class, \"static_v\", type);\n@@ -210,1 +214,1 @@\n-        assertEquals(vh.varType(), char.class);\n+        assertEquals(vh.varType(), type);\n@@ -222,1 +226,1 @@\n-                    VarHandleTestAccessChar.class, \"final_v\", char.class);\n+                    VarHandleTestAccessChar.class, \"final_v\", type);\n@@ -227,1 +231,1 @@\n-                    VarHandleTestAccessChar.class, \"v\", char.class);\n+                    VarHandleTestAccessChar.class, \"v\", type);\n@@ -235,1 +239,1 @@\n-                VarHandleTestAccessChar.class, \"static_final_v\", char.class);\n+                VarHandleTestAccessChar.class, \"static_final_v\", type);\n@@ -240,1 +244,1 @@\n-                VarHandleTestAccessChar.class, \"static_v\", char.class);\n+                VarHandleTestAccessChar.class, \"static_v\", type);\n@@ -344,1 +348,0 @@\n-\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessChar.java","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -49,0 +51,2 @@\n+    static final Class<?> type = double.class;\n+\n@@ -83,1 +87,1 @@\n-                    VarHandleTestAccessDouble.class, \"final_v\" + postfix, double.class);\n+                    VarHandleTestAccessDouble.class, \"final_v\" + postfix, type);\n@@ -87,1 +91,1 @@\n-                    VarHandleTestAccessDouble.class, \"v\" + postfix, double.class);\n+                    VarHandleTestAccessDouble.class, \"v\" + postfix, type);\n@@ -91,1 +95,1 @@\n-                VarHandleTestAccessDouble.class, \"static_final_v\" + postfix, double.class);\n+                VarHandleTestAccessDouble.class, \"static_final_v\" + postfix, type);\n@@ -95,1 +99,1 @@\n-                VarHandleTestAccessDouble.class, \"static_v\" + postfix, double.class);\n+                VarHandleTestAccessDouble.class, \"static_v\" + postfix, type);\n@@ -114,1 +118,1 @@\n-                VarHandleTestAccessDouble.class, \"final_v\", double.class);\n+                VarHandleTestAccessDouble.class, \"final_v\", type);\n@@ -117,1 +121,1 @@\n-                VarHandleTestAccessDouble.class, \"v\", double.class);\n+                VarHandleTestAccessDouble.class, \"v\", type);\n@@ -120,1 +124,1 @@\n-            VarHandleTestAccessDouble.class, \"static_final_v\", double.class);\n+            VarHandleTestAccessDouble.class, \"static_final_v\", type);\n@@ -123,1 +127,1 @@\n-            VarHandleTestAccessDouble.class, \"static_v\", double.class);\n+            VarHandleTestAccessDouble.class, \"static_v\", type);\n@@ -210,1 +214,1 @@\n-        assertEquals(vh.varType(), double.class);\n+        assertEquals(vh.varType(), type);\n@@ -222,1 +226,1 @@\n-                    VarHandleTestAccessDouble.class, \"final_v\", double.class);\n+                    VarHandleTestAccessDouble.class, \"final_v\", type);\n@@ -227,1 +231,1 @@\n-                    VarHandleTestAccessDouble.class, \"v\", double.class);\n+                    VarHandleTestAccessDouble.class, \"v\", type);\n@@ -235,1 +239,1 @@\n-                VarHandleTestAccessDouble.class, \"static_final_v\", double.class);\n+                VarHandleTestAccessDouble.class, \"static_final_v\", type);\n@@ -240,1 +244,1 @@\n-                VarHandleTestAccessDouble.class, \"static_v\", double.class);\n+                VarHandleTestAccessDouble.class, \"static_v\", type);\n@@ -379,1 +383,0 @@\n-\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessDouble.java","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -49,0 +51,2 @@\n+    static final Class<?> type = float.class;\n+\n@@ -83,1 +87,1 @@\n-                    VarHandleTestAccessFloat.class, \"final_v\" + postfix, float.class);\n+                    VarHandleTestAccessFloat.class, \"final_v\" + postfix, type);\n@@ -87,1 +91,1 @@\n-                    VarHandleTestAccessFloat.class, \"v\" + postfix, float.class);\n+                    VarHandleTestAccessFloat.class, \"v\" + postfix, type);\n@@ -91,1 +95,1 @@\n-                VarHandleTestAccessFloat.class, \"static_final_v\" + postfix, float.class);\n+                VarHandleTestAccessFloat.class, \"static_final_v\" + postfix, type);\n@@ -95,1 +99,1 @@\n-                VarHandleTestAccessFloat.class, \"static_v\" + postfix, float.class);\n+                VarHandleTestAccessFloat.class, \"static_v\" + postfix, type);\n@@ -114,1 +118,1 @@\n-                VarHandleTestAccessFloat.class, \"final_v\", float.class);\n+                VarHandleTestAccessFloat.class, \"final_v\", type);\n@@ -117,1 +121,1 @@\n-                VarHandleTestAccessFloat.class, \"v\", float.class);\n+                VarHandleTestAccessFloat.class, \"v\", type);\n@@ -120,1 +124,1 @@\n-            VarHandleTestAccessFloat.class, \"static_final_v\", float.class);\n+            VarHandleTestAccessFloat.class, \"static_final_v\", type);\n@@ -123,1 +127,1 @@\n-            VarHandleTestAccessFloat.class, \"static_v\", float.class);\n+            VarHandleTestAccessFloat.class, \"static_v\", type);\n@@ -210,1 +214,1 @@\n-        assertEquals(vh.varType(), float.class);\n+        assertEquals(vh.varType(), type);\n@@ -222,1 +226,1 @@\n-                    VarHandleTestAccessFloat.class, \"final_v\", float.class);\n+                    VarHandleTestAccessFloat.class, \"final_v\", type);\n@@ -227,1 +231,1 @@\n-                    VarHandleTestAccessFloat.class, \"v\", float.class);\n+                    VarHandleTestAccessFloat.class, \"v\", type);\n@@ -235,1 +239,1 @@\n-                VarHandleTestAccessFloat.class, \"static_final_v\", float.class);\n+                VarHandleTestAccessFloat.class, \"static_final_v\", type);\n@@ -240,1 +244,1 @@\n-                VarHandleTestAccessFloat.class, \"static_v\", float.class);\n+                VarHandleTestAccessFloat.class, \"static_v\", type);\n@@ -379,1 +383,0 @@\n-\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessFloat.java","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -49,0 +51,2 @@\n+    static final Class<?> type = int.class;\n+\n@@ -83,1 +87,1 @@\n-                    VarHandleTestAccessInt.class, \"final_v\" + postfix, int.class);\n+                    VarHandleTestAccessInt.class, \"final_v\" + postfix, type);\n@@ -87,1 +91,1 @@\n-                    VarHandleTestAccessInt.class, \"v\" + postfix, int.class);\n+                    VarHandleTestAccessInt.class, \"v\" + postfix, type);\n@@ -91,1 +95,1 @@\n-                VarHandleTestAccessInt.class, \"static_final_v\" + postfix, int.class);\n+                VarHandleTestAccessInt.class, \"static_final_v\" + postfix, type);\n@@ -95,1 +99,1 @@\n-                VarHandleTestAccessInt.class, \"static_v\" + postfix, int.class);\n+                VarHandleTestAccessInt.class, \"static_v\" + postfix, type);\n@@ -114,1 +118,1 @@\n-                VarHandleTestAccessInt.class, \"final_v\", int.class);\n+                VarHandleTestAccessInt.class, \"final_v\", type);\n@@ -117,1 +121,1 @@\n-                VarHandleTestAccessInt.class, \"v\", int.class);\n+                VarHandleTestAccessInt.class, \"v\", type);\n@@ -120,1 +124,1 @@\n-            VarHandleTestAccessInt.class, \"static_final_v\", int.class);\n+            VarHandleTestAccessInt.class, \"static_final_v\", type);\n@@ -123,1 +127,1 @@\n-            VarHandleTestAccessInt.class, \"static_v\", int.class);\n+            VarHandleTestAccessInt.class, \"static_v\", type);\n@@ -210,1 +214,1 @@\n-        assertEquals(vh.varType(), int.class);\n+        assertEquals(vh.varType(), type);\n@@ -222,1 +226,1 @@\n-                    VarHandleTestAccessInt.class, \"final_v\", int.class);\n+                    VarHandleTestAccessInt.class, \"final_v\", type);\n@@ -227,1 +231,1 @@\n-                    VarHandleTestAccessInt.class, \"v\", int.class);\n+                    VarHandleTestAccessInt.class, \"v\", type);\n@@ -235,1 +239,1 @@\n-                VarHandleTestAccessInt.class, \"static_final_v\", int.class);\n+                VarHandleTestAccessInt.class, \"static_final_v\", type);\n@@ -240,1 +244,1 @@\n-                VarHandleTestAccessInt.class, \"static_v\", int.class);\n+                VarHandleTestAccessInt.class, \"static_v\", type);\n@@ -344,1 +348,0 @@\n-\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessInt.java","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -49,0 +51,2 @@\n+    static final Class<?> type = long.class;\n+\n@@ -83,1 +87,1 @@\n-                    VarHandleTestAccessLong.class, \"final_v\" + postfix, long.class);\n+                    VarHandleTestAccessLong.class, \"final_v\" + postfix, type);\n@@ -87,1 +91,1 @@\n-                    VarHandleTestAccessLong.class, \"v\" + postfix, long.class);\n+                    VarHandleTestAccessLong.class, \"v\" + postfix, type);\n@@ -91,1 +95,1 @@\n-                VarHandleTestAccessLong.class, \"static_final_v\" + postfix, long.class);\n+                VarHandleTestAccessLong.class, \"static_final_v\" + postfix, type);\n@@ -95,1 +99,1 @@\n-                VarHandleTestAccessLong.class, \"static_v\" + postfix, long.class);\n+                VarHandleTestAccessLong.class, \"static_v\" + postfix, type);\n@@ -114,1 +118,1 @@\n-                VarHandleTestAccessLong.class, \"final_v\", long.class);\n+                VarHandleTestAccessLong.class, \"final_v\", type);\n@@ -117,1 +121,1 @@\n-                VarHandleTestAccessLong.class, \"v\", long.class);\n+                VarHandleTestAccessLong.class, \"v\", type);\n@@ -120,1 +124,1 @@\n-            VarHandleTestAccessLong.class, \"static_final_v\", long.class);\n+            VarHandleTestAccessLong.class, \"static_final_v\", type);\n@@ -123,1 +127,1 @@\n-            VarHandleTestAccessLong.class, \"static_v\", long.class);\n+            VarHandleTestAccessLong.class, \"static_v\", type);\n@@ -210,1 +214,1 @@\n-        assertEquals(vh.varType(), long.class);\n+        assertEquals(vh.varType(), type);\n@@ -222,1 +226,1 @@\n-                    VarHandleTestAccessLong.class, \"final_v\", long.class);\n+                    VarHandleTestAccessLong.class, \"final_v\", type);\n@@ -227,1 +231,1 @@\n-                    VarHandleTestAccessLong.class, \"v\", long.class);\n+                    VarHandleTestAccessLong.class, \"v\", type);\n@@ -235,1 +239,1 @@\n-                VarHandleTestAccessLong.class, \"static_final_v\", long.class);\n+                VarHandleTestAccessLong.class, \"static_final_v\", type);\n@@ -240,1 +244,1 @@\n-                VarHandleTestAccessLong.class, \"static_v\", long.class);\n+                VarHandleTestAccessLong.class, \"static_v\", type);\n@@ -344,1 +348,0 @@\n-\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessLong.java","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -49,0 +51,2 @@\n+    static final Class<?> type = short.class;\n+\n@@ -83,1 +87,1 @@\n-                    VarHandleTestAccessShort.class, \"final_v\" + postfix, short.class);\n+                    VarHandleTestAccessShort.class, \"final_v\" + postfix, type);\n@@ -87,1 +91,1 @@\n-                    VarHandleTestAccessShort.class, \"v\" + postfix, short.class);\n+                    VarHandleTestAccessShort.class, \"v\" + postfix, type);\n@@ -91,1 +95,1 @@\n-                VarHandleTestAccessShort.class, \"static_final_v\" + postfix, short.class);\n+                VarHandleTestAccessShort.class, \"static_final_v\" + postfix, type);\n@@ -95,1 +99,1 @@\n-                VarHandleTestAccessShort.class, \"static_v\" + postfix, short.class);\n+                VarHandleTestAccessShort.class, \"static_v\" + postfix, type);\n@@ -114,1 +118,1 @@\n-                VarHandleTestAccessShort.class, \"final_v\", short.class);\n+                VarHandleTestAccessShort.class, \"final_v\", type);\n@@ -117,1 +121,1 @@\n-                VarHandleTestAccessShort.class, \"v\", short.class);\n+                VarHandleTestAccessShort.class, \"v\", type);\n@@ -120,1 +124,1 @@\n-            VarHandleTestAccessShort.class, \"static_final_v\", short.class);\n+            VarHandleTestAccessShort.class, \"static_final_v\", type);\n@@ -123,1 +127,1 @@\n-            VarHandleTestAccessShort.class, \"static_v\", short.class);\n+            VarHandleTestAccessShort.class, \"static_v\", type);\n@@ -210,1 +214,1 @@\n-        assertEquals(vh.varType(), short.class);\n+        assertEquals(vh.varType(), type);\n@@ -222,1 +226,1 @@\n-                    VarHandleTestAccessShort.class, \"final_v\", short.class);\n+                    VarHandleTestAccessShort.class, \"final_v\", type);\n@@ -227,1 +231,1 @@\n-                    VarHandleTestAccessShort.class, \"v\", short.class);\n+                    VarHandleTestAccessShort.class, \"v\", type);\n@@ -235,1 +239,1 @@\n-                VarHandleTestAccessShort.class, \"static_final_v\", short.class);\n+                VarHandleTestAccessShort.class, \"static_final_v\", type);\n@@ -240,1 +244,1 @@\n-                VarHandleTestAccessShort.class, \"static_v\", short.class);\n+                VarHandleTestAccessShort.class, \"static_v\", type);\n@@ -344,1 +348,0 @@\n-\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessShort.java","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -49,0 +51,2 @@\n+    static final Class<?> type = String.class;\n+\n@@ -84,1 +88,1 @@\n-                    VarHandleTestAccessString.class, \"final_v\" + postfix, String.class);\n+                    VarHandleTestAccessString.class, \"final_v\" + postfix, type);\n@@ -88,1 +92,1 @@\n-                    VarHandleTestAccessString.class, \"v\" + postfix, String.class);\n+                    VarHandleTestAccessString.class, \"v\" + postfix, type);\n@@ -92,1 +96,1 @@\n-                VarHandleTestAccessString.class, \"static_final_v\" + postfix, String.class);\n+                VarHandleTestAccessString.class, \"static_final_v\" + postfix, type);\n@@ -96,1 +100,1 @@\n-                VarHandleTestAccessString.class, \"static_v\" + postfix, String.class);\n+                VarHandleTestAccessString.class, \"static_v\" + postfix, type);\n@@ -115,1 +119,1 @@\n-                VarHandleTestAccessString.class, \"final_v\", String.class);\n+                VarHandleTestAccessString.class, \"final_v\", type);\n@@ -118,1 +122,1 @@\n-                VarHandleTestAccessString.class, \"v\", String.class);\n+                VarHandleTestAccessString.class, \"v\", type);\n@@ -121,1 +125,1 @@\n-            VarHandleTestAccessString.class, \"static_final_v\", String.class);\n+            VarHandleTestAccessString.class, \"static_final_v\", type);\n@@ -124,1 +128,1 @@\n-            VarHandleTestAccessString.class, \"static_v\", String.class);\n+            VarHandleTestAccessString.class, \"static_v\", type);\n@@ -212,1 +216,1 @@\n-        assertEquals(vh.varType(), String.class);\n+        assertEquals(vh.varType(), type);\n@@ -224,1 +228,1 @@\n-                    VarHandleTestAccessString.class, \"final_v\", String.class);\n+                    VarHandleTestAccessString.class, \"final_v\", type);\n@@ -229,1 +233,1 @@\n-                    VarHandleTestAccessString.class, \"v\", String.class);\n+                    VarHandleTestAccessString.class, \"v\", type);\n@@ -237,1 +241,1 @@\n-                VarHandleTestAccessString.class, \"static_final_v\", String.class);\n+                VarHandleTestAccessString.class, \"static_final_v\", type);\n@@ -242,1 +246,1 @@\n-                VarHandleTestAccessString.class, \"static_v\", String.class);\n+                VarHandleTestAccessString.class, \"static_v\", type);\n@@ -397,1 +401,0 @@\n-\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessString.java","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -45,0 +47,2 @@\n+    static final Class<?> type = boolean.class;\n+\n@@ -47,1 +51,1 @@\n-    static boolean static_v;\n+    static boolean static_v = true;\n@@ -66,1 +70,1 @@\n-                VarHandleTestMethodHandleAccessBoolean.class, \"final_v\", boolean.class);\n+                VarHandleTestMethodHandleAccessBoolean.class, \"final_v\", type);\n@@ -69,1 +73,1 @@\n-                VarHandleTestMethodHandleAccessBoolean.class, \"v\", boolean.class);\n+                VarHandleTestMethodHandleAccessBoolean.class, \"v\", type);\n@@ -72,1 +76,1 @@\n-            VarHandleTestMethodHandleAccessBoolean.class, \"static_final_v\", boolean.class);\n+            VarHandleTestMethodHandleAccessBoolean.class, \"static_final_v\", type);\n@@ -75,1 +79,1 @@\n-            VarHandleTestMethodHandleAccessBoolean.class, \"static_v\", boolean.class);\n+            VarHandleTestMethodHandleAccessBoolean.class, \"static_v\", type);\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessBoolean.java","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -45,0 +47,2 @@\n+    static final Class<?> type = byte.class;\n+\n@@ -47,1 +51,1 @@\n-    static byte static_v;\n+    static byte static_v = (byte)0x01;\n@@ -66,1 +70,1 @@\n-                VarHandleTestMethodHandleAccessByte.class, \"final_v\", byte.class);\n+                VarHandleTestMethodHandleAccessByte.class, \"final_v\", type);\n@@ -69,1 +73,1 @@\n-                VarHandleTestMethodHandleAccessByte.class, \"v\", byte.class);\n+                VarHandleTestMethodHandleAccessByte.class, \"v\", type);\n@@ -72,1 +76,1 @@\n-            VarHandleTestMethodHandleAccessByte.class, \"static_final_v\", byte.class);\n+            VarHandleTestMethodHandleAccessByte.class, \"static_final_v\", type);\n@@ -75,1 +79,1 @@\n-            VarHandleTestMethodHandleAccessByte.class, \"static_v\", byte.class);\n+            VarHandleTestMethodHandleAccessByte.class, \"static_v\", type);\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessByte.java","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -45,0 +47,2 @@\n+    static final Class<?> type = char.class;\n+\n@@ -47,1 +51,1 @@\n-    static char static_v;\n+    static char static_v = '\\u0123';\n@@ -66,1 +70,1 @@\n-                VarHandleTestMethodHandleAccessChar.class, \"final_v\", char.class);\n+                VarHandleTestMethodHandleAccessChar.class, \"final_v\", type);\n@@ -69,1 +73,1 @@\n-                VarHandleTestMethodHandleAccessChar.class, \"v\", char.class);\n+                VarHandleTestMethodHandleAccessChar.class, \"v\", type);\n@@ -72,1 +76,1 @@\n-            VarHandleTestMethodHandleAccessChar.class, \"static_final_v\", char.class);\n+            VarHandleTestMethodHandleAccessChar.class, \"static_final_v\", type);\n@@ -75,1 +79,1 @@\n-            VarHandleTestMethodHandleAccessChar.class, \"static_v\", char.class);\n+            VarHandleTestMethodHandleAccessChar.class, \"static_v\", type);\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessChar.java","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -45,0 +47,2 @@\n+    static final Class<?> type = double.class;\n+\n@@ -47,1 +51,1 @@\n-    static double static_v;\n+    static double static_v = 1.0d;\n@@ -66,1 +70,1 @@\n-                VarHandleTestMethodHandleAccessDouble.class, \"final_v\", double.class);\n+                VarHandleTestMethodHandleAccessDouble.class, \"final_v\", type);\n@@ -69,1 +73,1 @@\n-                VarHandleTestMethodHandleAccessDouble.class, \"v\", double.class);\n+                VarHandleTestMethodHandleAccessDouble.class, \"v\", type);\n@@ -72,1 +76,1 @@\n-            VarHandleTestMethodHandleAccessDouble.class, \"static_final_v\", double.class);\n+            VarHandleTestMethodHandleAccessDouble.class, \"static_final_v\", type);\n@@ -75,1 +79,1 @@\n-            VarHandleTestMethodHandleAccessDouble.class, \"static_v\", double.class);\n+            VarHandleTestMethodHandleAccessDouble.class, \"static_v\", type);\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessDouble.java","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -45,0 +47,2 @@\n+    static final Class<?> type = float.class;\n+\n@@ -47,1 +51,1 @@\n-    static float static_v;\n+    static float static_v = 1.0f;\n@@ -66,1 +70,1 @@\n-                VarHandleTestMethodHandleAccessFloat.class, \"final_v\", float.class);\n+                VarHandleTestMethodHandleAccessFloat.class, \"final_v\", type);\n@@ -69,1 +73,1 @@\n-                VarHandleTestMethodHandleAccessFloat.class, \"v\", float.class);\n+                VarHandleTestMethodHandleAccessFloat.class, \"v\", type);\n@@ -72,1 +76,1 @@\n-            VarHandleTestMethodHandleAccessFloat.class, \"static_final_v\", float.class);\n+            VarHandleTestMethodHandleAccessFloat.class, \"static_final_v\", type);\n@@ -75,1 +79,1 @@\n-            VarHandleTestMethodHandleAccessFloat.class, \"static_v\", float.class);\n+            VarHandleTestMethodHandleAccessFloat.class, \"static_v\", type);\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessFloat.java","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -45,0 +47,2 @@\n+    static final Class<?> type = int.class;\n+\n@@ -47,1 +51,1 @@\n-    static int static_v;\n+    static int static_v = 0x01234567;\n@@ -66,1 +70,1 @@\n-                VarHandleTestMethodHandleAccessInt.class, \"final_v\", int.class);\n+                VarHandleTestMethodHandleAccessInt.class, \"final_v\", type);\n@@ -69,1 +73,1 @@\n-                VarHandleTestMethodHandleAccessInt.class, \"v\", int.class);\n+                VarHandleTestMethodHandleAccessInt.class, \"v\", type);\n@@ -72,1 +76,1 @@\n-            VarHandleTestMethodHandleAccessInt.class, \"static_final_v\", int.class);\n+            VarHandleTestMethodHandleAccessInt.class, \"static_final_v\", type);\n@@ -75,1 +79,1 @@\n-            VarHandleTestMethodHandleAccessInt.class, \"static_v\", int.class);\n+            VarHandleTestMethodHandleAccessInt.class, \"static_v\", type);\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessInt.java","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -45,0 +47,2 @@\n+    static final Class<?> type = long.class;\n+\n@@ -47,1 +51,1 @@\n-    static long static_v;\n+    static long static_v = 0x0123456789ABCDEFL;\n@@ -66,1 +70,1 @@\n-                VarHandleTestMethodHandleAccessLong.class, \"final_v\", long.class);\n+                VarHandleTestMethodHandleAccessLong.class, \"final_v\", type);\n@@ -69,1 +73,1 @@\n-                VarHandleTestMethodHandleAccessLong.class, \"v\", long.class);\n+                VarHandleTestMethodHandleAccessLong.class, \"v\", type);\n@@ -72,1 +76,1 @@\n-            VarHandleTestMethodHandleAccessLong.class, \"static_final_v\", long.class);\n+            VarHandleTestMethodHandleAccessLong.class, \"static_final_v\", type);\n@@ -75,1 +79,1 @@\n-            VarHandleTestMethodHandleAccessLong.class, \"static_v\", long.class);\n+            VarHandleTestMethodHandleAccessLong.class, \"static_v\", type);\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessLong.java","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -45,0 +47,2 @@\n+    static final Class<?> type = short.class;\n+\n@@ -47,1 +51,1 @@\n-    static short static_v;\n+    static short static_v = (short)0x0123;\n@@ -66,1 +70,1 @@\n-                VarHandleTestMethodHandleAccessShort.class, \"final_v\", short.class);\n+                VarHandleTestMethodHandleAccessShort.class, \"final_v\", type);\n@@ -69,1 +73,1 @@\n-                VarHandleTestMethodHandleAccessShort.class, \"v\", short.class);\n+                VarHandleTestMethodHandleAccessShort.class, \"v\", type);\n@@ -72,1 +76,1 @@\n-            VarHandleTestMethodHandleAccessShort.class, \"static_final_v\", short.class);\n+            VarHandleTestMethodHandleAccessShort.class, \"static_final_v\", type);\n@@ -75,1 +79,1 @@\n-            VarHandleTestMethodHandleAccessShort.class, \"static_v\", short.class);\n+            VarHandleTestMethodHandleAccessShort.class, \"static_v\", type);\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessShort.java","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -45,0 +47,2 @@\n+    static final Class<?> type = String.class;\n+\n@@ -47,1 +51,1 @@\n-    static String static_v;\n+    static String static_v = \"foo\";\n@@ -66,1 +70,1 @@\n-                VarHandleTestMethodHandleAccessString.class, \"final_v\", String.class);\n+                VarHandleTestMethodHandleAccessString.class, \"final_v\", type);\n@@ -69,1 +73,1 @@\n-                VarHandleTestMethodHandleAccessString.class, \"v\", String.class);\n+                VarHandleTestMethodHandleAccessString.class, \"v\", type);\n@@ -72,1 +76,1 @@\n-            VarHandleTestMethodHandleAccessString.class, \"static_final_v\", String.class);\n+            VarHandleTestMethodHandleAccessString.class, \"static_final_v\", type);\n@@ -75,1 +79,1 @@\n-            VarHandleTestMethodHandleAccessString.class, \"static_v\", String.class);\n+            VarHandleTestMethodHandleAccessString.class, \"static_v\", type);\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessString.java","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+#warn\n+\n@@ -26,1 +28,2 @@\n- * @run testng\/othervm -Diters=10   -Xint                                                   VarHandleTestAccess$Type$\n+#if[Value]\n+ * @compile -XDenablePrimitiveClasses Point.java Value.java VarHandleTestAccess$Type$.java\n@@ -31,0 +34,6 @@\n+ * @run testng\/othervm -XX:+EnableValhalla -XX:+EnablePrimitiveClasses -Diters=10    -Xint                   VarHandleTestAccess$Type$\n+ * @run testng\/othervm -XX:+EnableValhalla -XX:+EnablePrimitiveClasses -Diters=2000 -XX:CompileThresholdScaling=0.1 -XX:TieredStopAtLevel=1 VarHandleTestAccess$Type$\n+ * @run testng\/othervm -XX:+EnableValhalla -XX:+EnablePrimitiveClasses -Diters=2000 -XX:CompileThresholdScaling=0.1                         VarHandleTestAccess$Type$\n+ * @run testng\/othervm -XX:+EnableValhalla -XX:+EnablePrimitiveClasses -Diters=2000 -XX:CompileThresholdScaling=0.1 -XX:-TieredCompilation  VarHandleTestAccess$Type$\n+#else[Value]\n+ * @run testng\/othervm -Diters=10    -Xint                   VarHandleTestAccess$Type$\n@@ -34,0 +43,1 @@\n+#end[Value]\n@@ -46,0 +56,3 @@\n+#if[Point]\n+import jdk.internal.value.PrimitiveClass;\n+#end[Point]\n@@ -49,0 +62,6 @@\n+#if[Point]\n+    static final Class<?> type = PrimitiveClass.asValueType($type$.class);\n+#else[Point]\n+    static final Class<?> type = $type$.class;\n+#end[Point]\n+\n@@ -75,1 +94,1 @@\n-#if[String]\n+#if[Object]\n@@ -77,1 +96,1 @@\n-#end[String]\n+#end[Object]\n@@ -86,1 +105,1 @@\n-                    VarHandleTestAccess$Type$.class, \"final_v\" + postfix, $type$.class);\n+                    VarHandleTestAccess$Type$.class, \"final_v\" + postfix, type);\n@@ -90,1 +109,1 @@\n-                    VarHandleTestAccess$Type$.class, \"v\" + postfix, $type$.class);\n+                    VarHandleTestAccess$Type$.class, \"v\" + postfix, type);\n@@ -94,1 +113,1 @@\n-                VarHandleTestAccess$Type$.class, \"static_final_v\" + postfix, $type$.class);\n+                VarHandleTestAccess$Type$.class, \"static_final_v\" + postfix, type);\n@@ -98,1 +117,1 @@\n-                VarHandleTestAccess$Type$.class, \"static_v\" + postfix, $type$.class);\n+                VarHandleTestAccess$Type$.class, \"static_v\" + postfix, type);\n@@ -121,1 +140,1 @@\n-                VarHandleTestAccess$Type$.class, \"final_v\", $type$.class);\n+                VarHandleTestAccess$Type$.class, \"final_v\", type);\n@@ -124,1 +143,1 @@\n-                VarHandleTestAccess$Type$.class, \"v\", $type$.class);\n+                VarHandleTestAccess$Type$.class, \"v\", type);\n@@ -127,1 +146,1 @@\n-            VarHandleTestAccess$Type$.class, \"static_final_v\", $type$.class);\n+            VarHandleTestAccess$Type$.class, \"static_final_v\", type);\n@@ -130,1 +149,1 @@\n-            VarHandleTestAccess$Type$.class, \"static_v\", $type$.class);\n+            VarHandleTestAccess$Type$.class, \"static_v\", type);\n@@ -133,1 +152,1 @@\n-#if[String]\n+#if[Object]\n@@ -135,1 +154,1 @@\n-#end[String]\n+#end[Object]\n@@ -252,1 +271,1 @@\n-        assertEquals(vh.varType(), $type$.class);\n+        assertEquals(vh.varType(), type);\n@@ -264,1 +283,1 @@\n-                    VarHandleTestAccess$Type$.class, \"final_v\", $type$.class);\n+                    VarHandleTestAccess$Type$.class, \"final_v\", type);\n@@ -269,1 +288,1 @@\n-                    VarHandleTestAccess$Type$.class, \"v\", $type$.class);\n+                    VarHandleTestAccess$Type$.class, \"v\", type);\n@@ -277,1 +296,1 @@\n-                VarHandleTestAccess$Type$.class, \"static_final_v\", $type$.class);\n+                VarHandleTestAccess$Type$.class, \"static_final_v\", type);\n@@ -282,1 +301,1 @@\n-                VarHandleTestAccess$Type$.class, \"static_v\", $type$.class);\n+                VarHandleTestAccess$Type$.class, \"static_v\", type);\n@@ -317,1 +336,1 @@\n-#if[String]\n+#if[Object]\n@@ -320,1 +339,1 @@\n-#end[String]\n+#end[Object]\n@@ -327,1 +346,1 @@\n-#if[String]\n+#if[Object]\n@@ -331,1 +350,1 @@\n-#end[String]\n+#end[Object]\n@@ -490,1 +509,0 @@\n-\n@@ -2006,1 +2024,1 @@\n-#if[String]\n+#if[Object]\n@@ -2087,1 +2105,1 @@\n-#end[String]\n+#end[Object]\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/X-VarHandleTestAccess.java.template","additions":43,"deletions":25,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+#warn\n+\n@@ -26,0 +28,2 @@\n+#if[Value]\n+ * @compile -XDenablePrimitiveClasses Point.java Value.java VarHandleTestMethodHandleAccess$Type$.java\n@@ -28,0 +32,2 @@\n+ * @run testng\/othervm -XX:+EnableValhalla -XX:+EnablePrimitiveClasses -Diters=200 -XX:CompileThresholdScaling=0.1 VarHandleTestMethodHandleAccess$Type$\n+#else[Value]\n@@ -29,0 +35,1 @@\n+#end[Value]\n@@ -42,0 +49,3 @@\n+#if[Point]\n+import jdk.internal.value.PrimitiveClass;\n+#end[Point]\n@@ -45,0 +55,6 @@\n+#if[Point]\n+    static final Class<?> type = PrimitiveClass.asValueType($type$.class);\n+#else[Point]\n+    static final Class<?> type = $type$.class;\n+#end[Point]\n+\n@@ -47,1 +63,1 @@\n-    static $type$ static_v;\n+    static $type$ static_v = $value1$;\n@@ -66,1 +82,1 @@\n-                VarHandleTestMethodHandleAccess$Type$.class, \"final_v\", $type$.class);\n+                VarHandleTestMethodHandleAccess$Type$.class, \"final_v\", type);\n@@ -69,1 +85,1 @@\n-                VarHandleTestMethodHandleAccess$Type$.class, \"v\", $type$.class);\n+                VarHandleTestMethodHandleAccess$Type$.class, \"v\", type);\n@@ -72,1 +88,1 @@\n-            VarHandleTestMethodHandleAccess$Type$.class, \"static_final_v\", $type$.class);\n+            VarHandleTestMethodHandleAccess$Type$.class, \"static_final_v\", type);\n@@ -75,1 +91,1 @@\n-            VarHandleTestMethodHandleAccess$Type$.class, \"static_v\", $type$.class);\n+            VarHandleTestMethodHandleAccess$Type$.class, \"static_v\", type);\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/X-VarHandleTestMethodHandleAccess.java.template","additions":21,"deletions":5,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -123,1 +123,1 @@\n-            \"clone\", \"finalize\", \"getClass\", \"hashCode\",\n+            \"clone\", \"finalize\", \"getClass\", \"hashCode\", \"isValueObject\",\n","filename":"test\/langtools\/tools\/javac\/records\/RecordCompilationTests.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,6 @@\n+CheckFinal.java:14:13: compiler.err.cant.assign.val.to.var: final, fi\n+CheckFinal.java:15:13: compiler.err.cant.assign.val.to.var: final, fe\n+CheckFinal.java:17:13: compiler.err.cant.assign.val.to.var: static final, xsf\n+CheckFinal.java:19:29: compiler.err.cant.inherit.from.final: CheckFinal\n+CheckFinal.java:19:42: compiler.err.identity.type.has.value.super.type: compiler.misc.anonymous.class: CheckFinal, CheckFinal\n+5 errors\n","filename":"test\/langtools\/tools\/javac\/valhalla\/primitive-classes\/CheckFinal.out","additions":6,"deletions":0,"binary":false,"changes":6,"status":"added"},{"patch":"@@ -0,0 +1,2 @@\n+CheckStaticFinalAssign.java:15:9: compiler.err.cant.assign.val.to.var: static final, x\n+1 error\n","filename":"test\/langtools\/tools\/javac\/valhalla\/primitive-classes\/CheckStaticFinalAssign.out","additions":2,"deletions":0,"binary":false,"changes":2,"status":"added"},{"patch":"@@ -0,0 +1,1269 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * PrimitiveClassesCompilationTests\n+ *\n+ * @test\n+ * @bug 8297207\n+ * @summary Negative compilation tests, and positive compilation (smoke) tests for Primitive Classes\n+ * @library \/lib\/combo \/tools\/lib\n+ * @modules\n+ *     jdk.compiler\/com.sun.tools.javac.util\n+ *     jdk.compiler\/com.sun.tools.javac.api\n+ *     jdk.compiler\/com.sun.tools.javac.main\n+ *     jdk.compiler\/com.sun.tools.javac.code\n+ *     jdk.jdeps\/com.sun.tools.classfile\n+ * @build toolbox.ToolBox toolbox.JavacTask\n+ * @run testng PrimitiveClassesCompilationTests\n+ *\/\n+\n+import java.io.File;\n+\n+import java.util.List;\n+\n+import com.sun.tools.classfile.ClassFile;\n+import com.sun.tools.classfile.Code_attribute;\n+import com.sun.tools.classfile.ConstantPool;\n+import com.sun.tools.classfile.ConstantPool.CONSTANT_Class_info;\n+import com.sun.tools.classfile.ConstantPool.CONSTANT_Fieldref_info;\n+import com.sun.tools.classfile.ConstantPool.CONSTANT_Methodref_info;\n+import com.sun.tools.classfile.Field;\n+import com.sun.tools.classfile.Instruction;\n+import com.sun.tools.classfile.Method;\n+\n+import com.sun.tools.javac.code.Flags;\n+\n+import static org.testng.Assert.assertTrue;\n+import org.testng.annotations.Test;\n+\n+import tools.javac.combo.CompilationTestCase;\n+\n+import toolbox.ToolBox;\n+\n+@Test\n+public class PrimitiveClassesCompilationTests extends CompilationTestCase {\n+\n+    private static String[] DEFAULT_OPTIONS = {\"-XDenablePrimitiveClasses\"};\n+\n+    ToolBox tb = new ToolBox();\n+\n+    public PrimitiveClassesCompilationTests() {\n+        setDefaultFilename(\"PrimitiveClassTest.java\");\n+        setCompileOptions(DEFAULT_OPTIONS);\n+    }\n+\n+    public void testSupers() {\n+        assertOK(\n+                \"\"\"\n+                interface GoodSuperInterface {}\n+                abstract class GoodSuper extends Object {}\n+                primitive class PC extends GoodSuper implements GoodSuperInterface {}\n+                \"\"\");\n+\n+        assertOK(\n+                \"\"\"\n+                abstract class Integer extends Number {\n+                    public double doubleValue() { return 0; }\n+                    public float floatValue() { return 0; }\n+                    public long longValue() { return 0; }\n+                    public int intValue() { return 0; }\n+                }\n+                primitive class PC extends Integer {}\n+                \"\"\");\n+\n+        assertOK(\n+                \"\"\"\n+                primitive class PC extends Number {\n+                    public double doubleValue() { return 0; }\n+                    public float floatValue() { return 0; }\n+                    public long longValue() { return 0; }\n+                    public int intValue() { return 0; }\n+                }\n+                \"\"\");\n+\n+        assertOK(\n+                \"\"\"\n+                abstract class SuperWithStaticField {\n+                    static int x;\n+                }\n+                primitive class PC extends SuperWithStaticField {}\n+                \"\"\");\n+\n+        assertOK(\n+                \"\"\"\n+                abstract class SuperWithEmptyNoArgCtor {\n+                    public SuperWithEmptyNoArgCtor() {\n+                        \/\/ Programmer supplied ctor but injected super call\n+                    }\n+                }\n+                abstract class SuperWithEmptyNoArgCtor_01 extends SuperWithEmptyNoArgCtor {\n+                    public SuperWithEmptyNoArgCtor_01() {\n+                        super();  \/\/ programmer coded chaining no-arg constructor\n+                    }\n+                }\n+                abstract class SuperWithEmptyNoArgCtor_02 extends SuperWithEmptyNoArgCtor_01 {\n+                    \/\/ Synthesized chaining no-arg constructor\n+                }\n+                primitive class PC extends SuperWithEmptyNoArgCtor_02 {}\n+                \"\"\");\n+\n+        assertFail(\"compiler.err.concrete.supertype.for.value.class\",\n+                \"\"\"\n+                class BadSuper {}\n+                primitive class PC extends BadSuper {}\n+                \"\"\");\n+\n+        assertFail(\"compiler.err.instance.field.not.allowed\",\n+                \"\"\"\n+                abstract class SuperWithInstanceField {\n+                    int x;\n+                }\n+                abstract class SuperWithInstanceField_01 extends SuperWithInstanceField {}\n+                primitive class PC extends SuperWithInstanceField_01 {}\n+                \"\"\");\n+\n+        assertFail(\"compiler.err.abstract.value.class.no.arg.constructor.must.be.empty\",\n+                \"\"\"\n+                abstract class SuperWithNonEmptyNoArgCtor {\n+                    public SuperWithNonEmptyNoArgCtor() {\n+                        System.out.println(\"Non-Empty\");\n+                    }\n+                }\n+                abstract class SuperWithNonEmptyNoArgCtor_01 extends SuperWithNonEmptyNoArgCtor {}\n+                primitive class PC extends SuperWithNonEmptyNoArgCtor_01 {}\n+                \"\"\");\n+\n+        assertFail(\"compiler.err.abstract.value.class.constructor.cannot.take.arguments\",\n+                \"\"\"\n+                abstract class SuperWithArgedCtor {\n+                    public SuperWithArgedCtor() {}\n+                    public SuperWithArgedCtor(String s) {\n+                    }\n+                }\n+                abstract class SuperWithArgedCtor_01 extends SuperWithArgedCtor {}\n+                primitive class PC extends SuperWithArgedCtor_01 {}\n+                \"\"\");\n+\n+        assertFail(\"compiler.err.abstract.value.class.declares.init.block\",\n+                \"\"\"\n+                abstract class SuperWithInstanceInit {\n+                    {\n+                        System.out.println(\"Disqualified from being super\");\n+                    }\n+                }\n+                abstract class SuperWithInstanceInit_01 extends SuperWithInstanceInit {\n+                    {\n+                        \/\/ Not disqualified since it is a meaningless empty block.\n+                    }\n+                }\n+                primitive class PC extends SuperWithInstanceInit_01 {}\n+                \"\"\");\n+\n+        assertFail(\"compiler.err.super.class.method.cannot.be.synchronized\",\n+                \"\"\"\n+                abstract class SuperWithSynchronizedMethod {\n+                    synchronized void foo() {}\n+                }\n+                abstract class SuperWithSynchronizedMethod_1 extends SuperWithSynchronizedMethod {}\n+                primitive class PC extends SuperWithSynchronizedMethod_1 {}\n+                \"\"\");\n+\n+        assertFail(\"compiler.err.abstract.value.class.cannot.be.inner\",\n+                \"\"\"\n+                class Outer {\n+                    abstract class InnerSuper {}\n+                }\n+                primitive class PC extends Outer.InnerSuper {}\n+                \"\"\");\n+    }\n+\n+    public void testFinalFields() {\n+        String[] sources = new String[] {\n+                \"\"\"\n+                primitive class Test {\n+                    final int x = 10;\n+                    Test() {\n+                        x = 10;\n+                    }\n+                }\n+                \"\"\",\n+                \"\"\"\n+                primitive class Test {\n+                    final int x = 10;\n+                    void foo() {\n+                        x = 10;\n+                    }\n+                }\n+                \"\"\"\n+        };\n+        for (String source : sources) {\n+            assertFail(\"compiler.err.cant.assign.val.to.var\", source);\n+        }\n+\n+        assertFail(\"compiler.err.var.might.already.be.assigned\",\n+                \"\"\"\n+                primitive class Test {\n+                    final int x;\n+                    Test() {\n+                        x = 10;\n+                        x = 10;\n+                    }\n+                }\n+                \"\"\"\n+        );\n+    }\n+\n+    public void testWithFieldNeg() {\n+        String[] sources = new String[] {\n+                \"\"\"\n+                primitive final class A {\n+                    final int x = 10;\n+                    primitive final class B {\n+                        final A a = A.default;\n+                        void foo(A a) {\n+                            a.x = 100;\n+                        }\n+                    }\n+                }\n+                \"\"\",\n+                \"\"\"\n+                primitive final class A {\n+                    static final int sx = 10;\n+                    primitive final class B {\n+                        final A a = A.default;\n+                        void foo(A a) {\n+                            a.sx = 100;\n+                        }\n+                    }\n+                }\n+                \"\"\",\n+                \"\"\"\n+                primitive final class A {\n+                    final int x = 10;\n+                    primitive final class B {\n+                        final A a = A.default;\n+                    }\n+                    void withfield(B b) {\n+                            b.a.x = 11;\n+                    }\n+                }\n+                \"\"\",\n+                \"\"\"\n+                primitive final class A {\n+                    final int x = 10;\n+                    void foo(A a) {\n+                        a.x = 100;\n+                    }\n+                }\n+                \"\"\",\n+                \"\"\"\n+                primitive final class A {\n+                    final int x = 10;\n+                    void foo(A a) {\n+                        (a).x = 100;\n+                    }\n+                }\n+                \"\"\",\n+                \"\"\"\n+                primitive final class A {\n+                    final int x = 10;\n+                    void foo(final A fa) {\n+                        fa.x = 100;\n+                    }\n+                }\n+                \"\"\",\n+                \"\"\"\n+                primitive final class A {\n+                    final int x = 10;\n+                    void foo() {\n+                        x = 100;\n+                    }\n+                }\n+                \"\"\",\n+                \"\"\"\n+                primitive final class A {\n+                    final int x = 10;\n+                    void foo() {\n+                        this.x = 100;\n+                    }\n+                }\n+                \"\"\",\n+                \"\"\"\n+                primitive final class A {\n+                    final int x = 10;\n+                    void foo() {\n+                        A.this.x = 100;\n+                    }\n+                }\n+                \"\"\",\n+        };\n+        for (String source : sources) {\n+            assertFail(\"compiler.err.cant.assign.val.to.var\", source);\n+        }\n+    }\n+\n+    public void testIllegalModifiers() {\n+        assertFail(\"compiler.err.illegal.combination.of.modifiers\", \"primitive interface I {}\");\n+        assertFail(\"compiler.err.mod.not.allowed.here\",\n+                \"\"\"\n+                class Test {\n+                    primitive public void m() {}\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.mod.not.allowed.here\",\n+                \"\"\"\n+                class Test {\n+                    void m() {\n+                        int[] ia = new primitive int[10];\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.mod.not.allowed.here\",\n+                \"\"\"\n+                class Test {\n+                    void m() {\n+                        new primitive String(\"Hello\");\n+                    }\n+                }\n+                \"\"\");\n+        \/*\n+        \/\/ this test is passing, not sure if this is correct\n+        assertFail(\"compiler.err.mod.not.allowed.here\",\n+                \"\"\"\n+                class Test {\n+                    interface I {}\n+                    void m() {\n+                        new primitive I() {};\n+                    }\n+                }\n+                \"\"\");\n+        *\/\n+    }\n+\n+    public void testPrimitivesAsTypeParams() {\n+        String[] sources = new String[] {\n+                \"\"\"\n+                import java.util.ArrayList;\n+                primitive class ValueOverGenericsTest {\n+                    ArrayList<ValueOverGenericsTest> ax = null;\n+                }\n+                \"\"\",\n+                \"\"\"\n+                import java.util.ArrayList;\n+                primitive class ValueOverGenericsTest {\n+                    void foo(ArrayList<? extends ValueOverGenericsTest> p) {}\n+                }\n+                \"\"\",\n+                \"\"\"\n+                import java.util.ArrayList;\n+                primitive class ValueOverGenericsTest {\n+                    void foo() {\n+                        new <ValueOverGenericsTest> ArrayList<Object>();\n+                    }\n+                }\n+                \"\"\",\n+                \"\"\"\n+                import java.util.ArrayList;\n+                primitive class ValueOverGenericsTest {\n+                    void foo() {\n+                        this.<ValueOverGenericsTest>foo();\n+                    }\n+                }\n+                \"\"\",\n+                \"\"\"\n+                import java.util.ArrayList;\n+                primitive class ValueOverGenericsTest {\n+                    void foo() {\n+                        Object o = (ValueOverGenericsTest & Serializable) null;\n+                    }\n+                }\n+                \"\"\",\n+        };\n+        for (String source : sources) {\n+            assertFail(\"compiler.err.type.found.req\", source);\n+        }\n+    }\n+\n+    public void testLocalPrimitiveClasses() {\n+        assertFail(\"compiler.err.cant.inherit.from.final\",\n+                \"\"\"\n+                class ValueModifierTest {\n+                    interface Value {}\n+                    void goo() {\n+                        primitive class Value {}\n+                        new Value() {};\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.cant.inherit.from.final\",\n+                \"\"\"\n+                class ValueModifierTest {\n+                    interface Value {}\n+                    void goo() {\n+                        primitive class Value {}\n+                        new primitive Value() {};\n+                    }\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testDefaultOnUnknownClass() {\n+        assertFail(\"compiler.err.cant.resolve.location\",\n+                \"\"\"\n+                class Test {\n+                    void m() {\n+                        Object o = Unknown.default;\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.cant.resolve.location\",\n+                \"\"\"\n+                class Test {\n+                    void m() {\n+                        Object o = Unknown1.Unknown2.default;\n+                    }\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testUncheckedDefaultWarning() {\n+        String[] previousOptions = getCompileOptions();\n+        try {\n+            String[] testOptions = {\"-Xlint:all\", \"-XDenablePrimitiveClasses\"};\n+            setCompileOptions(testOptions);\n+            assertOKWithWarning(\"compiler.warn.prob.found.req\",\n+                    \"\"\"\n+                    primitive class UncheckedDefault<E> {\n+                        void m() {\n+                            UncheckedDefault<String> foo = UncheckedDefault.default;\n+                        }\n+                    }\n+                    \"\"\");\n+        } finally {\n+            setCompileOptions(previousOptions);\n+        }\n+    }\n+\n+    public void testRefProjection() {\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                primitive class PC {\n+                    void foo() {\n+                        PC x = null;\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                primitive class PC {\n+                    void foo() {\n+                        PC.ref x = null;\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                primitive class PC {\n+                    void foo(PC x) {\n+                        PC.ref xq = null;\n+                        xq = x;\n+                        xq = (PC.ref) x;\n+                        xq = (PC) x;\n+                        x = xq;\n+                        x = (PC.ref) xq;\n+                        x = (PC) xq;\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                primitive class PC {\n+                    void foo() {\n+                        PC[] xa = new PC[] { null };\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                primitive class PC {\n+                    void foo() {\n+                        PC.ref [] xqa = new PC.ref[] { null };\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                primitive class PC {\n+                    void foo(PC[] xa) {\n+                        PC.ref[] xqa = xa;\n+                        xqa = (PC.ref[]) xa;\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                primitive class PC {\n+                    void foo(PC[] xa, PC.ref[] xqa) {\n+                        xa = xqa;\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                primitive class PC {\n+                    void foo(PC[] xa, PC.ref[] xqa) {\n+                        xa = (PC[]) xqa;\n+                    }\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testSuperInvocation() {\n+        assertFail(\"compiler.err.call.to.super.not.allowed.in.value.ctor\",\n+                \"\"\"\n+                primitive class PC {\n+                    PC(String s) {\n+                        super();  \/\/ Error.\n+                    }\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testProjectionInstantiation() {\n+        assertFail(\"compiler.err.projection.cant.be.instantiated\",\n+                \"\"\"\n+                primitive class PC {\n+                    void m() {\n+                        new PC.ref();\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.projection.cant.be.instantiated\",\n+                \"\"\"\n+                primitive class PC {\n+                    void m() {\n+                        new PC.val();\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                import java.util.function.Supplier;\n+                primitive class PC {\n+                    void m() {\n+                        foo(PC::new);\n+                    }\n+                    static void foo(Supplier<PC.ref> sx) {}\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.projection.cant.be.instantiated\",\n+                \"\"\"\n+                import java.util.function.Supplier;\n+                primitive class PC {\n+                    void m() {\n+                        foo(PC.ref::new);\n+                    }\n+                    static void foo(Supplier<PC.ref> sx) {}\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.projection.cant.be.instantiated\",\n+                \"\"\"\n+                import java.util.function.Supplier;\n+                primitive class PC {\n+                    void m() {\n+                        foo(PC.val::new);\n+                    }\n+                    static void foo(Supplier<PC.ref> sx) {}\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testOverloadResolution() {\n+        assertFail(\"compiler.err.ref.ambiguous\",\n+                \"\"\"\n+                class OverloadingPhaseTest {\n+                    static primitive class V {}\n+                    static String roo(V.ref v, int i) {\n+                        return \"\";\n+                    }\n+                    static String roo(V.ref v, Integer i) {\n+                        return \"\";\n+                    }\n+                    void m(V o) {\n+                        String result = roo(o, 0);\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.ref.ambiguous\",\n+                \"\"\"\n+                class OverloadingPhaseTest {\n+                    static primitive class V {}\n+                    static String roo(V.ref v, int i) {\n+                        return \"\";\n+                    }\n+                    static String roo(V.ref v, Integer i) {\n+                        return \"\";\n+                    }\n+                    void m(V o) {\n+                        String result = roo(o, Integer.valueOf(0));\n+                    }\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testNoVolatileFields() {\n+        assertFail(\"compiler.err.illegal.combination.of.modifiers\",\n+                \"\"\"\n+                primitive class Bar {\n+                    volatile int i = 0;\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testDualPath() {\n+        assertFail(\"compiler.err.already.defined\",\n+                \"\"\"\n+                primitive class DualPathInnerType  {\n+                    class Inner { }\n+\n+                    static DualPathInnerType.Inner xi = new DualPathInnerType().new Inner();\n+                    DualPathInnerType.ref.Inner xri = xi;\n+\n+                    void f (DualPathInnerType.Inner xri) {}\n+                    void f (DualPathInnerType.ref.Inner xri) {}\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testGenericArray() {\n+        String[] previousOptions = getCompileOptions();\n+        try {\n+            String[] testOptions = {\"-Xlint:all\", \"-XDenablePrimitiveClasses\"};\n+            setCompileOptions(testOptions);\n+            assertOKWithWarning(\"compiler.warn.prob.found.req\",\n+                    \"\"\"\n+                    class Test {\n+                        primitive class Value<T> {\n+                            T t = null;\n+                            void foo() {\n+                                Value<T>[] v = new Value[1];\n+                            }\n+                        }\n+                    }\n+                    \"\"\");\n+            assertOKWithWarning(\"compiler.warn.prob.found.req\",\n+                    \"\"\"\n+                    class Test {\n+                        primitive class Value<T> {\n+                            T t = null;\n+                            void foo() {\n+                                Value<Test>[] vx = new Value[1];\n+                            }\n+                        }\n+                    }\n+                    \"\"\");\n+            assertOKWithWarning(\"compiler.warn.prob.found.req\",\n+                    \"\"\"\n+                    class Test {\n+                        primitive class Value<T> {\n+                            T t = null;\n+                            void foo() {\n+                                Value<String>[] vs = new Value[1];\n+                            }\n+                        }\n+                    }\n+                    \"\"\");\n+            assertOKWithWarning(\"compiler.warn.prob.found.req\",\n+                    \"\"\"\n+                    class Test {\n+                        primitive class Value<T> {\n+                            T t = null;\n+                            void foo(Value<T>[] v) {\n+                                v = (Value<T> []) new Value[1];\n+                            }\n+                        }\n+                    }\n+                    \"\"\");\n+            assertOKWithWarning(\"compiler.warn.prob.found.req\",\n+                    \"\"\"\n+                    class Test {\n+                        primitive class Value<T> {\n+                            T t = null;\n+                            void foo(Value<Test>[] vx) {\n+                                vx = (Value<Test>[]) new Value[1];\n+                            }\n+                        }\n+                    }\n+                    \"\"\");\n+            assertOKWithWarning(\"compiler.warn.prob.found.req\",\n+                    \"\"\"\n+                    class Test {\n+                        primitive class Value<T> {\n+                            T t = null;\n+                            void foo(Value<String>[] vs) {\n+                                vs = (Value<String>[]) new Value[1];\n+                            }\n+                        }\n+                    }\n+                    \"\"\");\n+            assertFail(\"compiler.err.prob.found.req\",\n+                    \"\"\"\n+                    class Test {\n+                        primitive class Value<T> {\n+                            T t = null;\n+                            void foo(Value<Test>[] vx, Value<String>[] vs) {\n+                                vx = vs;\n+                            }\n+                        }\n+                    }\n+                    \"\"\");\n+        } finally {\n+            setCompileOptions(previousOptions);\n+        }\n+    }\n+\n+    public void testAdditionalGenericTests() {\n+        assertOK(\n+                \"\"\"\n+                abstract class Low<T, U> {}\n+                abstract class Mid<T, U> extends Low<U, T> {}\n+                abstract class High<T, U> extends Mid<U, T> {}\n+\n+                primitive class GenericInlineTest<T, U> extends High<U, T> {\n+                    void foo() {\n+                        GenericInlineTest<String, Integer> g = new GenericInlineTest<String, Integer>();\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                abstract class Low<T, U> {}\n+                abstract class Mid<T, U> extends Low<U, T> {}\n+                abstract class High<T, U> extends Mid<U, T> {}\n+\n+                primitive class GenericInlineTest<T, U> extends High<U, T> {\n+                    void foo(GenericInlineTest<String, Integer> g, High<String, Integer> h1) {\n+                        h1 = g;\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                abstract class Low<T, U> {}\n+                abstract class Mid<T, U> extends Low<U, T> {}\n+                abstract class High<T, U> extends Mid<U, T> {}\n+\n+                primitive class GenericInlineTest<T, U> extends High<U, T> {\n+                    void foo(GenericInlineTest<String, Integer> g, High<Integer, String> h2) {\n+                        h2 = g;\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                abstract class Low<T, U> {}\n+                abstract class Mid<T, U> extends Low<U, T> {}\n+                abstract class High<T, U> extends Mid<U, T> {}\n+\n+                primitive class GenericInlineTest<T, U> extends High<U, T> {\n+                    void foo(GenericInlineTest<String, Integer> g, Mid<String, Integer> m1) {\n+                        m1 = g;\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                abstract class Low<T, U> {}\n+                abstract class Mid<T, U> extends Low<U, T> {}\n+                abstract class High<T, U> extends Mid<U, T> {}\n+\n+                primitive class GenericInlineTest<T, U> extends High<U, T> {\n+                    void foo(GenericInlineTest<String, Integer> g, Mid<Integer, String> m2) {\n+                        m2 = g;\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                abstract class Low<T, U> {}\n+                abstract class Mid<T, U> extends Low<U, T> {}\n+                abstract class High<T, U> extends Mid<U, T> {}\n+\n+                primitive class GenericInlineTest<T, U> extends High<U, T> {\n+                    void foo(GenericInlineTest<String, Integer> g, Low<String, Integer> l1) {\n+                        l1 = g;\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                abstract class Low<T, U> {}\n+                abstract class Mid<T, U> extends Low<U, T> {}\n+                abstract class High<T, U> extends Mid<U, T> {}\n+\n+                primitive class GenericInlineTest<T, U> extends High<U, T> {\n+                    void foo(GenericInlineTest<String, Integer> g, Low<Integer, String> l2) {\n+                        l2 = g;\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                abstract class Low<T, U> {}\n+                abstract class Mid<T, U> extends Low<U, T> {}\n+                abstract class High<T, U> extends Mid<U, T> {}\n+\n+                primitive class GenericInlineTest<T, U> extends High<U, T> {\n+                    void foo(GenericInlineTest<String, Integer> g, Low<Integer, String> l2) {\n+                        l2 = g;\n+                        g = l2;\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                abstract class Low<T, U> {}\n+                abstract class Mid<T, U> extends Low<U, T> {}\n+                abstract class High<T, U> extends Mid<U, T> {}\n+\n+                primitive class GenericInlineTest<T, U> extends High<U, T> {\n+                    void foo(GenericInlineTest<String, Integer> g, Low<Integer, String> l2) {\n+                        l2 = g;\n+                        g = (GenericInlineTest<String, Integer>) l2;\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                abstract class Low<T, U> {}\n+                abstract class Mid<T, U> extends Low<U, T> {}\n+                abstract class High<T, U> extends Mid<U, T> {}\n+\n+                primitive class GenericInlineTest<T, U> extends High<U, T> {\n+                    void foo(GenericInlineTest<String, Integer> g, GenericInlineTest.ref<String, Integer> r1) {\n+                        r1 = g;\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                abstract class Low<T, U> {}\n+                abstract class Mid<T, U> extends Low<U, T> {}\n+                abstract class High<T, U> extends Mid<U, T> {}\n+\n+                primitive class GenericInlineTest<T, U> extends High<U, T> {\n+                    void foo(GenericInlineTest<String, Integer> g, GenericInlineTest.ref<Integer, String> r2) {\n+                        r2 = g;\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                abstract class Low<T, U> {}\n+                abstract class Mid<T, U> extends Low<U, T> {}\n+                abstract class High<T, U> extends Mid<U, T> {}\n+\n+                primitive class GenericInlineTest<T, U> extends High<U, T> {\n+                    void foo(GenericInlineTest<String, Integer> g, GenericInlineTest.ref<String, Integer> r1) {\n+                        r1 = g;\n+                        g = r1;\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                abstract class Low<T, U> {}\n+                abstract class Mid<T, U> extends Low<U, T> {}\n+                abstract class High<T, U> extends Mid<U, T> {}\n+\n+                primitive class GenericInlineTest<T, U> extends High<U, T> {\n+                    void foo(GenericInlineTest<String, Integer> g, GenericInlineTest.ref<Integer, String> r2) {\n+                        r2 = g;\n+                        g = r2;\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                abstract class Low<T, U> {}\n+                abstract class Mid<T, U> extends Low<U, T> {}\n+                abstract class High<T, U> extends Mid<U, T> {}\n+\n+                primitive class GenericInlineTest<T, U> extends High<U, T> {\n+                    void foo(GenericInlineTest<String, Integer> g, GenericInlineTest.ref<Integer, String> r2) {\n+                        r2 = g;\n+                        g = (GenericInlineTest<String, Integer>) r2;\n+                    }\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testValRefTokensNegative() {\n+        assertFail(\"compiler.err.cant.resolve.location\",\n+                \"\"\"\n+                class ValRefTokensNegativeTest {\n+                    ValRefTokensNegativeTest.ref aa = null;\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.cant.resolve.location\",\n+                \"\"\"\n+                class ValRefTokensNegativeTest {\n+                    static ValRefTokensNegativeTest.val bb = ValRefTokensNegativeTest.default;\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                class ValRefTokensNegativeTest {\n+                    EmptyValue empty = EmptyValue.default;\n+\n+                    static class ValRefTokensTestWrapper {\n+                        ValRefTokensNegativeTest val = ValRefTokensNegativeTest.default;\n+                        ValRefTokensNegativeTest ref = ValRefTokensNegativeTest.default;\n+                    }\n+\n+                    public EmptyValue test(int x) {\n+                        ValRefTokensTestWrapper w = new ValRefTokensTestWrapper();\n+                        return x == 0 ? w.val.empty : w.ref.empty;\n+                    }\n+\n+                    static class EmptyValue {\n+                        static int x = 42;\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.cant.resolve.location\",\n+                \"\"\"\n+                class ValRefTokensNegativeTest {\n+                    int valx() {\n+                        return EmptyValue.val.x;\n+                    }\n+\n+                    static class EmptyValue {\n+                        static int x = 42;\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.cant.resolve.location\",\n+                \"\"\"\n+                class ValRefTokensNegativeTest {\n+                    int refx() {\n+                        return EmptyValue.ref.x;\n+                    }\n+                    static class EmptyValue {\n+                        static int x = 42;\n+                    }\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testPrimitiveAsTypeName() {\n+        String[] previousOptions = getCompileOptions();\n+        try {\n+            String[] testOptions = {\"--source\", \"16\"};\n+            setCompileOptions(testOptions);\n+            assertFail(\"compiler.err.primitive.classes.not.supported\",\n+                    \"\"\"\n+                    class primitive {\n+                        primitive x;\n+                        primitive foo(int l) {}\n+                        Object o = new primitive primitive() {};\n+                    }\n+                    \"\"\");\n+            setCompileOptions(previousOptions);\n+            assertFail(\"compiler.err.restricted.type.not.allowed\",\n+                    \"\"\"\n+                    class primitive {}\n+                    \"\"\");\n+        } finally {\n+            setCompileOptions(previousOptions);\n+        }\n+    }\n+\n+    public void testMiscThisLeak() {\n+        assertFail(\"compiler.err.this.exposed.prematurely\",\n+                \"\"\"\n+                class MiscThisLeak {\n+                    interface I {\n+                        void foo();\n+                    }\n+                    primitive class V {\n+                        int f;\n+                        V() {\n+                            I i = this::foo;\n+                        }\n+\n+                        void foo() {}\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                class MiscThisLeak {\n+                    interface I {\n+                        void foo();\n+                    }\n+                    primitive class V {\n+                        int f;\n+                        V() {\n+                            I i = MiscThisLeak.this::foo;\n+                            f = 10;\n+                        }\n+\n+                        void foo() {}\n+                    }\n+                    void foo() {}\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.this.exposed.prematurely\",\n+                \"\"\"\n+                class MiscThisLeak {\n+                    interface I {\n+                        void foo();\n+                    }\n+                    primitive class V {\n+                        class K {}\n+                        int f;\n+                        V() {\n+                            new K();\n+                            f = 10;\n+                        }\n+\n+                        void foo() {}\n+                    }\n+                    void foo() {}\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.this.exposed.prematurely\",\n+                \"\"\"\n+                class MiscThisLeak {\n+                    interface I {\n+                        void foo();\n+                    }\n+                    primitive class V {\n+                        class K {}\n+                        int f;\n+                        V() {\n+                            this.new K();\n+                            f = 10;\n+                        }\n+\n+                        void foo() {}\n+                    }\n+                    void foo() {}\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                class MiscThisLeak {\n+                    interface I {\n+                        void foo();\n+                    }\n+                    primitive class V {\n+                        class K {}\n+                        int f;\n+                        V() {\n+                            f = 10;\n+                            I i = this::foo;\n+                        }\n+                        void foo() {}\n+                    }\n+                    void foo() {}\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testCovariantArrayTest() {\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                class CovariantArrayTest {\n+                    primitive class V {\n+                        public final int v1;\n+                        private V () { v1 = 0; }\n+                    }\n+                    void m(int[] ia, Object[] oa) {\n+                        oa = (Object[])ia;\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                class CovariantArrayTest {\n+                    primitive class V {\n+                        public final int v1;\n+                        private V () { v1 = 0; }\n+                    }\n+                    void m(int[] ia, Object[] oa) {\n+                        oa = ia;\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                class CovariantArrayTest {\n+                    primitive class V {\n+                        public final int v1;\n+                        private V () { v1 = 0; }\n+                    }\n+                    void m(int[] ia, Object[] oa) {\n+                        V[] va = new V[1];\n+                        Object[] oa2 = (Object[])va;\n+                        oa2 = va;\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                class CovariantArrayTest {\n+                    primitive class V {\n+                        public final int v1;\n+                        private V () { v1 = 0; }\n+                    }\n+                    void m(int[] ia, Object[] oa) {\n+                        V[] va = new V[1];\n+                        Object[] oa2 = (Object[])va;\n+                        oa2 = va;\n+                        va = oa2;\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                class CovariantArrayTest {\n+                    primitive class V {\n+                        public final int v1;\n+                        private V () { v1 = 0; }\n+                    }\n+                    void m(int[] ia, Object[] oa) {\n+                        V[] va = new V[1];\n+                        Object[] oa2 = (Object[])va;\n+                        oa2 = va;\n+                        va = (V[]) oa2;\n+                    }\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testConflictingSuperInterface() {\n+        assertFail(\"compiler.err.cant.inherit.diff.arg\",\n+                \"\"\"\n+                class ConflictingSuperInterfaceTest {\n+                    interface I<T> {}\n+                    static abstract class S implements I<String> {}\n+                    primitive static class Foo extends S implements I<Integer> {\n+                        String s = \"\";\n+                    }\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testClassLiteralTypingNegativeTest() {\n+        String[] previousOptions = getCompileOptions();\n+        try {\n+            String[] testOptions = {\"--add-exports\", \"java.base\/jdk.internal.value=ALL-UNNAMED\", \"-XDenablePrimitiveClasses\"};\n+            setCompileOptions(testOptions);\n+            assertFail(\"compiler.err.prob.found.req\",\n+                    \"\"\"\n+                    import jdk.internal.value.PrimitiveClass;\n+                    class ClassLiteralTypingNegativeTest {\n+                        interface I {}\n+                        static primitive class Foo implements I {\n+                            final int value = 0;\n+                            void m() {\n+                                Class<? extends Foo.ref> cFooRef = PrimitiveClass.asValueType(Foo.class);\n+                            }\n+                        }\n+                    }\n+                    \"\"\");\n+            assertOK(\n+                    \"\"\"\n+                    import jdk.internal.value.PrimitiveClass;\n+                    class ClassLiteralTypingNegativeTest {\n+                        interface I {}\n+                        static primitive class Foo implements I {\n+                            final int value = 0;\n+                            void m() {\n+                                Class<? extends Foo.ref> cFooRef = new Foo().getClass();\n+                            }\n+                        }\n+                    }\n+                    \"\"\");\n+            assertOK(\n+                    \"\"\"\n+                    import jdk.internal.value.PrimitiveClass;\n+                    class ClassLiteralTypingNegativeTest {\n+                        interface I {}\n+                        static primitive class Foo implements I {\n+                            final int value = 0;\n+                            void m() {\n+                                Class<? extends Foo.ref> cFooRef = Foo.ref.class;\n+                            }\n+                        }\n+                    }\n+                    \"\"\");\n+            assertFail(\"compiler.err.prob.found.req\",\n+                    \"\"\"\n+                    import jdk.internal.value.PrimitiveClass;\n+                    class ClassLiteralTypingNegativeTest {\n+                        interface I {}\n+                        static primitive class Foo implements I {\n+                            final int value = 0;\n+                            void m() {\n+                                Class<? extends Foo.ref> cFooRef = Foo.val.class;\n+                            }\n+                        }\n+                    }\n+                    \"\"\");\n+            assertOK(\n+                    \"\"\"\n+                    import jdk.internal.value.PrimitiveClass;\n+                    class ClassLiteralTypingNegativeTest {\n+                        interface I {}\n+                        static primitive class Foo implements I {\n+                            final int value = 0;\n+                            void m() {\n+                                Foo.val xv = new Foo();\n+                                Class<? extends Foo.ref> cFooRef = xv.getClass();\n+                            }\n+                        }\n+                    }\n+                    \"\"\");\n+            assertOK(\n+                    \"\"\"\n+                    import jdk.internal.value.PrimitiveClass;\n+                    class ClassLiteralTypingNegativeTest {\n+                        interface I {}\n+                        static primitive class Foo implements I {\n+                            final int value = 0;\n+                            void m() {\n+                                Foo.ref xr = new Foo();\n+                                Class<? extends Foo.ref> cFooRef = xr.getClass();\n+                            }\n+                        }\n+                    }\n+                    \"\"\");\n+        } finally {\n+            setCompileOptions(previousOptions);\n+        }\n+    }\n+}\n","filename":"test\/langtools\/tools\/javac\/valhalla\/primitive-classes\/PrimitiveClassesCompilationTests.java","additions":1269,"deletions":0,"binary":false,"changes":1269,"status":"added"},{"patch":"@@ -0,0 +1,849 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * ValueObjectCompilationTests\n+ *\n+ * @test\n+ * @bug 8287136 8292630 8279368 8287136 8287770 8279840 8279672 8292753 8287763 8279901 8287767 8293183 8293120\n+ * @summary Negative compilation tests, and positive compilation (smoke) tests for Value Objects\n+ * @library \/lib\/combo \/tools\/lib\n+ * @modules\n+ *     jdk.compiler\/com.sun.tools.javac.util\n+ *     jdk.compiler\/com.sun.tools.javac.api\n+ *     jdk.compiler\/com.sun.tools.javac.main\n+ *     jdk.compiler\/com.sun.tools.javac.code\n+ *     jdk.jdeps\/com.sun.tools.classfile\n+ * @build toolbox.ToolBox toolbox.JavacTask\n+ * @run testng ValueObjectCompilationTests\n+ *\/\n+\n+import java.io.File;\n+\n+import java.util.List;\n+\n+import com.sun.tools.classfile.ClassFile;\n+import com.sun.tools.classfile.Code_attribute;\n+import com.sun.tools.classfile.ConstantPool;\n+import com.sun.tools.classfile.ConstantPool.CONSTANT_Class_info;\n+import com.sun.tools.classfile.ConstantPool.CONSTANT_Fieldref_info;\n+import com.sun.tools.classfile.ConstantPool.CONSTANT_Methodref_info;\n+import com.sun.tools.classfile.Field;\n+import com.sun.tools.classfile.Instruction;\n+import com.sun.tools.classfile.Method;\n+\n+import com.sun.tools.javac.code.Flags;\n+\n+import static org.testng.Assert.assertTrue;\n+import org.testng.annotations.Test;\n+\n+import tools.javac.combo.CompilationTestCase;\n+\n+import toolbox.ToolBox;\n+\n+@Test\n+public class ValueObjectCompilationTests extends CompilationTestCase {\n+\n+    public ValueObjectCompilationTests() {\n+        setDefaultFilename(\"ValueObjectsTest.java\");\n+    }\n+\n+    public void testAbstractValueClassConstraints() {\n+        assertFail(\"compiler.err.instance.field.not.allowed\",\n+                \"\"\"\n+                abstract value class V {\n+                    int f;  \/\/ Error, abstract value class may not declare an instance field.\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.abstract.value.class.cannot.be.inner\",\n+                \"\"\"\n+                class Outer {\n+                    abstract value class V {\n+                        \/\/ Error, an abstract value class cant be an inner class\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.mod.not.allowed.here\",\n+                \"\"\"\n+                abstract value class V {\n+                    synchronized void foo() {\n+                     \/\/ Error, abstract value class may not declare a synchronized instance method.\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.abstract.value.class.declares.init.block\",\n+                \"\"\"\n+                abstract value class V {\n+                    { int f = 42; } \/\/ Error, abstract value class may not declare an instance initializer.\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.abstract.value.class.constructor.cannot.take.arguments\",\n+                \"\"\"\n+                abstract value class V {\n+                    V(int x) {}  \/\/ Error, abstract value class may not declare a non-trivial constructor.\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testAnnotationsConstraints() {\n+        assertFail(\"compiler.err.illegal.combination.of.modifiers\",\n+                \"\"\"\n+                identity @interface IA {}\n+                \"\"\");\n+        assertFail(\"compiler.err.illegal.combination.of.modifiers\",\n+                \"\"\"\n+                value @interface IA {}\n+                \"\"\");\n+    }\n+\n+    public void testCheckFeatureSourceLevel() {\n+        setCompileOptions(new String[]{\"--release\", \"13\"});\n+        assertFail(\"compiler.err.feature.not.supported.in.source.plural\",\n+                \"\"\"\n+                value class V {\n+                    public int v = 42;\n+                }\n+                \"\"\");\n+        setCompileOptions(new String[]{});\n+    }\n+\n+    public void testSuperClassConstraints() {\n+        assertFail(\"compiler.err.instance.field.not.allowed\",\n+                \"\"\"\n+                abstract class I { \/\/ identity class since it declares an instance field.\n+                    int f;\n+                }\n+                value class V extends I {}\n+                \"\"\");\n+\n+        assertFail(\"compiler.err.abstract.value.class.cannot.be.inner\",\n+                \"\"\"\n+                class Outer {\n+                    abstract class I {} \/\/ has identity since is an inner class\n+                    static value class V extends I\n+                }\n+                \"\"\");\n+\n+        assertFail(\"compiler.err.super.class.method.cannot.be.synchronized\",\n+                \"\"\"\n+                abstract class I { \/\/ has identity since it declared a synchronized instance method.\n+                    synchronized void foo() {}\n+                }\n+                value class V extends I {}\n+                \"\"\");\n+\n+        assertFail(\"compiler.err.abstract.value.class.declares.init.block\",\n+                \"\"\"\n+                abstract class I { \/\/ has identity since it declares an instance initializer\n+                    { int f = 42; }\n+                }\n+                value class V extends I {}\n+                \"\"\");\n+\n+        assertFail(\"compiler.err.abstract.value.class.constructor.cannot.take.arguments\",\n+                \"\"\"\n+                abstract class I { \/\/ has identity since it declares a non-trivial constructor\n+                    I(int x) {}\n+                }\n+                value class V extends I {}\n+                \"\"\");\n+        assertFail(\"compiler.err.concrete.supertype.for.value.class\",\n+                \"\"\"\n+                class ConcreteSuperType {\n+                    static abstract value class V extends ConcreteSuperType {}  \/\/ Error: concrete super.\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testSynchronizeOnValueInterfaceInstance() {\n+        assertFail(\"compiler.err.type.found.req\",\n+                \"\"\"\n+                value interface VI {\n+                    default void foo(VI vi) {\n+                        synchronized (vi) {} \/\/ Error\n+                    }\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testRepeatedModifiers() {\n+        String[] previousOptions = getCompileOptions();\n+        try {\n+            String[] testOptions = {\"-XDenablePrimitiveClasses\"};\n+            setCompileOptions(testOptions);\n+            String[] sources = new String[] {\n+                    \"static static class StaticTest {}\",\n+                    \"native native class NativeTest {}\",\n+                    \"value value primitive class ValueTest {}\",\n+                    \"primitive primitive value class PrimitiveTest {}\"\n+            };\n+            for (String source : sources) {\n+                assertFail(\"compiler.err.repeated.modifier\", source);\n+            }\n+        } finally {\n+            setCompileOptions(previousOptions);\n+        }\n+    }\n+\n+    public void testParserTest() {\n+        assertOK(\n+                \"\"\"\n+                value class Substring implements CharSequence {\n+                    private String str;\n+                    private int start;\n+                    private int end;\n+\n+                    public Substring(String str, int start, int end) {\n+                        checkBounds(start, end, str.length());\n+                        this.str = str;\n+                        this.start = start;\n+                        this.end = end;\n+                    }\n+\n+                    public int length() {\n+                        return end - start;\n+                    }\n+\n+                    public char charAt(int i) {\n+                        checkBounds(0, i, length());\n+                        return str.charAt(start + i);\n+                    }\n+\n+                    public Substring subSequence(int s, int e) {\n+                        checkBounds(s, e, length());\n+                        return new Substring(str, start + s, start + e);\n+                    }\n+\n+                    public String toString() {\n+                        return str.substring(start, end);\n+                    }\n+\n+                    private static void checkBounds(int start, int end, int length) {\n+                        if (start < 0 || end < start || length < end)\n+                            throw new IndexOutOfBoundsException();\n+                    }\n+                }\n+                \"\"\"\n+        );\n+    }\n+\n+    public void testSemanticsViolations() {\n+        assertFail(\"compiler.err.cant.inherit.from.final\",\n+                \"\"\"\n+                value class Base {}\n+                class Subclass extends Base {}\n+                \"\"\");\n+        assertFail(\"compiler.err.abstract.value.class.cannot.be.inner\",\n+                \"\"\"\n+                class Outer {\n+                    abstract value class AbsValue {}\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.cant.assign.val.to.var\",\n+                \"\"\"\n+                value class Point {\n+                    int x = 10;\n+                    int y;\n+                    Point (int x, int y) {\n+                        this.x = x; \/\/ Error, final field 'x' is already assigned to.\n+                        this.y = y; \/\/ OK.\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.cant.assign.val.to.var\",\n+                \"\"\"\n+                value class Point {\n+                    int x;\n+                    int y;\n+                    Point (int x, int y) {\n+                        this.x = x;\n+                        this.y = y;\n+                    }\n+\n+                    void foo(Point p) {\n+                        this.y = p.y; \/\/ Error, y is final and can't be written outside of ctor.\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.var.might.not.have.been.initialized\",\n+                \"\"\"\n+                value class Point {\n+                    int x;\n+                    int y;\n+                    Point (int x, int y) {\n+                        this.x = x;\n+                        \/\/ y hasn't been initialized\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.illegal.combination.of.modifiers\",\n+                \"\"\"\n+                value identity class ValueIdentity {}\n+                \"\"\");\n+        assertFail(\"compiler.err.illegal.combination.of.modifiers\",\n+                \"\"\"\n+                identity value class IdentityValue {}\n+                \"\"\");\n+        assertFail(\"compiler.err.call.to.super.not.allowed.in.value.ctor\",\n+                \"\"\"\n+                value class V {\n+                    V() {\n+                        super();\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.mod.not.allowed.here\",\n+                \"\"\"\n+                value class V {\n+                    synchronized void foo() {}\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                value class V {\n+                    synchronized static void soo() {}\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.type.found.req\",\n+                \"\"\"\n+                value class V {\n+                    { synchronized(this) {} }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.mod.not.allowed.here\",\n+                \"\"\"\n+                value record R() {\n+                    synchronized void foo() { } \/\/ Error;\n+                    synchronized static void soo() {} \/\/ OK.\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.this.exposed.prematurely\",\n+                \"\"\"\n+                value class V {\n+                    int x;\n+                    V() {\n+                        foo(this); \/\/ Error.\n+                        x = 10;\n+                    }\n+                    void foo(V v) {}\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                value class V {\n+                    int x;\n+                    V() {\n+                        x = 10;\n+                        foo(this); \/\/ Ok.\n+                    }\n+                    void foo(V v) {}\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.type.found.req\",\n+                \"\"\"\n+                interface I {}\n+                value interface VI extends I {}\n+                class C {}\n+                value class VC<T extends VC> {\n+                    void m(T t) {\n+                        synchronized(t) {} \/\/ error\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.type.found.req\",\n+                \"\"\"\n+                interface I {}\n+                value interface VI extends I {}\n+                class C {}\n+                value class VC<T extends VC> {\n+                    void foo(Object o) {\n+                        synchronized ((VC & I)o) {} \/\/ error\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.type.found.req\",\n+                \"\"\"\n+                interface I {}\n+                value interface VI extends I {}\n+                class C {}\n+                value class VC<T extends VC> {\n+                    void bar(Object o) {\n+                        synchronized ((I & VI)o) {} \/\/ error\n+                    }\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testNontrivialConstructor() {\n+        assertOK(\n+                \"\"\"\n+                abstract value class V {\n+                    public V() {} \/\/ trivial ctor\n+                }\n+                \"\"\"\n+        );\n+        assertFail(\"compiler.err.abstract.value.class.constructor.has.weaker.access\",\n+                \"\"\"\n+                abstract value class V {\n+                    private V() {} \/\/ non-trivial, more restrictive access than the class.\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.abstract.value.class.constructor.cannot.take.arguments\",\n+                \"\"\"\n+                abstract value class V {\n+                    public V(int x) {} \/\/ non-trivial ctor as it declares formal parameters.\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.abstract.value.class.constructor.cannot.be.generic\",\n+                \"\"\"\n+                abstract value class V {\n+                    <T> V() {} \/\/ non trivial as it declares type parameters.\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.abstract.value.class.constructor.cannot.throw\",\n+                \"\"\"\n+                abstract value class V {\n+                    V() throws Exception {} \/\/ non-trivial as it throws\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.abstract.value.class.no.arg.constructor.must.be.empty\",\n+                \"\"\"\n+                abstract value class V {\n+                    V() {\n+                        System.out.println(\"\");\n+                    } \/\/ non-trivial as it has a body.\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testFunctionalInterface() {\n+        assertFail(\"compiler.err.bad.functional.intf.anno.1\",\n+                \"\"\"\n+                @FunctionalInterface\n+                identity interface I { \/\/ Error\n+                    void m();\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.bad.functional.intf.anno.1\",\n+                \"\"\"\n+                @FunctionalInterface\n+                value interface K { \/\/ Error\n+                    void m();\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                identity interface L {\n+                    void m();\n+                }\n+                class Test {\n+                    void foo() {\n+                        var t = (L) () -> {}; \/\/ Error\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                value interface M {\n+                    void m();\n+                }\n+                class Test {\n+                    void foo() {\n+                        var u = (M) () -> {}; \/\/ Error\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.bad.functional.intf.anno.1\",\n+                \"\"\"\n+                identity interface I {\n+                    void m();\n+                }\n+\n+                @FunctionalInterface\n+                interface J extends I  {}\n+                \"\"\");\n+        assertFail(\"compiler.err.bad.functional.intf.anno.1\",\n+                \"\"\"\n+                value interface I {\n+                    void m();\n+                }\n+\n+                @FunctionalInterface\n+                interface J extends I  {}\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                identity interface I {}\n+                interface K extends I {}\n+                interface J {\n+                    void m();\n+                }\n+                class Test {\n+                    void foo() {\n+                        J j = (J&K)() -> {};\n+                    }\n+                }\n+                \"\"\");\n+        assertFail(\"compiler.err.prob.found.req\",\n+                \"\"\"\n+                value interface I {}\n+                interface K extends I {}\n+                interface J {\n+                    void m();\n+                }\n+                class Test {\n+                    void foo() {\n+                        J j = (J&K)() -> {};\n+                    }\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testSupers() {\n+        assertFail(\"compiler.err.mutually.incompatible.supers\",\n+                \"\"\"\n+                identity interface II {}\n+                value interface VI {}\n+                abstract class X implements II, VI {}\n+                \"\"\");\n+        assertFail(\"compiler.err.value.type.has.identity.super.type\",\n+                \"\"\"\n+                identity interface II {}\n+                interface GII extends II {} \/\/ OK.\n+                value interface BVI extends GII {} \/\/ Error\n+                \"\"\");\n+        assertFail(\"compiler.err.identity.type.has.value.super.type\",\n+                \"\"\"\n+                value interface VI {}\n+                interface GVI extends VI {} \/\/ OK.\n+                identity interface BII extends GVI {} \/\/ Error\n+                \"\"\");\n+        assertFail(\"compiler.err.value.type.has.identity.super.type\",\n+                \"\"\"\n+                identity interface II {}\n+                value class BVC implements II {} \/\/ Error\n+                \"\"\");\n+        assertFail(\"compiler.err.identity.type.has.value.super.type\",\n+                \"\"\"\n+                value interface VI {}\n+                class BIC implements VI {} \/\/ Error\n+                \"\"\");\n+        assertFail(\"compiler.err.identity.type.has.value.super.type\",\n+                \"\"\"\n+                value interface I {}\n+                class Test {\n+                    I i = new I() {};\n+                }\n+                \"\"\");\n+    }\n+\n+    public void testInteractionWithSealedClasses() {\n+        assertOK(\n+                \"\"\"\n+                abstract sealed value class SC {}\n+                value class VC extends SC {}\n+                \"\"\"\n+        );assertOK(\n+                \"\"\"\n+                abstract sealed value interface SI {}\n+                value class VC implements SI {}\n+                \"\"\"\n+        );\n+        assertOK(\n+                \"\"\"\n+                abstract sealed identity class SC {}\n+                final identity class IC extends SC {}\n+                non-sealed identity class IC2 extends SC {}\n+                final identity class IC3 extends IC2 {}\n+                \"\"\"\n+        );\n+        assertOK(\n+                \"\"\"\n+                abstract sealed identity interface SI {}\n+                final identity class IC implements SI {}\n+                non-sealed identity class IC2 implements SI {}\n+                final identity class IC3 extends IC2 {}\n+                \"\"\"\n+        );\n+        assertFail(\"compiler.err.mod.not.allowed.here\",\n+                \"\"\"\n+                abstract sealed value class SC {}\n+                non-sealed value class VC extends SC {}\n+                \"\"\"\n+        );\n+        assertFail(\"compiler.err.mod.not.allowed.here\",\n+                \"\"\"\n+                sealed value interface SI {}\n+                non-sealed value class VC implements SI {}\n+                \"\"\"\n+        );\n+    }\n+\n+    public void testCheckClassFileFlags() throws Exception {\n+        for (String source : List.of(\n+                \"\"\"\n+                interface I {}\n+                class Test {\n+                    I i = new I() {};\n+                }\n+                \"\"\",\n+                \"\"\"\n+                class C {}\n+                class Test {\n+                    C c = new C() {};\n+                }\n+                \"\"\",\n+                \"\"\"\n+                class Test {\n+                    Object o = new Object() {};\n+                }\n+                \"\"\",\n+                \"\"\"\n+                class Test {\n+                    \/\/ abstract inner class is implicitly an `identity` class\n+                    abstract class Inner {}\n+                }\n+                \"\"\"\n+        )) {\n+            File dir = assertOK(true, source);\n+            for (final File fileEntry : dir.listFiles()) {\n+                if (fileEntry.getName().contains(\"$\")) {\n+                    ClassFile classFile = ClassFile.read(fileEntry);\n+                    assertTrue((classFile.access_flags.flags & Flags.ACC_IDENTITY) != 0);\n+                }\n+            }\n+        }\n+\n+        for (String source : List.of(\n+                \"\"\"\n+                identity interface I {}\n+                class Sub implements I {}\n+                \"\"\",\n+                \"\"\"\n+                abstract class A {\n+                    \/\/ declares a non-static field so it is implicitly an identity class\n+                    int i;\n+                }\n+                \"\"\",\n+                \"\"\"\n+                abstract class A {\n+                    \/\/ declares a synchronized method so it is implicitly an identity class\n+                    synchronized void m() {}\n+                }\n+                \"\"\",\n+                \"\"\"\n+                class C {\n+                    \/\/ declares a synchronized method so it is implicitly an identity class\n+                    synchronized void m() {}\n+                }\n+                \"\"\",\n+                \"\"\"\n+                abstract class A {\n+                    int i;\n+                    \/\/ declares an instance initializer so it is implicitly an identity class\n+                    { i = 0; }\n+                }\n+                \"\"\",\n+                \"\"\"\n+                abstract class A {\n+                    \/\/ declares a non-trivial constructor\n+                    A(int i) {}\n+                }\n+                \"\"\",\n+                \"\"\"\n+                    enum E {}\n+                \"\"\",\n+                \"\"\"\n+                    identity enum E {}\n+                \"\"\",\n+                \"\"\"\n+                    record R() {}\n+                \"\"\",\n+                \"\"\"\n+                   identity record R() {}\n+                \"\"\"\n+        )) {\n+            File dir = assertOK(true, source);\n+            for (final File fileEntry : dir.listFiles()) {\n+                ClassFile classFile = ClassFile.read(fileEntry);\n+                assertTrue(classFile.access_flags.is(Flags.ACC_IDENTITY));\n+                assertTrue(!classFile.access_flags.is(Flags.VALUE_CLASS));\n+            }\n+        }\n+\n+        {\n+            String source =\n+                    \"\"\"\n+                            value interface I {}\n+                            abstract class A implements I {} \/\/ not a value class as it doens't have the value modifier\n+                            value class Sub extends A {} \/\/implicitly final\n+                            \"\"\";\n+            File dir = assertOK(true, source);\n+            for (final File fileEntry : dir.listFiles()) {\n+                ClassFile classFile = ClassFile.read(fileEntry);\n+                switch (classFile.getName()) {\n+                    case \"Sub\":\n+                        assertTrue((classFile.access_flags.flags & (Flags.VALUE_CLASS | Flags.FINAL)) != 0);\n+                        break;\n+                    case \"A\":\n+                        assertTrue((classFile.access_flags.flags & (Flags.ABSTRACT)) != 0);\n+                        break;\n+                    case \"I\":\n+                        assertTrue((classFile.access_flags.flags & (Flags.INTERFACE | Flags.VALUE_CLASS)) != 0);\n+                        break;\n+                    default:\n+                        throw new AssertionError(\"you shoulnd't be here\");\n+                }\n+            }\n+        }\n+\n+        for (String source : List.of(\n+                \"\"\"\n+                value class V {\n+                    int i = 0;\n+                    static int j;\n+                }\n+                \"\"\",\n+                \"\"\"\n+                abstract value class A {\n+                    static int j;\n+                }\n+\n+                value class V extends A {\n+                    int i = 0;\n+                }\n+                \"\"\"\n+        )) {\n+            File dir = assertOK(true, source);\n+            for (final File fileEntry : dir.listFiles()) {\n+                ClassFile classFile = ClassFile.read(fileEntry);\n+                for (Field field : classFile.fields) {\n+                    if (!field.access_flags.is(Flags.STATIC)) {\n+                        assertTrue(field.access_flags.is(Flags.FINAL));\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    public void testCheckVnew() throws Exception {\n+        for (String source : List.of(\n+                \"\"\"\n+                abstract value class A {}\n+                \"\"\",\n+                \"\"\"\n+                value class A {}\n+                \"\"\"\n+        )) {\n+            File dir = assertOK(true, source);\n+            for (final File fileEntry : dir.listFiles()) {\n+                ClassFile classFile = ClassFile.read(fileEntry);\n+                boolean isAbstract = classFile.access_flags.is(Flags.ABSTRACT);\n+                for (Method method : classFile.methods) {\n+                    if (isAbstract) {\n+                        assertTrue(method.getName(classFile.constant_pool).equals(\"<init>\"));\n+                        assertTrue(!method.access_flags.is(Flags.STATIC));\n+                    } else {\n+                        assertTrue(method.getName(classFile.constant_pool).equals(\"<vnew>\"));\n+                        assertTrue(method.access_flags.is(Flags.STATIC));\n+                        assertTrue(!method.access_flags.is(Flags.ABSTRACT));\n+                        assertTrue(method.descriptor.getReturnType(classFile.constant_pool).equals(\"A\"));\n+                    }\n+                }\n+            }\n+        }\n+\n+        \/\/ check that <vnew> is invoked with invokestatic\n+        for (String source : List.of(\n+                \"\"\"\n+                value class A {\n+                    void FIND_ME() {\n+                        A a = new A();\n+                    }\n+                }\n+                \"\"\"\n+        )) {\n+            File dir = assertOK(true, source);\n+            for (final File fileEntry : dir.listFiles()) {\n+                ClassFile classFile = ClassFile.read(fileEntry);\n+                for (Method method : classFile.methods) {\n+                    if (method.getName(classFile.constant_pool).equals(\"FIND_ME\")) {\n+                        Code_attribute code_attribute = (Code_attribute)method.attributes.get(\"Code\");\n+                        boolean firstInst = true;\n+                        for (Instruction inst: code_attribute.getInstructions()) {\n+                            if (firstInst) {\n+                                assertTrue(inst.getMnemonic().equals(\"invokestatic\"));\n+                                CONSTANT_Methodref_info methodInfo =\n+                                        (CONSTANT_Methodref_info)classFile.constant_pool.get(inst.getUnsignedShort(1));\n+                                assertTrue(methodInfo.getClassInfo().getName().equals(\"A\"));\n+                                assertTrue(methodInfo.getNameAndTypeInfo().getName().equals(\"<vnew>\"));\n+                                break;\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        \/\/ checking the aconst_init and withfield instructions\n+        for (String source : List.of(\n+                \"\"\"\n+                value class A {\n+                    int i;\n+                    String s;\n+\n+                    A(int i, String s) {\n+                        this.i = i;\n+                        this.s = s;\n+                    }\n+                }\n+                \"\"\"\n+        )) {\n+            File dir = assertOK(true, source);\n+            for (final File fileEntry : dir.listFiles()) {\n+                ClassFile classFile = ClassFile.read(fileEntry);\n+                for (Method method : classFile.methods) {\n+                    if (method.getName(classFile.constant_pool).equals(\"<vnew>\")) {\n+                        Code_attribute code_attribute = (Code_attribute)method.attributes.get(\"Code\");\n+                        for (Instruction inst: code_attribute.getInstructions()) {\n+                            if (inst.getMnemonic().equals(\"aconst_init\")) {\n+                                CONSTANT_Class_info classInfo =\n+                                        (CONSTANT_Class_info)classFile.constant_pool.get(inst.getUnsignedShort(1));\n+                                assertTrue(classInfo.getName().equals(\"A\"));\n+                            } else if (inst.getMnemonic().equals(\"withfield\")) {\n+                                CONSTANT_Fieldref_info fieldInfo = (CONSTANT_Fieldref_info)classFile.constant_pool.get(inst.getUnsignedShort(1));\n+                                assertTrue(fieldInfo.getClassName().equals(\"A\"));\n+                                ConstantPool.CONSTANT_NameAndType_info nameAndType = fieldInfo.getNameAndTypeInfo();\n+                                if (nameAndType.getName().equals(\"i\")) {\n+                                    assertTrue(nameAndType.getType().equals(\"I\"));\n+                                } else if (nameAndType.getName().equals(\"s\")) {\n+                                    assertTrue(nameAndType.getType().equals(\"Ljava\/lang\/String;\"));\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+}\n","filename":"test\/langtools\/tools\/javac\/valhalla\/value-objects\/ValueObjectCompilationTests.java","additions":849,"deletions":0,"binary":false,"changes":849,"status":"added"},{"patch":"@@ -0,0 +1,12 @@\n+WithFieldNegativeTests.java:16:14: compiler.err.cant.assign.val.to.var: final, x\n+WithFieldNegativeTests.java:17:14: compiler.err.cant.assign.val.to.var: static final, sx\n+WithFieldNegativeTests.java:22:16: compiler.err.cant.assign.val.to.var: final, x\n+WithFieldNegativeTests.java:26:10: compiler.err.cant.assign.val.to.var: final, x\n+WithFieldNegativeTests.java:27:12: compiler.err.cant.assign.val.to.var: final, x\n+WithFieldNegativeTests.java:28:11: compiler.err.cant.assign.val.to.var: final, x\n+WithFieldNegativeTests.java:29:9: compiler.err.cant.assign.val.to.var: final, x\n+WithFieldNegativeTests.java:30:13: compiler.err.cant.assign.val.to.var: final, x\n+WithFieldNegativeTests.java:31:15: compiler.err.cant.assign.val.to.var: final, x\n+WithFieldNegativeTests.java:37:10: compiler.err.cant.assign.val.to.var: final, x\n+WithFieldNegativeTests.java:38:10: compiler.err.cant.assign.val.to.var: static final, sx\n+11 errors\n","filename":"test\/langtools\/tools\/javac\/valhalla\/value-objects\/WithFieldNegativeTests.out","additions":12,"deletions":0,"binary":false,"changes":12,"status":"added"},{"patch":"@@ -154,0 +154,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"}]}