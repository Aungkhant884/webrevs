{"files":[{"patch":"@@ -1889,0 +1889,2 @@\n+  __ verified_entry(C, 0);\n+  __ bind(*_verified_entry);\n@@ -2279,1 +2281,31 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n+\n+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+{\n+  MacroAssembler _masm(&cbuf);\n+\n+  if (!_verified) {\n+    Label skip;\n+    __ cmp_klass(j_rarg0, rscratch2, rscratch1);\n+    __ br(Assembler::EQ, skip);\n+      __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+    __ bind(skip);\n+\n+  } else {\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    __ unpack_inline_args(ra_->C, _receiver_only);\n+    __ b(*_verified_entry);\n+  }\n+}\n@@ -2281,0 +2313,8 @@\n+\n+uint MachVEPNode::size(PhaseRegAlloc* ra_) const\n+{\n+  return MachNode::size(ra_); \/\/ too many variables; just compute it the hard way\n+}\n+\n+\n+\/\/=============================================================================\n@@ -2302,0 +2342,1 @@\n+  Label skip;\n@@ -2303,0 +2344,1 @@\n+  \/\/ UseCompressedClassPointers logic are inside cmp_klass\n@@ -2304,1 +2346,1 @@\n-  Label skip;\n+\n@@ -2733,1 +2775,0 @@\n-\n@@ -8680,0 +8721,15 @@\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -8695,0 +8751,31 @@\n+instruct castN2I(iRegINoSp dst, iRegN src) %{\n+  match(Set dst (CastN2I src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"movw $dst, $src\\t# compressed ptr -> int\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct castI2N(iRegNNoSp dst, iRegI src) %{\n+  match(Set dst (CastI2N src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"movw $dst, $src\\t# int -> compressed ptr\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\n@@ -14679,1 +14766,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n@@ -14681,1 +14768,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":92,"deletions":5,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -1308,1 +1309,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1338,1 +1343,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1431,0 +1440,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -1480,0 +1493,33 @@\n+void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label& is_value) {\n+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  andr(temp_reg, temp_reg, JVM_ACC_INLINE);\n+  cbnz(temp_reg, is_value);\n+}\n+\n+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_inline_field_shift, is_inline);\n+}\n+\n+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbz(flags, ConstantPoolCacheEntry::is_inline_field_shift, not_inline);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label& is_flattened_array) {\n+  load_storage_props(temp_reg, oop);\n+  andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);\n+  cbnz(temp_reg, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  load_storage_props(temp_reg, oop);\n+  andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);\n+  cbnz(temp_reg, is_null_free_array);\n+}\n+\n@@ -3784,1 +3830,1 @@\n-void MacroAssembler::load_klass(Register dst, Register src) {\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n@@ -3787,1 +3833,0 @@\n-    decode_klass_not_null(dst);\n@@ -3793,0 +3838,10 @@\n+void MacroAssembler::load_klass(Register dst, Register src) {\n+  load_metadata(dst, src);\n+  if (UseCompressedClassPointers) {\n+    andr(dst, dst, oopDesc::compressed_klass_mask());\n+    decode_klass_not_null(dst);\n+  } else {\n+    ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);\n+  }\n+}\n+\n@@ -3824,0 +3879,9 @@\n+void MacroAssembler::load_storage_props(Register dst, Register src) {\n+  load_metadata(dst, src);\n+  if (UseCompressedClassPointers) {\n+    asrw(dst, dst, oopDesc::narrow_storage_props_shift);\n+  } else {\n+    asr(dst, dst, oopDesc::wide_storage_props_shift);\n+  }\n+}\n+\n@@ -4161,1 +4225,2 @@\n-                                     Register tmp1, Register thread_tmp) {\n+                                     Register tmp1, Register thread_tmp, Register tmp3) {\n+\n@@ -4166,1 +4231,1 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4168,1 +4233,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4192,2 +4257,2 @@\n-                                    Register thread_tmp, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);\n+                                    Register thread_tmp, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4198,1 +4263,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -5271,0 +5336,389 @@\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+\n+\/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->frame_size_in_bytes();\n+  assert(framesize % (2 * wordSize) == 0, \"must preserve 2 * wordSize alignment\");\n+\n+  \/\/ insert a nop at the start of the prolog so we can patch in a\n+  \/\/ branch if we need to invalidate the method later\n+  nop();\n+\n+  int bangsize = C->bang_size_in_bytes();\n+  if (C->need_stack_bang(bangsize) && UseStackBanging)\n+     generate_stack_overflow_check(bangsize);\n+\n+  build_frame(framesize);\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  cmp(r0, (u1) 1);\n+  br(Assembler::EQ, skip);\n+  int call_offset = -1;\n+\n+  Label slow_case;\n+\n+  \/\/ Try to allocate a new buffered inline type (from the heap)\n+  if (UseTLAB) {\n+\n+    if (vk != NULL) {\n+      \/\/ Called from C1, where the return type is statically known.\n+      mov(r1, (intptr_t)vk->get_InlineKlass());\n+      jint lh = vk->layout_helper();\n+      assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+      mov(r14, lh);\n+    } else {\n+       \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+       andr(r1, r0, -2);\n+       \/\/ get obj size\n+       ldrw(r14, Address(rscratch1 \/*klass*\/, Klass::layout_helper_offset()));\n+    }\n+\n+     ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+\n+     \/\/ check whether we have space in TLAB,\n+     \/\/ rscratch1 contains pointer to just allocated obj\n+      lea(r14, Address(r13, r14));\n+      ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));\n+\n+      cmp(r14, rscratch1);\n+      br(Assembler::GT, slow_case);\n+\n+      \/\/ OK we have room in TLAB,\n+      \/\/ Set new TLAB top\n+      str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+\n+      \/\/ Set new class always locked\n+      mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());\n+      str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));\n+\n+      store_klass_gap(r13, zr);  \/\/ zero klass gap for compressed oops\n+      if (vk == NULL) {\n+        \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n+         mov(r0, r1);\n+      }\n+\n+      store_klass(r13, r1);  \/\/ klass\n+\n+      if (vk != NULL) {\n+        \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+        mov(r0, r13);\n+        far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+      } else {\n+\n+        \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+        ldr(r1, Address(r0, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+        ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));\n+\n+        \/\/ Mov new class to r0 and call pack_handler\n+        mov(r0, r13);\n+        blr(r1);\n+      }\n+      b(skip);\n+  }\n+\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    ldr(rscratch1, RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    blr(rscratch1);\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        mov(to->as_Register(), from->as_Register());\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,\n+                                          int& to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {\n+  Register fromReg = from->is_reg() ? from->as_Register() : noreg;\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+\n+\n+  int vt = 1;\n+  bool done = true;\n+  bool mark_done = true;\n+  do {\n+    sig_index--;\n+    BasicType bt = sig->at(sig_index)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      vt--;\n+    } else if (bt == T_VOID &&\n+               sig->at(sig_index-1)._bt != T_LONG &&\n+               sig->at(sig_index-1)._bt != T_DOUBLE) {\n+      vt++;\n+    } else if (SigEntry::is_reserved_entry(sig, sig_index)) {\n+      to_index--; \/\/ Ignore this\n+    } else {\n+      assert(to_index >= 0, \"invalid to_index\");\n+      VMRegPair pair_to = regs_to[to_index--];\n+      VMReg to = pair_to.first();\n+\n+      if (bt == T_VOID) continue;\n+\n+      int idx = (int) to->value();\n+      if (reg_state[idx] == reg_readonly) {\n+         if (idx != from->value()) {\n+           mark_done = false;\n+         }\n+         done = false;\n+         continue;\n+      } else if (reg_state[idx] == reg_written) {\n+        continue;\n+      } else {\n+        assert(reg_state[idx] == reg_writable, \"must be writable\");\n+        reg_state[idx] = reg_written;\n+      }\n+\n+      if (fromReg == noreg) {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        ldr(rscratch2, Address(sp, st_off));\n+        fromReg = rscratch2;\n+      }\n+\n+      int off = sig->at(sig_index)._offset;\n+      assert(off > 0, \"offset in object should be positive\");\n+      bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+\n+      Address fromAddr = Address(fromReg, off);\n+      bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+\n+      if (!to->is_FloatRegister()) {\n+\n+        Register dst = to->is_stack() ? rscratch1 : to->as_Register();\n+\n+        if (is_oop) {\n+          load_heap_oop(dst, fromAddr);\n+        } else {\n+          load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+        }\n+        if (to->is_stack()) {\n+          int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+          str(dst, Address(sp, st_off));\n+        }\n+      } else {\n+        if (bt == T_DOUBLE) {\n+          ldrd(to->as_FloatRegister(), fromAddr);\n+        } else {\n+          assert(bt == T_FLOAT, \"must be float\");\n+          ldrs(to->as_FloatRegister(), fromAddr);\n+        }\n+     }\n+\n+    }\n+\n+  } while (vt != 0);\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],\n+                                        int ret_off, int extra_stack_offset) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"must be\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  Register val_array = r0;\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r10;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r1;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);\n+  VMRegPair from_pair;\n+  BasicType bt;\n+\n+  while (stream.next(from_pair, bt)) {\n+    int off = sig->at(stream.sig_cc_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    VMReg from_r1 = from_pair.first();\n+    VMReg from_r2 = from_pair.second();\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+\n+    if (!from_r1->is_FloatRegister()) {\n+      Register from_reg;\n+      if (from_r1->is_stack()) {\n+        from_reg = from_reg_tmp;\n+        int ld_off = from_r1->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        from_reg = from_r1->as_Register();\n+      }\n+\n+      if (is_oop) {\n+        DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;\n+        store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);\n+      } else {\n+        store_sized_value(dst, from_reg, size_in_bytes);\n+      }\n+    } else {\n+      if (from_r2->is_valid()) {\n+        strd(from_r1->as_FloatRegister(), dst);\n+      } else {\n+        strs(from_r1->as_FloatRegister(), dst);\n+      }\n+    }\n+\n+    reg_state[from_r1->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_cc_index();\n+  from_index = stream.regs_cc_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+\/\/ Unpack all inline type arguments passed as oops\n+void MacroAssembler::unpack_inline_args(Compile* C, bool receiver_only) {\n+  int sp_inc = unpack_inline_args_common(C, receiver_only);\n+  \/\/ Emit code for verified entry and save increment for stack repair on return\n+  verified_entry(C, sp_inc);\n+}\n+\n+int MacroAssembler::shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,\n+                                        BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,\n+                                        int args_passed, int args_on_stack, VMRegPair* regs,            \/\/ from\n+                                        int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { \/\/ to\n+  \/\/ Check if we need to extend the stack for packing\/unpacking\n+  int sp_inc = (args_on_stack_to - args_on_stack) * VMRegImpl::stack_slot_size;\n+  if (sp_inc > 0) {\n+    sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+    if (!is_packing) {\n+      \/\/ Save the return address, adjust the stack (make sure it is properly\n+      \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+      \/\/ (Note: C1 does this in C1_MacroAssembler::scalarized_entry).\n+      \/\/ FIXME: We need not to preserve return address on aarch64\n+      pop(rscratch1);\n+      sub(sp, sp, sp_inc);\n+      push(rscratch1);\n+    }\n+  } else {\n+    \/\/ The scalarized calling convention needs less stack space than the unscalarized one.\n+    \/\/ No need to extend the stack, the caller will take care of these adjustments.\n+    sp_inc = 0;\n+  }\n+\n+  int ret_off; \/\/ make sure we don't overwrite the return address\n+  if (is_packing) {\n+    \/\/ For C1 code, the VIEP doesn't have reserved slots, so we store the returned address at\n+    \/\/ rsp[0] during shuffling.\n+    ret_off = 0;\n+  } else {\n+    \/\/ C2 code ensures that sp_inc is a reserved slot.\n+    ret_off = sp_inc;\n+  }\n+\n+  return shuffle_inline_args_common(is_packing, receiver_only, extra_stack_offset,\n+                                    sig_bt, sig_cc,\n+                                    args_passed, args_on_stack, regs,\n+                                    args_passed_to, args_on_stack_to, regs_to,\n+                                    sp_inc, ret_off);\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v0->as_VMReg() : r14->as_VMReg();\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":464,"deletions":10,"binary":false,"changes":474,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -32,0 +33,4 @@\n+#include \"runtime\/signature.hpp\"\n+\n+\n+class ciInlineKlass;\n@@ -615,0 +620,10 @@\n+  void test_klass_is_value(Register klass, Register temp_reg, Label& is_value);\n+\n+  void test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline);\n+  void test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline);\n+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened);\n+\n+  \/\/ Check klass\/oops is flat inline type array (oop->_klass->_layout_helper & vt_bit)\n+  void test_flattened_array_oop(Register klass, Register temp_reg, Label& is_flattened_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+\n@@ -822,0 +837,3 @@\n+  void load_metadata(Register dst, Register src);\n+  void load_storage_props(Register dst, Register src);\n+\n@@ -834,1 +852,1 @@\n-                       Register tmp1, Register tmp_thread);\n+                       Register tmp1, Register tmp_thread, Register tmp3 = noreg);\n@@ -846,1 +864,1 @@\n-                      Register tmp_thread = noreg, DecoratorSet decorators = 0);\n+                      Register tmp_thread = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);\n@@ -1186,0 +1204,31 @@\n+\n+  enum RegState {\n+     reg_readonly,\n+     reg_writable,\n+     reg_written\n+  };\n+\n+  void verified_entry(Compile* C, int sp_inc);\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+\n+\/\/ Unpack all inline type arguments passed as oops\n+  void unpack_inline_args(Compile* C, bool receiver_only);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to, int& to_index,\n+                            RegState reg_state[], int ret_off, int extra_stack_offset);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],\n+                          int ret_off, int extra_stack_offset);\n+  void restore_stack(Compile* C);\n+\n+  int shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,\n+                          BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,\n+                          int args_passed, int args_on_stack, VMRegPair* regs,\n+                          int args_passed_to, int args_on_stack_to, VMRegPair* regs_to);\n+  bool shuffle_inline_args_spill(bool is_packing,  const GrowableArray<SigEntry>* sig_cc, int sig_cc_index,\n+                                 VMRegPair* regs_from, int from_index, int regs_from_count,\n+                                 RegState* reg_state, int sp_inc, int extra_stack_offset);\n+  VMReg spill_reg_for(VMReg reg);\n+\n+\n@@ -1251,0 +1300,2 @@\n+  void fill_words(Register base, uint64_t cnt, Register value);\n+\n@@ -1387,0 +1438,3 @@\n+\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":56,"deletions":2,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -307,1 +307,1 @@\n-    \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+    \/\/ T_OBJECT, T_INLINE_TYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n@@ -311,1 +311,1 @@\n-    Label is_long, is_float, is_double, exit;\n+    Label is_long, is_float, is_double, is_value, exit;\n@@ -315,0 +315,2 @@\n+    __ cmp(j_rarg1, (u1)T_INLINE_TYPE);\n+    __ br(Assembler::EQ, is_value);\n@@ -369,0 +371,13 @@\n+    __ BIND(is_value);\n+    if (InlineTypeReturnedAsFields) {\n+      \/\/ Check for flattened return value\n+      __ cbz(r0, is_long);\n+      \/\/ Initialize pre-allocated buffer\n+      __ mov(r1, r0);\n+      __ andr(r1, r1, -2);\n+      __ ldr(r1, Address(r1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));\n+      __ ldr(r0, Address(j_rarg2, 0));\n+      __ blr(r1);\n+      __ b(exit);\n+    }\n@@ -1809,1 +1824,1 @@\n-    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, AS_RAW);  \/\/ store the oop\n+    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n@@ -5881,0 +5896,178 @@\n+  \/\/ Call here from the interpreter or compiled code to either load\n+  \/\/ multiple returned values from the inline type instance being\n+  \/\/ returned to registers or to store returned values to a newly\n+  \/\/ allocated inline type instance.\n+  address generate_return_value_stub(address destination, const char* name, bool has_res) {\n+\n+    \/\/ Information about frame layout at time of blocking runtime call.\n+    \/\/ Note that we only have to preserve callee-saved registers since\n+    \/\/ the compilers are responsible for supplying a continuation point\n+    \/\/ if they expect all registers to be preserved.\n+    \/\/ n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0\n+    enum layout {\n+      rfp_off = 0, rfp_off2,\n+\n+      j_rarg7_off, j_rarg7_2,\n+      j_rarg6_off, j_rarg6_2,\n+      j_rarg5_off, j_rarg5_2,\n+      j_rarg4_off, j_rarg4_2,\n+      j_rarg3_off, j_rarg3_2,\n+      j_rarg2_off, j_rarg2_2,\n+      j_rarg1_off, j_rarg1_2,\n+      j_rarg0_off, j_rarg0_2,\n+\n+      j_farg0_off, j_farg0_2,\n+      j_farg1_off, j_farg1_2,\n+      j_farg2_off, j_farg2_2,\n+      j_farg3_off, j_farg3_2,\n+      j_farg4_off, j_farg4_2,\n+      j_farg5_off, j_farg5_2,\n+      j_farg6_off, j_farg6_2,\n+      j_farg7_off, j_farg7_2,\n+\n+      return_off, return_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    int insts_size = 512;\n+    int locs_size  = 64;\n+\n+    CodeBuffer code(name, insts_size, locs_size);\n+    OopMapSet* oop_maps  = new OopMapSet();\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+\n+    address start = __ pc();\n+\n+    const Address f7_save       (rfp, j_farg7_off * wordSize);\n+    const Address f6_save       (rfp, j_farg6_off * wordSize);\n+    const Address f5_save       (rfp, j_farg5_off * wordSize);\n+    const Address f4_save       (rfp, j_farg4_off * wordSize);\n+    const Address f3_save       (rfp, j_farg3_off * wordSize);\n+    const Address f2_save       (rfp, j_farg2_off * wordSize);\n+    const Address f1_save       (rfp, j_farg1_off * wordSize);\n+    const Address f0_save       (rfp, j_farg0_off * wordSize);\n+\n+    const Address r0_save      (rfp, j_rarg0_off * wordSize);\n+    const Address r1_save      (rfp, j_rarg1_off * wordSize);\n+    const Address r2_save      (rfp, j_rarg2_off * wordSize);\n+    const Address r3_save      (rfp, j_rarg3_off * wordSize);\n+    const Address r4_save      (rfp, j_rarg4_off * wordSize);\n+    const Address r5_save      (rfp, j_rarg5_off * wordSize);\n+    const Address r6_save      (rfp, j_rarg6_off * wordSize);\n+    const Address r7_save      (rfp, j_rarg7_off * wordSize);\n+\n+    \/\/ Generate oop map\n+    OopMap* map = new OopMap(framesize, 0);\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(rfp_off), rfp->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg7_off), j_rarg7->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg6_off), j_rarg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+    \/\/ This is an inlined and slightly modified version of call_VM\n+    \/\/ which has the ability to fetch the return PC out of\n+    \/\/ thread-local storage and also sets up last_Java_sp slightly\n+    \/\/ differently than the real call_VM\n+\n+    __ enter(); \/\/ Save FP and LR before call\n+\n+    assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n+\n+    \/\/ lr and fp are already in place\n+    __ sub(sp, rfp, ((unsigned)framesize - 4) << LogBytesPerInt); \/\/ prolog\n+\n+    __ strd(j_farg7, f7_save);\n+    __ strd(j_farg6, f6_save);\n+    __ strd(j_farg5, f5_save);\n+    __ strd(j_farg4, f4_save);\n+    __ strd(j_farg3, f3_save);\n+    __ strd(j_farg2, f2_save);\n+    __ strd(j_farg1, f1_save);\n+    __ strd(j_farg0, f0_save);\n+\n+    __ str(j_rarg0, r0_save);\n+    __ str(j_rarg1, r1_save);\n+    __ str(j_rarg2, r2_save);\n+    __ str(j_rarg3, r3_save);\n+    __ str(j_rarg4, r4_save);\n+    __ str(j_rarg5, r5_save);\n+    __ str(j_rarg6, r6_save);\n+    __ str(j_rarg7, r7_save);\n+\n+    int frame_complete = __ pc() - start;\n+\n+    \/\/ Set up last_Java_sp and last_Java_fp\n+    address the_pc = __ pc();\n+    __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n+\n+    \/\/ Call runtime\n+    __ mov(c_rarg0, rthread);\n+    __ mov(c_rarg1, r0);\n+\n+    BLOCK_COMMENT(\"call runtime_entry\");\n+    __ mov(rscratch1, destination);\n+    __ blr(rscratch1);\n+\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    __ reset_last_Java_frame(false);\n+    __ maybe_isb();\n+\n+    __ ldrd(j_farg7, f7_save);\n+    __ ldrd(j_farg6, f6_save);\n+    __ ldrd(j_farg5, f5_save);\n+    __ ldrd(j_farg4, f4_save);\n+    __ ldrd(j_farg3, f3_save);\n+    __ ldrd(j_farg3, f2_save);\n+    __ ldrd(j_farg1, f1_save);\n+    __ ldrd(j_farg0, f0_save);\n+\n+    __ ldr(j_rarg0, r0_save);\n+    __ ldr(j_rarg1, r1_save);\n+    __ ldr(j_rarg2, r2_save);\n+    __ ldr(j_rarg3, r3_save);\n+    __ ldr(j_rarg4, r4_save);\n+    __ ldr(j_rarg5, r5_save);\n+    __ ldr(j_rarg6, r6_save);\n+    __ ldr(j_rarg7, r7_save);\n+\n+    __ leave();\n+\n+    \/\/ check for pending exceptions\n+    Label pending;\n+    __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ cmp(rscratch1, (u1)NULL_WORD);\n+    __ br(Assembler::NE, pending);\n+\n+    if (has_res) {\n+      __ get_vm_result(r0, rthread);\n+    }\n+    __ ret(lr);\n+\n+    __ bind(pending);\n+    __ ldr(r0, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+\n+    \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+    int frame_size_in_words = (framesize >> (LogBytesPerWord - LogBytesPerInt));\n+    RuntimeStub* stub =\n+      RuntimeStub::new_runtime_stub(name, &code, frame_complete, frame_size_in_words, oop_maps, false);\n+\n+    return stub->entry_point();\n+  }\n+\n@@ -5931,0 +6124,5 @@\n+    StubRoutines::_load_inline_type_fields_in_regs =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), \"store_inline_type_fields_to_buf\", true);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":201,"deletions":3,"binary":false,"changes":204,"status":"modified"},{"patch":"@@ -145,1 +145,1 @@\n-  __ store_heap_oop(dst, val, r10, r1, decorators);\n+  __ store_heap_oop(dst, val, r10, r1, noreg, decorators);\n@@ -168,0 +168,1 @@\n+  case Bytecodes::_fast_qputfield:\n@@ -744,4 +745,4 @@\n-    \/\/ ??? convention: move array into r3 for exception message\n-  __ mov(r3, array);\n-  __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n-  __ br(rscratch1);\n+  \/\/ ??? convention: move array into r3 for exception message\n+   __ mov(r3, array);\n+   __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n+   __ br(rscratch1);\n@@ -807,5 +808,15 @@\n-  __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n-  do_oop_load(_masm,\n-              Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)),\n-              r0,\n-              IS_ARRAY);\n+  if (UseFlatArray) {\n+    Label is_flat_array, done;\n+\n+    __ test_flattened_array_oop(r0, r8 \/*temp*\/, is_flat_array);\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+\n+    __ b(done);\n+    __ bind(is_flat_array);\n+    __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_load), r0, r1);\n+    __ bind(done);\n+  } else {\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+  }\n@@ -1108,0 +1119,2 @@\n+\n+  \/\/ FIXME: Could we remove the line below?\n@@ -1113,0 +1126,5 @@\n+  Label  is_flat_array;\n+  if (UseFlatArray) {\n+    __ test_flattened_array_oop(r3, r8 \/*temp*\/, is_flat_array);\n+  }\n+\n@@ -1115,0 +1133,1 @@\n+\n@@ -1117,2 +1136,1 @@\n-  __ ldr(r0, Address(r0,\n-                     ObjArrayKlass::element_klass_offset()));\n+  __ ldr(r0, Address(r0, ObjArrayKlass::element_klass_offset()));\n@@ -1123,0 +1141,1 @@\n+\n@@ -1129,0 +1148,1 @@\n+\n@@ -1132,0 +1152,1 @@\n+\n@@ -1142,0 +1163,13 @@\n+  if (EnableValhalla) {\n+    Label is_null_into_value_array_npe, store_null;\n+\n+    \/\/ No way to store null in flat array\n+    __ test_null_free_array_oop(r3, r8, is_null_into_value_array_npe);\n+    __ b(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n+\n@@ -1144,0 +1178,40 @@\n+  __ b(done);\n+\n+  if (EnableValhalla) {\n+     Label is_type_ok;\n+\n+    \/\/ store non-null value\n+    __ bind(is_flat_array);\n+\n+    \/\/ Simplistic type check...\n+    \/\/ r0 - value, r2 - index, r3 - array.\n+\n+    \/\/ Profile the not-null value's klass.\n+    \/\/ Load value class\n+     __ load_klass(r1, r0);\n+     __ profile_typecheck(r2, r1, r0); \/\/ blows r2, and r0\n+\n+    \/\/ flat value array needs exact type match\n+    \/\/ is \"r8 == r0\" (value subclass == array element superclass)\n+\n+    \/\/ Move element klass into r0\n+\n+     __ load_klass(r0, r3);\n+\n+     __ ldr(r0, Address(r0, ArrayKlass::element_klass_offset()));\n+     __ cmp(r0, r1);\n+     __ br(Assembler::EQ, is_type_ok);\n+\n+     __ profile_typecheck_failed(r2);\n+     __ b(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));\n+\n+     __ bind(is_type_ok);\n+\n+    \/\/ Reload from TOS to be safe, because of profile_typecheck that blows r2 and r0.\n+    \/\/ FIXME: Should we really do it?\n+     __ ldr(r1, at_tos());  \/\/ value\n+     __ mov(r2, r3); \/\/ array, ldr(r2, at_tos_p2());\n+     __ ldr(r3, at_tos_p1()); \/\/ index\n+     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_store), r1, r2, r3);\n+  }\n+\n@@ -2014,2 +2088,1 @@\n-void TemplateTable::if_acmp(Condition cc)\n-{\n+void TemplateTable::if_acmp(Condition cc) {\n@@ -2018,1 +2091,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -2020,0 +2093,36 @@\n+\n+  Register is_value_mask = rscratch1;\n+  __ mov(is_value_mask, markWord::always_locked_pattern);\n+\n+  if (EnableValhalla) {\n+    __ cmp(r1, r0);\n+    __ br(Assembler::EQ, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either r0 or r1 is null\n+    __ andr(r2, r0, r1);\n+    __ cbz(r2, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ ldr(r2, Address(r1, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r2, r2, is_value_mask);\n+    __ ldr(r4, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r4, r4, is_value_mask);\n+    __ andr(r2, r2, r4);\n+    __ cmp(r2,  is_value_mask);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(r2, r1);\n+    __ load_metadata(r4, r0);\n+    __ cmp(r2, r4);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(r0, r1, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(r0, r1, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -2022,0 +2131,1 @@\n+  __ bind(taken);\n@@ -2027,0 +2137,10 @@\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored... r0 answer, jmp to outcome...\n+  __ cbz(r0, not_subst);\n+  __ b(is_subst);\n+}\n+\n+\n@@ -2499,2 +2619,1 @@\n-  __ ubfxw(flags, raw_flags, ConstantPoolCacheEntry::tos_state_shift,\n-           ConstantPoolCacheEntry::tos_state_bits);\n+  __ ubfxw(flags, raw_flags, ConstantPoolCacheEntry::tos_state_shift, ConstantPoolCacheEntry::tos_state_bits);\n@@ -2535,4 +2654,61 @@\n-  do_oop_load(_masm, field, r0, IN_HEAP);\n-  __ push(atos);\n-  if (rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+  if (!EnableValhalla) {\n+    do_oop_load(_masm, field, r0, IN_HEAP);\n+    __ push(atos);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+    }\n+    __ b(Done);\n+  } else { \/\/ Valhalla\n+\n+    if (is_static) {\n+      __ load_heap_oop(r0, field);\n+      Label is_inline, isUninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_inline_type(raw_flags, r8 \/*temp*\/, is_inline);\n+        \/\/ Not inline case\n+        __ push(atos);\n+        __ b(Done);\n+      \/\/ Inline case, must not return null even if uninitialized\n+      __ bind(is_inline);\n+        __ cbz(r0, isUninitialized);\n+          __ push(atos);\n+          __ b(Done);\n+        __ bind(isUninitialized);\n+          __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);\n+          __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field), obj, raw_flags);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(Done);\n+    } else {\n+      Label isFlattened, isInitialized, is_inline, rewrite_inline;\n+        __ test_field_is_inline_type(raw_flags, r8 \/*temp*\/, is_inline);\n+        \/\/ Non-inline field case\n+        __ load_heap_oop(r0, field);\n+        __ push(atos);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+        }\n+        __ b(Done);\n+      __ bind(is_inline);\n+        __ test_field_is_inlined(raw_flags, r8 \/* temp *\/, isFlattened);\n+         \/\/ Non-inline field case\n+          __ load_heap_oop(r0, field);\n+          __ cbnz(r0, isInitialized);\n+            __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);\n+            __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_inline_type_field), obj, raw_flags);\n+          __ bind(isInitialized);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(rewrite_inline);\n+        __ bind(isFlattened);\n+          __ ldr(r10, Address(cache, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));\n+          __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);\n+          call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), obj, raw_flags, r10);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+         patch_bytecode(Bytecodes::_fast_qgetfield, bc, r1);\n+      }\n+      __ b(Done);\n+    }\n@@ -2540,1 +2716,0 @@\n-  __ b(Done);\n@@ -2710,0 +2885,1 @@\n+  const Register flags2 = r6;\n@@ -2732,0 +2908,2 @@\n+  __ mov(flags2, flags);\n+\n@@ -2774,8 +2952,50 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, r0, IN_HEAP);\n-    if (rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n-    }\n-    __ b(Done);\n+     if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n+      }\n+      __ b(Done);\n+     } else { \/\/ Valhalla\n+\n+      __ pop(atos);\n+      if (is_static) {\n+        Label not_inline;\n+         __ test_field_is_not_inline_type(flags2, r8 \/* temp *\/, not_inline);\n+         __ null_check(r0);\n+         __ bind(not_inline);\n+         do_oop_store(_masm, field, r0, IN_HEAP);\n+         __ b(Done);\n+      } else {\n+        Label is_inline, isFlattened, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_inline_type(flags2, r8 \/*temp*\/, is_inline);\n+        \/\/ Not inline case\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ bind(rewrite_not_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+        \/\/ Implementation of the inline semantic\n+        __ bind(is_inline);\n+        __ null_check(r0);\n+        __ test_field_is_inlined(flags2, r8 \/*temp*\/, isFlattened);\n+        \/\/ Not inline case\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ b(rewrite_inline);\n+        __ bind(isFlattened);\n+        pop_and_check_object(obj);\n+        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, off, obj);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_qputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+      }\n+     }  \/\/ Valhalla\n@@ -2921,0 +3141,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -2947,0 +3168,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -3000,0 +3222,13 @@\n+  case Bytecodes::_fast_qputfield: \/\/fall through\n+   {\n+      Label isFlattened, done;\n+      __ null_check(r0);\n+      __ test_field_is_flattened(r3, r8 \/* temp *\/, isFlattened);\n+      \/\/ No Flattened case\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      __ b(done);\n+      __ bind(isFlattened);\n+      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, r1, r2);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3097,0 +3332,26 @@\n+  case Bytecodes::_fast_qgetfield:\n+    {\n+       Label isFlattened, isInitialized, Done;\n+       \/\/ FIXME: We don't need to reload registers multiple times, but stay close to x86 code\n+       __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n+       __ test_field_is_inlined(r9, r8 \/* temp *\/, isFlattened);\n+        \/\/ Non-flattened field case\n+        __ mov(r9, r0);\n+        __ load_heap_oop(r0, field);\n+        __ cbnz(r0, isInitialized);\n+          __ mov(r0, r9);\n+          __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n+          __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);\n+          __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_inline_type_field), r0, r9);\n+        __ bind(isInitialized);\n+        __ verify_oop(r0);\n+        __ b(Done);\n+      __ bind(isFlattened);\n+        __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n+        __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);\n+        __ ldr(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));\n+        call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), r0, r9, r3);\n+        __ verify_oop(r0);\n+      __ bind(Done);\n+    }\n+    break;\n@@ -3652,0 +3913,24 @@\n+void TemplateTable::defaultvalue() {\n+  transition(vtos, atos);\n+  __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);\n+  __ get_constant_pool(c_rarg1);\n+  call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::defaultvalue),\n+          c_rarg1, c_rarg2);\n+  __ verify_oop(r0);\n+  \/\/ Must prevent reordering of stores for object initialization with stores that publish the new object.\n+  __ membar(Assembler::StoreStore);\n+}\n+\n+void TemplateTable::withfield() {\n+  transition(vtos, atos);\n+  resolve_cache_and_index(f2_byte, c_rarg1 \/*cache*\/, c_rarg2 \/*index*\/, sizeof(u2));\n+\n+  \/\/ n.b. unlike x86 cache is now rcpool plus the indexed offset\n+  \/\/ so using rcpool to meet shared code expectations\n+\n+  call_VM(r1, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), rcpool);\n+  __ verify_oop(r1);\n+  __ add(esp, esp, r0);\n+  __ mov(r0, r1);\n+}\n+\n@@ -3723,0 +4008,3 @@\n+  __ b(done);\n+  __ bind(is_null);\n+\n@@ -3725,4 +4013,16 @@\n-    __ b(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n+  if (EnableValhalla) {\n+    \/\/ Get cpool & tags index\n+    __ get_cpool_and_tags(r2, r3); \/\/ r2=cpool, r3=tags array\n+    __ get_unsigned_2_byte_index_at_bcp(r19, 1); \/\/ r19=index\n+     \/\/ See if bytecode has already been quicked\n+    __ add(rscratch1, r3, Array<u1>::base_offset_in_bytes());\n+    __ lea(r1, Address(rscratch1, r19));\n+    __ ldarb(r1, r1);\n+    \/\/ See if CP entry is a Q-descriptor\n+    __ andr (r1, r1, JVM_CONSTANT_QDescBit);\n+    __ cmp(r1, (u1) JVM_CONSTANT_QDescBit);\n+    __ br(Assembler::NE, done);\n+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":334,"deletions":34,"binary":false,"changes":368,"status":"modified"},{"patch":"@@ -70,0 +70,3 @@\n+define_pd_global(bool, InlineTypePassFieldsAsArgs, false);\n+define_pd_global(bool, InlineTypeReturnedAsFields, false);\n+\n","filename":"src\/hotspot\/cpu\/ppc\/globals_ppc.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -504,0 +504,4 @@\n+  if (EnableValhalla && !UseBiasedLocking) {\n+    \/\/ Mask always_locked bit such that we go to the slow path if object is an inline type\n+    andptr(tmpReg, ~((int) markWord::biased_lock_bit_in_place));\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -51,0 +52,1 @@\n+#include \"vmreg_x86.inline.hpp\"\n@@ -52,0 +54,3 @@\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1636,0 +1641,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2604,0 +2613,94 @@\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  testl(temp_reg, JVM_ACC_INLINE);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  movl(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  testl(temp_reg, InstanceKlass::misc_flags_is_empty_inline_type());\n+  jcc(Assembler::notZero, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::zero, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inlined_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inlined);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,\n+                                              Label&is_flattened_array) {\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(temp_reg, oop, tmp_load_klass);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flattened_array_layout(temp_reg, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flattened_array) {\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(temp_reg, oop, tmp_load_klass);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flattened_array_layout(temp_reg, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(temp_reg, oop, tmp_load_klass);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_null_free_array_layout(temp_reg, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(temp_reg, oop, tmp_load_klass);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_null_free_array_layout(temp_reg, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flattened_array);\n+}\n+void MacroAssembler::test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_bit_inplace);\n+  jcc(Assembler::notZero, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_bit_inplace);\n+  jcc(Assembler::zero, is_non_null_free_array);\n+}\n+\n+\n@@ -3302,0 +3405,129 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax, according to barrier asm eden_allocate\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+    jcc(Assembler::equal, L);\n+    stop(\"klass not initialized\");\n+    bind(L);\n+  }\n+#endif\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+  const bool allow_shared_alloc =\n+    Universe::heap()->supports_inline_contig_alloc();\n+\n+  push(klass);\n+  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(klass);\n+#ifndef _LP64\n+  if (UseTLAB || allow_shared_alloc) {\n+    get_thread(thread);\n+  }\n+#endif \/\/ _LP64\n+\n+  if (UseTLAB) {\n+    tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    \/\/ Allocation in the shared Eden, if allowed.\n+    \/\/\n+    eden_allocate(thread, new_obj, layout_size, 0, t2, slow_case);\n+  }\n+\n+  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n+  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n+  if (UseTLAB || allow_shared_alloc) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      decrement(layout_size, sizeof(oopDesc));\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 1*oopSize), zero);\n+        NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 2*oopSize), zero));\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+#ifdef _LP64\n+    xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+#endif\n+    movptr(t2, klass);         \/\/ preserve klass\n+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    store_klass(new_obj, t2, tmp_store_klass);  \/\/ src klass reg is potentially compressed\n+\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -3379,0 +3611,50 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  movptr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(inline_klass, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  movptr(inline_klass, Address(inline_klass, index, Address::times_ptr));\n+}\n+\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  movptr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  movl(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  movptr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset, Address::times_1);\n+  load_heap_oop(obj, field);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n+\n@@ -3727,1 +4009,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -3825,1 +4111,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4321,0 +4611,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -4330,1 +4628,1 @@\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -4363,1 +4661,1 @@\n-                                     Register tmp1, Register tmp2) {\n+                                     Register tmp1, Register tmp2, Register tmp3) {\n@@ -4368,1 +4666,23 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+  } else {\n+    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+  }\n+}\n+\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n@@ -4370,1 +4690,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2);\n+    lea(data, Address(oop, offset));\n@@ -4374,0 +4694,18 @@\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE)));\n+}\n+\n@@ -4395,2 +4733,2 @@\n-                                    Register tmp2, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2);\n+                                    Register tmp2, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2, tmp3);\n@@ -4401,1 +4739,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -4716,1 +5054,5 @@\n-void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n@@ -4769,0 +5111,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, C->output()->sp_inc_offset()), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -4797,5 +5145,0 @@\n-\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->nmethod_entry_barrier(this);\n-  }\n@@ -4805,1 +5148,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, XMMRegister xtmp) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp) {\n@@ -4809,0 +5152,1 @@\n+  movdq(xtmp, val);\n@@ -4810,1 +5154,2 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -4812,1 +5157,1 @@\n-    pxor(xtmp, xtmp);\n+    punpcklqdq(xtmp, xtmp);\n@@ -4856,1 +5201,360 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp, bool is_large) {\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  Label slow_case;\n+\n+  \/\/ Try to allocate a new buffered inline type (from the heap)\n+  if (UseTLAB) {\n+    \/\/ FIXME -- for smaller code, the inline allocation (and the slow case) should be moved inside the pack handler.\n+    if (vk != NULL) {\n+      \/\/ Called from C1, where the return type is statically known.\n+      movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+      jint lh = vk->layout_helper();\n+      assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+      movl(r14, lh);\n+    } else {\n+      \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+      mov(rbx, rax);\n+      andptr(rbx, -2);\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+    }\n+\n+    movptr(r13, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));\n+    lea(r14, Address(r13, r14, Address::times_1));\n+    cmpptr(r14, Address(r15_thread, in_bytes(JavaThread::tlab_end_offset())));\n+    jcc(Assembler::above, slow_case);\n+    movptr(Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())), r14);\n+    movptr(Address(r13, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::always_locked_prototype().value());\n+\n+    xorl(rax, rax); \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(r13, rax);  \/\/ zero klass gap for compressed oops\n+\n+    if (vk == NULL) {\n+      \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n+      mov(rax, rbx);\n+    }\n+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    store_klass(r13, rbx, tmp_store_klass);  \/\/ klass\n+\n+    \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+    if (vk != NULL) {\n+      \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+      mov(rax, r13);\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      mov(rax, r13);\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        assert(st_off != ret_off, \"overwriting return address at %d\", st_off);\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        assert(st_off != ret_off, \"overwriting return address at %d\", st_off);\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, int& from_index, VMRegPair* regs_to,\n+                                          int& to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {\n+  Register fromReg = from->is_reg() ? from->as_Register() : noreg;\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+\n+  int vt = 1;\n+  bool done = true;\n+  bool mark_done = true;\n+  do {\n+    sig_index--;\n+    BasicType bt = sig->at(sig_index)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      vt--;\n+    } else if (bt == T_VOID &&\n+               sig->at(sig_index-1)._bt != T_LONG &&\n+               sig->at(sig_index-1)._bt != T_DOUBLE) {\n+      vt++;\n+    } else if (SigEntry::is_reserved_entry(sig, sig_index)) {\n+      to_index--; \/\/ Ignore this\n+    } else {\n+      assert(to_index >= 0, \"invalid to_index\");\n+      VMRegPair pair_to = regs_to[to_index--];\n+      VMReg to = pair_to.first();\n+\n+      if (bt == T_VOID) continue;\n+\n+      int idx = (int)to->value();\n+      if (reg_state[idx] == reg_readonly) {\n+         if (idx != from->value()) {\n+           mark_done = false;\n+         }\n+         done = false;\n+         continue;\n+      } else if (reg_state[idx] == reg_written) {\n+        continue;\n+      } else {\n+        assert(reg_state[idx] == reg_writable, \"must be writable\");\n+        reg_state[idx] = reg_written;\n+       }\n+\n+      if (fromReg == noreg) {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        movq(r10, Address(rsp, st_off));\n+        fromReg = r10;\n+      }\n+\n+      int off = sig->at(sig_index)._offset;\n+      assert(off > 0, \"offset in object should be positive\");\n+      bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+\n+      Address fromAddr = Address(fromReg, off);\n+      bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+      if (!to->is_XMMRegister()) {\n+        Register dst = to->is_stack() ? r13 : to->as_Register();\n+        if (is_oop) {\n+          load_heap_oop(dst, fromAddr);\n+        } else {\n+          load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+        }\n+        if (to->is_stack()) {\n+          int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+          assert(st_off != ret_off, \"overwriting return address at %d\", st_off);\n+          movq(Address(rsp, st_off), dst);\n+        }\n+      } else {\n+        if (bt == T_DOUBLE) {\n+          movdbl(to->as_XMMRegister(), fromAddr);\n+        } else {\n+          assert(bt == T_FLOAT, \"must be float\");\n+          movflt(to->as_XMMRegister(), fromAddr);\n+        }\n+      }\n+    }\n+  } while (vt != 0);\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],\n+                                        int ret_off, int extra_stack_offset) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"must be\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  Register val_array = rax;\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14; \/\/ Be careful with r14 because it's used for spilling\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);\n+  VMRegPair from_pair;\n+  BasicType bt;\n+  while (stream.next(from_pair, bt)) {\n+    int off = sig->at(stream.sig_cc_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    VMReg from_r1 = from_pair.first();\n+    VMReg from_r2 = from_pair.second();\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+    if (!from_r1->is_XMMRegister()) {\n+      Register from_reg;\n+      if (from_r1->is_stack()) {\n+        from_reg = from_reg_tmp;\n+        int ld_off = from_r1->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        load_sized_value(from_reg, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        from_reg = from_r1->as_Register();\n+      }\n+      assert_different_registers(dst.base(), from_reg, tmp1, tmp2, tmp3, val_array);\n+      if (is_oop) {\n+        store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, from_reg, size_in_bytes);\n+      }\n+    } else {\n+      if (from_r2->is_valid()) {\n+        movdbl(dst, from_r1->as_XMMRegister());\n+      } else {\n+        movflt(dst, from_r1->as_XMMRegister());\n+      }\n+    }\n+    reg_state[from_r1->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_cc_index();\n+  from_index = stream.regs_cc_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+\/\/ Unpack all inline type arguments passed as oops\n+void MacroAssembler::unpack_inline_args(Compile* C, bool receiver_only) {\n+  int sp_inc = unpack_inline_args_common(C, receiver_only);\n+  \/\/ Emit code for verified entry and save increment for stack repair on return\n+  verified_entry(C, sp_inc);\n+}\n+\n+void MacroAssembler::shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,\n+                                         BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,\n+                                         int args_passed, int args_on_stack, VMRegPair* regs,\n+                                         int args_passed_to, int args_on_stack_to, VMRegPair* regs_to, int sp_inc) {\n+  \/\/ Check if we need to extend the stack for packing\/unpacking\n+  if (sp_inc > 0 && !is_packing) {\n+    \/\/ Save the return address, adjust the stack (make sure it is properly\n+    \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+    \/\/ (Note: C1 does this in C1_MacroAssembler::scalarized_entry).\n+    pop(r13);\n+    subptr(rsp, sp_inc);\n+    push(r13);\n+  }\n+\n+  int ret_off; \/\/ make sure we don't overwrite the return address\n+  if (is_packing) {\n+    \/\/ For C1 code, the VIEP doesn't have reserved slots, so we store the returned address at\n+    \/\/ rsp[0] during shuffling.\n+    ret_off = 0;\n+  } else {\n+    \/\/ C2 code ensures that sp_inc is a reserved slot.\n+    ret_off = sp_inc;\n+  }\n+\n+  shuffle_inline_args_common(is_packing, receiver_only, extra_stack_offset,\n+                             sig_bt, sig_cc,\n+                             args_passed, args_on_stack, regs,\n+                             args_passed_to, args_on_stack_to, regs_to,\n+                             sp_inc, ret_off);\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    movq(rbp, Address(rsp, initial_framesize));\n+    addq(rsp, Address(rsp, sp_inc_offset));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only) {\n@@ -4861,1 +5565,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"tmp register must be eax for rep stos\");\n@@ -4868,4 +5572,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n-\n@@ -4884,1 +5584,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -4893,1 +5593,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -4897,2 +5597,1 @@\n-    movptr(tmp, base);\n-    xmm_clear_mem(tmp, cnt, xtmp);\n+    xmm_clear_mem(base, cnt, val, xtmp);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":727,"deletions":28,"binary":false,"changes":755,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -152,1 +153,1 @@\n-  __ store_heap_oop(dst, val, rdx, rbx, decorators);\n+  __ store_heap_oop(dst, val, rdx, rbx, noreg, decorators);\n@@ -175,0 +176,1 @@\n+  case Bytecodes::_fast_qputfield:\n@@ -367,0 +369,1 @@\n+  __ andl(rdx, ~JVM_CONSTANT_QDescBit);\n@@ -818,9 +821,27 @@\n-  \/\/ rax: index\n-  \/\/ rdx: array\n-  index_check(rdx, rax); \/\/ kills rbx\n-  do_oop_load(_masm,\n-              Address(rdx, rax,\n-                      UseCompressedOops ? Address::times_4 : Address::times_ptr,\n-                      arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n-              rax,\n-              IS_ARRAY);\n+  Register array = rdx;\n+  Register index = rax;\n+\n+  index_check(array, index); \/\/ kills rbx\n+  __ profile_array(rbx, array, rcx);\n+  if (UseFlatArray) {\n+    Label is_flat_array, done;\n+    __ test_flattened_array_oop(array, rbx, is_flat_array);\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+    __ jmp(done);\n+    __ bind(is_flat_array);\n+    __ read_flattened_element(array, index, rbx, rcx, rax);\n+    __ bind(done);\n+  } else {\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+  }\n+  __ profile_element(rbx, rax, rcx);\n@@ -1112,1 +1133,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1124,0 +1145,4 @@\n+\n+  __ profile_array(rdi, rdx, rbx);\n+  __ profile_element(rdi, rax, rbx);\n+\n@@ -1127,0 +1152,1 @@\n+  \/\/ Move array class to rdi\n@@ -1128,0 +1154,6 @@\n+  __ load_klass(rdi, rdx, tmp_load_klass);\n+  if (UseFlatArray) {\n+    __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+    __ test_flattened_array_layout(rbx, is_flat_array);\n+  }\n+\n@@ -1130,3 +1162,2 @@\n-  \/\/ Move superklass into rax\n-  __ load_klass(rax, rdx, tmp_load_klass);\n-  __ movptr(rax, Address(rax,\n+  \/\/ Move array element superklass into rax\n+  __ movptr(rax, Address(rdi,\n@@ -1137,1 +1168,2 @@\n-  __ gen_subtype_check(rbx, ok_is_subtype);\n+  \/\/ is \"rbx <: rax\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(rbx, ok_is_subtype, false);\n@@ -1155,1 +1187,2 @@\n-  __ profile_null_seen(rbx);\n+  if (EnableValhalla) {\n+    Label is_null_into_value_array_npe, store_null;\n@@ -1157,0 +1190,9 @@\n+    \/\/ No way to store null in null-free array\n+    __ test_null_free_array_oop(rdx, rbx, is_null_into_value_array_npe);\n+    __ jmp(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ jump(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n@@ -1159,0 +1201,7 @@\n+  __ jmp(done);\n+\n+  if (EnableValhalla) {\n+    Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n+\n+    \/\/ Simplistic type check...\n@@ -1160,0 +1209,27 @@\n+    \/\/ Profile the not-null value's klass.\n+    __ load_klass(rbx, rax, tmp_load_klass);\n+    \/\/ Move element klass into rax\n+    __ movptr(rax, Address(rdi, ArrayKlass::element_klass_offset()));\n+    \/\/ flat value array needs exact type match\n+    \/\/ is \"rax == rbx\" (value subclass == array element superclass)\n+    __ cmpptr(rax, rbx);\n+    __ jccb(Assembler::equal, is_type_ok);\n+\n+    __ jump(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));\n+\n+    __ bind(is_type_ok);\n+    \/\/ rbx: value's klass\n+    \/\/ rdx: array\n+    \/\/ rdi: array klass\n+    __ test_klass_is_empty_inline_type(rbx, rax, done);\n+\n+    \/\/ calc dst for copy\n+    __ movl(rax, at_tos_p1()); \/\/ index\n+    __ data_for_value_array_index(rdx, rdi, rax, rax);\n+\n+    \/\/ ...and src for copy\n+    __ movptr(rcx, at_tos());  \/\/ value\n+    __ data_for_oop(rcx, rcx, rbx);\n+\n+    __ access_value_copy(IN_HEAP, rcx, rax, rbx);\n+  }\n@@ -2406,1 +2482,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -2408,0 +2484,36 @@\n+\n+  const int is_inline_type_mask = markWord::always_locked_pattern;\n+  if (EnableValhalla) {\n+    __ cmpoop(rdx, rax);\n+    __ jcc(Assembler::equal, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either rax or rdx is null\n+    __ movptr(rbx, rdx);\n+    __ andptr(rbx, rax);\n+    __ testptr(rbx, rbx);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ movptr(rbx, Address(rdx, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, is_inline_type_mask);\n+    __ movptr(rcx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, is_inline_type_mask);\n+    __ andptr(rbx, rcx);\n+    __ cmpl(rbx, is_inline_type_mask);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(rbx, rdx);\n+    __ load_metadata(rcx, rax);\n+    __ cmpptr(rbx, rcx);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(rax, rdx, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(rax, rdx, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -2410,0 +2522,1 @@\n+  __ bind(taken);\n@@ -2415,0 +2528,9 @@\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored...rax answer, jmp to outcome...\n+  __ testl(rax, rax);\n+  __ jcc(Assembler::zero, not_subst);\n+  __ jmp(is_subst);\n+}\n+\n@@ -2681,1 +2803,2 @@\n-  __ remove_activation(state, rbcp);\n+\n+  __ remove_activation(state, rbcp, true, true, true);\n@@ -2879,0 +3002,1 @@\n+  const Register flags2 = rdx;\n@@ -2884,2 +3008,0 @@\n-  if (!is_static) pop_and_check_object(obj);\n-\n@@ -2888,1 +3010,9 @@\n-  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj;\n+  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj, notInlineType;\n+\n+  if (!is_static) {\n+    __ movptr(rcx, Address(cache, index, Address::times_ptr,\n+                           in_bytes(ConstantPoolCache::base_offset() +\n+                                    ConstantPoolCacheEntry::f1_offset())));\n+  }\n+\n+  __ movl(flags2, flags);\n@@ -2898,0 +3028,1 @@\n+  if (!is_static) pop_and_check_object(obj);\n@@ -2907,0 +3038,1 @@\n+\n@@ -2909,1 +3041,1 @@\n-\n+   if (!is_static) pop_and_check_object(obj);\n@@ -2924,4 +3056,82 @@\n-  do_oop_load(_masm, field, rax);\n-  __ push(atos);\n-  if (!is_static && rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+  if (!EnableValhalla) {\n+    if (!is_static) pop_and_check_object(obj);\n+    do_oop_load(_masm, field, rax);\n+    __ push(atos);\n+    if (!is_static && rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+    }\n+    __ jmp(Done);\n+  } else {\n+    if (is_static) {\n+      __ load_heap_oop(rax, field);\n+      Label is_inline_type, uninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);\n+        \/\/ field is not an inline type\n+        __ push(atos);\n+        __ jmp(Done);\n+      \/\/ field is an inline type, must not return null even if uninitialized\n+      __ bind(is_inline_type);\n+        __ testptr(rax, rax);\n+        __ jcc(Assembler::zero, uninitialized);\n+          __ push(atos);\n+          __ jmp(Done);\n+        __ bind(uninitialized);\n+          __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+#ifdef _LP64\n+          Label slow_case, finish;\n+          __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+          __ jcc(Assembler::notEqual, slow_case);\n+        __ get_default_value_oop(rcx, off, rax);\n+        __ jmp(finish);\n+        __ bind(slow_case);\n+#endif \/\/ LP64\n+          __ call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field),\n+                 obj, flags2);\n+#ifdef _LP64\n+          __ bind(finish);\n+#endif \/\/ _LP64\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(Done);\n+    } else {\n+      Label is_inlined, nonnull, is_inline_type, rewrite_inline;\n+      __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);\n+        \/\/ field is not an inline type\n+        pop_and_check_object(obj);\n+        __ load_heap_oop(rax, field);\n+        __ push(atos);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+        }\n+        __ jmp(Done);\n+      __ bind(is_inline_type);\n+        __ test_field_is_inlined(flags2, rscratch1, is_inlined);\n+          \/\/ field is not inlined\n+          __ movptr(rax, rcx);  \/\/ small dance required to preserve the klass_holder somewhere\n+          pop_and_check_object(obj);\n+          __ push(rax);\n+          __ load_heap_oop(rax, field);\n+          __ pop(rcx);\n+          __ testptr(rax, rax);\n+          __ jcc(Assembler::notZero, nonnull);\n+            __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+            __ get_inline_type_field_klass(rcx, flags2, rbx);\n+            __ get_default_value_oop(rbx, rcx, rax);\n+          __ bind(nonnull);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(rewrite_inline);\n+        __ bind(is_inlined);\n+        \/\/ field is inlined\n+          __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+          pop_and_check_object(rax);\n+          __ read_inlined_field(rcx, flags2, rbx, rax);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_qgetfield, bc, rbx);\n+      }\n+      __ jmp(Done);\n+    }\n@@ -2929,1 +3139,0 @@\n-  __ jmp(Done);\n@@ -2932,0 +3141,3 @@\n+\n+  if (!is_static) pop_and_check_object(obj);\n+\n@@ -3031,0 +3243,15 @@\n+void TemplateTable::withfield() {\n+  transition(vtos, atos);\n+\n+  Register cache = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n+  Register index = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n+\n+  resolve_cache_and_index(f2_byte, cache, index, sizeof(u2));\n+\n+  call_VM(rbx, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), cache);\n+  \/\/ new value type is returned in rbx\n+  \/\/ stack adjustement is returned in rax\n+  __ verify_oop(rbx);\n+  __ addptr(rsp, rax);\n+  __ movptr(rax, rbx);\n+}\n@@ -3126,0 +3353,1 @@\n+  const Register flags2 = rdx;\n@@ -3142,0 +3370,1 @@\n+  __ movl(flags2, flags);\n@@ -3144,1 +3373,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags, flags2);\n@@ -3150,1 +3379,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags, flags2);\n@@ -3156,1 +3385,1 @@\n-                                              Register obj, Register off, Register flags) {\n+                                              Register obj, Register off, Register flags, Register flags2) {\n@@ -3163,1 +3392,1 @@\n-        notLong, notFloat, notObj;\n+        notLong, notFloat, notObj, notInlineType;\n@@ -3206,6 +3435,53 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, rax);\n-    if (!is_static && rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+    if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, rax);\n+      if (!is_static && rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+      }\n+      __ jmp(Done);\n+    } else {\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_inline_type;\n+        __ test_field_is_not_inline_type(flags2, rscratch1, is_inline_type);\n+        __ null_check(rax);\n+        __ bind(is_inline_type);\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(Done);\n+      } else {\n+        Label is_inline_type, is_inlined, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);\n+        \/\/ Not an inline type\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        __ bind(rewrite_not_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(is_inline_type);\n+        __ null_check(rax);\n+        __ test_field_is_inlined(flags2, rscratch1, is_inlined);\n+        \/\/ field is not inlined\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(rewrite_inline);\n+        __ bind(is_inlined);\n+        \/\/ field is inlined\n+        pop_and_check_object(obj);\n+        assert_different_registers(rax, rdx, obj, off);\n+        __ load_klass(rdx, rax, rscratch1);\n+        __ data_for_oop(rax, rax, rdx);\n+        __ addptr(obj, off);\n+        __ access_value_copy(IN_HEAP, rax, obj, rdx);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_qputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+      }\n@@ -3213,1 +3489,0 @@\n-    __ jmp(Done);\n@@ -3352,0 +3627,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -3377,0 +3653,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/ fall through\n@@ -3416,0 +3693,4 @@\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rscratch2, rdx);  \/\/ saving flags for is_inlined test\n+  }\n+\n@@ -3429,1 +3710,4 @@\n-  fast_storefield_helper(field, rax);\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rdx, rscratch2);  \/\/ restoring flags for is_inlined test\n+  }\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3435,1 +3719,4 @@\n-  fast_storefield_helper(field, rax);\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rdx, rscratch2);  \/\/ restoring flags for is_inlined test\n+  }\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3440,1 +3727,1 @@\n-void TemplateTable::fast_storefield_helper(Address field, Register rax) {\n+void TemplateTable::fast_storefield_helper(Address field, Register rax, Register flags) {\n@@ -3444,0 +3731,17 @@\n+  case Bytecodes::_fast_qputfield:\n+    {\n+      Label is_inlined, done;\n+      __ null_check(rax);\n+      __ test_field_is_inlined(flags, rscratch1, is_inlined);\n+      \/\/ field is not inlined\n+      do_oop_store(_masm, field, rax);\n+      __ jmp(done);\n+      __ bind(is_inlined);\n+      \/\/ field is inlined\n+      __ load_klass(rdx, rax, rscratch1);\n+      __ data_for_oop(rax, rax, rdx);\n+      __ lea(rcx, field);\n+      __ access_value_copy(IN_HEAP, rax, rcx, rdx);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3445,1 +3749,3 @@\n-    do_oop_store(_masm, field, rax);\n+    {\n+      do_oop_store(_masm, field, rax);\n+    }\n@@ -3515,1 +3821,1 @@\n-  __ movptr(rbx, Address(rcx, rbx, Address::times_ptr,\n+  __ movptr(rdx, Address(rcx, rbx, Address::times_ptr,\n@@ -3522,1 +3828,1 @@\n-  Address field(rax, rbx, Address::times_1);\n+  Address field(rax, rdx, Address::times_1);\n@@ -3526,0 +3832,39 @@\n+  case Bytecodes::_fast_qgetfield:\n+    {\n+      Label is_inlined, nonnull, Done;\n+      __ movptr(rscratch1, Address(rcx, rbx, Address::times_ptr,\n+                                   in_bytes(ConstantPoolCache::base_offset() +\n+                                            ConstantPoolCacheEntry::flags_offset())));\n+      __ test_field_is_inlined(rscratch1, rscratch2, is_inlined);\n+        \/\/ field is not inlined\n+        __ load_heap_oop(rax, field);\n+        __ testptr(rax, rax);\n+        __ jcc(Assembler::notZero, nonnull);\n+          __ movl(rdx, Address(rcx, rbx, Address::times_ptr,\n+                             in_bytes(ConstantPoolCache::base_offset() +\n+                                      ConstantPoolCacheEntry::flags_offset())));\n+          __ andl(rdx, ConstantPoolCacheEntry::field_index_mask);\n+          __ movptr(rcx, Address(rcx, rbx, Address::times_ptr,\n+                                       in_bytes(ConstantPoolCache::base_offset() +\n+                                                ConstantPoolCacheEntry::f1_offset())));\n+          __ get_inline_type_field_klass(rcx, rdx, rbx);\n+          __ get_default_value_oop(rbx, rcx, rax);\n+        __ bind(nonnull);\n+        __ verify_oop(rax);\n+        __ jmp(Done);\n+      __ bind(is_inlined);\n+      \/\/ field is inlined\n+        __ push(rdx); \/\/ save offset\n+        __ movl(rdx, Address(rcx, rbx, Address::times_ptr,\n+                           in_bytes(ConstantPoolCache::base_offset() +\n+                                    ConstantPoolCacheEntry::flags_offset())));\n+        __ andl(rdx, ConstantPoolCacheEntry::field_index_mask);\n+        __ movptr(rcx, Address(rcx, rbx, Address::times_ptr,\n+                                     in_bytes(ConstantPoolCache::base_offset() +\n+                                              ConstantPoolCacheEntry::f1_offset())));\n+        __ pop(rbx); \/\/ restore offset\n+        __ read_inlined_field(rcx, rdx, rbx, rax);\n+      __ bind(Done);\n+      __ verify_oop(rax);\n+    }\n+    break;\n@@ -3999,3 +4344,1 @@\n-  Label slow_case_no_pop;\n-  Label initialize_header;\n-  Label initialize_object;  \/\/ including clearing the fields\n+  Label is_not_value;\n@@ -4011,1 +4354,1 @@\n-  __ jcc(Assembler::notEqual, slow_case_no_pop);\n+  __ jcc(Assembler::notEqual, slow_case);\n@@ -4015,1 +4358,7 @@\n-  __ push(rcx);  \/\/ save the contexts of klass for initializing the header\n+\n+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InstanceKlass::_kind_inline_type);\n+  __ jcc(Assembler::notEqual, is_not_value);\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_InstantiationError));\n+\n+  __ bind(is_not_value);\n@@ -4018,1 +4367,0 @@\n-  \/\/ make sure klass is fully initialized\n@@ -4022,19 +4370,2 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ movl(rdx, Address(rcx, Klass::layout_helper_offset()));\n-  \/\/ test to see if it has a finalizer or is malformed in some way\n-  __ testl(rdx, Klass::_lh_instance_slow_path_bit);\n-  __ jcc(Assembler::notZero, slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/  Else If inline contiguous allocations are enabled:\n-  \/\/    Try to allocate in eden.\n-  \/\/    If fails due to heap end, go to slow path.\n-  \/\/\n-  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n+  __ allocate_instance(rcx, rax, rdx, rbx, true, slow_case);\n+  __ jmp(done);\n@@ -4042,2 +4373,2 @@\n-  const bool allow_shared_alloc =\n-    Universe::heap()->supports_inline_contig_alloc();\n+  \/\/ slow case\n+  __ bind(slow_case);\n@@ -4045,6 +4376,2 @@\n-  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n-#ifndef _LP64\n-  if (UseTLAB || allow_shared_alloc) {\n-    __ get_thread(thread);\n-  }\n-#endif \/\/ _LP64\n+  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n+  Register rarg2 = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n@@ -4052,15 +4379,4 @@\n-  if (UseTLAB) {\n-    __ tlab_allocate(thread, rax, rdx, 0, rcx, rbx, slow_case);\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ jmp(initialize_header);\n-    } else {\n-      \/\/ initialize both the header and fields\n-      __ jmp(initialize_object);\n-    }\n-  } else {\n-    \/\/ Allocation in the shared Eden, if allowed.\n-    \/\/\n-    \/\/ rdx: instance size in bytes\n-    __ eden_allocate(thread, rax, rdx, 0, rbx, slow_case);\n-  }\n+  __ get_constant_pool(rarg1);\n+  __ get_unsigned_2_byte_index_at_bcp(rarg2, 1);\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);\n+   __ verify_oop(rax);\n@@ -4068,24 +4384,3 @@\n-  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n-  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n-  if (UseTLAB || allow_shared_alloc) {\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    __ bind(initialize_object);\n-    __ decrement(rdx, sizeof(oopDesc));\n-    __ jcc(Assembler::zero, initialize_header);\n-\n-    \/\/ Initialize topmost object field, divide rdx by 8, check if odd and\n-    \/\/ test if zero.\n-    __ xorl(rcx, rcx);    \/\/ use zero reg to clear memory (shorter code)\n-    __ shrl(rdx, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n-\n-    \/\/ rdx must have been multiple of 8\n-#ifdef ASSERT\n-    \/\/ make sure rdx was multiple of 8\n-    Label L;\n-    \/\/ Ignore partial flag stall after shrl() since it is debug VM\n-    __ jcc(Assembler::carryClear, L);\n-    __ stop(\"object size is not multiple of 2 - adjust this code\");\n-    __ bind(L);\n-    \/\/ rdx must be > 0, no extra check needed here\n-#endif\n+  \/\/ continue\n+  __ bind(done);\n+}\n@@ -4093,8 +4388,2 @@\n-    \/\/ initialize remaining object fields: rdx was a multiple of 8\n-    { Label loop;\n-    __ bind(loop);\n-    __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n-    __ decrement(rdx);\n-    __ jcc(Assembler::notZero, loop);\n-    }\n+void TemplateTable::defaultvalue() {\n+  transition(vtos, atos);\n@@ -4102,17 +4391,3 @@\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    if (UseBiasedLocking) {\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);\n-    } else {\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()),\n-                (intptr_t)markWord::prototype().value()); \/\/ header\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-    }\n-#ifdef _LP64\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n-#endif\n-    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-    __ store_klass(rax, rcx, tmp_store_klass);  \/\/ klass\n+  Label slow_case;\n+  Label done;\n+  Label is_value;\n@@ -4120,8 +4395,2 @@\n-    {\n-      SkipIfEqual skip_if(_masm, &DTraceAllocProbes, 0);\n-      \/\/ Trigger dtrace event for fastpath\n-      __ push(atos);\n-      __ call_VM_leaf(\n-           CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), rax);\n-      __ pop(atos);\n-    }\n+  __ get_unsigned_2_byte_index_at_bcp(rdx, 1);\n+  __ get_cpool_and_tags(rcx, rax);\n@@ -4129,2 +4398,25 @@\n-    __ jmp(done);\n-  }\n+  \/\/ Make sure the class we're about to instantiate has been resolved.\n+  \/\/ This is done before loading InstanceKlass to be consistent with the order\n+  \/\/ how Constant Pool is updated (see ConstantPool::klass_at_put)\n+  const int tags_offset = Array<u1>::base_offset_in_bytes();\n+  __ cmpb(Address(rax, rdx, Address::times_1, tags_offset), JVM_CONSTANT_Class);\n+  __ jcc(Assembler::notEqual, slow_case);\n+\n+  \/\/ get InstanceKlass\n+  __ load_resolved_klass_at_index(rcx, rcx, rdx);\n+\n+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InstanceKlass::_kind_inline_type);\n+  __ jcc(Assembler::equal, is_value);\n+\n+  \/\/ in the future, defaultvalue will just return null instead of throwing an exception\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_IncompatibleClassChangeError));\n+\n+  __ bind(is_value);\n+\n+  \/\/ make sure klass is fully initialized\n+  __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+  __ jcc(Assembler::notEqual, slow_case);\n+\n+  \/\/ have a resolved InlineKlass in rcx, return the default value oop from it\n+  __ get_default_value_oop(rcx, rdx, rax);\n+  __ jmp(done);\n@@ -4132,4 +4424,1 @@\n-  \/\/ slow case\n-  __ pop(rcx);   \/\/ restore stack pointer to what it was when we came in.\n-  __ bind(slow_case_no_pop);\n-  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n+  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n@@ -4140,3 +4429,4 @@\n-  __ get_constant_pool(rarg1);\n-  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);\n-   __ verify_oop(rax);\n+  __ get_constant_pool(rarg1);\n+\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::defaultvalue),\n+      rarg1, rarg2);\n@@ -4145,1 +4435,1 @@\n-  \/\/ continue\n+  __ verify_oop(rax);\n@@ -4185,4 +4475,5 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+      Address::times_1,\n+      Array<u1>::base_offset_in_bytes()));\n+  __ andl (rdx, ~JVM_CONSTANT_QDescBit);\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4227,0 +4518,3 @@\n+  __ jmp(done);\n+\n+  __ bind(is_null);\n@@ -4230,4 +4524,15 @@\n-    __ jmp(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n+  if (EnableValhalla) {\n+    \/\/ Get cpool & tags index\n+    __ get_cpool_and_tags(rcx, rdx); \/\/ rcx=cpool, rdx=tags array\n+    __ get_unsigned_2_byte_index_at_bcp(rbx, 1); \/\/ rbx=index\n+    \/\/ See if CP entry is a Q-descriptor\n+    __ movzbl(rcx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+    __ andl (rcx, JVM_CONSTANT_QDescBit);\n+    __ cmpl(rcx, JVM_CONSTANT_QDescBit);\n+    __ jcc(Assembler::notEqual, done);\n+    __ jump(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+  }\n+\n@@ -4249,4 +4554,5 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+  __ andl (rdx, ~JVM_CONSTANT_QDescBit);\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4305,1 +4611,0 @@\n-\n@@ -4369,0 +4674,11 @@\n+  const int is_inline_type_mask = markWord::always_locked_pattern;\n+  Label has_identity;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ andptr(rbx, is_inline_type_mask);\n+  __ cmpl(rbx, is_inline_type_mask);\n+  __ jcc(Assembler::notEqual, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n@@ -4468,0 +4784,11 @@\n+  const int is_inline_type_mask = markWord::always_locked_pattern;\n+  Label has_identity;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ andptr(rbx, is_inline_type_mask);\n+  __ cmpl(rbx, is_inline_type_mask);\n+  __ jcc(Assembler::notEqual, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":499,"deletions":172,"binary":false,"changes":671,"status":"modified"},{"patch":"@@ -1483,1 +1483,1 @@\n-  if (!UseFastStosb && UseSSE >= 2 && UseUnalignedLoadStores) {\n+  if (UseSSE >= 2 && UseUnalignedLoadStores) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+\n@@ -53,0 +54,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -85,0 +87,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -138,0 +141,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        60\n+\n@@ -177,1 +182,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -507,1 +512,8 @@\n-        cp->unresolved_klass_at_put(index, class_index, num_klasses++);\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n+        if (name->is_Q_signature()) {\n+          cp->unresolved_qdescriptor_at_put(index, class_index, num_klasses++);\n+        } else {\n+          cp->unresolved_klass_at_put(index, class_index, num_klasses++);\n+        }\n@@ -761,2 +773,2 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+            if (name != vmSymbols::object_initializer_name()) {\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -768,1 +780,12 @@\n-              if (name == vmSymbols::object_initializer_name()) {\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else if (!signature->is_void_method_signature()\n+                         && ref_kind == JVM_REF_invokeStatic) {\n+                \/\/ also OK, could be a static factory call\n+              } else {\n@@ -927,3 +950,4 @@\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n+                                       bool is_inline_type,\n@@ -931,0 +955,7 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n+                                       bool* const is_declared_atomic,\n@@ -937,1 +968,1 @@\n-    _local_interfaces = Universe::the_empty_instance_klass_array();\n+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(0);\n@@ -940,3 +971,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);\n-\n-    int index;\n+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(itfs_len);\n+    int index = 0;\n@@ -960,1 +990,1 @@\n-        \/\/ Call resolve_super so classcircularity is checked\n+        \/\/ Call resolve_super so class circularity is checked\n@@ -978,1 +1008,14 @@\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+      InstanceKlass* ik = InstanceKlass::cast(interf);\n+      if (is_inline_type && ik->invalid_inline_super()) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(\n+          THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_IncompatibleClassChangeError(),\n+          \"Inline type %s attempts to implement interface java.lang.IdentityObject\",\n+          _class_name->as_klass_external_name());\n+        return;\n+      }\n+      if (ik->invalid_inline_super()) {\n+        set_invalid_inline_super();\n+      }\n+      if (ik->has_nonstatic_concrete_methods()) {\n@@ -981,1 +1024,7 @@\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      if (ik->is_declared_atomic()) {\n+        *is_declared_atomic = true;\n+      }\n+      if (ik->name() == vmSymbols::java_lang_IdentityObject()) {\n+        _implements_identityObject = true;\n+      }\n+      _temp_local_interfaces->append(ik);\n@@ -999,1 +1048,1 @@\n-        const InstanceKlass* const k = _local_interfaces->at(index);\n+        const InstanceKlass* const k = _temp_local_interfaces->at(index);\n@@ -1475,0 +1524,1 @@\n+  STATIC_INLINE,        \/\/ inline type field\n@@ -1480,0 +1530,1 @@\n+  NONSTATIC_INLINE,\n@@ -1499,6 +1550,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  NONSTATIC_OOP,       \/\/ T_INLINE_TYPE = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20,\n@@ -1519,6 +1571,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  STATIC_OOP,          \/\/ T_INLINE_TYPE = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20\n@@ -1527,1 +1580,1 @@\n-static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type) {\n+static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type, bool is_inline_type) {\n@@ -1531,0 +1584,3 @@\n+  if (is_inline_type) {\n+    result = is_static ? STATIC_INLINE : NONSTATIC_INLINE;\n+  }\n@@ -1544,2 +1600,2 @@\n-  FieldAllocationType update(bool is_static, BasicType type) {\n-    FieldAllocationType atype = basic_type_to_atype(is_static, type);\n+  FieldAllocationType update(bool is_static, BasicType type, bool is_inline_type) {\n+    FieldAllocationType atype = basic_type_to_atype(is_static, type, is_inline_type);\n@@ -1559,0 +1615,1 @@\n+                                   bool is_inline_type,\n@@ -1581,1 +1638,5 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);\n@@ -1611,0 +1672,1 @@\n+  int instance_fields_count = 0;\n@@ -1615,0 +1677,4 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, is_interface, is_inline_type, CHECK);\n@@ -1616,2 +1682,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1633,0 +1697,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1692,1 +1757,1 @@\n-    const FieldAllocationType atype = fac->update(is_static, type);\n+    const FieldAllocationType atype = fac->update(is_static, type, type == T_INLINE_TYPE);\n@@ -1737,1 +1802,1 @@\n-      const FieldAllocationType atype = fac->update(false, type);\n+      const FieldAllocationType atype = fac->update(false, type, false);\n@@ -1743,0 +1808,29 @@\n+  if (is_inline_type) {\n+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);\n+    field->initialize(JVM_ACC_FIELD_INTERNAL | JVM_ACC_STATIC,\n+                      vmSymbols::default_value_name_enum,\n+                      vmSymbols::object_signature_enum,\n+                      0);\n+    const BasicType type = Signature::basic_type(vmSymbols::object_signature());\n+    const FieldAllocationType atype = fac->update(true, type, false);\n+    field->set_allocation_type(atype);\n+    index++;\n+  }\n+\n+  if (is_inline_type && instance_fields_count == 0) {\n+    _is_empty_inline_type = true;\n+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);\n+    field->initialize(JVM_ACC_FIELD_INTERNAL,\n+        vmSymbols::empty_marker_name_enum,\n+        vmSymbols::byte_signature_enum,\n+        0);\n+    const BasicType type = Signature::basic_type(vmSymbols::byte_signature());\n+    const FieldAllocationType atype = fac->update(false, type, false);\n+    field->set_allocation_type(atype);\n+    index++;\n+  }\n+\n+  if (instance_fields_count > 0) {\n+    _has_nonstatic_fields = true;\n+  }\n+\n@@ -2058,0 +2152,5 @@\n+  const char* class_note = \"\";\n+  if (is_inline_type() && name == vmSymbols::object_initializer_name()) {\n+    class_note = \" (an inline class)\";\n+  }\n+\n@@ -2061,2 +2160,2 @@\n-      \"%s \\\"%s\\\" in class %s has illegal signature \\\"%s\\\"\", type,\n-      name->as_C_string(), _class_name->as_C_string(), sig->as_C_string());\n+      \"%s \\\"%s\\\" in class %s%s has illegal signature \\\"%s\\\"\", type,\n+      name->as_C_string(), _class_name->as_C_string(), class_note, sig->as_C_string());\n@@ -2327,0 +2426,1 @@\n+                                      bool is_inline_type,\n@@ -2367,5 +2467,50 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n-  }\n-\n-  if (name == vmSymbols::object_initializer_name() && is_interface) {\n-    classfile_parse_error(\"Interface cannot have a method named <init>, class file %s\", CHECK_NULL);\n+    verify_legal_method_modifiers(flags, is_interface, is_inline_type, name, CHECK_NULL);\n+  }\n+\n+  if (name == vmSymbols::object_initializer_name()) {\n+    if (is_interface) {\n+      classfile_parse_error(\"Interface cannot have a method named <init>, class file %s\", CHECK_NULL);\n+    } else if (!is_inline_type && signature->is_void_method_signature()) {\n+      \/\/ OK, a constructor\n+    } else if (is_inline_type && !signature->is_void_method_signature()) {\n+      \/\/ also OK, a static factory, as long as the return value is good\n+      bool ok = false;\n+      SignatureStream ss((Symbol*) signature, true);\n+      while (!ss.at_return_type())  ss.next();\n+      if (ss.is_reference()) {\n+        Symbol* ret = ss.as_symbol();\n+        const Symbol* required = class_name();\n+        if (is_hidden()) {\n+          \/\/ The original class name in hidden classes gets changed.  So using\n+          \/\/ the original name in the return type is no longer valid.\n+          \/\/ Note that expecting the return type for inline hidden class factory\n+          \/\/ methods to be java.lang.Object works around a JVM Spec issue for\n+          \/\/ hidden classes.\n+          required = vmSymbols::java_lang_Object();\n+        }\n+        ok = (ret == required);\n+      }\n+      if (!ok) {\n+        throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+      }\n+    } else {\n+      \/\/ not OK, so throw the same error as in verify_legal_method_signature.\n+      throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+    }\n+    \/\/ A declared <init> method must always be either a non-static\n+    \/\/ object constructor, with a void return, or else it must be a\n+    \/\/ static factory method, with a non-void return.  No other\n+    \/\/ definition of <init> is possible.\n+    \/\/\n+    \/\/ The verifier (in verify_invoke_instructions) will inspect the\n+    \/\/ signature of any attempt to invoke <init>, and ensures that it\n+    \/\/ returns non-void if and only if it is being invoked by\n+    \/\/ invokestatic, and void if and only if it is being invoked by\n+    \/\/ invokespecial.\n+    \/\/\n+    \/\/ When a symbolic reference to <init> is resolved for a\n+    \/\/ particular invocation mode (special or static), the mode is\n+    \/\/ matched to the JVM_ACC_STATIC modifier of the <init> method.\n+    \/\/ Thus, it is impossible to statically invoke a constructor, and\n+    \/\/ impossible to \"new + invokespecial\" a static factory, either\n+    \/\/ through bytecode or through reflection.\n@@ -2922,0 +3067,1 @@\n+                                    bool is_inline_type,\n@@ -2946,0 +3092,1 @@\n+                                    is_inline_type,\n@@ -3138,2 +3285,2 @@\n-    \/\/ Access flags\n-    jint flags;\n+\n+    jint recognized_modifiers = RECOGNIZED_INNER_CLASS_MODIFIERS;\n@@ -3142,3 +3289,5 @@\n-      flags = cfs->get_u2_fast() & (RECOGNIZED_INNER_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-    } else {\n-      flags = cfs->get_u2_fast() & RECOGNIZED_INNER_CLASS_MODIFIERS;\n+      recognized_modifiers |= JVM_ACC_MODULE;\n+    }\n+    \/\/ JVM_ACC_INLINE is defined for class file version 55 and later\n+    if (supports_inline_types()) {\n+      recognized_modifiers |= JVM_ACC_INLINE;\n@@ -3146,0 +3295,4 @@\n+\n+    \/\/ Access flags\n+    jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+\n@@ -3518,3 +3671,2 @@\n-  return _major_version == JVM_CLASSFILE_MAJOR_VERSION &&\n-         _minor_version == JAVA_PREVIEW_MINOR_VERSION &&\n-         Arguments::enable_preview();\n+  \/\/ temporarily disable the sealed type preview feature check\n+  return _major_version == JVM_CLASSFILE_MAJOR_VERSION;\n@@ -4005,1 +4157,2 @@\n-    check_property(_class_name == vmSymbols::java_lang_Object(),\n+    check_property(_class_name == vmSymbols::java_lang_Object()\n+                   || (_access_flags.get_flags() & JVM_ACC_INLINE),\n@@ -4148,0 +4301,19 @@\n+void ClassFileParser::throwInlineTypeLimitation(THREAD_AND_LOCATION_DECL,\n+                                                const char* msg,\n+                                                const Symbol* name,\n+                                                const Symbol* sig) const {\n+\n+  ResourceMark rm(THREAD);\n+  if (name == NULL || sig == NULL) {\n+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"class: %s - %s\", _class_name->as_C_string(), msg);\n+  }\n+  else {\n+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"\\\"%s\\\" sig: \\\"%s\\\" class: %s - %s\", name->as_C_string(), sig->as_C_string(),\n+        _class_name->as_C_string(), msg);\n+  }\n+}\n+\n@@ -4181,0 +4353,5 @@\n+      if (ik->is_inline_klass()) {\n+        Thread *THREAD = Thread::current();\n+        throwInlineTypeLimitation(THREAD_AND_LOCATION, \"Inline Types do not support Cloneable\");\n+        return;\n+      }\n@@ -4221,0 +4398,5 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 55 and later\n+  return _major_version >= JAVA_11_VERSION;\n+}\n+\n@@ -4264,3 +4446,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4287,0 +4470,5 @@\n+\n+    if (length == 1 && result->at(0) == SystemDictionary::IdentityObject_klass()) {\n+      return Universe::the_single_IdentityObject_klass_array();\n+    }\n+\n@@ -4519,0 +4707,1 @@\n+  const bool is_inline_type = (flags & JVM_ACC_INLINE) != 0;\n@@ -4520,0 +4709,1 @@\n+  assert(supports_inline_types() || !is_inline_type, \"JVM_ACC_INLINE should not be set\");\n@@ -4530,0 +4720,11 @@\n+  if (is_inline_type && !EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    Exceptions::fthrow(\n+      THREAD_AND_LOCATION,\n+      vmSymbols::java_lang_ClassFormatError(),\n+      \"Class modifier ACC_INLINE in class %s requires option -XX:+EnableValhalla\",\n+      _class_name->as_C_string()\n+    );\n+    return;\n+  }\n+\n@@ -4544,1 +4745,2 @@\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (is_inline_type && (is_interface || is_abstract || is_enum || !is_final))) {\n@@ -4546,0 +4748,2 @@\n+    const char* class_note = \"\";\n+    if (is_inline_type)  class_note = \" (an inline class)\";\n@@ -4549,2 +4753,2 @@\n-      \"Illegal class modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags\n+      \"Illegal class modifiers in class %s%s: 0x%X\",\n+      _class_name->as_C_string(), class_note, flags\n@@ -4629,0 +4833,1 @@\n+                                                   bool is_inline_type,\n@@ -4653,0 +4858,4 @@\n+    } else {\n+      if (is_inline_type && !is_static && !is_final) {\n+        is_illegal = true;\n+      }\n@@ -4669,0 +4878,1 @@\n+                                                    bool is_inline_type,\n@@ -4689,0 +4899,1 @@\n+  const char* class_note = \"\";\n@@ -4723,1 +4934,1 @@\n-        if (is_static || is_final || is_synchronized || is_native ||\n+        if (is_final || is_synchronized || is_native ||\n@@ -4727,0 +4938,9 @@\n+        if (!is_static && !is_inline_type) {\n+          \/\/ OK, an object constructor in a regular class\n+        } else if (is_static && is_inline_type) {\n+          \/\/ OK, a static init factory in an inline class\n+        } else {\n+          \/\/ but no other combinations are allowed\n+          is_illegal = true;\n+          class_note = (is_inline_type ? \" (an inline class)\" : \" (not an inline class)\");\n+        }\n@@ -4728,4 +4948,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || is_strict)))) {\n-            is_illegal = true;\n+        if (is_inline_type && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (an inline class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || is_strict)))) {\n+              is_illegal = true;\n+            }\n@@ -4743,2 +4968,2 @@\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+      \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+      name->as_C_string(), _class_name->as_C_string(), class_note, flags);\n@@ -4902,1 +5127,10 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_INLINE_TYPE:\n+      \/\/ Can't enable this check until JDK upgrades the bytecode generators\n+      \/\/ if (_major_version < CONSTANT_CLASS_DESCRIPTORS ) {\n+      \/\/   classfile_parse_error(\"Class name contains illegal Q-signature \"\n+      \/\/                                    \"in descriptor in class file %s\",\n+      \/\/                                    CHECK_0);\n+      \/\/ }\n+      \/\/ fall through\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -4913,1 +5147,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -4968,0 +5202,3 @@\n+    } else if (_major_version >= CONSTANT_CLASS_DESCRIPTORS && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; and Q...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -5065,0 +5302,3 @@\n+  if (!supports_inline_types() && (signature->is_Q_signature() || signature->is_Q_array_signature())) {\n+    throwIllegalSignature(\"Field\", name, signature, CHECK);\n+  }\n@@ -5117,1 +5357,1 @@\n-        \/\/ All internal methods must return void\n+        \/\/ All constructor methods must return void\n@@ -5121,0 +5361,16 @@\n+        \/\/ All static init methods must return the current class\n+        if ((length >= 3) && (p[length-1] == JVM_SIGNATURE_ENDCLASS)\n+            && name == vmSymbols::object_initializer_name()) {\n+          nextp = skip_over_field_signature(p, true, length, CHECK_0);\n+          if (nextp && ((int)length == (nextp - p))) {\n+            \/\/ The actual class will be checked against current class\n+            \/\/ when the method is defined (see parse_method).\n+            \/\/ A reference to a static init with a bad return type\n+            \/\/ will load and verify OK, but will fail to link.\n+            return args_size;\n+          }\n+        }\n+        \/\/ The distinction between static factory methods and\n+        \/\/ constructors depends on the JVM_ACC_STATIC modifier.\n+        \/\/ This distinction must be reflected in a void or non-void\n+        \/\/ return. For declared methods, the check is in parse_method.\n@@ -5278,0 +5534,6 @@\n+  if (ik->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    oop val = ik->allocate_instance(CHECK_NULL);\n+    vk->set_default_value(val);\n+  }\n+\n@@ -5281,0 +5543,34 @@\n+\/\/ Return true if the specified class is not a valid super class for an inline type.\n+\/\/ A valid super class for an inline type is abstract, has no instance fields,\n+\/\/ does not implement interface java.lang.IdentityObject (checked elsewhere), has\n+\/\/ an empty body-less no-arg constructor, and no synchronized instance methods.\n+\/\/ This function doesn't check if the class's super types are invalid.  Those checks\n+\/\/ are done elsewhere.  The final determination of whether or not a class is an\n+\/\/ invalid super type for an inline class is done in fill_instance_klass().\n+bool ClassFileParser::is_invalid_super_for_inline_type() {\n+  if (class_name() == vmSymbols::java_lang_IdentityObject()) {\n+    return true;\n+  }\n+  if (is_interface() || class_name() == vmSymbols::java_lang_Object()) {\n+    return false;\n+  }\n+  if (!access_flags().is_abstract() || _has_nonstatic_fields) {\n+    return true;\n+  } else {\n+    \/\/ Look at each method\n+    for (int x = 0; x < _methods->length(); x++) {\n+      const Method* const method = _methods->at(x);\n+      if (method->is_synchronized() && !method->is_static()) {\n+        return true;\n+\n+      } else if (method->name() == vmSymbols::object_initializer_name()) {\n+        if (method->signature() != vmSymbols::void_method_signature() ||\n+            !method->is_vanilla_constructor()) {\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -5313,0 +5609,15 @@\n+  if (_field_info->_is_naturally_atomic && ik->is_inline_klass()) {\n+    ik->set_is_naturally_atomic();\n+  }\n+  if (_is_empty_inline_type) {\n+    ik->set_is_empty_inline_type();\n+  }\n+\n+  if (this->_invalid_inline_super) {\n+    ik->set_invalid_inline_super();\n+  }\n+\n+  if (_has_injected_identityObject) {\n+    ik->set_has_injected_identityObject();\n+  }\n+\n@@ -5314,1 +5625,1 @@\n-  ik->set_static_oop_field_count(_fac->count[STATIC_OOP]);\n+  ik->set_static_oop_field_count(_fac->count[STATIC_OOP] + _fac->count[STATIC_INLINE]);\n@@ -5364,0 +5675,3 @@\n+  if (_is_declared_atomic) {\n+    ik->set_is_declared_atomic();\n+  }\n@@ -5475,0 +5789,30 @@\n+  int nfields = ik->java_fields_count();\n+  if (ik->is_inline_klass()) nfields++;\n+  for (int i = 0; i < nfields; i++) {\n+    if (ik->field_is_inline_type(i) && ((ik->field_access_flags(i) & JVM_ACC_STATIC) == 0)) {\n+      Symbol* klass_name = ik->field_signature(i)->fundamental_name(CHECK);\n+      \/\/ Inline classes for instance fields must have been pre-loaded\n+      \/\/ Inline classes for static fields might not have been loaded yet\n+      Klass* klass = SystemDictionary::find(klass_name,\n+          Handle(THREAD, ik->class_loader()),\n+          Handle(THREAD, ik->protection_domain()), CHECK);\n+      if (klass != NULL) {\n+        assert(klass->access_flags().is_inline_type(), \"Inline type expected\");\n+        ik->set_inline_type_field_klass(i, klass);\n+      }\n+      klass_name->decrement_refcount();\n+    } else\n+      if (is_inline_type() && ((ik->field_access_flags(i) & JVM_ACC_FIELD_INTERNAL) != 0)\n+        && ((ik->field_access_flags(i) & JVM_ACC_STATIC) != 0)) {\n+      InlineKlass::cast(ik)->set_default_value_offset(ik->field_offset(i));\n+    }\n+  }\n+\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_alignment(_alignment);\n+    vk->set_first_field_offset(_first_field_offset);\n+    vk->set_exact_size_in_bytes(_exact_size_in_bytes);\n+    InlineKlass::cast(ik)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -5520,0 +5864,4 @@\n+  if (ik->name() == vmSymbols::java_lang_IdentityObject()) {\n+    Universe::initialize_the_single_IdentityObject_klass_array(ik, CHECK);\n+  }\n+\n@@ -5622,0 +5970,1 @@\n+  _temp_local_interfaces(NULL),\n@@ -5661,0 +6010,9 @@\n+  _has_inline_type_fields(false),\n+  _has_nonstatic_fields(false),\n+  _is_empty_inline_type(false),\n+  _is_naturally_atomic(false),\n+  _is_declared_atomic(false),\n+  _invalid_inline_super(false),\n+  _invalid_identity_super(false),\n+  _implements_identityObject(false),\n+  _has_injected_identityObject(false),\n@@ -5871,2 +6229,1 @@\n-  \/\/ Access flags\n-  jint flags;\n+  jint recognized_modifiers = JVM_RECOGNIZED_CLASS_MODIFIERS;\n@@ -5875,3 +6232,1 @@\n-    flags = stream->get_u2_fast() & (JVM_RECOGNIZED_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-  } else {\n-    flags = stream->get_u2_fast() & JVM_RECOGNIZED_CLASS_MODIFIERS;\n+    recognized_modifiers |= JVM_ACC_MODULE;\n@@ -5879,0 +6234,7 @@\n+  \/\/ JVM_ACC_INLINE is defined for class file version 55 and later\n+  if (supports_inline_types()) {\n+    recognized_modifiers |= JVM_ACC_INLINE;\n+  }\n+\n+  \/\/ Access flags\n+  jint flags = stream->get_u2_fast() & recognized_modifiers;\n@@ -5999,0 +6361,1 @@\n+                   is_inline_type(),\n@@ -6000,0 +6363,1 @@\n+                   &_is_declared_atomic,\n@@ -6002,1 +6366,1 @@\n-  assert(_local_interfaces != NULL, \"invariant\");\n+  assert(_temp_local_interfaces != NULL, \"invariant\");\n@@ -6007,1 +6371,2 @@\n-               _access_flags.is_interface(),\n+               is_interface(),\n+               is_inline_type(),\n@@ -6019,1 +6384,2 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                is_inline_type(),\n@@ -6090,3 +6456,3 @@\n-    check_property(_local_interfaces == Universe::the_empty_instance_klass_array(),\n-                   \"java.lang.Object cannot implement an interface in class file %s\",\n-                   CHECK);\n+    check_property(_temp_local_interfaces->length() == 0,\n+        \"java.lang.Object cannot implement an interface in class file %s\",\n+        CHECK);\n@@ -6097,1 +6463,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -6118,0 +6484,3 @@\n+    if (_super_klass->is_declared_atomic()) {\n+      _is_declared_atomic = true;\n+    }\n@@ -6130,0 +6499,37 @@\n+\n+    \/\/ For an inline class, only java\/lang\/Object or special abstract classes\n+    \/\/ are acceptable super classes.\n+    if (is_inline_type()) {\n+      const InstanceKlass* super_ik = _super_klass;\n+      if (super_ik->invalid_inline_super()) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(\n+          THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_IncompatibleClassChangeError(),\n+          \"inline class %s has an invalid super class %s\",\n+          _class_name->as_klass_external_name(),\n+          _super_klass->external_name());\n+        return;\n+      }\n+    }\n+  }\n+\n+  if (_class_name == vmSymbols::java_lang_NonTearable() && _loader_data->class_loader() == NULL) {\n+    \/\/ This is the original source of this condition.\n+    \/\/ It propagates by inheritance, as if testing \"instanceof NonTearable\".\n+    _is_declared_atomic = true;\n+  } else if (*ForceNonTearable != '\\0') {\n+    \/\/ Allow a command line switch to force the same atomicity property:\n+    const char* class_name_str = _class_name->as_C_string();\n+    if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+      _is_declared_atomic = true;\n+    }\n+  }\n+\n+  \/\/ Set ik->invalid_inline_super field to TRUE if already marked as invalid,\n+  \/\/ if super is marked invalid, or if is_invalid_super_for_inline_type()\n+  \/\/ returns true\n+  if (invalid_inline_super() ||\n+      (_super_klass != NULL && _super_klass->invalid_inline_super()) ||\n+      is_invalid_super_for_inline_type()) {\n+    set_invalid_inline_super();\n@@ -6132,0 +6538,19 @@\n+  if (!is_inline_type() && invalid_inline_super() && (_super_klass == NULL || !_super_klass->invalid_inline_super())\n+      && !_implements_identityObject && class_name() != vmSymbols::java_lang_IdentityObject()) {\n+    _temp_local_interfaces->append(SystemDictionary::IdentityObject_klass());\n+    _has_injected_identityObject = true;\n+  }\n+  int itfs_len = _temp_local_interfaces->length();\n+  if (itfs_len == 0) {\n+    _local_interfaces = Universe::the_empty_instance_klass_array();\n+  } else if (itfs_len == 1 && _temp_local_interfaces->at(0) == SystemDictionary::IdentityObject_klass()) {\n+    _local_interfaces = Universe::the_single_IdentityObject_klass_array();\n+  } else {\n+    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);\n+    for (int i = 0; i < itfs_len; i++) {\n+      _local_interfaces->at_put(i, _temp_local_interfaces->at(i));\n+    }\n+  }\n+  _temp_local_interfaces = NULL;\n+  assert(_local_interfaces != NULL, \"invariant\");\n+\n@@ -6160,1 +6585,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -6166,0 +6591,19 @@\n+\n+  for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {\n+    if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE  && !fs.access_flags().is_static()) {\n+      \/\/ Pre-load inline class\n+      Klass* klass = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+          Handle(THREAD, _loader_data->class_loader()),\n+          _protection_domain, true, CHECK);\n+      assert(klass != NULL, \"Sanity check\");\n+      if (!klass->access_flags().is_inline_type()) {\n+        assert(klass->is_instance_klass(), \"Sanity check\");\n+        ResourceMark rm(THREAD);\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"Class %s expects class %s to be an inline type, but it is not\",\n+                    _class_name->as_C_string(),\n+                    InstanceKlass::cast(klass)->external_name()));\n+      }\n+    }\n+  }\n+\n@@ -6168,2 +6612,9 @@\n-                        _parsed_annotations->is_contended(), _field_info);\n-  lb.build_layout();\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      loader_data(), _protection_domain, _field_info);\n+  lb.build_layout(CHECK);\n+  if (is_inline_type()) {\n+    _alignment = lb.get_alignment();\n+    _first_field_offset = lb.get_first_field_offset();\n+    _exact_size_in_bytes = lb.get_exact_size_in_byte();\n+  }\n+  _has_inline_type_fields = _field_info->_has_inline_fields;\n@@ -6171,1 +6622,1 @@\n-  \/\/ Compute reference typ\n+  \/\/ Compute reference type\n@@ -6173,1 +6624,0 @@\n-\n@@ -6205,0 +6655,1 @@\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":537,"deletions":86,"binary":false,"changes":623,"status":"modified"},{"patch":"@@ -527,0 +527,2 @@\n+  do_signature(getValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;)Ljava\/lang\/Object;\")                   \\\n+  do_signature(putValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;Ljava\/lang\/Object;)V\")                  \\\n@@ -537,0 +539,3 @@\n+  do_name(getValue_name,\"getValue\")             do_name(putValue_name,\"putValue\")                                       \\\n+  do_name(makePrivateBuffer_name,\"makePrivateBuffer\")                                                                   \\\n+  do_name(finishPrivateBuffer_name,\"finishPrivateBuffer\")                                                               \\\n@@ -547,0 +552,1 @@\n+  do_intrinsic(_getValue,           jdk_internal_misc_Unsafe,     getValue_name, getValue_signature,             F_RN)  \\\n@@ -556,0 +562,4 @@\n+  do_intrinsic(_putValue,           jdk_internal_misc_Unsafe,     putValue_name, putValue_signature,             F_RN)  \\\n+                                                                                                                        \\\n+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \\\n+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+  template(java_lang_IdentityObject,                  \"java\/lang\/IdentityObject\")                 \\\n@@ -67,0 +68,1 @@\n+  template(java_lang_NonTearable,                     \"java\/lang\/NonTearable\")                    \\\n@@ -459,0 +461,2 @@\n+  template(default_value_name,                        \".default\")                                 \\\n+  template(empty_marker_name,                         \".empty\")                                   \\\n@@ -529,0 +533,1 @@\n+  template(object_object_boolean_signature,           \"(Ljava\/lang\/Object;Ljava\/lang\/Object;)Z\") \\\n@@ -676,0 +681,5 @@\n+  template(java_lang_invoke_ValueBootstrapMethods, \"java\/lang\/invoke\/ValueBootstrapMethods\")                      \\\n+  template(isSubstitutable_name,                   \"isSubstitutable\")                                             \\\n+  template(inlineObjectHashCode_name,              \"inlineObjectHashCode\")                                        \\\n+                                                                                                                  \\\n+  template(jdk_internal_vm_jni_SubElementSelector, \"jdk\/internal\/vm\/jni\/SubElementSelector\")                      \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -145,0 +145,1 @@\n+  virtual bool is_buffered_inline_type_blob() const   { return false; }\n@@ -398,0 +399,1 @@\n+  friend class BufferedInlineTypeBlob;\n@@ -403,1 +405,2 @@\n-  BufferBlob(const char* name, int size, CodeBuffer* cb);\n+  BufferBlob(const char* name, int header_size, int size, CodeBuffer* cb);\n+  BufferBlob(const char* name, int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n@@ -436,1 +439,1 @@\n-  AdapterBlob(int size, CodeBuffer* cb);\n+  AdapterBlob(int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n@@ -440,1 +443,5 @@\n-  static AdapterBlob* create(CodeBuffer* cb);\n+  static AdapterBlob* create(CodeBuffer* cb,\n+                             int frame_complete,\n+                             int frame_size,\n+                             OopMapSet* oop_maps,\n+                             bool caller_must_gc_arguments = false);\n@@ -444,0 +451,2 @@\n+\n+  bool caller_must_gc_arguments(JavaThread* thread) const { return true; }\n@@ -474,0 +483,22 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ BufferedInlineTypeBlob : used for pack\/unpack handlers\n+\n+class BufferedInlineTypeBlob: public BufferBlob {\n+private:\n+  const int _pack_fields_off;\n+  const int _pack_fields_jobject_off;\n+  const int _unpack_fields_off;\n+\n+  BufferedInlineTypeBlob(int size, CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off);\n+\n+public:\n+  \/\/ Creation\n+  static BufferedInlineTypeBlob* create(CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off);\n+\n+  address pack_fields() const { return code_begin() + _pack_fields_off; }\n+  address pack_fields_jobject() const { return code_begin() + _pack_fields_jobject_off; }\n+  address unpack_fields() const { return code_begin() + _unpack_fields_off; }\n+\n+  \/\/ Typing\n+  virtual bool is_buffered_inline_type_blob() const { return true; }\n+};\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":34,"deletions":3,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -162,1 +162,2 @@\n-  assert(_desc->tos_in()  == tos_in , \"inconsistent tos_in  information\");\n+  assert(_desc->tos_in()  == tos_in,\n+         \"inconsistent tos_in  information\");\n@@ -233,0 +234,1 @@\n+\n@@ -285,1 +287,1 @@\n-  def(Bytecodes::_aaload              , ____|____|____|____, itos, atos, aaload              ,  _           );\n+  def(Bytecodes::_aaload              , ____|____|clvm|____, itos, atos, aaload              ,  _           );\n@@ -437,0 +439,3 @@\n+  def(Bytecodes::_breakpoint          , ubcp|disp|clvm|____, vtos, vtos, _breakpoint         ,  _           );\n+  def(Bytecodes::_defaultvalue        , ubcp|____|clvm|____, vtos, atos, defaultvalue        , _            );\n+  def(Bytecodes::_withfield           , ubcp|____|clvm|____, vtos, atos, withfield           , _            );\n@@ -455,0 +460,1 @@\n+  def(Bytecodes::_fast_qgetfield      , ubcp|____|clvm|____, atos, atos, fast_accessfield    ,  atos        );\n@@ -464,0 +470,1 @@\n+  def(Bytecodes::_fast_qputfield      , ubcp|____|clvm|____, atos, vtos, fast_storefield ,   atos        );\n@@ -500,0 +507,1 @@\n+\n","filename":"src\/hotspot\/share\/interpreter\/templateTable.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -296,0 +296,1 @@\n+  static void withfield();\n@@ -298,0 +299,1 @@\n+  static void defaultvalue();\n","filename":"src\/hotspot\/share\/interpreter\/templateTable.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1190,1 +1190,1 @@\n-  _debug_recorder->describe_scope(pc_offset, method, NULL, bci, reexecute, throw_exception, is_mh_invoke, return_oop,\n+  _debug_recorder->describe_scope(pc_offset, method, NULL, bci, reexecute, throw_exception, is_mh_invoke, return_oop, false,\n@@ -1356,0 +1356,2 @@\n+        _offsets.set_value(CodeOffsets::Verified_Inline_Entry, pc_offset);\n+        _offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, pc_offset);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -320,1 +320,0 @@\n-    assert(type == _method_entry_ref, \"only special type allowed for now\");\n@@ -323,1 +322,1 @@\n-    _builder->add_special_ref(type, src_obj, field_offset);\n+    _builder->add_special_ref(type, src_obj, field_offset, ref->size() * BytesPerWord);\n@@ -367,4 +366,0 @@\n-void ArchiveBuilder::add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset) {\n-  _special_refs->append(SpecialRefInfo(type, src_obj, field_offset));\n-}\n-\n@@ -516,2 +511,18 @@\n-    assert(s.type() == MetaspaceClosure::_method_entry_ref, \"only special type allowed for now\");\n-    assert(*src_p == *dst_p, \"must be a copy\");\n+\n+    MetaspaceClosure::assert_valid(s.type());\n+    switch (s.type()) {\n+    case MetaspaceClosure::_method_entry_ref:\n+      assert(*src_p == *dst_p, \"must be a copy\");\n+      break;\n+    case MetaspaceClosure::_internal_pointer_ref:\n+      {\n+        \/\/ *src_p points to a location inside src_obj. Let's make *dst_p point to\n+        \/\/ the same location inside dst_obj.\n+        size_t off = pointer_delta(*((address*)src_p), src_obj, sizeof(u1));\n+        assert(off < s.src_obj_size_in_bytes(), \"must point to internal address\");\n+        *((address*)dst_p) = dst_obj + off;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n","filename":"src\/hotspot\/share\/memory\/archiveBuilder.cpp","additions":19,"deletions":8,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -61,1 +63,3 @@\n-  f(TypeArrayKlass)\n+  f(TypeArrayKlass) \\\n+  f(FlatArrayKlass) \\\n+  f(InlineKlass)\n","filename":"src\/hotspot\/share\/memory\/cppVtables.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -219,1 +219,1 @@\n-  int lh = array_layout_helper(tag, hsize, etype, exact_log2(esize));\n+  int lh = array_layout_helper(tag, false, hsize, etype, exact_log2(esize));\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+  FlatArrayKlassID,\n@@ -51,1 +52,1 @@\n-const uint KLASS_ID_COUNT = 6;\n+const uint KLASS_ID_COUNT = 7;\n@@ -101,1 +102,1 @@\n-  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops\n+  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops, 0xA0 if value types\n@@ -363,1 +364,1 @@\n-  static const int _lh_array_tag_bits          = 2;\n+  static const int _lh_array_tag_bits          = 3;\n@@ -365,2 +366,10 @@\n-  static const int _lh_array_tag_obj_value     = ~0x01;   \/\/ 0x80000000 >> 30\n-  static const unsigned int _lh_array_tag_type_value = 0Xffffffff; \/\/ ~0x00,  \/\/ 0xC0000000 >> 30\n+  static const unsigned int _lh_array_tag_type_value = 0Xfffffffc;\n+  static const unsigned int _lh_array_tag_vt_value   = 0Xfffffffd;\n+  static const unsigned int _lh_array_tag_obj_value  = 0Xfffffffe;\n+\n+  \/\/ null-free array flag bit under the array tag bits, shift one more to get array tag value\n+  static const int _lh_null_free_shift = _lh_array_tag_shift - 1;\n+  static const int _lh_null_free_mask  = 1;\n+\n+  static const jint _lh_array_tag_vt_value_bit_inplace = (jint) (1 << _lh_array_tag_shift);\n+  static const jint _lh_null_free_bit_inplace = (jint) (_lh_null_free_mask << _lh_null_free_shift);\n@@ -384,2 +393,1 @@\n-    \/\/ _lh_array_tag_type_value == (lh >> _lh_array_tag_shift);\n-    return (juint)lh >= (juint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+    return (juint) _lh_array_tag_type_value == (juint)(lh >> _lh_array_tag_shift);\n@@ -388,2 +396,13 @@\n-    \/\/ _lh_array_tag_obj_value == (lh >> _lh_array_tag_shift);\n-    return (jint)lh < (jint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+    return (juint)_lh_array_tag_obj_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_flatArray(jint lh) {\n+    return (juint)_lh_array_tag_vt_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_null_free(jint lh) {\n+    assert(layout_helper_is_flatArray(lh) || layout_helper_is_objArray(lh), \"must be array of inline types\");\n+    return ((lh >> _lh_null_free_shift) & _lh_null_free_mask);\n+  }\n+  static jint layout_helper_set_null_free(jint lh) {\n+    lh |= (_lh_null_free_mask << _lh_null_free_shift);\n+    assert(layout_helper_is_null_free(lh), \"Bad encoding\");\n+    return lh;\n@@ -400,1 +419,1 @@\n-    assert(btvalue >= T_BOOLEAN && btvalue <= T_OBJECT, \"sanity\");\n+    assert((btvalue >= T_BOOLEAN && btvalue <= T_OBJECT) || btvalue == T_INLINE_TYPE, \"sanity\");\n@@ -421,1 +440,1 @@\n-    assert(l2esz <= LogBytesPerLong,\n+    assert(layout_helper_element_type(lh) == T_INLINE_TYPE || l2esz <= LogBytesPerLong,\n@@ -425,1 +444,1 @@\n-  static jint array_layout_helper(jint tag, int hsize, BasicType etype, int log2_esize) {\n+  static jint array_layout_helper(jint tag, bool null_free, int hsize, BasicType etype, int log2_esize) {\n@@ -427,0 +446,1 @@\n+      |    ((null_free ? 1 : 0) <<  _lh_null_free_shift)\n@@ -567,0 +587,2 @@\n+  \/\/ For value classes, this returns the name with a leading 'Q' and a trailing ';'\n+  \/\/     and the package separators as '\/'.\n@@ -582,0 +604,1 @@\n+  virtual bool is_flatArray_klass_slow()    const { return false; }\n@@ -583,0 +606,2 @@\n+  \/\/ current implementation uses this method even in non debug builds\n+  virtual bool is_inline_klass_slow()       const { return false; }\n@@ -608,0 +633,5 @@\n+  inline  bool is_inline_klass()              const { return is_inline_klass_slow(); } \/\/temporary hack\n+  inline  bool is_flatArray_klass()           const { return assert_same_query(\n+                                                    layout_helper_is_flatArray(layout_helper()),\n+                                                    is_flatArray_klass_slow()); }\n+\n@@ -610,0 +640,2 @@\n+  inline bool is_null_free_array_klass()      const { return layout_helper_is_null_free(layout_helper()); }\n+\n@@ -645,1 +677,4 @@\n-  markWord prototype_header() const      { return _prototype_header; }\n+  markWord prototype_header() const     { return _prototype_header; }\n+  static inline markWord default_prototype_header(Klass* k) {\n+    return (k == NULL) ? markWord::prototype() : k->prototype_header();\n+  }\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":48,"deletions":13,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -98,1 +98,1 @@\n-  return ObjectSynchronizer::identity_hash_value_for(object);\n+  return ObjectSynchronizer::FastHashCode(THREAD, object());\n@@ -117,1 +117,1 @@\n-  return !SafepointSynchronize::is_at_safepoint();\n+  return !SafepointSynchronize::is_at_safepoint() ;\n@@ -140,0 +140,2 @@\n+bool oopDesc::is_value_noinline()             const { return is_inline_type();         }\n+bool oopDesc::is_flatArray_noinline()         const { return is_flatArray();           }\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+typedef class     flatArrayOopDesc*           flatArrayOop;\n@@ -148,0 +149,1 @@\n+DEF_OOP(flatArray);\n@@ -181,0 +183,1 @@\n+class     InlineKlass;\n@@ -184,0 +187,1 @@\n+class     FlatArrayKlass;\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -780,0 +780,3 @@\n+  product(bool, UseArrayLoadStoreProfile, true,                             \\\n+          \"Take advantage of profiling at array load\/store\")                \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -565,0 +565,3 @@\n+  if (n->is_InlineTypeBase()) {\n+    C->add_inline_type(n);\n+  }\n@@ -642,0 +645,3 @@\n+  if (is_InlineTypeBase()) {\n+    compile->remove_inline_type(this);\n+  }\n@@ -1415,0 +1421,3 @@\n+      if (dead->is_InlineTypeBase()) {\n+        igvn->C->remove_inline_type(dead);\n+      }\n@@ -2177,1 +2186,3 @@\n-      assert(i >= req() || i == 0 || is_Region() || is_Phi() || is_ArrayCopy() || (is_Unlock() && i == req()-1)\n+      assert(i >= req() || i == 0 || is_Region() || is_Phi() || is_ArrayCopy() ||\n+             (is_Allocate() && i >= AllocateNode::InlineTypeNode) ||\n+             (is_Unlock() && i == req()-1)\n@@ -2179,1 +2190,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+             \"only region, phi, arraycopy, allocate or unlock nodes have null data edges\");\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -104,0 +104,1 @@\n+class MachPrologNode;\n@@ -110,0 +111,1 @@\n+class MachVEPNode;\n@@ -153,0 +155,3 @@\n+class InlineTypeBaseNode;\n+class InlineTypeNode;\n+class InlineTypePtrNode;\n@@ -665,0 +670,2 @@\n+      DEFINE_CLASS_ID(MachProlog,       Mach, 8)\n+      DEFINE_CLASS_ID(MachVEP,          Mach, 9)\n@@ -679,0 +686,3 @@\n+      DEFINE_CLASS_ID(InlineTypeBase, Type, 8)\n+        DEFINE_CLASS_ID(InlineType, InlineTypeBase, 0)\n+        DEFINE_CLASS_ID(InlineTypePtr, InlineTypeBase, 1)\n@@ -857,0 +867,1 @@\n+  DEFINE_CLASS_QUERY(MachProlog)\n@@ -863,0 +874,1 @@\n+  DEFINE_CLASS_QUERY(MachVEP)\n@@ -886,0 +898,3 @@\n+  DEFINE_CLASS_QUERY(InlineType)\n+  DEFINE_CLASS_QUERY(InlineTypeBase)\n+  DEFINE_CLASS_QUERY(InlineTypePtr)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -52,0 +52,2 @@\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -478,0 +480,1 @@\n+  bool is_inlined = InstanceKlass::cast(k1)->field_is_inlined(slot);\n@@ -479,1 +482,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset);\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset, is_inlined);\n@@ -498,1 +501,1 @@\n-  if (m->is_initializer()) {\n+  if (m->is_object_constructor() || m->is_static_init_factory()) {\n@@ -560,1 +563,0 @@\n-\n@@ -896,1 +898,2 @@\n-    case T_OBJECT:      push_object(va_arg(_ap, jobject)); break;\n+    case T_OBJECT:\n+    case T_INLINE_TYPE: push_object(va_arg(_ap, jobject)); break;\n@@ -932,1 +935,2 @@\n-    case T_OBJECT:      push_object((_ap++)->l); break;\n+    case T_OBJECT:\n+    case T_INLINE_TYPE: push_object((_ap++)->l); break;\n@@ -1072,5 +1076,19 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherArray ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == NULL) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), NULL);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherArray ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  } else {\n+    JavaValue jvalue(T_INLINE_TYPE);\n+    JNI_ArgumentPusherArray ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, NULL, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -1078,1 +1096,1 @@\n-JNI_END\n+  JNI_END\n@@ -1092,5 +1110,19 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherVaArg ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == NULL) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), NULL);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  } else {\n+    JavaValue jvalue(T_INLINE_TYPE);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, NULL, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -1112,8 +1144,25 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  va_list args;\n-  va_start(args, methodID);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherVaArg ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n-  va_end(args);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == NULL) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), NULL);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    va_list args;\n+    va_start(args, methodID);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+    va_end(args);\n+  } else {\n+    va_list args;\n+    va_start(args, methodID);\n+    JavaValue jvalue(T_INLINE_TYPE);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, NULL, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    va_end(args);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -1897,1 +1946,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset());\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset(), fd.is_inlined());\n@@ -1908,0 +1957,1 @@\n+  oop res = NULL;\n@@ -1913,2 +1963,12 @@\n-  oop loaded_obj = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n-  jobject ret = JNIHandles::make_local(THREAD, loaded_obj);\n+  if (!jfieldIDWorkaround::is_inlined_jfieldID(fieldID)) {\n+    res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instance can have inlined fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);  \/\/ performance bottleneck\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineKlass* field_vklass = InlineKlass::cast(holder->get_inline_type_field_klass(fd.index()));\n+    res = field_vklass->read_inlined_field(o, ik->field_offset(fd.index()), CHECK_NULL);\n+  }\n+  jobject ret = JNIHandles::make_local(THREAD, res);\n@@ -2012,1 +2072,12 @@\n-  HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  if (!jfieldIDWorkaround::is_inlined_jfieldID(fieldID)) {\n+    HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instances can have inlined fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineKlass* vklass = InlineKlass::cast(holder->get_inline_type_field_klass(fd.index()));\n+    oop v = JNIHandles::resolve_non_null(value);\n+    vklass->write_inlined_field(o, offset, v, CHECK);\n+  }\n@@ -2449,4 +2520,13 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  if (a->is_within_bounds(index)) {\n-    ret = JNIHandles::make_local(THREAD, a->obj_at(index));\n-    return ret;\n+  oop res = NULL;\n+  arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+  if (arr->is_within_bounds(index)) {\n+    if (arr->is_flatArray()) {\n+      flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+      flatArrayHandle vah(thread, a);\n+      res = flatArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK_NULL);\n+      assert(res != NULL, \"Must be set in one of two paths above\");\n+    } else {\n+      assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+      objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+      res = a->obj_at(index);\n+    }\n@@ -2456,1 +2536,1 @@\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n+    ss.print(\"Index %d out of bounds for length %d\", index,arr->length());\n@@ -2459,0 +2539,2 @@\n+  ret = JNIHandles::make_local(THREAD, res);\n+  return ret;\n@@ -2466,1 +2548,1 @@\n- HOTSPOT_JNI_SETOBJECTARRAYELEMENT_ENTRY(env, array, index, value);\n+  HOTSPOT_JNI_SETOBJECTARRAYELEMENT_ENTRY(env, array, index, value);\n@@ -2469,24 +2551,51 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  oop v = JNIHandles::resolve(value);\n-  if (a->is_within_bounds(index)) {\n-    if (v == NULL || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n-      a->obj_at_put(index, v);\n-    } else {\n-      ResourceMark rm(THREAD);\n-      stringStream ss;\n-      Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n-      ss.print(\"type mismatch: can not store %s to %s[%d]\",\n-               v->klass()->external_name(),\n-               bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n-               index);\n-      for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n-        ss.print(\"[]\");\n-      }\n-      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n-    }\n-  } else {\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n-    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n-  }\n+   bool oob = false;\n+   int length = -1;\n+   oop res = NULL;\n+   arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+   if (arr->is_within_bounds(index)) {\n+     if (arr->is_flatArray()) {\n+       flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       FlatArrayKlass* vaklass = FlatArrayKlass::cast(a->klass());\n+       InlineKlass* element_vklass = vaklass->element_klass();\n+       if (v != NULL && v->is_a(element_vklass)) {\n+         a->value_copy_to_index(v, index);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *kl = FlatArrayKlass::cast(a->klass());\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             kl->external_name(),\n+             index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     } else {\n+       assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+       objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       if (v == NULL || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n+         a->obj_at_put(index, v);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n+                 index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     }\n+   } else {\n+     ResourceMark rm(THREAD);\n+     stringStream ss;\n+     ss.print(\"Index %d out of bounds for length %d\", index, arr->length());\n+     THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n+   }\n@@ -3288,0 +3397,277 @@\n+JNI_ENTRY(void*, jni_GetFlattenedArrayElements(JNIEnv* env, jarray array, jboolean* isCopy))\n+  JNIWrapper(\"jni_GetFlattenedArrayElements\");\n+  if (isCopy != NULL) {\n+    *isCopy = JNI_FALSE;\n+  }\n+  arrayOop ar = arrayOop(JNIHandles::resolve_non_null(array));\n+  if (!ar->is_array()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!ar->is_flatArray()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass());\n+  if (vak->contains_oops()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Flattened array contains oops\");\n+  }\n+  oop a = lock_gc_or_pin_object(thread, array);\n+  flatArrayOop vap = flatArrayOop(a);\n+  void* ret = vap->value_at_addr(0, vak->layout_helper());\n+  return ret;\n+JNI_END\n+\n+JNI_ENTRY(void, jni_ReleaseFlattenedArrayElements(JNIEnv* env, jarray array, void* elem, jint mode))\n+  JNIWrapper(\"jni_ReleaseFlattenedArrayElements\");\n+  unlock_gc_or_unpin_object(thread, array);\n+JNI_END\n+\n+JNI_ENTRY(jsize, jni_GetFlattenedArrayElementSize(JNIEnv* env, jarray array)) {\n+  JNIWrapper(\"jni_GetFlattenedElementSize\");\n+  arrayOop a = arrayOop(JNIHandles::resolve_non_null(array));\n+  if (!a->is_array()) {\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!a->is_flatArray()) {\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(a->klass());\n+  jsize ret = vak->element_byte_size();\n+  return ret;\n+}\n+JNI_END\n+\n+JNI_ENTRY(jclass, jni_GetFlattenedArrayElementClass(JNIEnv* env, jarray array))\n+  JNIWrapper(\"jni_GetArrayElementClass\");\n+  arrayOop a = arrayOop(JNIHandles::resolve_non_null(array));\n+  if (!a->is_array()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!a->is_flatArray()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(a->klass());\n+  InlineKlass* vk = vak->element_klass();\n+  return (jclass) JNIHandles::make_local(vk->java_mirror());\n+JNI_END\n+\n+JNI_ENTRY(jsize, jni_GetFieldOffsetInFlattenedLayout(JNIEnv* env, jclass clazz, const char *name, const char *signature, jboolean* is_inlined))\n+  JNIWrapper(\"jni_GetFieldOffsetInFlattenedLayout\");\n+\n+  oop mirror = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(mirror);\n+  if (!k->is_inline_klass()) {\n+    ResourceMark rm;\n+        THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), err_msg(\"%s has not flattened layout\", k->external_name()));\n+  }\n+  InlineKlass* vk = InlineKlass::cast(k);\n+\n+  TempNewSymbol fieldname = SymbolTable::probe(name, (int)strlen(name));\n+  TempNewSymbol signame = SymbolTable::probe(signature, (int)strlen(signature));\n+  if (fieldname == NULL || signame == NULL) {\n+    ResourceMark rm;\n+    THROW_MSG_0(vmSymbols::java_lang_NoSuchFieldError(), err_msg(\"%s.%s %s\", vk->external_name(), name, signature));\n+  }\n+\n+  assert(vk->is_initialized(), \"If a flattened array has been created, the element klass must have been initialized\");\n+\n+  fieldDescriptor fd;\n+  if (!vk->is_instance_klass() ||\n+      !InstanceKlass::cast(vk)->find_field(fieldname, signame, false, &fd)) {\n+    ResourceMark rm;\n+    THROW_MSG_0(vmSymbols::java_lang_NoSuchFieldError(), err_msg(\"%s.%s %s\", vk->external_name(), name, signature));\n+  }\n+\n+  int offset = fd.offset() - vk->first_field_offset();\n+  if (is_inlined != NULL) {\n+    *is_inlined = fd.is_inlined();\n+  }\n+  return (jsize)offset;\n+JNI_END\n+\n+JNI_ENTRY(jobject, jni_CreateSubElementSelector(JNIEnv* env, jarray array))\n+  JNIWrapper(\"jni_CreateSubElementSelector\");\n+\n+  oop ar = JNIHandles::resolve_non_null(array);\n+  if (!ar->is_array()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!ar->is_flatArray()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  flatArrayHandle ar_h(THREAD, flatArrayOop(ar));\n+  Klass* ses_k = SystemDictionary::resolve_or_null(vmSymbols::jdk_internal_vm_jni_SubElementSelector(),\n+        Handle(THREAD, SystemDictionary::java_system_loader()), Handle(), CHECK_NULL);\n+  InstanceKlass* ses_ik = InstanceKlass::cast(ses_k);\n+  ses_ik->initialize(CHECK_NULL);\n+  Klass* elementKlass = ArrayKlass::cast(ar_h()->klass())->element_klass();\n+  oop ses = ses_ik->allocate_instance(CHECK_NULL);\n+  Handle ses_h(THREAD, ses);\n+  jdk_internal_vm_jni_SubElementSelector::setArrayElementType(ses_h(), elementKlass->java_mirror());\n+  jdk_internal_vm_jni_SubElementSelector::setSubElementType(ses_h(), elementKlass->java_mirror());\n+  jdk_internal_vm_jni_SubElementSelector::setOffset(ses_h(), 0);\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlined(ses_h(), true);   \/\/ by definition, top element of a flattened array is inlined\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlineType(ses_h(), true); \/\/ by definition, top element of a flattened array is an inline type\n+  return JNIHandles::make_local(ses_h());\n+JNI_END\n+\n+JNI_ENTRY(jobject, jni_GetSubElementSelector(JNIEnv* env, jobject selector, jfieldID fieldID))\n+  JNIWrapper(\"jni_GetSubElementSelector\");\n+\n+  oop slct = JNIHandles::resolve_non_null(selector);\n+  if (slct->klass()->name() != vmSymbols::jdk_internal_vm_jni_SubElementSelector()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a SubElementSelector\");\n+  }\n+  jboolean is_inlined = jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct);\n+  if (!is_inlined) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"SubElement is not inlined\");\n+  }\n+  oop semirror = jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct);\n+  Klass* k = java_lang_Class::as_Klass(semirror);\n+  if (!k->is_inline_klass()) {\n+    ResourceMark rm;\n+        THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), err_msg(\"%s is not an inline type\", k->external_name()));\n+  }\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  assert(vk->is_initialized(), \"If a flattened array has been created, the element klass must have been initialized\");\n+  int field_offset = jfieldIDWorkaround::from_instance_jfieldID(vk, fieldID);\n+  fieldDescriptor fd;\n+  if (!vk->find_field_from_offset(field_offset, false, &fd)) {\n+    THROW_NULL(vmSymbols::java_lang_NoSuchFieldError());\n+  }\n+  Handle arrayElementMirror(THREAD, jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct));\n+  \/\/ offset of the SubElement is offset of the original SubElement plus the offset of the field inside the element\n+  int offset = fd.offset() - vk->first_field_offset() + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+  InstanceKlass* sesklass = InstanceKlass::cast(JNIHandles::resolve_non_null(selector)->klass());\n+  oop res = sesklass->allocate_instance(CHECK_NULL);\n+  Handle res_h(THREAD, res);\n+  jdk_internal_vm_jni_SubElementSelector::setArrayElementType(res_h(), arrayElementMirror());\n+  InstanceKlass* holder = fd.field_holder();\n+  BasicType bt = Signature::basic_type(fd.signature());\n+  if (is_java_primitive(bt)) {\n+    jdk_internal_vm_jni_SubElementSelector::setSubElementType(res_h(), java_lang_Class::primitive_mirror(bt));\n+  } else {\n+    Klass* fieldKlass = SystemDictionary::resolve_or_fail(fd.signature(), Handle(THREAD, holder->class_loader()),\n+        Handle(THREAD, holder->protection_domain()), true, CHECK_NULL);\n+    jdk_internal_vm_jni_SubElementSelector::setSubElementType(res_h(),fieldKlass->java_mirror());\n+  }\n+  jdk_internal_vm_jni_SubElementSelector::setOffset(res_h(), offset);\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlined(res_h(), fd.is_inlined());\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlineType(res_h(), fd.is_inline_type());\n+  return JNIHandles::make_local(res_h());\n+JNI_END\n+\n+JNI_ENTRY(jobject, jni_GetObjectSubElement(JNIEnv* env, jarray array, jobject selector, int index))\n+  JNIWrapper(\"jni_GetObjectSubElement\");\n+\n+  flatArrayOop ar =  (flatArrayOop)JNIHandles::resolve_non_null(array);\n+  oop slct = JNIHandles::resolve_non_null(selector);\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass());\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\");\n+  }\n+  oop res = NULL;\n+  if (!jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct)) {\n+    int offset = (address)ar->base() - cast_from_oop<address>(ar) + index * vak->element_byte_size()\n+                      + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+    res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(ar, offset);\n+  } else {\n+    Handle slct_h(THREAD, slct);\n+    InlineKlass* fieldKlass = InlineKlass::cast(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)));\n+    res = fieldKlass->allocate_instance_buffer(CHECK_NULL);\n+    \/\/ The array might have been moved by the GC, refreshing the arrayOop\n+    ar =  (flatArrayOop)JNIHandles::resolve_non_null(array);\n+    address addr = (address)ar->value_at_addr(index, vak->layout_helper())\n+              + jdk_internal_vm_jni_SubElementSelector::getOffset(slct_h());\n+    fieldKlass->inline_copy_payload_to_new_oop(addr, res);\n+  }\n+  return JNIHandles::make_local(res);\n+JNI_END\n+\n+JNI_ENTRY(void, jni_SetObjectSubElement(JNIEnv* env, jarray array, jobject selector, int index, jobject value))\n+  JNIWrapper(\"jni_SetObjectSubElement\");\n+\n+  flatArrayOop ar =  (flatArrayOop)JNIHandles::resolve_non_null(array);\n+  oop slct = JNIHandles::resolve_non_null(selector);\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass());\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\");\n+  }\n+  oop val = JNIHandles::resolve(value);\n+  if (val == NULL) {\n+    if (jdk_internal_vm_jni_SubElementSelector::getIsInlineType(slct)) {\n+      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), \"null cannot be stored in a flattened array\");\n+    }\n+  } else {\n+    if (!val->is_a(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)))) {\n+      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), \"type mismatch\");\n+    }\n+  }\n+  if (!jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct)) {\n+    int offset = (address)ar->base() - cast_from_oop<address>(ar) + index * vak->element_byte_size()\n+                  + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+    HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(ar, offset, JNIHandles::resolve(value));\n+  } else {\n+    InlineKlass* fieldKlass = InlineKlass::cast(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)));\n+    address addr = (address)ar->value_at_addr(index, vak->layout_helper())\n+                  + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+    fieldKlass->inline_copy_oop_to_payload(JNIHandles::resolve_non_null(value), addr);\n+  }\n+JNI_END\n+\n+#define DEFINE_GETSUBELEMENT(ElementType,Result,ElementBasicType) \\\n+\\\n+JNI_ENTRY(ElementType, \\\n+          jni_Get##Result##SubElement(JNIEnv *env, jarray array, jobject selector, int index)) \\\n+  JNIWrapper(\"Get\" XSTR(Result) \"SubElement\"); \\\n+  flatArrayOop ar = (flatArrayOop)JNIHandles::resolve_non_null(array); \\\n+  oop slct = JNIHandles::resolve_non_null(selector); \\\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass()); \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) { \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\"); \\\n+  } \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct) != java_lang_Class::primitive_mirror(ElementBasicType)) { \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Wrong SubElement type\"); \\\n+  } \\\n+  address addr = (address)ar->value_at_addr(index, vak->layout_helper()) \\\n+               + jdk_internal_vm_jni_SubElementSelector::getOffset(slct); \\\n+  ElementType result = *(ElementType*)addr; \\\n+  return result; \\\n+JNI_END\n+\n+DEFINE_GETSUBELEMENT(jboolean, Boolean,T_BOOLEAN)\n+DEFINE_GETSUBELEMENT(jbyte, Byte, T_BYTE)\n+DEFINE_GETSUBELEMENT(jshort, Short,T_SHORT)\n+DEFINE_GETSUBELEMENT(jchar, Char,T_CHAR)\n+DEFINE_GETSUBELEMENT(jint, Int,T_INT)\n+DEFINE_GETSUBELEMENT(jlong, Long,T_LONG)\n+DEFINE_GETSUBELEMENT(jfloat, Float,T_FLOAT)\n+DEFINE_GETSUBELEMENT(jdouble, Double,T_DOUBLE)\n+\n+#define DEFINE_SETSUBELEMENT(ElementType,Result,ElementBasicType) \\\n+\\\n+JNI_ENTRY(void, \\\n+          jni_Set##Result##SubElement(JNIEnv *env, jarray array, jobject selector, int index, ElementType value)) \\\n+  JNIWrapper(\"Get\" XSTR(Result) \"SubElement\"); \\\n+  flatArrayOop ar = (flatArrayOop)JNIHandles::resolve_non_null(array); \\\n+  oop slct = JNIHandles::resolve_non_null(selector); \\\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass()); \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) { \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\"); \\\n+  } \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct) != java_lang_Class::primitive_mirror(ElementBasicType)) { \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Wrong SubElement type\"); \\\n+  } \\\n+  address addr = (address)ar->value_at_addr(index, vak->layout_helper()) \\\n+               + jdk_internal_vm_jni_SubElementSelector::getOffset(slct); \\\n+  *(ElementType*)addr = value; \\\n+JNI_END\n+\n+DEFINE_SETSUBELEMENT(jboolean, Boolean,T_BOOLEAN)\n+DEFINE_SETSUBELEMENT(jbyte, Byte, T_BYTE)\n+DEFINE_SETSUBELEMENT(jshort, Short,T_SHORT)\n+DEFINE_SETSUBELEMENT(jchar, Char,T_CHAR)\n+DEFINE_SETSUBELEMENT(jint, Int,T_INT)\n+DEFINE_SETSUBELEMENT(jlong, Long,T_LONG)\n+DEFINE_SETSUBELEMENT(jfloat, Float,T_FLOAT)\n+DEFINE_SETSUBELEMENT(jdouble, Double,T_DOUBLE)\n+\n@@ -3571,1 +3957,32 @@\n-    jni_GetModule\n+    jni_GetModule,\n+\n+    \/\/ Flattened arrays features\n+\n+    jni_GetFlattenedArrayElements,\n+    jni_ReleaseFlattenedArrayElements,\n+    jni_GetFlattenedArrayElementClass,\n+    jni_GetFlattenedArrayElementSize,\n+    jni_GetFieldOffsetInFlattenedLayout,\n+\n+    jni_CreateSubElementSelector,\n+    jni_GetSubElementSelector,\n+    jni_GetObjectSubElement,\n+    jni_SetObjectSubElement,\n+\n+    jni_GetBooleanSubElement,\n+    jni_GetByteSubElement,\n+    jni_GetShortSubElement,\n+    jni_GetCharSubElement,\n+    jni_GetIntSubElement,\n+    jni_GetLongSubElement,\n+    jni_GetFloatSubElement,\n+    jni_GetDoubleSubElement,\n+\n+    jni_SetBooleanSubElement,\n+    jni_SetByteSubElement,\n+    jni_SetShortSubElement,\n+    jni_SetCharSubElement,\n+    jni_SetIntSubElement,\n+    jni_SetLongSubElement,\n+    jni_SetFloatSubElement,\n+    jni_SetDoubleSubElement\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":476,"deletions":59,"binary":false,"changes":535,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-#include \"memory\/iterator.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n@@ -55,0 +55,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -60,0 +61,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -1825,0 +1827,92 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return NULL;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(aoop->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return NULL;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayOop result_array =\n+      oopFactory::new_objArray(SystemDictionary::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  instanceOop ioop = ih();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ioop->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array);\n+WB_END\n+\n+class CollectOops : public BasicOopIterateClosure {\n+ public:\n+  GrowableArray<Handle>* array;\n+\n+  objArrayOop create_results(TRAPS) {\n+    objArrayOop result_array =\n+        oopFactory::new_objArray(SystemDictionary::Object_klass(), array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < array->length(); i++) {\n+      result_array->obj_at_put(i, array->at(i)());\n+    }\n+    return result_array;\n+  }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return (jobjectArray)JNIHandles::make_local(THREAD, create_results(THREAD));\n+  }\n+\n+  void add_oop(oop o) {\n+    \/\/ Value might be oop, but JLS can't see as Object, just iterate through it...\n+    if (o != NULL && o->is_inline_type()) {\n+      o->oop_iterate(this);\n+    } else {\n+      array->append(Handle(Thread::current(), o));\n+    }\n+  }\n+\n+  void do_oop(oop* o) { add_oop(*o); }\n+  void do_oop(narrowOop* v) { add_oop(CompressedOops::decode(*v)); }\n+};\n+\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+\n+  JNIHandles::resolve(thing)->oop_iterate(&collectOops);\n+\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  StackFrameStream sfs(thread);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, NULL, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\n@@ -2488,0 +2582,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":101,"deletions":1,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -2167,0 +2167,10 @@\n+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypePassFieldsAsArgs)) {\n+    FLAG_SET_CMDLINE(InlineTypePassFieldsAsArgs, false);\n+    warning(\"InlineTypePassFieldsAsArgs is not supported on this platform\");\n+  }\n+\n+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypeReturnedAsFields)) {\n+    FLAG_SET_CMDLINE(InlineTypeReturnedAsFields, false);\n+    warning(\"InlineTypeReturnedAsFields is not supported on this platform\");\n+  }\n+\n@@ -3089,0 +3099,18 @@\n+  if (EnableValhalla) {\n+    \/\/ create_property(\"valhalla.enableValhalla\", \"true\", InternalProperty)\n+    const char* prop_name = \"valhalla.enableValhalla\";\n+    const char* prop_value = \"true\";\n+    const size_t prop_len = strlen(prop_name) + strlen(prop_value) + 2;\n+    char* property = AllocateHeap(prop_len, mtArguments);\n+    int ret = jio_snprintf(property, prop_len, \"%s=%s\", prop_name, prop_value);\n+    if (ret < 0 || ret >= (int)prop_len) {\n+      FreeHeap(property);\n+      return JNI_ENOMEM;\n+    }\n+    bool added = add_property(property, UnwriteableProperty, InternalProperty);\n+    FreeHeap(property);\n+    if (!added) {\n+      return JNI_ENOMEM;\n+    }\n+  }\n+\n@@ -4175,0 +4203,5 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !is_dumping_archive())) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported\n+    InlineTypePassFieldsAsArgs = false;\n+    InlineTypeReturnedAsFields = false;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":33,"deletions":0,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -784,0 +784,18 @@\n+  notproduct(bool, PrintInlineLayout, false,                                \\\n+          \"Print field layout for each inline type\")                        \\\n+                                                                            \\\n+  notproduct(bool, PrintFlatArrayLayout, false,                             \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxSize, -1,                                \\\n+          \"Max size for flattening inline array elements, <0 no limit\")     \\\n+                                                                            \\\n+  product(intx, InlineFieldMaxFlatSize, 128,                                \\\n+          \"Max size for flattening inline type fields, <0 no limit\")        \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  product(bool, InlineArrayAtomicAccess, false,                             \\\n+          \"Atomic inline array accesses by-default, for all inline arrays\") \\\n+                                                                            \\\n@@ -799,1 +817,1 @@\n-  product(bool, UseBiasedLocking, false,                                    \\\n+  product(bool, UseBiasedLocking, true,                                     \\\n@@ -2479,0 +2497,20 @@\n+  product(bool, EnableValhalla, true,                                       \\\n+          \"Enable experimental Valhalla features\")                          \\\n+                                                                            \\\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressInlineTypeReturnedAsFields, false,                    \\\n+          \"Stress return of fields instead of an inline type reference\")    \\\n+                                                                            \\\n+  develop(bool, ScalarizeInlineTypes, true,                                 \\\n+          \"Scalarize inline types in compiled code\")                        \\\n+                                                                            \\\n+  product(ccstrlist, ForceNonTearable, \"\", DIAGNOSTIC,                      \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n@@ -2487,0 +2525,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":40,"deletions":1,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -1675,0 +1676,1 @@\n+  set_return_buffered_value(NULL);\n@@ -2870,0 +2872,3 @@\n+  \/\/ Because this method is used to verify oops, it must support\n+  \/\/ oops in buffered values\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -446,0 +446,1 @@\n+ public:\n@@ -1017,0 +1018,1 @@\n+  friend class VTBuffer;\n@@ -1075,0 +1077,1 @@\n+  oop           _return_buffered_value; \/\/ buffered value being returned\n@@ -1543,0 +1546,3 @@\n+  oop return_buffered_value() const              { return _return_buffered_value; }\n+  void set_return_buffered_value(oop val)        { _return_buffered_value = val; }\n+\n@@ -1782,0 +1788,1 @@\n+  static ByteSize return_buffered_value_offset() { return byte_offset_of(JavaThread, _return_buffered_value); }\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"oops\/array.hpp\"\n+#include \"oops\/oop.hpp\"\n@@ -416,0 +418,6 @@\n+  void appendAll(const Array<E>* l) {\n+    for (int i = 0; i < l->length(); i++) {\n+      this->at_put_grow(this->_len, l->at(i), E());\n+    }\n+  }\n+\n@@ -756,2 +764,2 @@\n-  GrowableArrayFilterIterator(const GrowableArrayIterator<E>& begin, UnaryPredicate filter_predicate) :\n-      _array(begin._array), _position(begin._position), _predicate(filter_predicate) {\n+  GrowableArrayFilterIterator(const GrowableArray<E>* array, UnaryPredicate filter_predicate) :\n+      _array(array), _position(0), _predicate(filter_predicate) {\n@@ -759,1 +767,1 @@\n-    while(_position != _array->length() && !_predicate(_array->at(_position))) {\n+    while(!at_end() && !_predicate(_array->at(_position))) {\n@@ -768,1 +776,1 @@\n-    } while(_position != _array->length() && !_predicate(_array->at(_position)));\n+    } while(!at_end() && !_predicate(_array->at(_position)));\n@@ -793,0 +801,4 @@\n+\n+  bool at_end() const {\n+    return _array == NULL || _position == _array->end()._position;\n+  }\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -200,3 +200,4 @@\n-    private static final int ANNOTATION= 0x00002000;\n-    private static final int ENUM      = 0x00004000;\n-    private static final int SYNTHETIC = 0x00001000;\n+    private static final int ANNOTATION = 0x00002000;\n+    private static final int ENUM       = 0x00004000;\n+    private static final int SYNTHETIC  = 0x00001000;\n+    private static final int INLINE     = 0x00000100;\n@@ -236,2 +237,3 @@\n-        return (isInterface() ? \"interface \" : (isPrimitive() ? \"\" : \"class \"))\n-            + getName();\n+        return (isInlineClass() ? \"inline \" : \"\")\n+               + (isInterface() ? \"interface \" : (isPrimitive() ? \"\" : \"class \"))\n+               + getName();\n@@ -299,0 +301,4 @@\n+                if (isInlineClass()) {\n+                    sb.append(\"inline\");\n+                    sb.append(' ');\n+                }\n@@ -473,2 +479,2 @@\n-                                            ClassLoader loader,\n-                                            Class<?> caller)\n+                                    ClassLoader loader,\n+                                    Class<?> caller)\n@@ -552,0 +558,178 @@\n+    \/**\n+     * Returns {@code true} if this class is an inline class.\n+     *\n+     * @return {@code true} if this class is an inline class\n+     * @since Valhalla\n+     *\/\n+    public boolean isInlineClass() {\n+        return (this.getModifiers() & INLINE) != 0;\n+    }\n+\n+    \/**\n+     * Returns a {@code Class} object representing the <em>value projection<\/em>\n+     * type of this class if this {@code Class} represents the reference projection\n+     * type of an {@linkplain #isInlineClass() inline class}.\n+     * If this {@code Class} represents the value projection type\n+     * of an inline class, then this method returns this class.\n+     * Otherwise an empty {@link Optional} is returned.\n+     *\n+     * @return the {@code Class} object representing the value projection type of\n+     *         this class if this class is the value projection type\n+     *         or the reference projection type of an inline class;\n+     *         an empty {@link Optional} otherwise\n+     * @since Valhalla\n+     *\/\n+    public Optional<Class<?>> valueType() {\n+        if (isPrimitive() || isInterface() || isArray())\n+            return Optional.empty();\n+\n+        Class<?>[] valRefTypes = getProjectionTypes();\n+        return valRefTypes.length > 0 ? Optional.of(valRefTypes[0]) : Optional.empty();\n+    }\n+\n+    \/**\n+     * Returns a {@code Class} object representing the reference type\n+     * of this class.\n+     * <p>\n+     * If this {@code Class} represents an {@linkplain #isInlineClass()\n+     * inline class} with a reference projection type, then this method\n+     * returns the <em>reference projection<\/em> type of this inline class.\n+     * <p>\n+     * If this {@code Class} represents the reference projection type\n+     * of an inline class, then this method returns this class.\n+     * <p>\n+     * If this class is an {@linkplain #isInlineClass() inline class}\n+     * without a reference projection, then this method returns an empty\n+     * {@code Optional}.\n+     * <p>\n+     * If this class is an identity class, then this method returns this\n+     * class.\n+     * <p>\n+     * Otherwise this method returns an empty {@code Optional}.\n+     *\n+     * @return the {@code Class} object representing a reference type for\n+     *         this class if present; an empty {@link Optional} otherwise.\n+     * @since Valhalla\n+     *\/\n+    public Optional<Class<?>> referenceType() {\n+        if (isPrimitive()) return Optional.empty();\n+        if (isInterface() || isArray()) return Optional.of(this);\n+\n+        Class<?>[] valRefTypes = getProjectionTypes();\n+        return valRefTypes.length == 2 ? Optional.of(valRefTypes[1]) : Optional.empty();\n+    }\n+\n+    \/*\n+     * Returns true if this Class object represents a reference projection\n+     * type for an inline class.\n+     *\n+     * A reference projection type must be a sealed abstract class that\n+     * permits the inline projection type to extend.  The inline projection\n+     * type and reference projection type for an inline type must be of\n+     * the same package.\n+     *\/\n+    private boolean isReferenceProjectionType() {\n+        if (isPrimitive() || isArray() || isInterface() || isInlineClass())\n+            return false;\n+\n+        int mods = getModifiers();\n+        if (!Modifier.isAbstract(mods)) {\n+            return false;\n+        }\n+\n+        Class<?>[] valRefTypes = getProjectionTypes();\n+        return valRefTypes.length == 2 && valRefTypes[1] == this;\n+    }\n+\n+    private transient Class<?>[] projectionTypes;\n+    private Class<?>[] getProjectionTypes() {\n+        if (isPrimitive() || isArray() || isInterface())\n+            return null;\n+\n+        Class<?>[] valRefTypes = projectionTypes;\n+        if (valRefTypes == null) {\n+            \/\/ C.ensureProjectionTypesInited calls initProjectionTypes that may\n+            \/\/ call D.ensureProjectionTypesInited where D is its superclass.\n+            \/\/ So initProjectionTypes is called without holding any lock to\n+            \/\/ avoid potential deadlock when multiple threads attempt to\n+            \/\/ initialize the projection types for C and E where D is\n+            \/\/ the superclass of both C and E (which is an error case)\n+            valRefTypes = newProjectionTypeArray();\n+        }\n+        synchronized (this) {\n+            \/\/ set the projection types if not set\n+            if (projectionTypes == null) {\n+                projectionTypes = valRefTypes;\n+            }\n+        }\n+        return projectionTypes;\n+    }\n+\n+    \/*\n+     * Returns an array of Class object whose element at index 0 represents the\n+     * value projection type and element at index 1 represents the reference\n+     * projection type if present.\n+     *\n+     * If this Class object is neither a value projection type nor\n+     * a reference projection type for an inline class, then an empty array\n+     * is returned.\n+     *\/\n+    private Class<?>[] newProjectionTypeArray() {\n+        if (isPrimitive() || isArray() || isInterface())\n+            return null;\n+\n+        if (isInlineClass()) {\n+            Class<?> superClass = getSuperclass();\n+            if (superClass != Object.class && superClass.isReferenceProjectionType()) {\n+                return new Class<?>[] { this, superClass };\n+            } else {\n+                return new Class<?>[] { this };\n+            }\n+        } else {\n+            Class<?> valType = valueProjectionType();\n+            if (valType != null) {\n+                return new Class<?>[] { valType, this};\n+            } else {\n+                return EMPTY_CLASS_ARRAY;\n+            }\n+        }\n+    }\n+\n+    \/*\n+     * Returns the value projection type if this Class represents\n+     * a reference projection type.  If this class is an inline class\n+     * then this method returns this class.  Otherwise, returns null.\n+     *\/\n+    private Class<?> valueProjectionType() {\n+        if (isPrimitive() || isArray() || isInterface())\n+            return null;\n+\n+        if (isInlineClass())\n+            return this;\n+\n+        int mods = getModifiers();\n+        if (!Modifier.isAbstract(mods)) {\n+            return null;\n+        }\n+\n+        \/\/ A reference projection type must be a sealed abstract class\n+        \/\/ that permits the inline projection type to extend.\n+        \/\/ The inline projection type and reference projection type for\n+        \/\/ an inline type must be of the same package.\n+        String[] subclassNames = getPermittedSubclasses0();\n+        if (subclassNames.length == 1) {\n+            String cn = subclassNames[0].replace('\/', '.');\n+            int index = cn.lastIndexOf('.');\n+            String pn = index > 0 ? cn.substring(0, index) : \"\";\n+            if (pn.equals(getPackageName())) {\n+                try {\n+                    Class<?> valType = Class.forName(cn, false, getClassLoader());\n+                    if (valType.isInlineClass()) {\n+                        return valType;\n+                    }\n+                } catch (ClassNotFoundException e) {}\n+            }\n+        }\n+        return null;\n+    }\n+\n@@ -831,0 +1015,2 @@\n+     * <tr><th scope=\"row\"> {@linkplain #isInlineClass() inline class} with <a href=\"ClassLoader.html#binary-name\">binary name<\/a> <i>N<\/i>\n+     *                                      <td style=\"text-align:center\"> {@code Q}<em>N<\/em>{@code ;}\n@@ -849,0 +1035,2 @@\n+     * Point.class.getName()\n+     *     returns \"Point\"\n@@ -851,0 +1039,4 @@\n+     * (new Point[3]).getClass().getName()\n+     *     returns \"[QPoint;\"\n+     * (new Point.ref[3][4]).getClass().getName()\n+     *     returns \"[[LPoint$ref;\"\n@@ -1281,1 +1473,0 @@\n-\n@@ -1292,1 +1483,0 @@\n-\n@@ -1673,1 +1863,1 @@\n-                return cl.getName() + \"[]\".repeat(dimensions);\n+                return cl.getTypeName() + \"[]\".repeat(dimensions);\n@@ -3808,1 +3998,3 @@\n-     * null and is not assignable to the type T.\n+     * {@code null} and is not assignable to the type T.\n+     * @throws NullPointerException if this is an {@linkplain #isInlineClass()\n+     * inline type} and the object is {@code null}\n@@ -3815,0 +4007,3 @@\n+        if (isInlineClass() && obj == null)\n+            throw new NullPointerException(getName() + \" is an inline class\");\n+\n@@ -4110,1 +4305,1 @@\n-         return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n+        return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n@@ -4323,1 +4518,3 @@\n-        } else if (isHidden()) {\n+        }\n+        String typeDesc = isInlineClass() ? \"Q\" : \"L\";\n+        if (isHidden()) {\n@@ -4326,1 +4523,1 @@\n-            return \"L\" + name.substring(0, index).replace('.', '\/')\n+            return typeDesc + name.substring(0, index).replace('.', '\/')\n@@ -4329,1 +4526,1 @@\n-            return \"L\" + getName().replace('.', '\/') + \";\";\n+            return typeDesc + getName().replace('.', '\/') + \";\";\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Class.java","additions":212,"deletions":15,"binary":false,"changes":227,"status":"modified"},{"patch":"@@ -29,0 +29,4 @@\n+import jdk.internal.access.SharedSecrets;\n+\n+import java.lang.invoke.ValueBootstrapMethods;\n+import java.util.Objects;\n@@ -43,0 +47,3 @@\n+     *\n+     * @apiNote {@link Objects#newIdentity java.util.Objects.newIdentity()}\n+     * should be used instead of {@code new Object()}.\n@@ -225,5 +232,5 @@\n-     * The {@code toString} method for class {@code Object}\n-     * returns a string consisting of the name of the class of which the\n-     * object is an instance, the at-sign character `{@code @}', and\n-     * the unsigned hexadecimal representation of the hash code of the\n-     * object. In other words, this method returns a string equal to the\n+     * If this object is an instance of an identity class, then\n+     * the {@code toString} method returns a string consisting of the name\n+     * of the class of which the object is an instance, the at-sign character\n+     * `{@code @}', and the unsigned hexadecimal representation of the hash code\n+     * of the object. In other words, this method returns a string equal to the\n@@ -235,0 +242,6 @@\n+     * <p>\n+     * If this object is an instance of an inline class, then\n+     * the {@code toString} method returns a string which contains\n+     * the name of the inline class, and string representations of\n+     * all its fields.  The precise format produced by this method\n+     * is unspecified and subject to change.\n@@ -239,1 +252,5 @@\n-        return getClass().getName() + \"@\" + Integer.toHexString(hashCode());\n+        if (getClass().isInlineClass()) {\n+            return SharedSecrets.getJavaLangInvokeAccess().inlineObjectToString(this);\n+        } else {\n+            return getClass().getName() + \"@\" + Integer.toHexString(hashCode());\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Object.java","additions":23,"deletions":6,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -2276,1 +2276,0 @@\n-\n","filename":"src\/java.base\/share\/classes\/java\/lang\/System.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1857,0 +1857,5 @@\n+\n+            @Override\n+            public String inlineObjectToString(Object o) {\n+                return ValueBootstrapMethods.inlineObjectToString(o);\n+            }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandleImpl.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -399,0 +399,4 @@\n+        if (referent != null && referent.getClass().isInlineClass()) {\n+            throw new IllegalArgumentException(\"cannot reference an inline value of type: \" +\n+                    referent.getClass().getName());\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/Reference.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import java.util.Objects;\n@@ -145,2 +146,12 @@\n-    public static native Object get(Object array, int index)\n-        throws IllegalArgumentException, ArrayIndexOutOfBoundsException;\n+    public static Object get(Object array, int index)\n+        throws IllegalArgumentException, ArrayIndexOutOfBoundsException {\n+        Class<?> componentType = array.getClass().getComponentType();\n+        if (componentType != null && !componentType.isPrimitive()) {\n+            Object[] objArray = (Object[]) array.getClass().cast(array);\n+            return objArray[index];\n+        } else {\n+            return getReferenceOrPrimitive(array, index);\n+        }\n+    }\n+\n+    private static native Object getReferenceOrPrimitive(Object array, int index);\n@@ -317,2 +328,12 @@\n-    public static native void set(Object array, int index, Object value)\n-        throws IllegalArgumentException, ArrayIndexOutOfBoundsException;\n+    public static void set(Object array, int index, Object value)\n+        throws IllegalArgumentException, ArrayIndexOutOfBoundsException {\n+        Class<?> componentType = array.getClass().getComponentType();\n+        if (componentType != null && !componentType.isPrimitive()) {\n+            Object[] objArray = (Object[]) array.getClass().cast(array);\n+            objArray[index] = componentType.cast(value);\n+        } else {\n+            setReferenceOrPrimitive(array, index, value);\n+        }\n+    }\n+\n+    private static native void setReferenceOrPrimitive(Object array, int index, Object value);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Array.java","additions":25,"deletions":4,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -217,0 +217,1 @@\n+     *\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Method.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -180,0 +180,14 @@\n+    private static final int JVM_ACC_FIELD_INLINED = 0x00008000; \/\/ HotSpot-specific bit\n+\n+    \/**\n+     * Returns true if the given field is flattened.\n+     *\/\n+    public boolean isFlattened(Field f) {\n+        return (f.getModifiers() & JVM_ACC_FIELD_INLINED) == JVM_ACC_FIELD_INLINED;\n+    }\n+\n+    \/**\n+     * Returns true if the given class is a flattened array.\n+     *\/\n+    public native boolean isFlattenedArray(Class<?> arrayClass);\n+\n@@ -182,0 +196,3 @@\n+     * This method can return a reference to either an object or value\n+     * or a null reference.\n+     *\n@@ -189,0 +206,2 @@\n+     * This method can store a reference to either an object or value\n+     * or a null reference.\n@@ -200,0 +219,101 @@\n+    \/**\n+     * Fetches a value of type {@code <V>} from a given Java variable.\n+     * More specifically, fetches a field or array element within the given\n+     * {@code o} object at the given offset, or (if {@code o} is null)\n+     * from the memory address whose numerical value is the given offset.\n+     *\n+     * @param o Java heap object in which the variable resides, if any, else\n+     *        null\n+     * @param offset indication of where the variable resides in a Java heap\n+     *        object, if any, else a memory address locating the variable\n+     *        statically\n+     * @param vc inline class\n+     * @param <V> the type of a value\n+     * @return the value fetched from the indicated Java variable\n+     * @throws RuntimeException No defined exceptions are thrown, not even\n+     *         {@link NullPointerException}\n+     *\/\n+    @IntrinsicCandidate\n+    public native <V> V getValue(Object o, long offset, Class<?> vc);\n+\n+    \/**\n+     * Stores the given value into a given Java variable.\n+     *\n+     * Unless the reference {@code o} being stored is either null\n+     * or matches the field type, the results are undefined.\n+     *\n+     * @param o Java heap object in which the variable resides, if any, else\n+     *        null\n+     * @param offset indication of where the variable resides in a Java heap\n+     *        object, if any, else a memory address locating the variable\n+     *        statically\n+     * @param vc inline class\n+     * @param v the value to store into the indicated Java variable\n+     * @param <V> the type of a value\n+     * @throws RuntimeException No defined exceptions are thrown, not even\n+     *         {@link NullPointerException}\n+     *\/\n+    @IntrinsicCandidate\n+    public native <V> void putValue(Object o, long offset, Class<?> vc, V v);\n+\n+    \/**\n+     * Fetches a reference value of type {@code vc} from a given Java variable.\n+     * This method can return a reference to a value or a null reference\n+     * for a nullable-projection of an inline type.\n+     *\n+     * @param vc inline class\n+     *\/\n+    public Object getReference(Object o, long offset, Class<?> vc) {\n+        Object ref = getReference(o, offset);\n+        if (ref == null && vc.isInlineClass()) {\n+            \/\/ If the type of the returned reference is a regular inline type\n+            \/\/ return an uninitialized default value if null\n+            ref = uninitializedDefaultValue(vc);\n+        }\n+        return ref;\n+    }\n+\n+    public Object getReferenceVolatile(Object o, long offset, Class<?> vc) {\n+        Object ref = getReferenceVolatile(o, offset);\n+        if (ref == null && vc.isInlineClass()) {\n+            \/\/ If the type of the returned reference is a regular inline type\n+            \/\/ return an uninitialized default value if null\n+            ref = uninitializedDefaultValue(vc);\n+        }\n+        return ref;\n+    }\n+\n+    \/**\n+     * Returns an uninitialized default value of the given inline class.\n+     *\/\n+    public native <V> V uninitializedDefaultValue(Class<?> vc);\n+\n+    \/**\n+     * Returns an object instance with a private buffered value whose layout\n+     * and contents is exactly the given value instance.  The return object\n+     * is in the larval state that can be updated using the unsafe put operation.\n+     *\n+     * @param value a value instance\n+     * @param <V> the type of the given value instance\n+     *\/\n+    @IntrinsicCandidate\n+    public native <V> V makePrivateBuffer(V value);\n+\n+    \/**\n+     * Exits the larval state and returns a value instance.\n+     *\n+     * @param value a value instance\n+     * @param <V> the type of the given value instance\n+     *\/\n+    @IntrinsicCandidate\n+    public native <V> V finishPrivateBuffer(V value);\n+\n+    \/**\n+     * Returns the header size of the given inline class\n+     *\n+     * @param vc inline class\n+     * @param <V> value clas\n+     * @return the header size of the inline class\n+     *\/\n+    public native <V> long valueHeaderSize(Class<V> vc);\n+\n@@ -1232,0 +1352,11 @@\n+    \/**\n+     * Return the size of the object in the heap.\n+     * @param o an object\n+     * @return the objects's size\n+     * @since Valhalla\n+     *\/\n+    public long getObjectSize(Object o) {\n+        if (o == null)\n+            throw new NullPointerException();\n+        return getObjectSize0(o);\n+    }\n@@ -1437,0 +1568,47 @@\n+    private final boolean isInlineType(Object o) {\n+        return o != null && o.getClass().isInlineClass();\n+    }\n+\n+    \/*\n+     * For inline type, CAS should do substitutability test as opposed\n+     * to two pointers comparison.\n+     *\n+     * Perhaps we can keep the xxxObject methods for compatibility and\n+     * change the JDK 13 xxxReference method signature freely.\n+     *\/\n+    public final <V> boolean compareAndSetReference(Object o, long offset,\n+                                                    Class<?> valueType,\n+                                                    V expected,\n+                                                    V x) {\n+        if (valueType.isInlineClass() || isInlineType(expected)) {\n+            synchronized (valueLock) {\n+                Object witness = getReference(o, offset);\n+                if (witness == expected) {\n+                    putReference(o, offset, x);\n+                    return true;\n+                } else {\n+                    return false;\n+                }\n+            }\n+        } else {\n+            return compareAndSetReference(o, offset, expected, x);\n+        }\n+    }\n+\n+    @ForceInline\n+    public final <V> boolean compareAndSetValue(Object o, long offset,\n+                                                Class<?> valueType,\n+                                                V expected,\n+                                                V x) {\n+        synchronized (valueLock) {\n+            Object witness = getValue(o, offset, valueType);\n+            if (witness == expected) {\n+                putValue(o, offset, valueType, x);\n+                return true;\n+            }\n+            else {\n+                return false;\n+            }\n+        }\n+    }\n+\n@@ -1442,0 +1620,31 @@\n+    public final <V> Object compareAndExchangeReference(Object o, long offset,\n+                                                        Class<?> valueType,\n+                                                        V expected,\n+                                                        V x) {\n+        if (valueType.isInlineClass() || isInlineType(expected)) {\n+            synchronized (valueLock) {\n+                Object witness = getReference(o, offset);\n+                if (witness == expected) {\n+                    putReference(o, offset, x);\n+                }\n+                return witness;\n+            }\n+        } else {\n+            return compareAndExchangeReference(o, offset, expected, x);\n+        }\n+    }\n+\n+    @ForceInline\n+    public final <V> Object compareAndExchangeValue(Object o, long offset,\n+                                                    Class<?> valueType,\n+                                                    V expected,\n+                                                    V x) {\n+        synchronized (valueLock) {\n+            Object witness = getValue(o, offset, valueType);\n+            if (witness == expected) {\n+                putValue(o, offset, valueType, x);\n+            }\n+            return witness;\n+        }\n+    }\n+\n@@ -1449,0 +1658,15 @@\n+    public final <V> Object compareAndExchangeReferenceAcquire(Object o, long offset,\n+                                                               Class<?> valueType,\n+                                                               V expected,\n+                                                               V x) {\n+        return compareAndExchangeReference(o, offset, valueType, expected, x);\n+    }\n+\n+    @ForceInline\n+    public final <V> Object compareAndExchangeValueAcquire(Object o, long offset,\n+                                                           Class<?> valueType,\n+                                                           V expected,\n+                                                           V x) {\n+        return compareAndExchangeValue(o, offset, valueType, expected, x);\n+    }\n+\n@@ -1456,0 +1680,15 @@\n+    public final <V> Object compareAndExchangeReferenceRelease(Object o, long offset,\n+                                                               Class<?> valueType,\n+                                                               V expected,\n+                                                               V x) {\n+        return compareAndExchangeReference(o, offset, valueType, expected, x);\n+    }\n+\n+    @ForceInline\n+    public final <V> Object compareAndExchangeValueRelease(Object o, long offset,\n+                                                           Class<?> valueType,\n+                                                           V expected,\n+                                                           V x) {\n+        return compareAndExchangeValue(o, offset, valueType, expected, x);\n+    }\n+\n@@ -1463,0 +1702,19 @@\n+    public final <V> boolean weakCompareAndSetReferencePlain(Object o, long offset,\n+                                                             Class<?> valueType,\n+                                                             V expected,\n+                                                             V x) {\n+        if (valueType.isInlineClass() || isInlineType(expected)) {\n+            return compareAndSetReference(o, offset, valueType, expected, x);\n+        } else {\n+            return weakCompareAndSetReferencePlain(o, offset, expected, x);\n+        }\n+    }\n+\n+    @ForceInline\n+    public final <V> boolean weakCompareAndSetValuePlain(Object o, long offset,\n+                                                         Class<?> valueType,\n+                                                         V expected,\n+                                                         V x) {\n+        return compareAndSetValue(o, offset, valueType, expected, x);\n+    }\n+\n@@ -1470,0 +1728,19 @@\n+    public final <V> boolean weakCompareAndSetReferenceAcquire(Object o, long offset,\n+                                                               Class<?> valueType,\n+                                                               V expected,\n+                                                               V x) {\n+        if (valueType.isInlineClass() || isInlineType(expected)) {\n+            return compareAndSetReference(o, offset, valueType, expected, x);\n+        } else {\n+            return weakCompareAndSetReferencePlain(o, offset, expected, x);\n+        }\n+    }\n+\n+    @ForceInline\n+    public final <V> boolean weakCompareAndSetValueAcquire(Object o, long offset,\n+                                                           Class<?> valueType,\n+                                                           V expected,\n+                                                           V x) {\n+        return compareAndSetValue(o, offset, valueType, expected, x);\n+    }\n+\n@@ -1477,0 +1754,19 @@\n+    public final <V> boolean weakCompareAndSetReferenceRelease(Object o, long offset,\n+                                                               Class<?> valueType,\n+                                                               V expected,\n+                                                               V x) {\n+        if (valueType.isInlineClass() || isInlineType(expected)) {\n+            return compareAndSetReference(o, offset, valueType, expected, x);\n+        } else {\n+            return weakCompareAndSetReferencePlain(o, offset, expected, x);\n+        }\n+    }\n+\n+    @ForceInline\n+    public final <V> boolean weakCompareAndSetValueRelease(Object o, long offset,\n+                                                           Class<?> valueType,\n+                                                           V expected,\n+                                                           V x) {\n+        return compareAndSetValue(o, offset, valueType, expected, x);\n+    }\n+\n@@ -1484,0 +1780,19 @@\n+    public final <V> boolean weakCompareAndSetReference(Object o, long offset,\n+                                                        Class<?> valueType,\n+                                                        V expected,\n+                                                        V x) {\n+        if (valueType.isInlineClass() || isInlineType(expected)) {\n+            return compareAndSetReference(o, offset, valueType, expected, x);\n+        } else {\n+            return weakCompareAndSetReferencePlain(o, offset, expected, x);\n+        }\n+    }\n+\n+    @ForceInline\n+    public final <V> boolean weakCompareAndSetValue(Object o, long offset,\n+                                                    Class<?> valueType,\n+                                                    V expected,\n+                                                    V x) {\n+        return compareAndSetValue(o, offset, valueType, expected, x);\n+    }\n+\n@@ -2099,0 +2414,13 @@\n+    \/**\n+     * Global lock for atomic and volatile strength access to any value of\n+     * an inline type.  This is a temporary workaround until better localized\n+     * atomic access mechanisms are supported for inline types.\n+     *\/\n+    private static final Object valueLock = new Object();\n+\n+    public final <V> Object getValueVolatile(Object base, long offset, Class<?> valueType) {\n+        synchronized (valueLock) {\n+            return getValue(base, offset, valueType);\n+        }\n+    }\n+\n@@ -2106,0 +2434,6 @@\n+    public final <V> void putValueVolatile(Object o, long offset, Class<?> valueType, V x) {\n+        synchronized (valueLock) {\n+            putValue(o, offset, valueType, x);\n+        }\n+    }\n+\n@@ -2178,0 +2512,4 @@\n+    public final <V> Object getValueAcquire(Object base, long offset, Class<?> valueType) {\n+        return getValueVolatile(base, offset, valueType);\n+    }\n+\n@@ -2242,0 +2580,4 @@\n+    public final <V> void putValueRelease(Object o, long offset, Class<?> valueType, V x) {\n+        putValueVolatile(o, offset, valueType, x);\n+    }\n+\n@@ -2298,0 +2640,4 @@\n+    public final <V> Object getValueOpaque(Object base, long offset, Class<?> valueType) {\n+        return getValueVolatile(base, offset, valueType);\n+    }\n+\n@@ -2352,0 +2698,4 @@\n+    public final <V> void putValueOpaque(Object o, long offset, Class<?> valueType, V x) {\n+        putValueVolatile(o, offset, valueType, x);\n+    }\n+\n@@ -2786,0 +3136,9 @@\n+    @SuppressWarnings(\"unchecked\")\n+    public final <V> Object getAndSetValue(Object o, long offset, Class<?> valueType, V newValue) {\n+        synchronized (valueLock) {\n+            Object oldValue = getValue(o, offset, valueType);\n+            putValue(o, offset, valueType, newValue);\n+            return oldValue;\n+        }\n+    }\n+\n@@ -2795,0 +3154,5 @@\n+    @ForceInline\n+    public final <V> Object getAndSetValueRelease(Object o, long offset, Class<?> valueType, V newValue) {\n+        return getAndSetValue(o, offset, valueType, newValue);\n+    }\n+\n@@ -2804,0 +3168,5 @@\n+    @ForceInline\n+    public final <V> Object getAndSetValueAcquire(Object o, long offset, Class<?> valueType, V newValue) {\n+        return getAndSetValue(o, offset, valueType, newValue);\n+    }\n+\n@@ -3857,0 +4226,1 @@\n+    private native long getObjectSize0(Object o);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/misc\/Unsafe.java","additions":370,"deletions":0,"binary":false,"changes":370,"status":"modified"},{"patch":"@@ -132,1 +132,0 @@\n-\n@@ -346,1 +345,0 @@\n-\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -415,37 +415,58 @@\n-        switch ((short)(sym.flags() & AccessFlags)) {\n-        case PRIVATE:\n-            return\n-                (env.enclClass.sym == sym.owner \/\/ fast special case\n-                 ||\n-                 env.enclClass.sym.outermostClass() ==\n-                 sym.owner.outermostClass())\n-                &&\n-                sym.isInheritedIn(site.tsym, types);\n-        case 0:\n-            return\n-                (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n-                 ||\n-                 env.toplevel.packge == sym.packge())\n-                &&\n-                isAccessible(env, site, checkInner)\n-                &&\n-                sym.isInheritedIn(site.tsym, types)\n-                &&\n-                notOverriddenIn(site, sym);\n-        case PROTECTED:\n-            return\n-                (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n-                 ||\n-                 env.toplevel.packge == sym.packge()\n-                 ||\n-                 isProtectedAccessible(sym, env.enclClass.sym, site)\n-                 ||\n-                 \/\/ OK to select instance method or field from 'super' or type name\n-                 \/\/ (but type names should be disallowed elsewhere!)\n-                 env.info.selectSuper && (sym.flags() & STATIC) == 0 && sym.kind != TYP)\n-                &&\n-                isAccessible(env, site, checkInner)\n-                &&\n-                notOverriddenIn(site, sym);\n-        default: \/\/ this case includes erroneous combinations as well\n-            return isAccessible(env, site, checkInner) && notOverriddenIn(site, sym);\n+        ClassSymbol enclosingCsym = env.enclClass.sym;\n+        if (sym.kind == MTH || sym.kind == VAR) {\n+            \/* If any inline types are involved, ask the same question in the reference universe,\n+               where the hierarchy is navigable\n+            *\/\n+            if (site.isValue())\n+                site = site.referenceProjection();\n+            if (sym.owner.isValue())\n+                sym = sym.referenceProjection();\n+            if (env.enclClass.sym.isValue())\n+                env.enclClass.sym = env.enclClass.sym.referenceProjection();\n+        } else if (sym.kind == TYP) {\n+            \/\/ A type is accessible in a reference projection if it was\n+            \/\/ accessible in the value projection.\n+            if (site.isReferenceProjection())\n+                site = site.valueProjection();\n+        }\n+        try {\n+            switch ((short)(sym.flags() & AccessFlags)) {\n+                case PRIVATE:\n+                    return\n+                            (env.enclClass.sym == sym.owner \/\/ fast special case\n+                                    ||\n+                                    env.enclClass.sym.outermostClass() ==\n+                                            sym.owner.outermostClass())\n+                                    &&\n+                                    sym.isInheritedIn(site.tsym, types);\n+                case 0:\n+                    return\n+                            (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n+                                    ||\n+                                    env.toplevel.packge == sym.packge())\n+                                    &&\n+                                    isAccessible(env, site, checkInner)\n+                                    &&\n+                                    sym.isInheritedIn(site.tsym, types)\n+                                    &&\n+                                    notOverriddenIn(site, sym);\n+                case PROTECTED:\n+                    return\n+                            (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n+                                    ||\n+                                    env.toplevel.packge == sym.packge()\n+                                    ||\n+                                    isProtectedAccessible(sym, env.enclClass.sym, site)\n+                                    ||\n+                                    \/\/ OK to select instance method or field from 'super' or type name\n+                                    \/\/ (but type names should be disallowed elsewhere!)\n+                                    env.info.selectSuper && (sym.flags() & STATIC) == 0 && sym.kind != TYP)\n+                                    &&\n+                                    isAccessible(env, site, checkInner)\n+                                    &&\n+                                    notOverriddenIn(site, sym);\n+                default: \/\/ this case includes erroneous combinations as well\n+                    return isAccessible(env, site, checkInner) && notOverriddenIn(site, sym);\n+            }\n+        } finally {\n+            env.enclClass.sym = enclosingCsym;\n@@ -464,5 +485,12 @@\n-        else {\n-            Symbol s2 = ((MethodSymbol)sym).implementation(site.tsym, types, true);\n-            return (s2 == null || s2 == sym || sym.owner == s2.owner ||\n-                    !types.isSubSignature(types.memberType(site, s2), types.memberType(site, sym)));\n-        }\n+\n+        \/* If any inline types are involved, ask the same question in the reference universe,\n+           where the hierarchy is navigable\n+        *\/\n+        if (site.isValue())\n+            site = site.referenceProjection();\n+        if (sym.owner.isValue())\n+            sym = sym.referenceProjection();\n+\n+        Symbol s2 = ((MethodSymbol)sym).implementation(site.tsym, types, true);\n+        return (s2 == null || s2 == sym || sym.owner == s2.owner ||\n+                !types.isSubSignature(types.memberType(site, s2), types.memberType(site, sym)));\n@@ -2188,0 +2216,4 @@\n+        \/\/ ATM, inner\/nested types are members of only the declaring inline class,\n+        \/\/ although accessible via the reference projection.\n+        if (c.isReferenceProjection())\n+            c = (TypeSymbol) c.valueProjection();\n@@ -2245,0 +2277,16 @@\n+        return findMemberTypeInternal(env,site, name, c);\n+    }\n+\n+    \/** Find qualified member type.\n+     *  @param env       The current environment.\n+     *  @param site      The original type from where the selection takes\n+     *                   place.\n+     *  @param name      The type's name.\n+     *  @param c         The class to search for the member type. This is\n+     *                   always a superclass or implemented interface of\n+     *                   site's class.\n+     *\/\n+    Symbol findMemberTypeInternal(Env<AttrContext> env,\n+                          Type site,\n+                          Name name,\n+                          TypeSymbol c) {\n@@ -2306,0 +2354,8 @@\n+        return findTypeInternal(env, name);\n+    }\n+\n+    \/** Find an unqualified type symbol.\n+     *  @param env       The current environment.\n+     *  @param name      The type's name.\n+     *\/\n+    Symbol findTypeInternal(Env<AttrContext> env, Name name) {\n@@ -2941,0 +2997,7 @@\n+                    ClassSymbol refProjection = newConstr.owner.isValue() ?\n+                                                     (ClassSymbol) newConstr.owner.referenceProjection() : null;\n+                    if (refProjection != null) {\n+                        MethodSymbol clone = newConstr.clone(refProjection);\n+                        clone.projection = newConstr;\n+                        newConstr.projection = clone;\n+                    }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Resolve.java","additions":105,"deletions":42,"binary":false,"changes":147,"status":"modified"},{"patch":"@@ -288,1 +288,1 @@\n-                HotSpotResolvedObjectTypeImpl[] types = new HotSpotResolvedObjectTypeImpl[2];\n+                HotSpotResolvedObjectTypeImpl[] types = new HotSpotResolvedObjectTypeImpl[3];\n@@ -291,0 +291,1 @@\n+                types[2] = runtime().getJavaLangIdentityObject();\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotResolvedObjectTypeImpl.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2559,1 +2559,1 @@\n-        return isKind(doctree, VALUE);\n+        return isKind(doctree, Kind.VALUE);\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/toolkit\/util\/Utils.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -282,1 +282,3 @@\n-            assertEquals(expected.length, actual.length);\n+            \/\/ With injection of the IdentityObject interface by the JVM, the number of\n+            \/\/ interfaces visible through reflection and through JVMCI could differ by one\n+            assertTrue(expected.length == actual.length || (actual.length - expected.length) == 1);\n","filename":"test\/hotspot\/jtreg\/compiler\/jvmci\/jdk.vm.ci.runtime.test\/src\/jdk\/vm\/ci\/runtime\/test\/TestResolvedJavaType.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,831 @@\n+\/*\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.valhalla.inlinetypes;\n+\n+import compiler.whitebox.CompilerWhiteBoxTest;\n+import jdk.test.lib.Asserts;\n+import jdk.test.lib.management.InputArguments;\n+import jdk.test.lib.Platform;\n+import jdk.test.lib.process.ProcessTools;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.Utils;\n+import sun.hotspot.WhiteBox;\n+\n+import java.lang.annotation.Retention;\n+import java.lang.annotation.RetentionPolicy;\n+import java.lang.annotation.Repeatable;\n+import java.lang.invoke.*;\n+import java.lang.reflect.Method;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Hashtable;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.stream.Stream;\n+import java.util.TreeMap;\n+import java.util.function.BooleanSupplier;\n+\n+\/\/ Mark method as test\n+@Retention(RetentionPolicy.RUNTIME)\n+@Repeatable(Tests.class)\n+@interface Test {\n+    \/\/ Regular expression used to match forbidden IR nodes\n+    \/\/ in the C2 IR emitted for this test.\n+    String failOn() default \"\";\n+    \/\/ Regular expressions used to match and count IR nodes.\n+    String[] match() default { };\n+    int[] matchCount() default { };\n+    int compLevel() default InlineTypeTest.COMP_LEVEL_ANY;\n+    int valid() default 0;\n+}\n+\n+@Retention(RetentionPolicy.RUNTIME)\n+@interface Tests {\n+    Test[] value();\n+}\n+\n+\/\/ Force method inlining during compilation\n+@Retention(RetentionPolicy.RUNTIME)\n+@interface ForceInline { }\n+\n+\/\/ Prevent method inlining during compilation\n+@Retention(RetentionPolicy.RUNTIME)\n+@interface DontInline { }\n+\n+\/\/ Prevent method compilation\n+@Retention(RetentionPolicy.RUNTIME)\n+@interface DontCompile { }\n+\n+\/\/ Force method compilation\n+@Retention(RetentionPolicy.RUNTIME)\n+@interface ForceCompile {\n+    int compLevel() default InlineTypeTest.COMP_LEVEL_ANY;\n+}\n+\n+\/\/ Number of warmup iterations\n+@Retention(RetentionPolicy.RUNTIME)\n+@interface Warmup {\n+    int value();\n+}\n+\n+\/\/ Do not enqueue the test method for compilation immediately after warmup loops have finished. Instead\n+\/\/ let the test method be compiled with on-stack-replacement.\n+@Retention(RetentionPolicy.RUNTIME)\n+@interface OSRCompileOnly {}\n+\n+\/\/ Skip this test temporarily for C1 testing\n+@Retention(RetentionPolicy.RUNTIME)\n+@interface TempSkipForC1 {\n+    String reason() default \"\";\n+}\n+\n+public abstract class InlineTypeTest {\n+    protected static final WhiteBox WHITE_BOX = WhiteBox.getWhiteBox();\n+\n+    protected static final int COMP_LEVEL_ANY               = -2;\n+    protected static final int COMP_LEVEL_ALL               = -2;\n+    protected static final int COMP_LEVEL_AOT               = -1;\n+    protected static final int COMP_LEVEL_NONE              =  0;\n+    protected static final int COMP_LEVEL_SIMPLE            =  1;     \/\/ C1\n+    protected static final int COMP_LEVEL_LIMITED_PROFILE   =  2;     \/\/ C1, invocation & backedge counters\n+    protected static final int COMP_LEVEL_FULL_PROFILE      =  3;     \/\/ C1, invocation & backedge counters + mdo\n+    protected static final int COMP_LEVEL_FULL_OPTIMIZATION =  4;     \/\/ C2 or JVMCI\n+\n+    protected static final boolean TieredCompilation = (Boolean)WHITE_BOX.getVMFlag(\"TieredCompilation\");\n+    protected static final long TieredStopAtLevel = (Long)WHITE_BOX.getVMFlag(\"TieredStopAtLevel\");\n+    static final boolean TEST_C1 = TieredCompilation && TieredStopAtLevel < COMP_LEVEL_FULL_OPTIMIZATION;\n+\n+    \/\/ Random test values\n+    public static final int  rI = Utils.getRandomInstance().nextInt() % 1000;\n+    public static final long rL = Utils.getRandomInstance().nextLong() % 1000;\n+    public static final double rD = Utils.getRandomInstance().nextDouble() % 1000;\n+\n+    \/\/ User defined settings\n+    protected static final boolean XCOMP = Platform.isComp();\n+    private static final boolean PRINT_GRAPH = true;\n+    private static final boolean VERBOSE = Boolean.parseBoolean(System.getProperty(\"Verbose\", \"false\"));\n+    private static final boolean PRINT_TIMES = Boolean.parseBoolean(System.getProperty(\"PrintTimes\", \"false\"));\n+    private static final boolean COMPILE_COMMANDS = Boolean.parseBoolean(System.getProperty(\"CompileCommands\", \"true\")) && !XCOMP;\n+    private static       boolean VERIFY_IR = Boolean.parseBoolean(System.getProperty(\"VerifyIR\", \"true\")) && !XCOMP && !TEST_C1 && COMPILE_COMMANDS;\n+    private static final boolean VERIFY_VM = Boolean.parseBoolean(System.getProperty(\"VerifyVM\", \"false\"));\n+    private static final String SCENARIOS = System.getProperty(\"Scenarios\", \"\");\n+    private static final String TESTLIST = System.getProperty(\"Testlist\", \"\");\n+    private static final String EXCLUDELIST = System.getProperty(\"Exclude\", \"\");\n+    private static final int WARMUP = Integer.parseInt(System.getProperty(\"Warmup\", \"251\"));\n+    private static final boolean DUMP_REPLAY = Boolean.parseBoolean(System.getProperty(\"DumpReplay\", \"false\"));\n+    private static final boolean FLIP_C1_C2 = Boolean.parseBoolean(System.getProperty(\"FlipC1C2\", \"false\"));\n+    private static final boolean GC_AFTER = Boolean.parseBoolean(System.getProperty(\"GCAfter\", \"false\"));\n+    private static final int OSR_TEST_TIMEOUT = Integer.parseInt(System.getProperty(\"OSRTestTimeOut\", \"5000\"));\n+    protected static final boolean STRESS_CC = Boolean.parseBoolean(System.getProperty(\"StressCC\", \"false\"));\n+    private static final boolean SHUFFLE_TESTS = Boolean.parseBoolean(System.getProperty(\"ShuffleTests\", \"true\"));\n+\n+    \/\/ \"jtreg -DXcomp=true\" runs all the scenarios with -Xcomp. This is faster than \"jtreg -javaoptions:-Xcomp\".\n+    protected static final boolean RUN_SCENARIOS_WITH_XCOMP = Boolean.parseBoolean(System.getProperty(\"Xcomp\", \"false\"));\n+\n+    \/\/ Pre-defined settings\n+    private static final String[] defaultFlags = {\n+        \"-XX:-BackgroundCompilation\"};\n+    private static final String[] compileCommandFlags = {\n+        \"-XX:CompileCommand=quiet\",\n+        \"-XX:CompileCommand=compileonly,java.lang.invoke.*::*\",\n+        \"-XX:CompileCommand=compileonly,java.lang.Long::sum\",\n+        \"-XX:CompileCommand=compileonly,java.lang.Object::<init>\",\n+        \"-XX:CompileCommand=inline,compiler.valhalla.inlinetypes.MyValue*::<init>\",\n+        \"-XX:CompileCommand=compileonly,compiler.valhalla.inlinetypes.*::*\"};\n+    private static final String[] printFlags = {\n+        \"-XX:+PrintCompilation\", \"-XX:+PrintIdeal\", \"-XX:+UnlockDiagnosticVMOptions\", \"-XX:+PrintOptoAssembly\"};\n+    private static final String[] verifyFlags = {\n+        \"-XX:+VerifyOops\", \"-XX:+VerifyStack\", \"-XX:+VerifyLastFrame\", \"-XX:+VerifyBeforeGC\", \"-XX:+VerifyAfterGC\",\n+        \"-XX:+VerifyDuringGC\", \"-XX:+VerifyAdapterSharing\"};\n+\n+    protected static final int InlineTypePassFieldsAsArgsOn = 0x1;\n+    protected static final int InlineTypePassFieldsAsArgsOff = 0x2;\n+    protected static final int InlineTypeArrayFlattenOn = 0x4;\n+    protected static final int InlineTypeArrayFlattenOff = 0x8;\n+    protected static final int InlineTypeReturnedAsFieldsOn = 0x10;\n+    protected static final int InlineTypeReturnedAsFieldsOff = 0x20;\n+    protected static final int AlwaysIncrementalInlineOn = 0x40;\n+    protected static final int AlwaysIncrementalInlineOff = 0x80;\n+    protected static final int G1GCOn = 0x100;\n+    protected static final int G1GCOff = 0x200;\n+    protected static final int ZGCOn = 0x400;\n+    protected static final int ZGCOff = 0x800;\n+    protected static final int ArrayLoadStoreProfileOn = 0x1000;\n+    protected static final int ArrayLoadStoreProfileOff = 0x2000;\n+    protected static final int TypeProfileOn = 0x4000;\n+    protected static final int TypeProfileOff = 0x8000;\n+    protected static final boolean InlineTypePassFieldsAsArgs = (Boolean)WHITE_BOX.getVMFlag(\"InlineTypePassFieldsAsArgs\");\n+    protected static final boolean InlineTypeArrayFlatten = (WHITE_BOX.getIntxVMFlag(\"FlatArrayElementMaxSize\") == -1);\n+    protected static final boolean InlineTypeReturnedAsFields = (Boolean)WHITE_BOX.getVMFlag(\"InlineTypeReturnedAsFields\");\n+    protected static final boolean AlwaysIncrementalInline = (Boolean)WHITE_BOX.getVMFlag(\"AlwaysIncrementalInline\");\n+    protected static final boolean G1GC = (Boolean)WHITE_BOX.getVMFlag(\"UseG1GC\");\n+    protected static final boolean ZGC = (Boolean)WHITE_BOX.getVMFlag(\"UseZGC\");\n+    protected static final boolean VerifyOops = (Boolean)WHITE_BOX.getVMFlag(\"VerifyOops\");\n+    protected static final boolean UseArrayLoadStoreProfile = (Boolean)WHITE_BOX.getVMFlag(\"UseArrayLoadStoreProfile\");\n+    protected static final long TypeProfileLevel = (Long)WHITE_BOX.getVMFlag(\"TypeProfileLevel\");\n+\n+    protected static final Hashtable<String, Method> tests = new Hashtable<String, Method>();\n+    protected static final boolean USE_COMPILER = WHITE_BOX.getBooleanVMFlag(\"UseCompiler\");\n+    protected static final boolean PRINT_IDEAL  = WHITE_BOX.getBooleanVMFlag(\"PrintIdeal\");\n+\n+    \/\/ Regular expressions used to match nodes in the PrintIdeal output\n+    protected static final String START = \"(\\\\d+ (.*\";\n+    protected static final String MID = \".*)+ ===.*\";\n+    protected static final String END = \")|\";\n+    \/\/ Generic allocation\n+    protected static final String ALLOC_G  = \"(.*call,static  wrapper for: _new_instance_Java\" + END;\n+    protected static final String ALLOCA_G = \"(.*call,static  wrapper for: _new_array_Java\" + END;\n+    \/\/ Inline type allocation\n+    protected static final String ALLOC  = \"(.*precise klass compiler\/valhalla\/inlinetypes\/MyValue.*\\\\R(.*(movl|xorl|nop|spill).*\\\\R)*.*_new_instance_Java\" + END;\n+    protected static final String ALLOCA = \"(.*precise klass \\\\[(L|Q)compiler\/valhalla\/inlinetypes\/MyValue.*\\\\R(.*(movl|xorl|nop|spill).*\\\\R)*.*_new_array_Java\" + END;\n+    protected static final String LOAD   = START + \"Load(B|S|I|L|F|D|P|N)\" + MID + \"@compiler\/valhalla\/inlinetypes\/MyValue.*\" + END;\n+    protected static final String LOADK  = START + \"LoadK\" + MID + END;\n+    protected static final String STORE  = START + \"Store(B|C|S|I|L|F|D|P|N)\" + MID + \"@compiler\/valhalla\/inlinetypes\/MyValue.*\" + END;\n+    protected static final String LOOP   = START + \"Loop\" + MID + \"\" + END;\n+    protected static final String COUNTEDLOOP = START + \"CountedLoop\\\\b\" + MID + \"\" + END;\n+    protected static final String COUNTEDLOOP_MAIN = START + \"CountedLoop\\\\b\" + MID + \"main\" + END;\n+    protected static final String TRAP   = START + \"CallStaticJava\" + MID + \"uncommon_trap.*(unstable_if|predicate)\" + END;\n+    protected static final String RETURN = START + \"Return\" + MID + \"returns\" + END;\n+    protected static final String LINKTOSTATIC = START + \"CallStaticJava\" + MID + \"linkToStatic\" + END;\n+    protected static final String NPE = START + \"CallStaticJava\" + MID + \"null_check\" + END;\n+    protected static final String CALL = START + \"CallStaticJava\" + MID + END;\n+    protected static final String STORE_INLINE_FIELDS = START + \"CallStaticJava\" + MID + \"store_inline_type_fields\" + END;\n+    protected static final String SCOBJ = \"(.*# ScObj.*\" + END;\n+    protected static final String LOAD_UNKNOWN_INLINE = \"(.*call_leaf,runtime  load_unknown_inline.*\" + END;\n+    protected static final String STORE_UNKNOWN_INLINE = \"(.*call_leaf,runtime  store_unknown_inline.*\" + END;\n+    protected static final String INLINE_ARRAY_NULL_GUARD = \"(.*call,static  wrapper for: uncommon_trap.*reason='null_check' action='none'.*\" + END;\n+    protected static final String INTRINSIC_SLOW_PATH = \"(.*call,static  wrapper for: uncommon_trap.*reason='intrinsic_or_type_checked_inlining'.*\" + END;\n+    protected static final String CLONE_INTRINSIC_SLOW_PATH = \"(.*call,static.*java.lang.Object::clone.*\" + END;\n+    protected static final String CLASS_CHECK_TRAP = START + \"CallStaticJava\" + MID + \"uncommon_trap.*class_check\" + END;\n+    protected static final String NULL_CHECK_TRAP = START + \"CallStaticJava\" + MID + \"uncommon_trap.*null_check\" + END;\n+    protected static final String RANGE_CHECK_TRAP = START + \"CallStaticJava\" + MID + \"uncommon_trap.*range_check\" + END;\n+    protected static final String UNHANDLED_TRAP = START + \"CallStaticJava\" + MID + \"uncommon_trap.*unhandled\" + END;\n+    protected static final String PREDICATE_TRAP = START + \"CallStaticJava\" + MID + \"uncommon_trap.*predicate\" + END;\n+    protected static final String MEMBAR = START + \"MemBar\" + MID + END;\n+    protected static final String CHECKCAST_ARRAY = \"(cmp.*precise klass \\\\[(L|Q)compiler\/valhalla\/inlinetypes\/MyValue.*\" + END;\n+    protected static final String CHECKCAST_ARRAYCOPY = \"(.*call_leaf_nofp,runtime  checkcast_arraycopy.*\" + END;\n+    protected static final String JLONG_ARRAYCOPY = \"(.*call_leaf_nofp,runtime  jlong_disjoint_arraycopy.*\" + END;\n+    protected static final String FIELD_ACCESS = \"(.*Field: *\" + END;\n+\n+    public static String[] concat(String prefix[], String... extra) {\n+        ArrayList<String> list = new ArrayList<String>();\n+        if (prefix != null) {\n+            for (String s : prefix) {\n+                list.add(s);\n+            }\n+        }\n+        if (extra != null) {\n+            for (String s : extra) {\n+                list.add(s);\n+            }\n+        }\n+\n+        return list.toArray(new String[list.size()]);\n+    }\n+\n+    \/**\n+     * Override getNumScenarios and getVMParameters if you want to run with more than\n+     * the 6 built-in scenarios\n+     *\/\n+    public int getNumScenarios() {\n+        return 6;\n+    }\n+\n+    \/**\n+     * VM parameters for the 5 built-in test scenarios. If your test needs to append\n+     * extra parameters for (some of) these scenarios, override getExtraVMParameters().\n+     *\/\n+    public String[] getVMParameters(int scenario) {\n+        switch (scenario) {\n+        case 0: return new String[] {\n+                \"-XX:+AlwaysIncrementalInline\",\n+                \"-XX:FlatArrayElementMaxOops=5\",\n+                \"-XX:FlatArrayElementMaxSize=-1\",\n+                \"-XX:-UseArrayLoadStoreProfile\",\n+                \"-XX:InlineFieldMaxFlatSize=-1\",\n+                \"-XX:+InlineTypePassFieldsAsArgs\",\n+                \"-XX:+InlineTypeReturnedAsFields\"};\n+        case 1: return new String[] {\n+                \"-XX:-UseCompressedOops\",\n+                \"-XX:FlatArrayElementMaxOops=5\",\n+                \"-XX:FlatArrayElementMaxSize=-1\",\n+                \"-XX:-UseArrayLoadStoreProfile\",\n+                \"-XX:InlineFieldMaxFlatSize=-1\",\n+                \"-XX:-InlineTypePassFieldsAsArgs\",\n+                \"-XX:-InlineTypeReturnedAsFields\"};\n+        case 2: return new String[] {\n+                \"-XX:-UseCompressedOops\",\n+                \"-XX:FlatArrayElementMaxOops=0\",\n+                \"-XX:FlatArrayElementMaxSize=0\",\n+                \"-XX:-UseArrayLoadStoreProfile\",\n+                \"-XX:InlineFieldMaxFlatSize=-1\",\n+                \"-XX:+InlineTypePassFieldsAsArgs\",\n+                \"-XX:+InlineTypeReturnedAsFields\",\n+                \"-XX:+StressInlineTypeReturnedAsFields\"};\n+        case 3: return new String[] {\n+                \"-DVerifyIR=false\",\n+                \"-XX:+AlwaysIncrementalInline\",\n+                \"-XX:FlatArrayElementMaxOops=0\",\n+                \"-XX:FlatArrayElementMaxSize=0\",\n+                \"-XX:InlineFieldMaxFlatSize=0\",\n+                \"-XX:+InlineTypePassFieldsAsArgs\",\n+                \"-XX:+InlineTypeReturnedAsFields\"};\n+        case 4: return new String[] {\n+                \"-DVerifyIR=false\",\n+                \"-XX:FlatArrayElementMaxOops=-1\",\n+                \"-XX:FlatArrayElementMaxSize=-1\",\n+                \"-XX:InlineFieldMaxFlatSize=0\",\n+                \"-XX:+InlineTypePassFieldsAsArgs\",\n+                \"-XX:-InlineTypeReturnedAsFields\",\n+                \"-XX:-ReduceInitialCardMarks\"};\n+        case 5: return new String[] {\n+                \"-XX:+AlwaysIncrementalInline\",\n+                \"-XX:FlatArrayElementMaxOops=5\",\n+                \"-XX:FlatArrayElementMaxSize=-1\",\n+                \"-XX:-UseArrayLoadStoreProfile\",\n+                \"-XX:InlineFieldMaxFlatSize=-1\",\n+                \"-XX:-InlineTypePassFieldsAsArgs\",\n+                \"-XX:-InlineTypeReturnedAsFields\"};\n+        }\n+        return null;\n+    }\n+\n+    \/**\n+     * Override this method and return a non-null reason if the given scenario should be\n+     * ignored (due to an existing bug, etc).\n+     *\/\n+    String isScenarioIgnored(int scenario) {\n+        return null;\n+    }\n+\n+    \/**\n+     * Override this method to provide extra parameters for selected scenarios\n+     *\/\n+    public String[] getExtraVMParameters(int scenario) {\n+        return null;\n+    }\n+\n+    public static void main(String[] args) throws Throwable {\n+        if (args.length != 1) {\n+            throw new RuntimeException(\"Usage: @run main\/othervm\/timeout=120 -Xbootclasspath\/a:.\" +\n+                                       \" -XX:+IgnoreUnrecognizedVMOptions -XX:+UnlockDiagnosticVMOptions\" +\n+                                       \" -XX:+UnlockExperimentalVMOptions -XX:+WhiteBoxAPI\" +\n+                                       \" compiler.valhalla.inlinetypes.InlineTypeTest <YourTestMainClass>\");\n+        }\n+        String testMainClassName = args[0];\n+        Class testMainClass = Class.forName(testMainClassName);\n+        InlineTypeTest test = (InlineTypeTest)testMainClass.newInstance();\n+        List<String> scenarios = null;\n+        if (!SCENARIOS.isEmpty()) {\n+           scenarios = Arrays.asList(SCENARIOS.split(\",\"));\n+        }\n+        for (int i=0; i<test.getNumScenarios(); i++) {\n+            String reason;\n+            if ((reason = test.isScenarioIgnored(i)) != null) {\n+                System.out.println(\"Scenario #\" + i + \" is ignored: \" + reason);\n+            } else if (scenarios != null && !scenarios.contains(Integer.toString(i))) {\n+                System.out.println(\"Scenario #\" + i + \" is skipped due to -Dscenarios=\" + SCENARIOS);\n+            } else {\n+                System.out.println(\"Scenario #\" + i + \" -------- \");\n+                String[] cmds = InputArguments.getVmInputArgs();\n+                if (RUN_SCENARIOS_WITH_XCOMP) {\n+                    cmds = concat(cmds, \"-Xcomp\");\n+                }\n+                cmds = concat(cmds, test.getVMParameters(i));\n+                cmds = concat(cmds, test.getExtraVMParameters(i));\n+                cmds = concat(cmds, testMainClassName);\n+\n+                OutputAnalyzer oa = ProcessTools.executeTestJvm(cmds);\n+                String output = oa.getOutput();\n+                oa.shouldHaveExitValue(0);\n+                System.out.println(output);\n+            }\n+        }\n+    }\n+\n+    \/\/ To exclude test cases, use -DExclude=<case1>,<case2>,...\n+    \/\/ Each case can be just the method name, or can be <class>.<method>. The latter form is useful\n+    \/\/ when you are running several tests at the same time.\n+    \/\/\n+    \/\/ jtreg -DExclude=test12 TestArrays.java\n+    \/\/ jtreg -DExclude=test34 TestLWorld.java\n+    \/\/ -- or --\n+    \/\/ jtreg -DExclude=TestArrays.test12,TestLWorld.test34 TestArrays.java TestLWorld.java\n+    \/\/\n+    private List<String> buildExcludeList() {\n+        List<String> exclude = null;\n+        String classPrefix = getClass().getSimpleName() + \".\";\n+        if (!EXCLUDELIST.isEmpty()) {\n+            exclude = new ArrayList(Arrays.asList(EXCLUDELIST.split(\",\")));\n+            for (int i = exclude.size() - 1; i >= 0; i--) {\n+                String ex = exclude.get(i);\n+                if (ex.indexOf(\".\") > 0) {\n+                    if (ex.startsWith(classPrefix)) {\n+                        ex = ex.substring(classPrefix.length());\n+                        exclude.set(i, ex);\n+                    } else {\n+                        exclude.remove(i);\n+                    }\n+                }\n+            }\n+        }\n+        return exclude;\n+    }\n+\n+    protected InlineTypeTest() {\n+        List<String> list = null;\n+        if (!TESTLIST.isEmpty()) {\n+           list = Arrays.asList(TESTLIST.split(\",\"));\n+        }\n+        List<String> exclude = buildExcludeList();\n+\n+        \/\/ Gather all test methods and put them in Hashtable\n+        for (Method m : getClass().getDeclaredMethods()) {\n+            Test[] annos = m.getAnnotationsByType(Test.class);\n+            if (annos.length != 0 &&\n+                ((list == null || list.contains(m.getName())) && (exclude == null || !exclude.contains(m.getName())))) {\n+                tests.put(getClass().getSimpleName() + \"::\" + m.getName(), m);\n+            } else if (annos.length == 0 && m.getName().startsWith(\"test\")) {\n+                try {\n+                    getClass().getMethod(m.getName() + \"_verifier\", boolean.class);\n+                    throw new RuntimeException(m.getName() + \" has a verifier method but no @Test annotation\");\n+                } catch (NoSuchMethodException e) {\n+                    \/\/ Expected\n+                }\n+            }\n+        }\n+    }\n+\n+    protected void run(String[] args, Class<?>... classes) throws Throwable {\n+        if (args.length == 0) {\n+            \/\/ Spawn a new VM instance\n+            execute_vm();\n+        } else {\n+            \/\/ Execute tests in the VM spawned by the above code.\n+            Asserts.assertTrue(args.length == 1 && args[0].equals(\"run\"), \"must be\");\n+            run(classes);\n+        }\n+    }\n+\n+    private void execute_vm() throws Throwable {\n+        Asserts.assertFalse(tests.isEmpty(), \"no tests to execute\");\n+        String[] vmInputArgs = InputArguments.getVmInputArgs();\n+        for (String arg : vmInputArgs) {\n+            if (arg.startsWith(\"-XX:CompileThreshold\")) {\n+                \/\/ Disable IR verification if non-default CompileThreshold is set\n+                VERIFY_IR = false;\n+            }\n+        }\n+        \/\/ Each VM is launched with flags in this order, so the later ones can override the earlier one:\n+        \/\/     VERIFY_IR\/VERIFY_VM flags specified below\n+        \/\/     vmInputArgs, which consists of:\n+        \/\/        @run options\n+        \/\/        getVMParameters()\n+        \/\/        getExtraVMParameters()\n+        \/\/     defaultFlags\n+        \/\/     compileCommandFlags\n+        String cmds[] = null;\n+        if (VERIFY_IR) {\n+            \/\/ Add print flags for IR verification\n+            cmds = concat(cmds, printFlags);\n+            \/\/ Always trap for exception throwing to not confuse IR verification\n+            cmds = concat(cmds, \"-XX:-OmitStackTraceInFastThrow\");\n+        }\n+        if (VERIFY_VM) {\n+            cmds = concat(cmds, verifyFlags);\n+        }\n+        cmds = concat(cmds, vmInputArgs);\n+        cmds = concat(cmds, defaultFlags);\n+        if (COMPILE_COMMANDS) {\n+          cmds = concat(cmds, compileCommandFlags);\n+        }\n+\n+        \/\/ Run tests in own process and verify output\n+        cmds = concat(cmds, getClass().getName(), \"run\");\n+        OutputAnalyzer oa = ProcessTools.executeTestJvm(cmds);\n+        \/\/ If ideal graph printing is enabled\/supported, verify output\n+        String output = oa.getOutput();\n+        oa.shouldHaveExitValue(0);\n+        if (VERIFY_IR) {\n+            if (output.contains(\"PrintIdeal enabled\")) {\n+                parseOutput(output);\n+            } else {\n+                System.out.println(output);\n+                System.out.println(\"WARNING: IR verification failed! Running with -Xint, -Xcomp or release build?\");\n+            }\n+        }\n+    }\n+\n+    static final class TestAnnotation {\n+        private final int flag;\n+        private final BooleanSupplier predicate;\n+\n+        private static final TestAnnotation testAnnotations[] = {\n+            new TestAnnotation(InlineTypePassFieldsAsArgsOn, () -> InlineTypePassFieldsAsArgs),\n+            new TestAnnotation(InlineTypePassFieldsAsArgsOff, () -> !InlineTypePassFieldsAsArgs),\n+            new TestAnnotation(InlineTypeArrayFlattenOn, () -> InlineTypeArrayFlatten),\n+            new TestAnnotation(InlineTypeArrayFlattenOff, () -> !InlineTypeArrayFlatten),\n+            new TestAnnotation(InlineTypeReturnedAsFieldsOn, () -> InlineTypeReturnedAsFields),\n+            new TestAnnotation(InlineTypeReturnedAsFieldsOff, () -> !InlineTypeReturnedAsFields),\n+            new TestAnnotation(AlwaysIncrementalInlineOn, () -> AlwaysIncrementalInline),\n+            new TestAnnotation(AlwaysIncrementalInlineOff, () -> !AlwaysIncrementalInline),\n+            new TestAnnotation(G1GCOn, () -> G1GC),\n+            new TestAnnotation(G1GCOff, () -> !G1GC),\n+            new TestAnnotation(ZGCOn, () -> ZGC),\n+            new TestAnnotation(ZGCOff, () -> !ZGC),\n+            new TestAnnotation(ArrayLoadStoreProfileOn, () -> UseArrayLoadStoreProfile),\n+            new TestAnnotation(ArrayLoadStoreProfileOff, () -> !UseArrayLoadStoreProfile),\n+            new TestAnnotation(TypeProfileOn, () -> TypeProfileLevel == 222),\n+            new TestAnnotation(TypeProfileOff, () -> TypeProfileLevel == 0),\n+        };\n+\n+        private TestAnnotation(int flag, BooleanSupplier predicate) {\n+            this.flag = flag;\n+            this.predicate = predicate;\n+        }\n+\n+        private boolean match(Test a) {\n+            return (a.valid() & flag) != 0 && predicate.getAsBoolean();\n+        }\n+\n+        static boolean find(Test a) {\n+            Stream<TestAnnotation> s = Arrays.stream(testAnnotations).filter(t -> t.match(a));\n+            long c = s.count();\n+            if (c > 1) {\n+                throw new RuntimeException(\"At most one Test annotation should match\");\n+            }\n+            return c > 0;\n+        }\n+    }\n+\n+    private void parseOutput(String output) throws Exception {\n+        Pattern comp_re = Pattern.compile(\"\\\\n\\\\s+\\\\d+\\\\s+\\\\d+\\\\s+(%| )(s| )(!| )b(n| )\\\\s+\\\\d?\\\\s+\\\\S+\\\\.(?<name>[^.]+::\\\\S+)\\\\s+(?<osr>@ \\\\d+\\\\s+)?[(]\\\\d+ bytes[)]\");\n+        Matcher m = comp_re.matcher(output);\n+        Map<String,String> compilations = new LinkedHashMap<>();\n+        int prev = 0;\n+        String methodName = null;\n+        while (m.find()) {\n+            if (prev == 0) {\n+                \/\/ Print header\n+                System.out.print(output.substring(0, m.start()+1));\n+            } else if (methodName != null) {\n+                compilations.put(methodName, output.substring(prev, m.start()+1));\n+            }\n+            if (m.group(\"osr\") != null) {\n+                methodName = null;\n+            } else {\n+                methodName = m.group(\"name\");\n+            }\n+            prev = m.end();\n+        }\n+        if (prev == 0) {\n+            \/\/ Print header\n+            System.out.print(output);\n+        } else if (methodName != null) {\n+            compilations.put(methodName, output.substring(prev));\n+        }\n+        \/\/ Iterate over compilation output\n+        for (String testName : compilations.keySet()) {\n+            Method test = tests.get(testName);\n+            if (test == null) {\n+                \/\/ Skip helper methods\n+                continue;\n+            }\n+            String graph = compilations.get(testName);\n+            if (PRINT_GRAPH) {\n+                System.out.println(\"\\nGraph for \" + testName + \"\\n\" + graph);\n+            }\n+            \/\/ Parse graph using regular expressions to determine if it contains forbidden nodes\n+            Test[] annos = test.getAnnotationsByType(Test.class);\n+            Test anno = Arrays.stream(annos).filter(TestAnnotation::find).findFirst().orElse(null);\n+            if (anno == null) {\n+                Object[] res = Arrays.stream(annos).filter(a -> a.valid() == 0).toArray();\n+                if (res.length != 1) {\n+                    throw new RuntimeException(\"Only one Test annotation should match\");\n+                }\n+                anno = (Test)res[0];\n+            }\n+            String regexFail = anno.failOn();\n+            if (!regexFail.isEmpty()) {\n+                Pattern pattern = Pattern.compile(regexFail.substring(0, regexFail.length()-1));\n+                Matcher matcher = pattern.matcher(graph);\n+                boolean found = matcher.find();\n+                Asserts.assertFalse(found, \"Graph for '\" + testName + \"' contains forbidden node:\\n\" + (found ? matcher.group() : \"\"));\n+            }\n+            String[] regexMatch = anno.match();\n+            int[] matchCount = anno.matchCount();\n+            for (int i = 0; i < regexMatch.length; ++i) {\n+                Pattern pattern = Pattern.compile(regexMatch[i].substring(0, regexMatch[i].length()-1));\n+                Matcher matcher = pattern.matcher(graph);\n+                int count = 0;\n+                String nodes = \"\";\n+                while (matcher.find()) {\n+                    count++;\n+                    nodes += matcher.group() + \"\\n\";\n+                }\n+                if (matchCount[i] < 0) {\n+                    Asserts.assertLTE(Math.abs(matchCount[i]), count, \"Graph for '\" + testName + \"' contains different number of match nodes (expected >= \" + Math.abs(matchCount[i]) + \" but got \" + count + \"):\\n\" + nodes);\n+                } else {\n+                    Asserts.assertEQ(matchCount[i], count, \"Graph for '\" + testName + \"' contains different number of match nodes (expected \" + matchCount[i] + \" but got \" + count + \"):\\n\" + nodes);\n+                }\n+            }\n+            tests.remove(testName);\n+            System.out.println(testName + \" passed\");\n+        }\n+        \/\/ Check if all tests were compiled\n+        if (tests.size() != 0) {\n+            for (String name : tests.keySet()) {\n+                System.out.println(\"Test '\" + name + \"' not compiled!\");\n+            }\n+            throw new RuntimeException(\"Not all tests were compiled\");\n+        }\n+    }\n+\n+    private void setup(Class<?> clazz) {\n+        if (XCOMP) {\n+            \/\/ Don't control compilation if -Xcomp is enabled\n+            return;\n+        }\n+        if (DUMP_REPLAY) {\n+            \/\/ Generate replay compilation files\n+            String directive = \"[{ match: \\\"*.*\\\", DumpReplay: true }]\";\n+            if (WHITE_BOX.addCompilerDirective(directive) != 1) {\n+                throw new RuntimeException(\"Failed to add compiler directive\");\n+            }\n+        }\n+\n+        Method[] methods = clazz.getDeclaredMethods();\n+        for (Method m : methods) {\n+            if (m.isAnnotationPresent(Test.class)) {\n+                \/\/ Don't inline tests\n+                WHITE_BOX.testSetDontInlineMethod(m, true);\n+            }\n+            if (m.isAnnotationPresent(DontCompile.class)) {\n+                WHITE_BOX.makeMethodNotCompilable(m, COMP_LEVEL_ANY, true);\n+                WHITE_BOX.makeMethodNotCompilable(m, COMP_LEVEL_ANY, false);\n+                WHITE_BOX.testSetDontInlineMethod(m, true);\n+            }\n+            if (m.isAnnotationPresent(ForceInline.class)) {\n+                Asserts.assertFalse(m.isAnnotationPresent(DontInline.class), \"Method \" + m.getName() + \" has contradicting DontInline annotation\");\n+                WHITE_BOX.testSetForceInlineMethod(m, true);\n+            }\n+            if (m.isAnnotationPresent(DontInline.class)) {\n+                Asserts.assertFalse(m.isAnnotationPresent(ForceInline.class), \"Method \" + m.getName() + \" has contradicting ForceInline annotation\");\n+                WHITE_BOX.testSetDontInlineMethod(m, true);\n+            }\n+            if (STRESS_CC) {\n+                \/\/ Exclude some methods from compilation with C2 to stress test the calling convention\n+                if (Utils.getRandomInstance().nextBoolean()) {\n+                    System.out.println(\"Excluding from C2 compilation: \" + m);\n+                    WHITE_BOX.makeMethodNotCompilable(m, COMP_LEVEL_FULL_OPTIMIZATION, false);\n+                }\n+            }\n+        }\n+        \/\/ Only force compilation now because above annotations affect inlining\n+        for (Method m : methods) {\n+            if (m.isAnnotationPresent(ForceCompile.class)) {\n+                Asserts.assertFalse(m.isAnnotationPresent(DontCompile.class), \"Method \" + m.getName() + \" has contradicting DontCompile annotation\");\n+                int compLevel = getCompLevel(m.getAnnotation(ForceCompile.class));\n+                enqueueMethodForCompilation(m, compLevel);\n+            }\n+        }\n+        \/\/ Compile class initializers\n+        int compLevel = getCompLevel(null);\n+        WHITE_BOX.enqueueInitializerForCompilation(clazz, compLevel);\n+    }\n+\n+    private void run(Class<?>... classes) throws Exception {\n+        if (USE_COMPILER && PRINT_IDEAL && !XCOMP && !STRESS_CC) {\n+            System.out.println(\"PrintIdeal enabled\");\n+        }\n+        System.out.format(\"rI = %d, rL = %d\\n\", rI, rL);\n+\n+        setup(getClass());\n+        for (Class<?> clazz : classes) {\n+            setup(clazz);\n+        }\n+\n+        TreeMap<Long, String> durations = (PRINT_TIMES || VERBOSE) ? new TreeMap<Long, String>() : null;\n+        List<Method> testList = new ArrayList<Method>(tests.values());\n+        if (SHUFFLE_TESTS) {\n+            \/\/ Execute tests in random order (execution sequence affects profiling)\n+            Collections.shuffle(testList, Utils.getRandomInstance());\n+        }\n+        for (Method test : testList) {\n+            if (VERBOSE) {\n+                System.out.println(\"Starting \" + test.getName());\n+            }\n+            TempSkipForC1 c1skip = test.getAnnotation(TempSkipForC1.class);\n+            if (TEST_C1 && c1skip != null) {\n+                System.out.println(\"Skipped \" + test.getName() + \" for C1 testing: \" + c1skip.reason());\n+                continue;\n+            }\n+            long startTime = System.nanoTime();\n+            Method verifier = getClass().getMethod(test.getName() + \"_verifier\", boolean.class);\n+            \/\/ Warmup using verifier method\n+            Warmup anno = test.getAnnotation(Warmup.class);\n+            int warmup = anno == null ? WARMUP : anno.value();\n+            for (int i = 0; i < warmup; ++i) {\n+                verifier.invoke(this, true);\n+            }\n+            boolean osrOnly = (test.getAnnotation(OSRCompileOnly.class) != null);\n+            int compLevel = getCompLevel(test.getAnnotation(Test.class));\n+\n+            \/\/ C1 generates a lot of code when VerifyOops is enabled and may run out of space (for a small\n+            \/\/ number of test cases).\n+            boolean maybeCodeBufferOverflow = (TEST_C1 && VerifyOops);\n+\n+            if (osrOnly) {\n+                long started = System.currentTimeMillis();\n+                boolean stateCleared = false;\n+                for (;;) {\n+                    long elapsed = System.currentTimeMillis() - started;\n+                    int level = WHITE_BOX.getMethodCompilationLevel(test);\n+                    if (maybeCodeBufferOverflow && elapsed > 5000 && (!WHITE_BOX.isMethodCompiled(test, false) || level != compLevel)) {\n+                        System.out.println(\"Temporarily disabling VerifyOops\");\n+                        try {\n+                            WHITE_BOX.setBooleanVMFlag(\"VerifyOops\", false);\n+                            if (!stateCleared) {\n+                                WHITE_BOX.clearMethodState(test);\n+                                stateCleared = true;\n+                            }\n+                            verifier.invoke(this, false);\n+                        } finally {\n+                            WHITE_BOX.setBooleanVMFlag(\"VerifyOops\", true);\n+                            System.out.println(\"Re-enabled VerifyOops\");\n+                        }\n+                    } else {\n+                        verifier.invoke(this, false);\n+                    }\n+\n+                    boolean b = WHITE_BOX.isMethodCompiled(test, false);\n+                    if (VERBOSE) {\n+                        System.out.println(\"Is \" + test.getName() + \" compiled? \" + b);\n+                    }\n+                    if (b || XCOMP || STRESS_CC || !USE_COMPILER) {\n+                        \/\/ Don't control compilation if -Xcomp is enabled, or if compiler is disabled\n+                        break;\n+                    }\n+                    Asserts.assertTrue(OSR_TEST_TIMEOUT < 0 || elapsed < OSR_TEST_TIMEOUT, test + \" not compiled after \" + OSR_TEST_TIMEOUT + \" ms\");\n+                }\n+            } else {\n+                \/\/ Trigger compilation\n+                enqueueMethodForCompilation(test, compLevel);\n+                if (maybeCodeBufferOverflow && !WHITE_BOX.isMethodCompiled(test, false)) {\n+                    \/\/ Let's disable VerifyOops temporarily and retry.\n+                    WHITE_BOX.setBooleanVMFlag(\"VerifyOops\", false);\n+                    WHITE_BOX.clearMethodState(test);\n+                    enqueueMethodForCompilation(test, compLevel);\n+                    WHITE_BOX.setBooleanVMFlag(\"VerifyOops\", true);\n+                }\n+                if (!STRESS_CC && USE_COMPILER) {\n+                    Asserts.assertTrue(WHITE_BOX.isMethodCompiled(test, false), test + \" not compiled\");\n+                    int level = WHITE_BOX.getMethodCompilationLevel(test);\n+                    Asserts.assertEQ(level, compLevel, \"Unexpected compilation level for \" + test);\n+                }\n+                \/\/ Check result\n+                verifier.invoke(this, false);\n+            }\n+            if (PRINT_TIMES || VERBOSE) {\n+                long endTime = System.nanoTime();\n+                long duration = (endTime - startTime);\n+                durations.put(duration, test.getName());\n+                if (VERBOSE) {\n+                    System.out.println(\"Done \" + test.getName() + \": \" + duration + \" ns = \" + (duration \/ 1000000) + \" ms\");\n+                }\n+            }\n+            if (GC_AFTER) {\n+                System.out.println(\"doing GC\");\n+                System.gc();\n+            }\n+        }\n+\n+        \/\/ Print execution times\n+        if (PRINT_TIMES) {\n+          System.out.println(\"\\n\\nTest execution times:\");\n+          for (Map.Entry<Long, String> entry : durations.entrySet()) {\n+              System.out.format(\"%-10s%15d ns\\n\", entry.getValue() + \":\", entry.getKey());\n+          }\n+        }\n+    }\n+\n+    \/\/ Get the appropriate compilation level for a method, according to the\n+    \/\/ given annotation, as well as the current test scenario and VM options.\n+    \/\/\n+    private int getCompLevel(Object annotation) {\n+        int compLevel;\n+        if (annotation == null) {\n+            compLevel = COMP_LEVEL_ANY;\n+        } else if (annotation instanceof Test) {\n+            compLevel = ((Test)annotation).compLevel();\n+        } else {\n+            compLevel = ((ForceCompile)annotation).compLevel();\n+        }\n+\n+        return restrictCompLevel(compLevel);\n+    }\n+\n+    \/\/ Get the appropriate level as permitted by the test scenario and VM options.\n+    private static int restrictCompLevel(int compLevel) {\n+        if (compLevel == COMP_LEVEL_ANY) {\n+            compLevel = COMP_LEVEL_FULL_OPTIMIZATION;\n+        }\n+        if (FLIP_C1_C2) {\n+            \/\/ Effectively treat all (compLevel = C1) as (compLevel = C2), and\n+            \/\/                       (compLevel = C2) as (compLevel = C1).\n+            if (compLevel == COMP_LEVEL_SIMPLE) {\n+                compLevel = COMP_LEVEL_FULL_OPTIMIZATION;\n+            } else if (compLevel == COMP_LEVEL_FULL_OPTIMIZATION) {\n+                compLevel = COMP_LEVEL_SIMPLE;\n+            }\n+        }\n+        if (!TEST_C1 && compLevel < COMP_LEVEL_FULL_OPTIMIZATION) {\n+            compLevel = COMP_LEVEL_FULL_OPTIMIZATION;\n+        }\n+        if (TieredCompilation && compLevel > (int)TieredStopAtLevel) {\n+            compLevel = (int)TieredStopAtLevel;\n+        }\n+        return compLevel;\n+    }\n+\n+    public static void enqueueMethodForCompilation(Method m, int level) {\n+        level = restrictCompLevel(level);\n+        if (VERBOSE) {\n+            System.out.println(\"enqueueMethodForCompilation \" + m + \", level = \" + level);\n+        }\n+        WHITE_BOX.enqueueMethodForCompilation(m, level);\n+    }\n+\n+    \/\/ Unlike C2, C1 intrinsics never deoptimize System.arraycopy. Instead, we fall back to\n+    \/\/ a normal method invocation when encountering flattened arrays.\n+    static boolean isCompiledByC2(Method m) {\n+        return USE_COMPILER && !XCOMP && WHITE_BOX.isMethodCompiled(m, false) &&\n+            WHITE_BOX.getMethodCompilationLevel(m, false) >= COMP_LEVEL_FULL_OPTIMIZATION;\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/InlineTypeTest.java","additions":831,"deletions":0,"binary":false,"changes":831,"status":"added"},{"patch":"@@ -151,0 +151,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n","filename":"test\/lib\/sun\/hotspot\/WhiteBox.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"}]}