{"files":[{"patch":"@@ -113,0 +113,1 @@\n+          - build hotspot optimized\n@@ -124,0 +125,3 @@\n+          - flavor: build hotspot optimized\n+            flags: --with-debug-level=optimized --disable-precompiled-headers\n+            build-target: hotspot\n@@ -388,1 +392,1 @@\n-    runs-on: \"windows-latest\"\n+    runs-on: \"windows-2019\"\n@@ -468,0 +472,6 @@\n+      - name: Ensure a specific version of MSVC is installed\n+        run: >\n+          Start-Process -FilePath 'C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vs_installer.exe' -Wait -NoNewWindow -ArgumentList \n+          'modify --installPath \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\" --quiet \n+          --add Microsoft.VisualStudio.Component.VC.14.27.x86.x64'\n+\n@@ -474,0 +484,1 @@\n+          --with-msvc-toolset-version=14.27\n@@ -504,1 +515,1 @@\n-    runs-on: \"windows-latest\"\n+    runs-on: \"windows-2019\"\n@@ -705,1 +716,1 @@\n-    runs-on: \"macos-latest\"\n+    runs-on: \"macos-10.15\"\n@@ -775,0 +786,3 @@\n+      - name: Select Xcode version\n+        run: sudo xcode-select --switch \/Applications\/Xcode_11.3.1.app\/Contents\/Developer\n+\n@@ -805,1 +819,1 @@\n-    runs-on: \"macos-latest\"\n+    runs-on: \"macos-10.15\"\n@@ -918,0 +932,3 @@\n+      - name: Select Xcode version\n+        run: sudo xcode-select --switch \/Applications\/Xcode_11.3.1.app\/Contents\/Developer\n+\n","filename":".github\/workflows\/submit.yml","additions":21,"deletions":4,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -259,0 +259,7 @@\n+    \/\/ Extra settings for release profiles\n+    common.release_profile_base = {\n+        configure_args: [\n+            \"--enable-reproducible-build\",\n+            \"--with-source-date=current\",\n+        ],\n+    };\n@@ -271,0 +278,6 @@\n+    \/\/ Extra settings for optimized profiles\n+    common.optimized_suffix = \"-optimized\";\n+    common.optimized_profile_base = {\n+        configure_args: [\"--with-debug-level=optimized\"],\n+        labels: \"optimized\",\n+    };\n@@ -513,0 +526,7 @@\n+    \/\/ Generate optimized versions of all the main profiles\n+    common.main_profile_names.forEach(function (name) {\n+        var optName = name + common.optimized_suffix;\n+        profiles[optName] = concatObjects(profiles[name],\n+                                          common.optimized_profile_base);\n+        profiles[optName].default_make_targets = [ \"hotspot\" ];\n+    });\n@@ -800,0 +820,7 @@\n+    \/\/ After creating all derived profiles, we can add the release profile base\n+    \/\/ to the main profiles\n+    common.main_profile_names.forEach(function (name) {\n+        profiles[name] = concatObjects(profiles[name],\n+            common.release_profile_base);\n+    });\n+\n","filename":"make\/conf\/jib-profiles.js","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -2540,5 +2540,0 @@\n-\/\/ AES support not yet implemented\n-const bool Matcher::pass_original_key_for_aes() {\n-  return false;\n-}\n-\n@@ -4127,7 +4122,0 @@\n-  \/\/ Number of stack slots between incoming argument block and the start of\n-  \/\/ a new frame.  The PROLOG must add this many slots to the stack.  The\n-  \/\/ EPILOG must remove this many slots. aarch64 needs two slots for\n-  \/\/ return address and fp.\n-  \/\/ TODO think this is correct but check\n-  in_preserve_stack_slots(4);\n-\n@@ -4152,19 +4140,0 @@\n-  \/\/ Body of function which returns an integer array locating\n-  \/\/ arguments either in registers or in stack slots.  Passed an array\n-  \/\/ of ideal registers called \"sig\" and a \"length\" count.  Stack-slot\n-  \/\/ offsets are based on outgoing arguments, i.e. a CALLER setting up\n-  \/\/ arguments for a CALLEE.  Incoming stack arguments are\n-  \/\/ automatically biased by the preserve_stack_slots field above.\n-\n-  calling_convention\n-  %{\n-    \/\/ No difference between ingoing\/outgoing just pass false\n-    SharedRuntime::java_calling_convention(sig_bt, regs, length, false);\n-  %}\n-\n-  c_calling_convention\n-  %{\n-    \/\/ This is obviously always outgoing\n-    (void) SharedRuntime::c_calling_convention(sig_bt, regs, NULL, length);\n-  %}\n-\n@@ -8929,0 +8898,11 @@\n+instruct castLL(iRegL dst)\n+%{\n+  match(Set dst (CastLL dst));\n+\n+  size(0);\n+  format %{ \"# castLL of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n@@ -12444,1 +12424,1 @@\n- \n+\n@@ -12751,1 +12731,1 @@\n-\/\/ Rotations \n+\/\/ Rotations\n@@ -12826,3 +12806,1 @@\n-\n-\/\/ rol expander\n-instruct rolL_rReg(iRegLNoSp dst, iRegL src, iRegI shift, rFlagsReg cr)\n+instruct rorI_imm(iRegINoSp dst, iRegI src, immI shift)\n@@ -12830,1 +12808,1 @@\n-  effect(DEF dst, USE src, USE shift);\n+  match(Set dst (RotateRight src shift));\n@@ -12832,17 +12810,2 @@\n-  format %{ \"rol    $dst, $src, $shift\" %}\n-  ins_cost(INSN_COST * 3);\n-  ins_encode %{\n-    __ subw(rscratch1, zr, as_Register($shift$$reg));\n-    __ rorv(as_Register($dst$$reg), as_Register($src$$reg),\n-            rscratch1);\n-    %}\n-  ins_pipe(ialu_reg_reg_vshift);\n-%}\n-\n-\/\/ This pattern is automatically generated from aarch64_ad.m4\n-\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n-\n-\/\/ rol expander\n-instruct rolI_rReg(iRegINoSp dst, iRegI src, iRegI shift, rFlagsReg cr)\n-%{\n-  effect(DEF dst, USE src, USE shift);\n+  ins_cost(INSN_COST);\n+  format %{ \"ror    $dst, $src, $shift\" %}\n@@ -12850,28 +12813,2 @@\n-  format %{ \"rol    $dst, $src, $shift\" %}\n-  ins_cost(INSN_COST * 3);\n-    __ subw(rscratch1, zr, as_Register($shift$$reg));\n-    __ rorvw(as_Register($dst$$reg), as_Register($src$$reg),\n-            rscratch1);\n-    %}\n-  ins_pipe(ialu_reg_reg_vshift);\n-%}\n-\n-\/\/ This pattern is automatically generated from aarch64_ad.m4\n-\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n-instruct rolL_rReg_Var_C_64(iRegLNoSp dst, iRegL src, iRegI shift, immI_64 c_64, rFlagsReg cr)\n-%{\n-  match(Set dst (OrL (LShiftL src shift) (URShiftL src (SubI c_64 shift))));\n-\n-  expand %{\n-    rolL_rReg(dst, src, shift, cr);\n-  %}\n-%}\n-\n-\/\/ This pattern is automatically generated from aarch64_ad.m4\n-\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n-instruct rolL_rReg_Var_C0(iRegLNoSp dst, iRegL src, iRegI shift, immI0 c0, rFlagsReg cr)\n-%{\n-  match(Set dst (OrL (LShiftL src shift) (URShiftL src (SubI c0 shift))));\n-\n-  expand %{\n-    rolL_rReg(dst, src, shift, cr);\n+     __ extrw(as_Register($dst$$reg), as_Register($src$$reg), as_Register($src$$reg),\n+               $shift$$constant & 0x1f);\n@@ -12880,0 +12817,1 @@\n+  ins_pipe(ialu_reg_reg_vshift);\n@@ -12884,1 +12822,1 @@\n-instruct rolI_rReg_Var_C_32(iRegINoSp dst, iRegI src, iRegI shift, immI_32 c_32, rFlagsReg cr)\n+instruct rorL_imm(iRegLNoSp dst, iRegL src, immI shift)\n@@ -12886,6 +12824,1 @@\n-  match(Set dst (OrI (LShiftI src shift) (URShiftI src (SubI c_32 shift))));\n-\n-  expand %{\n-    rolI_rReg(dst, src, shift, cr);\n-  %}\n-%}\n+  match(Set dst (RotateRight src shift));\n@@ -12893,5 +12826,2 @@\n-\/\/ This pattern is automatically generated from aarch64_ad.m4\n-\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n-instruct rolI_rReg_Var_C0(iRegINoSp dst, iRegI src, iRegI shift, immI0 c0, rFlagsReg cr)\n-%{\n-  match(Set dst (OrI (LShiftI src shift) (URShiftI src (SubI c0 shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ror    $dst, $src, $shift\" %}\n@@ -12899,2 +12829,3 @@\n-  expand %{\n-    rolI_rReg(dst, src, shift, cr);\n+  ins_encode %{\n+     __ extr(as_Register($dst$$reg), as_Register($src$$reg), as_Register($src$$reg),\n+               $shift$$constant & 0x3f);\n@@ -12902,0 +12833,1 @@\n+  ins_pipe(ialu_reg_reg_vshift);\n@@ -12906,3 +12838,1 @@\n-\n-\/\/ ror expander\n-instruct rorL_rReg(iRegLNoSp dst, iRegL src, iRegI shift, rFlagsReg cr)\n+instruct rorI_reg(iRegINoSp dst, iRegI src, iRegI shift)\n@@ -12910,1 +12840,1 @@\n-  effect(DEF dst, USE src, USE shift);\n+  match(Set dst (RotateRight src shift));\n@@ -12912,1 +12842,2 @@\n-  format %{ \"ror    $dst, $src, $shift\" %}\n+  format %{ \"ror    $dst, $src, $shift\" %}\n+\n@@ -12915,3 +12846,2 @@\n-    __ rorv(as_Register($dst$$reg), as_Register($src$$reg),\n-            as_Register($shift$$reg));\n-    %}\n+     __ rorvw(as_Register($dst$$reg), as_Register($src$$reg), as_Register($shift$$reg));\n+  %}\n@@ -12923,3 +12853,1 @@\n-\n-\/\/ ror expander\n-instruct rorI_rReg(iRegINoSp dst, iRegI src, iRegI shift, rFlagsReg cr)\n+instruct rorL_reg(iRegLNoSp dst, iRegL src, iRegI shift)\n@@ -12927,1 +12855,1 @@\n-  effect(DEF dst, USE src, USE shift);\n+  match(Set dst (RotateRight src shift));\n@@ -12929,1 +12857,2 @@\n-  format %{ \"ror    $dst, $src, $shift\" %}\n+  format %{ \"ror    $dst, $src, $shift\" %}\n+\n@@ -12932,3 +12861,2 @@\n-    __ rorvw(as_Register($dst$$reg), as_Register($src$$reg),\n-            as_Register($shift$$reg));\n-    %}\n+     __ rorv(as_Register($dst$$reg), as_Register($src$$reg), as_Register($shift$$reg));\n+  %}\n@@ -12940,1 +12868,1 @@\n-instruct rorL_rReg_Var_C_64(iRegLNoSp dst, iRegL src, iRegI shift, immI_64 c_64, rFlagsReg cr)\n+instruct rolI_reg(iRegINoSp dst, iRegI src, iRegI shift)\n@@ -12942,6 +12870,1 @@\n-  match(Set dst (OrL (URShiftL src shift) (LShiftL src (SubI c_64 shift))));\n-\n-  expand %{\n-    rorL_rReg(dst, src, shift, cr);\n-  %}\n-%}\n+  match(Set dst (RotateLeft src shift));\n@@ -12949,5 +12872,2 @@\n-\/\/ This pattern is automatically generated from aarch64_ad.m4\n-\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n-instruct rorL_rReg_Var_C0(iRegLNoSp dst, iRegL src, iRegI shift, immI0 c0, rFlagsReg cr)\n-%{\n-  match(Set dst (OrL (URShiftL src shift) (LShiftL src (SubI c0 shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"rol    $dst, $src, $shift\" %}\n@@ -12955,2 +12875,3 @@\n-  expand %{\n-    rorL_rReg(dst, src, shift, cr);\n+  ins_encode %{\n+     __ subw(rscratch1, zr, as_Register($shift$$reg));\n+     __ rorvw(as_Register($dst$$reg), as_Register($src$$reg), rscratch1);\n@@ -12958,0 +12879,1 @@\n+  ins_pipe(ialu_reg_reg_vshift);\n@@ -12962,1 +12884,1 @@\n-instruct rorI_rReg_Var_C_32(iRegINoSp dst, iRegI src, iRegI shift, immI_32 c_32, rFlagsReg cr)\n+instruct rolL_reg(iRegLNoSp dst, iRegL src, iRegI shift)\n@@ -12964,6 +12886,1 @@\n-  match(Set dst (OrI (URShiftI src shift) (LShiftI src (SubI c_32 shift))));\n-\n-  expand %{\n-    rorI_rReg(dst, src, shift, cr);\n-  %}\n-%}\n+  match(Set dst (RotateLeft src shift));\n@@ -12971,5 +12888,2 @@\n-\/\/ This pattern is automatically generated from aarch64_ad.m4\n-\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n-instruct rorI_rReg_Var_C0(iRegINoSp dst, iRegI src, iRegI shift, immI0 c0, rFlagsReg cr)\n-%{\n-  match(Set dst (OrI (URShiftI src shift) (LShiftI src (SubI c0 shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"rol    $dst, $src, $shift\" %}\n@@ -12977,2 +12891,3 @@\n-  expand %{\n-    rorI_rReg(dst, src, shift, cr);\n+  ins_encode %{\n+     __ subw(rscratch1, zr, as_Register($shift$$reg));\n+     __ rorv(as_Register($dst$$reg), as_Register($src$$reg), rscratch1);\n@@ -12980,0 +12895,1 @@\n+  ins_pipe(ialu_reg_reg_vshift);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":56,"deletions":140,"binary":false,"changes":196,"status":"modified"},{"patch":"@@ -412,1 +412,1 @@\n-  SharedRuntime::java_calling_convention(signature, args, 5, true);\n+  SharedRuntime::java_calling_convention(signature, args, 5);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -812,1 +812,7 @@\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+    \/\/ Fast check for recursive lock.\n+    \/\/\n+    \/\/ Can apply the optimization only if this is a stack lock\n+    \/\/ allocated in this thread. For efficiency, we can focus on\n+    \/\/ recently allocated stack locks (instead of reading the stack\n+    \/\/ base and checking whether 'mark' points inside the current\n+    \/\/ thread stack):\n@@ -814,1 +820,13 @@\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n+    \/\/  2) sp <= mark < mark + os::pagesize()\n+    \/\/\n+    \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n+    \/\/ neither apply the optimization for an inflated lock allocated\n+    \/\/ just above the thread stack (this is why condition 1 matters)\n+    \/\/ nor apply the optimization if the stack lock is inside the stack\n+    \/\/ of another thread. The latter is avoided even in case of overflow\n+    \/\/ because we have guard pages at the end of all stacks. Hence, if\n+    \/\/ we go over the stack base and hit the stack of another thread,\n+    \/\/ this should not be in a writeable area that could contain a\n+    \/\/ stack lock allocated by that thread. As a consequence, a stack\n+    \/\/ lock less than page size away from sp is guaranteed to be\n+    \/\/ owned by the current thread.\n@@ -817,1 +835,1 @@\n-    \/\/ expression: ((mark - rsp) & (7 - os::vm_page_size())),\n+    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n@@ -820,1 +838,1 @@\n-    \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n+    \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":22,"deletions":4,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -280,2 +280,1 @@\n-                                           int total_args_passed,\n-                                           int is_outgoing) {\n+                                           int total_args_passed) {\n@@ -2794,0 +2793,9 @@\n+\/\/ Number of stack slots between incoming argument block and the start of\n+\/\/ a new frame.  The PROLOG must add this many slots to the stack.  The\n+\/\/ EPILOG must remove this many slots. aarch64 needs two slots for\n+\/\/ return address and fp.\n+\/\/ TODO think this is correct but check\n+uint SharedRuntime::in_preserve_stack_slots() {\n+  return 4;\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1530,1 +1530,0 @@\n-  Label done;\n@@ -1540,2 +1539,1 @@\n-    __ li(Rdst, is_unordered_less ? -1 : 1);\n-    __ bso(CCR0, done);\n+    __ set_cmpu3(Rdst, is_unordered_less); \/\/ is_unordered_less ? -1 : 1\n@@ -1544,0 +1542,1 @@\n+    __ set_cmp3(Rdst);  \/\/ set result as follows: <: -1, =: 0, >: 1\n@@ -1547,5 +1546,0 @@\n-  __ mfcr(R0); \/\/ set bit 32..33 as follows: <: 0b10, =: 0b00, >: 0b01\n-  __ srwi(Rdst, R0, 30);\n-  __ srawi(R0, R0, 31);\n-  __ orr(Rdst, R0, Rdst); \/\/ set result as follows: <: -1, =: 0, >: 1\n-  __ bind(done);\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":2,"deletions":8,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -607,1 +607,1 @@\n-  SharedRuntime::java_calling_convention(signature, args, 5, true);\n+  SharedRuntime::java_calling_convention(signature, args, 5);\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -271,1 +271,1 @@\n-void ShenandoahBarrierSetAssembler::load_reference_barrier(MacroAssembler* masm, Register dst, Address src, ShenandoahBarrierSet::AccessKind kind) {\n+void ShenandoahBarrierSetAssembler::load_reference_barrier(MacroAssembler* masm, Register dst, Address src, DecoratorSet decorators) {\n@@ -274,0 +274,6 @@\n+  bool is_strong  = ShenandoahBarrierSet::is_strong_access(decorators);\n+  bool is_weak    = ShenandoahBarrierSet::is_weak_access(decorators);\n+  bool is_phantom = ShenandoahBarrierSet::is_phantom_access(decorators);\n+  bool is_native  = ShenandoahBarrierSet::is_native_access(decorators);\n+  bool is_narrow  = UseCompressedOops && !is_native;\n+\n@@ -295,1 +301,1 @@\n-  if (kind == ShenandoahBarrierSet::AccessKind::NORMAL) {\n+  if (is_strong) {\n@@ -360,20 +366,16 @@\n-  switch (kind) {\n-    case ShenandoahBarrierSet::AccessKind::NORMAL:\n-      if (UseCompressedOops) {\n-        __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow), arg0, arg1);\n-      } else {\n-        __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier), arg0, arg1);\n-      }\n-      break;\n-    case ShenandoahBarrierSet::AccessKind::WEAK:\n-      if (UseCompressedOops) {\n-        __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow), arg0, arg1);\n-      } else {\n-        __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), arg0, arg1);\n-      }\n-      break;\n-    case ShenandoahBarrierSet::AccessKind::NATIVE:\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), arg0, arg1);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n+  if (is_strong) {\n+    if (is_narrow) {\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong_narrow), arg0, arg1);\n+    } else {\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong), arg0, arg1);\n+    }\n+  } else if (is_weak) {\n+    if (is_narrow) {\n+      __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow), arg0, arg1);\n+    } else {\n+      __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), arg0, arg1);\n+    }\n+  } else {\n+    assert(is_phantom, \"only remaining strength\");\n+    assert(!is_narrow, \"phantom access cannot be narrow\");\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_phantom), arg0, arg1);\n@@ -404,1 +406,1 @@\n-  if  (kind == ShenandoahBarrierSet::AccessKind::NORMAL) {\n+  if  (is_strong) {\n@@ -502,2 +504,1 @@\n-    ShenandoahBarrierSet::AccessKind kind = ShenandoahBarrierSet::access_kind(decorators, type);\n-    load_reference_barrier(masm, dst, src, kind);\n+    load_reference_barrier(masm, dst, src, decorators);\n@@ -816,0 +817,6 @@\n+  DecoratorSet decorators = stub->decorators();\n+  bool is_strong  = ShenandoahBarrierSet::is_strong_access(decorators);\n+  bool is_weak    = ShenandoahBarrierSet::is_weak_access(decorators);\n+  bool is_phantom = ShenandoahBarrierSet::is_phantom_access(decorators);\n+  bool is_native  = ShenandoahBarrierSet::is_native_access(decorators);\n+\n@@ -831,8 +838,5 @@\n-  \/\/ Check for null.\n-  __ testptr(res, res);\n-  __ jcc(Assembler::zero, *stub->continuation());\n-\n-  \/\/ Check for object being in the collection set.\n-  __ mov(tmp1, res);\n-  __ shrptr(tmp1, ShenandoahHeapRegion::region_size_bytes_shift_jint());\n-  __ movptr(tmp2, (intptr_t) ShenandoahHeap::in_cset_fast_test_addr());\n+  if (is_strong) {\n+    \/\/ Check for object being in the collection set.\n+    __ mov(tmp1, res);\n+    __ shrptr(tmp1, ShenandoahHeapRegion::region_size_bytes_shift_jint());\n+    __ movptr(tmp2, (intptr_t) ShenandoahHeap::in_cset_fast_test_addr());\n@@ -840,2 +844,2 @@\n-  __ movbool(tmp2, Address(tmp2, tmp1, Address::times_1));\n-  __ testbool(tmp2);\n+    __ movbool(tmp2, Address(tmp2, tmp1, Address::times_1));\n+    __ testbool(tmp2);\n@@ -843,4 +847,4 @@\n-  \/\/ On x86_32, C1 register allocator can give us the register without 8-bit support.\n-  \/\/ Do the full-register access and test to avoid compilation failures.\n-  __ movptr(tmp2, Address(tmp2, tmp1, Address::times_1));\n-  __ testptr(tmp2, 0xFF);\n+    \/\/ On x86_32, C1 register allocator can give us the register without 8-bit support.\n+    \/\/ Do the full-register access and test to avoid compilation failures.\n+    __ movptr(tmp2, Address(tmp2, tmp1, Address::times_1));\n+    __ testptr(tmp2, 0xFF);\n@@ -848,1 +852,2 @@\n-  __ jcc(Assembler::zero, *stub->continuation());\n+    __ jcc(Assembler::zero, *stub->continuation());\n+  }\n@@ -853,12 +858,11 @@\n-  switch (stub->kind()) {\n-    case ShenandoahBarrierSet::AccessKind::NORMAL:\n-      __ call(RuntimeAddress(bs->load_reference_barrier_normal_rt_code_blob()->code_begin()));\n-      break;\n-    case ShenandoahBarrierSet::AccessKind::WEAK:\n-      __ call(RuntimeAddress(bs->load_reference_barrier_weak_rt_code_blob()->code_begin()));\n-      break;\n-    case ShenandoahBarrierSet::AccessKind::NATIVE:\n-      __ call(RuntimeAddress(bs->load_reference_barrier_native_rt_code_blob()->code_begin()));\n-      break;\n-    default:\n-      ShouldNotReachHere();\n+  if (is_strong) {\n+    if (is_native) {\n+      __ call(RuntimeAddress(bs->load_reference_barrier_strong_native_rt_code_blob()->code_begin()));\n+    } else {\n+      __ call(RuntimeAddress(bs->load_reference_barrier_strong_rt_code_blob()->code_begin()));\n+    }\n+  } else if (is_weak) {\n+    __ call(RuntimeAddress(bs->load_reference_barrier_weak_rt_code_blob()->code_begin()));\n+  } else {\n+    assert(is_phantom, \"only remaining strength\");\n+    __ call(RuntimeAddress(bs->load_reference_barrier_phantom_rt_code_blob()->code_begin()));\n@@ -929,1 +933,1 @@\n-void ShenandoahBarrierSetAssembler::generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, ShenandoahBarrierSet::AccessKind kind) {\n+void ShenandoahBarrierSetAssembler::generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, DecoratorSet decorators) {\n@@ -935,0 +939,5 @@\n+  bool is_strong  = ShenandoahBarrierSet::is_strong_access(decorators);\n+  bool is_weak    = ShenandoahBarrierSet::is_weak_access(decorators);\n+  bool is_phantom = ShenandoahBarrierSet::is_phantom_access(decorators);\n+  bool is_native  = ShenandoahBarrierSet::is_native_access(decorators);\n+\n@@ -938,9 +947,4 @@\n-  switch (kind) {\n-    case ShenandoahBarrierSet::AccessKind::NORMAL:\n-      if (UseCompressedOops) {\n-        __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow), c_rarg0, c_rarg1);\n-      } else {\n-        __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier), c_rarg0, c_rarg1);\n-      }\n-      break;\n-    case ShenandoahBarrierSet::AccessKind::WEAK:\n+  if (is_strong) {\n+    if (is_native) {\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong), c_rarg0, c_rarg1);\n+    } else {\n@@ -948,1 +952,1 @@\n-        __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow), c_rarg0, c_rarg1);\n+        __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong_narrow), c_rarg0, c_rarg1);\n@@ -950,1 +954,1 @@\n-        __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), c_rarg0, c_rarg1);\n+        __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong), c_rarg0, c_rarg1);\n@@ -952,2 +956,6 @@\n-      break;\n-    case ShenandoahBarrierSet::AccessKind::NATIVE:\n+    }\n+  } else if (is_weak) {\n+    assert(!is_native, \"weak must not be called off-heap\");\n+    if (UseCompressedOops) {\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow), c_rarg0, c_rarg1);\n+    } else {\n@@ -955,3 +963,5 @@\n-      break;\n-    default:\n-      ShouldNotReachHere();\n+    }\n+  } else {\n+    assert(is_phantom, \"only remaining strength\");\n+    assert(is_native, \"phantom must only be called off-heap\");\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_phantom), c_rarg0, c_rarg1);\n@@ -962,10 +972,7 @@\n-  switch (kind) {\n-    case ShenandoahBarrierSet::AccessKind::NORMAL:\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier), rax, rbx);\n-      break;\n-    case ShenandoahBarrierSet::AccessKind::WEAK:\n-    case ShenandoahBarrierSet::AccessKind::NATIVE:\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), rax, rbx);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n+  if (is_strong) {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong), rax, rbx);\n+  } else if (is_weak) {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), rax, rbx);\n+  } else {\n+    assert(is_phantom, \"only remaining strength\");\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_phantom), rax, rbx);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.cpp","additions":86,"deletions":79,"binary":false,"changes":165,"status":"modified"},{"patch":"@@ -67,1 +67,1 @@\n-  void generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, ShenandoahBarrierSet::AccessKind kind);\n+  void generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, DecoratorSet decorators);\n@@ -70,1 +70,1 @@\n-  void load_reference_barrier(MacroAssembler* masm, Register dst, Address src, ShenandoahBarrierSet::AccessKind kind);\n+  void load_reference_barrier(MacroAssembler* masm, Register dst, Address src, DecoratorSet decorators);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1404,1 +1404,7 @@\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+    \/\/ Fast check for recursive lock.\n+    \/\/\n+    \/\/ Can apply the optimization only if this is a stack lock\n+    \/\/ allocated in this thread. For efficiency, we can focus on\n+    \/\/ recently allocated stack locks (instead of reading the stack\n+    \/\/ base and checking whether 'mark' points inside the current\n+    \/\/ thread stack):\n@@ -1408,0 +1414,12 @@\n+    \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n+    \/\/ neither apply the optimization for an inflated lock allocated\n+    \/\/ just above the thread stack (this is why condition 1 matters)\n+    \/\/ nor apply the optimization if the stack lock is inside the stack\n+    \/\/ of another thread. The latter is avoided even in case of overflow\n+    \/\/ because we have guard pages at the end of all stacks. Hence, if\n+    \/\/ we go over the stack base and hit the stack of another thread,\n+    \/\/ this should not be in a writeable area that could contain a\n+    \/\/ stack lock allocated by that thread. As a consequence, a stack\n+    \/\/ lock less than page size away from rsp is guaranteed to be\n+    \/\/ owned by the current thread.\n+    \/\/\n@@ -1412,1 +1430,1 @@\n-    \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n+    \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":20,"deletions":2,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n","filename":"src\/hotspot\/cpu\/x86\/jniFastGetField_x86_64.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -421,2 +421,1 @@\n-                                           int total_args_passed,\n-                                           int is_outgoing) {\n+                                           int total_args_passed) {\n@@ -2221,0 +2220,8 @@\n+\/\/ Number of stack slots between incoming argument block and the start of\n+\/\/ a new frame.  The PROLOG must add this many slots to the stack.  The\n+\/\/ EPILOG must remove this many slots.  Intel needs one slot for\n+\/\/ return address and one for rbp, (must save rbp)\n+uint SharedRuntime::in_preserve_stack_slots() {\n+  return 2+VerifyStackAtCalls;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -455,2 +455,1 @@\n-                                           int total_args_passed,\n-                                           int is_outgoing) {\n+                                           int total_args_passed) {\n@@ -2858,0 +2857,9 @@\n+\n+\/\/ Number of stack slots between incoming argument block and the start of\n+\/\/ a new frame.  The PROLOG must add this many slots to the stack.  The\n+\/\/ EPILOG must remove this many slots.  amd64 needs two slots for\n+\/\/ return address.\n+uint SharedRuntime::in_preserve_stack_slots() {\n+  return 4 + 2 * VerifyStackAtCalls;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3159,6 +3159,0 @@\n-  \/\/ Number of stack slots between incoming argument block and the start of\n-  \/\/ a new frame.  The PROLOG must add this many slots to the stack.  The\n-  \/\/ EPILOG must remove this many slots.  Intel needs one slot for\n-  \/\/ return address and one for rbp, (must save rbp)\n-  in_preserve_stack_slots(2+VerifyStackAtCalls);\n-\n@@ -3180,23 +3174,0 @@\n-  \/\/ Body of function which returns an integer array locating\n-  \/\/ arguments either in registers or in stack slots.  Passed an array\n-  \/\/ of ideal registers called \"sig\" and a \"length\" count.  Stack-slot\n-  \/\/ offsets are based on outgoing arguments, i.e. a CALLER setting up\n-  \/\/ arguments for a CALLEE.  Incoming stack arguments are\n-  \/\/ automatically biased by the preserve_stack_slots field above.\n-  calling_convention %{\n-    \/\/ No difference between ingoing\/outgoing just pass false\n-    SharedRuntime::java_calling_convention(sig_bt, regs, length, false);\n-  %}\n-\n-\n-  \/\/ Body of function which returns an integer array locating\n-  \/\/ arguments either in registers or in stack slots.  Passed an array\n-  \/\/ of ideal registers called \"sig\" and a \"length\" count.  Stack-slot\n-  \/\/ offsets are based on outgoing arguments, i.e. a CALLER setting up\n-  \/\/ arguments for a CALLEE.  Incoming stack arguments are\n-  \/\/ automatically biased by the preserve_stack_slots field above.\n-  c_calling_convention %{\n-    \/\/ This is obviously always outgoing\n-    (void) SharedRuntime::c_calling_convention(sig_bt, regs, \/*regs2=*\/NULL, length);\n-  %}\n-\n@@ -7176,0 +7147,8 @@\n+instruct castLL( eRegL dst ) %{\n+  match(Set dst (CastLL dst));\n+  format %{ \"#castLL of $dst\" %}\n+  ins_encode( \/*empty encoding*\/ );\n+  ins_cost(0);\n+  ins_pipe( empty );\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":8,"deletions":29,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -2753,6 +2753,0 @@\n-  \/\/ Number of stack slots between incoming argument block and the start of\n-  \/\/ a new frame.  The PROLOG must add this many slots to the stack.  The\n-  \/\/ EPILOG must remove this many slots.  amd64 needs two slots for\n-  \/\/ return address.\n-  in_preserve_stack_slots(4 + 2 * VerifyStackAtCalls);\n-\n@@ -2774,19 +2768,0 @@\n-  \/\/ Body of function which returns an integer array locating\n-  \/\/ arguments either in registers or in stack slots.  Passed an array\n-  \/\/ of ideal registers called \"sig\" and a \"length\" count.  Stack-slot\n-  \/\/ offsets are based on outgoing arguments, i.e. a CALLER setting up\n-  \/\/ arguments for a CALLEE.  Incoming stack arguments are\n-  \/\/ automatically biased by the preserve_stack_slots field above.\n-\n-  calling_convention\n-  %{\n-    \/\/ No difference between ingoing\/outgoing just pass false\n-    SharedRuntime::java_calling_convention(sig_bt, regs, length, false);\n-  %}\n-\n-  c_calling_convention\n-  %{\n-    \/\/ This is obviously always outgoing\n-    (void) SharedRuntime::c_calling_convention(sig_bt, regs, \/*regs2=*\/NULL, length);\n-  %}\n-\n@@ -7496,0 +7471,11 @@\n+instruct castLL(rRegL dst)\n+%{\n+  match(Set dst (CastLL dst));\n+\n+  size(0);\n+  format %{ \"# castLL of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":11,"deletions":25,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -125,1 +125,1 @@\n-  int args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, args_passed, false);\n+  int args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, args_passed);\n@@ -130,1 +130,1 @@\n-  int args_on_stack_cc = SharedRuntime::java_calling_convention(sig_bt, regs_cc, args_passed_cc, false);\n+  int args_on_stack_cc = SharedRuntime::java_calling_convention(sig_bt, regs_cc, args_passed_cc);\n","filename":"src\/hotspot\/share\/asm\/macroAssembler_common.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -75,1 +75,1 @@\n-  intptr_t out_preserve = SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs, outgoing);\n+  intptr_t out_preserve = SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n","filename":"src\/hotspot\/share\/c1\/c1_FrameMap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1346,0 +1346,102 @@\n+void LIRGenerator::do_getObjectSize(Intrinsic* x) {\n+  assert(x->number_of_arguments() == 3, \"wrong type\");\n+  LIR_Opr result_reg = rlock_result(x);\n+\n+  LIRItem value(x->argument_at(2), this);\n+  value.load_item();\n+\n+  LIR_Opr klass = new_register(T_METADATA);\n+  __ move(new LIR_Address(value.result(), oopDesc::klass_offset_in_bytes(), T_ADDRESS), klass, NULL);\n+  LIR_Opr layout = new_register(T_INT);\n+  __ move(new LIR_Address(klass, in_bytes(Klass::layout_helper_offset()), T_INT), layout);\n+\n+  LabelObj* L_done = new LabelObj();\n+  LabelObj* L_array = new LabelObj();\n+\n+  __ cmp(lir_cond_lessEqual, layout, 0);\n+  __ branch(lir_cond_lessEqual, L_array->label());\n+\n+  \/\/ Instance case: the layout helper gives us instance size almost directly,\n+  \/\/ but we need to mask out the _lh_instance_slow_path_bit.\n+  __ convert(Bytecodes::_i2l, layout, result_reg);\n+\n+  assert((int) Klass::_lh_instance_slow_path_bit < BytesPerLong, \"clear bit\");\n+  jlong mask = ~(jlong) right_n_bits(LogBytesPerLong);\n+  __ logical_and(result_reg, LIR_OprFact::longConst(mask), result_reg);\n+\n+  __ branch(lir_cond_always, L_done->label());\n+\n+  \/\/ Array case: size is round(header + element_size*arraylength).\n+  \/\/ Since arraylength is different for every array instance, we have to\n+  \/\/ compute the whole thing at runtime.\n+\n+  __ branch_destination(L_array->label());\n+\n+  int round_mask = MinObjAlignmentInBytes - 1;\n+\n+  \/\/ Figure out header sizes first.\n+  LIR_Opr hss = LIR_OprFact::intConst(Klass::_lh_header_size_shift);\n+  LIR_Opr hsm = LIR_OprFact::intConst(Klass::_lh_header_size_mask);\n+\n+  LIR_Opr header_size = new_register(T_INT);\n+  __ move(layout, header_size);\n+  LIR_Opr tmp = new_register(T_INT);\n+  __ unsigned_shift_right(header_size, hss, header_size, tmp);\n+  __ logical_and(header_size, hsm, header_size);\n+  __ add(header_size, LIR_OprFact::intConst(round_mask), header_size);\n+\n+  \/\/ Figure out the array length in bytes\n+  assert(Klass::_lh_log2_element_size_shift == 0, \"use shift in place\");\n+  LIR_Opr l2esm = LIR_OprFact::intConst(Klass::_lh_log2_element_size_mask);\n+  __ logical_and(layout, l2esm, layout);\n+\n+  LIR_Opr length_int = new_register(T_INT);\n+  __ move(new LIR_Address(value.result(), arrayOopDesc::length_offset_in_bytes(), T_INT), length_int);\n+\n+#ifdef _LP64\n+  LIR_Opr length = new_register(T_LONG);\n+  __ convert(Bytecodes::_i2l, length_int, length);\n+#endif\n+\n+  \/\/ Shift-left awkwardness. Normally it is just:\n+  \/\/   __ shift_left(length, layout, length);\n+  \/\/ But C1 cannot perform shift_left with non-constant count, so we end up\n+  \/\/ doing the per-bit loop dance here. x86_32 also does not know how to shift\n+  \/\/ longs, so we have to act on ints.\n+  LabelObj* L_shift_loop = new LabelObj();\n+  LabelObj* L_shift_exit = new LabelObj();\n+\n+  __ branch_destination(L_shift_loop->label());\n+  __ cmp(lir_cond_equal, layout, 0);\n+  __ branch(lir_cond_equal, L_shift_exit->label());\n+\n+#ifdef _LP64\n+  __ shift_left(length, 1, length);\n+#else\n+  __ shift_left(length_int, 1, length_int);\n+#endif\n+\n+  __ sub(layout, LIR_OprFact::intConst(1), layout);\n+\n+  __ branch(lir_cond_always, L_shift_loop->label());\n+  __ branch_destination(L_shift_exit->label());\n+\n+  \/\/ Mix all up, round, and push to the result.\n+#ifdef _LP64\n+  LIR_Opr header_size_long = new_register(T_LONG);\n+  __ convert(Bytecodes::_i2l, header_size, header_size_long);\n+  __ add(length, header_size_long, length);\n+  if (round_mask != 0) {\n+    __ logical_and(length, LIR_OprFact::longConst(~round_mask), length);\n+  }\n+  __ move(length, result_reg);\n+#else\n+  __ add(length_int, header_size, length_int);\n+  if (round_mask != 0) {\n+    __ logical_and(length_int, LIR_OprFact::intConst(~round_mask), length_int);\n+  }\n+  __ convert(Bytecodes::_i2l, length_int, result_reg);\n+#endif\n+\n+  __ branch_destination(L_done->label());\n+}\n@@ -3550,0 +3652,1 @@\n+  case vmIntrinsics::_getObjectSize:  do_getObjectSize(x); break;\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":103,"deletions":0,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -259,0 +259,1 @@\n+  void do_getObjectSize(Intrinsic* x);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1945,2 +1945,2 @@\n-    if (con != NULL && !con->is_pinned()) {\n-      \/\/ unpinned constants may have no register, so add mapping from constant to interval\n+    if (con != NULL && (!con->is_pinned() || con->operand()->is_constant())) {\n+      \/\/ Need a mapping from constant to interval if unpinned (may have no register) or if the operand is a constant (no register).\n@@ -1953,1 +1953,0 @@\n-\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1138,0 +1138,1 @@\n+    _method_Scoped,\n@@ -2217,0 +2218,5 @@\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_misc_Scoped_signature): {\n+      if (_location != _in_method)  break;  \/\/ only allow for methods\n+      if (!privileged)              break;  \/\/ only allow in privileged code\n+      return _method_Scoped;\n+    }\n@@ -2274,0 +2280,2 @@\n+  if (has_annotation(_method_Scoped))\n+    m->set_scoped(true);\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/javaThreadStatus.hpp\"\n@@ -894,1 +895,1 @@\n-  if (k->is_shared() && k->has_raw_archived_mirror()) {\n+  if (k->is_shared() && k->has_archived_mirror_index()) {\n@@ -901,1 +902,1 @@\n-      k->clear_has_raw_archived_mirror();\n+      k->clear_archived_mirror_index();\n@@ -1173,3 +1174,3 @@\n-  if (k->has_raw_archived_mirror()) {\n-    assert(k->archived_java_mirror_raw() != NULL, \"no archived mirror\");\n-    return k->archived_java_mirror_raw();\n+  if (k->has_archived_mirror_index()) {\n+    assert(k->archived_java_mirror() != NULL, \"no archived mirror\");\n+    return k->archived_java_mirror();\n@@ -1213,3 +1214,1 @@\n-  k->set_archived_java_mirror_raw(archived_mirror);\n-\n-  k->set_has_raw_archived_mirror();\n+  k->set_archived_java_mirror(archived_mirror);\n@@ -1333,4 +1332,5 @@\n-  oop m = HeapShared::materialize_archived_object(k->archived_java_mirror_raw_narrow());\n-  if (m == NULL) {\n-    return false;\n-  }\n+  oop m = k->archived_java_mirror();\n+  assert(m != NULL, \"must have stored non-null archived mirror\");\n+\n+  \/\/ Sanity: clear it now to prevent re-initialization if any of the following fails\n+  k->clear_archived_mirror_index();\n@@ -1362,1 +1362,0 @@\n-  k->clear_has_raw_archived_mirror();\n@@ -1840,2 +1839,2 @@\n-                                         java_lang_Thread::ThreadStatus status) {\n-  java_thread->int_field_put(_thread_status_offset, status);\n+                                         JavaThreadStatus status) {\n+  java_thread->int_field_put(_thread_status_offset, static_cast<int>(status));\n@@ -1845,1 +1844,1 @@\n-java_lang_Thread::ThreadStatus java_lang_Thread::get_thread_status(oop java_thread) {\n+JavaThreadStatus java_lang_Thread::get_thread_status(oop java_thread) {\n@@ -1851,1 +1850,1 @@\n-  return (java_lang_Thread::ThreadStatus)java_thread->int_field(_thread_status_offset);\n+  return static_cast<JavaThreadStatus>(java_thread->int_field(_thread_status_offset));\n@@ -1864,1 +1863,1 @@\n-  ThreadStatus status = (java_lang_Thread::ThreadStatus)java_thread->int_field(_thread_status_offset);\n+  JavaThreadStatus status = static_cast<JavaThreadStatus>(java_thread->int_field(_thread_status_offset));\n@@ -1866,9 +1865,9 @@\n-    case NEW                      : return \"NEW\";\n-    case RUNNABLE                 : return \"RUNNABLE\";\n-    case SLEEPING                 : return \"TIMED_WAITING (sleeping)\";\n-    case IN_OBJECT_WAIT           : return \"WAITING (on object monitor)\";\n-    case IN_OBJECT_WAIT_TIMED     : return \"TIMED_WAITING (on object monitor)\";\n-    case PARKED                   : return \"WAITING (parking)\";\n-    case PARKED_TIMED             : return \"TIMED_WAITING (parking)\";\n-    case BLOCKED_ON_MONITOR_ENTER : return \"BLOCKED (on object monitor)\";\n-    case TERMINATED               : return \"TERMINATED\";\n+    case JavaThreadStatus::NEW                      : return \"NEW\";\n+    case JavaThreadStatus::RUNNABLE                 : return \"RUNNABLE\";\n+    case JavaThreadStatus::SLEEPING                 : return \"TIMED_WAITING (sleeping)\";\n+    case JavaThreadStatus::IN_OBJECT_WAIT           : return \"WAITING (on object monitor)\";\n+    case JavaThreadStatus::IN_OBJECT_WAIT_TIMED     : return \"TIMED_WAITING (on object monitor)\";\n+    case JavaThreadStatus::PARKED                   : return \"WAITING (parking)\";\n+    case JavaThreadStatus::PARKED_TIMED             : return \"TIMED_WAITING (parking)\";\n+    case JavaThreadStatus::BLOCKED_ON_MONITOR_ENTER : return \"BLOCKED (on object monitor)\";\n+    case JavaThreadStatus::TERMINATED               : return \"TERMINATED\";\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":26,"deletions":27,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"jvmtifiles\/jvmti.h\"\n@@ -424,32 +423,1 @@\n-  \/\/ Java Thread Status for JVMTI and M&M use.\n-  \/\/ This thread status info is saved in threadStatus field of\n-  \/\/ java.lang.Thread java class.\n-  enum ThreadStatus {\n-    NEW                      = 0,\n-    RUNNABLE                 = JVMTI_THREAD_STATE_ALIVE +          \/\/ runnable \/ running\n-                               JVMTI_THREAD_STATE_RUNNABLE,\n-    SLEEPING                 = JVMTI_THREAD_STATE_ALIVE +          \/\/ Thread.sleep()\n-                               JVMTI_THREAD_STATE_WAITING +\n-                               JVMTI_THREAD_STATE_WAITING_WITH_TIMEOUT +\n-                               JVMTI_THREAD_STATE_SLEEPING,\n-    IN_OBJECT_WAIT           = JVMTI_THREAD_STATE_ALIVE +          \/\/ Object.wait()\n-                               JVMTI_THREAD_STATE_WAITING +\n-                               JVMTI_THREAD_STATE_WAITING_INDEFINITELY +\n-                               JVMTI_THREAD_STATE_IN_OBJECT_WAIT,\n-    IN_OBJECT_WAIT_TIMED     = JVMTI_THREAD_STATE_ALIVE +          \/\/ Object.wait(long)\n-                               JVMTI_THREAD_STATE_WAITING +\n-                               JVMTI_THREAD_STATE_WAITING_WITH_TIMEOUT +\n-                               JVMTI_THREAD_STATE_IN_OBJECT_WAIT,\n-    PARKED                   = JVMTI_THREAD_STATE_ALIVE +          \/\/ LockSupport.park()\n-                               JVMTI_THREAD_STATE_WAITING +\n-                               JVMTI_THREAD_STATE_WAITING_INDEFINITELY +\n-                               JVMTI_THREAD_STATE_PARKED,\n-    PARKED_TIMED             = JVMTI_THREAD_STATE_ALIVE +          \/\/ LockSupport.park(long)\n-                               JVMTI_THREAD_STATE_WAITING +\n-                               JVMTI_THREAD_STATE_WAITING_WITH_TIMEOUT +\n-                               JVMTI_THREAD_STATE_PARKED,\n-    BLOCKED_ON_MONITOR_ENTER = JVMTI_THREAD_STATE_ALIVE +          \/\/ (re-)entering a synchronization block\n-                               JVMTI_THREAD_STATE_BLOCKED_ON_MONITOR_ENTER,\n-    TERMINATED               = JVMTI_THREAD_STATE_TERMINATED\n-  };\n-  static void set_thread_status(oop java_thread_oop, ThreadStatus status);\n+  static void set_thread_status(oop java_thread_oop, JavaThreadStatus status);\n@@ -458,1 +426,1 @@\n-  static ThreadStatus get_thread_status(oop java_thread_oop);\n+  static JavaThreadStatus get_thread_status(oop java_thread_oop);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":2,"deletions":34,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2073,1 +2073,1 @@\n-  \/\/ Initialize basic classes\n+  \/\/ Resolve basic classes\n@@ -2075,0 +2075,4 @@\n+  \/\/ Resolve classes used by archived heap objects\n+  if (UseSharedSpaces) {\n+    HeapShared::resolve_classes(CHECK);\n+  }\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -453,0 +453,1 @@\n+  case vmIntrinsics::_base64_decodeBlock:\n@@ -711,1 +712,1 @@\n-#ifdef ASSERT\n+#ifndef PRODUCT\n@@ -729,1 +730,1 @@\n-  assert(((ID4(1021,1022,1023,15) >> shift) & mask) == 1021, \"\");\n+  assert(((ID4(1021,1022,1023,7) >> shift) & mask) == 1021, \"\");\n@@ -736,1 +737,1 @@\n-  assert(((ID4(1021,1022,1023,15) >> shift) & mask) == 1022, \"\");\n+  assert(((ID4(1021,1022,1023,7) >> shift) & mask) == 1022, \"\");\n@@ -743,1 +744,1 @@\n-  assert(((ID4(1021,1022,1023,15) >> shift) & mask) == 1023, \"\");\n+  assert(((ID4(1021,1022,1023,7) >> shift) & mask) == 1023, \"\");\n@@ -750,1 +751,1 @@\n-  assert(((ID4(1021,1022,1023,15) >> shift) & mask) == 15, \"\");\n+  assert(((ID4(1021,1022,1023,7) >> shift) & mask) == 7, \"\");\n@@ -753,1 +754,1 @@\n-#endif \/\/ ASSERT\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -340,0 +340,2 @@\n+  do_intrinsic(_Preconditions_checkLongIndex, jdk_internal_util_Preconditions, checkIndex_name, Preconditions_checkLongIndex_signature, F_S)   \\\n+   do_signature(Preconditions_checkLongIndex_signature,          \"(JJLjava\/util\/function\/BiFunction;)J\")                \\\n@@ -451,0 +453,6 @@\n+  \/* support for java.util.Base64.Decoder*\/                                                                             \\\n+  do_class(java_util_Base64_Decoder, \"java\/util\/Base64$Decoder\")                                                        \\\n+  do_intrinsic(_base64_decodeBlock, java_util_Base64_Decoder, decodeBlock_name, decodeBlock_signature, F_R)             \\\n+   do_name(decodeBlock_name, \"decodeBlock\")                                                                             \\\n+   do_signature(decodeBlock_signature, \"([BII[BIZ)I\")                                                                   \\\n+                                                                                                                        \\\n@@ -521,0 +529,4 @@\n+  do_intrinsic(_getObjectSize,   sun_instrument_InstrumentationImpl, getObjectSize_name, getObjectSize_signature, F_RN) \\\n+   do_name(     getObjectSize_name,                               \"getObjectSize0\")                                     \\\n+   do_alias(    getObjectSize_signature,                          long_object_long_signature)                           \\\n+                                                                                                                        \\\n@@ -1095,1 +1107,1 @@\n-#ifdef ASSERT\n+#ifndef PRODUCT\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -144,0 +144,1 @@\n+  template(sun_instrument_InstrumentationImpl,        \"sun\/instrument\/InstrumentationImpl\")       \\\n@@ -307,0 +308,1 @@\n+  template(jdk_internal_misc_Scoped_signature,               \"Ljdk\/internal\/misc\/ScopedMemoryAccess$Scoped;\") \\\n@@ -527,0 +529,1 @@\n+  template(long_object_long_signature,                \"(JLjava\/lang\/Object;)J\")                   \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,0 +40,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -306,2 +306,2 @@\n-  return (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier)) ||\n-         (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow)) ||\n+  return (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong)) ||\n+         (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong_narrow)) ||\n@@ -309,1 +309,2 @@\n-         (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow));\n+         (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow)) ||\n+         (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_phantom));\n@@ -549,2 +550,1 @@\n-    ShenandoahBarrierSet::AccessKind kind = ShenandoahBarrierSet::access_kind(decorators, type);\n-    load = new ShenandoahLoadReferenceBarrierNode(NULL, load, kind);\n+    load = new ShenandoahLoadReferenceBarrierNode(NULL, load, decorators);\n@@ -647,2 +647,1 @@\n-    ShenandoahBarrierSet::AccessKind kind = ShenandoahBarrierSet::access_kind(access.decorators(), access.type());\n-    load_store = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, load_store, kind));\n+    load_store = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, load_store, access.decorators()));\n@@ -716,2 +715,1 @@\n-    ShenandoahBarrierSet::AccessKind kind = ShenandoahBarrierSet::access_kind(access.decorators(), access.type());\n-    result = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, result, kind));\n+    result = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, result, access.decorators()));\n@@ -1065,1 +1063,1 @@\n-    \/\/ If one input is NULL, then step over the barriers normal LRB barriers on the other input\n+    \/\/ If one input is NULL, then step over the strong LRB barriers on the other input\n@@ -1068,1 +1066,1 @@\n-          ((ShenandoahLoadReferenceBarrierNode*)in2)->kind() != ShenandoahBarrierSet::AccessKind::NORMAL)) {\n+          !ShenandoahBarrierSet::is_strong_access(((ShenandoahLoadReferenceBarrierNode*)in2)->decorators()))) {\n@@ -1073,1 +1071,1 @@\n-          ((ShenandoahLoadReferenceBarrierNode*)in1)->kind() != ShenandoahBarrierSet::AccessKind::NORMAL)) {\n+          !ShenandoahBarrierSet::is_strong_access(((ShenandoahLoadReferenceBarrierNode*)in1)->decorators()))) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":10,"deletions":12,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2015, 2020, Red Hat, Inc. All rights reserved.\n@@ -52,1 +52,1 @@\n-    PhaseIdealLoop ideal_loop(igvn, LoopOptsShenandoahExpand);\n+    PhaseIdealLoop::optimize(igvn, LoopOptsShenandoahExpand);\n@@ -454,0 +454,3 @@\n+        \"decodeBlock\",\n+        { { TypeFunc::Parms, ShenandoahLoad },  { TypeFunc::Parms+3, ShenandoahStore },   { -1, ShenandoahNone },\n+          { -1,  ShenandoahNone},                 { -1,  ShenandoahNone},                 { -1,  ShenandoahNone} },\n@@ -959,1 +962,1 @@\n-                                               ShenandoahBarrierSet::AccessKind kind, PhaseIdealLoop* phase) {\n+                                               DecoratorSet decorators, PhaseIdealLoop* phase) {\n@@ -972,2 +975,18 @@\n-  switch (kind) {\n-    case ShenandoahBarrierSet::AccessKind::NATIVE:\n+  bool is_strong  = ShenandoahBarrierSet::is_strong_access(decorators);\n+  bool is_weak    = ShenandoahBarrierSet::is_weak_access(decorators);\n+  bool is_phantom = ShenandoahBarrierSet::is_phantom_access(decorators);\n+  bool is_native  = ShenandoahBarrierSet::is_native_access(decorators);\n+  bool is_narrow  = UseCompressedOops && !is_native;\n+  if (is_strong) {\n+    if (is_narrow) {\n+      calladdr = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong_narrow);\n+      name = \"load_reference_barrier_strong_narrow\";\n+    } else {\n+      calladdr = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong);\n+      name = \"load_reference_barrier_strong\";\n+    }\n+  } else if (is_weak) {\n+    if (is_narrow) {\n+      calladdr = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow);\n+      name = \"load_reference_barrier_weak_narrow\";\n+    } else {\n@@ -975,15 +994,6 @@\n-      name = \"load_reference_barrier_native\";\n-      break;\n-    case ShenandoahBarrierSet::AccessKind::WEAK:\n-      calladdr = LP64_ONLY(UseCompressedOops) NOT_LP64(false) ?\n-                 CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow) :\n-                 CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak);\n-      break;\n-    case ShenandoahBarrierSet::AccessKind::NORMAL:\n-      calladdr = LP64_ONLY(UseCompressedOops) NOT_LP64(false) ?\n-                 CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow) :\n-                 CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier);\n-      name = \"load_reference_barrier\";\n-      break;\n-    default:\n-      ShouldNotReachHere();\n+    }\n+  } else {\n+    assert(is_phantom, \"only remaining strength\");\n+    assert(!is_narrow, \"phantom access cannot be narrow\");\n+    calladdr = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_phantom);\n+    name = \"load_reference_barrier_phantom\";\n@@ -1354,1 +1364,1 @@\n-    if (lrb->kind() == ShenandoahBarrierSet::AccessKind::NORMAL) {\n+    if (ShenandoahBarrierSet::is_strong_access(lrb->decorators())) {\n@@ -1405,1 +1415,1 @@\n-    call_lrb_stub(ctrl, val, addr, result_mem, raw_mem, lrb->kind(), phase);\n+    call_lrb_stub(ctrl, val, addr, result_mem, raw_mem, lrb->decorators(), phase);\n@@ -2900,2 +2910,2 @@\n-ShenandoahLoadReferenceBarrierNode::ShenandoahLoadReferenceBarrierNode(Node* ctrl, Node* obj, ShenandoahBarrierSet::AccessKind kind)\n-: Node(ctrl, obj), _kind(kind) {\n+ShenandoahLoadReferenceBarrierNode::ShenandoahLoadReferenceBarrierNode(Node* ctrl, Node* obj, DecoratorSet decorators)\n+: Node(ctrl, obj), _decorators(decorators) {\n@@ -2905,2 +2915,2 @@\n-ShenandoahBarrierSet::AccessKind ShenandoahLoadReferenceBarrierNode::kind() const {\n-  return _kind;\n+DecoratorSet ShenandoahLoadReferenceBarrierNode::decorators() const {\n+  return _decorators;\n@@ -2913,0 +2923,4 @@\n+static DecoratorSet mask_decorators(DecoratorSet decorators) {\n+  return decorators & (ON_STRONG_OOP_REF | ON_WEAK_OOP_REF | ON_PHANTOM_OOP_REF | ON_UNKNOWN_OOP_REF | IN_NATIVE);\n+}\n+\n@@ -2915,13 +2929,1 @@\n-  switch (_kind) {\n-    case ShenandoahBarrierSet::AccessKind::NORMAL:\n-      hash += 0;\n-      break;\n-    case ShenandoahBarrierSet::AccessKind::WEAK:\n-      hash += 1;\n-      break;\n-    case ShenandoahBarrierSet::AccessKind::NATIVE:\n-      hash += 2;\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-  }\n+  hash += mask_decorators(_decorators);\n@@ -2933,1 +2935,1 @@\n-         _kind == ((const ShenandoahLoadReferenceBarrierNode&)n)._kind;\n+         mask_decorators(_decorators) == mask_decorators(((const ShenandoahLoadReferenceBarrierNode&)n)._decorators);\n@@ -2945,1 +2947,1 @@\n-  if (kind() == ShenandoahBarrierSet::AccessKind::NORMAL) {\n+  if (ShenandoahBarrierSet::is_strong_access(decorators())) {\n@@ -2961,1 +2963,1 @@\n-  if (kind() == ShenandoahBarrierSet::AccessKind::NORMAL) {\n+  if (ShenandoahBarrierSet::is_strong_access(decorators())) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":44,"deletions":42,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -36,12 +36,0 @@\n-public:\n-  enum class AccessKind {\n-    \/\/ Regular in-heap access on reference fields\n-    NORMAL,\n-\n-    \/\/ Off-heap reference access\n-    NATIVE,\n-\n-    \/\/ In-heap reference access on referent fields of j.l.r.Reference objects\n-    WEAK\n-  };\n-\n@@ -68,1 +56,16 @@\n-  static AccessKind access_kind(DecoratorSet decorators, BasicType type);\n+\n+  static bool is_strong_access(DecoratorSet decorators) {\n+    return (decorators & (ON_WEAK_OOP_REF | ON_PHANTOM_OOP_REF | ON_UNKNOWN_OOP_REF)) == 0;\n+  }\n+\n+  static bool is_weak_access(DecoratorSet decorators) {\n+    return (decorators & (ON_WEAK_OOP_REF | ON_UNKNOWN_OOP_REF)) != 0;\n+  }\n+\n+  static bool is_phantom_access(DecoratorSet decorators) {\n+    return (decorators & ON_PHANTOM_OOP_REF) != 0;\n+  }\n+\n+  static bool is_native_access(DecoratorSet decorators) {\n+    return (decorators & IN_NATIVE) != 0;\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":16,"deletions":13,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -105,2 +105,2 @@\n-  \/\/ Prevent resurrection of unreachable non-strorg references.\n-  if (!HasDecorator<decorators, ON_STRONG_OOP_REF>::value && obj != NULL &&\n+  \/\/ Prevent resurrection of unreachable phantom (i.e. weak-native) references.\n+  if (HasDecorator<decorators, ON_PHANTOM_OOP_REF>::value && obj != NULL &&\n@@ -112,0 +112,7 @@\n+  \/\/ Prevent resurrection of unreachable weak references.\n+  if ((HasDecorator<decorators, ON_WEAK_OOP_REF>::value || HasDecorator<decorators, ON_UNKNOWN_OOP_REF>::value) &&\n+      obj != NULL && _heap->is_concurrent_weak_root_in_progress() &&\n+      !_heap->marking_context()->is_marked_strong(obj)) {\n+    return NULL;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreter.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -310,0 +310,1 @@\n+  static_field(StubRoutines,                _base64_decodeBlock,                              address)                               \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -121,0 +121,1 @@\n+  LOG_TAG(nmt) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"oops\/objArrayOop.hpp\"\n@@ -52,0 +53,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n@@ -53,0 +55,2 @@\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"runtime\/init.hpp\"\n@@ -108,0 +112,4 @@\n+GrowableArrayCHeap<oop, mtClassShared>* HeapShared::_pending_roots = NULL;\n+narrowOop HeapShared::_roots_narrow;\n+OopHandle HeapShared::_roots;\n+\n@@ -117,0 +125,7 @@\n+  if (is_mapped()) {\n+    _roots = OopHandle(Universe::vm_global(), decode_from_archive(_roots_narrow));\n+    if (!MetaspaceShared::use_full_module_graph()) {\n+      \/\/ Need to remove all the archived java.lang.Module objects from HeapShared::roots().\n+      ClassLoaderDataShared::clear_archived_oops();\n+    }\n+  }\n@@ -169,0 +184,64 @@\n+int HeapShared::append_root(oop obj) {\n+  assert(DumpSharedSpaces, \"dump-time only\");\n+\n+  \/\/ No GC should happen since we aren't scanning _pending_roots.\n+  assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n+\n+  if (_pending_roots == NULL) {\n+    _pending_roots = new GrowableArrayCHeap<oop, mtClassShared>(500);\n+  }\n+\n+  return _pending_roots->append(obj);\n+}\n+\n+objArrayOop HeapShared::roots() {\n+  if (DumpSharedSpaces) {\n+    assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n+    if (!is_heap_object_archiving_allowed()) {\n+      return NULL;\n+    }\n+  } else {\n+    assert(UseSharedSpaces, \"must be\");\n+  }\n+\n+  objArrayOop roots = (objArrayOop)_roots.resolve();\n+  assert(roots != NULL, \"should have been initialized\");\n+  return roots;\n+}\n+\n+void HeapShared::set_roots(narrowOop roots) {\n+  assert(UseSharedSpaces, \"runtime only\");\n+  assert(open_archive_heap_region_mapped(), \"must be\");\n+  _roots_narrow = roots;\n+}\n+\n+\/\/ Returns an objArray that contains all the roots of the archived objects\n+oop HeapShared::get_root(int index, bool clear) {\n+  assert(index >= 0, \"sanity\");\n+  if (DumpSharedSpaces) {\n+    assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n+    assert(_pending_roots != NULL, \"sanity\");\n+    return _pending_roots->at(index);\n+  } else {\n+    assert(UseSharedSpaces, \"must be\");\n+    assert(!_roots.is_empty(), \"must have loaded shared heap\");\n+    oop result = roots()->obj_at(index);\n+    if (clear) {\n+      clear_root(index);\n+    }\n+    return result;\n+  }\n+}\n+\n+void HeapShared::clear_root(int index) {\n+  assert(index >= 0, \"sanity\");\n+  assert(UseSharedSpaces, \"must be\");\n+  if (open_archive_heap_region_mapped()) {\n+    if (log_is_enabled(Debug, cds, heap)) {\n+      oop old = roots()->obj_at(index);\n+      log_debug(cds, heap)(\"Clearing root %d: was \" PTR_FORMAT, index, p2i(old));\n+    }\n+    roots()->obj_at_put(index, NULL);\n+  }\n+}\n+\n@@ -204,2 +283,5 @@\n-    log_debug(cds, heap)(\"Archived heap object \" PTR_FORMAT \" ==> \" PTR_FORMAT,\n-                         p2i(obj), p2i(archived_oop));\n+    if (log_is_enabled(Debug, cds, heap)) {\n+      ResourceMark rm;\n+      log_debug(cds, heap)(\"Archived heap object \" PTR_FORMAT \" ==> \" PTR_FORMAT \" : %s\",\n+                           p2i(obj), p2i(archived_oop), obj->klass()->external_name());\n+    }\n@@ -217,10 +299,0 @@\n-oop HeapShared::materialize_archived_object(narrowOop v) {\n-  assert(archive_heap_region_fixed(),\n-         \"must be called after archive heap regions are fixed\");\n-  if (!CompressedOops::is_null(v)) {\n-    oop obj = HeapShared::decode_from_archive(v);\n-    return G1CollectedHeap::heap()->materialize_archived_object(obj);\n-  }\n-  return NULL;\n-}\n-\n@@ -263,2 +335,2 @@\n-void HeapShared::archive_java_heap_objects(GrowableArray<MemRegion> *closed,\n-                                           GrowableArray<MemRegion> *open) {\n+void HeapShared::archive_java_heap_objects(GrowableArray<MemRegion>* closed,\n+                                           GrowableArray<MemRegion>* open) {\n@@ -280,4 +352,0 @@\n-    if (MetaspaceShared::use_full_module_graph()) {\n-      ClassLoaderDataShared::init_archived_oops();\n-    }\n-\n@@ -332,0 +400,1 @@\n+    ClassLoaderDataShared::init_archived_oops();\n@@ -334,0 +403,2 @@\n+  copy_roots();\n+\n@@ -338,0 +409,29 @@\n+\/\/ Copy _pending_archive_roots into an objArray\n+void HeapShared::copy_roots() {\n+  int length = _pending_roots != NULL ? _pending_roots->length() : 0;\n+  int size = objArrayOopDesc::object_size(length);\n+  Klass* k = Universe::objectArrayKlassObj(); \/\/ already relocated to point to archived klass\n+  HeapWord* mem = G1CollectedHeap::heap()->archive_mem_allocate(size);\n+\n+  memset(mem, 0, size * BytesPerWord);\n+  {\n+    \/\/ This is copied from MemAllocator::finish\n+    if (UseBiasedLocking) {\n+      oopDesc::set_mark(mem, k->prototype_header());\n+    } else {\n+      oopDesc::set_mark(mem, markWord::prototype());\n+    }\n+    oopDesc::release_set_klass(mem, k);\n+  }\n+  {\n+    \/\/ This is copied from ObjArrayAllocator::initialize\n+    arrayOopDesc::set_length(mem, length);\n+  }\n+\n+  _roots = OopHandle(Universe::vm_global(), (oop)mem);\n+  for (int i = 0; i < length; i++) {\n+    roots()->obj_at_put(i, _pending_roots->at(i));\n+  }\n+  log_info(cds)(\"archived obj roots[%d] = %d words, klass = %p, obj = %p\", length, size, k, mem);\n+}\n+\n@@ -377,1 +477,1 @@\n-      new(ResourceObj::C_HEAP, mtClass) GrowableArray<juint>(10, mtClass);\n+      new(ResourceObj::C_HEAP, mtClass) GrowableArray<int>(10, mtClass);\n@@ -379,3 +479,2 @@\n-  _subgraph_entry_fields->append((juint)static_field_offset);\n-  _subgraph_entry_fields->append(CompressedOops::narrow_oop_value(v));\n-  _subgraph_entry_fields->append(is_closed_archive ? 1 : 0);\n+  _subgraph_entry_fields->append(static_field_offset);\n+  _subgraph_entry_fields->append(HeapShared::append_root(v));\n@@ -386,1 +485,1 @@\n-void KlassSubGraphInfo::add_subgraph_object_klass(Klass* orig_k, Klass *relocated_k) {\n+void KlassSubGraphInfo::add_subgraph_object_klass(Klass* orig_k, Klass* relocated_k) {\n@@ -440,0 +539,18 @@\n+  _has_non_early_klasses |= is_non_early_klass(orig_k);\n+}\n+\n+bool KlassSubGraphInfo::is_non_early_klass(Klass* k) {\n+  if (k->is_objArray_klass()) {\n+    k = ObjArrayKlass::cast(k)->bottom_klass();\n+  }\n+  if (k->is_instance_klass()) {\n+    if (!SystemDictionaryShared::is_early_klass(InstanceKlass::cast(k))) {\n+      ResourceMark rm;\n+      log_info(cds, heap)(\"non-early: %s\", k->external_name());\n+      return true;\n+    } else {\n+      return false;\n+    }\n+  } else {\n+    return false;\n+  }\n@@ -448,0 +565,8 @@\n+  _has_non_early_klasses = info->has_non_early_klasses();\n+\n+  if (_has_non_early_klasses) {\n+    ResourceMark rm;\n+    log_info(cds, heap)(\n+          \"Subgraph of klass %s has non-early klasses and cannot be used when JVMTI ClassFileLoadHook is enabled\",\n+          _k->external_name());\n+  }\n@@ -450,1 +575,1 @@\n-  GrowableArray<juint>* entry_fields = info->subgraph_entry_fields();\n+  GrowableArray<int>* entry_fields = info->subgraph_entry_fields();\n@@ -453,1 +578,1 @@\n-    assert(num_entry_fields % 3 == 0, \"sanity\");\n+    assert(num_entry_fields % 2 == 0, \"sanity\");\n@@ -455,1 +580,1 @@\n-      MetaspaceShared::new_ro_array<juint>(num_entry_fields);\n+      MetaspaceShared::new_ro_array<int>(num_entry_fields);\n@@ -528,0 +653,62 @@\n+static void verify_the_heap(Klass* k, const char* which) {\n+  if (VerifyArchivedFields) {\n+    ResourceMark rm;\n+    log_info(cds, heap)(\"Verify heap %s initializing static field(s) in %s\",\n+                        which, k->external_name());\n+    VM_Verify verify_op;\n+    VMThread::execute(&verify_op);\n+    if (!FLAG_IS_DEFAULT(VerifyArchivedFields)) {\n+      \/\/ If VerifyArchivedFields has a non-default value (e.g., specified on the command-line), do\n+      \/\/ more expensive checks.\n+      if (is_init_completed()) {\n+        FlagSetting fs1(VerifyBeforeGC, true);\n+        FlagSetting fs2(VerifyDuringGC, true);\n+        FlagSetting fs3(VerifyAfterGC,  true);\n+        Universe::heap()->collect(GCCause::_java_lang_system_gc);\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ Before GC can execute, we must ensure that all oops reachable from HeapShared::roots()\n+\/\/ have a valid klass. I.e., oopDesc::klass() must have already been resolved.\n+\/\/\n+\/\/ Note: if a ArchivedKlassSubGraphInfoRecord contains non-early classes, and JVMTI\n+\/\/ ClassFileLoadHook is enabled, it's possible for this class to be dynamically replaced. In\n+\/\/ this case, we will not load the ArchivedKlassSubGraphInfoRecord and will clear its roots.\n+void HeapShared::resolve_classes(TRAPS) {\n+  if (!is_mapped()) {\n+    return; \/\/ nothing to do\n+  }\n+  resolve_classes_for_subgraphs(closed_archive_subgraph_entry_fields,\n+                                num_closed_archive_subgraph_entry_fields,\n+                                CHECK);\n+  resolve_classes_for_subgraphs(open_archive_subgraph_entry_fields,\n+                                num_open_archive_subgraph_entry_fields,\n+                                CHECK);\n+  resolve_classes_for_subgraphs(fmg_open_archive_subgraph_entry_fields,\n+                                num_fmg_open_archive_subgraph_entry_fields,\n+                                CHECK);\n+}\n+\n+void HeapShared::resolve_classes_for_subgraphs(ArchivableStaticFieldInfo fields[],\n+                                               int num, TRAPS) {\n+  for (int i = 0; i < num; i++) {\n+    ArchivableStaticFieldInfo* info = &fields[i];\n+    TempNewSymbol klass_name = SymbolTable::new_symbol(info->klass_name);\n+    InstanceKlass* k = SystemDictionaryShared::find_builtin_class(klass_name);\n+    assert(k != NULL && k->is_shared_boot_class(), \"sanity\");\n+    resolve_classes_for_subgraph_of(k, CHECK);\n+  }\n+}\n+\n+void HeapShared::resolve_classes_for_subgraph_of(Klass* k, TRAPS) {\n+ const ArchivedKlassSubGraphInfoRecord* record = resolve_or_init_classes_for_subgraph_of(k, \/*do_init=*\/false, THREAD);\n+ if (HAS_PENDING_EXCEPTION) {\n+   CLEAR_PENDING_EXCEPTION;\n+ }\n+ if (record == NULL) {\n+   clear_archived_roots_of(k);\n+ }\n+}\n+\n@@ -529,1 +716,1 @@\n-  if (!open_archive_heap_region_mapped()) {\n+  if (!is_mapped()) {\n@@ -532,0 +719,19 @@\n+\n+  const ArchivedKlassSubGraphInfoRecord* record =\n+    resolve_or_init_classes_for_subgraph_of(k, \/*do_init=*\/true, THREAD);\n+\n+  if (HAS_PENDING_EXCEPTION) {\n+    CLEAR_PENDING_EXCEPTION;\n+    \/\/ None of the field value will be set if there was an exception when initializing the classes.\n+    \/\/ The java code will not see any of the archived objects in the\n+    \/\/ subgraphs referenced from k in this case.\n+    return;\n+  }\n+\n+  if (record != NULL) {\n+    init_archived_fields_for(k, record, THREAD);\n+  }\n+}\n+\n+const ArchivedKlassSubGraphInfoRecord*\n+HeapShared::resolve_or_init_classes_for_subgraph_of(Klass* k, bool do_init, TRAPS) {\n@@ -541,1 +747,6 @@\n-      return;\n+      if (log_is_enabled(Info, cds, heap)) {\n+        ResourceMark rm;\n+        log_info(cds, heap)(\"subgraph %s cannot be used because full module graph is disabled\",\n+                            k->external_name());\n+      }\n+      return NULL;\n@@ -544,1 +755,11 @@\n-    int i;\n+    if (record->has_non_early_klasses() && JvmtiExport::should_post_class_file_load_hook()) {\n+      if (log_is_enabled(Info, cds, heap)) {\n+        ResourceMark rm;\n+        log_info(cds, heap)(\"subgraph %s cannot be used because JVMTI ClassFileLoadHook is enabled\",\n+                            k->external_name());\n+      }\n+      return NULL;\n+    }\n+\n+    resolve_or_init(k, do_init, CHECK_NULL);\n+\n@@ -549,19 +770,2 @@\n-      for (i = 0; i < klasses->length(); i++) {\n-        Klass* obj_k = klasses->at(i);\n-        Klass* resolved_k = SystemDictionary::resolve_or_null(\n-                                              (obj_k)->name(), THREAD);\n-        if (resolved_k != obj_k) {\n-          assert(!SystemDictionary::is_well_known_klass(resolved_k),\n-                 \"shared well-known classes must not be replaced by JVMTI ClassFileLoadHook\");\n-          ResourceMark rm(THREAD);\n-          log_info(cds, heap)(\"Failed to load subgraph because %s was not loaded from archive\",\n-                              resolved_k->external_name());\n-          return;\n-        }\n-        if ((obj_k)->is_instance_klass()) {\n-          InstanceKlass* ik = InstanceKlass::cast(obj_k);\n-          ik->initialize(THREAD);\n-        } else if ((obj_k)->is_objArray_klass()) {\n-          ObjArrayKlass* oak = ObjArrayKlass::cast(obj_k);\n-          oak->initialize(THREAD);\n-        }\n+      for (int i = 0; i < klasses->length(); i++) {\n+        resolve_or_init(klasses->at(i), do_init, CHECK_NULL);\n@@ -570,0 +774,1 @@\n+  }\n@@ -571,6 +776,45 @@\n-    if (HAS_PENDING_EXCEPTION) {\n-      CLEAR_PENDING_EXCEPTION;\n-      \/\/ None of the field value will be set if there was an exception.\n-      \/\/ The java code will not see any of the archived objects in the\n-      \/\/ subgraphs referenced from k in this case.\n-      return;\n+  return record;\n+}\n+\n+void HeapShared::resolve_or_init(Klass* k, bool do_init, TRAPS) {\n+  if (!do_init) {\n+    if (k->class_loader_data() == NULL) {\n+      Klass* resolved_k = SystemDictionary::resolve_or_null(k->name(), CHECK);\n+      assert(resolved_k == k, \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+    }\n+  } else {\n+    assert(k->class_loader_data() != NULL, \"must have been resolved by HeapShared::resolve_classes\");\n+    if (k->is_instance_klass()) {\n+      InstanceKlass* ik = InstanceKlass::cast(k);\n+      ik->initialize(CHECK);\n+    } else if (k->is_objArray_klass()) {\n+      ObjArrayKlass* oak = ObjArrayKlass::cast(k);\n+      oak->initialize(CHECK);\n+    }\n+  }\n+}\n+\n+void HeapShared::init_archived_fields_for(Klass* k, const ArchivedKlassSubGraphInfoRecord* record, TRAPS) {\n+  verify_the_heap(k, \"before\");\n+\n+  \/\/ Load the subgraph entry fields from the record and store them back to\n+  \/\/ the corresponding fields within the mirror.\n+  oop m = k->java_mirror();\n+  Array<int>* entry_field_records = record->entry_field_records();\n+  if (entry_field_records != NULL) {\n+    int efr_len = entry_field_records->length();\n+    assert(efr_len % 2 == 0, \"sanity\");\n+    for (int i = 0; i < efr_len; i += 2) {\n+      int field_offset = entry_field_records->at(i);\n+      int root_index = entry_field_records->at(i+1);\n+      oop v = get_root(root_index, \/*clear=*\/true);\n+      m->obj_field_put(field_offset, v);\n+      log_debug(cds, heap)(\"  \" PTR_FORMAT \" init field @ %2d = \" PTR_FORMAT, p2i(k), field_offset, p2i(v));\n+    }\n+\n+    \/\/ Done. Java code can see the archived sub-graphs referenced from k's\n+    \/\/ mirror after this point.\n+    if (log_is_enabled(Info, cds, heap)) {\n+      ResourceMark rm;\n+      log_info(cds, heap)(\"initialize_from_archived_subgraph %s \" PTR_FORMAT \"%s\",\n+                          k->external_name(), p2i(k), JvmtiExport::is_early_phase() ? \" (early)\" : \"\");\n@@ -578,0 +822,4 @@\n+  }\n+\n+  verify_the_heap(k, \"after \");\n+}\n@@ -579,4 +827,5 @@\n-    \/\/ Load the subgraph entry fields from the record and store them back to\n-    \/\/ the corresponding fields within the mirror.\n-    oop m = k->java_mirror();\n-    Array<juint>* entry_field_records = record->entry_field_records();\n+void HeapShared::clear_archived_roots_of(Klass* k) {\n+  unsigned int hash = SystemDictionaryShared::hash_for_shared_dictionary(k);\n+  const ArchivedKlassSubGraphInfoRecord* record = _run_time_subgraph_info_table.lookup(k, hash, 0);\n+  if (record != NULL) {\n+    Array<int>* entry_field_records = record->entry_field_records();\n@@ -585,29 +834,4 @@\n-      assert(efr_len % 3 == 0, \"sanity\");\n-      for (i = 0; i < efr_len;) {\n-        int field_offset = entry_field_records->at(i);\n-        narrowOop nv = CompressedOops::narrow_oop_cast(entry_field_records->at(i+1));\n-        int is_closed_archive = entry_field_records->at(i+2);\n-        oop v;\n-        if (is_closed_archive == 0) {\n-          \/\/ It's an archived object in the open archive heap regions, not shared.\n-          \/\/ The object refereced by the field becomes 'known' by GC from this\n-          \/\/ point. All objects in the subgraph reachable from the object are\n-          \/\/ also 'known' by GC.\n-          v = materialize_archived_object(nv);\n-        } else {\n-          \/\/ Shared object in the closed archive heap regions. Decode directly.\n-          assert(!CompressedOops::is_null(nv), \"shared object is null\");\n-          v = HeapShared::decode_from_archive(nv);\n-        }\n-        m->obj_field_put(field_offset, v);\n-        i += 3;\n-\n-        log_debug(cds, heap)(\"  \" PTR_FORMAT \" init field @ %2d = \" PTR_FORMAT, p2i(k), field_offset, p2i(v));\n-      }\n-\n-      \/\/ Done. Java code can see the archived sub-graphs referenced from k's\n-      \/\/ mirror after this point.\n-      if (log_is_enabled(Info, cds, heap)) {\n-        ResourceMark rm;\n-        log_info(cds, heap)(\"initialize_from_archived_subgraph %s \" PTR_FORMAT,\n-                            k->external_name(), p2i(k));\n+      assert(efr_len % 2 == 0, \"sanity\");\n+      for (int i = 0; i < efr_len; i += 2) {\n+        int root_index = entry_field_records->at(i+1);\n+        clear_root(root_index);\n","filename":"src\/hotspot\/share\/memory\/heapShared.cpp","additions":311,"deletions":87,"binary":false,"changes":398,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n@@ -509,1 +510,1 @@\n-  CDS_JAVA_HEAP_ONLY(ClassLoaderDataShared::serialize(soc));\n+  CDS_JAVA_HEAP_ONLY(ClassLoaderDataShared::serialize(soc);)\n@@ -1481,2 +1482,0 @@\n-\n-          disable_full_module_graph(); \/\/ Disabled temporarily for JDK-8253081\n@@ -1752,0 +1751,1 @@\n+    dynamic_mapinfo->unmap_region(MetaspaceShared::bm);\n","filename":"src\/hotspot\/share\/memory\/metaspaceShared.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n@@ -377,0 +378,1 @@\n+      _cache->clear_archived_references();\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -783,1 +783,1 @@\n-  if (CompressedOops::is_null(_archived_references)) {\n+  if (_archived_references_index < 0) {\n@@ -786,1 +786,8 @@\n-  return HeapShared::materialize_archived_object(_archived_references);\n+  return HeapShared::get_root(_archived_references_index);\n+}\n+\n+void ConstantPoolCache::clear_archived_references() {\n+  if (_archived_references_index >= 0) {\n+    HeapShared::clear_root(_archived_references_index);\n+    _archived_references_index = -1;\n+  }\n@@ -791,1 +798,1 @@\n-  _archived_references = CompressedOops::encode(o);\n+  _archived_references_index = HeapShared::append_root(o);\n","filename":"src\/hotspot\/share\/oops\/cpCache.cpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -430,1 +430,1 @@\n-  CDS_JAVA_HEAP_ONLY(narrowOop _archived_references;)\n+  CDS_JAVA_HEAP_ONLY(int _archived_references_index;)\n@@ -457,0 +457,1 @@\n+  void clear_archived_references() NOT_CDS_JAVA_HEAP_RETURN;\n","filename":"src\/hotspot\/share\/oops\/cpCache.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n@@ -207,1 +208,1 @@\n-  CDS_JAVA_HEAP_ONLY(_archived_mirror = narrowOop::null;)\n+  CDS_JAVA_HEAP_ONLY(_archived_mirror_index = -1;)\n@@ -576,1 +577,0 @@\n-    \/\/ Restore class_loader_data to the null class loader data\n@@ -579,1 +579,1 @@\n-    \/\/ Add to null class loader list first before creating the mirror\n+    \/\/ Add to class loader list first before creating the mirror\n@@ -600,1 +600,1 @@\n-  if (this->has_raw_archived_mirror()) {\n+  if (this->has_archived_mirror_index()) {\n@@ -615,1 +615,1 @@\n-    this->clear_has_raw_archived_mirror();\n+    this->clear_archived_mirror_index();\n@@ -628,4 +628,3 @@\n-\/\/ Used at CDS dump time to access the archived mirror. No GC barrier.\n-oop Klass::archived_java_mirror_raw() {\n-  assert(has_raw_archived_mirror(), \"must have raw archived mirror\");\n-  return CompressedOops::decode(_archived_mirror);\n+oop Klass::archived_java_mirror() {\n+  assert(has_archived_mirror_index(), \"must have archived mirror\");\n+  return HeapShared::get_root(_archived_mirror_index);\n@@ -634,3 +633,5 @@\n-narrowOop Klass::archived_java_mirror_raw_narrow() {\n-  assert(has_raw_archived_mirror(), \"must have raw archived mirror\");\n-  return _archived_mirror;\n+void Klass::clear_archived_mirror_index() {\n+  if (_archived_mirror_index >= 0) {\n+    HeapShared::clear_root(_archived_mirror_index);\n+  }\n+  _archived_mirror_index = -1;\n@@ -640,1 +641,1 @@\n-void Klass::set_archived_java_mirror_raw(oop m) {\n+void Klass::set_archived_java_mirror(oop m) {\n@@ -642,1 +643,1 @@\n-  _archived_mirror = CompressedOops::encode(m);\n+  _archived_mirror_index = HeapShared::append_root(m);\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":15,"deletions":14,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -180,1 +180,0 @@\n-    _has_raw_archived_mirror = 1,\n@@ -185,3 +184,1 @@\n-  \/\/ The _archived_mirror is set at CDS dump time pointing to the cached mirror\n-  \/\/ in the open archive heap region when archiving java object is supported.\n-  CDS_JAVA_HEAP_ONLY(narrowOop _archived_mirror;)\n+  CDS_JAVA_HEAP_ONLY(int _archived_mirror_index;)\n@@ -266,3 +263,2 @@\n-  oop archived_java_mirror_raw() NOT_CDS_JAVA_HEAP_RETURN_(NULL); \/\/ no GC barrier\n-  narrowOop archived_java_mirror_raw_narrow() NOT_CDS_JAVA_HEAP_RETURN_(narrowOop::null); \/\/ no GC barrier\n-  void set_archived_java_mirror_raw(oop m) NOT_CDS_JAVA_HEAP_RETURN; \/\/ no GC barrier\n+  oop archived_java_mirror() NOT_CDS_JAVA_HEAP_RETURN_(NULL);\n+  void set_archived_java_mirror(oop m) NOT_CDS_JAVA_HEAP_RETURN;\n@@ -311,9 +307,3 @@\n-  void set_has_raw_archived_mirror() {\n-    CDS_ONLY(_shared_class_flags |= _has_raw_archived_mirror;)\n-  }\n-  void clear_has_raw_archived_mirror() {\n-    CDS_ONLY(_shared_class_flags &= ~_has_raw_archived_mirror;)\n-  }\n-  bool has_raw_archived_mirror() const {\n-    CDS_ONLY(return (_shared_class_flags & _has_raw_archived_mirror) != 0;)\n-    NOT_CDS(return false;)\n+  bool has_archived_mirror_index() const {\n+    CDS_JAVA_HEAP_ONLY(return _archived_mirror_index >= 0;)\n+    NOT_CDS_JAVA_HEAP(return false);\n@@ -322,0 +312,2 @@\n+  void clear_archived_mirror_index() NOT_CDS_JAVA_HEAP_RETURN;\n+\n@@ -557,1 +549,1 @@\n-    if (has_raw_archived_mirror()) {\n+    if (has_archived_mirror_index()) {\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":9,"deletions":17,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -97,1 +97,2 @@\n-    _c2_needs_stack_repair = 1 << 10\n+    _c2_needs_stack_repair = 1 << 10,\n+    _scoped                = 1 << 11\n@@ -928,0 +929,8 @@\n+  bool is_scoped() const {\n+    return (_flags & _scoped) != 0;\n+  }\n+\n+  void set_scoped(bool x) {\n+    _flags = x ? (_flags | _scoped) : (_flags & ~_scoped);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -96,1 +96,1 @@\n-          range(0, max_jint)                                                \\\n+          range(1, max_jint)                                                \\\n@@ -100,1 +100,1 @@\n-          range(0, max_jint)                                                \\\n+          range(1, max_jint)                                                \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -645,0 +645,1 @@\n+  case vmIntrinsics::_base64_decodeBlock:\n@@ -655,0 +656,2 @@\n+  case vmIntrinsics::_Preconditions_checkLongIndex:\n+  case vmIntrinsics::_getObjectSize:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -71,2 +71,2 @@\n-void StartNode::calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const {\n-  Matcher::calling_convention( sig_bt, parm_regs, argcnt, false );\n+void StartNode::calling_convention(BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt) const {\n+  SharedRuntime::java_calling_convention(sig_bt, parm_regs, argcnt);\n@@ -710,1 +710,1 @@\n-  Matcher::calling_convention( sig_bt, parm_regs, argcnt, true );\n+  SharedRuntime::java_calling_convention(sig_bt, parm_regs, argcnt);\n@@ -724,1 +724,1 @@\n-        OptoRegPair regs = match->c_return_value(ideal_reg,true);\n+        OptoRegPair regs = match->c_return_value(ideal_reg);\n@@ -1318,1 +1318,1 @@\n-void CallRuntimeNode::calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const {\n+void CallRuntimeNode::calling_convention(BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt) const {\n@@ -1326,1 +1326,1 @@\n-  Matcher::c_calling_convention( sig_bt, parm_regs, argcnt );\n+  SharedRuntime::c_calling_convention(sig_bt, parm_regs, \/*regs2=*\/nullptr, argcnt);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -102,0 +102,5 @@\n+  case Op_CastLL: {\n+    Node* cast = new CastLLNode(n, t, carry_dependency);\n+    cast->set_req(0, c);\n+    return cast;\n+  }\n@@ -114,0 +119,14 @@\n+Node* ConstraintCastNode::make(Node* c, Node *n, const Type *t, BasicType bt) {\n+  switch(bt) {\n+  case T_INT: {\n+    return make_cast(Op_CastII, c, n, t, false);\n+  }\n+  case T_LONG: {\n+    return make_cast(Op_CastLL, c, n, t, false);\n+  }\n+  default:\n+    fatal(\"Bad basic type %s\", type2name(bt));\n+  }\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/castnode.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+  static Node* make(Node* c, Node *n, const Type *t, BasicType bt);\n@@ -95,0 +96,10 @@\n+class CastLLNode: public ConstraintCastNode {\n+public:\n+  CastLLNode(Node* n, const Type* t, bool carry_dependency = false)\n+          : ConstraintCastNode(n, t, carry_dependency){\n+    init_class_id(Class_CastLL);\n+  }\n+  virtual int Opcode() const;\n+  virtual uint ideal_reg() const { return Op_RegL; }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/castnode.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1116,1 +1116,1 @@\n-              return TypeInt::make(MIN2(lo->_lo, hi->_lo) , hi->_hi, 3);\n+              return TypeInt::make(MIN2(lo->_lo, hi->_lo) , hi->_hi, 3)->filter_speculative(_type);\n@@ -1118,1 +1118,1 @@\n-              return TypeInt::make(lo->_lo, MAX2(lo->_hi, hi->_hi), 3);\n+              return TypeInt::make(lo->_lo, MAX2(lo->_hi, hi->_hi), 3)->filter_speculative(_type);\n@@ -2002,1 +2002,1 @@\n-      assert(phi_type->isa_int() || phi_type->isa_ptr(), \"bad phi type\");\n+      assert(phi_type->isa_int() || phi_type->isa_ptr() || phi_type->isa_long(), \"bad phi type\");\n@@ -2008,0 +2008,2 @@\n+      } else if (phi_type->isa_long()) {\n+        cast = ConstraintCastNode::make_cast(Op_CastLL, r, uin, phi_type, true);\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -64,0 +64,1 @@\n+macro(CastLL)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -1055,1 +1056,3 @@\n-  uint in_preserve_stack_slots();\n+  uint in_preserve_stack_slots() {\n+    return SharedRuntime::in_preserve_stack_slots();\n+  }\n@@ -1060,3 +1063,3 @@\n-  \/\/ On Sparc this describes the words reserved for storing a register window\n-  \/\/ when an interrupt occurs.\n-  static uint out_preserve_stack_slots();\n+  static uint out_preserve_stack_slots() {\n+    return SharedRuntime::out_preserve_stack_slots();\n+  }\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1107,0 +1107,1 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"decodeBlock\") == 0 ||\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -115,0 +115,7 @@\n+  Node* integercon(jlong con, BasicType bt)   const {\n+    if (bt == T_INT) {\n+      return intcon(checked_cast<jint>(con));\n+    }\n+    assert(bt == T_LONG, \"basic type not an int or long\");\n+    return longcon(con);\n+  }\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -491,1 +491,2 @@\n-  case vmIntrinsics::_Preconditions_checkIndex: return inline_preconditions_checkIndex();\n+  case vmIntrinsics::_Preconditions_checkIndex: return inline_preconditions_checkIndex(T_INT);\n+  case vmIntrinsics::_Preconditions_checkLongIndex: return inline_preconditions_checkIndex(T_LONG);\n@@ -582,0 +583,2 @@\n+  case vmIntrinsics::_base64_decodeBlock:\n+    return inline_base64_decodeBlock();\n@@ -667,0 +670,3 @@\n+  case vmIntrinsics::_getObjectSize:\n+    return inline_getObjectSize();\n+\n@@ -1004,1 +1010,1 @@\n-bool LibraryCallKit::inline_preconditions_checkIndex() {\n+bool LibraryCallKit::inline_preconditions_checkIndex(BasicType bt) {\n@@ -1006,1 +1012,1 @@\n-  Node* length = argument(1);\n+  Node* length = bt == T_INT ? argument(1) : argument(2);\n@@ -1011,1 +1017,2 @@\n-  Node* len_pos_cmp = _gvn.transform(new CmpINode(length, intcon(0)));\n+  \/\/ check that length is positive\n+  Node* len_pos_cmp = _gvn.transform(CmpNode::make(length, integercon(0, bt), bt));\n@@ -1020,0 +1027,7 @@\n+  \/\/ length is now known postive, add a cast node to make this explicit\n+  jlong upper_bound = _gvn.type(length)->is_integer(bt)->hi_as_long();\n+  Node* casted_length = ConstraintCastNode::make(control(), length, TypeInteger::make(0, upper_bound, Type::WidenMax, bt), bt);\n+  casted_length = _gvn.transform(casted_length);\n+  replace_in_map(length, casted_length);\n+  length = casted_length;\n+\n@@ -1024,1 +1038,2 @@\n-  Node* rc_cmp = _gvn.transform(new CmpUNode(index, length));\n+  \/\/ Use an unsigned comparison for the range check itself\n+  Node* rc_cmp = _gvn.transform(CmpNode::make(index, length, bt, true));\n@@ -1044,2 +1059,2 @@\n-  Node* result = new CastIINode(index, TypeInt::make(0, _gvn.type(length)->is_int()->_hi, Type::WidenMax));\n-  result->set_req(0, control());\n+  \/\/ index is now known to be >= 0 and < length, cast it\n+  Node* result = ConstraintCastNode::make(control(), index, TypeInteger::make(0, upper_bound, Type::WidenMax, bt), bt);\n@@ -5960,16 +5975,4 @@\n-  if (Matcher::pass_original_key_for_aes()) {\n-    \/\/ on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to\n-    \/\/ compatibility issues between Java key expansion and SPARC crypto instructions\n-    Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);\n-    if (original_k_start == NULL) return false;\n-\n-    \/\/ Call the stub.\n-    make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),\n-                      stubAddr, stubName, TypePtr::BOTTOM,\n-                      src_start, dest_start, k_start, original_k_start);\n-  } else {\n-    \/\/ Call the stub.\n-    make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),\n-                      stubAddr, stubName, TypePtr::BOTTOM,\n-                      src_start, dest_start, k_start);\n-  }\n+  \/\/ Call the stub.\n+  make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),\n+                    stubAddr, stubName, TypePtr::BOTTOM,\n+                    src_start, dest_start, k_start);\n@@ -6058,19 +6061,5 @@\n-  Node* cbcCrypt;\n-  if (Matcher::pass_original_key_for_aes()) {\n-    \/\/ on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to\n-    \/\/ compatibility issues between Java key expansion and SPARC crypto instructions\n-    Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);\n-    if (original_k_start == NULL) return false;\n-\n-    \/\/ Call the stub, passing src_start, dest_start, k_start, r_start, src_len and original_k_start\n-    cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,\n-                                 OptoRuntime::cipherBlockChaining_aescrypt_Type(),\n-                                 stubAddr, stubName, TypePtr::BOTTOM,\n-                                 src_start, dest_start, k_start, r_start, len, original_k_start);\n-  } else {\n-    \/\/ Call the stub, passing src_start, dest_start, k_start, r_start and src_len\n-    cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,\n-                                 OptoRuntime::cipherBlockChaining_aescrypt_Type(),\n-                                 stubAddr, stubName, TypePtr::BOTTOM,\n-                                 src_start, dest_start, k_start, r_start, len);\n-  }\n+  \/\/ Call the stub, passing src_start, dest_start, k_start, r_start and src_len\n+  Node* cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                     OptoRuntime::cipherBlockChaining_aescrypt_Type(),\n+                                     stubAddr, stubName, TypePtr::BOTTOM,\n+                                     src_start, dest_start, k_start, r_start, len);\n@@ -6155,9 +6144,4 @@\n-  Node* ecbCrypt;\n-  if (Matcher::pass_original_key_for_aes()) {\n-    \/\/ no SPARC version for AES\/ECB intrinsics now.\n-    return false;\n-  }\n-  ecbCrypt = make_runtime_call(RC_LEAF | RC_NO_FP,\n-                               OptoRuntime::electronicCodeBook_aescrypt_Type(),\n-                               stubAddr, stubName, TypePtr::BOTTOM,\n-                               src_start, dest_start, k_start, len);\n+  Node* ecbCrypt = make_runtime_call(RC_LEAF | RC_NO_FP,\n+                                     OptoRuntime::electronicCodeBook_aescrypt_Type(),\n+                                     stubAddr, stubName, TypePtr::BOTTOM,\n+                                     src_start, dest_start, k_start, len);\n@@ -6239,9 +6223,4 @@\n-  Node* ctrCrypt;\n-  if (Matcher::pass_original_key_for_aes()) {\n-    \/\/ no SPARC version for AES\/CTR intrinsics now.\n-    return false;\n-  }\n-  ctrCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,\n-                               OptoRuntime::counterMode_aescrypt_Type(),\n-                               stubAddr, stubName, TypePtr::BOTTOM,\n-                               src_start, dest_start, k_start, cnt_start, len, saved_encCounter_start, used);\n+  Node* ctrCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                     OptoRuntime::counterMode_aescrypt_Type(),\n+                                     stubAddr, stubName, TypePtr::BOTTOM,\n+                                     src_start, dest_start, k_start, cnt_start, len, saved_encCounter_start, used);\n@@ -6280,11 +6259,0 @@\n-\/\/------------------------------get_original_key_start_from_aescrypt_object-----------------------\n-Node * LibraryCallKit::get_original_key_start_from_aescrypt_object(Node *aescrypt_object) {\n-  Node* objAESCryptKey = load_field_from_object(aescrypt_object, \"lastKey\", \"[B\", \/*is_exact*\/ false);\n-  assert (objAESCryptKey != NULL, \"wrong version of com.sun.crypto.provider.AESCrypt\");\n-  if (objAESCryptKey == NULL) return (Node *) NULL;\n-\n-  \/\/ now have the array, need to get the start address of the lastKey array\n-  Node* original_k_start = array_element_address(objAESCryptKey, intcon(0), T_BYTE);\n-  return original_k_start;\n-}\n-\n@@ -6523,0 +6491,34 @@\n+bool LibraryCallKit::inline_base64_decodeBlock() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseBASE64Intrinsics, \"need Base64 intrinsics support\");\n+  assert(callee()->signature()->size() == 6, \"base64_decodeBlock has 6 parameters\");\n+  stubAddr = StubRoutines::base64_decodeBlock();\n+  stubName = \"decodeBlock\";\n+\n+  if (!stubAddr) return false;\n+  Node* base64obj = argument(0);\n+  Node* src = argument(1);\n+  Node* src_offset = argument(2);\n+  Node* len = argument(3);\n+  Node* dest = argument(4);\n+  Node* dest_offset = argument(5);\n+  Node* isURL = argument(6);\n+\n+  src = must_be_not_null(src, true);\n+  dest = must_be_not_null(dest, true);\n+\n+  Node* src_start = array_element_address(src, intcon(0), T_BYTE);\n+  assert(src_start, \"source array is NULL\");\n+  Node* dest_start = array_element_address(dest, intcon(0), T_BYTE);\n+  assert(dest_start, \"destination array is NULL\");\n+\n+  Node* call = make_runtime_call(RC_LEAF,\n+                                 OptoRuntime::base64_decodeBlock_Type(),\n+                                 stubAddr, stubName, TypePtr::BOTTOM,\n+                                 src_start, src_offset, len, dest_start, dest_offset, isURL);\n+  Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+  set_result(result);\n+  return true;\n+}\n+\n@@ -7051,0 +7053,116 @@\n+\n+\/\/------------------------------- inline_getObjectSize --------------------------------------\n+\/\/\n+\/\/ Calculate the runtime size of the object\/array.\n+\/\/   native long sun.instrument.InstrumentationImpl.getObjectSize0(long nativeAgent, Object objectToSize);\n+\/\/\n+bool LibraryCallKit::inline_getObjectSize() {\n+  Node* obj = argument(3);\n+  Node* klass_node = load_object_klass(obj);\n+\n+  jint  layout_con = Klass::_lh_neutral_value;\n+  Node* layout_val = get_layout_helper(klass_node, layout_con);\n+  int   layout_is_con = (layout_val == NULL);\n+\n+  if (layout_is_con) {\n+    \/\/ Layout helper is constant, can figure out things at compile time.\n+\n+    if (Klass::layout_helper_is_instance(layout_con)) {\n+      \/\/ Instance case:  layout_con contains the size itself.\n+      Node *size = longcon(Klass::layout_helper_size_in_bytes(layout_con));\n+      set_result(size);\n+    } else {\n+      \/\/ Array case: size is round(header + element_size*arraylength).\n+      \/\/ Since arraylength is different for every array instance, we have to\n+      \/\/ compute the whole thing at runtime.\n+\n+      Node* arr_length = load_array_length(obj);\n+\n+      int round_mask = MinObjAlignmentInBytes - 1;\n+      int hsize  = Klass::layout_helper_header_size(layout_con);\n+      int eshift = Klass::layout_helper_log2_element_size(layout_con);\n+\n+      if ((round_mask & ~right_n_bits(eshift)) == 0) {\n+        round_mask = 0;  \/\/ strength-reduce it if it goes away completely\n+      }\n+      assert((hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n+      Node* header_size = intcon(hsize + round_mask);\n+\n+      Node* lengthx = ConvI2X(arr_length);\n+      Node* headerx = ConvI2X(header_size);\n+\n+      Node* abody = lengthx;\n+      if (eshift != 0) {\n+        abody = _gvn.transform(new LShiftXNode(lengthx, intcon(eshift)));\n+      }\n+      Node* size = _gvn.transform( new AddXNode(headerx, abody) );\n+      if (round_mask != 0) {\n+        size = _gvn.transform( new AndXNode(size, MakeConX(~round_mask)) );\n+      }\n+      size = ConvX2L(size);\n+      set_result(size);\n+    }\n+  } else {\n+    \/\/ Layout helper is not constant, need to test for array-ness at runtime.\n+\n+    enum { _instance_path = 1, _array_path, PATH_LIMIT };\n+    RegionNode* result_reg = new RegionNode(PATH_LIMIT);\n+    PhiNode* result_val = new PhiNode(result_reg, TypeLong::LONG);\n+    record_for_igvn(result_reg);\n+\n+    Node* array_ctl = generate_array_guard(klass_node, NULL);\n+    if (array_ctl != NULL) {\n+      \/\/ Array case: size is round(header + element_size*arraylength).\n+      \/\/ Since arraylength is different for every array instance, we have to\n+      \/\/ compute the whole thing at runtime.\n+\n+      PreserveJVMState pjvms(this);\n+      set_control(array_ctl);\n+      Node* arr_length = load_array_length(obj);\n+\n+      int round_mask = MinObjAlignmentInBytes - 1;\n+      Node* mask = intcon(round_mask);\n+\n+      Node* hss = intcon(Klass::_lh_header_size_shift);\n+      Node* hsm = intcon(Klass::_lh_header_size_mask);\n+      Node* header_size = _gvn.transform(new URShiftINode(layout_val, hss));\n+      header_size = _gvn.transform(new AndINode(header_size, hsm));\n+      header_size = _gvn.transform(new AddINode(header_size, mask));\n+\n+      \/\/ There is no need to mask or shift this value.\n+      \/\/ The semantics of LShiftINode include an implicit mask to 0x1F.\n+      assert(Klass::_lh_log2_element_size_shift == 0, \"use shift in place\");\n+      Node* elem_shift = layout_val;\n+\n+      Node* lengthx = ConvI2X(arr_length);\n+      Node* headerx = ConvI2X(header_size);\n+\n+      Node* abody = _gvn.transform(new LShiftXNode(lengthx, elem_shift));\n+      Node* size = _gvn.transform(new AddXNode(headerx, abody));\n+      if (round_mask != 0) {\n+        size = _gvn.transform(new AndXNode(size, MakeConX(~round_mask)));\n+      }\n+      size = ConvX2L(size);\n+\n+      result_reg->init_req(_array_path, control());\n+      result_val->init_req(_array_path, size);\n+    }\n+\n+    if (!stopped()) {\n+      \/\/ Instance case: the layout helper gives us instance size almost directly,\n+      \/\/ but we need to mask out the _lh_instance_slow_path_bit.\n+      Node* size = ConvI2X(layout_val);\n+      assert((int) Klass::_lh_instance_slow_path_bit < BytesPerLong, \"clear bit\");\n+      Node* mask = MakeConX(~(intptr_t) right_n_bits(LogBytesPerLong));\n+      size = _gvn.transform(new AndXNode(size, mask));\n+      size = ConvX2L(size);\n+\n+      result_reg->init_req(_instance_path, control());\n+      result_val->init_req(_instance_path, size);\n+    }\n+\n+    set_result(result_reg, result_val);\n+  }\n+\n+  return true;\n+}\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":189,"deletions":71,"binary":false,"changes":260,"status":"modified"},{"patch":"@@ -277,1 +277,1 @@\n-  bool inline_preconditions_checkIndex();\n+  bool inline_preconditions_checkIndex(BasicType bt);\n@@ -309,1 +309,0 @@\n-  Node* get_original_key_start_from_aescrypt_object(Node* aescrypt_object);\n@@ -312,0 +311,1 @@\n+  bool inline_base64_decodeBlock();\n@@ -380,0 +380,2 @@\n+\n+  bool inline_getObjectSize();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n@@ -3024,1 +3025,1 @@\n-      debug_only(int old_macro_count = C->macro_count(););\n+      DEBUG_ONLY(int old_macro_count = C->macro_count();)\n@@ -3040,1 +3041,1 @@\n-      debug_only(int old_macro_count = C->macro_count(););\n+      DEBUG_ONLY(int old_macro_count = C->macro_count();)\n@@ -3094,1 +3095,1 @@\n-      debug_only(int old_macro_count = C->macro_count(););\n+      DEBUG_ONLY(int old_macro_count = C->macro_count();)\n@@ -3183,1 +3184,1 @@\n-    debug_only(int old_macro_count = C->macro_count(););\n+    DEBUG_ONLY(int old_macro_count = C->macro_count();)\n@@ -3187,1 +3188,0 @@\n-      assert(C->macro_count() == (old_macro_count - 1), \"expansion must have deleted one node from macro list\");\n@@ -3191,1 +3191,0 @@\n-      assert(C->macro_count() == (old_macro_count - 1), \"expansion must have deleted one node from macro list\");\n@@ -3195,1 +3194,0 @@\n-      assert(C->macro_count() == (old_macro_count - 1), \"expansion must have deleted one node from macro list\");\n@@ -3199,1 +3197,0 @@\n-      assert(C->macro_count() == (old_macro_count - 1), \"expansion must have deleted one node from macro list\");\n@@ -3213,1 +3210,1 @@\n-    assert(C->macro_count() < macro_count, \"must have deleted a node from macro list\");\n+    assert(C->macro_count() == (old_macro_count - 1), \"expansion must have deleted one node from macro list\");\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":6,"deletions":9,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -195,1 +195,1 @@\n-    OptoRegPair regs = return_value(ireg, false);\n+    OptoRegPair regs = return_value(ireg);\n@@ -786,1 +786,1 @@\n-  OptoReg::Name reg = find_receiver(false);\n+  OptoReg::Name reg = find_receiver();\n@@ -2041,1 +2041,1 @@\n-OptoReg::Name Matcher::find_receiver( bool is_outgoing ) {\n+OptoReg::Name Matcher::find_receiver() {\n@@ -2044,1 +2044,1 @@\n-  calling_convention(&sig_bt, &regs, 1, is_outgoing);\n+  SharedRuntime::java_calling_convention(&sig_bt, &regs, 1);\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -356,3 +356,0 @@\n-  \/\/ Should original key array reference be passed to AES stubs\n-  static const bool pass_original_key_for_aes();\n-\n@@ -378,4 +375,0 @@\n-  \/\/ Array mapping arguments to registers.  Argument 0 is usually the 'this'\n-  \/\/ pointer.  Registers can include stack-slots and regular registers.\n-  static void calling_convention( BasicType *, VMRegPair *, uint len, bool is_outgoing );\n-\n@@ -384,1 +377,1 @@\n-  static OptoReg::Name  find_receiver( bool is_outgoing );\n+  static OptoReg::Name  find_receiver();\n@@ -389,4 +382,4 @@\n-  \/\/ Return value register.  On Intel it is EAX.  On Sparc i0\/o0.\n-  static OptoRegPair   return_value(uint ideal_reg, bool is_outgoing);\n-  static OptoRegPair c_return_value(uint ideal_reg, bool is_outgoing);\n-  RegMask*             _return_values_mask;\n+  \/\/ Return value register.  On Intel it is EAX.\n+  static OptoRegPair   return_value(uint ideal_reg);\n+  static OptoRegPair c_return_value(uint ideal_reg);\n+  RegMask*            _return_values_mask;\n@@ -429,3 +422,0 @@\n-  \/\/ Array mapping arguments to registers.  Argument 0 is usually the 'this'\n-  \/\/ pointer.  Registers can include stack-slots and regular registers.\n-  static void c_calling_convention( BasicType*, VMRegPair *, uint );\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":5,"deletions":15,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1512,0 +1512,16 @@\n+Node* RotateLeftNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  const Type *t1 = phase->type(in(1));\n+  const Type *t2 = phase->type(in(2));\n+  if (t2->isa_int() && t2->is_int()->is_con()) {\n+    if (t1->isa_int()) {\n+      int lshift = t2->is_int()->get_con() & 31;\n+      return new RotateRightNode(in(1), phase->intcon(32 - (lshift & 31)), TypeInt::INT);\n+    } else {\n+      assert(t1->isa_long(), \"Type must be a long\");\n+      int lshift = t2->is_int()->get_con() & 63;\n+      return new RotateRightNode(in(1), phase->intcon(64 - (lshift & 63)), TypeLong::LONG);\n+    }\n+  }\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/mulnode.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -666,1 +666,1 @@\n-void Node::grow( uint len ) {\n+void Node::grow(uint len) {\n@@ -2277,18 +2277,0 @@\n-\/\/------------------------------walk-------------------------------------------\n-\/\/ Graph walk, with both pre-order and post-order functions\n-void Node::walk(NFunc pre, NFunc post, void *env) {\n-  VectorSet visited; \/\/ Setup for local walk\n-  walk_(pre, post, env, visited);\n-}\n-\n-void Node::walk_(NFunc pre, NFunc post, void *env, VectorSet &visited) {\n-  if( visited.test_set(_idx) ) return;\n-  pre(*this,env);               \/\/ Call the pre-order walk function\n-  for( uint i=0; i<_max; i++ )\n-    if( in(i) )                 \/\/ Input exists and is not walked?\n-      in(i)->walk_(pre,post,env,visited); \/\/ Walk it with pre & post functions\n-  post(*this,env);              \/\/ Call the post-order walk function\n-}\n-\n-void Node::nop(Node &, void*) {}\n-\n@@ -2302,1 +2284,0 @@\n-static RegMask _not_used_at_all;\n@@ -2306,1 +2287,1 @@\n-  return _not_used_at_all;\n+  return RegMask::Empty;\n@@ -2311,1 +2292,1 @@\n-  return _not_used_at_all;\n+  return RegMask::Empty;\n@@ -2314,10 +2295,0 @@\n-\/\/=============================================================================\n-\/\/-----------------------------------------------------------------------------\n-void Node_Array::reset( Arena *new_arena ) {\n-  _a->Afree(_nodes,_max*sizeof(Node*));\n-  _max   = 0;\n-  _nodes = NULL;\n-  _a     = new_arena;\n-}\n-\n-\/\/------------------------------clear------------------------------------------\n@@ -2329,7 +2300,2 @@\n-\/\/-----------------------------------------------------------------------------\n-void Node_Array::grow( uint i ) {\n-  if( !_max ) {\n-    _max = 1;\n-    _nodes = (Node**)_a->Amalloc( _max * sizeof(Node*) );\n-    _nodes[0] = NULL;\n-  }\n+void Node_Array::grow(uint i) {\n+  assert(_max > 0, \"invariant\");\n@@ -2342,4 +2308,5 @@\n-\/\/-----------------------------------------------------------------------------\n-void Node_Array::insert( uint i, Node *n ) {\n-  if( _nodes[_max-1] ) grow(_max);      \/\/ Get more space if full\n-  Copy::conjoint_words_to_higher((HeapWord*)&_nodes[i], (HeapWord*)&_nodes[i+1], ((_max-i-1)*sizeof(Node*)));\n+void Node_Array::insert(uint i, Node* n) {\n+  if (_nodes[_max - 1]) {\n+    grow(_max);\n+  }\n+  Copy::conjoint_words_to_higher((HeapWord*)&_nodes[i], (HeapWord*)&_nodes[i + 1], ((_max - i - 1) * sizeof(Node*)));\n@@ -2349,9 +2316,3 @@\n-\/\/-----------------------------------------------------------------------------\n-void Node_Array::remove( uint i ) {\n-  Copy::conjoint_words_to_lower((HeapWord*)&_nodes[i+1], (HeapWord*)&_nodes[i], ((_max-i-1)*sizeof(Node*)));\n-  _nodes[_max-1] = NULL;\n-}\n-\n-\/\/-----------------------------------------------------------------------------\n-void Node_Array::sort( C_sort_func_t func) {\n-  qsort( _nodes, _max, sizeof( Node* ), func );\n+void Node_Array::remove(uint i) {\n+  Copy::conjoint_words_to_lower((HeapWord*)&_nodes[i + 1], (HeapWord*)&_nodes[i], ((_max - i - 1) * sizeof(Node*)));\n+  _nodes[_max - 1] = NULL;\n@@ -2360,1 +2321,0 @@\n-\/\/-----------------------------------------------------------------------------\n@@ -2363,3 +2323,3 @@\n-  for( uint i = 0; i < _max; i++ ) {\n-    Node *nn = _nodes[i];\n-    if( nn != NULL ) {\n+  for (uint i = 0; i < _max; i++) {\n+    Node* nn = _nodes[i];\n+    if (nn != NULL) {\n@@ -2428,1 +2388,3 @@\n-      if (found != NULL) return NULL;\n+      if (found != NULL) {\n+        return NULL;\n+      }\n@@ -2471,2 +2433,2 @@\n-  for( i = 0; i < _cnt; i++ )\n-    if( _nodes[i] == n )\n+  for (i = 0; i < _cnt; i++) {\n+    if (_nodes[i] == n) {\n@@ -2474,0 +2436,2 @@\n+    }\n+  }\n@@ -2475,1 +2439,1 @@\n-  if( i < _cnt )\n+  if (i < _cnt) {\n@@ -2477,0 +2441,1 @@\n+  }\n@@ -2482,3 +2447,3 @@\n-  for( uint i = 0; i < _cnt; i++ )\n-    if( _nodes[i] ) {\n-      tty->print(\"%5d--> \",i);\n+  for (uint i = 0; i < _cnt; i++) {\n+    if (_nodes[i]) {\n+      tty->print(\"%5d--> \", i);\n@@ -2487,0 +2452,1 @@\n+  }\n@@ -2492,1 +2458,1 @@\n-  for( uint i = 0; i < _cnt; i++ )\n+  for (uint i = 0; i < _cnt; i++) {\n@@ -2498,0 +2464,1 @@\n+  }\n@@ -2519,1 +2486,0 @@\n-\n@@ -2525,5 +2491,1 @@\n-      map(i,Node_List::pop());\n-      \/\/ Node *replacement = Node_List::pop();\n-      \/\/ if( i != size() ) { \/\/ Check if removing last entry\n-      \/\/   _nodes[i] = replacement;\n-      \/\/ }\n+      map(i, Node_List::pop());\n@@ -2549,2 +2511,2 @@\n-  for (uint i=0; i < sz; i++) {\n-    if (idx == index_at(i) )\n+  for (uint i = 0; i < sz; i++) {\n+    if (idx == index_at(i)) {\n@@ -2552,0 +2514,1 @@\n+    }\n@@ -2560,1 +2523,1 @@\n-  if( !Verbose && !WizardMode ) {\n+  if (!Verbose && !WizardMode) {\n@@ -2574,3 +2537,4 @@\n-bool TypeNode::cmp( const Node &n ) const\n-{ return !Type::cmp( _type, ((TypeNode&)n)._type ); }\n-const Type *TypeNode::bottom_type() const { return _type; }\n+bool TypeNode::cmp(const Node& n) const {\n+  return !Type::cmp(_type, ((TypeNode&)n)._type);\n+}\n+const Type* TypeNode::bottom_type() const { return _type; }\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":40,"deletions":76,"binary":false,"changes":116,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+class CastLLNode;\n@@ -167,4 +168,0 @@\n-typedef void (*NFunc)(Node&,void*);\n-extern \"C\" {\n-  typedef int (*C_sort_func_t)(const void *, const void *);\n-}\n@@ -682,0 +679,1 @@\n+        DEFINE_CLASS_ID(CastLL, ConstraintCast, 2)\n@@ -834,0 +832,1 @@\n+  DEFINE_CLASS_QUERY(CastLL)\n@@ -1138,10 +1137,0 @@\n-\/\/----------------- Graph walking\n-public:\n-  \/\/ Walk and apply member functions recursively.\n-  \/\/ Supplied (this) pointer is root.\n-  void walk(NFunc pre, NFunc post, void *env);\n-  static void nop(Node &, void*); \/\/ Dummy empty function\n-  static void packregion( Node &n, void* );\n-private:\n-  void walk_(NFunc pre, NFunc post, void *env, VectorSet &visited);\n-\n@@ -1150,0 +1139,1 @@\n+ private:\n@@ -1503,1 +1493,1 @@\n-  Arena *_a;                    \/\/ Arena to allocate in\n+  Arena* _a;                    \/\/ Arena to allocate in\n@@ -1505,1 +1495,1 @@\n-  Node **_nodes;\n+  Node** _nodes;\n@@ -1508,3 +1498,3 @@\n-  Node_Array(Arena *a) : _a(a), _max(OptoNodeListSize) {\n-    _nodes = NEW_ARENA_ARRAY( a, Node *, OptoNodeListSize );\n-    for( int i = 0; i < OptoNodeListSize; i++ ) {\n+  Node_Array(Arena* a) : _a(a), _max(OptoNodeListSize) {\n+    _nodes = NEW_ARENA_ARRAY(a, Node*, OptoNodeListSize);\n+    for (int i = 0; i < OptoNodeListSize; i++) {\n@@ -1515,1 +1505,1 @@\n-  Node_Array(Node_Array *na) : _a(na->_a), _max(na->_max), _nodes(na->_nodes) {}\n+  Node_Array(Node_Array* na) : _a(na->_a), _max(na->_max), _nodes(na->_nodes) {}\n@@ -1518,2 +1508,2 @@\n-  Node *at( uint i ) const { assert(i<_max,\"oob\"); return _nodes[i]; }\n-  Node **adr() { return _nodes; }\n+  Node* at(uint i) const { assert(i<_max,\"oob\"); return _nodes[i]; }\n+  Node** adr() { return _nodes; }\n@@ -1524,2 +1514,0 @@\n-  void sort( C_sort_func_t func);\n-  void reset( Arena *new_a );   \/\/ Zap mapping to empty; reclaim storage\n@@ -1548,1 +1536,0 @@\n-  Node *rpop() { Node *b = _nodes[0]; _nodes[0]=_nodes[--_cnt]; return b;}\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":12,"deletions":25,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -277,0 +277,1 @@\n+  ConNode* integercon(jlong l, BasicType bt);\n","filename":"src\/hotspot\/share\/opto\/phaseX.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n@@ -794,3 +795,0 @@\n-  if (Matcher::pass_original_key_for_aes()) {\n-    num_args = 4;\n-  }\n@@ -803,3 +801,0 @@\n-  if (Matcher::pass_original_key_for_aes()) {\n-    fields[argp++] = TypePtr::NOTNULL;    \/\/ original k array\n-  }\n@@ -887,3 +882,0 @@\n-  if (Matcher::pass_original_key_for_aes()) {\n-    num_args = 6;\n-  }\n@@ -898,3 +890,0 @@\n-  if (Matcher::pass_original_key_for_aes()) {\n-    fields[argp++] = TypePtr::NOTNULL;    \/\/ original k array\n-  }\n@@ -915,3 +904,0 @@\n-  if (Matcher::pass_original_key_for_aes()) {\n-     num_args = 5;\n-  }\n@@ -925,3 +911,0 @@\n-  if (Matcher::pass_original_key_for_aes()) {\n-     fields[argp++] = TypePtr::NOTNULL;    \/\/ original k array\n-  }\n@@ -942,3 +925,0 @@\n-  if (Matcher::pass_original_key_for_aes()) {\n-    num_args = 8;\n-  }\n@@ -955,3 +935,0 @@\n-  if (Matcher::pass_original_key_for_aes()) {\n-    fields[argp++] = TypePtr::NOTNULL; \/\/ original k array\n-  }\n@@ -1203,0 +1180,21 @@\n+\/\/ Base64 decode function\n+const TypeFunc* OptoRuntime::base64_decodeBlock_Type() {\n+  int argcnt = 6;\n+\n+  const Type** fields = TypeTuple::fields(argcnt);\n+  int argp = TypeFunc::Parms;\n+  fields[argp++] = TypePtr::NOTNULL;    \/\/ src array\n+  fields[argp++] = TypeInt::INT;        \/\/ src offset\n+  fields[argp++] = TypeInt::INT;        \/\/ src length\n+  fields[argp++] = TypePtr::NOTNULL;    \/\/ dest array\n+  fields[argp++] = TypeInt::INT;        \/\/ dest offset\n+  fields[argp++] = TypeInt::BOOL;       \/\/ isURL\n+  assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+argcnt, fields);\n+\n+  \/\/ result type needed\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms + 0] = TypeInt::INT; \/\/ count of bytes written to dst\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+  return TypeFunc::make(domain, range);\n+}\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":22,"deletions":24,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -295,0 +295,1 @@\n+  static const TypeFunc* base64_decodeBlock_Type();\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -566,0 +566,1 @@\n+\n@@ -568,0 +569,18 @@\n+CmpNode *CmpNode::make(Node *in1, Node *in2, BasicType bt, bool unsigned_comp) {\n+  switch (bt) {\n+    case T_INT:\n+      if (unsigned_comp) {\n+        return new CmpUNode(in1, in2);\n+      }\n+      return new CmpINode(in1, in2);\n+    case T_LONG:\n+      if (unsigned_comp) {\n+        return new CmpULNode(in1, in2);\n+      }\n+      return new CmpLNode(in1, in2);\n+    default:\n+      fatal(\"Not implemented for %s\", type2name(bt));\n+  }\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -143,0 +143,2 @@\n+  static CmpNode *make(Node *in1, Node *in2, BasicType bt, bool unsigned_comp = false);\n+\n","filename":"src\/hotspot\/share\/opto\/subnode.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -772,1 +772,1 @@\n-  Dict *tdic = new (type_arena) Dict( (CmpKey)Type::cmp,(Hash)Type::uhash, type_arena, 128 );\n+  Dict *tdic = new (type_arena) Dict(*_shared_type_dict, type_arena);\n@@ -774,7 +774,0 @@\n-\n-  \/\/ Transfer the shared types.\n-  DictI i(_shared_type_dict);\n-  for( ; i.test(); ++i ) {\n-    Type* t = (Type*)i._value;\n-    tdic->Insert(t,t);  \/\/ New Type, insert into Type table\n-  }\n@@ -1420,0 +1413,8 @@\n+const TypeInteger* TypeInteger::make(jlong lo, jlong hi, int w, BasicType bt) {\n+  if (bt == T_INT) {\n+    return TypeInt::make(checked_cast<jint>(lo), checked_cast<jint>(hi), w);\n+  }\n+  assert(bt == T_LONG, \"basic type not an int or long\");\n+  return TypeLong::make(lo, hi, w);\n+}\n+\n@@ -1445,1 +1446,1 @@\n-TypeInt::TypeInt( jint lo, jint hi, int w ) : Type(Int), _lo(lo), _hi(hi), _widen(w) {\n+TypeInt::TypeInt( jint lo, jint hi, int w ) : TypeInteger(Int), _lo(lo), _hi(hi), _widen(w) {\n@@ -1705,1 +1706,1 @@\n-TypeLong::TypeLong( jlong lo, jlong hi, int w ) : Type(Long), _lo(lo), _hi(hi), _widen(w) {\n+TypeLong::TypeLong(jlong lo, jlong hi, int w) : TypeInteger(Long), _lo(lo), _hi(hi), _widen(w) {\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":11,"deletions":10,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -50,2 +50,3 @@\n-class   TypeInt;\n-class   TypeLong;\n+class   TypeInteger;\n+class     TypeInt;\n+class     TypeLong;\n@@ -311,0 +312,3 @@\n+  const TypeInteger* isa_integer() const;\n+  const TypeInteger* is_integer(BasicType bt) const;\n+  const TypeInteger* isa_integer(BasicType bt) const;\n@@ -558,0 +562,14 @@\n+class TypeInteger : public Type {\n+protected:\n+  TypeInteger(TYPES t) : Type(t) {}\n+\n+public:\n+  virtual jlong hi_as_long() const = 0;\n+  virtual jlong lo_as_long() const = 0;\n+  jlong get_con_as_long(BasicType bt) const;\n+\n+  static const TypeInteger* make(jlong lo, jlong hi, int w, BasicType bt);\n+};\n+\n+\n+\n@@ -561,1 +579,1 @@\n-class TypeInt : public Type {\n+class TypeInt : public TypeInteger {\n@@ -590,0 +608,4 @@\n+\n+  virtual jlong hi_as_long() const { return _hi; }\n+  virtual jlong lo_as_long() const { return _lo; }\n+\n@@ -624,1 +646,1 @@\n-class TypeLong : public Type {\n+class TypeLong : public TypeInteger {\n@@ -653,0 +675,2 @@\n+  virtual jlong hi_as_long() const { return _hi; }\n+  virtual jlong lo_as_long() const { return _lo; }\n@@ -1692,0 +1716,9 @@\n+inline const TypeInteger *Type::is_integer(BasicType bt) const {\n+  assert((bt == T_INT && _base == Int) || (bt == T_LONG && _base == Long), \"Not an Int\");\n+  return (TypeInteger*)this;\n+}\n+\n+inline const TypeInteger *Type::isa_integer(BasicType bt) const {\n+  return (((bt == T_INT && _base == Int) || (bt == T_LONG && _base == Long)) ? (TypeInteger*)this : NULL);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":37,"deletions":4,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -244,0 +244,4 @@\n+    ciInstanceKlass* iklass = vec_box->box_type()->klass()->as_instance_klass();\n+    int n_fields = iklass->nof_nonstatic_fields();\n+    assert(n_fields == 1, \"sanity\");\n+\n@@ -249,1 +253,1 @@\n-                                               first_ind, \/*n_fields=*\/1);\n+                                               first_ind, n_fields);\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"classfile\/javaThreadStatus.hpp\"\n@@ -4434,1 +4435,1 @@\n-              java_lang_Thread::RUNNABLE);\n+              JavaThreadStatus::RUNNABLE);\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -103,1 +103,0 @@\n-#include \"gc\/g1\/heterogeneousHeapRegionManager.hpp\"\n@@ -513,109 +512,0 @@\n-#if INCLUDE_G1GC || INCLUDE_PARALLELGC\n-WB_ENTRY(jlong, WB_DramReservedStart(JNIEnv* env, jobject o))\n-#if INCLUDE_G1GC\n-  if (UseG1GC) {\n-    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n-    HeapWord* base = g1h->reserved().start();\n-    if (g1h->is_heterogeneous_heap()) {\n-      uint start_region = HeterogeneousHeapRegionManager::manager()->start_index_of_dram();\n-      return (jlong)(base + start_region * HeapRegion::GrainBytes);\n-    } else {\n-      return (jlong)base;\n-    }\n-  }\n-#endif \/\/ INCLUDE_G1GC\n-#if INCLUDE_PARALLELGC\n-  if (UseParallelGC) {\n-    ParallelScavengeHeap* ps_heap = ParallelScavengeHeap::heap();\n-    if (AllocateOldGenAt != NULL) {\n-      MemRegion reserved = ps_heap->young_gen()->reserved();\n-      return (jlong)reserved.start();\n-    } else {\n-      return (jlong)ps_heap->base();\n-    }\n-  }\n-#endif \/\/ INCLUDE_PARALLELGC\n-  THROW_MSG_0(vmSymbols::java_lang_UnsupportedOperationException(), \"WB_DramReservedStart: enabled only for G1 and Parallel GC\");\n-WB_END\n-\n-WB_ENTRY(jlong, WB_DramReservedEnd(JNIEnv* env, jobject o))\n-#if INCLUDE_G1GC\n-  if (UseG1GC) {\n-    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n-    HeapWord* base = g1h->reserved().start();\n-    if (g1h->is_heterogeneous_heap()) {\n-      uint end_region = HeterogeneousHeapRegionManager::manager()->end_index_of_dram();\n-      return (jlong)(base + (end_region + 1) * HeapRegion::GrainBytes - 1);\n-    } else {\n-      return (jlong)base + G1Arguments::heap_max_size_bytes();\n-    }\n-  }\n-#endif \/\/ INCLUDE_G1GC\n-#if INCLUDE_PARALLELGC\n-  if (UseParallelGC) {\n-    ParallelScavengeHeap* ps_heap = ParallelScavengeHeap::heap();\n-    if (AllocateOldGenAt != NULL) {\n-      MemRegion reserved = ps_heap->young_gen()->reserved();\n-      return (jlong)reserved.end();\n-    } else {\n-      return (jlong)ps_heap->reserved_region().end();\n-    }\n-  }\n-#endif \/\/ INCLUDE_PARALLELGC\n-  THROW_MSG_0(vmSymbols::java_lang_UnsupportedOperationException(), \"WB_DramReservedEnd: enabled only for G1 and Parallel GC\");\n-WB_END\n-\n-WB_ENTRY(jlong, WB_NvdimmReservedStart(JNIEnv* env, jobject o))\n-#if INCLUDE_G1GC\n-  if (UseG1GC) {\n-    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n-    if (g1h->is_heterogeneous_heap()) {\n-      uint start_region = HeterogeneousHeapRegionManager::manager()->start_index_of_nvdimm();\n-      return (jlong)(g1h->reserved().start() + start_region * HeapRegion::GrainBytes);\n-    } else {\n-      THROW_MSG_0(vmSymbols::java_lang_UnsupportedOperationException(), \"WB_NvdimmReservedStart: Old gen is not allocated on NV-DIMM using AllocateOldGenAt flag\");\n-    }\n-  }\n-#endif \/\/ INCLUDE_G1GC\n-#if INCLUDE_PARALLELGC\n-  if (UseParallelGC) {\n-    ParallelScavengeHeap* ps_heap = ParallelScavengeHeap::heap();\n-    if (AllocateOldGenAt != NULL) {\n-      MemRegion reserved = ps_heap->old_gen()->reserved();\n-      return (jlong)reserved.start();\n-    } else {\n-      THROW_MSG_0(vmSymbols::java_lang_UnsupportedOperationException(), \"WB_NvdimmReservedStart: Old gen is not allocated on NV-DIMM using AllocateOldGenAt flag\");\n-    }\n-  }\n-#endif \/\/ INCLUDE_PARALLELGC\n-  THROW_MSG_0(vmSymbols::java_lang_UnsupportedOperationException(), \"WB_NvdimmReservedStart: enabled only for G1 and Parallel GC\");\n-WB_END\n-\n-WB_ENTRY(jlong, WB_NvdimmReservedEnd(JNIEnv* env, jobject o))\n-#if INCLUDE_G1GC\n-  if (UseG1GC) {\n-    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n-    if (g1h->is_heterogeneous_heap()) {\n-      uint end_region = HeterogeneousHeapRegionManager::manager()->start_index_of_nvdimm();\n-      return (jlong)(g1h->reserved().start() + (end_region + 1) * HeapRegion::GrainBytes - 1);\n-    } else {\n-      THROW_MSG_0(vmSymbols::java_lang_UnsupportedOperationException(), \"WB_NvdimmReservedEnd: Old gen is not allocated on NV-DIMM using AllocateOldGenAt flag\");\n-    }\n-  }\n-#endif \/\/ INCLUDE_G1GC\n-#if INCLUDE_PARALLELGC\n-  if (UseParallelGC) {\n-    ParallelScavengeHeap* ps_heap = ParallelScavengeHeap::heap();\n-    if (AllocateOldGenAt != NULL) {\n-      MemRegion reserved = ps_heap->old_gen()->reserved();\n-      return (jlong)reserved.end();\n-      } else {\n-      THROW_MSG_0(vmSymbols::java_lang_UnsupportedOperationException(), \"WB_NvdimmReservedEnd: Old gen is not allocated on NV-DIMM using AllocateOldGenAt flag\");\n-    }\n-  }\n-#endif \/\/ INCLUDE_PARALLELGC\n-  THROW_MSG_0(vmSymbols::java_lang_UnsupportedOperationException(), \"WB_NvdimmReservedEnd: enabled only for G1 and Parallel GC\");\n-WB_END\n-\n-#endif \/\/ INCLUDE_G1GC || INCLUDE_PARALLELGC\n-\n@@ -2516,6 +2406,0 @@\n-#if INCLUDE_G1GC || INCLUDE_PARALLELGC\n-  {CC\"dramReservedStart\",   CC\"()J\",                  (void*)&WB_DramReservedStart },\n-  {CC\"dramReservedEnd\",     CC\"()J\",                  (void*)&WB_DramReservedEnd },\n-  {CC\"nvdimmReservedStart\", CC\"()J\",                  (void*)&WB_NvdimmReservedStart },\n-  {CC\"nvdimmReservedEnd\",   CC\"()J\",                  (void*)&WB_NvdimmReservedEnd },\n-#endif \/\/ INCLUDE_G1GC || INCLUDE_PARALLELGC\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":0,"deletions":116,"binary":false,"changes":116,"status":"modified"},{"patch":"@@ -532,2 +532,0 @@\n-  { \"InitialBootClassLoaderMetaspaceSize\", JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },\n-  { \"UseLargePagesInMetaspace\",            JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },\n@@ -556,0 +554,2 @@\n+  { \"InitialBootClassLoaderMetaspaceSize\", JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },\n+  { \"UseLargePagesInMetaspace\",            JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },\n@@ -2164,2 +2164,0 @@\n-  status = status && GCArguments::check_args_consistency();\n-\n@@ -4116,1 +4114,0 @@\n-  UNSUPPORTED_OPTION_NULL(AllocateOldGenAt);\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -200,4 +200,0 @@\n-  product(bool, UseLargePagesInMetaspace, false,                            \\\n-          \"(Deprecated) Use large page memory in metaspace. \"               \\\n-          \"Only used if UseLargePages is enabled.\")                         \\\n-                                                                            \\\n@@ -967,5 +963,0 @@\n-  product(size_t, InitialBootClassLoaderMetaspaceSize,                      \\\n-          NOT_LP64(2200*K) LP64_ONLY(4*M),                                  \\\n-          \"(Deprecated) Initial size of the boot class loader data metaspace\") \\\n-          range(30*K, max_uintx\/BytesPerWord)                               \\\n-                                                                            \\\n@@ -2491,6 +2482,0 @@\n-  product(ccstr, AllocateOldGenAt, NULL, EXPERIMENTAL,                      \\\n-          \"Path to the directoy where a temporary file will be \"            \\\n-          \"created to use as the backing store for old generation.\"         \\\n-          \"File of size Xmx is pre-allocated for performance reason, so\"    \\\n-          \"we need that much space available\")                              \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/javaCalls.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -741,23 +741,0 @@\n-  \/\/ Check for pending. async. exceptions or suspends - except if the\n-  \/\/ thread was blocked inside the VM. has_special_runtime_exit_condition()\n-  \/\/ is called last since it grabs a lock and we only want to do that when\n-  \/\/ we must.\n-  \/\/\n-  \/\/ Note: we never deliver an async exception at a polling point as the\n-  \/\/ compiler may not have an exception handler for it. The polling\n-  \/\/ code will notice the async and deoptimize and the exception will\n-  \/\/ be delivered. (Polling at a return point is ok though). Sure is\n-  \/\/ a lot of bother for a deprecated feature...\n-  \/\/\n-  \/\/ We don't deliver an async exception if the thread state is\n-  \/\/ _thread_in_native_trans so JNI functions won't be called with\n-  \/\/ a surprising pending exception. If the thread state is going back to java,\n-  \/\/ async exception is checked in check_special_condition_for_native_trans().\n-\n-  if (state != _thread_blocked_trans &&\n-      state != _thread_in_vm_trans &&\n-      thread->has_special_runtime_exit_condition()) {\n-    thread->handle_special_runtime_exit_condition(\n-      !thread->is_at_poll_safepoint() && (state != _thread_in_native_trans));\n-  }\n-\n@@ -983,6 +960,1 @@\n-    SafepointMechanism::process_if_requested(self);\n-    \/\/ We have to wait if we are here because of a handshake for object deoptimization.\n-    if (self->is_obj_deopt_suspend()) {\n-      self->wait_for_object_deoptimization();\n-    }\n-    self->check_and_handle_async_exceptions();\n+    SafepointMechanism::process_if_requested_with_exit_check(self, true \/* check asyncs *\/);\n@@ -1001,1 +973,0 @@\n-    set_at_poll_safepoint(true);\n@@ -1006,0 +977,1 @@\n+    set_at_poll_safepoint(true);\n@@ -1007,5 +979,6 @@\n-    SafepointMechanism::process_if_requested(self);\n-    \/\/ We have to wait if we are here because of a handshake for object deoptimization.\n-    if (self->is_obj_deopt_suspend()) {\n-      self->wait_for_object_deoptimization();\n-    }\n+    \/\/ We never deliver an async exception at a polling point as the\n+    \/\/ compiler may not have an exception handler for it. The polling\n+    \/\/ code will notice the pending async exception, deoptimize and\n+    \/\/ the exception will be delivered. (Polling at a return point\n+    \/\/ is ok though). Sure is a lot of bother for a deprecated feature...\n+    SafepointMechanism::process_if_requested_with_exit_check(self, false \/* check asyncs *\/);\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":8,"deletions":35,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -1942,2 +1942,1 @@\n-  const bool is_outgoing = method->is_method_handle_intrinsic();\n-  int comp_args_on_stack = java_calling_convention(sig_bt, regs_without_member_name, total_args_passed - 1, is_outgoing);\n+  int comp_args_on_stack = java_calling_convention(sig_bt, regs_without_member_name, total_args_passed - 1);\n@@ -3199,5 +3198,2 @@\n-      \/\/ Now get the compiled-Java layout as input (or output) arguments.\n-      \/\/ NOTE: Stubs for compiled entry points of method handle intrinsics\n-      \/\/ are just trampolines so the argument registers must be outgoing ones.\n-      const bool is_outgoing = method->is_method_handle_intrinsic();\n-      int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed, is_outgoing);\n+      \/\/ Now get the compiled-Java arguments layout.\n+      int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed);\n@@ -3247,1 +3243,1 @@\n-  (void) java_calling_convention(&sig_bt, &regs, 1, true);\n+  (void) java_calling_convention(&sig_bt, &regs, 1);\n@@ -3278,1 +3274,1 @@\n-  comp_args_on_stack = java_calling_convention(sig_bt, regs, cnt, true);\n+  comp_args_on_stack = java_calling_convention(sig_bt, regs, cnt);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":5,"deletions":9,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -387,3 +387,1 @@\n-  \/\/ 4-bytes higher. So for sparc because the register window save area is at\n-  \/\/ the bottom of the frame the first 16 words will be skipped and SharedInfo::stack0\n-  \/\/ will be just above it. (\n+  \/\/ 4-bytes higher.\n@@ -391,1 +389,1 @@\n-  static int java_calling_convention(const BasicType* sig_bt, VMRegPair* regs, int total_args_passed, int is_outgoing);\n+  static int java_calling_convention(const BasicType* sig_bt, VMRegPair* regs, int total_args_passed);\n@@ -395,1 +393,1 @@\n-    return java_calling_convention(sig_bt, regs, total_args_passed, false);\n+    return java_calling_convention(sig_bt, regs, total_args_passed);\n@@ -491,0 +489,5 @@\n+  \/\/ Stack slots that may be unused by the calling convention but must\n+  \/\/ otherwise be preserved.  On Intel this includes the return address.\n+  \/\/ On PowerPC it includes the 4 words holding the old TOC & LR glue.\n+  static uint in_preserve_stack_slots();\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":8,"deletions":5,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -138,0 +138,1 @@\n+address StubRoutines::_base64_decodeBlock                  = NULL;\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -219,0 +219,1 @@\n+  static address _base64_decodeBlock;\n@@ -404,0 +405,1 @@\n+  static address base64_decodeBlock()    { return _base64_decodeBlock; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -751,18 +751,0 @@\n-\/\/\n-\/\/ Performance concern:\n-\/\/ OrderAccess::storestore() calls release() which at one time stored 0\n-\/\/ into the global volatile OrderAccess::dummy variable. This store was\n-\/\/ unnecessary for correctness. Many threads storing into a common location\n-\/\/ causes considerable cache migration or \"sloshing\" on large SMP systems.\n-\/\/ As such, I avoided using OrderAccess::storestore(). In some cases\n-\/\/ OrderAccess::fence() -- which incurs local latency on the executing\n-\/\/ processor -- is a better choice as it scales on SMP systems.\n-\/\/\n-\/\/ See http:\/\/blogs.oracle.com\/dave\/entry\/biased_locking_in_hotspot for\n-\/\/ a discussion of coherency costs. Note that all our current reference\n-\/\/ platforms provide strong ST-ST order, so the issue is moot on IA32,\n-\/\/ x64, and SPARC.\n-\/\/\n-\/\/ As a general policy we use \"volatile\" to control compiler-based reordering\n-\/\/ and explicit fences (barriers) to control for architectural reordering\n-\/\/ performed by the CPU(s) or platform.\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":0,"deletions":18,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"classfile\/javaThreadStatus.hpp\"\n@@ -503,1 +504,1 @@\n-                                          java_lang_Thread::RUNNABLE);\n+                                          JavaThreadStatus::RUNNABLE);\n@@ -891,1 +892,1 @@\n-                                      java_lang_Thread::RUNNABLE);\n+                                      JavaThreadStatus::RUNNABLE);\n@@ -1822,1 +1823,1 @@\n-  java_lang_Thread::set_thread_status(threadObj(), java_lang_Thread::TERMINATED);\n+  java_lang_Thread::set_thread_status(threadObj(), JavaThreadStatus::TERMINATED);\n@@ -2412,4 +2413,4 @@\n-  } while (is_external_suspend());\n-  if (state != _thread_in_native) {\n-    SafepointMechanism::process_if_requested(this);\n-  }\n+    if (state != _thread_in_native) {\n+      SafepointMechanism::process_if_requested(this);\n+    }\n+  } while (is_external_suspend());\n@@ -2496,1 +2497,0 @@\n-\n@@ -2499,11 +2499,1 @@\n-  if (thread->is_external_suspend()) {\n-    thread->java_suspend_self_with_safepoint_check();\n-  } else {\n-    SafepointMechanism::process_if_requested(thread);\n-  }\n-\n-  if (thread->is_obj_deopt_suspend()) {\n-    thread->wait_for_object_deoptimization();\n-  }\n-\n-  JFR_ONLY(SUSPEND_THREAD_CONDITIONAL(thread);)\n+  SafepointMechanism::process_if_requested_with_exit_check(thread, false \/* check asyncs *\/);\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":9,"deletions":19,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -33,1 +33,0 @@\n-#include \"prims\/jvmtiExport.hpp\"\n@@ -66,0 +65,1 @@\n+class JvmtiSampledObjectAllocEventCollector;\n@@ -67,0 +67,1 @@\n+class JvmtiVMObjectAllocEventCollector;\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"classfile\/javaThreadStatus.hpp\"\n@@ -576,0 +577,1 @@\n+     static_field(StubRoutines,                _base64_decodeBlock,                           address)                               \\\n@@ -2336,1 +2338,1 @@\n-  \/* java_lang_Thread::ThreadStatus enum *\/                               \\\n+  \/* JavaThreadStatus enum               *\/                               \\\n@@ -2339,9 +2341,9 @@\n-  declare_constant(java_lang_Thread::NEW)                                 \\\n-  declare_constant(java_lang_Thread::RUNNABLE)                            \\\n-  declare_constant(java_lang_Thread::SLEEPING)                            \\\n-  declare_constant(java_lang_Thread::IN_OBJECT_WAIT)                      \\\n-  declare_constant(java_lang_Thread::IN_OBJECT_WAIT_TIMED)                \\\n-  declare_constant(java_lang_Thread::PARKED)                              \\\n-  declare_constant(java_lang_Thread::PARKED_TIMED)                        \\\n-  declare_constant(java_lang_Thread::BLOCKED_ON_MONITOR_ENTER)            \\\n-  declare_constant(java_lang_Thread::TERMINATED)                          \\\n+  declare_constant(JavaThreadStatus::NEW)                                 \\\n+  declare_constant(JavaThreadStatus::RUNNABLE)                            \\\n+  declare_constant(JavaThreadStatus::SLEEPING)                            \\\n+  declare_constant(JavaThreadStatus::IN_OBJECT_WAIT)                      \\\n+  declare_constant(JavaThreadStatus::IN_OBJECT_WAIT_TIMED)                \\\n+  declare_constant(JavaThreadStatus::PARKED)                              \\\n+  declare_constant(JavaThreadStatus::PARKED_TIMED)                        \\\n+  declare_constant(JavaThreadStatus::BLOCKED_ON_MONITOR_ENTER)            \\\n+  declare_constant(JavaThreadStatus::TERMINATED)                          \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":12,"deletions":10,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -1772,34 +1772,3 @@\n-            public VarHandle memoryAccessVarHandle(Class<?> carrier, long alignmentMask,\n-                                                   ByteOrder order, long offset, long[] strides) {\n-                return VarHandles.makeMemoryAddressViewHandle(carrier, alignmentMask, order, offset, strides);\n-            }\n-\n-            @Override\n-            public Class<?> memoryAddressCarrier(VarHandle handle) {\n-                return checkMemoryAccessHandle(handle).carrier();\n-            }\n-\n-            @Override\n-            public long memoryAddressAlignmentMask(VarHandle handle) {\n-                return checkMemoryAccessHandle(handle).alignmentMask;\n-            }\n-\n-            @Override\n-            public ByteOrder memoryAddressByteOrder(VarHandle handle) {\n-                return checkMemoryAccessHandle(handle).be ?\n-                        ByteOrder.BIG_ENDIAN : ByteOrder.LITTLE_ENDIAN;\n-            }\n-\n-            @Override\n-            public long memoryAddressOffset(VarHandle handle) {\n-                return checkMemoryAccessHandle(handle).offset;\n-            }\n-\n-            @Override\n-            public long[] memoryAddressStrides(VarHandle handle) {\n-                return checkMemoryAccessHandle(handle).strides();\n-            }\n-\n-            @Override\n-            public boolean isMemoryAccessVarHandle(VarHandle handle) {\n-                return asMemoryAccessVarHandle(handle) != null;\n+            public VarHandle memoryAccessVarHandle(Class<?> carrier, boolean skipAlignmentMaskCheck, long alignmentMask,\n+                                                   ByteOrder order) {\n+                return VarHandles.makeMemoryAddressViewHandle(carrier, skipAlignmentMaskCheck, alignmentMask, order);\n@@ -1837,21 +1806,0 @@\n-\n-            private MemoryAccessVarHandleBase asMemoryAccessVarHandle(VarHandle handle) {\n-                if (handle instanceof MemoryAccessVarHandleBase) {\n-                    return (MemoryAccessVarHandleBase)handle;\n-                } else if (handle.target() instanceof MemoryAccessVarHandleBase) {\n-                    \/\/ skip first adaptation, since we have to step over MemoryAddressProxy\n-                    \/\/ see JDK-8237349\n-                    return (MemoryAccessVarHandleBase)handle.target();\n-                } else {\n-                    return null;\n-                }\n-            }\n-\n-            private MemoryAccessVarHandleBase checkMemoryAccessHandle(VarHandle handle) {\n-                MemoryAccessVarHandleBase base = asMemoryAccessVarHandle(handle);\n-                if (base == null) {\n-                    throw new IllegalArgumentException(\"Not a memory access varhandle: \" + handle);\n-                }\n-                return base;\n-            }\n-\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandleImpl.java","additions":3,"deletions":55,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -336,0 +336,1 @@\n+     * @param skipAlignmentMaskCheck if true, only the base part of the address will be checked for alignment.\n@@ -338,2 +339,0 @@\n-     * @param offset a constant offset for the access.\n-     * @param strides the scale factors with which to multiply given access coordinates.\n@@ -342,2 +341,2 @@\n-    static VarHandle makeMemoryAddressViewHandle(Class<?> carrier, long alignmentMask,\n-                                                 ByteOrder byteOrder, long offset, long[] strides) {\n+    static VarHandle makeMemoryAddressViewHandle(Class<?> carrier, boolean skipAlignmentMaskCheck, long alignmentMask,\n+                                                 ByteOrder byteOrder) {\n@@ -349,11 +348,18 @@\n-\n-        Map<Integer, MethodHandle> carrierFactory = ADDRESS_FACTORIES.get(carrier);\n-        MethodHandle fac = carrierFactory.computeIfAbsent(strides.length,\n-                dims -> new MemoryAccessVarHandleGenerator(carrier, dims)\n-                            .generateHandleFactory());\n-\n-        try {\n-            boolean exact = false;\n-            return maybeAdapt((VarHandle)fac.invoke(be, size, offset, alignmentMask, exact, strides));\n-        } catch (Throwable ex) {\n-            throw new IllegalStateException(ex);\n+        boolean exact = false;\n+\n+        if (carrier == byte.class) {\n+            return maybeAdapt(new MemoryAccessVarHandleByteHelper(skipAlignmentMaskCheck, be, size, alignmentMask, exact));\n+        } else if (carrier == char.class) {\n+            return maybeAdapt(new MemoryAccessVarHandleCharHelper(skipAlignmentMaskCheck, be, size, alignmentMask, exact));\n+        } else if (carrier == short.class) {\n+            return maybeAdapt(new MemoryAccessVarHandleShortHelper(skipAlignmentMaskCheck, be, size, alignmentMask, exact));\n+        } else if (carrier == int.class) {\n+            return maybeAdapt(new MemoryAccessVarHandleIntHelper(skipAlignmentMaskCheck, be, size, alignmentMask, exact));\n+        } else if (carrier == float.class) {\n+            return maybeAdapt(new MemoryAccessVarHandleFloatHelper(skipAlignmentMaskCheck, be, size, alignmentMask, exact));\n+        } else if (carrier == long.class) {\n+            return maybeAdapt(new MemoryAccessVarHandleLongHelper(skipAlignmentMaskCheck, be, size, alignmentMask, exact));\n+        } else if (carrier == double.class) {\n+            return maybeAdapt(new MemoryAccessVarHandleDoubleHelper(skipAlignmentMaskCheck, be, size, alignmentMask, exact));\n+        } else {\n+            throw new IllegalStateException(\"Cannot get here\");\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/VarHandles.java","additions":21,"deletions":15,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -356,0 +356,2 @@\n+        assert this instanceof FinalReference;\n+        assert next == this; \/\/ I.e. FinalReference is inactive\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/Reference.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -666,1 +666,1 @@\n-                enclosingClass,\n+                resolveToOwnerType(enclosingClass),\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Constructor.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -439,0 +439,76 @@\n+\n+    \/**\n+     * Checks if the {@code index} is within the bounds of the range from\n+     * {@code 0} (inclusive) to {@code length} (exclusive).\n+     *\n+     * <p>The {@code index} is defined to be out of bounds if any of the\n+     * following inequalities is true:\n+     * <ul>\n+     *  <li>{@code index < 0}<\/li>\n+     *  <li>{@code index >= length}<\/li>\n+     *  <li>{@code length < 0}, which is implied from the former inequalities<\/li>\n+     * <\/ul>\n+     *\n+     * @param index the index\n+     * @param length the upper-bound (exclusive) of the range\n+     * @return {@code index} if it is within bounds of the range\n+     * @throws IndexOutOfBoundsException if the {@code index} is out of bounds\n+     * @since 16\n+     *\/\n+    @ForceInline\n+    public static\n+    long checkIndex(long index, long length) {\n+        return Preconditions.checkIndex(index, length, null);\n+    }\n+\n+    \/**\n+     * Checks if the sub-range from {@code fromIndex} (inclusive) to\n+     * {@code toIndex} (exclusive) is within the bounds of range from {@code 0}\n+     * (inclusive) to {@code length} (exclusive).\n+     *\n+     * <p>The sub-range is defined to be out of bounds if any of the following\n+     * inequalities is true:\n+     * <ul>\n+     *  <li>{@code fromIndex < 0}<\/li>\n+     *  <li>{@code fromIndex > toIndex}<\/li>\n+     *  <li>{@code toIndex > length}<\/li>\n+     *  <li>{@code length < 0}, which is implied from the former inequalities<\/li>\n+     * <\/ul>\n+     *\n+     * @param fromIndex the lower-bound (inclusive) of the sub-range\n+     * @param toIndex the upper-bound (exclusive) of the sub-range\n+     * @param length the upper-bound (exclusive) the range\n+     * @return {@code fromIndex} if the sub-range within bounds of the range\n+     * @throws IndexOutOfBoundsException if the sub-range is out of bounds\n+     * @since 16\n+     *\/\n+    public static\n+    long checkFromToIndex(long fromIndex, long toIndex, long length) {\n+        return Preconditions.checkFromToIndex(fromIndex, toIndex, length, null);\n+    }\n+\n+    \/**\n+     * Checks if the sub-range from {@code fromIndex} (inclusive) to\n+     * {@code fromIndex + size} (exclusive) is within the bounds of range from\n+     * {@code 0} (inclusive) to {@code length} (exclusive).\n+     *\n+     * <p>The sub-range is defined to be out of bounds if any of the following\n+     * inequalities is true:\n+     * <ul>\n+     *  <li>{@code fromIndex < 0}<\/li>\n+     *  <li>{@code size < 0}<\/li>\n+     *  <li>{@code fromIndex + size > length}, taking into account integer overflow<\/li>\n+     *  <li>{@code length < 0}, which is implied from the former inequalities<\/li>\n+     * <\/ul>\n+     *\n+     * @param fromIndex the lower-bound (inclusive) of the sub-interval\n+     * @param size the size of the sub-range\n+     * @param length the upper-bound (exclusive) of the range\n+     * @return {@code fromIndex} if the sub-range within bounds of the range\n+     * @throws IndexOutOfBoundsException if the sub-range is out of bounds\n+     * @since 16\n+     *\/\n+    public static\n+    long checkFromIndexSize(long fromIndex, long size, long length) {\n+        return Preconditions.checkFromIndexSize(fromIndex, size, length, null);\n+    }\n","filename":"src\/java.base\/share\/classes\/java\/util\/Objects.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -84,38 +84,2 @@\n-    VarHandle memoryAccessVarHandle(Class<?> carrier, long alignmentMask,\n-                                    ByteOrder order, long offset, long[] strides);\n-\n-    \/**\n-     * Is {@code handle} a memory access varhandle?\n-     * Used by {@code jdk.incubator.foreign.MemoryHandles}.\n-     *\/\n-    boolean isMemoryAccessVarHandle(VarHandle handle);\n-\n-    \/**\n-     * Returns the carrier associated with a memory access var handle.\n-     * Used by {@code jdk.incubator.foreign.MemoryHandles}.\n-     *\/\n-    Class<?> memoryAddressCarrier(VarHandle handle);\n-\n-    \/**\n-     * Returns the alignment mask associated with a memory access var handle.\n-     * Used by {@code jdk.incubator.foreign.MemoryHandles}.\n-     *\/\n-    long memoryAddressAlignmentMask(VarHandle handle);\n-\n-    \/**\n-     * Returns the byte order associated with a memory access var handle.\n-     * Used by {@code jdk.incubator.foreign.MemoryHandles}.\n-     *\/\n-    ByteOrder memoryAddressByteOrder(VarHandle handle);\n-\n-    \/**\n-     * Returns the offset associated with a memory access var handle.\n-     * Used by {@code jdk.incubator.foreign.MemoryHandles}.\n-     *\/\n-    long memoryAddressOffset(VarHandle handle);\n-\n-    \/**\n-     * Returns the strides associated with a memory access var handle.\n-     * Used by {@code jdk.incubator.foreign.MemoryHandles}.\n-     *\/\n-    long[] memoryAddressStrides(VarHandle handle);\n+    VarHandle memoryAccessVarHandle(Class<?> carrier, boolean skipAlignmentMaskCheck, long alignmentMask,\n+                                    ByteOrder order);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangInvokeAccess.java","additions":2,"deletions":38,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -219,1 +219,2 @@\n-        java.desktop;\n+        java.desktop,\n+        jdk.incubator.foreign;\n@@ -231,0 +232,1 @@\n+        java.instrument,\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -445,0 +445,1 @@\n+                            \"jdk\/internal\/util\/Preconditions.checkIndex(JJLjava\/util\/function\/BiFunction;)J\",\n@@ -446,0 +447,8 @@\n+            if (config.useBase64Intrinsics()) {\n+                \/\/ Currently implemented on ppc64le only, but could be implemented on others\n+                add(toBeInvestigated,\n+                            \"java\/util\/Base64$Decoder.decodeBlock([BII[BIZ)I\");\n+            } else {\n+                add(ignore,\n+                            \"java\/util\/Base64$Decoder.decodeBlock([BII[BIZ)I\");\n+            }\n@@ -540,0 +549,5 @@\n+        if (isJDK16OrHigher()) {\n+            add(toBeInvestigated,\n+                                \"sun\/instrument\/InstrumentationImpl.getObjectSize0(JLjava\/lang\/Object;)J\");\n+        }\n+\n","filename":"src\/jdk.internal.vm.compiler\/share\/classes\/org.graalvm.compiler.hotspot.test\/src\/org\/graalvm\/compiler\/hotspot\/test\/CheckGraalIntrinsics.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -165,1 +165,0 @@\n-runtime\/cds\/serviceability\/ReplaceCriticalClassesForSubgraphs.java 8253081 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -45,2 +45,1 @@\n-  gc \\\n-  -gc\/nvdimm\n+  gc\n@@ -76,1 +75,0 @@\n- -gc\/nvdimm \\\n@@ -221,2 +219,1 @@\n-  -gc\/shenandoah \\\n-  -gc\/nvdimm\n+  -gc\/shenandoah\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -94,0 +94,2 @@\n+        String unmapPrefix = \".*Unmapping region #3 at base 0x.*\";\n+        String unmapPattern = unmapPrefix + \"(Bitmap)\";\n@@ -124,1 +126,6 @@\n-                        output.shouldContain(runtimeMsg);\n+                        output.shouldContain(runtimeMsg)\n+                              \/\/ Check that there are two of the following lines in\n+                              \/\/ the output. One for static archive and one for\n+                              \/\/ dynamic archive:\n+                              \/\/ Unmapping region #3 at base 0x<hex digits> (Bitmap)\n+                              .shouldMatchByLine(unmapPrefix, \"Hello World\", unmapPattern);\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/DynamicArchiveRelocationTest.java","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -120,1 +120,0 @@\n-        map.put(\"test.vm.gc.nvdimm\", this::isNvdimmTestEnabled);\n@@ -548,5 +547,0 @@\n-    private String isNvdimmTestEnabled() {\n-        String isEnabled = System.getenv(\"TEST_VM_GC_NVDIMM\");\n-        return \"\" + \"true\".equalsIgnoreCase(isEnabled);\n-    }\n-\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -206,4 +206,0 @@\n-  public native long    dramReservedStart();\n-  public native long    dramReservedEnd();\n-  public native long    nvdimmReservedStart();\n-  public native long    nvdimmReservedEnd();\n","filename":"test\/lib\/sun\/hotspot\/WhiteBox.java","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"}]}