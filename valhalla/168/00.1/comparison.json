{"files":[{"patch":"@@ -492,0 +492,1 @@\n+d2aa5d494481a1039a092d70efa1f5c9826c5b77 lw1_0\n@@ -514,0 +515,1 @@\n+6132641c6ff61b7b8f3f10b9cd385aafadbd72ef lworld_stable\n@@ -570,0 +572,4 @@\n+6132641c6ff61b7b8f3f10b9cd385aafadbd72ef lworld_stable\n+2b098533f1e52d7d541121409b745d9420886945 lworld_stable\n+2b098533f1e52d7d541121409b745d9420886945 lworld_stable\n+7c637fd25e7d6fccdab1098bedd48ed195a86cc7 lworld_stable\n","filename":".hgtags","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3251,0 +3251,1 @@\n+    (Constant INLINE_OBJECT = 'Q' \"'Q' - an inline object (objectID size).\")\n","filename":"make\/data\/jdwp\/jdwp.spec","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1628,0 +1628,2 @@\n+  __ verified_entry(C, 0);\n+  __ bind(*_verified_entry);\n@@ -1974,1 +1976,31 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n+\n+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+{\n+  MacroAssembler _masm(&cbuf);\n+\n+  if (!_verified) {\n+    Label skip;\n+    __ cmp_klass(j_rarg0, rscratch2, rscratch1);\n+    __ br(Assembler::EQ, skip);\n+      __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+    __ bind(skip);\n+\n+  } else {\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    __ unpack_inline_args(ra_->C, _receiver_only);\n+    __ b(*_verified_entry);\n+  }\n+}\n@@ -1976,0 +2008,8 @@\n+\n+uint MachVEPNode::size(PhaseRegAlloc* ra_) const\n+{\n+  return MachNode::size(ra_); \/\/ too many variables; just compute it the hard way\n+}\n+\n+\n+\/\/=============================================================================\n@@ -1997,0 +2037,1 @@\n+  Label skip;\n@@ -1998,0 +2039,1 @@\n+  \/\/ UseCompressedClassPointers logic are inside cmp_klass\n@@ -1999,1 +2041,1 @@\n-  Label skip;\n+\n@@ -2406,1 +2448,0 @@\n-\n@@ -8266,0 +8307,15 @@\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -8281,0 +8337,31 @@\n+instruct castN2I(iRegINoSp dst, iRegN src) %{\n+  match(Set dst (CastN2I src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"movw $dst, $src\\t# compressed ptr -> int\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct castI2N(iRegNNoSp dst, iRegI src) %{\n+  match(Set dst (CastI2N src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"movw $dst, $src\\t# int -> compressed ptr\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\n@@ -14114,1 +14201,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n@@ -14116,1 +14203,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":92,"deletions":5,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -231,1 +233,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), needs_stack_repair(), NULL);\n@@ -446,1 +448,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -497,0 +499,15 @@\n+  ciMethod* method = compilation()->method();\n+\n+  ciType* return_type = method->return_type();\n+  if (InlineTypeReturnedAsFields && return_type->is_inlinetype()) {\n+    ciInlineKlass* vk = return_type->as_inline_klass();\n+    if (vk->can_be_returned_as_fields()) {\n+      address unpack_handler = vk->unpack_handler();\n+      assert(unpack_handler != NULL, \"must be\");\n+      __ far_call(RuntimeAddress(unpack_handler));\n+      \/\/ At this point, rax points to the value object (for interpreter or C1 caller).\n+      \/\/ The fields of the object are copied into registers (for C2 caller).\n+    }\n+  }\n+\n+\n@@ -498,1 +515,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -508,0 +525,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -553,0 +574,1 @@\n+    case T_INLINE_TYPE:\n@@ -554,3 +576,1 @@\n-        if (patch_code == lir_patch_none) {\n-          jobject2reg(c->as_jobject(), dest->as_register());\n-        } else {\n+        if (patch_code != lir_patch_none) {\n@@ -558,0 +578,2 @@\n+        } else {\n+          jobject2reg(c->as_jobject(), dest->as_register());\n@@ -599,0 +621,1 @@\n+  case T_INLINE_TYPE:\n@@ -665,0 +688,1 @@\n+  case T_INLINE_TYPE:\n@@ -667,0 +691,2 @@\n+    \/\/ Non-null case is not handled on aarch64 but handled on x86\n+    \/\/ FIXME: do we need to add it here?\n@@ -705,1 +731,1 @@\n-    if (src->type() == T_OBJECT) {\n+    if (src->type() == T_OBJECT || src->type() == T_INLINE_TYPE) {\n@@ -799,0 +825,1 @@\n+    case T_INLINE_TYPE: \/\/ fall through\n@@ -924,1 +951,1 @@\n-  if (addr->base()->type() == T_OBJECT) {\n+  if (addr->base()->type() == T_OBJECT || addr->base()->type() == T_INLINE_TYPE) {\n@@ -948,0 +975,1 @@\n+    case T_INLINE_TYPE: \/\/ fall through\n@@ -1013,0 +1041,1 @@\n+      __ andr(dest->as_register(), dest->as_register(), oopDesc::compressed_klass_mask());\n@@ -1014,0 +1043,2 @@\n+    } else {\n+      __   ubfm(dest->as_register(), dest->as_register(), 0, 63 - oopDesc::storage_props_nof_bits);\n@@ -1018,0 +1049,14 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, NULL);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n@@ -1209,1 +1254,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->type() == T_INLINE_TYPE ||\n@@ -1521,0 +1566,122 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing an array that *may* be a flattened array (the declared type\n+  \/\/ Object[], interface[], or VT?[]). If this array is flattened, take slow path.\n+\n+  __ load_storage_props(op->tmp()->as_register(), op->array()->as_register());\n+  __ tst(op->tmp()->as_register(), ArrayStorageProperties::flattened_value);\n+  __ br(Assembler::NE, *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ We are storing into the array.\n+    Label skip;\n+    __ tst(op->tmp()->as_register(), ArrayStorageProperties::null_free_value);\n+    __ br(Assembler::EQ, skip);\n+    \/\/ The array is not flattened, but it is null_free. If we are storing\n+    \/\/ a null, take the slow path (which will throw NPE).\n+    __ cbz(op->value()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ This is called when we use aastore into a an array declared as \"[LVT;\",\n+  \/\/ where we know VT is not flattened (due to FlatArrayElementMaxSize, etc).\n+  \/\/ However, we need to do a NULL check if the actual array is a \"[QVT;\".\n+\n+  __ load_storage_props(op->tmp()->as_register(), op->array()->as_register());\n+  __ mov(rscratch1, (uint64_t) ArrayStorageProperties::null_free_value);\n+  __ cmp(op->tmp()->as_register(), rscratch1);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmp(left, right);\n+  __ br(Assembler::EQ, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  {\n+    __ cbz(left, L_oops_not_equal);\n+    __ cbz(right, L_oops_not_equal);\n+  }\n+\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Value object check -- if either of the operands is not a value object,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are value objects\n+  if ((left_klass == NULL || right_klass == NULL) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = rscratch1; \/* op->tmp1()->as_register(); *\/\n+    Register tmp2  = rscratch2; \/* op->tmp2()->as_register(); *\/\n+\n+    __ mov(tmp1, (intptr_t)markWord::always_locked_pattern);\n+\n+    __ ldr(tmp2, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, tmp2);\n+\n+    __ ldr(tmp2, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, tmp2);\n+\n+    __ mov(tmp2, (intptr_t)markWord::always_locked_pattern);\n+    __ cmp(tmp1, tmp2);\n+    __ br(Assembler::NE, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != NULL && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ b(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedOops) {\n+      __ ldrw(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldrw(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpw(left_klass_op, right_klass_op);\n+    } else {\n+      __ ldr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmp(left_klass_op, right_klass_op);\n+    }\n+\n+    __ br(Assembler::EQ, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  \/\/ We've returned from the stub. op->result_opr() contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+\n+  if (op->result_opr()->type() == T_LONG) {\n+    __ cbzw(op->result_opr()->as_register(), L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  } else {\n+    __ cbz(op->result_opr()->as_register(), L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  }\n+\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+\n+}\n+\n+\n@@ -1964,0 +2131,1 @@\n+      case T_INLINE_TYPE:\n@@ -2130,0 +2298,1 @@\n+    case T_INLINE_TYPE:\n@@ -2166,0 +2335,1 @@\n+    case T_INLINE_TYPE:\n@@ -2210,0 +2380,13 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest) {\n+  __ load_storage_props(tmp, obj);\n+  if (is_dest) {\n+    \/\/ We also take slow path if it's a null_free destination array, just in case the source array\n+    \/\/ contains NULLs.\n+    __ tst(tmp, ArrayStorageProperties::flattened_value | ArrayStorageProperties::null_free_value);\n+  } else {\n+    __ tst(tmp, ArrayStorageProperties::flattened_value);\n+  }\n+  __ br(Assembler::NE, *slow_path->entry());\n+}\n+\n+\n@@ -2231,0 +2414,16 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ b(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false);\n+  }\n+\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true);\n+  }\n+\n+\n+\n@@ -3130,0 +3329,1 @@\n+  case T_INLINE_TYPE:\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":209,"deletions":9,"binary":false,"changes":218,"status":"modified"},{"patch":"@@ -577,0 +577,1 @@\n+    case T_INLINE_TYPE :\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -81,1 +81,1 @@\n-                                   Address dst, Register val, Register tmp1, Register tmp2) {\n+                                   Address dst, Register val, Register tmp1, Register tmp2, Register tmp3) {\n@@ -84,0 +84,2 @@\n+  bool is_not_null = (decorators & IS_NOT_NULL) != 0;\n+\n@@ -87,6 +89,7 @@\n-    val = val == noreg ? zr : val;\n-    if (in_heap) {\n-      if (UseCompressedOops) {\n-        assert(!dst.uses(val), \"not enough registers\");\n-        if (val != zr) {\n-          __ encode_heap_oop(val);\n+   if (in_heap) {\n+      if (val == noreg) {\n+        assert(!is_not_null, \"inconsistent access\");\n+        if (UseCompressedOops) {\n+          __ strw(zr, dst);\n+        } else {\n+          __ str(zr, dst);\n@@ -94,2 +97,11 @@\n-        __ strw(val, dst);\n-        __ str(val, dst);\n+        if (UseCompressedOops) {\n+          assert(!dst.uses(val), \"not enough registers\");\n+          if (is_not_null) {\n+            __ encode_heap_oop_not_null(val);\n+          } else {\n+            __ encode_heap_oop(val);\n+          }\n+          __ strw(val, dst);\n+        } else {\n+          __ str(val, dst);\n+        }\n@@ -100,0 +112,1 @@\n+      assert(val != noreg, \"not supported\");\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.cpp","additions":22,"deletions":9,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -129,1 +129,2 @@\n-                                        Register tmp2) {\n+                                        Register tmp2,\n+                                        Register tmp3) {\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zBarrierSetAssembler_aarch64.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -670,0 +671,1 @@\n+\n@@ -686,0 +688,27 @@\n+\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    Label skip;\n+    \/\/ Test if the return type is an inline type\n+    ldr(rscratch1, Address(rfp, frame::interpreter_frame_method_offset * wordSize));\n+    ldr(rscratch1, Address(rscratch1, Method::const_offset()));\n+    ldrb(rscratch1, Address(rscratch1, ConstMethod::result_type_offset()));\n+    cmpw(rscratch1, (u1) T_INLINE_TYPE);\n+    br(Assembler::NE, skip);\n+\n+    \/\/ We are returning an inline type, load its fields into registers\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+\n+    load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+    cbz(rscratch1, skip);\n+\n+    blr(rscratch1);\n+\n+    \/\/ call above kills the value in r1. Reload it.\n+    ldr(r1, Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n+\n+\n@@ -739,0 +768,5 @@\n+    if (EnableValhalla && !UseBiasedLocking) {\n+      \/\/ For slow path is_always_locked, using biased, which is never natural for !UseBiasLocking\n+      andr(swap_reg, swap_reg, ~((int) markWord::biased_lock_bit_in_place));\n+    }\n+\n@@ -1704,1 +1738,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1750,1 +1784,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":36,"deletions":2,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -258,0 +258,4 @@\n+void InterpreterRuntime::SignatureHandlerGenerator::pass_valuetype() {\n+   pass_object();\n+}\n+\n@@ -328,0 +332,5 @@\n+  virtual void pass_valuetype() {\n+    \/\/ values are handled with oops, like objects\n+    pass_object();\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/interpreterRT_aarch64.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -1317,1 +1318,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1347,1 +1352,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1440,0 +1449,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -1489,0 +1502,33 @@\n+void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label& is_value) {\n+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  andr(temp_reg, temp_reg, JVM_ACC_INLINE);\n+  cbnz(temp_reg, is_value);\n+}\n+\n+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_inline_field_shift, is_inline);\n+}\n+\n+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbz(flags, ConstantPoolCacheEntry::is_inline_field_shift, not_inline);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label& is_flattened_array) {\n+  load_storage_props(temp_reg, oop);\n+  andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);\n+  cbnz(temp_reg, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  load_storage_props(temp_reg, oop);\n+  andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);\n+  cbnz(temp_reg, is_null_free_array);\n+}\n+\n@@ -3720,1 +3766,1 @@\n-void MacroAssembler::load_klass(Register dst, Register src) {\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n@@ -3723,1 +3769,0 @@\n-    decode_klass_not_null(dst);\n@@ -3729,0 +3774,10 @@\n+void MacroAssembler::load_klass(Register dst, Register src) {\n+  load_metadata(dst, src);\n+  if (UseCompressedClassPointers) {\n+    andr(dst, dst, oopDesc::compressed_klass_mask());\n+    decode_klass_not_null(dst);\n+  } else {\n+    ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);\n+  }\n+}\n+\n@@ -3760,0 +3815,9 @@\n+void MacroAssembler::load_storage_props(Register dst, Register src) {\n+  load_metadata(dst, src);\n+  if (UseCompressedClassPointers) {\n+    asrw(dst, dst, oopDesc::narrow_storage_props_shift);\n+  } else {\n+    asr(dst, dst, oopDesc::wide_storage_props_shift);\n+  }\n+}\n+\n@@ -4097,1 +4161,2 @@\n-                                     Register tmp1, Register thread_tmp) {\n+                                     Register tmp1, Register thread_tmp, Register tmp3) {\n+\n@@ -4102,1 +4167,1 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4104,1 +4169,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4128,2 +4193,2 @@\n-                                    Register thread_tmp, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);\n+                                    Register thread_tmp, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4134,1 +4199,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -5207,0 +5272,389 @@\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+\n+\/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->frame_size_in_bytes();\n+  assert(framesize % (2 * wordSize) == 0, \"must preserve 2 * wordSize alignment\");\n+\n+  \/\/ insert a nop at the start of the prolog so we can patch in a\n+  \/\/ branch if we need to invalidate the method later\n+  nop();\n+\n+  int bangsize = C->bang_size_in_bytes();\n+  if (C->need_stack_bang(bangsize) && UseStackBanging)\n+     generate_stack_overflow_check(bangsize);\n+\n+  build_frame(framesize);\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  cmp(r0, (u1) 1);\n+  br(Assembler::EQ, skip);\n+  int call_offset = -1;\n+\n+  Label slow_case;\n+\n+  \/\/ Try to allocate a new buffered inline type (from the heap)\n+  if (UseTLAB) {\n+\n+    if (vk != NULL) {\n+      \/\/ Called from C1, where the return type is statically known.\n+      mov(r1, (intptr_t)vk->get_InlineKlass());\n+      jint lh = vk->layout_helper();\n+      assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+      mov(r14, lh);\n+    } else {\n+       \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+       andr(r1, r0, -2);\n+       \/\/ get obj size\n+       ldrw(r14, Address(rscratch1 \/*klass*\/, Klass::layout_helper_offset()));\n+    }\n+\n+     ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+\n+     \/\/ check whether we have space in TLAB,\n+     \/\/ rscratch1 contains pointer to just allocated obj\n+      lea(r14, Address(r13, r14));\n+      ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));\n+\n+      cmp(r14, rscratch1);\n+      br(Assembler::GT, slow_case);\n+\n+      \/\/ OK we have room in TLAB,\n+      \/\/ Set new TLAB top\n+      str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+\n+      \/\/ Set new class always locked\n+      mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());\n+      str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));\n+\n+      store_klass_gap(r13, zr);  \/\/ zero klass gap for compressed oops\n+      if (vk == NULL) {\n+        \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n+         mov(r0, r1);\n+      }\n+\n+      store_klass(r13, r1);  \/\/ klass\n+\n+      if (vk != NULL) {\n+        \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+        mov(r0, r13);\n+        far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+      } else {\n+\n+        \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+        ldr(r1, Address(r0, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+        ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));\n+\n+        \/\/ Mov new class to r0 and call pack_handler\n+        mov(r0, r13);\n+        blr(r1);\n+      }\n+      b(skip);\n+  }\n+\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    ldr(rscratch1, RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    blr(rscratch1);\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        mov(to->as_Register(), from->as_Register());\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,\n+                                          int& to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {\n+  Register fromReg = from->is_reg() ? from->as_Register() : noreg;\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+\n+\n+  int vt = 1;\n+  bool done = true;\n+  bool mark_done = true;\n+  do {\n+    sig_index--;\n+    BasicType bt = sig->at(sig_index)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      vt--;\n+    } else if (bt == T_VOID &&\n+               sig->at(sig_index-1)._bt != T_LONG &&\n+               sig->at(sig_index-1)._bt != T_DOUBLE) {\n+      vt++;\n+    } else if (SigEntry::is_reserved_entry(sig, sig_index)) {\n+      to_index--; \/\/ Ignore this\n+    } else {\n+      assert(to_index >= 0, \"invalid to_index\");\n+      VMRegPair pair_to = regs_to[to_index--];\n+      VMReg to = pair_to.first();\n+\n+      if (bt == T_VOID) continue;\n+\n+      int idx = (int) to->value();\n+      if (reg_state[idx] == reg_readonly) {\n+         if (idx != from->value()) {\n+           mark_done = false;\n+         }\n+         done = false;\n+         continue;\n+      } else if (reg_state[idx] == reg_written) {\n+        continue;\n+      } else {\n+        assert(reg_state[idx] == reg_writable, \"must be writable\");\n+        reg_state[idx] = reg_written;\n+      }\n+\n+      if (fromReg == noreg) {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        ldr(rscratch2, Address(sp, st_off));\n+        fromReg = rscratch2;\n+      }\n+\n+      int off = sig->at(sig_index)._offset;\n+      assert(off > 0, \"offset in object should be positive\");\n+      bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+\n+      Address fromAddr = Address(fromReg, off);\n+      bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+\n+      if (!to->is_FloatRegister()) {\n+\n+        Register dst = to->is_stack() ? rscratch1 : to->as_Register();\n+\n+        if (is_oop) {\n+          load_heap_oop(dst, fromAddr);\n+        } else {\n+          load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+        }\n+        if (to->is_stack()) {\n+          int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+          str(dst, Address(sp, st_off));\n+        }\n+      } else {\n+        if (bt == T_DOUBLE) {\n+          ldrd(to->as_FloatRegister(), fromAddr);\n+        } else {\n+          assert(bt == T_FLOAT, \"must be float\");\n+          ldrs(to->as_FloatRegister(), fromAddr);\n+        }\n+     }\n+\n+    }\n+\n+  } while (vt != 0);\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],\n+                                        int ret_off, int extra_stack_offset) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"must be\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  Register val_array = r0;\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r10;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r1;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);\n+  VMRegPair from_pair;\n+  BasicType bt;\n+\n+  while (stream.next(from_pair, bt)) {\n+    int off = sig->at(stream.sig_cc_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    VMReg from_r1 = from_pair.first();\n+    VMReg from_r2 = from_pair.second();\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+\n+    if (!from_r1->is_FloatRegister()) {\n+      Register from_reg;\n+      if (from_r1->is_stack()) {\n+        from_reg = from_reg_tmp;\n+        int ld_off = from_r1->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        from_reg = from_r1->as_Register();\n+      }\n+\n+      if (is_oop) {\n+        DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;\n+        store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);\n+      } else {\n+        store_sized_value(dst, from_reg, size_in_bytes);\n+      }\n+    } else {\n+      if (from_r2->is_valid()) {\n+        strd(from_r1->as_FloatRegister(), dst);\n+      } else {\n+        strs(from_r1->as_FloatRegister(), dst);\n+      }\n+    }\n+\n+    reg_state[from_r1->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_cc_index();\n+  from_index = stream.regs_cc_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+\/\/ Unpack all inline type arguments passed as oops\n+void MacroAssembler::unpack_inline_args(Compile* C, bool receiver_only) {\n+  int sp_inc = unpack_inline_args_common(C, receiver_only);\n+  \/\/ Emit code for verified entry and save increment for stack repair on return\n+  verified_entry(C, sp_inc);\n+}\n+\n+int MacroAssembler::shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,\n+                                        BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,\n+                                        int args_passed, int args_on_stack, VMRegPair* regs,            \/\/ from\n+                                        int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { \/\/ to\n+  \/\/ Check if we need to extend the stack for packing\/unpacking\n+  int sp_inc = (args_on_stack_to - args_on_stack) * VMRegImpl::stack_slot_size;\n+  if (sp_inc > 0) {\n+    sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+    if (!is_packing) {\n+      \/\/ Save the return address, adjust the stack (make sure it is properly\n+      \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+      \/\/ (Note: C1 does this in C1_MacroAssembler::scalarized_entry).\n+      \/\/ FIXME: We need not to preserve return address on aarch64\n+      pop(rscratch1);\n+      sub(sp, sp, sp_inc);\n+      push(rscratch1);\n+    }\n+  } else {\n+    \/\/ The scalarized calling convention needs less stack space than the unscalarized one.\n+    \/\/ No need to extend the stack, the caller will take care of these adjustments.\n+    sp_inc = 0;\n+  }\n+\n+  int ret_off; \/\/ make sure we don't overwrite the return address\n+  if (is_packing) {\n+    \/\/ For C1 code, the VIEP doesn't have reserved slots, so we store the returned address at\n+    \/\/ rsp[0] during shuffling.\n+    ret_off = 0;\n+  } else {\n+    \/\/ C2 code ensures that sp_inc is a reserved slot.\n+    ret_off = sp_inc;\n+  }\n+\n+  return shuffle_inline_args_common(is_packing, receiver_only, extra_stack_offset,\n+                                    sig_bt, sig_cc,\n+                                    args_passed, args_on_stack, regs,\n+                                    args_passed_to, args_on_stack_to, regs_to,\n+                                    sp_inc, ret_off);\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v0->as_VMReg() : r14->as_VMReg();\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":464,"deletions":10,"binary":false,"changes":474,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -32,0 +33,4 @@\n+#include \"runtime\/signature.hpp\"\n+\n+\n+class ciInlineKlass;\n@@ -611,0 +616,10 @@\n+  void test_klass_is_value(Register klass, Register temp_reg, Label& is_value);\n+\n+  void test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline);\n+  void test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline);\n+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened);\n+\n+  \/\/ Check klass\/oops is flat inline type array (oop->_klass->_layout_helper & vt_bit)\n+  void test_flattened_array_oop(Register klass, Register temp_reg, Label& is_flattened_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+\n@@ -818,0 +833,3 @@\n+  void load_metadata(Register dst, Register src);\n+  void load_storage_props(Register dst, Register src);\n+\n@@ -830,1 +848,1 @@\n-                       Register tmp1, Register tmp_thread);\n+                       Register tmp1, Register tmp_thread, Register tmp3 = noreg);\n@@ -842,1 +860,1 @@\n-                      Register tmp_thread = noreg, DecoratorSet decorators = 0);\n+                      Register tmp_thread = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);\n@@ -1175,0 +1193,31 @@\n+\n+  enum RegState {\n+     reg_readonly,\n+     reg_writable,\n+     reg_written\n+  };\n+\n+  void verified_entry(Compile* C, int sp_inc);\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+\n+\/\/ Unpack all inline type arguments passed as oops\n+  void unpack_inline_args(Compile* C, bool receiver_only);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to, int& to_index,\n+                            RegState reg_state[], int ret_off, int extra_stack_offset);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],\n+                          int ret_off, int extra_stack_offset);\n+  void restore_stack(Compile* C);\n+\n+  int shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,\n+                          BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,\n+                          int args_passed, int args_on_stack, VMRegPair* regs,\n+                          int args_passed_to, int args_on_stack_to, VMRegPair* regs_to);\n+  bool shuffle_inline_args_spill(bool is_packing,  const GrowableArray<SigEntry>* sig_cc, int sig_cc_index,\n+                                 VMRegPair* regs_from, int from_index, int regs_from_count,\n+                                 RegState* reg_state, int sp_inc, int extra_stack_offset);\n+  VMReg spill_reg_for(VMReg reg);\n+\n+\n@@ -1240,0 +1289,2 @@\n+  void fill_words(Register base, uint64_t cnt, Register value);\n+\n@@ -1361,0 +1412,3 @@\n+\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":56,"deletions":2,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -294,0 +295,1 @@\n+    case T_INLINE_TYPE:\n@@ -327,0 +329,84 @@\n+\n+\/\/ const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_int = 6;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt, VMRegPair *regs, int total_args_passed) {\n+\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  \/\/ r1, r2 used to address klasses and states, exclude it from return convention to avoid colision\n+\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+     r0 \/* j_rarg7 *\/, j_rarg6, j_rarg5, j_rarg4, j_rarg3, j_rarg2\n+  };\n+\n+  static const FloatRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3, j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        \/\/ Should we have gurantee here?\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+      \/\/ Should T_METADATA be added to java_calling_convention as well ?\n+    case T_METADATA:\n+    case T_INLINE_TYPE:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -357,19 +443,42 @@\n-static void gen_c2i_adapter(MacroAssembler *masm,\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n-                            const VMRegPair *regs,\n-                            Label& skip_fixup) {\n-  \/\/ Before we get into the guts of the C2I adapter, see if we should be here\n-  \/\/ at all.  We've come from compiled code and are attempting to jump to the\n-  \/\/ interpreter, which means the caller made a static call to get here\n-  \/\/ (vcalls always get a compiled target if there is one).  Check for a\n-  \/\/ compiled target.  If there is one, we need to patch the caller's call.\n-  patch_callers_callsite(masm);\n-\n-  __ bind(skip_fixup);\n-\n-  int words_pushed = 0;\n-\n-  \/\/ Since all args are passed on the stack, total_args_passed *\n-  \/\/ Interpreter::stackElementSize is the space we need.\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+     for (int i = 0; i < sig_extended->length(); i++) {\n+       BasicType bt = sig_extended->at(i)._bt;\n+       if (SigEntry::is_reserved_entry(sig_extended, i)) {\n+         \/\/ Ignore reserved entry\n+       } else if (bt == T_INLINE_TYPE) {\n+         \/\/ In sig_extended, an inline type argument starts with:\n+         \/\/ T_INLINE_TYPE, followed by the types of the fields of the\n+         \/\/ inline type and T_VOID to mark the end of the value\n+         \/\/ type. Inline types are flattened so, for instance, in the\n+         \/\/ case of an inline type with an int field and an inline type\n+         \/\/ field that itself has 2 fields, an int and a long:\n+         \/\/ T_INLINE_TYPE T_INT T_INLINE_TYPE T_INT T_LONG T_VOID (second\n+         \/\/ slot for the T_LONG) T_VOID (inner T_INLINE_TYPE) T_VOID\n+         \/\/ (outer T_INLINE_TYPE)\n+         total_args_passed++;\n+         int vt = 1;\n+         do {\n+           i++;\n+           BasicType bt = sig_extended->at(i)._bt;\n+           BasicType prev_bt = sig_extended->at(i-1)._bt;\n+           if (bt == T_INLINE_TYPE) {\n+             vt++;\n+           } else if (bt == T_VOID &&\n+                      prev_bt != T_LONG &&\n+                      prev_bt != T_DOUBLE) {\n+             vt--;\n+           }\n+         } while (vt != 0);\n+       } else {\n+         total_args_passed++;\n+       }\n+     }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n@@ -377,1 +486,2 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+  return total_args_passed;\n+}\n@@ -379,3 +489,1 @@\n-  __ mov(r13, sp);\n-  \/\/ stack is aligned, keep it that way\n-  extraspace = align_up(extraspace, 2*wordSize);\n+static void gen_c2i_adapter_helper(MacroAssembler* masm, BasicType bt, const VMRegPair& reg_pair, int extraspace, const Address& to) {\n@@ -384,13 +492,1 @@\n-  if (extraspace)\n-    __ sub(sp, sp, extraspace);\n-\n-  \/\/ Now write the args into the outgoing interpreter space\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n+    assert(bt != T_INLINE_TYPE || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n@@ -411,2 +507,5 @@\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n+    \/\/ int next_off = st_off - Interpreter::stackElementSize;\n+\n+    VMReg r_1 = reg_pair.first();\n+    VMReg r_2 = reg_pair.second();\n+\n@@ -415,1 +514,1 @@\n-      continue;\n+      return;\n@@ -417,0 +516,1 @@\n+\n@@ -419,3 +519,2 @@\n-      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size\n-                    + extraspace\n-                    + words_pushed * wordSize);\n+      \/\/ words_pushed is always 0 so we don't use it.\n+      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace \/* + word_pushed * wordSize *\/);\n@@ -425,1 +524,1 @@\n-        __ str(rscratch1, Address(sp, st_off));\n+        __ str(rscratch1, to);\n@@ -428,16 +527,1 @@\n-\n-\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ str(rscratch1, Address(sp, next_off));\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaaaull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ str(rscratch1, Address(sp, st_off));\n-        }\n+        __ str(rscratch1, to);\n@@ -448,19 +532,1 @@\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ str(r, Address(sp, st_off));\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ jlong\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaabull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-          __ str(r, Address(sp, next_off));\n-        } else {\n-          __ str(r, Address(sp, st_off));\n-        }\n-      }\n+      __ str(r, to);\n@@ -471,1 +537,1 @@\n-        __ strs(r_1->as_FloatRegister(), Address(sp, st_off));\n+        __ strs(r_1->as_FloatRegister(), to);\n@@ -473,6 +539,1 @@\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaacull);\n-        __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        __ strd(r_1->as_FloatRegister(), Address(sp, next_off));\n+        __ strd(r_1->as_FloatRegister(), to);\n@@ -480,0 +541,160 @@\n+   }\n+}\n+\n+static void gen_c2i_adapter(MacroAssembler *masm,\n+                            const GrowableArray<SigEntry>* sig_extended,\n+                            const VMRegPair *regs,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+\n+  \/\/ Before we get into the guts of the C2I adapter, see if we should be here\n+  \/\/ at all.  We've come from compiled code and are attempting to jump to the\n+  \/\/ interpreter, which means the caller made a static call to get here\n+  \/\/ (vcalls always get a compiled target if there is one).  Check for a\n+  \/\/ compiled target.  If there is one, we need to patch the caller's call.\n+  patch_callers_callsite(masm);\n+\n+  __ bind(skip_fixup);\n+\n+  bool has_inline_argument = false;\n+\n+  if (InlineTypePassFieldsAsArgs) {\n+      \/\/ Is there an inline type argument?\n+     for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+       has_inline_argument = (sig_extended->at(i)._bt == T_INLINE_TYPE);\n+     }\n+     if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);\n+\n+      frame_complete = __ offset();\n+      address the_pc = __ pc();\n+\n+      __ set_last_Java_frame(noreg, noreg, the_pc, rscratch1);\n+\n+      __ mov(c_rarg0, rthread);\n+      __ mov(c_rarg1, r1);\n+      __ mov(c_rarg2, (int64_t)alloc_inline_receiver);\n+\n+      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+      __ blr(rscratch1);\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ cbz(r0, no_exception);\n+\n+      __ str(zr, Address(rthread, JavaThread::vm_result_offset()));\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ b(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(r10, rthread);\n+      __ get_vm_result_2(r1, rthread); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n+  int words_pushed = 0;\n+\n+  \/\/ Since all args are passed on the stack, total_args_passed *\n+  \/\/ Interpreter::stackElementSize is the space we need.\n+\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n+  int extraspace = (total_args_passed * Interpreter::stackElementSize) + wordSize;\n+\n+  \/\/ stack is aligned, keep it that way\n+  extraspace = align_up(extraspace, 2 * wordSize);\n+\n+  __ mov(r13, sp);\n+\n+  if (extraspace)\n+    __ sub(sp, sp, extraspace);\n+\n+  \/\/ Now write the args into the outgoing interpreter space\n+\n+  int ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+  bool has_oop_field = false;\n+\n+  for (int next_arg_comp = 0; next_arg_comp < total_args_passed; next_arg_comp++) {\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    \/\/ offset to start parameters\n+    int st_off   = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;\n+\n+    if (!InlineTypePassFieldsAsArgs || bt != T_INLINE_TYPE) {\n+\n+            if (SigEntry::is_reserved_entry(sig_extended, next_arg_comp)) {\n+               continue; \/\/ Ignore reserved entry\n+            }\n+\n+            if (bt == T_VOID) {\n+               assert(next_arg_comp > 0 && (sig_extended->at(next_arg_comp - 1)._bt == T_LONG || sig_extended->at(next_arg_comp - 1)._bt == T_DOUBLE), \"missing half\");\n+               next_arg_int ++;\n+               continue;\n+             }\n+\n+             int next_off = st_off - Interpreter::stackElementSize;\n+             int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+\n+             gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp], extraspace, Address(sp, offset));\n+             next_arg_int ++;\n+   } else {\n+       ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_INLINE_TYPE);\n+      __ load_heap_oop(rscratch1, Address(r10, index));\n+      next_vt_arg++;\n+      next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of value\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp - 1)._bt;\n+        if (bt == T_INLINE_TYPE) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n+        } else if (SigEntry::is_reserved_entry(sig_extended, next_arg_comp)) {\n+          \/\/ Ignore reserved entry\n+        } else {\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          assert(off > 0, \"offset in object should be positive\");\n+\n+          bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+          has_oop_field = has_oop_field || is_oop;\n+\n+          gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp - ignored], extraspace, Address(r11, off));\n+        }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ str(rscratch1, Address(sp, st_off));\n+   }\n+\n+  }\n+\n+\/\/ If an inline type was allocated and initialized, apply post barrier to all oop fields\n+  if (has_inline_argument && has_oop_field) {\n+    __ push(r13); \/\/ save senderSP\n+    __ push(r1); \/\/ save callee\n+    \/\/ Allocate argument register save area\n+    if (frame::arg_reg_save_area_bytes != 0) {\n+      __ sub(sp, sp, frame::arg_reg_save_area_bytes);\n@@ -481,0 +702,7 @@\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::apply_post_barriers), rthread, r10);\n+    \/\/ De-allocate argument register save area\n+    if (frame::arg_reg_save_area_bytes != 0) {\n+      __ add(sp, sp, frame::arg_reg_save_area_bytes);\n+    }\n+    __ pop(r1); \/\/ restore callee\n+    __ pop(r13); \/\/ restore sender SP\n@@ -489,0 +717,1 @@\n+void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm, int comp_args_on_stack, const GrowableArray<SigEntry>* sig, const VMRegPair *regs) {\n@@ -490,5 +719,0 @@\n-void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,\n-                                    int total_args_passed,\n-                                    int comp_args_on_stack,\n-                                    const BasicType *sig_bt,\n-                                    const VMRegPair *regs) {\n@@ -554,1 +778,1 @@\n-  int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)>>LogBytesPerWord;\n+  int comp_words_on_stack = 0;\n@@ -556,2 +780,3 @@\n-    __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n-    __ andr(sp, rscratch1, -16);\n+     comp_words_on_stack = align_up(comp_args_on_stack * VMRegImpl::stack_slot_size, wordSize) >> LogBytesPerWord;\n+     __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n+     __ andr(sp, rscratch1, -16);\n@@ -576,0 +801,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -578,2 +805,5 @@\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+    BasicType bt = sig->at(i)._bt;\n+\n+    assert(bt != T_INLINE_TYPE, \"i2c adapter doesn't unpack inline typ args\");\n+    if (bt == T_VOID) {\n+      assert(i > 0 && (sig->at(i - 1)._bt == T_LONG || sig->at(i - 1)._bt == T_DOUBLE), \"missing half\");\n@@ -584,0 +814,1 @@\n+    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(), \"scrambled load targets?\");\n@@ -585,3 +816,1 @@\n-    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),\n-            \"scrambled load targets?\");\n-    int ld_off = (total_args_passed - i - 1)*Interpreter::stackElementSize;\n+    int ld_off = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n@@ -602,1 +831,1 @@\n-      int st_off = regs[i].first()->reg2stack()*VMRegImpl::stack_slot_size;\n+      int st_off = regs[i].first()->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -619,2 +848,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n@@ -623,11 +851,28 @@\n-        __ str(rscratch2, Address(sp, st_off));\n-      }\n-    } else if (r_1->is_Register()) {  \/\/ Register argument\n-      Register r = r_1->as_Register();\n-      if (r_2->is_valid()) {\n-        \/\/\n-        \/\/ We are using two VMRegs. This can be either T_OBJECT,\n-        \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n-        \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n-        \/\/ So we must adjust where to pick up the data to match the\n-        \/\/ interpreter.\n+         __ str(rscratch2, Address(sp, st_off));\n+       }\n+     } else if (r_1->is_Register()) {  \/\/ Register argument\n+       Register r = r_1->as_Register();\n+       if (r_2->is_valid()) {\n+         \/\/\n+         \/\/ We are using two VMRegs. This can be either T_OBJECT,\n+         \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n+         \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n+         \/\/ So we must adjust where to pick up the data to match the\n+         \/\/ interpreter.\n+\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n+\n+         \/\/ this can be a misaligned move\n+         __ ldr(r, Address(esp, offset));\n+       } else {\n+         \/\/ sign extend and use a full word?\n+         __ ldrw(r, Address(esp, ld_off));\n+       }\n+     } else {\n+       if (!r_2->is_valid()) {\n+         __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n+       } else {\n+         __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n+       }\n+     }\n+   }\n@@ -635,17 +880,0 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n-\n-        \/\/ this can be a misaligned move\n-        __ ldr(r, Address(esp, offset));\n-      } else {\n-        \/\/ sign extend and use a full word?\n-        __ ldrw(r, Address(esp, ld_off));\n-      }\n-    } else {\n-      if (!r_2->is_valid()) {\n-        __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n-      } else {\n-        __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n-      }\n-    }\n-  }\n@@ -664,1 +892,0 @@\n-\n@@ -668,13 +895,1 @@\n-\/\/ ---------------------------------------------------------------\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                                            int total_args_passed,\n-                                                            int comp_args_on_stack,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n-  address i2c_entry = __ pc();\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n-\n-  address c2i_unverified_entry = __ pc();\n-  Label skip_fixup;\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n@@ -715,0 +930,34 @@\n+}\n+\n+\n+\/\/ ---------------------------------------------------------------\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n+                                                            int comp_args_on_stack,\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n+\n+  address i2c_entry = __ pc();\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n+\n+  address c2i_unverified_entry = __ pc();\n+  Label skip_fixup;\n+\n+  gen_inline_cache_check(masm, skip_fixup);\n+\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    Label unused;\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+    skip_fixup = unused;\n+  }\n@@ -716,0 +965,1 @@\n+  \/\/ Scalarized c2i adapter\n@@ -720,0 +970,1 @@\n+\n@@ -722,4 +973,4 @@\n-\n-      __ ldrw(rscratch1, Address(rmethod, Method::access_flags_offset()));\n-      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n-      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n+        Register flags  = rscratch1;\n+      __ ldrw(flags, Address(rmethod, Method::access_flags_offset()));\n+      __ tst(flags, JVM_ACC_STATIC);\n+      __ br(Assembler::NE, L_skip_barrier); \/\/ non-static\n@@ -729,3 +980,6 @@\n-    __ load_method_holder(rscratch2, rmethod);\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    Register klass = rscratch1;\n+    __ load_method_holder(klass, rmethod);\n+    \/\/ We pass rthread to this function on x86\n+    __ clinit_barrier(klass, rscratch2, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n@@ -742,0 +996,14 @@\n+  address c2i_unverified_inline_entry = c2i_unverified_entry;\n+\n+ \/\/ Non-scalarized c2i adapter\n+  address c2i_inline_entry = c2i_entry;\n+  if (regs != regs_cc) {\n+    Label inline_entry_skip_fixup;\n+    c2i_unverified_inline_entry = __ pc();\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    Label unused;\n+    gen_c2i_adapter(masm, sig, regs, inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+  }\n+\n@@ -743,1 +1011,8 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+\n+  \/\/ The c2i adapter might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+\n+  bool caller_must_gc_arguments = (regs != regs_cc);\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words + 10, oop_maps, caller_must_gc_arguments);\n+\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -786,0 +1061,1 @@\n+      case T_INLINE_TYPE:\n@@ -1637,0 +1913,1 @@\n+      case T_INLINE_TYPE:\n@@ -1824,0 +2101,1 @@\n+  case T_INLINE_TYPE:\n@@ -3049,0 +3327,105 @@\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler _masm(&buffer);\n+  MacroAssembler* masm = &_masm;\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ strs(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_DOUBLE) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(r0, val);\n+      \/\/ We don't need barriers because the destination is a newly allocated object.\n+      \/\/ Also, we cannot use store_heap_oop(to, val) because it uses r8 as tmp.\n+      if (UseCompressedOops) {\n+        __ encode_heap_oop(val);\n+        __ str(val, to);\n+      } else {\n+        __ str(val, to);\n+      }\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ store_sized_value(to, r_1->as_Register(), size_in_bytes);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(lr);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ ldrs(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ ldrd(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+       assert_different_registers(r0, r_1->as_Register());\n+       __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(lr);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, unpack_fields_off);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":534,"deletions":151,"binary":false,"changes":685,"status":"modified"},{"patch":"@@ -307,1 +307,1 @@\n-    \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+    \/\/ T_OBJECT, T_INLINE_TYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n@@ -311,1 +311,1 @@\n-    Label is_long, is_float, is_double, exit;\n+    Label is_long, is_float, is_double, is_value, exit;\n@@ -315,0 +315,2 @@\n+    __ cmp(j_rarg1, (u1)T_INLINE_TYPE);\n+    __ br(Assembler::EQ, is_value);\n@@ -369,0 +371,13 @@\n+    __ BIND(is_value);\n+    if (InlineTypeReturnedAsFields) {\n+      \/\/ Check for flattened return value\n+      __ cbz(r0, is_long);\n+      \/\/ Initialize pre-allocated buffer\n+      __ mov(r1, r0);\n+      __ andr(r1, r1, -2);\n+      __ ldr(r1, Address(r1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));\n+      __ ldr(r0, Address(j_rarg2, 0));\n+      __ blr(r1);\n+      __ b(exit);\n+    }\n@@ -1801,1 +1816,1 @@\n-    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, AS_RAW);  \/\/ store the oop\n+    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n@@ -5701,0 +5716,178 @@\n+  \/\/ Call here from the interpreter or compiled code to either load\n+  \/\/ multiple returned values from the inline type instance being\n+  \/\/ returned to registers or to store returned values to a newly\n+  \/\/ allocated inline type instance.\n+  address generate_return_value_stub(address destination, const char* name, bool has_res) {\n+\n+    \/\/ Information about frame layout at time of blocking runtime call.\n+    \/\/ Note that we only have to preserve callee-saved registers since\n+    \/\/ the compilers are responsible for supplying a continuation point\n+    \/\/ if they expect all registers to be preserved.\n+    \/\/ n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0\n+    enum layout {\n+      rfp_off = 0, rfp_off2,\n+\n+      j_rarg7_off, j_rarg7_2,\n+      j_rarg6_off, j_rarg6_2,\n+      j_rarg5_off, j_rarg5_2,\n+      j_rarg4_off, j_rarg4_2,\n+      j_rarg3_off, j_rarg3_2,\n+      j_rarg2_off, j_rarg2_2,\n+      j_rarg1_off, j_rarg1_2,\n+      j_rarg0_off, j_rarg0_2,\n+\n+      j_farg0_off, j_farg0_2,\n+      j_farg1_off, j_farg1_2,\n+      j_farg2_off, j_farg2_2,\n+      j_farg3_off, j_farg3_2,\n+      j_farg4_off, j_farg4_2,\n+      j_farg5_off, j_farg5_2,\n+      j_farg6_off, j_farg6_2,\n+      j_farg7_off, j_farg7_2,\n+\n+      return_off, return_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    int insts_size = 512;\n+    int locs_size  = 64;\n+\n+    CodeBuffer code(name, insts_size, locs_size);\n+    OopMapSet* oop_maps  = new OopMapSet();\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+\n+    address start = __ pc();\n+\n+    const Address f7_save       (rfp, j_farg7_off * wordSize);\n+    const Address f6_save       (rfp, j_farg6_off * wordSize);\n+    const Address f5_save       (rfp, j_farg5_off * wordSize);\n+    const Address f4_save       (rfp, j_farg4_off * wordSize);\n+    const Address f3_save       (rfp, j_farg3_off * wordSize);\n+    const Address f2_save       (rfp, j_farg2_off * wordSize);\n+    const Address f1_save       (rfp, j_farg1_off * wordSize);\n+    const Address f0_save       (rfp, j_farg0_off * wordSize);\n+\n+    const Address r0_save      (rfp, j_rarg0_off * wordSize);\n+    const Address r1_save      (rfp, j_rarg1_off * wordSize);\n+    const Address r2_save      (rfp, j_rarg2_off * wordSize);\n+    const Address r3_save      (rfp, j_rarg3_off * wordSize);\n+    const Address r4_save      (rfp, j_rarg4_off * wordSize);\n+    const Address r5_save      (rfp, j_rarg5_off * wordSize);\n+    const Address r6_save      (rfp, j_rarg6_off * wordSize);\n+    const Address r7_save      (rfp, j_rarg7_off * wordSize);\n+\n+    \/\/ Generate oop map\n+    OopMap* map = new OopMap(framesize, 0);\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(rfp_off), rfp->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg7_off), j_rarg7->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg6_off), j_rarg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+    \/\/ This is an inlined and slightly modified version of call_VM\n+    \/\/ which has the ability to fetch the return PC out of\n+    \/\/ thread-local storage and also sets up last_Java_sp slightly\n+    \/\/ differently than the real call_VM\n+\n+    __ enter(); \/\/ Save FP and LR before call\n+\n+    assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n+\n+    \/\/ lr and fp are already in place\n+    __ sub(sp, rfp, ((unsigned)framesize - 4) << LogBytesPerInt); \/\/ prolog\n+\n+    __ strd(j_farg7, f7_save);\n+    __ strd(j_farg6, f6_save);\n+    __ strd(j_farg5, f5_save);\n+    __ strd(j_farg4, f4_save);\n+    __ strd(j_farg3, f3_save);\n+    __ strd(j_farg2, f2_save);\n+    __ strd(j_farg1, f1_save);\n+    __ strd(j_farg0, f0_save);\n+\n+    __ str(j_rarg0, r0_save);\n+    __ str(j_rarg1, r1_save);\n+    __ str(j_rarg2, r2_save);\n+    __ str(j_rarg3, r3_save);\n+    __ str(j_rarg4, r4_save);\n+    __ str(j_rarg5, r5_save);\n+    __ str(j_rarg6, r6_save);\n+    __ str(j_rarg7, r7_save);\n+\n+    int frame_complete = __ pc() - start;\n+\n+    \/\/ Set up last_Java_sp and last_Java_fp\n+    address the_pc = __ pc();\n+    __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n+\n+    \/\/ Call runtime\n+    __ mov(c_rarg0, rthread);\n+    __ mov(c_rarg1, r0);\n+\n+    BLOCK_COMMENT(\"call runtime_entry\");\n+    __ mov(rscratch1, destination);\n+    __ blr(rscratch1);\n+\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    __ reset_last_Java_frame(false);\n+    __ maybe_isb();\n+\n+    __ ldrd(j_farg7, f7_save);\n+    __ ldrd(j_farg6, f6_save);\n+    __ ldrd(j_farg5, f5_save);\n+    __ ldrd(j_farg4, f4_save);\n+    __ ldrd(j_farg3, f3_save);\n+    __ ldrd(j_farg3, f2_save);\n+    __ ldrd(j_farg1, f1_save);\n+    __ ldrd(j_farg0, f0_save);\n+\n+    __ ldr(j_rarg0, r0_save);\n+    __ ldr(j_rarg1, r1_save);\n+    __ ldr(j_rarg2, r2_save);\n+    __ ldr(j_rarg3, r3_save);\n+    __ ldr(j_rarg4, r4_save);\n+    __ ldr(j_rarg5, r5_save);\n+    __ ldr(j_rarg6, r6_save);\n+    __ ldr(j_rarg7, r7_save);\n+\n+    __ leave();\n+\n+    \/\/ check for pending exceptions\n+    Label pending;\n+    __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ cmp(rscratch1, (u1)NULL_WORD);\n+    __ br(Assembler::NE, pending);\n+\n+    if (has_res) {\n+      __ get_vm_result(r0, rthread);\n+    }\n+    __ ret(lr);\n+\n+    __ bind(pending);\n+    __ ldr(r0, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+\n+    \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+    int frame_size_in_words = (framesize >> (LogBytesPerWord - LogBytesPerInt));\n+    RuntimeStub* stub =\n+      RuntimeStub::new_runtime_stub(name, &code, frame_complete, frame_size_in_words, oop_maps, false);\n+\n+    return stub->entry_point();\n+  }\n+\n@@ -5751,0 +5944,5 @@\n+    StubRoutines::_load_inline_type_fields_in_regs =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), \"store_inline_type_fields_to_buf\", true);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":201,"deletions":3,"binary":false,"changes":204,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -438,0 +439,5 @@\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(NULL, true);\n+  }\n+\n@@ -556,0 +562,1 @@\n+  case T_INLINE_TYPE: \/\/ fall through (value types are handled with oops)\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -151,1 +151,1 @@\n-  __ store_heap_oop(dst, val, r10, r1, decorators);\n+  __ store_heap_oop(dst, val, r10, r1, noreg, decorators);\n@@ -174,0 +174,1 @@\n+  case Bytecodes::_fast_qputfield:\n@@ -750,4 +751,4 @@\n-    \/\/ ??? convention: move array into r3 for exception message\n-  __ mov(r3, array);\n-  __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n-  __ br(rscratch1);\n+  \/\/ ??? convention: move array into r3 for exception message\n+   __ mov(r3, array);\n+   __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n+   __ br(rscratch1);\n@@ -813,5 +814,15 @@\n-  __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n-  do_oop_load(_masm,\n-              Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)),\n-              r0,\n-              IS_ARRAY);\n+  if (UseFlatArray) {\n+    Label is_flat_array, done;\n+\n+    __ test_flattened_array_oop(r0, r8 \/*temp*\/, is_flat_array);\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+\n+    __ b(done);\n+    __ bind(is_flat_array);\n+    __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_load), r0, r1);\n+    __ bind(done);\n+  } else {\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+  }\n@@ -1114,0 +1125,2 @@\n+\n+  \/\/ FIXME: Could we remove the line below?\n@@ -1119,0 +1132,5 @@\n+  Label  is_flat_array;\n+  if (UseFlatArray) {\n+    __ test_flattened_array_oop(r3, r8 \/*temp*\/, is_flat_array);\n+  }\n+\n@@ -1121,0 +1139,1 @@\n+\n@@ -1123,2 +1142,1 @@\n-  __ ldr(r0, Address(r0,\n-                     ObjArrayKlass::element_klass_offset()));\n+  __ ldr(r0, Address(r0, ObjArrayKlass::element_klass_offset()));\n@@ -1129,0 +1147,1 @@\n+\n@@ -1135,0 +1154,1 @@\n+\n@@ -1138,0 +1158,1 @@\n+\n@@ -1148,0 +1169,13 @@\n+  if (EnableValhalla) {\n+    Label is_null_into_value_array_npe, store_null;\n+\n+    \/\/ No way to store null in flat array\n+    __ test_null_free_array_oop(r3, r8, is_null_into_value_array_npe);\n+    __ b(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n+\n@@ -1150,0 +1184,40 @@\n+  __ b(done);\n+\n+  if (EnableValhalla) {\n+     Label is_type_ok;\n+\n+    \/\/ store non-null value\n+    __ bind(is_flat_array);\n+\n+    \/\/ Simplistic type check...\n+    \/\/ r0 - value, r2 - index, r3 - array.\n+\n+    \/\/ Profile the not-null value's klass.\n+    \/\/ Load value class\n+     __ load_klass(r1, r0);\n+     __ profile_typecheck(r2, r1, r0); \/\/ blows r2, and r0\n+\n+    \/\/ flat value array needs exact type match\n+    \/\/ is \"r8 == r0\" (value subclass == array element superclass)\n+\n+    \/\/ Move element klass into r0\n+\n+     __ load_klass(r0, r3);\n+\n+     __ ldr(r0, Address(r0, ArrayKlass::element_klass_offset()));\n+     __ cmp(r0, r1);\n+     __ br(Assembler::EQ, is_type_ok);\n+\n+     __ profile_typecheck_failed(r2);\n+     __ b(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));\n+\n+     __ bind(is_type_ok);\n+\n+    \/\/ Reload from TOS to be safe, because of profile_typecheck that blows r2 and r0.\n+    \/\/ FIXME: Should we really do it?\n+     __ ldr(r1, at_tos());  \/\/ value\n+     __ mov(r2, r3); \/\/ array, ldr(r2, at_tos_p2());\n+     __ ldr(r3, at_tos_p1()); \/\/ index\n+     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_store), r1, r2, r3);\n+  }\n+\n@@ -2020,2 +2094,1 @@\n-void TemplateTable::if_acmp(Condition cc)\n-{\n+void TemplateTable::if_acmp(Condition cc) {\n@@ -2024,1 +2097,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -2026,0 +2099,36 @@\n+\n+  Register is_value_mask = rscratch1;\n+  __ mov(is_value_mask, markWord::always_locked_pattern);\n+\n+  if (EnableValhalla) {\n+    __ cmp(r1, r0);\n+    __ br(Assembler::EQ, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either r0 or r1 is null\n+    __ andr(r2, r0, r1);\n+    __ cbz(r2, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ ldr(r2, Address(r1, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r2, r2, is_value_mask);\n+    __ ldr(r4, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r4, r4, is_value_mask);\n+    __ andr(r2, r2, r4);\n+    __ cmp(r2,  is_value_mask);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(r2, r1);\n+    __ load_metadata(r4, r0);\n+    __ cmp(r2, r4);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(r0, r1, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(r0, r1, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -2028,0 +2137,1 @@\n+  __ bind(taken);\n@@ -2033,0 +2143,10 @@\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored... r0 answer, jmp to outcome...\n+  __ cbz(r0, not_subst);\n+  __ b(is_subst);\n+}\n+\n+\n@@ -2505,2 +2625,1 @@\n-  __ ubfxw(flags, raw_flags, ConstantPoolCacheEntry::tos_state_shift,\n-           ConstantPoolCacheEntry::tos_state_bits);\n+  __ ubfxw(flags, raw_flags, ConstantPoolCacheEntry::tos_state_shift, ConstantPoolCacheEntry::tos_state_bits);\n@@ -2541,4 +2660,61 @@\n-  do_oop_load(_masm, field, r0, IN_HEAP);\n-  __ push(atos);\n-  if (rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+  if (!EnableValhalla) {\n+    do_oop_load(_masm, field, r0, IN_HEAP);\n+    __ push(atos);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+    }\n+    __ b(Done);\n+  } else { \/\/ Valhalla\n+\n+    if (is_static) {\n+      __ load_heap_oop(r0, field);\n+      Label is_inline, isUninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_inline_type(raw_flags, r8 \/*temp*\/, is_inline);\n+        \/\/ Not inline case\n+        __ push(atos);\n+        __ b(Done);\n+      \/\/ Inline case, must not return null even if uninitialized\n+      __ bind(is_inline);\n+        __ cbz(r0, isUninitialized);\n+          __ push(atos);\n+          __ b(Done);\n+        __ bind(isUninitialized);\n+          __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);\n+          __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field), obj, raw_flags);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(Done);\n+    } else {\n+      Label isFlattened, isInitialized, is_inline, rewrite_inline;\n+        __ test_field_is_inline_type(raw_flags, r8 \/*temp*\/, is_inline);\n+        \/\/ Non-inline field case\n+        __ load_heap_oop(r0, field);\n+        __ push(atos);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+        }\n+        __ b(Done);\n+      __ bind(is_inline);\n+        __ test_field_is_inlined(raw_flags, r8 \/* temp *\/, isFlattened);\n+         \/\/ Non-inline field case\n+          __ load_heap_oop(r0, field);\n+          __ cbnz(r0, isInitialized);\n+            __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);\n+            __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_inline_type_field), obj, raw_flags);\n+          __ bind(isInitialized);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(rewrite_inline);\n+        __ bind(isFlattened);\n+          __ ldr(r10, Address(cache, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));\n+          __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);\n+          call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), obj, raw_flags, r10);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+         patch_bytecode(Bytecodes::_fast_qgetfield, bc, r1);\n+      }\n+      __ b(Done);\n+    }\n@@ -2546,1 +2722,0 @@\n-  __ b(Done);\n@@ -2716,0 +2891,1 @@\n+  const Register flags2 = r6;\n@@ -2738,0 +2914,2 @@\n+  __ mov(flags2, flags);\n+\n@@ -2780,8 +2958,50 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, r0, IN_HEAP);\n-    if (rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n-    }\n-    __ b(Done);\n+     if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n+      }\n+      __ b(Done);\n+     } else { \/\/ Valhalla\n+\n+      __ pop(atos);\n+      if (is_static) {\n+        Label not_inline;\n+         __ test_field_is_not_inline_type(flags2, r8 \/* temp *\/, not_inline);\n+         __ null_check(r0);\n+         __ bind(not_inline);\n+         do_oop_store(_masm, field, r0, IN_HEAP);\n+         __ b(Done);\n+      } else {\n+        Label is_inline, isFlattened, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_inline_type(flags2, r8 \/*temp*\/, is_inline);\n+        \/\/ Not inline case\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ bind(rewrite_not_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+        \/\/ Implementation of the inline semantic\n+        __ bind(is_inline);\n+        __ null_check(r0);\n+        __ test_field_is_inlined(flags2, r8 \/*temp*\/, isFlattened);\n+        \/\/ Not inline case\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ b(rewrite_inline);\n+        __ bind(isFlattened);\n+        pop_and_check_object(obj);\n+        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, off, obj);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_qputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+      }\n+     }  \/\/ Valhalla\n@@ -2927,0 +3147,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -2953,0 +3174,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -3006,0 +3228,13 @@\n+  case Bytecodes::_fast_qputfield: \/\/fall through\n+   {\n+      Label isFlattened, done;\n+      __ null_check(r0);\n+      __ test_field_is_flattened(r3, r8 \/* temp *\/, isFlattened);\n+      \/\/ No Flattened case\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      __ b(done);\n+      __ bind(isFlattened);\n+      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, r1, r2);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3103,0 +3338,26 @@\n+  case Bytecodes::_fast_qgetfield:\n+    {\n+       Label isFlattened, isInitialized, Done;\n+       \/\/ FIXME: We don't need to reload registers multiple times, but stay close to x86 code\n+       __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n+       __ test_field_is_inlined(r9, r8 \/* temp *\/, isFlattened);\n+        \/\/ Non-flattened field case\n+        __ mov(r9, r0);\n+        __ load_heap_oop(r0, field);\n+        __ cbnz(r0, isInitialized);\n+          __ mov(r0, r9);\n+          __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n+          __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);\n+          __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_inline_type_field), r0, r9);\n+        __ bind(isInitialized);\n+        __ verify_oop(r0);\n+        __ b(Done);\n+      __ bind(isFlattened);\n+        __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n+        __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);\n+        __ ldr(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));\n+        call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), r0, r9, r3);\n+        __ verify_oop(r0);\n+      __ bind(Done);\n+    }\n+    break;\n@@ -3658,0 +3919,24 @@\n+void TemplateTable::defaultvalue() {\n+  transition(vtos, atos);\n+  __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);\n+  __ get_constant_pool(c_rarg1);\n+  call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::defaultvalue),\n+          c_rarg1, c_rarg2);\n+  __ verify_oop(r0);\n+  \/\/ Must prevent reordering of stores for object initialization with stores that publish the new object.\n+  __ membar(Assembler::StoreStore);\n+}\n+\n+void TemplateTable::withfield() {\n+  transition(vtos, atos);\n+  resolve_cache_and_index(f2_byte, c_rarg1 \/*cache*\/, c_rarg2 \/*index*\/, sizeof(u2));\n+\n+  \/\/ n.b. unlike x86 cache is now rcpool plus the indexed offset\n+  \/\/ so using rcpool to meet shared code expectations\n+\n+  call_VM(r1, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), rcpool);\n+  __ verify_oop(r1);\n+  __ add(esp, esp, r0);\n+  __ mov(r0, r1);\n+}\n+\n@@ -3729,0 +4014,3 @@\n+  __ b(done);\n+  __ bind(is_null);\n+\n@@ -3731,4 +4019,16 @@\n-    __ b(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n+  if (EnableValhalla) {\n+    \/\/ Get cpool & tags index\n+    __ get_cpool_and_tags(r2, r3); \/\/ r2=cpool, r3=tags array\n+    __ get_unsigned_2_byte_index_at_bcp(r19, 1); \/\/ r19=index\n+     \/\/ See if bytecode has already been quicked\n+    __ add(rscratch1, r3, Array<u1>::base_offset_in_bytes());\n+    __ lea(r1, Address(rscratch1, r19));\n+    __ ldarb(r1, r1);\n+    \/\/ See if CP entry is a Q-descriptor\n+    __ andr (r1, r1, JVM_CONSTANT_QDescBit);\n+    __ cmp(r1, (u1) JVM_CONSTANT_QDescBit);\n+    __ br(Assembler::NE, done);\n+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":334,"deletions":34,"binary":false,"changes":368,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -496,0 +497,1 @@\n+    case T_INLINE_TYPE:\n@@ -529,0 +531,82 @@\n+\/\/ Same as java_calling_convention() but for multiple return\n+\/\/ values. There's no way to store them on the stack so if we don't\n+\/\/ have enough registers, multiple values can't be returned.\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    rax, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+  static const XMMRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3,\n+    j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_INLINE_TYPE:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+    case T_METADATA:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -571,0 +655,108 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+    for (int i = 0; i < sig_extended->length(); i++) {\n+      BasicType bt = sig_extended->at(i)._bt;\n+      if (SigEntry::is_reserved_entry(sig_extended, i)) {\n+        \/\/ Ignore reserved entry\n+      } else if (bt == T_INLINE_TYPE) {\n+        \/\/ In sig_extended, an inline type argument starts with:\n+        \/\/ T_INLINE_TYPE, followed by the types of the fields of the\n+        \/\/ inline type and T_VOID to mark the end of the value\n+        \/\/ type. Inline types are flattened so, for instance, in the\n+        \/\/ case of an inline type with an int field and an inline type\n+        \/\/ field that itself has 2 fields, an int and a long:\n+        \/\/ T_INLINE_TYPE T_INT T_INLINE_TYPE T_INT T_LONG T_VOID (second\n+        \/\/ slot for the T_LONG) T_VOID (inner T_INLINE_TYPE) T_VOID\n+        \/\/ (outer T_INLINE_TYPE)\n+        total_args_passed++;\n+        int vt = 1;\n+        do {\n+          i++;\n+          BasicType bt = sig_extended->at(i)._bt;\n+          BasicType prev_bt = sig_extended->at(i-1)._bt;\n+          if (bt == T_INLINE_TYPE) {\n+            vt++;\n+          } else if (bt == T_VOID &&\n+                     prev_bt != T_LONG &&\n+                     prev_bt != T_DOUBLE) {\n+            vt--;\n+          }\n+        } while (vt != 0);\n+      } else {\n+        total_args_passed++;\n+      }\n+    }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  assert(bt != T_INLINE_TYPE || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"must be invalid\");\n+    return;\n+  }\n+\n+  if (!r_1->is_XMMRegister()) {\n+    Register val = rax;\n+    if (r_1->is_stack()) {\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, rscratch1);\n+    if (is_oop) {\n+      __ push(r13);\n+      __ push(rbx);\n+      __ store_heap_oop(to, val, rscratch1, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      __ pop(rbx);\n+      __ pop(r13);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    }\n+  }\n+}\n@@ -573,3 +765,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -577,1 +767,6 @@\n-                            Label& skip_fixup) {\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n@@ -587,0 +782,42 @@\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_INLINE_TYPE);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types.\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);\n+\n+      frame_complete = __ offset();\n+\n+      __ set_last_Java_frame(noreg, noreg, NULL);\n+\n+      __ mov(c_rarg0, r15_thread);\n+      __ mov(c_rarg1, rbx);\n+      __ mov64(c_rarg2, (int64_t)alloc_inline_receiver);\n+      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+      __ jcc(Assembler::equal, no_exception);\n+\n+      __ movptr(Address(r15_thread, JavaThread::vm_result_offset()), (int)NULL_WORD);\n+      __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+      __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(rscratch2, r15_thread); \/\/ Use rscratch2 (r11) as temporary because rscratch1 (r10) is trashed by movptr()\n+      __ get_vm_result_2(rbx, r15_thread); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n@@ -591,1 +828,1 @@\n-\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n@@ -609,54 +846,19 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n-\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n-    }\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rax\n-      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ movl(rax, Address(rsp, ld_off));\n-        __ movptr(Address(rsp, st_off), rax);\n-\n-      } else {\n-\n-        __ movq(rax, Address(rsp, ld_off));\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ movq(Address(rsp, next_off), rax);\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n-          __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ movq(Address(rsp, st_off), rax);\n-        }\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_INLINE_TYPE,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_INLINE_TYPE\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_INLINE_TYPE) {\n+      if (SigEntry::is_reserved_entry(sig_extended, next_arg_comp)) {\n+        continue; \/\/ Ignore reserved entry\n@@ -665,27 +867,7 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ movl(Address(rsp, st_off), r);\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ long\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaab));\n-          __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-          __ movq(Address(rsp, next_off), r);\n-        } else {\n-          __ movptr(Address(rsp, st_off), r);\n-        }\n-      }\n-    } else {\n-      assert(r_1->is_XMMRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ movflt(Address(rsp, st_off), r_1->as_XMMRegister());\n-      } else {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(rsp, offset), extraspace, false);\n+      next_arg_int++;\n@@ -693,0 +875,1 @@\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n@@ -694,1 +877,1 @@\n-        __ mov64(rax, CONST64(0xdeadffffdeadaaac));\n+        __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n@@ -696,2 +879,39 @@\n-#endif \/* ASSERT *\/\n-        __ movdbl(Address(rsp, next_off), r_1->as_XMMRegister());\n+#endif \/* ASSERT *\/\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_INLINE_TYPE);\n+      __ load_heap_oop(r14, Address(rscratch2, index));\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp-1)._bt;\n+        if (bt == T_INLINE_TYPE) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID &&\n+                   prev_bt != T_LONG &&\n+                   prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n+        } else if (SigEntry::is_reserved_entry(sig_extended, next_arg_comp)) {\n+          \/\/ Ignore reserved entry\n+        } else {\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(r14, off), extraspace, is_oop);\n+        }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ movptr(Address(rsp, st_off), r14);\n@@ -721,2 +941,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>* sig,\n@@ -815,1 +1034,1 @@\n-  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_offset())));\n+  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_inline_offset())));\n@@ -829,0 +1048,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -832,1 +1053,3 @@\n-    if (sig_bt[i] == T_VOID) {\n+    BasicType bt = sig->at(i)._bt;\n+    assert(bt != T_INLINE_TYPE, \"i2c adapter doesn't unpack inline type args\");\n+    if (bt == T_VOID) {\n@@ -835,1 +1058,2 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      BasicType prev_bt = (i > 0) ? sig->at(i-1)._bt : T_ILLEGAL;\n+      assert(i > 0 && (prev_bt == T_LONG || prev_bt == T_DOUBLE), \"missing half\");\n@@ -877,1 +1101,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -892,1 +1116,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -923,1 +1147,1 @@\n-  \/\/ only needed becaus eof c2 resolve stubs return Method* as a result in\n+  \/\/ only needed because of c2 resolve stubs return Method* as a result in\n@@ -929,0 +1153,22 @@\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Label ok;\n+\n+  Register holder = rax;\n+  Register receiver = j_rarg0;\n+  Register temp = rbx;\n+\n+  __ load_klass(temp, receiver, rscratch1);\n+  __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n+  __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n+  __ jcc(Assembler::equal, ok);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+\n+  __ bind(ok);\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);\n+  __ jcc(Assembler::equal, skip_fixup);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n+\n@@ -931,4 +1177,8 @@\n-                                                            int total_args_passed,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n@@ -937,2 +1187,1 @@\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -951,11 +1200,1 @@\n-  Label ok;\n-\n-  Register holder = rax;\n-  Register receiver = j_rarg0;\n-  Register temp = rbx;\n-  {\n-    __ load_klass(temp, receiver, rscratch1);\n-    __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ jcc(Assembler::equal, ok);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -964,7 +1203,9 @@\n-    __ bind(ok);\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);\n-    __ jcc(Assembler::equal, skip_fixup);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    Label inline_ro_skip_fixup;\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, inline_ro_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n@@ -973,0 +1214,1 @@\n+  \/\/ Scalarized c2i adapter\n@@ -1001,1 +1243,14 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, true);\n+\n+  address c2i_unverified_inline_entry = c2i_unverified_entry;\n+\n+  \/\/ Non-scalarized c2i adapter\n+  address c2i_inline_entry = c2i_entry;\n+  if (regs != regs_cc) {\n+    Label inline_entry_skip_fixup;\n+    c2i_unverified_inline_entry = __ pc();\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+  }\n@@ -1004,1 +1259,7 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+\n+  \/\/ The c2i adapters might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  bool caller_must_gc_arguments = (regs != regs_cc);\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n+\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -1062,0 +1323,1 @@\n+      case T_INLINE_TYPE:\n@@ -1412,1 +1674,1 @@\n-          map->set_oop(VMRegImpl::stack2reg(slot));;\n+          map->set_oop(VMRegImpl::stack2reg(slot));\n@@ -1446,0 +1708,1 @@\n+        case T_INLINE_TYPE:\n@@ -2359,0 +2622,1 @@\n+      case T_INLINE_TYPE:\n@@ -2494,0 +2758,4 @@\n+    if (EnableValhalla && !UseBiasedLocking) {\n+      \/\/ For slow path is_always_locked, using biased, which is never natural for !UseBiasLocking\n+      __ andptr(swap_reg, ~((int) markWord::biased_lock_bit_in_place));\n+    }\n@@ -2555,0 +2823,1 @@\n+  case T_INLINE_TYPE:           \/\/ Really a handle\n@@ -4073,0 +4342,111 @@\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  __ movptr(rax, Address(r13, 0));\n+  __ resolve_jobject(rax \/* value *\/,\n+                     r15_thread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ movptr(Address(r13, 0), rax);\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r14, r13, rbx, rscratch1);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r14, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(0);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(rax, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(rax, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  if (StressInlineTypeReturnedAsFields) {\n+    __ load_klass(rax, rax, rscratch1);\n+    __ orptr(rax, 1);\n+  }\n+\n+  __ ret(0);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":504,"deletions":124,"binary":false,"changes":628,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -157,1 +158,1 @@\n-  __ store_heap_oop(dst, val, rdx, rbx, decorators);\n+  __ store_heap_oop(dst, val, rdx, rbx, noreg, decorators);\n@@ -180,0 +181,1 @@\n+  case Bytecodes::_fast_qputfield:\n@@ -372,0 +374,1 @@\n+  __ andl(rdx, ~JVM_CONSTANT_QDescBit);\n@@ -823,9 +826,27 @@\n-  \/\/ rax: index\n-  \/\/ rdx: array\n-  index_check(rdx, rax); \/\/ kills rbx\n-  do_oop_load(_masm,\n-              Address(rdx, rax,\n-                      UseCompressedOops ? Address::times_4 : Address::times_ptr,\n-                      arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n-              rax,\n-              IS_ARRAY);\n+  Register array = rdx;\n+  Register index = rax;\n+\n+  index_check(array, index); \/\/ kills rbx\n+  __ profile_array(rbx, array, rcx);\n+  if (UseFlatArray) {\n+    Label is_flat_array, done;\n+    __ test_flattened_array_oop(array, rbx, is_flat_array);\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+    __ jmp(done);\n+    __ bind(is_flat_array);\n+    __ read_flattened_element(array, index, rbx, rcx, rax);\n+    __ bind(done);\n+  } else {\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+  }\n+  __ profile_element(rbx, rax, rcx);\n@@ -1117,1 +1138,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1129,0 +1150,4 @@\n+\n+  __ profile_array(rdi, rdx, rbx);\n+  __ profile_element(rdi, rax, rbx);\n+\n@@ -1132,0 +1157,1 @@\n+  \/\/ Move array class to rdi\n@@ -1133,0 +1159,6 @@\n+  __ load_klass(rdi, rdx, tmp_load_klass);\n+  if (UseFlatArray) {\n+    __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+    __ test_flattened_array_layout(rbx, is_flat_array);\n+  }\n+\n@@ -1135,3 +1167,2 @@\n-  \/\/ Move superklass into rax\n-  __ load_klass(rax, rdx, tmp_load_klass);\n-  __ movptr(rax, Address(rax,\n+  \/\/ Move array element superklass into rax\n+  __ movptr(rax, Address(rdi,\n@@ -1142,1 +1173,2 @@\n-  __ gen_subtype_check(rbx, ok_is_subtype);\n+  \/\/ is \"rbx <: rax\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(rbx, ok_is_subtype, false);\n@@ -1160,1 +1192,2 @@\n-  __ profile_null_seen(rbx);\n+  if (EnableValhalla) {\n+    Label is_null_into_value_array_npe, store_null;\n@@ -1162,0 +1195,9 @@\n+    \/\/ No way to store null in null-free array\n+    __ test_null_free_array_oop(rdx, rbx, is_null_into_value_array_npe);\n+    __ jmp(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ jump(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n@@ -1164,0 +1206,7 @@\n+  __ jmp(done);\n+\n+  if (EnableValhalla) {\n+    Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n+\n+    \/\/ Simplistic type check...\n@@ -1165,0 +1214,27 @@\n+    \/\/ Profile the not-null value's klass.\n+    __ load_klass(rbx, rax, tmp_load_klass);\n+    \/\/ Move element klass into rax\n+    __ movptr(rax, Address(rdi, ArrayKlass::element_klass_offset()));\n+    \/\/ flat value array needs exact type match\n+    \/\/ is \"rax == rbx\" (value subclass == array element superclass)\n+    __ cmpptr(rax, rbx);\n+    __ jccb(Assembler::equal, is_type_ok);\n+\n+    __ jump(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));\n+\n+    __ bind(is_type_ok);\n+    \/\/ rbx: value's klass\n+    \/\/ rdx: array\n+    \/\/ rdi: array klass\n+    __ test_klass_is_empty_inline_type(rbx, rax, done);\n+\n+    \/\/ calc dst for copy\n+    __ movl(rax, at_tos_p1()); \/\/ index\n+    __ data_for_value_array_index(rdx, rdi, rax, rax);\n+\n+    \/\/ ...and src for copy\n+    __ movptr(rcx, at_tos());  \/\/ value\n+    __ data_for_oop(rcx, rcx, rbx);\n+\n+    __ access_value_copy(IN_HEAP, rcx, rax, rbx);\n+  }\n@@ -2411,1 +2487,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -2413,0 +2489,36 @@\n+\n+  const int is_inline_type_mask = markWord::always_locked_pattern;\n+  if (EnableValhalla) {\n+    __ cmpoop(rdx, rax);\n+    __ jcc(Assembler::equal, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either rax or rdx is null\n+    __ movptr(rbx, rdx);\n+    __ andptr(rbx, rax);\n+    __ testptr(rbx, rbx);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ movptr(rbx, Address(rdx, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, is_inline_type_mask);\n+    __ movptr(rcx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, is_inline_type_mask);\n+    __ andptr(rbx, rcx);\n+    __ cmpl(rbx, is_inline_type_mask);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(rbx, rdx);\n+    __ load_metadata(rcx, rax);\n+    __ cmpptr(rbx, rcx);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(rax, rdx, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(rax, rdx, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -2415,0 +2527,1 @@\n+  __ bind(taken);\n@@ -2420,0 +2533,9 @@\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored...rax answer, jmp to outcome...\n+  __ testl(rax, rax);\n+  __ jcc(Assembler::zero, not_subst);\n+  __ jmp(is_subst);\n+}\n+\n@@ -2686,1 +2808,2 @@\n-  __ remove_activation(state, rbcp);\n+\n+  __ remove_activation(state, rbcp, true, true, true);\n@@ -2884,0 +3007,1 @@\n+  const Register flags2 = rdx;\n@@ -2889,2 +3013,0 @@\n-  if (!is_static) pop_and_check_object(obj);\n-\n@@ -2893,1 +3015,9 @@\n-  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj;\n+  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj, notInlineType;\n+\n+  if (!is_static) {\n+    __ movptr(rcx, Address(cache, index, Address::times_ptr,\n+                           in_bytes(ConstantPoolCache::base_offset() +\n+                                    ConstantPoolCacheEntry::f1_offset())));\n+  }\n+\n+  __ movl(flags2, flags);\n@@ -2903,0 +3033,1 @@\n+  if (!is_static) pop_and_check_object(obj);\n@@ -2912,0 +3043,1 @@\n+\n@@ -2914,1 +3046,1 @@\n-\n+   if (!is_static) pop_and_check_object(obj);\n@@ -2929,4 +3061,82 @@\n-  do_oop_load(_masm, field, rax);\n-  __ push(atos);\n-  if (!is_static && rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+  if (!EnableValhalla) {\n+    if (!is_static) pop_and_check_object(obj);\n+    do_oop_load(_masm, field, rax);\n+    __ push(atos);\n+    if (!is_static && rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+    }\n+    __ jmp(Done);\n+  } else {\n+    if (is_static) {\n+      __ load_heap_oop(rax, field);\n+      Label is_inline_type, uninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);\n+        \/\/ field is not an inline type\n+        __ push(atos);\n+        __ jmp(Done);\n+      \/\/ field is an inline type, must not return null even if uninitialized\n+      __ bind(is_inline_type);\n+        __ testptr(rax, rax);\n+        __ jcc(Assembler::zero, uninitialized);\n+          __ push(atos);\n+          __ jmp(Done);\n+        __ bind(uninitialized);\n+          __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+#ifdef _LP64\n+          Label slow_case, finish;\n+          __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+          __ jcc(Assembler::notEqual, slow_case);\n+        __ get_default_value_oop(rcx, off, rax);\n+        __ jmp(finish);\n+        __ bind(slow_case);\n+#endif \/\/ LP64\n+          __ call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field),\n+                 obj, flags2);\n+#ifdef _LP64\n+          __ bind(finish);\n+#endif \/\/ _LP64\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(Done);\n+    } else {\n+      Label is_inlined, nonnull, is_inline_type, rewrite_inline;\n+      __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);\n+        \/\/ field is not an inline type\n+        pop_and_check_object(obj);\n+        __ load_heap_oop(rax, field);\n+        __ push(atos);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+        }\n+        __ jmp(Done);\n+      __ bind(is_inline_type);\n+        __ test_field_is_inlined(flags2, rscratch1, is_inlined);\n+          \/\/ field is not inlined\n+          __ movptr(rax, rcx);  \/\/ small dance required to preserve the klass_holder somewhere\n+          pop_and_check_object(obj);\n+          __ push(rax);\n+          __ load_heap_oop(rax, field);\n+          __ pop(rcx);\n+          __ testptr(rax, rax);\n+          __ jcc(Assembler::notZero, nonnull);\n+            __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+            __ get_inline_type_field_klass(rcx, flags2, rbx);\n+            __ get_default_value_oop(rbx, rcx, rax);\n+          __ bind(nonnull);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(rewrite_inline);\n+        __ bind(is_inlined);\n+        \/\/ field is inlined\n+          __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+          pop_and_check_object(rax);\n+          __ read_inlined_field(rcx, flags2, rbx, rax);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_qgetfield, bc, rbx);\n+      }\n+      __ jmp(Done);\n+    }\n@@ -2934,1 +3144,0 @@\n-  __ jmp(Done);\n@@ -2937,0 +3146,3 @@\n+\n+  if (!is_static) pop_and_check_object(obj);\n+\n@@ -3036,0 +3248,15 @@\n+void TemplateTable::withfield() {\n+  transition(vtos, atos);\n+\n+  Register cache = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n+  Register index = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n+\n+  resolve_cache_and_index(f2_byte, cache, index, sizeof(u2));\n+\n+  call_VM(rbx, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), cache);\n+  \/\/ new value type is returned in rbx\n+  \/\/ stack adjustement is returned in rax\n+  __ verify_oop(rbx);\n+  __ addptr(rsp, rax);\n+  __ movptr(rax, rbx);\n+}\n@@ -3131,0 +3358,1 @@\n+  const Register flags2 = rdx;\n@@ -3147,0 +3375,1 @@\n+  __ movl(flags2, flags);\n@@ -3149,1 +3378,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags, flags2);\n@@ -3155,1 +3384,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags, flags2);\n@@ -3161,1 +3390,1 @@\n-                                              Register obj, Register off, Register flags) {\n+                                              Register obj, Register off, Register flags, Register flags2) {\n@@ -3168,1 +3397,1 @@\n-        notLong, notFloat, notObj;\n+        notLong, notFloat, notObj, notInlineType;\n@@ -3211,6 +3440,53 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, rax);\n-    if (!is_static && rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+    if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, rax);\n+      if (!is_static && rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+      }\n+      __ jmp(Done);\n+    } else {\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_inline_type;\n+        __ test_field_is_not_inline_type(flags2, rscratch1, is_inline_type);\n+        __ null_check(rax);\n+        __ bind(is_inline_type);\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(Done);\n+      } else {\n+        Label is_inline_type, is_inlined, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);\n+        \/\/ Not an inline type\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        __ bind(rewrite_not_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(is_inline_type);\n+        __ null_check(rax);\n+        __ test_field_is_inlined(flags2, rscratch1, is_inlined);\n+        \/\/ field is not inlined\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(rewrite_inline);\n+        __ bind(is_inlined);\n+        \/\/ field is inlined\n+        pop_and_check_object(obj);\n+        assert_different_registers(rax, rdx, obj, off);\n+        __ load_klass(rdx, rax, rscratch1);\n+        __ data_for_oop(rax, rax, rdx);\n+        __ addptr(obj, off);\n+        __ access_value_copy(IN_HEAP, rax, obj, rdx);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_qputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+      }\n@@ -3218,1 +3494,0 @@\n-    __ jmp(Done);\n@@ -3357,0 +3632,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -3382,0 +3658,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/ fall through\n@@ -3421,0 +3698,4 @@\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rscratch2, rdx);  \/\/ saving flags for is_inlined test\n+  }\n+\n@@ -3434,1 +3715,4 @@\n-  fast_storefield_helper(field, rax);\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rdx, rscratch2);  \/\/ restoring flags for is_inlined test\n+  }\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3440,1 +3724,4 @@\n-  fast_storefield_helper(field, rax);\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rdx, rscratch2);  \/\/ restoring flags for is_inlined test\n+  }\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3445,1 +3732,1 @@\n-void TemplateTable::fast_storefield_helper(Address field, Register rax) {\n+void TemplateTable::fast_storefield_helper(Address field, Register rax, Register flags) {\n@@ -3449,0 +3736,17 @@\n+  case Bytecodes::_fast_qputfield:\n+    {\n+      Label is_inlined, done;\n+      __ null_check(rax);\n+      __ test_field_is_inlined(flags, rscratch1, is_inlined);\n+      \/\/ field is not inlined\n+      do_oop_store(_masm, field, rax);\n+      __ jmp(done);\n+      __ bind(is_inlined);\n+      \/\/ field is inlined\n+      __ load_klass(rdx, rax, rscratch1);\n+      __ data_for_oop(rax, rax, rdx);\n+      __ lea(rcx, field);\n+      __ access_value_copy(IN_HEAP, rax, rcx, rdx);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3450,1 +3754,3 @@\n-    do_oop_store(_masm, field, rax);\n+    {\n+      do_oop_store(_masm, field, rax);\n+    }\n@@ -3520,1 +3826,1 @@\n-  __ movptr(rbx, Address(rcx, rbx, Address::times_ptr,\n+  __ movptr(rdx, Address(rcx, rbx, Address::times_ptr,\n@@ -3527,1 +3833,1 @@\n-  Address field(rax, rbx, Address::times_1);\n+  Address field(rax, rdx, Address::times_1);\n@@ -3531,0 +3837,39 @@\n+  case Bytecodes::_fast_qgetfield:\n+    {\n+      Label is_inlined, nonnull, Done;\n+      __ movptr(rscratch1, Address(rcx, rbx, Address::times_ptr,\n+                                   in_bytes(ConstantPoolCache::base_offset() +\n+                                            ConstantPoolCacheEntry::flags_offset())));\n+      __ test_field_is_inlined(rscratch1, rscratch2, is_inlined);\n+        \/\/ field is not inlined\n+        __ load_heap_oop(rax, field);\n+        __ testptr(rax, rax);\n+        __ jcc(Assembler::notZero, nonnull);\n+          __ movl(rdx, Address(rcx, rbx, Address::times_ptr,\n+                             in_bytes(ConstantPoolCache::base_offset() +\n+                                      ConstantPoolCacheEntry::flags_offset())));\n+          __ andl(rdx, ConstantPoolCacheEntry::field_index_mask);\n+          __ movptr(rcx, Address(rcx, rbx, Address::times_ptr,\n+                                       in_bytes(ConstantPoolCache::base_offset() +\n+                                                ConstantPoolCacheEntry::f1_offset())));\n+          __ get_inline_type_field_klass(rcx, rdx, rbx);\n+          __ get_default_value_oop(rbx, rcx, rax);\n+        __ bind(nonnull);\n+        __ verify_oop(rax);\n+        __ jmp(Done);\n+      __ bind(is_inlined);\n+      \/\/ field is inlined\n+        __ push(rdx); \/\/ save offset\n+        __ movl(rdx, Address(rcx, rbx, Address::times_ptr,\n+                           in_bytes(ConstantPoolCache::base_offset() +\n+                                    ConstantPoolCacheEntry::flags_offset())));\n+        __ andl(rdx, ConstantPoolCacheEntry::field_index_mask);\n+        __ movptr(rcx, Address(rcx, rbx, Address::times_ptr,\n+                                     in_bytes(ConstantPoolCache::base_offset() +\n+                                              ConstantPoolCacheEntry::f1_offset())));\n+        __ pop(rbx); \/\/ restore offset\n+        __ read_inlined_field(rcx, rdx, rbx, rax);\n+      __ bind(Done);\n+      __ verify_oop(rax);\n+    }\n+    break;\n@@ -4004,3 +4349,1 @@\n-  Label slow_case_no_pop;\n-  Label initialize_header;\n-  Label initialize_object;  \/\/ including clearing the fields\n+  Label is_not_value;\n@@ -4016,1 +4359,1 @@\n-  __ jcc(Assembler::notEqual, slow_case_no_pop);\n+  __ jcc(Assembler::notEqual, slow_case);\n@@ -4020,1 +4363,7 @@\n-  __ push(rcx);  \/\/ save the contexts of klass for initializing the header\n+\n+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InstanceKlass::_kind_inline_type);\n+  __ jcc(Assembler::notEqual, is_not_value);\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_InstantiationError));\n+\n+  __ bind(is_not_value);\n@@ -4023,1 +4372,0 @@\n-  \/\/ make sure klass is fully initialized\n@@ -4027,19 +4375,2 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ movl(rdx, Address(rcx, Klass::layout_helper_offset()));\n-  \/\/ test to see if it has a finalizer or is malformed in some way\n-  __ testl(rdx, Klass::_lh_instance_slow_path_bit);\n-  __ jcc(Assembler::notZero, slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/  Else If inline contiguous allocations are enabled:\n-  \/\/    Try to allocate in eden.\n-  \/\/    If fails due to heap end, go to slow path.\n-  \/\/\n-  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n+  __ allocate_instance(rcx, rax, rdx, rbx, true, slow_case);\n+  __ jmp(done);\n@@ -4047,2 +4378,2 @@\n-  const bool allow_shared_alloc =\n-    Universe::heap()->supports_inline_contig_alloc();\n+  \/\/ slow case\n+  __ bind(slow_case);\n@@ -4050,6 +4381,2 @@\n-  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n-#ifndef _LP64\n-  if (UseTLAB || allow_shared_alloc) {\n-    __ get_thread(thread);\n-  }\n-#endif \/\/ _LP64\n+  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n+  Register rarg2 = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n@@ -4057,15 +4384,4 @@\n-  if (UseTLAB) {\n-    __ tlab_allocate(thread, rax, rdx, 0, rcx, rbx, slow_case);\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ jmp(initialize_header);\n-    } else {\n-      \/\/ initialize both the header and fields\n-      __ jmp(initialize_object);\n-    }\n-  } else {\n-    \/\/ Allocation in the shared Eden, if allowed.\n-    \/\/\n-    \/\/ rdx: instance size in bytes\n-    __ eden_allocate(thread, rax, rdx, 0, rbx, slow_case);\n-  }\n+  __ get_constant_pool(rarg1);\n+  __ get_unsigned_2_byte_index_at_bcp(rarg2, 1);\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);\n+   __ verify_oop(rax);\n@@ -4073,24 +4389,3 @@\n-  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n-  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n-  if (UseTLAB || allow_shared_alloc) {\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    __ bind(initialize_object);\n-    __ decrement(rdx, sizeof(oopDesc));\n-    __ jcc(Assembler::zero, initialize_header);\n-\n-    \/\/ Initialize topmost object field, divide rdx by 8, check if odd and\n-    \/\/ test if zero.\n-    __ xorl(rcx, rcx);    \/\/ use zero reg to clear memory (shorter code)\n-    __ shrl(rdx, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n-\n-    \/\/ rdx must have been multiple of 8\n-#ifdef ASSERT\n-    \/\/ make sure rdx was multiple of 8\n-    Label L;\n-    \/\/ Ignore partial flag stall after shrl() since it is debug VM\n-    __ jcc(Assembler::carryClear, L);\n-    __ stop(\"object size is not multiple of 2 - adjust this code\");\n-    __ bind(L);\n-    \/\/ rdx must be > 0, no extra check needed here\n-#endif\n+  \/\/ continue\n+  __ bind(done);\n+}\n@@ -4098,8 +4393,2 @@\n-    \/\/ initialize remaining object fields: rdx was a multiple of 8\n-    { Label loop;\n-    __ bind(loop);\n-    __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n-    __ decrement(rdx);\n-    __ jcc(Assembler::notZero, loop);\n-    }\n+void TemplateTable::defaultvalue() {\n+  transition(vtos, atos);\n@@ -4107,17 +4396,3 @@\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    if (UseBiasedLocking) {\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);\n-    } else {\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()),\n-                (intptr_t)markWord::prototype().value()); \/\/ header\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-    }\n-#ifdef _LP64\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n-#endif\n-    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-    __ store_klass(rax, rcx, tmp_store_klass);  \/\/ klass\n+  Label slow_case;\n+  Label done;\n+  Label is_value;\n@@ -4125,8 +4400,2 @@\n-    {\n-      SkipIfEqual skip_if(_masm, &DTraceAllocProbes, 0);\n-      \/\/ Trigger dtrace event for fastpath\n-      __ push(atos);\n-      __ call_VM_leaf(\n-           CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), rax);\n-      __ pop(atos);\n-    }\n+  __ get_unsigned_2_byte_index_at_bcp(rdx, 1);\n+  __ get_cpool_and_tags(rcx, rax);\n@@ -4134,2 +4403,25 @@\n-    __ jmp(done);\n-  }\n+  \/\/ Make sure the class we're about to instantiate has been resolved.\n+  \/\/ This is done before loading InstanceKlass to be consistent with the order\n+  \/\/ how Constant Pool is updated (see ConstantPool::klass_at_put)\n+  const int tags_offset = Array<u1>::base_offset_in_bytes();\n+  __ cmpb(Address(rax, rdx, Address::times_1, tags_offset), JVM_CONSTANT_Class);\n+  __ jcc(Assembler::notEqual, slow_case);\n+\n+  \/\/ get InstanceKlass\n+  __ load_resolved_klass_at_index(rcx, rcx, rdx);\n+\n+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InstanceKlass::_kind_inline_type);\n+  __ jcc(Assembler::equal, is_value);\n+\n+  \/\/ in the future, defaultvalue will just return null instead of throwing an exception\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_IncompatibleClassChangeError));\n+\n+  __ bind(is_value);\n+\n+  \/\/ make sure klass is fully initialized\n+  __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+  __ jcc(Assembler::notEqual, slow_case);\n+\n+  \/\/ have a resolved InlineKlass in rcx, return the default value oop from it\n+  __ get_default_value_oop(rcx, rdx, rax);\n+  __ jmp(done);\n@@ -4137,4 +4429,1 @@\n-  \/\/ slow case\n-  __ pop(rcx);   \/\/ restore stack pointer to what it was when we came in.\n-  __ bind(slow_case_no_pop);\n-  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n+  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n@@ -4145,3 +4434,4 @@\n-  __ get_constant_pool(rarg1);\n-  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);\n-   __ verify_oop(rax);\n+  __ get_constant_pool(rarg1);\n+\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::defaultvalue),\n+      rarg1, rarg2);\n@@ -4150,1 +4440,1 @@\n-  \/\/ continue\n+  __ verify_oop(rax);\n@@ -4190,4 +4480,5 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+      Address::times_1,\n+      Array<u1>::base_offset_in_bytes()));\n+  __ andl (rdx, ~JVM_CONSTANT_QDescBit);\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4232,0 +4523,3 @@\n+  __ jmp(done);\n+\n+  __ bind(is_null);\n@@ -4235,4 +4529,15 @@\n-    __ jmp(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n+  if (EnableValhalla) {\n+    \/\/ Get cpool & tags index\n+    __ get_cpool_and_tags(rcx, rdx); \/\/ rcx=cpool, rdx=tags array\n+    __ get_unsigned_2_byte_index_at_bcp(rbx, 1); \/\/ rbx=index\n+    \/\/ See if CP entry is a Q-descriptor\n+    __ movzbl(rcx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+    __ andl (rcx, JVM_CONSTANT_QDescBit);\n+    __ cmpl(rcx, JVM_CONSTANT_QDescBit);\n+    __ jcc(Assembler::notEqual, done);\n+    __ jump(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+  }\n+\n@@ -4254,4 +4559,5 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+  __ andl (rdx, ~JVM_CONSTANT_QDescBit);\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4310,1 +4616,0 @@\n-\n@@ -4374,0 +4679,11 @@\n+  const int is_inline_type_mask = markWord::always_locked_pattern;\n+  Label has_identity;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ andptr(rbx, is_inline_type_mask);\n+  __ cmpl(rbx, is_inline_type_mask);\n+  __ jcc(Assembler::notEqual, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n@@ -4473,0 +4789,11 @@\n+  const int is_inline_type_mask = markWord::always_locked_pattern;\n+  Label has_identity;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ andptr(rbx, is_inline_type_mask);\n+  __ cmpl(rbx, is_inline_type_mask);\n+  __ jcc(Assembler::notEqual, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":499,"deletions":172,"binary":false,"changes":671,"status":"modified"},{"patch":"@@ -1496,1 +1496,1 @@\n-  if (!UseFastStosb && UseSSE >= 2 && UseUnalignedLoadStores) {\n+  if (UseSSE >= 2 && UseUnalignedLoadStores) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -377,0 +378,10 @@\n+void ClassLoaderData::inline_classes_do(void f(InlineKlass*)) {\n+  \/\/ Lock-free access requires load_acquire\n+  for (Klass* k = Atomic::load_acquire(&_klasses); k != NULL; k = k->next_link()) {\n+    if (k->is_inline_klass()) {\n+      f(InlineKlass::cast(k));\n+    }\n+    assert(k != k->next_link(), \"no loops!\");\n+  }\n+}\n+\n@@ -543,0 +554,2 @@\n+  inline_classes_do(InlineKlass::cleanup);\n+\n@@ -837,1 +850,5 @@\n-        MetadataFactory::free_metadata(this, (InstanceKlass*)m);\n+        if (!((Klass*)m)->is_inline_klass()) {\n+          MetadataFactory::free_metadata(this, (InstanceKlass*)m);\n+        } else {\n+          MetadataFactory::free_metadata(this, (InlineKlass*)m);\n+        }\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.cpp","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -46,0 +46,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -47,1 +49,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -1006,1 +1008,6 @@\n-      if (k->is_typeArray_klass()) {\n+      if (k->is_flatArray_klass()) {\n+        Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+        assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+        InlineKlass* vk = InlineKlass::cast(InstanceKlass::cast(element_klass));\n+        comp_mirror = Handle(THREAD, vk->java_mirror());\n+      } else if (k->is_typeArray_klass()) {\n@@ -1105,0 +1112,1 @@\n+      case T_INLINE_TYPE:\n@@ -1209,0 +1217,6 @@\n+  if (k->is_inline_klass()) {\n+    \/\/ Inline types have a val type mirror and a ref type mirror. Don't handle this for now. TODO:CDS\n+    k->set_java_mirror_handle(OopHandle());\n+    return NULL;\n+  }\n+\n@@ -1536,0 +1550,1 @@\n+  bool is_value = false;\n@@ -1541,0 +1556,1 @@\n+    is_value = k->is_inline_klass();\n@@ -1547,1 +1563,7 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    if (is_value) {\n+      st->print(\"Q\");\n+    } else {\n+      st->print(\"L\");\n+    }\n+  }\n@@ -1569,1 +1591,1 @@\n-      int         siglen = (int) strlen(sigstr);\n+      int siglen = (int) strlen(sigstr);\n@@ -2534,2 +2556,2 @@\n-      \/\/ This is simlar to classic VM.\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      \/\/ This is similar to classic VM (before HotSpot).\n+      if (method->is_object_constructor() &&\n@@ -3904,1 +3926,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n@@ -4706,0 +4728,71 @@\n+\/\/ jdk_internal_vm_jni_SubElementSelector\n+\n+int jdk_internal_vm_jni_SubElementSelector::_arrayElementType_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_subElementType_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_offset_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_isInlined_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_isInlineType_offset;\n+\n+#define SUBELEMENT_SELECTOR_FIELDS_DO(macro) \\\n+  macro(_arrayElementType_offset,  k, \"arrayElementType\", class_signature, false); \\\n+  macro(_subElementType_offset,    k, \"subElementType\",   class_signature, false); \\\n+  macro(_offset_offset,            k, \"offset\",           int_signature,   false); \\\n+  macro(_isInlined_offset,         k, \"isInlined\",        bool_signature,  false); \\\n+  macro(_isInlineType_offset,      k, \"isInlineType\",     bool_signature,  false);\n+\n+void jdk_internal_vm_jni_SubElementSelector::compute_offsets() {\n+  InstanceKlass* k = SystemDictionary::jdk_internal_vm_jni_SubElementSelector_klass();\n+  SUBELEMENT_SELECTOR_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+}\n+\n+#if INCLUDE_CDS\n+void jdk_internal_vm_jni_SubElementSelector::serialize_offsets(SerializeClosure* f) {\n+  SUBELEMENT_SELECTOR_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+#undef SUBELEMENT_SELECTOR_FIELDS_DO\n+\n+Symbol* jdk_internal_vm_jni_SubElementSelector::symbol() {\n+  return vmSymbols::jdk_internal_vm_jni_SubElementSelector();\n+}\n+\n+oop jdk_internal_vm_jni_SubElementSelector::getArrayElementType(oop obj) {\n+  return obj->obj_field(_arrayElementType_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setArrayElementType(oop obj, oop type) {\n+  obj->obj_field_put(_arrayElementType_offset, type);\n+}\n+\n+oop jdk_internal_vm_jni_SubElementSelector::getSubElementType(oop obj) {\n+  return obj->obj_field(_subElementType_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setSubElementType(oop obj, oop type) {\n+  obj->obj_field_put(_subElementType_offset, type);\n+}\n+\n+int jdk_internal_vm_jni_SubElementSelector::getOffset(oop obj) {\n+  return obj->int_field(_offset_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setOffset(oop obj, int offset) {\n+  obj->int_field_put(_offset_offset, offset);\n+}\n+\n+bool jdk_internal_vm_jni_SubElementSelector::getIsInlined(oop obj) {\n+  return obj->bool_field(_isInlined_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setIsInlined(oop obj, bool b) {\n+  obj->bool_field_put(_isInlined_offset, b);\n+}\n+\n+bool jdk_internal_vm_jni_SubElementSelector::getIsInlineType(oop obj) {\n+  return obj->bool_field(_isInlineType_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setIsInlineType(oop obj, bool b) {\n+  obj->bool_field_put(_isInlineType_offset, b);\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":100,"deletions":7,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -72,0 +73,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -80,0 +82,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -327,1 +330,1 @@\n-    \/\/ Ignore wrapping L and ;.\n+    \/\/ Ignore wrapping L and ;. (and Q and ; for value types);\n@@ -359,0 +362,4 @@\n+      if ((class_name->is_Q_array_signature() && !k->is_inline_klass()) ||\n+          (!class_name->is_Q_array_signature() && k->is_inline_klass())) {\n+            THROW_MSG_NULL(vmSymbols::java_lang_IncompatibleClassChangeError(), \"L\/Q mismatch on bottom type\");\n+          }\n@@ -368,1 +375,0 @@\n-\n@@ -512,0 +518,45 @@\n+Klass* SystemDictionary::resolve_inline_type_field_or_fail(AllFieldStream* fs,\n+                                                           Handle class_loader,\n+                                                           Handle protection_domain,\n+                                                           bool throw_error,\n+                                                           TRAPS) {\n+  Symbol* class_name = fs->signature()->fundamental_name(THREAD);\n+  class_loader = Handle(THREAD, java_lang_ClassLoader::non_reflection_class_loader(class_loader()));\n+  ClassLoaderData* loader_data = class_loader_data(class_loader);\n+  unsigned int p_hash = placeholders()->compute_hash(class_name);\n+  int p_index = placeholders()->hash_to_index(p_hash);\n+  bool throw_circularity_error = false;\n+  PlaceholderEntry* oldprobe;\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    oldprobe = placeholders()->get_entry(p_index, p_hash, class_name, loader_data);\n+    if (oldprobe != NULL &&\n+      oldprobe->check_seen_thread(THREAD, PlaceholderTable::INLINE_TYPE_FIELD)) {\n+      throw_circularity_error = true;\n+\n+    } else {\n+      placeholders()->find_and_add(p_index, p_hash, class_name, loader_data,\n+                                   PlaceholderTable::INLINE_TYPE_FIELD, NULL, THREAD);\n+    }\n+  }\n+\n+  Klass* klass = NULL;\n+  if (!throw_circularity_error) {\n+    klass = SystemDictionary::resolve_or_fail(class_name, class_loader,\n+                                               protection_domain, true, THREAD);\n+  } else {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG_NULL(vmSymbols::java_lang_ClassCircularityError(), class_name->as_C_string());\n+  }\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    placeholders()->find_and_remove(p_index, p_hash, class_name, loader_data,\n+                                    PlaceholderTable::INLINE_TYPE_FIELD, THREAD);\n+  }\n+\n+  class_name->decrement_refcount();\n+  return klass;\n+}\n+\n@@ -1040,1 +1091,1 @@\n-    if (t != T_OBJECT) {\n+    if (t != T_OBJECT && t != T_INLINE_TYPE) {\n@@ -1424,0 +1475,18 @@\n+\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik->fields(), ik->constants()); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        if (!fs.access_flags().is_static()) {\n+          \/\/ Pre-load inline class\n+          Klass* real_k = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+            class_loader, protection_domain, true, CHECK_NULL);\n+          Klass* k = ik->get_inline_type_field_klass_or_null(fs.index());\n+          if (real_k != k) {\n+            \/\/ oops, the app has substituted a different version of k!\n+            return NULL;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1460,0 +1529,7 @@\n+\n+  if (ik->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    oop val = ik->allocate_instance(CHECK_NULL);\n+    vk->set_default_value(val);\n+  }\n+\n@@ -1523,0 +1599,15 @@\n+  if (klass->has_inline_type_fields()) {\n+    for (AllFieldStream fs(klass->fields(), klass->constants()); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        if (!fs.access_flags().is_static()) {\n+          Klass* real_k = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+            Handle(THREAD, loader_data->class_loader()), domain, true, CHECK);\n+          Klass* k = klass->get_inline_type_field_klass_or_null(fs.index());\n+          assert(real_k == k, \"oops, the app has substituted a different version of k!\");\n+        } else {\n+          klass->reset_inline_type_field_klass(fs.index());\n+        }\n+      }\n+    }\n+  }\n+\n@@ -2338,1 +2429,1 @@\n-    if (t != T_OBJECT) {\n+    if (t != T_OBJECT && t != T_INLINE_TYPE) {\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":95,"deletions":4,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+#define INLINE_TYPE_MAJOR_VERSION                       56\n@@ -274,1 +275,1 @@\n-    \/\/ We need to skip the following four for bootstraping\n+    \/\/ We need to skip the following four for bootstrapping\n@@ -495,0 +496,7 @@\n+    case WRONG_INLINE_TYPE:\n+      ss->print(\"Type \");\n+      _type.details(ss);\n+      ss->print(\" and type \");\n+      _expected.details(ss);\n+      ss->print(\" must be identical inline types.\");\n+      break;\n@@ -589,0 +597,8 @@\n+VerificationType reference_or_inline_type(InstanceKlass* klass) {\n+  if (klass->is_inline_klass()) {\n+    return VerificationType::inline_type(klass->name());\n+  } else {\n+    return VerificationType::reference_type(klass->name());\n+  }\n+}\n+\n@@ -593,1 +609,1 @@\n-  _this_type = VerificationType::reference_type(klass->name());\n+  _this_type = reference_or_inline_type(klass);\n@@ -1038,1 +1054,1 @@\n-          if (!atype.is_reference_array()) {\n+          if (!atype.is_nonscalar_array()) {\n@@ -1212,1 +1228,1 @@\n-          if (!atype.is_reference_array()) {\n+          if (!atype.is_nonscalar_array()) {\n@@ -1612,1 +1628,1 @@\n-            VerificationType::reference_check(), CHECK_VERIFY(this));\n+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -1617,1 +1633,1 @@\n-            VerificationType::reference_check(), CHECK_VERIFY(this));\n+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -1668,1 +1684,1 @@\n-            VerificationType::reference_check(), CHECK_VERIFY(this));\n+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -1680,1 +1696,1 @@\n-          if (_method->name() == vmSymbols::object_initializer_name() &&\n+          if (_method->is_object_constructor() &&\n@@ -1700,0 +1716,11 @@\n+        case Bytecodes::_withfield :\n+          if (_klass->major_version() < INLINE_TYPE_MAJOR_VERSION) {\n+            class_format_error(\n+              \"withfield not supported by this class file version (%d.%d), class %s\",\n+              _klass->major_version(), _klass->minor_version(), _klass->external_name());\n+            return;\n+          }\n+          \/\/ pass FALSE, operand can't be an array type for withfield.\n+          verify_field_instructions(\n+            &bcs, &current_frame, cp, false, CHECK_VERIFY(this));\n+          no_control_flow = false; break;\n@@ -1703,4 +1730,0 @@\n-          verify_invoke_instructions(\n-            &bcs, code_length, &current_frame, (bci >= ex_min && bci < ex_max),\n-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));\n-          no_control_flow = false; break;\n@@ -1711,1 +1734,1 @@\n-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));\n+            &this_uninit, cp, &stackmap_table, CHECK_VERIFY(this));\n@@ -1729,0 +1752,22 @@\n+        case Bytecodes::_defaultvalue :\n+        {\n+          if (_klass->major_version() < INLINE_TYPE_MAJOR_VERSION) {\n+            class_format_error(\n+              \"defaultvalue not supported by this class file version (%d.%d), class %s\",\n+              _klass->major_version(), _klass->minor_version(), _klass->external_name());\n+            return;\n+          }\n+          index = bcs.get_index_u2();\n+          verify_cp_class_type(bci, index, cp, CHECK_VERIFY(this));\n+          VerificationType ref_type = cp_index_to_type(index, cp, CHECK_VERIFY(this));\n+          if (!ref_type.is_object()) {\n+            verify_error(ErrorContext::bad_type(bci,\n+                TypeOrigin::cp(index, ref_type)),\n+                \"Illegal defaultvalue instruction\");\n+            return;\n+          }\n+          VerificationType inline_type =\n+            VerificationType::change_ref_to_inline_type(ref_type);\n+          current_frame.push_stack(inline_type, CHECK_VERIFY(this));\n+          no_control_flow = false; break;\n+        }\n@@ -1769,3 +1814,3 @@\n-        case Bytecodes::_monitorexit :\n-          current_frame.pop_stack(\n-            VerificationType::reference_check(), CHECK_VERIFY(this));\n+        case Bytecodes::_monitorexit : {\n+          VerificationType ref = current_frame.pop_stack(\n+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -1773,0 +1818,1 @@\n+        }\n@@ -2031,0 +2077,1 @@\n+\n@@ -2145,1 +2192,1 @@\n-            | (1 << JVM_CONSTANT_String)  | (1 << JVM_CONSTANT_Class)\n+            | (1 << JVM_CONSTANT_String) | (1 << JVM_CONSTANT_Class)\n@@ -2321,1 +2368,1 @@\n-    (!allow_arrays || !ref_class_type.is_array())) {\n+      (!allow_arrays || !ref_class_type.is_array())) {\n@@ -2328,0 +2375,1 @@\n+\n@@ -2357,0 +2405,19 @@\n+    case Bytecodes::_withfield: {\n+      for (int i = n - 1; i >= 0; i--) {\n+        current_frame->pop_stack(field_type[i], CHECK_VERIFY(this));\n+      }\n+      \/\/ stack_object_type and target_class_type must be the same inline type.\n+      stack_object_type =\n+        current_frame->pop_stack(VerificationType::inline_type_check(), CHECK_VERIFY(this));\n+      VerificationType target_inline_type =\n+        VerificationType::change_ref_to_inline_type(target_class_type);\n+      if (!stack_object_type.equals(target_inline_type)) {\n+        verify_error(ErrorContext::bad_inline_type(bci,\n+            current_frame->stack_top_ctx(),\n+            TypeOrigin::cp(index, target_class_type)),\n+            \"Invalid type on operand stack in withfield instruction\");\n+        return;\n+      }\n+      current_frame->push_stack(target_inline_type, CHECK_VERIFY(this));\n+      break;\n+    }\n@@ -2765,1 +2832,1 @@\n-    bool in_try_block, bool *this_uninit, VerificationType return_type,\n+    bool in_try_block, bool *this_uninit,\n@@ -2797,1 +2864,1 @@\n-  \/\/ Get referenced class type\n+  \/\/ Get referenced class\n@@ -2863,2 +2930,4 @@\n-    \/\/ Make sure <init> can only be invoked by invokespecial\n-    if (opcode != Bytecodes::_invokespecial ||\n+    \/\/ Make sure <init> can only be invoked by invokespecial or invokestatic.\n+    \/\/ The allowed invocation mode of <init> depends on its signature.\n+    if ((opcode != Bytecodes::_invokespecial &&\n+         opcode != Bytecodes::_invokestatic) ||\n@@ -2873,1 +2942,1 @@\n-                  current_class()->super()->name()))) {\n+                  current_class()->super()->name()))) { \/\/ super() can never be an inline_type.\n@@ -2880,2 +2949,2 @@\n-      VerificationType unsafe_anonymous_host_type =\n-                        VerificationType::reference_type(current_class()->unsafe_anonymous_host()->name());\n+      InstanceKlass* unsafe_host = current_class()->unsafe_anonymous_host();\n+      VerificationType unsafe_anonymous_host_type = reference_or_inline_type(unsafe_host);\n@@ -2888,1 +2957,1 @@\n-                             current_class()->unsafe_anonymous_host(),\n+                             unsafe_host,\n@@ -2918,0 +2987,1 @@\n+      \/\/ (use of <init> as a static factory is handled under invokestatic)\n@@ -2932,3 +3002,4 @@\n-          VerificationType hosttype =\n-            VerificationType::reference_type(current_class()->unsafe_anonymous_host()->name());\n-          bool subtype = hosttype.is_assignable_from(top, this, false, CHECK_VERIFY(this));\n+\n+          InstanceKlass* unsafe_host = current_class()->unsafe_anonymous_host();\n+          VerificationType host_type = reference_or_inline_type(unsafe_host);\n+          bool subtype = host_type.is_assignable_from(top, this, false, CHECK_VERIFY(this));\n@@ -2987,4 +3058,3 @@\n-    if (method_name == vmSymbols::object_initializer_name()) {\n-      \/\/ <init> method must have a void return type\n-      \/* Unreachable?  Class file parser verifies that methods with '<' have\n-       * void return *\/\n+    if (method_name == vmSymbols::object_initializer_name() &&\n+        opcode != Bytecodes::_invokestatic) {\n+      \/\/ an <init> method must have a void return type, unless it's a static factory\n@@ -3003,0 +3073,8 @@\n+  } else {\n+    \/\/ an <init> method may not have a void return type, if it's a static factory\n+    if (method_name == vmSymbols::object_initializer_name() &&\n+        opcode != Bytecodes::_invokespecial) {\n+      verify_error(ErrorContext::bad_code(bci),\n+          \"Return type must be non-void in <init> static factory method\");\n+      return;\n+    }\n@@ -3050,1 +3128,2 @@\n-    \/\/ add one dimension to component with 'L' prepended and ';' postpended.\n+    char Q_or_L = component_type.is_inline_type() ? JVM_SIGNATURE_INLINE_TYPE : JVM_SIGNATURE_CLASS;\n+    \/\/ add one dimension to component with 'L' or 'Q' prepended and ';' appended.\n@@ -3054,1 +3133,1 @@\n-                         JVM_SIGNATURE_ARRAY, JVM_SIGNATURE_CLASS, component_name);\n+                         JVM_SIGNATURE_ARRAY, Q_or_L, component_name);\n@@ -3096,1 +3175,1 @@\n-    index, VerificationType::reference_check(), CHECK_VERIFY(this));\n+    index, VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -3133,1 +3212,1 @@\n-    VerificationType::reference_check(), CHECK_VERIFY(this));\n+    VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n","filename":"src\/hotspot\/share\/classfile\/verifier.cpp","additions":116,"deletions":37,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -68,0 +68,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -46,0 +46,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -73,0 +76,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -218,0 +222,4 @@\n+  if (klass->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_InstantiationError());\n+  }\n+\n@@ -242,0 +250,179 @@\n+void copy_primitive_argument(intptr_t* addr, Handle instance, int offset, BasicType type) {\n+  switch (type) {\n+  case T_BOOLEAN:\n+    instance()->bool_field_put(offset, (jboolean)*((int*)addr));\n+    break;\n+  case T_CHAR:\n+    instance()->char_field_put(offset, (jchar) *((int*)addr));\n+    break;\n+  case T_FLOAT:\n+    instance()->float_field_put(offset, (jfloat)*((float*)addr));\n+    break;\n+  case T_DOUBLE:\n+    instance()->double_field_put(offset, (jdouble)*((double*)addr));\n+    break;\n+  case T_BYTE:\n+    instance()->byte_field_put(offset, (jbyte)*((int*)addr));\n+    break;\n+  case T_SHORT:\n+    instance()->short_field_put(offset, (jshort)*((int*)addr));\n+    break;\n+  case T_INT:\n+    instance()->int_field_put(offset, (jint)*((int*)addr));\n+    break;\n+  case T_LONG:\n+    instance()->long_field_put(offset, (jlong)*((long long*)addr));\n+    break;\n+  case T_OBJECT:\n+  case T_ARRAY:\n+  case T_INLINE_TYPE:\n+    fatal(\"Should not be handled with this method\");\n+    break;\n+  default:\n+    fatal(\"Unsupported BasicType\");\n+  }\n+}\n+\n+JRT_ENTRY(void, InterpreterRuntime::defaultvalue(JavaThread* thread, ConstantPool* pool, int index))\n+  \/\/ Getting the InlineKlass\n+  Klass* k = pool->klass_at(index, CHECK);\n+  if (!k->is_inline_klass()) {\n+    \/\/ inconsistency with 'new' which throws an InstantiationError\n+    \/\/ in the future, defaultvalue will just return null instead of throwing an exception\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+  assert(k->is_inline_klass(), \"defaultvalue argument must be the inline type class\");\n+  InlineKlass* vklass = InlineKlass::cast(k);\n+\n+  vklass->initialize(THREAD);\n+  oop res = vklass->default_value();\n+  thread->set_vm_result(res);\n+JRT_END\n+\n+JRT_ENTRY(int, InterpreterRuntime::withfield(JavaThread* thread, ConstantPoolCache* cp_cache))\n+  LastFrameAccessor last_frame(thread);\n+  \/\/ Getting the InlineKlass\n+  int index = ConstantPool::decode_cpcache_index(last_frame.get_index_u2_cpcache(Bytecodes::_withfield));\n+  ConstantPoolCacheEntry* cp_entry = cp_cache->entry_at(index);\n+  assert(cp_entry->is_resolved(Bytecodes::_withfield), \"Should have been resolved\");\n+  Klass* klass = cp_entry->f1_as_klass();\n+  assert(klass->is_inline_klass(), \"withfield only applies to inline types\");\n+  InlineKlass* vklass = InlineKlass::cast(klass);\n+\n+  \/\/ Getting Field information\n+  int offset = cp_entry->f2_as_index();\n+  int field_index = cp_entry->field_index();\n+  int field_offset = cp_entry->f2_as_offset();\n+  Symbol* field_signature = vklass->field_signature(field_index);\n+  BasicType field_type = Signature::basic_type(field_signature);\n+  int return_offset = (type2size[field_type] + type2size[T_OBJECT]) * AbstractInterpreter::stackElementSize;\n+\n+  \/\/ Getting old value\n+  frame& f = last_frame.get_frame();\n+  jint tos_idx = f.interpreter_frame_expression_stack_size() - 1;\n+  int vt_offset = type2size[field_type];\n+  oop old_value = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx - vt_offset);\n+  assert(old_value != NULL && oopDesc::is_oop(old_value) && old_value->is_inline_type(),\"Verifying receiver\");\n+  Handle old_value_h(THREAD, old_value);\n+\n+  \/\/ Creating new value by copying the one passed in argument\n+  instanceOop new_value = vklass->allocate_instance_buffer(\n+      CHECK_((type2size[field_type]) * AbstractInterpreter::stackElementSize));\n+  Handle new_value_h = Handle(THREAD, new_value);\n+  vklass->inline_copy_oop_to_new_oop(old_value_h(), new_value_h());\n+\n+  \/\/ Updating the field specified in arguments\n+  if (field_type == T_ARRAY || field_type == T_OBJECT) {\n+    oop aoop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);\n+    assert(aoop == NULL || oopDesc::is_oop(aoop),\"argument must be a reference type\");\n+    new_value_h()->obj_field_put(field_offset, aoop);\n+  } else if (field_type == T_INLINE_TYPE) {\n+    if (cp_entry->is_inlined()) {\n+      oop vt_oop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);\n+      assert(vt_oop != NULL && oopDesc::is_oop(vt_oop) && vt_oop->is_inline_type(),\"argument must be an inline type\");\n+      InlineKlass* field_vk = InlineKlass::cast(vklass->get_inline_type_field_klass(field_index));\n+      assert(vt_oop != NULL && field_vk == vt_oop->klass(), \"Must match\");\n+      field_vk->write_inlined_field(new_value_h(), offset, vt_oop, CHECK_(return_offset));\n+    } else { \/\/ not inlined\n+      oop voop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);\n+      if (voop == NULL && cp_entry->is_inline_type()) {\n+        THROW_(vmSymbols::java_lang_NullPointerException(), return_offset);\n+      }\n+      assert(voop == NULL || oopDesc::is_oop(voop),\"checking argument\");\n+      new_value_h()->obj_field_put(field_offset, voop);\n+    }\n+  } else { \/\/ not T_OBJECT nor T_ARRAY nor T_INLINE_TYPE\n+    intptr_t* addr = f.interpreter_frame_expression_stack_at(tos_idx);\n+    copy_primitive_argument(addr, new_value_h, field_offset, field_type);\n+  }\n+\n+  \/\/ returning result\n+  thread->set_vm_result(new_value_h());\n+  return return_offset;\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::uninitialized_static_inline_type_field(JavaThread* thread, oopDesc* mirror, int index))\n+  \/\/ The interpreter tries to access an inline static field that has not been initialized.\n+  \/\/ This situation can happen in different scenarios:\n+  \/\/   1 - if the load or initialization of the field failed during step 8 of\n+  \/\/       the initialization of the holder of the field, in this case the access to the field\n+  \/\/       must fail\n+  \/\/   2 - it can also happen when the initialization of the holder class triggered the initialization of\n+  \/\/       another class which accesses this field in its static initializer, in this case the\n+  \/\/       access must succeed to allow circularity\n+  \/\/ The code below tries to load and initialize the field's class again before returning the default value.\n+  \/\/ If the field was not initialized because of an error, a exception should be thrown.\n+  \/\/ If the class is being initialized, the default value is returned.\n+  instanceHandle mirror_h(THREAD, (instanceOop)mirror);\n+  InstanceKlass* klass = InstanceKlass::cast(java_lang_Class::as_Klass(mirror));\n+  if (klass->is_being_initialized() && klass->is_reentrant_initialization(THREAD)) {\n+    int offset = klass->field_offset(index);\n+    Klass* field_k = klass->get_inline_type_field_klass_or_null(index);\n+    if (field_k == NULL) {\n+      field_k = SystemDictionary::resolve_or_fail(klass->field_signature(index)->fundamental_name(THREAD),\n+          Handle(THREAD, klass->class_loader()),\n+          Handle(THREAD, klass->protection_domain()),\n+          true, CHECK);\n+      assert(field_k != NULL, \"Should have been loaded or an exception thrown above\");\n+      klass->set_inline_type_field_klass(index, field_k);\n+    }\n+    field_k->initialize(CHECK);\n+    oop defaultvalue = InlineKlass::cast(field_k)->default_value();\n+    \/\/ It is safe to initialized the static field because 1) the current thread is the initializing thread\n+    \/\/ and is the only one that can access it, and 2) the field is actually not initialized (i.e. null)\n+    \/\/ otherwise the JVM should not be executing this code.\n+    mirror->obj_field_put(offset, defaultvalue);\n+    thread->set_vm_result(defaultvalue);\n+  } else {\n+    assert(klass->is_in_error_state(), \"If not initializing, initialization must have failed to get there\");\n+    ResourceMark rm(THREAD);\n+    const char* desc = \"Could not initialize class \";\n+    const char* className = klass->external_name();\n+    size_t msglen = strlen(desc) + strlen(className) + 1;\n+    char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+    if (NULL == message) {\n+      \/\/ Out of memory: can't create detailed error message\n+      THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), className);\n+    } else {\n+      jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+      THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), message);\n+    }\n+  }\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::read_inlined_field(JavaThread* thread, oopDesc* obj, int index, Klass* field_holder))\n+  Handle obj_h(THREAD, obj);\n+\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+\n+  assert(field_holder->is_instance_klass(), \"Sanity check\");\n+  InstanceKlass* klass = InstanceKlass::cast(field_holder);\n+\n+  assert(klass->field_is_inlined(index), \"Sanity check\");\n+\n+  InlineKlass* field_vklass = InlineKlass::cast(klass->get_inline_type_field_klass(index));\n+  assert(field_vklass->is_initialized(), \"Must be initialized at this point\");\n+\n+  oop res = field_vklass->read_inlined_field(obj_h(), klass->field_offset(index), CHECK);\n+  thread->set_vm_result(res);\n+JRT_END\n@@ -251,1 +438,8 @@\n-  objArrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n+  bool      is_qtype_desc = pool->tag_at(index).is_Qdescriptor_klass();\n+  arrayOop obj;\n+  if ((!klass->is_array_klass()) && is_qtype_desc) { \/\/ Logically creates elements, ensure klass init\n+    klass->initialize(CHECK);\n+    obj = oopFactory::new_flatArray(klass, size, CHECK);\n+  } else {\n+    obj = oopFactory::new_objArray(klass, size, CHECK);\n+  }\n@@ -255,0 +449,10 @@\n+JRT_ENTRY(void, InterpreterRuntime::value_array_load(JavaThread* thread, arrayOopDesc* array, int index))\n+  flatArrayHandle vah(thread, (flatArrayOop)array);\n+  oop value_holder = flatArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK);\n+  thread->set_vm_result(value_holder);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::value_array_store(JavaThread* thread, void* val, arrayOopDesc* array, int index))\n+  assert(val != NULL, \"can't store null into flat array\");\n+  ((flatArrayOop)array)->value_copy_to_index((oop)val, index);\n+JRT_END\n@@ -260,2 +464,3 @@\n-  int          i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n-  Klass* klass   = constants->klass_at(i, CHECK);\n+  int i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n+  Klass* klass = constants->klass_at(i, CHECK);\n+  bool is_qtype = klass->name()->is_Q_array_signature();\n@@ -266,0 +471,4 @@\n+  if (is_qtype) { \/\/ Logically creates elements, ensure klass init\n+    klass->initialize(CHECK);\n+  }\n+\n@@ -290,0 +499,23 @@\n+JRT_ENTRY(jboolean, InterpreterRuntime::is_substitutable(JavaThread* thread, oopDesc* aobj, oopDesc* bobj))\n+  assert(oopDesc::is_oop(aobj) && oopDesc::is_oop(bobj), \"must be valid oops\");\n+\n+  Handle ha(THREAD, aobj);\n+  Handle hb(THREAD, bobj);\n+  JavaValue result(T_BOOLEAN);\n+  JavaCallArguments args;\n+  args.push_oop(ha);\n+  args.push_oop(hb);\n+  methodHandle method(thread, Universe::is_substitutable_method());\n+  JavaCalls::call(&result, method, &args, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    \/\/ Something really bad happened because isSubstitutable() should not throw exceptions\n+    \/\/ If it is an error, just let it propagate\n+    \/\/ If it is an exception, wrap it into an InternalError\n+    if (!PENDING_EXCEPTION->is_a(SystemDictionary::Error_klass())) {\n+      Handle e(THREAD, PENDING_EXCEPTION);\n+      CLEAR_PENDING_EXCEPTION;\n+      THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in substitutability test\", e, false);\n+    }\n+  }\n+  return result.get_jboolean();\n+JRT_END\n@@ -613,0 +845,4 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_InstantiationError(JavaThread* thread))\n+  THROW(vmSymbols::java_lang_InstantiationError());\n+JRT_END\n+\n@@ -643,1 +879,1 @@\n-                    bytecode == Bytecodes::_putstatic);\n+                    bytecode == Bytecodes::_putstatic || bytecode == Bytecodes::_withfield);\n@@ -645,0 +881,1 @@\n+  bool is_inline_type  = bytecode == Bytecodes::_withfield;\n@@ -688,3 +925,9 @@\n-    get_code = ((is_static) ? Bytecodes::_getstatic : Bytecodes::_getfield);\n-    if ((is_put && !has_initialized_final_update) || !info.access_flags().is_final()) {\n-      put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_putfield);\n+    if (is_static) {\n+      get_code = Bytecodes::_getstatic;\n+    } else {\n+      get_code = Bytecodes::_getfield;\n+    }\n+    if (is_put && is_inline_type) {\n+        put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_withfield);\n+    } else if ((is_put && !has_initialized_final_update) || !info.access_flags().is_final()) {\n+        put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_putfield);\n@@ -703,0 +946,2 @@\n+    info.is_inlined(),\n+    info.is_inline_type(),\n@@ -952,0 +1197,1 @@\n+  case Bytecodes::_withfield:\n@@ -1183,0 +1429,1 @@\n+  bool is_inlined = cp_entry->is_inlined();\n@@ -1191,1 +1438,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(cp_entry_f1, cp_entry->f2_as_index(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(cp_entry_f1, cp_entry->f2_as_index(), is_static, is_inlined);\n@@ -1221,0 +1468,6 @@\n+\n+  \/\/ Both Q-signatures and L-signatures are mapped to atos\n+  if (cp_entry->flag_state() == atos && ik->field_signature(index)->is_Q_signature()) {\n+    sig_type = JVM_SIGNATURE_INLINE_TYPE;\n+  }\n+\n@@ -1222,0 +1475,1 @@\n+  bool is_inlined = cp_entry->is_inlined();\n@@ -1224,1 +1478,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, cp_entry->f2_as_index(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, cp_entry->f2_as_index(), is_static, is_inlined);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":263,"deletions":9,"binary":false,"changes":272,"status":"modified"},{"patch":"@@ -1260,1 +1260,1 @@\n-              Deoptimization::reassign_fields(fst.current(), fst.register_map(), objects, realloc_failures, false);\n+              Deoptimization::reassign_fields(fst.current(), fst.register_map(), objects, realloc_failures, false, CHECK_NULL);\n@@ -1520,1 +1520,1 @@\n-  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false);\n+  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false, THREAD);\n@@ -1919,1 +1919,1 @@\n-    if (m->is_initializer() && !m->is_static()) {\n+    if (m->is_object_constructor()) {\n@@ -1949,1 +1949,1 @@\n-    if (!m->is_initializer() && !m->is_overpass()) {\n+    if (!(m->is_object_constructor() || m->is_class_initializer()) && !m->is_overpass()) {\n@@ -2541,2 +2541,1 @@\n-  if (m->is_initializer()) {\n-    if (m->is_static_initializer()) {\n+  if (m->is_class_initializer()) {\n@@ -2545,1 +2544,2 @@\n-    }\n+  }\n+  else if (m->is_object_constructor() || m->is_static_init_factory()) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -146,0 +146,2 @@\n+LatestMethodCache* Universe::_is_substitutable_cache  = NULL;\n+LatestMethodCache* Universe::_inline_type_hash_code_cache = NULL;\n@@ -154,0 +156,1 @@\n+Array<InstanceKlass*>* Universe::_the_single_IdentityObject_klass_array = NULL;\n@@ -242,0 +245,1 @@\n+  it->push(&_the_single_IdentityObject_klass_array);\n@@ -248,0 +252,2 @@\n+  _is_substitutable_cache->metaspace_pointers_do(it);\n+  _inline_type_hash_code_cache->metaspace_pointers_do(it);\n@@ -279,0 +285,1 @@\n+  f->do_ptr((void**)&_the_single_IdentityObject_klass_array);\n@@ -284,0 +291,2 @@\n+  _is_substitutable_cache->serialize(f);\n+  _inline_type_hash_code_cache->serialize(f);\n@@ -331,1 +340,1 @@\n-        _the_array_interfaces_array     = MetadataFactory::new_array<Klass*>(null_cld, 2, NULL, CHECK);\n+        _the_array_interfaces_array     = MetadataFactory::new_array<Klass*>(null_cld, 3, NULL, CHECK);\n@@ -358,0 +367,5 @@\n+      assert(_the_array_interfaces_array->at(2) ==\n+                   SystemDictionary::IdentityObject_klass(), \"u3\");\n+\n+      assert(_the_single_IdentityObject_klass_array->at(0) ==\n+          SystemDictionary::IdentityObject_klass(), \"u3\");\n@@ -364,0 +378,1 @@\n+      _the_array_interfaces_array->at_put(2, SystemDictionary::IdentityObject_klass());\n@@ -484,0 +499,8 @@\n+void Universe::initialize_the_single_IdentityObject_klass_array(InstanceKlass* ik, TRAPS) {\n+    assert(_the_single_IdentityObject_klass_array == NULL, \"Must not be initialized twice\");\n+    assert(ik->name() == vmSymbols::java_lang_IdentityObject(), \"Must be\");\n+    Array<InstanceKlass*>* array = MetadataFactory::new_array<InstanceKlass*>(ik->class_loader_data(), 1, NULL, CHECK);\n+    array->at_put(0, ik);\n+    _the_single_IdentityObject_klass_array = array;\n+  }\n+\n@@ -765,1 +788,0 @@\n-\n@@ -788,0 +810,2 @@\n+  Universe::_is_substitutable_cache = new LatestMethodCache();\n+  Universe::_inline_type_hash_code_cache = new LatestMethodCache();\n@@ -956,0 +980,11 @@\n+\n+  \/\/ Set up substitutability testing\n+  ResourceMark rm;\n+  initialize_known_method(_is_substitutable_cache,\n+                          SystemDictionary::ValueBootstrapMethods_klass(),\n+                          vmSymbols::isSubstitutable_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true, CHECK);\n+  initialize_known_method(_inline_type_hash_code_cache,\n+                          SystemDictionary::ValueBootstrapMethods_klass(),\n+                          vmSymbols::inlineObjectHashCode_name()->as_C_string(),\n+                          vmSymbols::object_int_signature(), true, CHECK);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":37,"deletions":2,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -130,0 +130,2 @@\n+  static LatestMethodCache* _is_substitutable_cache;   \/\/ ValueBootstrapMethods.isSubstitutable() method\n+  static LatestMethodCache* _inline_type_hash_code_cache;  \/\/ ValueBootstrapMethods.inlineObjectHashCode() method\n@@ -135,0 +137,1 @@\n+  static Array<InstanceKlass*>* _the_single_IdentityObject_klass_array;\n@@ -157,0 +160,1 @@\n+\n@@ -287,0 +291,3 @@\n+  static Method*      is_substitutable_method()       { return _is_substitutable_cache->get_method(); }\n+  static Method*      inline_type_hash_code_method()  { return _inline_type_hash_code_cache->get_method(); }\n+\n@@ -311,0 +318,6 @@\n+  static Array<InstanceKlass*>*  the_single_IdentityObject_klass_array() {\n+    assert(_the_single_IdentityObject_klass_array != NULL, \"Must be initialized before use\");\n+    assert(_the_single_IdentityObject_klass_array->length() == 1, \"Sanity check\");\n+    return _the_single_IdentityObject_klass_array;\n+  }\n+  static void initialize_the_single_IdentityObject_klass_array(InstanceKlass* ik, TRAPS);\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -57,0 +57,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -116,1 +117,0 @@\n-\n@@ -153,0 +153,5 @@\n+address Method::get_c2i_inline_entry() {\n+  assert(adapter() != NULL, \"must have\");\n+  return adapter()->get_c2i_inline_entry();\n+}\n+\n@@ -158,0 +163,5 @@\n+address Method::get_c2i_unverified_inline_entry() {\n+  assert(adapter() != NULL, \"must have\");\n+  return adapter()->get_c2i_unverified_inline_entry();\n+}\n+\n@@ -350,0 +360,2 @@\n+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_inline_ro_entry);\n+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_inline_entry);\n@@ -594,0 +606,18 @@\n+\/\/ InlineKlass the method is declared to return. This must not\n+\/\/ safepoint as it is called with references live on the stack at\n+\/\/ locations the GC is unaware of.\n+InlineKlass* Method::returned_inline_type(Thread* thread) const {\n+  SignatureStream ss(signature());\n+  while (!ss.at_return_type()) {\n+    ss.next();\n+  }\n+  Handle class_loader(thread, method_holder()->class_loader());\n+  Handle protection_domain(thread, method_holder()->protection_domain());\n+  Klass* k = NULL;\n+  {\n+    NoSafepointVerifier nsv;\n+    k = ss.as_klass(class_loader, protection_domain, SignatureStream::ReturnNull, thread);\n+  }\n+  assert(k != NULL && !thread->has_pending_exception(), \"can't resolve klass\");\n+  return InlineKlass::cast(k);\n+}\n@@ -604,1 +634,1 @@\n-  \/\/   aload_0\n+  \/\/   aload_0, _fast_aload_0, or _nofast_aload_0\n@@ -628,1 +658,2 @@\n-  if (cb[0] != Bytecodes::_aload_0 || cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {\n+  if ((cb[0] != Bytecodes::_aload_0 && cb[0] != Bytecodes::_fast_aload_0 && cb[0] != Bytecodes::_nofast_aload_0) ||\n+       cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {\n@@ -780,7 +811,2 @@\n-bool Method::is_initializer() const {\n-  return is_object_initializer() || is_static_initializer();\n-}\n-\n-bool Method::has_valid_initializer_flags() const {\n-  return (is_static() ||\n-          method_holder()->major_version() < 51);\n+bool Method::is_object_constructor_or_class_initializer() const {\n+  return (is_object_constructor() || is_class_initializer());\n@@ -789,1 +815,1 @@\n-bool Method::is_static_initializer() const {\n+bool Method::is_class_initializer() const {\n@@ -793,2 +819,8 @@\n-  return name() == vmSymbols::class_initializer_name() &&\n-         has_valid_initializer_flags();\n+  return (name() == vmSymbols::class_initializer_name() &&\n+          (is_static() ||\n+           method_holder()->major_version() < 51));\n+}\n+\n+\/\/ A method named <init>, if non-static, is a classic object constructor.\n+bool Method::is_object_constructor() const {\n+   return name() == vmSymbols::object_initializer_name() && !is_static();\n@@ -797,2 +829,3 @@\n-bool Method::is_object_initializer() const {\n-   return name() == vmSymbols::object_initializer_name();\n+\/\/ A static method named <init> is a factory for an inline class.\n+bool Method::is_static_init_factory() const {\n+   return name() == vmSymbols::object_initializer_name() && is_static();\n@@ -856,1 +889,1 @@\n-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {\n+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {\n@@ -872,1 +905,3 @@\n-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;\n+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {\n+      return false;\n+    }\n@@ -1041,0 +1076,2 @@\n+    _from_compiled_inline_entry = NULL;\n+    _from_compiled_inline_ro_entry = NULL;\n@@ -1043,0 +1080,2 @@\n+    _from_compiled_inline_entry = adapter()->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = adapter()->get_c2i_inline_ro_entry();\n@@ -1087,0 +1126,1 @@\n+\n@@ -1089,1 +1129,9 @@\n-           \"must be NULL during dump time, to be initialized at run time\");\n+           \"instructions must be zeros during dump time, to be initialized at run time\");\n+\n+    _from_compiled_inline_ro_entry = cds_adapter->get_c2i_inline_ro_entry_trampoline();\n+    assert(*((int*)_from_compiled_inline_ro_entry) == 0,\n+           \"instructions must be zeros during dump time, to be initialized at run time\");\n+\n+    _from_compiled_inline_entry = cds_adapter->get_c2i_inline_entry_trampoline();\n+    assert(*((int*)_from_compiled_inline_entry) == 0,\n+           \"instructions must be zeros during dump time, to be initialized at run time\");\n@@ -1242,0 +1290,2 @@\n+    assert(mh->_from_compiled_inline_entry != NULL, \"must be\");\n+    assert(mh->_from_compiled_inline_ro_entry != NULL, \"must be\");\n@@ -1245,0 +1295,2 @@\n+    mh->_from_compiled_inline_entry = adapter->get_c2i_inline_entry();\n+    mh->_from_compiled_inline_ro_entry = adapter->get_c2i_inline_ro_entry();\n@@ -1252,0 +1304,12 @@\n+#if 0\n+  \/*\n+   * CDS:TODO --\n+   * \"Q\" classes in the method signature must be resolved during link_method.\n+   * However, at this point we are still inside method_holder()->restore_unshareable_info.\n+   * If we try to resolve method_holder(), or multually dependent classes, it will\n+   * cause deadlock and other ill effects.\n+   *\n+   * For now, lets do method linking inside InstanceKlass::link_class(). Optimization\n+   * may be possible if we know that resolution will never happen.\n+   *\/\n+\n@@ -1258,0 +1322,1 @@\n+#endif\n@@ -1260,1 +1325,1 @@\n-address Method::from_compiled_entry_no_trampoline() const {\n+address Method::from_compiled_entry_no_trampoline(bool caller_is_c1) const {\n@@ -1262,2 +1327,7 @@\n-  if (code) {\n-    return code->verified_entry_point();\n+  if (caller_is_c1) {\n+    \/\/ C1 - inline type arguments are passed as objects\n+    if (code) {\n+      return code->verified_inline_entry_point();\n+    } else {\n+      return adapter()->get_c2i_inline_entry();\n+    }\n@@ -1265,1 +1335,6 @@\n-    return adapter()->get_c2i_entry();\n+    \/\/ C2 - inline type arguments may be passed as fields\n+    if (code) {\n+      return code->verified_entry_point();\n+    } else {\n+      return adapter()->get_c2i_entry();\n+    }\n@@ -1282,0 +1357,12 @@\n+address Method::verified_inline_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_entry != NULL, \"must be set\");\n+  return _from_compiled_inline_entry;\n+}\n+\n+address Method::verified_inline_ro_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_ro_entry != NULL, \"must be set\");\n+  return _from_compiled_inline_ro_entry;\n+}\n+\n@@ -1313,0 +1400,2 @@\n+  mh->_from_compiled_inline_entry = code->verified_inline_entry_point();\n+  mh->_from_compiled_inline_ro_entry = code->verified_inline_ro_entry_point();\n@@ -2344,0 +2433,2 @@\n+  if (valid_itable_index())\n+    st->print_cr(\" - itable index:      %d\",   itable_index());\n@@ -2421,0 +2512,1 @@\n+  if (WizardMode) access_flags().print_on(st);\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":114,"deletions":22,"binary":false,"changes":136,"status":"modified"},{"patch":"@@ -66,0 +66,2 @@\n+macro(CastI2N)\n+macro(CastN2I)\n@@ -323,0 +325,2 @@\n+macro(InlineType)\n+macro(InlineTypePtr)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -408,0 +409,7 @@\n+  \/\/ Remove useless inline type nodes\n+  for (int i = _inline_type_nodes->length() - 1; i >= 0; i--) {\n+    Node* vt = _inline_type_nodes->at(i);\n+    if (!useful.member(vt)) {\n+      _inline_type_nodes->remove(vt);\n+    }\n+  }\n@@ -634,4 +642,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -644,1 +650,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -769,0 +775,4 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n@@ -923,0 +933,3 @@\n+  _has_flattened_accesses = false;\n+  _flattened_accesses_share_alias = true;\n+\n@@ -1006,0 +1019,1 @@\n+  _inline_type_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);\n@@ -1234,1 +1248,2 @@\n-    assert(InlineUnsafeOps, \"indeterminate pointers come only from unsafe ops\");\n+    bool default_value_load = EnableValhalla && tj->is_instptr()->klass() == ciEnv::current()->Class_klass();\n+    assert(InlineUnsafeOps || default_value_load, \"indeterminate pointers come only from unsafe ops\");\n@@ -1247,0 +1262,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1251,1 +1275,1 @@\n-      tj = ta = TypeAryPtr::make(ptr, ta->ary(), ta->klass(), true, offset, ta->instance_id());\n+      tj = ta = TypeAryPtr::make(ptr, ta->ary(), ta->klass(), true, Type::Offset(offset), ta->field_offset(), ta->instance_id());\n@@ -1257,0 +1281,2 @@\n+    \/\/ For flattened inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1260,1 +1286,1 @@\n-        tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),ta->ary(),ta->klass(),false,offset);\n+        tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1274,1 +1300,1 @@\n-        tj = ta = TypeAryPtr::make(ptr,ta->ary(),ta->klass(),false,offset);\n+        tj = ta = TypeAryPtr::make(ptr,ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1280,1 +1306,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,ta->klass(),false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1285,1 +1311,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());\n@@ -1289,1 +1315,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInlineType::BOTTOM && _flattened_accesses_share_alias) {\n+      const TypeAry *tary = TypeAry::make(TypeInlineType::BOTTOM, ta->size());\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1296,1 +1327,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1302,1 +1333,1 @@\n-      tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta->ary(),ta->klass(),false,offset);\n+      tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1316,1 +1347,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,Type::Offset(offset));\n@@ -1324,1 +1355,1 @@\n-      tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,offset);\n+      tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,Type::Offset(offset));\n@@ -1327,1 +1358,1 @@\n-      tj = to = TypeInstPtr::make(to->ptr(),to->klass(),to->klass_is_exact(),to->const_oop(),to->offset(), to->instance_id());\n+      tj = to = TypeInstPtr::make(to->ptr(),to->klass(),to->klass_is_exact(),to->const_oop(),Type::Offset(to->offset()), to->klass()->flatten_array(), to->instance_id());\n@@ -1334,1 +1365,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, Type::Offset(offset));\n@@ -1348,1 +1379,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, offset, to->instance_id());\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, Type::Offset(offset), canonical_holder->flatten_array(), to->instance_id());\n@@ -1350,1 +1381,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, offset);\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, Type::Offset(offset));\n@@ -1367,1 +1398,1 @@\n-                                   offset);\n+                                   Type::Offset(offset));\n@@ -1371,1 +1402,1 @@\n-    if( klass->is_obj_array_klass() ) {\n+    if (klass != NULL && klass->is_obj_array_klass()) {\n@@ -1375,1 +1406,1 @@\n-      tj = tk = TypeKlassPtr::make( TypePtr::NotNull, k, offset );\n+      tj = tk = TypeKlassPtr::make(TypePtr::NotNull, k, Type::Offset(offset));\n@@ -1391,1 +1422,1 @@\n-      tj = tk = TypeKlassPtr::make( TypePtr::NotNull, tk->klass(), offset );\n+      tj = tk = TypeKlassPtr::make(TypePtr::NotNull, tk->klass(), Type::Offset(offset));\n@@ -1530,1 +1561,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1534,3 +1565,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = NULL;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1586,0 +1620,1 @@\n+    ciField* field = NULL;\n@@ -1592,0 +1627,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1593,1 +1629,9 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (elemtype->isa_inlinetype() &&\n+          elemtype->inline_klass() != NULL &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->first_field_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1605,0 +1649,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1615,1 +1661,0 @@\n-      ciField* field;\n@@ -1622,0 +1667,4 @@\n+      } else if (tinst->klass()->is_inlinetype()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1623,1 +1672,1 @@\n-        ciInstanceKlass *k = tinst->klass()->as_instance_klass();\n+        ciInstanceKlass* k = tinst->klass()->as_instance_klass();\n@@ -1626,7 +1675,14 @@\n-      assert(field == NULL ||\n-             original_field == NULL ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset() == original_field->offset() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != NULL)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == NULL ||\n+           original_field == NULL ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset() == original_field->offset() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != NULL) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_aryptr()->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1637,3 +1693,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1641,6 +1698,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == NULL) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == NULL) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1818,0 +1876,348 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineTypeBase(), \"unexpected node\");\n+  if (_inline_type_nodes != NULL) {\n+    _inline_type_nodes->push(n);\n+  }\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineTypeBase(), \"unexpected node\");\n+  if (_inline_type_nodes != NULL && _inline_type_nodes->contains(n)) {\n+    _inline_type_nodes->remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    assert(!n->is_InlineType(), \"chain of inline type nodes\");\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineTypePtr()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool post_ea) {\n+  \/\/ Make inline types scalar in safepoints\n+  for (int i = _inline_type_nodes->length()-1; i >= 0; i--) {\n+    InlineTypeBaseNode* vt = _inline_type_nodes->at(i)->as_InlineTypeBase();\n+    vt->make_scalar_in_safepoints(&igvn);\n+  }\n+  \/\/ Remove InlineTypePtr nodes only after EA to give scalar replacement a chance\n+  \/\/ to remove buffer allocations. InlineType nodes are kept until loop opts and\n+  \/\/ removed via InlineTypeNode::remove_redundant_allocations.\n+  if (post_ea) {\n+    while (_inline_type_nodes->length() > 0) {\n+      InlineTypeBaseNode* vt = _inline_type_nodes->pop()->as_InlineTypeBase();\n+      if (vt->is_InlineTypePtr()) {\n+        igvn.replace_node(vt, vt->get_oop());\n+      }\n+    }\n+  }\n+  \/\/ Make sure that the return value does not keep an unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = NULL;\n+    for (uint i = 1; i < root()->req(); i++){\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == NULL, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != NULL) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flattened_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flattened_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flattened array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flattened array) correct. We're done with parsing so we\n+  \/\/ now know all flattened array accesses in this compile\n+  \/\/ unit. Let's move flattened array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flattened memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flattened array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = NULL;\n+      if (n->Opcode() == Op_StoreCM) {\n+        adr_type = get_adr_type(get_alias_index(n->in(MemNode::OopStore)->adr_type()));\n+      } else {\n+        adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      }\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != NULL) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flattened_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flattened array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != NULL &&\n+          ace->_adr_type->isa_aryptr() &&\n+          ace->_adr_type->is_aryptr()->is_flat()) {\n+        ace->_adr_type = NULL;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the NULL adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = NULL;\n+      if (m->Opcode() == Op_StoreCM) {\n+        adr_type = m->in(MemNode::OopStore)->adr_type();\n+        Node* clone = new StoreCMNode(m->in(MemNode::Control), m->in(MemNode::Memory), m->in(MemNode::Address),\n+                                      m->adr_type(), m->in(MemNode::ValueIn), m->in(MemNode::OopStore),\n+                                      get_alias_index(adr_type));\n+        igvn.register_new_node_with_optimizer(clone);\n+        igvn.replace_node(m, clone);\n+      } else {\n+        adr_type = m->adr_type();\n+#ifdef ASSERT\n+        m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      }\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flattened array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = NULL;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = NULL;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == NULL) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const Type* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat()) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flattened arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flattened array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const Type* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat()) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != NULL) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct();\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const Type* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+}\n+\n+\n@@ -2097,0 +2503,7 @@\n+  if (_inline_type_nodes->length() > 0) {\n+    \/\/ Do this once all inlining is over to avoid getting inconsistent debug info\n+    process_inline_types(igvn);\n+  }\n+\n+  adjust_flattened_array_access_aliases(igvn);\n+\n@@ -2129,0 +2542,5 @@\n+  if (_inline_type_nodes->length() > 0) {\n+    \/\/ Process inline types again now that EA might have simplified the graph\n+    process_inline_types(igvn, \/* post_ea= *\/ true);\n+  }\n+\n@@ -2771,0 +3189,1 @@\n+\n@@ -3509,0 +3928,8 @@\n+#ifdef ASSERT\n+  case Op_InlineTypePtr:\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n+#endif\n@@ -3856,2 +4283,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -3865,1 +4292,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -3986,1 +4413,1 @@\n-  if (StressReflectiveCode) {\n+  if (StressReflectiveCode || superk == NULL || subk == NULL) {\n@@ -3995,1 +4422,2 @@\n-  if (superelem->is_array_klass())\n+  if (superelem->is_array_klass()) {\n+    ciArrayKlass* ak = superelem->as_array_klass();\n@@ -3997,0 +4425,1 @@\n+  }\n@@ -4457,0 +4886,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == NULL || tb == NULL ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are NULL.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(NULL, a));\n+    b = phase->transform(new CastP2XNode(NULL, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":502,"deletions":52,"binary":false,"changes":554,"status":"modified"},{"patch":"@@ -144,0 +144,10 @@\n+    if ((n->Opcode() == Op_LoadX || n->Opcode() == Op_StoreX) &&\n+        !n->in(MemNode::Address)->is_AddP() &&\n+        _igvn->type(n->in(MemNode::Address))->isa_oopptr()) {\n+      \/\/ Load\/Store at mark work address is at offset 0 so has no AddP which confuses EA\n+      Node* addp = new AddPNode(n->in(MemNode::Address), n->in(MemNode::Address), _igvn->MakeConX(0));\n+      _igvn->register_new_node_with_optimizer(addp);\n+      _igvn->replace_input_of(n, MemNode::Address, addp);\n+      ideal_nodes.push(addp);\n+      _nodes.at_put_grow(addp->_idx, NULL, NULL);\n+    }\n@@ -382,0 +392,11 @@\n+      } else if (n->as_Call()->tf()->returns_inline_type_as_fields()) {\n+        bool returns_oop = false;\n+        for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax && !returns_oop; i++) {\n+          ProjNode* pn = n->fast_out(i)->as_Proj();\n+          if (pn->_con >= TypeFunc::Parms && pn->bottom_type()->isa_ptr()) {\n+            returns_oop = true;\n+          }\n+        }\n+        if (returns_oop) {\n+          add_call_node(n->as_Call());\n+        }\n@@ -413,0 +434,1 @@\n+    case Op_InlineTypePtr:\n@@ -485,2 +507,4 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_Call() &&\n+          (n->in(0)->as_Call()->returns_pointer() || n->bottom_type()->isa_ptr())) {\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+               n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -582,0 +606,1 @@\n+    case Op_InlineTypePtr:\n@@ -641,2 +666,4 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_Call() &&\n+          (n->in(0)->as_Call()->returns_pointer()|| n->bottom_type()->isa_ptr())) {\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+               n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -802,1 +829,1 @@\n-  assert(call->returns_pointer(), \"only for call which returns pointer\");\n+  assert(call->returns_pointer() || call->tf()->returns_inline_type_as_fields(), \"only for call which returns pointer\");\n@@ -889,1 +916,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -936,1 +963,1 @@\n-      const TypeTuple * d = call->tf()->domain();\n+      const TypeTuple * d = call->tf()->domain_sig();\n@@ -966,1 +993,4 @@\n-                               (aat->isa_aryptr() && aat->isa_aryptr()->klass()->is_obj_array_klass()));\n+                               (aat->isa_aryptr() && aat->isa_aryptr()->klass()->is_obj_array_klass()) ||\n+                               (aat->isa_aryptr() && aat->isa_aryptr()->elem() != NULL &&\n+                                aat->isa_aryptr()->is_flat() &&\n+                                aat->isa_aryptr()->elem()->inline_klass()->contains_oops()));\n@@ -1007,0 +1037,3 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"vectorizedMismatch\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"load_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_unknown_inline\") == 0 ||\n@@ -1068,1 +1101,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -1112,1 +1145,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_cc();\n@@ -1632,1 +1665,1 @@\n-                tty->print_cr(\"----------missed referernce to object-----------\");\n+                tty->print_cr(\"----------missed reference to object------------\");\n@@ -1634,1 +1667,1 @@\n-                tty->print_cr(\"----------object referernced by init store -----\");\n+                tty->print_cr(\"----------object referenced by init store-------\");\n@@ -1704,1 +1737,1 @@\n-         ptn->set_scalar_replaceable(false);\n+        ptn->set_scalar_replaceable(false);\n@@ -1867,1 +1900,3 @@\n-          if (not_global_escape(alock->obj_node())) {\n+          const Type* obj_type = igvn->type(alock->obj_node());\n+          if (not_global_escape(alock->obj_node()) &&\n+              !obj_type->isa_inlinetype() && !obj_type->is_inlinetypeptr()) {\n@@ -2069,0 +2104,1 @@\n+  int field_offset = adr_type->isa_aryptr() ? adr_type->isa_aryptr()->field_offset().get() : Type::OffsetBot;\n@@ -2070,1 +2106,1 @@\n-  if (offset == Type::OffsetBot) {\n+  if (offset == Type::OffsetBot && field_offset == Type::OffsetBot) {\n@@ -2082,1 +2118,1 @@\n-      ciField* field = _compile->alias_type(adr_type->isa_instptr())->field();\n+      ciField* field = _compile->alias_type(adr_type->is_ptr())->field();\n@@ -2102,1 +2138,7 @@\n-        bt = elemtype->array_element_basic_type();\n+        if (elemtype->isa_inlinetype() && field_offset != Type::OffsetBot) {\n+          ciInlineKlass* vk = elemtype->inline_klass();\n+          field_offset += vk->first_field_offset();\n+          bt = vk->get_field_by_offset(field_offset, false)->layout_type();\n+        } else {\n+          bt = elemtype->array_element_basic_type();\n+        }\n@@ -2120,1 +2162,1 @@\n-  assert(!_collecting, \"should not call when contructed graph\");\n+  assert(!_collecting, \"should not call when constructed graph\");\n@@ -2281,3 +2323,1 @@\n-  const TypePtr *t_ptr = adr_type->isa_ptr();\n-  assert(t_ptr != NULL, \"must be a pointer type\");\n-  return t_ptr->offset();\n+  return adr_type->is_ptr()->flattened_offset();\n@@ -2437,1 +2477,8 @@\n-    t = base_t->add_offset(offs)->is_oopptr();\n+    if (base_t->isa_aryptr() != NULL) {\n+      \/\/ In the case of a flattened inline type array, each field has its\n+      \/\/ own slice so we need to extract the field being accessed from\n+      \/\/ the address computation\n+      t = base_t->isa_aryptr()->add_field_offset_and_offset(offs)->is_oopptr();\n+    } else {\n+      t = base_t->add_offset(offs)->is_oopptr();\n+    }\n@@ -2439,1 +2486,1 @@\n-  int inst_id =  base_t->instance_id();\n+  int inst_id = base_t->instance_id();\n@@ -2453,1 +2500,1 @@\n-  \/\/ It could happened when CHA type is different from MDO type on a dead path\n+  \/\/ It could happen when CHA type is different from MDO type on a dead path\n@@ -2463,1 +2510,12 @@\n-  const TypeOopPtr *tinst = base_t->add_offset(t->offset())->is_oopptr();\n+  const TypePtr* tinst = base_t->add_offset(t->offset());\n+  if (tinst->isa_aryptr() && t->isa_aryptr()) {\n+    \/\/ In the case of a flattened inline type array, each field has its\n+    \/\/ own slice so we need to keep track of the field being accessed.\n+    tinst = tinst->is_aryptr()->with_field_offset(t->is_aryptr()->field_offset().get());\n+    \/\/ Keep array properties (not flat\/null-free)\n+    tinst = tinst->is_aryptr()->update_properties(t->is_aryptr());\n+    if (tinst == NULL) {\n+      return false; \/\/ Skip dead path with inconsistent properties\n+    }\n+  }\n+\n@@ -3146,0 +3204,7 @@\n+          if (tn_t->isa_aryptr()) {\n+            \/\/ Keep array properties (not flat\/null-free)\n+            tinst = tinst->is_aryptr()->update_properties(tn_t->is_aryptr());\n+            if (tinst == NULL) {\n+              continue; \/\/ Skip dead path with inconsistent properties\n+            }\n+          }\n@@ -3171,1 +3236,1 @@\n-      if(use->is_Mem() && use->in(MemNode::Address) == n) {\n+      if (use->is_Mem() && use->in(MemNode::Address) == n) {\n@@ -3207,0 +3272,8 @@\n+      } else if (use->Opcode() == Op_Return) {\n+        assert(_compile->tf()->returns_inline_type_as_fields(), \"must return an inline type\");\n+        \/\/ Get InlineKlass by removing the tag bit from the metadata pointer\n+        Node* klass = use->in(TypeFunc::Parms);\n+        intptr_t ptr = igvn->type(klass)->isa_rawptr()->get_con();\n+        clear_nth_bit(ptr, 0);\n+        assert(Metaspace::contains((void*)ptr), \"should be klass\");\n+        assert(((InlineKlass*)ptr)->contains_oops(), \"returned inline type must contain a reference field\");\n@@ -3218,1 +3291,1 @@\n-              op == Op_SubTypeCheck ||\n+              op == Op_SubTypeCheck || op == Op_InlineType || op == Op_InlineTypePtr ||\n@@ -3286,0 +3359,3 @@\n+    } else if (n->is_CallLeaf() && n->as_CallLeaf()->_name != NULL &&\n+               strcmp(n->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+      n = n->as_CallLeaf()->proj_out(TypeFunc::Memory);\n@@ -3326,1 +3402,1 @@\n-      } else if(use->is_Mem()) {\n+      } else if (use->is_Mem()) {\n@@ -3335,0 +3411,4 @@\n+      } else if (use->is_CallLeaf() && use->as_CallLeaf()->_name != NULL &&\n+                 strcmp(use->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+        \/\/ store_unknown_inline overwrites destination array\n+        memnode_worklist.append_if_missing(use);\n@@ -3356,1 +3436,1 @@\n-  \/\/            instance type to the the input corresponding to its alias index.\n+  \/\/            instance type to the input corresponding to its alias index.\n@@ -3428,2 +3508,2 @@\n-  \/\/ to recursively process Phi's encounted on the input memory\n-  \/\/ chains as is done in split_memory_phi() since they  will\n+  \/\/ to recursively process Phi's encountered on the input memory\n+  \/\/ chains as is done in split_memory_phi() since they will\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":111,"deletions":31,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -79,1 +79,2 @@\n-         ProfileTripFailed=131072};\n+         ProfileTripFailed=131072,\n+         FlattenedArrays=262144};\n@@ -104,0 +105,1 @@\n+  bool is_flattened_arrays() const { return _loop_flags & FlattenedArrays; }\n@@ -118,0 +120,1 @@\n+  void mark_flattened_arrays() { _loop_flags |= FlattenedArrays; }\n@@ -1232,1 +1235,1 @@\n-  IfNode* find_unswitching_candidate(const IdealLoopTree *loop) const;\n+  IfNode* find_unswitching_candidate(const IdealLoopTree *loop, Node_List& unswitch_iffs) const;\n@@ -1354,0 +1357,1 @@\n+  bool flatten_array_element_type_check(Node *n);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -64,0 +65,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return NULL;\n+  }\n+\n@@ -1223,0 +1230,96 @@\n+bool PhaseIdealLoop::flatten_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flattened array and the load of the value\n+  \/\/ happens with a flattened array check then: push the type check\n+  \/\/ through the phi of the flattened array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == NULL) {\n+    return false;\n+  }\n+\n+  assert(obj != NULL && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != NULL, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      _igvn.set_type(cast_clone, cast_clone->Value(&_igvn));\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1229,0 +1332,4 @@\n+  if (flatten_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1493,0 +1600,6 @@\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(&_igvn, this);\n+    return; \/\/ n is now dead\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":113,"deletions":0,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -85,50 +87,0 @@\n-void PhaseMacroExpand::migrate_outs(Node *old, Node *target) {\n-  assert(old != NULL, \"sanity\");\n-  for (DUIterator_Fast imax, i = old->fast_outs(imax); i < imax; i++) {\n-    Node* use = old->fast_out(i);\n-    _igvn.rehash_node_delayed(use);\n-    imax -= replace_input(use, old, target);\n-    \/\/ back up iterator\n-    --i;\n-  }\n-  assert(old->outcnt() == 0, \"all uses must be deleted\");\n-}\n-\n-void PhaseMacroExpand::copy_call_debug_info(CallNode *oldcall, CallNode * newcall) {\n-  \/\/ Copy debug information and adjust JVMState information\n-  uint old_dbg_start = oldcall->tf()->domain()->cnt();\n-  uint new_dbg_start = newcall->tf()->domain()->cnt();\n-  int jvms_adj  = new_dbg_start - old_dbg_start;\n-  assert (new_dbg_start == newcall->req(), \"argument count mismatch\");\n-\n-  \/\/ SafePointScalarObject node could be referenced several times in debug info.\n-  \/\/ Use Dict to record cloned nodes.\n-  Dict* sosn_map = new Dict(cmpkey,hashkey);\n-  for (uint i = old_dbg_start; i < oldcall->req(); i++) {\n-    Node* old_in = oldcall->in(i);\n-    \/\/ Clone old SafePointScalarObjectNodes, adjusting their field contents.\n-    if (old_in != NULL && old_in->is_SafePointScalarObject()) {\n-      SafePointScalarObjectNode* old_sosn = old_in->as_SafePointScalarObject();\n-      uint old_unique = C->unique();\n-      Node* new_in = old_sosn->clone(sosn_map);\n-      if (old_unique != C->unique()) { \/\/ New node?\n-        new_in->set_req(0, C->root()); \/\/ reset control edge\n-        new_in = transform_later(new_in); \/\/ Register new node.\n-      }\n-      old_in = new_in;\n-    }\n-    newcall->add_req(old_in);\n-  }\n-\n-  \/\/ JVMS may be shared so clone it before we modify it\n-  newcall->set_jvms(oldcall->jvms() != NULL ? oldcall->jvms()->clone_deep(C) : NULL);\n-  for (JVMState *jvms = newcall->jvms(); jvms != NULL; jvms = jvms->caller()) {\n-    jvms->set_map(newcall);\n-    jvms->set_locoff(jvms->locoff()+jvms_adj);\n-    jvms->set_stkoff(jvms->stkoff()+jvms_adj);\n-    jvms->set_monoff(jvms->monoff()+jvms_adj);\n-    jvms->set_scloff(jvms->scloff()+jvms_adj);\n-    jvms->set_endoff(jvms->endoff()+jvms_adj);\n-  }\n-}\n-\n@@ -187,1 +139,1 @@\n-  copy_call_debug_info(oldcall, call);\n+  call->copy_call_debug_info(&_igvn, oldcall);\n@@ -295,1 +247,1 @@\n-        int adr_offset = atype->offset();\n+        int adr_offset = atype->flattened_offset();\n@@ -338,1 +290,1 @@\n-   } else if (mem->Opcode() == Op_StrInflatedCopy) {\n+    } else if (mem->Opcode() == Op_StrInflatedCopy) {\n@@ -383,1 +335,3 @@\n-      const TypePtr* adr_type = NULL;\n+      Node* base = ac->in(ArrayCopyNode::Src);\n+      const TypePtr* adr_type = _igvn.type(base)->is_ptr();\n+      assert(adr_type->isa_aryptr(), \"only arrays here\");\n@@ -386,2 +340,2 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(off);\n+        adr_type = _igvn.type(adr)->is_ptr();\n+        assert(adr_type == _igvn.type(base)->is_aryptr()->add_field_offset_and_offset(off), \"incorrect address type\");\n@@ -394,0 +348,5 @@\n+        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n+          \/\/ Non constant offset in the array: we can't statically\n+          \/\/ determine the value\n+          return NULL;\n+        }\n@@ -401,7 +360,5 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(Type::OffsetBot);\n-        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n-          \/\/ Non constant offset in the array: we can't statically\n-          \/\/ determine the value\n-          return NULL;\n-        }\n+        \/\/ In the case of a flattened inline type array, each field has its\n+        \/\/ own slice so we need to extract the field being accessed from\n+        \/\/ the address computation\n+        adr_type = adr_type->is_aryptr()->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot);\n+        adr = _igvn.transform(new CastPPNode(adr, adr_type));\n@@ -418,0 +375,1 @@\n+      assert(res->isa_DecodeN(), \"should be narrow oop\");\n@@ -433,1 +391,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flattened_offset();\n@@ -472,1 +430,7 @@\n-        values.at_put(j, _igvn.zerocon(ft));\n+        Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+        if (default_value != NULL) {\n+          values.at_put(j, default_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawDefaultValue) == NULL, \"default value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -489,1 +453,7 @@\n-        values.at_put(j, _igvn.zerocon(ft));\n+        Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+        if (default_value != NULL) {\n+          values.at_put(j, default_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawDefaultValue) == NULL, \"default value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -535,1 +505,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flattened_offset();\n@@ -537,1 +507,0 @@\n-  Node *alloc_ctrl = alloc->in(TypeFunc::Control);\n@@ -553,1 +522,1 @@\n-        done = true; \/\/ Something go wrong.\n+        done = true; \/\/ Something went wrong.\n@@ -563,1 +532,1 @@\n-             atype->is_known_instance_field() && atype->offset() == offset &&\n+             atype->is_known_instance_field() && atype->flattened_offset() == offset &&\n@@ -595,0 +564,5 @@\n+      Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+      if (default_value != NULL) {\n+        return default_value;\n+      }\n+      assert(alloc->in(AllocateNode::RawDefaultValue) == NULL, \"default value may not be null\");\n@@ -626,1 +600,1 @@\n-  \/\/ Something go wrong.\n+  \/\/ Something went wrong.\n@@ -630,0 +604,37 @@\n+\/\/ Search the last value stored into the inline type's fields.\n+Node* PhaseMacroExpand::inline_type_from_mem(Node* mem, Node* ctl, ciInlineKlass* vk, const TypeAryPtr* adr_type, int offset, AllocateNode* alloc) {\n+  \/\/ Subtract the offset of the first field to account for the missing oop header\n+  offset -= vk->first_field_offset();\n+  \/\/ Create a new InlineTypeNode and retrieve the field values from memory\n+  InlineTypeNode* vt = InlineTypeNode::make_uninitialized(_igvn, vk)->as_InlineType();\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); ++i) {\n+    ciType* field_type = vt->field_type(i);\n+    int field_offset = offset + vt->field_offset(i);\n+    \/\/ Each inline type field has its own memory slice\n+    adr_type = adr_type->with_field_offset(field_offset);\n+    Node* value = NULL;\n+    if (vt->field_is_flattened(i)) {\n+      value = inline_type_from_mem(mem, ctl, field_type->as_inline_klass(), adr_type, field_offset, alloc);\n+    } else {\n+      const Type* ft = Type::get_const_type(field_type);\n+      BasicType bt = field_type->basic_type();\n+      if (UseCompressedOops && !is_java_primitive(bt)) {\n+        ft = ft->make_narrowoop();\n+        bt = T_NARROWOOP;\n+      }\n+      value = value_from_mem(mem, ctl, bt, ft, adr_type, alloc);\n+      if (value != NULL && ft->isa_narrowoop()) {\n+        assert(UseCompressedOops, \"unexpected narrow oop\");\n+        value = transform_later(new DecodeNNode(value, value->get_ptr_type()));\n+      }\n+    }\n+    if (value != NULL) {\n+      vt->set_field_value(i, value);\n+    } else {\n+      \/\/ We might have reached the TrackedInitializationLimit\n+      return NULL;\n+    }\n+  }\n+  return transform_later(vt);\n+}\n+\n@@ -682,1 +693,1 @@\n-              NOT_PRODUCT(fail_eliminate = \"Not store field referrence\";)\n+              NOT_PRODUCT(fail_eliminate = \"Not store field reference\";)\n@@ -710,0 +721,4 @@\n+      } else if (use->is_InlineType() && use->isa_InlineType()->get_oop() == res) {\n+        \/\/ ok to eliminate\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ store to mark work\n@@ -721,1 +736,1 @@\n-          }else {\n+          } else {\n@@ -727,0 +742,3 @@\n+      } else {\n+        assert(use->Opcode() == Op_CastP2X, \"should be\");\n+        assert(!use->has_out_with(Op_OrL), \"should have been removed because oop is never null\");\n@@ -789,0 +807,4 @@\n+      if (elem_type->is_inlinetype() && !klass->is_flat_array_klass()) {\n+        assert(basic_elem_type == T_INLINE_TYPE, \"unexpected element basic type\");\n+        basic_elem_type = T_OBJECT;\n+      }\n@@ -791,0 +813,4 @@\n+      if (klass->is_flat_array_klass()) {\n+        \/\/ Flattened inline type array\n+        element_size = klass->as_flat_array_klass()->element_byte_size();\n+      }\n@@ -796,0 +822,1 @@\n+  Unique_Node_List value_worklist;\n@@ -822,0 +849,1 @@\n+        assert(!field->is_flattened(), \"flattened inline type fields should not have safepoint uses\");\n@@ -849,3 +877,9 @@\n-      const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n-\n-      Node *field_val = value_from_mem(mem, ctl, basic_elem_type, field_type, field_addr_type, alloc);\n+      Node* field_val = NULL;\n+      const TypeOopPtr* field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n+      if (klass->is_flat_array_klass()) {\n+        ciInlineKlass* vk = elem_type->as_inline_klass();\n+        assert(vk->flatten_array(), \"must be flattened\");\n+        field_val = inline_type_from_mem(mem, ctl, vk, field_addr_type->isa_aryptr(), 0, alloc);\n+      } else {\n+        field_val = value_from_mem(mem, ctl, basic_elem_type, field_type, field_addr_type, alloc);\n+      }\n@@ -909,1 +943,4 @@\n-      if (UseCompressedOops && field_type->isa_narrowoop()) {\n+      if (field_val->is_InlineType()) {\n+        \/\/ Keep track of inline types to scalarize them later\n+        value_worklist.push(field_val);\n+      } else if (UseCompressedOops && field_type->isa_narrowoop()) {\n@@ -930,0 +967,5 @@\n+  \/\/ Scalarize inline types that were added to the safepoint\n+  for (uint i = 0; i < value_worklist.size(); ++i) {\n+    Node* vt = value_worklist.at(i);\n+    vt->as_InlineType()->make_scalar_in_safepoints(&_igvn);\n+  }\n@@ -945,1 +987,1 @@\n-void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc) {\n+void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc, bool inline_alloc) {\n@@ -957,10 +999,7 @@\n-#ifdef ASSERT\n-            \/\/ Verify that there is no dependent MemBarVolatile nodes,\n-            \/\/ they should be removed during IGVN, see MemBarNode::Ideal().\n-            for (DUIterator_Fast pmax, p = n->fast_outs(pmax);\n-                                       p < pmax; p++) {\n-              Node* mb = n->fast_out(p);\n-              assert(mb->is_Initialize() || !mb->is_MemBar() ||\n-                     mb->req() <= MemBarNode::Precedent ||\n-                     mb->in(MemBarNode::Precedent) != n,\n-                     \"MemBarVolatile should be eliminated for non-escaping object\");\n+            for (DUIterator_Fast pmax, p = n->fast_outs(pmax); p < pmax; p++) {\n+              MemBarNode* mb = n->fast_out(p)->isa_MemBar();\n+              if (mb != NULL && mb->req() <= MemBarNode::Precedent && mb->in(MemBarNode::Precedent) == n) {\n+                \/\/ MemBarVolatiles should have been removed by MemBarNode::Ideal() for non-inline allocations\n+                assert(inline_alloc, \"MemBarVolatile should be eliminated for non-escaping object\");\n+                mb->remove(&_igvn);\n+              }\n@@ -968,1 +1007,0 @@\n-#endif\n@@ -992,2 +1030,1 @@\n-          CallProjections callprojs;\n-          ac->extract_projections(&callprojs, true);\n+          CallProjections* callprojs = ac->extract_projections(true);\n@@ -995,3 +1032,3 @@\n-          _igvn.replace_node(callprojs.fallthrough_ioproj, ac->in(TypeFunc::I_O));\n-          _igvn.replace_node(callprojs.fallthrough_memproj, ac->in(TypeFunc::Memory));\n-          _igvn.replace_node(callprojs.fallthrough_catchproj, ac->in(TypeFunc::Control));\n+          _igvn.replace_node(callprojs->fallthrough_ioproj, ac->in(TypeFunc::I_O));\n+          _igvn.replace_node(callprojs->fallthrough_memproj, ac->in(TypeFunc::Memory));\n+          _igvn.replace_node(callprojs->fallthrough_catchproj, ac->in(TypeFunc::Control));\n@@ -1014,0 +1051,6 @@\n+      } else if (use->is_InlineType()) {\n+        assert(use->isa_InlineType()->get_oop() == res, \"unexpected inline type use\");\n+        _igvn.rehash_node_delayed(use);\n+        use->isa_InlineType()->set_oop(_igvn.zerocon(T_INLINE_TYPE));\n+      } else if (use->is_Store()) {\n+        _igvn.replace_node(use, use->in(MemNode::Memory));\n@@ -1047,0 +1090,5 @@\n+          \/\/ Inline type buffer allocations are followed by a membar\n+          Node* membar_after = ctrl_proj->unique_ctrl_out();\n+          if (inline_alloc && membar_after->Opcode() == Op_MemBarCPUOrder) {\n+            membar_after->as_MemBar()->remove(&_igvn);\n+          }\n@@ -1065,0 +1113,4 @@\n+      } else if (use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarStoreStore\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1096,1 +1148,1 @@\n-  if (!EliminateAllocations || JvmtiExport::can_pop_frame() || !alloc->_is_non_escaping) {\n+  if (!EliminateAllocations || JvmtiExport::can_pop_frame()) {\n@@ -1101,1 +1153,7 @@\n-  Node* res = alloc->result_cast();\n+\n+  \/\/ Attempt to eliminate inline type buffer allocations\n+  \/\/ regardless of usage and escape\/replaceable status.\n+  bool inline_alloc = tklass->klass()->is_inlinetype();\n+  if (!alloc->_is_non_escaping && !inline_alloc) {\n+    return false;\n+  }\n@@ -1103,3 +1161,4 @@\n-  \/\/ regardless scalar replacable status.\n-  bool boxing_alloc = C->eliminate_boxing() &&\n-                      tklass->klass()->is_instance_klass()  &&\n+  \/\/ regardless of scalar replaceable status.\n+  Node* res = alloc->result_cast();\n+  bool boxing_alloc = (res == NULL) && C->eliminate_boxing() &&\n+                      tklass->klass()->is_instance_klass() &&\n@@ -1107,1 +1166,1 @@\n-  if (!alloc->_is_scalar_replaceable && (!boxing_alloc || (res != NULL))) {\n+  if (!alloc->_is_scalar_replaceable && !boxing_alloc && !inline_alloc) {\n@@ -1119,1 +1178,1 @@\n-    assert(res == NULL, \"sanity\");\n+    assert(res == NULL || inline_alloc, \"sanity\");\n@@ -1124,0 +1183,1 @@\n+      assert(!inline_alloc, \"Inline type allocations should not have safepoint uses\");\n@@ -1144,1 +1204,1 @@\n-  process_users_of_allocation(alloc);\n+  process_users_of_allocation(alloc, inline_alloc);\n@@ -1168,1 +1228,1 @@\n-  const TypeTuple* r = boxing->tf()->range();\n+  const TypeTuple* r = boxing->tf()->range_sig();\n@@ -1369,1 +1429,1 @@\n-    IfNode *toobig_iff = new IfNode(ctrl, initial_slow_test, PROB_MIN, COUNT_UNKNOWN);\n+    IfNode* toobig_iff = new IfNode(ctrl, initial_slow_test, PROB_MIN, COUNT_UNKNOWN);\n@@ -1372,1 +1432,1 @@\n-    Node *toobig_true = new IfTrueNode( toobig_iff );\n+    Node* toobig_true = new IfTrueNode(toobig_iff);\n@@ -1375,1 +1435,1 @@\n-    toobig_false = new IfFalseNode( toobig_iff );\n+    toobig_false = new IfFalseNode(toobig_iff);\n@@ -1414,0 +1474,1 @@\n+\n@@ -1472,0 +1533,3 @@\n+  } else {\n+    \/\/ Let the runtime know if this is a larval allocation\n+    call->init_req(TypeFunc::Parms+1, _igvn.intcon(alloc->_larval));\n@@ -1476,1 +1540,1 @@\n-  copy_call_debug_info((CallNode *) alloc,  call);\n+  call->copy_call_debug_info(&_igvn, alloc);\n@@ -1504,1 +1568,1 @@\n-    migrate_outs(_memproj_fallthrough, result_phi_rawmem);\n+    _igvn.replace_in_uses(_memproj_fallthrough, result_phi_rawmem);\n@@ -1508,1 +1572,1 @@\n-  if (_memproj_catchall != NULL ) {\n+  if (_memproj_catchall != NULL) {\n@@ -1513,1 +1577,1 @@\n-    migrate_outs(_memproj_catchall, _memproj_fallthrough);\n+    _igvn.replace_in_uses(_memproj_catchall, _memproj_fallthrough);\n@@ -1523,1 +1587,1 @@\n-    migrate_outs(_ioproj_fallthrough, result_phi_i_o);\n+    _igvn.replace_in_uses(_ioproj_fallthrough, result_phi_i_o);\n@@ -1527,1 +1591,1 @@\n-  if (_ioproj_catchall != NULL ) {\n+  if (_ioproj_catchall != NULL) {\n@@ -1532,1 +1596,1 @@\n-    migrate_outs(_ioproj_catchall, _ioproj_fallthrough);\n+    _igvn.replace_in_uses(_ioproj_catchall, _ioproj_fallthrough);\n@@ -1600,1 +1664,1 @@\n-    migrate_outs(_fallthroughcatchproj, ctrl);\n+    _igvn.replace_in_uses(_fallthroughcatchproj, ctrl);\n@@ -1613,1 +1677,1 @@\n-    migrate_outs(_memproj_fallthrough, mem);\n+    _igvn.replace_in_uses(_memproj_fallthrough, mem);\n@@ -1617,1 +1681,1 @@\n-    migrate_outs(_ioproj_fallthrough, i_o);\n+    _igvn.replace_in_uses(_ioproj_fallthrough, i_o);\n@@ -1743,5 +1807,4 @@\n-Node*\n-PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n-                                    Node* control, Node* rawmem, Node* object,\n-                                    Node* klass_node, Node* length,\n-                                    Node* size_in_bytes) {\n+Node* PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n+                                          Node* control, Node* rawmem, Node* object,\n+                                          Node* klass_node, Node* length,\n+                                          Node* size_in_bytes) {\n@@ -1750,1 +1813,1 @@\n-  Node* mark_node = alloc->make_ideal_mark(&_igvn, object, control, rawmem);\n+  Node* mark_node = alloc->make_ideal_mark(&_igvn, control, rawmem);\n@@ -1782,0 +1845,2 @@\n+                                            alloc->in(AllocateNode::DefaultValue),\n+                                            alloc->in(AllocateNode::RawDefaultValue),\n@@ -2162,0 +2227,2 @@\n+  const Type* obj_type = _igvn.type(alock->obj_node());\n+  assert(!obj_type->isa_inlinetype() && !obj_type->is_inlinetypeptr(), \"Eliminating lock on inline type\");\n@@ -2443,0 +2510,42 @@\n+  const TypeOopPtr* objptr = _igvn.type(obj)->make_oopptr();\n+  if (objptr->can_be_inline_type()) {\n+    \/\/ Deoptimize and re-execute if a value\n+    assert(EnableValhalla, \"should only be used if inline types are enabled\");\n+    Node* mark = make_load(slow_path, mem, obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X->basic_type());\n+    Node* value_mask = _igvn.MakeConX(markWord::always_locked_pattern);\n+    Node* is_value = _igvn.transform(new AndXNode(mark, value_mask));\n+    Node* cmp = _igvn.transform(new CmpXNode(is_value, value_mask));\n+    Node* bol = _igvn.transform(new BoolNode(cmp, BoolTest::eq));\n+    Node* unc_ctrl = generate_slow_guard(&slow_path, bol, NULL);\n+\n+    int trap_request = Deoptimization::make_trap_request(Deoptimization::Reason_class_check, Deoptimization::Action_none);\n+    address call_addr = SharedRuntime::uncommon_trap_blob()->entry_point();\n+    const TypePtr* no_memory_effects = NULL;\n+    JVMState* jvms = lock->jvms();\n+    CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\",\n+                                           jvms->bci(), no_memory_effects);\n+\n+    unc->init_req(TypeFunc::Control, unc_ctrl);\n+    unc->init_req(TypeFunc::I_O, lock->i_o());\n+    unc->init_req(TypeFunc::Memory, mem); \/\/ may gc ptrs\n+    unc->init_req(TypeFunc::FramePtr,  lock->in(TypeFunc::FramePtr));\n+    unc->init_req(TypeFunc::ReturnAdr, lock->in(TypeFunc::ReturnAdr));\n+    unc->init_req(TypeFunc::Parms+0, _igvn.intcon(trap_request));\n+    unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+    unc->copy_call_debug_info(&_igvn, lock);\n+\n+    assert(unc->peek_monitor_box() == box, \"wrong monitor\");\n+    assert(unc->peek_monitor_obj() == obj, \"wrong monitor\");\n+\n+    \/\/ pop monitor and push obj back on stack: we trap before the monitorenter\n+    unc->pop_monitor();\n+    unc->grow_stack(unc->jvms(), 1);\n+    unc->set_stack(unc->jvms(), unc->jvms()->stk_size()-1, obj);\n+\n+    _igvn.register_new_node_with_optimizer(unc);\n+\n+    Node* ctrl = _igvn.transform(new ProjNode(unc, TypeFunc::Control));\n+    Node* halt = _igvn.transform(new HaltNode(ctrl, lock->in(TypeFunc::FramePtr), \"monitor enter on value-type\"));\n+    C->root()->add_req(halt);\n+  }\n+\n@@ -2544,0 +2653,205 @@\n+\/\/ An inline type might be returned from the call but we don't know its\n+\/\/ type. Either we get a buffered inline type (and nothing needs to be done)\n+\/\/ or one of the inlines being returned is the klass of the inline type\n+\/\/ and we need to allocate an inline type instance of that type and\n+\/\/ initialize it with other values being returned. In that case, we\n+\/\/ first try a fast path allocation and initialize the value with the\n+\/\/ inline klass's pack handler or we fall back to a runtime call.\n+void PhaseMacroExpand::expand_mh_intrinsic_return(CallStaticJavaNode* call) {\n+  assert(call->method()->is_method_handle_intrinsic(), \"must be a method handle intrinsic call\");\n+  Node* ret = call->proj_out_or_null(TypeFunc::Parms);\n+  if (ret == NULL) {\n+    return;\n+  }\n+  const TypeFunc* tf = call->_tf;\n+  const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+  const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+  call->_tf = new_tf;\n+  \/\/ Make sure the change of type is applied before projections are processed by igvn\n+  _igvn.set_type(call, call->Value(&_igvn));\n+  _igvn.set_type(ret, ret->Value(&_igvn));\n+\n+  \/\/ Before any new projection is added:\n+  CallProjections* projs = call->extract_projections(true, true);\n+\n+  Node* ctl = new Node(1);\n+  Node* mem = new Node(1);\n+  Node* io = new Node(1);\n+  Node* ex_ctl = new Node(1);\n+  Node* ex_mem = new Node(1);\n+  Node* ex_io = new Node(1);\n+  Node* res = new Node(1);\n+\n+  Node* cast = transform_later(new CastP2XNode(ctl, res));\n+  Node* mask = MakeConX(0x1);\n+  Node* masked = transform_later(new AndXNode(cast, mask));\n+  Node* cmp = transform_later(new CmpXNode(masked, mask));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* allocation_iff = new IfNode(ctl, bol, PROB_MAX, COUNT_UNKNOWN);\n+  transform_later(allocation_iff);\n+  Node* allocation_ctl = transform_later(new IfTrueNode(allocation_iff));\n+  Node* no_allocation_ctl = transform_later(new IfFalseNode(allocation_iff));\n+\n+  Node* no_allocation_res = transform_later(new CheckCastPPNode(no_allocation_ctl, res, TypeInstPtr::BOTTOM));\n+\n+  Node* mask2 = MakeConX(-2);\n+  Node* masked2 = transform_later(new AndXNode(cast, mask2));\n+  Node* rawklassptr = transform_later(new CastX2PNode(masked2));\n+  Node* klass_node = transform_later(new CheckCastPPNode(allocation_ctl, rawklassptr, TypeKlassPtr::OBJECT_OR_NULL));\n+\n+  Node* slowpath_bol = NULL;\n+  Node* top_adr = NULL;\n+  Node* old_top = NULL;\n+  Node* new_top = NULL;\n+  if (UseTLAB) {\n+    Node* end_adr = NULL;\n+    set_eden_pointers(top_adr, end_adr);\n+    Node* end = make_load(ctl, mem, end_adr, 0, TypeRawPtr::BOTTOM, T_ADDRESS);\n+    old_top = new LoadPNode(ctl, mem, top_adr, TypeRawPtr::BOTTOM, TypeRawPtr::BOTTOM, MemNode::unordered);\n+    transform_later(old_top);\n+    Node* layout_val = make_load(NULL, mem, klass_node, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);\n+    Node* size_in_bytes = ConvI2X(layout_val);\n+    new_top = new AddPNode(top(), old_top, size_in_bytes);\n+    transform_later(new_top);\n+    Node* slowpath_cmp = new CmpPNode(new_top, end);\n+    transform_later(slowpath_cmp);\n+    slowpath_bol = new BoolNode(slowpath_cmp, BoolTest::ge);\n+    transform_later(slowpath_bol);\n+  } else {\n+    slowpath_bol = intcon(1);\n+    top_adr = top();\n+    old_top = top();\n+    new_top = top();\n+  }\n+  IfNode* slowpath_iff = new IfNode(allocation_ctl, slowpath_bol, PROB_UNLIKELY_MAG(4), COUNT_UNKNOWN);\n+  transform_later(slowpath_iff);\n+\n+  Node* slowpath_true = new IfTrueNode(slowpath_iff);\n+  transform_later(slowpath_true);\n+\n+  CallStaticJavaNode* slow_call = new CallStaticJavaNode(OptoRuntime::store_inline_type_fields_Type(),\n+                                                         StubRoutines::store_inline_type_fields_to_buf(),\n+                                                         \"store_inline_type_fields\",\n+                                                         call->jvms()->bci(),\n+                                                         TypePtr::BOTTOM);\n+  slow_call->init_req(TypeFunc::Control, slowpath_true);\n+  slow_call->init_req(TypeFunc::Memory, mem);\n+  slow_call->init_req(TypeFunc::I_O, io);\n+  slow_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+  slow_call->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  slow_call->init_req(TypeFunc::Parms, res);\n+\n+  Node* slow_ctl = transform_later(new ProjNode(slow_call, TypeFunc::Control));\n+  Node* slow_mem = transform_later(new ProjNode(slow_call, TypeFunc::Memory));\n+  Node* slow_io = transform_later(new ProjNode(slow_call, TypeFunc::I_O));\n+  Node* slow_res = transform_later(new ProjNode(slow_call, TypeFunc::Parms));\n+  Node* slow_catc = transform_later(new CatchNode(slow_ctl, slow_io, 2));\n+  Node* slow_norm = transform_later(new CatchProjNode(slow_catc, CatchProjNode::fall_through_index, CatchProjNode::no_handler_bci));\n+  Node* slow_excp = transform_later(new CatchProjNode(slow_catc, CatchProjNode::catch_all_index,    CatchProjNode::no_handler_bci));\n+\n+  Node* ex_r = new RegionNode(3);\n+  Node* ex_mem_phi = new PhiNode(ex_r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* ex_io_phi = new PhiNode(ex_r, Type::ABIO);\n+  ex_r->init_req(1, slow_excp);\n+  ex_mem_phi->init_req(1, slow_mem);\n+  ex_io_phi->init_req(1, slow_io);\n+  ex_r->init_req(2, ex_ctl);\n+  ex_mem_phi->init_req(2, ex_mem);\n+  ex_io_phi->init_req(2, ex_io);\n+\n+  transform_later(ex_r);\n+  transform_later(ex_mem_phi);\n+  transform_later(ex_io_phi);\n+\n+  Node* slowpath_false = new IfFalseNode(slowpath_iff);\n+  transform_later(slowpath_false);\n+  Node* rawmem = new StorePNode(slowpath_false, mem, top_adr, TypeRawPtr::BOTTOM, new_top, MemNode::unordered);\n+  transform_later(rawmem);\n+  Node* mark_node = makecon(TypeRawPtr::make((address)markWord::always_locked_prototype().value()));\n+  rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::mark_offset_in_bytes(), mark_node, T_ADDRESS);\n+  rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  if (UseCompressedClassPointers) {\n+    rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::klass_gap_offset_in_bytes(), intcon(0), T_INT);\n+  }\n+  Node* fixed_block  = make_load(slowpath_false, rawmem, klass_node, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+  Node* pack_handler = make_load(slowpath_false, rawmem, fixed_block, in_bytes(InlineKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+\n+  CallLeafNoFPNode* handler_call = new CallLeafNoFPNode(OptoRuntime::pack_inline_type_Type(),\n+                                                        NULL,\n+                                                        \"pack handler\",\n+                                                        TypeRawPtr::BOTTOM);\n+  handler_call->init_req(TypeFunc::Control, slowpath_false);\n+  handler_call->init_req(TypeFunc::Memory, rawmem);\n+  handler_call->init_req(TypeFunc::I_O, top());\n+  handler_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+  handler_call->init_req(TypeFunc::ReturnAdr, top());\n+  handler_call->init_req(TypeFunc::Parms, pack_handler);\n+  handler_call->init_req(TypeFunc::Parms+1, old_top);\n+\n+  \/\/ We don't know how many values are returned. This assumes the\n+  \/\/ worst case, that all available registers are used.\n+  for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+    if (domain->field_at(i) == Type::HALF) {\n+      slow_call->init_req(i, top());\n+      handler_call->init_req(i+1, top());\n+      continue;\n+    }\n+    Node* proj = transform_later(new ProjNode(call, i));\n+    slow_call->init_req(i, proj);\n+    handler_call->init_req(i+1, proj);\n+  }\n+\n+  \/\/ We can safepoint at that new call\n+  slow_call->copy_call_debug_info(&_igvn, call);\n+  transform_later(slow_call);\n+  transform_later(handler_call);\n+\n+  Node* handler_ctl = transform_later(new ProjNode(handler_call, TypeFunc::Control));\n+  rawmem = transform_later(new ProjNode(handler_call, TypeFunc::Memory));\n+  Node* slowpath_false_res = transform_later(new ProjNode(handler_call, TypeFunc::Parms));\n+\n+  MergeMemNode* slowpath_false_mem = MergeMemNode::make(mem);\n+  slowpath_false_mem->set_memory_at(Compile::AliasIdxRaw, rawmem);\n+  transform_later(slowpath_false_mem);\n+\n+  Node* r = new RegionNode(4);\n+  Node* mem_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* io_phi = new PhiNode(r, Type::ABIO);\n+  Node* res_phi = new PhiNode(r, TypeInstPtr::BOTTOM);\n+\n+  r->init_req(1, no_allocation_ctl);\n+  mem_phi->init_req(1, mem);\n+  io_phi->init_req(1, io);\n+  res_phi->init_req(1, no_allocation_res);\n+  r->init_req(2, slow_norm);\n+  mem_phi->init_req(2, slow_mem);\n+  io_phi->init_req(2, slow_io);\n+  res_phi->init_req(2, slow_res);\n+  r->init_req(3, handler_ctl);\n+  mem_phi->init_req(3, slowpath_false_mem);\n+  io_phi->init_req(3, io);\n+  res_phi->init_req(3, slowpath_false_res);\n+\n+  transform_later(r);\n+  transform_later(mem_phi);\n+  transform_later(io_phi);\n+  transform_later(res_phi);\n+\n+  assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+  _igvn.replace_in_uses(projs->fallthrough_catchproj, r);\n+  _igvn.replace_in_uses(projs->fallthrough_memproj, mem_phi);\n+  _igvn.replace_in_uses(projs->fallthrough_ioproj, io_phi);\n+  _igvn.replace_in_uses(projs->resproj[0], res_phi);\n+  _igvn.replace_in_uses(projs->catchall_catchproj, ex_r);\n+  _igvn.replace_in_uses(projs->catchall_memproj, ex_mem_phi);\n+  _igvn.replace_in_uses(projs->catchall_ioproj, ex_io_phi);\n+\n+  _igvn.replace_node(ctl, projs->fallthrough_catchproj);\n+  _igvn.replace_node(mem, projs->fallthrough_memproj);\n+  _igvn.replace_node(io, projs->fallthrough_ioproj);\n+  _igvn.replace_node(res, projs->resproj[0]);\n+  _igvn.replace_node(ex_ctl, projs->catchall_catchproj);\n+  _igvn.replace_node(ex_mem, projs->catchall_memproj);\n+  _igvn.replace_node(ex_io, projs->catchall_ioproj);\n+ }\n+\n@@ -2569,1 +2883,1 @@\n-      subklass = _igvn.transform(LoadKlassNode::make(_igvn, NULL, C->immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+      subklass = _igvn.transform(LoadKlassNode::make(_igvn, NULL, C->immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeKlassPtr::OBJECT));\n@@ -2625,2 +2939,5 @@\n-      case Node::Class_CallStaticJava:\n-        success = eliminate_boxing_node(n->as_CallStaticJava());\n+      case Node::Class_CallStaticJava: {\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          success = eliminate_boxing_node(n->as_CallStaticJava());\n+        }\n@@ -2628,0 +2945,1 @@\n+      }\n@@ -2674,4 +2992,7 @@\n-        \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n-        C->remove_macro_node(n);\n-        _igvn._worklist.push(n);\n-        success = true;\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n+          C->remove_macro_node(n);\n+          _igvn._worklist.push(n);\n+          success = true;\n+        }\n@@ -2771,0 +3092,5 @@\n+    case Node::Class_CallStaticJava:\n+      expand_mh_intrinsic_return(n->as_CallStaticJava());\n+      C->remove_macro_node(n);\n+      assert(C->macro_count() == (old_macro_count - 1), \"expansion must have deleted one node from macro list\");\n+      break;\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":456,"deletions":130,"binary":false,"changes":586,"status":"modified"},{"patch":"@@ -557,0 +557,3 @@\n+  if (n->is_InlineTypeBase()) {\n+    C->add_inline_type(n);\n+  }\n@@ -634,0 +637,3 @@\n+  if (is_InlineTypeBase()) {\n+    compile->remove_inline_type(this);\n+  }\n@@ -1407,0 +1413,3 @@\n+      if (dead->is_InlineTypeBase()) {\n+        igvn->C->remove_inline_type(dead);\n+      }\n@@ -2161,1 +2170,3 @@\n-      assert(i >= req() || i == 0 || is_Region() || is_Phi() || is_ArrayCopy() || (is_Unlock() && i == req()-1)\n+      assert(i >= req() || i == 0 || is_Region() || is_Phi() || is_ArrayCopy() ||\n+             (is_Allocate() && i >= AllocateNode::InlineTypeNode) ||\n+             (is_Unlock() && i == req()-1)\n@@ -2163,1 +2174,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+             \"only region, phi, arraycopy, allocate or unlock nodes have null data edges\");\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -104,0 +104,1 @@\n+class MachPrologNode;\n@@ -110,0 +111,1 @@\n+class MachVEPNode;\n@@ -153,0 +155,3 @@\n+class InlineTypeBaseNode;\n+class InlineTypeNode;\n+class InlineTypePtrNode;\n@@ -665,0 +670,2 @@\n+      DEFINE_CLASS_ID(MachProlog,       Mach, 8)\n+      DEFINE_CLASS_ID(MachVEP,          Mach, 9)\n@@ -679,0 +686,3 @@\n+      DEFINE_CLASS_ID(InlineTypeBase, Type, 8)\n+        DEFINE_CLASS_ID(InlineType, InlineTypeBase, 0)\n+        DEFINE_CLASS_ID(InlineTypePtr, InlineTypeBase, 1)\n@@ -857,0 +867,1 @@\n+  DEFINE_CLASS_QUERY(MachProlog)\n@@ -863,0 +874,1 @@\n+  DEFINE_CLASS_QUERY(MachVEP)\n@@ -886,0 +898,3 @@\n+  DEFINE_CLASS_QUERY(InlineType)\n+  DEFINE_CLASS_QUERY(InlineTypeBase)\n+  DEFINE_CLASS_QUERY(InlineTypePtr)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -52,0 +52,2 @@\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -478,0 +480,1 @@\n+  bool is_inlined = InstanceKlass::cast(k1)->field_is_inlined(slot);\n@@ -479,1 +482,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset);\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset, is_inlined);\n@@ -498,1 +501,1 @@\n-  if (m->is_initializer()) {\n+  if (m->is_object_constructor() || m->is_static_init_factory()) {\n@@ -560,1 +563,0 @@\n-\n@@ -895,1 +897,2 @@\n-    case T_OBJECT:      push_object(va_arg(_ap, jobject)); break;\n+    case T_OBJECT:\n+    case T_INLINE_TYPE: push_object(va_arg(_ap, jobject)); break;\n@@ -931,1 +934,2 @@\n-    case T_OBJECT:      push_object((_ap++)->l); break;\n+    case T_OBJECT:\n+    case T_INLINE_TYPE: push_object((_ap++)->l); break;\n@@ -1071,5 +1075,19 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherArray ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == NULL) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), NULL);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherArray ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  } else {\n+    JavaValue jvalue(T_INLINE_TYPE);\n+    JNI_ArgumentPusherArray ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, NULL, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -1077,1 +1095,1 @@\n-JNI_END\n+  JNI_END\n@@ -1091,5 +1109,19 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherVaArg ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == NULL) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), NULL);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  } else {\n+    JavaValue jvalue(T_INLINE_TYPE);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, NULL, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -1111,8 +1143,25 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  va_list args;\n-  va_start(args, methodID);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherVaArg ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n-  va_end(args);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == NULL) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), NULL);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    va_list args;\n+    va_start(args, methodID);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+    va_end(args);\n+  } else {\n+    va_list args;\n+    va_start(args, methodID);\n+    JavaValue jvalue(T_INLINE_TYPE);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, NULL, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    va_end(args);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -1895,1 +1944,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset());\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset(), fd.is_inlined());\n@@ -1906,0 +1955,1 @@\n+  oop res = NULL;\n@@ -1911,2 +1961,12 @@\n-  oop loaded_obj = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n-  jobject ret = JNIHandles::make_local(THREAD, loaded_obj);\n+  if (!jfieldIDWorkaround::is_inlined_jfieldID(fieldID)) {\n+    res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instance can have inlined fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);  \/\/ performance bottleneck\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineKlass* field_vklass = InlineKlass::cast(holder->get_inline_type_field_klass(fd.index()));\n+    res = field_vklass->read_inlined_field(o, ik->field_offset(fd.index()), CHECK_NULL);\n+  }\n+  jobject ret = JNIHandles::make_local(THREAD, res);\n@@ -2010,1 +2070,12 @@\n-  HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  if (!jfieldIDWorkaround::is_inlined_jfieldID(fieldID)) {\n+    HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instances can have inlined fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineKlass* vklass = InlineKlass::cast(holder->get_inline_type_field_klass(fd.index()));\n+    oop v = JNIHandles::resolve_non_null(value);\n+    vklass->write_inlined_field(o, offset, v, CHECK);\n+  }\n@@ -2447,4 +2518,14 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  if (a->is_within_bounds(index)) {\n-    ret = JNIHandles::make_local(THREAD, a->obj_at(index));\n-    return ret;\n+  oop res = NULL;\n+  arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+  if (arr->is_within_bounds(index)) {\n+    if (arr->is_flatArray()) {\n+      flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+      arrayHandle ah(THREAD, a);\n+      flatArrayHandle vah(thread, a);\n+      res = flatArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK_NULL);\n+      assert(res != NULL, \"Must be set in one of two paths above\");\n+    } else {\n+      assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+      objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+      res = a->obj_at(index);\n+    }\n@@ -2454,1 +2535,1 @@\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n+    ss.print(\"Index %d out of bounds for length %d\", index,arr->length());\n@@ -2457,0 +2538,2 @@\n+  ret = JNIHandles::make_local(THREAD, res);\n+  return ret;\n@@ -2464,1 +2547,1 @@\n- HOTSPOT_JNI_SETOBJECTARRAYELEMENT_ENTRY(env, array, index, value);\n+  HOTSPOT_JNI_SETOBJECTARRAYELEMENT_ENTRY(env, array, index, value);\n@@ -2467,24 +2550,51 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  oop v = JNIHandles::resolve(value);\n-  if (a->is_within_bounds(index)) {\n-    if (v == NULL || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n-      a->obj_at_put(index, v);\n-    } else {\n-      ResourceMark rm(THREAD);\n-      stringStream ss;\n-      Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n-      ss.print(\"type mismatch: can not store %s to %s[%d]\",\n-               v->klass()->external_name(),\n-               bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n-               index);\n-      for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n-        ss.print(\"[]\");\n-      }\n-      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n-    }\n-  } else {\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n-    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n-  }\n+   bool oob = false;\n+   int length = -1;\n+   oop res = NULL;\n+   arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+   if (arr->is_within_bounds(index)) {\n+     if (arr->is_flatArray()) {\n+       flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       FlatArrayKlass* vaklass = FlatArrayKlass::cast(a->klass());\n+       InlineKlass* element_vklass = vaklass->element_klass();\n+       if (v != NULL && v->is_a(element_vklass)) {\n+         a->value_copy_to_index(v, index);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *kl = FlatArrayKlass::cast(a->klass());\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             kl->external_name(),\n+             index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     } else {\n+       assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+       objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       if (v == NULL || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n+         a->obj_at_put(index, v);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n+                 index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     }\n+   } else {\n+     ResourceMark rm(THREAD);\n+     stringStream ss;\n+     ss.print(\"Index %d out of bounds for length %d\", index, arr->length());\n+     THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n+   }\n@@ -3277,0 +3387,275 @@\n+JNI_ENTRY(void*, jni_GetFlattenedArrayElements(JNIEnv* env, jarray array, jboolean* isCopy))\n+  JNIWrapper(\"jni_GetFlattenedArrayElements\");\n+  if (isCopy != NULL) {\n+    *isCopy = JNI_FALSE;\n+  }\n+  arrayOop ar = arrayOop(JNIHandles::resolve_non_null(array));\n+  if (!ar->is_array()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!ar->is_flatArray()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass());\n+  if (vak->contains_oops()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Flattened array contains oops\");\n+  }\n+  oop a = lock_gc_or_pin_object(thread, array);\n+  flatArrayOop vap = flatArrayOop(a);\n+  void* ret = vap->value_at_addr(0, vak->layout_helper());\n+  return ret;\n+JNI_END\n+\n+JNI_ENTRY(void, jni_ReleaseFlattenedArrayElements(JNIEnv* env, jarray array, void* elem, jint mode))\n+  JNIWrapper(\"jni_ReleaseFlattenedArrayElements\");\n+  unlock_gc_or_unpin_object(thread, array);\n+JNI_END\n+\n+JNI_ENTRY(jsize, jni_GetFlattenedArrayElementSize(JNIEnv* env, jarray array)) {\n+  JNIWrapper(\"jni_GetFlattenedElementSize\");\n+  arrayOop a = arrayOop(JNIHandles::resolve_non_null(array));\n+  if (!a->is_array()) {\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!a->is_flatArray()) {\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(a->klass());\n+  jsize ret = vak->element_byte_size();\n+  return ret;\n+}\n+JNI_END\n+\n+JNI_ENTRY(jclass, jni_GetFlattenedArrayElementClass(JNIEnv* env, jarray array))\n+  JNIWrapper(\"jni_GetArrayElementClass\");\n+  arrayOop a = arrayOop(JNIHandles::resolve_non_null(array));\n+  if (!a->is_array()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!a->is_flatArray()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(a->klass());\n+  InlineKlass* vk = vak->element_klass();\n+  return (jclass) JNIHandles::make_local(vk->java_mirror());\n+JNI_END\n+\n+JNI_ENTRY(jsize, jni_GetFieldOffsetInFlattenedLayout(JNIEnv* env, jclass clazz, const char *name, const char *signature, jboolean* is_inlined))\n+  JNIWrapper(\"jni_GetFieldOffsetInFlattenedLayout\");\n+\n+  oop mirror = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(mirror);\n+  if (!k->is_inline_klass()) {\n+    ResourceMark rm;\n+        THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), err_msg(\"%s has not flattened layout\", k->external_name()));\n+  }\n+  InlineKlass* vk = InlineKlass::cast(k);\n+\n+  TempNewSymbol fieldname = SymbolTable::probe(name, (int)strlen(name));\n+  TempNewSymbol signame = SymbolTable::probe(signature, (int)strlen(signature));\n+  if (fieldname == NULL || signame == NULL) {\n+    ResourceMark rm;\n+    THROW_MSG_0(vmSymbols::java_lang_NoSuchFieldError(), err_msg(\"%s.%s %s\", vk->external_name(), name, signature));\n+  }\n+\n+  assert(vk->is_initialized(), \"If a flattened array has been created, the element klass must have been initialized\");\n+\n+  fieldDescriptor fd;\n+  if (!vk->is_instance_klass() ||\n+      !InstanceKlass::cast(vk)->find_field(fieldname, signame, false, &fd)) {\n+    ResourceMark rm;\n+    THROW_MSG_0(vmSymbols::java_lang_NoSuchFieldError(), err_msg(\"%s.%s %s\", vk->external_name(), name, signature));\n+  }\n+\n+  int offset = fd.offset() - vk->first_field_offset();\n+  if (is_inlined != NULL) {\n+    *is_inlined = fd.is_inlined();\n+  }\n+  return (jsize)offset;\n+JNI_END\n+\n+JNI_ENTRY(jobject, jni_CreateSubElementSelector(JNIEnv* env, jarray array))\n+  JNIWrapper(\"jni_CreateSubElementSelector\");\n+\n+  arrayOop ar = arrayOop(JNIHandles::resolve_non_null(array));\n+  if (!ar->is_array()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!ar->is_flatArray()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  Klass* ses_k = SystemDictionary::resolve_or_null(vmSymbols::jdk_internal_vm_jni_SubElementSelector(),\n+        Handle(THREAD, SystemDictionary::java_system_loader()), Handle(), CHECK_NULL);\n+  InstanceKlass* ses_ik = InstanceKlass::cast(ses_k);\n+  ses_ik->initialize(CHECK_NULL);\n+  Klass* elementKlass = ArrayKlass::cast(ar->klass())->element_klass();\n+  oop ses = ses_ik->allocate_instance(CHECK_NULL);\n+  Handle ses_h(THREAD, ses);\n+  jdk_internal_vm_jni_SubElementSelector::setArrayElementType(ses_h(), elementKlass->java_mirror());\n+  jdk_internal_vm_jni_SubElementSelector::setSubElementType(ses_h(), elementKlass->java_mirror());\n+  jdk_internal_vm_jni_SubElementSelector::setOffset(ses_h(), 0);\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlined(ses_h(), true);   \/\/ by definition, top element of a flattened array is inlined\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlineType(ses_h(), true); \/\/ by definition, top element of a flattened array is an inline type\n+  return JNIHandles::make_local(ses_h());\n+JNI_END\n+\n+JNI_ENTRY(jobject, jni_GetSubElementSelector(JNIEnv* env, jobject selector, jfieldID fieldID))\n+  JNIWrapper(\"jni_GetSubElementSelector\");\n+\n+  oop slct = JNIHandles::resolve_non_null(selector);\n+  if (slct->klass()->name() != vmSymbols::jdk_internal_vm_jni_SubElementSelector()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a SubElementSelector\");\n+  }\n+  jboolean is_inlined = jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct);\n+  if (!is_inlined) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"SubElement is not inlined\");\n+  }\n+  oop semirror = jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct);\n+  Klass* k = java_lang_Class::as_Klass(semirror);\n+  if (!k->is_inline_klass()) {\n+    ResourceMark rm;\n+        THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), err_msg(\"%s is not an inline type\", k->external_name()));\n+  }\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  assert(vk->is_initialized(), \"If a flattened array has been created, the element klass must have been initialized\");\n+  int field_offset = jfieldIDWorkaround::from_instance_jfieldID(vk, fieldID);\n+  fieldDescriptor fd;\n+  if (!vk->find_field_from_offset(field_offset, false, &fd)) {\n+    THROW_NULL(vmSymbols::java_lang_NoSuchFieldError());\n+  }\n+  Handle arrayElementMirror(THREAD, jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct));\n+  \/\/ offset of the SubElement is offset of the original SubElement plus the offset of the field inside the element\n+  int offset = fd.offset() - vk->first_field_offset() + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+  InstanceKlass* sesklass = InstanceKlass::cast(JNIHandles::resolve_non_null(selector)->klass());\n+  oop res = sesklass->allocate_instance(CHECK_NULL);\n+  Handle res_h(THREAD, res);\n+  jdk_internal_vm_jni_SubElementSelector::setArrayElementType(res_h(), arrayElementMirror());\n+  InstanceKlass* holder = fd.field_holder();\n+  BasicType bt = Signature::basic_type(fd.signature());\n+  if (is_java_primitive(bt)) {\n+    jdk_internal_vm_jni_SubElementSelector::setSubElementType(res_h(), java_lang_Class::primitive_mirror(bt));\n+  } else {\n+    Klass* fieldKlass = SystemDictionary::resolve_or_fail(fd.signature(), Handle(THREAD, holder->class_loader()),\n+        Handle(THREAD, holder->protection_domain()), true, CHECK_NULL);\n+    jdk_internal_vm_jni_SubElementSelector::setSubElementType(res_h(),fieldKlass->java_mirror());\n+  }\n+  jdk_internal_vm_jni_SubElementSelector::setOffset(res_h(), offset);\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlined(res_h(), fd.is_inlined());\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlineType(res_h(), fd.is_inline_type());\n+  return JNIHandles::make_local(res_h());\n+JNI_END\n+\n+JNI_ENTRY(jobject, jni_GetObjectSubElement(JNIEnv* env, jarray array, jobject selector, int index))\n+  JNIWrapper(\"jni_GetObjectSubElement\");\n+\n+  flatArrayOop ar =  (flatArrayOop)JNIHandles::resolve_non_null(array);\n+  oop slct = JNIHandles::resolve_non_null(selector);\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass());\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\");\n+  }\n+  oop res = NULL;\n+  if (!jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct)) {\n+    int offset = (address)ar->base() - cast_from_oop<address>(ar) + index * vak->element_byte_size()\n+                      + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+    res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(ar, offset);\n+  } else {\n+    InlineKlass* fieldKlass = InlineKlass::cast(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)));\n+    res = fieldKlass->allocate_instance_buffer(CHECK_NULL);\n+    \/\/ The array might have been moved by the GC, refreshing the arrayOop\n+    ar =  (flatArrayOop)JNIHandles::resolve_non_null(array);\n+    address addr = (address)ar->value_at_addr(index, vak->layout_helper())\n+              + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+    fieldKlass->inline_copy_payload_to_new_oop(addr, res);\n+  }\n+  return JNIHandles::make_local(res);\n+JNI_END\n+\n+JNI_ENTRY(void, jni_SetObjectSubElement(JNIEnv* env, jarray array, jobject selector, int index, jobject value))\n+  JNIWrapper(\"jni_SetObjectSubElement\");\n+\n+  flatArrayOop ar =  (flatArrayOop)JNIHandles::resolve_non_null(array);\n+  oop slct = JNIHandles::resolve_non_null(selector);\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass());\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\");\n+  }\n+  oop val = JNIHandles::resolve(value);\n+  if (val == NULL) {\n+    if (jdk_internal_vm_jni_SubElementSelector::getIsInlineType(slct)) {\n+      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), \"null cannot be stored in a flattened array\");\n+    }\n+  } else {\n+    if (!val->is_a(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)))) {\n+      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), \"type mismatch\");\n+    }\n+  }\n+  if (!jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct)) {\n+    int offset = (address)ar->base() - cast_from_oop<address>(ar) + index * vak->element_byte_size()\n+                  + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+    HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(ar, offset, JNIHandles::resolve(value));\n+  } else {\n+    InlineKlass* fieldKlass = InlineKlass::cast(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)));\n+    address addr = (address)ar->value_at_addr(index, vak->layout_helper())\n+                  + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+    fieldKlass->inline_copy_oop_to_payload(JNIHandles::resolve_non_null(value), addr);\n+  }\n+JNI_END\n+\n+#define DEFINE_GETSUBELEMENT(ElementType,Result,ElementBasicType) \\\n+\\\n+JNI_ENTRY(ElementType, \\\n+          jni_Get##Result##SubElement(JNIEnv *env, jarray array, jobject selector, int index)) \\\n+  JNIWrapper(\"Get\" XSTR(Result) \"SubElement\"); \\\n+  flatArrayOop ar = (flatArrayOop)JNIHandles::resolve_non_null(array); \\\n+  oop slct = JNIHandles::resolve_non_null(selector); \\\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass()); \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) { \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\"); \\\n+  } \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct) != java_lang_Class::primitive_mirror(ElementBasicType)) { \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Wrong SubElement type\"); \\\n+  } \\\n+  address addr = (address)ar->value_at_addr(index, vak->layout_helper()) \\\n+               + jdk_internal_vm_jni_SubElementSelector::getOffset(slct); \\\n+  ElementType result = *(ElementType*)addr; \\\n+  return result; \\\n+JNI_END\n+\n+DEFINE_GETSUBELEMENT(jboolean, Boolean,T_BOOLEAN)\n+DEFINE_GETSUBELEMENT(jbyte, Byte, T_BYTE)\n+DEFINE_GETSUBELEMENT(jshort, Short,T_SHORT)\n+DEFINE_GETSUBELEMENT(jchar, Char,T_CHAR)\n+DEFINE_GETSUBELEMENT(jint, Int,T_INT)\n+DEFINE_GETSUBELEMENT(jlong, Long,T_LONG)\n+DEFINE_GETSUBELEMENT(jfloat, Float,T_FLOAT)\n+DEFINE_GETSUBELEMENT(jdouble, Double,T_DOUBLE)\n+\n+#define DEFINE_SETSUBELEMENT(ElementType,Result,ElementBasicType) \\\n+\\\n+JNI_ENTRY(void, \\\n+          jni_Set##Result##SubElement(JNIEnv *env, jarray array, jobject selector, int index, ElementType value)) \\\n+  JNIWrapper(\"Get\" XSTR(Result) \"SubElement\"); \\\n+  flatArrayOop ar = (flatArrayOop)JNIHandles::resolve_non_null(array); \\\n+  oop slct = JNIHandles::resolve_non_null(selector); \\\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass()); \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) { \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\"); \\\n+  } \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct) != java_lang_Class::primitive_mirror(ElementBasicType)) { \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Wrong SubElement type\"); \\\n+  } \\\n+  address addr = (address)ar->value_at_addr(index, vak->layout_helper()) \\\n+               + jdk_internal_vm_jni_SubElementSelector::getOffset(slct); \\\n+  *(ElementType*)addr = value; \\\n+JNI_END\n+\n+DEFINE_SETSUBELEMENT(jboolean, Boolean,T_BOOLEAN)\n+DEFINE_SETSUBELEMENT(jbyte, Byte, T_BYTE)\n+DEFINE_SETSUBELEMENT(jshort, Short,T_SHORT)\n+DEFINE_SETSUBELEMENT(jchar, Char,T_CHAR)\n+DEFINE_SETSUBELEMENT(jint, Int,T_INT)\n+DEFINE_SETSUBELEMENT(jlong, Long,T_LONG)\n+DEFINE_SETSUBELEMENT(jfloat, Float,T_FLOAT)\n+DEFINE_SETSUBELEMENT(jdouble, Double,T_DOUBLE)\n+\n@@ -3560,1 +3945,32 @@\n-    jni_GetModule\n+    jni_GetModule,\n+\n+    \/\/ Flattened arrays features\n+\n+    jni_GetFlattenedArrayElements,\n+    jni_ReleaseFlattenedArrayElements,\n+    jni_GetFlattenedArrayElementClass,\n+    jni_GetFlattenedArrayElementSize,\n+    jni_GetFieldOffsetInFlattenedLayout,\n+\n+    jni_CreateSubElementSelector,\n+    jni_GetSubElementSelector,\n+    jni_GetObjectSubElement,\n+    jni_SetObjectSubElement,\n+\n+    jni_GetBooleanSubElement,\n+    jni_GetByteSubElement,\n+    jni_GetShortSubElement,\n+    jni_GetCharSubElement,\n+    jni_GetIntSubElement,\n+    jni_GetLongSubElement,\n+    jni_GetFloatSubElement,\n+    jni_GetDoubleSubElement,\n+\n+    jni_SetBooleanSubElement,\n+    jni_SetByteSubElement,\n+    jni_SetShortSubElement,\n+    jni_SetCharSubElement,\n+    jni_SetIntSubElement,\n+    jni_SetLongSubElement,\n+    jni_SetFloatSubElement,\n+    jni_SetDoubleSubElement\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":475,"deletions":59,"binary":false,"changes":534,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -656,1 +657,22 @@\n-  return handle == NULL ? 0 : ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)) ;\n+  if (handle == NULL) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, Universe::inline_type_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(SystemDictionary::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return ObjectSynchronizer::FastHashCode(THREAD, obj);\n+  }\n@@ -712,0 +734,1 @@\n+       klass->is_inline_klass() ||\n@@ -1254,1 +1277,5 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n+    if (ik->has_injected_identityObject()) {\n+      size--;\n+    }\n@@ -1257,1 +1284,1 @@\n-    size = 2;\n+    size = 3;\n@@ -1266,0 +1293,1 @@\n+    int cursor = 0;\n@@ -1267,2 +1295,5 @@\n-      Klass* k = InstanceKlass::cast(klass)->local_interfaces()->at(index);\n-      result->obj_at_put(index, k->java_mirror());\n+      InstanceKlass* ik = InstanceKlass::cast(klass);\n+      Klass* k = ik->local_interfaces()->at(index);\n+      if (!ik->has_injected_identityObject() || k != SystemDictionary::IdentityObject_klass()) {\n+        result->obj_at_put(cursor++, k->java_mirror());\n+      }\n@@ -1271,1 +1302,1 @@\n-    \/\/ All arrays implement java.lang.Cloneable and java.io.Serializable\n+    \/\/ All arrays implement java.lang.Cloneable, java.io.Serializable and java.lang.IdentityObject\n@@ -1274,0 +1305,1 @@\n+    result->obj_at_put(2, SystemDictionary::IdentityObject_klass()->java_mirror());\n@@ -1893,0 +1925,2 @@\n+  bool is_ctor = (method->is_object_constructor() ||\n+                  method->is_static_init_factory());\n@@ -1894,1 +1928,1 @@\n-    return (method->is_initializer() && !method->is_static());\n+    return is_ctor;\n@@ -1896,1 +1930,3 @@\n-    return  (!method->is_initializer() && !method->is_overpass());\n+    return (!is_ctor &&\n+            !method->is_class_initializer() &&\n+            !method->is_overpass());\n@@ -1958,0 +1994,2 @@\n+        assert(method->is_object_constructor() ||\n+               method->is_static_init_factory(), \"must be\");\n@@ -2215,3 +2253,1 @@\n-  if (!m->is_initializer() || m->is_static()) {\n-    method = Reflection::new_method(m, true, CHECK_NULL);\n-  } else {\n+  if (m->is_object_constructor() || m->is_static_init_factory()) {\n@@ -2219,0 +2255,2 @@\n+  } else {\n+    method = Reflection::new_method(m, true, CHECK_NULL);\n@@ -2506,0 +2544,39 @@\n+\/\/ Arrays support \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+JVM_ENTRY(jboolean, JVM_ArrayIsAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  JVMWrapper(\"JVM_ArrayIsAccessAtomic\");\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == NULL) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  return ArrayKlass::cast(k)->element_access_is_atomic();\n+JVM_END\n+\n+JVM_ENTRY(jobject, JVM_ArrayEnsureAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  JVMWrapper(\"JVM_ArrayEnsureAccessAtomic\");\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == NULL) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vk = FlatArrayKlass::cast(k);\n+    if (!vk->element_access_is_atomic()) {\n+      \/**\n+       * Need to decide how to implement:\n+       *\n+       * 1) Change to objArrayOop layout, therefore oop->klass() differs so\n+       * then \"<atomic>[Qfoo;\" klass needs to subclass \"[Qfoo;\" to pass through\n+       * \"checkcast\" & \"instanceof\"\n+       *\n+       * 2) Use extra header in the flatArrayOop to flag atomicity required and\n+       * possibly per instance lock structure. Said info, could be placed in\n+       * \"trailer\" rather than disturb the current arrayOop\n+       *\/\n+      Unimplemented();\n+    }\n+  }\n+  return array;\n+JVM_END\n+\n@@ -2685,1 +2762,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3680,1 +3757,1 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3701,0 +3778,1 @@\n+  objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3702,1 +3780,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":91,"deletions":14,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -501,1 +501,2 @@\n-  if (ty_sign[0] == JVM_SIGNATURE_CLASS &&\n+  if ((ty_sign[0] == JVM_SIGNATURE_CLASS ||\n+       ty_sign[0] == JVM_SIGNATURE_INLINE_TYPE) &&\n@@ -569,0 +570,1 @@\n+  case T_INLINE_TYPE:\n@@ -688,1 +690,1 @@\n-      if (_type == T_OBJECT) {\n+      if (_type == T_OBJECT || _type == T_INLINE_TYPE) {\n@@ -705,1 +707,2 @@\n-      case T_OBJECT: {\n+      case T_OBJECT:\n+      case T_INLINE_TYPE: {\n@@ -726,1 +729,2 @@\n-        case T_OBJECT: {\n+        case T_OBJECT:\n+        case T_INLINE_TYPE: {\n","filename":"src\/hotspot\/share\/prims\/jvmtiImpl.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -124,5 +124,5 @@\n-  IS_METHOD            = java_lang_invoke_MemberName::MN_IS_METHOD,\n-  IS_CONSTRUCTOR       = java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR,\n-  IS_FIELD             = java_lang_invoke_MemberName::MN_IS_FIELD,\n-  IS_TYPE              = java_lang_invoke_MemberName::MN_IS_TYPE,\n-  CALLER_SENSITIVE     = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,\n+  IS_METHOD             = java_lang_invoke_MemberName::MN_IS_METHOD,\n+  IS_OBJECT_CONSTRUCTOR = java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR,\n+  IS_FIELD              = java_lang_invoke_MemberName::MN_IS_FIELD,\n+  IS_TYPE               = java_lang_invoke_MemberName::MN_IS_TYPE,\n+  CALLER_SENSITIVE      = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,\n@@ -130,5 +130,5 @@\n-  REFERENCE_KIND_SHIFT = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,\n-  REFERENCE_KIND_MASK  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,\n-  SEARCH_SUPERCLASSES  = java_lang_invoke_MemberName::MN_SEARCH_SUPERCLASSES,\n-  SEARCH_INTERFACES    = java_lang_invoke_MemberName::MN_SEARCH_INTERFACES,\n-  ALL_KINDS      = IS_METHOD | IS_CONSTRUCTOR | IS_FIELD | IS_TYPE\n+  REFERENCE_KIND_SHIFT  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,\n+  REFERENCE_KIND_MASK   = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,\n+  SEARCH_SUPERCLASSES   = java_lang_invoke_MemberName::MN_SEARCH_SUPERCLASSES,\n+  SEARCH_INTERFACES     = java_lang_invoke_MemberName::MN_SEARCH_INTERFACES,\n+  ALL_KINDS      = IS_METHOD | IS_OBJECT_CONSTRUCTOR | IS_FIELD | IS_TYPE\n@@ -145,1 +145,1 @@\n-    flags |= IS_CONSTRUCTOR;\n+    flags |= IS_OBJECT_CONSTRUCTOR;\n@@ -164,1 +164,1 @@\n-    case IS_CONSTRUCTOR:\n+    case IS_OBJECT_CONSTRUCTOR:\n@@ -306,2 +306,2 @@\n-    } else if (m->is_initializer()) {\n-      flags |= IS_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);\n+    } else if (m->is_object_constructor()) {\n+      flags |= IS_OBJECT_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);\n@@ -345,0 +345,3 @@\n+  if (fd.is_inlined()) {\n+    flags |= JVM_ACC_FIELD_INLINED;\n+  }\n@@ -794,1 +797,1 @@\n-  case IS_CONSTRUCTOR:\n+  case IS_OBJECT_CONSTRUCTOR:\n@@ -800,1 +803,3 @@\n-        if (name == vmSymbols::object_initializer_name()) {\n+        if (name != vmSymbols::object_initializer_name()) {\n+          break;                \/\/ will throw after end of switch\n+        } else if (type->is_void_method_signature()) {\n@@ -803,1 +808,2 @@\n-          break;                \/\/ will throw after end of switch\n+          \/\/ LinkageError unless it returns something reasonable\n+          LinkResolver::resolve_static_call(result, link_info, false, THREAD);\n@@ -863,1 +869,1 @@\n-  case IS_CONSTRUCTOR:\n+  case IS_OBJECT_CONSTRUCTOR:\n@@ -942,1 +948,1 @@\n-      match_flags &= ~(IS_CONSTRUCTOR | IS_METHOD);\n+      match_flags &= ~(IS_OBJECT_CONSTRUCTOR | IS_METHOD);\n@@ -972,1 +978,1 @@\n-  if ((match_flags & (IS_METHOD | IS_CONSTRUCTOR)) != 0) {\n+  if ((match_flags & (IS_METHOD | IS_OBJECT_CONSTRUCTOR)) != 0) {\n@@ -977,2 +983,2 @@\n-    bool negate_name_test = false;\n-    \/\/ fix name so that it captures the intention of IS_CONSTRUCTOR\n+    bool ctor_ok = true, sfac_ok = true;\n+    \/\/ fix name so that it captures the intention of IS_OBJECT_CONSTRUCTOR\n@@ -986,1 +992,2 @@\n-    } else if (!(match_flags & IS_CONSTRUCTOR)) {\n+      sfac_ok = false;\n+    } else if (!(match_flags & IS_OBJECT_CONSTRUCTOR)) {\n@@ -988,6 +995,1 @@\n-      if (name == NULL) {\n-        name = init_name;\n-        negate_name_test = true; \/\/ if we see the name, we *omit* the entry\n-      } else if (name == init_name) {\n-        return 0;               \/\/ no methods of this constructor name\n-      }\n+      ctor_ok = false;  \/\/ but sfac_ok is true, so we might find <init>\n@@ -1003,1 +1005,1 @@\n-      if (name != NULL && ((m_name != name) ^ negate_name_test))\n+      if (name != NULL && m_name != name)\n@@ -1007,0 +1009,4 @@\n+      if (m_name == init_name) {  \/\/ might be either ctor or sfac\n+        if (m->is_object_constructor()  && !ctor_ok)  continue;\n+        if (m->is_static_init_factory() && !sfac_ok)  continue;\n+      }\n@@ -1106,1 +1112,1 @@\n-    template(java_lang_invoke_MemberName,MN_IS_CONSTRUCTOR) \\\n+    template(java_lang_invoke_MemberName,MN_IS_OBJECT_CONSTRUCTOR) \\\n@@ -1236,1 +1242,1 @@\n-               (flags & ALL_KINDS) == IS_CONSTRUCTOR) {\n+               (flags & ALL_KINDS) == IS_OBJECT_CONSTRUCTOR) {\n","filename":"src\/hotspot\/share\/prims\/methodHandles.cpp","additions":38,"deletions":32,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -35,0 +35,2 @@\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logStream.hpp\"\n@@ -37,0 +39,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -42,0 +47,1 @@\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n@@ -150,1 +156,0 @@\n-\n@@ -235,0 +240,1 @@\n+      assert(!_obj->is_inline_type() || _obj->mark().is_larval_state(), \"must be an object instance or a larval inline type\");\n@@ -239,1 +245,0 @@\n-\n@@ -261,0 +266,62 @@\n+#ifdef ASSERT\n+\/*\n+ * Get the field descriptor of the field of the given object at the given offset.\n+ *\/\n+static bool get_field_descriptor(oop p, jlong offset, fieldDescriptor* fd) {\n+  bool found = false;\n+  Klass* k = p->klass();\n+  if (k->is_instance_klass()) {\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    found = ik->find_field_from_offset((int)offset, false, fd);\n+    if (!found && ik->is_mirror_instance_klass()) {\n+      Klass* k2 = java_lang_Class::as_Klass(p);\n+      if (k2->is_instance_klass()) {\n+        ik = InstanceKlass::cast(k2);\n+        found = ik->find_field_from_offset((int)offset, true, fd);\n+      }\n+    }\n+  }\n+  return found;\n+}\n+#endif \/\/ ASSERT\n+\n+static void assert_and_log_unsafe_value_access(oop p, jlong offset, InlineKlass* vk) {\n+  Klass* k = p->klass();\n+#ifdef ASSERT\n+  if (k->is_instance_klass()) {\n+    assert_field_offset_sane(p, offset);\n+    fieldDescriptor fd;\n+    bool found = get_field_descriptor(p, offset, &fd);\n+    if (found) {\n+      assert(found, \"value field not found\");\n+      assert(fd.is_inlined(), \"field not flat\");\n+    } else {\n+      if (log_is_enabled(Trace, valuetypes)) {\n+        log_trace(valuetypes)(\"not a field in %s at offset \" SIZE_FORMAT_HEX,\n+                              p->klass()->external_name(), offset);\n+      }\n+    }\n+  } else if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+    int index = (offset - vak->array_header_in_bytes()) \/ vak->element_byte_size();\n+    address dest = (address)((flatArrayOop)p)->value_at_addr(index, vak->layout_helper());\n+    assert(dest == (cast_from_oop<address>(p) + offset), \"invalid offset\");\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+#endif \/\/ ASSERT\n+  if (log_is_enabled(Trace, valuetypes)) {\n+    if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      int index = (offset - vak->array_header_in_bytes()) \/ vak->element_byte_size();\n+      address dest = (address)((flatArrayOop)p)->value_at_addr(index, vak->layout_helper());\n+      log_trace(valuetypes)(\"%s array type %s index %d element size %d offset \" SIZE_FORMAT_HEX \" at \" INTPTR_FORMAT,\n+                            p->klass()->external_name(), vak->external_name(),\n+                            index, vak->element_byte_size(), offset, p2i(dest));\n+    } else {\n+      log_trace(valuetypes)(\"%s field type %s at offset \" SIZE_FORMAT_HEX,\n+                            p->klass()->external_name(), vk->external_name(), offset);\n+    }\n+  }\n+}\n+\n@@ -275,0 +342,1 @@\n+  assert(!p->is_inline_type() || p->mark().is_larval_state(), \"must be an object instance or a larval inline type\");\n@@ -278,0 +346,58 @@\n+UNSAFE_ENTRY(jlong, Unsafe_ValueHeaderSize(JNIEnv *env, jobject unsafe, jclass c)) {\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  return vk->first_field_offset();\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jboolean, Unsafe_IsFlattenedArray(JNIEnv *env, jobject unsafe, jclass c)) {\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));\n+  return k->is_flatArray_klass();\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_UninitializedDefaultValue(JNIEnv *env, jobject unsafe, jclass vc)) {\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  oop v = vk->default_value();\n+  return JNIHandles::make_local(THREAD, v);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_GetValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc)) {\n+  oop base = JNIHandles::resolve(obj);\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  assert_and_log_unsafe_value_access(base, offset, vk);\n+  Handle base_h(THREAD, base);\n+  oop v = vk->read_inlined_field(base_h(), offset, CHECK_NULL);\n+  return JNIHandles::make_local(THREAD, v);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(void, Unsafe_PutValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc, jobject value)) {\n+  oop base = JNIHandles::resolve(obj);\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  assert(!base->is_inline_type() || base->mark().is_larval_state(), \"must be an object instance or a larval inline type\");\n+  assert_and_log_unsafe_value_access(base, offset, vk);\n+  oop v = JNIHandles::resolve(value);\n+  vk->write_inlined_field(base, offset, v, CHECK);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_MakePrivateBuffer(JNIEnv *env, jobject unsafe, jobject value)) {\n+  oop v = JNIHandles::resolve_non_null(value);\n+  assert(v->is_inline_type(), \"must be an inline type instance\");\n+  Handle vh(THREAD, v);\n+  InlineKlass* vk = InlineKlass::cast(v->klass());\n+  instanceOop new_value = vk->allocate_instance_buffer(CHECK_NULL);\n+  vk->inline_copy_oop_to_new_oop(vh(),  new_value);\n+  markWord mark = new_value->mark();\n+  new_value->set_mark(mark.enter_larval_state());\n+  return JNIHandles::make_local(THREAD, new_value);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_FinishPrivateBuffer(JNIEnv *env, jobject unsafe, jobject value)) {\n+  oop v = JNIHandles::resolve(value);\n+  assert(v->mark().is_larval_state(), \"must be a larval value\");\n+  markWord mark = v->mark();\n+  v->set_mark(mark.exit_larval_state());\n+  return JNIHandles::make_local(THREAD, v);\n+} UNSAFE_END\n+\n@@ -614,0 +740,5 @@\n+  } else if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+    InlineKlass* vklass = vak->element_klass();\n+    base = vak->array_header_in_bytes();\n+    scale = vak->element_byte_size();\n@@ -649,0 +780,6 @@\n+UNSAFE_ENTRY(jlong, Unsafe_GetObjectSize0(JNIEnv* env, jobject o, jobject obj))\n+  oop p = JNIHandles::resolve(obj);\n+  return p->size() * HeapWordSize;\n+UNSAFE_END\n+\n+\n@@ -778,0 +915,2 @@\n+\/\/\n+\/\/ An anonymous class cannot be an inline type.\n@@ -873,0 +1012,2 @@\n+  assert(!anonk->is_inline_klass(), \"unsafe anonymous class cannot be inline class\");\n+\n@@ -1075,4 +1216,4 @@\n-    {CC \"get\" #Type,      CC \"(\" OBJ \"J)\" #Desc,       FN_PTR(Unsafe_Get##Type)}, \\\n-    {CC \"put\" #Type,      CC \"(\" OBJ \"J\" #Desc \")V\",   FN_PTR(Unsafe_Put##Type)}, \\\n-    {CC \"get\" #Type \"Volatile\",      CC \"(\" OBJ \"J)\" #Desc,       FN_PTR(Unsafe_Get##Type##Volatile)}, \\\n-    {CC \"put\" #Type \"Volatile\",      CC \"(\" OBJ \"J\" #Desc \")V\",   FN_PTR(Unsafe_Put##Type##Volatile)}\n+    {CC \"get\"  #Type,      CC \"(\" OBJ \"J)\" #Desc,                 FN_PTR(Unsafe_Get##Type)}, \\\n+    {CC \"put\"  #Type,      CC \"(\" OBJ \"J\" #Desc \")V\",             FN_PTR(Unsafe_Put##Type)}, \\\n+    {CC \"get\"  #Type \"Volatile\",      CC \"(\" OBJ \"J)\" #Desc,      FN_PTR(Unsafe_Get##Type##Volatile)}, \\\n+    {CC \"put\"  #Type \"Volatile\",      CC \"(\" OBJ \"J\" #Desc \")V\",  FN_PTR(Unsafe_Put##Type##Volatile)}\n@@ -1087,0 +1228,8 @@\n+    {CC \"isFlattenedArray\", CC \"(\" CLS \")Z\",                     FN_PTR(Unsafe_IsFlattenedArray)},\n+    {CC \"getValue\",         CC \"(\" OBJ \"J\" CLS \")\" OBJ,          FN_PTR(Unsafe_GetValue)},\n+    {CC \"putValue\",         CC \"(\" OBJ \"J\" CLS OBJ \")V\",         FN_PTR(Unsafe_PutValue)},\n+    {CC \"uninitializedDefaultValue\", CC \"(\" CLS \")\" OBJ,         FN_PTR(Unsafe_UninitializedDefaultValue)},\n+    {CC \"makePrivateBuffer\",     CC \"(\" OBJ \")\" OBJ,             FN_PTR(Unsafe_MakePrivateBuffer)},\n+    {CC \"finishPrivateBuffer\",   CC \"(\" OBJ \")\" OBJ,             FN_PTR(Unsafe_FinishPrivateBuffer)},\n+    {CC \"valueHeaderSize\",       CC \"(\" CLS \")J\",                FN_PTR(Unsafe_ValueHeaderSize)},\n+\n@@ -1109,0 +1258,1 @@\n+    {CC \"getObjectSize0\",     CC \"(Ljava\/lang\/Object;)J\", FN_PTR(Unsafe_GetObjectSize0)},\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":156,"deletions":6,"binary":false,"changes":162,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-#include \"memory\/iterator.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n@@ -55,0 +55,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -60,0 +61,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -1847,0 +1849,92 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return NULL;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(aoop->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return NULL;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayOop result_array =\n+      oopFactory::new_objArray(SystemDictionary::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  instanceOop ioop = ih();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ioop->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array);\n+WB_END\n+\n+class CollectOops : public BasicOopIterateClosure {\n+ public:\n+  GrowableArray<Handle>* array;\n+\n+  objArrayOop create_results(TRAPS) {\n+    objArrayOop result_array =\n+        oopFactory::new_objArray(SystemDictionary::Object_klass(), array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < array->length(); i++) {\n+      result_array->obj_at_put(i, array->at(i)());\n+    }\n+    return result_array;\n+  }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return (jobjectArray)JNIHandles::make_local(THREAD, create_results(THREAD));\n+  }\n+\n+  void add_oop(oop o) {\n+    \/\/ Value might be oop, but JLS can't see as Object, just iterate through it...\n+    if (o != NULL && o->is_inline_type()) {\n+      o->oop_iterate(this);\n+    } else {\n+      array->append(Handle(Thread::current(), o));\n+    }\n+  }\n+\n+  void do_oop(oop* o) { add_oop(*o); }\n+  void do_oop(narrowOop* v) { add_oop(CompressedOops::decode(*v)); }\n+};\n+\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+\n+  JNIHandles::resolve(thing)->oop_iterate(&collectOops);\n+\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  StackFrameStream sfs(thread);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, NULL, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\n@@ -2479,0 +2573,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":101,"deletions":1,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -115,0 +115,1 @@\n+  VMRegImpl::set_regName();  \/\/ need this before generate_stubs (for printing oop maps).\n@@ -125,1 +126,0 @@\n-  VMRegImpl::set_regName(); \/\/ need this before generate_stubs (for printing oop maps).\n","filename":"src\/hotspot\/share\/runtime\/init.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"runtime\/javaCalls.hpp\"\n@@ -308,0 +309,37 @@\n+bool JNIHandles::is_same_object(jobject handle1, jobject handle2) {\n+  oop obj1 = resolve_no_keepalive(handle1);\n+  oop obj2 = resolve_no_keepalive(handle2);\n+\n+  bool ret = obj1 == obj2;\n+\n+  if (EnableValhalla) {\n+    if (!ret && obj1 != NULL && obj2 != NULL && obj1->klass() == obj2->klass() && obj1->klass()->is_inline_klass()) {\n+      \/\/ The two references are different, they are not null and they are both inline types,\n+      \/\/ a full substitutability test is required, calling ValueBootstrapMethods.isSubstitutable()\n+      \/\/ (similarly to InterpreterRuntime::is_substitutable)\n+      Thread* THREAD = Thread::current();\n+      Handle ha(THREAD, obj1);\n+      Handle hb(THREAD, obj2);\n+      JavaValue result(T_BOOLEAN);\n+      JavaCallArguments args;\n+      args.push_oop(ha);\n+      args.push_oop(hb);\n+      methodHandle method(THREAD, Universe::is_substitutable_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        \/\/ Something really bad happened because isSubstitutable() should not throw exceptions\n+        \/\/ If it is an error, just let it propagate\n+        \/\/ If it is an exception, wrap it into an InternalError\n+        if (!PENDING_EXCEPTION->is_a(SystemDictionary::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in substitutability test\", e, false);\n+        }\n+      }\n+      ret = result.get_jboolean();\n+    }\n+  }\n+\n+  return ret;\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/jniHandles.cpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -54,0 +55,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -349,1 +351,5 @@\n-    return oopFactory::new_objArray(k, length, THREAD);\n+    if (k->is_inline_klass()) {\n+      return oopFactory::new_flatArray(k, length, THREAD);\n+    } else {\n+      return oopFactory::new_objArray(k, length, THREAD);\n+    }\n@@ -791,3 +797,0 @@\n-  if (log_is_enabled(Debug, class, resolve)) {\n-    trace_class_resolution(nt);\n-  }\n@@ -800,2 +803,1 @@\n-  assert(!method()->is_initializer() ||\n-         (for_constant_pool_access && method()->is_static()),\n+  assert(!method()->name()->starts_with('<') || for_constant_pool_access,\n@@ -850,1 +852,3 @@\n-  assert(method()->is_initializer(), \"should call new_method instead\");\n+  assert(method()->is_object_constructor() ||\n+         method()->is_static_init_factory(),\n+         \"should call new_method instead\");\n@@ -903,1 +907,5 @@\n-  java_lang_reflect_Field::set_modifiers(rh(), fd->access_flags().as_int() & JVM_RECOGNIZED_FIELD_MODIFIERS);\n+  int modifiers = fd->access_flags().as_int() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n+  if (fd->is_inlined()) {\n+    modifiers |= JVM_ACC_FIELD_INLINED;\n+  }\n+  java_lang_reflect_Field::set_modifiers(rh(), modifiers);\n@@ -1178,0 +1186,2 @@\n+  } else if (java_lang_Class::as_Klass(return_type_mirror)->is_inline_klass()) {\n+    rtype = T_INLINE_TYPE;\n@@ -1212,0 +1222,16 @@\n+\n+  \/\/ Special case for factory methods\n+  if (!method->signature()->is_void_method_signature()) {\n+    assert(klass->is_inline_klass(), \"inline classes must use factory methods\");\n+    Handle no_receiver; \/\/ null instead of receiver\n+    BasicType rtype;\n+    if (klass->is_hidden()) {\n+      rtype = T_OBJECT;\n+    } else {\n+      rtype = T_INLINE_TYPE;\n+    }\n+    return invoke(klass, method, no_receiver, override, ptypes, rtype, args, false, CHECK_NULL);\n+  }\n+\n+  \/\/ main branch of code creates a non-inline object:\n+  assert(!klass->is_inline_klass(), \"classic constructors are only for non-inline classes\");\n","filename":"src\/hotspot\/share\/runtime\/reflection.cpp","additions":34,"deletions":8,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -162,0 +162,13 @@\n+#define CHECK_THROW_NOSYNC_IMSE(obj)  \\\n+  if ((obj)->mark().is_always_locked()) {  \\\n+    ResourceMark rm(THREAD);                \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n+#define CHECK_THROW_NOSYNC_IMSE_0(obj)  \\\n+    if ((obj)->mark().is_always_locked()) {  \\\n+    ResourceMark rm(THREAD);                  \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n+\n@@ -456,0 +469,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -506,1 +520,1 @@\n-\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -564,0 +578,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -609,0 +624,4 @@\n+  if (EnableValhalla && mark.is_always_locked()) {\n+    return;\n+  }\n+  assert(!EnableValhalla || !object->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -672,0 +691,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -686,0 +706,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -708,0 +729,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -727,0 +749,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -770,0 +793,1 @@\n+  CHECK_THROW_NOSYNC_IMSE_0(obj);\n@@ -794,0 +818,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -809,0 +834,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -826,0 +852,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -1000,0 +1027,4 @@\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+    \/\/ VM should be calling bootstrap method\n+    ShouldNotReachHere();\n+  }\n@@ -1141,6 +1172,0 @@\n-\/\/ Deprecated -- use FastHashCode() instead.\n-\n-intptr_t ObjectSynchronizer::identity_hash_value_for(Handle obj) {\n-  return FastHashCode(Thread::current(), obj());\n-}\n-\n@@ -1150,0 +1175,3 @@\n+  if (EnableValhalla && h_obj->mark().is_always_locked()) {\n+    return false;\n+  }\n@@ -1815,0 +1843,4 @@\n+  if (EnableValhalla) {\n+    guarantee(!object->klass()->is_inline_klass(), \"Attempt to inflate inline type\");\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":39,"deletions":7,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -1640,0 +1641,1 @@\n+  set_return_buffered_value(NULL);\n@@ -2846,0 +2848,3 @@\n+  \/\/ Because this method is used to verify oops, it must support\n+  \/\/ oops in buffered values\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -445,0 +445,1 @@\n+ public:\n@@ -1017,0 +1018,1 @@\n+  friend class VTBuffer;\n@@ -1075,0 +1077,1 @@\n+  oop           _return_buffered_value; \/\/ buffered value being returned\n@@ -1555,0 +1558,3 @@\n+  oop return_buffered_value() const              { return _return_buffered_value; }\n+  void set_return_buffered_value(oop val)        { _return_buffered_value = val; }\n+\n@@ -1794,0 +1800,1 @@\n+  static ByteSize return_buffered_value_offset() { return byte_offset_of(JavaThread, _return_buffered_value); }\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -236,1 +236,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags,                                   u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags,                                   u4)                                    \\\n@@ -1579,0 +1579,1 @@\n+  declare_c2_type(CastI2NNode, TypeNode)                                  \\\n@@ -1582,0 +1583,1 @@\n+  declare_c2_type(CastN2INode, Node)                                      \\\n@@ -1622,0 +1624,1 @@\n+  declare_c2_type(MachVEPNode, MachIdealNode)                             \\\n@@ -2275,0 +2278,2 @@\n+  declare_constant(InstanceKlass::_misc_invalid_inline_super)             \\\n+  declare_constant(InstanceKlass::_misc_invalid_identity_super)           \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -200,3 +200,4 @@\n-    private static final int ANNOTATION= 0x00002000;\n-    private static final int ENUM      = 0x00004000;\n-    private static final int SYNTHETIC = 0x00001000;\n+    private static final int ANNOTATION = 0x00002000;\n+    private static final int ENUM       = 0x00004000;\n+    private static final int SYNTHETIC  = 0x00001000;\n+    private static final int INLINE     = 0x00000100;\n@@ -236,2 +237,3 @@\n-        return (isInterface() ? \"interface \" : (isPrimitive() ? \"\" : \"class \"))\n-            + getName();\n+        return (isInlineClass() ? \"inline \" : \"\")\n+               + (isInterface() ? \"interface \" : (isPrimitive() ? \"\" : \"class \"))\n+               + getName();\n@@ -299,0 +301,4 @@\n+                if (isInlineClass()) {\n+                    sb.append(\"inline\");\n+                    sb.append(' ');\n+                }\n@@ -473,2 +479,2 @@\n-                                            ClassLoader loader,\n-                                            Class<?> caller)\n+                                    ClassLoader loader,\n+                                    Class<?> caller)\n@@ -552,0 +558,178 @@\n+    \/**\n+     * Returns {@code true} if this class is an inline class.\n+     *\n+     * @return {@code true} if this class is an inline class\n+     * @since Valhalla\n+     *\/\n+    public boolean isInlineClass() {\n+        return (this.getModifiers() & INLINE) != 0;\n+    }\n+\n+    \/**\n+     * Returns a {@code Class} object representing the <em>value projection<\/em>\n+     * type of this class if this {@code Class} represents the reference projection\n+     * type of an {@linkplain #isInlineClass() inline class}.\n+     * If this {@code Class} represents the value projection type\n+     * of an inline class, then this method returns this class.\n+     * Otherwise an empty {@link Optional} is returned.\n+     *\n+     * @return the {@code Class} object representing the value projection type of\n+     *         this class if this class is the value projection type\n+     *         or the reference projection type of an inline class;\n+     *         an empty {@link Optional} otherwise\n+     * @since Valhalla\n+     *\/\n+    public Optional<Class<?>> valueType() {\n+        if (isPrimitive() || isInterface() || isArray())\n+            return Optional.empty();\n+\n+        Class<?>[] valRefTypes = getProjectionTypes();\n+        return valRefTypes.length > 0 ? Optional.of(valRefTypes[0]) : Optional.empty();\n+    }\n+\n+    \/**\n+     * Returns a {@code Class} object representing the reference type\n+     * of this class.\n+     * <p>\n+     * If this {@code Class} represents an {@linkplain #isInlineClass()\n+     * inline class} with a reference projection type, then this method\n+     * returns the <em>reference projection<\/em> type of this inline class.\n+     * <p>\n+     * If this {@code Class} represents the reference projection type\n+     * of an inline class, then this method returns this class.\n+     * <p>\n+     * If this class is an {@linkplain #isInlineClass() inline class}\n+     * without a reference projection, then this method returns an empty\n+     * {@code Optional}.\n+     * <p>\n+     * If this class is an identity class, then this method returns this\n+     * class.\n+     * <p>\n+     * Otherwise this method returns an empty {@code Optional}.\n+     *\n+     * @return the {@code Class} object representing a reference type for\n+     *         this class if present; an empty {@link Optional} otherwise.\n+     * @since Valhalla\n+     *\/\n+    public Optional<Class<?>> referenceType() {\n+        if (isPrimitive()) return Optional.empty();\n+        if (isInterface() || isArray()) return Optional.of(this);\n+\n+        Class<?>[] valRefTypes = getProjectionTypes();\n+        return valRefTypes.length == 2 ? Optional.of(valRefTypes[1]) : Optional.empty();\n+    }\n+\n+    \/*\n+     * Returns true if this Class object represents a reference projection\n+     * type for an inline class.\n+     *\n+     * A reference projection type must be a sealed abstract class that\n+     * permits the inline projection type to extend.  The inline projection\n+     * type and reference projection type for an inline type must be of\n+     * the same package.\n+     *\/\n+    private boolean isReferenceProjectionType() {\n+        if (isPrimitive() || isArray() || isInterface() || isInlineClass())\n+            return false;\n+\n+        int mods = getModifiers();\n+        if (!Modifier.isAbstract(mods)) {\n+            return false;\n+        }\n+\n+        Class<?>[] valRefTypes = getProjectionTypes();\n+        return valRefTypes.length == 2 && valRefTypes[1] == this;\n+    }\n+\n+    private transient Class<?>[] projectionTypes;\n+    private Class<?>[] getProjectionTypes() {\n+        if (isPrimitive() || isArray() || isInterface())\n+            return null;\n+\n+        Class<?>[] valRefTypes = projectionTypes;\n+        if (valRefTypes == null) {\n+            \/\/ C.ensureProjectionTypesInited calls initProjectionTypes that may\n+            \/\/ call D.ensureProjectionTypesInited where D is its superclass.\n+            \/\/ So initProjectionTypes is called without holding any lock to\n+            \/\/ avoid potential deadlock when multiple threads attempt to\n+            \/\/ initialize the projection types for C and E where D is\n+            \/\/ the superclass of both C and E (which is an error case)\n+            valRefTypes = newProjectionTypeArray();\n+        }\n+        synchronized (this) {\n+            \/\/ set the projection types if not set\n+            if (projectionTypes == null) {\n+                projectionTypes = valRefTypes;\n+            }\n+        }\n+        return projectionTypes;\n+    }\n+\n+    \/*\n+     * Returns an array of Class object whose element at index 0 represents the\n+     * value projection type and element at index 1 represents the reference\n+     * projection type if present.\n+     *\n+     * If this Class object is neither a value projection type nor\n+     * a reference projection type for an inline class, then an empty array\n+     * is returned.\n+     *\/\n+    private Class<?>[] newProjectionTypeArray() {\n+        if (isPrimitive() || isArray() || isInterface())\n+            return null;\n+\n+        if (isInlineClass()) {\n+            Class<?> superClass = getSuperclass();\n+            if (superClass != Object.class && superClass.isReferenceProjectionType()) {\n+                return new Class<?>[] { this, superClass };\n+            } else {\n+                return new Class<?>[] { this };\n+            }\n+        } else {\n+            Class<?> valType = valueProjectionType();\n+            if (valType != null) {\n+                return new Class<?>[] { valType, this};\n+            } else {\n+                return EMPTY_CLASS_ARRAY;\n+            }\n+        }\n+    }\n+\n+    \/*\n+     * Returns the value projection type if this Class represents\n+     * a reference projection type.  If this class is an inline class\n+     * then this method returns this class.  Otherwise, returns null.\n+     *\/\n+    private Class<?> valueProjectionType() {\n+        if (isPrimitive() || isArray() || isInterface())\n+            return null;\n+\n+        if (isInlineClass())\n+            return this;\n+\n+        int mods = getModifiers();\n+        if (!Modifier.isAbstract(mods)) {\n+            return null;\n+        }\n+\n+        \/\/ A reference projection type must be a sealed abstract class\n+        \/\/ that permits the inline projection type to extend.\n+        \/\/ The inline projection type and reference projection type for\n+        \/\/ an inline type must be of the same package.\n+        String[] subclassNames = getPermittedSubclasses0();\n+        if (subclassNames.length == 1) {\n+            String cn = subclassNames[0].replace('\/', '.');\n+            int index = cn.lastIndexOf('.');\n+            String pn = index > 0 ? cn.substring(0, index) : \"\";\n+            if (pn.equals(getPackageName())) {\n+                try {\n+                    Class<?> valType = Class.forName(cn, false, getClassLoader());\n+                    if (valType.isInlineClass()) {\n+                        return valType;\n+                    }\n+                } catch (ClassNotFoundException e) {}\n+            }\n+        }\n+        return null;\n+    }\n+\n@@ -831,0 +1015,2 @@\n+     * <tr><th scope=\"row\"> {@linkplain #isInlineClass() inline class} with <a href=\"ClassLoader.html#binary-name\">binary name<\/a> <i>N<\/i>\n+     *                                      <td style=\"text-align:center\"> {@code Q}<em>N<\/em>{@code ;}\n@@ -849,0 +1035,2 @@\n+     * Point.class.getName()\n+     *     returns \"Point\"\n@@ -851,0 +1039,4 @@\n+     * (new Point[3]).getClass().getName()\n+     *     returns \"[QPoint;\"\n+     * (new Point.ref[3][4]).getClass().getName()\n+     *     returns \"[[LPoint$ref;\"\n@@ -1281,1 +1473,0 @@\n-\n@@ -1292,1 +1483,0 @@\n-\n@@ -1673,1 +1863,1 @@\n-                return cl.getName() + \"[]\".repeat(dimensions);\n+                return cl.getTypeName() + \"[]\".repeat(dimensions);\n@@ -3808,1 +3998,3 @@\n-     * null and is not assignable to the type T.\n+     * {@code null} and is not assignable to the type T.\n+     * @throws NullPointerException if this is an {@linkplain #isInlineClass()\n+     * inline type} and the object is {@code null}\n@@ -3815,0 +4007,3 @@\n+        if (isInlineClass() && obj == null)\n+            throw new NullPointerException(getName() + \" is an inline class\");\n+\n@@ -4110,1 +4305,1 @@\n-         return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n+        return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n@@ -4323,1 +4518,3 @@\n-        } else if (isHidden()) {\n+        }\n+        String typeDesc = isInlineClass() ? \"Q\" : \"L\";\n+        if (isHidden()) {\n@@ -4326,1 +4523,1 @@\n-            return \"L\" + name.substring(0, index).replace('.', '\/')\n+            return typeDesc + name.substring(0, index).replace('.', '\/')\n@@ -4329,1 +4526,1 @@\n-            return \"L\" + getName().replace('.', '\/') + \";\";\n+            return typeDesc + getName().replace('.', '\/') + \";\";\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Class.java","additions":212,"deletions":15,"binary":false,"changes":227,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1994, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1994, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,2 @@\n+import java.util.Objects;\n+\n@@ -43,0 +45,3 @@\n+     *\n+     * @apiNote {@link Objects#newIdentity java.util.Objects.newIdentity()}\n+     * should be used instead of {@code new Object()}.\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Object.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -399,0 +399,4 @@\n+        if (referent != null && referent.getClass().isInlineClass()) {\n+            throw new IllegalArgumentException(\"cannot reference an inline value of type: \" +\n+                    referent.getClass().getName());\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/Reference.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -185,0 +185,1 @@\n+     * <li>fields declared in a {@linkplain Class#isInlineClass() inline class}<\/li>\n@@ -311,8 +312,0 @@\n-        Module callerModule = caller.getModule();\n-        Module declaringModule = declaringClass.getModule();\n-\n-        if (callerModule == declaringModule) return true;\n-        if (callerModule == Object.class.getModule()) return true;\n-        if (!declaringModule.isNamed()) return true;\n-\n-        String pn = declaringClass.getPackageName();\n@@ -326,0 +319,7 @@\n+        Module callerModule = caller.getModule();\n+        Module declaringModule = declaringClass.getModule();\n+\n+        if (callerModule == declaringModule) return true;\n+        if (callerModule == Object.class.getModule()) return true;\n+        if (!declaringModule.isNamed()) return true;\n+\n@@ -328,0 +328,1 @@\n+        String pn = declaringClass.getPackageName();\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/AccessibleObject.java","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -181,0 +181,1 @@\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Constructor.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -171,1 +171,4 @@\n-        if (flag) checkCanSetAccessible(Reflection.getCallerClass());\n+\n+        if (flag) {\n+            checkCanSetAccessible(Reflection.getCallerClass());\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Field.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -217,0 +217,1 @@\n+     *\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Method.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -330,3 +330,3 @@\n-    static final int BRIDGE    = 0x00000040;\n-    static final int VARARGS   = 0x00000080;\n-    static final int SYNTHETIC = 0x00001000;\n+    static final int BRIDGE      = 0x00000040;\n+    static final int VARARGS     = 0x00000080;\n+    static final int SYNTHETIC   = 0x00001000;\n@@ -334,2 +334,3 @@\n-    static final int ENUM      = 0x00004000;\n-    static final int MANDATED  = 0x00008000;\n+    static final int ENUM        = 0x00004000;\n+    static final int MANDATED    = 0x00008000;\n+    static final int FLATTENED   = 0x00008000;      \/\/ HotSpot-specific bit\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Modifier.java","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -449,1 +449,1 @@\n-        visit(V14, accessFlags, dotToSlash(className), null,\n+        visit(V16, accessFlags, dotToSlash(className), null,\n@@ -816,1 +816,5 @@\n-                mv.visitTypeInsn(CHECKCAST, dotToSlash(type.getName()));\n+                String internalName = dotToSlash(type.getName());\n+                if (type.isInlineClass()) {\n+                    internalName = 'Q' + internalName + \";\";\n+                }\n+                mv.visitTypeInsn(CHECKCAST, internalName);\n@@ -867,3 +871,5 @@\n-         * class to get its Class object at runtime.  The code is written to\n-         * the supplied stream.  Note that the code generated by this method\n-         * may cause the checked ClassNotFoundException to be thrown.\n+         * class to get its Class object at runtime.  And also generate code\n+         * to invoke Class.asPrimaryType if the class is regular value type.\n+         *\n+         * The code is written to the supplied stream.  Note that the code generated\n+         * by this method may caused the checked ClassNotFoundException to be thrown.\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/ProxyGenerator.java","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -66,0 +66,6 @@\n+    \/**\n+     * The modifier {@code value}\n+     * @since 1.11\n+     *\/\n+    VALUE,\n+\n","filename":"src\/java.compiler\/share\/classes\/javax\/lang\/model\/element\/Modifier.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -314,0 +314,5 @@\n+        \/**\n+         * Used for instances of {@link WithFieldTree}.\n+         *\/\n+        WITH_FIELD(WithFieldTree.class),\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/tree\/Tree.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -503,0 +503,8 @@\n+    \/**\n+     * Visits a WithFieldTree node.\n+     * @param node the node being visited\n+     * @param p a parameter value\n+     * @return a result value\n+     *\/\n+    R visitWithField(WithFieldTree node, P p);\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/tree\/TreeVisitor.java","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -413,37 +413,58 @@\n-        switch ((short)(sym.flags() & AccessFlags)) {\n-        case PRIVATE:\n-            return\n-                (env.enclClass.sym == sym.owner \/\/ fast special case\n-                 ||\n-                 env.enclClass.sym.outermostClass() ==\n-                 sym.owner.outermostClass())\n-                &&\n-                sym.isInheritedIn(site.tsym, types);\n-        case 0:\n-            return\n-                (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n-                 ||\n-                 env.toplevel.packge == sym.packge())\n-                &&\n-                isAccessible(env, site, checkInner)\n-                &&\n-                sym.isInheritedIn(site.tsym, types)\n-                &&\n-                notOverriddenIn(site, sym);\n-        case PROTECTED:\n-            return\n-                (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n-                 ||\n-                 env.toplevel.packge == sym.packge()\n-                 ||\n-                 isProtectedAccessible(sym, env.enclClass.sym, site)\n-                 ||\n-                 \/\/ OK to select instance method or field from 'super' or type name\n-                 \/\/ (but type names should be disallowed elsewhere!)\n-                 env.info.selectSuper && (sym.flags() & STATIC) == 0 && sym.kind != TYP)\n-                &&\n-                isAccessible(env, site, checkInner)\n-                &&\n-                notOverriddenIn(site, sym);\n-        default: \/\/ this case includes erroneous combinations as well\n-            return isAccessible(env, site, checkInner) && notOverriddenIn(site, sym);\n+        ClassSymbol enclosingCsym = env.enclClass.sym;\n+        if (sym.kind == MTH || sym.kind == VAR) {\n+            \/* If any inline types are involved, ask the same question in the reference universe,\n+               where the hierarchy is navigable\n+            *\/\n+            if (site.isValue())\n+                site = site.referenceProjection();\n+            if (sym.owner.isValue())\n+                sym = sym.referenceProjection();\n+            if (env.enclClass.sym.isValue())\n+                env.enclClass.sym = env.enclClass.sym.referenceProjection();\n+        } else if (sym.kind == TYP) {\n+            \/\/ A type is accessible in a reference projection if it was\n+            \/\/ accessible in the value projection.\n+            if (site.isReferenceProjection())\n+                site = site.valueProjection();\n+        }\n+        try {\n+            switch ((short)(sym.flags() & AccessFlags)) {\n+                case PRIVATE:\n+                    return\n+                            (env.enclClass.sym == sym.owner \/\/ fast special case\n+                                    ||\n+                                    env.enclClass.sym.outermostClass() ==\n+                                            sym.owner.outermostClass())\n+                                    &&\n+                                    sym.isInheritedIn(site.tsym, types);\n+                case 0:\n+                    return\n+                            (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n+                                    ||\n+                                    env.toplevel.packge == sym.packge())\n+                                    &&\n+                                    isAccessible(env, site, checkInner)\n+                                    &&\n+                                    sym.isInheritedIn(site.tsym, types)\n+                                    &&\n+                                    notOverriddenIn(site, sym);\n+                case PROTECTED:\n+                    return\n+                            (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n+                                    ||\n+                                    env.toplevel.packge == sym.packge()\n+                                    ||\n+                                    isProtectedAccessible(sym, env.enclClass.sym, site)\n+                                    ||\n+                                    \/\/ OK to select instance method or field from 'super' or type name\n+                                    \/\/ (but type names should be disallowed elsewhere!)\n+                                    env.info.selectSuper && (sym.flags() & STATIC) == 0 && sym.kind != TYP)\n+                                    &&\n+                                    isAccessible(env, site, checkInner)\n+                                    &&\n+                                    notOverriddenIn(site, sym);\n+                default: \/\/ this case includes erroneous combinations as well\n+                    return isAccessible(env, site, checkInner) && notOverriddenIn(site, sym);\n+            }\n+        } finally {\n+            env.enclClass.sym = enclosingCsym;\n@@ -462,5 +483,12 @@\n-        else {\n-            Symbol s2 = ((MethodSymbol)sym).implementation(site.tsym, types, true);\n-            return (s2 == null || s2 == sym || sym.owner == s2.owner ||\n-                    !types.isSubSignature(types.memberType(site, s2), types.memberType(site, sym)));\n-        }\n+\n+        \/* If any inline types are involved, ask the same question in the reference universe,\n+           where the hierarchy is navigable\n+        *\/\n+        if (site.isValue())\n+            site = site.referenceProjection();\n+        if (sym.owner.isValue())\n+            sym = sym.referenceProjection();\n+\n+        Symbol s2 = ((MethodSymbol)sym).implementation(site.tsym, types, true);\n+        return (s2 == null || s2 == sym || sym.owner == s2.owner ||\n+                !types.isSubSignature(types.memberType(site, s2), types.memberType(site, sym)));\n@@ -2186,0 +2214,4 @@\n+        \/\/ ATM, inner\/nested types are members of only the declaring inline class,\n+        \/\/ although accessible via the reference projection.\n+        if (c.isReferenceProjection())\n+            c = (TypeSymbol) c.valueProjection();\n@@ -2243,0 +2275,16 @@\n+        return findMemberTypeInternal(env,site, name, c);\n+    }\n+\n+    \/** Find qualified member type.\n+     *  @param env       The current environment.\n+     *  @param site      The original type from where the selection takes\n+     *                   place.\n+     *  @param name      The type's name.\n+     *  @param c         The class to search for the member type. This is\n+     *                   always a superclass or implemented interface of\n+     *                   site's class.\n+     *\/\n+    Symbol findMemberTypeInternal(Env<AttrContext> env,\n+                          Type site,\n+                          Name name,\n+                          TypeSymbol c) {\n@@ -2306,0 +2354,8 @@\n+        return findTypeInternal(env, name);\n+    }\n+\n+    \/** Find an unqualified type symbol.\n+     *  @param env       The current environment.\n+     *  @param name      The type's name.\n+     *\/\n+    Symbol findTypeInternal(Env<AttrContext> env, Name name) {\n@@ -2929,0 +2985,7 @@\n+                    ClassSymbol refProjection = newConstr.owner.isValue() ?\n+                                                     (ClassSymbol) newConstr.owner.referenceProjection() : null;\n+                    if (refProjection != null) {\n+                        MethodSymbol clone = newConstr.clone(refProjection);\n+                        clone.projection = newConstr;\n+                        newConstr.projection = clone;\n+                    }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Resolve.java","additions":105,"deletions":42,"binary":false,"changes":147,"status":"modified"},{"patch":"@@ -2574,1 +2574,1 @@\n-        return isKind(doctree, VALUE);\n+        return isKind(doctree, Kind.VALUE);\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/toolkit\/util\/Utils.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -66,0 +66,77 @@\n+# Valhalla\n+compiler\/arguments\/CheckCICompilerCount.java                        8205030 generic-all\n+compiler\/arguments\/CheckCompileThresholdScaling.java                8205030 generic-all\n+compiler\/codecache\/CheckSegmentedCodeCache.java                     8205030 generic-all\n+compiler\/codecache\/cli\/TestSegmentedCodeCacheOption.java            8205030 generic-all\n+compiler\/codecache\/cli\/codeheapsize\/TestCodeHeapSizeOptions.java    8205030 generic-all\n+compiler\/codecache\/cli\/printcodecache\/TestPrintCodeCacheOption.java 8205030 generic-all\n+compiler\/whitebox\/OSRFailureLevel4Test.java                         8205030 generic-all\n+\n+compiler\/aot\/cli\/DisabledAOTWithLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/SingleAOTOptionTest.java 8226295 generic-all\n+compiler\/aot\/cli\/MultipleAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileClassWithDebugTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileModuleTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/AtFileTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionWrongFileTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ClasspathOptionUnknownClassTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileDirectoryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionNotExistingTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileClassTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileJarTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/IgnoreErrorsTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileAbsoluteDirectoryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/NonExistingAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/SingleAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/IncorrectAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/RecompilationTest.java 8226295 generic-all\n+compiler\/aot\/SharedUsageTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/ClassSearchTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/SearchPathTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/module\/ModuleSourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/ClassSourceTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/directory\/DirectorySourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/jar\/JarSourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/NativeOrderOutputStreamTest.java 8226295 generic-all\n+compiler\/aot\/verification\/vmflags\/TrackedFlagTest.java 8226295 generic-all\n+compiler\/aot\/verification\/vmflags\/NotTrackedFlagTest.java 8226295 generic-all\n+compiler\/aot\/verification\/ClassAndLibraryNotMatchTest.java 8226295 generic-all\n+compiler\/aot\/DeoptimizationTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SelfChanged.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SelfChangedCDS.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SuperChanged.java 8226295 generic-all\n+\n@@ -89,0 +166,22 @@\n+# Valhalla TODO:\n+runtime\/CompressedOops\/CompressedClassPointers.java 8210258 generic-all\n+runtime\/RedefineTests\/RedefineLeak.java 8205032 generic-all\n+runtime\/SharedArchiveFile\/BootAppendTests.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/CdsDifferentCompactStrings.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/CdsDifferentObjectAlignment.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/NonBootLoaderClasses.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/PrintSharedArchiveAndExit.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedArchiveFile.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedStringsDedup.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedStringsRunAuto.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedSymbolTableBucketSize.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SpaceUtilizationCheck.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/TestInterpreterMethodEntries.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformInterfaceAndImplementor.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformSuperAndSubClasses.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformSuperSubTwoPckgs.java 8210258 generic-all\n+runtime\/appcds\/ClassLoaderTest.java 8210258 generic-all\n+runtime\/appcds\/HelloTest.java 8210258 generic-all\n+runtime\/appcds\/sharedStrings\/SharedStringsBasic.java 8210258 generic-all\n+\n+\n@@ -102,0 +201,26 @@\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":125,"deletions":0,"binary":false,"changes":125,"status":"modified"}]}