{"files":[{"patch":"@@ -666,0 +666,2 @@\n+e3f940bd3c8fcdf4ca704c6eb1ac745d155859d5 jdk-15+36\n+5c18d696c7ce724ca36df13933aa53f50e12b9e0 jdk-16+11\n","filename":".hgtags","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -140,1 +140,14 @@\n-  intptr_t* locals = interpreter_frame->sender_sp() + max_locals - 1;\n+  \/\/\n+  \/\/ The interpreted method entry on AArch64 aligns SP to 16 bytes\n+  \/\/ before generating the fixed part of the activation frame. So there\n+  \/\/ may be a gap between the locals block and the saved sender SP. For\n+  \/\/ an interpreted caller we need to recreate this gap and exactly\n+  \/\/ align the incoming parameters with the caller's temporary\n+  \/\/ expression stack. For other types of caller frame it doesn't\n+  \/\/ matter.\n+  intptr_t* locals;\n+  if (caller->is_interpreted_frame()) {\n+    locals = caller->interpreter_frame_last_sp() + caller_actual_parameters - 1;\n+  } else {\n+    locals = interpreter_frame->sender_sp() + max_locals - 1;\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/abstractInterpreter_aarch64.cpp","additions":15,"deletions":2,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -301,1 +301,1 @@\n-bool LIRGenerator::strength_reduce_multiply(LIR_Opr left, int c, LIR_Opr result, LIR_Opr tmp) {\n+bool LIRGenerator::strength_reduce_multiply(LIR_Opr left, jint c, LIR_Opr result, LIR_Opr tmp) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1590,0 +1590,3 @@\n+\n+  \/\/ Padding between locals and fixed part of activation frame to ensure\n+  \/\/ SP is always 16-byte aligned.\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"memory\/heapInspection.hpp\"\n@@ -47,0 +48,1 @@\n+class AbstractGangTask;\n@@ -88,0 +90,5 @@\n+class ParallelObjectIterator : public CHeapObj<mtGC> {\n+public:\n+  virtual void object_iterate(ObjectClosure* cl, uint worker_id) = 0;\n+};\n+\n@@ -411,0 +418,4 @@\n+  virtual ParallelObjectIterator* parallel_object_iterator(uint thread_num) {\n+    return NULL;\n+  }\n+\n@@ -492,1 +503,1 @@\n-  virtual WorkGang* get_safepoint_workers() { return NULL; }\n+  virtual WorkGang* safepoint_workers() { return NULL; }\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2013, 2020, Red Hat, Inc. All rights reserved.\n@@ -36,6 +36,0 @@\n-public:\n-  enum ArrayCopyStoreValMode {\n-    NONE,\n-    RESOLVE_BARRIER,\n-    EVAC_BARRIER\n-  };\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":1,"deletions":7,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -342,1 +342,1 @@\n-  \/\/ like clone, finalize, registerNatives.\n+  \/\/ like clone and finalize.\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"classfile\/systemDictionary.hpp\"\n@@ -32,0 +31,1 @@\n+#include \"memory\/archiveBuilder.hpp\"\n@@ -34,2 +34,0 @@\n-#include \"memory\/metadataFactory.hpp\"\n-#include \"memory\/metaspace.hpp\"\n@@ -39,4 +37,0 @@\n-#include \"oops\/compressedOops.hpp\"\n-#include \"oops\/objArrayKlass.hpp\"\n-#include \"prims\/jvmtiRedefineClasses.hpp\"\n-#include \"runtime\/handles.inline.hpp\"\n@@ -47,0 +41,1 @@\n+#include \"utilities\/align.hpp\"\n@@ -49,17 +44,2 @@\n-#ifndef O_BINARY       \/\/ if defined (Win32) use binary files.\n-#define O_BINARY 0     \/\/ otherwise do nothing.\n-#endif\n-\n-class DynamicArchiveBuilder : ResourceObj {\n-  static unsigned my_hash(const address& a) {\n-    return primitive_hash<address>(a);\n-  }\n-  static bool my_equals(const address& a0, const address& a1) {\n-    return primitive_equals<address>(a0, a1);\n-  }\n-  typedef ResourceHashtable<\n-      address, address,\n-      DynamicArchiveBuilder::my_hash,   \/\/ solaris compiler doesn't like: primitive_hash<address>\n-      DynamicArchiveBuilder::my_equals, \/\/ solaris compiler doesn't like: primitive_equals<address>\n-      16384, ResourceObj::C_HEAP> RelocationTable;\n-  RelocationTable _new_loc_table;\n+class DynamicArchiveBuilder : public ArchiveBuilder {\n+public:\n@@ -68,1 +48,0 @@\n-\n@@ -109,17 +88,2 @@\n-  template <typename T> T get_new_loc(T obj) {\n-    address* pp = _new_loc_table.get((address)obj);\n-    if (pp == NULL) {\n-      \/\/ Excluded klasses are not copied\n-      return NULL;\n-    } else {\n-      return (T)*pp;\n-    }\n-  }\n-\n-  address get_new_loc(MetaspaceClosure::Ref* ref) {\n-    return get_new_loc(ref->obj());\n-  }\n-\n-  template <typename T> bool has_new_loc(T obj) {\n-    address* pp = _new_loc_table.get((address)obj);\n-    return pp != NULL;\n+  template <typename T> T get_dumped_addr(T obj) {\n+    return (T)ArchiveBuilder::get_dumped_addr((address)obj);\n@@ -150,345 +114,0 @@\n-protected:\n-  enum FollowMode {\n-    make_a_copy, point_to_it, set_to_null\n-  };\n-\n-  void copy(MetaspaceClosure::Ref* ref, bool read_only) {\n-    int bytes = ref->size() * BytesPerWord;\n-    address old_obj = ref->obj();\n-    address new_obj = copy_impl(ref, read_only, bytes);\n-\n-    assert(new_obj != NULL, \"must be\");\n-    assert(new_obj != old_obj, \"must be\");\n-    bool isnew = _new_loc_table.put(old_obj, new_obj);\n-    assert(isnew, \"must be\");\n-  }\n-\n-  \/\/ Make a shallow copy of each eligible MetaspaceObj into the buffer.\n-  class ShallowCopier: public UniqueMetaspaceClosure {\n-    DynamicArchiveBuilder* _builder;\n-    bool _read_only;\n-  public:\n-    ShallowCopier(DynamicArchiveBuilder* shuffler, bool read_only)\n-      : _builder(shuffler), _read_only(read_only) {}\n-\n-    virtual bool do_unique_ref(Ref* orig_obj, bool read_only) {\n-      \/\/ This method gets called on each *original* object\n-      \/\/ reachable from _builder->iterate_roots(). Each orig_obj is\n-      \/\/ called exactly once.\n-      FollowMode mode = _builder->follow_ref(orig_obj);\n-\n-      if (mode == point_to_it) {\n-        if (read_only == _read_only) {\n-          log_debug(cds, dynamic)(\"ptr : \" PTR_FORMAT \" %s\", p2i(orig_obj->obj()),\n-                                  MetaspaceObj::type_name(orig_obj->msotype()));\n-          address p = orig_obj->obj();\n-          bool isnew = _builder->_new_loc_table.put(p, p);\n-          assert(isnew, \"must be\");\n-        }\n-        return false;\n-      }\n-\n-      if (mode == set_to_null) {\n-        log_debug(cds, dynamic)(\"nul : \" PTR_FORMAT \" %s\", p2i(orig_obj->obj()),\n-                                MetaspaceObj::type_name(orig_obj->msotype()));\n-        return false;\n-      }\n-\n-      if (read_only == _read_only) {\n-        \/\/ Make a shallow copy of orig_obj in a buffer (maintained\n-        \/\/ by copy_impl in a subclass of DynamicArchiveBuilder).\n-        _builder->copy(orig_obj, read_only);\n-      }\n-      return true;\n-    }\n-  };\n-\n-  \/\/ Relocate all embedded pointer fields within a MetaspaceObj's shallow copy\n-  class ShallowCopyEmbeddedRefRelocator: public UniqueMetaspaceClosure {\n-    DynamicArchiveBuilder* _builder;\n-  public:\n-    ShallowCopyEmbeddedRefRelocator(DynamicArchiveBuilder* shuffler)\n-      : _builder(shuffler) {}\n-\n-    \/\/ This method gets called on each *original* object reachable\n-    \/\/ from _builder->iterate_roots(). Each orig_obj is\n-    \/\/ called exactly once.\n-    virtual bool do_unique_ref(Ref* orig_ref, bool read_only) {\n-      FollowMode mode = _builder->follow_ref(orig_ref);\n-\n-      if (mode == point_to_it) {\n-        \/\/ We did not make a copy of this object\n-        \/\/ and we have nothing to update\n-        assert(_builder->get_new_loc(orig_ref) == NULL ||\n-               _builder->get_new_loc(orig_ref) == orig_ref->obj(), \"must be\");\n-        return false;\n-      }\n-\n-      if (mode == set_to_null) {\n-        \/\/ We did not make a copy of this object\n-        \/\/ and we have nothing to update\n-        assert(!_builder->has_new_loc(orig_ref->obj()), \"must not be copied or pointed to\");\n-        return false;\n-      }\n-\n-      \/\/ - orig_obj points to the original object.\n-      \/\/ - new_obj points to the shallow copy (created by ShallowCopier)\n-      \/\/   of orig_obj. new_obj is NULL if the orig_obj is excluded\n-      address orig_obj = orig_ref->obj();\n-      address new_obj  = _builder->get_new_loc(orig_ref);\n-\n-      assert(new_obj != orig_obj, \"must be\");\n-#ifdef ASSERT\n-      if (new_obj == NULL) {\n-        if (orig_ref->msotype() == MetaspaceObj::ClassType) {\n-          Klass* k = (Klass*)orig_obj;\n-          assert(k->is_instance_klass() &&\n-                 SystemDictionaryShared::is_excluded_class(InstanceKlass::cast(k)),\n-                 \"orig_obj must be excluded Class\");\n-        }\n-      }\n-#endif\n-\n-      log_debug(cds, dynamic)(\"Relocating \" PTR_FORMAT \" %s\", p2i(new_obj),\n-                              MetaspaceObj::type_name(orig_ref->msotype()));\n-      if (new_obj != NULL) {\n-        EmbeddedRefUpdater updater(_builder, orig_obj, new_obj);\n-        orig_ref->metaspace_pointers_do(&updater);\n-      }\n-\n-      return true; \/\/ keep recursing until every object is visited exactly once.\n-    }\n-\n-    virtual void push_special(SpecialRef type, Ref* ref, intptr_t* p) {\n-      \/\/ TODO:CDS - JDK-8234693 will consolidate this with an almost identical method in metaspaceShared.cpp\n-      assert_valid(type);\n-      address obj = ref->obj();\n-      address new_obj = _builder->get_new_loc(ref);\n-      size_t offset = pointer_delta(p, obj,  sizeof(u1));\n-      intptr_t* new_p = (intptr_t*)(new_obj + offset);\n-      switch (type) {\n-      case _method_entry_ref:\n-        assert(*p == *new_p, \"must be a copy\");\n-        break;\n-      case _internal_pointer_ref:\n-        {\n-          size_t off = pointer_delta(*((address*)p), obj, sizeof(u1));\n-          assert(0 <= intx(off) && intx(off) < ref->size() * BytesPerWord, \"must point to internal address\");\n-          *((address*)new_p) = new_obj + off;\n-        }\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-      }\n-      ArchivePtrMarker::mark_pointer((address*)new_p);\n-    }\n-  };\n-\n-  class EmbeddedRefUpdater: public MetaspaceClosure {\n-    DynamicArchiveBuilder* _builder;\n-    address _orig_obj;\n-    address _new_obj;\n-  public:\n-    EmbeddedRefUpdater(DynamicArchiveBuilder* shuffler, address orig_obj, address new_obj) :\n-      _builder(shuffler), _orig_obj(orig_obj), _new_obj(new_obj) {}\n-\n-    \/\/ This method gets called once for each pointer field F of orig_obj.\n-    \/\/ We update new_obj->F to point to the new location of orig_obj->F.\n-    \/\/\n-    \/\/ Example: Klass*  0x100 is copied to 0x400\n-    \/\/          Symbol* 0x200 is copied to 0x500\n-    \/\/\n-    \/\/ Let orig_obj == 0x100; and\n-    \/\/     new_obj  == 0x400; and\n-    \/\/     ((Klass*)orig_obj)->_name == 0x200;\n-    \/\/ Then this function effectively assigns\n-    \/\/     ((Klass*)new_obj)->_name = 0x500;\n-    virtual bool do_ref(Ref* ref, bool read_only) {\n-      address new_pointee = NULL;\n-\n-      if (ref->not_null()) {\n-        address old_pointee = ref->obj();\n-\n-        FollowMode mode = _builder->follow_ref(ref);\n-        if (mode == point_to_it) {\n-          new_pointee = old_pointee;\n-        } else if (mode == set_to_null) {\n-          new_pointee = NULL;\n-        } else {\n-          new_pointee = _builder->get_new_loc(old_pointee);\n-        }\n-      }\n-\n-      const char* kind = MetaspaceObj::type_name(ref->msotype());\n-      \/\/ offset of this field inside the original object\n-      intx offset = (address)ref->addr() - _orig_obj;\n-      _builder->update_pointer((address*)(_new_obj + offset), new_pointee, kind, offset);\n-\n-      \/\/ We can't mark the pointer here, because DynamicArchiveBuilder::sort_methods\n-      \/\/ may re-layout the [iv]tables, which would change the offset(s) in an InstanceKlass\n-      \/\/ that would contain pointers. Therefore, we must mark the pointers after\n-      \/\/ sort_methods(), using PointerMarker.\n-      return false; \/\/ Do not recurse.\n-    }\n-  };\n-\n-  class ExternalRefUpdater: public MetaspaceClosure {\n-    DynamicArchiveBuilder* _builder;\n-\n-  public:\n-    ExternalRefUpdater(DynamicArchiveBuilder* shuffler) : _builder(shuffler) {}\n-\n-    virtual bool do_ref(Ref* ref, bool read_only) {\n-      \/\/ ref is a pointer that lives OUTSIDE of the buffer, but points to an object inside the buffer\n-      if (ref->not_null()) {\n-        address new_loc = _builder->get_new_loc(ref);\n-        const char* kind = MetaspaceObj::type_name(ref->msotype());\n-        _builder->update_pointer(ref->addr(), new_loc, kind, 0);\n-        _builder->mark_pointer(ref->addr());\n-      }\n-      return false; \/\/ Do not recurse.\n-    }\n-  };\n-\n-  class PointerMarker: public UniqueMetaspaceClosure {\n-    DynamicArchiveBuilder* _builder;\n-\n-  public:\n-    PointerMarker(DynamicArchiveBuilder* shuffler) : _builder(shuffler) {}\n-\n-    virtual bool do_unique_ref(Ref* ref, bool read_only) {\n-      if (_builder->is_in_buffer_space(ref->obj())) {\n-        EmbeddedRefMarker ref_marker(_builder);\n-        ref->metaspace_pointers_do(&ref_marker);\n-        return true; \/\/ keep recursing until every buffered object is visited exactly once.\n-      } else {\n-        return false;\n-      }\n-    }\n-  };\n-\n-  class EmbeddedRefMarker: public MetaspaceClosure {\n-    DynamicArchiveBuilder* _builder;\n-\n-  public:\n-    EmbeddedRefMarker(DynamicArchiveBuilder* shuffler) : _builder(shuffler) {}\n-    virtual bool do_ref(Ref* ref, bool read_only) {\n-      if (ref->not_null()) {\n-        _builder->mark_pointer(ref->addr());\n-      }\n-      return false; \/\/ Do not recurse.\n-    }\n-  };\n-\n-  void update_pointer(address* addr, address value, const char* kind, uintx offset, bool is_mso_pointer=true) {\n-    \/\/ Propagate the the mask bits to the new value -- see comments above MetaspaceClosure::obj()\n-    if (is_mso_pointer) {\n-      const uintx FLAG_MASK = 0x03;\n-      uintx mask_bits = uintx(*addr) & FLAG_MASK;\n-      value = (address)(uintx(value) | mask_bits);\n-    }\n-\n-    if (*addr != value) {\n-      log_debug(cds, dynamic)(\"Update (%18s*) %3d [\" PTR_FORMAT \"] \" PTR_FORMAT \" -> \" PTR_FORMAT,\n-                              kind, int(offset), p2i(addr), p2i(*addr), p2i(value));\n-      *addr = value;\n-    }\n-  }\n-\n-private:\n-  GrowableArray<Symbol*>* _symbols; \/\/ symbols to dump\n-  GrowableArray<InstanceKlass*>* _klasses; \/\/ klasses to dump\n-\n-  void append(InstanceKlass* k) { _klasses->append(k); }\n-  void append(Symbol* s)        { _symbols->append(s); }\n-\n-  class GatherKlassesAndSymbols : public UniqueMetaspaceClosure {\n-    DynamicArchiveBuilder* _builder;\n-    bool _read_only;\n-\n-  public:\n-    GatherKlassesAndSymbols(DynamicArchiveBuilder* builder)\n-      : _builder(builder) {}\n-\n-    virtual bool do_unique_ref(Ref* ref, bool read_only) {\n-      if (_builder->follow_ref(ref) != make_a_copy) {\n-        return false;\n-      }\n-      if (ref->msotype() == MetaspaceObj::ClassType) {\n-        Klass* klass = (Klass*)ref->obj();\n-        assert(klass->is_klass(), \"must be\");\n-        if (klass->is_instance_klass()) {\n-          InstanceKlass* ik = InstanceKlass::cast(klass);\n-          assert(!SystemDictionaryShared::is_excluded_class(ik), \"must be\");\n-          _builder->append(ik);\n-          _builder->_estimated_metsapceobj_bytes += BytesPerWord; \/\/ See RunTimeSharedClassInfo::get_for()\n-        }\n-      } else if (ref->msotype() == MetaspaceObj::SymbolType) {\n-        _builder->append((Symbol*)ref->obj());\n-      }\n-\n-      int bytes = ref->size() * BytesPerWord;\n-      _builder->_estimated_metsapceobj_bytes += bytes;\n-\n-      return true;\n-    }\n-  };\n-\n-  FollowMode follow_ref(MetaspaceClosure::Ref *ref) {\n-    address obj = ref->obj();\n-    if (MetaspaceShared::is_in_shared_metaspace(obj)) {\n-      \/\/ Don't dump existing shared metadata again.\n-      return point_to_it;\n-    } else if (ref->msotype() == MetaspaceObj::MethodDataType) {\n-      return set_to_null;\n-    } else {\n-      if (ref->msotype() == MetaspaceObj::ClassType) {\n-        Klass* klass = (Klass*)ref->obj();\n-        assert(klass->is_klass(), \"must be\");\n-        if (klass->is_instance_klass()) {\n-          InstanceKlass* ik = InstanceKlass::cast(klass);\n-          if (SystemDictionaryShared::is_excluded_class(ik)) {\n-            ResourceMark rm;\n-            log_debug(cds, dynamic)(\"Skipping class (excluded): %s\", klass->external_name());\n-            return set_to_null;\n-          }\n-        } else if (klass->is_array_klass()) {\n-          \/\/ Don't support archiving of array klasses for now.\n-          ResourceMark rm;\n-          log_debug(cds, dynamic)(\"Skipping class (array): %s\", klass->external_name());\n-          return set_to_null;\n-        }\n-      }\n-\n-      return make_a_copy;\n-    }\n-  }\n-\n-  address copy_impl(MetaspaceClosure::Ref* ref, bool read_only, int bytes) {\n-    if (ref->msotype() == MetaspaceObj::ClassType) {\n-      \/\/ Save a pointer immediate in front of an InstanceKlass, so\n-      \/\/ we can do a quick lookup from InstanceKlass* -> RunTimeSharedClassInfo*\n-      \/\/ without building another hashtable. See RunTimeSharedClassInfo::get_for()\n-      \/\/ in systemDictionaryShared.cpp.\n-      address obj = ref->obj();\n-      Klass* klass = (Klass*)obj;\n-      if (klass->is_instance_klass()) {\n-        SystemDictionaryShared::validate_before_archiving(InstanceKlass::cast(klass));\n-        current_dump_space()->allocate(sizeof(address), BytesPerWord);\n-      }\n-    }\n-    address p = (address)current_dump_space()->allocate(bytes);\n-    address obj = ref->obj();\n-    log_debug(cds, dynamic)(\"COPY: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %5d %s\",\n-                            p2i(obj), p2i(p), bytes,\n-                            MetaspaceObj::type_name(ref->msotype()));\n-    memcpy(p, obj, bytes);\n-    intptr_t* archived_vtable = MetaspaceShared::get_archived_cpp_vtable(ref->msotype(), p);\n-    if (archived_vtable != NULL) {\n-      update_pointer((address*)p, (address)archived_vtable, \"vtb\", 0, \/*is_mso_pointer*\/false);\n-      mark_pointer((address*)p);\n-    }\n-\n-    return (address)p;\n-  }\n-\n@@ -502,1 +121,0 @@\n-  size_t _estimated_metsapceobj_bytes;   \/\/ all archived MetsapceObj's.\n@@ -515,1 +133,1 @@\n-  void set_symbols_permanent();\n+  void remark_pointers_for_instance_klass(InstanceKlass* k, bool should_mark) const;\n@@ -537,5 +155,1 @@\n-  DynamicArchiveBuilder() {\n-    _klasses = new (ResourceObj::C_HEAP, mtClass) GrowableArray<InstanceKlass*>(100, mtClass);\n-    _symbols = new (ResourceObj::C_HEAP, mtClass) GrowableArray<Symbol*>(1000, mtClass);\n-\n-    _estimated_metsapceobj_bytes = 0;\n+  DynamicArchiveBuilder() : ArchiveBuilder(NULL, NULL) {\n@@ -585,0 +199,2 @@\n+    SystemDictionaryShared::start_dumping();\n+\n@@ -589,10 +205,1 @@\n-    {\n-      ResourceMark rm;\n-      GatherKlassesAndSymbols gatherer(this);\n-\n-      SystemDictionaryShared::dumptime_classes_do(&gatherer);\n-      SymbolTable::metaspace_pointers_do(&gatherer);\n-      FileMapInfo::metaspace_pointers_do(&gatherer);\n-\n-      gatherer.finish();\n-    }\n+    gather_klasses_and_symbols();\n@@ -602,0 +209,1 @@\n+    set_dump_regions(MetaspaceShared::read_write_dump_space(), MetaspaceShared::read_only_dump_space());\n@@ -610,0 +218,1 @@\n+    gather_source_objs();\n@@ -613,1 +222,1 @@\n-                           _klasses->length(), _symbols->length());\n+                           klasses()->length(), symbols()->length());\n@@ -615,8 +224,1 @@\n-    {\n-      assert(current_dump_space() == MetaspaceShared::read_write_dump_space(),\n-             \"Current dump space is not rw space\");\n-      \/\/ shallow-copy RW objects, if necessary\n-      ResourceMark rm;\n-      ShallowCopier rw_copier(this, false);\n-      iterate_roots(&rw_copier);\n-    }\n+    dump_rw_region();\n@@ -626,22 +228,3 @@\n-    {\n-      start_dump_space(ro_space);\n-\n-      \/\/ shallow-copy RO objects, if necessary\n-      ResourceMark rm;\n-      ShallowCopier ro_copier(this, true);\n-      iterate_roots(&ro_copier);\n-    }\n-\n-    {\n-      log_info(cds)(\"Relocating embedded pointers ... \");\n-      ResourceMark rm;\n-      ShallowCopyEmbeddedRefRelocator emb_reloc(this);\n-      iterate_roots(&emb_reloc);\n-    }\n-\n-    {\n-      log_info(cds)(\"Relocating external roots ... \");\n-      ResourceMark rm;\n-      ExternalRefUpdater ext_reloc(this);\n-      iterate_roots(&ext_reloc);\n-    }\n+    start_dump_space(ro_space);\n+    dump_ro_region();\n+    relocate_pointers();\n@@ -653,4 +236,1 @@\n-      set_symbols_permanent();\n-\n-      \/\/ Note that these tables still point to the *original* objects\n-      \/\/ (because they were not processed by ExternalRefUpdater), so\n+      \/\/ Note that these tables still point to the *original* objects, so\n@@ -673,0 +253,2 @@\n+\n+    log_info(cds)(\"Make classes shareable\");\n@@ -675,4 +257,2 @@\n-    {\n-      log_info(cds)(\"Adjust lambda proxy class dictionary\");\n-      SystemDictionaryShared::adjust_lambda_proxy_class_dictionary();\n-    }\n+    log_info(cds)(\"Adjust lambda proxy class dictionary\");\n+    SystemDictionaryShared::adjust_lambda_proxy_class_dictionary();\n@@ -680,7 +260,2 @@\n-    {\n-      log_info(cds)(\"Final relocation of pointers ... \");\n-      ResourceMark rm;\n-      PointerMarker marker(this);\n-      iterate_roots(&marker);\n-      relocate_buffer_to_target();\n-    }\n+    log_info(cds)(\"Final relocation of pointers ... \");\n+    relocate_buffer_to_target();\n@@ -695,10 +270,4 @@\n-  void iterate_roots(MetaspaceClosure* it) {\n-    int i;\n-    int num_klasses = _klasses->length();\n-    for (i = 0; i < num_klasses; i++) {\n-      it->push(&_klasses->at(i));\n-    }\n-\n-    int num_symbols = _symbols->length();\n-    for (i = 0; i < num_symbols; i++) {\n-      it->push(&_symbols->at(i));\n+  virtual void iterate_roots(MetaspaceClosure* it, bool is_relocating_pointers) {\n+    if (!is_relocating_pointers) {\n+      SystemDictionaryShared::dumptime_classes_do(it);\n+      SymbolTable::metaspace_pointers_do(it);\n@@ -706,10 +275,0 @@\n-\n-\n-    \/\/ Do not call these again, as we have already collected all the classes and symbols\n-    \/\/ that we want to archive. Also, these calls would corrupt the tables when\n-    \/\/ ExternalRefUpdater is used.\n-    \/\/\n-    \/\/ SystemDictionaryShared::dumptime_classes_do(it);\n-    \/\/ SymbolTable::metaspace_pointers_do(it);\n-\n-    it->finish();\n@@ -722,1 +281,0 @@\n-\n@@ -812,4 +370,6 @@\n-  for (int i = 0; i < _klasses->length(); i++) {\n-    InstanceKlass* ik = _klasses->at(i);\n-    Array<Method*>* methods = ik->methods();\n-    total += each_method_bytes * methods->length();\n+  for (int i = 0; i < klasses()->length(); i++) {\n+    Klass* k = klasses()->at(i);\n+    if (k->is_instance_klass()) {\n+      Array<Method*>* methods = InstanceKlass::cast(k)->methods();\n+      total += each_method_bytes * methods->length();\n+    }\n@@ -827,2 +387,6 @@\n-  for (int i = 0; i < _klasses->length(); i++) {\n-    InstanceKlass* ik = _klasses->at(i);\n+  for (int i = 0; i < klasses()->length(); i++) {\n+    Klass* k = klasses()->at(i);\n+    if (!k->is_instance_klass()) {\n+      continue;\n+    }\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n@@ -861,1 +425,1 @@\n-  int i, count = _klasses->length();\n+  int i, count = klasses()->length();\n@@ -865,2 +429,4 @@\n-    InstanceKlass* ik = _klasses->at(i);\n-    sort_methods(ik);\n+    Klass* k = klasses()->at(i);\n+    if (k->is_instance_klass()) {\n+      sort_methods(InstanceKlass::cast(k));\n+    }\n@@ -870,1 +436,5 @@\n-    InstanceKlass* ik = _klasses->at(i);\n+    Klass* k = klasses()->at(i);\n+    if (!k->is_instance_klass()) {\n+      continue;\n+    }\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n@@ -905,0 +475,5 @@\n+  \/\/ Method sorting may re-layout the [iv]tables, which would change the offset(s)\n+  \/\/ of the locations in an InstanceKlass that would contain pointers. Let's clear\n+  \/\/ all the existing pointer marking bits, and re-mark the pointers after sorting.\n+  remark_pointers_for_instance_klass(ik, false);\n+\n@@ -935,6 +510,3 @@\n-}\n-void DynamicArchiveBuilder::set_symbols_permanent() {\n-  int count = _symbols->length();\n-  for (int i=0; i<count; i++) {\n-    Symbol* s = _symbols->at(i);\n-    s->set_permanent();\n+  \/\/ Set all the pointer marking bits after sorting.\n+  remark_pointers_for_instance_klass(ik, true);\n+}\n@@ -943,3 +515,8 @@\n-    if (log_is_enabled(Trace, cds, dynamic)) {\n-      ResourceMark rm;\n-      log_trace(cds, dynamic)(\"symbols[%4i] = \" PTR_FORMAT \" %s\", i, p2i(to_target(s)), s->as_quoted_ascii());\n+template<bool should_mark>\n+class PointerRemarker: public MetaspaceClosure {\n+public:\n+  virtual bool do_ref(Ref* ref, bool read_only) {\n+    if (should_mark) {\n+      ArchivePtrMarker::mark_pointer(ref->addr());\n+    } else {\n+      ArchivePtrMarker::clear_pointer(ref->addr());\n@@ -947,0 +524,13 @@\n+    return false; \/\/ don't recurse\n+  }\n+};\n+\n+void DynamicArchiveBuilder::remark_pointers_for_instance_klass(InstanceKlass* k, bool should_mark) const {\n+  if (should_mark) {\n+    PointerRemarker<true> marker;\n+    k->metaspace_pointers_do(&marker);\n+    marker.finish();\n+  } else {\n+    PointerRemarker<false> marker;\n+    k->metaspace_pointers_do(&marker);\n+    marker.finish();\n@@ -1032,2 +622,2 @@\n-  int num_klasses = _klasses->length();\n-  int num_symbols = _symbols->length();\n+  int num_klasses = klasses()->length();\n+  int num_symbols = symbols()->length();\n@@ -1061,1 +651,0 @@\n-\n@@ -1099,1 +688,1 @@\n-  address buff_obj = _builder->get_new_loc(orig_obj);\n+  address buff_obj = _builder->get_dumped_addr(orig_obj);\n@@ -1118,1 +707,1 @@\n-  address buff_obj = _builder->get_new_loc(orig_obj);\n+  address buff_obj = _builder->get_dumped_addr(orig_obj);\n","filename":"src\/hotspot\/share\/memory\/dynamicArchive.cpp","additions":84,"deletions":495,"binary":false,"changes":579,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -242,0 +243,35 @@\n+\/\/ Return false if the entry could not be recorded on account\n+\/\/ of running out of space required to create a new entry.\n+bool KlassInfoTable::merge_entry(const KlassInfoEntry* cie) {\n+  Klass*          k = cie->klass();\n+  KlassInfoEntry* elt = lookup(k);\n+  \/\/ elt may be NULL if it's a new klass for which we\n+  \/\/ could not allocate space for a new entry in the hashtable.\n+  if (elt != NULL) {\n+    elt->set_count(elt->count() + cie->count());\n+    elt->set_words(elt->words() + cie->words());\n+    _size_of_instances_in_words += cie->words();\n+    return true;\n+  }\n+  return false;\n+}\n+\n+class KlassInfoTableMergeClosure : public KlassInfoClosure {\n+private:\n+  KlassInfoTable* _dest;\n+  bool _success;\n+public:\n+  KlassInfoTableMergeClosure(KlassInfoTable* table) : _dest(table), _success(true) {}\n+  void do_cinfo(KlassInfoEntry* cie) {\n+    _success &= _dest->merge_entry(cie);\n+  }\n+  bool success() { return _success; }\n+};\n+\n+\/\/ merge from table\n+bool KlassInfoTable::merge(KlassInfoTable* table) {\n+  KlassInfoTableMergeClosure closure(this);\n+  table->iterate(&closure);\n+  return closure.success();\n+}\n+\n@@ -619,1 +655,1 @@\n-  size_t _missed_count;\n+  uintx _missed_count;\n@@ -633,1 +669,1 @@\n-  size_t missed_count() { return _missed_count; }\n+  uintx missed_count() { return _missed_count; }\n@@ -641,2 +677,9 @@\n-size_t HeapInspection::populate_table(KlassInfoTable* cit, BoolObjectClosure *filter) {\n-  ResourceMark rm;\n+\/\/ Heap inspection for every worker.\n+\/\/ When native OOM happens for KlassInfoTable, set _success to false.\n+void ParHeapInspectTask::work(uint worker_id) {\n+  uintx missed_count = 0;\n+  bool merge_success = true;\n+  if (!Atomic::load(&_success)) {\n+    \/\/ other worker has failed on parallel iteration.\n+    return;\n+  }\n@@ -644,0 +687,51 @@\n+  KlassInfoTable cit(false);\n+  if (cit.allocation_failed()) {\n+    \/\/ fail to allocate memory, stop parallel mode\n+    Atomic::store(&_success, false);\n+    return;\n+  }\n+  RecordInstanceClosure ric(&cit, _filter);\n+  _poi->object_iterate(&ric, worker_id);\n+  missed_count = ric.missed_count();\n+  {\n+    MutexLocker x(&_mutex);\n+    merge_success = _shared_cit->merge(&cit);\n+  }\n+  if (merge_success) {\n+    Atomic::add(&_missed_count, missed_count);\n+  } else {\n+    Atomic::store(&_success, false);\n+  }\n+}\n+\n+uintx HeapInspection::populate_table(KlassInfoTable* cit, BoolObjectClosure *filter, uint parallel_thread_num) {\n+\n+  \/\/ Try parallel first.\n+  if (parallel_thread_num > 1) {\n+    ResourceMark rm;\n+\n+    WorkGang* gang = Universe::heap()->safepoint_workers();\n+    if (gang != NULL) {\n+      \/\/ The GC provided a WorkGang to be used during a safepoint.\n+\n+      \/\/ Can't run with more threads than provided by the WorkGang.\n+      WithUpdatedActiveWorkers update_and_restore(gang, parallel_thread_num);\n+\n+      ParallelObjectIterator* poi = Universe::heap()->parallel_object_iterator(gang->active_workers());\n+      if (poi != NULL) {\n+        \/\/ The GC supports parallel object iteration.\n+\n+        ParHeapInspectTask task(poi, cit, filter);\n+        \/\/ Run task with the active workers.\n+        gang->run_task(&task);\n+\n+        delete poi;\n+        if (task.success()) {\n+          return task.missed_count();\n+        }\n+      }\n+    }\n+  }\n+\n+  ResourceMark rm;\n+  \/\/ If no parallel iteration available, run serially.\n@@ -649,1 +743,1 @@\n-void HeapInspection::heap_inspection(outputStream* st) {\n+void HeapInspection::heap_inspection(outputStream* st, uint parallel_thread_num) {\n@@ -655,1 +749,1 @@\n-    size_t missed_count = populate_table(&cit);\n+    uintx missed_count = populate_table(&cit, NULL, parallel_thread_num);\n@@ -657,1 +751,1 @@\n-      log_info(gc, classhisto)(\"WARNING: Ran out of C-heap; undercounted \" SIZE_FORMAT\n+      log_info(gc, classhisto)(\"WARNING: Ran out of C-heap; undercounted \" UINTX_FORMAT\n","filename":"src\/hotspot\/share\/memory\/heapInspection.cpp","additions":101,"deletions":7,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -33,0 +33,3 @@\n+#include \"gc\/shared\/workgroup.hpp\"\n+\n+class ParallelObjectIterator;\n@@ -54,1 +57,1 @@\n-  long            _instance_count;\n+  uint64_t        _instance_count;\n@@ -56,1 +59,1 @@\n-  long            _index;\n+  int64_t         _index;\n@@ -68,7 +71,7 @@\n-  Klass* klass()  const      { return _klass; }\n-  long count()    const      { return _instance_count; }\n-  void set_count(long ct)    { _instance_count = ct; }\n-  size_t words()  const      { return _instance_words; }\n-  void set_words(size_t wds) { _instance_words = wds; }\n-  void set_index(long index) { _index = index; }\n-  long index()    const      { return _index; }\n+  Klass* klass()  const          { return _klass; }\n+  uint64_t count()    const      { return _instance_count; }\n+  void set_count(uint64_t ct)    { _instance_count = ct; }\n+  size_t words()  const          { return _instance_words; }\n+  void set_words(size_t wds)     { _instance_words = wds; }\n+  void set_index(int64_t index)  { _index = index; }\n+  int64_t index()    const       { return _index; }\n@@ -125,0 +128,2 @@\n+  bool merge(KlassInfoTable* table);\n+  bool merge_entry(const KlassInfoEntry* cie);\n@@ -219,2 +224,2 @@\n-  void heap_inspection(outputStream* st) NOT_SERVICES_RETURN;\n-  size_t populate_table(KlassInfoTable* cit, BoolObjectClosure* filter = NULL) NOT_SERVICES_RETURN_(0);\n+  void heap_inspection(outputStream* st, uint parallel_thread_num = 1) NOT_SERVICES_RETURN;\n+  uintx populate_table(KlassInfoTable* cit, BoolObjectClosure* filter = NULL, uint parallel_thread_num = 1) NOT_SERVICES_RETURN_(0);\n@@ -226,0 +231,35 @@\n+\/\/ Parallel heap inspection task. Parallel inspection can fail due to\n+\/\/ a native OOM when allocating memory for TL-KlassInfoTable.\n+\/\/ _success will be set false on an OOM, and serial inspection tried.\n+class ParHeapInspectTask : public AbstractGangTask {\n+ private:\n+  ParallelObjectIterator* _poi;\n+  KlassInfoTable* _shared_cit;\n+  BoolObjectClosure* _filter;\n+  uintx _missed_count;\n+  bool _success;\n+  Mutex _mutex;\n+\n+ public:\n+  ParHeapInspectTask(ParallelObjectIterator* poi,\n+                     KlassInfoTable* shared_cit,\n+                     BoolObjectClosure* filter) :\n+      AbstractGangTask(\"Iterating heap\"),\n+      _poi(poi),\n+      _shared_cit(shared_cit),\n+      _filter(filter),\n+      _missed_count(0),\n+      _success(true),\n+      _mutex(Mutex::leaf, \"Parallel heap iteration data merge lock\") {}\n+\n+  uintx missed_count() const {\n+    return _missed_count;\n+  }\n+\n+  bool success() {\n+    return _success;\n+  }\n+\n+  virtual void work(uint worker_id);\n+};\n+\n","filename":"src\/hotspot\/share\/memory\/heapInspection.hpp","additions":51,"deletions":11,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -63,2 +63,1 @@\n-\/\/ objects allocated during archive creation time. See ArchiveCompactor in\n-\/\/ metaspaceShared.cpp for an example.\n+\/\/ objects allocated during archive creation time. See ArchiveBuilder for an example.\n@@ -115,0 +114,1 @@\n+    bool _keep_after_pushing;\n@@ -116,0 +116,1 @@\n+    void* _user_data;\n@@ -120,1 +121,1 @@\n-    Ref(Writability w) : _writability(w), _next(NULL) {}\n+    Ref(Writability w) : _writability(w), _keep_after_pushing(false), _next(NULL), _user_data(NULL) {}\n@@ -144,0 +145,4 @@\n+    void set_keep_after_pushing()   { _keep_after_pushing = true; }\n+    bool keep_after_pushing()       { return _keep_after_pushing; }\n+    void set_user_data(void* data)  { _user_data = data; }\n+    void* user_data()               { return _user_data; }\n@@ -250,2 +255,3 @@\n-  \/\/ If recursion is too deep, save the Refs in _pending_refs, and push them later using\n-  \/\/ MetaspaceClosure::finish()\n+  \/\/ Normally, chains of references like a->b->c->d are iterated recursively. However,\n+  \/\/ if recursion is too deep, we save the Refs in _pending_refs, and push them later in\n+  \/\/ MetaspaceClosure::finish(). This avoids overflowing the C stack.\n@@ -255,0 +261,1 @@\n+  Ref* _enclosing_ref;\n@@ -260,1 +267,1 @@\n-  MetaspaceClosure(): _pending_refs(NULL), _nest_level(0) {}\n+  MetaspaceClosure(): _pending_refs(NULL), _nest_level(0), _enclosing_ref(NULL) {}\n@@ -265,0 +272,20 @@\n+  \/\/ enclosing_ref() is used to compute the offset of a field in a C++ class. For example\n+  \/\/ class Foo { intx scala; Bar* ptr; }\n+  \/\/    Foo *f = 0x100;\n+  \/\/ when the f->ptr field is iterated with do_ref() on 64-bit platforms, we will have\n+  \/\/    do_ref(Ref* r) {\n+  \/\/       r->addr() == 0x108;                \/\/ == &f->ptr;\n+  \/\/       enclosing_ref()->obj() == 0x100;   \/\/ == foo\n+  \/\/ So we know that we are iterating upon a field at offset 8 of the object at 0x100.\n+  \/\/\n+  \/\/ Note that if we have stack overflow, do_pending_ref(r) will be called first and\n+  \/\/ do_ref(r) will be called later, for the same r. In this case, enclosing_ref() is valid only\n+  \/\/ when do_pending_ref(r) is called, and will return NULL when do_ref(r) is called.\n+  Ref* enclosing_ref() const {\n+    return _enclosing_ref;\n+  }\n+\n+  \/\/ This is called when a reference is placed in _pending_refs. Override this\n+  \/\/ function if you're using enclosing_ref(). See notes above.\n+  virtual void do_pending_ref(Ref* ref) {}\n+\n@@ -292,1 +319,5 @@\n-    push_special(_method_entry_ref, new ObjectRef<T>(mpp, _default), p);\n+    Ref* ref = new ObjectRef<T>(mpp, _default);\n+    push_special(_method_entry_ref, ref, p);\n+    if (!ref->keep_after_pushing()) {\n+      delete ref;\n+    }\n@@ -296,1 +327,5 @@\n-    push_special(_internal_pointer_ref, new ObjectRef<T>(mpp, _default), p);\n+    Ref* ref = new ObjectRef<T>(mpp, _default);\n+    push_special(_internal_pointer_ref, ref, p);\n+    if (!ref->keep_after_pushing()) {\n+      delete ref;\n+    }\n","filename":"src\/hotspot\/share\/memory\/metaspaceClosure.hpp","additions":44,"deletions":9,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -26,1 +26,0 @@\n-#include \"jvm.h\"\n@@ -30,1 +29,0 @@\n-#include \"classfile\/dictionary.hpp\"\n@@ -39,1 +37,0 @@\n-#include \"gc\/shared\/softRefPolicy.hpp\"\n@@ -44,0 +41,1 @@\n+#include \"memory\/archiveBuilder.hpp\"\n@@ -64,1 +62,0 @@\n-#include \"prims\/jvmtiRedefineClasses.hpp\"\n@@ -68,1 +65,0 @@\n-#include \"runtime\/signature.hpp\"\n@@ -115,2 +111,2 @@\n-\/\/ [3] ArchiveCompactor copies RW metadata into the rw region.\n-\/\/ [4] ArchiveCompactor copies RO metadata into the ro region.\n+\/\/ [3] ArchiveBuilder copies RW metadata into the rw region.\n+\/\/ [4] ArchiveBuilder copies RO metadata into the ro region.\n@@ -388,1 +384,1 @@\n-    \/\/   is done, ArchiveCompactor will copy the class metadata into this\n+    \/\/   is done, ArchiveBuilder will copy the class metadata into this\n@@ -629,4 +625,0 @@\n-static int global_klass_compare(Klass** a, Klass **b) {\n-  return a[0]->name()->fast_compare(b[0]->name());\n-}\n-\n@@ -637,58 +629,0 @@\n-static void collect_array_classes(Klass* k) {\n-  _global_klass_objects->append_if_missing(k);\n-  if (k->is_array_klass()) {\n-    \/\/ Add in the array classes too\n-    ArrayKlass* ak = ArrayKlass::cast(k);\n-    Klass* h = ak->higher_dimension();\n-    if (h != NULL) {\n-      h->array_klasses_do(collect_array_classes);\n-    }\n-  }\n-}\n-\n-class CollectClassesClosure : public KlassClosure {\n-  void do_klass(Klass* k) {\n-    if (k->is_instance_klass() &&\n-        SystemDictionaryShared::is_excluded_class(InstanceKlass::cast(k))) {\n-      \/\/ Don't add to the _global_klass_objects\n-    } else {\n-      _global_klass_objects->append_if_missing(k);\n-    }\n-    if (k->is_array_klass()) {\n-      \/\/ Add in the array classes too\n-      ArrayKlass* ak = ArrayKlass::cast(k);\n-      Klass* h = ak->higher_dimension();\n-      if (h != NULL) {\n-        h->array_klasses_do(collect_array_classes);\n-      }\n-    }\n-  }\n-};\n-\n-\/\/ Global object for holding symbols that created during class loading. See SymbolTable::new_symbol\n-static GrowableArray<Symbol*>* _global_symbol_objects = NULL;\n-\n-static int compare_symbols_by_address(Symbol** a, Symbol** b) {\n-  if (a[0] < b[0]) {\n-    return -1;\n-  } else if (a[0] == b[0]) {\n-    ResourceMark rm;\n-    log_warning(cds)(\"Duplicated symbol %s unexpected\", (*a)->as_C_string());\n-    return 0;\n-  } else {\n-    return 1;\n-  }\n-}\n-\n-void MetaspaceShared::add_symbol(Symbol* sym) {\n-  MutexLocker ml(CDSAddSymbol_lock, Mutex::_no_safepoint_check_flag);\n-  if (_global_symbol_objects == NULL) {\n-    _global_symbol_objects = new (ResourceObj::C_HEAP, mtSymbol) GrowableArray<Symbol*>(2048, mtSymbol);\n-  }\n-  _global_symbol_objects->append(sym);\n-}\n-\n-GrowableArray<Symbol*>* MetaspaceShared::collected_symbols() {\n-  return _global_symbol_objects;\n-}\n-\n@@ -1078,142 +1012,0 @@\n-\/\/ This is for dumping detailed statistics for the allocations\n-\/\/ in the shared spaces.\n-class DumpAllocStats : public ResourceObj {\n-public:\n-\n-  \/\/ Here's poor man's enum inheritance\n-#define SHAREDSPACE_OBJ_TYPES_DO(f) \\\n-  METASPACE_OBJ_TYPES_DO(f) \\\n-  f(SymbolHashentry) \\\n-  f(SymbolBucket) \\\n-  f(StringHashentry) \\\n-  f(StringBucket) \\\n-  f(Other)\n-\n-  enum Type {\n-    \/\/ Types are MetaspaceObj::ClassType, MetaspaceObj::SymbolType, etc\n-    SHAREDSPACE_OBJ_TYPES_DO(METASPACE_OBJ_TYPE_DECLARE)\n-    _number_of_types\n-  };\n-\n-  static const char * type_name(Type type) {\n-    switch(type) {\n-    SHAREDSPACE_OBJ_TYPES_DO(METASPACE_OBJ_TYPE_NAME_CASE)\n-    default:\n-      ShouldNotReachHere();\n-      return NULL;\n-    }\n-  }\n-\n-public:\n-  enum { RO = 0, RW = 1 };\n-\n-  int _counts[2][_number_of_types];\n-  int _bytes [2][_number_of_types];\n-\n-  DumpAllocStats() {\n-    memset(_counts, 0, sizeof(_counts));\n-    memset(_bytes,  0, sizeof(_bytes));\n-  };\n-\n-  void record(MetaspaceObj::Type type, int byte_size, bool read_only) {\n-    assert(int(type) >= 0 && type < MetaspaceObj::_number_of_types, \"sanity\");\n-    int which = (read_only) ? RO : RW;\n-    _counts[which][type] ++;\n-    _bytes [which][type] += byte_size;\n-  }\n-\n-  void record_other_type(int byte_size, bool read_only) {\n-    int which = (read_only) ? RO : RW;\n-    _bytes [which][OtherType] += byte_size;\n-  }\n-  void print_stats(int ro_all, int rw_all, int mc_all);\n-};\n-\n-void DumpAllocStats::print_stats(int ro_all, int rw_all, int mc_all) {\n-  \/\/ Calculate size of data that was not allocated by Metaspace::allocate()\n-  MetaspaceSharedStats *stats = MetaspaceShared::stats();\n-\n-  \/\/ symbols\n-  _counts[RO][SymbolHashentryType] = stats->symbol.hashentry_count;\n-  _bytes [RO][SymbolHashentryType] = stats->symbol.hashentry_bytes;\n-\n-  _counts[RO][SymbolBucketType] = stats->symbol.bucket_count;\n-  _bytes [RO][SymbolBucketType] = stats->symbol.bucket_bytes;\n-\n-  \/\/ strings\n-  _counts[RO][StringHashentryType] = stats->string.hashentry_count;\n-  _bytes [RO][StringHashentryType] = stats->string.hashentry_bytes;\n-\n-  _counts[RO][StringBucketType] = stats->string.bucket_count;\n-  _bytes [RO][StringBucketType] = stats->string.bucket_bytes;\n-\n-  \/\/ TODO: count things like dictionary, vtable, etc\n-  _bytes[RW][OtherType] += mc_all;\n-  rw_all += mc_all; \/\/ mc is mapped Read\/Write\n-\n-  \/\/ prevent divide-by-zero\n-  if (ro_all < 1) {\n-    ro_all = 1;\n-  }\n-  if (rw_all < 1) {\n-    rw_all = 1;\n-  }\n-\n-  int all_ro_count = 0;\n-  int all_ro_bytes = 0;\n-  int all_rw_count = 0;\n-  int all_rw_bytes = 0;\n-\n-\/\/ To make fmt_stats be a syntactic constant (for format warnings), use #define.\n-#define fmt_stats \"%-20s: %8d %10d %5.1f | %8d %10d %5.1f | %8d %10d %5.1f\"\n-  const char *sep = \"--------------------+---------------------------+---------------------------+--------------------------\";\n-  const char *hdr = \"                        ro_cnt   ro_bytes     % |   rw_cnt   rw_bytes     % |  all_cnt  all_bytes     %\";\n-\n-  LogMessage(cds) msg;\n-\n-  msg.debug(\"Detailed metadata info (excluding st regions; rw stats include mc regions):\");\n-  msg.debug(\"%s\", hdr);\n-  msg.debug(\"%s\", sep);\n-  for (int type = 0; type < int(_number_of_types); type ++) {\n-    const char *name = type_name((Type)type);\n-    int ro_count = _counts[RO][type];\n-    int ro_bytes = _bytes [RO][type];\n-    int rw_count = _counts[RW][type];\n-    int rw_bytes = _bytes [RW][type];\n-    int count = ro_count + rw_count;\n-    int bytes = ro_bytes + rw_bytes;\n-\n-    double ro_perc = percent_of(ro_bytes, ro_all);\n-    double rw_perc = percent_of(rw_bytes, rw_all);\n-    double perc    = percent_of(bytes, ro_all + rw_all);\n-\n-    msg.debug(fmt_stats, name,\n-                         ro_count, ro_bytes, ro_perc,\n-                         rw_count, rw_bytes, rw_perc,\n-                         count, bytes, perc);\n-\n-    all_ro_count += ro_count;\n-    all_ro_bytes += ro_bytes;\n-    all_rw_count += rw_count;\n-    all_rw_bytes += rw_bytes;\n-  }\n-\n-  int all_count = all_ro_count + all_rw_count;\n-  int all_bytes = all_ro_bytes + all_rw_bytes;\n-\n-  double all_ro_perc = percent_of(all_ro_bytes, ro_all);\n-  double all_rw_perc = percent_of(all_rw_bytes, rw_all);\n-  double all_perc    = percent_of(all_bytes, ro_all + rw_all);\n-\n-  msg.debug(\"%s\", sep);\n-  msg.debug(fmt_stats, \"Total\",\n-                       all_ro_count, all_ro_bytes, all_ro_perc,\n-                       all_rw_count, all_rw_bytes, all_rw_perc,\n-                       all_count, all_bytes, all_perc);\n-\n-  assert(all_ro_bytes == ro_all, \"everything should have been counted\");\n-  assert(all_rw_bytes == rw_all, \"everything should have been counted\");\n-\n-#undef fmt_stats\n-}\n-\n@@ -1236,1 +1028,0 @@\n-  void print_class_stats();\n@@ -1250,165 +1041,1 @@\n-\/\/ ArchiveCompactor --\n-\/\/\n-\/\/ This class is the central piece of shared archive compaction -- all metaspace data are\n-\/\/ initially allocated outside of the shared regions. ArchiveCompactor copies the\n-\/\/ metaspace data into their final location in the shared regions.\n-\n-class ArchiveCompactor : AllStatic {\n-  static const int INITIAL_TABLE_SIZE = 8087;\n-  static const int MAX_TABLE_SIZE     = 1000000;\n-\n-  static DumpAllocStats* _alloc_stats;\n-\n-  typedef KVHashtable<address, address, mtInternal> RelocationTable;\n-  static RelocationTable* _new_loc_table;\n-\n-public:\n-  static void initialize() {\n-    _alloc_stats = new(ResourceObj::C_HEAP, mtInternal)DumpAllocStats;\n-    _new_loc_table = new RelocationTable(INITIAL_TABLE_SIZE);\n-  }\n-  static DumpAllocStats* alloc_stats() {\n-    return _alloc_stats;\n-  }\n-\n-  \/\/ Use this when you allocate space with MetaspaceShare::read_only_space_alloc()\n-  \/\/ outside of ArchiveCompactor::allocate(). These are usually for misc tables\n-  \/\/ that are allocated in the RO space.\n-  class OtherROAllocMark {\n-    char* _oldtop;\n-  public:\n-    OtherROAllocMark() {\n-      _oldtop = _ro_region.top();\n-    }\n-    ~OtherROAllocMark() {\n-      char* newtop = _ro_region.top();\n-      ArchiveCompactor::alloc_stats()->record_other_type(int(newtop - _oldtop), true);\n-    }\n-  };\n-\n-  static void allocate(MetaspaceClosure::Ref* ref, bool read_only) {\n-    address obj = ref->obj();\n-    int bytes = ref->size() * BytesPerWord;\n-    char* p;\n-    size_t alignment = BytesPerWord;\n-    char* oldtop;\n-    char* newtop;\n-\n-    if (read_only) {\n-      oldtop = _ro_region.top();\n-      p = _ro_region.allocate(bytes, alignment);\n-      newtop = _ro_region.top();\n-    } else {\n-      oldtop = _rw_region.top();\n-      if (ref->msotype() == MetaspaceObj::ClassType) {\n-        \/\/ Save a pointer immediate in front of an InstanceKlass, so\n-        \/\/ we can do a quick lookup from InstanceKlass* -> RunTimeSharedClassInfo*\n-        \/\/ without building another hashtable. See RunTimeSharedClassInfo::get_for()\n-        \/\/ in systemDictionaryShared.cpp.\n-        Klass* klass = (Klass*)obj;\n-        if (klass->is_instance_klass()) {\n-          SystemDictionaryShared::validate_before_archiving(InstanceKlass::cast(klass));\n-          _rw_region.allocate(sizeof(address), BytesPerWord);\n-        }\n-      }\n-      p = _rw_region.allocate(bytes, alignment);\n-      newtop = _rw_region.top();\n-    }\n-    memcpy(p, obj, bytes);\n-\n-    intptr_t* archived_vtable = MetaspaceShared::get_archived_cpp_vtable(ref->msotype(), (address)p);\n-    if (archived_vtable != NULL) {\n-      *(address*)p = (address)archived_vtable;\n-      ArchivePtrMarker::mark_pointer((address*)p);\n-    }\n-\n-    assert(_new_loc_table->lookup(obj) == NULL, \"each object can be relocated at most once\");\n-    _new_loc_table->add(obj, (address)p);\n-    log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d\", p2i(obj), p2i(p), bytes);\n-    if (_new_loc_table->maybe_grow(MAX_TABLE_SIZE)) {\n-      log_info(cds, hashtables)(\"Expanded _new_loc_table to %d\", _new_loc_table->table_size());\n-    }\n-    _alloc_stats->record(ref->msotype(), int(newtop - oldtop), read_only);\n-  }\n-\n-  static address get_new_loc(MetaspaceClosure::Ref* ref) {\n-    address* pp = _new_loc_table->lookup(ref->obj());\n-    assert(pp != NULL, \"must be\");\n-    return *pp;\n-  }\n-\n-private:\n-  \/\/ Makes a shallow copy of visited MetaspaceObj's\n-  class ShallowCopier: public UniqueMetaspaceClosure {\n-    bool _read_only;\n-  public:\n-    ShallowCopier(bool read_only) : _read_only(read_only) {}\n-\n-    virtual bool do_unique_ref(Ref* ref, bool read_only) {\n-      if (read_only == _read_only) {\n-        allocate(ref, read_only);\n-      }\n-      return true; \/\/ recurse into ref.obj()\n-    }\n-  };\n-\n-  \/\/ Relocate embedded pointers within a MetaspaceObj's shallow copy\n-  class ShallowCopyEmbeddedRefRelocator: public UniqueMetaspaceClosure {\n-  public:\n-    virtual bool do_unique_ref(Ref* ref, bool read_only) {\n-      address new_loc = get_new_loc(ref);\n-      RefRelocator refer;\n-      ref->metaspace_pointers_do_at(&refer, new_loc);\n-      return true; \/\/ recurse into ref.obj()\n-    }\n-    virtual void push_special(SpecialRef type, Ref* ref, intptr_t* p) {\n-      assert_valid(type);\n-\n-      address obj = ref->obj();\n-      address new_obj = get_new_loc(ref);\n-      size_t offset = pointer_delta(p, obj,  sizeof(u1));\n-      intptr_t* new_p = (intptr_t*)(new_obj + offset);\n-      switch (type) {\n-      case _method_entry_ref:\n-        assert(*p == *new_p, \"must be a copy\");\n-        break;\n-      case _internal_pointer_ref:\n-        {\n-          size_t off = pointer_delta(*((address*)p), obj, sizeof(u1));\n-          assert(0 <= intx(off) && intx(off) < ref->size() * BytesPerWord, \"must point to internal address\");\n-          *((address*)new_p) = new_obj + off;\n-        }\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-      }\n-      ArchivePtrMarker::mark_pointer((address*)new_p);\n-    }\n-  };\n-\n-  \/\/ Relocate a reference to point to its shallow copy\n-  class RefRelocator: public MetaspaceClosure {\n-  public:\n-    virtual bool do_ref(Ref* ref, bool read_only) {\n-      if (ref->not_null()) {\n-        ref->update(get_new_loc(ref));\n-        ArchivePtrMarker::mark_pointer(ref->addr());\n-      }\n-      return false; \/\/ Do not recurse.\n-    }\n-  };\n-\n-#ifdef ASSERT\n-  class IsRefInArchiveChecker: public MetaspaceClosure {\n-  public:\n-    virtual bool do_ref(Ref* ref, bool read_only) {\n-      if (ref->not_null()) {\n-        char* obj = (char*)ref->obj();\n-        assert(_ro_region.contains(obj) || _rw_region.contains(obj),\n-               \"must be relocated to point to CDS archive\");\n-      }\n-      return false; \/\/ Do not recurse.\n-    }\n-  };\n-#endif\n-\n+class StaticArchiveBuilder : public ArchiveBuilder {\n@@ -1416,66 +1043,2 @@\n-  static void copy_and_compact() {\n-    ResourceMark rm;\n-\n-    log_info(cds)(\"Scanning all metaspace objects ... \");\n-    {\n-      \/\/ allocate and shallow-copy RW objects, immediately following the MC region\n-      log_info(cds)(\"Allocating RW objects ... \");\n-      _mc_region.pack(&_rw_region);\n-\n-      ResourceMark rm;\n-      ShallowCopier rw_copier(false);\n-      iterate_roots(&rw_copier);\n-    }\n-    {\n-      \/\/ allocate and shallow-copy of RO object, immediately following the RW region\n-      log_info(cds)(\"Allocating RO objects ... \");\n-      _rw_region.pack(&_ro_region);\n-\n-      ResourceMark rm;\n-      ShallowCopier ro_copier(true);\n-      iterate_roots(&ro_copier);\n-    }\n-    {\n-      log_info(cds)(\"Relocating embedded pointers ... \");\n-      ResourceMark rm;\n-      ShallowCopyEmbeddedRefRelocator emb_reloc;\n-      iterate_roots(&emb_reloc);\n-    }\n-    {\n-      log_info(cds)(\"Relocating external roots ... \");\n-      ResourceMark rm;\n-      RefRelocator ext_reloc;\n-      iterate_roots(&ext_reloc);\n-    }\n-    {\n-      log_info(cds)(\"Fixing symbol identity hash ... \");\n-      os::init_random(0x12345678);\n-      GrowableArray<Symbol*>* all_symbols = MetaspaceShared::collected_symbols();\n-      all_symbols->sort(compare_symbols_by_address);\n-      for (int i = 0; i < all_symbols->length(); i++) {\n-        assert(all_symbols->at(i)->is_permanent(), \"archived symbols must be permanent\");\n-        all_symbols->at(i)->update_identity_hash();\n-      }\n-    }\n-#ifdef ASSERT\n-    {\n-      log_info(cds)(\"Verifying external roots ... \");\n-      ResourceMark rm;\n-      IsRefInArchiveChecker checker;\n-      iterate_roots(&checker);\n-    }\n-#endif\n-  }\n-\n-  \/\/ We must relocate the System::_well_known_klasses only after we have copied the\n-  \/\/ java objects in during dump_java_heap_objects(): during the object copy, we operate on\n-  \/\/ old objects which assert that their klass is the original klass.\n-  static void relocate_well_known_klasses() {\n-    {\n-      log_info(cds)(\"Relocating SystemDictionary::_well_known_klasses[] ... \");\n-      ResourceMark rm;\n-      RefRelocator ext_reloc;\n-      SystemDictionary::well_known_klasses_do(&ext_reloc);\n-    }\n-    \/\/ NOTE: after this point, we shouldn't have any globals that can reach the old\n-    \/\/ objects.\n+  StaticArchiveBuilder(DumpRegion* rw_region, DumpRegion* ro_region)\n+    : ArchiveBuilder(rw_region, ro_region) {}\n@@ -1483,31 +1046,1 @@\n-    \/\/ We cannot use any of the objects in the heap anymore (except for the\n-    \/\/ shared strings) because their headers no longer point to valid Klasses.\n-  }\n-\n-  static void iterate_roots(MetaspaceClosure* it) {\n-    \/\/ To ensure deterministic contents in the archive, we just need to ensure that\n-    \/\/ we iterate the MetsapceObjs in a deterministic order. It doesn't matter where\n-    \/\/ the MetsapceObjs are located originally, as they are copied sequentially into\n-    \/\/ the archive during the iteration.\n-    \/\/\n-    \/\/ The only issue here is that the symbol table and the system directories may be\n-    \/\/ randomly ordered, so we copy the symbols and klasses into two arrays and sort\n-    \/\/ them deterministically.\n-    \/\/\n-    \/\/ During -Xshare:dump, the order of Symbol creation is strictly determined by\n-    \/\/ the SharedClassListFile (class loading is done in a single thread and the JIT\n-    \/\/ is disabled). Also, Symbols are allocated in monotonically increasing addresses\n-    \/\/ (see Symbol::operator new(size_t, int)). So if we iterate the Symbols by\n-    \/\/ ascending address order, we ensure that all Symbols are copied into deterministic\n-    \/\/ locations in the archive.\n-    GrowableArray<Symbol*>* symbols = _global_symbol_objects;\n-    for (int i = 0; i < symbols->length(); i++) {\n-      it->push(symbols->adr_at(i));\n-    }\n-    if (_global_klass_objects != NULL) {\n-      \/\/ Need to fix up the pointers\n-      for (int i = 0; i < _global_klass_objects->length(); i++) {\n-        \/\/ NOTE -- this requires that the vtable is NOT yet patched, or else we are hosed.\n-        it->push(_global_klass_objects->adr_at(i));\n-      }\n-    }\n+  virtual void iterate_roots(MetaspaceClosure* it, bool is_relocating_pointers) {\n@@ -1519,11 +1052,0 @@\n-\n-    it->finish();\n-  }\n-\n-  static Klass* get_relocated_klass(Klass* orig_klass) {\n-    assert(DumpSharedSpaces, \"dump time only\");\n-    address* pp = _new_loc_table->lookup((address)orig_klass);\n-    assert(pp != NULL, \"must be\");\n-    Klass* klass = (Klass*)(*pp);\n-    assert(klass->is_klass(), \"must be\");\n-    return klass;\n@@ -1533,3 +1055,0 @@\n-DumpAllocStats* ArchiveCompactor::_alloc_stats;\n-ArchiveCompactor::RelocationTable* ArchiveCompactor::_new_loc_table;\n-\n@@ -1544,1 +1063,1 @@\n-  ArchiveCompactor::OtherROAllocMark mark;\n+  ArchiveBuilder::OtherROAllocMark mark;\n@@ -1568,21 +1087,0 @@\n-void VM_PopulateDumpSharedSpace::print_class_stats() {\n-  log_info(cds)(\"Number of classes %d\", _global_klass_objects->length());\n-  {\n-    int num_type_array = 0, num_obj_array = 0, num_inst = 0;\n-    for (int i = 0; i < _global_klass_objects->length(); i++) {\n-      Klass* k = _global_klass_objects->at(i);\n-      if (k->is_instance_klass()) {\n-        num_inst ++;\n-      } else if (k->is_objArray_klass()) {\n-        num_obj_array ++;\n-      } else {\n-        assert(k->is_typeArray_klass(), \"sanity\");\n-        num_type_array ++;\n-      }\n-    }\n-    log_info(cds)(\"    instance classes   = %5d\", num_inst);\n-    log_info(cds)(\"    obj array classes  = %5d\", num_obj_array);\n-    log_info(cds)(\"    type array classes = %5d\", num_type_array);\n-  }\n-}\n-\n@@ -1639,1 +1137,1 @@\n-  \/\/ (2) ArchiveCompactor needs to work with a stable set of MetaspaceObjs.\n+  \/\/ (2) ArchiveBuilder needs to work with a stable set of MetaspaceObjs.\n@@ -1661,5 +1159,3 @@\n-  _global_klass_objects = new GrowableArray<Klass*>(1000);\n-  CollectClassesClosure collect_classes;\n-  ClassLoaderDataGraph::loaded_classes_do(&collect_classes);\n-  _global_klass_objects->sort(global_klass_compare);\n-  print_class_stats();\n+  StaticArchiveBuilder builder(&_rw_region, &_ro_region);\n+  builder.gather_klasses_and_symbols();\n+  _global_klass_objects = builder.klasses();\n@@ -1678,0 +1174,2 @@\n+  builder.gather_source_objs();\n+\n@@ -1682,2 +1180,5 @@\n-  ArchiveCompactor::initialize();\n-  ArchiveCompactor::copy_and_compact();\n+  _mc_region.pack(&_rw_region);\n+  builder.dump_rw_region();\n+  _rw_region.pack(&_ro_region);\n+  builder.dump_ro_region();\n+  builder.relocate_pointers();\n@@ -1692,1 +1193,1 @@\n-  ArchiveCompactor::relocate_well_known_klasses();\n+  builder.relocate_well_known_klasses();\n@@ -1735,2 +1236,1 @@\n-    ArchiveCompactor::alloc_stats()->print_stats(int(_ro_region.used()), int(_rw_region.used()),\n-                                                 int(_mc_region.used()));\n+    builder.print_stats(int(_ro_region.used()), int(_rw_region.used()), int(_mc_region.used()));\n@@ -1822,1 +1322,1 @@\n-  Klass* k = ArchiveCompactor::get_relocated_klass(o->klass());\n+  Klass* k = ArchiveBuilder::get_relocated_klass(o->klass());\n@@ -1828,1 +1328,1 @@\n-  k = ArchiveCompactor::get_relocated_klass(k);\n+  k = ArchiveBuilder::get_relocated_klass(k);\n@@ -2035,1 +1535,1 @@\n-  ArchiveCompactor::OtherROAllocMark mark;\n+  ArchiveBuilder::OtherROAllocMark mark;\n","filename":"src\/hotspot\/share\/memory\/metaspaceShared.cpp","additions":25,"deletions":525,"binary":false,"changes":550,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-#include \"code\/dependencies.hpp\"\n@@ -43,1 +42,0 @@\n-#include \"interpreter\/interpreter.hpp\"\n@@ -47,1 +45,0 @@\n-#include \"memory\/filemap.hpp\"\n@@ -56,2 +53,0 @@\n-#include \"oops\/constantPool.hpp\"\n-#include \"oops\/instanceClassLoaderKlass.hpp\"\n@@ -60,1 +55,0 @@\n-#include \"oops\/instanceRefKlass.hpp\"\n@@ -68,1 +62,0 @@\n-#include \"runtime\/deoptimization.hpp\"\n@@ -73,3 +66,0 @@\n-#include \"runtime\/javaCalls.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-#include \"runtime\/synchronizer.hpp\"\n@@ -78,1 +68,0 @@\n-#include \"runtime\/vmOperations.hpp\"\n@@ -82,3 +71,0 @@\n-#include \"utilities\/copy.hpp\"\n-#include \"utilities\/events.hpp\"\n-#include \"utilities\/hashtable.inline.hpp\"\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2037,1 +2037,1 @@\n-long GenerateOopMap::_total_byte_count = 0;\n+uint64_t GenerateOopMap::_total_byte_count = 0;\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -325,1 +325,1 @@\n-  static long         _total_byte_count;    \/\/ Holds cumulative number of bytes inspected\n+  static uint64_t     _total_byte_count;    \/\/ Holds cumulative number of bytes inspected\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1620,2 +1620,2 @@\n-int  klassItable::_total_classes;   \/\/ Total no. of classes with itables\n-long klassItable::_total_size;      \/\/ Total no. of bytes used for itables\n+int    klassItable::_total_classes;   \/\/ Total no. of classes with itables\n+size_t klassItable::_total_size;      \/\/ Total no. of bytes used for itables\n@@ -1626,1 +1626,2 @@\n- tty->print_cr(\"%6lu K uses for itables (average by class: %ld bytes)\", _total_size \/ K, _total_size \/ _total_classes);\n+ tty->print_cr(SIZE_FORMAT_W(6) \" K uses for itables (average by class: \" SIZE_FORMAT \" bytes)\",\n+               _total_size \/ K, _total_size \/ _total_classes);\n","filename":"src\/hotspot\/share\/oops\/klassVtable.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -326,1 +326,1 @@\n-  NOT_PRODUCT(static long _total_size;)      \/\/ Total no. of bytes used for itables\n+  NOT_PRODUCT(static size_t _total_size;)    \/\/ Total no. of bytes used for itables\n","filename":"src\/hotspot\/share\/oops\/klassVtable.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -103,0 +103,4 @@\n+\/\/\n+\/\/  - INFLATING() is a distinguished markword value that is used when\n+\/\/    inflating an existing stack-lock into an ObjectMonitor. See below\n+\/\/    for is_being_inflated() and INFLATING().\n@@ -261,1 +265,1 @@\n-  \/\/ an existing stacklock.  0 indicates the markword is \"BUSY\".\n+  \/\/ an existing stack-lock.  0 indicates the markword is \"BUSY\".\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -52,0 +52,4 @@\n+RegMask Matcher::caller_save_regmask;\n+RegMask Matcher::caller_save_regmask_exclude_soe;\n+RegMask Matcher::mh_caller_save_regmask;\n+RegMask Matcher::mh_caller_save_regmask_exclude_soe;\n@@ -640,2 +644,2 @@\n-  \/\/ Make up debug masks.  Any spill slot plus callee-save registers.\n-  \/\/ Caller-save registers are assumed to be trashable by the various\n+  \/\/ Make up debug masks.  Any spill slot plus callee-save (SOE) registers.\n+  \/\/ Caller-save (SOC, AS) registers are assumed to be trashable by the various\n@@ -643,13 +647,13 @@\n-  *idealreg2debugmask  [Op_RegN]= *idealreg2spillmask[Op_RegN];\n-  *idealreg2debugmask  [Op_RegI]= *idealreg2spillmask[Op_RegI];\n-  *idealreg2debugmask  [Op_RegL]= *idealreg2spillmask[Op_RegL];\n-  *idealreg2debugmask  [Op_RegF]= *idealreg2spillmask[Op_RegF];\n-  *idealreg2debugmask  [Op_RegD]= *idealreg2spillmask[Op_RegD];\n-  *idealreg2debugmask  [Op_RegP]= *idealreg2spillmask[Op_RegP];\n-\n-  *idealreg2mhdebugmask[Op_RegN]= *idealreg2spillmask[Op_RegN];\n-  *idealreg2mhdebugmask[Op_RegI]= *idealreg2spillmask[Op_RegI];\n-  *idealreg2mhdebugmask[Op_RegL]= *idealreg2spillmask[Op_RegL];\n-  *idealreg2mhdebugmask[Op_RegF]= *idealreg2spillmask[Op_RegF];\n-  *idealreg2mhdebugmask[Op_RegD]= *idealreg2spillmask[Op_RegD];\n-  *idealreg2mhdebugmask[Op_RegP]= *idealreg2spillmask[Op_RegP];\n+  *idealreg2debugmask  [Op_RegN] = *idealreg2spillmask[Op_RegN];\n+  *idealreg2debugmask  [Op_RegI] = *idealreg2spillmask[Op_RegI];\n+  *idealreg2debugmask  [Op_RegL] = *idealreg2spillmask[Op_RegL];\n+  *idealreg2debugmask  [Op_RegF] = *idealreg2spillmask[Op_RegF];\n+  *idealreg2debugmask  [Op_RegD] = *idealreg2spillmask[Op_RegD];\n+  *idealreg2debugmask  [Op_RegP] = *idealreg2spillmask[Op_RegP];\n+\n+  *idealreg2mhdebugmask[Op_RegN] = *idealreg2spillmask[Op_RegN];\n+  *idealreg2mhdebugmask[Op_RegI] = *idealreg2spillmask[Op_RegI];\n+  *idealreg2mhdebugmask[Op_RegL] = *idealreg2spillmask[Op_RegL];\n+  *idealreg2mhdebugmask[Op_RegF] = *idealreg2spillmask[Op_RegF];\n+  *idealreg2mhdebugmask[Op_RegD] = *idealreg2spillmask[Op_RegD];\n+  *idealreg2mhdebugmask[Op_RegP] = *idealreg2spillmask[Op_RegP];\n@@ -658,1 +662,1 @@\n-  \/\/ callee-saved registers from debug info\n+  \/\/ callee-saved (SOE) registers from debug info\n@@ -660,31 +664,16 @@\n-\n-  for( i=OptoReg::Name(0); i<OptoReg::Name(_last_Mach_Reg); i = OptoReg::add(i,1) ) {\n-    \/\/ registers the caller has to save do not work\n-    if( _register_save_policy[i] == 'C' ||\n-        _register_save_policy[i] == 'A' ||\n-        (_register_save_policy[i] == 'E' && exclude_soe) ) {\n-      idealreg2debugmask  [Op_RegN]->Remove(i);\n-      idealreg2debugmask  [Op_RegI]->Remove(i); \/\/ Exclude save-on-call\n-      idealreg2debugmask  [Op_RegL]->Remove(i); \/\/ registers from debug\n-      idealreg2debugmask  [Op_RegF]->Remove(i); \/\/ masks\n-      idealreg2debugmask  [Op_RegD]->Remove(i);\n-      idealreg2debugmask  [Op_RegP]->Remove(i);\n-\n-      idealreg2mhdebugmask[Op_RegN]->Remove(i);\n-      idealreg2mhdebugmask[Op_RegI]->Remove(i);\n-      idealreg2mhdebugmask[Op_RegL]->Remove(i);\n-      idealreg2mhdebugmask[Op_RegF]->Remove(i);\n-      idealreg2mhdebugmask[Op_RegD]->Remove(i);\n-      idealreg2mhdebugmask[Op_RegP]->Remove(i);\n-    }\n-  }\n-\n-  \/\/ Subtract the register we use to save the SP for MethodHandle\n-  \/\/ invokes to from the debug mask.\n-  const RegMask save_mask = method_handle_invoke_SP_save_mask();\n-  idealreg2mhdebugmask[Op_RegN]->SUBTRACT(save_mask);\n-  idealreg2mhdebugmask[Op_RegI]->SUBTRACT(save_mask);\n-  idealreg2mhdebugmask[Op_RegL]->SUBTRACT(save_mask);\n-  idealreg2mhdebugmask[Op_RegF]->SUBTRACT(save_mask);\n-  idealreg2mhdebugmask[Op_RegD]->SUBTRACT(save_mask);\n-  idealreg2mhdebugmask[Op_RegP]->SUBTRACT(save_mask);\n+  RegMask* caller_save_mask = exclude_soe ? &caller_save_regmask_exclude_soe : &caller_save_regmask;\n+  RegMask* mh_caller_save_mask = exclude_soe ? &mh_caller_save_regmask_exclude_soe : &mh_caller_save_regmask;\n+\n+  idealreg2debugmask[Op_RegN]->SUBTRACT(*caller_save_mask);\n+  idealreg2debugmask[Op_RegI]->SUBTRACT(*caller_save_mask);\n+  idealreg2debugmask[Op_RegL]->SUBTRACT(*caller_save_mask);\n+  idealreg2debugmask[Op_RegF]->SUBTRACT(*caller_save_mask);\n+  idealreg2debugmask[Op_RegD]->SUBTRACT(*caller_save_mask);\n+  idealreg2debugmask[Op_RegP]->SUBTRACT(*caller_save_mask);\n+\n+  idealreg2mhdebugmask[Op_RegN]->SUBTRACT(*mh_caller_save_mask);\n+  idealreg2mhdebugmask[Op_RegI]->SUBTRACT(*mh_caller_save_mask);\n+  idealreg2mhdebugmask[Op_RegL]->SUBTRACT(*mh_caller_save_mask);\n+  idealreg2mhdebugmask[Op_RegF]->SUBTRACT(*mh_caller_save_mask);\n+  idealreg2mhdebugmask[Op_RegD]->SUBTRACT(*mh_caller_save_mask);\n+  idealreg2mhdebugmask[Op_RegP]->SUBTRACT(*mh_caller_save_mask);\n@@ -904,2 +893,2 @@\n-  \/\/ Copy the register names over into the shared world\n-  for( i=OptoReg::Name(0); i<OptoReg::Name(_last_Mach_Reg); i = OptoReg::add(i,1) ) {\n+  for (i = OptoReg::Name(0); i < OptoReg::Name(_last_Mach_Reg); i = OptoReg::add(i, 1)) {\n+    \/\/ Copy the register names over into the shared world.\n@@ -909,0 +898,14 @@\n+\n+    \/\/ Set up regmasks used to exclude save-on-call (and always-save) registers from debug masks.\n+    if (_register_save_policy[i] == 'C' ||\n+        _register_save_policy[i] == 'A') {\n+      caller_save_regmask.Insert(i);\n+      mh_caller_save_regmask.Insert(i);\n+    }\n+    \/\/ Exclude save-on-entry registers from debug masks for stub compilations.\n+    if (_register_save_policy[i] == 'C' ||\n+        _register_save_policy[i] == 'A' ||\n+        _register_save_policy[i] == 'E') {\n+      caller_save_regmask_exclude_soe.Insert(i);\n+      mh_caller_save_regmask_exclude_soe.Insert(i);\n+    }\n@@ -911,0 +914,6 @@\n+  \/\/ Also exclude the register we use to save the SP for MethodHandle\n+  \/\/ invokes to from the corresponding MH debug masks\n+  const RegMask sp_save_mask = method_handle_invoke_SP_save_mask();\n+  mh_caller_save_regmask.OR(sp_save_mask);\n+  mh_caller_save_regmask_exclude_soe.OR(sp_save_mask);\n+\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":58,"deletions":49,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -181,0 +181,4 @@\n+  static RegMask caller_save_regmask;\n+  static RegMask caller_save_regmask_exclude_soe;\n+  static RegMask mh_caller_save_regmask;\n+  static RegMask mh_caller_save_regmask_exclude_soe;\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -536,0 +536,32 @@\n+  ArrayCopyNode* ac = find_array_copy_clone(phase, ld_alloc, mem);\n+  if (ac != NULL) {\n+    return ac;\n+  } else if (mem->is_Proj() && mem->in(0) != NULL && mem->in(0)->is_ArrayCopy()) {\n+    ArrayCopyNode* ac = mem->in(0)->as_ArrayCopy();\n+\n+    if (ac->is_arraycopy_validated() ||\n+        ac->is_copyof_validated() ||\n+        ac->is_copyofrange_validated()) {\n+      Node* ld_addp = in(MemNode::Address);\n+      if (ld_addp->is_AddP()) {\n+        Node* ld_base = ld_addp->in(AddPNode::Address);\n+        Node* ld_offs = ld_addp->in(AddPNode::Offset);\n+\n+        Node* dest = ac->in(ArrayCopyNode::Dest);\n+\n+        if (dest == ld_base) {\n+          const TypeX *ld_offs_t = phase->type(ld_offs)->isa_intptr_t();\n+          if (ac->modifies(ld_offs_t->_lo, ld_offs_t->_hi, phase, can_see_stored_value)) {\n+            return ac;\n+          }\n+          if (!can_see_stored_value) {\n+            mem = ac->in(TypeFunc::Memory);\n+          }\n+        }\n+      }\n+    }\n+  }\n+  return NULL;\n+}\n+\n+ArrayCopyNode* MemNode::find_array_copy_clone(PhaseTransform* phase, Node* ld_alloc, Node* mem) const {\n@@ -537,1 +569,1 @@\n-                                               mem->in(0)->Opcode() == Op_MemBarCPUOrder)) {\n+                                                 mem->in(0)->Opcode() == Op_MemBarCPUOrder)) {\n@@ -562,24 +594,0 @@\n-  } else if (mem->is_Proj() && mem->in(0) != NULL && mem->in(0)->is_ArrayCopy()) {\n-    ArrayCopyNode* ac = mem->in(0)->as_ArrayCopy();\n-\n-    if (ac->is_arraycopy_validated() ||\n-        ac->is_copyof_validated() ||\n-        ac->is_copyofrange_validated()) {\n-      Node* ld_addp = in(MemNode::Address);\n-      if (ld_addp->is_AddP()) {\n-        Node* ld_base = ld_addp->in(AddPNode::Address);\n-        Node* ld_offs = ld_addp->in(AddPNode::Offset);\n-\n-        Node* dest = ac->in(ArrayCopyNode::Dest);\n-\n-        if (dest == ld_base) {\n-          const TypeX *ld_offs_t = phase->type(ld_offs)->isa_intptr_t();\n-          if (ac->modifies(ld_offs_t->_lo, ld_offs_t->_hi, phase, can_see_stored_value)) {\n-            return ac;\n-          }\n-          if (!can_see_stored_value) {\n-            mem = ac->in(TypeFunc::Memory);\n-          }\n-        }\n-      }\n-    }\n@@ -1120,1 +1128,6 @@\n-      return phase->zerocon(memory_type());\n+      if (ReduceBulkZeroing || find_array_copy_clone(phase, ld_alloc, in(MemNode::Memory)) == NULL) {\n+        \/\/ If ReduceBulkZeroing is disabled, we need to check if the allocation does not belong to an\n+        \/\/ ArrayCopyNode clone. If it does, then we cannot assume zero since the initialization is done\n+        \/\/ by the ArrayCopyNode.\n+        return phase->zerocon(memory_type());\n+      }\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":39,"deletions":26,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -96,0 +96,1 @@\n+  ArrayCopyNode* find_array_copy_clone(PhaseTransform* phase, Node* ld_alloc, Node* mem) const;\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -941,1 +941,2 @@\n-      table[3*j+2] = profile == NULL ? 1 : profile->count_at(j);\n+      \/\/ Handle overflow when converting from uint to jint\n+      table[3*j+2] = (profile == NULL) ? 1 : MIN2<uint>(max_jint, profile->count_at(j));\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -525,1 +525,0 @@\n-  { \"ForceNUMA\",                    JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },\n@@ -555,0 +554,2 @@\n+  { \"UseSemaphoreGCThreadsSynchronization\", JDK_Version::undefined(), JDK_Version::jdk(16), JDK_Version::jdk(17) },\n+  { \"ForceNUMA\",                     JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -194,3 +194,0 @@\n-  product(bool, ForceNUMA, false,                                           \\\n-          \"(Deprecated) Force NUMA optimizations on single-node\/UMA systems\") \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -591,1 +591,1 @@\n-  WorkGang* cleanup_workers = heap->get_safepoint_workers();\n+  WorkGang* cleanup_workers = heap->safepoint_workers();\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1041,5 +1041,5 @@\n-      \/\/ Relaxing assertion for bug 6320749.\n-      assert(Universe::verify_in_progress() ||\n-             !SafepointSynchronize::is_at_safepoint(),\n-             \"biases should not be seen by VM thread here\");\n-      BiasedLocking::revoke(hobj, JavaThread::current());\n+      if (SafepointSynchronize::is_at_safepoint()) {\n+        BiasedLocking::revoke_at_safepoint(hobj);\n+      } else {\n+        BiasedLocking::revoke(hobj, self);\n+      }\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1792,1 +1792,1 @@\n-  WorkGang* gang = ch->get_safepoint_workers();\n+  WorkGang* gang = ch->safepoint_workers();\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -49,0 +49,4 @@\n+#ifndef ATTRIBUTE_FLATTEN\n+#define ATTRIBUTE_FLATTEN\n+#endif\n+\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -716,1 +716,1 @@\n-     * @return the kind of this tree.\n+     * @return the kind of this tree\n@@ -724,2 +724,2 @@\n-     * @param <R> result type of this operation.\n-     * @param <D> type of additional data.\n+     * @param <R> the result type of this operation\n+     * @param <D> the type of additional data\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/tree\/Tree.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -57,1 +57,1 @@\n-     * @param defaultValue the default value to be returned by the default action.\n+     * @param defaultValue the default value to be returned by the default action\n@@ -88,1 +88,1 @@\n-     *      if none were called.\n+     *      if none were called\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/util\/SimpleTreeVisitor.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -967,1 +967,1 @@\n-     * {@inheritDoc} This implementation returns {@code null}.\n+     * {@inheritDoc} This implementation scans the children in left to right order.\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/util\/TreeScanner.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -213,0 +213,5 @@\n+        \/**\n+          * Warn about compiler generation of a default constructor.\n+          *\/\n+        MISSING_EXPLICIT_CTOR(\"missing-explicit-ctor\"),\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Lint.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+import javax.lang.model.element.NestingKind;\n@@ -4062,0 +4063,53 @@\n+    \/**\n+     * Check for a default constructor in an exported package.\n+     *\/\n+    void checkDefaultConstructor(ClassSymbol c, DiagnosticPosition pos) {\n+        if (lint.isEnabled(LintCategory.MISSING_EXPLICIT_CTOR) &&\n+            ((c.flags() & (ENUM | RECORD)) == 0) &&\n+            !c.isAnonymous() &&\n+            ((c.flags() & PUBLIC) != 0) &&\n+            Feature.MODULES.allowedInSource(source)) {\n+            NestingKind nestingKind = c.getNestingKind();\n+            switch (nestingKind) {\n+                case ANONYMOUS,\n+                     LOCAL -> {return;}\n+                case TOP_LEVEL -> {;} \/\/ No additional checks needed\n+                case MEMBER -> {\n+                    \/\/ For nested member classes, all the enclosing\n+                    \/\/ classes must be public.\n+                    Symbol owner = c.owner;\n+                    while (owner != null && owner.kind == TYP) {\n+                        if ((owner.flags() & PUBLIC) == 0)\n+                            return;\n+                        owner = owner.owner;\n+                    }\n+                }\n+            }\n+\n+            \/\/ Only check classes in named packages exported by its module\n+            PackageSymbol pkg = c.packge();\n+            if (!pkg.isUnnamed()) {\n+                ModuleSymbol modle = pkg.modle;\n+                for (ExportsDirective exportDir : modle.exports) {\n+                    \/\/ Report warning only if the containing\n+                    \/\/ package is unconditionally exported\n+                    if (exportDir.packge.equals(pkg)) {\n+                        if (exportDir.modules == null || exportDir.modules.isEmpty()) {\n+                            \/\/ Warning may be suppressed by\n+                            \/\/ annotations; check again for being\n+                            \/\/ enabled in the deferred context.\n+                            deferredLintHandler.report(() -> {\n+                                if (lint.isEnabled(LintCategory.MISSING_EXPLICIT_CTOR))\n+                                   log.warning(LintCategory.MISSING_EXPLICIT_CTOR,\n+                                               pos, Warnings.MissingExplicitCtor(c, pkg, modle));\n+                                                       });\n+                        } else {\n+                            return;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return;\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Check.java","additions":54,"deletions":0,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -1014,0 +1014,1 @@\n+                chk.checkDefaultConstructor(sym, tree.pos());\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/TypeEnter.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1767,0 +1767,4 @@\n+# 0: symbol, 1: symbol, 2: symbol\n+compiler.warn.missing-explicit-ctor=\\\n+    class {0} in exported package {1} declares no explicit constructors, thereby exposing a default constructor to clients of module {2}\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -185,0 +185,3 @@\n+javac.opt.Xlint.desc.missing-explicit-ctor=\\\n+    Warn about missing explicit constructors in public classes in exported packages.\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/javac.properties","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-import com.sun.tools.javac.util.List;\n+\n@@ -169,1 +169,1 @@\n-    \/**************************************************************************\n+    \/* ************************************************************************\n@@ -173,1 +173,1 @@\n-    \/** Exception to propagate IOException through visitXXX methods *\/\n+    \/** Exception to propagate IOException through visitXYZ methods *\/\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/Pretty.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -878,1 +878,1 @@\n-     * delcaration to which the member belongs to is not generic.\n+     * declaration to which the member belongs to is not generic.\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/toolkit\/util\/Utils.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -45,1 +45,0 @@\n-compiler\/codecache\/stress\/OverloadCompileQueueTest.java 8166554 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"}]}