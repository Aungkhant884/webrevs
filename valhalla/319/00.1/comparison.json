{"files":[{"patch":"@@ -1403,0 +1403,1 @@\n+        args = concat(args, \"--with-version-pre=\" + version_numbers.get(\"DEFAULT_PROMOTED_VERSION_PRE\"));\n","filename":"make\/conf\/jib-profiles.js","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1901,0 +1901,2 @@\n+  __ verified_entry(C, 0);\n+  __ bind(*_verified_entry);\n@@ -1949,6 +1951,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -2011,5 +2007,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {\n-  \/\/ Variable size. Determine dynamically.\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2298,1 +2289,23 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n+\n+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+{\n+  MacroAssembler _masm(&cbuf);\n+\n+  if (!_verified) {\n+    Label skip;\n+    __ cmp_klass(j_rarg0, rscratch2, rscratch1);\n+    __ br(Assembler::EQ, skip);\n+      __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+    __ bind(skip);\n@@ -2300,0 +2313,11 @@\n+  } else {\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    __ b(*_verified_entry);\n+  }\n+}\n+\n+\/\/=============================================================================\n@@ -2321,0 +2345,1 @@\n+  Label skip;\n@@ -2322,0 +2347,1 @@\n+  \/\/ UseCompressedClassPointers logic are inside cmp_klass\n@@ -2323,1 +2349,1 @@\n-  Label skip;\n+\n@@ -2331,5 +2357,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2766,1 +2787,0 @@\n-\n@@ -8709,0 +8729,15 @@\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -14684,1 +14719,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n@@ -14686,1 +14721,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":56,"deletions":21,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -105,0 +106,6 @@\n+void LIRGenerator::init_temps_for_substitutability_check(LIR_Opr& tmp1, LIR_Opr& tmp2) {\n+  tmp1 = new_register(T_INT);\n+  tmp2 = LIR_OprFact::illegalOpr;\n+}\n+\n+\n@@ -336,1 +343,1 @@\n-  if (UseBiasedLocking) {\n+  if (UseBiasedLocking || x->maybe_inlinetype()) {\n@@ -344,0 +351,6 @@\n+\n+  CodeStub* throw_imse_stub =\n+      x->maybe_inlinetype() ?\n+      new SimpleExceptionStub(Runtime1::throw_illegal_monitor_state_exception_id, LIR_OprFact::illegalOpr, state_for(x)) :\n+      NULL;\n+\n@@ -348,1 +361,1 @@\n-                        x->monitor_no(), info_for_exception, info);\n+                        x->monitor_no(), info_for_exception, info, throw_imse_stub);\n@@ -1156,0 +1169,16 @@\n+void LIRGenerator::do_NewInlineTypeInstance(NewInlineTypeInstance* x) {\n+  \/\/ Mapping to do_NewInstance (same code)\n+  CodeEmitInfo* info = state_for(x, x->state());\n+  x->set_to_object_type();\n+  LIR_Opr reg = result_register_for(x->type());\n+  new_instance(reg, x->klass(), x->is_unresolved(),\n+             FrameMap::r2_oop_opr,\n+             FrameMap::r5_oop_opr,\n+             FrameMap::r4_oop_opr,\n+             LIR_OprFact::illegalOpr,\n+             FrameMap::r3_metadata_opr, info);\n+  LIR_Opr result = rlock_result(x);\n+  __ move(reg, result);\n+\n+}\n+\n@@ -1201,2 +1230,2 @@\n-  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info);\n-  ciKlass* obj = (ciKlass*) ciObjArrayKlass::make(x->klass());\n+  ciKlass* obj = (ciKlass*) x->exact_type();\n+  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info, x->is_null_free());\n@@ -1206,0 +1235,1 @@\n+\n@@ -1207,1 +1237,5 @@\n-  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path);\n+  if (x->is_null_free()) {\n+    __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_INLINE_TYPE, klass_reg, slow_path);\n+  } else {\n+    __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path);\n+  }\n@@ -1283,0 +1317,3 @@\n+  if (x->is_null_free()) {\n+    __ null_check(obj.result(), new CodeEmitInfo(info_for_exception));\n+  }\n@@ -1301,0 +1338,2 @@\n+\n+\n@@ -1304,1 +1343,2 @@\n-               x->profiled_method(), x->profiled_bci());\n+               x->profiled_method(), x->profiled_bci(), x->is_null_free());\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":46,"deletions":6,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -31,0 +31,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -94,0 +96,6 @@\n+\n+  if (EnableValhalla && !UseBiasedLocking) {\n+    \/\/ Mask always_locked bit such that we go to the slow path if object is an inline type\n+    andr(hdr, hdr, ~markWord::biased_lock_bit_in_place);\n+  }\n+\n@@ -341,1 +349,1 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes, bool needs_stack_repair, Label* verified_inline_entry_label) {\n@@ -346,0 +354,6 @@\n+\n+  guarantee(needs_stack_repair == false, \"Stack repair should not be true\");\n+  if (verified_inline_entry_label != NULL) {\n+    bind(*verified_inline_entry_label);\n+  }\n+\n@@ -353,1 +367,4 @@\n-void C1_MacroAssembler::remove_frame(int framesize) {\n+void C1_MacroAssembler::remove_frame(int framesize, bool needs_stack_repair) {\n+\n+  guarantee(needs_stack_repair == false, \"Stack repair should not be true\");\n+\n@@ -357,0 +374,11 @@\n+void C1_MacroAssembler::verified_inline_entry() {\n+  if (C1Breakpoint || VerifyFPU || !UseStackBanging) {\n+    \/\/ Verified Entry first instruction should be 5 bytes long for correct\n+    \/\/ patching by patch_verified_entry().\n+    \/\/\n+    \/\/ C1Breakpoint and VerifyFPU have one byte first instruction.\n+    \/\/ Also first instruction will be one byte \"push(rbp)\" if stack banging\n+    \/\/ code is not generated (see build_frame() above).\n+    \/\/ For all these cases generate long instruction first.\n+    nop();\n+  }\n@@ -358,5 +386,2 @@\n-void C1_MacroAssembler::verified_entry() {\n-  \/\/ If we have to make this method not-entrant we'll overwrite its\n-  \/\/ first instruction with a jump.  For this action to be legal we\n-  \/\/ must ensure that this first instruction is a B, BL, NOP, BKPT,\n-  \/\/ SVC, HVC, or SMC.  Make it a NOP.\n+  \/\/ build frame\n+  \/\/ verify_FPU(0, \"method_entry\");\n@@ -366,0 +391,79 @@\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  \/\/ This function required to support for InlineTypePassFieldsAsArgs\n+  if (C1Breakpoint || VerifyFPU || !UseStackBanging) {\n+    \/\/ Verified Entry first instruction should be 5 bytes long for correct\n+    \/\/ patching by patch_verified_entry().\n+    \/\/\n+    \/\/ C1Breakpoint and VerifyFPU have one byte first instruction.\n+    \/\/ Also first instruction will be one byte \"push(rbp)\" if stack banging\n+    \/\/ code is not generated (see build_frame() above).\n+    \/\/ For all these cases generate long instruction first.\n+    nop();\n+  }\n+\n+  nop();\n+  \/\/ verify_FPU(0, \"method_entry\");\n+\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+\n+  GrowableArray<SigEntry>* sig   = &ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? &ces->sig_cc_ro() : &ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  \/\/ Create a temp frame so we can call into runtime. It must be properly set up to accomodate GC.\n+  int sp_inc = (args_on_stack - args_on_stack_cc) * VMRegImpl::stack_slot_size;\n+  if (sp_inc > 0) {\n+    sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+    sub(sp, sp, sp_inc);\n+  } else {\n+    sp_inc = 0;\n+  }\n+\n+  sub(sp, sp, frame_size_in_bytes);\n+  if (sp_inc > 0) {\n+    int real_frame_size = frame_size_in_bytes +\n+           + wordSize  \/\/ pushed rbp\n+           + wordSize  \/\/ returned address pushed by the stack extension code\n+           + sp_inc;   \/\/ stack extension\n+    mov(rscratch1, real_frame_size);\n+    str(rscratch1, Address(sp, frame_size_in_bytes - wordSize));\n+  }\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  mov(r1, (intptr_t) ces->method());\n+  if (is_inline_ro_entry) {\n+    far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id)));\n+  } else {\n+    far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ Remove the temp frame\n+  add(sp, sp, frame_size_in_bytes);\n+\n+  int n = shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                              args_passed_cc, regs_cc,            \/\/ from\n+                              args_passed, args_on_stack, regs);  \/\/ to\n+  assert(sp_inc == n, \"must be\");\n+\n+  if (sp_inc != 0) {\n+    \/\/ Do the stack banging here, and skip over the stack repair code in the\n+    \/\/ verified_inline_entry (which has a different real_frame_size).\n+    assert(sp_inc > 0, \"stack should not shrink\");\n+    generate_stack_overflow_check(bang_size_in_bytes);\n+    decrement(sp, frame_size_in_bytes);\n+  }\n+\n+  b(verified_inline_entry_label);\n+  return rt_call_offset;\n+}\n+\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":111,"deletions":7,"binary":false,"changes":118,"status":"modified"},{"patch":"@@ -67,0 +67,3 @@\n+define_pd_global(bool, InlineTypePassFieldsAsArgs, false);\n+define_pd_global(bool, InlineTypeReturnedAsFields, false);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/globals_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -323,0 +324,1 @@\n+    case T_INLINE_TYPE:\n@@ -356,0 +358,84 @@\n+\n+\/\/ const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_int = 6;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt, VMRegPair *regs, int total_args_passed) {\n+\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  \/\/ r1, r2 used to address klasses and states, exclude it from return convention to avoid colision\n+\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+     r0 \/* j_rarg7 *\/, j_rarg6, j_rarg5, j_rarg4, j_rarg3, j_rarg2\n+  };\n+\n+  static const FloatRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3, j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        \/\/ Should we have gurantee here?\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+      \/\/ Should T_METADATA be added to java_calling_convention as well ?\n+    case T_METADATA:\n+    case T_INLINE_TYPE:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -389,19 +475,40 @@\n-static void gen_c2i_adapter(MacroAssembler *masm,\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n-                            const VMRegPair *regs,\n-                            Label& skip_fixup) {\n-  \/\/ Before we get into the guts of the C2I adapter, see if we should be here\n-  \/\/ at all.  We've come from compiled code and are attempting to jump to the\n-  \/\/ interpreter, which means the caller made a static call to get here\n-  \/\/ (vcalls always get a compiled target if there is one).  Check for a\n-  \/\/ compiled target.  If there is one, we need to patch the caller's call.\n-  patch_callers_callsite(masm);\n-\n-  __ bind(skip_fixup);\n-\n-  int words_pushed = 0;\n-\n-  \/\/ Since all args are passed on the stack, total_args_passed *\n-  \/\/ Interpreter::stackElementSize is the space we need.\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+     for (int i = 0; i < sig_extended->length(); i++) {\n+       BasicType bt = sig_extended->at(i)._bt;\n+       if (bt == T_INLINE_TYPE) {\n+         \/\/ In sig_extended, an inline type argument starts with:\n+         \/\/ T_INLINE_TYPE, followed by the types of the fields of the\n+         \/\/ inline type and T_VOID to mark the end of the value\n+         \/\/ type. Inline types are flattened so, for instance, in the\n+         \/\/ case of an inline type with an int field and an inline type\n+         \/\/ field that itself has 2 fields, an int and a long:\n+         \/\/ T_INLINE_TYPE T_INT T_INLINE_TYPE T_INT T_LONG T_VOID (second\n+         \/\/ slot for the T_LONG) T_VOID (inner T_INLINE_TYPE) T_VOID\n+         \/\/ (outer T_INLINE_TYPE)\n+         total_args_passed++;\n+         int vt = 1;\n+         do {\n+           i++;\n+           BasicType bt = sig_extended->at(i)._bt;\n+           BasicType prev_bt = sig_extended->at(i-1)._bt;\n+           if (bt == T_INLINE_TYPE) {\n+             vt++;\n+           } else if (bt == T_VOID &&\n+                      prev_bt != T_LONG &&\n+                      prev_bt != T_DOUBLE) {\n+             vt--;\n+           }\n+         } while (vt != 0);\n+       } else {\n+         total_args_passed++;\n+       }\n+     }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n@@ -409,1 +516,2 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+  return total_args_passed;\n+}\n@@ -411,3 +519,1 @@\n-  __ mov(r13, sp);\n-  \/\/ stack is aligned, keep it that way\n-  extraspace = align_up(extraspace, 2*wordSize);\n+static void gen_c2i_adapter_helper(MacroAssembler* masm, BasicType bt, const VMRegPair& reg_pair, int extraspace, const Address& to) {\n@@ -416,13 +522,1 @@\n-  if (extraspace)\n-    __ sub(sp, sp, extraspace);\n-\n-  \/\/ Now write the args into the outgoing interpreter space\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n+    assert(bt != T_INLINE_TYPE || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n@@ -443,2 +537,5 @@\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n+    \/\/ int next_off = st_off - Interpreter::stackElementSize;\n+\n+    VMReg r_1 = reg_pair.first();\n+    VMReg r_2 = reg_pair.second();\n+\n@@ -447,1 +544,1 @@\n-      continue;\n+      return;\n@@ -449,0 +546,1 @@\n+\n@@ -451,3 +549,2 @@\n-      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size\n-                    + extraspace\n-                    + words_pushed * wordSize);\n+      \/\/ words_pushed is always 0 so we don't use it.\n+      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace \/* + word_pushed * wordSize *\/);\n@@ -457,1 +554,1 @@\n-        __ str(rscratch1, Address(sp, st_off));\n+        __ str(rscratch1, to);\n@@ -460,16 +557,1 @@\n-\n-\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ str(rscratch1, Address(sp, next_off));\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaaaull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ str(rscratch1, Address(sp, st_off));\n-        }\n+        __ str(rscratch1, to);\n@@ -480,19 +562,1 @@\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ str(r, Address(sp, st_off));\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ jlong\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaabull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-          __ str(r, Address(sp, next_off));\n-        } else {\n-          __ str(r, Address(sp, st_off));\n-        }\n-      }\n+      __ str(r, to);\n@@ -503,1 +567,1 @@\n-        __ strs(r_1->as_FloatRegister(), Address(sp, st_off));\n+        __ strs(r_1->as_FloatRegister(), to);\n@@ -505,6 +569,1 @@\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaacull);\n-        __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        __ strd(r_1->as_FloatRegister(), Address(sp, next_off));\n+        __ strd(r_1->as_FloatRegister(), to);\n@@ -512,0 +571,153 @@\n+   }\n+}\n+\n+static void gen_c2i_adapter(MacroAssembler *masm,\n+                            const GrowableArray<SigEntry>* sig_extended,\n+                            const VMRegPair *regs,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+\n+  \/\/ Before we get into the guts of the C2I adapter, see if we should be here\n+  \/\/ at all.  We've come from compiled code and are attempting to jump to the\n+  \/\/ interpreter, which means the caller made a static call to get here\n+  \/\/ (vcalls always get a compiled target if there is one).  Check for a\n+  \/\/ compiled target.  If there is one, we need to patch the caller's call.\n+  patch_callers_callsite(masm);\n+\n+  __ bind(skip_fixup);\n+\n+  bool has_inline_argument = false;\n+\n+  if (InlineTypePassFieldsAsArgs) {\n+      \/\/ Is there an inline type argument?\n+     for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+       has_inline_argument = (sig_extended->at(i)._bt == T_INLINE_TYPE);\n+     }\n+     if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);\n+\n+      frame_complete = __ offset();\n+      address the_pc = __ pc();\n+\n+      __ set_last_Java_frame(noreg, noreg, the_pc, rscratch1);\n+\n+      __ mov(c_rarg0, rthread);\n+      __ mov(c_rarg1, r1);\n+      __ mov(c_rarg2, (int64_t)alloc_inline_receiver);\n+\n+      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+      __ blr(rscratch1);\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ cbz(r0, no_exception);\n+\n+      __ str(zr, Address(rthread, JavaThread::vm_result_offset()));\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ b(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(r10, rthread);\n+      __ get_vm_result_2(r1, rthread); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n+  int words_pushed = 0;\n+\n+  \/\/ Since all args are passed on the stack, total_args_passed *\n+  \/\/ Interpreter::stackElementSize is the space we need.\n+\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n+  int extraspace = (total_args_passed * Interpreter::stackElementSize) + wordSize;\n+\n+  \/\/ stack is aligned, keep it that way\n+  extraspace = align_up(extraspace, 2 * wordSize);\n+\n+  __ mov(r13, sp);\n+\n+  if (extraspace)\n+    __ sub(sp, sp, extraspace);\n+\n+  \/\/ Now write the args into the outgoing interpreter space\n+\n+  int ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+  bool has_oop_field = false;\n+\n+  for (int next_arg_comp = 0; next_arg_comp < total_args_passed; next_arg_comp++) {\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    \/\/ offset to start parameters\n+    int st_off   = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;\n+\n+    if (!InlineTypePassFieldsAsArgs || bt != T_INLINE_TYPE) {\n+      if (bt == T_VOID) {\n+         assert(next_arg_comp > 0 && (sig_extended->at(next_arg_comp - 1)._bt == T_LONG || sig_extended->at(next_arg_comp - 1)._bt == T_DOUBLE), \"missing half\");\n+         next_arg_int ++;\n+         continue;\n+       }\n+\n+       int next_off = st_off - Interpreter::stackElementSize;\n+       int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+\n+       gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp], extraspace, Address(sp, offset));\n+       next_arg_int ++;\n+   } else {\n+       ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_INLINE_TYPE);\n+      __ load_heap_oop(rscratch1, Address(r10, index));\n+      next_vt_arg++;\n+      next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of value\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp - 1)._bt;\n+        if (bt == T_INLINE_TYPE) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n+        } else {\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          assert(off > 0, \"offset in object should be positive\");\n+\n+          bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+          has_oop_field = has_oop_field || is_oop;\n+\n+          gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp - ignored], extraspace, Address(r11, off));\n+        }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ str(rscratch1, Address(sp, st_off));\n+   }\n+\n+  }\n+\n+\/\/ If an inline type was allocated and initialized, apply post barrier to all oop fields\n+  if (has_inline_argument && has_oop_field) {\n+    __ push(r13); \/\/ save senderSP\n+    __ push(r1); \/\/ save callee\n+    \/\/ Allocate argument register save area\n+    if (frame::arg_reg_save_area_bytes != 0) {\n+      __ sub(sp, sp, frame::arg_reg_save_area_bytes);\n@@ -513,0 +725,7 @@\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::apply_post_barriers), rthread, r10);\n+    \/\/ De-allocate argument register save area\n+    if (frame::arg_reg_save_area_bytes != 0) {\n+      __ add(sp, sp, frame::arg_reg_save_area_bytes);\n+    }\n+    __ pop(r1); \/\/ restore callee\n+    __ pop(r13); \/\/ restore sender SP\n@@ -521,0 +740,1 @@\n+void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm, int comp_args_on_stack, const GrowableArray<SigEntry>* sig, const VMRegPair *regs) {\n@@ -522,5 +742,0 @@\n-void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,\n-                                    int total_args_passed,\n-                                    int comp_args_on_stack,\n-                                    const BasicType *sig_bt,\n-                                    const VMRegPair *regs) {\n@@ -586,1 +801,1 @@\n-  int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)>>LogBytesPerWord;\n+  int comp_words_on_stack = 0;\n@@ -588,2 +803,3 @@\n-    __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n-    __ andr(sp, rscratch1, -16);\n+     comp_words_on_stack = align_up(comp_args_on_stack * VMRegImpl::stack_slot_size, wordSize) >> LogBytesPerWord;\n+     __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n+     __ andr(sp, rscratch1, -16);\n@@ -608,0 +824,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -610,2 +828,5 @@\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+    BasicType bt = sig->at(i)._bt;\n+\n+    assert(bt != T_INLINE_TYPE, \"i2c adapter doesn't unpack inline typ args\");\n+    if (bt == T_VOID) {\n+      assert(i > 0 && (sig->at(i - 1)._bt == T_LONG || sig->at(i - 1)._bt == T_DOUBLE), \"missing half\");\n@@ -616,0 +837,1 @@\n+    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(), \"scrambled load targets?\");\n@@ -617,3 +839,1 @@\n-    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),\n-            \"scrambled load targets?\");\n-    int ld_off = (total_args_passed - i - 1)*Interpreter::stackElementSize;\n+    int ld_off = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n@@ -634,1 +854,1 @@\n-      int st_off = regs[i].first()->reg2stack()*VMRegImpl::stack_slot_size;\n+      int st_off = regs[i].first()->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -651,2 +871,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n@@ -655,11 +874,28 @@\n-        __ str(rscratch2, Address(sp, st_off));\n-      }\n-    } else if (r_1->is_Register()) {  \/\/ Register argument\n-      Register r = r_1->as_Register();\n-      if (r_2->is_valid()) {\n-        \/\/\n-        \/\/ We are using two VMRegs. This can be either T_OBJECT,\n-        \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n-        \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n-        \/\/ So we must adjust where to pick up the data to match the\n-        \/\/ interpreter.\n+         __ str(rscratch2, Address(sp, st_off));\n+       }\n+     } else if (r_1->is_Register()) {  \/\/ Register argument\n+       Register r = r_1->as_Register();\n+       if (r_2->is_valid()) {\n+         \/\/\n+         \/\/ We are using two VMRegs. This can be either T_OBJECT,\n+         \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n+         \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n+         \/\/ So we must adjust where to pick up the data to match the\n+         \/\/ interpreter.\n+\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n+\n+         \/\/ this can be a misaligned move\n+         __ ldr(r, Address(esp, offset));\n+       } else {\n+         \/\/ sign extend and use a full word?\n+         __ ldrw(r, Address(esp, ld_off));\n+       }\n+     } else {\n+       if (!r_2->is_valid()) {\n+         __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n+       } else {\n+         __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n+       }\n+     }\n+   }\n@@ -667,17 +903,0 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n-\n-        \/\/ this can be a misaligned move\n-        __ ldr(r, Address(esp, offset));\n-      } else {\n-        \/\/ sign extend and use a full word?\n-        __ ldrw(r, Address(esp, ld_off));\n-      }\n-    } else {\n-      if (!r_2->is_valid()) {\n-        __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n-      } else {\n-        __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n-      }\n-    }\n-  }\n@@ -696,1 +915,0 @@\n-\n@@ -700,13 +918,1 @@\n-\/\/ ---------------------------------------------------------------\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                                            int total_args_passed,\n-                                                            int comp_args_on_stack,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n-  address i2c_entry = __ pc();\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n-\n-  address c2i_unverified_entry = __ pc();\n-  Label skip_fixup;\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n@@ -747,0 +953,34 @@\n+}\n+\n+\n+\/\/ ---------------------------------------------------------------\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n+                                                            int comp_args_on_stack,\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n+\n+  address i2c_entry = __ pc();\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n+\n+  address c2i_unverified_entry = __ pc();\n+  Label skip_fixup;\n+\n+  gen_inline_cache_check(masm, skip_fixup);\n+\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    Label unused;\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+    skip_fixup = unused;\n+  }\n@@ -748,0 +988,1 @@\n+  \/\/ Scalarized c2i adapter\n@@ -752,0 +993,1 @@\n+\n@@ -754,4 +996,4 @@\n-\n-      __ ldrw(rscratch1, Address(rmethod, Method::access_flags_offset()));\n-      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n-      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n+        Register flags  = rscratch1;\n+      __ ldrw(flags, Address(rmethod, Method::access_flags_offset()));\n+      __ tst(flags, JVM_ACC_STATIC);\n+      __ br(Assembler::NE, L_skip_barrier); \/\/ non-static\n@@ -761,3 +1003,6 @@\n-    __ load_method_holder(rscratch2, rmethod);\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    Register klass = rscratch1;\n+    __ load_method_holder(klass, rmethod);\n+    \/\/ We pass rthread to this function on x86\n+    __ clinit_barrier(klass, rscratch2, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n@@ -774,0 +1019,14 @@\n+  address c2i_unverified_inline_entry = c2i_unverified_entry;\n+\n+ \/\/ Non-scalarized c2i adapter\n+  address c2i_inline_entry = c2i_entry;\n+  if (regs != regs_cc) {\n+    Label inline_entry_skip_fixup;\n+    c2i_unverified_inline_entry = __ pc();\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    Label unused;\n+    gen_c2i_adapter(masm, sig, regs, inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+  }\n+\n@@ -775,1 +1034,8 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+\n+  \/\/ The c2i adapter might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+\n+  bool caller_must_gc_arguments = (regs != regs_cc);\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words + 10, oop_maps, caller_must_gc_arguments);\n+\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -818,0 +1084,1 @@\n+      case T_INLINE_TYPE:\n@@ -1632,0 +1899,1 @@\n+      case T_INLINE_TYPE:\n@@ -1819,0 +2087,1 @@\n+  case T_INLINE_TYPE:\n@@ -3059,0 +3328,105 @@\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler _masm(&buffer);\n+  MacroAssembler* masm = &_masm;\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ strs(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_DOUBLE) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(r0, val);\n+      \/\/ We don't need barriers because the destination is a newly allocated object.\n+      \/\/ Also, we cannot use store_heap_oop(to, val) because it uses r8 as tmp.\n+      if (UseCompressedOops) {\n+        __ encode_heap_oop(val);\n+        __ str(val, to);\n+      } else {\n+        __ str(val, to);\n+      }\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ store_sized_value(to, r_1->as_Register(), size_in_bytes);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(lr);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ ldrs(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ ldrd(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+       assert_different_registers(r0, r_1->as_Register());\n+       __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(lr);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, unpack_fields_off);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":525,"deletions":151,"binary":false,"changes":676,"status":"modified"},{"patch":"@@ -309,1 +309,1 @@\n-    \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+    \/\/ T_OBJECT, T_INLINE_TYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n@@ -313,1 +313,1 @@\n-    Label is_long, is_float, is_double, exit;\n+    Label is_long, is_float, is_double, is_value, exit;\n@@ -317,0 +317,2 @@\n+    __ cmp(j_rarg1, (u1)T_INLINE_TYPE);\n+    __ br(Assembler::EQ, is_value);\n@@ -371,0 +373,13 @@\n+    __ BIND(is_value);\n+    if (InlineTypeReturnedAsFields) {\n+      \/\/ Check for flattened return value\n+      __ cbz(r0, is_long);\n+      \/\/ Initialize pre-allocated buffer\n+      __ mov(r1, r0);\n+      __ andr(r1, r1, -2);\n+      __ ldr(r1, Address(r1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));\n+      __ ldr(r0, Address(j_rarg2, 0));\n+      __ blr(r1);\n+      __ b(exit);\n+    }\n@@ -1843,1 +1858,1 @@\n-    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, AS_RAW);  \/\/ store the oop\n+    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n@@ -6507,0 +6522,178 @@\n+  \/\/ Call here from the interpreter or compiled code to either load\n+  \/\/ multiple returned values from the inline type instance being\n+  \/\/ returned to registers or to store returned values to a newly\n+  \/\/ allocated inline type instance.\n+  address generate_return_value_stub(address destination, const char* name, bool has_res) {\n+\n+    \/\/ Information about frame layout at time of blocking runtime call.\n+    \/\/ Note that we only have to preserve callee-saved registers since\n+    \/\/ the compilers are responsible for supplying a continuation point\n+    \/\/ if they expect all registers to be preserved.\n+    \/\/ n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0\n+    enum layout {\n+      rfp_off = 0, rfp_off2,\n+\n+      j_rarg7_off, j_rarg7_2,\n+      j_rarg6_off, j_rarg6_2,\n+      j_rarg5_off, j_rarg5_2,\n+      j_rarg4_off, j_rarg4_2,\n+      j_rarg3_off, j_rarg3_2,\n+      j_rarg2_off, j_rarg2_2,\n+      j_rarg1_off, j_rarg1_2,\n+      j_rarg0_off, j_rarg0_2,\n+\n+      j_farg0_off, j_farg0_2,\n+      j_farg1_off, j_farg1_2,\n+      j_farg2_off, j_farg2_2,\n+      j_farg3_off, j_farg3_2,\n+      j_farg4_off, j_farg4_2,\n+      j_farg5_off, j_farg5_2,\n+      j_farg6_off, j_farg6_2,\n+      j_farg7_off, j_farg7_2,\n+\n+      return_off, return_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    int insts_size = 512;\n+    int locs_size  = 64;\n+\n+    CodeBuffer code(name, insts_size, locs_size);\n+    OopMapSet* oop_maps  = new OopMapSet();\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+\n+    address start = __ pc();\n+\n+    const Address f7_save       (rfp, j_farg7_off * wordSize);\n+    const Address f6_save       (rfp, j_farg6_off * wordSize);\n+    const Address f5_save       (rfp, j_farg5_off * wordSize);\n+    const Address f4_save       (rfp, j_farg4_off * wordSize);\n+    const Address f3_save       (rfp, j_farg3_off * wordSize);\n+    const Address f2_save       (rfp, j_farg2_off * wordSize);\n+    const Address f1_save       (rfp, j_farg1_off * wordSize);\n+    const Address f0_save       (rfp, j_farg0_off * wordSize);\n+\n+    const Address r0_save      (rfp, j_rarg0_off * wordSize);\n+    const Address r1_save      (rfp, j_rarg1_off * wordSize);\n+    const Address r2_save      (rfp, j_rarg2_off * wordSize);\n+    const Address r3_save      (rfp, j_rarg3_off * wordSize);\n+    const Address r4_save      (rfp, j_rarg4_off * wordSize);\n+    const Address r5_save      (rfp, j_rarg5_off * wordSize);\n+    const Address r6_save      (rfp, j_rarg6_off * wordSize);\n+    const Address r7_save      (rfp, j_rarg7_off * wordSize);\n+\n+    \/\/ Generate oop map\n+    OopMap* map = new OopMap(framesize, 0);\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(rfp_off), rfp->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg7_off), j_rarg7->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg6_off), j_rarg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+    \/\/ This is an inlined and slightly modified version of call_VM\n+    \/\/ which has the ability to fetch the return PC out of\n+    \/\/ thread-local storage and also sets up last_Java_sp slightly\n+    \/\/ differently than the real call_VM\n+\n+    __ enter(); \/\/ Save FP and LR before call\n+\n+    assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n+\n+    \/\/ lr and fp are already in place\n+    __ sub(sp, rfp, ((unsigned)framesize - 4) << LogBytesPerInt); \/\/ prolog\n+\n+    __ strd(j_farg7, f7_save);\n+    __ strd(j_farg6, f6_save);\n+    __ strd(j_farg5, f5_save);\n+    __ strd(j_farg4, f4_save);\n+    __ strd(j_farg3, f3_save);\n+    __ strd(j_farg2, f2_save);\n+    __ strd(j_farg1, f1_save);\n+    __ strd(j_farg0, f0_save);\n+\n+    __ str(j_rarg0, r0_save);\n+    __ str(j_rarg1, r1_save);\n+    __ str(j_rarg2, r2_save);\n+    __ str(j_rarg3, r3_save);\n+    __ str(j_rarg4, r4_save);\n+    __ str(j_rarg5, r5_save);\n+    __ str(j_rarg6, r6_save);\n+    __ str(j_rarg7, r7_save);\n+\n+    int frame_complete = __ pc() - start;\n+\n+    \/\/ Set up last_Java_sp and last_Java_fp\n+    address the_pc = __ pc();\n+    __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n+\n+    \/\/ Call runtime\n+    __ mov(c_rarg0, rthread);\n+    __ mov(c_rarg1, r0);\n+\n+    BLOCK_COMMENT(\"call runtime_entry\");\n+    __ mov(rscratch1, destination);\n+    __ blr(rscratch1);\n+\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    __ reset_last_Java_frame(false);\n+    __ maybe_isb();\n+\n+    __ ldrd(j_farg7, f7_save);\n+    __ ldrd(j_farg6, f6_save);\n+    __ ldrd(j_farg5, f5_save);\n+    __ ldrd(j_farg4, f4_save);\n+    __ ldrd(j_farg3, f3_save);\n+    __ ldrd(j_farg3, f2_save);\n+    __ ldrd(j_farg1, f1_save);\n+    __ ldrd(j_farg0, f0_save);\n+\n+    __ ldr(j_rarg0, r0_save);\n+    __ ldr(j_rarg1, r1_save);\n+    __ ldr(j_rarg2, r2_save);\n+    __ ldr(j_rarg3, r3_save);\n+    __ ldr(j_rarg4, r4_save);\n+    __ ldr(j_rarg5, r5_save);\n+    __ ldr(j_rarg6, r6_save);\n+    __ ldr(j_rarg7, r7_save);\n+\n+    __ leave();\n+\n+    \/\/ check for pending exceptions\n+    Label pending;\n+    __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ cmp(rscratch1, (u1)NULL_WORD);\n+    __ br(Assembler::NE, pending);\n+\n+    if (has_res) {\n+      __ get_vm_result(r0, rthread);\n+    }\n+    __ ret(lr);\n+\n+    __ bind(pending);\n+    __ ldr(r0, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+\n+    \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+    int frame_size_in_words = (framesize >> (LogBytesPerWord - LogBytesPerInt));\n+    RuntimeStub* stub =\n+      RuntimeStub::new_runtime_stub(name, &code, frame_complete, frame_size_in_words, oop_maps, false);\n+\n+    return stub->entry_point();\n+  }\n+\n@@ -6557,0 +6750,5 @@\n+    StubRoutines::_load_inline_type_fields_in_regs =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), \"store_inline_type_fields_to_buf\", true);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":201,"deletions":3,"binary":false,"changes":204,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -440,0 +441,5 @@\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(NULL, true);\n+  }\n+\n@@ -558,0 +564,1 @@\n+  case T_INLINE_TYPE: \/\/ fall through (value types are handled with oops)\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -147,1 +147,1 @@\n-  __ store_heap_oop(dst, val, r10, r1, decorators);\n+  __ store_heap_oop(dst, val, r10, r1, noreg, decorators);\n@@ -170,0 +170,1 @@\n+  case Bytecodes::_fast_qputfield:\n@@ -746,4 +747,4 @@\n-    \/\/ ??? convention: move array into r3 for exception message\n-  __ mov(r3, array);\n-  __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n-  __ br(rscratch1);\n+  \/\/ ??? convention: move array into r3 for exception message\n+   __ mov(r3, array);\n+   __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n+   __ br(rscratch1);\n@@ -809,5 +810,15 @@\n-  __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n-  do_oop_load(_masm,\n-              Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)),\n-              r0,\n-              IS_ARRAY);\n+  if (UseFlatArray) {\n+    Label is_flat_array, done;\n+\n+    __ test_flattened_array_oop(r0, r8 \/*temp*\/, is_flat_array);\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+\n+    __ b(done);\n+    __ bind(is_flat_array);\n+    __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_load), r0, r1);\n+    __ bind(done);\n+  } else {\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+  }\n@@ -1110,0 +1121,2 @@\n+\n+  \/\/ FIXME: Could we remove the line below?\n@@ -1115,0 +1128,5 @@\n+  Label  is_flat_array;\n+  if (UseFlatArray) {\n+    __ test_flattened_array_oop(r3, r8 \/*temp*\/, is_flat_array);\n+  }\n+\n@@ -1117,0 +1135,1 @@\n+\n@@ -1119,2 +1138,1 @@\n-  __ ldr(r0, Address(r0,\n-                     ObjArrayKlass::element_klass_offset()));\n+  __ ldr(r0, Address(r0, ObjArrayKlass::element_klass_offset()));\n@@ -1125,0 +1143,1 @@\n+\n@@ -1131,0 +1150,1 @@\n+\n@@ -1134,0 +1154,1 @@\n+\n@@ -1144,0 +1165,13 @@\n+  if (EnableValhalla) {\n+    Label is_null_into_value_array_npe, store_null;\n+\n+    \/\/ No way to store null in flat array\n+    __ test_null_free_array_oop(r3, r8, is_null_into_value_array_npe);\n+    __ b(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n+\n@@ -1146,0 +1180,40 @@\n+  __ b(done);\n+\n+  if (EnableValhalla) {\n+     Label is_type_ok;\n+\n+    \/\/ store non-null value\n+    __ bind(is_flat_array);\n+\n+    \/\/ Simplistic type check...\n+    \/\/ r0 - value, r2 - index, r3 - array.\n+\n+    \/\/ Profile the not-null value's klass.\n+    \/\/ Load value class\n+     __ load_klass(r1, r0);\n+     __ profile_typecheck(r2, r1, r0); \/\/ blows r2, and r0\n+\n+    \/\/ flat value array needs exact type match\n+    \/\/ is \"r8 == r0\" (value subclass == array element superclass)\n+\n+    \/\/ Move element klass into r0\n+\n+     __ load_klass(r0, r3);\n+\n+     __ ldr(r0, Address(r0, ArrayKlass::element_klass_offset()));\n+     __ cmp(r0, r1);\n+     __ br(Assembler::EQ, is_type_ok);\n+\n+     __ profile_typecheck_failed(r2);\n+     __ b(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));\n+\n+     __ bind(is_type_ok);\n+\n+    \/\/ Reload from TOS to be safe, because of profile_typecheck that blows r2 and r0.\n+    \/\/ FIXME: Should we really do it?\n+     __ ldr(r1, at_tos());  \/\/ value\n+     __ mov(r2, r3); \/\/ array, ldr(r2, at_tos_p2());\n+     __ ldr(r3, at_tos_p1()); \/\/ index\n+     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_store), r1, r2, r3);\n+  }\n+\n@@ -1956,2 +2030,1 @@\n-void TemplateTable::if_acmp(Condition cc)\n-{\n+void TemplateTable::if_acmp(Condition cc) {\n@@ -1960,1 +2033,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -1962,0 +2035,36 @@\n+\n+  Register is_value_mask = rscratch1;\n+  __ mov(is_value_mask, markWord::always_locked_pattern);\n+\n+  if (EnableValhalla) {\n+    __ cmp(r1, r0);\n+    __ br(Assembler::EQ, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either r0 or r1 is null\n+    __ andr(r2, r0, r1);\n+    __ cbz(r2, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ ldr(r2, Address(r1, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r2, r2, is_value_mask);\n+    __ ldr(r4, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r4, r4, is_value_mask);\n+    __ andr(r2, r2, r4);\n+    __ cmp(r2,  is_value_mask);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(r2, r1);\n+    __ load_metadata(r4, r0);\n+    __ cmp(r2, r4);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(r0, r1, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(r0, r1, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -1964,0 +2073,1 @@\n+  __ bind(taken);\n@@ -1969,0 +2079,10 @@\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored... r0 answer, jmp to outcome...\n+  __ cbz(r0, not_subst);\n+  __ b(is_subst);\n+}\n+\n+\n@@ -2441,2 +2561,1 @@\n-  __ ubfxw(flags, raw_flags, ConstantPoolCacheEntry::tos_state_shift,\n-           ConstantPoolCacheEntry::tos_state_bits);\n+  __ ubfxw(flags, raw_flags, ConstantPoolCacheEntry::tos_state_shift, ConstantPoolCacheEntry::tos_state_bits);\n@@ -2477,4 +2596,61 @@\n-  do_oop_load(_masm, field, r0, IN_HEAP);\n-  __ push(atos);\n-  if (rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+  if (!EnableValhalla) {\n+    do_oop_load(_masm, field, r0, IN_HEAP);\n+    __ push(atos);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+    }\n+    __ b(Done);\n+  } else { \/\/ Valhalla\n+\n+    if (is_static) {\n+      __ load_heap_oop(r0, field);\n+      Label is_inline, isUninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_inline_type(raw_flags, r8 \/*temp*\/, is_inline);\n+        \/\/ Not inline case\n+        __ push(atos);\n+        __ b(Done);\n+      \/\/ Inline case, must not return null even if uninitialized\n+      __ bind(is_inline);\n+        __ cbz(r0, isUninitialized);\n+          __ push(atos);\n+          __ b(Done);\n+        __ bind(isUninitialized);\n+          __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);\n+          __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field), obj, raw_flags);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(Done);\n+    } else {\n+      Label isFlattened, isInitialized, is_inline, rewrite_inline;\n+        __ test_field_is_inline_type(raw_flags, r8 \/*temp*\/, is_inline);\n+        \/\/ Non-inline field case\n+        __ load_heap_oop(r0, field);\n+        __ push(atos);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+        }\n+        __ b(Done);\n+      __ bind(is_inline);\n+        __ test_field_is_inlined(raw_flags, r8 \/* temp *\/, isFlattened);\n+         \/\/ Non-inline field case\n+          __ load_heap_oop(r0, field);\n+          __ cbnz(r0, isInitialized);\n+            __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);\n+            __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_inline_type_field), obj, raw_flags);\n+          __ bind(isInitialized);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(rewrite_inline);\n+        __ bind(isFlattened);\n+          __ ldr(r10, Address(cache, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));\n+          __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);\n+          call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), obj, raw_flags, r10);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+         patch_bytecode(Bytecodes::_fast_qgetfield, bc, r1);\n+      }\n+      __ b(Done);\n+    }\n@@ -2482,1 +2658,0 @@\n-  __ b(Done);\n@@ -2652,0 +2827,1 @@\n+  const Register flags2 = r6;\n@@ -2674,0 +2850,2 @@\n+  __ mov(flags2, flags);\n+\n@@ -2716,8 +2894,50 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, r0, IN_HEAP);\n-    if (rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n-    }\n-    __ b(Done);\n+     if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n+      }\n+      __ b(Done);\n+     } else { \/\/ Valhalla\n+\n+      __ pop(atos);\n+      if (is_static) {\n+        Label not_inline;\n+         __ test_field_is_not_inline_type(flags2, r8 \/* temp *\/, not_inline);\n+         __ null_check(r0);\n+         __ bind(not_inline);\n+         do_oop_store(_masm, field, r0, IN_HEAP);\n+         __ b(Done);\n+      } else {\n+        Label is_inline, isFlattened, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_inline_type(flags2, r8 \/*temp*\/, is_inline);\n+        \/\/ Not inline case\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ bind(rewrite_not_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+        \/\/ Implementation of the inline semantic\n+        __ bind(is_inline);\n+        __ null_check(r0);\n+        __ test_field_is_inlined(flags2, r8 \/*temp*\/, isFlattened);\n+        \/\/ Not inline case\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ b(rewrite_inline);\n+        __ bind(isFlattened);\n+        pop_and_check_object(obj);\n+        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, off, obj);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_qputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+      }\n+     }  \/\/ Valhalla\n@@ -2863,0 +3083,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -2889,0 +3110,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -2942,0 +3164,13 @@\n+  case Bytecodes::_fast_qputfield: \/\/fall through\n+   {\n+      Label isFlattened, done;\n+      __ null_check(r0);\n+      __ test_field_is_flattened(r3, r8 \/* temp *\/, isFlattened);\n+      \/\/ No Flattened case\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      __ b(done);\n+      __ bind(isFlattened);\n+      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, r1, r2);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3039,0 +3274,26 @@\n+  case Bytecodes::_fast_qgetfield:\n+    {\n+       Label isFlattened, isInitialized, Done;\n+       \/\/ FIXME: We don't need to reload registers multiple times, but stay close to x86 code\n+       __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n+       __ test_field_is_inlined(r9, r8 \/* temp *\/, isFlattened);\n+        \/\/ Non-flattened field case\n+        __ mov(r9, r0);\n+        __ load_heap_oop(r0, field);\n+        __ cbnz(r0, isInitialized);\n+          __ mov(r0, r9);\n+          __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n+          __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);\n+          __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_inline_type_field), r0, r9);\n+        __ bind(isInitialized);\n+        __ verify_oop(r0);\n+        __ b(Done);\n+      __ bind(isFlattened);\n+        __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n+        __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);\n+        __ ldr(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));\n+        call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), r0, r9, r3);\n+        __ verify_oop(r0);\n+      __ bind(Done);\n+    }\n+    break;\n@@ -3589,0 +3850,24 @@\n+void TemplateTable::defaultvalue() {\n+  transition(vtos, atos);\n+  __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);\n+  __ get_constant_pool(c_rarg1);\n+  call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::defaultvalue),\n+          c_rarg1, c_rarg2);\n+  __ verify_oop(r0);\n+  \/\/ Must prevent reordering of stores for object initialization with stores that publish the new object.\n+  __ membar(Assembler::StoreStore);\n+}\n+\n+void TemplateTable::withfield() {\n+  transition(vtos, atos);\n+  resolve_cache_and_index(f2_byte, c_rarg1 \/*cache*\/, c_rarg2 \/*index*\/, sizeof(u2));\n+\n+  \/\/ n.b. unlike x86 cache is now rcpool plus the indexed offset\n+  \/\/ so using rcpool to meet shared code expectations\n+\n+  call_VM(r1, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), rcpool);\n+  __ verify_oop(r1);\n+  __ add(esp, esp, r0);\n+  __ mov(r0, r1);\n+}\n+\n@@ -3660,0 +3945,3 @@\n+  __ b(done);\n+  __ bind(is_null);\n+\n@@ -3662,4 +3950,16 @@\n-    __ b(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n+  if (EnableValhalla) {\n+    \/\/ Get cpool & tags index\n+    __ get_cpool_and_tags(r2, r3); \/\/ r2=cpool, r3=tags array\n+    __ get_unsigned_2_byte_index_at_bcp(r19, 1); \/\/ r19=index\n+     \/\/ See if bytecode has already been quicked\n+    __ add(rscratch1, r3, Array<u1>::base_offset_in_bytes());\n+    __ lea(r1, Address(rscratch1, r19));\n+    __ ldarb(r1, r1);\n+    \/\/ See if CP entry is a Q-descriptor\n+    __ andr (r1, r1, JVM_CONSTANT_QDescBit);\n+    __ cmp(r1, (u1) JVM_CONSTANT_QDescBit);\n+    __ br(Assembler::NE, done);\n+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":334,"deletions":34,"binary":false,"changes":368,"status":"modified"},{"patch":"@@ -3206,0 +3206,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1804,1 +1804,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n@@ -1845,1 +1845,1 @@\n-    profile_obj_type(ret, R28_mdx, -in_bytes(ReturnTypeEntry::size()), tmp1, tmp2);\n+    profile_obj_type(ret, R28_mdx, -in_bytes(SingleTypeEntry::size()), tmp1, tmp2);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/frame.inline.hpp\"\n@@ -73,0 +74,5 @@\n+  if (EnableValhalla) {\n+    assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n+    \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+    andptr(hdr, ~((int) markWord::inline_type_bit_in_place));\n+  }\n@@ -162,1 +168,2 @@\n-  if (UseBiasedLocking && !len->is_valid()) {\n+  if (EnableValhalla) {\n+    \/\/ Need to copy markWord::prototype header for klass\n@@ -324,0 +331,12 @@\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_inc, bool needs_stack_repair) {\n+  push(rbp);\n+  if (PreserveFramePointer) {\n+    mov(rbp, rsp);\n+  }\n+#if !defined(_LP64) && defined(COMPILER2)\n+  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_aot_or_jvmci()) {\n+      \/\/ c2 leaves fpu stack dirty. Clean it on entry\n+      empty_FPU_stack();\n+    }\n+#endif \/\/ !_LP64 && COMPILER2\n+  decrement(rsp, frame_size_in_bytes);\n@@ -325,2 +344,16 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n-  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  if (needs_stack_repair) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    int real_frame_size = sp_inc + frame_size_in_bytes + wordSize;\n+    movptr(Address(rsp, frame_size_in_bytes - wordSize), real_frame_size);\n+  }\n+}\n+\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n+  if (has_scalarized_args) {\n+    \/\/ Initialize orig_pc to detect deoptimization during buffering in the entry points\n+    movptr(Address(rsp, sp_offset_for_orig_pc - frame_size_in_bytes - wordSize), 0);\n+  }\n+  if (!needs_stack_repair && verified_inline_entry_label != NULL) {\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -332,0 +365,1 @@\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n@@ -334,11 +368,1 @@\n-  push(rbp);\n-  if (PreserveFramePointer) {\n-    mov(rbp, rsp);\n-  }\n-#if !defined(_LP64) && defined(COMPILER2)\n-  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_aot_or_jvmci()) {\n-    \/\/ c2 leaves fpu stack dirty. Clean it on entry\n-    empty_FPU_stack();\n-  }\n-#endif \/\/ !_LP64 && COMPILER2\n-  decrement(rsp, frame_size_in_bytes); \/\/ does not emit code for frame_size == 0\n+  build_frame_helper(frame_size_in_bytes, 0, needs_stack_repair);\n@@ -348,5 +372,5 @@\n-}\n-\n-void C1_MacroAssembler::remove_frame(int frame_size_in_bytes) {\n-  increment(rsp, frame_size_in_bytes);  \/\/ Does not emit code for frame_size == 0\n-  pop(rbp);\n+  if (needs_stack_repair && verified_inline_entry_label != NULL) {\n+    \/\/ Jump here from the scalarized entry points that require additional stack space\n+    \/\/ for packing scalarized arguments and therefore already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -356,1 +380,0 @@\n-\n@@ -373,0 +396,70 @@\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  GrowableArray<SigEntry>* sig    = &ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? &ces->sig_cc_ro() : &ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    \/\/ Two additional slots to account for return address\n+    sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n+    sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+    \/\/ Save the return address, adjust the stack (make sure it is properly\n+    \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+    \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n+    pop(r13);\n+    subptr(rsp, sp_inc);\n+    push(r13);\n+  }\n+\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_inc, ces->c1_needs_stack_repair());\n+\n+  \/\/ Initialize orig_pc to detect deoptimization during buffering in below runtime call\n+  movptr(Address(rsp, sp_offset_for_orig_pc), 0);\n+\n+  \/\/ The runtime call might safepoint, make sure nmethod entry barrier is executed\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->nmethod_entry_barrier(this);\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  movptr(rbx, (intptr_t)(ces->method()));\n+  if (is_inline_ro_entry) {\n+    call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id)));\n+  } else {\n+    call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ Remove the temp frame\n+  addptr(rsp, frame_size_in_bytes);\n+  pop(rbp);\n+\n+  shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs,          \/\/ to\n+                      sp_inc);\n+\n+  if (ces->c1_needs_stack_repair()) {\n+    \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+    \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+    build_frame_helper(frame_size_in_bytes, sp_inc, true);\n+  }\n+\n+  jmp(verified_inline_entry_label);\n+  return rt_call_offset;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":113,"deletions":20,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -1023,0 +1023,1 @@\n+    case new_instance_no_inline_id:\n@@ -1031,0 +1032,2 @@\n+        } else if (id == new_instance_no_inline_id) {\n+          __ set_info(\"new_instance_no_inline\", dont_gc_arguments);\n@@ -1095,1 +1098,6 @@\n-        int call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n+        int call_offset;\n+        if (id == new_instance_no_inline_id) {\n+          call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance_no_inline), klass);\n+        } else {\n+          call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n+        }\n@@ -1128,0 +1136,1 @@\n+    case new_flat_array_id:\n@@ -1135,1 +1144,1 @@\n-        } else {\n+        } else if (id == new_object_array_id) {\n@@ -1137,0 +1146,2 @@\n+        } else {\n+          __ set_info(\"new_flat_array\", dont_gc_arguments);\n@@ -1146,6 +1157,23 @@\n-          int tag = ((id == new_type_array_id)\n-                     ? Klass::_lh_array_tag_type_value\n-                     : Klass::_lh_array_tag_obj_value);\n-          __ cmpl(t0, tag);\n-          __ jcc(Assembler::equal, ok);\n-          __ stop(\"assert(is an array klass)\");\n+          switch (id) {\n+          case new_type_array_id:\n+            __ cmpl(t0, Klass::_lh_array_tag_type_value);\n+            __ jcc(Assembler::equal, ok);\n+            __ stop(\"assert(is a type array klass)\");\n+            break;\n+          case new_object_array_id:\n+            __ cmpl(t0, Klass::_lh_array_tag_obj_value); \/\/ new \"[Ljava\/lang\/Object;\"\n+            __ jcc(Assembler::equal, ok);\n+            __ cmpl(t0, Klass::_lh_array_tag_vt_value);  \/\/ new \"[LVT;\"\n+            __ jcc(Assembler::equal, ok);\n+            __ stop(\"assert(is an object or inline type array klass)\");\n+            break;\n+          case new_flat_array_id:\n+            \/\/ new \"[QVT;\"\n+            __ cmpl(t0, Klass::_lh_array_tag_vt_value);  \/\/ the array can be flattened.\n+            __ jcc(Assembler::equal, ok);\n+            __ cmpl(t0, Klass::_lh_array_tag_obj_value); \/\/ the array cannot be flattened (due to InlineArrayElementMaxFlatSize, etc)\n+            __ jcc(Assembler::equal, ok);\n+            __ stop(\"assert(is an object or inline type array klass)\");\n+            break;\n+          default:  ShouldNotReachHere();\n+          }\n@@ -1203,1 +1231,1 @@\n-        } else {\n+        } else if (id == new_object_array_id) {\n@@ -1205,0 +1233,3 @@\n+        } else {\n+          assert(id == new_flat_array_id, \"must be\");\n+          call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_flat_array), klass, length);\n@@ -1236,0 +1267,77 @@\n+    case load_flattened_array_id:\n+      {\n+        StubFrame f(sasm, \"load_flattened_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 3);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, rax); \/\/ rax,: array\n+        f.load_argument(0, rbx); \/\/ rbx,: index\n+        int call_offset = __ call_RT(rax, noreg, CAST_FROM_FN_PTR(address, load_flattened_array), rax, rbx);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_rax(sasm);\n+\n+        \/\/ rax,: loaded element at array[index]\n+        __ verify_oop(rax);\n+      }\n+      break;\n+\n+    case store_flattened_array_id:\n+      {\n+        StubFrame f(sasm, \"store_flattened_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 4);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(2, rax); \/\/ rax,: array\n+        f.load_argument(1, rbx); \/\/ rbx,: index\n+        f.load_argument(0, rcx); \/\/ rcx,: value\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, store_flattened_array), rax, rbx, rcx);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_rax(sasm);\n+      }\n+      break;\n+\n+    case substitutability_check_id:\n+      {\n+        StubFrame f(sasm, \"substitutability_check\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 3);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, rax); \/\/ rax,: left\n+        f.load_argument(0, rbx); \/\/ rbx,: right\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, substitutability_check), rax, rbx);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_rax(sasm);\n+\n+        \/\/ rax,: are the two operands substitutable\n+      }\n+      break;\n+\n+\n+    case buffer_inline_args_id:\n+    case buffer_inline_args_no_receiver_id:\n+      {\n+        const char* name = (id == buffer_inline_args_id) ?\n+          \"buffer_inline_args\" : \"buffer_inline_args_no_receiver\";\n+        StubFrame f(sasm, name, dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 2);\n+        Register method = rbx;\n+        address entry = (id == buffer_inline_args_id) ?\n+          CAST_FROM_FN_PTR(address, buffer_inline_args) :\n+          CAST_FROM_FN_PTR(address, buffer_inline_args_no_receiver);\n+        int call_offset = __ call_RT(rax, noreg, entry, method);\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_rax(sasm);\n+        __ verify_oop(rax);  \/\/ rax: an array of buffered value objects\n+      }\n+      break;\n+\n@@ -1338,1 +1446,1 @@\n-      { StubFrame f(sasm, \"throw_incompatible_class_cast_exception\", dont_gc_arguments);\n+      { StubFrame f(sasm, \"throw_incompatible_class_change_error\", dont_gc_arguments);\n@@ -1343,0 +1451,6 @@\n+    case throw_illegal_monitor_state_exception_id:\n+      { StubFrame f(sasm, \"throw_illegal_monitor_state_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_illegal_monitor_state_exception), false);\n+      }\n+      break;\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":124,"deletions":10,"binary":false,"changes":134,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-define_pd_global(bool, TieredCompilation,              false);\n+define_pd_global(bool, TieredCompilation,              true);\n","filename":"src\/hotspot\/cpu\/x86\/c1_globals_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -590,1 +590,1 @@\n-              Address dst, Register val, Register tmp1, Register tmp2) {\n+              Address dst, Register val, Register tmp1, Register tmp2, Register tmp3) {\n@@ -628,1 +628,1 @@\n-      BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg);\n+      BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg, noreg);\n@@ -631,1 +631,1 @@\n-      BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg);\n+      BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg, noreg);\n@@ -635,1 +635,1 @@\n-    BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2);\n+    BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2, tmp3);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -92,0 +92,3 @@\n+define_pd_global(bool, InlineTypePassFieldsAsArgs, LP64_ONLY(true) NOT_LP64(false));\n+define_pd_global(bool, InlineTypeReturnedAsFields, LP64_ONLY(true) NOT_LP64(false));\n+\n","filename":"src\/hotspot\/cpu\/x86\/globals_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -196,1 +196,1 @@\n-  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype );\n+  void gen_subtype_check(Register sub_klass, Label &ok_is_subtype, bool profile = true);\n@@ -236,0 +236,23 @@\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+  \/\/ Allocate instance in \"obj\" and read in the content of the inline field\n+  \/\/ NOTES:\n+  \/\/   - input holder object via \"obj\", which must be rax,\n+  \/\/     will return new instance via the same reg\n+  \/\/   - assumes holder_klass and valueKlass field klass have both been resolved\n+  \/\/   - 32 bits: kills rdi and rsi\n+  void read_inlined_field(Register holder_klass,\n+                            Register field_index, Register field_offset,\n+                            Register obj = rax);\n+\n+  \/\/ Allocate value buffer in \"obj\" and read in flattened element at the given index\n+  \/\/ NOTES:\n+  \/\/   - Return via \"obj\" must be rax\n+  \/\/   - kills all given regs\n+  \/\/   - 32 bits: kills rdi and rsi\n+  void read_flattened_element(Register array, Register index,\n+                              Register t1, Register t2,\n+                              Register obj = rax);\n+\n@@ -276,1 +299,1 @@\n-  void profile_not_taken_branch(Register mdp);\n+  void profile_not_taken_branch(Register mdp, bool acmp = false);\n@@ -289,0 +312,3 @@\n+  void profile_array(Register mdp, Register array, Register tmp);\n+  void profile_element(Register mdp, Register element, Register tmp);\n+  void profile_acmp(Register mdp, Register left, Register right, Register tmp);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.hpp","additions":28,"deletions":2,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -50,0 +51,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -53,0 +55,1 @@\n+#include \"vmreg_x86.inline.hpp\"\n@@ -54,0 +57,3 @@\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1628,0 +1634,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2654,0 +2664,140 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  testl(temp_reg, JVM_ACC_INLINE);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  testptr(object, object);\n+  jcc(Assembler::equal, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  movl(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  testl(temp_reg, InstanceKlass::misc_flags_is_empty_inline_type());\n+  jcc(Assembler::notZero, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::zero, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inlined_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inlined);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,\n+                                              Label&is_flattened_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flattened_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flattened_array_layout(temp_reg, is_flattened_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flattened_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flattened_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flattened_array_layout(temp_reg, is_non_flattened_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::nullfree_array_bit_in_place, true, is_null_free_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_null_free_array_layout(temp_reg, is_null_free_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::nullfree_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_null_free_array_layout(temp_reg, is_non_null_free_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_bit_inplace);\n+  jcc(Assembler::notZero, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_bit_inplace);\n+  jcc(Assembler::zero, is_non_null_free_array);\n+}\n+\n+\n@@ -3461,0 +3611,129 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax, according to barrier asm eden_allocate\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+    jcc(Assembler::equal, L);\n+    stop(\"klass not initialized\");\n+    bind(L);\n+  }\n+#endif\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+  const bool allow_shared_alloc =\n+    Universe::heap()->supports_inline_contig_alloc();\n+\n+  push(klass);\n+  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(klass);\n+#ifndef _LP64\n+  if (UseTLAB || allow_shared_alloc) {\n+    get_thread(thread);\n+  }\n+#endif \/\/ _LP64\n+\n+  if (UseTLAB) {\n+    tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    \/\/ Allocation in the shared Eden, if allowed.\n+    \/\/\n+    eden_allocate(thread, new_obj, layout_size, 0, t2, slow_case);\n+  }\n+\n+  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n+  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n+  if (UseTLAB || allow_shared_alloc) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      decrement(layout_size, sizeof(oopDesc));\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 1*oopSize), zero);\n+        NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 2*oopSize), zero));\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+#ifdef _LP64\n+    xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+#endif\n+    movptr(t2, klass);         \/\/ preserve klass\n+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    store_klass(new_obj, t2, tmp_store_klass);  \/\/ src klass reg is potentially compressed\n+\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -3538,0 +3817,50 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  movptr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(inline_klass, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  movptr(inline_klass, Address(inline_klass, index, Address::times_ptr));\n+}\n+\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  movptr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  movl(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  movptr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset, Address::times_1);\n+  load_heap_oop(obj, field);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n+\n@@ -3886,1 +4215,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -3945,1 +4278,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4447,0 +4784,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -4456,1 +4801,1 @@\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -4489,1 +4834,1 @@\n-                                     Register tmp1, Register tmp2) {\n+                                     Register tmp1, Register tmp2, Register tmp3) {\n@@ -4494,1 +4839,23 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+  } else {\n+    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+  }\n+}\n+\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n@@ -4496,1 +4863,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2);\n+    lea(data, Address(oop, offset));\n@@ -4500,0 +4867,18 @@\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE)));\n+}\n+\n@@ -4521,2 +4906,2 @@\n-                                    Register tmp2, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2);\n+                                    Register tmp2, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2, tmp3);\n@@ -4527,1 +4912,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -4841,0 +5226,1 @@\n+#ifdef COMPILER2\n@@ -4842,1 +5228,5 @@\n-void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n@@ -4895,0 +5285,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, C->output()->sp_inc_offset()), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -4923,5 +5319,1 @@\n-\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->nmethod_entry_barrier(this);\n-  }\n+#endif \/\/ COMPILER2\n@@ -4933,1 +5325,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp) {\n@@ -4939,1 +5331,1 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_512bit);\n+    evpbroadcastq(xtmp, val, AVX_512bit);\n@@ -4941,1 +5333,3 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -4943,1 +5337,2 @@\n-    pxor(xtmp, xtmp);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n@@ -4966,1 +5361,1 @@\n-    fill64_masked_avx(3, base, 0, xtmp, k2, cnt, rtmp, true);\n+    fill64_masked_avx(3, base, 0, xtmp, k2, cnt, val, true);\n@@ -4985,1 +5380,1 @@\n-    fill32_masked_avx(3, base, 0, xtmp, k2, cnt, rtmp);\n+    fill32_masked_avx(3, base, 0, xtmp, k2, cnt, val);\n@@ -4998,0 +5393,296 @@\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  Label slow_case;\n+\n+  \/\/ Try to allocate a new buffered inline type (from the heap)\n+  if (UseTLAB) {\n+    \/\/ FIXME -- for smaller code, the inline allocation (and the slow case) should be moved inside the pack handler.\n+    if (vk != NULL) {\n+      \/\/ Called from C1, where the return type is statically known.\n+      movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+      jint lh = vk->layout_helper();\n+      assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+      movl(r14, lh);\n+    } else {\n+      \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+      mov(rbx, rax);\n+      andptr(rbx, -2);\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+    }\n+\n+    movptr(r13, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));\n+    lea(r14, Address(r13, r14, Address::times_1));\n+    cmpptr(r14, Address(r15_thread, in_bytes(JavaThread::tlab_end_offset())));\n+    jcc(Assembler::above, slow_case);\n+    movptr(Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())), r14);\n+    movptr(Address(r13, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+\n+    xorl(rax, rax); \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(r13, rax);  \/\/ zero klass gap for compressed oops\n+\n+    if (vk == NULL) {\n+      \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n+      mov(rax, rbx);\n+    }\n+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    store_klass(r13, rbx, tmp_store_klass);  \/\/ klass\n+\n+    \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+    if (vk != NULL) {\n+      \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+      mov(rax, r13);\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      mov(rax, r13);\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must bevalid\");\n+  Register fromReg;\n+  if (from->is_reg()) {\n+    fromReg = from->as_Register();\n+  } else {\n+    int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+    movq(r10, Address(rsp, st_off));\n+    fromReg = r10;\n+  }\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  while (stream.next(toReg, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+     if (idx != from->value()) {\n+       mark_done = false;\n+     }\n+     done = false;\n+     continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    } else {\n+      assert(reg_state[idx] == reg_writable, \"must be writable\");\n+      reg_state[idx] = reg_written;\n+    }\n+\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? r13 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  Register val_array = rax;\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14; \/\/ Be careful with r14 because it's used for spilling\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  while (stream.next(fromReg, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+    reg_state[fromReg->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    movq(rbp, Address(rsp, initial_framesize));\n+    addq(rsp, Address(rsp, sp_inc_offset));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n@@ -5069,1 +5760,1 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp, bool is_large) {\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only) {\n@@ -5074,1 +5765,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"val register must be eax for rep stos\");\n@@ -5080,3 +5771,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n@@ -5096,1 +5784,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -5105,1 +5793,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -5109,1 +5797,1 @@\n-    xmm_clear_mem(base, cnt, tmp, xtmp);\n+    xmm_clear_mem(base, cnt, val, xtmp);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":717,"deletions":29,"binary":false,"changes":746,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -33,0 +34,2 @@\n+class ciInlineKlass;\n+\n@@ -102,0 +105,31 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type);\n+  void test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type);\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type);\n+\n+  \/\/ Get the default value oop for the given InlineKlass\n+  void get_default_value_oop(Register inline_klass, Register temp_reg, Register obj);\n+  \/\/ The empty value oop, for the given InlineKlass (\"empty\" as in no instance fields)\n+  \/\/ get_default_value_oop with extra assertion for empty inline klass\n+  void get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj);\n+\n+  void test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline);\n+  void test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline);\n+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined);\n+\n+  \/\/ Check oops for special arrays, i.e. flattened and\/or null-free\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flattened_array_oop(Register oop, Register temp_reg, Label&is_flattened_array);\n+  void test_non_flattened_array_oop(Register oop, Register temp_reg, Label&is_non_flattened_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flatten or null-free arrays...\n+  void test_flattened_array_layout(Register lh, Label& is_flattened_array);\n+  void test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array);\n+  void test_null_free_array_layout(Register lh, Label& is_null_free_array);\n+  void test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array);\n+\n@@ -322,0 +356,1 @@\n+  void load_metadata(Register dst, Register src);\n@@ -328,1 +363,11 @@\n-                       Register tmp1, Register tmp2);\n+                       Register tmp1, Register tmp2, Register tmp3 = noreg);\n+\n+  void access_value_copy(DecoratorSet decorators, Register src, Register dst, Register inline_klass);\n+\n+  \/\/ inline type data payload offsets...\n+  void first_field_offset(Register inline_klass, Register offset);\n+  void data_for_oop(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -339,1 +384,1 @@\n-                      Register tmp2 = noreg, DecoratorSet decorators = 0);\n+                      Register tmp2 = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);\n@@ -515,0 +560,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -534,0 +588,3 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n@@ -699,1 +756,2 @@\n-  void andptr(Register src1, Register src2) { LP64_ONLY(andq(src1, src2)) NOT_LP64(andl(src1, src2)) ; }\n+  void andptr(Register dst, Register src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n+  void andptr(Register dst, Address src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n@@ -1681,1 +1739,15 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(Compile* C, int sp_inc = 0);\n+\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[]);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset);\n+  VMReg spill_reg_for(VMReg reg);\n@@ -1685,1 +1757,1 @@\n-  void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large);\n+  void clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":77,"deletions":5,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -155,1 +155,5 @@\n-  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_offset() :\n+  \/\/ The following jump might pass an inline type argument that was erased to Object as oop to a\n+  \/\/ callee that expects inline type arguments to be passed as fields. We need to call the compiled\n+  \/\/ value entry (_code->inline_entry_point() or _adapter->c2i_inline_entry()) which will take care\n+  \/\/ of translating between the calling conventions.\n+  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_inline_offset() :\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -472,0 +472,1 @@\n+    case T_INLINE_TYPE:\n@@ -522,0 +523,9 @@\n+const uint SharedRuntime::java_return_convention_max_int = 1;\n+const uint SharedRuntime::java_return_convention_max_float = 1;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  Unimplemented();\n+  return 0;\n+}\n+\n@@ -583,3 +593,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>& sig_extended,\n@@ -587,1 +595,5 @@\n-                            Label& skip_fixup) {\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet*& oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words) {\n@@ -609,1 +621,1 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+  int extraspace = sig_extended.length() * Interpreter::stackElementSize;\n@@ -620,3 +632,3 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+  for (int i = 0; i < sig_extended.length(); i++) {\n+    if (sig_extended.at(i)._bt == T_VOID) {\n+      assert(i > 0 && (sig_extended.at(i-1)._bt == T_LONG || sig_extended.at(i-1)._bt == T_DOUBLE), \"missing half\");\n@@ -627,1 +639,1 @@\n-    int st_off = ((total_args_passed - 1) - i) * Interpreter::stackElementSize;\n+    int st_off = ((sig_extended.length() - 1) - i) * Interpreter::stackElementSize;\n@@ -677,1 +689,1 @@\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n+        if (sig_extended.at(i)._bt == T_LONG || sig_extended.at(i)._bt == T_DOUBLE) {\n@@ -694,1 +706,1 @@\n-        assert(sig_bt[i] == T_DOUBLE || sig_bt[i] == T_LONG, \"wrong type\");\n+        assert(sig_extended.at(i)._bt == T_DOUBLE || sig_extended.at(i)._bt == T_LONG, \"wrong type\");\n@@ -727,2 +739,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>& sig_extended,\n@@ -731,0 +742,1 @@\n+\n@@ -819,2 +831,2 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n+  for (int i = 0; i < sig_extended.length(); i++) {\n+    if (sig_extended.at(i)._bt == T_VOID) {\n@@ -823,1 +835,1 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      assert(i > 0 && (sig_extended.at(i-1)._bt == T_LONG || sig_extended.at(i-1)._bt == T_DOUBLE), \"missing half\");\n@@ -832,1 +844,1 @@\n-    int ld_off = (total_args_passed - i) * Interpreter::stackElementSize;\n+    int ld_off = (sig_extended.length() - i) * Interpreter::stackElementSize;\n@@ -873,1 +885,1 @@\n-        const int offset = (NOT_LP64(true ||) sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (NOT_LP64(true ||) sig_extended.at(i)._bt==T_LONG||sig_extended.at(i)._bt==T_DOUBLE)?\n@@ -891,1 +903,1 @@\n-        const int offset = (NOT_LP64(true ||) sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (NOT_LP64(true ||) sig_extended.at(i)._bt==T_LONG||sig_extended.at(i)._bt==T_DOUBLE)?\n@@ -939,2 +951,1 @@\n-                                                            int total_args_passed,\n-                                                            const BasicType *sig_bt,\n+                                                            const GrowableArray<SigEntry>& sig_extended,\n@@ -943,1 +954,2 @@\n-                                                            AdapterFingerPrint* fingerprint) {\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n@@ -946,1 +958,1 @@\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig_extended, regs);\n@@ -986,1 +998,4 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  OopMapSet* oop_maps = NULL;\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+  gen_c2i_adapter(masm, sig_extended, regs, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words);\n@@ -989,0 +1004,1 @@\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps);\n@@ -1012,0 +1028,1 @@\n+    case T_INLINE_TYPE:\n@@ -1703,0 +1720,1 @@\n+      case T_INLINE_TYPE:\n@@ -1884,0 +1902,1 @@\n+  case T_INLINE_TYPE:           \/\/ Really a handle\n@@ -2982,0 +3001,5 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  Unimplemented();\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":48,"deletions":24,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -512,0 +513,1 @@\n+    case T_INLINE_TYPE:\n@@ -545,0 +547,82 @@\n+\/\/ Same as java_calling_convention() but for multiple return\n+\/\/ values. There's no way to store them on the stack so if we don't\n+\/\/ have enough registers, multiple values can't be returned.\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    rax, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+  static const XMMRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3,\n+    j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_INLINE_TYPE:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+    case T_METADATA:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -587,0 +671,106 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+    for (int i = 0; i < sig_extended->length(); i++) {\n+      BasicType bt = sig_extended->at(i)._bt;\n+      if (bt == T_INLINE_TYPE) {\n+        \/\/ In sig_extended, an inline type argument starts with:\n+        \/\/ T_INLINE_TYPE, followed by the types of the fields of the\n+        \/\/ inline type and T_VOID to mark the end of the value\n+        \/\/ type. Inline types are flattened so, for instance, in the\n+        \/\/ case of an inline type with an int field and an inline type\n+        \/\/ field that itself has 2 fields, an int and a long:\n+        \/\/ T_INLINE_TYPE T_INT T_INLINE_TYPE T_INT T_LONG T_VOID (second\n+        \/\/ slot for the T_LONG) T_VOID (inner T_INLINE_TYPE) T_VOID\n+        \/\/ (outer T_INLINE_TYPE)\n+        total_args_passed++;\n+        int vt = 1;\n+        do {\n+          i++;\n+          BasicType bt = sig_extended->at(i)._bt;\n+          BasicType prev_bt = sig_extended->at(i-1)._bt;\n+          if (bt == T_INLINE_TYPE) {\n+            vt++;\n+          } else if (bt == T_VOID &&\n+                     prev_bt != T_LONG &&\n+                     prev_bt != T_DOUBLE) {\n+            vt--;\n+          }\n+        } while (vt != 0);\n+      } else {\n+        total_args_passed++;\n+      }\n+    }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  assert(bt != T_INLINE_TYPE || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"must be invalid\");\n+    return;\n+  }\n+\n+  if (!r_1->is_XMMRegister()) {\n+    Register val = rax;\n+    if (r_1->is_stack()) {\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, rscratch1);\n+    if (is_oop) {\n+      __ push(r13);\n+      __ push(rbx);\n+      __ store_heap_oop(to, val, rscratch1, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      __ pop(rbx);\n+      __ pop(r13);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    }\n+  }\n+}\n@@ -589,3 +779,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -593,1 +781,6 @@\n-                            Label& skip_fixup) {\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n@@ -603,0 +796,42 @@\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_INLINE_TYPE);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types.\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ false);\n+\n+      frame_complete = __ offset();\n+\n+      __ set_last_Java_frame(noreg, noreg, NULL);\n+\n+      __ mov(c_rarg0, r15_thread);\n+      __ mov(c_rarg1, rbx);\n+      __ mov64(c_rarg2, (int64_t)alloc_inline_receiver);\n+      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+      __ jcc(Assembler::equal, no_exception);\n+\n+      __ movptr(Address(r15_thread, JavaThread::vm_result_offset()), (int)NULL_WORD);\n+      __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+      __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(rscratch2, r15_thread); \/\/ Use rscratch2 (r11) as temporary because rscratch1 (r10) is trashed by movptr()\n+      __ get_vm_result_2(rbx, r15_thread); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n@@ -607,1 +842,1 @@\n-\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n@@ -625,46 +860,24 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n-\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n-    }\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rax\n-      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ movl(rax, Address(rsp, ld_off));\n-        __ movptr(Address(rsp, st_off), rax);\n-\n-      } else {\n-\n-        __ movq(rax, Address(rsp, ld_off));\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ movq(Address(rsp, next_off), rax);\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_INLINE_TYPE,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_INLINE_TYPE\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_INLINE_TYPE) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(rsp, offset), extraspace, false);\n+      next_arg_int++;\n@@ -673,7 +886,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n-          __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ movq(Address(rsp, st_off), rax);\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n+        __ movptr(Address(rsp, st_off), rax);\n@@ -681,16 +891,25 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ movl(Address(rsp, st_off), r);\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ long\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaab));\n-          __ movptr(Address(rsp, st_off), rax);\n-          __ movq(Address(rsp, next_off), r);\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_INLINE_TYPE);\n+      __ load_heap_oop(r14, Address(rscratch2, index));\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp-1)._bt;\n+        if (bt == T_INLINE_TYPE) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID &&\n+                   prev_bt != T_LONG &&\n+                   prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -699,1 +918,6 @@\n-          __ movptr(Address(rsp, st_off), r);\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(r14, off), extraspace, is_oop);\n@@ -701,14 +925,3 @@\n-      }\n-    } else {\n-      assert(r_1->is_XMMRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ movflt(Address(rsp, st_off), r_1->as_XMMRegister());\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov64(rax, CONST64(0xdeadffffdeadaaac));\n-        __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        __ movdbl(Address(rsp, next_off), r_1->as_XMMRegister());\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ movptr(Address(rsp, st_off), r14);\n@@ -737,2 +950,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>* sig,\n@@ -831,1 +1043,1 @@\n-  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_offset())));\n+  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_inline_offset())));\n@@ -845,0 +1057,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -848,1 +1062,3 @@\n-    if (sig_bt[i] == T_VOID) {\n+    BasicType bt = sig->at(i)._bt;\n+    assert(bt != T_INLINE_TYPE, \"i2c adapter doesn't unpack inline type args\");\n+    if (bt == T_VOID) {\n@@ -851,1 +1067,2 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      BasicType prev_bt = (i > 0) ? sig->at(i-1)._bt : T_ILLEGAL;\n+      assert(i > 0 && (prev_bt == T_LONG || prev_bt == T_DOUBLE), \"missing half\");\n@@ -893,1 +1110,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -908,1 +1125,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -939,1 +1156,1 @@\n-  \/\/ only needed becaus eof c2 resolve stubs return Method* as a result in\n+  \/\/ only needed because of c2 resolve stubs return Method* as a result in\n@@ -945,0 +1162,22 @@\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Label ok;\n+\n+  Register holder = rax;\n+  Register receiver = j_rarg0;\n+  Register temp = rbx;\n+\n+  __ load_klass(temp, receiver, rscratch1);\n+  __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n+  __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n+  __ jcc(Assembler::equal, ok);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+\n+  __ bind(ok);\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);\n+  __ jcc(Assembler::equal, skip_fixup);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n+\n@@ -947,4 +1186,8 @@\n-                                                            int total_args_passed,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n@@ -953,2 +1196,1 @@\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -967,11 +1209,1 @@\n-  Label ok;\n-\n-  Register holder = rax;\n-  Register receiver = j_rarg0;\n-  Register temp = rbx;\n-  {\n-    __ load_klass(temp, receiver, rscratch1);\n-    __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ jcc(Assembler::equal, ok);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -980,7 +1212,9 @@\n-    __ bind(ok);\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);\n-    __ jcc(Assembler::equal, skip_fixup);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+    skip_fixup.reset();\n@@ -989,0 +1223,1 @@\n+  \/\/ Scalarized c2i adapter\n@@ -1017,1 +1252,14 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, true);\n+\n+  address c2i_unverified_inline_entry = c2i_unverified_entry;\n+\n+  \/\/ Non-scalarized c2i adapter\n+  address c2i_inline_entry = c2i_entry;\n+  if (regs != regs_cc) {\n+    Label inline_entry_skip_fixup;\n+    c2i_unverified_inline_entry = __ pc();\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+  }\n@@ -1020,1 +1268,7 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+\n+  \/\/ The c2i adapters might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  bool caller_must_gc_arguments = (regs != regs_cc);\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n+\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -1078,0 +1332,1 @@\n+      case T_INLINE_TYPE:\n@@ -2111,0 +2366,1 @@\n+      case T_INLINE_TYPE:\n@@ -2246,0 +2502,6 @@\n+    if (EnableValhalla) {\n+      assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      __ andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+    }\n+\n@@ -2305,0 +2567,1 @@\n+  case T_INLINE_TYPE:           \/\/ Really a handle\n@@ -4019,0 +4282,111 @@\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  __ movptr(rax, Address(r13, 0));\n+  __ resolve_jobject(rax \/* value *\/,\n+                     r15_thread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ movptr(Address(r13, 0), rax);\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r14, r13, rbx, rscratch1);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r14, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(0);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(rax, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(rax, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  if (StressInlineTypeReturnedAsFields) {\n+    __ load_klass(rax, rax, rscratch1);\n+    __ orptr(rax, 1);\n+  }\n+\n+  __ ret(0);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":497,"deletions":123,"binary":false,"changes":620,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -62,1 +63,1 @@\n-int TemplateInterpreter::InterpreterCodeSize = JVMCI_ONLY(268) NOT_JVMCI(256) * 1024;\n+int TemplateInterpreter::InterpreterCodeSize = JVMCI_ONLY(280) NOT_JVMCI(268) * 1024;\n@@ -210,0 +211,4 @@\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(NULL);\n+  }\n+\n@@ -352,0 +357,1 @@\n+  case T_INLINE_TYPE: \/\/ fall through (inline types are handled with oops)\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -154,1 +155,1 @@\n-  __ store_heap_oop(dst, val, rdx, rbx, decorators);\n+  __ store_heap_oop(dst, val, rdx, rbx, noreg, decorators);\n@@ -177,0 +178,1 @@\n+  case Bytecodes::_fast_qputfield:\n@@ -369,0 +371,1 @@\n+  __ andl(rdx, ~JVM_CONSTANT_QDescBit);\n@@ -820,9 +823,27 @@\n-  \/\/ rax: index\n-  \/\/ rdx: array\n-  index_check(rdx, rax); \/\/ kills rbx\n-  do_oop_load(_masm,\n-              Address(rdx, rax,\n-                      UseCompressedOops ? Address::times_4 : Address::times_ptr,\n-                      arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n-              rax,\n-              IS_ARRAY);\n+  Register array = rdx;\n+  Register index = rax;\n+\n+  index_check(array, index); \/\/ kills rbx\n+  __ profile_array(rbx, array, rcx);\n+  if (UseFlatArray) {\n+    Label is_flat_array, done;\n+    __ test_flattened_array_oop(array, rbx, is_flat_array);\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+    __ jmp(done);\n+    __ bind(is_flat_array);\n+    __ read_flattened_element(array, index, rbx, rcx, rax);\n+    __ bind(done);\n+  } else {\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+  }\n+  __ profile_element(rbx, rax, rcx);\n@@ -1114,1 +1135,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1126,0 +1147,4 @@\n+\n+  __ profile_array(rdi, rdx, rbx);\n+  __ profile_element(rdi, rax, rbx);\n+\n@@ -1129,0 +1154,1 @@\n+  \/\/ Move array class to rdi\n@@ -1130,0 +1156,6 @@\n+  __ load_klass(rdi, rdx, tmp_load_klass);\n+  if (UseFlatArray) {\n+    __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+    __ test_flattened_array_layout(rbx, is_flat_array);\n+  }\n+\n@@ -1132,3 +1164,2 @@\n-  \/\/ Move superklass into rax\n-  __ load_klass(rax, rdx, tmp_load_klass);\n-  __ movptr(rax, Address(rax,\n+  \/\/ Move array element superklass into rax\n+  __ movptr(rax, Address(rdi,\n@@ -1139,1 +1170,2 @@\n-  __ gen_subtype_check(rbx, ok_is_subtype);\n+  \/\/ is \"rbx <: rax\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(rbx, ok_is_subtype, false);\n@@ -1157,1 +1189,2 @@\n-  __ profile_null_seen(rbx);\n+  if (EnableValhalla) {\n+    Label is_null_into_value_array_npe, store_null;\n@@ -1159,0 +1192,9 @@\n+    \/\/ No way to store null in null-free array\n+    __ test_null_free_array_oop(rdx, rbx, is_null_into_value_array_npe);\n+    __ jmp(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ jump(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n@@ -1161,0 +1203,7 @@\n+  __ jmp(done);\n+\n+  if (EnableValhalla) {\n+    Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n+\n+    \/\/ Simplistic type check...\n@@ -1162,0 +1211,27 @@\n+    \/\/ Profile the not-null value's klass.\n+    __ load_klass(rbx, rax, tmp_load_klass);\n+    \/\/ Move element klass into rax\n+    __ movptr(rax, Address(rdi, ArrayKlass::element_klass_offset()));\n+    \/\/ flat value array needs exact type match\n+    \/\/ is \"rax == rbx\" (value subclass == array element superclass)\n+    __ cmpptr(rax, rbx);\n+    __ jccb(Assembler::equal, is_type_ok);\n+\n+    __ jump(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));\n+\n+    __ bind(is_type_ok);\n+    \/\/ rbx: value's klass\n+    \/\/ rdx: array\n+    \/\/ rdi: array klass\n+    __ test_klass_is_empty_inline_type(rbx, rax, done);\n+\n+    \/\/ calc dst for copy\n+    __ movl(rax, at_tos_p1()); \/\/ index\n+    __ data_for_value_array_index(rdx, rdi, rax, rax);\n+\n+    \/\/ ...and src for copy\n+    __ movptr(rcx, at_tos());  \/\/ value\n+    __ data_for_oop(rcx, rcx, rbx);\n+\n+    __ access_value_copy(IN_HEAP, rcx, rax, rbx);\n+  }\n@@ -2352,1 +2428,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -2354,0 +2430,36 @@\n+\n+  __ profile_acmp(rbx, rdx, rax, rcx);\n+\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  if (EnableValhalla) {\n+    __ cmpoop(rdx, rax);\n+    __ jcc(Assembler::equal, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either rax or rdx is null\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+    __ testptr(rdx, rdx);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ movptr(rbx, Address(rdx, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, is_inline_type_mask);\n+    __ cmpptr(rbx, is_inline_type_mask);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(rbx, rdx);\n+    __ load_metadata(rcx, rax);\n+    __ cmpptr(rbx, rcx);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(rax, rdx, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(rax, rdx, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -2356,0 +2468,1 @@\n+  __ bind(taken);\n@@ -2358,1 +2471,10 @@\n-  __ profile_not_taken_branch(rax);\n+  __ profile_not_taken_branch(rax, true);\n+}\n+\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored...rax answer, jmp to outcome...\n+  __ testl(rax, rax);\n+  __ jcc(Assembler::zero, not_subst);\n+  __ jmp(is_subst);\n@@ -2627,1 +2749,2 @@\n-  __ remove_activation(state, rbcp);\n+\n+  __ remove_activation(state, rbcp, true, true, true);\n@@ -2825,0 +2948,1 @@\n+  const Register flags2 = rdx;\n@@ -2830,2 +2954,0 @@\n-  if (!is_static) pop_and_check_object(obj);\n-\n@@ -2834,1 +2956,9 @@\n-  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj;\n+  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj, notInlineType;\n+\n+  if (!is_static) {\n+    __ movptr(rcx, Address(cache, index, Address::times_ptr,\n+                           in_bytes(ConstantPoolCache::base_offset() +\n+                                    ConstantPoolCacheEntry::f1_offset())));\n+  }\n+\n+  __ movl(flags2, flags);\n@@ -2844,0 +2974,1 @@\n+  if (!is_static) pop_and_check_object(obj);\n@@ -2853,0 +2984,1 @@\n+\n@@ -2855,1 +2987,1 @@\n-\n+   if (!is_static) pop_and_check_object(obj);\n@@ -2870,4 +3002,82 @@\n-  do_oop_load(_masm, field, rax);\n-  __ push(atos);\n-  if (!is_static && rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+  if (!EnableValhalla) {\n+    if (!is_static) pop_and_check_object(obj);\n+    do_oop_load(_masm, field, rax);\n+    __ push(atos);\n+    if (!is_static && rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+    }\n+    __ jmp(Done);\n+  } else {\n+    if (is_static) {\n+      __ load_heap_oop(rax, field);\n+      Label is_inline_type, uninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);\n+        \/\/ field is not an inline type\n+        __ push(atos);\n+        __ jmp(Done);\n+      \/\/ field is an inline type, must not return null even if uninitialized\n+      __ bind(is_inline_type);\n+        __ testptr(rax, rax);\n+        __ jcc(Assembler::zero, uninitialized);\n+          __ push(atos);\n+          __ jmp(Done);\n+        __ bind(uninitialized);\n+          __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+#ifdef _LP64\n+          Label slow_case, finish;\n+          __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+          __ jcc(Assembler::notEqual, slow_case);\n+        __ get_default_value_oop(rcx, off, rax);\n+        __ jmp(finish);\n+        __ bind(slow_case);\n+#endif \/\/ LP64\n+          __ call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field),\n+                 obj, flags2);\n+#ifdef _LP64\n+          __ bind(finish);\n+#endif \/\/ _LP64\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(Done);\n+    } else {\n+      Label is_inlined, nonnull, is_inline_type, rewrite_inline;\n+      __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);\n+        \/\/ field is not an inline type\n+        pop_and_check_object(obj);\n+        __ load_heap_oop(rax, field);\n+        __ push(atos);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+        }\n+        __ jmp(Done);\n+      __ bind(is_inline_type);\n+        __ test_field_is_inlined(flags2, rscratch1, is_inlined);\n+          \/\/ field is not inlined\n+          __ movptr(rax, rcx);  \/\/ small dance required to preserve the klass_holder somewhere\n+          pop_and_check_object(obj);\n+          __ push(rax);\n+          __ load_heap_oop(rax, field);\n+          __ pop(rcx);\n+          __ testptr(rax, rax);\n+          __ jcc(Assembler::notZero, nonnull);\n+            __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+            __ get_inline_type_field_klass(rcx, flags2, rbx);\n+            __ get_default_value_oop(rbx, rcx, rax);\n+          __ bind(nonnull);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(rewrite_inline);\n+        __ bind(is_inlined);\n+        \/\/ field is inlined\n+          __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+          pop_and_check_object(rax);\n+          __ read_inlined_field(rcx, flags2, rbx, rax);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_qgetfield, bc, rbx);\n+      }\n+      __ jmp(Done);\n+    }\n@@ -2875,1 +3085,0 @@\n-  __ jmp(Done);\n@@ -2878,0 +3087,3 @@\n+\n+  if (!is_static) pop_and_check_object(obj);\n+\n@@ -2977,0 +3189,15 @@\n+void TemplateTable::withfield() {\n+  transition(vtos, atos);\n+\n+  Register cache = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n+  Register index = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n+\n+  resolve_cache_and_index(f2_byte, cache, index, sizeof(u2));\n+\n+  call_VM(rbx, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), cache);\n+  \/\/ new value type is returned in rbx\n+  \/\/ stack adjustement is returned in rax\n+  __ verify_oop(rbx);\n+  __ addptr(rsp, rax);\n+  __ movptr(rax, rbx);\n+}\n@@ -3072,0 +3299,1 @@\n+  const Register flags2 = rdx;\n@@ -3088,0 +3316,1 @@\n+  __ movl(flags2, flags);\n@@ -3090,1 +3319,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags, flags2);\n@@ -3096,1 +3325,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags, flags2);\n@@ -3102,1 +3331,1 @@\n-                                              Register obj, Register off, Register flags) {\n+                                              Register obj, Register off, Register flags, Register flags2) {\n@@ -3109,1 +3338,1 @@\n-        notLong, notFloat, notObj;\n+        notLong, notFloat, notObj, notInlineType;\n@@ -3152,6 +3381,53 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, rax);\n-    if (!is_static && rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+    if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, rax);\n+      if (!is_static && rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+      }\n+      __ jmp(Done);\n+    } else {\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_inline_type;\n+        __ test_field_is_not_inline_type(flags2, rscratch1, is_inline_type);\n+        __ null_check(rax);\n+        __ bind(is_inline_type);\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(Done);\n+      } else {\n+        Label is_inline_type, is_inlined, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);\n+        \/\/ Not an inline type\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        __ bind(rewrite_not_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(is_inline_type);\n+        __ null_check(rax);\n+        __ test_field_is_inlined(flags2, rscratch1, is_inlined);\n+        \/\/ field is not inlined\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(rewrite_inline);\n+        __ bind(is_inlined);\n+        \/\/ field is inlined\n+        pop_and_check_object(obj);\n+        assert_different_registers(rax, rdx, obj, off);\n+        __ load_klass(rdx, rax, rscratch1);\n+        __ data_for_oop(rax, rax, rdx);\n+        __ addptr(obj, off);\n+        __ access_value_copy(IN_HEAP, rax, obj, rdx);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_qputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+      }\n@@ -3159,1 +3435,0 @@\n-    __ jmp(Done);\n@@ -3298,0 +3573,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -3323,0 +3599,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/ fall through\n@@ -3362,0 +3639,4 @@\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rscratch2, rdx);  \/\/ saving flags for is_inlined test\n+  }\n+\n@@ -3375,1 +3656,4 @@\n-  fast_storefield_helper(field, rax);\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rdx, rscratch2);  \/\/ restoring flags for is_inlined test\n+  }\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3381,1 +3665,4 @@\n-  fast_storefield_helper(field, rax);\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rdx, rscratch2);  \/\/ restoring flags for is_inlined test\n+  }\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3386,1 +3673,1 @@\n-void TemplateTable::fast_storefield_helper(Address field, Register rax) {\n+void TemplateTable::fast_storefield_helper(Address field, Register rax, Register flags) {\n@@ -3390,0 +3677,17 @@\n+  case Bytecodes::_fast_qputfield:\n+    {\n+      Label is_inlined, done;\n+      __ null_check(rax);\n+      __ test_field_is_inlined(flags, rscratch1, is_inlined);\n+      \/\/ field is not inlined\n+      do_oop_store(_masm, field, rax);\n+      __ jmp(done);\n+      __ bind(is_inlined);\n+      \/\/ field is inlined\n+      __ load_klass(rdx, rax, rscratch1);\n+      __ data_for_oop(rax, rax, rdx);\n+      __ lea(rcx, field);\n+      __ access_value_copy(IN_HEAP, rax, rcx, rdx);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3391,1 +3695,3 @@\n-    do_oop_store(_masm, field, rax);\n+    {\n+      do_oop_store(_masm, field, rax);\n+    }\n@@ -3461,1 +3767,1 @@\n-  __ movptr(rbx, Address(rcx, rbx, Address::times_ptr,\n+  __ movptr(rdx, Address(rcx, rbx, Address::times_ptr,\n@@ -3468,1 +3774,1 @@\n-  Address field(rax, rbx, Address::times_1);\n+  Address field(rax, rdx, Address::times_1);\n@@ -3472,0 +3778,39 @@\n+  case Bytecodes::_fast_qgetfield:\n+    {\n+      Label is_inlined, nonnull, Done;\n+      __ movptr(rscratch1, Address(rcx, rbx, Address::times_ptr,\n+                                   in_bytes(ConstantPoolCache::base_offset() +\n+                                            ConstantPoolCacheEntry::flags_offset())));\n+      __ test_field_is_inlined(rscratch1, rscratch2, is_inlined);\n+        \/\/ field is not inlined\n+        __ load_heap_oop(rax, field);\n+        __ testptr(rax, rax);\n+        __ jcc(Assembler::notZero, nonnull);\n+          __ movl(rdx, Address(rcx, rbx, Address::times_ptr,\n+                             in_bytes(ConstantPoolCache::base_offset() +\n+                                      ConstantPoolCacheEntry::flags_offset())));\n+          __ andl(rdx, ConstantPoolCacheEntry::field_index_mask);\n+          __ movptr(rcx, Address(rcx, rbx, Address::times_ptr,\n+                                       in_bytes(ConstantPoolCache::base_offset() +\n+                                                ConstantPoolCacheEntry::f1_offset())));\n+          __ get_inline_type_field_klass(rcx, rdx, rbx);\n+          __ get_default_value_oop(rbx, rcx, rax);\n+        __ bind(nonnull);\n+        __ verify_oop(rax);\n+        __ jmp(Done);\n+      __ bind(is_inlined);\n+      \/\/ field is inlined\n+        __ push(rdx); \/\/ save offset\n+        __ movl(rdx, Address(rcx, rbx, Address::times_ptr,\n+                           in_bytes(ConstantPoolCache::base_offset() +\n+                                    ConstantPoolCacheEntry::flags_offset())));\n+        __ andl(rdx, ConstantPoolCacheEntry::field_index_mask);\n+        __ movptr(rcx, Address(rcx, rbx, Address::times_ptr,\n+                                     in_bytes(ConstantPoolCache::base_offset() +\n+                                              ConstantPoolCacheEntry::f1_offset())));\n+        __ pop(rbx); \/\/ restore offset\n+        __ read_inlined_field(rcx, rdx, rbx, rax);\n+      __ bind(Done);\n+      __ verify_oop(rax);\n+    }\n+    break;\n@@ -3940,3 +4285,1 @@\n-  Label slow_case_no_pop;\n-  Label initialize_header;\n-  Label initialize_object;  \/\/ including clearing the fields\n+  Label is_not_value;\n@@ -3952,1 +4295,1 @@\n-  __ jcc(Assembler::notEqual, slow_case_no_pop);\n+  __ jcc(Assembler::notEqual, slow_case);\n@@ -3956,1 +4299,7 @@\n-  __ push(rcx);  \/\/ save the contexts of klass for initializing the header\n+\n+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InstanceKlass::_kind_inline_type);\n+  __ jcc(Assembler::notEqual, is_not_value);\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_InstantiationError));\n+\n+  __ bind(is_not_value);\n@@ -3959,1 +4308,0 @@\n-  \/\/ make sure klass is fully initialized\n@@ -3963,19 +4311,2 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ movl(rdx, Address(rcx, Klass::layout_helper_offset()));\n-  \/\/ test to see if it has a finalizer or is malformed in some way\n-  __ testl(rdx, Klass::_lh_instance_slow_path_bit);\n-  __ jcc(Assembler::notZero, slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/  Else If inline contiguous allocations are enabled:\n-  \/\/    Try to allocate in eden.\n-  \/\/    If fails due to heap end, go to slow path.\n-  \/\/\n-  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n+  __ allocate_instance(rcx, rax, rdx, rbx, true, slow_case);\n+  __ jmp(done);\n@@ -3983,2 +4314,2 @@\n-  const bool allow_shared_alloc =\n-    Universe::heap()->supports_inline_contig_alloc();\n+  \/\/ slow case\n+  __ bind(slow_case);\n@@ -3986,6 +4317,2 @@\n-  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n-#ifndef _LP64\n-  if (UseTLAB || allow_shared_alloc) {\n-    __ get_thread(thread);\n-  }\n-#endif \/\/ _LP64\n+  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n+  Register rarg2 = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n@@ -3993,15 +4320,4 @@\n-  if (UseTLAB) {\n-    __ tlab_allocate(thread, rax, rdx, 0, rcx, rbx, slow_case);\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ jmp(initialize_header);\n-    } else {\n-      \/\/ initialize both the header and fields\n-      __ jmp(initialize_object);\n-    }\n-  } else {\n-    \/\/ Allocation in the shared Eden, if allowed.\n-    \/\/\n-    \/\/ rdx: instance size in bytes\n-    __ eden_allocate(thread, rax, rdx, 0, rbx, slow_case);\n-  }\n+  __ get_constant_pool(rarg1);\n+  __ get_unsigned_2_byte_index_at_bcp(rarg2, 1);\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);\n+   __ verify_oop(rax);\n@@ -4009,24 +4325,3 @@\n-  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n-  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n-  if (UseTLAB || allow_shared_alloc) {\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    __ bind(initialize_object);\n-    __ decrement(rdx, sizeof(oopDesc));\n-    __ jcc(Assembler::zero, initialize_header);\n-\n-    \/\/ Initialize topmost object field, divide rdx by 8, check if odd and\n-    \/\/ test if zero.\n-    __ xorl(rcx, rcx);    \/\/ use zero reg to clear memory (shorter code)\n-    __ shrl(rdx, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n-\n-    \/\/ rdx must have been multiple of 8\n-#ifdef ASSERT\n-    \/\/ make sure rdx was multiple of 8\n-    Label L;\n-    \/\/ Ignore partial flag stall after shrl() since it is debug VM\n-    __ jcc(Assembler::carryClear, L);\n-    __ stop(\"object size is not multiple of 2 - adjust this code\");\n-    __ bind(L);\n-    \/\/ rdx must be > 0, no extra check needed here\n-#endif\n+  \/\/ continue\n+  __ bind(done);\n+}\n@@ -4034,8 +4329,2 @@\n-    \/\/ initialize remaining object fields: rdx was a multiple of 8\n-    { Label loop;\n-    __ bind(loop);\n-    __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n-    __ decrement(rdx);\n-    __ jcc(Assembler::notZero, loop);\n-    }\n+void TemplateTable::defaultvalue() {\n+  transition(vtos, atos);\n@@ -4043,17 +4332,3 @@\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    if (UseBiasedLocking) {\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);\n-    } else {\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()),\n-                (intptr_t)markWord::prototype().value()); \/\/ header\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-    }\n-#ifdef _LP64\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n-#endif\n-    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-    __ store_klass(rax, rcx, tmp_store_klass);  \/\/ klass\n+  Label slow_case;\n+  Label done;\n+  Label is_value;\n@@ -4061,8 +4336,2 @@\n-    {\n-      SkipIfEqual skip_if(_masm, &DTraceAllocProbes, 0);\n-      \/\/ Trigger dtrace event for fastpath\n-      __ push(atos);\n-      __ call_VM_leaf(\n-           CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), rax);\n-      __ pop(atos);\n-    }\n+  __ get_unsigned_2_byte_index_at_bcp(rdx, 1);\n+  __ get_cpool_and_tags(rcx, rax);\n@@ -4070,2 +4339,25 @@\n-    __ jmp(done);\n-  }\n+  \/\/ Make sure the class we're about to instantiate has been resolved.\n+  \/\/ This is done before loading InstanceKlass to be consistent with the order\n+  \/\/ how Constant Pool is updated (see ConstantPool::klass_at_put)\n+  const int tags_offset = Array<u1>::base_offset_in_bytes();\n+  __ cmpb(Address(rax, rdx, Address::times_1, tags_offset), JVM_CONSTANT_Class);\n+  __ jcc(Assembler::notEqual, slow_case);\n+\n+  \/\/ get InstanceKlass\n+  __ load_resolved_klass_at_index(rcx, rcx, rdx);\n+\n+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InstanceKlass::_kind_inline_type);\n+  __ jcc(Assembler::equal, is_value);\n+\n+  \/\/ in the future, defaultvalue will just return null instead of throwing an exception\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_IncompatibleClassChangeError));\n+\n+  __ bind(is_value);\n+\n+  \/\/ make sure klass is fully initialized\n+  __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+  __ jcc(Assembler::notEqual, slow_case);\n+\n+  \/\/ have a resolved InlineKlass in rcx, return the default value oop from it\n+  __ get_default_value_oop(rcx, rdx, rax);\n+  __ jmp(done);\n@@ -4073,4 +4365,1 @@\n-  \/\/ slow case\n-  __ pop(rcx);   \/\/ restore stack pointer to what it was when we came in.\n-  __ bind(slow_case_no_pop);\n-  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n+  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n@@ -4081,3 +4370,4 @@\n-  __ get_constant_pool(rarg1);\n-  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);\n-   __ verify_oop(rax);\n+  __ get_constant_pool(rarg1);\n+\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::defaultvalue),\n+      rarg1, rarg2);\n@@ -4086,1 +4376,1 @@\n-  \/\/ continue\n+  __ verify_oop(rax);\n@@ -4126,4 +4416,5 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+      Address::times_1,\n+      Array<u1>::base_offset_in_bytes()));\n+  __ andl (rdx, ~JVM_CONSTANT_QDescBit);\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4168,0 +4459,3 @@\n+  __ jmp(done);\n+\n+  __ bind(is_null);\n@@ -4171,4 +4465,15 @@\n-    __ jmp(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n+  if (EnableValhalla) {\n+    \/\/ Get cpool & tags index\n+    __ get_cpool_and_tags(rcx, rdx); \/\/ rcx=cpool, rdx=tags array\n+    __ get_unsigned_2_byte_index_at_bcp(rbx, 1); \/\/ rbx=index\n+    \/\/ See if CP entry is a Q-descriptor\n+    __ movzbl(rcx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+    __ andl (rcx, JVM_CONSTANT_QDescBit);\n+    __ cmpl(rcx, JVM_CONSTANT_QDescBit);\n+    __ jcc(Assembler::notEqual, done);\n+    __ jump(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+  }\n+\n@@ -4190,4 +4495,5 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+  __ andl (rdx, ~JVM_CONSTANT_QDescBit);\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4246,1 +4552,0 @@\n-\n@@ -4310,0 +4615,4 @@\n+  Label is_inline_type;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ test_markword_is_inline_type(rbx, is_inline_type);\n+\n@@ -4399,0 +4708,5 @@\n+\n+  __ bind(is_inline_type);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                    InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n@@ -4409,0 +4723,11 @@\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  Label has_identity;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ andptr(rbx, is_inline_type_mask);\n+  __ cmpl(rbx, is_inline_type_mask);\n+  __ jcc(Assembler::notEqual, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":498,"deletions":173,"binary":false,"changes":671,"status":"modified"},{"patch":"@@ -1541,1 +1541,1 @@\n-  if (!UseFastStosb && UseSSE >= 2 && UseUnalignedLoadStores) {\n+  if (UseSSE >= 2 && UseUnalignedLoadStores) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -73,0 +73,3 @@\n+define_pd_global(bool, InlineTypePassFieldsAsArgs, false);\n+define_pd_global(bool, InlineTypeReturnedAsFields, false);\n+\n","filename":"src\/hotspot\/cpu\/zero\/globals_zero.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -204,0 +204,3 @@\n+  virtual address inline_entry_point() const { return _code + _meta->verified_entry_offset(); }\n+  virtual address verified_inline_entry_point() const { return _code + _meta->verified_entry_offset(); }\n+  virtual address verified_inline_ro_entry_point() const { return _code + _meta->verified_entry_offset(); }\n","filename":"src\/hotspot\/share\/aot\/aotCompiledMethod.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -568,0 +568,1 @@\n+, _compiled_entry_signature(method->get_Method())\n@@ -584,0 +585,4 @@\n+  {\n+    ResetNoHandleMark rnhm; \/\/ Huh? Required when doing class lookup of the Q-types\n+    _compiled_entry_signature.compute_calling_conventions();\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -95,0 +96,1 @@\n+  CompiledEntrySignature _compiled_entry_signature;\n@@ -262,0 +264,4 @@\n+  bool profile_array_accesses() {\n+    return env()->comp_level() == CompLevel_full_profile &&\n+      C1UpdateMethodData;\n+  }\n@@ -290,0 +296,7 @@\n+\n+  const CompiledEntrySignature* compiled_entry_signature() const {\n+    return &_compiled_entry_signature;\n+  }\n+  bool needs_stack_repair() const {\n+    return compiled_entry_signature()->c1_needs_stack_repair();\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -660,0 +662,11 @@\n+  \/\/ Record this newly allocated object\n+  void new_instance(NewInlineTypeInstance* object) {\n+    int index = _newobjects.length();\n+    _newobjects.append(object);\n+    if (_fields.at_grow(index, NULL) == NULL) {\n+      _fields.at_put(index, new FieldBuffer());\n+    } else {\n+      _fields.at(index)->kill();\n+    }\n+  }\n+\n@@ -942,0 +955,7 @@\n+  if (x->as_NewInlineTypeInstance() != NULL && x->as_NewInlineTypeInstance()->in_larval_state()) {\n+    if (x->as_NewInlineTypeInstance()->on_stack_count() == 1) {\n+      x->as_NewInlineTypeInstance()->set_not_larva_anymore();\n+    } else {\n+      x->as_NewInlineTypeInstance()->increment_on_stack_count();\n+    }\n+  }\n@@ -948,0 +968,3 @@\n+  if (x->as_NewInlineTypeInstance() != NULL) {\n+    x->as_NewInlineTypeInstance()->set_local_index(index);\n+  }\n@@ -976,0 +999,3 @@\n+  if (x->as_NewInlineTypeInstance() != NULL) {\n+    x->as_NewInlineTypeInstance()->set_local_index(index);\n+  }\n@@ -981,1 +1007,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = NULL;\n+  int array_idx = state()->stack_size() - 2;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flattened_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flattened arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -991,1 +1025,53 @@\n-  push(as_ValueType(type), append(new LoadIndexed(array, index, length, type, state_before)));\n+\n+  LoadIndexed* load_indexed = NULL;\n+  Instruction* result = NULL;\n+  if (array->is_loaded_flattened_array()) {\n+    ciType* array_type = array->declared_type();\n+    ciInlineKlass* elem_klass = array_type->as_flat_array_klass()->element_klass()->as_inline_klass();\n+\n+    bool can_delay_access = false;\n+    ciBytecodeStream s(method());\n+    s.force_bci(bci());\n+    s.next();\n+    if (s.cur_bc() == Bytecodes::_getfield) {\n+      bool will_link;\n+      ciField* next_field = s.get_field(will_link);\n+      bool next_needs_patching = !next_field->holder()->is_loaded() ||\n+                                 !next_field->will_link(method(), Bytecodes::_getfield) ||\n+                                 PatchALot;\n+      can_delay_access = !next_needs_patching;\n+    }\n+    if (can_delay_access) {\n+      \/\/ potentially optimizable array access, storing information for delayed decision\n+      LoadIndexed* li = new LoadIndexed(array, index, length, type, state_before);\n+      DelayedLoadIndexed* dli = new DelayedLoadIndexed(li, state_before);\n+      li->set_delayed(dli);\n+      set_pending_load_indexed(dli);\n+      return; \/\/ Nothing else to do for now\n+    } else {\n+      if (elem_klass->is_empty()) {\n+        \/\/ No need to create a new instance, the default instance will be used instead\n+        load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+        apush(append(load_indexed));\n+      } else {\n+        NewInlineTypeInstance* new_instance = new NewInlineTypeInstance(elem_klass, state_before);\n+        _memory->new_instance(new_instance);\n+        apush(append_split(new_instance));\n+        load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+        load_indexed->set_vt(new_instance);\n+      }\n+    }\n+  } else {\n+    load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+    if (profile_array_accesses() && is_reference_type(type)) {\n+      compilation()->set_would_profile(true);\n+      load_indexed->set_should_profile(true);\n+      load_indexed->set_profiled_method(method());\n+      load_indexed->set_profiled_bci(bci());\n+    }\n+  }\n+  result = append(load_indexed);\n+  assert(!load_indexed->should_profile() || load_indexed == result, \"should not be optimized out\");\n+  if (!array->is_loaded_flattened_array()) {\n+    push(as_ValueType(type), result);\n+  }\n@@ -997,1 +1083,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = NULL;\n+  int array_idx = state()->stack_size() - 3;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flattened_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flattened arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -1020,5 +1114,2 @@\n-  StoreIndexed* result = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n-  append(result);\n-  _memory->store_value(value);\n-  if (type == T_OBJECT && is_profiling()) {\n-    \/\/ Note that we'd collect profile data in this method if we wanted it.\n+  StoreIndexed* store_indexed = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n+  if (profile_array_accesses() && is_reference_type(type) && !array->is_loaded_flattened_array()) {\n@@ -1027,6 +1118,3 @@\n-\n-    if (profile_checkcasts()) {\n-      result->set_profiled_method(method());\n-      result->set_profiled_bci(bci());\n-      result->set_should_profile(true);\n-    }\n+    store_indexed->set_should_profile(true);\n+    store_indexed->set_profiled_method(method());\n+    store_indexed->set_profiled_bci(bci());\n@@ -1034,0 +1122,3 @@\n+  Instruction* result = append(store_indexed);\n+  assert(!store_indexed->should_profile() || store_indexed == result, \"should not be optimized out\");\n+  _memory->store_value(value);\n@@ -1036,1 +1127,0 @@\n-\n@@ -1040,1 +1130,2 @@\n-      { state()->raw_pop();\n+      { Value w = state()->raw_pop();\n+        update_larva_stack_count(w);\n@@ -1044,2 +1135,4 @@\n-      { state()->raw_pop();\n-        state()->raw_pop();\n+      { Value w1 = state()->raw_pop();\n+        Value w2 = state()->raw_pop();\n+        update_larva_stack_count(w1);\n+        update_larva_stack_count(w2);\n@@ -1050,0 +1143,1 @@\n+        update_larval_state(w);\n@@ -1057,0 +1151,1 @@\n+        update_larval_state(w1);\n@@ -1066,0 +1161,11 @@\n+        \/\/ special handling for the dup_x2\/pop sequence (see JDK-8251046)\n+        if (w1 != NULL && w1->as_NewInlineTypeInstance() != NULL) {\n+          ciBytecodeStream s(method());\n+          s.force_bci(bci());\n+          s.next();\n+          if (s.cur_bc() != Bytecodes::_pop) {\n+            w1->as_NewInlineTypeInstance()->set_not_larva_anymore();\n+          }  else {\n+            w1->as_NewInlineTypeInstance()->increment_on_stack_count();\n+           }\n+        }\n@@ -1075,0 +1181,2 @@\n+        update_larval_state(w1);\n+        update_larval_state(w2);\n@@ -1085,0 +1193,2 @@\n+        update_larval_state(w1);\n+        update_larval_state(w2);\n@@ -1097,0 +1207,2 @@\n+        update_larval_state(w1);\n+        update_larval_state(w2);\n@@ -1228,0 +1340,27 @@\n+\n+  bool subst_check = false;\n+  if (EnableValhalla && (stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne)) {\n+    ValueType* left_vt = x->type();\n+    ValueType* right_vt = y->type();\n+    if (left_vt->is_object()) {\n+      assert(right_vt->is_object(), \"must be\");\n+      ciKlass* left_klass = x->as_loaded_klass_or_null();\n+      ciKlass* right_klass = y->as_loaded_klass_or_null();\n+\n+      if (left_klass == NULL || right_klass == NULL) {\n+        \/\/ The klass is still unloaded, or came from a Phi node. Go slow case;\n+        subst_check = true;\n+      } else if (left_klass->can_be_inline_klass() || right_klass->can_be_inline_klass()) {\n+        \/\/ Either operand may be a value object, but we're not sure. Go slow case;\n+        subst_check = true;\n+      } else {\n+        \/\/ No need to do substitutability check\n+      }\n+    }\n+  }\n+  if ((stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne) &&\n+      is_profiling() && profile_branches()) {\n+    compilation()->set_would_profile(true);\n+    append(new ProfileACmpTypes(method(), bci(), x, y));\n+  }\n+\n@@ -1230,1 +1369,1 @@\n-  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic()) ? state_before : NULL, is_bb));\n+  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic() || subst_check) ? state_before : NULL, is_bb, subst_check));\n@@ -1481,1 +1620,1 @@\n-  if (method()->name() == ciSymbols::object_initializer_name() &&\n+  if (method()->is_object_constructor() &&\n@@ -1632,0 +1771,13 @@\n+void GraphBuilder::copy_inline_content(ciInlineKlass* vk, Value src, int src_off, Value dest, int dest_off, ValueStack* state_before) {\n+  assert(vk->nof_nonstatic_fields() > 0, \"Empty inline type access should be removed\");\n+  for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+    ciField* inner_field = vk->nonstatic_field_at(i);\n+    assert(!inner_field->is_flattened(), \"the iteration over nested fields is handled by the loop itself\");\n+    int off = inner_field->offset() - vk->first_field_offset();\n+    LoadField* load = new LoadField(src, src_off + off, inner_field, false, state_before, false);\n+    Value replacement = append(load);\n+    StoreField* store = new StoreField(dest, dest_off + off, inner_field, replacement, false, state_before, false);\n+    append(store);\n+  }\n+}\n+\n@@ -1638,0 +1790,1 @@\n+\n@@ -1641,1 +1794,1 @@\n-                              PatchALot;\n+                              (!field->is_flattened() && PatchALot);\n@@ -1660,1 +1813,1 @@\n-  if (field->is_final() && (code == Bytecodes::_putfield)) {\n+  if (field->is_final() && code == Bytecodes::_putfield) {\n@@ -1671,1 +1824,1 @@\n-  const int offset = !needs_patching ? field->offset() : -1;\n+  int offset = !needs_patching ? field->offset() : -1;\n@@ -1681,0 +1834,3 @@\n+      } else if (field_type == T_INLINE_TYPE && field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Loading from a field of an empty inline type. Just return the default instance.\n+        constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n@@ -1688,2 +1844,3 @@\n-        push(type, append(new LoadField(append(obj), offset, field, true,\n-                                        state_before, needs_patching)));\n+        LoadField* load_field = new LoadField(append(obj), offset, field, true,\n+                                        state_before, needs_patching);\n+        push(type, append(load_field));\n@@ -1698,1 +1855,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1702,0 +1859,4 @@\n+      if (field_type == T_INLINE_TYPE && field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Storing to a field of an empty inline type. Ignore.\n+        break;\n+      }\n@@ -1708,14 +1869,30 @@\n-      obj = apop();\n-      ObjectType* obj_type = obj->type()->as_ObjectType();\n-      if (field->is_constant() && obj_type->is_constant() && !PatchALot) {\n-        ciObject* const_oop = obj_type->constant_value();\n-        if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n-          ciConstant field_value = field->constant_value_of(const_oop);\n-          if (field_value.is_valid()) {\n-            constant = make_constant(field_value, field);\n-            \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n-            if (field->is_call_site_target()) {\n-              ciCallSite* call_site = const_oop->as_call_site();\n-              if (!call_site->is_fully_initialized_constant_call_site()) {\n-                ciMethodHandle* target = field_value.as_object()->as_method_handle();\n-                dependency_recorder()->assert_call_site_target_value(call_site, target);\n+      if (state_before == NULL && field->is_flattened()) {\n+        \/\/ Save the entire state and re-execute on deopt when accessing flattened fields\n+        assert(Interpreter::bytecode_should_reexecute(code), \"should reexecute\");\n+        state_before = copy_state_before();\n+      }\n+      if (!has_pending_field_access() && !has_pending_load_indexed()) {\n+        obj = apop();\n+        ObjectType* obj_type = obj->type()->as_ObjectType();\n+        if (field_type == T_INLINE_TYPE && field->type()->as_inline_klass()->is_empty()) {\n+          \/\/ Loading from a field of an empty inline type. Just return the default instance.\n+          null_check(obj);\n+          constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+        } else if (field->is_constant() && !field->is_flattened() && obj_type->is_constant() && !PatchALot) {\n+          ciObject* const_oop = obj_type->constant_value();\n+          if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n+            ciConstant field_value = field->constant_value_of(const_oop);\n+            if (field_value.is_valid()) {\n+              if (field->signature()->is_Q_signature() && field_value.is_null_or_zero()) {\n+                \/\/ Non-flattened inline type field. Replace null by the default value.\n+                constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+              } else {\n+                constant = make_constant(field_value, field);\n+              }\n+              \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n+              if (field->is_call_site_target()) {\n+                ciCallSite* call_site = const_oop->as_call_site();\n+                if (!call_site->is_fully_initialized_constant_call_site()) {\n+                  ciMethodHandle* target = field_value.as_object()->as_method_handle();\n+                  dependency_recorder()->assert_call_site_target_value(call_site, target);\n+                }\n@@ -1733,19 +1910,15 @@\n-        LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n-        Value replacement = !needs_patching ? _memory->load(load) : load;\n-        if (replacement != load) {\n-          assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n-          \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n-          \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n-          BasicType bt = field->type()->basic_type();\n-          switch (bt) {\n-          case T_BOOLEAN:\n-          case T_BYTE:\n-            replacement = append(new Convert(Bytecodes::_i2b, replacement, as_ValueType(bt)));\n-            break;\n-          case T_CHAR:\n-            replacement = append(new Convert(Bytecodes::_i2c, replacement, as_ValueType(bt)));\n-            break;\n-          case T_SHORT:\n-            replacement = append(new Convert(Bytecodes::_i2s, replacement, as_ValueType(bt)));\n-            break;\n-          default:\n+        if (!field->is_flattened()) {\n+          if (has_pending_field_access()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            obj = pending_field_access()->obj();\n+            offset += pending_field_access()->offset() - field->holder()->as_inline_klass()->first_field_offset();\n+            field = pending_field_access()->holder()->get_field_by_offset(offset, false);\n+            assert(field != NULL, \"field not found\");\n+            set_pending_field_access(NULL);\n+          } else if (has_pending_load_indexed()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+            LoadIndexed* li = pending_load_indexed()->load_instr();\n+            li->set_type(type);\n+            push(type, append(li));\n+            set_pending_load_indexed(NULL);\n@@ -1754,1 +1927,24 @@\n-          push(type, replacement);\n+          LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n+          Value replacement = !needs_patching ? _memory->load(load) : load;\n+          if (replacement != load) {\n+            assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n+            \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n+            \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n+            switch (field_type) {\n+            case T_BOOLEAN:\n+            case T_BYTE:\n+              replacement = append(new Convert(Bytecodes::_i2b, replacement, type));\n+              break;\n+            case T_CHAR:\n+              replacement = append(new Convert(Bytecodes::_i2c, replacement, type));\n+              break;\n+            case T_SHORT:\n+              replacement = append(new Convert(Bytecodes::_i2s, replacement, type));\n+              break;\n+            default:\n+              break;\n+            }\n+            push(type, replacement);\n+          } else {\n+            push(type, append(load));\n+          }\n@@ -1756,1 +1952,57 @@\n-          push(type, append(load));\n+          \/\/ Look at the next bytecode to check if we can delay the field access\n+          bool can_delay_access = false;\n+          ciBytecodeStream s(method());\n+          s.force_bci(bci());\n+          s.next();\n+          if (s.cur_bc() == Bytecodes::_getfield && !needs_patching) {\n+            ciField* next_field = s.get_field(will_link);\n+            bool next_needs_patching = !next_field->holder()->is_loaded() ||\n+                                       !next_field->will_link(method(), Bytecodes::_getfield) ||\n+                                       PatchALot;\n+            can_delay_access = !next_needs_patching;\n+          }\n+          if (can_delay_access) {\n+            if (has_pending_load_indexed()) {\n+              pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+            } else if (has_pending_field_access()) {\n+              pending_field_access()->inc_offset(offset - field->holder()->as_inline_klass()->first_field_offset());\n+            } else {\n+              null_check(obj);\n+              DelayedFieldAccess* dfa = new DelayedFieldAccess(obj, field->holder(), field->offset());\n+              set_pending_field_access(dfa);\n+            }\n+          } else {\n+            ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+            scope()->set_wrote_final();\n+            scope()->set_wrote_fields();\n+            if (inline_klass->is_empty()) {\n+              apush(append(new Constant(new InstanceConstant(inline_klass->default_instance()))));\n+              if (has_pending_field_access()) {\n+                set_pending_field_access(NULL);\n+              } else if (has_pending_load_indexed()) {\n+                set_pending_load_indexed(NULL);\n+              }\n+            } else if (has_pending_load_indexed()) {\n+              assert(!needs_patching, \"Can't patch delayed field access\");\n+              pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+              NewInlineTypeInstance* vt = new NewInlineTypeInstance(inline_klass, pending_load_indexed()->state_before());\n+              _memory->new_instance(vt);\n+              pending_load_indexed()->load_instr()->set_vt(vt);\n+              apush(append_split(vt));\n+              append(pending_load_indexed()->load_instr());\n+              set_pending_load_indexed(NULL);\n+            } else {\n+              NewInlineTypeInstance* new_instance = new NewInlineTypeInstance(inline_klass, state_before);\n+              _memory->new_instance(new_instance);\n+              apush(append_split(new_instance));\n+              assert(!needs_patching, \"Can't patch flattened inline type field access\");\n+              if (has_pending_field_access()) {\n+                copy_inline_content(inline_klass, pending_field_access()->obj(),\n+                                    pending_field_access()->offset() + field->offset() - field->holder()->as_inline_klass()->first_field_offset(),\n+                                    new_instance, inline_klass->first_field_offset(), state_before);\n+                set_pending_field_access(NULL);\n+              } else {\n+                copy_inline_content(inline_klass, obj, field->offset(), new_instance, inline_klass->first_field_offset(), state_before);\n+              }\n+            }\n+          }\n@@ -1767,1 +2019,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1771,4 +2023,13 @@\n-      StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n-      if (!needs_patching) store = _memory->store(store);\n-      if (store != NULL) {\n-        append(store);\n+      if (field_type == T_INLINE_TYPE && field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Storing to a field of an empty inline type. Ignore.\n+        null_check(obj);\n+      } else if (!field->is_flattened()) {\n+        StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n+        if (!needs_patching) store = _memory->store(store);\n+        if (store != NULL) {\n+          append(store);\n+        }\n+      } else {\n+        assert(!needs_patching, \"Can't patch flattened inline type field access\");\n+        ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+        copy_inline_content(inline_klass, val, inline_klass->first_field_offset(), obj, offset, state_before);\n@@ -1784,0 +2045,75 @@\n+\/\/ Baseline version of withfield, allocate every time\n+void GraphBuilder::withfield(int field_index)\n+{\n+  \/\/ Save the entire state and re-execute on deopt\n+  ValueStack* state_before = copy_state_before();\n+  state_before->set_should_reexecute(true);\n+\n+  bool will_link;\n+  ciField* field_modify = stream()->get_field(will_link);\n+  ciInstanceKlass* holder = field_modify->holder();\n+  BasicType field_type = field_modify->type()->basic_type();\n+  ValueType* type = as_ValueType(field_type);\n+  Value val = pop(type);\n+  Value obj = apop();\n+\n+  if (!holder->is_loaded()) {\n+    apush(append_split(new Deoptimize(state_before)));\n+    return;\n+  }\n+\n+  \/\/ call will_link again to determine if the field is valid.\n+  const bool needs_patching = !field_modify->will_link(method(), Bytecodes::_withfield) ||\n+                              (!field_modify->is_flattened() && PatchALot);\n+  const int offset_modify = !needs_patching ? field_modify->offset() : -1;\n+  assert(holder->is_inlinetype(), \"must be an inline klass\");\n+\n+  scope()->set_wrote_final();\n+  scope()->set_wrote_fields();\n+\n+  NewInlineTypeInstance* new_instance;\n+  if (obj->as_NewInlineTypeInstance() != NULL && obj->as_NewInlineTypeInstance()->in_larval_state()) {\n+    new_instance = obj->as_NewInlineTypeInstance();\n+    apush(append_split(new_instance));\n+  } else {\n+    new_instance = new NewInlineTypeInstance(holder->as_inline_klass(), state_before);\n+    _memory->new_instance(new_instance);\n+    apush(append_split(new_instance));\n+\n+    \/\/ Initialize fields which are not modified\n+    for (int i = 0; i < holder->nof_nonstatic_fields(); i++) {\n+      ciField* field = holder->nonstatic_field_at(i);\n+      int offset = field->offset();\n+      \/\/ Don't use offset_modify here, it might be set to -1 if needs_patching\n+      if (offset != field_modify->offset()) {\n+        if (field->is_flattened()) {\n+          ciInlineKlass* vk = field->type()->as_inline_klass();\n+          if (!vk->is_empty()) {\n+            copy_inline_content(vk, obj, offset, new_instance, vk->first_field_offset(), state_before);\n+          }\n+        } else {\n+          LoadField* load = new LoadField(obj, offset, field, false, state_before, false);\n+          Value replacement = append(load);\n+          StoreField* store = new StoreField(new_instance, offset, field, replacement, false, state_before, false);\n+          append(store);\n+        }\n+      }\n+    }\n+  }\n+\n+  \/\/ Field to modify\n+  if (field_type == T_BOOLEAN) {\n+    Value mask = append(new Constant(new IntConstant(1)));\n+    val = append(new LogicOp(Bytecodes::_iand, val, mask));\n+  }\n+  if (field_modify->is_flattened()) {\n+    assert(!needs_patching, \"Can't patch flattened inline type field access\");\n+    ciInlineKlass* vk = field_modify->type()->as_inline_klass();\n+    if (!vk->is_empty()) {\n+      copy_inline_content(vk, val, vk->first_field_offset(), new_instance, offset_modify, state_before);\n+    }\n+  } else {\n+    StoreField* store = new StoreField(new_instance, offset_modify, field_modify, val, false, state_before, needs_patching);\n+    append(store);\n+  }\n+}\n@@ -1869,1 +2205,1 @@\n-  if (bc_raw == Bytecodes::_invokespecial && !target->is_object_initializer()) {\n+  if (bc_raw == Bytecodes::_invokespecial && !target->is_object_constructor()) {\n@@ -2124,1 +2460,2 @@\n-  Invoke* result = new Invoke(code, result_type, recv, args, vtable_index, target, state_before);\n+  Invoke* result = new Invoke(code, result_type, recv, args, vtable_index, target, state_before,\n+                              declared_signature->return_type()->is_inlinetype());\n@@ -2151,0 +2488,10 @@\n+void GraphBuilder::default_value(int klass_index) {\n+  bool will_link;\n+  ciKlass* klass = stream()->get_klass(will_link);\n+  if (!stream()->is_unresolved_klass() && klass->is_inlinetype()) {\n+    ciInlineKlass* vk = klass->as_inline_klass();\n+    apush(append(new Constant(new InstanceConstant(vk->default_instance()))));\n+  } else {\n+    apush(append_split(new Deoptimize(copy_state_before())));\n+  }\n+}\n@@ -2161,0 +2508,1 @@\n+  bool null_free = stream()->is_inline_klass();\n@@ -2162,1 +2510,1 @@\n-  NewArray* n = new NewObjectArray(klass, ipop(), state_before);\n+  NewArray* n = new NewObjectArray(klass, ipop(), state_before, null_free);\n@@ -2187,0 +2535,1 @@\n+  bool null_free = stream()->is_inline_klass();\n@@ -2188,1 +2537,1 @@\n-  CheckCast* c = new CheckCast(klass, apop(), state_before);\n+  CheckCast* c = new CheckCast(klass, apop(), state_before, null_free);\n@@ -2227,0 +2576,19 @@\n+  bool maybe_inlinetype = false;\n+  if (bci == InvocationEntryBci) {\n+    \/\/ Called by GraphBuilder::inline_sync_entry.\n+#ifdef ASSERT\n+    ciType* obj_type = x->declared_type();\n+    assert(obj_type == NULL || !obj_type->is_inlinetype(), \"inline types cannot have synchronized methods\");\n+#endif\n+  } else {\n+    \/\/ We are compiling a monitorenter bytecode\n+    if (EnableValhalla) {\n+      ciType* obj_type = x->declared_type();\n+      if (obj_type == NULL || obj_type->as_klass()->can_be_inline_klass()) {\n+        \/\/ If we're (possibly) locking on an inline type, check for markWord::always_locked_pattern\n+        \/\/ and throw IMSE. (obj_type is null for Phi nodes, so let's just be conservative).\n+        maybe_inlinetype = true;\n+      }\n+    }\n+  }\n+\n@@ -2229,1 +2597,1 @@\n-  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before), bci);\n+  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before, maybe_inlinetype), bci);\n@@ -2403,1 +2771,3 @@\n-    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci(), \"invalid bci\");\n+    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci()\n+           || has_pending_field_access() || has_pending_load_indexed(), \"invalid bci\");\n+\n@@ -2891,0 +3261,2 @@\n+      case Bytecodes::_defaultvalue   : default_value(s.get_index_u2()); break;\n+      case Bytecodes::_withfield      : withfield(s.get_index_u2()); break;\n@@ -3176,1 +3548,2 @@\n-    state->store_local(idx, new Local(method()->holder(), objectType, idx, true));\n+    state->store_local(idx, new Local(method()->holder(), objectType, idx,\n+             \/*receiver*\/ true, \/*null_free*\/ method()->holder()->is_flat_array_klass()));\n@@ -3188,1 +3561,1 @@\n-    state->store_local(idx, new Local(type, vt, idx, false));\n+    state->store_local(idx, new Local(type, vt, idx, false, type->is_inlinetype()));\n@@ -3208,0 +3581,2 @@\n+  , _pending_field_access(NULL)\n+  , _pending_load_indexed(NULL)\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":449,"deletions":74,"binary":false,"changes":523,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -56,0 +58,1 @@\n+      case Bytecodes::_withfield:\n@@ -63,0 +66,1 @@\n+      case Bytecodes::_defaultvalue:\n@@ -119,0 +123,1 @@\n+  _verified_inline_entry.reset();\n@@ -340,1 +345,1 @@\n-void LIR_Assembler::add_call_info(int pc_offset, CodeEmitInfo* cinfo) {\n+void LIR_Assembler::add_call_info(int pc_offset, CodeEmitInfo* cinfo, bool maybe_return_as_fields) {\n@@ -342,1 +347,1 @@\n-  cinfo->record_debug_info(compilation()->debug_info_recorder(), pc_offset);\n+  cinfo->record_debug_info(compilation()->debug_info_recorder(), pc_offset, maybe_return_as_fields);\n@@ -485,0 +490,6 @@\n+  ciInlineKlass* vk;\n+  if (op->maybe_return_as_fields(&vk)) {\n+    int offset = store_inline_type_fields_to_buf(vk);\n+    add_call_info(offset, op->info(), true);\n+  }\n+\n@@ -592,0 +603,135 @@\n+void LIR_Assembler::add_scalarized_entry_info(int pc_offset) {\n+  flush_debug_info(pc_offset);\n+  DebugInformationRecorder* debug_info = compilation()->debug_info_recorder();\n+  \/\/ The VEP and VIEP(RO) of a C1-compiled method call buffer_inline_args_xxx()\n+  \/\/ before doing any argument shuffling. This call may cause GC. When GC happens,\n+  \/\/ all the parameters are still as passed by the caller, so we just use\n+  \/\/ map->set_include_argument_oops() inside frame::sender_for_compiled_frame(RegisterMap* map).\n+  \/\/ There's no need to build a GC map here.\n+  OopMap* oop_map = new OopMap(0, 0);\n+  debug_info->add_safepoint(pc_offset, oop_map);\n+  DebugToken* locvals = debug_info->create_scope_values(NULL); \/\/ FIXME is this needed (for Java debugging to work properly??)\n+  DebugToken* expvals = debug_info->create_scope_values(NULL); \/\/ FIXME is this needed (for Java debugging to work properly??)\n+  DebugToken* monvals = debug_info->create_monitor_values(NULL); \/\/ FIXME: need testing with synchronized method\n+  bool reexecute = false;\n+  bool return_oop = false; \/\/ This flag will be ignored since it used only for C2 with escape analysis.\n+  bool rethrow_exception = false;\n+  bool is_method_handle_invoke = false;\n+  debug_info->describe_scope(pc_offset, methodHandle(), method(), 0, reexecute, rethrow_exception, is_method_handle_invoke, return_oop, false, locvals, expvals, monvals);\n+  debug_info->end_safepoint(pc_offset);\n+}\n+\n+\/\/ The entries points of C1-compiled methods can have the following types:\n+\/\/ (1) Methods with no inline type args\n+\/\/ (2) Methods with inline type receiver but no inline type args\n+\/\/     VIEP_RO is the same as VIEP\n+\/\/ (3) Methods with non-inline type receiver and some inline type args\n+\/\/     VIEP_RO is the same as VEP\n+\/\/ (4) Methods with inline type receiver and other inline type args\n+\/\/     Separate VEP, VIEP and VIEP_RO\n+\/\/\n+\/\/ (1)               (2)                 (3)                    (4)\n+\/\/ UEP\/UIEP:         VEP:                UEP:                   UEP:\n+\/\/   check_icache      pack receiver       check_icache           check_icache\n+\/\/ VEP\/VIEP\/VIEP_RO    jump to VIEP      VEP\/VIEP_RO:           VIEP_RO:\n+\/\/   body            UEP\/UIEP:             pack inline args       pack inline args (except receiver)\n+\/\/                     check_icache        jump to VIEP           jump to VIEP\n+\/\/                   VIEP\/VIEP_RO        UIEP:                  VEP:\n+\/\/                     body                check_icache           pack all inline args\n+\/\/                                       VIEP:                    jump to VIEP\n+\/\/                                         body                 UIEP:\n+\/\/                                                                check_icache\n+\/\/                                                              VIEP:\n+\/\/                                                                body\n+void LIR_Assembler::emit_std_entries() {\n+  offsets()->set_value(CodeOffsets::OSR_Entry, _masm->offset());\n+\n+  _masm->align(CodeEntryAlignment);\n+  const CompiledEntrySignature* ces = compilation()->compiled_entry_signature();\n+  if (ces->has_scalarized_args()) {\n+    assert(InlineTypePassFieldsAsArgs && method()->get_Method()->has_scalarized_args(), \"must be\");\n+    CodeOffsets::Entries ro_entry_type = ces->c1_inline_ro_entry_type();\n+\n+    \/\/ UEP: check icache and fall-through\n+    if (ro_entry_type != CodeOffsets::Verified_Inline_Entry) {\n+      offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+      if (needs_icache(method())) {\n+        check_icache();\n+      }\n+    }\n+\n+    \/\/ VIEP_RO: pack all value parameters, except the receiver\n+    if (ro_entry_type == CodeOffsets::Verified_Inline_Entry_RO) {\n+      emit_std_entry(CodeOffsets::Verified_Inline_Entry_RO, ces);\n+    }\n+\n+    \/\/ VEP: pack all value parameters\n+    _masm->align(CodeEntryAlignment);\n+    emit_std_entry(CodeOffsets::Verified_Entry, ces);\n+\n+    \/\/ UIEP: check icache and fall-through\n+    _masm->align(CodeEntryAlignment);\n+    offsets()->set_value(CodeOffsets::Inline_Entry, _masm->offset());\n+    if (ro_entry_type == CodeOffsets::Verified_Inline_Entry) {\n+      \/\/ Special case if we have VIEP == VIEP(RO):\n+      \/\/ this means UIEP (called by C1) == UEP (called by C2).\n+      offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+    }\n+    if (needs_icache(method())) {\n+      check_icache();\n+    }\n+\n+    \/\/ VIEP: all value parameters are passed as refs - no packing.\n+    emit_std_entry(CodeOffsets::Verified_Inline_Entry, NULL);\n+\n+    if (ro_entry_type != CodeOffsets::Verified_Inline_Entry_RO) {\n+      \/\/ The VIEP(RO) is the same as VEP or VIEP\n+      assert(ro_entry_type == CodeOffsets::Verified_Entry ||\n+             ro_entry_type == CodeOffsets::Verified_Inline_Entry, \"must be\");\n+      offsets()->set_value(CodeOffsets::Verified_Inline_Entry_RO,\n+                           offsets()->value(ro_entry_type));\n+    }\n+  } else {\n+    \/\/ All 3 entries are the same (no inline type packing)\n+    offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+    offsets()->set_value(CodeOffsets::Inline_Entry, _masm->offset());\n+    if (needs_icache(method())) {\n+      check_icache();\n+    }\n+    emit_std_entry(CodeOffsets::Verified_Inline_Entry, NULL);\n+    offsets()->set_value(CodeOffsets::Verified_Entry, offsets()->value(CodeOffsets::Verified_Inline_Entry));\n+    offsets()->set_value(CodeOffsets::Verified_Inline_Entry_RO, offsets()->value(CodeOffsets::Verified_Inline_Entry));\n+  }\n+}\n+\n+void LIR_Assembler::emit_std_entry(CodeOffsets::Entries entry, const CompiledEntrySignature* ces) {\n+  offsets()->set_value(entry, _masm->offset());\n+  _masm->verified_entry();\n+  switch (entry) {\n+  case CodeOffsets::Verified_Entry: {\n+    if (needs_clinit_barrier_on_entry(method())) {\n+      clinit_barrier(method());\n+    }\n+    int rt_call_offset = _masm->verified_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()), _verified_inline_entry);\n+    add_scalarized_entry_info(rt_call_offset);\n+    break;\n+  }\n+  case CodeOffsets::Verified_Inline_Entry_RO: {\n+    assert(!needs_clinit_barrier_on_entry(method()), \"can't be static\");\n+    int rt_call_offset = _masm->verified_inline_ro_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()), _verified_inline_entry);\n+    add_scalarized_entry_info(rt_call_offset);\n+    break;\n+  }\n+  case CodeOffsets::Verified_Inline_Entry: {\n+    if (needs_clinit_barrier_on_entry(method())) {\n+      clinit_barrier(method());\n+    }\n+    build_frame();\n+    offsets()->set_value(CodeOffsets::Frame_Complete, _masm->offset());\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n+    break;\n+  }\n+}\n@@ -605,13 +751,1 @@\n-      \/\/ init offsets\n-      offsets()->set_value(CodeOffsets::OSR_Entry, _masm->offset());\n-      _masm->align(CodeEntryAlignment);\n-      if (needs_icache(compilation()->method())) {\n-        check_icache();\n-      }\n-      offsets()->set_value(CodeOffsets::Verified_Entry, _masm->offset());\n-      _masm->verified_entry();\n-      if (needs_clinit_barrier_on_entry(compilation()->method())) {\n-        clinit_barrier(compilation()->method());\n-      }\n-      build_frame();\n-      offsets()->set_value(CodeOffsets::Frame_Complete, _masm->offset());\n+      emit_std_entries();\n@@ -671,0 +805,4 @@\n+    case lir_check_orig_pc:\n+      check_orig_pc();\n+      break;\n+\n@@ -764,1 +902,2 @@\n-  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()),\n+                     needs_stack_repair(), method()->has_scalarized_args(), &_verified_inline_entry);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":155,"deletions":16,"binary":false,"changes":171,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -214,0 +216,2 @@\n+  assert(!_gen->in_conditional_code(), \"LIRItem cannot be loaded in conditional code\");\n+\n@@ -641,1 +645,2 @@\n-void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info) {\n+void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no,\n+                                 CodeEmitInfo* info_for_exception, CodeEmitInfo* info, CodeStub* throw_imse_stub) {\n@@ -644,1 +649,1 @@\n-  CodeStub* slow_path = new MonitorEnterStub(object, lock, info);\n+  CodeStub* slow_path = new MonitorEnterStub(object, lock, info, throw_imse_stub, scratch);\n@@ -647,1 +652,1 @@\n-  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception);\n+  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception, throw_imse_stub);\n@@ -671,4 +676,9 @@\n-void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n-  klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n-  \/\/ If klass is not loaded we do not know if the klass has finalizers:\n-  if (UseFastNewInstance && klass->is_loaded()\n+void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, bool allow_inline, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n+  if (allow_inline) {\n+    assert(!is_unresolved && klass->is_loaded(), \"inline type klass should be resolved\");\n+    __ metadata2reg(klass->constant_encoding(), klass_reg);\n+  } else {\n+    klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n+  }\n+  \/\/ If klass is not loaded we do not know if the klass has finalizers or is an unexpected inline klass\n+  if (UseFastNewInstance && klass->is_loaded() && (allow_inline || !klass->is_inlinetype())\n@@ -688,2 +698,2 @@\n-    CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, Runtime1::new_instance_id);\n-    __ branch(lir_cond_always, slow_path);\n+    CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, allow_inline ? Runtime1::new_instance_id : Runtime1::new_instance_no_inline_id);\n+    __ jump(slow_path);\n@@ -789,0 +799,10 @@\n+  if (!src->is_loaded_flattened_array() && !dst->is_loaded_flattened_array()) {\n+    flags &= ~LIR_OpArrayCopy::always_slow_path;\n+  }\n+  if (!src->maybe_flattened_array()) {\n+    flags &= ~LIR_OpArrayCopy::src_inlinetype_check;\n+  }\n+  if (!dst->maybe_flattened_array() && !dst->maybe_null_free_array()) {\n+    flags &= ~LIR_OpArrayCopy::dst_inlinetype_check;\n+  }\n+\n@@ -1537,2 +1557,4 @@\n-  _constants.append(c);\n-  _reg_for_constants.append(result);\n+  if (!in_conditional_code()) {\n+    _constants.append(c);\n+    _reg_for_constants.append(result);\n+  }\n@@ -1542,0 +1564,6 @@\n+void LIRGenerator::set_in_conditional_code(bool v) {\n+  assert(v != _in_conditional_code, \"must change state\");\n+  _in_conditional_code = v;\n+}\n+\n+\n@@ -1633,0 +1661,5 @@\n+  if (!inline_type_field_access_prolog(x, info)) {\n+    \/\/ Field store will always deopt due to unloaded field or holder klass\n+    return;\n+  }\n+\n@@ -1654,0 +1687,167 @@\n+\/\/ FIXME -- I can't find any other way to pass an address to access_load_at().\n+class TempResolvedAddress: public Instruction {\n+ public:\n+  TempResolvedAddress(ValueType* type, LIR_Opr addr) : Instruction(type) {\n+    set_operand(addr);\n+  }\n+  virtual void input_values_do(ValueVisitor*) {}\n+  virtual void visit(InstructionVisitor* v)   {}\n+  virtual const char* name() const  { return \"TempResolvedAddress\"; }\n+};\n+\n+LIR_Opr LIRGenerator::get_and_load_element_address(LIRItem& array, LIRItem& index) {\n+  ciType* array_type = array.value()->declared_type();\n+  ciFlatArrayKlass* flat_array_klass = array_type->as_flat_array_klass();\n+  assert(flat_array_klass->is_loaded(), \"must be\");\n+\n+  int array_header_size = flat_array_klass->array_header_in_bytes();\n+  int shift = flat_array_klass->log2_element_size();\n+\n+#ifndef _LP64\n+  LIR_Opr index_op = new_register(T_INT);\n+  \/\/ FIXME -- on 32-bit, the shift below can overflow, so we need to check that\n+  \/\/ the top (shift+1) bits of index_op must be zero, or\n+  \/\/ else throw ArrayIndexOutOfBoundsException\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::intConst(const_index << shift), index_op);\n+  } else {\n+    __ shift_left(index_op, shift, index.result());\n+  }\n+#else\n+  LIR_Opr index_op = new_register(T_LONG);\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::longConst(const_index << shift), index_op);\n+  } else {\n+    __ convert(Bytecodes::_i2l, index.result(), index_op);\n+    \/\/ Need to shift manually, as LIR_Address can scale only up to 3.\n+    __ shift_left(index_op, shift, index_op);\n+  }\n+#endif\n+\n+  LIR_Opr elm_op = new_pointer_register();\n+  LIR_Address* elm_address = new LIR_Address(array.result(), index_op, array_header_size, T_ADDRESS);\n+  __ leal(LIR_OprFact::address(elm_address), elm_op);\n+  return elm_op;\n+}\n+\n+void LIRGenerator::access_sub_element(LIRItem& array, LIRItem& index, LIR_Opr& result, ciField* field, int sub_offset) {\n+  assert(field != NULL, \"Need a subelement type specified\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  BasicType subelt_type = field->type()->basic_type();\n+  TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(subelt_type), elm_op);\n+  LIRItem elm_item(elm_resolved_addr, this);\n+\n+  DecoratorSet decorators = IN_HEAP;\n+  access_load_at(decorators, subelt_type,\n+                     elm_item, LIR_OprFact::intConst(sub_offset), result,\n+                     NULL, NULL);\n+\n+  if (field->signature()->is_Q_signature()) {\n+    assert(field->type()->as_inline_klass()->is_loaded(), \"Must be\");\n+    LabelObj* L_end = new LabelObj();\n+    __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(NULL));\n+    __ branch(lir_cond_notEqual, L_end->label());\n+    set_in_conditional_code(true);\n+    Constant* default_value = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+    __ move(load_constant(default_value), result);\n+    __ branch_destination(L_end->label());\n+    set_in_conditional_code(false);\n+  }\n+}\n+\n+void LIRGenerator::access_flattened_array(bool is_load, LIRItem& array, LIRItem& index, LIRItem& obj_item,\n+                                          ciField* field, int sub_offset) {\n+  assert(sub_offset == 0 || field != NULL, \"Sanity check\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  ciInlineKlass* elem_klass = NULL;\n+  if (field != NULL) {\n+    elem_klass = field->type()->as_inline_klass();\n+  } else {\n+    elem_klass = array.value()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+  }\n+  for (int i = 0; i < elem_klass->nof_nonstatic_fields(); i++) {\n+    ciField* inner_field = elem_klass->nonstatic_field_at(i);\n+    assert(!inner_field->is_flattened(), \"flattened fields must have been expanded\");\n+    int obj_offset = inner_field->offset();\n+    int elm_offset = obj_offset - elem_klass->first_field_offset() + sub_offset; \/\/ object header is not stored in array.\n+    BasicType field_type = inner_field->type()->basic_type();\n+\n+    \/\/ Types which are smaller than int are still passed in an int register.\n+    BasicType reg_type = field_type;\n+    switch (reg_type) {\n+    case T_BYTE:\n+    case T_BOOLEAN:\n+    case T_SHORT:\n+    case T_CHAR:\n+      reg_type = T_INT;\n+      break;\n+    default:\n+      break;\n+    }\n+\n+    LIR_Opr temp = new_register(reg_type);\n+    TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(field_type), elm_op);\n+    LIRItem elm_item(elm_resolved_addr, this);\n+\n+    DecoratorSet decorators = IN_HEAP;\n+    if (is_load) {\n+      access_load_at(decorators, field_type,\n+                     elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                     NULL, NULL);\n+      access_store_at(decorators, field_type,\n+                      obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                      NULL, NULL);\n+    } else {\n+      access_load_at(decorators, field_type,\n+                     obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                     NULL, NULL);\n+      access_store_at(decorators, field_type,\n+                      elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                      NULL, NULL);\n+    }\n+  }\n+}\n+\n+void LIRGenerator::check_flattened_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path) {\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_flattened_array(array, value, tmp, slow_path);\n+}\n+\n+void LIRGenerator::check_null_free_array(LIRItem& array, LIRItem& value, CodeEmitInfo* info) {\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+  __ branch(lir_cond_equal, L_end->label());\n+  __ null_check(value.result(), info);\n+  __ branch_destination(L_end->label());\n+}\n+\n+bool LIRGenerator::needs_flattened_array_store_check(StoreIndexed* x) {\n+  if (x->elt_type() == T_OBJECT && x->array()->maybe_flattened_array()) {\n+    ciType* type = x->value()->declared_type();\n+    if (type != NULL && type->is_klass()) {\n+      ciKlass* klass = type->as_klass();\n+      if (!klass->can_be_inline_klass() || (klass->is_inlinetype() && !klass->as_inline_klass()->flatten_array())) {\n+        \/\/ This is known to be a non-flattened object. If the array is flattened,\n+        \/\/ it will be caught by the code generated by array_store_check().\n+        return false;\n+      }\n+    }\n+    \/\/ We're not 100% sure, so let's do the flattened_array_store_check.\n+    return true;\n+  }\n+  return false;\n+}\n+\n+bool LIRGenerator::needs_null_free_array_store_check(StoreIndexed* x) {\n+  return x->elt_type() == T_OBJECT && x->array()->maybe_null_free_array();\n+}\n+\n@@ -1656,0 +1856,2 @@\n+  assert(x->elt_type() != T_ARRAY, \"never used\");\n+  bool is_loaded_flattened_array = x->array()->is_loaded_flattened_array();\n@@ -1659,3 +1861,3 @@\n-  bool needs_store_check = obj_store && (x->value()->as_Constant() == NULL ||\n-                                         !get_jobject_constant(x->value())->is_null_object() ||\n-                                         x->should_profile());\n+  bool needs_store_check = obj_store && !(is_loaded_flattened_array && x->is_exact_flattened_array_store()) &&\n+                                        (x->value()->as_Constant() == NULL ||\n+                                         !get_jobject_constant(x->value())->is_null_object());\n@@ -1674,2 +1876,3 @@\n-\n-  if (needs_store_check || x->check_boolean()) {\n+\n+  if (needs_store_check || x->check_boolean()\n+      || is_loaded_flattened_array || needs_flattened_array_store_check(x) || needs_null_free_array_store_check(x)) {\n@@ -1704,0 +1907,16 @@\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flattened_array()) {\n+      \/\/ No need to profile a store to a flattened array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      ciMethodData* md = NULL;\n+      ciArrayLoadStoreData* load_store = NULL;\n+      profile_array_type(x, md, load_store);\n+      if (x->array()->maybe_null_free_array()) {\n+        profile_null_free_array(array, md, load_store);\n+      }\n+      profile_element_type(x->value(), md, load_store);\n+    }\n+  }\n+\n@@ -1706,1 +1925,1 @@\n-    array_store_check(value.result(), array.result(), store_check_info, x->profiled_method(), x->profiled_bci());\n+    array_store_check(value.result(), array.result(), store_check_info, NULL, -1);\n@@ -1709,4 +1928,26 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-  if (x->check_boolean()) {\n-    decorators |= C1_MASK_BOOLEAN;\n-  }\n+  if (is_loaded_flattened_array) {\n+    if (!x->value()->is_null_free()) {\n+      __ null_check(value.result(), new CodeEmitInfo(range_check_info));\n+    }\n+    \/\/ If array element is an empty inline type, no need to copy anything\n+    if (!x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+      access_flattened_array(false, array, index, value);\n+    }\n+  } else {\n+    StoreFlattenedArrayStub* slow_path = NULL;\n+\n+    if (needs_flattened_array_store_check(x)) {\n+      \/\/ Check if we indeed have a flattened array\n+      index.load_item();\n+      slow_path = new StoreFlattenedArrayStub(array.result(), index.result(), value.result(), state_for(x, x->state_before()));\n+      check_flattened_array(array.result(), value.result(), slow_path);\n+      set_in_conditional_code(true);\n+    } else if (needs_null_free_array_store_check(x)) {\n+      CodeEmitInfo* info = new CodeEmitInfo(range_check_info);\n+      check_null_free_array(array, value, info);\n+    }\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    if (x->check_boolean()) {\n+      decorators |= C1_MASK_BOOLEAN;\n+    }\n@@ -1714,2 +1955,7 @@\n-  access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n-                  NULL, null_check_info);\n+    access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n+                    NULL, null_check_info);\n+    if (slow_path != NULL) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+  }\n@@ -1805,0 +2051,25 @@\n+bool LIRGenerator::inline_type_field_access_prolog(AccessField* x, CodeEmitInfo* info) {\n+  ciField* field = x->field();\n+  assert(!field->is_flattened(), \"Flattened field access should have been expanded\");\n+  if (!field->signature()->is_Q_signature()) {\n+    return true; \/\/ Not an inline type field\n+  }\n+  \/\/ Deoptimize if the access is non-static and requires patching (holder not loaded\n+  \/\/ or not accessible) because then we only have partial field information and the\n+  \/\/ field could be flattened (see ciField constructor).\n+  bool could_be_flat = !x->is_static() && x->needs_patching();\n+  \/\/ Deoptimize if we load from a static field with an unloaded type because we need\n+  \/\/ the default value if the field is null.\n+  bool could_be_null = x->is_static() && x->as_LoadField() != NULL && !field->type()->is_loaded();\n+  assert(!could_be_null || !field->holder()->is_loaded(), \"inline type field should be loaded\");\n+  if (could_be_flat || could_be_null) {\n+    assert(x->needs_patching(), \"no deopt required\");\n+    CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),\n+                                        Deoptimization::Reason_unloaded,\n+                                        Deoptimization::Action_make_not_entrant);\n+    __ jump(stub);\n+    return false;\n+  }\n+  return true;\n+}\n+\n@@ -1834,0 +2105,7 @@\n+  if (!inline_type_field_access_prolog(x, info)) {\n+    \/\/ Field load will always deopt due to unloaded field or holder klass\n+    LIR_Opr result = rlock_result(x, field_type);\n+    __ move(LIR_OprFact::oopConst(NULL), result);\n+    return;\n+  }\n+\n@@ -1862,0 +2140,25 @@\n+\n+  ciField* field = x->field();\n+  if (field->signature()->is_Q_signature()) {\n+    \/\/ Load from non-flattened inline type field requires\n+    \/\/ a null check to replace null with the default value.\n+    ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+    assert(inline_klass->is_loaded(), \"field klass must be loaded\");\n+\n+    ciInstanceKlass* holder = field->holder();\n+    if (field->is_static() && holder->is_loaded()) {\n+      ciObject* val = holder->java_mirror()->field_value(field).as_object();\n+      if (!val->is_null_object()) {\n+        \/\/ Static field is initialized, we don need to perform a null check.\n+        return;\n+      }\n+    }\n+    LabelObj* L_end = new LabelObj();\n+    __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(NULL));\n+    __ branch(lir_cond_notEqual, L_end->label());\n+    set_in_conditional_code(true);\n+    Constant* default_value = new Constant(new InstanceConstant(inline_klass->default_instance()));\n+    __ move(load_constant(default_value), result);\n+    __ branch_destination(L_end->label());\n+    set_in_conditional_code(false);\n+  }\n@@ -1976,1 +2279,38 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  ciMethodData* md = NULL;\n+  ciArrayLoadStoreData* load_store = NULL;\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flattened_array()) {\n+      \/\/ No need to profile a load from a flattened array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      profile_array_type(x, md, load_store);\n+    }\n+  }\n+\n+  Value element;\n+  if (x->vt() != NULL) {\n+    assert(x->array()->is_loaded_flattened_array(), \"must be\");\n+    \/\/ Find the destination address (of the NewInlineTypeInstance).\n+    LIRItem obj_item(x->vt(), this);\n+\n+    access_flattened_array(true, array, index, obj_item,\n+                           x->delayed() == NULL ? 0 : x->delayed()->field(),\n+                           x->delayed() == NULL ? 0 : x->delayed()->offset());\n+    set_no_result(x);\n+  } else if (x->delayed() != NULL) {\n+    assert(x->array()->is_loaded_flattened_array(), \"must be\");\n+    LIR_Opr result = rlock_result(x, x->delayed()->field()->type()->basic_type());\n+    access_sub_element(array, index, result,\n+                       x->delayed() == NULL ? 0 : x->delayed()->field(),\n+                       x->delayed() == NULL ? 0 : x->delayed()->offset());\n+  } else if (x->array() != NULL && x->array()->is_loaded_flattened_array() &&\n+             x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+    \/\/ Load the default instance instead of reading the element\n+    ciInlineKlass* elem_klass = x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    Constant* default_value = new Constant(new InstanceConstant(elem_klass->default_instance()));\n+    __ move(load_constant(default_value), result);\n+  } else {\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    LoadFlattenedArrayStub* slow_path = NULL;\n@@ -1978,4 +2318,29 @@\n-  LIR_Opr result = rlock_result(x, x->elt_type());\n-  access_load_at(decorators, x->elt_type(),\n-                 array, index.result(), result,\n-                 NULL, null_check_info);\n+    if (x->should_profile() && x->array()->maybe_null_free_array()) {\n+      profile_null_free_array(array, md, load_store);\n+    }\n+\n+    if (x->elt_type() == T_OBJECT && x->array()->maybe_flattened_array()) {\n+      assert(x->delayed() == NULL, \"Delayed LoadIndexed only apply to loaded_flattened_arrays\");\n+      index.load_item();\n+      \/\/ if we are loading from flattened array, load it using a runtime call\n+      slow_path = new LoadFlattenedArrayStub(array.result(), index.result(), result, state_for(x, x->state_before()));\n+      check_flattened_array(array.result(), LIR_OprFact::illegalOpr, slow_path);\n+      set_in_conditional_code(true);\n+    }\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    access_load_at(decorators, x->elt_type(),\n+                   array, index.result(), result,\n+                   NULL, null_check_info);\n+\n+    if (slow_path != NULL) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+\n+    element = x;\n+  }\n+\n+  if (x->should_profile()) {\n+    profile_element_type(element, md, load_store);\n+  }\n@@ -1984,0 +2349,9 @@\n+void LIRGenerator::do_Deoptimize(Deoptimize* x) {\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+  CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),\n+                                      Deoptimization::Reason_unloaded,\n+                                      Deoptimization::Action_make_not_entrant);\n+  __ jump(stub);\n+  LIR_Opr reg = rlock_result(x, T_OBJECT);\n+  __ move(LIR_OprFact::oopConst(NULL), reg);\n+}\n@@ -2647,1 +3021,1 @@\n-  if (do_update) {\n+  if (do_update && signature_at_call_k != NULL) {\n@@ -2732,0 +3106,44 @@\n+void LIRGenerator::profile_flags(ciMethodData* md, ciProfileData* data, int flag, LIR_Condition condition) {\n+  assert(md != NULL && data != NULL, \"should have been initialized\");\n+  LIR_Opr mdp = new_register(T_METADATA);\n+  __ metadata2reg(md->constant_encoding(), mdp);\n+  LIR_Address* addr = new LIR_Address(mdp, md->byte_offset_of_slot(data, DataLayout::flags_offset()), T_BYTE);\n+  LIR_Opr flags = new_register(T_INT);\n+  __ move(addr, flags);\n+  if (condition != lir_cond_always) {\n+    LIR_Opr update = new_register(T_INT);\n+    __ cmove(condition, LIR_OprFact::intConst(0), LIR_OprFact::intConst(flag), update, T_INT);\n+  } else {\n+    __ logical_or(flags, LIR_OprFact::intConst(flag), flags);\n+  }\n+  __ store(flags, addr);\n+}\n+\n+void LIRGenerator::profile_null_free_array(LIRItem array, ciMethodData* md, ciArrayLoadStoreData* load_store) {\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+\n+  profile_flags(md, load_store, ArrayLoadStoreData::null_free_array_byte_constant(), lir_cond_equal);\n+}\n+\n+void LIRGenerator::profile_array_type(AccessIndexed* x, ciMethodData*& md, ciArrayLoadStoreData*& load_store) {\n+  int bci = x->profiled_bci();\n+  md = x->profiled_method()->method_data();\n+  assert(md != NULL, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(bci);\n+  assert(data != NULL && data->is_ArrayLoadStoreData(), \"incorrect profiling entry\");\n+  load_store = (ciArrayLoadStoreData*)data;\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_store, ArrayLoadStoreData::array_offset()), 0,\n+               load_store->array()->type(), x->array(), mdp, true, NULL, NULL);\n+}\n+\n+void LIRGenerator::profile_element_type(Value element, ciMethodData* md, ciArrayLoadStoreData* load_store) {\n+  assert(md != NULL && load_store != NULL, \"should have been initialized\");\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_store, ArrayLoadStoreData::element_offset()), 0,\n+               load_store->element()->type(), element, mdp, false, NULL, NULL);\n+}\n+\n+\n@@ -2816,0 +3234,8 @@\n+  if (method()->has_scalarized_args()) {\n+    \/\/ Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized inline type arguments\n+    \/\/ in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.\n+    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, 0), NULL, false);\n+    CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);\n+    __ append(new LIR_Op0(lir_check_orig_pc));\n+    __ branch(lir_cond_notEqual, deopt_stub);\n+  }\n@@ -2831,0 +3257,18 @@\n+void LIRGenerator::invoke_load_one_argument(LIRItem* param, LIR_Opr loc) {\n+  if (loc->is_register()) {\n+    param->load_item_force(loc);\n+  } else {\n+    LIR_Address* addr = loc->as_address_ptr();\n+    param->load_for_store(addr->type());\n+    assert(addr->type() != T_INLINE_TYPE, \"not supported yet\");\n+    if (addr->type() == T_OBJECT) {\n+      __ move_wide(param->result(), addr);\n+    } else {\n+      if (addr->type() == T_LONG || addr->type() == T_DOUBLE) {\n+        __ unaligned_move(param->result(), addr);\n+      } else {\n+        __ move(param->result(), addr);\n+      }\n+    }\n+  }\n+}\n@@ -2838,14 +3282,1 @@\n-    if (loc->is_register()) {\n-      param->load_item_force(loc);\n-    } else {\n-      LIR_Address* addr = loc->as_address_ptr();\n-      param->load_for_store(addr->type());\n-      if (addr->type() == T_OBJECT) {\n-        __ move_wide(param->result(), addr);\n-      } else\n-        if (addr->type() == T_LONG || addr->type() == T_DOUBLE) {\n-          __ unaligned_move(param->result(), addr);\n-        } else {\n-          __ move(param->result(), addr);\n-        }\n-    }\n+    invoke_load_one_argument(param, loc);\n@@ -3032,1 +3463,1 @@\n-  if (can_inline_as_constant(right.value())) {\n+  if (can_inline_as_constant(right.value()) && !x->substitutability_check()) {\n@@ -3035,0 +3466,1 @@\n+    \/\/ substitutability_check() needs to use right as a base register.\n@@ -3042,3 +3474,60 @@\n-  LIR_Opr reg = rlock_result(x);\n-  __ cmp(lir_cond(x->cond()), left.result(), right.result());\n-  __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  if (x->substitutability_check()) {\n+    substitutability_check(x, left, right, t_val, f_val);\n+  } else {\n+    LIR_Opr reg = rlock_result(x);\n+    __ cmp(lir_cond(x->cond()), left.result(), right.result());\n+    __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  }\n+}\n+\n+void LIRGenerator::substitutability_check(IfOp* x, LIRItem& left, LIRItem& right, LIRItem& t_val, LIRItem& f_val) {\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  bool is_acmpeq = (x->cond() == If::eql);\n+  LIR_Opr equal_result     = is_acmpeq ? t_val.result() : f_val.result();\n+  LIR_Opr not_equal_result = is_acmpeq ? f_val.result() : t_val.result();\n+  LIR_Opr result = rlock_result(x);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+}\n+\n+void LIRGenerator::substitutability_check(If* x, LIRItem& left, LIRItem& right) {\n+  LIR_Opr equal_result     = LIR_OprFact::intConst(1);\n+  LIR_Opr not_equal_result = LIR_OprFact::intConst(0);\n+  LIR_Opr result = new_register(T_INT);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  __ cmp(lir_cond(x->cond()), result, equal_result);\n+}\n+\n+void LIRGenerator::substitutability_check_common(Value left_val, Value right_val, LIRItem& left, LIRItem& right,\n+                                                 LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result,\n+                                                 CodeEmitInfo* info) {\n+  LIR_Opr tmp1 = LIR_OprFact::illegalOpr;\n+  LIR_Opr tmp2 = LIR_OprFact::illegalOpr;\n+  LIR_Opr left_klass_op = LIR_OprFact::illegalOpr;\n+  LIR_Opr right_klass_op = LIR_OprFact::illegalOpr;\n+\n+  ciKlass* left_klass  = left_val ->as_loaded_klass_or_null();\n+  ciKlass* right_klass = right_val->as_loaded_klass_or_null();\n+\n+  if ((left_klass == NULL || right_klass == NULL) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    init_temps_for_substitutability_check(tmp1, tmp2);\n+  }\n+\n+  if (left_klass != NULL && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+  } else {\n+    BasicType t_klass = UseCompressedOops ? T_INT : T_METADATA;\n+    left_klass_op = new_register(t_klass);\n+    right_klass_op = new_register(t_klass);\n+  }\n+\n+  CodeStub* slow_path = new SubstitutabilityCheckStub(left.result(), right.result(), info);\n+  __ substitutability_check(result, left.result(), right.result(), equal_result, not_equal_result,\n+                            tmp1, tmp2,\n+                            left_klass, right_klass, left_klass_op, right_klass_op, info, slow_path);\n@@ -3358,1 +3847,1 @@\n-    ciReturnTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n+    ciSingleTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n@@ -3379,0 +3868,47 @@\n+bool LIRGenerator::profile_inline_klass(ciMethodData* md, ciProfileData* data, Value value, int flag) {\n+  ciKlass* klass = value->as_loaded_klass_or_null();\n+  if (klass != NULL) {\n+    if (klass->is_inlinetype()) {\n+      profile_flags(md, data, flag, lir_cond_always);\n+    } else if (klass->can_be_inline_klass()) {\n+      return false;\n+    }\n+  } else {\n+    return false;\n+  }\n+  return true;\n+}\n+\n+\n+void LIRGenerator::do_ProfileACmpTypes(ProfileACmpTypes* x) {\n+  ciMethod* method = x->method();\n+  assert(method != NULL, \"method should be set if branch is profiled\");\n+  ciMethodData* md = method->method_data_or_null();\n+  assert(md != NULL, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(x->bci());\n+  assert(data != NULL, \"must have profiling data\");\n+  assert(data->is_ACmpData(), \"need BranchData for two-way branches\");\n+  ciACmpData* acmp = (ciACmpData*)data;\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()), 0,\n+               acmp->left()->type(), x->left(), mdp, !x->left_maybe_null(), NULL, NULL);\n+  int flags_offset = md->byte_offset_of_slot(data, DataLayout::flags_offset());\n+  if (!profile_inline_klass(md, acmp, x->left(), ACmpData::left_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->left(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::left_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()),\n+               in_bytes(ACmpData::right_offset()) - in_bytes(ACmpData::left_offset()),\n+               acmp->right()->type(), x->right(), mdp, !x->right_maybe_null(), NULL, NULL);\n+  if (!profile_inline_klass(md, acmp, x->right(), ACmpData::right_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->right(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::right_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":584,"deletions":48,"binary":false,"changes":632,"status":"modified"},{"patch":"@@ -55,0 +55,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -126,0 +128,1 @@\n+int Runtime1::_new_flat_array_slowcase_cnt = 0;\n@@ -128,0 +131,5 @@\n+int Runtime1::_load_flattened_array_slowcase_cnt = 0;\n+int Runtime1::_store_flattened_array_slowcase_cnt = 0;\n+int Runtime1::_substitutability_check_slowcase_cnt = 0;\n+int Runtime1::_buffer_inline_args_slowcase_cnt = 0;\n+int Runtime1::_buffer_inline_args_no_receiver_slowcase_cnt = 0;\n@@ -137,0 +145,1 @@\n+int Runtime1::_throw_illegal_monitor_state_exception_count = 0;\n@@ -358,4 +367,1 @@\n-\n-JRT_ENTRY(void, Runtime1::new_instance(JavaThread* thread, Klass* klass))\n-  NOT_PRODUCT(_new_instance_slowcase_cnt++;)\n-\n+static void allocate_instance(JavaThread* thread, Klass* klass, TRAPS) {\n@@ -371,0 +377,5 @@\n+}\n+\n+JRT_ENTRY(void, Runtime1::new_instance(JavaThread* thread, Klass* klass))\n+  NOT_PRODUCT(_new_instance_slowcase_cnt++;)\n+  allocate_instance(thread, klass, CHECK);\n@@ -373,0 +384,9 @@\n+\/\/ Same as new_instance but throws error for inline klasses\n+JRT_ENTRY(void, Runtime1::new_instance_no_inline(JavaThread* thread, Klass* klass))\n+  NOT_PRODUCT(_new_instance_slowcase_cnt++;)\n+  if (klass->is_inline_klass()) {\n+    SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_InstantiationError());\n+  } else {\n+    allocate_instance(thread, klass, CHECK);\n+  }\n+JRT_END\n@@ -400,1 +420,1 @@\n-  Klass* elem_klass = ObjArrayKlass::cast(array_klass)->element_klass();\n+  Klass* elem_klass = ArrayKlass::cast(array_klass)->element_klass();\n@@ -411,0 +431,22 @@\n+JRT_ENTRY(void, Runtime1::new_flat_array(JavaThread* thread, Klass* array_klass, jint length))\n+  NOT_PRODUCT(_new_flat_array_slowcase_cnt++;)\n+\n+  \/\/ Note: no handle for klass needed since they are not used\n+  \/\/       anymore after new_objArray() and no GC can happen before.\n+  \/\/       (This may have to change if this code changes!)\n+  assert(array_klass->is_klass(), \"not a class\");\n+  Handle holder(THREAD, array_klass->klass_holder()); \/\/ keep the klass alive\n+  Klass* elem_klass = ArrayKlass::cast(array_klass)->element_klass();\n+  assert(elem_klass->is_inline_klass(), \"must be\");\n+  \/\/ Logically creates elements, ensure klass init\n+  elem_klass->initialize(CHECK);\n+  arrayOop obj = oopFactory::new_flatArray(elem_klass, length, CHECK);\n+  thread->set_vm_result(obj);\n+  \/\/ This is pretty rare but this runtime patch is stressful to deoptimization\n+  \/\/ if we deoptimize here so force a deopt to stress the path.\n+  if (DeoptimizeALot) {\n+    deopt_caller();\n+  }\n+JRT_END\n+\n+\n@@ -422,0 +464,77 @@\n+static void profile_flat_array(JavaThread* thread) {\n+  ResourceMark rm(thread);\n+  vframeStream vfst(thread, true);\n+  assert(!vfst.at_end(), \"Java frame must exist\");\n+  int bci = vfst.bci();\n+  Method* method = vfst.method();\n+  MethodData* md = method->method_data();\n+  if (md != NULL) {\n+    ProfileData* data = md->bci_to_data(bci);\n+    assert(data != NULL && data->is_ArrayLoadStoreData(), \"incorrect profiling entry\");\n+    ArrayLoadStoreData* load_store = (ArrayLoadStoreData*)data;\n+    load_store->set_flat_array();\n+  }\n+}\n+\n+JRT_ENTRY(void, Runtime1::load_flattened_array(JavaThread* thread, flatArrayOopDesc* array, int index))\n+  assert(array->klass()->is_flatArray_klass(), \"should not be called\");\n+  profile_flat_array(thread);\n+\n+  NOT_PRODUCT(_load_flattened_array_slowcase_cnt++;)\n+  assert(array->length() > 0 && index < array->length(), \"already checked\");\n+  flatArrayHandle vah(thread, array);\n+  oop obj = flatArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK);\n+  thread->set_vm_result(obj);\n+JRT_END\n+\n+\n+JRT_ENTRY(void, Runtime1::store_flattened_array(JavaThread* thread, flatArrayOopDesc* array, int index, oopDesc* value))\n+  if (array->klass()->is_flatArray_klass()) {\n+    profile_flat_array(thread);\n+  }\n+\n+  NOT_PRODUCT(_store_flattened_array_slowcase_cnt++;)\n+  if (value == NULL) {\n+    assert(array->klass()->is_flatArray_klass() || array->klass()->is_null_free_array_klass(), \"should not be called\");\n+    SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_NullPointerException());\n+  } else {\n+    assert(array->klass()->is_flatArray_klass(), \"should not be called\");\n+    array->value_copy_to_index(value, index);\n+  }\n+JRT_END\n+\n+\n+JRT_ENTRY(int, Runtime1::substitutability_check(JavaThread* thread, oopDesc* left, oopDesc* right))\n+  NOT_PRODUCT(_substitutability_check_slowcase_cnt++;)\n+  JavaCallArguments args;\n+  args.push_oop(Handle(THREAD, left));\n+  args.push_oop(Handle(THREAD, right));\n+  JavaValue result(T_BOOLEAN);\n+  JavaCalls::call_static(&result,\n+                         vmClasses::ValueBootstrapMethods_klass(),\n+                         vmSymbols::isSubstitutable_name(),\n+                         vmSymbols::object_object_boolean_signature(),\n+                         &args, CHECK_0);\n+  return result.get_jboolean() ? 1 : 0;\n+JRT_END\n+\n+\n+extern \"C\" void ps();\n+\n+void Runtime1::buffer_inline_args_impl(JavaThread* thread, Method* m, bool allocate_receiver) {\n+  Thread* THREAD = thread;\n+  methodHandle method(thread, m); \/\/ We are inside the verified_entry or verified_inline_ro_entry of this method.\n+  oop obj = SharedRuntime::allocate_inline_types_impl(thread, method, allocate_receiver, CHECK);\n+  thread->set_vm_result(obj);\n+}\n+\n+JRT_ENTRY(void, Runtime1::buffer_inline_args(JavaThread* thread, Method* method))\n+  NOT_PRODUCT(_buffer_inline_args_slowcase_cnt++;)\n+  buffer_inline_args_impl(thread, method, true);\n+JRT_END\n+\n+JRT_ENTRY(void, Runtime1::buffer_inline_args_no_receiver(JavaThread* thread, Method* method))\n+  NOT_PRODUCT(_buffer_inline_args_no_receiver_slowcase_cnt++;)\n+  buffer_inline_args_impl(thread, method, false);\n+JRT_END\n+\n@@ -711,0 +830,7 @@\n+JRT_ENTRY(void, Runtime1::throw_illegal_monitor_state_exception(JavaThread* thread))\n+  NOT_PRODUCT(_throw_illegal_monitor_state_exception_count++;)\n+  ResourceMark rm(thread);\n+  SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_IllegalMonitorStateException());\n+JRT_END\n+\n+\n@@ -913,0 +1039,1 @@\n+    assert(!result.is_inlined(), \"Can not patch access to flattened field\");\n@@ -955,0 +1082,5 @@\n+      case Bytecodes::_defaultvalue:\n+        { Bytecode_defaultvalue bdefaultvalue(caller_method(), caller_method->bcp_from(bci));\n+          k = caller_method->constants()->klass_at(bdefaultvalue.index(), CHECK);\n+        }\n+        break;\n@@ -958,0 +1090,4 @@\n+          if (k->name()->is_Q_array_signature()) {\n+            \/\/ Logically creates elements, ensure klass init\n+            k->initialize(CHECK);\n+          }\n@@ -1467,0 +1603,1 @@\n+  tty->print_cr(\" _new_flat_array_slowcase_cnt:    %d\", _new_flat_array_slowcase_cnt);\n@@ -1469,0 +1606,6 @@\n+  tty->print_cr(\" _load_flattened_array_slowcase_cnt:   %d\", _load_flattened_array_slowcase_cnt);\n+  tty->print_cr(\" _store_flattened_array_slowcase_cnt:  %d\", _store_flattened_array_slowcase_cnt);\n+  tty->print_cr(\" _substitutability_check_slowcase_cnt: %d\", _substitutability_check_slowcase_cnt);\n+  tty->print_cr(\" _buffer_inline_args_slowcase_cnt:%d\", _buffer_inline_args_slowcase_cnt);\n+  tty->print_cr(\" _buffer_inline_args_no_receiver_slowcase_cnt:%d\", _buffer_inline_args_no_receiver_slowcase_cnt);\n+\n@@ -1479,0 +1622,1 @@\n+  tty->print_cr(\" _throw_illegal_monitor_state_exception_count:  %d:\", _throw_illegal_monitor_state_exception_count);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":149,"deletions":5,"binary":false,"changes":154,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -464,1 +465,3 @@\n-      (sym->char_at(1) == JVM_SIGNATURE_ARRAY || sym->char_at(1) == JVM_SIGNATURE_CLASS)) {\n+      (sym->char_at(1) == JVM_SIGNATURE_ARRAY ||\n+       sym->char_at(1) == JVM_SIGNATURE_CLASS ||\n+       sym->char_at(1) == JVM_SIGNATURE_INLINE_TYPE )) {\n@@ -477,1 +480,1 @@\n-      return ciObjArrayKlass::make_impl(elem_klass);\n+      return ciArrayKlass::make(elem_klass);\n@@ -503,0 +506,15 @@\n+  int i = 0;\n+  while (sym->char_at(i) == JVM_SIGNATURE_ARRAY) {\n+    i++;\n+  }\n+  if (i > 0 && sym->char_at(i) == JVM_SIGNATURE_INLINE_TYPE) {\n+    \/\/ An unloaded array class of inline types is an ObjArrayKlass, an\n+    \/\/ unloaded inline type class is an InstanceKlass. For consistency,\n+    \/\/ make the signature of the unloaded array of inline type use L\n+    \/\/ rather than Q.\n+    char *new_name = CURRENT_THREAD_ENV->name_buffer(sym->utf8_length()+1);\n+    strncpy(new_name, (char*)sym->base(), sym->utf8_length());\n+    new_name[i] = JVM_SIGNATURE_CLASS;\n+    new_name[sym->utf8_length()] = '\\0';\n+    return get_unloaded_klass(accessing_klass, ciSymbol::make(new_name));\n+  }\n@@ -533,1 +551,1 @@\n-    klass =  ConstantPool::klass_at_if_loaded(cpool, index);\n+    klass = ConstantPool::klass_at_if_loaded(cpool, index);\n@@ -585,0 +603,8 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciEnv::is_inline_klass\n+\/\/\n+\/\/ Check if the klass is an inline klass.\n+bool ciEnv::is_inline_klass(const constantPoolHandle& cpool, int index) {\n+  GUARDED_VM_ENTRY(return cpool->klass_name_at(index)->is_Q_signature();)\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":29,"deletions":3,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -137,0 +137,2 @@\n+  bool       is_inline_klass(const constantPoolHandle& cpool,\n+                             int klass_index);\n@@ -201,0 +203,4 @@\n+  ciFlatArrayKlass* get_flat_array_klass(Klass* o) {\n+    if (o == NULL) return NULL;\n+    return get_metadata(o)->as_flat_array_klass();\n+  }\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-    _known_to_link_with_put(NULL), _known_to_link_with_get(NULL) {\n+  _is_flattened(false), _known_to_link_with_put(NULL), _known_to_link_with_get(NULL) {\n@@ -219,0 +219,23 @@\n+\/\/ Special copy constructor used to flatten inline type fields by\n+\/\/ copying the fields of the inline type to a new holder klass.\n+ciField::ciField(ciField* field, ciInstanceKlass* holder, int offset, bool is_final) {\n+  assert(field->holder()->is_inlinetype(), \"should only be used for inline type field flattening\");\n+  \/\/ Set the is_final flag\n+  jint final = is_final ? JVM_ACC_FINAL : ~JVM_ACC_FINAL;\n+  AccessFlags flags(field->flags().as_int() & final);\n+  _flags = ciFlags(flags);\n+  _holder = holder;\n+  _offset = offset;\n+  \/\/ Copy remaining fields\n+  _name = field->_name;\n+  _signature = field->_signature;\n+  _type = field->_type;\n+  \/\/ Trust final flattened fields\n+  _is_constant = is_final;\n+  _known_to_link_with_put = field->_known_to_link_with_put;\n+  _known_to_link_with_get = field->_known_to_link_with_get;\n+  _constant_value = field->_constant_value;\n+  assert(!field->is_flattened(), \"field must not be flattened\");\n+  _is_flattened = false;\n+}\n+\n@@ -236,0 +259,3 @@\n+  \/\/ Trust final fields in inline type buffers\n+  if (holder->is_inlinetype())\n+    return true;\n@@ -263,0 +289,1 @@\n+  _is_flattened = fd->is_inlined();\n@@ -375,2 +402,2 @@\n-         bc == Bytecodes::_getfield  || bc == Bytecodes::_putfield,\n-         \"unexpected bytecode\");\n+         bc == Bytecodes::_getfield  || bc == Bytecodes::_putfield  ||\n+         bc == Bytecodes::_withfield, \"unexpected bytecode\");\n@@ -459,0 +486,1 @@\n+  tty->print(\" is_flattened=%s\", bool_to_str(_is_flattened));\n","filename":"src\/hotspot\/share\/ci\/ciField.cpp","additions":31,"deletions":3,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+  bool has_vararg              () const { return (_flags & JVM_ACC_VARARGS                   ) != 0; }\n","filename":"src\/hotspot\/share\/ci\/ciFlags.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+    case T_INLINE_TYPE:  \/\/ fall through\n@@ -103,1 +104,2 @@\n-  assert(field->is_static() || klass()->is_subclass_of(field->holder()), \"invalid access - must be subclass\");\n+  assert(field->is_static() || field->holder()->is_inlinetype() || klass()->is_subclass_of(field->holder()),\n+         \"invalid access - must be subclass\");\n","filename":"src\/hotspot\/share\/ci\/ciInstance.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -39,0 +41,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -121,2 +124,3 @@\n-                                 jobject loader, jobject protection_domain)\n-  : ciKlass(name, T_OBJECT)\n+                                 jobject loader, jobject protection_domain,\n+                                 BasicType bt)\n+  : ciKlass(name, bt)\n@@ -128,1 +132,1 @@\n-  _nonstatic_fields = NULL;\n+  _nonstatic_fields = NULL;            \/\/ initialized lazily by compute_nonstatic_fields\n@@ -338,1 +342,1 @@\n-    _flags.print_klass_flags();\n+    _flags.print_klass_flags(st);\n@@ -342,1 +346,1 @@\n-      _super->print_name();\n+      _super->print_name_on(st);\n@@ -438,0 +442,23 @@\n+ciField* ciInstanceKlass::get_non_flattened_field_by_offset(int field_offset) {\n+  if (super() != NULL && super()->has_nonstatic_fields()) {\n+    ciField* f = super()->get_non_flattened_field_by_offset(field_offset);\n+    if (f != NULL) {\n+      return f;\n+    }\n+  }\n+\n+  VM_ENTRY_MARK;\n+  InstanceKlass* k = get_instanceKlass();\n+  Arena* arena = CURRENT_ENV->arena();\n+  for (JavaFieldStream fs(k); !fs.done(); fs.next()) {\n+    if (fs.access_flags().is_static())  continue;\n+    fieldDescriptor& fd = fs.field_descriptor();\n+    if (fd.offset() == field_offset) {\n+      ciField* f = new (arena) ciField(&fd);\n+      return f;\n+    }\n+  }\n+\n+  return NULL;\n+}\n+\n@@ -499,6 +526,1 @@\n-  int flen = fields->length();\n-\n-  \/\/ Now sort them by offset, ascending.\n-  \/\/ (In principle, they could mix with superclass fields.)\n-  fields->sort(sort_field_by_offset);\n-  return flen;\n+  return fields->length();\n@@ -508,3 +530,1 @@\n-GrowableArray<ciField*>*\n-ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>*\n-                                               super_fields) {\n+GrowableArray<ciField*>* ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields, bool flatten) {\n@@ -528,0 +548,1 @@\n+\n@@ -536,2 +557,22 @@\n-    ciField* field = new (arena) ciField(&fd);\n-    fields->append(field);\n+    if (fd.is_inlined() && flatten) {\n+      \/\/ Inline type fields are embedded\n+      int field_offset = fd.offset();\n+      \/\/ Get InlineKlass and adjust number of fields\n+      Klass* k = get_instanceKlass()->get_inline_type_field_klass(fd.index());\n+      ciInlineKlass* vk = CURRENT_ENV->get_klass(k)->as_inline_klass();\n+      flen += vk->nof_nonstatic_fields() - 1;\n+      \/\/ Iterate over fields of the flattened inline type and copy them to 'this'\n+      for (int i = 0; i < vk->nof_nonstatic_fields(); ++i) {\n+        ciField* flattened_field = vk->nonstatic_field_at(i);\n+        \/\/ Adjust offset to account for missing oop header\n+        int offset = field_offset + (flattened_field->offset() - vk->first_field_offset());\n+        \/\/ A flattened field can be treated as final if the non-flattened\n+        \/\/ field is declared final or the holder klass is an inline type itself.\n+        bool is_final = fd.is_final() || is_inlinetype();\n+        ciField* field = new (arena) ciField(flattened_field, this, offset, is_final);\n+        fields->append(field);\n+      }\n+    } else {\n+      ciField* field = new (arena) ciField(&fd);\n+      fields->append(field);\n+    }\n@@ -540,0 +581,3 @@\n+  \/\/ Now sort them by offset, ascending.\n+  \/\/ (In principle, they could mix with superclass fields.)\n+  fields->sort(sort_field_by_offset);\n@@ -637,0 +681,16 @@\n+bool ciInstanceKlass::can_be_inline_klass(bool is_exact) {\n+  if (!EnableValhalla) {\n+    return false;\n+  }\n+  if (!is_loaded() || is_inlinetype()) {\n+    \/\/ Not loaded or known to be an inline klass\n+    return true;\n+  }\n+  if (!is_exact) {\n+    \/\/ Not exact, check if this is a valid super for an inline klass\n+    VM_ENTRY_MARK;\n+    return !get_instanceKlass()->invalid_inline_super();\n+  }\n+  return false;\n+}\n+\n@@ -655,1 +715,2 @@\n-class StaticFinalFieldPrinter : public FieldClosure {\n+class StaticFieldPrinter : public FieldClosure {\n+protected:\n@@ -657,0 +718,8 @@\n+public:\n+  StaticFieldPrinter(outputStream* out) :\n+    _out(out) {\n+  }\n+  void do_field_helper(fieldDescriptor* fd, oop obj, bool flattened);\n+};\n+\n+class StaticFinalFieldPrinter : public StaticFieldPrinter {\n@@ -660,2 +729,1 @@\n-    _out(out),\n-    _holder(holder) {\n+    StaticFieldPrinter(out), _holder(holder) {\n@@ -666,46 +734,58 @@\n-      oop mirror = fd->field_holder()->java_mirror();\n-      _out->print(\"staticfield %s %s %s \", _holder, fd->name()->as_quoted_ascii(), fd->signature()->as_quoted_ascii());\n-      switch (fd->field_type()) {\n-        case T_BYTE:    _out->print_cr(\"%d\", mirror->byte_field(fd->offset()));   break;\n-        case T_BOOLEAN: _out->print_cr(\"%d\", mirror->bool_field(fd->offset()));   break;\n-        case T_SHORT:   _out->print_cr(\"%d\", mirror->short_field(fd->offset()));  break;\n-        case T_CHAR:    _out->print_cr(\"%d\", mirror->char_field(fd->offset()));   break;\n-        case T_INT:     _out->print_cr(\"%d\", mirror->int_field(fd->offset()));    break;\n-        case T_LONG:    _out->print_cr(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n-        case T_FLOAT: {\n-          float f = mirror->float_field(fd->offset());\n-          _out->print_cr(\"%d\", *(int*)&f);\n-          break;\n-        }\n-        case T_DOUBLE: {\n-          double d = mirror->double_field(fd->offset());\n-          _out->print_cr(INT64_FORMAT, *(int64_t*)&d);\n-          break;\n-        }\n-        case T_ARRAY:  \/\/ fall-through\n-        case T_OBJECT: {\n-          oop value =  mirror->obj_field_acquire(fd->offset());\n-          if (value == NULL) {\n-            _out->print_cr(\"null\");\n-          } else if (value->is_instance()) {\n-            assert(fd->field_type() == T_OBJECT, \"\");\n-            if (value->is_a(vmClasses::String_klass())) {\n-              const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n-              _out->print(\"\\\"%s\\\"\", (ascii_value != NULL) ? ascii_value : \"\");\n-            } else {\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print_cr(\"%s\", klass_name);\n-            }\n-          } else if (value->is_array()) {\n-            typeArrayOop ta = (typeArrayOop)value;\n-            _out->print(\"%d\", ta->length());\n-            if (value->is_objArray()) {\n-              objArrayOop oa = (objArrayOop)value;\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print(\" %s\", klass_name);\n-            }\n-            _out->cr();\n-          } else {\n-            ShouldNotReachHere();\n-          }\n-          break;\n+      InstanceKlass* holder = fd->field_holder();\n+      oop mirror = holder->java_mirror();\n+      _out->print(\"staticfield %s %s \", _holder, fd->name()->as_quoted_ascii());\n+      BasicType bt = fd->field_type();\n+      if (bt != T_OBJECT && bt != T_ARRAY) {\n+        _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+      }\n+      do_field_helper(fd, mirror, false);\n+      _out->cr();\n+    }\n+  }\n+};\n+\n+class InlineTypeFieldPrinter : public StaticFieldPrinter {\n+  oop _obj;\n+public:\n+  InlineTypeFieldPrinter(outputStream* out, oop obj) :\n+    StaticFieldPrinter(out), _obj(obj) {\n+  }\n+  void do_field(fieldDescriptor* fd) {\n+    do_field_helper(fd, _obj, true);\n+    _out->print(\" \");\n+  }\n+};\n+\n+void StaticFieldPrinter::do_field_helper(fieldDescriptor* fd, oop mirror, bool flattened) {\n+  BasicType bt = fd->field_type();\n+  switch (bt) {\n+    case T_BYTE:    _out->print(\"%d\", mirror->byte_field(fd->offset()));   break;\n+    case T_BOOLEAN: _out->print(\"%d\", mirror->bool_field(fd->offset()));   break;\n+    case T_SHORT:   _out->print(\"%d\", mirror->short_field(fd->offset()));  break;\n+    case T_CHAR:    _out->print(\"%d\", mirror->char_field(fd->offset()));   break;\n+    case T_INT:     _out->print(\"%d\", mirror->int_field(fd->offset()));    break;\n+    case T_LONG:    _out->print(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n+    case T_FLOAT: {\n+      float f = mirror->float_field(fd->offset());\n+      _out->print(\"%d\", *(int*)&f);\n+      break;\n+    }\n+    case T_DOUBLE: {\n+      double d = mirror->double_field(fd->offset());\n+      _out->print(INT64_FORMAT, *(int64_t*)&d);\n+      break;\n+    }\n+    case T_ARRAY:  \/\/ fall-through\n+    case T_OBJECT: {\n+      _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+      oop value =  mirror->obj_field_acquire(fd->offset());\n+      if (value == NULL) {\n+        _out->print_cr(\"null\");\n+      } else if (value->is_instance()) {\n+        assert(fd->field_type() == T_OBJECT, \"\");\n+        if (value->is_a(vmClasses::String_klass())) {\n+          const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n+          _out->print(\"\\\"%s\\\"\", (ascii_value != NULL) ? ascii_value : \"\");\n+         } else {\n+          const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+          _out->print(\"%s\", klass_name);\n@@ -713,2 +793,7 @@\n-        default:\n-          ShouldNotReachHere();\n+      } else if (value->is_array()) {\n+        typeArrayOop ta = (typeArrayOop)value;\n+        _out->print(\"%d\", ta->length());\n+        if (value->is_objArray() || value->is_flatArray()) {\n+          objArrayOop oa = (objArrayOop)value;\n+          const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+          _out->print(\" %s\", klass_name);\n@@ -716,0 +801,4 @@\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+      break;\n@@ -717,0 +806,24 @@\n+    case T_INLINE_TYPE: {\n+      ResetNoHandleMark rnhm;\n+      Thread* THREAD = Thread::current();\n+      SignatureStream ss(fd->signature(), false);\n+      Symbol* name = ss.as_symbol();\n+      assert(!HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+      InstanceKlass* holder = fd->field_holder();\n+      Klass* k = SystemDictionary::find(name, Handle(THREAD, holder->class_loader()),\n+                                        Handle(THREAD, holder->protection_domain()), THREAD);\n+      assert(k != NULL && !HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+      InlineKlass* vk = InlineKlass::cast(k);\n+      oop obj;\n+      if (flattened) {\n+        int field_offset = fd->offset() - vk->first_field_offset();\n+        obj = (oop) (cast_from_oop<address>(mirror) + field_offset);\n+      } else {\n+        obj =  mirror->obj_field_acquire(fd->offset());\n+      }\n+      InlineTypeFieldPrinter print_field(_out, obj);\n+      vk->do_nonstatic_fields(&print_field);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n@@ -718,1 +831,1 @@\n-};\n+}\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.cpp","additions":181,"deletions":68,"binary":false,"changes":249,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+\n@@ -86,1 +87,1 @@\n-  ciInstanceKlass(ciSymbol* name, jobject loader, jobject protection_domain);\n+  ciInstanceKlass(ciSymbol* name, jobject loader, jobject protection_domain, BasicType bt = T_OBJECT); \/\/ for unloaded klasses\n@@ -110,2 +111,2 @@\n-  int  compute_nonstatic_fields();\n-  GrowableArray<ciField*>* compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields);\n+  virtual int compute_nonstatic_fields();\n+  GrowableArray<ciField*>* compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields, bool flatten = true);\n@@ -212,0 +213,2 @@\n+  \/\/ get field descriptor at field_offset ignoring flattening\n+  ciField* get_non_flattened_field_by_offset(int field_offset);\n@@ -215,1 +218,1 @@\n-    if (_nonstatic_fields == NULL)\n+    if (_nonstatic_fields == NULL) {\n@@ -217,1 +220,1 @@\n-    else\n+    } else {\n@@ -219,0 +222,1 @@\n+    }\n@@ -266,0 +270,2 @@\n+  virtual bool can_be_inline_klass(bool is_exact = false);\n+\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.hpp","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -654,0 +655,33 @@\n+bool ciMethod::array_access_profiled_type(int bci, ciKlass*& array_type, ciKlass*& element_type, ProfilePtrKind& element_ptr, bool &flat_array, bool &null_free_array) {\n+  if (method_data() != NULL && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != NULL && data->is_ArrayLoadStoreData()) {\n+      ciArrayLoadStoreData* array_access = (ciArrayLoadStoreData*)data->as_ArrayLoadStoreData();\n+      array_type = array_access->array()->valid_type();\n+      element_type = array_access->element()->valid_type();\n+      element_ptr = array_access->element()->ptr_kind();\n+      flat_array = array_access->flat_array();\n+      null_free_array = array_access->null_free_array();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ciMethod::acmp_profiled_type(int bci, ciKlass*& left_type, ciKlass*& right_type, ProfilePtrKind& left_ptr, ProfilePtrKind& right_ptr, bool &left_inline_type, bool &right_inline_type) {\n+  if (method_data() != NULL && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != NULL && data->is_ACmpData()) {\n+      ciACmpData* acmp = (ciACmpData*)data->as_ACmpData();\n+      left_type = acmp->left()->valid_type();\n+      right_type = acmp->right()->valid_type();\n+      left_ptr = acmp->left()->ptr_kind();\n+      right_ptr = acmp->right()->ptr_kind();\n+      left_inline_type = acmp->left_inline_type();\n+      right_inline_type = acmp->right_inline_type();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -934,1 +968,1 @@\n-\/\/ ciMethod::is_object_initializer\n+\/\/ ciMethod::is_object_constructor\n@@ -936,2 +970,15 @@\n-bool ciMethod::is_object_initializer() const {\n-   return name() == ciSymbols::object_initializer_name();\n+bool ciMethod::is_object_constructor() const {\n+   return (name() == ciSymbols::object_initializer_name()\n+           && signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciMethod::is_static_init_factory\n+\/\/\n+bool ciMethod::is_static_init_factory() const {\n+   return (name() == ciSymbols::object_initializer_name()\n+           && !signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n@@ -1217,1 +1264,1 @@\n-bool ciMethod::is_initializer () const {         FETCH_FLAG_FROM_VM(is_initializer); }\n+bool ciMethod::is_object_constructor_or_class_initializer() const { FETCH_FLAG_FROM_VM(is_object_constructor_or_class_initializer); }\n@@ -1368,0 +1415,1 @@\n+  if (bt == T_INLINE_TYPE)   return T_OBJECT;\n@@ -1455,0 +1503,13 @@\n+\n+bool ciMethod::has_scalarized_args() const {\n+  VM_ENTRY_MARK;\n+  return get_Method()->has_scalarized_args();\n+}\n+\n+const GrowableArray<SigEntry>* ciMethod::get_sig_cc() {\n+  VM_ENTRY_MARK;\n+  if (get_Method()->adapter() == NULL) {\n+    return NULL;\n+  }\n+  return get_Method()->adapter()->get_sig_cc();\n+}\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":65,"deletions":4,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+  ProfileUnknownNull,\n@@ -205,1 +206,1 @@\n-  bool is_static_initializer() const { return get_Method()->is_static_initializer(); }\n+  bool is_class_initializer()  const { return get_Method()->is_class_initializer(); }\n@@ -265,1 +266,4 @@\n-\n+  bool          array_access_profiled_type(int bci, ciKlass*& array_type, ciKlass*& element_type, ProfilePtrKind& element_ptr, bool &flat_array, bool &null_free);\n+  bool          acmp_profiled_type(int bci, ciKlass*& left_type, ciKlass*& right_type,\n+                                   ProfilePtrKind& left_ptr, ProfilePtrKind& right_ptr,\n+                                   bool &left_inline_type, bool &right_inline_type);\n@@ -337,0 +341,1 @@\n+  bool has_vararg     () const                   { return flags().has_vararg(); }\n@@ -348,1 +353,0 @@\n-  bool is_initializer () const;\n@@ -353,0 +357,3 @@\n+  bool is_object_constructor() const;\n+  bool is_static_init_factory() const;\n+  bool is_object_constructor_or_class_initializer() const;\n@@ -354,1 +361,0 @@\n-  bool is_object_initializer() const;\n@@ -374,0 +380,4 @@\n+\n+  \/\/ Support for the inline type calling convention\n+  bool has_scalarized_args() const;\n+  const GrowableArray<SigEntry>* get_sig_cc();\n","filename":"src\/hotspot\/share\/ci\/ciMethod.hpp","additions":14,"deletions":4,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -69,1 +69,2 @@\n-           _base_element_klass->is_type_array_klass(), \"bad base klass\");\n+           _base_element_klass->is_type_array_klass() ||\n+           _base_element_klass->is_flat_array_klass(), \"bad base klass\");\n@@ -119,1 +120,1 @@\n-\n+  assert(base_name_sym->char_at(0) != JVM_SIGNATURE_INLINE_TYPE, \"unloaded array klass element should not have Q-type\");\n@@ -138,1 +139,0 @@\n-\n@@ -151,2 +151,1 @@\n-  \/\/ The array klass was unable to be made or the element klass was\n-  \/\/ not loaded.\n+  \/\/ The array klass was unable to be made or the element klass was not loaded.\n","filename":"src\/hotspot\/share\/ci\/ciObjArrayKlass.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+#include \"ci\/ciFlatArray.hpp\"\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -45,0 +47,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -361,0 +364,3 @@\n+  } else if (o->is_flatArray()) {\n+    flatArrayHandle h_ta(THREAD, (flatArrayOop)o);\n+    return new (arena()) ciFlatArray(h_ta);\n@@ -380,1 +386,3 @@\n-    if (k->is_instance_klass()) {\n+    if (k->is_inline_klass()) {\n+      return new (arena()) ciInlineKlass(k);\n+    } else if (k->is_instance_klass()) {\n@@ -382,0 +390,2 @@\n+    } else if (k->is_flatArray_klass()) {\n+      return new (arena()) ciFlatArrayKlass(k);\n@@ -491,1 +501,1 @@\n-    if (element_type == T_OBJECT) {\n+    if (element_type == T_OBJECT || element_type == T_INLINE_TYPE) {\n","filename":"src\/hotspot\/share\/ci\/ciObjectFactory.cpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -739,0 +740,1 @@\n+\n@@ -785,26 +787,79 @@\n-  \/\/ staticfield <klass> <name> <signature> <value>\n-  \/\/\n-  \/\/ Initialize a class and fill in the value for a static field.\n-  \/\/ This is useful when the compile was dependent on the value of\n-  \/\/ static fields but it's impossible to properly rerun the static\n-  \/\/ initializer.\n-  void process_staticfield(TRAPS) {\n-    InstanceKlass* k = (InstanceKlass *)parse_klass(CHECK);\n-\n-    if (k == NULL || ReplaySuppressInitializers == 0 ||\n-        (ReplaySuppressInitializers == 2 && k->class_loader() == NULL)) {\n-      return;\n-    }\n-\n-    assert(k->is_initialized(), \"must be\");\n-\n-    const char* field_name = parse_escaped_string();\n-    const char* field_signature = parse_string();\n-    fieldDescriptor fd;\n-    Symbol* name = SymbolTable::new_symbol(field_name);\n-    Symbol* sig = SymbolTable::new_symbol(field_signature);\n-    if (!k->find_local_field(name, sig, &fd) ||\n-        !fd.is_static() ||\n-        fd.has_initial_value()) {\n-      report_error(field_name);\n-      return;\n+  class InlineTypeFieldInitializer : public FieldClosure {\n+    oop _vt;\n+    CompileReplay* _replay;\n+  public:\n+    InlineTypeFieldInitializer(oop vt, CompileReplay* replay)\n+  : _vt(vt), _replay(replay) {}\n+\n+    void do_field(fieldDescriptor* fd) {\n+      BasicType bt = fd->field_type();\n+      const char* string_value = bt != T_INLINE_TYPE ? _replay->parse_escaped_string() : NULL;\n+      switch (bt) {\n+      case T_BYTE: {\n+        int value = atoi(string_value);\n+        _vt->byte_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_BOOLEAN: {\n+        int value = atoi(string_value);\n+        _vt->bool_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_SHORT: {\n+        int value = atoi(string_value);\n+        _vt->short_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_CHAR: {\n+        int value = atoi(string_value);\n+        _vt->char_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_INT: {\n+        int value = atoi(string_value);\n+        _vt->int_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_LONG: {\n+        jlong value;\n+        if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n+          fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n+          break;\n+        }\n+        _vt->long_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_FLOAT: {\n+        float value = atof(string_value);\n+        _vt->float_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        double value = atof(string_value);\n+        _vt->double_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_ARRAY:\n+      case T_OBJECT: {\n+        Thread* THREAD = Thread::current();\n+        bool res = _replay->process_staticfield_reference(string_value, _vt, fd, THREAD);\n+        assert(res, \"should succeed for arrays & objects\");\n+        break;\n+      }\n+      case T_INLINE_TYPE: {\n+        InlineKlass* vk = InlineKlass::cast(fd->field_holder()->get_inline_type_field_klass(fd->index()));\n+        if (fd->is_inlined()) {\n+          int field_offset = fd->offset() - vk->first_field_offset();\n+          oop obj = (oop)(cast_from_oop<address>(_vt) + field_offset);\n+          InlineTypeFieldInitializer init_fields(obj, _replay);\n+          vk->do_nonstatic_fields(&init_fields);\n+        } else {\n+          oop value = vk->allocate_instance(Thread::current());\n+          _vt->obj_field_put(fd->offset(), value);\n+        }\n+        break;\n+      }\n+      default: {\n+        fatal(\"Unhandled type: %s\", type2name(bt));\n+      }\n+      }\n@@ -812,0 +867,1 @@\n+  };\n@@ -813,1 +869,1 @@\n-    oop java_mirror = k->java_mirror();\n+  bool process_staticfield_reference(const char* field_signature, oop java_mirror, fieldDescriptor* fd, TRAPS) {\n@@ -820,4 +876,2 @@\n-        ArrayKlass* kelem = (ArrayKlass *)parse_klass(CHECK);\n-        if (kelem == NULL) {\n-          return;\n-        }\n+        Klass* k = resolve_klass(field_signature, CHECK_(true));\n+        ArrayKlass* kelem = (ArrayKlass *)k;\n@@ -833,1 +887,1 @@\n-        value = kelem->multi_allocate(rank, dims, CHECK);\n+        value = kelem->multi_allocate(rank, dims, CHECK_(true));\n@@ -836,1 +890,1 @@\n-          value = oopFactory::new_byteArray(length, CHECK);\n+          value = oopFactory::new_byteArray(length, CHECK_(true));\n@@ -838,1 +892,1 @@\n-          value = oopFactory::new_boolArray(length, CHECK);\n+          value = oopFactory::new_boolArray(length, CHECK_(true));\n@@ -840,1 +894,1 @@\n-          value = oopFactory::new_charArray(length, CHECK);\n+          value = oopFactory::new_charArray(length, CHECK_(true));\n@@ -842,1 +896,1 @@\n-          value = oopFactory::new_shortArray(length, CHECK);\n+          value = oopFactory::new_shortArray(length, CHECK_(true));\n@@ -844,1 +898,1 @@\n-          value = oopFactory::new_floatArray(length, CHECK);\n+          value = oopFactory::new_floatArray(length, CHECK_(true));\n@@ -846,1 +900,1 @@\n-          value = oopFactory::new_doubleArray(length, CHECK);\n+          value = oopFactory::new_doubleArray(length, CHECK_(true));\n@@ -848,1 +902,1 @@\n-          value = oopFactory::new_intArray(length, CHECK);\n+          value = oopFactory::new_intArray(length, CHECK_(true));\n@@ -850,1 +904,1 @@\n-          value = oopFactory::new_longArray(length, CHECK);\n+          value = oopFactory::new_longArray(length, CHECK_(true));\n@@ -853,2 +907,6 @@\n-          Klass* kelem = resolve_klass(field_signature + 1, CHECK);\n-          value = oopFactory::new_objArray(kelem, length, CHECK);\n+          Klass* kelem = resolve_klass(field_signature + 1, CHECK_(true));\n+          value = oopFactory::new_objArray(kelem, length, CHECK_(true));\n+        } else if (field_signature[0] == JVM_SIGNATURE_ARRAY &&\n+                   field_signature[1] == JVM_SIGNATURE_INLINE_TYPE) {\n+          Klass* kelem = resolve_klass(field_signature + 1, CHECK_(true));\n+          value = oopFactory::new_flatArray(kelem, length, CHECK_(true));\n@@ -859,0 +917,86 @@\n+      java_mirror->obj_field_put(fd->offset(), value);\n+      return true;\n+    } else if (strcmp(field_signature, \"Ljava\/lang\/String;\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      Handle value = java_lang_String::create_from_str(string_value, CHECK_(true));\n+      java_mirror->obj_field_put(fd->offset(), value());\n+      return true;\n+    } else if (field_signature[0] == 'L') {\n+      const char* instance = parse_escaped_string();\n+      Klass* k = resolve_klass(instance, CHECK_(true));\n+      oop value = InstanceKlass::cast(k)->allocate_instance(CHECK_(true));\n+      java_mirror->obj_field_put(fd->offset(), value);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  \/\/ Initialize a class and fill in the value for a static field.\n+  \/\/ This is useful when the compile was dependent on the value of\n+  \/\/ static fields but it's impossible to properly rerun the static\n+  \/\/ initializer.\n+  void process_staticfield(TRAPS) {\n+    InstanceKlass* k = (InstanceKlass *)parse_klass(CHECK);\n+\n+    if (k == NULL || ReplaySuppressInitializers == 0 ||\n+        (ReplaySuppressInitializers == 2 && k->class_loader() == NULL)) {\n+      return;\n+    }\n+\n+    assert(k->is_initialized(), \"must be\");\n+\n+    const char* field_name = parse_escaped_string();\n+    const char* field_signature = parse_string();\n+    fieldDescriptor fd;\n+    Symbol* name = SymbolTable::new_symbol(field_name);\n+    Symbol* sig = SymbolTable::new_symbol(field_signature);\n+    if (!k->find_local_field(name, sig, &fd) ||\n+        !fd.is_static() ||\n+        fd.has_initial_value()) {\n+      report_error(field_name);\n+      return;\n+    }\n+\n+    oop java_mirror = k->java_mirror();\n+    if (strcmp(field_signature, \"I\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->int_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"B\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->byte_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"C\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->char_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"S\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->short_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"Z\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->bool_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"J\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      jlong value;\n+      if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n+        fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n+        return;\n+      }\n+      java_mirror->long_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"F\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      float value = atof(string_value);\n+      java_mirror->float_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"D\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      double value = atof(string_value);\n+      java_mirror->double_field_put(fd.offset(), value);\n+    } else if (field_signature[0] == JVM_SIGNATURE_INLINE_TYPE) {\n+      Klass* kelem = resolve_klass(field_signature, CHECK);\n+      InlineKlass* vk = InlineKlass::cast(kelem);\n+      oop value = vk->allocate_instance(CHECK);\n+      InlineTypeFieldInitializer init_fields(value, this);\n+      vk->do_nonstatic_fields(&init_fields);\n@@ -861,37 +1005,2 @@\n-      const char* string_value = parse_escaped_string();\n-      if (strcmp(field_signature, \"I\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->int_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"B\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->byte_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"C\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->char_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"S\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->short_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"Z\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->bool_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"J\") == 0) {\n-        jlong value;\n-        if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n-          fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n-          return;\n-        }\n-        java_mirror->long_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"F\") == 0) {\n-        float value = atof(string_value);\n-        java_mirror->float_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"D\") == 0) {\n-        double value = atof(string_value);\n-        java_mirror->double_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"Ljava\/lang\/String;\") == 0) {\n-        Handle value = java_lang_String::create_from_str(string_value, CHECK);\n-        java_mirror->obj_field_put(fd.offset(), value());\n-      } else if (field_signature[0] == JVM_SIGNATURE_CLASS) {\n-        Klass* k = resolve_klass(string_value, CHECK);\n-        oop value = InstanceKlass::cast(k)->allocate_instance(CHECK);\n-        java_mirror->obj_field_put(fd.offset(), value);\n-      } else {\n+      bool res = process_staticfield_reference(field_signature, java_mirror, &fd, CHECK);\n+      if (!res)  {\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":188,"deletions":79,"binary":false,"changes":267,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-\/\/ This class represents a Java reference or primitive type.\n+\/\/ This class represents a Java reference, inline type or primitive type.\n@@ -48,1 +48,1 @@\n-  _basic_type = k->is_array_klass() ? T_ARRAY : T_OBJECT;\n+  _basic_type = k->is_array_klass() ? T_ARRAY : (k->is_inline_klass() ? T_INLINE_TYPE : T_OBJECT);\n","filename":"src\/hotspot\/share\/ci\/ciType.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+\n@@ -54,0 +55,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -86,0 +88,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -141,0 +144,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        61\n+\n@@ -180,1 +185,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -513,1 +518,8 @@\n-        cp->unresolved_klass_at_put(index, class_index, num_klasses++);\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n+        if (name->is_Q_signature()) {\n+          cp->unresolved_qdescriptor_at_put(index, class_index, num_klasses++);\n+        } else {\n+          cp->unresolved_klass_at_put(index, class_index, num_klasses++);\n+        }\n@@ -769,2 +781,2 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+            if (name != vmSymbols::object_initializer_name()) {\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -777,1 +789,12 @@\n-              if (name == vmSymbols::object_initializer_name()) {\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else if (!signature->is_void_method_signature()\n+                         && ref_kind == JVM_REF_invokeStatic) {\n+                \/\/ also OK, could be a static factory call\n+              } else {\n@@ -937,3 +960,4 @@\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n+                                       bool is_inline_type,\n@@ -941,0 +965,7 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n+                                       bool* const is_declared_atomic,\n@@ -947,1 +978,1 @@\n-    _local_interfaces = Universe::the_empty_instance_klass_array();\n+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(0);\n@@ -950,3 +981,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);\n-\n-    int index;\n+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(itfs_len);\n+    int index = 0;\n@@ -988,1 +1018,14 @@\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+      InstanceKlass* ik = InstanceKlass::cast(interf);\n+      if (is_inline_type && ik->invalid_inline_super()) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(\n+          THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_IncompatibleClassChangeError(),\n+          \"Inline type %s attempts to implement interface java.lang.IdentityObject\",\n+          _class_name->as_klass_external_name());\n+        return;\n+      }\n+      if (ik->invalid_inline_super()) {\n+        set_invalid_inline_super();\n+      }\n+      if (ik->has_nonstatic_concrete_methods()) {\n@@ -991,1 +1034,7 @@\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      if (ik->is_declared_atomic()) {\n+        *is_declared_atomic = true;\n+      }\n+      if (ik->name() == vmSymbols::java_lang_IdentityObject()) {\n+        _implements_identityObject = true;\n+      }\n+      _temp_local_interfaces->append(ik);\n@@ -1009,1 +1058,1 @@\n-        const InstanceKlass* const k = _local_interfaces->at(index);\n+        const InstanceKlass* const k = _temp_local_interfaces->at(index);\n@@ -1496,0 +1545,1 @@\n+  STATIC_INLINE,        \/\/ inline type field\n@@ -1501,0 +1551,1 @@\n+  NONSTATIC_INLINE,\n@@ -1520,6 +1571,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  NONSTATIC_OOP,       \/\/ T_INLINE_TYPE = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20,\n@@ -1540,6 +1592,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  STATIC_OOP,          \/\/ T_INLINE_TYPE = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20\n@@ -1548,1 +1601,1 @@\n-static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type) {\n+static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type, bool is_inline_type) {\n@@ -1552,0 +1605,3 @@\n+  if (is_inline_type) {\n+    result = is_static ? STATIC_INLINE : NONSTATIC_INLINE;\n+  }\n@@ -1565,2 +1621,2 @@\n-  void update(bool is_static, BasicType type) {\n-    FieldAllocationType atype = basic_type_to_atype(is_static, type);\n+  void update(bool is_static, BasicType type, bool is_inline_type) {\n+    FieldAllocationType atype = basic_type_to_atype(is_static, type, is_inline_type);\n@@ -1579,0 +1635,1 @@\n+                                   bool is_inline_type,\n@@ -1601,1 +1658,5 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);\n@@ -1631,0 +1692,1 @@\n+  int instance_fields_count = 0;\n@@ -1635,0 +1697,4 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, is_interface, is_inline_type, CHECK);\n@@ -1636,2 +1702,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1653,0 +1717,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1712,1 +1777,1 @@\n-    fac->update(is_static, type);\n+    fac->update(is_static, type, type == T_INLINE_TYPE);\n@@ -1756,1 +1821,1 @@\n-      fac->update(false, type);\n+      fac->update(false, type, false);\n@@ -1761,0 +1826,27 @@\n+  if (is_inline_type) {\n+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);\n+    field->initialize(JVM_ACC_FIELD_INTERNAL | JVM_ACC_STATIC,\n+                      (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(default_value_name)),\n+                      (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(object_signature)),\n+                      0);\n+    const BasicType type = Signature::basic_type(vmSymbols::object_signature());\n+    fac->update(true, type, false);\n+    index++;\n+  }\n+\n+  if (is_inline_type && instance_fields_count == 0) {\n+    _is_empty_inline_type = true;\n+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);\n+    field->initialize(JVM_ACC_FIELD_INTERNAL,\n+        (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(empty_marker_name)),\n+        (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(byte_signature)),\n+        0);\n+    const BasicType type = Signature::basic_type(vmSymbols::byte_signature());\n+    fac->update(false, type, false);\n+    index++;\n+  }\n+\n+  if (instance_fields_count > 0) {\n+    _has_nonstatic_fields = true;\n+  }\n+\n@@ -2078,0 +2170,5 @@\n+  const char* class_note = \"\";\n+  if (is_inline_type() && name == vmSymbols::object_initializer_name()) {\n+    class_note = \" (an inline class)\";\n+  }\n+\n@@ -2081,2 +2178,2 @@\n-      \"%s \\\"%s\\\" in class %s has illegal signature \\\"%s\\\"\", type,\n-      name->as_C_string(), _class_name->as_C_string(), sig->as_C_string());\n+      \"%s \\\"%s\\\" in class %s%s has illegal signature \\\"%s\\\"\", type,\n+      name->as_C_string(), _class_name->as_C_string(), class_note, sig->as_C_string());\n@@ -2371,0 +2468,1 @@\n+                                      bool is_inline_type,\n@@ -2412,1 +2510,1 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n+    verify_legal_method_modifiers(flags, is_interface, is_inline_type, name, CHECK_NULL);\n@@ -2415,3 +2513,48 @@\n-  if (name == vmSymbols::object_initializer_name() && is_interface) {\n-    classfile_parse_error(\"Interface cannot have a method named <init>, class file %s\", THREAD);\n-    return NULL;\n+  if (name == vmSymbols::object_initializer_name()) {\n+    if (is_interface) {\n+      classfile_parse_error(\"Interface cannot have a method named <init>, class file %s\", THREAD);\n+      return NULL;\n+    } else if (!is_inline_type && signature->is_void_method_signature()) {\n+      \/\/ OK, a constructor\n+    } else if (is_inline_type && !signature->is_void_method_signature()) {\n+      \/\/ also OK, a static factory, as long as the return value is good\n+      bool ok = false;\n+      SignatureStream ss((Symbol*) signature, true);\n+      while (!ss.at_return_type())  ss.next();\n+      if (ss.is_reference()) {\n+        Symbol* ret = ss.as_symbol();\n+        const Symbol* required = class_name();\n+        if (is_hidden()) {\n+          \/\/ The original class name in hidden classes gets changed.  So using\n+          \/\/ the original name in the return type is no longer valid.\n+          \/\/ Note that expecting the return type for inline hidden class factory\n+          \/\/ methods to be java.lang.Object works around a JVM Spec issue for\n+          \/\/ hidden classes.\n+          required = vmSymbols::java_lang_Object();\n+        }\n+        ok = (ret == required);\n+      }\n+      if (!ok) {\n+        throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+      }\n+    } else {\n+      \/\/ not OK, so throw the same error as in verify_legal_method_signature.\n+      throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+    }\n+    \/\/ A declared <init> method must always be either a non-static\n+    \/\/ object constructor, with a void return, or else it must be a\n+    \/\/ static factory method, with a non-void return.  No other\n+    \/\/ definition of <init> is possible.\n+    \/\/\n+    \/\/ The verifier (in verify_invoke_instructions) will inspect the\n+    \/\/ signature of any attempt to invoke <init>, and ensures that it\n+    \/\/ returns non-void if and only if it is being invoked by\n+    \/\/ invokestatic, and void if and only if it is being invoked by\n+    \/\/ invokespecial.\n+    \/\/\n+    \/\/ When a symbolic reference to <init> is resolved for a\n+    \/\/ particular invocation mode (special or static), the mode is\n+    \/\/ matched to the JVM_ACC_STATIC modifier of the <init> method.\n+    \/\/ Thus, it is impossible to statically invoke a constructor, and\n+    \/\/ impossible to \"new + invokespecial\" a static factory, either\n+    \/\/ through bytecode or through reflection.\n@@ -2985,0 +3128,1 @@\n+                                    bool is_inline_type,\n@@ -3009,0 +3153,1 @@\n+                                    is_inline_type,\n@@ -3276,2 +3421,2 @@\n-    \/\/ Access flags\n-    jint flags;\n+\n+    jint recognized_modifiers = RECOGNIZED_INNER_CLASS_MODIFIERS;\n@@ -3280,3 +3425,1 @@\n-      flags = cfs->get_u2_fast() & (RECOGNIZED_INNER_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-    } else {\n-      flags = cfs->get_u2_fast() & RECOGNIZED_INNER_CLASS_MODIFIERS;\n+      recognized_modifiers |= JVM_ACC_MODULE;\n@@ -3284,0 +3427,8 @@\n+    \/\/ JVM_ACC_INLINE is defined for class file version 55 and later\n+    if (supports_inline_types()) {\n+      recognized_modifiers |= JVM_ACC_INLINE;\n+    }\n+\n+    \/\/ Access flags\n+    jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+\n@@ -3664,3 +3815,2 @@\n-  return _major_version == JVM_CLASSFILE_MAJOR_VERSION &&\n-         _minor_version == JAVA_PREVIEW_MINOR_VERSION &&\n-         Arguments::enable_preview();\n+  \/\/ temporarily disable the sealed type preview feature check\n+  return _major_version == JVM_CLASSFILE_MAJOR_VERSION;\n@@ -4136,1 +4286,2 @@\n-    check_property(_class_name == vmSymbols::java_lang_Object(),\n+    check_property(_class_name == vmSymbols::java_lang_Object()\n+                   || (_access_flags.get_flags() & JVM_ACC_INLINE),\n@@ -4279,0 +4430,19 @@\n+void ClassFileParser::throwInlineTypeLimitation(THREAD_AND_LOCATION_DECL,\n+                                                const char* msg,\n+                                                const Symbol* name,\n+                                                const Symbol* sig) const {\n+\n+  ResourceMark rm(THREAD);\n+  if (name == NULL || sig == NULL) {\n+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"class: %s - %s\", _class_name->as_C_string(), msg);\n+  }\n+  else {\n+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"\\\"%s\\\" sig: \\\"%s\\\" class: %s - %s\", name->as_C_string(), sig->as_C_string(),\n+        _class_name->as_C_string(), msg);\n+  }\n+}\n+\n@@ -4312,0 +4482,5 @@\n+      if (ik->is_inline_klass()) {\n+        Thread *THREAD = Thread::current();\n+        throwInlineTypeLimitation(THREAD_AND_LOCATION, \"Inline Types do not support Cloneable\");\n+        return;\n+      }\n@@ -4352,0 +4527,5 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 55 and later\n+  return _major_version >= JAVA_11_VERSION;\n+}\n+\n@@ -4395,3 +4575,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4418,0 +4599,5 @@\n+\n+    if (length == 1 && result->at(0) == vmClasses::IdentityObject_klass()) {\n+      return Universe::the_single_IdentityObject_klass_array();\n+    }\n+\n@@ -4631,0 +4817,1 @@\n+  const bool is_inline_type = (flags & JVM_ACC_INLINE) != 0;\n@@ -4632,0 +4819,1 @@\n+  assert(supports_inline_types() || !is_inline_type, \"JVM_ACC_INLINE should not be set\");\n@@ -4642,0 +4830,11 @@\n+  if (is_inline_type && !EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    Exceptions::fthrow(\n+      THREAD_AND_LOCATION,\n+      vmSymbols::java_lang_ClassFormatError(),\n+      \"Class modifier ACC_INLINE in class %s requires option -XX:+EnableValhalla\",\n+      _class_name->as_C_string()\n+    );\n+    return;\n+  }\n+\n@@ -4656,1 +4855,2 @@\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (is_inline_type && (is_interface || is_abstract || is_enum || !is_final))) {\n@@ -4658,0 +4858,2 @@\n+    const char* class_note = \"\";\n+    if (is_inline_type)  class_note = \" (an inline class)\";\n@@ -4661,2 +4863,2 @@\n-      \"Illegal class modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags\n+      \"Illegal class modifiers in class %s%s: 0x%X\",\n+      _class_name->as_C_string(), class_note, flags\n@@ -4732,0 +4934,1 @@\n+                                                   bool is_inline_type,\n@@ -4756,0 +4959,4 @@\n+    } else {\n+      if (is_inline_type && !is_static && !is_final) {\n+        is_illegal = true;\n+      }\n@@ -4772,0 +4979,1 @@\n+                                                    bool is_inline_type,\n@@ -4792,0 +5000,1 @@\n+  const char* class_note = \"\";\n@@ -4826,1 +5035,1 @@\n-        if (is_static || is_final || is_synchronized || is_native ||\n+        if (is_final || is_synchronized || is_native ||\n@@ -4830,0 +5039,9 @@\n+        if (!is_static && !is_inline_type) {\n+          \/\/ OK, an object constructor in a regular class\n+        } else if (is_static && is_inline_type) {\n+          \/\/ OK, a static init factory in an inline class\n+        } else {\n+          \/\/ but no other combinations are allowed\n+          is_illegal = true;\n+          class_note = (is_inline_type ? \" (an inline class)\" : \" (not an inline class)\");\n+        }\n@@ -4831,4 +5049,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || is_strict)))) {\n-            is_illegal = true;\n+        if (is_inline_type && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (an inline class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || is_strict)))) {\n+              is_illegal = true;\n+            }\n@@ -4846,2 +5069,2 @@\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+      \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+      name->as_C_string(), _class_name->as_C_string(), class_note, flags);\n@@ -5005,1 +5228,10 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_INLINE_TYPE:\n+      \/\/ Can't enable this check until JDK upgrades the bytecode generators\n+      \/\/ if (_major_version < CONSTANT_CLASS_DESCRIPTORS ) {\n+      \/\/   classfile_parse_error(\"Class name contains illegal Q-signature \"\n+      \/\/                                    \"in descriptor in class file %s\",\n+      \/\/                                    CHECK_0);\n+      \/\/ }\n+      \/\/ fall through\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -5016,1 +5248,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -5072,0 +5304,3 @@\n+    } else if (_major_version >= CONSTANT_CLASS_DESCRIPTORS && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; and Q...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -5169,0 +5404,3 @@\n+  if (!supports_inline_types() && (signature->is_Q_signature() || signature->is_Q_array_signature())) {\n+    throwIllegalSignature(\"Field\", name, signature, CHECK);\n+  }\n@@ -5221,1 +5459,1 @@\n-        \/\/ All internal methods must return void\n+        \/\/ All constructor methods must return void\n@@ -5225,0 +5463,16 @@\n+        \/\/ All static init methods must return the current class\n+        if ((length >= 3) && (p[length-1] == JVM_SIGNATURE_ENDCLASS)\n+            && name == vmSymbols::object_initializer_name()) {\n+          nextp = skip_over_field_signature(p, true, length, CHECK_0);\n+          if (nextp && ((int)length == (nextp - p))) {\n+            \/\/ The actual class will be checked against current class\n+            \/\/ when the method is defined (see parse_method).\n+            \/\/ A reference to a static init with a bad return type\n+            \/\/ will load and verify OK, but will fail to link.\n+            return args_size;\n+          }\n+        }\n+        \/\/ The distinction between static factory methods and\n+        \/\/ constructors depends on the JVM_ACC_STATIC modifier.\n+        \/\/ This distinction must be reflected in a void or non-void\n+        \/\/ return. For declared methods, the check is in parse_method.\n@@ -5381,0 +5635,6 @@\n+  if (ik->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    oop val = ik->allocate_instance(CHECK_NULL);\n+    vk->set_default_value(val);\n+  }\n+\n@@ -5384,0 +5644,34 @@\n+\/\/ Return true if the specified class is not a valid super class for an inline type.\n+\/\/ A valid super class for an inline type is abstract, has no instance fields,\n+\/\/ does not implement interface java.lang.IdentityObject (checked elsewhere), has\n+\/\/ an empty body-less no-arg constructor, and no synchronized instance methods.\n+\/\/ This function doesn't check if the class's super types are invalid.  Those checks\n+\/\/ are done elsewhere.  The final determination of whether or not a class is an\n+\/\/ invalid super type for an inline class is done in fill_instance_klass().\n+bool ClassFileParser::is_invalid_super_for_inline_type() {\n+  if (class_name() == vmSymbols::java_lang_IdentityObject()) {\n+    return true;\n+  }\n+  if (is_interface() || class_name() == vmSymbols::java_lang_Object()) {\n+    return false;\n+  }\n+  if (!access_flags().is_abstract() || _has_nonstatic_fields) {\n+    return true;\n+  } else {\n+    \/\/ Look at each method\n+    for (int x = 0; x < _methods->length(); x++) {\n+      const Method* const method = _methods->at(x);\n+      if (method->is_synchronized() && !method->is_static()) {\n+        return true;\n+\n+      } else if (method->name() == vmSymbols::object_initializer_name()) {\n+        if (method->signature() != vmSymbols::void_method_signature() ||\n+            !method->is_vanilla_constructor()) {\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -5416,0 +5710,12 @@\n+  if (_field_info->_is_naturally_atomic && ik->is_inline_klass()) {\n+    ik->set_is_naturally_atomic();\n+  }\n+\n+  if (this->_invalid_inline_super) {\n+    ik->set_invalid_inline_super();\n+  }\n+\n+  if (_has_injected_identityObject) {\n+    ik->set_has_injected_identityObject();\n+  }\n+\n@@ -5417,1 +5723,1 @@\n-  ik->set_static_oop_field_count(_fac->count[STATIC_OOP]);\n+  ik->set_static_oop_field_count(_fac->count[STATIC_OOP] + _fac->count[STATIC_INLINE]);\n@@ -5467,0 +5773,3 @@\n+  if (_is_declared_atomic) {\n+    ik->set_is_declared_atomic();\n+  }\n@@ -5578,0 +5887,37 @@\n+  bool all_fields_empty = true;\n+  int nfields = ik->java_fields_count();\n+  if (ik->is_inline_klass()) nfields++;\n+  for (int i = 0; i < nfields; i++) {\n+    if (((ik->field_access_flags(i) & JVM_ACC_STATIC) == 0)) {\n+      if (ik->field_is_inline_type(i)) {\n+        Symbol* klass_name = ik->field_signature(i)->fundamental_name(CHECK);\n+        \/\/ Inline classes for instance fields must have been pre-loaded\n+        \/\/ Inline classes for static fields might not have been loaded yet\n+        Klass* klass = SystemDictionary::find(klass_name,\n+            Handle(THREAD, ik->class_loader()),\n+            Handle(THREAD, ik->protection_domain()), CHECK);\n+        assert(klass != NULL, \"Just checking\");\n+        assert(klass->access_flags().is_inline_type(), \"Inline type expected\");\n+        ik->set_inline_type_field_klass(i, klass);\n+        klass_name->decrement_refcount();\n+        if (!InlineKlass::cast(klass)->is_empty_inline_type()) { all_fields_empty = false; }\n+      } else {\n+        all_fields_empty = false;\n+      }\n+    } else if (is_inline_type() && ((ik->field_access_flags(i) & JVM_ACC_FIELD_INTERNAL) != 0)) {\n+      InlineKlass::cast(ik)->set_default_value_offset(ik->field_offset(i));\n+    }\n+  }\n+\n+  if (_is_empty_inline_type || (is_inline_type() && all_fields_empty)) {\n+    ik->set_is_empty_inline_type();\n+  }\n+\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_alignment(_alignment);\n+    vk->set_first_field_offset(_first_field_offset);\n+    vk->set_exact_size_in_bytes(_exact_size_in_bytes);\n+    InlineKlass::cast(ik)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -5623,0 +5969,4 @@\n+  if (ik->name() == vmSymbols::java_lang_IdentityObject()) {\n+    Universe::initialize_the_single_IdentityObject_klass_array(ik, CHECK);\n+  }\n+\n@@ -5725,0 +6075,1 @@\n+  _temp_local_interfaces(NULL),\n@@ -5764,0 +6115,9 @@\n+  _has_inline_type_fields(false),\n+  _has_nonstatic_fields(false),\n+  _is_empty_inline_type(false),\n+  _is_naturally_atomic(false),\n+  _is_declared_atomic(false),\n+  _invalid_inline_super(false),\n+  _invalid_identity_super(false),\n+  _implements_identityObject(false),\n+  _has_injected_identityObject(false),\n@@ -5974,2 +6334,1 @@\n-  \/\/ Access flags\n-  jint flags;\n+  jint recognized_modifiers = JVM_RECOGNIZED_CLASS_MODIFIERS;\n@@ -5978,3 +6337,5 @@\n-    flags = stream->get_u2_fast() & (JVM_RECOGNIZED_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-  } else {\n-    flags = stream->get_u2_fast() & JVM_RECOGNIZED_CLASS_MODIFIERS;\n+    recognized_modifiers |= JVM_ACC_MODULE;\n+  }\n+  \/\/ JVM_ACC_INLINE is defined for class file version 55 and later\n+  if (supports_inline_types()) {\n+    recognized_modifiers |= JVM_ACC_INLINE;\n@@ -5983,0 +6344,3 @@\n+  \/\/ Access flags\n+  jint flags = stream->get_u2_fast() & recognized_modifiers;\n+\n@@ -6103,0 +6467,1 @@\n+                   is_inline_type(),\n@@ -6104,0 +6469,1 @@\n+                   &_is_declared_atomic,\n@@ -6106,1 +6472,1 @@\n-  assert(_local_interfaces != NULL, \"invariant\");\n+  assert(_temp_local_interfaces != NULL, \"invariant\");\n@@ -6111,1 +6477,2 @@\n-               _access_flags.is_interface(),\n+               is_interface(),\n+               is_inline_type(),\n@@ -6123,1 +6490,2 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                is_inline_type(),\n@@ -6205,3 +6573,3 @@\n-    check_property(_local_interfaces == Universe::the_empty_instance_klass_array(),\n-                   \"java.lang.Object cannot implement an interface in class file %s\",\n-                   CHECK);\n+    check_property(_temp_local_interfaces->length() == 0,\n+        \"java.lang.Object cannot implement an interface in class file %s\",\n+        CHECK);\n@@ -6212,1 +6580,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -6233,0 +6601,3 @@\n+    if (_super_klass->is_declared_atomic()) {\n+      _is_declared_atomic = true;\n+    }\n@@ -6238,0 +6609,54 @@\n+\n+    \/\/ For an inline class, only java\/lang\/Object or special abstract classes\n+    \/\/ are acceptable super classes.\n+    if (is_inline_type()) {\n+      const InstanceKlass* super_ik = _super_klass;\n+      if (super_ik->invalid_inline_super()) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(\n+          THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_IncompatibleClassChangeError(),\n+          \"inline class %s has an invalid super class %s\",\n+          _class_name->as_klass_external_name(),\n+          _super_klass->external_name());\n+        return;\n+      }\n+    }\n+  }\n+\n+  if (_class_name == vmSymbols::java_lang_NonTearable() && _loader_data->class_loader() == NULL) {\n+    \/\/ This is the original source of this condition.\n+    \/\/ It propagates by inheritance, as if testing \"instanceof NonTearable\".\n+    _is_declared_atomic = true;\n+  } else if (*ForceNonTearable != '\\0') {\n+    \/\/ Allow a command line switch to force the same atomicity property:\n+    const char* class_name_str = _class_name->as_C_string();\n+    if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+      _is_declared_atomic = true;\n+    }\n+  }\n+\n+  \/\/ Set ik->invalid_inline_super field to TRUE if already marked as invalid,\n+  \/\/ if super is marked invalid, or if is_invalid_super_for_inline_type()\n+  \/\/ returns true\n+  if (invalid_inline_super() ||\n+      (_super_klass != NULL && _super_klass->invalid_inline_super()) ||\n+      is_invalid_super_for_inline_type()) {\n+    set_invalid_inline_super();\n+  }\n+\n+  if (!is_inline_type() && invalid_inline_super() && (_super_klass == NULL || !_super_klass->invalid_inline_super())\n+      && !_implements_identityObject && class_name() != vmSymbols::java_lang_IdentityObject()) {\n+    _temp_local_interfaces->append(vmClasses::IdentityObject_klass());\n+    _has_injected_identityObject = true;\n+  }\n+  int itfs_len = _temp_local_interfaces->length();\n+  if (itfs_len == 0) {\n+    _local_interfaces = Universe::the_empty_instance_klass_array();\n+  } else if (itfs_len == 1 && _temp_local_interfaces->at(0) == vmClasses::IdentityObject_klass()) {\n+    _local_interfaces = Universe::the_single_IdentityObject_klass_array();\n+  } else {\n+    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);\n+    for (int i = 0; i < itfs_len; i++) {\n+      _local_interfaces->at_put(i, _temp_local_interfaces->at(i));\n+    }\n@@ -6239,0 +6664,2 @@\n+  _temp_local_interfaces = NULL;\n+  assert(_local_interfaces != NULL, \"invariant\");\n@@ -6268,1 +6695,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -6274,0 +6701,19 @@\n+\n+  for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {\n+    if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE  && !fs.access_flags().is_static()) {\n+      \/\/ Pre-load inline class\n+      Klass* klass = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+          Handle(THREAD, _loader_data->class_loader()),\n+          _protection_domain, true, CHECK);\n+      assert(klass != NULL, \"Sanity check\");\n+      if (!klass->access_flags().is_inline_type()) {\n+        assert(klass->is_instance_klass(), \"Sanity check\");\n+        ResourceMark rm(THREAD);\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"Class %s expects class %s to be an inline type, but it is not\",\n+                    _class_name->as_C_string(),\n+                    InstanceKlass::cast(klass)->external_name()));\n+      }\n+    }\n+  }\n+\n@@ -6276,2 +6722,9 @@\n-                        _parsed_annotations->is_contended(), _field_info);\n-  lb.build_layout();\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      loader_data(), _protection_domain, _field_info);\n+  lb.build_layout(CHECK);\n+  if (is_inline_type()) {\n+    _alignment = lb.get_alignment();\n+    _first_field_offset = lb.get_first_field_offset();\n+    _exact_size_in_bytes = lb.get_exact_size_in_byte();\n+  }\n+  _has_inline_type_fields = _field_info->_has_inline_fields;\n@@ -6279,1 +6732,1 @@\n-  \/\/ Compute reference typ\n+  \/\/ Compute reference type\n@@ -6281,1 +6734,0 @@\n-\n@@ -6313,0 +6765,1 @@\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":537,"deletions":84,"binary":false,"changes":621,"status":"modified"},{"patch":"@@ -397,1 +397,27 @@\n-    if (k->local_interfaces()->length() != _interfaces->length()) {\n+    int actual_num_interfaces = k->local_interfaces()->length();\n+    int specified_num_interfaces = _interfaces->length();\n+    int expected_num_interfaces, i;\n+\n+    bool identity_object_implemented = false;\n+    bool identity_object_specified = false;\n+    for (i = 0; i < actual_num_interfaces; i++) {\n+      if (k->local_interfaces()->at(i) == vmClasses::IdentityObject_klass()) {\n+        identity_object_implemented = true;\n+        break;\n+      }\n+    }\n+    for (i = 0; i < specified_num_interfaces; i++) {\n+      if (lookup_class_by_id(_interfaces->at(i)) == vmClasses::IdentityObject_klass()) {\n+        identity_object_specified = true;\n+        break;\n+      }\n+    }\n+\n+    expected_num_interfaces = actual_num_interfaces;\n+    if (identity_object_implemented  && !identity_object_specified) {\n+      \/\/ Backwards compatibility -- older classlists do not know about\n+      \/\/ java.lang.IdentityObject.\n+      expected_num_interfaces--;\n+    }\n+\n+    if (specified_num_interfaces != expected_num_interfaces) {\n@@ -401,1 +427,1 @@\n-            _interfaces->length(), k->local_interfaces()->length());\n+            specified_num_interfaces, expected_num_interfaces);\n@@ -656,0 +682,6 @@\n+  if (interface_name == vmSymbols::java_lang_IdentityObject()) {\n+    \/\/ Backwards compatibility -- older classlists do not know about\n+    \/\/ java.lang.IdentityObject.\n+    return vmClasses::IdentityObject_klass();\n+  }\n+\n","filename":"src\/hotspot\/share\/classfile\/classListParser.cpp","additions":34,"deletions":2,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -205,1 +205,1 @@\n-    if (*start == JVM_SIGNATURE_CLASS) {\n+    if (*start == JVM_SIGNATURE_CLASS || *start == JVM_SIGNATURE_INLINE_TYPE) {\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -384,0 +385,10 @@\n+void ClassLoaderData::inline_classes_do(void f(InlineKlass*)) {\n+  \/\/ Lock-free access requires load_acquire\n+  for (Klass* k = Atomic::load_acquire(&_klasses); k != NULL; k = k->next_link()) {\n+    if (k->is_inline_klass()) {\n+      f(InlineKlass::cast(k));\n+    }\n+    assert(k != k->next_link(), \"no loops!\");\n+  }\n+}\n+\n@@ -550,0 +561,2 @@\n+  inline_classes_do(InlineKlass::cleanup);\n+\n@@ -844,1 +857,5 @@\n-        MetadataFactory::free_metadata(this, (InstanceKlass*)m);\n+        if (!((Klass*)m)->is_inline_klass()) {\n+          MetadataFactory::free_metadata(this, (InstanceKlass*)m);\n+        } else {\n+          MetadataFactory::free_metadata(this, (InlineKlass*)m);\n+        }\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.cpp","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,2 @@\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n@@ -35,0 +37,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -37,1 +40,0 @@\n-\n@@ -41,0 +43,1 @@\n+  _inline_klass(NULL),\n@@ -56,0 +59,1 @@\n+ _inline_klass(NULL),\n@@ -62,1 +66,1 @@\n-  assert(kind == REGULAR || kind == FLATTENED || kind == INHERITED,\n+  assert(kind == REGULAR || kind == INLINED || kind == INHERITED,\n@@ -78,1 +82,2 @@\n-  _primitive_fields(NULL),\n+  _small_primitive_fields(NULL),\n+  _big_primitive_fields(NULL),\n@@ -86,2 +91,4 @@\n-  if (_primitive_fields == NULL) {\n-    _primitive_fields = new(ResourceObj::RESOURCE_AREA, mtInternal) GrowableArray<LayoutRawBlock*>(INITIAL_LIST_SIZE);\n+  if (size >= oopSize) {\n+    add_to_big_primitive_list(block);\n+  } else {\n+    add_to_small_primitive_list(block);\n@@ -89,1 +96,0 @@\n-  _primitive_fields->append(block);\n@@ -102,0 +108,10 @@\n+void FieldGroup::add_inlined_field(AllFieldStream fs, InlineKlass* vk) {\n+  LayoutRawBlock* block = new LayoutRawBlock(fs.index(), LayoutRawBlock::INLINED, vk->get_exact_size_in_bytes(), vk->get_alignment(), false);\n+  block->set_inline_klass(vk);\n+  if (block->size() >= oopSize) {\n+    add_to_big_primitive_list(block);\n+  } else {\n+    add_to_small_primitive_list(block);\n+  }\n+}\n+\n@@ -103,2 +119,5 @@\n-  if (_primitive_fields != NULL) {\n-    _primitive_fields->sort(LayoutRawBlock::compare_size_inverted);\n+  if (_small_primitive_fields != NULL) {\n+    _small_primitive_fields->sort(LayoutRawBlock::compare_size_inverted);\n+  }\n+  if (_big_primitive_fields != NULL) {\n+    _big_primitive_fields->sort(LayoutRawBlock::compare_size_inverted);\n@@ -108,0 +127,14 @@\n+void FieldGroup::add_to_small_primitive_list(LayoutRawBlock* block) {\n+  if (_small_primitive_fields == NULL) {\n+    _small_primitive_fields = new(ResourceObj::RESOURCE_AREA, mtInternal) GrowableArray<LayoutRawBlock*>(INITIAL_LIST_SIZE);\n+  }\n+  _small_primitive_fields->append(block);\n+}\n+\n+void FieldGroup::add_to_big_primitive_list(LayoutRawBlock* block) {\n+  if (_big_primitive_fields == NULL) {\n+    _big_primitive_fields = new(ResourceObj::RESOURCE_AREA, mtInternal) GrowableArray<LayoutRawBlock*>(INITIAL_LIST_SIZE);\n+  }\n+  _big_primitive_fields->append(block);\n+}\n+\n@@ -141,1 +174,2 @@\n-      _start = _blocks;  \/\/ start allocating fields from the first empty block\n+      _start = _blocks; \/\/ Setting _start to _blocks instead of _last would allow subclasses\n+      \/\/ to allocate fields in empty slots of their super classes\n@@ -149,3 +183,5 @@\n-  LayoutRawBlock* block = _start;\n-  while (block->kind() != LayoutRawBlock::INHERITED && block->kind() != LayoutRawBlock::REGULAR\n-      && block->kind() != LayoutRawBlock::FLATTENED && block->kind() != LayoutRawBlock::PADDING) {\n+  LayoutRawBlock* block = _blocks;\n+  while (block != NULL\n+         && block->kind() != LayoutRawBlock::INHERITED\n+         && block->kind() != LayoutRawBlock::REGULAR\n+         && block->kind() != LayoutRawBlock::INLINED) {\n@@ -157,3 +193,2 @@\n-\n-\/\/ Insert a set of fields into a layout using a best-fit strategy.\n-\/\/ For each field, search for the smallest empty slot able to fit the field\n+\/\/ Insert a set of fields into a layout.\n+\/\/ For each field, search for an empty slot able to fit the field\n@@ -173,1 +208,0 @@\n-\n@@ -191,0 +225,1 @@\n+\n@@ -207,1 +242,0 @@\n-\n@@ -306,3 +340,11 @@\n-      int size = type2aelembytes(type);\n-      \/\/ INHERITED blocks are marked as non-reference because oop_maps are handled by their holder class\n-      LayoutRawBlock* block = new LayoutRawBlock(fs.index(), LayoutRawBlock::INHERITED, size, size, false);\n+      LayoutRawBlock* block;\n+      if (type == T_INLINE_TYPE) {\n+        InlineKlass* vk = InlineKlass::cast(ik->get_inline_type_field_klass(fs.index()));\n+        block = new LayoutRawBlock(fs.index(), LayoutRawBlock::INHERITED, vk->get_exact_size_in_bytes(),\n+                                   vk->get_alignment(), false);\n+\n+      } else {\n+        int size = type2aelembytes(type);\n+        \/\/ INHERITED blocks are marked as non-reference because oop_maps are handled by their holder class\n+        block = new LayoutRawBlock(fs.index(), LayoutRawBlock::INHERITED, size, size, false);\n+      }\n@@ -314,1 +356,0 @@\n-\n@@ -319,1 +360,0 @@\n-\n@@ -355,1 +395,0 @@\n-\n@@ -366,1 +405,0 @@\n-\n@@ -380,1 +418,0 @@\n-\n@@ -432,47 +469,46 @@\n-      case LayoutRawBlock::REGULAR: {\n-        FieldInfo* fi = FieldInfo::from_field_array(_fields, b->field_index());\n-        output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n-                         b->offset(),\n-                         fi->name(_cp)->as_C_string(),\n-                         fi->signature(_cp)->as_C_string(),\n-                         b->size(),\n-                         b->alignment(),\n-                         \"REGULAR\");\n-        break;\n-      }\n-      case LayoutRawBlock::FLATTENED: {\n-        FieldInfo* fi = FieldInfo::from_field_array(_fields, b->field_index());\n-        output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n-                         b->offset(),\n-                         fi->name(_cp)->as_C_string(),\n-                         fi->signature(_cp)->as_C_string(),\n-                         b->size(),\n-                         b->alignment(),\n-                         \"FLATTENED\");\n-        break;\n-      }\n-      case LayoutRawBlock::RESERVED: {\n-        output->print_cr(\" @%d %d\/- %s\",\n-                         b->offset(),\n-                         b->size(),\n-                         \"RESERVED\");\n-        break;\n-      }\n-      case LayoutRawBlock::INHERITED: {\n-        assert(!is_static, \"Static fields are not inherited in layouts\");\n-        assert(super != NULL, \"super klass must be provided to retrieve inherited fields info\");\n-        bool found = false;\n-        const InstanceKlass* ik = super;\n-        while (!found && ik != NULL) {\n-          for (AllFieldStream fs(ik->fields(), ik->constants()); !fs.done(); fs.next()) {\n-            if (fs.offset() == b->offset()) {\n-              output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n-                  b->offset(),\n-                  fs.name()->as_C_string(),\n-                  fs.signature()->as_C_string(),\n-                  b->size(),\n-                  b->size(), \/\/ so far, alignment constraint == size, will change with Valhalla\n-                  \"INHERITED\");\n-              found = true;\n-              break;\n-            }\n+    case LayoutRawBlock::REGULAR: {\n+      FieldInfo* fi = FieldInfo::from_field_array(_fields, b->field_index());\n+      output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n+                       b->offset(),\n+                       fi->name(_cp)->as_C_string(),\n+                       fi->signature(_cp)->as_C_string(),\n+                       b->size(),\n+                       b->alignment(),\n+                       \"REGULAR\");\n+      break;\n+    }\n+    case LayoutRawBlock::INLINED: {\n+      FieldInfo* fi = FieldInfo::from_field_array(_fields, b->field_index());\n+      output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n+                       b->offset(),\n+                       fi->name(_cp)->as_C_string(),\n+                       fi->signature(_cp)->as_C_string(),\n+                       b->size(),\n+                       b->alignment(),\n+                       \"INLINED\");\n+      break;\n+    }\n+    case LayoutRawBlock::RESERVED: {\n+      output->print_cr(\" @%d %d\/- %s\",\n+                       b->offset(),\n+                       b->size(),\n+                       \"RESERVED\");\n+      break;\n+    }\n+    case LayoutRawBlock::INHERITED: {\n+      assert(!is_static, \"Static fields are not inherited in layouts\");\n+      assert(super != NULL, \"super klass must be provided to retrieve inherited fields info\");\n+      bool found = false;\n+      const InstanceKlass* ik = super;\n+      while (!found && ik != NULL) {\n+        for (AllFieldStream fs(ik->fields(), ik->constants()); !fs.done(); fs.next()) {\n+          if (fs.offset() == b->offset()) {\n+            output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n+                b->offset(),\n+                fs.name()->as_C_string(),\n+                fs.signature()->as_C_string(),\n+                b->size(),\n+                b->size(), \/\/ so far, alignment constraint == size, will change with Valhalla\n+                \"INHERITED\");\n+            found = true;\n+            break;\n@@ -480,2 +516,1 @@\n-          ik = ik->java_super();\n-        break;\n+        ik = ik->java_super();\n@@ -484,12 +519,14 @@\n-      case LayoutRawBlock::EMPTY:\n-        output->print_cr(\" @%d %d\/1 %s\",\n-                         b->offset(),\n-                         b->size(),\n-                        \"EMPTY\");\n-        break;\n-      case LayoutRawBlock::PADDING:\n-        output->print_cr(\" @%d %d\/1 %s\",\n-                         b->offset(),\n-                         b->size(),\n-                        \"PADDING\");\n-        break;\n+      break;\n+    }\n+    case LayoutRawBlock::EMPTY:\n+      output->print_cr(\" @%d %d\/1 %s\",\n+                       b->offset(),\n+                       b->size(),\n+                       \"EMPTY\");\n+      break;\n+    case LayoutRawBlock::PADDING:\n+      output->print_cr(\" @%d %d\/1 %s\",\n+                       b->offset(),\n+                       b->size(),\n+                       \"PADDING\");\n+      break;\n@@ -502,1 +539,2 @@\n-      Array<u2>* fields, bool is_contended, FieldLayoutInfo* info) :\n+                                       Array<u2>* fields, bool is_contended, bool is_inline_type, ClassLoaderData* class_loader_data,\n+                                       Handle protection_domain, FieldLayoutInfo* info) :\n@@ -513,0 +551,2 @@\n+  _class_loader_data(class_loader_data),\n+  _protection_domain(protection_domain),\n@@ -515,0 +555,2 @@\n+  _first_field_offset(-1),\n+  _exact_size_in_bytes(-1),\n@@ -516,2 +558,7 @@\n-  _is_contended(is_contended) {}\n-\n+  _has_inline_type_fields(false),\n+  _is_contended(is_contended),\n+  _is_inline_type(is_inline_type),\n+  _has_flattening_information(is_inline_type),\n+  _has_nonatomic_values(false),\n+  _atomic_field_count(0)\n+ {}\n@@ -544,1 +591,1 @@\n-\/\/ Field sorting for regular classes:\n+\/\/ Field sorting for regular (non-inline) classes:\n@@ -549,0 +596,1 @@\n+\/\/   - field flattening decisions are taken in this method\n@@ -556,0 +604,1 @@\n+      _atomic_field_count++;  \/\/ we might decrement this\n@@ -571,13 +620,20 @@\n-      case T_BYTE:\n-      case T_CHAR:\n-      case T_DOUBLE:\n-      case T_FLOAT:\n-      case T_INT:\n-      case T_LONG:\n-      case T_SHORT:\n-      case T_BOOLEAN:\n-        group->add_primitive_field(fs, type);\n-        break;\n-      case T_OBJECT:\n-      case T_ARRAY:\n-        if (group != _static_fields) _nonstatic_oopmap_count++;\n+    case T_BYTE:\n+    case T_CHAR:\n+    case T_DOUBLE:\n+    case T_FLOAT:\n+    case T_INT:\n+    case T_LONG:\n+    case T_SHORT:\n+    case T_BOOLEAN:\n+      group->add_primitive_field(fs, type);\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY:\n+      if (group != _static_fields) _nonstatic_oopmap_count++;\n+      group->add_oop_field(fs);\n+      break;\n+    case T_INLINE_TYPE:\n+\/\/      fs.set_inline(true);\n+      _has_inline_type_fields = true;\n+      if (group == _static_fields) {\n+        \/\/ static fields are never inlined\n@@ -585,3 +641,37 @@\n-        break;\n-      default:\n-        fatal(\"Something wrong?\");\n+      } else {\n+        _has_flattening_information = true;\n+        \/\/ Flattening decision to be taken here\n+        \/\/ This code assumes all verification already have been performed\n+        \/\/ (field's type has been loaded and it is an inline klass)\n+        Thread* THREAD = Thread::current();\n+        Klass* klass =\n+            SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+                                                                Handle(THREAD, _class_loader_data->class_loader()),\n+                                                                _protection_domain, true, THREAD);\n+        assert(klass != NULL, \"Sanity check\");\n+        InlineKlass* vk = InlineKlass::cast(klass);\n+        bool too_big_to_flatten = (InlineFieldMaxFlatSize >= 0 &&\n+                                   (vk->size_helper() * HeapWordSize) > InlineFieldMaxFlatSize);\n+        bool too_atomic_to_flatten = vk->is_declared_atomic();\n+        bool too_volatile_to_flatten = fs.access_flags().is_volatile();\n+        if (vk->is_naturally_atomic()) {\n+          too_atomic_to_flatten = false;\n+          \/\/too_volatile_to_flatten = false; \/\/FIXME\n+          \/\/ volatile fields are currently never inlined, this could change in the future\n+        }\n+        if (!(too_big_to_flatten | too_atomic_to_flatten | too_volatile_to_flatten)) {\n+          group->add_inlined_field(fs, vk);\n+          _nonstatic_oopmap_count += vk->nonstatic_oop_map_count();\n+          fs.set_inlined(true);\n+          if (!vk->is_atomic()) {  \/\/ flat and non-atomic: take note\n+            _has_nonatomic_values = true;\n+            _atomic_field_count--;  \/\/ every other field is atomic but this one\n+          }\n+        } else {\n+          _nonstatic_oopmap_count++;\n+          group->add_oop_field(fs);\n+        }\n+      }\n+      break;\n+    default:\n+      fatal(\"Something wrong?\");\n@@ -599,0 +689,106 @@\n+\/* Field sorting for inline classes:\n+ *   - because inline classes are immutable, the @Contended annotation is ignored\n+ *     when computing their layout (with only read operation, there's no false\n+ *     sharing issue)\n+ *   - this method also records the alignment of the field with the most\n+ *     constraining alignment, this value is then used as the alignment\n+ *     constraint when flattening this inline type into another container\n+ *   - field flattening decisions are taken in this method (those decisions are\n+ *     currently only based in the size of the fields to be inlined, the size\n+ *     of the resulting instance is not considered)\n+ *\/\n+void FieldLayoutBuilder::inline_class_field_sorting(TRAPS) {\n+  assert(_is_inline_type, \"Should only be used for inline classes\");\n+  int alignment = 1;\n+  for (AllFieldStream fs(_fields, _constant_pool); !fs.done(); fs.next()) {\n+    FieldGroup* group = NULL;\n+    int field_alignment = 1;\n+    if (fs.access_flags().is_static()) {\n+      group = _static_fields;\n+    } else {\n+      _has_nonstatic_fields = true;\n+      _atomic_field_count++;  \/\/ we might decrement this\n+      group = _root_group;\n+    }\n+    assert(group != NULL, \"invariant\");\n+    BasicType type = Signature::basic_type(fs.signature());\n+    switch(type) {\n+    case T_BYTE:\n+    case T_CHAR:\n+    case T_DOUBLE:\n+    case T_FLOAT:\n+    case T_INT:\n+    case T_LONG:\n+    case T_SHORT:\n+    case T_BOOLEAN:\n+      if (group != _static_fields) {\n+        field_alignment = type2aelembytes(type); \/\/ alignment == size for primitive types\n+      }\n+      group->add_primitive_field(fs, type);\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY:\n+      if (group != _static_fields) {\n+        _nonstatic_oopmap_count++;\n+        field_alignment = type2aelembytes(type); \/\/ alignment == size for oops\n+      }\n+      group->add_oop_field(fs);\n+      break;\n+    case T_INLINE_TYPE: {\n+\/\/      fs.set_inline(true);\n+      _has_inline_type_fields = true;\n+      if (group == _static_fields) {\n+        \/\/ static fields are never inlined\n+        group->add_oop_field(fs);\n+      } else {\n+        \/\/ Flattening decision to be taken here\n+        \/\/ This code assumes all verifications have already been performed\n+        \/\/ (field's type has been loaded and it is an inline klass)\n+        Thread* THREAD = Thread::current();\n+        Klass* klass =\n+            SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+                Handle(THREAD, _class_loader_data->class_loader()),\n+                _protection_domain, true, CHECK);\n+        assert(klass != NULL, \"Sanity check\");\n+        InlineKlass* vk = InlineKlass::cast(klass);\n+        bool too_big_to_flatten = (InlineFieldMaxFlatSize >= 0 &&\n+                                   (vk->size_helper() * HeapWordSize) > InlineFieldMaxFlatSize);\n+        bool too_atomic_to_flatten = vk->is_declared_atomic();\n+        bool too_volatile_to_flatten = fs.access_flags().is_volatile();\n+        if (vk->is_naturally_atomic()) {\n+          too_atomic_to_flatten = false;\n+          \/\/too_volatile_to_flatten = false; \/\/FIXME\n+          \/\/ volatile fields are currently never inlined, this could change in the future\n+        }\n+        if (!(too_big_to_flatten | too_atomic_to_flatten | too_volatile_to_flatten)) {\n+          group->add_inlined_field(fs, vk);\n+          _nonstatic_oopmap_count += vk->nonstatic_oop_map_count();\n+          field_alignment = vk->get_alignment();\n+          fs.set_inlined(true);\n+          if (!vk->is_atomic()) {  \/\/ flat and non-atomic: take note\n+            _has_nonatomic_values = true;\n+            _atomic_field_count--;  \/\/ every other field is atomic but this one\n+          }\n+        } else {\n+          _nonstatic_oopmap_count++;\n+          field_alignment = type2aelembytes(T_OBJECT);\n+          group->add_oop_field(fs);\n+        }\n+      }\n+      break;\n+    }\n+    default:\n+      fatal(\"Unexpected BasicType\");\n+    }\n+    if (!fs.access_flags().is_static() && field_alignment > alignment) alignment = field_alignment;\n+  }\n+  _alignment = alignment;\n+  if (!_has_nonstatic_fields) {\n+    \/\/ There are a number of fixes required throughout the type system and JIT\n+    Exceptions::fthrow(THREAD_AND_LOCATION,\n+                       vmSymbols::java_lang_ClassFormatError(),\n+                       \"Value Types do not support zero instance size yet\");\n+    return;\n+  }\n+}\n+\n@@ -606,5 +802,7 @@\n-\/\/ Computation of regular classes layout is an evolution of the previous default layout\n-\/\/ (FieldAllocationStyle 1):\n-\/\/   - primitive fields are allocated first (from the biggest to the smallest)\n-\/\/   - then oop fields are allocated, either in existing gaps or at the end of\n-\/\/     the layout\n+\/* Computation of regular classes layout is an evolution of the previous default layout\n+ * (FieldAllocationStyle 1):\n+ *   - primitive fields (both primitive types and flattened inline types) are allocated\n+ *     first, from the biggest to the smallest\n+ *   - then oop fields are allocated (to increase chances to have contiguous oops and\n+ *     a simpler oopmap).\n+ *\/\n@@ -615,1 +813,0 @@\n-\n@@ -623,1 +820,2 @@\n-  _layout->add(_root_group->primitive_fields());\n+  _layout->add(_root_group->big_primitive_fields());\n+  _layout->add(_root_group->small_primitive_fields());\n@@ -631,1 +829,2 @@\n-      _layout->add(cg->primitive_fields(), start);\n+      _layout->add(cg->big_primitive_fields());\n+      _layout->add(cg->small_primitive_fields(), start);\n@@ -640,0 +839,4 @@\n+  \/\/ Warning: IntanceMirrorKlass expects static oops to be allocated first\n+  _static_layout->add_contiguously(_static_fields->oop_fields());\n+  _static_layout->add(_static_fields->big_primitive_fields());\n+  _static_layout->add(_static_fields->small_primitive_fields());\n@@ -641,2 +844,49 @@\n-  _static_layout->add_contiguously(this->_static_fields->oop_fields());\n-  _static_layout->add(this->_static_fields->primitive_fields());\n+  epilogue();\n+}\n+\n+\/* Computation of inline classes has a slightly different strategy than for\n+ * regular classes. Regular classes have their oop fields allocated at the end\n+ * of the layout to increase GC performances. Unfortunately, this strategy\n+ * increases the number of empty slots inside an instance. Because the purpose\n+ * of inline classes is to be embedded into other containers, it is critical\n+ * to keep their size as small as possible. For this reason, the allocation\n+ * strategy is:\n+ *   - big primitive fields (primitive types and flattened inline type smaller\n+ *     than an oop) are allocated first (from the biggest to the smallest)\n+ *   - then oop fields\n+ *   - then small primitive fields (from the biggest to the smallest)\n+ *\/\n+void FieldLayoutBuilder::compute_inline_class_layout(TRAPS) {\n+  prologue();\n+  inline_class_field_sorting(CHECK);\n+  \/\/ Inline types are not polymorphic, so they cannot inherit fields.\n+  \/\/ By consequence, at this stage, the layout must be composed of a RESERVED\n+  \/\/ block, followed by an EMPTY block.\n+  assert(_layout->start()->kind() == LayoutRawBlock::RESERVED, \"Unexpected\");\n+  assert(_layout->start()->next_block()->kind() == LayoutRawBlock::EMPTY, \"Unexpected\");\n+  LayoutRawBlock* first_empty = _layout->start()->next_block();\n+  if (first_empty->offset() % _alignment != 0) {\n+    LayoutRawBlock* padding = new LayoutRawBlock(LayoutRawBlock::PADDING, _alignment - (first_empty->offset() % _alignment));\n+    _layout->insert(first_empty, padding);\n+    _layout->set_start(padding->next_block());\n+  }\n+\n+  _layout->add(_root_group->big_primitive_fields());\n+  _layout->add(_root_group->oop_fields());\n+  _layout->add(_root_group->small_primitive_fields());\n+\n+  LayoutRawBlock* first_field = _layout->first_field_block();\n+   if (first_field != NULL) {\n+     _first_field_offset = _layout->first_field_block()->offset();\n+     _exact_size_in_bytes = _layout->last_block()->offset() - _layout->first_field_block()->offset();\n+   } else {\n+     \/\/ special case for empty value types\n+     _first_field_offset = _layout->blocks()->size();\n+     _exact_size_in_bytes = 0;\n+   }\n+  _exact_size_in_bytes = _layout->last_block()->offset() - _layout->first_field_block()->offset();\n+\n+  \/\/ Warning:: InstanceMirrorKlass expects static oops to be allocated first\n+  _static_layout->add_contiguously(_static_fields->oop_fields());\n+  _static_layout->add(_static_fields->big_primitive_fields());\n+  _static_layout->add(_static_fields->small_primitive_fields());\n@@ -647,0 +897,37 @@\n+void FieldLayoutBuilder::add_inlined_field_oopmap(OopMapBlocksBuilder* nonstatic_oop_maps,\n+                InlineKlass* vklass, int offset) {\n+  int diff = offset - vklass->first_field_offset();\n+  const OopMapBlock* map = vklass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* last_map = map + vklass->nonstatic_oop_map_count();\n+  while (map < last_map) {\n+    nonstatic_oop_maps->add(map->offset() + diff, map->count());\n+    map++;\n+  }\n+}\n+\n+void FieldLayoutBuilder::register_embedded_oops_from_list(OopMapBlocksBuilder* nonstatic_oop_maps, GrowableArray<LayoutRawBlock*>* list) {\n+  if (list != NULL) {\n+    for (int i = 0; i < list->length(); i++) {\n+      LayoutRawBlock* f = list->at(i);\n+      if (f->kind() == LayoutRawBlock::INLINED) {\n+        InlineKlass* vk = f->inline_klass();\n+        assert(vk != NULL, \"Should have been initialized\");\n+        if (vk->contains_oops()) {\n+          add_inlined_field_oopmap(nonstatic_oop_maps, vk, f->offset());\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+void FieldLayoutBuilder::register_embedded_oops(OopMapBlocksBuilder* nonstatic_oop_maps, FieldGroup* group) {\n+  if (group->oop_fields() != NULL) {\n+    for (int i = 0; i < group->oop_fields()->length(); i++) {\n+      LayoutRawBlock* b = group->oop_fields()->at(i);\n+      nonstatic_oop_maps->add(b->offset(), 1);\n+    }\n+  }\n+  register_embedded_oops_from_list(nonstatic_oop_maps, group->big_primitive_fields());\n+  register_embedded_oops_from_list(nonstatic_oop_maps, group->small_primitive_fields());\n+}\n+\n@@ -651,1 +938,0 @@\n-\n@@ -658,8 +944,1 @@\n-\n-  if (_root_group->oop_fields() != NULL) {\n-    for (int i = 0; i < _root_group->oop_fields()->length(); i++) {\n-      LayoutRawBlock* b = _root_group->oop_fields()->at(i);\n-      nonstatic_oop_maps->add(b->offset(), 1);\n-    }\n-  }\n-\n+  register_embedded_oops(nonstatic_oop_maps, _root_group);\n@@ -671,1 +950,1 @@\n-        nonstatic_oop_maps->add(cg->oop_fields()->at(0)->offset(), cg->oop_count());\n+        register_embedded_oops(nonstatic_oop_maps, cg);\n@@ -675,1 +954,0 @@\n-\n@@ -691,2 +969,16 @@\n-\n-  if (PrintFieldLayout) {\n+  _info->_has_inline_fields = _has_inline_type_fields;\n+\n+  \/\/ An inline type is naturally atomic if it has just one field, and\n+  \/\/ that field is simple enough.\n+  _info->_is_naturally_atomic = (_is_inline_type &&\n+                                 (_atomic_field_count <= 1) &&\n+                                 !_has_nonatomic_values &&\n+                                 _contended_groups.is_empty());\n+  \/\/ This may be too restrictive, since if all the fields fit in 64\n+  \/\/ bits we could make the decision to align instances of this class\n+  \/\/ to 64-bit boundaries, and load and store them as single words.\n+  \/\/ And on machines which supported larger atomics we could similarly\n+  \/\/ allow larger values to be atomic, if properly aligned.\n+\n+\n+  if (PrintFieldLayout || (PrintInlineLayout && _has_flattening_information)) {\n@@ -700,0 +992,5 @@\n+    if (_is_inline_type) {\n+      tty->print_cr(\"First field offset = %d\", _first_field_offset);\n+      tty->print_cr(\"Alignment = %d bytes\", _alignment);\n+      tty->print_cr(\"Exact size = %d bytes\", _exact_size_in_bytes);\n+    }\n@@ -704,2 +1001,6 @@\n-void FieldLayoutBuilder::build_layout() {\n-  compute_regular_layout();\n+void FieldLayoutBuilder::build_layout(TRAPS) {\n+  if (_is_inline_type) {\n+    compute_inline_class_layout(CHECK);\n+  } else {\n+    compute_regular_layout();\n+  }\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.cpp","additions":433,"deletions":132,"binary":false,"changes":565,"status":"modified"},{"patch":"@@ -50,0 +50,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -51,1 +53,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -1011,1 +1013,6 @@\n-      if (k->is_typeArray_klass()) {\n+      if (k->is_flatArray_klass()) {\n+        Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+        assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+        InlineKlass* vk = InlineKlass::cast(InstanceKlass::cast(element_klass));\n+        comp_mirror = Handle(THREAD, vk->java_mirror());\n+      } else if (k->is_typeArray_klass()) {\n@@ -1110,0 +1117,1 @@\n+      case T_INLINE_TYPE:\n@@ -1193,0 +1201,6 @@\n+  if (k->is_inline_klass()) {\n+    \/\/ Inline types have a val type mirror and a ref type mirror. Don't handle this for now. TODO:CDS\n+    k->clear_java_mirror_handle();\n+    return NULL;\n+  }\n+\n@@ -1520,0 +1534,1 @@\n+  bool is_value = false;\n@@ -1525,0 +1540,1 @@\n+    is_value = k->is_inline_klass();\n@@ -1531,1 +1547,7 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    if (is_value) {\n+      st->print(\"Q\");\n+    } else {\n+      st->print(\"L\");\n+    }\n+  }\n@@ -1553,1 +1575,1 @@\n-      int         siglen = (int) strlen(sigstr);\n+      int siglen = (int) strlen(sigstr);\n@@ -2518,2 +2540,2 @@\n-      \/\/ This is simlar to classic VM.\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      \/\/ This is similar to classic VM (before HotSpot).\n+      if (method->is_object_constructor() &&\n@@ -3953,1 +3975,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n@@ -4778,0 +4800,71 @@\n+\/\/ jdk_internal_vm_jni_SubElementSelector\n+\n+int jdk_internal_vm_jni_SubElementSelector::_arrayElementType_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_subElementType_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_offset_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_isInlined_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_isInlineType_offset;\n+\n+#define SUBELEMENT_SELECTOR_FIELDS_DO(macro) \\\n+  macro(_arrayElementType_offset,  k, \"arrayElementType\", class_signature, false); \\\n+  macro(_subElementType_offset,    k, \"subElementType\",   class_signature, false); \\\n+  macro(_offset_offset,            k, \"offset\",           int_signature,   false); \\\n+  macro(_isInlined_offset,         k, \"isInlined\",        bool_signature,  false); \\\n+  macro(_isInlineType_offset,      k, \"isInlineType\",     bool_signature,  false);\n+\n+void jdk_internal_vm_jni_SubElementSelector::compute_offsets() {\n+  InstanceKlass* k = vmClasses::jdk_internal_vm_jni_SubElementSelector_klass();\n+  SUBELEMENT_SELECTOR_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+}\n+\n+#if INCLUDE_CDS\n+void jdk_internal_vm_jni_SubElementSelector::serialize_offsets(SerializeClosure* f) {\n+  SUBELEMENT_SELECTOR_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+#undef SUBELEMENT_SELECTOR_FIELDS_DO\n+\n+Symbol* jdk_internal_vm_jni_SubElementSelector::symbol() {\n+  return vmSymbols::jdk_internal_vm_jni_SubElementSelector();\n+}\n+\n+oop jdk_internal_vm_jni_SubElementSelector::getArrayElementType(oop obj) {\n+  return obj->obj_field(_arrayElementType_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setArrayElementType(oop obj, oop type) {\n+  obj->obj_field_put(_arrayElementType_offset, type);\n+}\n+\n+oop jdk_internal_vm_jni_SubElementSelector::getSubElementType(oop obj) {\n+  return obj->obj_field(_subElementType_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setSubElementType(oop obj, oop type) {\n+  obj->obj_field_put(_subElementType_offset, type);\n+}\n+\n+int jdk_internal_vm_jni_SubElementSelector::getOffset(oop obj) {\n+  return obj->int_field(_offset_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setOffset(oop obj, int offset) {\n+  obj->int_field_put(_offset_offset, offset);\n+}\n+\n+bool jdk_internal_vm_jni_SubElementSelector::getIsInlined(oop obj) {\n+  return obj->bool_field(_isInlined_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setIsInlined(oop obj, bool b) {\n+  obj->bool_field_put(_isInlined_offset, b);\n+}\n+\n+bool jdk_internal_vm_jni_SubElementSelector::getIsInlineType(oop obj) {\n+  return obj->bool_field(_isInlineType_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setIsInlineType(oop obj, bool b) {\n+  obj->bool_field_put(_isInlineType_offset, b);\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":100,"deletions":7,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+  f(jdk_internal_vm_jni_SubElementSelector) \\\n@@ -309,0 +310,1 @@\n+  static int component_mirror_offset()     { CHECK_INIT(_component_mirror_offset); }\n@@ -325,2 +327,0 @@\n-  static int component_mirror_offset() { return _component_mirror_offset; }\n-\n@@ -1146,1 +1146,1 @@\n-    MN_IS_CONSTRUCTOR        = 0x00020000, \/\/ constructor\n+    MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, \/\/ constructor\n@@ -1693,1 +1693,0 @@\n-\n@@ -1709,0 +1708,25 @@\n+class jdk_internal_vm_jni_SubElementSelector : AllStatic {\n+ private:\n+  static int _arrayElementType_offset;\n+  static int _subElementType_offset;\n+  static int _offset_offset;\n+  static int _isInlined_offset;\n+  static int _isInlineType_offset;\n+ public:\n+  static Symbol* symbol();\n+  static void compute_offsets();\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+\n+  static oop getArrayElementType(oop obj);\n+  static void setArrayElementType(oop obj, oop type);\n+  static oop getSubElementType(oop obj);\n+  static void setSubElementType(oop obj, oop type);\n+  static int getOffset(oop obj);\n+  static void setOffset(oop obj, int offset);\n+  static bool getIsInlined(oop obj);\n+  static void setIsInlined(oop obj, bool b);\n+  static bool getIsInlineType(oop obj);\n+  static void setIsInlineType(oop obj, bool b);\n+};\n+\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":28,"deletions":4,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -44,0 +44,2 @@\n+\/\/ For INLINE_FIELD, set when loading inline type fields for\n+\/\/ class circularity checking.\n@@ -84,0 +86,3 @@\n+    case PlaceholderTable::INLINE_TYPE_FIELD:\n+       queuehead = _inlineTypeFieldQ;\n+       break;\n@@ -100,0 +105,3 @@\n+    case PlaceholderTable::INLINE_TYPE_FIELD:\n+       _inlineTypeFieldQ = seenthread;\n+       break;\n@@ -184,0 +192,1 @@\n+  entry->set_inlineTypeFieldQ(NULL);\n@@ -264,0 +273,1 @@\n+  case PlaceholderTable::INLINE_TYPE_FIELD: return \"INLINE_TYPE_FIELD\";\n@@ -329,1 +339,2 @@\n-          && (probe->defineThreadQ() == NULL) && (probe->definer() == NULL)) {\n+          && (probe->defineThreadQ() == NULL) && (probe->definer() == NULL)\n+          && (probe->inlineTypeFieldQ() == NULL)) {\n@@ -384,0 +395,3 @@\n+  st->print(\"inlineTypeFieldQ threads:\");\n+  inlineTypeFieldQ()->print_action_queue(st);\n+  st->cr();\n","filename":"src\/hotspot\/share\/classfile\/placeholders.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -76,0 +76,1 @@\n+\/\/ INLINE_TYPE_FIELD: needed to check for inline type fields circularity\n@@ -79,1 +80,2 @@\n-    DEFINE_CLASS = 3               \/\/ find_or_define class\n+    DEFINE_CLASS = 3,              \/\/ find_or_define class\n+    INLINE_TYPE_FIELD = 4          \/\/ inline type fields\n@@ -130,0 +132,1 @@\n+  SeenThread*       _inlineTypeFieldQ;  \/\/ queue of inline types for circularity checking\n@@ -164,0 +167,3 @@\n+  SeenThread*        inlineTypeFieldQ()    const { return _inlineTypeFieldQ; }\n+  void               set_inlineTypeFieldQ(SeenThread* SeenThread) { _inlineTypeFieldQ = SeenThread; }\n+\n@@ -190,0 +196,4 @@\n+  bool inline_type_field_in_progress() {\n+    return (_inlineTypeFieldQ != NULL);\n+  }\n+\n","filename":"src\/hotspot\/share\/classfile\/placeholders.hpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -69,0 +70,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -77,0 +79,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -288,1 +291,1 @@\n-    \/\/ Ignore wrapping L and ;.\n+    \/\/ Ignore wrapping L and ;. (and Q and ; for value types);\n@@ -316,0 +319,4 @@\n+      if ((class_name->is_Q_array_signature() && !k->is_inline_klass()) ||\n+          (!class_name->is_Q_array_signature() && k->is_inline_klass())) {\n+            THROW_MSG_NULL(vmSymbols::java_lang_IncompatibleClassChangeError(), \"L\/Q mismatch on bottom type\");\n+          }\n@@ -325,1 +332,0 @@\n-\n@@ -458,0 +464,44 @@\n+Klass* SystemDictionary::resolve_inline_type_field_or_fail(AllFieldStream* fs,\n+                                                           Handle class_loader,\n+                                                           Handle protection_domain,\n+                                                           bool throw_error,\n+                                                           TRAPS) {\n+  Symbol* class_name = fs->signature()->fundamental_name(THREAD);\n+  class_loader = Handle(THREAD, java_lang_ClassLoader::non_reflection_class_loader(class_loader()));\n+  ClassLoaderData* loader_data = class_loader_data(class_loader);\n+  unsigned int p_hash = placeholders()->compute_hash(class_name);\n+  bool throw_circularity_error = false;\n+  PlaceholderEntry* oldprobe;\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    oldprobe = placeholders()->get_entry(p_hash, class_name, loader_data);\n+    if (oldprobe != NULL &&\n+      oldprobe->check_seen_thread(THREAD, PlaceholderTable::INLINE_TYPE_FIELD)) {\n+      throw_circularity_error = true;\n+\n+    } else {\n+      placeholders()->find_and_add(p_hash, class_name, loader_data,\n+                                   PlaceholderTable::INLINE_TYPE_FIELD, NULL, THREAD);\n+    }\n+  }\n+\n+  Klass* klass = NULL;\n+  if (!throw_circularity_error) {\n+    klass = SystemDictionary::resolve_or_fail(class_name, class_loader,\n+                                               protection_domain, true, THREAD);\n+  } else {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG_NULL(vmSymbols::java_lang_ClassCircularityError(), class_name->as_C_string());\n+  }\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    placeholders()->find_and_remove(p_hash, class_name, loader_data,\n+                                    PlaceholderTable::INLINE_TYPE_FIELD, THREAD);\n+  }\n+\n+  class_name->decrement_refcount();\n+  return klass;\n+}\n+\n@@ -959,1 +1009,1 @@\n-    if (t != T_OBJECT) {\n+    if (t != T_OBJECT && t != T_INLINE_TYPE) {\n@@ -1323,0 +1373,18 @@\n+\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik->fields(), ik->constants()); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        if (!fs.access_flags().is_static()) {\n+          \/\/ Pre-load inline class\n+          Klass* real_k = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+            class_loader, protection_domain, true, CHECK_NULL);\n+          Klass* k = ik->get_inline_type_field_klass_or_null(fs.index());\n+          if (real_k != k) {\n+            \/\/ oops, the app has substituted a different version of k!\n+            return NULL;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1358,0 +1426,7 @@\n+\n+  if (ik->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    oop val = ik->allocate_instance(CHECK_NULL);\n+    vk->set_default_value(val);\n+  }\n+\n@@ -1927,1 +2002,1 @@\n-    if (t != T_OBJECT) {\n+    if (t != T_OBJECT && t != T_INLINE_TYPE) {\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":79,"deletions":4,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -76,0 +76,1 @@\n+class AllFieldStream;\n@@ -125,0 +126,6 @@\n+  static Klass* resolve_inline_type_field_or_fail(AllFieldStream* fs,\n+                                                  Handle class_loader,\n+                                                  Handle protection_domain,\n+                                                  bool throw_error,\n+                                                  TRAPS);\n+\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+  if (this_class->access_flags().is_inline_type()) return false;\n@@ -71,1 +72,2 @@\n-    \/\/ to interfaces java.lang.Cloneable and java.io.Serializable.\n+    \/\/ to interfaces java.lang.Cloneable, java.io.Serializable,\n+    \/\/ and java.lang.IdentityObject.\n@@ -75,1 +77,2 @@\n-      this_class == vmClasses::Serializable_klass();\n+      this_class == vmClasses::Serializable_klass() ||\n+      this_class == vmClasses::IdentityObject_klass();\n@@ -127,0 +130,29 @@\n+\n+\/*\n+    \/\/ This code implements non-covariance between inline type arrays and both\n+    \/\/ arrays of objects and arrays of interface types.  If covariance is\n+    \/\/ supported for inline type arrays then this code should be removed.\n+    if (comp_from.is_inline_type() && !comp_this.is_null() && comp_this.is_reference()) {\n+      \/\/ An array of inline types is not assignable to an array of java.lang.Objects.\n+      if (comp_this.name() == vmSymbols::java_lang_Object()) {\n+        return false;\n+      }\n+\n+      \/\/ Need to load 'comp_this' to see if it is an interface.\n+      InstanceKlass* klass = context->current_class();\n+      {\n+        HandleMark hm(THREAD);\n+        Klass* comp_this_class = SystemDictionary::resolve_or_fail(\n+            comp_this.name(), Handle(THREAD, klass->class_loader()),\n+            Handle(THREAD, klass->protection_domain()), true, CHECK_false);\n+        klass->class_loader_data()->record_dependency(comp_this_class);\n+        if (log_is_enabled(Debug, class, resolve)) {\n+          Verifier::trace_class_resolution(comp_this_class, klass);\n+        }\n+        \/\/ An array of inline types is not assignable to an array of interface types.\n+        if (comp_this_class->is_interface()) {\n+          return false;\n+        }\n+      }\n+    }\n+*\/\n@@ -135,0 +167,39 @@\n+bool VerificationType::is_inline_type_assignable_from(const VerificationType& from) const {\n+  \/\/ Check that 'from' is not null, is an inline type, and is the same inline type.\n+  assert(is_inline_type(), \"called with a non-inline type\");\n+  assert(!is_null(), \"inline type is not null\");\n+  return (!from.is_null() && from.is_inline_type() && name() == from.name());\n+}\n+\n+bool VerificationType::is_ref_assignable_from_inline_type(const VerificationType& from, ClassVerifier* context, TRAPS) const {\n+  assert(!from.is_null(), \"Inline type should not be null\");\n+  if (!is_null() && (name()->is_same_fundamental_type(from.name()) ||\n+      name() == vmSymbols::java_lang_Object())) {\n+    return true;\n+  }\n+\n+  \/\/ Need to load 'this' to see if it is an interface or supertype.\n+  InstanceKlass* klass = context->current_class();\n+  {\n+    HandleMark hm(THREAD);\n+    Klass* this_class = SystemDictionary::resolve_or_fail(\n+        name(), Handle(THREAD, klass->class_loader()),\n+        Handle(THREAD, klass->protection_domain()), true, CHECK_false);\n+    klass->class_loader_data()->record_dependency(this_class);\n+    if (log_is_enabled(Debug, class, resolve)) {\n+      Verifier::trace_class_resolution(this_class, klass);\n+    }\n+    if (this_class->is_interface()) {\n+      return true;\n+    } else {\n+      Klass* from_class = SystemDictionary::resolve_or_fail(\n+        from.name(), Handle(THREAD, klass->class_loader()),\n+        Handle(THREAD, klass->protection_domain()), true, CHECK_false);\n+      if (log_is_enabled(Debug, class, resolve)) {\n+        Verifier::trace_class_resolution(from_class, klass);\n+      }\n+      return from_class->is_subclass_of(this_class);\n+    }\n+  }\n+}\n+\n@@ -149,1 +220,2 @@\n-    case T_OBJECT: {\n+    case T_OBJECT:\n+    case T_INLINE_TYPE: {\n@@ -155,1 +227,3 @@\n-      return VerificationType::reference_type(component_copy);\n+      return (ss.type() == T_INLINE_TYPE) ?\n+        VerificationType::inline_type(component_copy) :\n+        VerificationType::reference_type(component_copy);\n@@ -181,0 +255,2 @@\n+    case InlineTypeQuery:  st->print(\"inline type\"); break;\n+    case NonScalarQuery:   st->print(\"reference or inline type\"); break;\n@@ -189,0 +265,2 @@\n+      } else if (is_inline_type()) {\n+        name()->print_Qvalue_on(st);\n","filename":"src\/hotspot\/share\/classfile\/verificationType.cpp","additions":82,"deletions":4,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -71,3 +71,3 @@\n-      \/\/ Bottom two bits determine if the type is a reference, primitive,\n-      \/\/ uninitialized or a query-type.\n-      TypeMask           = 0x00000003,\n+      \/\/ Bottom three bits determine if the type is a reference, inline type,\n+      \/\/ primitive, uninitialized or a query-type.\n+      TypeMask           = 0x00000007,\n@@ -76,1 +76,1 @@\n-      Reference          = 0x0,        \/\/ _sym contains the name\n+      Reference          = 0x0,        \/\/ _sym contains the name of an object\n@@ -80,0 +80,1 @@\n+      InlineType         = 0x4,        \/\/ _sym contains the name of an inline type\n@@ -86,0 +87,2 @@\n+      InlineTypeFlag     = 0x08,       \/\/ For inline type query types\n+      NonScalarFlag      = 0x10,       \/\/ For either inline type or reference queries\n@@ -117,1 +120,3 @@\n-      Category2_2ndQuery = (Category2_2ndFlag << 1 * BitsPerByte) | TypeQuery\n+      Category2_2ndQuery = (Category2_2ndFlag << 1 * BitsPerByte) | TypeQuery,\n+      InlineTypeQuery    = (InlineTypeFlag    << 1 * BitsPerByte) | TypeQuery,\n+      NonScalarQuery     = (NonScalarFlag     << 1 * BitsPerByte) | TypeQuery\n@@ -150,0 +155,2 @@\n+  static VerificationType inline_type_check()\n+    { return VerificationType(InlineTypeQuery); }\n@@ -156,0 +163,2 @@\n+  static VerificationType nonscalar_check()\n+    { return VerificationType(NonScalarQuery); }\n@@ -159,1 +168,1 @@\n-      assert(((uintptr_t)sh & 0x3) == 0, \"Symbols must be aligned\");\n+      assert(((uintptr_t)sh & TypeMask) == 0, \"Symbols must be aligned\");\n@@ -162,1 +171,1 @@\n-      \/\/ to descriminate between oops and primitives.\n+      \/\/ to discriminate between oops and primitives.\n@@ -170,0 +179,11 @@\n+  \/\/ For inline types, store the actual Symbol* and set the 3rd bit.\n+  \/\/ Provides a way for an inline type to be distinguished from a reference type.\n+  static VerificationType inline_type(Symbol* sh) {\n+      assert(((uintptr_t)sh & TypeMask) == 0, \"Symbols must be aligned\");\n+      assert((uintptr_t)sh != 0, \"Null is not a valid inline type\");\n+      \/\/ If the above assert fails in the future because oop* isn't aligned,\n+      \/\/ then this type encoding system will have to change to have a tag value\n+      \/\/ to discriminate between oops and primitives.\n+      return VerificationType((uintptr_t)sh | InlineType);\n+  }\n+\n@@ -185,1 +205,2 @@\n-  bool is_reference() const { return ((_u._data & TypeMask) == Reference); }\n+  bool is_reference() const { return (((_u._data & TypeMask) == Reference) && !is_inline_type_check()); }\n+  bool is_inline_type() const { return ((_u._data & TypeMask) == InlineType); }\n@@ -188,2 +209,2 @@\n-    \/\/ primitives, and references (including uninitialized refs).  Though\n-    \/\/ the 'query' types should technically return 'false' here, if we\n+    \/\/ primitives, references (including uninitialized refs) and inline types.\n+    \/\/ Though the 'query' types should technically return 'false' here, if we\n@@ -203,0 +224,2 @@\n+  bool is_inline_type_check() const { return _u._data == InlineTypeQuery; }\n+  bool is_nonscalar_check() const { return _u._data == NonScalarQuery; }\n@@ -221,0 +244,1 @@\n+  bool is_inline_type_array() const { return is_x_array(JVM_SIGNATURE_INLINE_TYPE); }\n@@ -223,0 +247,2 @@\n+  bool is_nonscalar_array() const\n+    { return is_object_array() || is_array_array() || is_inline_type_array(); }\n@@ -239,0 +265,6 @@\n+  static VerificationType change_ref_to_inline_type(VerificationType ref) {\n+    assert(ref.is_reference(), \"Bad arg\");\n+    assert(!ref.is_null(), \"Unexpected NULL\");\n+    return inline_type(ref.name());\n+  }\n+\n@@ -245,2 +277,2 @@\n-    assert(is_reference() && !is_null(), \"Must be a non-null reference\");\n-    return _u._sym;\n+    assert(!is_null() && (is_reference() || is_inline_type()), \"Must be a non-null reference or an inline type\");\n+    return (is_reference() ? _u._sym : ((Symbol*)(_u._data & ~(uintptr_t)InlineType)));\n@@ -251,2 +283,4 @@\n-      (is_reference() && t.is_reference() && !is_null() && !t.is_null() &&\n-       name() == t.name()));\n+            (((is_reference() && t.is_reference()) ||\n+             (is_inline_type() && t.is_inline_type())) &&\n+              !is_null() && !t.is_null() && name() == t.name()));\n+\n@@ -281,0 +315,5 @@\n+        case NonScalarQuery:\n+          return from.is_reference() || from.is_uninitialized() ||\n+                 from.is_inline_type();\n+        case InlineTypeQuery:\n+          return from.is_inline_type();\n@@ -288,1 +327,5 @@\n-          if (is_reference() && from.is_reference()) {\n+          if (is_inline_type()) {\n+            return is_inline_type_assignable_from(from);\n+          } else if (is_reference() && from.is_inline_type()) {\n+            return is_ref_assignable_from_inline_type(from, context, THREAD);\n+          } else if (is_reference() && from.is_reference()) {\n@@ -336,0 +379,5 @@\n+  bool is_inline_type_assignable_from(const VerificationType& from) const;\n+\n+  bool is_ref_assignable_from_inline_type(const VerificationType& from, ClassVerifier* context, TRAPS) const;\n+\n+\n","filename":"src\/hotspot\/share\/classfile\/verificationType.hpp","additions":63,"deletions":15,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+#define INLINE_TYPE_MAJOR_VERSION                       56\n@@ -277,1 +278,1 @@\n-    \/\/ We need to skip the following four for bootstraping\n+    \/\/ We need to skip the following four for bootstrapping\n@@ -498,0 +499,7 @@\n+    case WRONG_INLINE_TYPE:\n+      ss->print(\"Type \");\n+      _type.details(ss);\n+      ss->print(\" and type \");\n+      _expected.details(ss);\n+      ss->print(\" must be identical inline types.\");\n+      break;\n@@ -592,0 +600,8 @@\n+VerificationType reference_or_inline_type(InstanceKlass* klass) {\n+  if (klass->is_inline_klass()) {\n+    return VerificationType::inline_type(klass->name());\n+  } else {\n+    return VerificationType::reference_type(klass->name());\n+  }\n+}\n+\n@@ -596,1 +612,1 @@\n-  _this_type = VerificationType::reference_type(klass->name());\n+  _this_type = reference_or_inline_type(klass);\n@@ -1041,1 +1057,1 @@\n-          if (!atype.is_reference_array()) {\n+          if (!atype.is_nonscalar_array()) {\n@@ -1215,1 +1231,1 @@\n-          if (!atype.is_reference_array()) {\n+          if (!atype.is_nonscalar_array()) {\n@@ -1615,1 +1631,1 @@\n-            VerificationType::reference_check(), CHECK_VERIFY(this));\n+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -1620,1 +1636,1 @@\n-            VerificationType::reference_check(), CHECK_VERIFY(this));\n+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -1671,1 +1687,1 @@\n-            VerificationType::reference_check(), CHECK_VERIFY(this));\n+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -1683,1 +1699,1 @@\n-          if (_method->name() == vmSymbols::object_initializer_name() &&\n+          if (_method->is_object_constructor() &&\n@@ -1703,0 +1719,11 @@\n+        case Bytecodes::_withfield :\n+          if (_klass->major_version() < INLINE_TYPE_MAJOR_VERSION) {\n+            class_format_error(\n+              \"withfield not supported by this class file version (%d.%d), class %s\",\n+              _klass->major_version(), _klass->minor_version(), _klass->external_name());\n+            return;\n+          }\n+          \/\/ pass FALSE, operand can't be an array type for withfield.\n+          verify_field_instructions(\n+            &bcs, &current_frame, cp, false, CHECK_VERIFY(this));\n+          no_control_flow = false; break;\n@@ -1706,4 +1733,0 @@\n-          verify_invoke_instructions(\n-            &bcs, code_length, &current_frame, (bci >= ex_min && bci < ex_max),\n-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));\n-          no_control_flow = false; break;\n@@ -1714,1 +1737,1 @@\n-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));\n+            &this_uninit, cp, &stackmap_table, CHECK_VERIFY(this));\n@@ -1732,0 +1755,22 @@\n+        case Bytecodes::_defaultvalue :\n+        {\n+          if (_klass->major_version() < INLINE_TYPE_MAJOR_VERSION) {\n+            class_format_error(\n+              \"defaultvalue not supported by this class file version (%d.%d), class %s\",\n+              _klass->major_version(), _klass->minor_version(), _klass->external_name());\n+            return;\n+          }\n+          index = bcs.get_index_u2();\n+          verify_cp_class_type(bci, index, cp, CHECK_VERIFY(this));\n+          VerificationType ref_type = cp_index_to_type(index, cp, CHECK_VERIFY(this));\n+          if (!ref_type.is_object()) {\n+            verify_error(ErrorContext::bad_type(bci,\n+                TypeOrigin::cp(index, ref_type)),\n+                \"Illegal defaultvalue instruction\");\n+            return;\n+          }\n+          VerificationType inline_type =\n+            VerificationType::change_ref_to_inline_type(ref_type);\n+          current_frame.push_stack(inline_type, CHECK_VERIFY(this));\n+          no_control_flow = false; break;\n+        }\n@@ -1772,3 +1817,3 @@\n-        case Bytecodes::_monitorexit :\n-          current_frame.pop_stack(\n-            VerificationType::reference_check(), CHECK_VERIFY(this));\n+        case Bytecodes::_monitorexit : {\n+          VerificationType ref = current_frame.pop_stack(\n+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -1776,0 +1821,1 @@\n+        }\n@@ -2034,0 +2080,1 @@\n+\n@@ -2148,1 +2195,1 @@\n-            | (1 << JVM_CONSTANT_String)  | (1 << JVM_CONSTANT_Class)\n+            | (1 << JVM_CONSTANT_String) | (1 << JVM_CONSTANT_Class)\n@@ -2324,1 +2371,1 @@\n-    (!allow_arrays || !ref_class_type.is_array())) {\n+      (!allow_arrays || !ref_class_type.is_array())) {\n@@ -2331,0 +2378,1 @@\n+\n@@ -2360,0 +2408,19 @@\n+    case Bytecodes::_withfield: {\n+      for (int i = n - 1; i >= 0; i--) {\n+        current_frame->pop_stack(field_type[i], CHECK_VERIFY(this));\n+      }\n+      \/\/ stack_object_type and target_class_type must be the same inline type.\n+      stack_object_type =\n+        current_frame->pop_stack(VerificationType::inline_type_check(), CHECK_VERIFY(this));\n+      VerificationType target_inline_type =\n+        VerificationType::change_ref_to_inline_type(target_class_type);\n+      if (!stack_object_type.equals(target_inline_type)) {\n+        verify_error(ErrorContext::bad_inline_type(bci,\n+            current_frame->stack_top_ctx(),\n+            TypeOrigin::cp(index, target_class_type)),\n+            \"Invalid type on operand stack in withfield instruction\");\n+        return;\n+      }\n+      current_frame->push_stack(target_inline_type, CHECK_VERIFY(this));\n+      break;\n+    }\n@@ -2768,1 +2835,1 @@\n-    bool in_try_block, bool *this_uninit, VerificationType return_type,\n+    bool in_try_block, bool *this_uninit,\n@@ -2800,1 +2867,1 @@\n-  \/\/ Get referenced class type\n+  \/\/ Get referenced class\n@@ -2866,2 +2933,4 @@\n-    \/\/ Make sure <init> can only be invoked by invokespecial\n-    if (opcode != Bytecodes::_invokespecial ||\n+    \/\/ Make sure <init> can only be invoked by invokespecial or invokestatic.\n+    \/\/ The allowed invocation mode of <init> depends on its signature.\n+    if ((opcode != Bytecodes::_invokespecial &&\n+         opcode != Bytecodes::_invokestatic) ||\n@@ -2876,1 +2945,1 @@\n-                  current_class()->super()->name()))) {\n+                  current_class()->super()->name()))) { \/\/ super() can never be an inline_type.\n@@ -2883,2 +2952,2 @@\n-      VerificationType unsafe_anonymous_host_type =\n-                        VerificationType::reference_type(current_class()->unsafe_anonymous_host()->name());\n+      InstanceKlass* unsafe_host = current_class()->unsafe_anonymous_host();\n+      VerificationType unsafe_anonymous_host_type = reference_or_inline_type(unsafe_host);\n@@ -2891,1 +2960,1 @@\n-                             current_class()->unsafe_anonymous_host(),\n+                             unsafe_host,\n@@ -2921,0 +2990,1 @@\n+      \/\/ (use of <init> as a static factory is handled under invokestatic)\n@@ -2935,3 +3005,4 @@\n-          VerificationType hosttype =\n-            VerificationType::reference_type(current_class()->unsafe_anonymous_host()->name());\n-          bool subtype = hosttype.is_assignable_from(top, this, false, CHECK_VERIFY(this));\n+\n+          InstanceKlass* unsafe_host = current_class()->unsafe_anonymous_host();\n+          VerificationType host_type = reference_or_inline_type(unsafe_host);\n+          bool subtype = host_type.is_assignable_from(top, this, false, CHECK_VERIFY(this));\n@@ -2990,4 +3061,3 @@\n-    if (method_name == vmSymbols::object_initializer_name()) {\n-      \/\/ <init> method must have a void return type\n-      \/* Unreachable?  Class file parser verifies that methods with '<' have\n-       * void return *\/\n+    if (method_name == vmSymbols::object_initializer_name() &&\n+        opcode != Bytecodes::_invokestatic) {\n+      \/\/ an <init> method must have a void return type, unless it's a static factory\n@@ -3006,0 +3076,8 @@\n+  } else {\n+    \/\/ an <init> method may not have a void return type, if it's a static factory\n+    if (method_name == vmSymbols::object_initializer_name() &&\n+        opcode != Bytecodes::_invokespecial) {\n+      verify_error(ErrorContext::bad_code(bci),\n+          \"Return type must be non-void in <init> static factory method\");\n+      return;\n+    }\n@@ -3053,1 +3131,2 @@\n-    \/\/ add one dimension to component with 'L' prepended and ';' postpended.\n+    char Q_or_L = component_type.is_inline_type() ? JVM_SIGNATURE_INLINE_TYPE : JVM_SIGNATURE_CLASS;\n+    \/\/ add one dimension to component with 'L' or 'Q' prepended and ';' appended.\n@@ -3057,1 +3136,1 @@\n-                         JVM_SIGNATURE_ARRAY, JVM_SIGNATURE_CLASS, component_name);\n+                         JVM_SIGNATURE_ARRAY, Q_or_L, component_name);\n@@ -3099,1 +3178,1 @@\n-    index, VerificationType::reference_check(), CHECK_VERIFY(this));\n+    index, VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n@@ -3136,1 +3215,1 @@\n-    VerificationType::reference_check(), CHECK_VERIFY(this));\n+    VerificationType::nonscalar_check(), CHECK_VERIFY(this));\n","filename":"src\/hotspot\/share\/classfile\/verifier.cpp","additions":116,"deletions":37,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+  do_klass(IdentityObject_klass,                        java_lang_IdentityObject                              ) \\\n@@ -125,0 +126,1 @@\n+  do_klass(ValueBootstrapMethods_klass,                 java_lang_invoke_ValueBootstrapMethods                ) \\\n@@ -174,0 +176,1 @@\n+  do_klass(jdk_internal_vm_jni_SubElementSelector_klass, jdk_internal_vm_jni_SubElementSelector               ) \\\n","filename":"src\/hotspot\/share\/classfile\/vmClassMacros.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -294,0 +294,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -303,0 +305,1 @@\n+  case vmIntrinsics::_getValue:\n@@ -312,0 +315,1 @@\n+  case vmIntrinsics::_putValue:\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -245,1 +245,1 @@\n-                                    bool& needs_ic_stub_refill, TRAPS) {\n+                                    bool& needs_ic_stub_refill, bool caller_is_c1, TRAPS) {\n@@ -254,1 +254,1 @@\n-    entry = VtableStubs::find_itable_stub(itable_index);\n+    entry = VtableStubs::find_itable_stub(itable_index, caller_is_c1);\n@@ -277,1 +277,1 @@\n-    entry = VtableStubs::find_vtable_stub(vtable_index);\n+    entry = VtableStubs::find_vtable_stub(vtable_index, caller_is_c1);\n@@ -512,0 +512,1 @@\n+                                           bool caller_is_c1,\n@@ -537,1 +538,1 @@\n-      entry      = method_code->verified_entry_point();\n+      entry      = caller_is_c1 ? method_code->verified_inline_entry_point() : method_code->verified_entry_point();\n@@ -539,1 +540,1 @@\n-      entry      = method_code->entry_point();\n+      entry      = caller_is_c1 ? method_code->inline_entry_point() : method_code->entry_point();\n@@ -553,1 +554,2 @@\n-        info.set_interpreter_entry(method()->get_c2i_entry(), method());\n+        address entry = caller_is_c1 ? method()->get_c2i_inline_entry() : method()->get_c2i_entry();\n+        info.set_interpreter_entry(entry, method());\n@@ -559,1 +561,2 @@\n-      info.set_icholder_entry(method()->get_c2i_unverified_entry(), holder);\n+      entry = (caller_is_c1)? method()->get_c2i_unverified_inline_entry() : method()->get_c2i_unverified_entry();\n+      info.set_icholder_entry(entry, holder);\n@@ -658,1 +661,2 @@\n-void CompiledStaticCall::compute_entry(const methodHandle& m, bool caller_is_nmethod, StaticCallInfo& info) {\n+void CompiledStaticCall::compute_entry(const methodHandle& m, CompiledMethod* caller_nm, StaticCallInfo& info) {\n+  bool caller_is_nmethod = caller_nm->is_nmethod();\n@@ -669,1 +673,5 @@\n-    info._entry  = m_code->verified_entry_point();\n+    if (caller_nm->is_compiled_by_c1()) {\n+      info._entry = m_code->verified_inline_entry_point();\n+    } else {\n+      info._entry = m_code->verified_entry_point();\n+    }\n@@ -675,1 +683,8 @@\n-    info._entry      = m()->get_c2i_entry();\n+\n+    if (caller_nm->is_compiled_by_c1()) {\n+      \/\/ C1 -> interp: values passed as oops\n+      info._entry = m()->get_c2i_inline_entry();\n+    } else {\n+      \/\/ C2 -> interp: values passed fields\n+      info._entry = m()->get_c2i_entry();\n+    }\n","filename":"src\/hotspot\/share\/code\/compiledIC.cpp","additions":25,"deletions":10,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -202,0 +202,10 @@\n+  bool  needs_stack_repair() const {\n+    if (is_compiled_by_c1()) {\n+      return method()->c1_needs_stack_repair();\n+    } else if (is_compiled_by_c2()) {\n+      return method()->c2_needs_stack_repair();\n+    } else {\n+      return false;\n+    }\n+  }\n+\n@@ -218,0 +228,2 @@\n+  virtual address verified_inline_entry_point() const = 0;\n+  virtual address verified_inline_ro_entry_point() const = 0;\n@@ -224,0 +236,1 @@\n+  virtual address inline_entry_point() const = 0;\n","filename":"src\/hotspot\/share\/code\/compiledMethod.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -292,0 +292,1 @@\n+                                              bool        return_vt,\n@@ -311,0 +312,1 @@\n+  last_pd->set_return_vt(return_vt);\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -644,0 +644,6 @@\n+\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\"); \/\/ for the next 3 fields\n+    _inline_entry_point       = _entry_point;\n+    _verified_inline_entry_point = _verified_entry_point;\n+    _verified_inline_ro_entry_point = _verified_entry_point;\n+\n@@ -817,0 +823,3 @@\n+    _inline_entry_point       = code_begin()         + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point = code_begin()      + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin()   + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n@@ -936,0 +945,3 @@\n+static nmethod* _nmethod_to_print = NULL;\n+static const CompiledEntrySignature* _nmethod_to_print_ces = NULL;\n+\n@@ -937,0 +949,6 @@\n+  ResourceMark rm;\n+  CompiledEntrySignature ces(method());\n+  ces.compute_calling_conventions();\n+  \/\/ ces.compute_calling_conventions() needs to grab the ProtectionDomainSet_lock, so we\n+  \/\/ can't do that (inside nmethod::print_entry_parameters) while holding the ttyLocker.\n+  \/\/ Hence we have do compute it here and pass via a global. Yuck.\n@@ -938,0 +956,3 @@\n+  assert(_nmethod_to_print == NULL && _nmethod_to_print_ces == NULL, \"no nesting\");\n+  _nmethod_to_print = this;\n+  _nmethod_to_print_ces = &ces;\n@@ -1017,0 +1038,3 @@\n+\n+  _nmethod_to_print = NULL;\n+  _nmethod_to_print_ces = NULL;\n@@ -3096,0 +3120,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -3097,0 +3122,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -3106,0 +3133,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -3108,33 +3145,12 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != NULL) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n-    }\n-  }\n-\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != NULL) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n-    }\n-    if (m != NULL && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != NULL) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n@@ -3142,54 +3158,65 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      intptr_t out_preserve = SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n-        }\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n-        }\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+    }\n+  }\n+\n+  if (_nmethod_to_print != this) {\n+    return;\n+  }\n+  Method* m = method();\n+  if (m == NULL || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN4(entry_point(), verified_entry_point(), verified_inline_entry_point(), verified_inline_ro_entry_point());\n+  low = MIN2(low, inline_entry_point());\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  const CompiledEntrySignature* ces = _nmethod_to_print_ces;\n+  const GrowableArray<SigEntry>* sig_cc;\n+  const VMRegPair* regs;\n+  if (block_begin == verified_entry_point()) {\n+    sig_cc = &ces->sig_cc();\n+    regs = ces->regs_cc();\n+  } else if (block_begin == verified_inline_entry_point()) {\n+    sig_cc = &ces->sig();\n+    regs = ces->regs();\n+  } else if (block_begin == verified_inline_ro_entry_point()) {\n+    sig_cc = &ces->sig_cc_ro();\n+    regs = ces->regs_cc_ro();\n+  } else {\n+    return;\n+  }\n+\n+  bool has_this = !m->is_static();\n+  if (ces->has_inline_recv() && block_begin == verified_entry_point()) {\n+    \/\/ <this> argument is scalarized for verified_entry_point()\n+    has_this = false;\n+  }\n+  const char* spname = \"sp\"; \/\/ make arch-specific?\n+  int stack_slot_offset = this->frame_size() * wordSize;\n+  int tab1 = 14, tab2 = 24;\n+  int sig_index = 0;\n+  int arg_index = has_this ? -1 : 0;\n+  bool did_old_sp = false;\n+  for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n+    bool at_this = (arg_index == -1);\n+    bool at_old_sp = false;\n+    BasicType t = (*sig)._bt;\n+    if (at_this) {\n+      stream->print(\"  # this: \");\n+    } else {\n+      stream->print(\"  # parm%d: \", arg_index);\n+    }\n+    stream->move_to(tab1);\n+    VMReg fst = regs[sig_index].first();\n+    VMReg snd = regs[sig_index].second();\n+    if (fst->is_reg()) {\n+      stream->print(\"%s\", fst->name());\n+      if (snd->is_valid())  {\n+        stream->print(\":%s\", snd->name());\n@@ -3197,6 +3224,18 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n-        stream->print(\"  (%s of caller)\", spname);\n-        stream->cr();\n+    } else if (fst->is_stack()) {\n+      int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+      if (offset == stack_slot_offset)  at_old_sp = true;\n+      stream->print(\"[%s+0x%x]\", spname, offset);\n+    } else {\n+      stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+    }\n+    stream->print(\" \");\n+    stream->move_to(tab2);\n+    stream->print(\"= \");\n+    if (at_this) {\n+      m->method_holder()->print_value_on(stream);\n+    } else {\n+      bool did_name = false;\n+      if (is_reference_type(t)) {\n+        Symbol* name = (*sig)._symbol;\n+        name->print_value_on(stream);\n+        did_name = true;\n@@ -3204,0 +3243,2 @@\n+      if (!did_name)\n+        stream->print(\"%s\", type2name(t));\n@@ -3205,0 +3246,14 @@\n+    if (at_old_sp) {\n+      stream->print(\"  (%s of caller)\", spname);\n+      did_old_sp = true;\n+    }\n+    stream->cr();\n+    sig_index += type2size[t];\n+    arg_index += 1;\n+  }\n+  if (!did_old_sp) {\n+    stream->print(\"  # \");\n+    stream->move_to(tab1);\n+    stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+    stream->print(\"  (%s of caller)\", spname);\n+    stream->cr();\n@@ -3328,1 +3383,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_vt=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_vt());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":149,"deletions":94,"binary":false,"changes":243,"status":"modified"},{"patch":"@@ -1274,1 +1274,1 @@\n-          if (vfst.method()->is_static_initializer() ||\n+        if (vfst.method()->is_class_initializer() ||\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/compiler\/oopMap.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -508,1 +508,2 @@\n-    if (klass->is_array_klass()) {\n+    \/\/ CMH: Valhalla flat arrays can split this work up, but for now, doesn't\n+    if (klass->is_array_klass() && !klass->is_flatArray_klass()) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManager.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -253,0 +253,1 @@\n+  oop obj_buffer_allocate(Klass* klass, int size, TRAPS); \/\/ doesn't clear memory\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -150,1 +150,1 @@\n-    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -319,1 +319,1 @@\n-bool ShenandoahBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+void ShenandoahBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -326,1 +326,1 @@\n-  return Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n+  Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -106,1 +106,1 @@\n-    number_of_result_handlers = 10                              \/\/ number of result handlers for native calls\n+    number_of_result_handlers = 11                              \/\/ number of result handlers for native calls\n","filename":"src\/hotspot\/share\/interpreter\/abstractInterpreter.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -548,0 +548,1 @@\n+    case Bytecodes::_withfield:\n@@ -572,0 +573,1 @@\n+    case Bytecodes::_defaultvalue:\n","filename":"src\/hotspot\/share\/interpreter\/bytecodeTracer.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -47,0 +48,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -77,0 +81,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -222,0 +227,4 @@\n+  if (klass->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_InstantiationError());\n+  }\n+\n@@ -246,0 +255,179 @@\n+void copy_primitive_argument(intptr_t* addr, Handle instance, int offset, BasicType type) {\n+  switch (type) {\n+  case T_BOOLEAN:\n+    instance()->bool_field_put(offset, (jboolean)*((int*)addr));\n+    break;\n+  case T_CHAR:\n+    instance()->char_field_put(offset, (jchar) *((int*)addr));\n+    break;\n+  case T_FLOAT:\n+    instance()->float_field_put(offset, (jfloat)*((float*)addr));\n+    break;\n+  case T_DOUBLE:\n+    instance()->double_field_put(offset, (jdouble)*((double*)addr));\n+    break;\n+  case T_BYTE:\n+    instance()->byte_field_put(offset, (jbyte)*((int*)addr));\n+    break;\n+  case T_SHORT:\n+    instance()->short_field_put(offset, (jshort)*((int*)addr));\n+    break;\n+  case T_INT:\n+    instance()->int_field_put(offset, (jint)*((int*)addr));\n+    break;\n+  case T_LONG:\n+    instance()->long_field_put(offset, (jlong)*((long long*)addr));\n+    break;\n+  case T_OBJECT:\n+  case T_ARRAY:\n+  case T_INLINE_TYPE:\n+    fatal(\"Should not be handled with this method\");\n+    break;\n+  default:\n+    fatal(\"Unsupported BasicType\");\n+  }\n+}\n+\n+JRT_ENTRY(void, InterpreterRuntime::defaultvalue(JavaThread* thread, ConstantPool* pool, int index))\n+  \/\/ Getting the InlineKlass\n+  Klass* k = pool->klass_at(index, CHECK);\n+  if (!k->is_inline_klass()) {\n+    \/\/ inconsistency with 'new' which throws an InstantiationError\n+    \/\/ in the future, defaultvalue will just return null instead of throwing an exception\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+  assert(k->is_inline_klass(), \"defaultvalue argument must be the inline type class\");\n+  InlineKlass* vklass = InlineKlass::cast(k);\n+\n+  vklass->initialize(THREAD);\n+  oop res = vklass->default_value();\n+  thread->set_vm_result(res);\n+JRT_END\n+\n+JRT_ENTRY(int, InterpreterRuntime::withfield(JavaThread* thread, ConstantPoolCache* cp_cache))\n+  LastFrameAccessor last_frame(thread);\n+  \/\/ Getting the InlineKlass\n+  int index = ConstantPool::decode_cpcache_index(last_frame.get_index_u2_cpcache(Bytecodes::_withfield));\n+  ConstantPoolCacheEntry* cp_entry = cp_cache->entry_at(index);\n+  assert(cp_entry->is_resolved(Bytecodes::_withfield), \"Should have been resolved\");\n+  Klass* klass = cp_entry->f1_as_klass();\n+  assert(klass->is_inline_klass(), \"withfield only applies to inline types\");\n+  InlineKlass* vklass = InlineKlass::cast(klass);\n+\n+  \/\/ Getting Field information\n+  int offset = cp_entry->f2_as_index();\n+  int field_index = cp_entry->field_index();\n+  int field_offset = cp_entry->f2_as_offset();\n+  Symbol* field_signature = vklass->field_signature(field_index);\n+  BasicType field_type = Signature::basic_type(field_signature);\n+  int return_offset = (type2size[field_type] + type2size[T_OBJECT]) * AbstractInterpreter::stackElementSize;\n+\n+  \/\/ Getting old value\n+  frame& f = last_frame.get_frame();\n+  jint tos_idx = f.interpreter_frame_expression_stack_size() - 1;\n+  int vt_offset = type2size[field_type];\n+  oop old_value = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx - vt_offset);\n+  assert(old_value != NULL && oopDesc::is_oop(old_value) && old_value->is_inline_type(),\"Verifying receiver\");\n+  Handle old_value_h(THREAD, old_value);\n+\n+  \/\/ Creating new value by copying the one passed in argument\n+  instanceOop new_value = vklass->allocate_instance_buffer(\n+      CHECK_((type2size[field_type]) * AbstractInterpreter::stackElementSize));\n+  Handle new_value_h = Handle(THREAD, new_value);\n+  vklass->inline_copy_oop_to_new_oop(old_value_h(), new_value_h());\n+\n+  \/\/ Updating the field specified in arguments\n+  if (field_type == T_ARRAY || field_type == T_OBJECT) {\n+    oop aoop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);\n+    assert(aoop == NULL || oopDesc::is_oop(aoop),\"argument must be a reference type\");\n+    new_value_h()->obj_field_put(field_offset, aoop);\n+  } else if (field_type == T_INLINE_TYPE) {\n+    if (cp_entry->is_inlined()) {\n+      oop vt_oop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);\n+      assert(vt_oop != NULL && oopDesc::is_oop(vt_oop) && vt_oop->is_inline_type(),\"argument must be an inline type\");\n+      InlineKlass* field_vk = InlineKlass::cast(vklass->get_inline_type_field_klass(field_index));\n+      assert(vt_oop != NULL && field_vk == vt_oop->klass(), \"Must match\");\n+      field_vk->write_inlined_field(new_value_h(), offset, vt_oop, CHECK_(return_offset));\n+    } else { \/\/ not inlined\n+      oop voop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);\n+      if (voop == NULL && cp_entry->is_inline_type()) {\n+        THROW_(vmSymbols::java_lang_NullPointerException(), return_offset);\n+      }\n+      assert(voop == NULL || oopDesc::is_oop(voop),\"checking argument\");\n+      new_value_h()->obj_field_put(field_offset, voop);\n+    }\n+  } else { \/\/ not T_OBJECT nor T_ARRAY nor T_INLINE_TYPE\n+    intptr_t* addr = f.interpreter_frame_expression_stack_at(tos_idx);\n+    copy_primitive_argument(addr, new_value_h, field_offset, field_type);\n+  }\n+\n+  \/\/ returning result\n+  thread->set_vm_result(new_value_h());\n+  return return_offset;\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::uninitialized_static_inline_type_field(JavaThread* thread, oopDesc* mirror, int index))\n+  \/\/ The interpreter tries to access an inline static field that has not been initialized.\n+  \/\/ This situation can happen in different scenarios:\n+  \/\/   1 - if the load or initialization of the field failed during step 8 of\n+  \/\/       the initialization of the holder of the field, in this case the access to the field\n+  \/\/       must fail\n+  \/\/   2 - it can also happen when the initialization of the holder class triggered the initialization of\n+  \/\/       another class which accesses this field in its static initializer, in this case the\n+  \/\/       access must succeed to allow circularity\n+  \/\/ The code below tries to load and initialize the field's class again before returning the default value.\n+  \/\/ If the field was not initialized because of an error, a exception should be thrown.\n+  \/\/ If the class is being initialized, the default value is returned.\n+  instanceHandle mirror_h(THREAD, (instanceOop)mirror);\n+  InstanceKlass* klass = InstanceKlass::cast(java_lang_Class::as_Klass(mirror));\n+  if (klass->is_being_initialized() && klass->is_reentrant_initialization(THREAD)) {\n+    int offset = klass->field_offset(index);\n+    Klass* field_k = klass->get_inline_type_field_klass_or_null(index);\n+    if (field_k == NULL) {\n+      field_k = SystemDictionary::resolve_or_fail(klass->field_signature(index)->fundamental_name(THREAD),\n+          Handle(THREAD, klass->class_loader()),\n+          Handle(THREAD, klass->protection_domain()),\n+          true, CHECK);\n+      assert(field_k != NULL, \"Should have been loaded or an exception thrown above\");\n+      klass->set_inline_type_field_klass(index, field_k);\n+    }\n+    field_k->initialize(CHECK);\n+    oop defaultvalue = InlineKlass::cast(field_k)->default_value();\n+    \/\/ It is safe to initialized the static field because 1) the current thread is the initializing thread\n+    \/\/ and is the only one that can access it, and 2) the field is actually not initialized (i.e. null)\n+    \/\/ otherwise the JVM should not be executing this code.\n+    mirror->obj_field_put(offset, defaultvalue);\n+    thread->set_vm_result(defaultvalue);\n+  } else {\n+    assert(klass->is_in_error_state(), \"If not initializing, initialization must have failed to get there\");\n+    ResourceMark rm(THREAD);\n+    const char* desc = \"Could not initialize class \";\n+    const char* className = klass->external_name();\n+    size_t msglen = strlen(desc) + strlen(className) + 1;\n+    char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+    if (NULL == message) {\n+      \/\/ Out of memory: can't create detailed error message\n+      THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), className);\n+    } else {\n+      jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+      THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), message);\n+    }\n+  }\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::read_inlined_field(JavaThread* thread, oopDesc* obj, int index, Klass* field_holder))\n+  Handle obj_h(THREAD, obj);\n+\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+\n+  assert(field_holder->is_instance_klass(), \"Sanity check\");\n+  InstanceKlass* klass = InstanceKlass::cast(field_holder);\n+\n+  assert(klass->field_is_inlined(index), \"Sanity check\");\n+\n+  InlineKlass* field_vklass = InlineKlass::cast(klass->get_inline_type_field_klass(index));\n+  assert(field_vklass->is_initialized(), \"Must be initialized at this point\");\n+\n+  oop res = field_vklass->read_inlined_field(obj_h(), klass->field_offset(index), CHECK);\n+  thread->set_vm_result(res);\n+JRT_END\n@@ -255,1 +443,8 @@\n-  objArrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n+  bool      is_qtype_desc = pool->tag_at(index).is_Qdescriptor_klass();\n+  arrayOop obj;\n+  if ((!klass->is_array_klass()) && is_qtype_desc) { \/\/ Logically creates elements, ensure klass init\n+    klass->initialize(CHECK);\n+    obj = oopFactory::new_flatArray(klass, size, CHECK);\n+  } else {\n+    obj = oopFactory::new_objArray(klass, size, CHECK);\n+  }\n@@ -259,0 +454,10 @@\n+JRT_ENTRY(void, InterpreterRuntime::value_array_load(JavaThread* thread, arrayOopDesc* array, int index))\n+  flatArrayHandle vah(thread, (flatArrayOop)array);\n+  oop value_holder = flatArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK);\n+  thread->set_vm_result(value_holder);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::value_array_store(JavaThread* thread, void* val, arrayOopDesc* array, int index))\n+  assert(val != NULL, \"can't store null into flat array\");\n+  ((flatArrayOop)array)->value_copy_to_index((oop)val, index);\n+JRT_END\n@@ -264,2 +469,3 @@\n-  int          i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n-  Klass* klass   = constants->klass_at(i, CHECK);\n+  int i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n+  Klass* klass = constants->klass_at(i, CHECK);\n+  bool is_qtype = klass->name()->is_Q_array_signature();\n@@ -270,0 +476,4 @@\n+  if (is_qtype) { \/\/ Logically creates elements, ensure klass init\n+    klass->initialize(CHECK);\n+  }\n+\n@@ -294,0 +504,23 @@\n+JRT_ENTRY(jboolean, InterpreterRuntime::is_substitutable(JavaThread* thread, oopDesc* aobj, oopDesc* bobj))\n+  assert(oopDesc::is_oop(aobj) && oopDesc::is_oop(bobj), \"must be valid oops\");\n+\n+  Handle ha(THREAD, aobj);\n+  Handle hb(THREAD, bobj);\n+  JavaValue result(T_BOOLEAN);\n+  JavaCallArguments args;\n+  args.push_oop(ha);\n+  args.push_oop(hb);\n+  methodHandle method(thread, Universe::is_substitutable_method());\n+  JavaCalls::call(&result, method, &args, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    \/\/ Something really bad happened because isSubstitutable() should not throw exceptions\n+    \/\/ If it is an error, just let it propagate\n+    \/\/ If it is an exception, wrap it into an InternalError\n+    if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+      Handle e(THREAD, PENDING_EXCEPTION);\n+      CLEAR_PENDING_EXCEPTION;\n+      THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in substitutability test\", e, false);\n+    }\n+  }\n+  return result.get_jboolean();\n+JRT_END\n@@ -618,0 +851,4 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_InstantiationError(JavaThread* thread))\n+  THROW(vmSymbols::java_lang_InstantiationError());\n+JRT_END\n+\n@@ -652,1 +889,1 @@\n-                    bytecode == Bytecodes::_putstatic);\n+                    bytecode == Bytecodes::_putstatic || bytecode == Bytecodes::_withfield);\n@@ -654,0 +891,1 @@\n+  bool is_inline_type  = bytecode == Bytecodes::_withfield;\n@@ -697,3 +935,9 @@\n-    get_code = ((is_static) ? Bytecodes::_getstatic : Bytecodes::_getfield);\n-    if ((is_put && !has_initialized_final_update) || !info.access_flags().is_final()) {\n-      put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_putfield);\n+    if (is_static) {\n+      get_code = Bytecodes::_getstatic;\n+    } else {\n+      get_code = Bytecodes::_getfield;\n+    }\n+    if (is_put && is_inline_type) {\n+        put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_withfield);\n+    } else if ((is_put && !has_initialized_final_update) || !info.access_flags().is_final()) {\n+        put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_putfield);\n@@ -712,0 +956,2 @@\n+    info.is_inlined(),\n+    info.is_inline_type(),\n@@ -948,0 +1194,1 @@\n+  case Bytecodes::_withfield:\n@@ -1169,0 +1416,1 @@\n+  bool is_inlined = cp_entry->is_inlined();\n@@ -1177,1 +1425,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(cp_entry_f1, cp_entry->f2_as_index(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(cp_entry_f1, cp_entry->f2_as_index(), is_static, is_inlined);\n@@ -1207,0 +1455,6 @@\n+\n+  \/\/ Both Q-signatures and L-signatures are mapped to atos\n+  if (cp_entry->flag_state() == atos && ik->field_signature(index)->is_Q_signature()) {\n+    sig_type = JVM_SIGNATURE_INLINE_TYPE;\n+  }\n+\n@@ -1208,0 +1462,1 @@\n+  bool is_inlined = cp_entry->is_inlined();\n@@ -1210,1 +1465,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, cp_entry->f2_as_index(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, cp_entry->f2_as_index(), is_static, is_inlined);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":264,"deletions":9,"binary":false,"changes":273,"status":"modified"},{"patch":"@@ -66,0 +66,10 @@\n+  static void    defaultvalue  (JavaThread* thread, ConstantPool* pool, int index);\n+  static int     withfield     (JavaThread* thread, ConstantPoolCache* cp_cache);\n+  static void    uninitialized_static_inline_type_field(JavaThread* thread, oopDesc* mirror, int offset);\n+  static void    write_heap_copy (JavaThread* thread, oopDesc* value, int offset, oopDesc* rcv);\n+  static void    read_inlined_field(JavaThread* thread, oopDesc* value, int index, Klass* field_holder);\n+\n+  static void value_array_load(JavaThread* thread, arrayOopDesc* array, int index);\n+  static void value_array_store(JavaThread* thread, void* val, arrayOopDesc* array, int index);\n+\n+  static jboolean is_substitutable(JavaThread* thread, oopDesc* aobj, oopDesc* bobj);\n@@ -77,0 +87,1 @@\n+  static void    throw_InstantiationError(JavaThread* thread);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -981,0 +981,1 @@\n+         byte == Bytecodes::_withfield ||\n@@ -985,1 +986,2 @@\n-  bool is_put    = (byte == Bytecodes::_putfield  || byte == Bytecodes::_putstatic || byte == Bytecodes::_nofast_putfield);\n+  bool is_put    = (byte == Bytecodes::_putfield  || byte == Bytecodes::_putstatic ||\n+                    byte == Bytecodes::_nofast_putfield || byte == Bytecodes::_withfield);\n@@ -1023,0 +1025,2 @@\n+    \/\/ (3) by withfield when field is in a value type and the\n+    \/\/     selected class and current class are nest mates.\n@@ -1026,6 +1030,15 @@\n-        ResourceMark rm(THREAD);\n-        stringStream ss;\n-        ss.print(\"Update to %s final field %s.%s attempted from a different class (%s) than the field's declaring class\",\n-                 is_static ? \"static\" : \"non-static\", resolved_klass->external_name(), fd.name()->as_C_string(),\n-                current_klass->external_name());\n-        THROW_MSG(vmSymbols::java_lang_IllegalAccessError(), ss.as_string());\n+        \/\/ If byte code is a withfield check if they are nestmates.\n+        bool are_nestmates = false;\n+        if (sel_klass->is_instance_klass() &&\n+            InstanceKlass::cast(sel_klass)->is_inline_klass() &&\n+            current_klass->is_instance_klass()) {\n+          are_nestmates = InstanceKlass::cast(current_klass)->has_nestmate_access_to(InstanceKlass::cast(sel_klass), THREAD);\n+        }\n+        if (!are_nestmates) {\n+          ResourceMark rm(THREAD);\n+          stringStream ss;\n+          ss.print(\"Update to %s final field %s.%s attempted from a different class (%s) than the field's declaring class\",\n+                   is_static ? \"static\" : \"non-static\", resolved_klass->external_name(), fd.name()->as_C_string(),\n+                    current_klass->external_name());\n+          THROW_MSG(vmSymbols::java_lang_IllegalAccessError(), ss.as_string());\n+        }\n@@ -1039,1 +1052,1 @@\n-                                                   !m->is_static_initializer());\n+                                                   !m->is_class_initializer());\n@@ -1042,1 +1055,1 @@\n-                                                     !m->is_object_initializer());\n+                                                     !m->is_object_constructor());\n@@ -1162,0 +1175,2 @@\n+  \/\/ Since this method is never inherited from a super, any appearance here under\n+  \/\/ the wrong class would be an error.\n@@ -1235,1 +1250,1 @@\n-      \/\/ check if the method is not <init>\n+      \/\/ check if the method is not <init>, which is never inherited\n@@ -1655,2 +1670,2 @@\n-                             const methodHandle& attached_method,\n-                             Bytecodes::Code byte, TRAPS) {\n+                                  const methodHandle& attached_method,\n+                                  Bytecodes::Code byte, bool check_null_and_abstract, TRAPS) {\n@@ -1661,0 +1676,1 @@\n+  Klass* recv_klass = recv.is_null() ? defc : recv->klass();\n@@ -1663,2 +1679,2 @@\n-      resolve_virtual_call(result, recv, recv->klass(), link_info,\n-                           \/*check_null_and_abstract=*\/true, CHECK);\n+      resolve_virtual_call(result, recv, recv_klass, link_info,\n+                           check_null_and_abstract, CHECK);\n@@ -1667,2 +1683,2 @@\n-      resolve_interface_call(result, recv, recv->klass(), link_info,\n-                             \/*check_null_and_abstract=*\/true, CHECK);\n+      resolve_interface_call(result, recv, recv_klass, link_info,\n+                             check_null_and_abstract, CHECK);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":32,"deletions":16,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -173,1 +173,1 @@\n-    int  cp_index    = Bytes::get_Java_u2(p);\n+    int cp_index    = Bytes::get_Java_u2(p);\n@@ -209,1 +209,0 @@\n-\n@@ -454,1 +453,1 @@\n-                  if (!method->is_static_initializer()) {\n+                  if (!method->is_class_initializer()) {\n@@ -458,1 +457,1 @@\n-                  if (!method->is_object_initializer()) {\n+                  if (!method->is_object_constructor()) {\n@@ -470,0 +469,1 @@\n+      case Bytecodes::_withfield     : \/\/ fall through but may require more checks for correctness\n","filename":"src\/hotspot\/share\/interpreter\/rewriter.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -87,1 +87,4 @@\n-    if (!mh->is_native() && !mh->is_static() && !mh->is_initializer()) {\n+    if (!mh->is_native() &&\n+        !mh->is_static() &&\n+        !mh->is_object_constructor() &&\n+        !mh->is_class_initializer()) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompiler.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1264,1 +1264,1 @@\n-              Deoptimization::reassign_fields(fst.current(), fst.register_map(), objects, realloc_failures, false);\n+              Deoptimization::reassign_fields(fst.current(), fst.register_map(), objects, realloc_failures, false, CHECK_NULL);\n@@ -1524,1 +1524,1 @@\n-  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false);\n+  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false, THREAD);\n@@ -1923,1 +1923,1 @@\n-    if (m->is_initializer() && !m->is_static()) {\n+    if (m->is_object_constructor()) {\n@@ -1953,1 +1953,1 @@\n-    if (!m->is_initializer() && !m->is_overpass()) {\n+    if (!(m->is_object_constructor() || m->is_class_initializer()) && !m->is_overpass()) {\n@@ -2574,2 +2574,1 @@\n-  if (m->is_initializer()) {\n-    if (m->is_static_initializer()) {\n+  if (m->is_class_initializer()) {\n@@ -2578,1 +2577,2 @@\n-    }\n+  }\n+  else if (m->is_object_constructor() || m->is_static_init_factory()) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -161,1 +161,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags,                                   u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags,                                   u4)                                    \\\n@@ -530,0 +530,2 @@\n+  declare_constant(DataLayout::array_load_store_data_tag)                 \\\n+  declare_constant(DataLayout::acmp_data_tag)                             \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -192,0 +192,1 @@\n+  LOG_TAG(valuetypes) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -54,0 +54,2 @@\n+  address _c2i_inline_ro_entry_trampoline;\n+  address _c2i_inline_entry_trampoline;\n@@ -57,0 +59,2 @@\n+  address c2i_inline_ro_entry_trampoline() { return _c2i_inline_ro_entry_trampoline; }\n+  address c2i_inline_entry_trampoline() { return _c2i_inline_entry_trampoline; }\n@@ -59,0 +63,2 @@\n+  void set_c2i_inline_ro_entry_trampoline(address addr) { _c2i_inline_ro_entry_trampoline = addr; }\n+  void set_c2i_inline_entry_trampoline(address addr) { _c2i_inline_entry_trampoline = addr; }\n@@ -347,1 +353,0 @@\n-    assert(type == _method_entry_ref, \"only special type allowed for now\");\n@@ -350,1 +355,1 @@\n-    _builder->add_special_ref(type, src_obj, field_offset);\n+    _builder->add_special_ref(type, src_obj, field_offset, ref->size() * BytesPerWord);\n@@ -394,4 +399,0 @@\n-void ArchiveBuilder::add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset) {\n-  _special_refs->append(SpecialRefInfo(type, src_obj, field_offset));\n-}\n-\n@@ -543,2 +544,18 @@\n-    assert(s.type() == MetaspaceClosure::_method_entry_ref, \"only special type allowed for now\");\n-    assert(*src_p == *dst_p, \"must be a copy\");\n+\n+    MetaspaceClosure::assert_valid(s.type());\n+    switch (s.type()) {\n+    case MetaspaceClosure::_method_entry_ref:\n+      assert(*src_p == *dst_p, \"must be a copy\");\n+      break;\n+    case MetaspaceClosure::_internal_pointer_ref:\n+      {\n+        \/\/ *src_p points to a location inside src_obj. Let's make *dst_p point to\n+        \/\/ the same location inside dst_obj.\n+        size_t off = pointer_delta(*((address*)src_p), src_obj, sizeof(u1));\n+        assert(off < s.src_obj_size_in_bytes(), \"must point to internal address\");\n+        *((address*)dst_p) = dst_obj + off;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n@@ -836,0 +853,4 @@\n+        info->set_c2i_inline_ro_entry_trampoline(\n+         (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size()));\n+        info->set_c2i_inline_entry_trampoline(\n+         (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size()));\n@@ -859,1 +880,1 @@\n-    align_up(SharedRuntime::trampoline_size(), BytesPerWord) +\n+    align_up(SharedRuntime::trampoline_size(), BytesPerWord) * 3 +\n@@ -905,0 +926,2 @@\n+        m->set_from_compiled_inline_ro_entry(info->c2i_inline_ro_entry_trampoline());\n+        m->set_from_compiled_inline_entry(info->c2i_inline_entry_trampoline());\n","filename":"src\/hotspot\/share\/memory\/archiveBuilder.cpp","additions":32,"deletions":9,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+    DEBUG_ONLY(size_t _src_obj_size_in_bytes;)\n@@ -59,2 +60,4 @@\n-    SpecialRefInfo(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset)\n-      : _type(type), _src_obj(src_obj), _field_offset(field_offset) {}\n+    SpecialRefInfo(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset, size_t src_obj_size_in_bytes)\n+      : _type(type), _src_obj(src_obj), _field_offset(field_offset) {\n+      DEBUG_ONLY(_src_obj_size_in_bytes = src_obj_size_in_bytes);\n+    }\n@@ -65,0 +68,2 @@\n+\n+    DEBUG_ONLY(size_t src_obj_size_in_bytes() const { return _src_obj_size_in_bytes; })\n@@ -242,1 +247,3 @@\n-  void add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset);\n+  void add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset, size_t src_obj_size_in_bytes) {\n+    _special_refs->append(SpecialRefInfo(type, src_obj, field_offset, src_obj_size_in_bytes));\n+  }\n","filename":"src\/hotspot\/share\/memory\/archiveBuilder.hpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -38,0 +38,2 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n+#include \"runtime\/reflectionUtils.hpp\"\n@@ -40,0 +42,1 @@\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n@@ -519,0 +522,132 @@\n+\n+class FindClassByNameClosure : public KlassInfoClosure {\n+ private:\n+  GrowableArray<Klass*>* _klasses;\n+  Symbol* _classname;\n+ public:\n+  FindClassByNameClosure(GrowableArray<Klass*>* klasses, Symbol* classname) :\n+    _klasses(klasses), _classname(classname) { }\n+\n+  void do_cinfo(KlassInfoEntry* cie) {\n+    if (cie->klass()->name() == _classname) {\n+      _klasses->append(cie->klass());\n+    }\n+  }\n+};\n+\n+class FieldDesc {\n+private:\n+  Symbol* _name;\n+  Symbol* _signature;\n+  int _offset;\n+  int _index;\n+  InstanceKlass* _holder;\n+  AccessFlags _access_flags;\n+ public:\n+  FieldDesc() {\n+    _name = NULL;\n+    _signature = NULL;\n+    _offset = -1;\n+    _index = -1;\n+    _holder = NULL;\n+    _access_flags = AccessFlags();\n+  }\n+  FieldDesc(fieldDescriptor& fd) {\n+    _name = fd.name();\n+    _signature = fd.signature();\n+    _offset = fd.offset();\n+    _index = fd.index();\n+    _holder = fd.field_holder();\n+    _access_flags = fd.access_flags();\n+  }\n+  const Symbol* name() { return _name;}\n+  const Symbol* signature() { return _signature; }\n+  const int offset() { return _offset; }\n+  const int index() { return _index; }\n+  const InstanceKlass* holder() { return _holder; }\n+  const AccessFlags& access_flags() { return _access_flags; }\n+  const bool is_inline_type() { return Signature::basic_type(_signature) == T_INLINE_TYPE; }\n+};\n+\n+static int compare_offset(FieldDesc* f1, FieldDesc* f2) {\n+   return f1->offset() > f2->offset() ? 1 : -1;\n+}\n+\n+static void print_field(outputStream* st, int level, int offset, FieldDesc& fd, bool is_inline_type, bool is_inlined ) {\n+  const char* inlined_msg = \"\";\n+  if (is_inline_type) {\n+    inlined_msg = is_inlined ? \"inlined\" : \"not inlined\";\n+  }\n+  st->print_cr(\"  @ %d %*s \\\"%s\\\" %s %s %s\",\n+      offset, level * 3, \"\",\n+      fd.name()->as_C_string(),\n+      fd.signature()->as_C_string(),\n+      is_inline_type ? \" \/\/ inline type \" : \"\",\n+      inlined_msg);\n+}\n+\n+static void print_inlined_field(outputStream* st, int level, int offset, InstanceKlass* klass) {\n+  assert(klass->is_inline_klass(), \"Only inline types can be inlined\");\n+  InlineKlass* vklass = InlineKlass::cast(klass);\n+  GrowableArray<FieldDesc>* fields = new (ResourceObj::C_HEAP, mtServiceability) GrowableArray<FieldDesc>(100, mtServiceability);\n+  for (FieldStream fd(klass, false, false); !fd.eos(); fd.next()) {\n+    if (!fd.access_flags().is_static()) {\n+      fields->append(FieldDesc(fd.field_descriptor()));\n+    }\n+  }\n+  fields->sort(compare_offset);\n+  for(int i = 0; i < fields->length(); i++) {\n+    FieldDesc fd = fields->at(i);\n+    int offset2 = offset + fd.offset() - vklass->first_field_offset();\n+    print_field(st, level, offset2, fd,\n+        fd.is_inline_type(), fd.holder()->field_is_inlined(fd.index()));\n+    if (fd.holder()->field_is_inlined(fd.index())) {\n+      print_inlined_field(st, level + 1, offset2 ,\n+          InstanceKlass::cast(fd.holder()->get_inline_type_field_klass(fd.index())));\n+    }\n+  }\n+}\n+\n+void PrintClassLayout::print_class_layout(outputStream* st, char* class_name) {\n+  KlassInfoTable cit(true);\n+  if (cit.allocation_failed()) {\n+    st->print_cr(\"ERROR: Ran out of C-heap; hierarchy not generated\");\n+    return;\n+  }\n+\n+  Thread* THREAD = Thread::current();\n+\n+  Symbol* classname = SymbolTable::probe(class_name, (int)strlen(class_name));\n+\n+  GrowableArray<Klass*>* klasses = new (ResourceObj::C_HEAP, mtServiceability) GrowableArray<Klass*>(100, mtServiceability);\n+\n+  FindClassByNameClosure fbnc(klasses, classname);\n+  cit.iterate(&fbnc);\n+\n+  for(int i = 0; i < klasses->length(); i++) {\n+    Klass* klass = klasses->at(i);\n+    if (!klass->is_instance_klass()) continue;  \/\/ Skip\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    int tab = 1;\n+    st->print_cr(\"Class %s [@%s]:\", klass->name()->as_C_string(),\n+        klass->class_loader_data()->name()->as_C_string());\n+    ResourceMark rm;\n+    GrowableArray<FieldDesc>* fields = new (ResourceObj::C_HEAP, mtServiceability) GrowableArray<FieldDesc>(100, mtServiceability);\n+    for (FieldStream fd(ik, false, false); !fd.eos(); fd.next()) {\n+      if (!fd.access_flags().is_static()) {\n+        fields->append(FieldDesc(fd.field_descriptor()));\n+      }\n+    }\n+    fields->sort(compare_offset);\n+    for(int i = 0; i < fields->length(); i++) {\n+      FieldDesc fd = fields->at(i);\n+      print_field(st, 0, fd.offset(), fd, fd.is_inline_type(), fd.holder()->field_is_inlined(fd.index()));\n+      if (fd.holder()->field_is_inlined(fd.index())) {\n+        print_inlined_field(st, 1, fd.offset(),\n+            InstanceKlass::cast(fd.holder()->get_inline_type_field_klass(fd.index())));\n+      }\n+    }\n+  }\n+  st->cr();\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/heapInspection.cpp","additions":135,"deletions":0,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -138,1 +138,1 @@\n-  assert(!p->mark().has_bias_pattern(),\n+  assert(!UseBiasedLocking || !p->mark().has_bias_pattern(),\n@@ -277,1 +277,1 @@\n-    archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original));\n+    archived_oop->set_mark(markWord::prototype_for_klass(archived_oop->klass()).copy_set_hash(hash_original));\n","filename":"src\/hotspot\/share\/memory\/heapShared.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -82,1 +82,5 @@\n-    _method_entry_ref\n+    \/\/ A field that points to a method entry. E.g., Method::_i2i_entry\n+    _method_entry_ref,\n+\n+    \/\/ A field that points to a location inside the current object.\n+    _internal_pointer_ref,\n@@ -372,1 +376,9 @@\n-    push_special(_method_entry_ref, ref, (intptr_t*)p);\n+    push_special(_method_entry_ref, ref, p);\n+    if (!ref->keep_after_pushing()) {\n+      delete ref;\n+    }\n+  }\n+\n+  template <class T> void push_internal_pointer(T** mpp, intptr_t* p) {\n+    Ref* ref = new MSORef<T>(mpp, _default);\n+    push_special(_internal_pointer_ref, ref, p);\n@@ -381,1 +393,5 @@\n-    assert(type == _method_entry_ref, \"only special type allowed for now\");\n+    assert_valid(type);\n+  }\n+\n+  static void assert_valid(SpecialRef type) {\n+    assert(type == _method_entry_ref || type == _internal_pointer_ref, \"only special types allowed for now\");\n","filename":"src\/hotspot\/share\/memory\/metaspaceClosure.hpp","additions":19,"deletions":3,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -61,0 +61,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/memory\/metaspaceShared.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,0 +33,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n@@ -36,0 +39,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -126,0 +130,36 @@\n+arrayOop oopFactory::new_flatArray(Klass* klass, int length, TRAPS) {\n+  assert(klass->is_inline_klass(), \"Klass must be inline type\");\n+  \/\/ Request flattened, but we might not actually get it...either way \"null-free\" are the aaload\/aastore semantics\n+  Klass* array_klass = klass->array_klass(1, CHECK_NULL);\n+  assert(array_klass->is_null_free_array_klass(), \"Expect a null-free array class here\");\n+\n+  arrayOop oop;\n+  if (array_klass->is_flatArray_klass()) {\n+    oop = (arrayOop) FlatArrayKlass::cast(array_klass)->allocate(length, THREAD);\n+    assert(oop == NULL || oop->is_flatArray(), \"sanity\");\n+    assert(oop == NULL || oop->klass()->is_flatArray_klass(), \"sanity\");\n+  } else {\n+    oop = (arrayOop) ObjArrayKlass::cast(array_klass)->allocate(length, THREAD);\n+  }\n+  assert(oop == NULL || oop->klass()->is_null_free_array_klass(), \"sanity\");\n+  assert(oop == NULL || oop->is_nullfreeArray(), \"sanity\");\n+  return oop;\n+}\n+\n+objArrayHandle oopFactory::copy_flatArray_to_objArray(flatArrayHandle array, TRAPS) {\n+  int len = array->length();\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(array->klass());\n+  objArrayOop oarray = new_objectArray(array->length(), CHECK_(objArrayHandle()));\n+  objArrayHandle oarrayh(THREAD, oarray);\n+  vak->copy_array(array(), 0, oarrayh(), 0, len, CHECK_(objArrayHandle()));\n+  return oarrayh;\n+}\n+\n+objArrayHandle  oopFactory::ensure_objArray(oop array, TRAPS) {\n+  if (array != NULL && array->is_flatArray()) {\n+    return copy_flatArray_to_objArray(flatArrayHandle(THREAD, flatArrayOop(array)), THREAD);\n+  } else {\n+    return objArrayHandle(THREAD, objArrayOop(array));\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/oopFactory.cpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -121,0 +121,2 @@\n+LatestMethodCache* Universe::_is_substitutable_cache  = NULL;\n+LatestMethodCache* Universe::_inline_type_hash_code_cache = NULL;\n@@ -129,0 +131,1 @@\n+Array<InstanceKlass*>* Universe::_the_single_IdentityObject_klass_array = NULL;\n@@ -219,0 +222,1 @@\n+  it->push(&_the_single_IdentityObject_klass_array);\n@@ -225,0 +229,2 @@\n+  _is_substitutable_cache->metaspace_pointers_do(it);\n+  _inline_type_hash_code_cache->metaspace_pointers_do(it);\n@@ -267,0 +273,1 @@\n+  f->do_ptr((void**)&_the_single_IdentityObject_klass_array);\n@@ -272,0 +279,2 @@\n+  _is_substitutable_cache->serialize(f);\n+  _inline_type_hash_code_cache->serialize(f);\n@@ -321,1 +330,1 @@\n-        _the_array_interfaces_array     = MetadataFactory::new_array<Klass*>(null_cld, 2, NULL, CHECK);\n+        _the_array_interfaces_array     = MetadataFactory::new_array<Klass*>(null_cld, 3, NULL, CHECK);\n@@ -348,0 +357,5 @@\n+      assert(_the_array_interfaces_array->at(2) ==\n+                   vmClasses::IdentityObject_klass(), \"u3\");\n+\n+      assert(_the_single_IdentityObject_klass_array->at(0) ==\n+          vmClasses::IdentityObject_klass(), \"u3\");\n@@ -354,0 +368,1 @@\n+      _the_array_interfaces_array->at_put(2, vmClasses::IdentityObject_klass());\n@@ -458,0 +473,8 @@\n+void Universe::initialize_the_single_IdentityObject_klass_array(InstanceKlass* ik, TRAPS) {\n+    assert(_the_single_IdentityObject_klass_array == NULL, \"Must not be initialized twice\");\n+    assert(ik->name() == vmSymbols::java_lang_IdentityObject(), \"Must be\");\n+    Array<InstanceKlass*>* array = MetadataFactory::new_array<InstanceKlass*>(ik->class_loader_data(), 1, NULL, CHECK);\n+    array->at_put(0, ik);\n+    _the_single_IdentityObject_klass_array = array;\n+  }\n+\n@@ -746,1 +769,0 @@\n-\n@@ -769,0 +791,2 @@\n+  Universe::_is_substitutable_cache = new LatestMethodCache();\n+  Universe::_inline_type_hash_code_cache = new LatestMethodCache();\n@@ -927,0 +951,11 @@\n+\n+  \/\/ Set up substitutability testing\n+  ResourceMark rm;\n+  initialize_known_method(_is_substitutable_cache,\n+                          vmClasses::ValueBootstrapMethods_klass(),\n+                          vmSymbols::isSubstitutable_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true, CHECK);\n+  initialize_known_method(_inline_type_hash_code_cache,\n+                          vmClasses::ValueBootstrapMethods_klass(),\n+                          vmSymbols::inlineObjectHashCode_name()->as_C_string(),\n+                          vmSymbols::object_int_signature(), true, CHECK);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":37,"deletions":2,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -119,0 +119,2 @@\n+  static LatestMethodCache* _is_substitutable_cache;   \/\/ ValueBootstrapMethods.isSubstitutable() method\n+  static LatestMethodCache* _inline_type_hash_code_cache;  \/\/ ValueBootstrapMethods.inlineObjectHashCode() method\n@@ -124,0 +126,1 @@\n+  static Array<InstanceKlass*>* _the_single_IdentityObject_klass_array;\n@@ -146,0 +149,1 @@\n+\n@@ -262,0 +266,3 @@\n+  static Method*      is_substitutable_method()       { return _is_substitutable_cache->get_method(); }\n+  static Method*      inline_type_hash_code_method()  { return _inline_type_hash_code_cache->get_method(); }\n+\n@@ -286,0 +293,6 @@\n+  static Array<InstanceKlass*>*  the_single_IdentityObject_klass_array() {\n+    assert(_the_single_IdentityObject_klass_array != NULL, \"Must be initialized before use\");\n+    assert(_the_single_IdentityObject_klass_array->length() == 1, \"Sanity check\");\n+    return _the_single_IdentityObject_klass_array;\n+  }\n+  static void initialize_the_single_IdentityObject_klass_array(InstanceKlass* ik, TRAPS);\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"oops\/objArrayKlass.hpp\"\n@@ -101,0 +103,24 @@\n+Symbol* ArrayKlass::create_element_klass_array_name(Klass* element_klass, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  Symbol* name = NULL;\n+  bool is_qtype = element_klass->is_inline_klass();\n+  char *name_str = element_klass->name()->as_C_string();\n+  int len = element_klass->name()->utf8_length();\n+  char *new_str = NEW_RESOURCE_ARRAY(char, len + 4);\n+  int idx = 0;\n+  new_str[idx++] = JVM_SIGNATURE_ARRAY;\n+  if (element_klass->is_instance_klass()) { \/\/ it could be an array or simple type\n+    if (is_qtype) {\n+      new_str[idx++] = JVM_SIGNATURE_INLINE_TYPE;\n+    } else {\n+      new_str[idx++] = JVM_SIGNATURE_CLASS;\n+    }\n+  }\n+  memcpy(&new_str[idx], name_str, len * sizeof(char));\n+  idx += len;\n+  if (element_klass->is_instance_klass()) {\n+    new_str[idx++] = JVM_SIGNATURE_ENDCLASS;\n+  }\n+  new_str[idx++] = '\\0';\n+  return SymbolTable::new_symbol(new_str);\n+}\n@@ -156,0 +182,4 @@\n+oop ArrayKlass::component_mirror() const {\n+  return java_lang_Class::component_mirror(java_mirror());\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.cpp","additions":30,"deletions":0,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -45,0 +45,7 @@\n+  Klass* _element_klass;            \/\/ The klass of the elements of this array type\n+                                    \/\/ The element type must be registered for both object arrays\n+                                    \/\/ (incl. object arrays with value type elements) and value type\n+                                    \/\/ arrays containing flattened value types. However, the element\n+                                    \/\/ type must not be registered for arrays of primitive types.\n+                                    \/\/ TODO: Update the class hierarchy so that element klass appears\n+                                    \/\/ only in array that contain non-primitive types.\n@@ -51,0 +58,3 @@\n+  \/\/ Create array_name for element klass\n+  static Symbol* create_element_klass_array_name(Klass* element_klass, TRAPS);\n+\n@@ -52,0 +62,11 @@\n+  \/\/ Instance variables\n+  virtual Klass* element_klass() const      { return _element_klass; }\n+  virtual void set_element_klass(Klass* k)  { _element_klass = k; }\n+\n+  \/\/ Compiler\/Interpreter offset\n+  static ByteSize element_klass_offset() { return in_ByteSize(offset_of(ArrayKlass, _element_klass)); }\n+\n+  \/\/ Are loads and stores to this concrete array type atomic?\n+  \/\/ Note that Object[] is naturally atomic, but its subtypes may not be.\n+  virtual bool element_access_is_atomic() { return true; }\n+\n@@ -102,0 +123,2 @@\n+  oop component_mirror() const;\n+\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.hpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -233,1 +234,1 @@\n-      \/\/ All of these should have been reverted back to ClassIndex before calling\n+      \/\/ All of these should have been reverted back to Unresolved before calling\n@@ -257,0 +258,1 @@\n+  jbyte qdesc_bit = (name->is_Q_signature()) ? (jbyte) JVM_CONSTANT_QDescBit : 0;\n@@ -258,1 +260,1 @@\n-    release_tag_at_put(class_index, JVM_CONSTANT_Class);\n+    release_tag_at_put(class_index, JVM_CONSTANT_Class | qdesc_bit);\n@@ -260,1 +262,1 @@\n-    release_tag_at_put(class_index, JVM_CONSTANT_UnresolvedClass);\n+    release_tag_at_put(class_index, JVM_CONSTANT_UnresolvedClass | qdesc_bit);\n@@ -274,0 +276,1 @@\n+  assert(!k->name()->is_Q_signature(), \"Q-type without JVM_CONSTANT_QDescBit\");\n@@ -410,0 +413,1 @@\n+    jbyte qdesc_bit = tag_at(index).is_Qdescriptor_klass() ? (jbyte) JVM_CONSTANT_QDescBit : 0;\n@@ -411,1 +415,1 @@\n-      tag_at_put(index, JVM_CONSTANT_UnresolvedClass);\n+      tag_at_put(index, JVM_CONSTANT_UnresolvedClass | qdesc_bit);\n@@ -436,1 +440,1 @@\n-        tag_at_put(index, JVM_CONSTANT_UnresolvedClass);\n+        tag_at_put(index, JVM_CONSTANT_UnresolvedClass | qdesc_bit);\n@@ -486,0 +490,6 @@\n+void check_is_inline_type(Klass* k, TRAPS) {\n+  if (!k->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+}\n+\n@@ -519,0 +529,5 @@\n+  bool inline_type_signature = false;\n+  if (name->is_Q_signature()) {\n+    name = name->fundamental_name(THREAD);\n+    inline_type_signature = true;\n+  }\n@@ -528,0 +543,3 @@\n+  if (inline_type_signature) {\n+    name->decrement_refcount();\n+  }\n@@ -536,0 +554,16 @@\n+  if (!HAS_PENDING_EXCEPTION && inline_type_signature) {\n+    check_is_inline_type(k, THREAD);\n+  }\n+\n+  if (!HAS_PENDING_EXCEPTION) {\n+    Klass* bottom_klass = NULL;\n+    if (k->is_objArray_klass()) {\n+      bottom_klass = ObjArrayKlass::cast(k)->bottom_klass();\n+      assert(bottom_klass != NULL, \"Should be set\");\n+      assert(bottom_klass->is_instance_klass() || bottom_klass->is_typeArray_klass(), \"Sanity check\");\n+    } else if (k->is_flatArray_klass()) {\n+      bottom_klass = FlatArrayKlass::cast(k)->element_klass();\n+      assert(bottom_klass != NULL, \"Should be set\");\n+    }\n+  }\n+\n@@ -540,1 +574,5 @@\n-      save_and_throw_exception(this_cp, which, constantTag(JVM_CONSTANT_UnresolvedClass), CHECK_NULL);\n+      jbyte tag = JVM_CONSTANT_UnresolvedClass;\n+      if (this_cp->tag_at(which).is_Qdescriptor_klass()) {\n+        tag |= JVM_CONSTANT_QDescBit;\n+      }\n+      save_and_throw_exception(this_cp, which, constantTag(tag), CHECK_NULL);\n@@ -561,1 +599,5 @@\n-  this_cp->release_tag_at_put(which, JVM_CONSTANT_Class);\n+  jbyte tag = JVM_CONSTANT_Class;\n+  if (this_cp->tag_at(which).is_Qdescriptor_klass()) {\n+    tag |= JVM_CONSTANT_QDescBit;\n+  }\n+  this_cp->release_tag_at_put(which, tag);\n@@ -1885,0 +1927,6 @@\n+      case (JVM_CONSTANT_Class | JVM_CONSTANT_QDescBit): {\n+        idx1 = Bytes::get_Java_u2(bytes);\n+        printf(\"qclass        #%03d\", idx1);\n+        ent_size = 2;\n+        break;\n+      }\n@@ -1927,0 +1975,4 @@\n+      case (JVM_CONSTANT_UnresolvedClass | JVM_CONSTANT_QDescBit): {\n+        printf(\"UnresolvedQClass: %s\", WARN_MSG);\n+        break;\n+      }\n@@ -2098,0 +2150,1 @@\n+        assert(!tag_at(idx).is_Qdescriptor_klass(), \"Failed to encode QDesc\");\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":60,"deletions":7,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -307,1 +307,1 @@\n-  \/\/ For temporary use while constructing constant pool\n+  \/\/ For temporary use while constructing constant pool. Used during a retransform\/class redefinition as well.\n@@ -317,0 +317,9 @@\n+  void unresolved_qdescriptor_at_put(int which, int name_index, int resolved_klass_index) {\n+      release_tag_at_put(which, JVM_CONSTANT_UnresolvedClass | (jbyte) JVM_CONSTANT_QDescBit);\n+\n+      assert((name_index & 0xffff0000) == 0, \"must be\");\n+      assert((resolved_klass_index & 0xffff0000) == 0, \"must be\");\n+      *int_at_addr(which) =\n+        build_int_from_shorts((jushort)resolved_klass_index, (jushort)name_index);\n+    }\n+\n","filename":"src\/hotspot\/share\/oops\/constantPool.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -140,0 +140,2 @@\n+                                       bool is_inlined,\n+                                       bool is_inline_type,\n@@ -145,0 +147,1 @@\n+  assert(!is_inlined || is_inline_type, \"Sanity check\");\n@@ -147,1 +150,3 @@\n-                  ((is_final    ? 1 : 0) << is_final_shift),\n+                  ((is_final    ? 1 : 0) << is_final_shift) |\n+                  ((is_inlined  ? 1 : 0) << is_inlined_shift) |\n+                  ((is_inline_type ? 1 : 0) << is_inline_type_shift),\n@@ -305,0 +310,1 @@\n+      invoke_code = Bytecodes::_invokevirtual;\n@@ -320,1 +326,1 @@\n-    set_bytecode_2(Bytecodes::_invokevirtual);\n+    set_bytecode_2(invoke_code);\n","filename":"src\/hotspot\/share\/oops\/cpCache.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,504 @@\n+\/*\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/moduleEntry.hpp\"\n+#include \"classfile\/packageEntry.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/metadataFactory.hpp\"\n+#include \"memory\/metaspaceClosure.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/arrayKlass.inline.hpp\"\n+#include \"oops\/arrayOop.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/klass.inline.hpp\"\n+#include \"oops\/objArrayKlass.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/verifyOopClosure.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+#include \"oops\/flatArrayKlass.hpp\"\n+\n+\/\/ Allocation...\n+\n+FlatArrayKlass::FlatArrayKlass(Klass* element_klass, Symbol* name) : ArrayKlass(name, ID) {\n+  assert(element_klass->is_inline_klass(), \"Expected Inline\");\n+\n+  set_element_klass(InlineKlass::cast(element_klass));\n+  set_class_loader_data(element_klass->class_loader_data());\n+\n+  set_layout_helper(array_layout_helper(InlineKlass::cast(element_klass)));\n+  assert(is_array_klass(), \"sanity\");\n+  assert(is_flatArray_klass(), \"sanity\");\n+  assert(is_null_free_array_klass(), \"sanity\");\n+\n+  set_prototype_header(markWord::flat_array_prototype());\n+  assert(prototype_header().is_flat_array(), \"sanity\");\n+\n+#ifndef PRODUCT\n+  if (PrintFlatArrayLayout) {\n+    print();\n+  }\n+#endif\n+}\n+\n+InlineKlass* FlatArrayKlass::element_klass() const {\n+  return InlineKlass::cast(_element_klass);\n+}\n+\n+void FlatArrayKlass::set_element_klass(Klass* k) {\n+  _element_klass = k;\n+}\n+\n+FlatArrayKlass* FlatArrayKlass::allocate_klass(Klass* element_klass, TRAPS) {\n+  guarantee((!Universe::is_bootstrapping() || vmClasses::Object_klass_loaded()), \"Really ?!\");\n+  assert(UseFlatArray, \"Flatten array required\");\n+  assert(InlineKlass::cast(element_klass)->is_naturally_atomic() || (!InlineArrayAtomicAccess), \"Atomic by-default\");\n+\n+  \/*\n+   *  MVT->LWorld, now need to allocate secondaries array types, just like objArrayKlass...\n+   *  ...so now we are trying out covariant array types, just copy objArrayKlass\n+   *  TODO refactor any remaining commonality\n+   *\n+   *\/\n+  \/\/ Eagerly allocate the direct array supertype.\n+  Klass* super_klass = NULL;\n+  Klass* element_super = element_klass->super();\n+  if (element_super != NULL) {\n+    \/\/ The element type has a direct super.  E.g., String[] has direct super of Object[].\n+    super_klass = element_super->array_klass_or_null();\n+    bool supers_exist = super_klass != NULL;\n+    \/\/ Also, see if the element has secondary supertypes.\n+    \/\/ We need an array type for each.\n+    const Array<Klass*>* element_supers = element_klass->secondary_supers();\n+    for( int i = element_supers->length()-1; i >= 0; i-- ) {\n+      Klass* elem_super = element_supers->at(i);\n+      if (elem_super->array_klass_or_null() == NULL) {\n+        supers_exist = false;\n+        break;\n+      }\n+    }\n+    if (!supers_exist) {\n+      \/\/ Oops.  Not allocated yet.  Back out, allocate it, and retry.\n+      Klass* ek = NULL;\n+      {\n+        MutexUnlocker mu(MultiArray_lock);\n+        super_klass = element_super->array_klass(CHECK_NULL);\n+        for( int i = element_supers->length()-1; i >= 0; i-- ) {\n+          Klass* elem_super = element_supers->at(i);\n+          elem_super->array_klass(CHECK_NULL);\n+        }\n+        \/\/ Now retry from the beginning\n+        ek = element_klass->array_klass(CHECK_NULL);\n+      }  \/\/ re-lock\n+      return FlatArrayKlass::cast(ek);\n+    }\n+  }\n+\n+  Symbol* name = ArrayKlass::create_element_klass_array_name(element_klass, CHECK_NULL);\n+  ClassLoaderData* loader_data = element_klass->class_loader_data();\n+  int size = ArrayKlass::static_size(FlatArrayKlass::header_size());\n+  FlatArrayKlass* vak = new (loader_data, size, THREAD) FlatArrayKlass(element_klass, name);\n+\n+  ModuleEntry* module = vak->module();\n+  assert(module != NULL, \"No module entry for array\");\n+  complete_create_array_klass(vak, super_klass, module, CHECK_NULL);\n+\n+  loader_data->add_class(vak);\n+\n+  return vak;\n+}\n+\n+void FlatArrayKlass::initialize(TRAPS) {\n+  element_klass()->initialize(THREAD);\n+}\n+\n+\/\/ Oops allocation...\n+flatArrayOop FlatArrayKlass::allocate(int length, TRAPS) {\n+  check_array_allocation_length(length, max_elements(), CHECK_NULL);\n+  int size = flatArrayOopDesc::object_size(layout_helper(), length);\n+  return (flatArrayOop) Universe::heap()->array_allocate(this, size, length, true, THREAD);\n+}\n+\n+\n+oop FlatArrayKlass::multi_allocate(int rank, jint* last_size, TRAPS) {\n+  \/\/ For flatArrays this is only called for the last dimension\n+  assert(rank == 1, \"just checking\");\n+  int length = *last_size;\n+  return allocate(length, THREAD);\n+}\n+\n+jint FlatArrayKlass::array_layout_helper(InlineKlass* vk) {\n+  BasicType etype = T_INLINE_TYPE;\n+  int esize = log2i_exact(round_up_power_of_2(vk->get_exact_size_in_bytes()));\n+  int hsize = arrayOopDesc::base_offset_in_bytes(etype);\n+\n+  int lh = Klass::array_layout_helper(_lh_array_tag_vt_value, true, hsize, etype, esize);\n+\n+  assert(lh < (int)_lh_neutral_value, \"must look like an array layout\");\n+  assert(layout_helper_is_array(lh), \"correct kind\");\n+  assert(layout_helper_is_flatArray(lh), \"correct kind\");\n+  assert(!layout_helper_is_typeArray(lh), \"correct kind\");\n+  assert(!layout_helper_is_objArray(lh), \"correct kind\");\n+  assert(layout_helper_is_null_free(lh), \"correct kind\");\n+  assert(layout_helper_header_size(lh) == hsize, \"correct decode\");\n+  assert(layout_helper_element_type(lh) == etype, \"correct decode\");\n+  assert(layout_helper_log2_element_size(lh) == esize, \"correct decode\");\n+  assert((1 << esize) < BytesPerLong || is_aligned(hsize, HeapWordsPerLong), \"unaligned base\");\n+\n+  return lh;\n+}\n+\n+int FlatArrayKlass::oop_size(oop obj) const {\n+  assert(obj->klass()->is_flatArray_klass(),\"must be an flat array\");\n+  flatArrayOop array = flatArrayOop(obj);\n+  return array->object_size();\n+}\n+\n+\/\/ For now return the maximum number of array elements that will not exceed:\n+\/\/ nof bytes = \"max_jint * HeapWord\" since the \"oopDesc::oop_iterate_size\"\n+\/\/ returns \"int\" HeapWords, need fix for JDK-4718400 and JDK-8233189\n+jint FlatArrayKlass::max_elements() const {\n+  \/\/ Check the max number of heap words limit first (because of int32_t in oopDesc_oop_size() etc)\n+  size_t max_size = max_jint;\n+  max_size -= arrayOopDesc::header_size(T_INLINE_TYPE);\n+  max_size = align_down(max_size, MinObjAlignment);\n+  max_size <<= LogHeapWordSize;                                  \/\/ convert to max payload size in bytes\n+  max_size >>= layout_helper_log2_element_size(_layout_helper);  \/\/ divide by element size (in bytes) = max elements\n+  \/\/ Within int32_t heap words, still can't exceed Java array element limit\n+  if (max_size > max_jint) {\n+    max_size = max_jint;\n+  }\n+  assert((max_size >> LogHeapWordSize) <= max_jint, \"Overflow\");\n+  return (jint) max_size;\n+}\n+\n+oop FlatArrayKlass::protection_domain() const {\n+  return element_klass()->protection_domain();\n+}\n+\n+\/\/ Temp hack having this here: need to move towards Access API\n+static bool needs_backwards_copy(arrayOop s, int src_pos,\n+                                 arrayOop d, int dst_pos, int length) {\n+  return (s == d) && (dst_pos > src_pos) && (dst_pos - src_pos) < length;\n+}\n+\n+void FlatArrayKlass::copy_array(arrayOop s, int src_pos,\n+                                arrayOop d, int dst_pos, int length, TRAPS) {\n+\n+  assert(s->is_objArray() || s->is_flatArray(), \"must be obj or flat array\");\n+\n+   \/\/ Check destination\n+   if ((!d->is_flatArray()) && (!d->is_objArray())) {\n+     THROW(vmSymbols::java_lang_ArrayStoreException());\n+   }\n+\n+   \/\/ Check if all offsets and lengths are non negative\n+   if (src_pos < 0 || dst_pos < 0 || length < 0) {\n+     THROW(vmSymbols::java_lang_ArrayIndexOutOfBoundsException());\n+   }\n+   \/\/ Check if the ranges are valid\n+   if  ( (((unsigned int) length + (unsigned int) src_pos) > (unsigned int) s->length())\n+      || (((unsigned int) length + (unsigned int) dst_pos) > (unsigned int) d->length()) ) {\n+     THROW(vmSymbols::java_lang_ArrayIndexOutOfBoundsException());\n+   }\n+   \/\/ Check zero copy\n+   if (length == 0)\n+     return;\n+\n+   ArrayKlass* sk = ArrayKlass::cast(s->klass());\n+   ArrayKlass* dk = ArrayKlass::cast(d->klass());\n+   Klass* d_elem_klass = dk->element_klass();\n+   Klass* s_elem_klass = sk->element_klass();\n+   \/**** CMH: compare and contrast impl, re-factor once we find edge cases... ****\/\n+\n+   if (sk->is_flatArray_klass()) {\n+     assert(sk == this, \"Unexpected call to copy_array\");\n+     \/\/ Check subtype, all src homogeneous, so just once\n+     if (!s_elem_klass->is_subtype_of(d_elem_klass)) {\n+       THROW(vmSymbols::java_lang_ArrayStoreException());\n+     }\n+\n+     flatArrayOop sa = flatArrayOop(s);\n+     InlineKlass* s_elem_vklass = element_klass();\n+\n+     \/\/ flatArray-to-flatArray\n+     if (dk->is_flatArray_klass()) {\n+       \/\/ element types MUST be exact, subtype check would be dangerous\n+       if (dk != this) {\n+         THROW(vmSymbols::java_lang_ArrayStoreException());\n+       }\n+\n+       flatArrayOop da = flatArrayOop(d);\n+       address dst = (address) da->value_at_addr(dst_pos, layout_helper());\n+       address src = (address) sa->value_at_addr(src_pos, layout_helper());\n+       if (contains_oops()) {\n+         int elem_incr = 1 << log2_element_size();\n+         address src_end = src + (length << log2_element_size());\n+         if (needs_backwards_copy(s, src_pos, d, dst_pos, length)) {\n+           swap(src, src_end);\n+           dst = dst + (length << log2_element_size());\n+           do {\n+             src -= elem_incr;\n+             dst -= elem_incr;\n+             HeapAccess<>::value_copy(src, dst, s_elem_vklass);\n+           } while (src > src_end);\n+         } else {\n+           address src_end = src + (length << log2_element_size());\n+           while (src < src_end) {\n+             HeapAccess<>::value_copy(src, dst, s_elem_vklass);\n+             src += elem_incr;\n+             dst += elem_incr;\n+           }\n+         }\n+       } else {\n+         \/\/ we are basically a type array...don't bother limiting element copy\n+         \/\/ it would have to be a lot wasted space to be worth value_store() calls, need a setting here ?\n+         Copy::conjoint_memory_atomic(src, dst, (size_t)length << log2_element_size());\n+       }\n+     }\n+     else { \/\/ flatArray-to-objArray\n+       assert(dk->is_objArray_klass(), \"Expected objArray here\");\n+       \/\/ Need to allocate each new src elem payload -> dst oop\n+       objArrayHandle dh(THREAD, (objArrayOop)d);\n+       flatArrayHandle sh(THREAD, sa);\n+       int dst_end = dst_pos + length;\n+       while (dst_pos < dst_end) {\n+         oop o = flatArrayOopDesc::value_alloc_copy_from_index(sh, src_pos, CHECK);\n+         dh->obj_at_put(dst_pos, o);\n+         dst_pos++;\n+         src_pos++;\n+       }\n+     }\n+   } else {\n+     assert(s->is_objArray(), \"Expected objArray\");\n+     objArrayOop sa = objArrayOop(s);\n+     assert(d->is_flatArray(), \"Excepted flatArray\");  \/\/ objArray-to-flatArray\n+     InlineKlass* d_elem_vklass = InlineKlass::cast(d_elem_klass);\n+     flatArrayOop da = flatArrayOop(d);\n+\n+     int src_end = src_pos + length;\n+     int delem_incr = 1 << dk->log2_element_size();\n+     address dst = (address) da->value_at_addr(dst_pos, layout_helper());\n+     while (src_pos < src_end) {\n+       oop se = sa->obj_at(src_pos);\n+       if (se == NULL) {\n+         THROW(vmSymbols::java_lang_NullPointerException());\n+       }\n+       \/\/ Check exact type per element\n+       if (se->klass() != d_elem_klass) {\n+         THROW(vmSymbols::java_lang_ArrayStoreException());\n+       }\n+       d_elem_vklass->inline_copy_oop_to_payload(se, dst);\n+       dst += delem_incr;\n+       src_pos++;\n+     }\n+   }\n+}\n+\n+\n+Klass* FlatArrayKlass::array_klass_impl(bool or_null, int n, TRAPS) {\n+  assert(dimension() <= n, \"check order of chain\");\n+  int dim = dimension();\n+  if (dim == n) return this;\n+\n+  if (higher_dimension_acquire() == NULL) {\n+    if (or_null)  return NULL;\n+\n+    ResourceMark rm;\n+    {\n+      \/\/ Ensure atomic creation of higher dimensions\n+      MutexLocker mu(THREAD, MultiArray_lock);\n+\n+      \/\/ Check if another thread beat us\n+      if (higher_dimension() == NULL) {\n+\n+        \/\/ Create multi-dim klass object and link them together\n+        Klass* k =\n+          ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, CHECK_NULL);\n+        ObjArrayKlass* ak = ObjArrayKlass::cast(k);\n+        ak->set_lower_dimension(this);\n+        OrderAccess::storestore();\n+        release_set_higher_dimension(ak);\n+        assert(ak->is_objArray_klass(), \"incorrect initialization of ObjArrayKlass\");\n+      }\n+    }\n+  } else {\n+    CHECK_UNHANDLED_OOPS_ONLY(Thread::current()->clear_unhandled_oops());\n+  }\n+\n+  ObjArrayKlass *ak = ObjArrayKlass::cast(higher_dimension());\n+  if (or_null) {\n+    return ak->array_klass_or_null(n);\n+  }\n+  return ak->array_klass(n, THREAD);\n+}\n+\n+Klass* FlatArrayKlass::array_klass_impl(bool or_null, TRAPS) {\n+  return array_klass_impl(or_null, dimension() +  1, THREAD);\n+}\n+\n+ModuleEntry* FlatArrayKlass::module() const {\n+  assert(element_klass() != NULL, \"FlatArrayKlass returned unexpected NULL bottom_klass\");\n+  \/\/ The array is defined in the module of its bottom class\n+  return element_klass()->module();\n+}\n+\n+PackageEntry* FlatArrayKlass::package() const {\n+  assert(element_klass() != NULL, \"FlatArrayKlass returned unexpected NULL bottom_klass\");\n+  return element_klass()->package();\n+}\n+\n+bool FlatArrayKlass::can_be_primary_super_slow() const {\n+    return true;\n+}\n+\n+GrowableArray<Klass*>* FlatArrayKlass::compute_secondary_supers(int num_extra_slots,\n+                                                                Array<InstanceKlass*>* transitive_interfaces) {\n+  assert(transitive_interfaces == NULL, \"sanity\");\n+  \/\/ interfaces = { cloneable_klass, serializable_klass, elemSuper[], ... };\n+  Array<Klass*>* elem_supers = element_klass()->secondary_supers();\n+  int num_elem_supers = elem_supers == NULL ? 0 : elem_supers->length();\n+  int num_secondaries = num_extra_slots + 2 + num_elem_supers;\n+  if (num_secondaries == 2) {\n+    \/\/ Must share this for correct bootstrapping!\n+    set_secondary_supers(Universe::the_array_interfaces_array());\n+    return NULL;\n+  } else {\n+    GrowableArray<Klass*>* secondaries = new GrowableArray<Klass*>(num_elem_supers+3);\n+    secondaries->push(vmClasses::Cloneable_klass());\n+    secondaries->push(vmClasses::Serializable_klass());\n+    secondaries->push(vmClasses::IdentityObject_klass());\n+    for (int i = 0; i < num_elem_supers; i++) {\n+      Klass* elem_super = (Klass*) elem_supers->at(i);\n+      Klass* array_super = elem_super->array_klass_or_null();\n+      assert(array_super != NULL, \"must already have been created\");\n+      secondaries->push(array_super);\n+    }\n+    return secondaries;\n+  }\n+}\n+\n+void FlatArrayKlass::print_on(outputStream* st) const {\n+#ifndef PRODUCT\n+  assert(!is_objArray_klass(), \"Unimplemented\");\n+\n+  st->print(\"Flat Type Array: \");\n+  Klass::print_on(st);\n+\n+  st->print(\" - element klass: \");\n+  element_klass()->print_value_on(st);\n+  st->cr();\n+\n+  int elem_size = element_byte_size();\n+  st->print(\" - element size %i \", elem_size);\n+  st->print(\"aligned layout size %i\", 1 << layout_helper_log2_element_size(layout_helper()));\n+  st->cr();\n+#endif \/\/PRODUCT\n+}\n+\n+void FlatArrayKlass::print_value_on(outputStream* st) const {\n+  assert(is_klass(), \"must be klass\");\n+\n+  element_klass()->print_value_on(st);\n+  st->print(\"[]\");\n+}\n+\n+\n+#ifndef PRODUCT\n+void FlatArrayKlass::oop_print_on(oop obj, outputStream* st) {\n+  ArrayKlass::oop_print_on(obj, st);\n+  flatArrayOop va = flatArrayOop(obj);\n+  InlineKlass* vk = element_klass();\n+  int print_len = MIN2((intx) va->length(), MaxElementPrintSize);\n+  for(int index = 0; index < print_len; index++) {\n+    int off = (address) va->value_at_addr(index, layout_helper()) - cast_from_oop<address>(obj);\n+    st->print_cr(\" - Index %3d offset %3d: \", index, off);\n+    oop obj = (oop) ((address)va->value_at_addr(index, layout_helper()) - vk->first_field_offset());\n+    FieldPrinter print_field(st, obj);\n+    vk->do_nonstatic_fields(&print_field);\n+    st->cr();\n+  }\n+  int remaining = va->length() - print_len;\n+  if (remaining > 0) {\n+    st->print_cr(\" - <%d more elements, increase MaxElementPrintSize to print>\", remaining);\n+  }\n+}\n+#endif \/\/PRODUCT\n+\n+void FlatArrayKlass::oop_print_value_on(oop obj, outputStream* st) {\n+  assert(obj->is_flatArray(), \"must be flatArray\");\n+  st->print(\"a \");\n+  element_klass()->print_value_on(st);\n+  int len = flatArrayOop(obj)->length();\n+  st->print(\"[%d] \", len);\n+  obj->print_address_on(st);\n+  if (PrintMiscellaneous && (WizardMode || Verbose)) {\n+    int lh = layout_helper();\n+    st->print(\"{\");\n+    for (int i = 0; i < len; i++) {\n+      if (i > 4) {\n+        st->print(\"...\"); break;\n+      }\n+      st->print(\" \" INTPTR_FORMAT, (intptr_t)(void*)flatArrayOop(obj)->value_at_addr(i , lh));\n+    }\n+    st->print(\" }\");\n+  }\n+}\n+\n+\/\/ Verification\n+class VerifyElementClosure: public BasicOopIterateClosure {\n+ public:\n+  virtual void do_oop(oop* p)       { VerifyOopClosure::verify_oop.do_oop(p); }\n+  virtual void do_oop(narrowOop* p) { VerifyOopClosure::verify_oop.do_oop(p); }\n+};\n+\n+void FlatArrayKlass::oop_verify_on(oop obj, outputStream* st) {\n+  ArrayKlass::oop_verify_on(obj, st);\n+  guarantee(obj->is_flatArray(), \"must be flatArray\");\n+\n+  if (contains_oops()) {\n+    flatArrayOop va = flatArrayOop(obj);\n+    VerifyElementClosure ec;\n+    va->oop_iterate(&ec);\n+  }\n+}\n+\n+void FlatArrayKlass::verify_on(outputStream* st) {\n+  ArrayKlass::verify_on(st);\n+  guarantee(element_klass()->is_inline_klass(), \"should be inline type klass\");\n+}\n","filename":"src\/hotspot\/share\/oops\/flatArrayKlass.cpp","additions":504,"deletions":0,"binary":false,"changes":504,"status":"added"},{"patch":"@@ -72,0 +72,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -164,0 +165,2 @@\n+bool InstanceKlass::field_is_inline_type(int index) const { return Signature::basic_type(field(index)->signature(constants())) == T_INLINE_TYPE; }\n+\n@@ -471,1 +474,3 @@\n-                                       should_store_fingerprint(is_hidden_or_anonymous));\n+                                       should_store_fingerprint(is_hidden_or_anonymous),\n+                                       parser.has_inline_fields() ? parser.java_fields_count() : 0,\n+                                       parser.is_inline_type());\n@@ -485,2 +490,1 @@\n-    }\n-    else if (is_class_loader(class_name, parser)) {\n+    } else if (is_class_loader(class_name, parser)) {\n@@ -489,0 +493,3 @@\n+    } else if (parser.is_inline_type()) {\n+      \/\/ inline type\n+      ik = new (loader_data, size, THREAD) InlineKlass(parser);\n@@ -504,0 +511,7 @@\n+#ifdef ASSERT\n+  assert(ik->size() == size, \"\");\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -507,0 +521,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = NULL;\n+  address end = NULL;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -541,1 +578,3 @@\n-  _init_thread(NULL)\n+  _init_thread(NULL),\n+  _inline_type_field_klasses(NULL),\n+  _adr_inlineklass_fixed_block(NULL)\n@@ -550,0 +589,4 @@\n+    if (parser.has_inline_fields()) {\n+      set_has_inline_type_fields();\n+    }\n+    _java_fields_count = parser.java_fields_count();\n@@ -551,3 +594,3 @@\n-  assert(NULL == _methods, \"underlying memory not zeroed?\");\n-  assert(is_instance_klass(), \"is layout incorrect?\");\n-  assert(size_helper() == parser.layout_size(), \"incorrect size_helper?\");\n+    assert(NULL == _methods, \"underlying memory not zeroed?\");\n+    assert(is_instance_klass(), \"is layout incorrect?\");\n+    assert(size_helper() == parser.layout_size(), \"incorrect size_helper?\");\n@@ -560,0 +603,3 @@\n+  if (has_inline_type_fields()) {\n+    _inline_type_field_klasses = (const Klass**) adr_inline_type_field_klasses();\n+  }\n@@ -589,1 +635,2 @@\n-    if (ti != sti && ti != NULL && !ti->is_shared()) {\n+    if (ti != sti && ti != NULL && !ti->is_shared() &&\n+        ti != Universe::the_single_IdentityObject_klass_array()) {\n@@ -596,1 +643,2 @@\n-      local_interfaces != NULL && !local_interfaces->is_shared()) {\n+      local_interfaces != NULL && !local_interfaces->is_shared() &&\n+      local_interfaces != Universe::the_single_IdentityObject_klass_array()) {\n@@ -928,0 +976,56 @@\n+\n+  \/\/ If a class declares a method that uses an inline class as an argument\n+  \/\/ type or return inline type, this inline class must be loaded during the\n+  \/\/ linking of this class because size and properties of the inline class\n+  \/\/ must be known in order to be able to perform inline type optimizations.\n+  \/\/ The implementation below is an approximation of this rule, the code\n+  \/\/ iterates over all methods of the current class (including overridden\n+  \/\/ methods), not only the methods declared by this class. This\n+  \/\/ approximation makes the code simpler, and doesn't change the semantic\n+  \/\/ because classes declaring methods overridden by the current class are\n+  \/\/ linked (and have performed their own pre-loading) before the linking\n+  \/\/ of the current class.\n+\n+\n+  \/\/ Note:\n+  \/\/ Inline class types are loaded during\n+  \/\/ the loading phase (see ClassFileParser::post_process_parsed_stream()).\n+  \/\/ Inline class types used as element types for array creation\n+  \/\/ are not pre-loaded. Their loading is triggered by either anewarray\n+  \/\/ or multianewarray bytecodes.\n+\n+  \/\/ Could it be possible to do the following processing only if the\n+  \/\/ class uses inline types?\n+  {\n+    ResourceMark rm(THREAD);\n+    for (int i = 0; i < methods()->length(); i++) {\n+      Method* m = methods()->at(i);\n+      for (SignatureStream ss(m->signature()); !ss.is_done(); ss.next()) {\n+        if (ss.is_reference()) {\n+          if (ss.is_array()) {\n+            ss.skip_array_prefix();\n+          }\n+          if (ss.type() == T_INLINE_TYPE) {\n+            Symbol* symb = ss.as_symbol();\n+\n+            oop loader = class_loader();\n+            oop protection_domain = this->protection_domain();\n+            Klass* klass = SystemDictionary::resolve_or_fail(symb,\n+                                                             Handle(THREAD, loader), Handle(THREAD, protection_domain), true,\n+                                                             CHECK_false);\n+            if (klass == NULL) {\n+              THROW_(vmSymbols::java_lang_LinkageError(), false);\n+            }\n+            if (!klass->is_inline_klass()) {\n+              Exceptions::fthrow(\n+                THREAD_AND_LOCATION,\n+                vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                \"class %s is not an inline type\",\n+                klass->external_name());\n+            }\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -999,0 +1103,1 @@\n+\n@@ -1149,0 +1254,29 @@\n+  \/\/ Step 8\n+  \/\/ Initialize classes of inline fields\n+  {\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        Klass* klass = get_inline_type_field_klass_or_null(fs.index());\n+        if (fs.access_flags().is_static() && klass == NULL) {\n+          klass = SystemDictionary::resolve_or_fail(field_signature(fs.index())->fundamental_name(THREAD),\n+              Handle(THREAD, class_loader()),\n+              Handle(THREAD, protection_domain()),\n+              true, CHECK);\n+          if (klass == NULL) {\n+            THROW(vmSymbols::java_lang_NoClassDefFoundError());\n+          }\n+          if (!klass->is_inline_klass()) {\n+            THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+          }\n+          set_inline_type_field_klass(fs.index(), klass);\n+        }\n+        InstanceKlass::cast(klass)->initialize(CHECK);\n+        if (fs.access_flags().is_static()) {\n+          if (java_mirror()->obj_field(fs.offset()) == NULL) {\n+            java_mirror()->obj_field_put(fs.offset(), InlineKlass::cast(klass)->default_value());\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1153,1 +1287,1 @@\n-  \/\/ Step 8\n+  \/\/ Step 9\n@@ -1175,1 +1309,1 @@\n-  \/\/ Step 9\n+  \/\/ Step 10\n@@ -1183,1 +1317,1 @@\n-    \/\/ Step 10 and 11\n+    \/\/ Step 11 and 12\n@@ -1455,1 +1589,1 @@\n-  ObjArrayKlass* oak = array_klasses();\n+  ArrayKlass* oak = array_klasses();\n@@ -1471,1 +1605,1 @@\n-  if (clinit != NULL && clinit->has_valid_initializer_flags()) {\n+  if (clinit != NULL && clinit->is_class_initializer()) {\n@@ -1509,1 +1643,1 @@\n-    MutexLocker x(OopMapCacheAlloc_lock);\n+    MutexLocker x(OopMapCacheAlloc_lock,  Mutex::_no_safepoint_check_flag);\n@@ -1521,5 +1655,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n-\n@@ -1596,0 +1725,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->first_field_offset() && offset < (vk->first_field_offset() + vk->get_exact_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -1983,0 +2121,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited, not even as a static factory\n+    }\n@@ -2491,0 +2632,6 @@\n+\n+  if (has_inline_type_fields()) {\n+    for (int i = 0; i < java_fields_count(); i++) {\n+      it->push(&((Klass**)adr_inline_type_field_klasses())[i]);\n+    }\n+  }\n@@ -2526,0 +2673,8 @@\n+  if (has_inline_type_fields()) {\n+    for (AllFieldStream fs(fields(), constants()); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        reset_inline_type_field_klass(fs.index());\n+      }\n+    }\n+  }\n+\n@@ -2586,0 +2741,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2617,1 +2776,1 @@\n-  if (UseBiasedLocking && BiasedLocking::enabled()) {\n+  if (UseBiasedLocking && BiasedLocking::enabled() && !is_inline_klass()) {\n@@ -2788,1 +2947,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -2790,1 +2949,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = is_inline_klass() ? JVM_SIGNATURE_INLINE_TYPE : JVM_SIGNATURE_CLASS;\n@@ -3357,1 +3516,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3361,0 +3523,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3364,0 +3531,6 @@\n+    } else if (self != NULL && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3370,1 +3543,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(NULL, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == NULL) { st->print_cr(\"NULL\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == NULL) { st->print_cr(\"NULL\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3380,0 +3574,1 @@\n+  st->print(BULLET\"misc flags:        0x%x\", _misc_flags);                        st->cr();\n@@ -3406,15 +3601,3 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);                  st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);      st->cr();\n-  if (Verbose && default_methods() != NULL) {\n-    Array<Method*>* method_array = default_methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n+  st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3422,1 +3605,1 @@\n-    st->print(BULLET\"default vtable indices:   \"); default_vtable_indices()->print_value_on(st);       st->cr();\n+    st->print(BULLET\"default vtable indices:   \"); print_array_on(st, default_vtable_indices());\n@@ -3424,2 +3607,2 @@\n-  st->print(BULLET\"local interfaces:  \"); local_interfaces()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"trans. interfaces: \"); transitive_interfaces()->print_value_on(st); st->cr();\n+  st->print(BULLET\"local interfaces:  \"); print_array_on(st, local_interfaces());\n+  st->print(BULLET\"trans. interfaces: \"); print_array_on(st, transitive_interfaces());\n@@ -3482,1 +3665,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(NULL, start_of_itable(), itable_length(), st);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":227,"deletions":44,"binary":false,"changes":271,"status":"modified"},{"patch":"@@ -222,1 +222,1 @@\n-  int lh = array_layout_helper(tag, hsize, etype, exact_log2(esize));\n+  int lh = array_layout_helper(tag, false, hsize, etype, exact_log2(esize));\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -121,1 +122,0 @@\n-\n@@ -158,0 +158,5 @@\n+address Method::get_c2i_inline_entry() {\n+  assert(adapter() != NULL, \"must have\");\n+  return adapter()->get_c2i_inline_entry();\n+}\n+\n@@ -163,0 +168,5 @@\n+address Method::get_c2i_unverified_inline_entry() {\n+  assert(adapter() != NULL, \"must have\");\n+  return adapter()->get_c2i_unverified_inline_entry();\n+}\n+\n@@ -355,0 +365,2 @@\n+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_inline_ro_entry);\n+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_inline_entry);\n@@ -599,0 +611,18 @@\n+\/\/ InlineKlass the method is declared to return. This must not\n+\/\/ safepoint as it is called with references live on the stack at\n+\/\/ locations the GC is unaware of.\n+InlineKlass* Method::returned_inline_type(Thread* thread) const {\n+  SignatureStream ss(signature());\n+  while (!ss.at_return_type()) {\n+    ss.next();\n+  }\n+  Handle class_loader(thread, method_holder()->class_loader());\n+  Handle protection_domain(thread, method_holder()->protection_domain());\n+  Klass* k = NULL;\n+  {\n+    NoSafepointVerifier nsv;\n+    k = ss.as_klass(class_loader, protection_domain, SignatureStream::ReturnNull, thread);\n+  }\n+  assert(k != NULL && !thread->has_pending_exception(), \"can't resolve klass\");\n+  return InlineKlass::cast(k);\n+}\n@@ -609,1 +639,1 @@\n-  \/\/   aload_0\n+  \/\/   aload_0, _fast_aload_0, or _nofast_aload_0\n@@ -633,1 +663,2 @@\n-  if (cb[0] != Bytecodes::_aload_0 || cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {\n+  if ((cb[0] != Bytecodes::_aload_0 && cb[0] != Bytecodes::_fast_aload_0 && cb[0] != Bytecodes::_nofast_aload_0) ||\n+       cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {\n@@ -813,7 +844,2 @@\n-bool Method::is_initializer() const {\n-  return is_object_initializer() || is_static_initializer();\n-}\n-\n-bool Method::has_valid_initializer_flags() const {\n-  return (is_static() ||\n-          method_holder()->major_version() < 51);\n+bool Method::is_object_constructor_or_class_initializer() const {\n+  return (is_object_constructor() || is_class_initializer());\n@@ -822,1 +848,1 @@\n-bool Method::is_static_initializer() const {\n+bool Method::is_class_initializer() const {\n@@ -826,2 +852,8 @@\n-  return name() == vmSymbols::class_initializer_name() &&\n-         has_valid_initializer_flags();\n+  return (name() == vmSymbols::class_initializer_name() &&\n+          (is_static() ||\n+           method_holder()->major_version() < 51));\n+}\n+\n+\/\/ A method named <init>, if non-static, is a classic object constructor.\n+bool Method::is_object_constructor() const {\n+   return name() == vmSymbols::object_initializer_name() && !is_static();\n@@ -830,2 +862,3 @@\n-bool Method::is_object_initializer() const {\n-   return name() == vmSymbols::object_initializer_name();\n+\/\/ A static method named <init> is a factory for an inline class.\n+bool Method::is_static_init_factory() const {\n+   return name() == vmSymbols::object_initializer_name() && is_static();\n@@ -889,1 +922,1 @@\n-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {\n+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {\n@@ -905,1 +938,3 @@\n-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;\n+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {\n+      return false;\n+    }\n@@ -1074,0 +1109,2 @@\n+    _from_compiled_inline_entry = NULL;\n+    _from_compiled_inline_ro_entry = NULL;\n@@ -1076,0 +1113,2 @@\n+    _from_compiled_inline_entry = adapter()->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = adapter()->get_c2i_inline_ro_entry();\n@@ -1116,0 +1155,4 @@\n+  assert(*((int*)_from_compiled_inline_ro_entry) == 0,\n+         \"must be NULL during dump time, to be initialized at run time\");\n+  assert(*((int*)_from_compiled_inline_entry) == 0,\n+         \"must be NULL during dump time, to be initialized at run time\");\n@@ -1267,0 +1310,2 @@\n+    assert(mh->_from_compiled_inline_entry != NULL, \"must be\");\n+    assert(mh->_from_compiled_inline_ro_entry != NULL, \"must be\");\n@@ -1270,0 +1315,2 @@\n+    mh->_from_compiled_inline_entry = adapter->get_c2i_inline_entry();\n+    mh->_from_compiled_inline_ro_entry = adapter->get_c2i_inline_ro_entry();\n@@ -1277,0 +1324,12 @@\n+#if 0\n+  \/*\n+   * CDS:TODO --\n+   * \"Q\" classes in the method signature must be resolved during link_method.\n+   * However, at this point we are still inside method_holder()->restore_unshareable_info.\n+   * If we try to resolve method_holder(), or multually dependent classes, it will\n+   * cause deadlock and other ill effects.\n+   *\n+   * For now, lets do method linking inside InstanceKlass::link_class(). Optimization\n+   * may be possible if we know that resolution will never happen.\n+   *\/\n+\n@@ -1283,0 +1342,1 @@\n+#endif\n@@ -1285,1 +1345,1 @@\n-address Method::from_compiled_entry_no_trampoline() const {\n+address Method::from_compiled_entry_no_trampoline(bool caller_is_c1) const {\n@@ -1287,2 +1347,7 @@\n-  if (code) {\n-    return code->verified_entry_point();\n+  if (caller_is_c1) {\n+    \/\/ C1 - inline type arguments are passed as objects\n+    if (code) {\n+      return code->verified_inline_entry_point();\n+    } else {\n+      return adapter()->get_c2i_inline_entry();\n+    }\n@@ -1290,1 +1355,6 @@\n-    return adapter()->get_c2i_entry();\n+    \/\/ C2 - inline type arguments may be passed as fields\n+    if (code) {\n+      return code->verified_entry_point();\n+    } else {\n+      return adapter()->get_c2i_entry();\n+    }\n@@ -1307,0 +1377,12 @@\n+address Method::verified_inline_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_entry != NULL, \"must be set\");\n+  return _from_compiled_inline_entry;\n+}\n+\n+address Method::verified_inline_ro_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_ro_entry != NULL, \"must be set\");\n+  return _from_compiled_inline_ro_entry;\n+}\n+\n@@ -1338,0 +1420,2 @@\n+  mh->_from_compiled_inline_entry = code->verified_inline_entry_point();\n+  mh->_from_compiled_inline_ro_entry = code->verified_inline_ro_entry_point();\n@@ -2364,0 +2448,4 @@\n+#ifdef ASSERT\n+  if (valid_itable_index())\n+    st->print_cr(\" - itable index:      %d\",   itable_index());\n+#endif\n@@ -2441,0 +2529,1 @@\n+  if (WizardMode) access_flags().print_on(st);\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":110,"deletions":21,"binary":false,"changes":131,"status":"modified"},{"patch":"@@ -94,1 +94,4 @@\n-    _scoped                = 1 << 8\n+    _scalarized_args       = 1 << 8,\n+    _c1_needs_stack_repair = 1 << 9,\n+    _c2_needs_stack_repair = 1 << 10,\n+    _scoped                = 1 << 11\n@@ -107,1 +110,3 @@\n-  volatile address _from_compiled_entry;        \/\/ Cache of: _code ? _code->entry_point() : _adapter->c2i_entry()\n+  volatile address _from_compiled_entry;           \/\/ Cache of: _code ? _code->verified_entry_point()           : _adapter->c2i_entry()\n+  volatile address _from_compiled_inline_ro_entry; \/\/ Cache of: _code ? _code->verified_inline_ro_entry_point() : _adapter->c2i_inline_ro_entry()\n+  volatile address _from_compiled_inline_entry;    \/\/ Cache of: _code ? _code->verified_inline_entry_point()    : _adapter->c2i_inline_entry()\n@@ -146,1 +151,3 @@\n-  address from_compiled_entry_no_trampoline() const;\n+  address from_compiled_inline_ro_entry() const;\n+  address from_compiled_inline_entry() const;\n+  address from_compiled_entry_no_trampoline(bool caller_is_c1) const;\n@@ -452,0 +459,2 @@\n+  address verified_inline_code_entry();\n+  address verified_inline_ro_code_entry();\n@@ -476,1 +485,7 @@\n-    _from_compiled_entry =  entry;\n+    _from_compiled_entry = entry;\n+  }\n+  void set_from_compiled_inline_ro_entry(address entry) {\n+    _from_compiled_inline_ro_entry = entry;\n+  }\n+  void set_from_compiled_inline_entry(address entry) {\n+    _from_compiled_inline_entry = entry;\n@@ -481,0 +496,1 @@\n+  address get_c2i_inline_entry();\n@@ -482,0 +498,1 @@\n+  address get_c2i_unverified_inline_entry();\n@@ -594,1 +611,1 @@\n-  bool is_returning_fp() const                   { BasicType r = result_type(); return (r == T_FLOAT || r == T_DOUBLE); }\n+  InlineKlass* returned_inline_type(Thread* thread) const;\n@@ -667,6 +684,0 @@\n-  \/\/ returns true if the method is an initializer (<init> or <clinit>).\n-  bool is_initializer() const;\n-\n-  \/\/ returns true if the method is static OR if the classfile version < 51\n-  bool has_valid_initializer_flags() const;\n-\n@@ -675,1 +686,4 @@\n-  bool is_static_initializer() const;\n+  bool is_class_initializer() const;\n+\n+  \/\/ returns true if the method name is <init> and the method is not a static factory\n+  bool is_object_constructor() const;\n@@ -677,2 +691,6 @@\n-  \/\/ returns true if the method name is <init>\n-  bool is_object_initializer() const;\n+  \/\/ returns true if the method is an object constructor or class initializer\n+  \/\/ (non-static <init> or <clinit>), but false for factories (static <init>).\n+  bool is_object_constructor_or_class_initializer() const;\n+\n+  \/\/ returns true if the method name is <init> and the method is static\n+  bool is_static_init_factory() const;\n@@ -702,0 +720,2 @@\n+  static ByteSize from_compiled_inline_offset()  { return byte_offset_of(Method, _from_compiled_inline_entry); }\n+  static ByteSize from_compiled_inline_ro_offset(){ return byte_offset_of(Method, _from_compiled_inline_ro_entry); }\n@@ -703,0 +723,1 @@\n+  static ByteSize flags_offset()                 { return byte_offset_of(Method, _flags); }\n@@ -911,0 +932,24 @@\n+  bool has_scalarized_args() {\n+    return (_flags & _scalarized_args) != 0;\n+  }\n+\n+  void set_has_scalarized_args(bool x) {\n+    _flags = x ? (_flags | _scalarized_args) : (_flags & ~_scalarized_args);\n+  }\n+\n+  bool c1_needs_stack_repair() {\n+    return (_flags & _c1_needs_stack_repair) != 0;\n+  }\n+\n+  bool c2_needs_stack_repair() {\n+    return (_flags & _c2_needs_stack_repair) != 0;\n+  }\n+\n+  void set_c1_needs_stack_repair(bool x) {\n+    _flags = x ? (_flags | _c1_needs_stack_repair) : (_flags & ~_c1_needs_stack_repair);\n+  }\n+\n+  void set_c2_needs_stack_repair(bool x) {\n+    _flags = x ? (_flags | _c2_needs_stack_repair) : (_flags & ~_c2_needs_stack_repair);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":59,"deletions":14,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -142,1 +142,1 @@\n-    st->print(\"flags(%d) \", flags);\n+    st->print(\"flags(%d) %p\/%d\", flags, data(), in_bytes(DataLayout::flags_offset()));\n@@ -212,1 +212,1 @@\n-  assert(TypeStackSlotEntries::per_arg_count() > ReturnTypeEntry::static_cell_count(), \"code to test for arguments\/results broken\");\n+  assert(TypeStackSlotEntries::per_arg_count() > SingleTypeEntry::static_cell_count(), \"code to test for arguments\/results broken\");\n@@ -222,1 +222,1 @@\n-    ret_cell = ReturnTypeEntry::static_cell_count();\n+    ret_cell = SingleTypeEntry::static_cell_count();\n@@ -325,1 +325,1 @@\n-void ReturnTypeEntry::clean_weak_klass_links(bool always_clean) {\n+void SingleTypeEntry::clean_weak_klass_links(bool always_clean) {\n@@ -363,1 +363,1 @@\n-void ReturnTypeEntry::print_data_on(outputStream* st) const {\n+void SingleTypeEntry::print_data_on(outputStream* st) const {\n@@ -528,0 +528,4 @@\n+  if (data()->flags()) {\n+    tty->cr();\n+    tab(st);\n+  }\n@@ -651,0 +655,21 @@\n+void ArrayLoadStoreData::print_data_on(outputStream* st, const char* extra) const {\n+  print_shared(st, \"ArrayLoadStore\", extra);\n+  st->cr();\n+  tab(st, true);\n+  st->print(\"array\");\n+  _array.print_data_on(st);\n+  tab(st, true);\n+  st->print(\"element\");\n+  _element.print_data_on(st);\n+}\n+\n+void ACmpData::print_data_on(outputStream* st, const char* extra) const {\n+  BranchData::print_data_on(st, extra);\n+  tab(st, true);\n+  st->print(\"left\");\n+  _left.print_data_on(st);\n+  tab(st, true);\n+  st->print(\"right\");\n+  _right.print_data_on(st);\n+}\n+\n@@ -671,1 +696,0 @@\n-  case Bytecodes::_aastore:\n@@ -677,0 +701,3 @@\n+  case Bytecodes::_aaload:\n+  case Bytecodes::_aastore:\n+    return ArrayLoadStoreData::static_cell_count();\n@@ -716,2 +743,0 @@\n-  case Bytecodes::_if_acmpeq:\n-  case Bytecodes::_if_acmpne:\n@@ -721,0 +746,3 @@\n+  case Bytecodes::_if_acmpne:\n+  case Bytecodes::_if_acmpeq:\n+    return ACmpData::static_cell_count();\n@@ -779,0 +807,1 @@\n+  case Bytecodes::_aaload:\n@@ -982,1 +1011,0 @@\n-  case Bytecodes::_aastore:\n@@ -991,0 +1019,5 @@\n+  case Bytecodes::_aaload:\n+  case Bytecodes::_aastore:\n+    cell_count = ArrayLoadStoreData::static_cell_count();\n+    tag = DataLayout::array_load_store_data_tag;\n+    break;\n@@ -1062,2 +1095,0 @@\n-  case Bytecodes::_if_acmpeq:\n-  case Bytecodes::_if_acmpne:\n@@ -1069,0 +1100,5 @@\n+  case Bytecodes::_if_acmpeq:\n+  case Bytecodes::_if_acmpne:\n+    cell_count = ACmpData::static_cell_count();\n+    tag = DataLayout::acmp_data_tag;\n+    break;\n@@ -1136,0 +1172,4 @@\n+  case DataLayout::array_load_store_data_tag:\n+    return ((new ArrayLoadStoreData(this))->cell_count());\n+  case DataLayout::acmp_data_tag:\n+    return ((new ACmpData(this))->cell_count());\n@@ -1170,0 +1210,4 @@\n+  case DataLayout::array_load_store_data_tag:\n+    return new ArrayLoadStoreData(this);\n+  case DataLayout::acmp_data_tag:\n+    return new ACmpData(this);\n","filename":"src\/hotspot\/share\/oops\/methodData.cpp","additions":55,"deletions":11,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -100,19 +101,1 @@\n-  Symbol* name = NULL;\n-  {\n-    ResourceMark rm(THREAD);\n-    char *name_str = element_klass->name()->as_C_string();\n-    int len = element_klass->name()->utf8_length();\n-    char *new_str = NEW_RESOURCE_ARRAY(char, len + 4);\n-    int idx = 0;\n-    new_str[idx++] = JVM_SIGNATURE_ARRAY;\n-    if (element_klass->is_instance_klass()) { \/\/ it could be an array or simple type\n-      new_str[idx++] = JVM_SIGNATURE_CLASS;\n-    }\n-    memcpy(&new_str[idx], name_str, len * sizeof(char));\n-    idx += len;\n-    if (element_klass->is_instance_klass()) {\n-      new_str[idx++] = JVM_SIGNATURE_ENDCLASS;\n-    }\n-    new_str[idx++] = '\\0';\n-    name = SymbolTable::new_symbol(new_str);\n-  }\n+  Symbol* name = ArrayKlass::create_element_klass_array_name(element_klass, CHECK_NULL);\n@@ -146,0 +129,2 @@\n+  } else if (element_klass->is_flatArray_klass()) {\n+    bk = FlatArrayKlass::cast(element_klass)->element_klass();\n@@ -153,1 +138,7 @@\n-  set_layout_helper(array_layout_helper(T_OBJECT));\n+  jint lh = array_layout_helper(T_OBJECT);\n+  if (element_klass->is_inline_klass()) {\n+    lh = layout_helper_set_null_free(lh);\n+    set_prototype_header(markWord::nullfree_array_prototype());\n+    assert(prototype_header().is_nullfree_array(), \"sanity\");\n+  }\n+  set_layout_helper(lh);\n@@ -166,1 +157,2 @@\n-  return (objArrayOop)Universe::heap()->array_allocate(this, size, length,\n+  bool populate_null_free = is_null_free_array_klass();\n+  objArrayOop array =  (objArrayOop)Universe::heap()->array_allocate(this, size, length,\n@@ -168,0 +160,14 @@\n+  if (populate_null_free) {\n+    assert(dimension() == 1, \"Can only populate the final dimension\");\n+    assert(element_klass()->is_inline_klass(), \"Unexpected\");\n+    assert(!element_klass()->is_array_klass(), \"ArrayKlass unexpected here\");\n+    assert(!InlineKlass::cast(element_klass())->flatten_array(), \"Expected flatArrayOop allocation\");\n+    element_klass()->initialize(CHECK_NULL);\n+    \/\/ Populate default values...\n+    objArrayHandle array_h(THREAD, array);\n+    instanceOop value = (instanceOop) InlineKlass::cast(element_klass())->default_value();\n+    for (int i = 0; i < length; i++) {\n+      array_h->obj_at_put(i, value);\n+    }\n+  }\n+  return array;\n@@ -170,2 +176,0 @@\n-static int multi_alloc_counter = 0;\n-\n@@ -174,0 +178,8 @@\n+  if (rank == 1) { \/\/ last dim may be flatArray, check if we have any special storage requirements\n+    if (element_klass()->is_inline_klass()) {\n+      return oopFactory::new_flatArray(element_klass(), length, CHECK_NULL);\n+    } else {\n+      return oopFactory::new_objArray(element_klass(), length, CHECK_NULL);\n+    }\n+  }\n+  guarantee(rank > 1, \"Rank below 1\");\n@@ -180,16 +192,14 @@\n-  if (rank > 1) {\n-    if (length != 0) {\n-      for (int index = 0; index < length; index++) {\n-        ArrayKlass* ak = ArrayKlass::cast(ld_klass);\n-        oop sub_array = ak->multi_allocate(rank-1, &sizes[1], CHECK_NULL);\n-        h_array->obj_at_put(index, sub_array);\n-      }\n-    } else {\n-      \/\/ Since this array dimension has zero length, nothing will be\n-      \/\/ allocated, however the lower dimension values must be checked\n-      \/\/ for illegal values.\n-      for (int i = 0; i < rank - 1; ++i) {\n-        sizes += 1;\n-        if (*sizes < 0) {\n-          THROW_MSG_0(vmSymbols::java_lang_NegativeArraySizeException(), err_msg(\"%d\", *sizes));\n-        }\n+  if (length != 0) {\n+    for (int index = 0; index < length; index++) {\n+      ArrayKlass* ak = ArrayKlass::cast(ld_klass);\n+      oop sub_array = ak->multi_allocate(rank-1, &sizes[1], CHECK_NULL);\n+      h_array->obj_at_put(index, sub_array);\n+    }\n+  } else {\n+    \/\/ Since this array dimension has zero length, nothing will be\n+    \/\/ allocated, however the lower dimension values must be checked\n+    \/\/ for illegal values.\n+    for (int i = 0; i < rank - 1; ++i) {\n+      sizes += 1;\n+      if (*sizes < 0) {\n+        THROW_MSG_0(vmSymbols::java_lang_NegativeArraySizeException(), err_msg(\"%d\", *sizes));\n@@ -213,0 +223,3 @@\n+    \/\/ Perform null check if dst is null-free but src has no such guarantee\n+    bool null_check = ((!s->klass()->is_null_free_array_klass()) &&\n+        d->klass()->is_null_free_array_klass());\n@@ -214,2 +227,5 @@\n-      \/\/ elements are guaranteed to be subtypes, so no check necessary\n-      ArrayAccess<ARRAYCOPY_DISJOINT>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n+      if (null_check) {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_NOTNULL>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n+      } else {\n+        ArrayAccess<ARRAYCOPY_DISJOINT>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n+      }\n@@ -217,16 +233,4 @@\n-      \/\/ slow case: need individual subtype checks\n-      \/\/ note: don't use obj_at_put below because it includes a redundant store check\n-      if (!ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST>::oop_arraycopy(s, src_offset, d, dst_offset, length)) {\n-        ResourceMark rm(THREAD);\n-        stringStream ss;\n-        if (!bound->is_subtype_of(stype)) {\n-          ss.print(\"arraycopy: type mismatch: can not copy %s[] into %s[]\",\n-                   stype->external_name(), bound->external_name());\n-        } else {\n-          \/\/ oop_arraycopy should return the index in the source array that\n-          \/\/ contains the problematic oop.\n-          ss.print(\"arraycopy: element type mismatch: can not cast one of the elements\"\n-                   \" of %s[] to the type of the destination array, %s\",\n-                   stype->external_name(), bound->external_name());\n-        }\n-        THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+      if (null_check) {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST | ARRAYCOPY_NOTNULL>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n+      } else {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n@@ -242,0 +246,7 @@\n+  if (EnableValhalla) {\n+    if (d->is_flatArray()) {\n+      FlatArrayKlass::cast(d->klass())->copy_array(s, src_pos, d, dst_pos, length, THREAD);\n+      return;\n+    }\n+  }\n+\n@@ -314,1 +325,0 @@\n-\n@@ -333,2 +343,1 @@\n-        Klass* k =\n-          ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, CHECK_NULL);\n+        Klass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, CHECK_NULL);\n@@ -376,1 +385,1 @@\n-    GrowableArray<Klass*>* secondaries = new GrowableArray<Klass*>(num_elem_supers+2);\n+    GrowableArray<Klass*>* secondaries = new GrowableArray<Klass*>(num_elem_supers+3);\n@@ -379,0 +388,1 @@\n+    secondaries->push(vmClasses::IdentityObject_klass());\n@@ -430,1 +440,1 @@\n-  st->print(\" - instance klass: \");\n+  st->print(\" - element klass: \");\n@@ -492,1 +502,2 @@\n-  guarantee(bk->is_instance_klass() || bk->is_typeArray_klass(),  \"invalid bottom klass\");\n+  guarantee(bk->is_instance_klass() || bk->is_typeArray_klass() || bk->is_flatArray_klass(),\n+            \"invalid bottom klass\");\n@@ -498,0 +509,1 @@\n+  guarantee(obj->is_nullfreeArray() || (!is_null_free_array_klass()), \"null-free klass but not object\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":75,"deletions":63,"binary":false,"changes":138,"status":"modified"},{"patch":"@@ -42,0 +42,10 @@\n+\/\/\n+\/\/ oopDesc::_mark - the \"oop mark word\" encoding to be found separately in markWord.hpp\n+\/\/\n+\/\/ oopDesc::_metadata - encodes the object's klass pointer, as a raw pointer in \"_klass\"\n+\/\/                      or compressed pointer in \"_compressed_klass\"\n+\/\/\n+\/\/ The overall size of the _metadata field is dependent on \"UseCompressedClassPointers\",\n+\/\/ hence the terms \"narrow\" (32 bits) vs \"wide\" (64 bits).\n+\/\/\n+\n@@ -105,0 +115,3 @@\n+  inline bool is_inline_type()         const;\n+  inline bool is_flatArray()           const;\n+  inline bool is_nullfreeArray()       const;\n@@ -111,0 +124,3 @@\n+  bool is_value_noinline()             const;\n+  bool is_flatArray_noinline()         const;\n+  bool is_nullfreeArray_noinline()     const;\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -98,1 +98,0 @@\n-  \/\/ For typeArrays this is only called for the last dimension\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -87,2 +87,2 @@\n-  if (callee_method->is_initializer()) {\n-    return true; \/\/ constuctor\n+  if (callee_method->is_object_constructor()) {\n+    return true; \/\/ constructor\n@@ -90,1 +90,1 @@\n-  if (caller_method->is_initializer() &&\n+  if (caller_method->is_object_constructor_or_class_initializer() &&\n","filename":"src\/hotspot\/share\/opto\/bytecodeInfo.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -505,0 +505,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -514,0 +516,1 @@\n+  case vmIntrinsics::_getValue:\n@@ -523,0 +526,1 @@\n+  case vmIntrinsics::_putValue:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -396,1 +397,1 @@\n-bool RegionNode::try_clean_mem_phi(PhaseGVN *phase) {\n+Node* PhiNode::try_clean_mem_phi(PhaseGVN *phase) {\n@@ -416,2 +417,1 @@\n-  PhiNode* phi = has_unique_phi();\n-  if (phi && phi->type() == Type::MEMORY && req() == 3 && phi->is_diamond_phi(true)) {\n+  if (type() == Type::MEMORY && is_diamond_phi(true)) {\n@@ -419,1 +419,2 @@\n-    assert(phi->req() == 3, \"same as region\");\n+    assert(req() == 3, \"same as region\");\n+    Node* r = in(0);\n@@ -421,2 +422,2 @@\n-      Node *mem = phi->in(i);\n-      if (mem && mem->is_MergeMem() && in(i)->outcnt() == 1) {\n+      Node *mem = in(i);\n+      if (mem && mem->is_MergeMem() && r->in(i)->outcnt() == 1) {\n@@ -426,1 +427,1 @@\n-        Node* other = phi->in(j);\n+        Node* other = in(j);\n@@ -430,2 +431,1 @@\n-          phase->is_IterGVN()->replace_node(phi, m);\n-          return true;\n+          return m;\n@@ -436,1 +436,1 @@\n-  return false;\n+  return NULL;\n@@ -451,2 +451,9 @@\n-    if (has_phis && try_clean_mem_phi(phase)) {\n-      has_phis = false;\n+    if (has_phis) {\n+      PhiNode* phi = has_unique_phi();\n+      if (phi != NULL) {\n+        Node* m = phi->try_clean_mem_phi(phase);\n+        if (m != NULL) {\n+          phase->is_IterGVN()->replace_node(phi, m);\n+          has_phis = false;\n+        }\n+      }\n@@ -844,1 +851,2 @@\n-             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck()) {\n+             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck() ||\n+             cmp1->is_FlatArrayCheck() || cmp2->is_FlatArrayCheck()) {\n@@ -922,1 +930,1 @@\n-  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at), \"flatten at\");\n+  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at) || (flatten_phi_adr_type(at) == TypeAryPtr::INLINES && Compile::current()->flattened_accesses_share_alias()), \"flatten at\");\n@@ -1132,9 +1140,4 @@\n-  if (ttip != NULL) {\n-    ciKlass* k = ttip->klass();\n-    if (k->is_loaded() && k->is_interface())\n-      is_intf = true;\n-  }\n-  if (ttkp != NULL) {\n-    ciKlass* k = ttkp->klass();\n-    if (k->is_loaded() && k->is_interface())\n-      is_intf = true;\n+  if (ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {\n+    is_intf = true;\n+  } else if (ttkp != NULL && ttkp->is_loaded() && ttkp->klass()->is_interface()) {\n+    is_intf = true;\n@@ -1197,1 +1200,1 @@\n-    if (!t->empty() && ttip && ttip->is_loaded() && ttip->klass()->is_interface()) {\n+    if (!t->empty() && ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {\n@@ -1199,1 +1202,1 @@\n-    } else if (!t->empty() && ttkp && ttkp->is_loaded() && ttkp->klass()->is_interface()) {\n+    } else if (!t->empty() && ttkp != NULL && ttkp->is_loaded() && ttkp->klass()->is_interface()) {\n@@ -1361,0 +1364,8 @@\n+  if (phase->is_IterGVN()) {\n+    Node* m = try_clean_mem_phi(phase);\n+    if (m != NULL) {\n+      return m;\n+    }\n+  }\n+\n+\n@@ -1896,0 +1907,18 @@\n+  \/\/ If all inputs are inline types of the same type, push the inline type node down\n+  \/\/ through the phi because inline type nodes should be merged through their input values.\n+  if (req() > 2 && in(1) != NULL && in(1)->is_InlineTypeBase() && (can_reshape || in(1)->is_InlineType())) {\n+    int opcode = in(1)->Opcode();\n+    uint i = 2;\n+    \/\/ Check if inputs are values of the same type\n+    for (; i < req() && in(i) && in(i)->is_InlineTypeBase() && in(i)->cmp(*in(1)); i++) {\n+      assert(in(i)->Opcode() == opcode, \"mixing pointers and values?\");\n+    }\n+    if (i == req()) {\n+      InlineTypeBaseNode* vt = in(1)->as_InlineTypeBase()->clone_with_phis(phase, in(0));\n+      for (uint i = 2; i < req(); ++i) {\n+        vt->merge_with(phase, in(i)->as_InlineTypeBase(), i, i == (req()-1));\n+      }\n+      return vt;\n+    }\n+  }\n+\n@@ -2187,0 +2216,2 @@\n+    \/\/ TODO revisit this with JDK-8247216\n+    bool mergemem_only = true;\n@@ -2199,0 +2230,2 @@\n+      } else {\n+        mergemem_only = false;\n@@ -2203,1 +2236,1 @@\n-    if (!saw_self && adr_type() == TypePtr::BOTTOM)  merge_width = 0;\n+    if (!mergemem_only && !saw_self && adr_type() == TypePtr::BOTTOM)  merge_width = 0;\n@@ -2274,0 +2307,5 @@\n+            if (igvn) {\n+              \/\/ TODO revisit this with JDK-8247216\n+              \/\/ Put 'n' on the worklist because it might be modified by MergeMemStream::iteration_setup\n+              igvn->_worklist.push(n);\n+            }\n@@ -2692,0 +2730,6 @@\n+\n+  \/\/ CheckCastPPNode::Ideal() for inline types reuses the exception\n+  \/\/ paths of a call to perform an allocation: we can see a Phi here.\n+  if (in(1)->is_Phi()) {\n+    return this;\n+  }\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":70,"deletions":26,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -400,0 +401,3 @@\n+  if (dead->is_InlineTypeBase()) {\n+    remove_inline_type(dead);\n+  }\n@@ -441,0 +445,1 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n@@ -575,0 +580,1 @@\n+                  _inline_type_nodes (comp_arena(), 8, 0, NULL),\n@@ -679,4 +685,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -689,1 +693,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -824,0 +828,4 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n@@ -981,0 +989,3 @@\n+  _has_flattened_accesses = false;\n+  _flattened_accesses_share_alias = true;\n+\n@@ -1283,1 +1294,2 @@\n-    assert(InlineUnsafeOps, \"indeterminate pointers come only from unsafe ops\");\n+    bool default_value_load = EnableValhalla && tj->is_instptr()->klass() == ciEnv::current()->Class_klass();\n+    assert(InlineUnsafeOps || default_value_load, \"indeterminate pointers come only from unsafe ops\");\n@@ -1296,0 +1308,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1300,1 +1321,1 @@\n-      tj = ta = TypeAryPtr::make(ptr, ta->ary(), ta->klass(), true, offset, ta->instance_id());\n+      tj = ta = TypeAryPtr::make(ptr, ta->ary(), ta->klass(), true, Type::Offset(offset), ta->field_offset(), ta->instance_id());\n@@ -1306,0 +1327,2 @@\n+    \/\/ For flattened inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1309,1 +1332,1 @@\n-        tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),ta->ary(),ta->klass(),false,offset);\n+        tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1323,1 +1346,1 @@\n-        tj = ta = TypeAryPtr::make(ptr,ta->ary(),ta->klass(),false,offset);\n+        tj = ta = TypeAryPtr::make(ptr,ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1329,1 +1352,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,ta->klass(),false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1334,1 +1357,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());\n@@ -1338,1 +1361,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInlineType::BOTTOM && _flattened_accesses_share_alias) {\n+      const TypeAry *tary = TypeAry::make(TypeInlineType::BOTTOM, ta->size());\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1345,1 +1373,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1351,1 +1379,1 @@\n-      tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta->ary(),ta->klass(),false,offset);\n+      tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1365,1 +1393,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,Type::Offset(offset));\n@@ -1373,1 +1401,1 @@\n-      tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,offset);\n+      tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,Type::Offset(offset));\n@@ -1376,1 +1404,1 @@\n-      tj = to = TypeInstPtr::make(to->ptr(),to->klass(),to->klass_is_exact(),to->const_oop(),to->offset(), to->instance_id());\n+      tj = to = TypeInstPtr::make(to->ptr(),to->klass(),to->klass_is_exact(),to->const_oop(),Type::Offset(to->offset()), to->klass()->flatten_array(), to->instance_id());\n@@ -1383,1 +1411,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, Type::Offset(offset));\n@@ -1397,1 +1425,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, offset, to->instance_id());\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, Type::Offset(offset), canonical_holder->flatten_array(), to->instance_id());\n@@ -1399,1 +1427,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, offset);\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, Type::Offset(offset));\n@@ -1416,1 +1444,1 @@\n-                                   offset);\n+                                   Type::Offset(offset));\n@@ -1420,1 +1448,1 @@\n-    if( klass->is_obj_array_klass() ) {\n+    if (klass != NULL && klass->is_obj_array_klass()) {\n@@ -1424,1 +1452,1 @@\n-      tj = tk = TypeKlassPtr::make( TypePtr::NotNull, k, offset );\n+      tj = tk = TypeKlassPtr::make(TypePtr::NotNull, k, Type::Offset(offset));\n@@ -1440,1 +1468,1 @@\n-      tj = tk = TypeKlassPtr::make( TypePtr::NotNull, tk->klass(), offset );\n+      tj = tk = TypeKlassPtr::make(TypePtr::NotNull, tk->klass(), Type::Offset(offset));\n@@ -1579,1 +1607,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1583,3 +1611,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = NULL;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1635,0 +1666,1 @@\n+    ciField* field = NULL;\n@@ -1641,0 +1673,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1642,1 +1675,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (elemtype->isa_inlinetype() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->first_field_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1654,0 +1694,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1664,1 +1706,0 @@\n-      ciField* field;\n@@ -1671,0 +1712,4 @@\n+      } else if (tinst->klass()->is_inlinetype()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1672,1 +1717,1 @@\n-        ciInstanceKlass *k = tinst->klass()->as_instance_klass();\n+        ciInstanceKlass* k = tinst->klass()->as_instance_klass();\n@@ -1675,7 +1720,14 @@\n-      assert(field == NULL ||\n-             original_field == NULL ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset() == original_field->offset() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != NULL)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == NULL ||\n+           original_field == NULL ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset() == original_field->offset() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != NULL) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_aryptr()->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1686,3 +1738,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1690,6 +1743,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == NULL) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == NULL) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1863,0 +1917,368 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineTypeBase(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineTypeBase(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    assert(!n->is_InlineType(), \"chain of inline type nodes\");\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineTypePtr()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make inline types scalar in safepoints\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    InlineTypeBaseNode* vt = _inline_type_nodes.at(i)->as_InlineTypeBase();\n+    vt->make_scalar_in_safepoints(&igvn);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeBaseNode* vt = _inline_type_nodes.pop()->as_InlineTypeBase();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+      } else if (vt->is_InlineTypePtr()) {\n+        igvn.replace_node(vt, vt->get_oop());\n+      } else {\n+        igvn.replace_node(vt, igvn.C->top());\n+      }\n+    }\n+  }\n+  \/\/ TODO only check once we are removing, right?\n+  \/\/ Make sure that the return value does not keep an unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = NULL;\n+    for (uint i = 1; i < root()->req(); i++){\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == NULL, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != NULL) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flattened_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flattened_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flattened array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flattened array) correct. We're done with parsing so we\n+  \/\/ now know all flattened array accesses in this compile\n+  \/\/ unit. Let's move flattened array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flattened memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flattened array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = NULL;\n+      if (n->Opcode() == Op_StoreCM) {\n+        adr_type = get_adr_type(get_alias_index(n->in(MemNode::OopStore)->adr_type()));\n+      } else {\n+        adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      }\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != NULL) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flattened_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flattened array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != NULL &&\n+          ace->_adr_type->isa_aryptr() &&\n+          ace->_adr_type->is_aryptr()->is_flat()) {\n+        ace->_adr_type = NULL;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the NULL adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = NULL;\n+      if (m->Opcode() == Op_StoreCM) {\n+        adr_type = m->in(MemNode::OopStore)->adr_type();\n+        if (adr_type != TypeAryPtr::INLINES) {\n+          \/\/ store was optimized out and we lost track of the adr_type\n+          Node* clone = new StoreCMNode(m->in(MemNode::Control), m->in(MemNode::Memory), m->in(MemNode::Address),\n+                                        m->adr_type(), m->in(MemNode::ValueIn), m->in(MemNode::OopStore),\n+                                        get_alias_index(adr_type));\n+          igvn.register_new_node_with_optimizer(clone);\n+          igvn.replace_node(m, clone);\n+        }\n+      } else {\n+        adr_type = m->adr_type();\n+#ifdef ASSERT\n+        m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      }\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flattened array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = NULL;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = NULL;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == NULL) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const Type* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flattened arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flattened array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const Type* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != NULL) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const Type* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flattened_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != NULL) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n+\n@@ -2208,0 +2630,7 @@\n+  if (_inline_type_nodes.length() > 0) {\n+    \/\/ Do this once all inlining is over to avoid getting inconsistent debug info\n+    process_inline_types(igvn);\n+  }\n+\n+  adjust_flattened_array_access_aliases(igvn);\n+\n@@ -2311,0 +2740,6 @@\n+  if (_inline_type_nodes.length() > 0) {\n+    \/\/ Process inline type nodes again and remove them. From here\n+    \/\/ on we don't need to keep track of field values anymore.\n+    process_inline_types(igvn, \/* remove= *\/ true);\n+  }\n+\n@@ -2899,0 +3334,1 @@\n+\n@@ -3052,1 +3488,16 @@\n-      n->add_prec(prec);\n+      if (prec->is_MergeMem()) {\n+        MergeMemNode* mm = prec->as_MergeMem();\n+        Node* base = mm->base_memory();\n+        for (int i = AliasIdxRaw + 1; i < num_alias_types(); i++) {\n+          const Type* adr_type = get_adr_type(i);\n+          if (adr_type->isa_aryptr() && adr_type->is_aryptr()->is_flat()) {\n+            Node* m = mm->memory_at(i);\n+            n->add_prec(m);\n+          }\n+        }\n+        if (mm->outcnt() == 0) {\n+          mm->disconnect_inputs(this);\n+        }\n+      } else {\n+        n->add_prec(prec);\n+      }\n@@ -3623,0 +4074,8 @@\n+#ifdef ASSERT\n+  case Op_InlineTypePtr:\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n+#endif\n@@ -3970,2 +4429,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -3979,1 +4438,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4100,1 +4559,1 @@\n-  if (StressReflectiveCode) {\n+  if (StressReflectiveCode || superk == NULL || subk == NULL) {\n@@ -4109,1 +4568,2 @@\n-  if (superelem->is_array_klass())\n+  if (superelem->is_array_klass()) {\n+    ciArrayKlass* ak = superelem->as_array_klass();\n@@ -4111,0 +4571,1 @@\n+  }\n@@ -4552,0 +5013,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == NULL || tb == NULL ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are NULL.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(NULL, a));\n+    b = phase->transform(new CastP2XNode(NULL, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":535,"deletions":53,"binary":false,"changes":588,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+class CallNode;\n@@ -90,0 +91,1 @@\n+class InlineTypeBaseNode;\n@@ -306,0 +308,2 @@\n+  bool                  _has_flattened_accesses; \/\/ Any known flattened array accesses?\n+  bool                  _flattened_accesses_share_alias; \/\/ Initially all flattened array share a single slice\n@@ -321,0 +325,1 @@\n+  GrowableArray<Node*>  _inline_type_nodes;     \/\/ List of InlineType nodes\n@@ -597,0 +602,7 @@\n+  void          set_flattened_accesses()         { _has_flattened_accesses = true; }\n+  bool          flattened_accesses_share_alias() const { return _flattened_accesses_share_alias; }\n+  void          set_flattened_accesses_share_alias(bool z) { _flattened_accesses_share_alias = z; }\n+\n+  \/\/ Support for scalarized inline type calling convention\n+  bool              has_scalarized_args() const  { return _method != NULL && _method->has_scalarized_args(); }\n+  bool              needs_stack_repair()  const  { return _method != NULL && _method->get_Method()->c2_needs_stack_repair(); }\n@@ -711,0 +723,7 @@\n+  \/\/ Keep track of inline type nodes for later processing\n+  void add_inline_type(Node* n);\n+  void remove_inline_type(Node* n);\n+  void process_inline_types(PhaseIterGVN &igvn, bool remove = false);\n+\n+  void adjust_flattened_array_access_aliases(PhaseIterGVN& igvn);\n+\n@@ -850,1 +869,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = NULL) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = NULL, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }\n@@ -854,1 +873,1 @@\n-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }\n+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, NULL, uncached)->index(); }\n@@ -1090,1 +1109,1 @@\n-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);\n+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);\n@@ -1162,1 +1181,3 @@\n-  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  Node* optimize_acmp(PhaseGVN* phase, Node* a, Node* b);\n+\n+  \/\/ Auxiliary method for randomized fuzzing\/stressing\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":25,"deletions":4,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -243,1 +243,1 @@\n-      for (int i = node->req()-1; i >= 0; --i) {\n+      for (int i = node->len()-1; i >= 0; --i) {\n","filename":"src\/hotspot\/share\/opto\/gcm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -51,2 +51,2 @@\n-  const TypeTuple *jdomain = C->tf()->domain();\n-  const TypeTuple *jrange  = C->tf()->range();\n+  const TypeTuple *jdomain = C->tf()->domain_sig();\n+  const TypeTuple *jrange  = C->tf()->range_sig();\n@@ -279,1 +279,1 @@\n-    if (C->tf()->range()->cnt() > TypeFunc::Parms)\n+    if (C->tf()->range_sig()->cnt() > TypeFunc::Parms)\n","filename":"src\/hotspot\/share\/opto\/generateOptoStub.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -41,0 +43,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -44,0 +47,1 @@\n+#include \"opto\/narrowptrnode.hpp\"\n@@ -57,1 +61,1 @@\n-GraphKit::GraphKit(JVMState* jvms)\n+GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)\n@@ -60,1 +64,1 @@\n-    _gvn(*C->initial_gvn()),\n+    _gvn((gvn != NULL) ? *gvn : *C->initial_gvn()),\n@@ -63,0 +67,1 @@\n+  assert(gvn == NULL || !gvn->is_IterGVN() || gvn->is_IterGVN()->delay_transform(), \"delay transform should be enabled\");\n@@ -66,0 +71,7 @@\n+#ifdef ASSERT\n+  if (_gvn.is_IterGVN() != NULL) {\n+    assert(_gvn.is_IterGVN()->delay_transform(), \"Transformation must be delayed if IterGVN is used\");\n+    \/\/ Save the initial size of _for_igvn worklist for verification (see ~GraphKit)\n+    _worklist_size = _gvn.C->for_igvn()->size();\n+  }\n+#endif\n@@ -833,1 +845,1 @@\n-           (is_anewarray && code == Bytecodes::_multianewarray);\n+           (is_anewarray && (code == Bytecodes::_multianewarray));\n@@ -1093,0 +1105,9 @@\n+  case Bytecodes::_withfield: {\n+    bool ignored_will_link;\n+    ciField* field = method()->get_field_at_bci(bci(), ignored_will_link);\n+    int      size  = field->type()->size();\n+    inputs = size+1;\n+    depth = rsize - inputs;\n+    break;\n+  }\n+\n@@ -1175,1 +1196,1 @@\n-  return _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+  return _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeKlassPtr::OBJECT));\n@@ -1218,0 +1239,1 @@\n+    case T_INLINE_TYPE : \/\/ fall through\n@@ -1389,0 +1411,16 @@\n+Node* GraphKit::null2default(Node* value, ciInlineKlass* vk) {\n+  assert(!vk->is_scalarizable(), \"Should only be used for non scalarizable inline klasses\");\n+  Node* null_ctl = top();\n+  value = null_check_oop(value, &null_ctl);\n+  if (!null_ctl->is_top()) {\n+    \/\/ Return default value if oop is null\n+    Node* region = new RegionNode(3);\n+    region->init_req(1, control());\n+    region->init_req(2, null_ctl);\n+    value = PhiNode::make(region, value, TypeInstPtr::make(TypePtr::BotPTR, vk));\n+    value->set_req(2, InlineTypeNode::default_oop(gvn(), vk));\n+    set_control(gvn().transform(region));\n+    value = gvn().transform(value);\n+  }\n+  return value;\n+}\n@@ -1393,0 +1431,3 @@\n+  if (obj->is_InlineType()) {\n+    return obj;\n+  }\n@@ -1402,0 +1443,5 @@\n+  if (t->is_inlinetypeptr() && t->inline_klass()->is_scalarizable()) {\n+    \/\/ Scalarize inline type now that we know it's non-null\n+    cast = InlineTypeNode::make_from_oop(this, cast, t->inline_klass())->as_ptr(&gvn());\n+  }\n+\n@@ -1526,1 +1572,2 @@\n-  if (((bt == T_OBJECT) && C->do_escape_analysis()) || C->eliminate_boxing()) {\n+\n+  if (((bt == T_OBJECT || bt == T_INLINE_TYPE) && C->do_escape_analysis()) || C->eliminate_boxing()) {\n@@ -1577,1 +1624,2 @@\n-                                DecoratorSet decorators) {\n+                                DecoratorSet decorators,\n+                                bool safe_for_replace) {\n@@ -1590,0 +1638,7 @@\n+  if (val->is_InlineType()) {\n+    \/\/ Store to non-flattened field. Buffer the inline type and make sure\n+    \/\/ the store is re-executed if the allocation triggers deoptimization.\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    val = val->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n@@ -1606,1 +1661,2 @@\n-                               DecoratorSet decorators) {\n+                               DecoratorSet decorators,\n+                               Node* ctl) {\n@@ -1612,1 +1668,1 @@\n-  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr, ctl);\n@@ -1710,2 +1766,2 @@\n-void GraphKit::access_clone(Node* src, Node* dst, Node* size, bool is_array) {\n-  return _barrier_set->clone(this, src, dst, size, is_array);\n+void GraphKit::access_clone(Node* src_base, Node* dst_base, Node* countx, bool is_array) {\n+  return _barrier_set->clone(this, src_base, dst_base, countx, is_array);\n@@ -1718,0 +1774,5 @@\n+  ciKlass* arytype_klass = _gvn.type(ary)->is_aryptr()->klass();\n+  if (arytype_klass != NULL && arytype_klass->is_flat_array_klass()) {\n+    ciFlatArrayKlass* vak = arytype_klass->as_flat_array_klass();\n+    shift = vak->log2_element_size();\n+  }\n@@ -1738,0 +1799,1 @@\n+  assert(elembt != T_INLINE_TYPE, \"inline types are not supported by this method\");\n@@ -1748,6 +1810,32 @@\n-void GraphKit::set_arguments_for_java_call(CallJavaNode* call) {\n-  \/\/ Add the call arguments:\n-  uint nargs = call->method()->arg_size();\n-  for (uint i = 0; i < nargs; i++) {\n-    Node* arg = argument(i);\n-    call->init_req(i + TypeFunc::Parms, arg);\n+void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {\n+  PreserveReexecuteState preexecs(this);\n+  if (EnableValhalla) {\n+    \/\/ Make sure the call is \"re-executed\", if buffering of inline type arguments triggers deoptimization.\n+    \/\/ At this point, the call hasn't been executed yet, so we will only ever execute the call once.\n+    jvms()->set_should_reexecute(true);\n+    int arg_size = method()->get_declared_signature_at_bci(bci())->arg_size_for_bc(java_bc());\n+    inc_sp(arg_size);\n+  }\n+  \/\/ Add the call arguments\n+  const TypeTuple* domain = call->tf()->domain_sig();\n+  uint nargs = domain->cnt();\n+  for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i < nargs; i++) {\n+    Node* arg = argument(i-TypeFunc::Parms);\n+    const Type* t = domain->field_at(i);\n+    if (call->method()->has_scalarized_args() && t->is_inlinetypeptr() && !t->maybe_null() && t->inline_klass()->can_be_passed_as_fields()) {\n+      \/\/ We don't pass inline type arguments by reference but instead pass each field of the inline type\n+      InlineTypeNode* vt = arg->as_InlineType();\n+      vt->pass_fields(this, call, idx);\n+      \/\/ If an inline type argument is passed as fields, attach the Method* to the call site\n+      \/\/ to be able to access the extended signature later via attached_method_before_pc().\n+      \/\/ For example, see CompiledMethod::preserve_callee_argument_oops().\n+      call->set_override_symbolic_info(true);\n+      continue;\n+    } else if (arg->is_InlineType()) {\n+      \/\/ Pass inline type argument via oop to callee\n+      arg = arg->as_InlineType()->buffer(this);\n+      if (!is_late_inline) {\n+        arg = arg->as_InlineTypePtr()->get_oop();\n+      }\n+    }\n+    call->init_req(idx++, arg);\n@@ -1791,7 +1879,0 @@\n-  \/\/ Capture the return value, if any.\n-  Node* ret;\n-  if (call->method() == NULL ||\n-      call->method()->return_type()->basic_type() == T_VOID)\n-        ret = top();\n-  else  ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n-\n@@ -1810,0 +1891,15 @@\n+\n+  \/\/ Capture the return value, if any.\n+  Node* ret;\n+  if (call->method() == NULL || call->method()->return_type()->basic_type() == T_VOID) {\n+    ret = top();\n+  } else if (call->tf()->returns_inline_type_as_fields()) {\n+    \/\/ Return of multiple values (inline type fields): we create a\n+    \/\/ InlineType node, each field is a projection from the call.\n+    ciInlineKlass* vk = call->method()->return_type()->as_inline_klass();\n+    uint base_input = TypeFunc::Parms + 1;\n+    ret = InlineTypeNode::make_from_multi(this, call, vk, base_input, false);\n+  } else {\n+    ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+  }\n+\n@@ -1900,2 +1996,1 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true);\n+  CallProjections* callprojs = call->extract_projections(true);\n@@ -1910,2 +2005,2 @@\n-  if (callprojs.fallthrough_catchproj != NULL) {\n-    C->gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);\n+  if (callprojs->fallthrough_catchproj != NULL) {\n+    C->gvn_replace_by(callprojs->fallthrough_catchproj, final_ctl);\n@@ -1913,1 +2008,1 @@\n-  if (callprojs.fallthrough_memproj != NULL) {\n+  if (callprojs->fallthrough_memproj != NULL) {\n@@ -1918,1 +2013,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);\n+    C->gvn_replace_by(callprojs->fallthrough_memproj,   final_mem);\n@@ -1921,2 +2016,2 @@\n-  if (callprojs.fallthrough_ioproj != NULL) {\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);\n+  if (callprojs->fallthrough_ioproj != NULL) {\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj,    final_io);\n@@ -1926,2 +2021,3 @@\n-  if (callprojs.resproj != NULL && result != NULL) {\n-    C->gvn_replace_by(callprojs.resproj, result);\n+  if (callprojs->resproj[0] != NULL && result != NULL) {\n+    assert(callprojs->nb_resproj == 1, \"unexpected number of results\");\n+    C->gvn_replace_by(callprojs->resproj[0], result);\n@@ -1932,2 +2028,2 @@\n-    if (callprojs.catchall_catchproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, C->top());\n+    if (callprojs->catchall_catchproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, C->top());\n@@ -1935,2 +2031,2 @@\n-    if (callprojs.catchall_memproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_memproj,   C->top());\n+    if (callprojs->catchall_memproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_memproj,   C->top());\n@@ -1938,2 +2034,2 @@\n-    if (callprojs.catchall_ioproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    C->top());\n+    if (callprojs->catchall_ioproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    C->top());\n@@ -1942,2 +2038,2 @@\n-    if (callprojs.exobj != NULL) {\n-      C->gvn_replace_by(callprojs.exobj, C->top());\n+    if (callprojs->exobj != NULL) {\n+      C->gvn_replace_by(callprojs->exobj, C->top());\n@@ -1954,2 +2050,2 @@\n-    if (callprojs.catchall_catchproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, ekit.control());\n+    if (callprojs->catchall_catchproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, ekit.control());\n@@ -1958,1 +2054,1 @@\n-    if (callprojs.catchall_memproj != NULL) {\n+    if (callprojs->catchall_memproj != NULL) {\n@@ -1960,1 +2056,1 @@\n-      C->gvn_replace_by(callprojs.catchall_memproj,   ex_mem);\n+      C->gvn_replace_by(callprojs->catchall_memproj,   ex_mem);\n@@ -1963,2 +2059,2 @@\n-    if (callprojs.catchall_ioproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());\n+    if (callprojs->catchall_ioproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    ekit.i_o());\n@@ -1968,2 +2064,2 @@\n-    if (callprojs.exobj != NULL) {\n-      C->gvn_replace_by(callprojs.exobj, ex_oop);\n+    if (callprojs->exobj != NULL) {\n+      C->gvn_replace_by(callprojs->exobj, ex_oop);\n@@ -1983,1 +2079,1 @@\n-  if (callprojs.fallthrough_catchproj != NULL && !final_ctl->is_top() && do_replaced_nodes) {\n+  if (callprojs->fallthrough_catchproj != NULL && !final_ctl->is_top() && do_replaced_nodes) {\n@@ -2181,1 +2277,1 @@\n-    const TypePtr* ptr = (ptr_kind == ProfileMaybeNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n+    const TypePtr* ptr = (ptr_kind != ProfileNeverNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n@@ -2204,1 +2300,1 @@\n-    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::OffsetBot, TypeOopPtr::InstanceBot, speculative);\n+    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, speculative);\n@@ -2238,2 +2334,9 @@\n-      if (!data->as_BitData()->null_seen()) {\n-        ptr_kind = ProfileNeverNull;\n+      if (java_bc() == Bytecodes::_aastore) {\n+        ciKlass* array_type = NULL;\n+        ciKlass* element_type = NULL;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+        exact_kls = element_type;\n+        ptr_kind = element_ptr;\n@@ -2241,7 +2344,11 @@\n-        assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n-        ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n-        uint i = 0;\n-        for (; i < call->row_limit(); i++) {\n-          ciKlass* receiver = call->receiver(i);\n-          if (receiver != NULL) {\n-            break;\n+        if (!data->as_BitData()->null_seen()) {\n+          ptr_kind = ProfileNeverNull;\n+        } else {\n+          assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n+          ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n+          uint i = 0;\n+          for (; i < call->row_limit(); i++) {\n+            ciKlass* receiver = call->receiver(i);\n+            if (receiver != NULL) {\n+              break;\n+            }\n@@ -2249,0 +2356,1 @@\n+          ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2250,1 +2358,0 @@\n-        ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2269,1 +2376,1 @@\n-  int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+  int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2272,1 +2379,1 @@\n-    const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+    const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2343,1 +2450,1 @@\n-    int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+    int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2345,1 +2452,1 @@\n-      const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+      const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2599,1 +2706,1 @@\n-      const Type* type = call_type->domain()->field_at(TypeFunc::Parms + vm_unfiltered_arg_pos);\n+      const Type* type = call_type->domain_sig()->field_at(TypeFunc::Parms + vm_unfiltered_arg_pos);\n@@ -2610,1 +2717,1 @@\n-  uint n_returns = call_type->range()->cnt() - TypeFunc::Parms;\n+  uint n_returns = call_type->range_sig()->cnt() - TypeFunc::Parms;\n@@ -2618,1 +2725,1 @@\n-      const Type* type = call_type->range()->field_at(TypeFunc::Parms + vm_ret_pos);\n+      const Type* type = call_type->range_sig()->field_at(TypeFunc::Parms + vm_ret_pos);\n@@ -2946,0 +3053,4 @@\n+  const Type* sub_t = _gvn.type(obj_or_subklass);\n+  if (sub_t->isa_inlinetype()) {\n+    obj_or_subklass = makecon(TypeKlassPtr::make(sub_t->inline_klass()));\n+  }\n@@ -2950,1 +3061,1 @@\n-    if (!_gvn.type(obj_or_subklass)->isa_klassptr()) {\n+    if (!sub_t->isa_klassptr() && !sub_t->isa_inlinetype()) {\n@@ -2953,1 +3064,0 @@\n-\n@@ -2959,1 +3069,0 @@\n-  const TypePtr* adr_type = TypeKlassPtr::make(TypePtr::NotNull, C->env()->Object_klass(), Type::OffsetBot);\n@@ -2969,2 +3078,13 @@\n-                                    float prob,\n-                                    Node* *casted_receiver) {\n+                                    float prob, Node* *casted_receiver) {\n+  Node* fail = top();\n+  const Type* rec_t = _gvn.type(receiver);\n+  if (rec_t->isa_inlinetype()) {\n+    if (klass->equals(rec_t->inline_klass())) {\n+      (*casted_receiver) = receiver; \/\/ Always passes\n+    } else {\n+      (*casted_receiver) = top();    \/\/ Always fails\n+      fail = control();\n+      set_control(top());\n+    }\n+    return fail;\n+  }\n@@ -2973,7 +3093,1 @@\n-  Node* want_klass = makecon(tklass);\n-  Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass) );\n-  Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );\n-  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n-  set_control( _gvn.transform( new IfTrueNode (iff) ));\n-  Node* fail = _gvn.transform( new IfFalseNode(iff) );\n-\n+  fail = type_check(recv_klass, tklass, prob);\n@@ -2986,1 +3100,7 @@\n-  (*casted_receiver) = _gvn.transform(cast);\n+  Node* res = _gvn.transform(cast);\n+  if (recv_xtype->is_inlinetypeptr() && recv_xtype->inline_klass()->is_scalarizable()) {\n+    assert(!gvn().type(res)->maybe_null(), \"receiver should never be null\");\n+    res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass());\n+  }\n+\n+  (*casted_receiver) = res;\n@@ -2992,0 +3112,11 @@\n+Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,\n+                           float prob) {\n+  Node* want_klass = makecon(tklass);\n+  Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass));\n+  Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );\n+  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n+  set_control(  _gvn.transform( new IfTrueNode (iff)));\n+  Node* fail = _gvn.transform( new IfFalseNode(iff));\n+  return fail;\n+}\n+\n@@ -3031,0 +3162,3 @@\n+    if (java_bc() == Bytecodes::_aastore) {\n+      return ((ciArrayLoadStoreData*)data->as_ArrayLoadStoreData())->element()->ptr_kind() == ProfileNeverNull;\n+    }\n@@ -3110,1 +3244,14 @@\n-  ciKlass* exact_kls = spec_klass == NULL ? profile_has_unique_klass() : spec_klass;\n+  ciKlass* exact_kls = spec_klass;\n+  if (exact_kls == NULL) {\n+    if (java_bc() == Bytecodes::_aastore) {\n+      ciKlass* array_type = NULL;\n+      ciKlass* element_type = NULL;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      exact_kls = element_type;\n+    } else {\n+      exact_kls = profile_has_unique_klass();\n+    }\n+  }\n@@ -3215,0 +3362,1 @@\n+  bool is_value = obj->is_InlineType();\n@@ -3218,1 +3366,1 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = is_value ? obj : null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n@@ -3236,7 +3384,9 @@\n-  bool known_statically = false;\n-  if (_gvn.type(superklass)->singleton()) {\n-    ciKlass* superk = _gvn.type(superklass)->is_klassptr()->klass();\n-    ciKlass* subk = _gvn.type(obj)->is_oopptr()->klass();\n-    if (subk != NULL && subk->is_loaded()) {\n-      int static_res = C->static_subtype_check(superk, subk);\n-      known_statically = (static_res == Compile::SSC_always_true || static_res == Compile::SSC_always_false);\n+  if (!is_value) {\n+    bool known_statically = false;\n+    if (_gvn.type(superklass)->singleton()) {\n+      ciKlass* superk = _gvn.type(superklass)->is_klassptr()->klass();\n+      ciKlass* subk = _gvn.type(obj)->is_oopptr()->klass();\n+      if (subk != NULL && subk->is_loaded()) {\n+        int static_res = C->static_subtype_check(superk, subk);\n+        known_statically = (static_res == Compile::SSC_always_true || static_res == Compile::SSC_always_false);\n+      }\n@@ -3244,14 +3394,17 @@\n-  }\n-  if (!known_statically) {\n-    const TypeOopPtr* obj_type = _gvn.type(obj)->is_oopptr();\n-    \/\/ We may not have profiling here or it may not help us. If we\n-    \/\/ have a speculative type use it to perform an exact cast.\n-    ciKlass* spec_obj_type = obj_type->speculative_type();\n-    if (spec_obj_type != NULL || (ProfileDynamicTypes && data != NULL)) {\n-      Node* cast_obj = maybe_cast_profiled_receiver(not_null_obj, NULL, spec_obj_type, safe_for_replace);\n-      if (stopped()) {            \/\/ Profile disagrees with this path.\n-        set_control(null_ctl);    \/\/ Null is the only remaining possibility.\n-        return intcon(0);\n-      }\n-      if (cast_obj != NULL) {\n-        not_null_obj = cast_obj;\n+    if (!known_statically) {\n+      const TypeOopPtr* obj_type = _gvn.type(obj)->is_oopptr();\n+      \/\/ We may not have profiling here or it may not help us. If we\n+      \/\/ have a speculative type use it to perform an exact cast.\n+      ciKlass* spec_obj_type = obj_type->speculative_type();\n+      if (spec_obj_type != NULL || (ProfileDynamicTypes && data != NULL)) {\n+        Node* cast_obj = maybe_cast_profiled_receiver(not_null_obj, NULL, spec_obj_type, safe_for_replace);\n+        if (stopped()) {            \/\/ Profile disagrees with this path.\n+          set_control(null_ctl);    \/\/ Null is the only remaining possibility.\n+          return intcon(0);\n+        }\n+        if (cast_obj != NULL &&\n+            \/\/ A value that's sometimes null is not something we can optimize well\n+            !(cast_obj->is_InlineType() && null_ctl != top())) {\n+          not_null_obj = cast_obj;\n+          is_value = not_null_obj->is_InlineType();\n+        }\n@@ -3281,1 +3434,1 @@\n-  if (safe_for_replace) {\n+  if (safe_for_replace && !is_value) {\n@@ -3296,2 +3449,1 @@\n-Node* GraphKit::gen_checkcast(Node *obj, Node* superklass,\n-                              Node* *failure_control) {\n+Node* GraphKit::gen_checkcast(Node *obj, Node* superklass, Node* *failure_control) {\n@@ -3299,2 +3451,6 @@\n-  const TypeKlassPtr *tk = _gvn.type(superklass)->is_klassptr();\n-  const Type *toop = TypeOopPtr::make_from_klass(tk->klass());\n+  const TypeKlassPtr* tk = _gvn.type(superklass)->is_klassptr();\n+  const TypeOopPtr* toop = TypeOopPtr::make_from_klass(tk->klass());\n+\n+  \/\/ Check if inline types are involved\n+  bool from_inline = obj->is_InlineType();\n+  bool to_inline = tk->klass()->is_inlinetype();\n@@ -3309,3 +3465,11 @@\n-    const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n-    if (objtp != NULL && objtp->klass() != NULL) {\n-      switch (C->static_subtype_check(tk->klass(), objtp->klass())) {\n+    ciKlass* klass = NULL;\n+    if (from_inline) {\n+      klass = _gvn.type(obj)->inline_klass();\n+    } else {\n+      const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n+      if (objtp != NULL) {\n+        klass = objtp->klass();\n+      }\n+    }\n+    if (klass != NULL) {\n+      switch (C->static_subtype_check(tk->klass(), klass)) {\n@@ -3316,1 +3480,10 @@\n-        return record_profiled_receiver_for_speculation(obj);\n+        if (!from_inline) {\n+          obj = record_profiled_receiver_for_speculation(obj);\n+          if (to_inline) {\n+            obj = null_check(obj);\n+            if (toop->inline_klass()->is_scalarizable()) {\n+              obj = InlineTypeNode::make_from_oop(this, obj, toop->inline_klass());\n+            }\n+          }\n+        }\n+        return obj;\n@@ -3318,4 +3491,6 @@\n-        \/\/ It needs a null check because a null will *pass* the cast check.\n-        \/\/ A non-null value will always produce an exception.\n-        if (!objtp->maybe_null()) {\n-          builtin_throw(Deoptimization::Reason_class_check, makecon(TypeKlassPtr::make(objtp->klass())));\n+        if (from_inline || to_inline) {\n+          if (!from_inline) {\n+            null_check(obj);\n+          }\n+          \/\/ Inline type is never null. Always throw an exception.\n+          builtin_throw(Deoptimization::Reason_class_check, makecon(TypeKlassPtr::make(klass)));\n@@ -3323,2 +3498,10 @@\n-        } else if (!too_many_traps_or_recompiles(Deoptimization::Reason_null_assert)) {\n-          return null_assert(obj);\n+        } else {\n+          \/\/ It needs a null check because a null will *pass* the cast check.\n+          const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n+          if (!objtp->maybe_null()) {\n+            builtin_throw(Deoptimization::Reason_class_check, makecon(TypeKlassPtr::make(objtp->klass())));\n+            return top();\n+          } else if (!too_many_traps_or_recompiles(Deoptimization::Reason_null_assert)) {\n+            return null_assert(obj);\n+          }\n+          break; \/\/ Fall through to full check\n@@ -3326,1 +3509,0 @@\n-        break; \/\/ Fall through to full check\n@@ -3337,1 +3519,3 @@\n-    data = method()->method_data()->bci_to_data(bci());\n+    if (method()->method_data()->is_mature()) {\n+      data = method()->method_data()->bci_to_data(bci());\n+    }\n@@ -3345,0 +3529,3 @@\n+  _gvn.set_type(region, Type::CONTROL);\n+  _gvn.set_type(phi, toop);\n+\n@@ -3354,1 +3541,8 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = NULL;\n+  if (from_inline) {\n+    not_null_obj = obj;\n+  } else if (to_inline) {\n+    not_null_obj = null_check(obj);\n+  } else {\n+    not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  }\n@@ -3372,1 +3566,1 @@\n-  if (tk->klass_is_exact()) {\n+  if (!from_inline && tk->klass_is_exact()) {\n@@ -3383,0 +3577,7 @@\n+      if (cast_obj != NULL && cast_obj->is_InlineType()) {\n+        if (null_ctl != top()) {\n+          cast_obj = NULL; \/\/ A value that's sometimes null is not something we can optimize well\n+        } else {\n+          return cast_obj;\n+        }\n+      }\n@@ -3394,1 +3595,1 @@\n-    Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass );\n+    Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass);\n@@ -3397,1 +3598,1 @@\n-    cast_obj = _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));\n+    cast_obj = from_inline ? not_null_obj : _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));\n@@ -3403,1 +3604,7 @@\n-        builtin_throw(Deoptimization::Reason_class_check, load_object_klass(not_null_obj));\n+        Node* obj_klass = NULL;\n+        if (from_inline) {\n+          obj_klass = makecon(TypeKlassPtr::make(_gvn.type(not_null_obj)->inline_klass()));\n+        } else {\n+          obj_klass = load_object_klass(not_null_obj);\n+        }\n+        builtin_throw(Deoptimization::Reason_class_check, obj_klass);\n@@ -3430,1 +3637,114 @@\n-  return record_profiled_receiver_for_speculation(res);\n+  bool not_inline = !toop->can_be_inline_type();\n+  bool not_flattened = !UseFlatArray || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->flatten_array());\n+  if (EnableValhalla && not_flattened) {\n+    \/\/ Check if obj has been loaded from an array\n+    obj = obj->isa_DecodeN() ? obj->in(1) : obj;\n+    Node* array = NULL;\n+    if (obj->isa_Load()) {\n+      Node* address = obj->in(MemNode::Address);\n+      if (address->isa_AddP()) {\n+        array = address->as_AddP()->in(AddPNode::Base);\n+      }\n+    } else if (obj->is_Phi()) {\n+      Node* region = obj->in(0);\n+      \/\/ TODO make this more robust (see JDK-8231346)\n+      if (region->req() == 3 && region->in(2) != NULL && region->in(2)->in(0) != NULL) {\n+        IfNode* iff = region->in(2)->in(0)->isa_If();\n+        if (iff != NULL) {\n+          iff->is_flat_array_check(&_gvn, &array);\n+        }\n+      }\n+    }\n+    if (array != NULL) {\n+      const TypeAryPtr* ary_t = _gvn.type(array)->isa_aryptr();\n+      if (ary_t != NULL) {\n+        if (!ary_t->is_not_null_free() && not_inline) {\n+          \/\/ Casting array element to a non-inline-type, mark array as not null-free.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_null_free()));\n+          replace_in_map(array, cast);\n+        } else if (!ary_t->is_not_flat()) {\n+          \/\/ Casting array element to a non-flattened type, mark array as not flat.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_flat()));\n+          replace_in_map(array, cast);\n+        }\n+      }\n+    }\n+  }\n+\n+  if (!stopped() && !from_inline) {\n+    res = record_profiled_receiver_for_speculation(res);\n+    if (to_inline && toop->inline_klass()->is_scalarizable()) {\n+      assert(!gvn().type(res)->maybe_null(), \"Inline types are null-free\");\n+      res = InlineTypeNode::make_from_oop(this, res, toop->inline_klass());\n+    }\n+  }\n+  return res;\n+}\n+\n+Node* GraphKit::inline_type_test(Node* obj, bool is_inline) {\n+  Node* mark_adr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+  Node* mark = make_load(NULL, mark_adr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  Node* mask = MakeConX(markWord::inline_type_pattern);\n+  Node* masked = _gvn.transform(new AndXNode(mark, mask));\n+  Node* cmp = _gvn.transform(new CmpXNode(masked, mask));\n+  return _gvn.transform(new BoolNode(cmp, is_inline ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::array_lh_test(Node* klass, jint mask, jint val, bool eq) {\n+  Node* lh_adr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+  \/\/ Make sure to use immutable memory here to enable hoisting the check out of loops\n+  Node* lh_val = _gvn.transform(LoadNode::make(_gvn, NULL, immutable_memory(), lh_adr, lh_adr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+  Node* masked = _gvn.transform(new AndINode(lh_val, intcon(mask)));\n+  Node* cmp = _gvn.transform(new CmpINode(masked, intcon(val)));\n+  return _gvn.transform(new BoolNode(cmp, eq ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::flat_array_test(Node* ary, bool flat) {\n+  \/\/ We can't use immutable memory here because the mark word is mutable.\n+  \/\/ PhaseIdealLoop::move_flat_array_check_out_of_loop will make sure the\n+  \/\/ check is moved out of loops (mainly to enable loop unswitching).\n+  Node* mem = UseArrayMarkWordCheck ? memory(Compile::AliasIdxRaw) : immutable_memory();\n+  Node* cmp = _gvn.transform(new FlatArrayCheckNode(C, mem, ary));\n+  record_for_igvn(cmp); \/\/ Give it a chance to be optimized out by IGVN\n+  return _gvn.transform(new BoolNode(cmp, flat ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::null_free_array_test(Node* klass, bool null_free) {\n+  return array_lh_test(klass, Klass::_lh_null_free_bit_inplace, 0, !null_free);\n+}\n+\n+\/\/ Deoptimize if 'ary' is a null-free inline type array and 'val' is null\n+Node* GraphKit::inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {\n+  const Type* val_t = _gvn.type(val);\n+  if (val->is_InlineType() || !TypePtr::NULL_PTR->higher_equal(val_t)) {\n+    return ary; \/\/ Never null\n+  }\n+  RegionNode* region = new RegionNode(3);\n+  Node* null_ctl = top();\n+  null_check_oop(val, &null_ctl);\n+  if (null_ctl != top()) {\n+    PreserveJVMState pjvms(this);\n+    set_control(null_ctl);\n+    {\n+      \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(load_object_klass(ary), \/* null_free = *\/ false), PROB_MAX);\n+      inc_sp(nargs);\n+      uncommon_trap(Deoptimization::Reason_null_check,\n+                    Deoptimization::Action_none);\n+    }\n+    region->init_req(1, control());\n+  }\n+  region->init_req(2, control());\n+  set_control(_gvn.transform(region));\n+  record_for_igvn(region);\n+  const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+  if (val_t == TypePtr::NULL_PTR && !ary_t->is_not_null_free()) {\n+    \/\/ Since we were just successfully storing null, the array can't be null free.\n+    ary_t = ary_t->cast_to_not_null_free();\n+    Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+    if (safe_for_replace) {\n+      replace_in_map(ary, cast);\n+    }\n+    ary = cast;\n+  }\n+  return ary;\n@@ -3498,0 +3818,1 @@\n+\n@@ -3570,0 +3891,1 @@\n+  assert(!obj->is_InlineTypeBase(), \"should not unlock on inline type\");\n@@ -3610,2 +3932,8 @@\n-    bool    xklass = inst_klass->klass_is_exact();\n-    if (xklass || klass->is_array_klass()) {\n+    assert(klass != NULL, \"klass should not be NULL\");\n+    bool xklass = inst_klass->klass_is_exact();\n+    bool can_be_flattened = false;\n+    if (UseFlatArray && klass->is_obj_array_klass()) {\n+      ciKlass* elem = klass->as_obj_array_klass()->element_klass();\n+      can_be_flattened = elem->can_be_inline_klass() && (!elem->is_inlinetype() || elem->flatten_array());\n+    }\n+    if (xklass || (klass->is_array_klass() && !can_be_flattened)) {\n@@ -3673,0 +4001,1 @@\n+    _gvn.set_type(minit_in, Type::MEMORY);\n@@ -3680,3 +4009,26 @@\n-      const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n-      int            elemidx  = C->get_alias_index(telemref);\n-      hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      const TypeAryPtr* arytype = oop_type->is_aryptr();\n+      if (arytype->klass()->is_flat_array_klass()) {\n+        \/\/ Initially all flattened array accesses share a single slice\n+        \/\/ but that changes after parsing. Prepare the memory graph so\n+        \/\/ it can optimize flattened array accesses properly once they\n+        \/\/ don't share a single slice.\n+        assert(C->flattened_accesses_share_alias(), \"should be set at parse time\");\n+        C->set_flattened_accesses_share_alias(false);\n+        ciFlatArrayKlass* vak = arytype->klass()->as_flat_array_klass();\n+        ciInlineKlass* vk = vak->element_klass()->as_inline_klass();\n+        for (int i = 0, len = vk->nof_nonstatic_fields(); i < len; i++) {\n+          ciField* field = vk->nonstatic_field_at(i);\n+          if (field->offset() >= TrackedInitializationLimit * HeapWordSize)\n+            continue;  \/\/ do not bother to track really large numbers of fields\n+          int off_in_vt = field->offset() - vk->first_field_offset();\n+          const TypePtr* adr_type = arytype->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+          int fieldidx = C->get_alias_index(adr_type, true);\n+          hook_memory_on_init(*this, fieldidx, minit_in, minit_out);\n+        }\n+        C->set_flattened_accesses_share_alias(true);\n+        hook_memory_on_init(*this, C->get_alias_index(TypeAryPtr::INLINES), minit_in, minit_out);\n+      } else {\n+        const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n+        int            elemidx  = C->get_alias_index(telemref);\n+        hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      }\n@@ -3684,0 +4036,1 @@\n+      set_memory(minit_out, C->get_alias_index(oop_type)); \/\/ mark word\n@@ -3734,1 +4087,2 @@\n-                             bool deoptimize_on_exception) {\n+                             bool deoptimize_on_exception,\n+                             InlineTypeBaseNode* inline_type_node) {\n@@ -3741,1 +4095,1 @@\n-  int   layout_is_con = (layout_val == NULL);\n+  bool  layout_is_con = (layout_val == NULL);\n@@ -3792,1 +4146,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3799,1 +4153,1 @@\n-                                         initial_slow_test);\n+                                         initial_slow_test, inline_type_node);\n@@ -3805,1 +4159,1 @@\n-\/\/ helper for both newarray and anewarray\n+\/\/ helper for newarray and anewarray\n@@ -3815,1 +4169,1 @@\n-  int   layout_is_con = (layout_val == NULL);\n+  bool  layout_is_con = (layout_val == NULL);\n@@ -3845,1 +4199,1 @@\n-    fast_size_limit <<= (LogBytesPerLong - log2_esize);\n+    fast_size_limit <<= MAX2(LogBytesPerLong - log2_esize, 0);\n@@ -3863,1 +4217,1 @@\n-    BasicType etype  = Klass::layout_helper_element_type(layout_con);\n+    bool is_flat_array = Klass::layout_helper_is_flatArray(layout_con);\n@@ -3866,1 +4220,1 @@\n-    assert((hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n+    assert(is_flat_array || (hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n@@ -3950,1 +4304,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3959,0 +4313,63 @@\n+  const TypeKlassPtr* ary_klass = _gvn.type(klass_node)->isa_klassptr();\n+  const TypeOopPtr* ary_type = ary_klass->as_instance_type();\n+  const TypeAryPtr* ary_ptr = ary_type->isa_aryptr();\n+\n+  \/\/ Inline type array variants:\n+  \/\/ - null-ok:              MyValue.ref[] (ciObjArrayKlass \"[LMyValue$ref\")\n+  \/\/ - null-free:            MyValue.val[] (ciObjArrayKlass \"[QMyValue$val\")\n+  \/\/ - null-free, flattened: MyValue.val[] (ciFlatArrayKlass \"[QMyValue$val\")\n+  \/\/ Check if array is a null-free, non-flattened inline type array\n+  \/\/ that needs to be initialized with the default inline type.\n+  Node* default_value = NULL;\n+  Node* raw_default_value = NULL;\n+  if (ary_ptr != NULL && ary_ptr->klass_is_exact()) {\n+    \/\/ Array type is known\n+    ciKlass* elem_klass = ary_ptr->klass()->as_array_klass()->element_klass();\n+    if (elem_klass != NULL && elem_klass->is_inlinetype()) {\n+      ciInlineKlass* vk = elem_klass->as_inline_klass();\n+      if (!vk->flatten_array()) {\n+        default_value = InlineTypeNode::default_oop(gvn(), vk);\n+      }\n+    }\n+  } else if (ary_klass->klass()->can_be_inline_array_klass()) {\n+    \/\/ Array type is not known, add runtime checks\n+    assert(!ary_klass->klass_is_exact(), \"unexpected exact type\");\n+    Node* r = new RegionNode(3);\n+    default_value = new PhiNode(r, TypeInstPtr::BOTTOM);\n+\n+    Node* bol = array_lh_test(klass_node, Klass::_lh_array_tag_vt_value_bit_inplace | Klass::_lh_null_free_bit_inplace, Klass::_lh_null_free_bit_inplace);\n+    IfNode* iff = create_and_map_if(control(), bol, PROB_FAIR, COUNT_UNKNOWN);\n+\n+    \/\/ Null-free, non-flattened inline type array, initialize with the default value\n+    set_control(_gvn.transform(new IfTrueNode(iff)));\n+    Node* p = basic_plus_adr(klass_node, in_bytes(ArrayKlass::element_klass_offset()));\n+    Node* eklass = _gvn.transform(LoadKlassNode::make(_gvn, control(), immutable_memory(), p, TypeInstPtr::KLASS));\n+    Node* adr_fixed_block_addr = basic_plus_adr(eklass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    Node* adr_fixed_block = make_load(control(), adr_fixed_block_addr, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+    Node* default_value_offset_addr = basic_plus_adr(adr_fixed_block, in_bytes(InlineKlass::default_value_offset_offset()));\n+    Node* default_value_offset = make_load(control(), default_value_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+    Node* elem_mirror = load_mirror_from_klass(eklass);\n+    Node* default_value_addr = basic_plus_adr(elem_mirror, ConvI2X(default_value_offset));\n+    Node* val = access_load_at(elem_mirror, default_value_addr, _gvn.type(default_value_addr)->is_ptr(), TypeInstPtr::BOTTOM, T_OBJECT, IN_HEAP);\n+    r->init_req(1, control());\n+    default_value->init_req(1, val);\n+\n+    \/\/ Otherwise initialize with all zero\n+    r->init_req(2, _gvn.transform(new IfFalseNode(iff)));\n+    default_value->init_req(2, null());\n+\n+    set_control(_gvn.transform(r));\n+    default_value = _gvn.transform(default_value);\n+  }\n+  if (default_value != NULL) {\n+    if (UseCompressedOops) {\n+      \/\/ With compressed oops, the 64-bit init value is built from two 32-bit compressed oops\n+      default_value = _gvn.transform(new EncodePNode(default_value, default_value->bottom_type()->make_narrowoop()));\n+      Node* lower = _gvn.transform(new CastP2XNode(control(), default_value));\n+      Node* upper = _gvn.transform(new LShiftLNode(lower, intcon(32)));\n+      raw_default_value = _gvn.transform(new OrLNode(lower, upper));\n+    } else {\n+      raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));\n+    }\n+  }\n+\n@@ -3960,6 +4377,6 @@\n-  AllocateArrayNode* alloc\n-    = new AllocateArrayNode(C, AllocateArrayNode::alloc_type(TypeInt::INT),\n-                            control(), mem, i_o(),\n-                            size, klass_node,\n-                            initial_slow_test,\n-                            length);\n+  AllocateArrayNode* alloc = new AllocateArrayNode(C, AllocateArrayNode::alloc_type(TypeInt::INT),\n+                                                   control(), mem, i_o(),\n+                                                   size, klass_node,\n+                                                   initial_slow_test,\n+                                                   length, default_value,\n+                                                   raw_default_value);\n@@ -3972,1 +4389,0 @@\n-  const TypeOopPtr* ary_type = _gvn.type(klass_node)->is_klassptr()->as_instance_type();\n@@ -4130,1 +4546,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4133,2 +4549,2 @@\n-                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS),\n-                                                  ciTypeArrayKlass::make(T_BYTE), true, 0);\n+                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, true, true),\n+                                                  ciTypeArrayKlass::make(T_BYTE), true, Type::Offset(0));\n@@ -4147,1 +4563,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4159,1 +4575,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4169,1 +4585,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4280,1 +4696,9 @@\n-    return makecon(con_type);\n+    Node* con = makecon(con_type);\n+    \/\/ Check type of constant which might be more precise\n+    if (con_type->is_inlinetypeptr() && con_type->inline_klass()->is_scalarizable()) {\n+      assert(!con_type->is_zero_type(), \"Inline types are null-free\");\n+      con = InlineTypeNode::make_from_oop(this, con, con_type->inline_klass());\n+    } else if (con_type->is_zero_type() && field->type()->is_inlinetype()) {\n+      con = InlineTypeNode::default_oop(gvn(), field->type()->as_inline_klass());\n+    }\n+    return con;\n@@ -4284,0 +4708,9 @@\n+\n+\/\/---------------------------load_mirror_from_klass----------------------------\n+\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n+Node* GraphKit::load_mirror_from_klass(Node* klass) {\n+  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n+  Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+  \/\/ mirror = ((OopHandle)mirror)->resolve();\n+  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n+}\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":590,"deletions":157,"binary":false,"changes":747,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -317,0 +318,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -326,0 +329,1 @@\n+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_INLINE_TYPE,Relaxed, false);\n@@ -336,0 +340,1 @@\n+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_INLINE_TYPE,Relaxed, false);\n@@ -2179,2 +2184,2 @@\n-      assert(rtype == type, \"getter must return the expected value\");\n-      assert(sig->count() == 2, \"oop getter has 2 arguments\");\n+      assert(rtype == type || (rtype == T_OBJECT && type == T_INLINE_TYPE), \"getter must return the expected value\");\n+      assert(sig->count() == 2 || (type == T_INLINE_TYPE && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n@@ -2186,1 +2191,1 @@\n-      assert(sig->count() == 3, \"oop putter has 3 arguments\");\n+      assert(sig->count() == 3 || (type == T_INLINE_TYPE && sig->count() == 4), \"oop putter has 3 arguments\");\n@@ -2190,1 +2195,1 @@\n-      assert(vtype == type, \"putter must accept the expected value\");\n+      assert(vtype == type || (type == T_INLINE_TYPE && vtype == T_OBJECT), \"putter must accept the expected value\");\n@@ -2215,0 +2220,58 @@\n+\n+  ciInlineKlass* inline_klass = NULL;\n+  if (type == T_INLINE_TYPE) {\n+    Node* cls = null_check(argument(4));\n+    if (stopped()) {\n+      return true;\n+    }\n+    Node* kls = load_klass_from_mirror(cls, false, NULL, 0);\n+    const TypeKlassPtr* kls_t = _gvn.type(kls)->isa_klassptr();\n+    if (!kls_t->klass_is_exact()) {\n+      return false;\n+    }\n+    ciKlass* klass = kls_t->klass();\n+    if (!klass->is_inlinetype()) {\n+      return false;\n+    }\n+    inline_klass = klass->as_inline_klass();\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  if (base->is_InlineType()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (is_store) {\n+      if (!vt->is_allocated(&_gvn) || !_gvn.type(vt)->is_inlinetype()->larval()) {\n+        return false;\n+      }\n+      base = vt->get_oop();\n+    } else {\n+      if (offset->is_Con()) {\n+        long off = find_long_con(offset, 0);\n+        ciInlineKlass* vk = vt->type()->inline_klass();\n+        if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+          return false;\n+        }\n+\n+        ciField* field = vk->get_non_flattened_field_by_offset(off);\n+        if (field != NULL) {\n+          BasicType bt = field->layout_type();\n+          if (bt == T_ARRAY || bt == T_NARROWOOP || (bt == T_INLINE_TYPE && !field->is_flattened())) {\n+            bt = T_OBJECT;\n+          }\n+          if (bt == type && (bt != T_INLINE_TYPE || field->type() == inline_klass)) {\n+            set_result(vt->field_value_by_offset(off, false));\n+            return true;\n+          }\n+        }\n+      }\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      base = vt->buffer(this)->get_oop();\n+    }\n+  }\n+\n@@ -2220,1 +2283,1 @@\n-    if (type != T_OBJECT) {\n+    if (type != T_OBJECT && (inline_klass == NULL || !inline_klass->has_object_fields())) {\n@@ -2236,1 +2299,1 @@\n-  val = is_store ? argument(4) : NULL;\n+  val = is_store ? argument(4 + (type == T_INLINE_TYPE ? 1 : 0)) : NULL;\n@@ -2253,1 +2316,25 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = NULL;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->klass()->as_instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != NULL &&\n+        instptr->klass() == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (instptr->klass()->as_instance_klass()->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flattened_field_by_offset(off);\n+    }\n+    if (field != NULL) {\n+      bt = field->layout_type();\n+    }\n+    assert(bt == alias_type->basic_type() || bt == T_INLINE_TYPE, \"should match\");\n+    if (field != NULL && bt == T_INLINE_TYPE && !field->is_flattened()) {\n+      bt = T_OBJECT;\n+    }\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2274,0 +2361,21 @@\n+  if (type == T_INLINE_TYPE) {\n+    if (adr_type->isa_instptr()) {\n+      if (field == NULL || field->type() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else if (adr_type->isa_aryptr()) {\n+      const Type* elem = adr_type->is_aryptr()->elem();\n+      if (!elem->isa_inlinetype()) {\n+        mismatched = true;\n+      } else if (elem->inline_klass() != inline_klass) {\n+        mismatched = true;\n+      }\n+    }\n+    if (is_store) {\n+      const Type* val_t = _gvn.type(val);\n+      if (!val_t->isa_inlinetype() || val_t->inline_klass() != inline_klass) {\n+        return false;\n+      }\n+    }\n+  }\n+\n@@ -2286,4 +2394,8 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != NULL) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != NULL) {\n+        value_type = tjp;\n+      }\n+    } else if (type == T_INLINE_TYPE) {\n+      value_type = NULL;\n@@ -2293,4 +2405,0 @@\n-  receiver = null_check(receiver);\n-  if (stopped()) {\n-    return true;\n-  }\n@@ -2305,2 +2413,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != NULL && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != NULL && field->is_constant() && !field->is_flattened() && !mismatched) {\n@@ -2312,1 +2420,11 @@\n-      p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      if (type == T_INLINE_TYPE) {\n+        if (adr_type->isa_instptr() && !mismatched) {\n+          ciInstanceKlass* holder = adr_type->is_instptr()->klass()->as_instance_klass();\n+          int offset = adr_type->is_instptr()->offset();\n+          p = InlineTypeNode::make_from_flattened(this, inline_klass, base, base, holder, offset, decorators);\n+        } else {\n+          p = InlineTypeNode::make_from_flattened(this, inline_klass, base, adr, NULL, 0, decorators);\n+        }\n+      } else {\n+        p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      }\n@@ -2339,0 +2457,8 @@\n+    if (field != NULL && field->type()->is_inlinetype() && !field->is_flattened()) {\n+      \/\/ Load a non-flattened inline type from memory\n+      if (value_type->inline_klass()->is_scalarizable()) {\n+        p = InlineTypeNode::make_from_oop(this, p, value_type->inline_klass());\n+      } else {\n+        p = null2default(p, value_type->inline_klass());\n+      }\n+    }\n@@ -2350,1 +2476,17 @@\n-    access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    if (type == T_INLINE_TYPE) {\n+      if (adr_type->isa_instptr() && !mismatched) {\n+        ciInstanceKlass* holder = adr_type->is_instptr()->klass()->as_instance_klass();\n+        int offset = adr_type->is_instptr()->offset();\n+        val->as_InlineType()->store_flattened(this, base, base, holder, offset, decorators);\n+      } else {\n+        val->as_InlineType()->store_flattened(this, base, adr, NULL, 0, decorators);\n+      }\n+    } else {\n+      access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    }\n+  }\n+\n+  if (argument(1)->is_InlineType() && is_store) {\n+    Node* value = InlineTypeNode::make_from_oop(this, base, _gvn.type(base)->inline_klass());\n+    value = value->as_InlineType()->make_larval(this, false);\n+    replace_in_map(argument(1), value);\n@@ -2356,0 +2498,41 @@\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  if (!value->is_InlineType()) {\n+    return false;\n+  }\n+\n+  set_result(value->as_InlineType()->make_larval(this, true));\n+\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  if (!buffer->is_InlineType()) {\n+    return false;\n+  }\n+\n+  InlineTypeNode* vt = buffer->as_InlineType();\n+  if (!vt->is_allocated(&_gvn) || !_gvn.type(vt)->is_inlinetype()->larval()) {\n+    return false;\n+  }\n+\n+  set_result(vt->finish_larval(this));\n+\n+  return true;\n+}\n+\n@@ -2815,9 +2998,0 @@\n-\/\/---------------------------load_mirror_from_klass----------------------------\n-\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {\n-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n-  Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n-  \/\/ mirror = ((OopHandle)mirror)->resolve();\n-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n-}\n-\n@@ -2866,0 +3040,1 @@\n+\n@@ -2873,0 +3048,4 @@\n+Node* LibraryCallKit::generate_value_guard(Node* kls, RegionNode* region) {\n+  return generate_access_flags_guard(kls, JVM_ACC_INLINE, 0, region);\n+}\n+\n@@ -3070,1 +3249,7 @@\n-  const TypeOopPtr* tp = _gvn.type(obj)->isa_oopptr();\n+  ciKlass* obj_klass = NULL;\n+  const Type* obj_t = _gvn.type(obj);\n+  if (obj->is_InlineType()) {\n+    obj_klass = obj_t->inline_klass();\n+  } else if (obj_t->isa_oopptr()) {\n+    obj_klass = obj_t->is_oopptr()->klass();\n+  }\n@@ -3075,3 +3260,2 @@\n-  if (tm != NULL && tm->is_klass() &&\n-      tp != NULL && tp->klass() != NULL) {\n-    if (!tp->klass()->is_loaded()) {\n+  if (tm != NULL && tm->is_klass() && obj_klass != NULL) {\n+    if (!obj_klass->is_loaded()) {\n@@ -3081,1 +3265,8 @@\n-      int static_res = C->static_subtype_check(tm->as_klass(), tp->klass());\n+      if (!obj->is_InlineType() && tm->as_klass()->is_inlinetype()) {\n+        \/\/ Casting to .val, check for null\n+        obj = null_check(obj);\n+        if (stopped()) {\n+          return true;\n+        }\n+      }\n+      int static_res = C->static_subtype_check(tm->as_klass(), obj_klass);\n@@ -3111,1 +3302,1 @@\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -3122,0 +3313,19 @@\n+    if (EnableValhalla && !obj->is_InlineType()) {\n+      \/\/ Check if we are casting to .val\n+      Node* is_val_kls = generate_value_guard(kls, NULL);\n+      if (is_val_kls != NULL) {\n+        RegionNode* r = new RegionNode(3);\n+        record_for_igvn(r);\n+        r->init_req(1, control());\n+\n+        \/\/ Casting to .val, check for null\n+        set_control(is_val_kls);\n+        Node* null_ctr = top();\n+        null_check_oop(obj, &null_ctr);\n+        region->init_req(_npe_path, null_ctr);\n+        r->init_req(2, control());\n+\n+        set_control(_gvn.transform(r));\n+      }\n+    }\n+\n@@ -3128,1 +3338,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -3165,0 +3376,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -3167,0 +3379,1 @@\n+  record_for_igvn(prim_region);\n@@ -3191,2 +3404,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -3209,1 +3425,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -3214,1 +3431,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -3245,2 +3462,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind) {\n@@ -3252,9 +3468,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -3265,4 +3472,13 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;\n+      case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case FlatArray:      query = Klass::layout_helper_is_flatArray(layout_con); break;\n+      case NonFlatArray:   query = !Klass::layout_helper_is_flatArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -3278,0 +3494,28 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case ObjectArray:\n+    case NonObjectArray: {\n+      value = Klass::_lh_array_tag_obj_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == ObjectArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case FlatArray:\n+    case NonFlatArray: {\n+      value = 0;\n+      layout_val = _gvn.transform(new AndINode(layout_val, intcon(Klass::_lh_array_tag_vt_value_bit_inplace)));\n+      btest = (kind == FlatArray) ? BoolTest::ne : BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -3279,4 +3523,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -3284,3 +3525,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -3293,1 +3531,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -3438,1 +3676,13 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    ciKlass* klass = _gvn.type(klass_node)->is_klassptr()->klass();\n+    bool exclude_flat = UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == NULL || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        klass->can_be_inline_array_klass() && (!klass->is_flat_array_klass() || klass->as_flat_array_klass()->element_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_objArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n@@ -3442,1 +3692,1 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n@@ -3448,0 +3698,22 @@\n+    Node* original_kls = load_object_klass(original);\n+    \/\/ ArrayCopyNode:Ideal may transform the ArrayCopyNode to\n+    \/\/ loads\/stores but it is legal only if we're sure the\n+    \/\/ Arrays.copyOf would succeed. So we need all input arguments\n+    \/\/ to the copyOf to be validated, including that the copy to the\n+    \/\/ new array won't trigger an ArrayStoreException. That subtype\n+    \/\/ check can be optimized if we know something on the type of\n+    \/\/ the input array from type speculation.\n+    if (_gvn.type(klass_node)->singleton() && !stopped()) {\n+      ciKlass* subk   = _gvn.type(original_kls)->is_klassptr()->klass();\n+      ciKlass* superk = _gvn.type(klass_node)->is_klassptr()->klass();\n+\n+      int test = C->static_subtype_check(superk, subk);\n+      if (test != Compile::SSC_always_true && test != Compile::SSC_always_false) {\n+        const TypeOopPtr* t_original = _gvn.type(original)->is_oopptr();\n+        if (t_original->speculative_type() != NULL) {\n+          original = maybe_cast_profiled_obj(original, t_original->speculative_type(), true);\n+          original_kls = load_object_klass(original);\n+        }\n+      }\n+    }\n+\n@@ -3463,0 +3735,32 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != NULL && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_non_flatArray_guard(klass_node, bailout);\n+        }\n+      } else if (UseFlatArray && (orig_t == NULL || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!klass->is_flat_array_klass() && klass->can_be_inline_array_klass()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_flatArray_guard(original_kls, bailout);\n+        if (orig_t != NULL) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(null_free_array_test(klass_node), bailout);\n+      }\n+    }\n+\n@@ -3482,20 +3786,0 @@\n-      \/\/ ArrayCopyNode:Ideal may transform the ArrayCopyNode to\n-      \/\/ loads\/stores but it is legal only if we're sure the\n-      \/\/ Arrays.copyOf would succeed. So we need all input arguments\n-      \/\/ to the copyOf to be validated, including that the copy to the\n-      \/\/ new array won't trigger an ArrayStoreException. That subtype\n-      \/\/ check can be optimized if we know something on the type of\n-      \/\/ the input array from type speculation.\n-      if (_gvn.type(klass_node)->singleton()) {\n-        ciKlass* subk   = _gvn.type(load_object_klass(original))->is_klassptr()->klass();\n-        ciKlass* superk = _gvn.type(klass_node)->is_klassptr()->klass();\n-\n-        int test = C->static_subtype_check(superk, subk);\n-        if (test != Compile::SSC_always_true && test != Compile::SSC_always_false) {\n-          const TypeOopPtr* t_original = _gvn.type(original)->is_oopptr();\n-          if (t_original->speculative_type() != NULL) {\n-            original = maybe_cast_profiled_obj(original, t_original->speculative_type(), true);\n-          }\n-        }\n-      }\n-\n@@ -3505,1 +3789,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -3522,1 +3806,1 @@\n-                                                load_object_klass(original), klass_node);\n+                                                original_kls, klass_node);\n@@ -3644,1 +3928,6 @@\n-  Node* obj = NULL;\n+  Node* obj = argument(0);\n+\n+  if (obj->is_InlineType() || gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -3654,1 +3943,0 @@\n-    obj = argument(0);\n@@ -3694,0 +3982,1 @@\n+  \/\/ This also serves as guard against inline types (they have the always_locked_pattern set).\n@@ -3760,1 +4049,7 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    ciKlass* vk = _gvn.type(obj)->inline_klass();\n+    set_result(makecon(TypeInstPtr::make(vk->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -4022,1 +4317,8 @@\n-  access_clone(obj, alloc_obj, size, is_array);\n+  \/\/ Exclude the header but include array length to copy by 8 bytes words.\n+  \/\/ Can't use base_offset_in_bytes(bt) since basic type is unknown.\n+  int base_off = BarrierSetC2::arraycopy_payload_base_offset(is_array);\n+  Node* countx = size;\n+  countx = _gvn.transform(new SubXNode(countx, MakeConX(base_off)));\n+  countx = _gvn.transform(new URShiftXNode(countx, intcon(LogBytesPerLong)));\n+\n+  access_clone(obj, alloc_obj, countx, is_array);\n@@ -4065,1 +4367,6 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    if (obj->is_InlineType()) {\n+      return false;\n+    }\n+\n+    obj = null_check_receiver();\n@@ -4075,1 +4382,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -4107,0 +4415,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -4112,3 +4425,0 @@\n-      Node* obj_length = load_array_length(obj);\n-      Node* obj_size  = NULL;\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size, \/*deoptimize_on_exception=*\/true);\n@@ -4117,20 +4427,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);\n-        if (is_obja != NULL) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing) &&\n+          obj_type->klass()->can_be_inline_array_klass() &&\n+          (ary_ptr == NULL || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flattened inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_flatArray_guard(obj_klass, slow_region);\n@@ -4138,7 +4435,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -4147,7 +4437,43 @@\n-        copy_to_clone(obj, alloc_obj, obj_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(obj);\n+        Node* obj_size  = NULL;\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);\n+          if (is_obja != NULL) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, obj_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -4157,4 +4483,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -4321,2 +4643,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -4325,1 +4646,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -4337,1 +4658,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -4511,0 +4832,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -4514,0 +4837,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -4529,2 +4854,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -4573,0 +4897,2 @@\n+      slow_region->add_req(not_subtype_ctrl);\n+    }\n@@ -4574,6 +4900,28 @@\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n+    const Type* toop = TypeOopPtr::make_from_klass(dest_klass_t->klass());\n+    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n+    src_type = _gvn.type(src);\n+    top_src  = src_type->isa_aryptr();\n+\n+    \/\/ Handle flat inline type arrays (null-free arrays are handled by the subtype check above)\n+    if (!stopped() && UseFlatArray) {\n+      \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check). Handle flat src here.\n+      assert(top_dest == NULL || !top_dest->is_flat() || top_src->is_flat(), \"src array must be flat\");\n+      if (top_src != NULL && top_src->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (top_dest != NULL && !top_dest->is_flat()) {\n+          generate_non_flatArray_guard(dest_klass, slow_region);\n+          \/\/ Since dest is flat and src <: dest, dest must have the same type as src.\n+          top_dest = TypeOopPtr::make_from_klass(top_src->klass())->isa_aryptr();\n+          assert(top_dest->is_flat(), \"dest must be flat\");\n+          dest = _gvn.transform(new CheckCastPPNode(control(), dest, top_dest));\n+        }\n+      } else if (top_src == NULL || !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        assert(top_dest == NULL || !top_dest->is_flat(), \"dest array must not be flat\");\n+        generate_flatArray_guard(load_object_klass(src), slow_region);\n+        if (top_src != NULL) {\n+          top_src = top_src->cast_to_not_flat();\n+          src = _gvn.transform(new CheckCastPPNode(control(), src, top_src));\n+        }\n@@ -4582,0 +4930,1 @@\n+\n@@ -4589,4 +4938,0 @@\n-\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = TypeOopPtr::make_from_klass(dest_klass_t->klass());\n-    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":494,"deletions":149,"binary":false,"changes":643,"status":"modified"},{"patch":"@@ -80,1 +80,2 @@\n-         TransformedLongLoop=262144};\n+         TransformedLongLoop=262144,\n+         FlattenedArrays=524288};\n@@ -106,0 +107,1 @@\n+  bool is_flattened_arrays() const { return _loop_flags & FlattenedArrays; }\n@@ -121,0 +123,1 @@\n+  void mark_flattened_arrays() { _loop_flags |= FlattenedArrays; }\n@@ -1366,1 +1369,1 @@\n-  IfNode* find_unswitching_candidate(const IdealLoopTree *loop) const;\n+  IfNode* find_unswitching_candidate(const IdealLoopTree *loop, Node_List& unswitch_iffs) const;\n@@ -1487,0 +1490,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1488,0 +1492,1 @@\n+  bool flatten_array_element_type_check(Node *n);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -64,0 +65,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return NULL;\n+  }\n+\n@@ -969,0 +976,50 @@\n+\/\/ If UseArrayMarkWordCheck is enabled, we can't use immutable memory for the flat array check\n+\/\/ because we are loading the mark word which is mutable. Although the bits we are interested in\n+\/\/ are immutable (we check for markWord::unlocked_value), we need to use raw memory to not break\n+\/\/ anti dependency analysis. Below code will attempt to still move flat array checks out of loops,\n+\/\/ mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::Array)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    ResourceMark rm;\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -981,0 +1038,6 @@\n+\n+  if (UseArrayMarkWordCheck && n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1246,0 +1309,96 @@\n+bool PhaseIdealLoop::flatten_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flattened array and the load of the value\n+  \/\/ happens with a flattened array check then: push the type check\n+  \/\/ through the phi of the flattened array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == NULL) {\n+    return false;\n+  }\n+\n+  assert(obj != NULL && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != NULL, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      _igvn.set_type(cast_clone, cast_clone->Value(&_igvn));\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1252,0 +1411,4 @@\n+  if (flatten_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1528,0 +1691,5 @@\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(&_igvn, this);\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":168,"deletions":0,"binary":false,"changes":168,"status":"modified"},{"patch":"@@ -571,0 +571,3 @@\n+  if (n->is_InlineTypeBase()) {\n+    C->add_inline_type(n);\n+  }\n@@ -653,0 +656,3 @@\n+  if (is_InlineTypeBase()) {\n+    compile->remove_inline_type(this);\n+  }\n@@ -2186,1 +2192,3 @@\n-      assert(i >= req() || i == 0 || is_Region() || is_Phi() || is_ArrayCopy() || (is_Unlock() && i == req()-1)\n+      assert(i >= req() || i == 0 || is_Region() || is_Phi() || is_ArrayCopy() ||\n+             (is_Allocate() && i >= AllocateNode::InlineTypeNode) ||\n+             (is_Unlock() && i == req()-1)\n@@ -2188,1 +2196,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+             \"only region, phi, arraycopy, allocate or unlock nodes have null data edges\");\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -81,0 +81,1 @@\n+class FlatArrayCheckNode;\n@@ -113,0 +114,1 @@\n+class MachPrologNode;\n@@ -119,0 +121,1 @@\n+class MachVEPNode;\n@@ -163,0 +166,3 @@\n+class InlineTypeBaseNode;\n+class InlineTypeNode;\n+class InlineTypePtrNode;\n@@ -680,0 +686,2 @@\n+      DEFINE_CLASS_ID(MachProlog,       Mach, 8)\n+      DEFINE_CLASS_ID(MachVEP,          Mach, 9)\n@@ -695,0 +703,3 @@\n+      DEFINE_CLASS_ID(InlineTypeBase, Type, 8)\n+        DEFINE_CLASS_ID(InlineType, InlineTypeBase, 0)\n+        DEFINE_CLASS_ID(InlineTypePtr, InlineTypeBase, 1)\n@@ -729,3 +740,4 @@\n-        DEFINE_CLASS_ID(FastLock,   Cmp, 0)\n-        DEFINE_CLASS_ID(FastUnlock, Cmp, 1)\n-        DEFINE_CLASS_ID(SubTypeCheck,Cmp, 2)\n+        DEFINE_CLASS_ID(FastLock,       Cmp, 0)\n+        DEFINE_CLASS_ID(FastUnlock,     Cmp, 1)\n+        DEFINE_CLASS_ID(SubTypeCheck,   Cmp, 2)\n+        DEFINE_CLASS_ID(FlatArrayCheck, Cmp, 3)\n@@ -858,0 +870,1 @@\n+  DEFINE_CLASS_QUERY(FlatArrayCheck)\n@@ -890,0 +903,1 @@\n+  DEFINE_CLASS_QUERY(MachProlog)\n@@ -896,0 +910,1 @@\n+  DEFINE_CLASS_QUERY(MachVEP)\n@@ -920,0 +935,3 @@\n+  DEFINE_CLASS_QUERY(InlineType)\n+  DEFINE_CLASS_QUERY(InlineTypeBase)\n+  DEFINE_CLASS_QUERY(InlineTypePtr)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":21,"deletions":3,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -140,0 +140,3 @@\n+  \/\/ For the inline type calling convention\n+  int                    _sp_inc_slot;\n+  int                    _sp_inc_slot_offset_in_bytes;\n@@ -244,0 +247,2 @@\n+  int               sp_inc_offset()      const  { return _sp_inc_slot_offset_in_bytes; }\n+\n","filename":"src\/hotspot\/share\/opto\/output.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciSymbols.hpp\"\n@@ -39,0 +40,2 @@\n+#include \"opto\/idealKit.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -53,0 +56,17 @@\n+Node* Parse::record_profile_for_speculation_at_array_load(Node* ld) {\n+  \/\/ Feed unused profile data to type speculation\n+  if (UseTypeSpeculation && UseArrayLoadStoreProfile) {\n+    ciKlass* array_type = NULL;\n+    ciKlass* element_type = NULL;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool flat_array = true;\n+    bool null_free_array = true;\n+    method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+    if (element_type != NULL || element_ptr != ProfileMaybeNull) {\n+      ld = record_profile_for_speculation(ld, element_type, element_ptr);\n+    }\n+  }\n+  return ld;\n+}\n+\n+\n@@ -56,1 +76,0 @@\n-  bool big_val = bt == T_DOUBLE || bt == T_LONG;\n@@ -60,2 +79,126 @@\n-  pop();                      \/\/ index (already used)\n-  Node* array = pop();        \/\/ the array itself\n+  Node* idx = pop();\n+  Node* ary = pop();\n+\n+  \/\/ Handle inline type arrays\n+  const TypeOopPtr* elemptr = elemtype->make_oopptr();\n+  const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+  if (ary_t->is_flat()) {\n+    \/\/ Load from flattened inline type array\n+    Node* vt = InlineTypeNode::make_from_flattened(this, elemtype->inline_klass(), ary, adr);\n+    push(vt);\n+    return;\n+  } else if (ary_t->is_null_free()) {\n+    \/\/ Load from non-flattened inline type array (elements can never be null)\n+    bt = T_INLINE_TYPE;\n+  } else if (!ary_t->is_not_flat()) {\n+    \/\/ Cannot statically determine if array is flattened, emit runtime check\n+    assert(UseFlatArray && is_reference_type(bt) && elemptr->can_be_inline_type() && !ary_t->klass_is_exact() && !ary_t->is_not_null_free() &&\n+           (!elemptr->is_inlinetypeptr() || elemptr->inline_klass()->flatten_array()), \"array can't be flattened\");\n+    IdealKit ideal(this);\n+    IdealVariable res(ideal);\n+    ideal.declarations_done();\n+    ideal.if_then(flat_array_test(ary, \/* flat = *\/ false)); {\n+      \/\/ non-flattened\n+      assert(ideal.ctrl()->in(0)->as_If()->is_flat_array_check(&_gvn), \"Should be found\");\n+      sync_kit(ideal);\n+      const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n+      Node* ld = access_load_at(ary, adr, adr_type, elemptr, bt,\n+                                IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD);\n+      ideal.sync_kit(this);\n+      ideal.set(res, ld);\n+    } ideal.else_(); {\n+      \/\/ flattened\n+      sync_kit(ideal);\n+      if (elemptr->is_inlinetypeptr()) {\n+        \/\/ Element type is known, cast and load from flattened representation\n+        ciInlineKlass* vk = elemptr->inline_klass();\n+        assert(vk->flatten_array() && elemptr->maybe_null(), \"never\/always flat - should be optimized\");\n+        ciArrayKlass* array_klass = ciArrayKlass::make(vk);\n+        const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+        Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, arytype));\n+        Node* casted_adr = array_element_address(cast, idx, T_INLINE_TYPE, ary_t->size(), control());\n+        \/\/ Re-execute flattened array load if buffering triggers deoptimization\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        inc_sp(2);\n+        Node* vt = InlineTypeNode::make_from_flattened(this, vk, cast, casted_adr)->buffer(this, false);\n+        ideal.set(res, vt);\n+        ideal.sync_kit(this);\n+      } else {\n+        \/\/ Element type is unknown, emit runtime call\n+        Node* kls = load_object_klass(ary);\n+        Node* k_adr = basic_plus_adr(kls, in_bytes(ArrayKlass::element_klass_offset()));\n+        Node* elem_klass = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+        Node* obj_size  = NULL;\n+        kill_dead_locals();\n+        \/\/ Re-execute flattened array load if buffering triggers deoptimization\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_bci(_bci);\n+        jvms()->set_should_reexecute(true);\n+        inc_sp(2);\n+        Node* alloc_obj = new_instance(elem_klass, NULL, &obj_size, \/*deoptimize_on_exception=*\/true);\n+\n+        AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_obj, &_gvn);\n+        assert(alloc->maybe_set_complete(&_gvn), \"\");\n+        alloc->initialization()->set_complete_with_arraycopy();\n+\n+        \/\/ This membar keeps this access to an unknown flattened array\n+        \/\/ correctly ordered with other unknown and known flattened\n+        \/\/ array accesses.\n+        insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        \/\/ Unknown inline type might contain reference fields\n+        if (false && !bs->array_copy_requires_gc_barriers(false, T_OBJECT, false, BarrierSetC2::Parsing)) {\n+          \/\/ FIXME 8230656 also merge changes from 8238759 in\n+          int base_off = sizeof(instanceOopDesc);\n+          Node* dst_base = basic_plus_adr(alloc_obj, base_off);\n+          Node* countx = obj_size;\n+          countx = _gvn.transform(new SubXNode(countx, MakeConX(base_off)));\n+          countx = _gvn.transform(new URShiftXNode(countx, intcon(LogBytesPerLong)));\n+\n+          assert(Klass::_lh_log2_element_size_shift == 0, \"use shift in place\");\n+          Node* lhp = basic_plus_adr(kls, in_bytes(Klass::layout_helper_offset()));\n+          Node* elem_shift = make_load(NULL, lhp, TypeInt::INT, T_INT, MemNode::unordered);\n+          uint header = arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE);\n+          Node* base  = basic_plus_adr(ary, header);\n+          idx = Compile::conv_I2X_index(&_gvn, idx, TypeInt::POS, control());\n+          Node* scale = _gvn.transform(new LShiftXNode(idx, elem_shift));\n+          Node* adr = basic_plus_adr(ary, base, scale);\n+\n+          access_clone(adr, dst_base, countx, false);\n+        } else {\n+          ideal.sync_kit(this);\n+          ideal.make_leaf_call(OptoRuntime::load_unknown_inline_type(),\n+                               CAST_FROM_FN_PTR(address, OptoRuntime::load_unknown_inline),\n+                               \"load_unknown_inline\",\n+                               ary, idx, alloc_obj);\n+          sync_kit(ideal);\n+        }\n+\n+        \/\/ This makes sure no other thread sees a partially initialized buffered inline type\n+        insert_mem_bar_volatile(Op_MemBarStoreStore, Compile::AliasIdxRaw, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+\n+        \/\/ Same as MemBarCPUOrder above: keep this unknown flattened\n+        \/\/ array access correctly ordered with other flattened array\n+        \/\/ access\n+        insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+        \/\/ Prevent any use of the newly allocated inline type before it is fully initialized\n+        alloc_obj = new CastPPNode(alloc_obj, _gvn.type(alloc_obj), true);\n+        alloc_obj->set_req(0, control());\n+        alloc_obj = _gvn.transform(alloc_obj);\n+\n+        const Type* unknown_value = elemptr->is_instptr()->cast_to_flatten_array();\n+        alloc_obj = _gvn.transform(new CheckCastPPNode(control(), alloc_obj, unknown_value));\n+\n+        ideal.sync_kit(this);\n+        ideal.set(res, alloc_obj);\n+      }\n+    } ideal.end_if();\n+    sync_kit(ideal);\n+    Node* ld = _gvn.transform(ideal.value(res));\n+    ld = record_profile_for_speculation_at_array_load(ld);\n+    push_node(bt, ld);\n+    return;\n+  }\n@@ -67,2 +210,1 @@\n-\n-  Node* ld = access_load_at(array, adr, adr_type, elemtype, bt,\n+  Node* ld = access_load_at(ary, adr, adr_type, elemtype, bt,\n@@ -70,4 +212,9 @@\n-  if (big_val) {\n-    push_pair(ld);\n-  } else {\n-    push(ld);\n+  if (bt == T_INLINE_TYPE) {\n+    \/\/ Loading a non-flattened inline type from an array\n+    assert(!gvn().type(ld)->maybe_null(), \"inline type array elements should never be null\");\n+    if (elemptr->inline_klass()->is_scalarizable()) {\n+      ld = InlineTypeNode::make_from_oop(this, ld, elemptr->inline_klass());\n+    }\n+  }\n+  if (!ld->is_InlineType()) {\n+    ld = record_profile_for_speculation_at_array_load(ld);\n@@ -75,0 +222,2 @@\n+\n+  push_node(bt, ld);\n@@ -81,2 +230,1 @@\n-  bool big_val = bt == T_DOUBLE || bt == T_LONG;\n-  Node* adr = array_addressing(bt, big_val ? 2 : 1, elemtype);\n+  Node* adr = array_addressing(bt, type2size[bt], elemtype);\n@@ -84,0 +232,1 @@\n+  Node* cast_val = NULL;\n@@ -85,10 +234,2 @@\n-    array_store_check();\n-    if (stopped()) {\n-      return;\n-    }\n-  }\n-  Node* val;                  \/\/ Oop to store\n-  if (big_val) {\n-    val = pop_pair();\n-  } else {\n-    val = pop();\n+    cast_val = array_store_check(adr, elemtype);\n+    if (stopped()) return;\n@@ -96,2 +237,7 @@\n-  pop();                      \/\/ index (already used)\n-  Node* array = pop();        \/\/ the array itself\n+  Node* val = pop_node(bt); \/\/ Value to store\n+  Node* idx = pop();        \/\/ Index in the array\n+  Node* ary = pop();        \/\/ The array itself\n+\n+  const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+  const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n+  assert(adr->as_AddP()->in(AddPNode::Base) == ary, \"inconsistent address base\");\n@@ -101,0 +247,145 @@\n+  } else if (bt == T_OBJECT) {\n+    elemtype = elemtype->make_oopptr();\n+    const Type* tval = _gvn.type(cast_val);\n+    \/\/ We may have lost type information for 'val' here due to the casts\n+    \/\/ emitted by the array_store_check code (see JDK-6312651)\n+    \/\/ TODO Remove this code once JDK-6312651 is in.\n+    const Type* tval_init = _gvn.type(val);\n+    \/\/ Based on the value to be stored, try to determine if the array is not null-free and\/or not flat.\n+    \/\/ This is only legal for non-null stores because the array_store_check always passes for null, even\n+    \/\/ if the array is null-free. Null stores are handled in GraphKit::gen_inline_array_null_guard().\n+    bool not_inline = !tval->isa_inlinetype() &&\n+                      ((!tval_init->maybe_null() && !tval_init->is_oopptr()->can_be_inline_type()) ||\n+                       (!tval->maybe_null() && !tval->is_oopptr()->can_be_inline_type()));\n+    bool not_flattened = not_inline || ((tval_init->is_inlinetypeptr() || tval_init->isa_inlinetype()) && !tval_init->inline_klass()->flatten_array());\n+    if (!ary_t->is_not_null_free() && not_inline) {\n+      \/\/ Storing a non-inline type, mark array as not null-free (-> not flat).\n+      ary_t = ary_t->cast_to_not_null_free();\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+      replace_in_map(ary, cast);\n+      ary = cast;\n+    } else if (!ary_t->is_not_flat() && not_flattened) {\n+      \/\/ Storing a non-flattened value, mark array as not flat.\n+      ary_t = ary_t->cast_to_not_flat();\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+      replace_in_map(ary, cast);\n+      ary = cast;\n+    }\n+\n+    if (ary_t->is_flat()) {\n+      \/\/ Store to flattened inline type array\n+      if (!cast_val->is_InlineType()) {\n+        inc_sp(3);\n+        cast_val = null_check(cast_val);\n+        if (stopped()) return;\n+        dec_sp(3);\n+        cast_val = InlineTypeNode::make_from_oop(this, cast_val, ary_t->elem()->inline_klass());\n+      }\n+      \/\/ Re-execute flattened array store if buffering triggers deoptimization\n+      PreserveReexecuteState preexecs(this);\n+      inc_sp(3);\n+      jvms()->set_should_reexecute(true);\n+      cast_val->as_InlineType()->store_flattened(this, ary, adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+      return;\n+    } else if (ary_t->is_null_free()) {\n+      \/\/ Store to non-flattened inline type array (elements can never be null)\n+      if (!cast_val->is_InlineType() && tval->maybe_null()) {\n+        inc_sp(3);\n+        cast_val = null_check(cast_val);\n+        if (stopped()) return;\n+        dec_sp(3);\n+      }\n+      if (elemtype->inline_klass()->is_empty()) {\n+        \/\/ Ignore empty inline stores, array is already initialized.\n+        return;\n+      }\n+    } else if (!ary_t->is_not_flat() && (tval != TypePtr::NULL_PTR || StressReflectiveCode)) {\n+      \/\/ Array might be flattened, emit runtime checks (for NULL, a simple inline_array_null_guard is sufficient).\n+      assert(UseFlatArray && !not_flattened && elemtype->is_oopptr()->can_be_inline_type() &&\n+             !ary_t->klass_is_exact() && !ary_t->is_not_null_free(), \"array can't be flattened\");\n+      IdealKit ideal(this);\n+      ideal.if_then(flat_array_test(ary, \/* flat = *\/ false)); {\n+        \/\/ non-flattened\n+        assert(ideal.ctrl()->in(0)->as_If()->is_flat_array_check(&_gvn), \"Should be found\");\n+        sync_kit(ideal);\n+        Node* cast_ary = inline_array_null_guard(ary, cast_val, 3);\n+        inc_sp(3);\n+        access_store_at(cast_ary, adr, adr_type, cast_val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY, false);\n+        dec_sp(3);\n+        ideal.sync_kit(this);\n+      } ideal.else_(); {\n+        Node* val = cast_val;\n+        \/\/ flattened\n+        if (!val->is_InlineType() && tval->maybe_null()) {\n+          \/\/ Add null check\n+          sync_kit(ideal);\n+          Node* null_ctl = top();\n+          val = null_check_oop(val, &null_ctl);\n+          if (null_ctl != top()) {\n+            PreserveJVMState pjvms(this);\n+            inc_sp(3);\n+            set_control(null_ctl);\n+            uncommon_trap(Deoptimization::Reason_null_check, Deoptimization::Action_none);\n+            dec_sp(3);\n+          }\n+          ideal.sync_kit(this);\n+        }\n+        \/\/ Try to determine the inline klass\n+        ciInlineKlass* vk = NULL;\n+        if (tval->isa_inlinetype() || tval->is_inlinetypeptr()) {\n+          vk = tval->inline_klass();\n+        } else if (tval_init->isa_inlinetype() || tval_init->is_inlinetypeptr()) {\n+          vk = tval_init->inline_klass();\n+        } else if (elemtype->is_inlinetypeptr()) {\n+          vk = elemtype->inline_klass();\n+        }\n+        Node* casted_ary = ary;\n+        if (vk != NULL && !stopped()) {\n+          \/\/ Element type is known, cast and store to flattened representation\n+          sync_kit(ideal);\n+          assert(vk->flatten_array() && elemtype->maybe_null(), \"never\/always flat - should be optimized\");\n+          ciArrayKlass* array_klass = ciArrayKlass::make(vk);\n+          const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+          casted_ary = _gvn.transform(new CheckCastPPNode(control(), casted_ary, arytype));\n+          Node* casted_adr = array_element_address(casted_ary, idx, T_OBJECT, arytype->size(), control());\n+          if (!val->is_InlineType()) {\n+            assert(!gvn().type(val)->maybe_null(), \"inline type array elements should never be null\");\n+            val = InlineTypeNode::make_from_oop(this, val, vk);\n+          }\n+          \/\/ Re-execute flattened array store if buffering triggers deoptimization\n+          PreserveReexecuteState preexecs(this);\n+          inc_sp(3);\n+          jvms()->set_should_reexecute(true);\n+          val->as_InlineType()->store_flattened(this, casted_ary, casted_adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+          ideal.sync_kit(this);\n+        } else if (!ideal.ctrl()->is_top()) {\n+          \/\/ Element type is unknown, emit runtime call\n+          sync_kit(ideal);\n+\n+          \/\/ This membar keeps this access to an unknown flattened\n+          \/\/ array correctly ordered with other unknown and known\n+          \/\/ flattened array accesses.\n+          insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+          ideal.sync_kit(this);\n+\n+          ideal.make_leaf_call(OptoRuntime::store_unknown_inline_type(),\n+                               CAST_FROM_FN_PTR(address, OptoRuntime::store_unknown_inline),\n+                               \"store_unknown_inline\",\n+                               val, casted_ary, idx);\n+\n+          sync_kit(ideal);\n+          \/\/ Same as MemBarCPUOrder above: keep this unknown\n+          \/\/ flattened array access correctly ordered with other\n+          \/\/ flattened array accesses.\n+          insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+          ideal.sync_kit(this);\n+        }\n+      }\n+      ideal.end_if();\n+      sync_kit(ideal);\n+      return;\n+    } else if (!ary_t->is_not_null_free()) {\n+      \/\/ Array is not flattened but may be null free\n+      assert(elemtype->is_oopptr()->can_be_inline_type() && !ary_t->klass_is_exact(), \"array can't be null-free\");\n+      ary = inline_array_null_guard(ary, cast_val, 3, true);\n+    }\n@@ -102,3 +393,3 @@\n-  const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n-\n-  access_store_at(array, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+  inc_sp(3);\n+  access_store_at(ary, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+  dec_sp(3);\n@@ -204,0 +495,116 @@\n+  \/\/ This could be an access to an inline type array. We can't tell if it's\n+  \/\/ flat or not. Knowing the exact type avoids runtime checks and leads to\n+  \/\/ a much simpler graph shape. Check profile information.\n+  if (!arytype->is_flat() && !arytype->is_not_flat()) {\n+    \/\/ First check the speculative type\n+    Deoptimization::DeoptReason reason = Deoptimization::Reason_speculate_class_check;\n+    ciKlass* array_type = arytype->speculative_type();\n+    if (too_many_traps_or_recompiles(reason) || array_type == NULL) {\n+      \/\/ No speculative type, check profile data at this bci\n+      array_type = NULL;\n+      reason = Deoptimization::Reason_class_check;\n+      if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(reason)) {\n+        ciKlass* element_type = NULL;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      }\n+    }\n+    if (array_type != NULL) {\n+      \/\/ Speculate that this array has the exact type reported by profile data\n+      Node* better_ary = NULL;\n+      DEBUG_ONLY(Node* old_control = control();)\n+      Node* slow_ctl = type_check_receiver(ary, array_type, 1.0, &better_ary);\n+      if (stopped()) {\n+        \/\/ The check always fails and therefore profile information is incorrect. Don't use it.\n+        assert(old_control == slow_ctl, \"type check should have been removed\");\n+        set_control(slow_ctl);\n+      } else {\n+        { PreserveJVMState pjvms(this);\n+          set_control(slow_ctl);\n+          uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+        }\n+        replace_in_map(ary, better_ary);\n+        ary = better_ary;\n+        arytype  = _gvn.type(ary)->is_aryptr();\n+        elemtype = arytype->elem();\n+      }\n+    }\n+  } else if (UseTypeSpeculation && UseArrayLoadStoreProfile) {\n+    \/\/ No need to speculate: feed profile data at this bci for the\n+    \/\/ array to type speculation\n+    ciKlass* array_type = NULL;\n+    ciKlass* element_type = NULL;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool flat_array = true;\n+    bool null_free_array = true;\n+    method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+    if (array_type != NULL) {\n+      ary = record_profile_for_speculation(ary, array_type, ProfileMaybeNull);\n+    }\n+  }\n+\n+  \/\/ We have no exact array type from profile data. Check profile data\n+  \/\/ for a non null-free or non flat array. Non null-free implies non\n+  \/\/ flat so check this one first. Speculating on a non null-free\n+  \/\/ array doesn't help aaload but could be profitable for a\n+  \/\/ subsequent aastore.\n+  if (!arytype->is_null_free() && !arytype->is_not_null_free()) {\n+    bool null_free_array = true;\n+    Deoptimization::DeoptReason reason = Deoptimization::Reason_none;\n+    if (arytype->speculative() != NULL &&\n+        arytype->speculative()->is_aryptr()->is_not_null_free() &&\n+        !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+      null_free_array = false;\n+      reason = Deoptimization::Reason_speculate_class_check;\n+    } else if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {\n+      ciKlass* array_type = NULL;\n+      ciKlass* element_type = NULL;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      reason = Deoptimization::Reason_class_check;\n+    }\n+    if (!null_free_array) {\n+      { \/\/ Deoptimize if null-free array\n+        BuildCutout unless(this, null_free_array_test(load_object_klass(ary), \/* null_free = *\/ false), PROB_MAX);\n+        uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+      }\n+      assert(!stopped(), \"null-free array should have been caught earlier\");\n+      Node* better_ary = _gvn.transform(new CheckCastPPNode(control(), ary, arytype->cast_to_not_null_free()));\n+      replace_in_map(ary, better_ary);\n+      ary = better_ary;\n+      arytype = _gvn.type(ary)->is_aryptr();\n+    }\n+  }\n+\n+  if (!arytype->is_flat() && !arytype->is_not_flat()) {\n+    bool flat_array = true;\n+    Deoptimization::DeoptReason reason = Deoptimization::Reason_none;\n+    if (arytype->speculative() != NULL &&\n+        arytype->speculative()->is_aryptr()->is_not_flat() &&\n+        !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+      flat_array = false;\n+      reason = Deoptimization::Reason_speculate_class_check;\n+    } else if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(reason)) {\n+      ciKlass* array_type = NULL;\n+      ciKlass* element_type = NULL;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      reason = Deoptimization::Reason_class_check;\n+    }\n+    if (!flat_array) {\n+      { \/\/ Deoptimize if flat array\n+        BuildCutout unless(this, flat_array_test(ary, \/* flat = *\/ false), PROB_MAX);\n+        uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+      }\n+      assert(!stopped(), \"flat array should have been caught earlier\");\n+      Node* better_ary = _gvn.transform(new CheckCastPPNode(control(), ary, arytype->cast_to_not_flat()));\n+      replace_in_map(ary, better_ary);\n+      ary = better_ary;\n+      arytype = _gvn.type(ary)->is_aryptr();\n+    }\n+  }\n+\n@@ -1472,1 +1879,1 @@\n-      adjust_map_after_if(btest, c, prob, branch_block, next_block);\n+      adjust_map_after_if(btest, c, prob, branch_block);\n@@ -1490,2 +1897,1 @@\n-    adjust_map_after_if(BoolTest(btest).negate(), c, 1.0-prob,\n-                        next_block, branch_block);\n+    adjust_map_after_if(BoolTest(btest).negate(), c, 1.0-prob, next_block);\n@@ -1496,1 +1902,1 @@\n-void Parse::do_if(BoolTest::mask btest, Node* c) {\n+void Parse::do_if(BoolTest::mask btest, Node* c, bool new_path, Node** ctrl_taken) {\n@@ -1580,2 +1986,2 @@\n-      if (C->eliminate_boxing()) {\n-        \/\/ Mark the successor block as parsed\n+      if (C->eliminate_boxing() && !new_path) {\n+        \/\/ Mark the successor block as parsed (if we haven't created a new path)\n@@ -1585,1 +1991,1 @@\n-      adjust_map_after_if(taken_btest, c, prob, branch_block, next_block);\n+      adjust_map_after_if(taken_btest, c, prob, branch_block);\n@@ -1587,1 +1993,9 @@\n-        merge(target_bci);\n+        if (new_path) {\n+          \/\/ Merge by using a new path\n+          merge_new_path(target_bci);\n+        } else if (ctrl_taken != NULL) {\n+          \/\/ Don't merge but save taken branch to be wired by caller\n+          *ctrl_taken = control();\n+        } else {\n+          merge(target_bci);\n+        }\n@@ -1596,1 +2010,1 @@\n-  if (stopped()) {\n+  if (stopped() && ctrl_taken == NULL) {\n@@ -1598,1 +2012,1 @@\n-      \/\/ Mark the successor block as parsed\n+      \/\/ Mark the successor block as parsed (if caller does not re-wire control flow)\n@@ -1602,2 +2016,378 @@\n-    adjust_map_after_if(untaken_btest, c, untaken_prob,\n-                        next_block, branch_block);\n+    adjust_map_after_if(untaken_btest, c, untaken_prob, next_block);\n+  }\n+}\n+\n+\n+static ProfilePtrKind speculative_ptr_kind(const TypeOopPtr* t) {\n+  if (t->speculative() == NULL) {\n+    return ProfileUnknownNull;\n+  }\n+  if (t->speculative_always_null()) {\n+    return ProfileAlwaysNull;\n+  }\n+  if (t->speculative_maybe_null()) {\n+    return ProfileMaybeNull;\n+  }\n+  return ProfileNeverNull;\n+}\n+\n+void Parse::acmp_always_null_input(Node* input, const TypeOopPtr* tinput, BoolTest::mask btest, Node* eq_region) {\n+  inc_sp(2);\n+  Node* cast = null_check_common(input, T_OBJECT, true, NULL,\n+                                 !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check) &&\n+                                 speculative_ptr_kind(tinput) == ProfileAlwaysNull);\n+  dec_sp(2);\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      replace_in_map(input, cast);\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    replace_in_map(input, cast);\n+  }\n+}\n+\n+Node* Parse::acmp_null_check(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, Node*& null_ctl) {\n+  inc_sp(2);\n+  null_ctl = top();\n+  Node* cast = null_check_oop(input, &null_ctl,\n+                              input_ptr == ProfileNeverNull || (input_ptr == ProfileUnknownNull && !too_many_traps_or_recompiles(Deoptimization::Reason_null_check)),\n+                              false,\n+                              speculative_ptr_kind(tinput) == ProfileNeverNull &&\n+                              !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check));\n+  dec_sp(2);\n+  assert(!stopped(), \"null input should have been caught earlier\");\n+  return cast;\n+}\n+\n+void Parse::acmp_known_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, ciKlass* input_type, BoolTest::mask btest, Node* eq_region) {\n+  Node* ne_region = new RegionNode(1);\n+  Node* null_ctl;\n+  Node* cast = acmp_null_check(input, tinput, input_ptr, null_ctl);\n+  ne_region->add_req(null_ctl);\n+\n+  Node* slow_ctl = type_check_receiver(cast, input_type, 1.0, &cast);\n+  {\n+    PreserveJVMState pjvms(this);\n+    inc_sp(2);\n+    set_control(slow_ctl);\n+    Deoptimization::DeoptReason reason;\n+    if (tinput->speculative_type() != NULL && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+      reason = Deoptimization::Reason_speculate_class_check;\n+    } else {\n+      reason = Deoptimization::Reason_class_check;\n+    }\n+    uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+  }\n+  ne_region->add_req(control());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      if (null_ctl == top()) {\n+        replace_in_map(input, cast);\n+      }\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    if (null_ctl == top()) {\n+      replace_in_map(input, cast);\n+    }\n+    set_control(_gvn.transform(ne_region));\n+  }\n+}\n+\n+void Parse::acmp_unknown_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, BoolTest::mask btest, Node* eq_region) {\n+  Node* ne_region = new RegionNode(1);\n+  Node* null_ctl;\n+  Node* cast = acmp_null_check(input, tinput, input_ptr, null_ctl);\n+  ne_region->add_req(null_ctl);\n+\n+  {\n+    BuildCutout unless(this, inline_type_test(cast, \/* is_inline = *\/ false), PROB_MAX);\n+    inc_sp(2);\n+    uncommon_trap_exact(Deoptimization::Reason_class_check, Deoptimization::Action_maybe_recompile);\n+  }\n+\n+  ne_region->add_req(control());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      if (null_ctl == top()) {\n+        replace_in_map(input, cast);\n+      }\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    if (null_ctl == top()) {\n+      replace_in_map(input, cast);\n+    }\n+    set_control(_gvn.transform(ne_region));\n+  }\n+}\n+\n+void Parse::do_acmp(BoolTest::mask btest, Node* left, Node* right) {\n+  ciKlass* left_type = NULL;\n+  ciKlass* right_type = NULL;\n+  ProfilePtrKind left_ptr = ProfileUnknownNull;\n+  ProfilePtrKind right_ptr = ProfileUnknownNull;\n+  bool left_inline_type = true;\n+  bool right_inline_type = true;\n+\n+  \/\/ Leverage profiling at acmp\n+  if (UseACmpProfile) {\n+    method()->acmp_profiled_type(bci(), left_type, right_type, left_ptr, right_ptr, left_inline_type, right_inline_type);\n+    if (too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {\n+      left_type = NULL;\n+      right_type = NULL;\n+      left_inline_type = true;\n+      right_inline_type = true;\n+    }\n+    if (too_many_traps_or_recompiles(Deoptimization::Reason_null_check)) {\n+      left_ptr = ProfileUnknownNull;\n+      right_ptr = ProfileUnknownNull;\n+    }\n+  }\n+\n+  if (UseTypeSpeculation) {\n+    record_profile_for_speculation(left, left_type, left_ptr);\n+    record_profile_for_speculation(right, right_type, right_ptr);\n+  }\n+\n+  if (!EnableValhalla) {\n+    Node* cmp = CmpP(left, right);\n+    cmp = optimize_cmp_with_klass(cmp);\n+    do_if(btest, cmp);\n+    return;\n+  }\n+\n+  \/\/ Allocate inline type operands and re-execute on deoptimization\n+  if (left->is_InlineType()) {\n+    PreserveReexecuteState preexecs(this);\n+    inc_sp(2);\n+    jvms()->set_should_reexecute(true);\n+    left = left->as_InlineType()->buffer(this)->get_oop();\n+  }\n+  if (right->is_InlineType()) {\n+    PreserveReexecuteState preexecs(this);\n+    inc_sp(2);\n+    jvms()->set_should_reexecute(true);\n+    right = right->as_InlineType()->buffer(this)->get_oop();\n+  }\n+\n+  \/\/ First, do a normal pointer comparison\n+  const TypeOopPtr* tleft = _gvn.type(left)->isa_oopptr();\n+  const TypeOopPtr* tright = _gvn.type(right)->isa_oopptr();\n+  Node* cmp = CmpP(left, right);\n+  cmp = optimize_cmp_with_klass(cmp);\n+  if (tleft == NULL || !tleft->can_be_inline_type() ||\n+      tright == NULL || !tright->can_be_inline_type()) {\n+    \/\/ This is sufficient, if one of the operands can't be an inline type\n+    do_if(btest, cmp);\n+    return;\n+  }\n+  Node* eq_region = NULL;\n+  if (btest == BoolTest::eq) {\n+    do_if(btest, cmp, true);\n+    if (stopped()) {\n+      return;\n+    }\n+  } else {\n+    assert(btest == BoolTest::ne, \"only eq or ne\");\n+    Node* is_not_equal = NULL;\n+    eq_region = new RegionNode(3);\n+    {\n+      PreserveJVMState pjvms(this);\n+      do_if(btest, cmp, false, &is_not_equal);\n+      if (!stopped()) {\n+        eq_region->init_req(1, control());\n+      }\n+    }\n+    if (is_not_equal == NULL || is_not_equal->is_top()) {\n+      record_for_igvn(eq_region);\n+      set_control(_gvn.transform(eq_region));\n+      return;\n+    }\n+    set_control(is_not_equal);\n+  }\n+\n+  \/\/ Prefer speculative types if available\n+  if (!too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+    if (tleft->speculative_type() != NULL) {\n+      left_type = tleft->speculative_type();\n+    }\n+    if (tright->speculative_type() != NULL) {\n+      right_type = tright->speculative_type();\n+    }\n+  }\n+\n+  if (speculative_ptr_kind(tleft) != ProfileMaybeNull && speculative_ptr_kind(tleft) != ProfileUnknownNull) {\n+    ProfilePtrKind speculative_left_ptr = speculative_ptr_kind(tleft);\n+    if (speculative_left_ptr == ProfileAlwaysNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_assert)) {\n+      left_ptr = speculative_left_ptr;\n+    } else if (speculative_left_ptr == ProfileNeverNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check)) {\n+      left_ptr = speculative_left_ptr;\n+    }\n+  }\n+  if (speculative_ptr_kind(tright) != ProfileMaybeNull && speculative_ptr_kind(tright) != ProfileUnknownNull) {\n+    ProfilePtrKind speculative_right_ptr = speculative_ptr_kind(tright);\n+    if (speculative_right_ptr == ProfileAlwaysNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_assert)) {\n+      right_ptr = speculative_right_ptr;\n+    } else if (speculative_right_ptr == ProfileNeverNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check)) {\n+      right_ptr = speculative_right_ptr;\n+    }\n+  }\n+\n+  if (left_ptr == ProfileAlwaysNull) {\n+    \/\/ Comparison with null. Assert the input is indeed null and we're done.\n+    acmp_always_null_input(left, tleft, btest, eq_region);\n+    return;\n+  }\n+  if (right_ptr == ProfileAlwaysNull) {\n+    \/\/ Comparison with null. Assert the input is indeed null and we're done.\n+    acmp_always_null_input(right, tright, btest, eq_region);\n+    return;\n+  }\n+  if (left_type != NULL && !left_type->is_inlinetype()) {\n+    \/\/ Comparison with an object of known type\n+    acmp_known_non_inline_type_input(left, tleft, left_ptr, left_type, btest, eq_region);\n+    return;\n+  }\n+  if (right_type != NULL && !right_type->is_inlinetype()) {\n+    \/\/ Comparison with an object of known type\n+    acmp_known_non_inline_type_input(right, tright, right_ptr, right_type, btest, eq_region);\n+    return;\n+  }\n+  if (!left_inline_type) {\n+    \/\/ Comparison with an object known not to be an inline type\n+    acmp_unknown_non_inline_type_input(left, tleft, left_ptr, btest, eq_region);\n+    return;\n+  }\n+  if (!right_inline_type) {\n+    \/\/ Comparison with an object known not to be an inline type\n+    acmp_unknown_non_inline_type_input(right, tright, right_ptr, btest, eq_region);\n+    return;\n+  }\n+\n+  \/\/ Pointers are not equal, check if first operand is non-null\n+  Node* ne_region = new RegionNode(6);\n+  Node* null_ctl;\n+  Node* not_null_right = acmp_null_check(right, tright, right_ptr, null_ctl);\n+  ne_region->init_req(1, null_ctl);\n+\n+  \/\/ First operand is non-null, check if it is an inline type\n+  Node* is_value = inline_type_test(not_null_right);\n+  IfNode* is_value_iff = create_and_map_if(control(), is_value, PROB_FAIR, COUNT_UNKNOWN);\n+  Node* not_value = _gvn.transform(new IfFalseNode(is_value_iff));\n+  ne_region->init_req(2, not_value);\n+  set_control(_gvn.transform(new IfTrueNode(is_value_iff)));\n+\n+  \/\/ The first operand is an inline type, check if the second operand is non-null\n+  Node* not_null_left = acmp_null_check(left, tleft, left_ptr, null_ctl);\n+  ne_region->init_req(3, null_ctl);\n+\n+  \/\/ Check if both operands are of the same class.\n+  Node* kls_left = load_object_klass(not_null_left);\n+  Node* kls_right = load_object_klass(not_null_right);\n+  Node* kls_cmp = CmpP(kls_left, kls_right);\n+  Node* kls_bol = _gvn.transform(new BoolNode(kls_cmp, BoolTest::ne));\n+  IfNode* kls_iff = create_and_map_if(control(), kls_bol, PROB_FAIR, COUNT_UNKNOWN);\n+  Node* kls_ne = _gvn.transform(new IfTrueNode(kls_iff));\n+  set_control(_gvn.transform(new IfFalseNode(kls_iff)));\n+  ne_region->init_req(4, kls_ne);\n+\n+  if (stopped()) {\n+    record_for_igvn(ne_region);\n+    set_control(_gvn.transform(ne_region));\n+    if (btest == BoolTest::ne) {\n+      {\n+        PreserveJVMState pjvms(this);\n+        int target_bci = iter().get_dest();\n+        merge(target_bci);\n+      }\n+      record_for_igvn(eq_region);\n+      set_control(_gvn.transform(eq_region));\n+    }\n+    return;\n+  }\n+\n+  \/\/ Both operands are values types of the same class, we need to perform a\n+  \/\/ substitutability test. Delegate to ValueBootstrapMethods::isSubstitutable().\n+  Node* ne_io_phi = PhiNode::make(ne_region, i_o());\n+  Node* mem = reset_memory();\n+  Node* ne_mem_phi = PhiNode::make(ne_region, mem);\n+\n+  Node* eq_io_phi = NULL;\n+  Node* eq_mem_phi = NULL;\n+  if (eq_region != NULL) {\n+    eq_io_phi = PhiNode::make(eq_region, i_o());\n+    eq_mem_phi = PhiNode::make(eq_region, mem);\n+  }\n+\n+  set_all_memory(mem);\n+\n+  kill_dead_locals();\n+  ciMethod* subst_method = ciEnv::current()->ValueBootstrapMethods_klass()->find_method(ciSymbols::isSubstitutable_name(), ciSymbols::object_object_boolean_signature());\n+  CallStaticJavaNode *call = new CallStaticJavaNode(C, TypeFunc::make(subst_method), SharedRuntime::get_resolve_static_call_stub(), subst_method);\n+  call->set_override_symbolic_info(true);\n+  call->init_req(TypeFunc::Parms, not_null_left);\n+  call->init_req(TypeFunc::Parms+1, not_null_right);\n+  inc_sp(2);\n+  set_edges_for_java_call(call, false, false);\n+  Node* ret = set_results_for_java_call(call, false, true);\n+  dec_sp(2);\n+\n+  \/\/ Test the return value of ValueBootstrapMethods::isSubstitutable()\n+  Node* subst_cmp = _gvn.transform(new CmpINode(ret, intcon(1)));\n+  Node* ctl = C->top();\n+  if (btest == BoolTest::eq) {\n+    PreserveJVMState pjvms(this);\n+    do_if(btest, subst_cmp);\n+    if (!stopped()) {\n+      ctl = control();\n+    }\n+  } else {\n+    assert(btest == BoolTest::ne, \"only eq or ne\");\n+    PreserveJVMState pjvms(this);\n+    do_if(btest, subst_cmp, false, &ctl);\n+    if (!stopped()) {\n+      eq_region->init_req(2, control());\n+      eq_io_phi->init_req(2, i_o());\n+      eq_mem_phi->init_req(2, reset_memory());\n+    }\n+  }\n+  ne_region->init_req(5, ctl);\n+  ne_io_phi->init_req(5, i_o());\n+  ne_mem_phi->init_req(5, reset_memory());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  set_i_o(_gvn.transform(ne_io_phi));\n+  set_all_memory(_gvn.transform(ne_mem_phi));\n+\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+    set_i_o(_gvn.transform(eq_io_phi));\n+    set_all_memory(_gvn.transform(eq_mem_phi));\n@@ -1633,2 +2423,1 @@\n-void Parse::adjust_map_after_if(BoolTest::mask btest, Node* c, float prob,\n-                                Block* path, Block* other_path) {\n+void Parse::adjust_map_after_if(BoolTest::mask btest, Node* c, float prob, Block* path) {\n@@ -1844,0 +2633,4 @@\n+        if (obj->is_InlineType()) {\n+          assert(obj->as_InlineType()->is_allocated(&_gvn), \"must be allocated\");\n+          obj = obj->as_InlineType()->get_oop();\n+        }\n@@ -2688,14 +3481,19 @@\n-    if (!_gvn.type(b)->speculative_maybe_null() &&\n-        !too_many_traps(Deoptimization::Reason_speculate_null_check)) {\n-      inc_sp(1);\n-      Node* null_ctl = top();\n-      b = null_check_oop(b, &null_ctl, true, true, true);\n-      assert(null_ctl->is_top(), \"no null control here\");\n-      dec_sp(1);\n-    } else if (_gvn.type(b)->speculative_always_null() &&\n-               !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {\n-      inc_sp(1);\n-      b = null_assert(b);\n-      dec_sp(1);\n-    }\n-    c = _gvn.transform( new CmpPNode(b, a) );\n+    if (b->is_InlineType()) {\n+      \/\/ Return constant false because 'b' is always non-null\n+      c = _gvn.makecon(TypeInt::CC_GT);\n+    } else {\n+      if (!_gvn.type(b)->speculative_maybe_null() &&\n+          !too_many_traps(Deoptimization::Reason_speculate_null_check)) {\n+        inc_sp(1);\n+        Node* null_ctl = top();\n+        b = null_check_oop(b, &null_ctl, true, true, true);\n+        assert(null_ctl->is_top(), \"no null control here\");\n+        dec_sp(1);\n+      } else if (_gvn.type(b)->speculative_always_null() &&\n+                 !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {\n+        inc_sp(1);\n+        b = null_assert(b);\n+        dec_sp(1);\n+      }\n+      c = _gvn.transform( new CmpPNode(b, a) );\n+    }\n@@ -2712,3 +3510,1 @@\n-    c = _gvn.transform( new CmpPNode(b, a) );\n-    c = optimize_cmp_with_klass(c);\n-    do_if(btest, c);\n+    do_acmp(btest, b, a);\n@@ -2769,1 +3565,1 @@\n-    do_anewarray();\n+    do_newarray();\n@@ -2780,0 +3576,6 @@\n+  case Bytecodes::_defaultvalue:\n+    do_defaultvalue();\n+    break;\n+  case Bytecodes::_withfield:\n+    do_withfield();\n+    break;\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":860,"deletions":58,"binary":false,"changes":918,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -30,0 +32,2 @@\n+#include \"opto\/castnode.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -68,1 +72,0 @@\n-\n@@ -76,0 +79,1 @@\n+    assert(!iter().is_inline_klass(), \"Inline type should be loaded\");\n@@ -140,2 +144,1 @@\n-void Parse::array_store_check() {\n-\n+Node* Parse::array_store_check(Node*& adr, const Type*& elemtype) {\n@@ -152,1 +155,1 @@\n-    return;\n+    return obj;\n@@ -156,4 +159,1 @@\n-  int klass_offset = oopDesc::klass_offset_in_bytes();\n-  Node* p = basic_plus_adr( ary, ary, klass_offset );\n-  \/\/ p's type is array-of-OOPS plus klass_offset\n-  Node* array_klass = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeInstPtr::KLASS));\n+  Node* array_klass = load_object_klass(ary);\n@@ -161,1 +161,1 @@\n-  const TypeKlassPtr *tak = _gvn.type(array_klass)->is_klassptr();\n+  const TypeKlassPtr* tak = _gvn.type(array_klass)->is_klassptr();\n@@ -168,6 +168,26 @@\n-  if (MonomorphicArrayCheck\n-      && !too_many_traps(Deoptimization::Reason_array_check)\n-      && !tak->klass_is_exact()\n-      && tak != TypeKlassPtr::OBJECT) {\n-      \/\/ Regarding the fourth condition in the if-statement from above:\n-      \/\/\n+  if (MonomorphicArrayCheck && !tak->klass_is_exact()) {\n+    \/\/ Make a constant out of the inexact array klass\n+    const TypeKlassPtr* extak = NULL;\n+    const TypeOopPtr* ary_t = _gvn.type(ary)->is_oopptr();\n+    ciKlass* ary_spec = ary_t->speculative_type();\n+    Deoptimization::DeoptReason reason = Deoptimization::Reason_none;\n+    \/\/ Try to cast the array to an exact type from profile data. First\n+    \/\/ check the speculative type.\n+    if (ary_spec != NULL && !too_many_traps(Deoptimization::Reason_speculate_class_check)) {\n+      extak = TypeKlassPtr::make(ary_spec);\n+      reason = Deoptimization::Reason_speculate_class_check;\n+    } else if (UseArrayLoadStoreProfile) {\n+      \/\/ No speculative type: check profile data at this bci.\n+      reason = Deoptimization::Reason_class_check;\n+      if (!too_many_traps(reason)) {\n+        ciKlass* array_type = NULL;\n+        ciKlass* element_type = NULL;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+        if (array_type != NULL) {\n+          extak = TypeKlassPtr::make(array_type);\n+        }\n+      }\n+    } else if (!too_many_traps(Deoptimization::Reason_array_check) && tak != TypeKlassPtr::OBJECT) {\n@@ -192,14 +212,2 @@\n-\n-    always_see_exact_class = true;\n-    \/\/ (If no MDO at all, hope for the best, until a trap actually occurs.)\n-\n-    \/\/ Make a constant out of the inexact array klass\n-    const TypeKlassPtr *extak = tak->cast_to_exactness(true)->is_klassptr();\n-    Node* con = makecon(extak);\n-    Node* cmp = _gvn.transform(new CmpPNode( array_klass, con ));\n-    Node* bol = _gvn.transform(new BoolNode( cmp, BoolTest::eq ));\n-    Node* ctrl= control();\n-    { BuildCutout unless(this, bol, PROB_MAX);\n-      uncommon_trap(Deoptimization::Reason_array_check,\n-                    Deoptimization::Action_maybe_recompile,\n-                    tak->klass());\n+      extak = tak->cast_to_exactness(true)->is_klassptr();\n+      reason = Deoptimization::Reason_array_check;\n@@ -207,9 +215,29 @@\n-    if (stopped()) {          \/\/ MUST uncommon-trap?\n-      set_control(ctrl);      \/\/ Then Don't Do It, just fall into the normal checking\n-    } else {                  \/\/ Cast array klass to exactness:\n-      \/\/ Use the exact constant value we know it is.\n-      replace_in_map(array_klass,con);\n-      CompileLog* log = C->log();\n-      if (log != NULL) {\n-        log->elem(\"cast_up reason='monomorphic_array' from='%d' to='(exact)'\",\n-                  log->identify(tak->klass()));\n+    if (extak != NULL) {\n+      Node* con = makecon(extak);\n+      Node* cmp = _gvn.transform(new CmpPNode(array_klass, con));\n+      Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+      \/\/ Only do it if the check does not always pass\/fail\n+      if (!bol->is_Con()) {\n+        always_see_exact_class = true;\n+        { BuildCutout unless(this, bol, PROB_MAX);\n+          uncommon_trap(reason,\n+                        Deoptimization::Action_maybe_recompile,\n+                        tak->klass());\n+        }\n+        \/\/ Cast array klass to exactness\n+        replace_in_map(array_klass, con);\n+        array_klass = con;\n+        Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, extak->as_instance_type()));\n+        replace_in_map(ary, cast);\n+        ary = cast;\n+\n+        \/\/ Recompute element type and address\n+        const TypeAryPtr* arytype = _gvn.type(ary)->is_aryptr();\n+        elemtype = arytype->elem();\n+        adr = array_element_address(ary, idx, T_OBJECT, arytype->size(), control());\n+\n+        CompileLog* log = C->log();\n+        if (log != NULL) {\n+          log->elem(\"cast_up reason='monomorphic_array' from='%d' to='(exact)'\",\n+                    log->identify(tak->klass()));\n+        }\n@@ -217,1 +245,0 @@\n-      array_klass = con;      \/\/ Use cast value moving forward\n@@ -224,1 +251,2 @@\n-  int element_klass_offset = in_bytes(ObjArrayKlass::element_klass_offset());\n+  int element_klass_offset = in_bytes(ArrayKlass::element_klass_offset());\n+\n@@ -232,0 +260,6 @@\n+  \/\/ Handle inline type arrays\n+  if (elemtype->isa_inlinetype() != NULL || (elemtype->is_inlinetypeptr() && !elemtype->maybe_null())) {\n+    \/\/ We statically know that this is an inline type array, use precise klass ptr\n+    a_e_klass = makecon(TypeKlassPtr::make(elemtype->inline_klass()));\n+  }\n+\n@@ -233,2 +267,1 @@\n-  \/\/ Result is ignored, we just need the CFG effects.\n-  gen_checkcast(obj, a_e_klass);\n+  return gen_checkcast(obj, a_e_klass);\n@@ -245,0 +278,1 @@\n+  assert(!klass->is_inlinetype(), \"unexpected inline type\");\n@@ -281,0 +315,64 @@\n+\/\/------------------------------do_defaultvalue---------------------------------\n+void Parse::do_defaultvalue() {\n+  bool will_link;\n+  ciInlineKlass* vk = iter().get_klass(will_link)->as_inline_klass();\n+  assert(will_link && !iter().is_unresolved_klass(), \"defaultvalue: typeflow responsibility\");\n+\n+  if (C->needs_clinit_barrier(vk, method())) {\n+    clinit_barrier(vk, method());\n+    if (stopped())  return;\n+  }\n+\n+  InlineTypeNode* vt = InlineTypeNode::make_default(_gvn, vk);\n+  if (vk->is_scalarizable()) {\n+    push(vt);\n+  } else {\n+    push(vt->get_oop());\n+  }\n+}\n+\n+\/\/------------------------------do_withfield------------------------------------\n+void Parse::do_withfield() {\n+  bool will_link;\n+  ciField* field = iter().get_field(will_link);\n+  assert(will_link, \"withfield: typeflow responsibility\");\n+  Node* val = pop_node(field->layout_type());\n+  ciInlineKlass* holder_klass = field->holder()->as_inline_klass();\n+  Node* holder = pop();\n+  int nargs = 1 + field->type()->size();\n+\n+  if (!holder->is_InlineType()) {\n+    \/\/ Scalarize inline type holder\n+    assert(!gvn().type(holder)->maybe_null(), \"Inline types are null-free\");\n+    holder = InlineTypeNode::make_from_oop(this, holder, holder_klass);\n+  }\n+  if (!val->is_InlineType() && field->type()->is_inlinetype()) {\n+    \/\/ Scalarize inline type field value\n+    assert(!gvn().type(val)->maybe_null(), \"Inline types are null-free\");\n+    val = InlineTypeNode::make_from_oop(this, val, gvn().type(val)->inline_klass());\n+  } else if (val->is_InlineType() && !field->type()->is_inlinetype()) {\n+    \/\/ Field value needs to be allocated because it can be merged with an oop.\n+    \/\/ Re-execute withfield if buffering triggers deoptimization.\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    inc_sp(nargs);\n+    val = val->as_InlineType()->buffer(this);\n+  }\n+\n+  \/\/ Clone the inline type node and set the new field value\n+  InlineTypeNode* new_vt = holder->clone()->as_InlineType();\n+  new_vt->set_oop(_gvn.zerocon(T_INLINE_TYPE));\n+  gvn().set_type(new_vt, new_vt->bottom_type());\n+  new_vt->set_field_value_by_offset(field->offset(), val);\n+  Node* res = new_vt;\n+\n+  if (!holder_klass->is_scalarizable()) {\n+    \/\/ Re-execute withfield if buffering triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    inc_sp(nargs);\n+    res = new_vt->buffer(this)->get_oop();\n+  }\n+  push(_gvn.transform(res));\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/parseHelper.cpp","additions":140,"deletions":42,"binary":false,"changes":182,"status":"modified"},{"patch":"@@ -1204,6 +1204,0 @@\n-  if (_delay_transform) {\n-    \/\/ Register the node but don't optimize for now\n-    register_new_node_with_optimizer(n);\n-    return n;\n-  }\n-\n@@ -1216,0 +1210,6 @@\n+  if (_delay_transform) {\n+    \/\/ Add the node to the worklist but don't optimize for now\n+    _worklist.push(n);\n+    return n;\n+  }\n+\n@@ -1476,0 +1476,13 @@\n+void PhaseIterGVN::replace_in_uses(Node* n, Node* m) {\n+  assert(n != NULL, \"sanity\");\n+  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+    Node* u = n->fast_out(i);\n+    if (u != n) {\n+      rehash_node_delayed(u);\n+      int nb = u->replace_edge(n, m);\n+      --i, imax -= nb;\n+    }\n+  }\n+  assert(n->outcnt() == 0, \"all uses must be deleted\");\n+}\n+\n@@ -1576,0 +1589,9 @@\n+    \/\/ Inline type nodes can have other inline types as users. If an input gets\n+    \/\/ updated, make sure that inline type users get a chance for optimization.\n+    if (use->is_InlineTypeBase()) {\n+      for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+        Node* u = use->fast_out(i2);\n+        if (u->is_InlineTypeBase())\n+          _worklist.push(u);\n+      }\n+    }\n@@ -1621,0 +1643,8 @@\n+    if (use_op == Op_CastP2X) {\n+      for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+        Node* u = use->fast_out(i2);\n+        if (u->Opcode() == Op_AndX) {\n+          _worklist.push(u);\n+        }\n+      }\n+    }\n@@ -1645,0 +1675,11 @@\n+\n+    \/\/ Give CallStaticJavaNode::remove_useless_allocation a chance to run\n+    if (use->is_Region()) {\n+      Node* c = use;\n+      do {\n+        c = c->unique_ctrl_out();\n+      } while (c != NULL && c->is_Region());\n+      if (c != NULL && c->is_CallStaticJava() && c->as_CallStaticJava()->uncommon_trap_request() != 0) {\n+        _worklist.push(c);\n+      }\n+    }\n@@ -1815,0 +1856,8 @@\n+        if (m_op == Op_CastP2X) {\n+          for (DUIterator_Fast i2max, i2 = m->fast_outs(i2max); i2 < i2max; i2++) {\n+            Node* u = m->fast_out(i2);\n+            if (u->Opcode() == Op_AndX) {\n+              worklist.push(u);\n+            }\n+          }\n+        }\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":55,"deletions":6,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -49,0 +49,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -198,1 +200,1 @@\n-JRT_BLOCK_ENTRY(void, OptoRuntime::new_instance_C(Klass* klass, JavaThread* thread))\n+JRT_BLOCK_ENTRY(void, OptoRuntime::new_instance_C(Klass* klass, bool is_larval, JavaThread* thread))\n@@ -218,1 +220,5 @@\n-    oop result = InstanceKlass::cast(klass)->allocate_instance(THREAD);\n+    instanceOop result = InstanceKlass::cast(klass)->allocate_instance(THREAD);\n+    if (is_larval) {\n+      \/\/ Check if this is a larval buffer allocation\n+      result->set_mark(result->mark().enter_larval_state());\n+    }\n@@ -246,1 +252,4 @@\n-  if (array_type->is_typeArray_klass()) {\n+  if (array_type->is_flatArray_klass()) {\n+    Klass* elem_type = FlatArrayKlass::cast(array_type)->element_klass();\n+    result = oopFactory::new_flatArray(elem_type, len, THREAD);\n+  } else if (array_type->is_typeArray_klass()) {\n@@ -252,5 +261,1 @@\n-    \/\/ Although the oopFactory likes to work with the elem_type,\n-    \/\/ the compiler prefers the array_type, since it must already have\n-    \/\/ that latter value in hand for the fast path.\n-    Klass* elem_type = ObjArrayKlass::cast(array_type)->element_klass();\n-    result = oopFactory::new_objArray(elem_type, len, THREAD);\n+    result = ObjArrayKlass::cast(array_type)->allocate(len, THREAD);\n@@ -451,1 +456,1 @@\n-  const Type **fields = TypeTuple::fields(1);\n+  const Type **fields = TypeTuple::fields(2);\n@@ -453,1 +458,2 @@\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+1, fields);\n+  fields[TypeFunc::Parms+1] = TypeInt::BOOL;        \/\/ is_larval\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n@@ -571,1 +577,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1493,1 +1499,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1511,1 +1517,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1527,1 +1533,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1659,0 +1665,106 @@\n+\n+const TypeFunc *OptoRuntime::store_inline_type_fields_Type() {\n+  \/\/ create input type (domain)\n+  uint total = SharedRuntime::java_return_convention_max_int + SharedRuntime::java_return_convention_max_float*2;\n+  const Type **fields = TypeTuple::fields(total);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypePtr::BOTTOM;\n+  uint i = 1;\n+  for (; i < SharedRuntime::java_return_convention_max_int; i++) {\n+    fields[TypeFunc::Parms+i] = TypeInt::INT;\n+  }\n+  for (; i < total; i+=2) {\n+    fields[TypeFunc::Parms+i] = Type::DOUBLE;\n+    fields[TypeFunc::Parms+i+1] = Type::HALF;\n+  }\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + total, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1,fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+const TypeFunc *OptoRuntime::pack_inline_type_Type() {\n+  \/\/ create input type (domain)\n+  uint total = 1 + SharedRuntime::java_return_convention_max_int + SharedRuntime::java_return_convention_max_float*2;\n+  const Type **fields = TypeTuple::fields(total);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypeRawPtr::BOTTOM;\n+  fields[TypeFunc::Parms+1] = TypeRawPtr::BOTTOM;\n+  uint i = 2;\n+  for (; i < SharedRuntime::java_return_convention_max_int+1; i++) {\n+    fields[TypeFunc::Parms+i] = TypeInt::INT;\n+  }\n+  for (; i < total; i+=2) {\n+    fields[TypeFunc::Parms+i] = Type::DOUBLE;\n+    fields[TypeFunc::Parms+i+1] = Type::HALF;\n+  }\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + total, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1,fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+JRT_LEAF(void, OptoRuntime::load_unknown_inline(flatArrayOopDesc* array, int index, instanceOopDesc* buffer))\n+{\n+  array->value_copy_from_index(index, buffer);\n+}\n+JRT_END\n+\n+const TypeFunc* OptoRuntime::load_unknown_inline_type() {\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(3);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypeOopPtr::NOTNULL;\n+  fields[TypeFunc::Parms+1] = TypeInt::POS;\n+  fields[TypeFunc::Parms+2] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+3, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(0);\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms+0, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+JRT_LEAF(void, OptoRuntime::store_unknown_inline(instanceOopDesc* buffer, flatArrayOopDesc* array, int index))\n+{\n+  assert(buffer != NULL, \"can't store null into flat array\");\n+  array->value_copy_to_index(buffer, index);\n+}\n+JRT_END\n+\n+const TypeFunc* OptoRuntime::store_unknown_inline_type() {\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(3);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypeInstPtr::NOTNULL;\n+  fields[TypeFunc::Parms+1] = TypeOopPtr::NOTNULL;\n+  fields[TypeFunc::Parms+2] = TypeInt::POS;\n+\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+3, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(0);\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms+0, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":126,"deletions":14,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -55,0 +55,2 @@\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -420,0 +422,1 @@\n+  bool is_inlined = InstanceKlass::cast(k1)->field_is_inlined(slot);\n@@ -421,1 +424,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset);\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset, is_inlined);\n@@ -438,1 +441,1 @@\n-  if (m->is_initializer()) {\n+  if (m->is_object_constructor() || m->is_static_init_factory()) {\n@@ -496,1 +499,0 @@\n-\n@@ -801,1 +803,2 @@\n-    case T_OBJECT:      push_object(va_arg(_ap, jobject)); break;\n+    case T_OBJECT:\n+    case T_INLINE_TYPE: push_object(va_arg(_ap, jobject)); break;\n@@ -841,1 +844,2 @@\n-    case T_OBJECT:      push_object((_ap++)->l); break;\n+    case T_OBJECT:\n+    case T_INLINE_TYPE: push_object((_ap++)->l); break;\n@@ -977,5 +981,19 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherArray ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == NULL) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), NULL);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherArray ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  } else {\n+    JavaValue jvalue(T_INLINE_TYPE);\n+    JNI_ArgumentPusherArray ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, NULL, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -983,1 +1001,1 @@\n-JNI_END\n+  JNI_END\n@@ -995,5 +1013,19 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherVaArg ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == NULL) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), NULL);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  } else {\n+    JavaValue jvalue(T_INLINE_TYPE);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, NULL, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -1013,8 +1045,25 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  va_list args;\n-  va_start(args, methodID);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherVaArg ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n-  va_end(args);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == NULL) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), NULL);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    va_list args;\n+    va_start(args, methodID);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+    va_end(args);\n+  } else {\n+    va_list args;\n+    va_start(args, methodID);\n+    JavaValue jvalue(T_INLINE_TYPE);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, NULL, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    va_end(args);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -1771,1 +1820,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset());\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset(), fd.is_inlined());\n@@ -1781,0 +1830,1 @@\n+  oop res = NULL;\n@@ -1786,2 +1836,12 @@\n-  oop loaded_obj = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n-  jobject ret = JNIHandles::make_local(THREAD, loaded_obj);\n+  if (!jfieldIDWorkaround::is_inlined_jfieldID(fieldID)) {\n+    res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instance can have inlined fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);  \/\/ performance bottleneck\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineKlass* field_vklass = InlineKlass::cast(holder->get_inline_type_field_klass(fd.index()));\n+    res = field_vklass->read_inlined_field(o, ik->field_offset(fd.index()), CHECK_NULL);\n+  }\n+  jobject ret = JNIHandles::make_local(THREAD, res);\n@@ -1879,1 +1939,12 @@\n-  HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  if (!jfieldIDWorkaround::is_inlined_jfieldID(fieldID)) {\n+    HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instances can have inlined fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineKlass* vklass = InlineKlass::cast(holder->get_inline_type_field_klass(fd.index()));\n+    oop v = JNIHandles::resolve_non_null(value);\n+    vklass->write_inlined_field(o, offset, v, CHECK);\n+  }\n@@ -2296,4 +2367,13 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  if (a->is_within_bounds(index)) {\n-    ret = JNIHandles::make_local(THREAD, a->obj_at(index));\n-    return ret;\n+  oop res = NULL;\n+  arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+  if (arr->is_within_bounds(index)) {\n+    if (arr->is_flatArray()) {\n+      flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+      flatArrayHandle vah(thread, a);\n+      res = flatArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK_NULL);\n+      assert(res != NULL, \"Must be set in one of two paths above\");\n+    } else {\n+      assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+      objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+      res = a->obj_at(index);\n+    }\n@@ -2303,1 +2383,1 @@\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n+    ss.print(\"Index %d out of bounds for length %d\", index,arr->length());\n@@ -2306,0 +2386,2 @@\n+  ret = JNIHandles::make_local(THREAD, res);\n+  return ret;\n@@ -2315,24 +2397,51 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  oop v = JNIHandles::resolve(value);\n-  if (a->is_within_bounds(index)) {\n-    if (v == NULL || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n-      a->obj_at_put(index, v);\n-    } else {\n-      ResourceMark rm(THREAD);\n-      stringStream ss;\n-      Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n-      ss.print(\"type mismatch: can not store %s to %s[%d]\",\n-               v->klass()->external_name(),\n-               bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n-               index);\n-      for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n-        ss.print(\"[]\");\n-      }\n-      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n-    }\n-  } else {\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n-    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n-  }\n+   bool oob = false;\n+   int length = -1;\n+   oop res = NULL;\n+   arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+   if (arr->is_within_bounds(index)) {\n+     if (arr->is_flatArray()) {\n+       flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       FlatArrayKlass* vaklass = FlatArrayKlass::cast(a->klass());\n+       InlineKlass* element_vklass = vaklass->element_klass();\n+       if (v != NULL && v->is_a(element_vklass)) {\n+         a->value_copy_to_index(v, index);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *kl = FlatArrayKlass::cast(a->klass());\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             kl->external_name(),\n+             index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     } else {\n+       assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+       objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       if (v == NULL || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n+         a->obj_at_put(index, v);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n+                 index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     }\n+   } else {\n+     ResourceMark rm(THREAD);\n+     stringStream ss;\n+     ss.print(\"Index %d out of bounds for length %d\", index, arr->length());\n+     THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n+   }\n@@ -3112,0 +3221,261 @@\n+JNI_ENTRY(void*, jni_GetFlattenedArrayElements(JNIEnv* env, jarray array, jboolean* isCopy))\n+  if (isCopy != NULL) {\n+    *isCopy = JNI_FALSE;\n+  }\n+  arrayOop ar = arrayOop(JNIHandles::resolve_non_null(array));\n+  if (!ar->is_array()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!ar->is_flatArray()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass());\n+  if (vak->contains_oops()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Flattened array contains oops\");\n+  }\n+  oop a = lock_gc_or_pin_object(thread, array);\n+  flatArrayOop vap = flatArrayOop(a);\n+  void* ret = vap->value_at_addr(0, vak->layout_helper());\n+  return ret;\n+JNI_END\n+\n+JNI_ENTRY(void, jni_ReleaseFlattenedArrayElements(JNIEnv* env, jarray array, void* elem, jint mode))\n+  unlock_gc_or_unpin_object(thread, array);\n+JNI_END\n+\n+JNI_ENTRY(jsize, jni_GetFlattenedArrayElementSize(JNIEnv* env, jarray array)) {\n+  arrayOop a = arrayOop(JNIHandles::resolve_non_null(array));\n+  if (!a->is_array()) {\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!a->is_flatArray()) {\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(a->klass());\n+  jsize ret = vak->element_byte_size();\n+  return ret;\n+}\n+JNI_END\n+\n+JNI_ENTRY(jclass, jni_GetFlattenedArrayElementClass(JNIEnv* env, jarray array))\n+  arrayOop a = arrayOop(JNIHandles::resolve_non_null(array));\n+  if (!a->is_array()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!a->is_flatArray()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(a->klass());\n+  InlineKlass* vk = vak->element_klass();\n+  return (jclass) JNIHandles::make_local(vk->java_mirror());\n+JNI_END\n+\n+JNI_ENTRY(jsize, jni_GetFieldOffsetInFlattenedLayout(JNIEnv* env, jclass clazz, const char *name, const char *signature, jboolean* is_inlined))\n+  oop mirror = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(mirror);\n+  if (!k->is_inline_klass()) {\n+    ResourceMark rm;\n+        THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), err_msg(\"%s has not flattened layout\", k->external_name()));\n+  }\n+  InlineKlass* vk = InlineKlass::cast(k);\n+\n+  TempNewSymbol fieldname = SymbolTable::probe(name, (int)strlen(name));\n+  TempNewSymbol signame = SymbolTable::probe(signature, (int)strlen(signature));\n+  if (fieldname == NULL || signame == NULL) {\n+    ResourceMark rm;\n+    THROW_MSG_0(vmSymbols::java_lang_NoSuchFieldError(), err_msg(\"%s.%s %s\", vk->external_name(), name, signature));\n+  }\n+\n+  assert(vk->is_initialized(), \"If a flattened array has been created, the element klass must have been initialized\");\n+\n+  fieldDescriptor fd;\n+  if (!vk->is_instance_klass() ||\n+      !InstanceKlass::cast(vk)->find_field(fieldname, signame, false, &fd)) {\n+    ResourceMark rm;\n+    THROW_MSG_0(vmSymbols::java_lang_NoSuchFieldError(), err_msg(\"%s.%s %s\", vk->external_name(), name, signature));\n+  }\n+\n+  int offset = fd.offset() - vk->first_field_offset();\n+  if (is_inlined != NULL) {\n+    *is_inlined = fd.is_inlined();\n+  }\n+  return (jsize)offset;\n+JNI_END\n+\n+JNI_ENTRY(jobject, jni_CreateSubElementSelector(JNIEnv* env, jarray array))\n+  oop ar = JNIHandles::resolve_non_null(array);\n+  if (!ar->is_array()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!ar->is_flatArray()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  flatArrayHandle ar_h(THREAD, flatArrayOop(ar));\n+  Klass* ses_k = SystemDictionary::resolve_or_null(vmSymbols::jdk_internal_vm_jni_SubElementSelector(),\n+        Handle(THREAD, SystemDictionary::java_system_loader()), Handle(), CHECK_NULL);\n+  InstanceKlass* ses_ik = InstanceKlass::cast(ses_k);\n+  ses_ik->initialize(CHECK_NULL);\n+  Klass* elementKlass = ArrayKlass::cast(ar_h()->klass())->element_klass();\n+  oop ses = ses_ik->allocate_instance(CHECK_NULL);\n+  Handle ses_h(THREAD, ses);\n+  jdk_internal_vm_jni_SubElementSelector::setArrayElementType(ses_h(), elementKlass->java_mirror());\n+  jdk_internal_vm_jni_SubElementSelector::setSubElementType(ses_h(), elementKlass->java_mirror());\n+  jdk_internal_vm_jni_SubElementSelector::setOffset(ses_h(), 0);\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlined(ses_h(), true);   \/\/ by definition, top element of a flattened array is inlined\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlineType(ses_h(), true); \/\/ by definition, top element of a flattened array is an inline type\n+  return JNIHandles::make_local(ses_h());\n+JNI_END\n+\n+JNI_ENTRY(jobject, jni_GetSubElementSelector(JNIEnv* env, jobject selector, jfieldID fieldID))\n+  oop slct = JNIHandles::resolve_non_null(selector);\n+  if (slct->klass()->name() != vmSymbols::jdk_internal_vm_jni_SubElementSelector()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a SubElementSelector\");\n+  }\n+  jboolean is_inlined = jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct);\n+  if (!is_inlined) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"SubElement is not inlined\");\n+  }\n+  oop semirror = jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct);\n+  Klass* k = java_lang_Class::as_Klass(semirror);\n+  if (!k->is_inline_klass()) {\n+    ResourceMark rm;\n+        THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), err_msg(\"%s is not an inline type\", k->external_name()));\n+  }\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  assert(vk->is_initialized(), \"If a flattened array has been created, the element klass must have been initialized\");\n+  int field_offset = jfieldIDWorkaround::from_instance_jfieldID(vk, fieldID);\n+  fieldDescriptor fd;\n+  if (!vk->find_field_from_offset(field_offset, false, &fd)) {\n+    THROW_NULL(vmSymbols::java_lang_NoSuchFieldError());\n+  }\n+  Handle arrayElementMirror(THREAD, jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct));\n+  \/\/ offset of the SubElement is offset of the original SubElement plus the offset of the field inside the element\n+  int offset = fd.offset() - vk->first_field_offset() + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+  InstanceKlass* sesklass = InstanceKlass::cast(JNIHandles::resolve_non_null(selector)->klass());\n+  oop res = sesklass->allocate_instance(CHECK_NULL);\n+  Handle res_h(THREAD, res);\n+  jdk_internal_vm_jni_SubElementSelector::setArrayElementType(res_h(), arrayElementMirror());\n+  InstanceKlass* holder = fd.field_holder();\n+  BasicType bt = Signature::basic_type(fd.signature());\n+  if (is_java_primitive(bt)) {\n+    jdk_internal_vm_jni_SubElementSelector::setSubElementType(res_h(), java_lang_Class::primitive_mirror(bt));\n+  } else {\n+    Klass* fieldKlass = SystemDictionary::resolve_or_fail(fd.signature(), Handle(THREAD, holder->class_loader()),\n+        Handle(THREAD, holder->protection_domain()), true, CHECK_NULL);\n+    jdk_internal_vm_jni_SubElementSelector::setSubElementType(res_h(),fieldKlass->java_mirror());\n+  }\n+  jdk_internal_vm_jni_SubElementSelector::setOffset(res_h(), offset);\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlined(res_h(), fd.is_inlined());\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlineType(res_h(), fd.is_inline_type());\n+  return JNIHandles::make_local(res_h());\n+JNI_END\n+\n+JNI_ENTRY(jobject, jni_GetObjectSubElement(JNIEnv* env, jarray array, jobject selector, int index))\n+  flatArrayOop ar =  (flatArrayOop)JNIHandles::resolve_non_null(array);\n+  oop slct = JNIHandles::resolve_non_null(selector);\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass());\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\");\n+  }\n+  oop res = NULL;\n+  if (!jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct)) {\n+    int offset = (address)ar->base() - cast_from_oop<address>(ar) + index * vak->element_byte_size()\n+                      + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+    res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(ar, offset);\n+  } else {\n+    Handle slct_h(THREAD, slct);\n+    InlineKlass* fieldKlass = InlineKlass::cast(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)));\n+    res = fieldKlass->allocate_instance_buffer(CHECK_NULL);\n+    \/\/ The array might have been moved by the GC, refreshing the arrayOop\n+    ar =  (flatArrayOop)JNIHandles::resolve_non_null(array);\n+    address addr = (address)ar->value_at_addr(index, vak->layout_helper())\n+              + jdk_internal_vm_jni_SubElementSelector::getOffset(slct_h());\n+    fieldKlass->inline_copy_payload_to_new_oop(addr, res);\n+  }\n+  return JNIHandles::make_local(res);\n+JNI_END\n+\n+JNI_ENTRY(void, jni_SetObjectSubElement(JNIEnv* env, jarray array, jobject selector, int index, jobject value))\n+  flatArrayOop ar =  (flatArrayOop)JNIHandles::resolve_non_null(array);\n+  oop slct = JNIHandles::resolve_non_null(selector);\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass());\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\");\n+  }\n+  oop val = JNIHandles::resolve(value);\n+  if (val == NULL) {\n+    if (jdk_internal_vm_jni_SubElementSelector::getIsInlineType(slct)) {\n+      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), \"null cannot be stored in a flattened array\");\n+    }\n+  } else {\n+    if (!val->is_a(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)))) {\n+      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), \"type mismatch\");\n+    }\n+  }\n+  if (!jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct)) {\n+    int offset = (address)ar->base() - cast_from_oop<address>(ar) + index * vak->element_byte_size()\n+                  + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+    HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(ar, offset, JNIHandles::resolve(value));\n+  } else {\n+    InlineKlass* fieldKlass = InlineKlass::cast(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)));\n+    address addr = (address)ar->value_at_addr(index, vak->layout_helper())\n+                  + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+    fieldKlass->inline_copy_oop_to_payload(JNIHandles::resolve_non_null(value), addr);\n+  }\n+JNI_END\n+\n+#define DEFINE_GETSUBELEMENT(ElementType,Result,ElementBasicType) \\\n+\\\n+JNI_ENTRY(ElementType, \\\n+          jni_Get##Result##SubElement(JNIEnv *env, jarray array, jobject selector, int index)) \\\n+  flatArrayOop ar = (flatArrayOop)JNIHandles::resolve_non_null(array); \\\n+  oop slct = JNIHandles::resolve_non_null(selector); \\\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass()); \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) { \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\"); \\\n+  } \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct) != java_lang_Class::primitive_mirror(ElementBasicType)) { \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Wrong SubElement type\"); \\\n+  } \\\n+  address addr = (address)ar->value_at_addr(index, vak->layout_helper()) \\\n+               + jdk_internal_vm_jni_SubElementSelector::getOffset(slct); \\\n+  ElementType result = *(ElementType*)addr; \\\n+  return result; \\\n+JNI_END\n+\n+DEFINE_GETSUBELEMENT(jboolean, Boolean,T_BOOLEAN)\n+DEFINE_GETSUBELEMENT(jbyte, Byte, T_BYTE)\n+DEFINE_GETSUBELEMENT(jshort, Short,T_SHORT)\n+DEFINE_GETSUBELEMENT(jchar, Char,T_CHAR)\n+DEFINE_GETSUBELEMENT(jint, Int,T_INT)\n+DEFINE_GETSUBELEMENT(jlong, Long,T_LONG)\n+DEFINE_GETSUBELEMENT(jfloat, Float,T_FLOAT)\n+DEFINE_GETSUBELEMENT(jdouble, Double,T_DOUBLE)\n+\n+#define DEFINE_SETSUBELEMENT(ElementType,Result,ElementBasicType) \\\n+\\\n+JNI_ENTRY(void, \\\n+          jni_Set##Result##SubElement(JNIEnv *env, jarray array, jobject selector, int index, ElementType value)) \\\n+  flatArrayOop ar = (flatArrayOop)JNIHandles::resolve_non_null(array); \\\n+  oop slct = JNIHandles::resolve_non_null(selector); \\\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass()); \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) { \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\"); \\\n+  } \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct) != java_lang_Class::primitive_mirror(ElementBasicType)) { \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Wrong SubElement type\"); \\\n+  } \\\n+  address addr = (address)ar->value_at_addr(index, vak->layout_helper()) \\\n+               + jdk_internal_vm_jni_SubElementSelector::getOffset(slct); \\\n+  *(ElementType*)addr = value; \\\n+JNI_END\n+\n+DEFINE_SETSUBELEMENT(jboolean, Boolean,T_BOOLEAN)\n+DEFINE_SETSUBELEMENT(jbyte, Byte, T_BYTE)\n+DEFINE_SETSUBELEMENT(jshort, Short,T_SHORT)\n+DEFINE_SETSUBELEMENT(jchar, Char,T_CHAR)\n+DEFINE_SETSUBELEMENT(jint, Int,T_INT)\n+DEFINE_SETSUBELEMENT(jlong, Long,T_LONG)\n+DEFINE_SETSUBELEMENT(jfloat, Float,T_FLOAT)\n+DEFINE_SETSUBELEMENT(jdouble, Double,T_DOUBLE)\n+\n@@ -3395,1 +3765,32 @@\n-    jni_GetModule\n+    jni_GetModule,\n+\n+    \/\/ Flattened arrays features\n+\n+    jni_GetFlattenedArrayElements,\n+    jni_ReleaseFlattenedArrayElements,\n+    jni_GetFlattenedArrayElementClass,\n+    jni_GetFlattenedArrayElementSize,\n+    jni_GetFieldOffsetInFlattenedLayout,\n+\n+    jni_CreateSubElementSelector,\n+    jni_GetSubElementSelector,\n+    jni_GetObjectSubElement,\n+    jni_SetObjectSubElement,\n+\n+    jni_GetBooleanSubElement,\n+    jni_GetByteSubElement,\n+    jni_GetShortSubElement,\n+    jni_GetCharSubElement,\n+    jni_GetIntSubElement,\n+    jni_GetLongSubElement,\n+    jni_GetFloatSubElement,\n+    jni_GetDoubleSubElement,\n+\n+    jni_SetBooleanSubElement,\n+    jni_SetByteSubElement,\n+    jni_SetShortSubElement,\n+    jni_SetCharSubElement,\n+    jni_SetIntSubElement,\n+    jni_SetLongSubElement,\n+    jni_SetFloatSubElement,\n+    jni_SetDoubleSubElement\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":459,"deletions":58,"binary":false,"changes":517,"status":"modified"},{"patch":"@@ -283,1 +283,2 @@\n-      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT)) {\n+      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT) &&\n+      !(fd.field_type() == T_INLINE_TYPE && ftype == T_OBJECT)) {\n@@ -320,1 +321,2 @@\n-      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT)) {\n+      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT) &&\n+      !(fd.field_type() == T_INLINE_TYPE && ftype == T_OBJECT)) {\n@@ -371,1 +373,1 @@\n-check_is_obj_array(JavaThread* thr, jarray jArray) {\n+check_is_obj_or_inline_array(JavaThread* thr, jarray jArray) {\n@@ -373,1 +375,1 @@\n-  if (!aOop->is_objArray()) {\n+  if (!aOop->is_objArray() && !aOop->is_flatArray()) {\n@@ -493,1 +495,1 @@\n-      name[0] == JVM_SIGNATURE_CLASS &&            \/\/ 'L'\n+      (name[0] == JVM_SIGNATURE_CLASS || name[0] == JVM_SIGNATURE_INLINE_TYPE) && \/\/ 'L' or 'Q'\n@@ -1642,1 +1644,1 @@\n-      check_is_obj_array(thr, array);\n+      check_is_obj_or_inline_array(thr, array);\n@@ -1656,1 +1658,1 @@\n-      check_is_obj_array(thr, array);\n+      check_is_obj_or_inline_array(thr, array);\n@@ -2021,0 +2023,201 @@\n+JNI_ENTRY_CHECKED(void*,\n+    checked_jni_GetFlattenedArrayElements(JNIEnv* env, jarray array, jboolean* isCopy))\n+    functionEnter(thr);\n+    void* result = UNCHECKED()->GetFlattenedArrayElements(env, array, isCopy);\n+    functionExit(thr);\n+    return result;\n+\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(void,\n+    checked_jni_ReleaseFlattenedArrayElements(JNIEnv* env, jarray array, void* elem, jint mode))\n+    functionEnter(thr);\n+    UNCHECKED()->ReleaseFlattenedArrayElements(env, array, elem, mode);\n+    functionExit(thr);\n+    return;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jclass,\n+    checked_jni_GetFlattenedArrayElementClass(JNIEnv* env, jarray array))\n+    functionEnter(thr);\n+    jclass clazz = UNCHECKED()->GetFlattenedArrayElementClass(env, array);\n+    functionExit(thr);\n+    return clazz;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jsize,\n+    checked_jni_GetFlattenedArrayElementSize(JNIEnv* env, jarray array))\n+    functionEnter(thr);\n+    jsize size = UNCHECKED()->GetFlattenedArrayElementSize(env, array);\n+    functionExit(thr);\n+    return size;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jsize,\n+    checked_jni_GetFieldOffsetInFlattenedLayout(JNIEnv* env, jclass clazz, const char *name, const char *signature, jboolean* isFlattened))\n+    functionEnter(thr);\n+    jsize offset = UNCHECKED()->GetFieldOffsetInFlattenedLayout(env, clazz, name, signature, isFlattened);\n+    functionExit(thr);\n+    return offset;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jobject,\n+    checked_jni_CreateSubElementSelector(JNIEnv* env, jarray array))\n+    functionEnter(thr);\n+    jobject selector = UNCHECKED()->CreateSubElementSelector(env, array);\n+    functionExit(thr);\n+    return selector;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jobject,\n+    checked_jni_GetSubElementSelector(JNIEnv* env, jobject selector, jfieldID fieldID))\n+    functionEnter(thr);\n+    jobject res = UNCHECKED()->GetSubElementSelector(env, selector, fieldID);\n+    functionExit(thr);\n+    return res;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jobject,\n+    checked_jni_GetObjectSubElement(JNIEnv* env, jarray array, jobject selector, int index))\n+    functionEnter(thr);\n+    jobject res = UNCHECKED()->GetObjectSubElement(env, array, selector, index);\n+    functionExit(thr);\n+    return res;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(void,\n+    checked_jni_SetObjectSubElement(JNIEnv* env, jarray array, jobject selector, int index, jobject value))\n+    functionEnter(thr);\n+    UNCHECKED()->SetObjectSubElement(env, array, selector, index, value);\n+    functionExit(thr);\n+    return;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jboolean,\n+    checked_jni_GetBooleanSubElement(JNIEnv* env, jarray array, jobject selector, int index))\n+    functionEnter(thr);\n+    jboolean res = UNCHECKED()->GetBooleanSubElement(env, array, selector, index);\n+    functionExit(thr);\n+    return res;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(void,\n+    checked_jni_SetBooleanSubElement(JNIEnv* env, jarray array, jobject selector, int index, jboolean value))\n+    functionEnter(thr);\n+    UNCHECKED()->SetBooleanSubElement(env, array, selector, index, value);\n+    functionExit(thr);\n+    return;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jbyte,\n+    checked_jni_GetByteSubElement(JNIEnv* env, jarray array, jobject selector, int index))\n+    functionEnter(thr);\n+    jbyte res = UNCHECKED()->GetByteSubElement(env, array, selector, index);\n+    functionExit(thr);\n+    return res;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(void,\n+    checked_jni_SetByteSubElement(JNIEnv* env, jarray array, jobject selector, int index, jbyte value))\n+    functionEnter(thr);\n+    UNCHECKED()->SetByteSubElement(env, array, selector, index, value);\n+    functionExit(thr);\n+    return;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jshort,\n+    checked_jni_GetShortSubElement(JNIEnv* env, jarray array, jobject selector, int index))\n+    functionEnter(thr);\n+    jshort res = UNCHECKED()->GetShortSubElement(env, array, selector, index);\n+    functionExit(thr);\n+    return res;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(void,\n+    checked_jni_SetShortSubElement(JNIEnv* env, jarray array, jobject selector, int index, jshort value))\n+    functionEnter(thr);\n+    UNCHECKED()->SetShortSubElement(env, array, selector, index, value);\n+    functionExit(thr);\n+    return;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jchar,\n+    checked_jni_GetCharSubElement(JNIEnv* env, jarray array, jobject selector, int index))\n+    functionEnter(thr);\n+    jchar res = UNCHECKED()->GetCharSubElement(env, array, selector, index);\n+    functionExit(thr);\n+    return res;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(void,\n+    checked_jni_SetCharSubElement(JNIEnv* env, jarray array, jobject selector, int index, jchar value))\n+    functionEnter(thr);\n+    UNCHECKED()->SetCharSubElement(env, array, selector, index, value);\n+    functionExit(thr);\n+    return;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jint,\n+    checked_jni_GetIntSubElement(JNIEnv* env, jarray array, jobject selector, int index))\n+    functionEnter(thr);\n+    jint res = UNCHECKED()->GetIntSubElement(env, array, selector, index);\n+    functionExit(thr);\n+    return res;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(void,\n+    checked_jni_SetIntSubElement(JNIEnv* env, jarray array, jobject selector, int index, jint value))\n+    functionEnter(thr);\n+    UNCHECKED()->SetIntSubElement(env, array, selector, index, value);\n+    functionExit(thr);\n+    return;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jlong,\n+    checked_jni_GetLongSubElement(JNIEnv* env, jarray array, jobject selector, int index))\n+    functionEnter(thr);\n+    jlong res = UNCHECKED()->GetLongSubElement(env, array, selector, index);\n+    functionExit(thr);\n+    return res;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(void,\n+    checked_jni_SetLongSubElement(JNIEnv* env, jarray array, jobject selector, int index, jlong value))\n+    functionEnter(thr);\n+    UNCHECKED()->SetLongSubElement(env, array, selector, index, value);\n+    functionExit(thr);\n+    return;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jfloat,\n+    checked_jni_GetFloatSubElement(JNIEnv* env, jarray array, jobject selector, int index))\n+    functionEnter(thr);\n+    jfloat res = UNCHECKED()->GetFloatSubElement(env, array, selector, index);\n+    functionExit(thr);\n+    return res;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(void,\n+    checked_jni_SetFloatSubElement(JNIEnv* env, jarray array, jobject selector, int index, jfloat value))\n+    functionEnter(thr);\n+    UNCHECKED()->SetFloatSubElement(env, array, selector, index, value);\n+    functionExit(thr);\n+    return;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jdouble,\n+    checked_jni_GetDoubleSubElement(JNIEnv* env, jarray array, jobject selector, int index))\n+    functionEnter(thr);\n+    jdouble res = UNCHECKED()->GetDoubleSubElement(env, array, selector, index);\n+    functionExit(thr);\n+    return res;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(void,\n+    checked_jni_SetDoubleSubElement(JNIEnv* env, jarray array, jobject selector, int index, jdouble value))\n+    functionEnter(thr);\n+    UNCHECKED()->SetDoubleSubElement(env, array, selector, index, value);\n+    functionExit(thr);\n+    return;\n+JNI_END\n+\n@@ -2306,1 +2509,31 @@\n-    checked_jni_GetModule\n+    checked_jni_GetModule,\n+\n+    \/\/ Flattened arrays Features\n+    checked_jni_GetFlattenedArrayElements,\n+    checked_jni_ReleaseFlattenedArrayElements,\n+    checked_jni_GetFlattenedArrayElementClass,\n+    checked_jni_GetFlattenedArrayElementSize,\n+    checked_jni_GetFieldOffsetInFlattenedLayout,\n+\n+    checked_jni_CreateSubElementSelector,\n+    checked_jni_GetSubElementSelector,\n+    checked_jni_GetObjectSubElement,\n+    checked_jni_SetObjectSubElement,\n+\n+    checked_jni_GetBooleanSubElement,\n+    checked_jni_GetByteSubElement,\n+    checked_jni_GetShortSubElement,\n+    checked_jni_GetCharSubElement,\n+    checked_jni_GetIntSubElement,\n+    checked_jni_GetLongSubElement,\n+    checked_jni_GetFloatSubElement,\n+    checked_jni_GetDoubleSubElement,\n+\n+    checked_jni_SetBooleanSubElement,\n+    checked_jni_SetByteSubElement,\n+    checked_jni_SetShortSubElement,\n+    checked_jni_SetCharSubElement,\n+    checked_jni_SetIntSubElement,\n+    checked_jni_SetLongSubElement,\n+    checked_jni_SetFloatSubElement,\n+    checked_jni_SetDoubleSubElement\n","filename":"src\/hotspot\/share\/prims\/jniCheck.cpp","additions":241,"deletions":8,"binary":false,"changes":249,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -595,1 +596,22 @@\n-  return handle == NULL ? 0 : ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)) ;\n+  if (handle == NULL) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, Universe::inline_type_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return ObjectSynchronizer::FastHashCode(THREAD, obj);\n+  }\n@@ -647,0 +669,1 @@\n+       klass->is_inline_klass() ||\n@@ -1145,1 +1168,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1148,1 +1172,1 @@\n-    size = 2;\n+    size = 3;\n@@ -1158,1 +1182,2 @@\n-      Klass* k = InstanceKlass::cast(klass)->local_interfaces()->at(index);\n+      InstanceKlass* ik = InstanceKlass::cast(klass);\n+      Klass* k = ik->local_interfaces()->at(index);\n@@ -1162,1 +1187,1 @@\n-    \/\/ All arrays implement java.lang.Cloneable and java.io.Serializable\n+    \/\/ All arrays implement java.lang.Cloneable, java.io.Serializable and java.lang.IdentityObject\n@@ -1165,0 +1190,1 @@\n+    result->obj_at_put(2, vmClasses::IdentityObject_klass()->java_mirror());\n@@ -1760,0 +1786,2 @@\n+  bool is_ctor = (method->is_object_constructor() ||\n+                  method->is_static_init_factory());\n@@ -1761,1 +1789,1 @@\n-    return (method->is_initializer() && !method->is_static());\n+    return is_ctor;\n@@ -1763,1 +1791,3 @@\n-    return  (!method->is_initializer() && !method->is_overpass());\n+    return (!is_ctor &&\n+            !method->is_class_initializer() &&\n+            !method->is_overpass());\n@@ -1826,0 +1856,2 @@\n+        assert(method->is_object_constructor() ||\n+               method->is_static_init_factory(), \"must be\");\n@@ -2108,3 +2140,1 @@\n-  if (!m->is_initializer() || m->is_static()) {\n-    method = Reflection::new_method(m, true, CHECK_NULL);\n-  } else {\n+  if (m->is_object_constructor() || m->is_static_init_factory()) {\n@@ -2112,0 +2142,2 @@\n+  } else {\n+    method = Reflection::new_method(m, true, CHECK_NULL);\n@@ -2382,0 +2414,37 @@\n+\/\/ Arrays support \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+JVM_ENTRY(jboolean, JVM_ArrayIsAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == NULL) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  return ArrayKlass::cast(k)->element_access_is_atomic();\n+JVM_END\n+\n+JVM_ENTRY(jobject, JVM_ArrayEnsureAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == NULL) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vk = FlatArrayKlass::cast(k);\n+    if (!vk->element_access_is_atomic()) {\n+      \/**\n+       * Need to decide how to implement:\n+       *\n+       * 1) Change to objArrayOop layout, therefore oop->klass() differs so\n+       * then \"<atomic>[Qfoo;\" klass needs to subclass \"[Qfoo;\" to pass through\n+       * \"checkcast\" & \"instanceof\"\n+       *\n+       * 2) Use extra header in the flatArrayOop to flag atomicity required and\n+       * possibly per instance lock structure. Said info, could be placed in\n+       * \"trailer\" rather than disturb the current arrayOop\n+       *\/\n+      Unimplemented();\n+    }\n+  }\n+  return array;\n+JVM_END\n+\n@@ -2544,1 +2613,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3514,1 +3583,1 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3534,0 +3603,1 @@\n+  objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3535,1 +3605,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":82,"deletions":13,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"classfile\/vmClasses.hpp\"\n@@ -898,1 +899,1 @@\n-  write_u2(num_interfaces);\n+  write_u2(num_interfaces - (ik()->has_injected_identityObject() ? 1 : 0) );\n@@ -902,1 +903,3 @@\n-    write_u2(class_symbol_to_cpool_index(iik->name()));\n+    if (!ik()->has_injected_identityObject() || iik != vmClasses::IdentityObject_klass()) {\n+      write_u2(class_symbol_to_cpool_index(iik->name()));\n+    }\n","filename":"src\/hotspot\/share\/prims\/jvmtiClassFileReconstituter.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2516,1 +2516,2 @@\n-                                            src_st.access_flags().is_static());\n+                                            src_st.access_flags().is_static(),\n+                                            src_st.field_descriptor().is_inlined());\n@@ -2553,2 +2554,3 @@\n-    Array<InstanceKlass*>* interface_list = InstanceKlass::cast(k)->local_interfaces();\n-    const int result_length = (interface_list == NULL ? 0 : interface_list->length());\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    Array<InstanceKlass*>* interface_list = ik->local_interfaces();\n+    int result_length = (interface_list == NULL ? 0 : interface_list->length());\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -973,2 +973,4 @@\n-    \/\/ Revoke any biases before querying the mark word\n-    BiasedLocking::revoke_at_safepoint(hobj);\n+    if (UseBiasedLocking) {\n+      \/\/ Revoke any biases before querying the mark word\n+      BiasedLocking::revoke_at_safepoint(hobj);\n+    }\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnvBase.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include <oops\/inlineKlass.hpp>\n@@ -81,0 +82,1 @@\n+      \/\/ CMH flat arrays (InlineKlass)\n","filename":"src\/hotspot\/share\/prims\/jvmtiGetLoadedClasses.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -504,1 +504,2 @@\n-  if (ty_sign[0] == JVM_SIGNATURE_CLASS &&\n+  if ((ty_sign[0] == JVM_SIGNATURE_CLASS ||\n+       ty_sign[0] == JVM_SIGNATURE_INLINE_TYPE) &&\n@@ -572,0 +573,1 @@\n+  case T_INLINE_TYPE:\n@@ -697,1 +699,1 @@\n-      if (_type == T_OBJECT) {\n+      if (_type == T_OBJECT || _type == T_INLINE_TYPE) {\n@@ -715,1 +717,2 @@\n-      case T_OBJECT: {\n+      case T_OBJECT:\n+      case T_INLINE_TYPE: {\n@@ -736,1 +739,2 @@\n-        case T_OBJECT: {\n+        case T_OBJECT:\n+        case T_INLINE_TYPE: {\n","filename":"src\/hotspot\/share\/prims\/jvmtiImpl.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -337,0 +337,5 @@\n+  \/\/ Cannot redefine or retransform interface java.lang.IdentityObject.\n+  if (k->name() == vmSymbols::java_lang_IdentityObject()) {\n+    return false;\n+  }\n+\n@@ -607,2 +612,1 @@\n-    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be\n-    \/\/ here\n+    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be here\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -129,5 +129,5 @@\n-  IS_METHOD            = java_lang_invoke_MemberName::MN_IS_METHOD,\n-  IS_CONSTRUCTOR       = java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR,\n-  IS_FIELD             = java_lang_invoke_MemberName::MN_IS_FIELD,\n-  IS_TYPE              = java_lang_invoke_MemberName::MN_IS_TYPE,\n-  CALLER_SENSITIVE     = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,\n+  IS_METHOD             = java_lang_invoke_MemberName::MN_IS_METHOD,\n+  IS_OBJECT_CONSTRUCTOR = java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR,\n+  IS_FIELD              = java_lang_invoke_MemberName::MN_IS_FIELD,\n+  IS_TYPE               = java_lang_invoke_MemberName::MN_IS_TYPE,\n+  CALLER_SENSITIVE      = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,\n@@ -135,8 +135,8 @@\n-  REFERENCE_KIND_SHIFT = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,\n-  REFERENCE_KIND_MASK  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,\n-  SEARCH_SUPERCLASSES  = java_lang_invoke_MemberName::MN_SEARCH_SUPERCLASSES,\n-  SEARCH_INTERFACES    = java_lang_invoke_MemberName::MN_SEARCH_INTERFACES,\n-  LM_UNCONDITIONAL     = java_lang_invoke_MemberName::MN_UNCONDITIONAL_MODE,\n-  LM_MODULE            = java_lang_invoke_MemberName::MN_MODULE_MODE,\n-  LM_TRUSTED           = java_lang_invoke_MemberName::MN_TRUSTED_MODE,\n-  ALL_KINDS      = IS_METHOD | IS_CONSTRUCTOR | IS_FIELD | IS_TYPE\n+  REFERENCE_KIND_SHIFT  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,\n+  REFERENCE_KIND_MASK   = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,\n+  SEARCH_SUPERCLASSES   = java_lang_invoke_MemberName::MN_SEARCH_SUPERCLASSES,\n+  SEARCH_INTERFACES     = java_lang_invoke_MemberName::MN_SEARCH_INTERFACES,\n+  LM_UNCONDITIONAL      = java_lang_invoke_MemberName::MN_UNCONDITIONAL_MODE,\n+  LM_MODULE             = java_lang_invoke_MemberName::MN_MODULE_MODE,\n+  LM_TRUSTED            = java_lang_invoke_MemberName::MN_TRUSTED_MODE,\n+  ALL_KINDS      = IS_METHOD | IS_OBJECT_CONSTRUCTOR | IS_FIELD | IS_TYPE\n@@ -153,1 +153,1 @@\n-    flags |= IS_CONSTRUCTOR;\n+    flags |= IS_OBJECT_CONSTRUCTOR;\n@@ -172,1 +172,1 @@\n-    case IS_CONSTRUCTOR:\n+    case IS_OBJECT_CONSTRUCTOR:\n@@ -314,2 +314,2 @@\n-    } else if (m->is_initializer()) {\n-      flags |= IS_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);\n+    } else if (m->is_object_constructor()) {\n+      flags |= IS_OBJECT_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);\n@@ -353,0 +353,3 @@\n+  if (fd.is_inlined()) {\n+    flags |= JVM_ACC_FIELD_INLINED;\n+  }\n@@ -807,1 +810,1 @@\n-  case IS_CONSTRUCTOR:\n+  case IS_OBJECT_CONSTRUCTOR:\n@@ -813,1 +816,3 @@\n-        if (name == vmSymbols::object_initializer_name()) {\n+        if (name != vmSymbols::object_initializer_name()) {\n+          break;                \/\/ will throw after end of switch\n+        } else if (type->is_void_method_signature()) {\n@@ -816,1 +821,2 @@\n-          break;                \/\/ will throw after end of switch\n+          \/\/ LinkageError unless it returns something reasonable\n+          LinkResolver::resolve_static_call(result, link_info, false, THREAD);\n@@ -876,1 +882,1 @@\n-  case IS_CONSTRUCTOR:\n+  case IS_OBJECT_CONSTRUCTOR:\n@@ -955,1 +961,1 @@\n-      match_flags &= ~(IS_CONSTRUCTOR | IS_METHOD);\n+      match_flags &= ~(IS_OBJECT_CONSTRUCTOR | IS_METHOD);\n@@ -985,1 +991,1 @@\n-  if ((match_flags & (IS_METHOD | IS_CONSTRUCTOR)) != 0) {\n+  if ((match_flags & (IS_METHOD | IS_OBJECT_CONSTRUCTOR)) != 0) {\n@@ -990,2 +996,2 @@\n-    bool negate_name_test = false;\n-    \/\/ fix name so that it captures the intention of IS_CONSTRUCTOR\n+    bool ctor_ok = true, sfac_ok = true;\n+    \/\/ fix name so that it captures the intention of IS_OBJECT_CONSTRUCTOR\n@@ -999,1 +1005,2 @@\n-    } else if (!(match_flags & IS_CONSTRUCTOR)) {\n+      sfac_ok = false;\n+    } else if (!(match_flags & IS_OBJECT_CONSTRUCTOR)) {\n@@ -1001,6 +1008,1 @@\n-      if (name == NULL) {\n-        name = init_name;\n-        negate_name_test = true; \/\/ if we see the name, we *omit* the entry\n-      } else if (name == init_name) {\n-        return 0;               \/\/ no methods of this constructor name\n-      }\n+      ctor_ok = false;  \/\/ but sfac_ok is true, so we might find <init>\n@@ -1016,1 +1018,1 @@\n-      if (name != NULL && ((m_name != name) ^ negate_name_test))\n+      if (name != NULL && m_name != name)\n@@ -1020,0 +1022,4 @@\n+      if (m_name == init_name) {  \/\/ might be either ctor or sfac\n+        if (m->is_object_constructor()  && !ctor_ok)  continue;\n+        if (m->is_static_init_factory() && !sfac_ok)  continue;\n+      }\n@@ -1119,1 +1125,1 @@\n-    template(java_lang_invoke_MemberName,MN_IS_CONSTRUCTOR) \\\n+    template(java_lang_invoke_MemberName,MN_IS_OBJECT_CONSTRUCTOR) \\\n@@ -1263,1 +1269,1 @@\n-               (flags & ALL_KINDS) == IS_CONSTRUCTOR) {\n+               (flags & ALL_KINDS) == IS_OBJECT_CONSTRUCTOR) {\n","filename":"src\/hotspot\/share\/prims\/methodHandles.cpp","additions":41,"deletions":35,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logStream.hpp\"\n@@ -39,0 +41,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -45,0 +50,1 @@\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n@@ -150,1 +156,0 @@\n-\n@@ -235,0 +240,1 @@\n+      assert(!_obj->is_inline_type() || _obj->mark().is_larval_state(), \"must be an object instance or a larval inline type\");\n@@ -239,1 +245,0 @@\n-\n@@ -261,0 +266,62 @@\n+#ifdef ASSERT\n+\/*\n+ * Get the field descriptor of the field of the given object at the given offset.\n+ *\/\n+static bool get_field_descriptor(oop p, jlong offset, fieldDescriptor* fd) {\n+  bool found = false;\n+  Klass* k = p->klass();\n+  if (k->is_instance_klass()) {\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    found = ik->find_field_from_offset((int)offset, false, fd);\n+    if (!found && ik->is_mirror_instance_klass()) {\n+      Klass* k2 = java_lang_Class::as_Klass(p);\n+      if (k2->is_instance_klass()) {\n+        ik = InstanceKlass::cast(k2);\n+        found = ik->find_field_from_offset((int)offset, true, fd);\n+      }\n+    }\n+  }\n+  return found;\n+}\n+#endif \/\/ ASSERT\n+\n+static void assert_and_log_unsafe_value_access(oop p, jlong offset, InlineKlass* vk) {\n+  Klass* k = p->klass();\n+#ifdef ASSERT\n+  if (k->is_instance_klass()) {\n+    assert_field_offset_sane(p, offset);\n+    fieldDescriptor fd;\n+    bool found = get_field_descriptor(p, offset, &fd);\n+    if (found) {\n+      assert(found, \"value field not found\");\n+      assert(fd.is_inlined(), \"field not flat\");\n+    } else {\n+      if (log_is_enabled(Trace, valuetypes)) {\n+        log_trace(valuetypes)(\"not a field in %s at offset \" SIZE_FORMAT_HEX,\n+                              p->klass()->external_name(), offset);\n+      }\n+    }\n+  } else if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+    int index = (offset - vak->array_header_in_bytes()) \/ vak->element_byte_size();\n+    address dest = (address)((flatArrayOop)p)->value_at_addr(index, vak->layout_helper());\n+    assert(dest == (cast_from_oop<address>(p) + offset), \"invalid offset\");\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+#endif \/\/ ASSERT\n+  if (log_is_enabled(Trace, valuetypes)) {\n+    if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      int index = (offset - vak->array_header_in_bytes()) \/ vak->element_byte_size();\n+      address dest = (address)((flatArrayOop)p)->value_at_addr(index, vak->layout_helper());\n+      log_trace(valuetypes)(\"%s array type %s index %d element size %d offset \" SIZE_FORMAT_HEX \" at \" INTPTR_FORMAT,\n+                            p->klass()->external_name(), vak->external_name(),\n+                            index, vak->element_byte_size(), offset, p2i(dest));\n+    } else {\n+      log_trace(valuetypes)(\"%s field type %s at offset \" SIZE_FORMAT_HEX,\n+                            p->klass()->external_name(), vk->external_name(), offset);\n+    }\n+  }\n+}\n+\n@@ -275,0 +342,1 @@\n+  assert(!p->is_inline_type() || p->mark().is_larval_state(), \"must be an object instance or a larval inline type\");\n@@ -278,0 +346,58 @@\n+UNSAFE_ENTRY(jlong, Unsafe_ValueHeaderSize(JNIEnv *env, jobject unsafe, jclass c)) {\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  return vk->first_field_offset();\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jboolean, Unsafe_IsFlattenedArray(JNIEnv *env, jobject unsafe, jclass c)) {\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));\n+  return k->is_flatArray_klass();\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_UninitializedDefaultValue(JNIEnv *env, jobject unsafe, jclass vc)) {\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  oop v = vk->default_value();\n+  return JNIHandles::make_local(THREAD, v);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_GetValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc)) {\n+  oop base = JNIHandles::resolve(obj);\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  assert_and_log_unsafe_value_access(base, offset, vk);\n+  Handle base_h(THREAD, base);\n+  oop v = vk->read_inlined_field(base_h(), offset, CHECK_NULL);\n+  return JNIHandles::make_local(THREAD, v);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(void, Unsafe_PutValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc, jobject value)) {\n+  oop base = JNIHandles::resolve(obj);\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  assert(!base->is_inline_type() || base->mark().is_larval_state(), \"must be an object instance or a larval inline type\");\n+  assert_and_log_unsafe_value_access(base, offset, vk);\n+  oop v = JNIHandles::resolve(value);\n+  vk->write_inlined_field(base, offset, v, CHECK);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_MakePrivateBuffer(JNIEnv *env, jobject unsafe, jobject value)) {\n+  oop v = JNIHandles::resolve_non_null(value);\n+  assert(v->is_inline_type(), \"must be an inline type instance\");\n+  Handle vh(THREAD, v);\n+  InlineKlass* vk = InlineKlass::cast(v->klass());\n+  instanceOop new_value = vk->allocate_instance_buffer(CHECK_NULL);\n+  vk->inline_copy_oop_to_new_oop(vh(),  new_value);\n+  markWord mark = new_value->mark();\n+  new_value->set_mark(mark.enter_larval_state());\n+  return JNIHandles::make_local(THREAD, new_value);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_FinishPrivateBuffer(JNIEnv *env, jobject unsafe, jobject value)) {\n+  oop v = JNIHandles::resolve(value);\n+  assert(v->mark().is_larval_state(), \"must be a larval value\");\n+  markWord mark = v->mark();\n+  v->set_mark(mark.exit_larval_state());\n+  return JNIHandles::make_local(THREAD, v);\n+} UNSAFE_END\n+\n@@ -614,0 +740,5 @@\n+  } else if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+    InlineKlass* vklass = vak->element_klass();\n+    base = vak->array_header_in_bytes();\n+    scale = vak->element_byte_size();\n@@ -649,0 +780,6 @@\n+UNSAFE_ENTRY(jlong, Unsafe_GetObjectSize0(JNIEnv* env, jobject o, jobject obj))\n+  oop p = JNIHandles::resolve(obj);\n+  return p->size() * HeapWordSize;\n+UNSAFE_END\n+\n+\n@@ -778,0 +915,2 @@\n+\/\/\n+\/\/ An anonymous class cannot be an inline type.\n@@ -873,0 +1012,2 @@\n+  assert(!anonk->is_inline_klass(), \"unsafe anonymous class cannot be inline class\");\n+\n@@ -1066,4 +1207,4 @@\n-    {CC \"get\" #Type,      CC \"(\" OBJ \"J)\" #Desc,       FN_PTR(Unsafe_Get##Type)}, \\\n-    {CC \"put\" #Type,      CC \"(\" OBJ \"J\" #Desc \")V\",   FN_PTR(Unsafe_Put##Type)}, \\\n-    {CC \"get\" #Type \"Volatile\",      CC \"(\" OBJ \"J)\" #Desc,       FN_PTR(Unsafe_Get##Type##Volatile)}, \\\n-    {CC \"put\" #Type \"Volatile\",      CC \"(\" OBJ \"J\" #Desc \")V\",   FN_PTR(Unsafe_Put##Type##Volatile)}\n+    {CC \"get\"  #Type,      CC \"(\" OBJ \"J)\" #Desc,                 FN_PTR(Unsafe_Get##Type)}, \\\n+    {CC \"put\"  #Type,      CC \"(\" OBJ \"J\" #Desc \")V\",             FN_PTR(Unsafe_Put##Type)}, \\\n+    {CC \"get\"  #Type \"Volatile\",      CC \"(\" OBJ \"J)\" #Desc,      FN_PTR(Unsafe_Get##Type##Volatile)}, \\\n+    {CC \"put\"  #Type \"Volatile\",      CC \"(\" OBJ \"J\" #Desc \")V\",  FN_PTR(Unsafe_Put##Type##Volatile)}\n@@ -1078,0 +1219,8 @@\n+    {CC \"isFlattenedArray\", CC \"(\" CLS \")Z\",                     FN_PTR(Unsafe_IsFlattenedArray)},\n+    {CC \"getValue\",         CC \"(\" OBJ \"J\" CLS \")\" OBJ,          FN_PTR(Unsafe_GetValue)},\n+    {CC \"putValue\",         CC \"(\" OBJ \"J\" CLS OBJ \")V\",         FN_PTR(Unsafe_PutValue)},\n+    {CC \"uninitializedDefaultValue\", CC \"(\" CLS \")\" OBJ,         FN_PTR(Unsafe_UninitializedDefaultValue)},\n+    {CC \"makePrivateBuffer\",     CC \"(\" OBJ \")\" OBJ,             FN_PTR(Unsafe_MakePrivateBuffer)},\n+    {CC \"finishPrivateBuffer\",   CC \"(\" OBJ \")\" OBJ,             FN_PTR(Unsafe_FinishPrivateBuffer)},\n+    {CC \"valueHeaderSize\",       CC \"(\" CLS \")J\",                FN_PTR(Unsafe_ValueHeaderSize)},\n+\n@@ -1100,0 +1249,1 @@\n+    {CC \"getObjectSize0\",     CC \"(Ljava\/lang\/Object;)J\", FN_PTR(Unsafe_GetObjectSize0)},\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":156,"deletions":6,"binary":false,"changes":162,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -59,0 +60,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -65,0 +67,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -1808,0 +1811,92 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return NULL;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(aoop->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return NULL;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayOop result_array =\n+      oopFactory::new_objArray(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  instanceOop ioop = ih();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ioop->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array);\n+WB_END\n+\n+class CollectOops : public BasicOopIterateClosure {\n+ public:\n+  GrowableArray<Handle>* array;\n+\n+  objArrayOop create_results(TRAPS) {\n+    objArrayOop result_array =\n+        oopFactory::new_objArray(vmClasses::Object_klass(), array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < array->length(); i++) {\n+      result_array->obj_at_put(i, array->at(i)());\n+    }\n+    return result_array;\n+  }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return (jobjectArray)JNIHandles::make_local(THREAD, create_results(THREAD));\n+  }\n+\n+  void add_oop(oop o) {\n+    \/\/ Value might be oop, but JLS can't see as Object, just iterate through it...\n+    if (o != NULL && o->is_inline_type()) {\n+      o->oop_iterate(this);\n+    } else {\n+      array->append(Handle(Thread::current(), o));\n+    }\n+  }\n+\n+  void do_oop(oop* o) { add_oop(*o); }\n+  void do_oop(narrowOop* v) { add_oop(CompressedOops::decode(*v)); }\n+};\n+\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+\n+  JNIHandles::resolve(thing)->oop_iterate(&collectOops);\n+\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  StackFrameStream sfs(thread, false \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, NULL, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\n@@ -2513,0 +2608,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":101,"deletions":0,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -2023,0 +2023,10 @@\n+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypePassFieldsAsArgs)) {\n+    FLAG_SET_CMDLINE(InlineTypePassFieldsAsArgs, false);\n+    warning(\"InlineTypePassFieldsAsArgs is not supported on this platform\");\n+  }\n+\n+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypeReturnedAsFields)) {\n+    FLAG_SET_CMDLINE(InlineTypeReturnedAsFields, false);\n+    warning(\"InlineTypeReturnedAsFields is not supported on this platform\");\n+  }\n+\n@@ -2944,0 +2954,18 @@\n+  if (EnableValhalla) {\n+    \/\/ create_property(\"valhalla.enableValhalla\", \"true\", InternalProperty)\n+    const char* prop_name = \"valhalla.enableValhalla\";\n+    const char* prop_value = \"true\";\n+    const size_t prop_len = strlen(prop_name) + strlen(prop_value) + 2;\n+    char* property = AllocateHeap(prop_len, mtArguments);\n+    int ret = jio_snprintf(property, prop_len, \"%s=%s\", prop_name, prop_value);\n+    if (ret < 0 || ret >= (int)prop_len) {\n+      FreeHeap(property);\n+      return JNI_ENOMEM;\n+    }\n+    bool added = add_property(property, UnwriteableProperty, InternalProperty);\n+    FreeHeap(property);\n+    if (!added) {\n+      return JNI_ENOMEM;\n+    }\n+  }\n+\n@@ -3058,0 +3086,5 @@\n+  if (UseBiasedLocking) {\n+    jio_fprintf(defaultStream::error_stream(), \"Valhalla does not support use with UseBiasedLocking\");\n+    return JNI_ERR;\n+  }\n+\n@@ -4056,0 +4089,5 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !is_dumping_archive())) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported\n+    InlineTypePassFieldsAsArgs = false;\n+    InlineTypeReturnedAsFields = false;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -45,0 +45,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n@@ -50,0 +52,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -200,2 +203,13 @@\n-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n-  Handle return_value;\n+  ScopeDesc* scope = chunk->at(0)->scope();\n+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n+  \/\/ In case of the return of multiple values, we must take care\n+  \/\/ of all oop return values.\n+  GrowableArray<Handle> return_oops;\n+  InlineKlass* vk = NULL;\n+  if (save_oop_result && scope->return_vt()) {\n+    vk = InlineKlass::returned_inline_klass(map);\n+    if (vk != NULL) {\n+      vk->save_oop_fields(map, return_oops);\n+      save_oop_result = false;\n+    }\n+  }\n@@ -207,1 +221,1 @@\n-    return_value = Handle(thread, result);\n+    return_oops.push(Handle(thread, result));\n@@ -214,1 +228,1 @@\n-  if (objects != NULL) {\n+  if (objects != NULL || vk != NULL) {\n@@ -219,1 +233,8 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+      if (vk != NULL) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, CHECK_AND_CLEAR_(true));\n+      }\n+      if (objects != NULL) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+        bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, CHECK_AND_CLEAR_(true));\n+      }\n@@ -223,1 +244,8 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+      if (vk != NULL) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);\n+      }\n+      if (objects != NULL) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+        bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, THREAD);\n+      }\n@@ -226,2 +254,0 @@\n-    bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();\n-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal);\n@@ -236,1 +262,1 @@\n-  if (save_oop_result) {\n+  if (save_oop_result || vk != NULL) {\n@@ -238,1 +264,2 @@\n-    deoptee.set_saved_oop_result(&map, return_value());\n+    assert(return_oops.length() == 1, \"no inline type\");\n+    deoptee.set_saved_oop_result(&map, return_oops.pop()());\n@@ -569,1 +596,1 @@\n-  \/\/ If the sender is deoptimized the we must retrieve the address of the handler\n+  \/\/ If the sender is deoptimized we must retrieve the address of the handler\n@@ -1071,0 +1098,4 @@\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* ak = FlatArrayKlass::cast(k);\n+      \/\/ Inline type array must be zeroed because not all memory is reassigned\n+      obj = ak->allocate(sv->field_size(), THREAD);\n@@ -1100,0 +1131,15 @@\n+\/\/ We're deoptimizing at the return of a call, inline type fields are\n+\/\/ in registers. When we go back to the interpreter, it will expect a\n+\/\/ reference to an inline type instance. Allocate and initialize it from\n+\/\/ the register values here.\n+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {\n+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);\n+  if (new_vt == NULL) {\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);\n+  }\n+  return_oops.clear();\n+  return_oops.push(Handle(THREAD, new_vt));\n+  return false;\n+}\n+\n@@ -1272,0 +1318,1 @@\n+  InstanceKlass* _klass;\n@@ -1276,0 +1323,1 @@\n+    _klass = NULL;\n@@ -1285,1 +1333,5 @@\n-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal) {\n+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal, int base_offset, TRAPS) {\n+  if (svIndex >= sv->field_size()) {\n+    \/\/ No fields left to re-assign.\n+    return svIndex;\n+  }\n@@ -1294,0 +1346,9 @@\n+        if (field._type == T_INLINE_TYPE) {\n+          field._type = T_OBJECT;\n+        }\n+        if (fs.is_inlined()) {\n+          \/\/ Resolve klass of flattened inline type field\n+          Klass* vk = klass->get_inline_type_field_klass(fs.index());\n+          field._klass = InlineKlass::cast(vk);\n+          field._type = T_INLINE_TYPE;\n+        }\n@@ -1304,1 +1365,1 @@\n-    int offset = fields->at(i)._offset;\n+    int offset = base_offset + fields->at(i)._offset;\n@@ -1307,1 +1368,2 @@\n-      case T_OBJECT: case T_ARRAY:\n+      case T_OBJECT:\n+      case T_ARRAY:\n@@ -1312,0 +1374,9 @@\n+      case T_INLINE_TYPE: {\n+        \/\/ Recursively re-assign flattened inline type fields\n+        InstanceKlass* vk = fields->at(i)._klass;\n+        assert(vk != NULL, \"must be resolved\");\n+        offset -= InlineKlass::cast(vk)->first_field_offset(); \/\/ Adjust offset to omit oop header\n+        svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, skip_internal, offset, CHECK_0);\n+        continue; \/\/ Continue because we don't need to increment svIndex\n+      }\n+\n@@ -1387,0 +1458,14 @@\n+\/\/ restore fields of an eliminated inline type array\n+void Deoptimization::reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool skip_internal, TRAPS) {\n+  InlineKlass* vk = vak->element_klass();\n+  assert(vk->flatten_array(), \"should only be used for flattened inline type arrays\");\n+  \/\/ Adjust offset to omit oop header\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE) - InlineKlass::cast(vk)->first_field_offset();\n+  \/\/ Initialize all elements of the flattened inline type array\n+  for (int i = 0; i < sv->field_size(); i++) {\n+    ScopeValue* val = sv->field_at(i);\n+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));\n+    reassign_fields_by_klass(vk, fr, reg_map, val->as_ObjectValue(), 0, (oop)obj, skip_internal, offset, CHECK);\n+  }\n+}\n+\n@@ -1388,1 +1473,1 @@\n-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal) {\n+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal, TRAPS) {\n@@ -1413,1 +1498,4 @@\n-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal);\n+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal, 0, CHECK);\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      reassign_flat_array_elements(fr, reg_map, sv, (flatArrayOop) obj(), vak, skip_internal, CHECK);\n@@ -1479,1 +1567,0 @@\n-\n@@ -1483,1 +1570,3 @@\n-    Handle obj = sv->value();\n+    print_object(k, sv->value(), realloc_failures);\n+  }\n+}\n@@ -1485,9 +1574,10 @@\n-    tty->print(\"     object <\" INTPTR_FORMAT \"> of type \", p2i(sv->value()()));\n-    k->print_value();\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n-    if (obj.is_null()) {\n-      tty->print(\" allocation failed\");\n-    } else {\n-      tty->print(\" allocated (%d bytes)\", obj->size() * HeapWordSize);\n-    }\n-    tty->cr();\n+void Deoptimization::print_object(Klass* k, Handle obj, bool realloc_failures) {\n+  tty->print(\"     object <\" INTPTR_FORMAT \"> of type \", p2i(obj()));\n+  k->print_value();\n+  assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+  if (obj.is_null()) {\n+    tty->print(\" allocation failed\");\n+  } else {\n+    tty->print(\" allocated (%d bytes)\", obj->size() * HeapWordSize);\n+  }\n+  tty->cr();\n@@ -1495,3 +1585,2 @@\n-    if (Verbose && !obj.is_null()) {\n-      k->oop_print_on(obj(), tty);\n-    }\n+  if (Verbose && !obj.is_null()) {\n+    k->oop_print_on(obj(), tty);\n@@ -1709,1 +1798,1 @@\n-  \/\/ Deoptimize only if the frame comes from compile code.\n+  \/\/ Deoptimize only if the frame comes from compiled code.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":120,"deletions":31,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -184,0 +184,1 @@\n+  static bool realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS);\n@@ -186,1 +187,2 @@\n-  static void reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal);\n+  static void reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool skip_internal, TRAPS);\n+  static void reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal, TRAPS);\n@@ -190,1 +192,4 @@\n-  NOT_PRODUCT(static void print_objects(GrowableArray<ScopeValue*>* objects, bool realloc_failures);)\n+#ifndef PRODUCT\n+  static void print_objects(GrowableArray<ScopeValue*>* objects, bool realloc_failures);\n+  static void print_object(Klass* k, Handle obj, bool realloc_failures);\n+#endif\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -63,1 +64,1 @@\n-  return is_final() && (is_static() || ik->is_hidden() || ik->is_record());\n+  return is_final() && (is_static() || ik->is_hidden() || ik->is_record() || ik->is_inline_klass());\n@@ -156,1 +157,3 @@\n-  print_on(st);\n+  if (ft != T_INLINE_TYPE) {\n+    print_on(st);\n+  }\n@@ -195,7 +198,10 @@\n-    case T_ARRAY:\n-      st->print(\" \");\n-      NOT_LP64(as_int = obj->int_field(offset()));\n-      if (obj->obj_field(offset()) != NULL) {\n-        obj->obj_field(offset())->print_value_on(st);\n-      } else {\n-        st->print(\"NULL\");\n+    case T_INLINE_TYPE:\n+      if (is_inlined()) {\n+        \/\/ Print fields of inlined fields (recursively)\n+        InlineKlass* vk = InlineKlass::cast(field_holder()->get_inline_type_field_klass(index()));\n+        int field_offset = offset() - vk->first_field_offset();\n+        obj = (oop)(cast_from_oop<address>(obj) + field_offset);\n+        st->print_cr(\"Inline type field inlined '%s':\", vk->name()->as_C_string());\n+        FieldPrinter print_field(st, obj);\n+        vk->do_nonstatic_fields(&print_field);\n+        return; \/\/ Do not print underlying representation\n@@ -203,1 +209,2 @@\n-      break;\n+      \/\/ inline type field not inlined, fall through\n+    case T_ARRAY:\n@@ -230,0 +237,1 @@\n+  st->cr();\n","filename":"src\/hotspot\/share\/runtime\/fieldDescriptor.cpp","additions":18,"deletions":10,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -84,1 +84,4 @@\n-#endif \/\/ SHARE_RUNTIME_FIELDDESCRIPTOR_INLINE_HPP\n+inline bool fieldDescriptor::is_inlined()  const  { return field()->is_inlined(); }\n+inline bool fieldDescriptor::is_inline_type() const { return Signature::basic_type(field()->signature(_cp())) == T_INLINE_TYPE; }\n+\n+#endif \/\/ SHARE_RUNTIME_FIELDDESCRIPTOR_INLINE_HPP\n","filename":"src\/hotspot\/share\/runtime\/fieldDescriptor.inline.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -55,0 +56,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -289,0 +293,19 @@\n+\n+#ifdef COMPILER1\n+  if (cm->is_compiled_by_c1() && cm->method()->has_scalarized_args() &&\n+      pc() < cm->verified_inline_entry_point()) {\n+    \/\/ The VEP and VIEP(RO) of C1-compiled methods call into the runtime to buffer scalarized value\n+    \/\/ type args. We can't deoptimize at that point because the buffers have not yet been initialized.\n+    \/\/ Also, if the method is synchronized, we first need to acquire the lock.\n+    \/\/ Don't patch the return pc to delay deoptimization until we enter the method body (the check\n+    \/\/ addedin LIRGenerator::do_Base will detect the pending deoptimization by checking the original_pc).\n+#ifdef ASSERT\n+    NativeCall* call = nativeCall_before(this->pc());\n+    address dest = call->destination();\n+    assert(dest == Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id) ||\n+           dest == Runtime1::entry_for(Runtime1::buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+    return;\n+  }\n+#endif\n+\n@@ -686,1 +709,1 @@\n-                          OopClosure* f) {\n+                          OopClosure* f, BufferedValueClosure* bvt_f) {\n@@ -698,1 +721,3 @@\n-      _f->do_oop(addr);\n+      if (_f != NULL) {\n+        _f->do_oop(addr);\n+      }\n@@ -710,1 +735,3 @@\n-        _f->do_oop(addr);\n+        if (_f != NULL) {\n+          _f->do_oop(addr);\n+        }\n@@ -885,1 +912,1 @@\n-  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f);\n+  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f, NULL);\n@@ -897,0 +924,17 @@\n+void frame::buffered_values_interpreted_do(BufferedValueClosure* f) {\n+  assert(is_interpreted_frame(), \"Not an interpreted frame\");\n+  Thread *thread = Thread::current();\n+  methodHandle m (thread, interpreter_frame_method());\n+  jint      bci = interpreter_frame_bci();\n+\n+  assert(m->is_method(), \"checking frame value\");\n+  assert(!m->is_native() && bci >= 0 && bci < m->code_size(),\n+         \"invalid bci value\");\n+\n+  InterpreterFrameClosure blk(this, m->max_locals(), m->max_stack(), NULL, f);\n+\n+  \/\/ process locals & expression stack\n+  InterpreterOopMap mask;\n+  m->mask_for(bci, &mask);\n+  mask.iterate_oop(&blk);\n+}\n@@ -944,0 +988,1 @@\n+    assert(_offset < _arg_size, \"out of bounds\");\n@@ -961,5 +1006,1 @@\n-    _arg_size  = ArgumentSizeComputer(signature).size() + (has_receiver ? 1 : 0) + (has_appendix ? 1 : 0);\n-\n-    int arg_size;\n-    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &arg_size);\n-    assert(arg_size == _arg_size, \"wrong arg size\");\n+    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &_arg_size);\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":50,"deletions":9,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -372,0 +372,1 @@\n+  void buffered_values_interpreted_do(BufferedValueClosure* f);\n","filename":"src\/hotspot\/share\/runtime\/frame.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -805,0 +805,18 @@\n+  notproduct(bool, PrintInlineLayout, false,                                \\\n+          \"Print field layout for each inline type\")                        \\\n+                                                                            \\\n+  notproduct(bool, PrintFlatArrayLayout, false,                             \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxSize, -1,                                \\\n+          \"Max size for flattening inline array elements, <0 no limit\")     \\\n+                                                                            \\\n+  product(intx, InlineFieldMaxFlatSize, 128,                                \\\n+          \"Max size for flattening inline type fields, <0 no limit\")        \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  product(bool, InlineArrayAtomicAccess, false,                             \\\n+          \"Atomic inline array accesses by-default, for all inline arrays\") \\\n+                                                                            \\\n@@ -820,2 +838,2 @@\n-  product(bool, UseBiasedLocking, false,                                    \\\n-          \"(Deprecated) Enable biased locking in JVM\")                      \\\n+  product(bool, UseBiasedLocking, false,                                     \\\n+          \"(Deprecated) Enable biased locking in JVM (completely disabled by Valhalla)\") \\\n@@ -2082,0 +2100,23 @@\n+  product(bool, EnableValhalla, true,                                       \\\n+          \"Enable experimental Valhalla features\")                          \\\n+                                                                            \\\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressInlineTypeReturnedAsFields, false,                    \\\n+          \"Stress return of fields instead of an inline type reference\")    \\\n+                                                                            \\\n+  develop(bool, ScalarizeInlineTypes, true,                                 \\\n+          \"Scalarize inline types in compiled code\")                        \\\n+                                                                            \\\n+  product(bool, UseArrayMarkWordCheck, NOT_LP64(false) LP64_ONLY(true),     \\\n+          \"Use bits in the mark word to check for flat\/null-free arrays\")   \\\n+                                                                            \\\n+  product(ccstrlist, ForceNonTearable, \"\", DIAGNOSTIC,                      \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":43,"deletions":2,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"oops\/inlineKlass.hpp\"\n+#include \"runtime\/atomic.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/handles.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -159,4 +160,4 @@\n-    case T_BOOLEAN: \/\/ fall through\n-    case T_CHAR   : \/\/ fall through\n-    case T_SHORT  : \/\/ fall through\n-    case T_INT    : \/\/ fall through\n+    case T_BOOLEAN  : \/\/ fall through\n+    case T_CHAR     : \/\/ fall through\n+    case T_SHORT    : \/\/ fall through\n+    case T_INT      : \/\/ fall through\n@@ -164,2 +165,3 @@\n-    case T_OBJECT : \/\/ fall through\n-    case T_ARRAY  : \/\/ fall through\n+    case T_OBJECT   : \/\/ fall through\n+    case T_ARRAY    : \/\/ fall through\n+    case T_INLINE_TYPE: \/\/ fall through\n@@ -167,5 +169,5 @@\n-    case T_BYTE   : \/\/ fall through\n-    case T_VOID   : return T_INT;\n-    case T_LONG   : return T_LONG;\n-    case T_FLOAT  : return T_FLOAT;\n-    case T_DOUBLE : return T_DOUBLE;\n+    case T_BYTE     : \/\/ fall through\n+    case T_VOID     : return T_INT;\n+    case T_LONG     : return T_LONG;\n+    case T_FLOAT    : return T_FLOAT;\n+    case T_DOUBLE   : return T_DOUBLE;\n@@ -173,2 +175,3 @@\n-    case T_ARRAY  : \/\/ fall through\n-    case T_OBJECT:  return T_OBJECT;\n+    case T_ARRAY    : \/\/ fall through\n+    case T_OBJECT   : return T_OBJECT;\n+    case T_INLINE_TYPE: return T_INLINE_TYPE;\n@@ -302,0 +305,13 @@\n+\n+  \/\/ Special case for factory methods\n+  if (!constructor_signature->is_void_method_signature()) {\n+    assert(klass->is_inline_klass(), \"inline classes must use factory methods\");\n+    JavaValue factory_result(T_OBJECT);\n+    JavaCalls::call_static(&factory_result, klass,\n+                           vmSymbols::object_initializer_name(),\n+                           constructor_signature, args, CHECK_NH);\n+    return Handle(THREAD, (oop)factory_result.get_jobject());\n+  }\n+\n+  \/\/ main branch of code creates a non-inline object:\n+  assert(!klass->is_inline_klass(), \"classic constructors are only for non-inline classes\");\n@@ -404,0 +420,12 @@\n+  jobject value_buffer = NULL;\n+  if (InlineTypeReturnedAsFields && result->get_type() == T_INLINE_TYPE) {\n+    \/\/ Pre allocate a buffered inline type in case the result is returned\n+    \/\/ flattened by compiled code\n+    InlineKlass* vk = method->returned_inline_type(thread);\n+    if (vk->can_be_returned_as_fields()) {\n+      oop instance = vk->allocate_instance(CHECK);\n+      value_buffer = JNIHandles::make_local(thread, instance);\n+      result->set_jobject(value_buffer);\n+    }\n+  }\n+\n@@ -454,0 +482,1 @@\n+    JNIHandles::destroy_local(value_buffer);\n@@ -590,0 +619,1 @@\n+    case T_INLINE_TYPE:\n@@ -602,1 +632,1 @@\n-  if (is_reference_type(return_type)) return_type = T_OBJECT;\n+  if (return_type == T_ARRAY) return_type = T_OBJECT;\n","filename":"src\/hotspot\/share\/runtime\/javaCalls.cpp","additions":44,"deletions":14,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"classfile\/vmSymbols.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"runtime\/javaCalls.hpp\"\n@@ -307,0 +309,37 @@\n+bool JNIHandles::is_same_object(jobject handle1, jobject handle2) {\n+  oop obj1 = resolve_no_keepalive(handle1);\n+  oop obj2 = resolve_no_keepalive(handle2);\n+\n+  bool ret = obj1 == obj2;\n+\n+  if (EnableValhalla) {\n+    if (!ret && obj1 != NULL && obj2 != NULL && obj1->klass() == obj2->klass() && obj1->klass()->is_inline_klass()) {\n+      \/\/ The two references are different, they are not null and they are both inline types,\n+      \/\/ a full substitutability test is required, calling ValueBootstrapMethods.isSubstitutable()\n+      \/\/ (similarly to InterpreterRuntime::is_substitutable)\n+      Thread* THREAD = Thread::current();\n+      Handle ha(THREAD, obj1);\n+      Handle hb(THREAD, obj2);\n+      JavaValue result(T_BOOLEAN);\n+      JavaCallArguments args;\n+      args.push_oop(ha);\n+      args.push_oop(hb);\n+      methodHandle method(THREAD, Universe::is_substitutable_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        \/\/ Something really bad happened because isSubstitutable() should not throw exceptions\n+        \/\/ If it is an error, just let it propagate\n+        \/\/ If it is an exception, wrap it into an InternalError\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in substitutability test\", e, false);\n+        }\n+      }\n+      ret = result.get_jboolean();\n+    }\n+  }\n+\n+  return ret;\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/jniHandles.cpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -55,0 +56,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -350,1 +352,5 @@\n-    return oopFactory::new_objArray(k, length, THREAD);\n+    if (k->is_inline_klass()) {\n+      return oopFactory::new_flatArray(k, length, THREAD);\n+    } else {\n+      return oopFactory::new_objArray(k, length, THREAD);\n+    }\n@@ -792,3 +798,0 @@\n-  if (log_is_enabled(Debug, class, resolve)) {\n-    trace_class_resolution(nt);\n-  }\n@@ -801,2 +804,1 @@\n-  assert(!method()->is_initializer() ||\n-         (for_constant_pool_access && method()->is_static()),\n+  assert(!method()->name()->starts_with('<') || for_constant_pool_access,\n@@ -851,1 +853,3 @@\n-  assert(method()->is_initializer(), \"should call new_method instead\");\n+  assert(method()->is_object_constructor() ||\n+         method()->is_static_init_factory(),\n+         \"should call new_method instead\");\n@@ -904,1 +908,5 @@\n-  java_lang_reflect_Field::set_modifiers(rh(), fd->access_flags().as_int() & JVM_RECOGNIZED_FIELD_MODIFIERS);\n+  int modifiers = fd->access_flags().as_int() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n+  if (fd->is_inlined()) {\n+    modifiers |= JVM_ACC_FIELD_INLINED;\n+  }\n+  java_lang_reflect_Field::set_modifiers(rh(), modifiers);\n@@ -1179,0 +1187,2 @@\n+  } else if (java_lang_Class::as_Klass(return_type_mirror)->is_inline_klass()) {\n+    rtype = T_INLINE_TYPE;\n@@ -1213,0 +1223,16 @@\n+\n+  \/\/ Special case for factory methods\n+  if (!method->signature()->is_void_method_signature()) {\n+    assert(klass->is_inline_klass(), \"inline classes must use factory methods\");\n+    Handle no_receiver; \/\/ null instead of receiver\n+    BasicType rtype;\n+    if (klass->is_hidden()) {\n+      rtype = T_OBJECT;\n+    } else {\n+      rtype = T_INLINE_TYPE;\n+    }\n+    return invoke(klass, method, no_receiver, override, ptypes, rtype, args, false, CHECK_NULL);\n+  }\n+\n+  \/\/ main branch of code creates a non-inline object:\n+  assert(!klass->is_inline_klass(), \"classic constructors are only for non-inline classes\");\n","filename":"src\/hotspot\/share\/runtime\/reflection.cpp","additions":34,"deletions":8,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -925,0 +926,1 @@\n+    ResourceMark rm;\n@@ -926,2 +928,25 @@\n-    bool return_oop = nm->method()->is_returning_oop();\n-    Handle return_value;\n+    Method* method = nm->method();\n+    bool return_oop = method->is_returning_oop();\n+\n+    GrowableArray<Handle> return_values;\n+    InlineKlass* vk = NULL;\n+\n+    if (return_oop && InlineTypeReturnedAsFields) {\n+      SignatureStream ss(method->signature());\n+      while (!ss.at_return_type()) {\n+        ss.next();\n+      }\n+      if (ss.type() == T_INLINE_TYPE) {\n+        \/\/ Check if inline type is returned as fields\n+        vk = InlineKlass::returned_inline_klass(map);\n+        if (vk != NULL) {\n+          \/\/ We're at a safepoint at the return of a method that returns\n+          \/\/ multiple values. We must make sure we preserve the oop values\n+          \/\/ across the safepoint.\n+          assert(vk == method->returned_inline_type(thread()), \"bad inline klass\");\n+          vk->save_oop_fields(map, return_values);\n+          return_oop = false;\n+        }\n+      }\n+    }\n+\n@@ -934,1 +959,1 @@\n-      return_value = Handle(self, result);\n+      return_values.push(Handle(self, result));\n@@ -948,1 +973,4 @@\n-      caller_fr.set_saved_oop_result(&map, return_value());\n+      assert(return_values.length() == 1, \"only one return value\");\n+      caller_fr.set_saved_oop_result(&map, return_values.pop()());\n+    } else if (vk != NULL) {\n+      vk->restore_oop_results(map, return_values);\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -50,0 +51,2 @@\n+#include \"oops\/access.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -53,0 +56,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -54,0 +58,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -91,1 +96,0 @@\n-address             SharedRuntime::_resolve_static_call_entry;\n@@ -111,1 +115,0 @@\n-  _resolve_static_call_entry           = _resolve_static_call_blob->entry_point();\n@@ -1089,0 +1092,15 @@\n+  \/\/ Substitutability test implementation piggy backs on static call resolution\n+  Bytecodes::Code code = caller->java_code_at(bci);\n+  if (code == Bytecodes::_if_acmpeq || code == Bytecodes::_if_acmpne) {\n+    bc = Bytecodes::_invokestatic;\n+    methodHandle attached_method(THREAD, extract_attached_method(vfst));\n+    assert(attached_method.not_null(), \"must have attached method\");\n+    vmClasses::ValueBootstrapMethods_klass()->initialize(CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, false, CHECK_NH);\n+#ifdef ASSERT\n+    Method* is_subst = vmClasses::ValueBootstrapMethods_klass()->find_method(vmSymbols::isSubstitutable_name(), vmSymbols::object_object_boolean_signature());\n+    assert(callinfo.selected_method() == is_subst, \"must be isSubstitutable method\");\n+#endif\n+    return receiver;\n+  }\n+\n@@ -1124,0 +1142,6 @@\n+    } else {\n+      assert(attached_method->has_scalarized_args(), \"invalid use of attached method\");\n+      if (!attached_method->method_holder()->is_inline_klass()) {\n+        \/\/ Ignore the attached method in this case to not confuse below code\n+        attached_method = methodHandle(thread, NULL);\n+      }\n@@ -1132,0 +1156,1 @@\n+  bool check_null_and_abstract = true;\n@@ -1141,0 +1166,1 @@\n+    bool caller_is_c1 = false;\n@@ -1142,2 +1168,7 @@\n-    if (attached_method.is_null()) {\n-      Method* callee = bytecode.static_target(CHECK_NH);\n+    if (callerFrame.is_compiled_frame() && !callerFrame.is_deoptimized_frame()) {\n+      caller_is_c1 = callerFrame.cb()->is_compiled_by_c1();\n+    }\n+\n+    Method* callee = attached_method();\n+    if (callee == NULL) {\n+      callee = bytecode.static_target(CHECK_NH);\n@@ -1148,6 +1179,15 @@\n-\n-    \/\/ Retrieve from a compiled argument list\n-    receiver = Handle(THREAD, callerFrame.retrieve_receiver(&reg_map2));\n-\n-    if (receiver.is_null()) {\n-      THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+    if (!caller_is_c1 && callee->has_scalarized_args() && callee->method_holder()->is_inline_klass() &&\n+        InlineKlass::cast(callee->method_holder())->can_be_passed_as_fields()) {\n+      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n+      \/\/ Resolve the call without receiver null checking.\n+      assert(attached_method.not_null() && !attached_method->is_abstract(), \"must have non-abstract attached method\");\n+      if (bc == Bytecodes::_invokeinterface) {\n+        bc = Bytecodes::_invokevirtual; \/\/ C2 optimistically replaces interface calls by virtual calls\n+      }\n+      check_null_and_abstract = false;\n+    } else {\n+      \/\/ Retrieve from a compiled argument list\n+      receiver = Handle(THREAD, callerFrame.retrieve_receiver(&reg_map2));\n+      if (receiver.is_null()) {\n+        THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+      }\n@@ -1160,1 +1200,1 @@\n-    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);\n@@ -1169,1 +1209,1 @@\n-  if (has_receiver) {\n+  if (has_receiver && check_null_and_abstract) {\n@@ -1228,1 +1268,2 @@\n-                                           bool is_optimized, TRAPS) {\n+                                           bool is_optimized,\n+                                           bool* caller_is_c1, TRAPS) {\n@@ -1230,1 +1271,1 @@\n-  callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, THREAD);\n+  callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, caller_is_c1, THREAD);\n@@ -1247,1 +1288,1 @@\n-      callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, THREAD);\n+      callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, caller_is_c1, THREAD);\n@@ -1278,0 +1319,1 @@\n+  bool caller_is_c1 = caller_nm->is_compiled_by_c1();\n@@ -1280,1 +1322,9 @@\n-    assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, \"sanity check\");\n+    Klass* receiver_klass = NULL;\n+    if (!caller_is_c1 && callee_method->has_scalarized_args() && callee_method->method_holder()->is_inline_klass() &&\n+        InlineKlass::cast(callee_method->method_holder())->can_be_passed_as_fields()) {\n+      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n+      receiver_klass = callee_method->method_holder();\n+    } else {\n+      assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, \"sanity check\");\n+      receiver_klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver->klass();\n+    }\n@@ -1282,3 +1332,2 @@\n-    Klass* klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver->klass();\n-    CompiledIC::compute_monomorphic_entry(callee_method, klass,\n-                     is_optimized, static_bound, is_nmethod, virtual_call_info,\n+    CompiledIC::compute_monomorphic_entry(callee_method, receiver_klass,\n+                     is_optimized, static_bound, is_nmethod, caller_is_c1, virtual_call_info,\n@@ -1288,1 +1337,1 @@\n-    CompiledStaticCall::compute_entry(callee_method, is_nmethod, static_call_info);\n+    CompiledStaticCall::compute_entry(callee_method, caller_nm, static_call_info);\n@@ -1340,1 +1389,2 @@\n-                                               bool is_optimized, TRAPS) {\n+                                               bool is_optimized,\n+                                               bool* caller_is_c1, TRAPS) {\n@@ -1349,0 +1399,1 @@\n+  *caller_is_c1 = caller_nm->is_compiled_by_c1();\n@@ -1450,0 +1501,2 @@\n+  bool is_optimized = false;\n+  bool caller_is_c1 = false;\n@@ -1451,1 +1504,1 @@\n-    callee_method = SharedRuntime::handle_ic_miss_helper(thread, CHECK_NULL);\n+    callee_method = SharedRuntime::handle_ic_miss_helper(thread, is_optimized, caller_is_c1, CHECK_NULL);\n@@ -1456,2 +1509,1 @@\n-  assert(callee_method->verified_code_entry() != NULL, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  return entry_for_handle_wrong_method(callee_method, false, is_optimized, caller_is_c1);\n@@ -1500,0 +1552,3 @@\n+  bool is_static_call = false;\n+  bool is_optimized = false;\n+  bool caller_is_c1 = false;\n@@ -1502,1 +1557,1 @@\n-    callee_method = SharedRuntime::reresolve_call_site(thread, CHECK_NULL);\n+    callee_method = SharedRuntime::reresolve_call_site(thread, is_static_call, is_optimized, caller_is_c1, CHECK_NULL);\n@@ -1506,2 +1561,1 @@\n-  assert(callee_method->verified_code_entry() != NULL, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  return entry_for_handle_wrong_method(callee_method, is_static_call, is_optimized, caller_is_c1);\n@@ -1545,0 +1599,1 @@\n+  bool caller_is_c1;\n@@ -1546,1 +1601,1 @@\n-    callee_method = SharedRuntime::resolve_helper(thread, false, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(thread, false, false, &caller_is_c1, CHECK_NULL);\n@@ -1550,2 +1605,4 @@\n-  assert(callee_method->verified_code_entry() != NULL, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  address entry = caller_is_c1 ?\n+    callee_method->verified_inline_code_entry() : callee_method->verified_code_entry();\n+  assert(entry != NULL, \"Jump to zero!\");\n+  return entry;\n@@ -1558,0 +1615,1 @@\n+  bool caller_is_c1;\n@@ -1559,1 +1617,1 @@\n-    callee_method = SharedRuntime::resolve_helper(thread, true, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(thread, true, false, &caller_is_c1, CHECK_NULL);\n@@ -1563,2 +1621,4 @@\n-  assert(callee_method->verified_code_entry() != NULL, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  address entry = caller_is_c1 ?\n+    callee_method->verified_inline_code_entry() : callee_method->verified_inline_ro_code_entry();\n+  assert(entry != NULL, \"Jump to zero!\");\n+  return entry;\n@@ -1572,0 +1632,1 @@\n+  bool caller_is_c1;\n@@ -1573,1 +1634,1 @@\n-    callee_method = SharedRuntime::resolve_helper(thread, true, true, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(thread, true, true, &caller_is_c1, CHECK_NULL);\n@@ -1577,2 +1638,4 @@\n-  assert(callee_method->verified_code_entry() != NULL, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  address entry = caller_is_c1 ?\n+    callee_method->verified_inline_code_entry() : callee_method->verified_code_entry();\n+  assert(entry != NULL, \"Jump to zero!\");\n+  return entry;\n@@ -1589,1 +1652,1 @@\n-                                                   bool& needs_ic_stub_refill, TRAPS) {\n+                                                   bool& needs_ic_stub_refill, bool& is_optimized, bool caller_is_c1, TRAPS) {\n@@ -1600,0 +1663,1 @@\n+    is_optimized = true;\n@@ -1637,0 +1701,1 @@\n+                                            caller_nm->is_compiled_by_c1(),\n@@ -1645,1 +1710,1 @@\n-    bool successful = inline_cache->set_to_megamorphic(&call_info, bc, needs_ic_stub_refill, CHECK_false);\n+    bool successful = inline_cache->set_to_megamorphic(&call_info, bc, needs_ic_stub_refill, caller_is_c1, CHECK_false);\n@@ -1661,1 +1726,1 @@\n-methodHandle SharedRuntime::handle_ic_miss_helper(JavaThread *thread, TRAPS) {\n+methodHandle SharedRuntime::handle_ic_miss_helper(JavaThread *thread, bool& is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1681,1 +1746,3 @@\n-    methodHandle callee_method = SharedRuntime::reresolve_call_site(thread, CHECK_(methodHandle()));\n+    bool is_static_call = false;\n+    methodHandle callee_method = SharedRuntime::reresolve_call_site(thread, is_static_call, is_optimized, caller_is_c1, CHECK_(methodHandle()));\n+    assert(!is_static_call, \"IC miss at static call?\");\n@@ -1731,0 +1798,1 @@\n+  caller_is_c1 = caller_nm->is_compiled_by_c1();\n@@ -1736,1 +1804,1 @@\n-                                                     bc, call_info, needs_ic_stub_refill, CHECK_(methodHandle()));\n+                                                     bc, call_info, needs_ic_stub_refill, is_optimized, caller_is_c1, CHECK_(methodHandle()));\n@@ -1768,1 +1836,1 @@\n-methodHandle SharedRuntime::reresolve_call_site(JavaThread *thread, TRAPS) {\n+methodHandle SharedRuntime::reresolve_call_site(JavaThread *thread, bool& is_static_call, bool& is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1784,1 +1852,1 @@\n-    bool is_static_call = false;\n+    caller_is_c1 = caller_nm->is_compiled_by_c1();\n@@ -1829,0 +1897,1 @@\n+          is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);\n@@ -1854,1 +1923,0 @@\n-\n@@ -1948,2 +2016,0 @@\n-  address entry_point = moop->from_compiled_entry_no_trampoline();\n-\n@@ -1961,1 +2027,5 @@\n-  if (cb == NULL || !cb->is_compiled() || entry_point == moop->get_c2i_entry()) {\n+  if (cb == NULL || !cb->is_compiled()) {\n+    return;\n+  }\n+  address entry_point = moop->from_compiled_entry_no_trampoline(cb->is_compiled_by_c1());\n+  if (entry_point == moop->get_c2i_entry()) {\n@@ -2326,1 +2396,1 @@\n-  static int adapter_encoding(BasicType in) {\n+  static BasicType adapter_encoding(BasicType in) {\n@@ -2332,1 +2402,1 @@\n-        \/\/ There are all promoted to T_INT in the calling convention\n+        \/\/ They are all promoted to T_INT in the calling convention\n@@ -2359,1 +2429,1 @@\n-  AdapterFingerPrint(int total_args_passed, BasicType* sig_bt) {\n+  AdapterFingerPrint(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n@@ -2362,0 +2432,1 @@\n+    int total_args_passed = (sig != NULL) ? sig->length() : 0;\n@@ -2379,0 +2450,2 @@\n+    BasicType prev_bt = T_ILLEGAL;\n+    int vt_count = 0;\n@@ -2382,5 +2455,26 @@\n-        int bt = ((sig_index < total_args_passed)\n-                  ? adapter_encoding(sig_bt[sig_index++])\n-                  : 0);\n-        assert((bt & _basic_type_mask) == bt, \"must fit in 4 bits\");\n-        value = (value << _basic_type_bits) | bt;\n+        BasicType bt = T_ILLEGAL;\n+        if (sig_index < total_args_passed) {\n+          bt = sig->at(sig_index++)._bt;\n+          if (bt == T_INLINE_TYPE) {\n+            \/\/ Found start of inline type in signature\n+            assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+            if (sig_index == 1 && has_ro_adapter) {\n+              \/\/ With a ro_adapter, replace receiver inline type delimiter by T_VOID to prevent matching\n+              \/\/ with other adapters that have the same inline type as first argument and no receiver.\n+              bt = T_VOID;\n+            }\n+            vt_count++;\n+          } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+            \/\/ Found end of inline type in signature\n+            assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+            vt_count--;\n+            assert(vt_count >= 0, \"invalid vt_count\");\n+          } else if (vt_count == 0) {\n+            \/\/ Widen fields that are not part of a scalarized inline type argument\n+            bt = adapter_encoding(bt);\n+          }\n+          prev_bt = bt;\n+        }\n+        int bt_val = (bt == T_ILLEGAL) ? 0 : bt;\n+        assert((bt_val & _basic_type_mask) == bt_val, \"must fit in 4 bits\");\n+        value = (value << _basic_type_bits) | bt_val;\n@@ -2390,0 +2484,1 @@\n+    assert(vt_count == 0, \"invalid vt_count\");\n@@ -2475,1 +2570,3 @@\n-  AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry, address c2i_unverified_entry, address c2i_no_clinit_check_entry) {\n+  AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry,\n+                                 address c2i_inline_entry, address c2i_inline_ro_entry,\n+                                 address c2i_unverified_entry, address c2i_unverified_inline_entry, address c2i_no_clinit_check_entry) {\n@@ -2477,1 +2574,2 @@\n-    entry->init(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+    entry->init(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry,\n+                c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -2493,1 +2591,1 @@\n-  AdapterHandlerEntry* lookup(int total_args_passed, BasicType* sig_bt) {\n+  AdapterHandlerEntry* lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n@@ -2495,1 +2593,1 @@\n-    AdapterFingerPrint fp(total_args_passed, sig_bt);\n+    AdapterFingerPrint fp(sig, has_ro_adapter);\n@@ -2591,1 +2689,1 @@\n-const int AdapterHandlerLibrary_size = 16*K;\n+const int AdapterHandlerLibrary_size = 32*K;\n@@ -2615,1 +2713,1 @@\n-  _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(0, NULL),\n+  _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),\n@@ -2617,0 +2715,1 @@\n+                                                              wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,\n@@ -2623,0 +2722,2 @@\n+                                                      address c2i_inline_entry,\n+                                                      address c2i_inline_ro_entry,\n@@ -2624,0 +2725,1 @@\n+                                                      address c2i_unverified_inline_entry,\n@@ -2625,1 +2727,16 @@\n-  return _adapters->new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+  return _adapters->new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry,\n+                              c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n+}\n+\n+static void generate_trampoline(address trampoline, address destination) {\n+  if (*(int*)trampoline == 0) {\n+    CodeBuffer buffer(trampoline, (int)SharedRuntime::trampoline_size());\n+    MacroAssembler _masm(&buffer);\n+    SharedRuntime::generate_trampoline(&_masm, destination);\n+    assert(*(int*)trampoline != 0, \"Instruction(s) for trampoline must not be encoded as zeros.\");\n+      _masm.flush();\n+\n+    if (PrintInterpreter) {\n+      Disassembler::decode(buffer.insts_begin(), buffer.insts_end());\n+    }\n+  }\n@@ -2636,7 +2753,15 @@\n-    address trampoline = method->from_compiled_entry();\n-    if (*(int*)trampoline == 0) {\n-      CodeBuffer buffer(trampoline, (int)SharedRuntime::trampoline_size());\n-      MacroAssembler _masm(&buffer);\n-      SharedRuntime::generate_trampoline(&_masm, entry->get_c2i_entry());\n-      assert(*(int*)trampoline != 0, \"Instruction(s) for trampoline must not be encoded as zeros.\");\n-      _masm.flush();\n+    generate_trampoline(method->from_compiled_entry(),           entry->get_c2i_entry());\n+    generate_trampoline(method->from_compiled_inline_ro_entry(), entry->get_c2i_inline_ro_entry());\n+    generate_trampoline(method->from_compiled_inline_entry(),    entry->get_c2i_inline_entry());\n+  }\n+\n+  return entry;\n+}\n+\n+\n+CompiledEntrySignature::CompiledEntrySignature(Method* method) :\n+  _method(method), _num_inline_args(0), _has_inline_recv(false),\n+  _sig_cc(NULL), _sig_cc_ro(NULL), _regs(NULL), _regs_cc(NULL), _regs_cc_ro(NULL),\n+  _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),\n+  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _has_scalarized_args(false) {\n+  _sig = new GrowableArray<SigEntry>(method->size_of_parameters());\n@@ -2644,2 +2769,19 @@\n-      if (PrintInterpreter) {\n-        Disassembler::decode(buffer.insts_begin(), buffer.insts_end());\n+}\n+\n+int CompiledEntrySignature::compute_scalarized_cc(GrowableArray<SigEntry>*& sig_cc, VMRegPair*& regs_cc, bool scalar_receiver) {\n+  InstanceKlass* holder = _method->method_holder();\n+  sig_cc = new GrowableArray<SigEntry>(_method->size_of_parameters());\n+  if (!_method->is_static()) {\n+    if (holder->is_inline_klass() && scalar_receiver && InlineKlass::cast(holder)->can_be_passed_as_fields()) {\n+      sig_cc->appendAll(InlineKlass::cast(holder)->extended_sig());\n+    } else {\n+      SigEntry::add_entry(sig_cc, T_OBJECT, holder->name());\n+    }\n+  }\n+  for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {\n+    if (ss.type() == T_INLINE_TYPE) {\n+      InlineKlass* vk = ss.as_inline_klass(holder);\n+      if (vk->can_be_passed_as_fields()) {\n+        sig_cc->appendAll(vk->extended_sig());\n+      } else {\n+        SigEntry::add_entry(sig_cc, T_OBJECT, ss.as_symbol());\n@@ -2647,0 +2789,2 @@\n+    } else {\n+      SigEntry::add_entry(sig_cc, ss.type(), ss.as_symbol());\n@@ -2649,0 +2793,3 @@\n+  regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, sig_cc->length() + 2);\n+  return SharedRuntime::java_calling_convention(sig_cc, regs_cc);\n+}\n@@ -2650,1 +2797,103 @@\n-  return entry;\n+\/\/ See if we can save space by sharing the same entry for VIEP and VIEP(RO),\n+\/\/ or the same entry for VEP and VIEP(RO).\n+CodeOffsets::Entries CompiledEntrySignature::c1_inline_ro_entry_type() const {\n+  if (!has_scalarized_args()) {\n+    \/\/ VEP\/VIEP\/VIEP(RO) all share the same entry. There's no packing.\n+    return CodeOffsets::Verified_Entry;\n+  }\n+  if (_method->is_static()) {\n+    \/\/ Static methods don't need VIEP(RO)\n+    return CodeOffsets::Verified_Entry;\n+  }\n+\n+  if (has_inline_recv()) {\n+    if (num_inline_args() == 1) {\n+      \/\/ Share same entry for VIEP and VIEP(RO).\n+      \/\/ This is quite common: we have an instance method in an InlineKlass that has\n+      \/\/ no inline type args other than <this>.\n+      return CodeOffsets::Verified_Inline_Entry;\n+    } else {\n+      assert(num_inline_args() > 1, \"must be\");\n+      \/\/ No sharing:\n+      \/\/   VIEP(RO) -- <this> is passed as object\n+      \/\/   VEP      -- <this> is passed as fields\n+      return CodeOffsets::Verified_Inline_Entry_RO;\n+    }\n+  }\n+\n+  \/\/ Either a static method, or <this> is not an inline type\n+  if (args_on_stack_cc() != args_on_stack_cc_ro()) {\n+    \/\/ No sharing:\n+    \/\/ Some arguments are passed on the stack, and we have inserted reserved entries\n+    \/\/ into the VEP, but we never insert reserved entries into the VIEP(RO).\n+    return CodeOffsets::Verified_Inline_Entry_RO;\n+  } else {\n+    \/\/ Share same entry for VEP and VIEP(RO).\n+    return CodeOffsets::Verified_Entry;\n+  }\n+}\n+\n+\n+void CompiledEntrySignature::compute_calling_conventions() {\n+  \/\/ Get the (non-scalarized) signature and check for inline type arguments\n+  if (!_method->is_static()) {\n+    if (_method->method_holder()->is_inline_klass() && InlineKlass::cast(_method->method_holder())->can_be_passed_as_fields()) {\n+      _has_inline_recv = true;\n+      _num_inline_args++;\n+    }\n+    SigEntry::add_entry(_sig, T_OBJECT, _method->name());\n+  }\n+  for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_INLINE_TYPE) {\n+      if (ss.as_inline_klass(_method->method_holder())->can_be_passed_as_fields()) {\n+        _num_inline_args++;\n+      }\n+      bt = T_OBJECT;\n+    }\n+    SigEntry::add_entry(_sig, bt, ss.as_symbol());\n+  }\n+  if (_method->is_abstract() && !has_inline_arg()) {\n+    return;\n+  }\n+\n+  \/\/ Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n+\n+  \/\/ Now compute the scalarized calling convention if there are inline types in the signature\n+  _sig_cc = _sig;\n+  _sig_cc_ro = _sig;\n+  _regs_cc = _regs;\n+  _regs_cc_ro = _regs;\n+  _args_on_stack_cc = _args_on_stack;\n+  _args_on_stack_cc_ro = _args_on_stack;\n+\n+  if (has_inline_arg() && !_method->is_native()) {\n+    _args_on_stack_cc = compute_scalarized_cc(_sig_cc, _regs_cc, \/* scalar_receiver = *\/ true);\n+\n+    _sig_cc_ro = _sig_cc;\n+    _regs_cc_ro = _regs_cc;\n+    _args_on_stack_cc_ro = _args_on_stack_cc;\n+    if (_has_inline_recv) {\n+      \/\/ For interface calls, we need another entry point \/ adapter to unpack the receiver\n+      _args_on_stack_cc_ro = compute_scalarized_cc(_sig_cc_ro, _regs_cc_ro, \/* scalar_receiver = *\/ false);\n+    }\n+\n+    \/\/ Upper bound on stack arguments to avoid hitting the argument limit and\n+    \/\/ bailing out of compilation (\"unsupported incoming calling sequence\").\n+    \/\/ TODO we need a reasonable limit (flag?) here\n+    if (_args_on_stack_cc > 50) {\n+      \/\/ Don't scalarize inline type arguments\n+      _sig_cc = _sig;\n+      _sig_cc_ro = _sig;\n+      _regs_cc = _regs;\n+      _regs_cc_ro = _regs;\n+      _args_on_stack_cc = _args_on_stack;\n+      _args_on_stack_cc_ro = _args_on_stack;\n+    } else {\n+      _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+      _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+      _has_scalarized_args = true;\n+    }\n+  }\n@@ -2661,1 +2910,1 @@\n-  NOT_PRODUCT(int insts_size);\n+  NOT_PRODUCT(int insts_size = 0);\n@@ -2665,0 +2914,1 @@\n+\n@@ -2670,2 +2920,4 @@\n-    if (method->is_abstract()) {\n-      return _abstract_method_handler;\n+    CompiledEntrySignature ces(method());\n+    {\n+       MutexUnlocker mul(AdapterHandlerLibrary_lock);\n+       ces.compute_calling_conventions();\n@@ -2673,0 +2925,6 @@\n+    GrowableArray<SigEntry>& sig       = ces.sig();\n+    GrowableArray<SigEntry>& sig_cc    = ces.sig_cc();\n+    GrowableArray<SigEntry>& sig_cc_ro = ces.sig_cc_ro();\n+    VMRegPair* regs         = ces.regs();\n+    VMRegPair* regs_cc      = ces.regs_cc();\n+    VMRegPair* regs_cc_ro   = ces.regs_cc_ro();\n@@ -2674,2 +2932,5 @@\n-    \/\/ Fill in the signature array, for the calling-convention call.\n-    int total_args_passed = method->size_of_parameters(); \/\/ All args on stack\n+    if (ces.has_scalarized_args()) {\n+      method->set_has_scalarized_args(true);\n+      method->set_c1_needs_stack_repair(ces.c1_needs_stack_repair());\n+      method->set_c2_needs_stack_repair(ces.c2_needs_stack_repair());\n+    }\n@@ -2677,9 +2938,15 @@\n-    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n-    VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);\n-    int i = 0;\n-    if (!method->is_static())  \/\/ Pass in receiver first\n-      sig_bt[i++] = T_OBJECT;\n-    for (SignatureStream ss(method->signature()); !ss.at_return_type(); ss.next()) {\n-      sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n-      if (ss.type() == T_LONG || ss.type() == T_DOUBLE)\n-        sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n+    if (method->is_abstract()) {\n+      if (ces.has_scalarized_args()) {\n+        \/\/ Save a C heap allocated version of the signature for abstract methods with scalarized inline type arguments\n+        address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();\n+        entry = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),\n+                                                 StubRoutines::throw_AbstractMethodError_entry(),\n+                                                 wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,\n+                                                 wrong_method_abstract, wrong_method_abstract);\n+        GrowableArray<SigEntry>* heap_sig = new (ResourceObj::C_HEAP, mtInternal) GrowableArray<SigEntry>(sig_cc_ro.length(), mtInternal);\n+        heap_sig->appendAll(&sig_cc_ro);\n+        entry->set_sig_cc(heap_sig);\n+        return entry;\n+      } else {\n+        return _abstract_method_handler;\n+      }\n@@ -2687,1 +2954,0 @@\n-    assert(i == total_args_passed, \"\");\n@@ -2690,1 +2956,1 @@\n-    entry = _adapters->lookup(total_args_passed, sig_bt);\n+    entry = _adapters->lookup(&sig_cc, regs_cc != regs_cc_ro);\n@@ -2705,4 +2971,1 @@\n-    \/\/ Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage\n-    int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed);\n-\n-    fingerprint = new AdapterFingerPrint(total_args_passed, sig_bt);\n+    fingerprint = new AdapterFingerPrint(&sig_cc, regs_cc != regs_cc_ro);\n@@ -2727,3 +2990,2 @@\n-                                                     total_args_passed,\n-                                                     comp_args_on_stack,\n-                                                     sig_bt,\n+                                                     ces.args_on_stack(),\n+                                                     &sig,\n@@ -2731,1 +2993,14 @@\n-                                                     fingerprint);\n+                                                     &sig_cc,\n+                                                     regs_cc,\n+                                                     &sig_cc_ro,\n+                                                     regs_cc_ro,\n+                                                     fingerprint,\n+                                                     new_adapter);\n+\n+      if (ces.has_scalarized_args()) {\n+        \/\/ Save a C heap allocated version of the scalarized signature and store it in the adapter\n+        GrowableArray<SigEntry>* heap_sig = new (ResourceObj::C_HEAP, mtInternal) GrowableArray<SigEntry>(sig_cc.length(), mtInternal);\n+        heap_sig->appendAll(&sig_cc);\n+        entry->set_sig_cc(heap_sig);\n+      }\n+\n@@ -2735,0 +3010,3 @@\n+          if (!shared_entry->compare_code(buf->code_begin(), buffer.insts_size())) {\n+            method->print();\n+          }\n@@ -2745,1 +3023,0 @@\n-      new_adapter = AdapterBlob::create(&buffer);\n@@ -2801,0 +3078,2 @@\n+  assert(base <= _c2i_inline_entry || _c2i_inline_entry == NULL, \"\");\n+  assert(base <= _c2i_inline_ro_entry || _c2i_inline_ro_entry == NULL, \"\");\n@@ -2802,0 +3081,1 @@\n+  assert(base <= _c2i_unverified_inline_entry || _c2i_unverified_inline_entry == NULL, \"\");\n@@ -2814,0 +3094,4 @@\n+  if (_c2i_inline_entry != NULL)\n+    _c2i_inline_entry += delta;\n+  if (_c2i_inline_ro_entry != NULL)\n+    _c2i_inline_ro_entry += delta;\n@@ -2816,0 +3100,2 @@\n+  if (_c2i_unverified_inline_entry != NULL)\n+    _c2i_unverified_inline_entry += delta;\n@@ -2824,0 +3110,3 @@\n+  if (_sig_cc != NULL) {\n+    delete _sig_cc;\n+  }\n@@ -2907,1 +3196,2 @@\n-        sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n+        BasicType bt = ss.type();\n+        sig_bt[i++] = bt;  \/\/ Collect remaining bits of signature\n@@ -3133,0 +3423,6 @@\n+  if (get_c2i_entry() != NULL) {\n+    st->print(\" c2iVE: \" INTPTR_FORMAT, p2i(get_c2i_inline_entry()));\n+  }\n+  if (get_c2i_entry() != NULL) {\n+    st->print(\" c2iVROE: \" INTPTR_FORMAT, p2i(get_c2i_inline_ro_entry()));\n+  }\n@@ -3134,1 +3430,4 @@\n-    st->print(\" c2iUV: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iUE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+  }\n+  if (get_c2i_unverified_entry() != NULL) {\n+    st->print(\" c2iUVE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_inline_entry()));\n@@ -3219,0 +3518,206 @@\n+\n+\/\/ We are at a compiled code to interpreter call. We need backing\n+\/\/ buffers for all inline type arguments. Allocate an object array to\n+\/\/ hold them (convenient because once we're done with it we don't have\n+\/\/ to worry about freeing it).\n+oop SharedRuntime::allocate_inline_types_impl(JavaThread* thread, methodHandle callee, bool allocate_receiver, TRAPS) {\n+  assert(InlineTypePassFieldsAsArgs, \"no reason to call this\");\n+  ResourceMark rm;\n+\n+  int nb_slots = 0;\n+  InstanceKlass* holder = callee->method_holder();\n+  allocate_receiver &= !callee->is_static() && holder->is_inline_klass();\n+  if (allocate_receiver) {\n+    nb_slots++;\n+  }\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    if (ss.type() == T_INLINE_TYPE) {\n+      nb_slots++;\n+    }\n+  }\n+  objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);\n+  objArrayHandle array(THREAD, array_oop);\n+  int i = 0;\n+  if (allocate_receiver) {\n+    InlineKlass* vk = InlineKlass::cast(holder);\n+    oop res = vk->allocate_instance(CHECK_NULL);\n+    array->obj_at_put(i, res);\n+    i++;\n+  }\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    if (ss.type() == T_INLINE_TYPE) {\n+      InlineKlass* vk = ss.as_inline_klass(holder);\n+      oop res = vk->allocate_instance(CHECK_NULL);\n+      array->obj_at_put(i, res);\n+      i++;\n+    }\n+  }\n+  return array();\n+}\n+\n+JRT_ENTRY(void, SharedRuntime::allocate_inline_types(JavaThread* thread, Method* callee_method, bool allocate_receiver))\n+  methodHandle callee(thread, callee_method);\n+  oop array = SharedRuntime::allocate_inline_types_impl(thread, callee, allocate_receiver, CHECK);\n+  thread->set_vm_result(array);\n+  thread->set_vm_result_2(callee()); \/\/ TODO: required to keep callee live?\n+JRT_END\n+\n+\/\/ TODO remove this once the AARCH64 dependency is gone\n+\/\/ Iterate over the array of heap allocated inline types and apply the GC post barrier to all reference fields.\n+\/\/ This is called from the C2I adapter after inline type arguments are heap allocated and initialized.\n+JRT_LEAF(void, SharedRuntime::apply_post_barriers(JavaThread* thread, objArrayOopDesc* array))\n+{\n+  assert(InlineTypePassFieldsAsArgs, \"no reason to call this\");\n+  assert(oopDesc::is_oop(array), \"should be oop\");\n+  for (int i = 0; i < array->length(); ++i) {\n+    instanceOop valueOop = (instanceOop)array->obj_at(i);\n+    InlineKlass* vk = InlineKlass::cast(valueOop->klass());\n+    if (vk->contains_oops()) {\n+      const address dst_oop_addr = ((address) (void*) valueOop);\n+      OopMapBlock* map = vk->start_of_nonstatic_oop_maps();\n+      OopMapBlock* const end = map + vk->nonstatic_oop_map_count();\n+      while (map != end) {\n+        address doop_address = dst_oop_addr + map->offset();\n+        barrier_set_cast<ModRefBarrierSet>(BarrierSet::barrier_set())->\n+          write_ref_array((HeapWord*) doop_address, map->count());\n+        map++;\n+      }\n+    }\n+  }\n+}\n+JRT_END\n+\n+\/\/ We're returning from an interpreted method: load each field into a\n+\/\/ register following the calling convention\n+JRT_LEAF(void, SharedRuntime::load_inline_type_fields_in_regs(JavaThread* thread, oopDesc* res))\n+{\n+  assert(res->klass()->is_inline_klass(), \"only inline types here\");\n+  ResourceMark rm;\n+  RegisterMap reg_map(thread);\n+  frame stubFrame = thread->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+  assert(callerFrame.is_interpreted_frame(), \"should be coming from interpreter\");\n+\n+  InlineKlass* vk = InlineKlass::cast(res->klass());\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  if (regs == NULL) {\n+    \/\/ The fields of the inline klass don't fit in registers, bail out\n+    return;\n+  }\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    address loc = reg_map.location(pair.first());\n+    switch(bt) {\n+    case T_BOOLEAN:\n+      *(jboolean*)loc = res->bool_field(off);\n+      break;\n+    case T_CHAR:\n+      *(jchar*)loc = res->char_field(off);\n+      break;\n+    case T_BYTE:\n+      *(jbyte*)loc = res->byte_field(off);\n+      break;\n+    case T_SHORT:\n+      *(jshort*)loc = res->short_field(off);\n+      break;\n+    case T_INT: {\n+      *(jint*)loc = res->int_field(off);\n+      break;\n+    }\n+    case T_LONG:\n+#ifdef _LP64\n+      *(intptr_t*)loc = res->long_field(off);\n+#else\n+      Unimplemented();\n+#endif\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY: {\n+      *(oop*)loc = res->obj_field(off);\n+      break;\n+    }\n+    case T_FLOAT:\n+      *(jfloat*)loc = res->float_field(off);\n+      break;\n+    case T_DOUBLE:\n+      *(jdouble*)loc = res->double_field(off);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+#ifdef ASSERT\n+  VMRegPair pair = regs->at(0);\n+  address loc = reg_map.location(pair.first());\n+  assert(*(oopDesc**)loc == res, \"overwritten object\");\n+#endif\n+\n+  thread->set_vm_result(res);\n+}\n+JRT_END\n+\n+\/\/ We've returned to an interpreted method, the interpreter needs a\n+\/\/ reference to an inline type instance. Allocate it and initialize it\n+\/\/ from field's values in registers.\n+JRT_BLOCK_ENTRY(void, SharedRuntime::store_inline_type_fields_to_buf(JavaThread* thread, intptr_t res))\n+{\n+  ResourceMark rm;\n+  RegisterMap reg_map(thread);\n+  frame stubFrame = thread->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+\n+#ifdef ASSERT\n+  InlineKlass* verif_vk = InlineKlass::returned_inline_klass(reg_map);\n+#endif\n+\n+  if (!is_set_nth_bit(res, 0)) {\n+    \/\/ We're not returning with inline type fields in registers (the\n+    \/\/ calling convention didn't allow it for this inline klass)\n+    assert(!Metaspace::contains((void*)res), \"should be oop or pointer in buffer area\");\n+    thread->set_vm_result((oopDesc*)res);\n+    assert(verif_vk == NULL, \"broken calling convention\");\n+    return;\n+  }\n+\n+  clear_nth_bit(res, 0);\n+  InlineKlass* vk = (InlineKlass*)res;\n+  assert(verif_vk == vk, \"broken calling convention\");\n+  assert(Metaspace::contains((void*)res), \"should be klass\");\n+\n+  \/\/ Allocate handles for every oop field so they are safe in case of\n+  \/\/ a safepoint when allocating\n+  GrowableArray<Handle> handles;\n+  vk->save_oop_fields(reg_map, handles);\n+\n+  \/\/ It's unsafe to safepoint until we are here\n+  JRT_BLOCK;\n+  {\n+    Thread* THREAD = thread;\n+    oop vt = vk->realloc_result(reg_map, handles, CHECK);\n+    thread->set_vm_result(vt);\n+  }\n+  JRT_BLOCK_END;\n+}\n+JRT_END\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":604,"deletions":99,"binary":false,"changes":703,"status":"modified"},{"patch":"@@ -245,0 +245,12 @@\n+#define CHECK_THROW_NOSYNC_IMSE(obj)  \\\n+  if (EnableValhalla && (obj)->mark().is_inline_type()) {  \\\n+    ResourceMark rm(THREAD);                \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n+#define CHECK_THROW_NOSYNC_IMSE_0(obj)  \\\n+    if (EnableValhalla && (obj)->mark().is_inline_type()) {  \\\n+    ResourceMark rm(THREAD);                  \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n@@ -272,0 +284,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -321,0 +334,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -434,0 +448,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -447,1 +462,1 @@\n-  assert(!mark.has_bias_pattern(), \"should not see bias pattern here\");\n+  assert(!UseBiasedLocking || !mark.has_bias_pattern(), \"should not see bias pattern here\");\n@@ -483,0 +498,4 @@\n+  if (EnableValhalla && mark.is_inline_type()) {\n+    return;\n+  }\n+  assert(!EnableValhalla || !object->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -485,0 +504,1 @@\n+         !UseBiasedLocking ||\n@@ -546,0 +566,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -560,0 +581,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -586,0 +608,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -605,0 +628,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -609,0 +633,1 @@\n+    assert(!obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n@@ -610,1 +635,0 @@\n-  assert(!obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n@@ -647,0 +671,1 @@\n+  CHECK_THROW_NOSYNC_IMSE_0(obj);\n@@ -671,0 +696,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -686,0 +712,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -703,0 +730,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -852,0 +880,4 @@\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+    \/\/ VM should be calling bootstrap method\n+    ShouldNotReachHere();\n+  }\n@@ -879,1 +911,1 @@\n-    assert(!mark.has_bias_pattern(), \"invariant\");\n+    assert(!UseBiasedLocking || !mark.has_bias_pattern(), \"invariant\");\n@@ -982,6 +1014,0 @@\n-\/\/ Deprecated -- use FastHashCode() instead.\n-\n-intptr_t ObjectSynchronizer::identity_hash_value_for(Handle obj) {\n-  return FastHashCode(Thread::current(), obj());\n-}\n-\n@@ -991,0 +1017,3 @@\n+  if (EnableValhalla && h_obj->mark().is_inline_type()) {\n+    return false;\n+  }\n@@ -1206,0 +1235,4 @@\n+  if (EnableValhalla) {\n+    guarantee(!object->klass()->is_inline_klass(), \"Attempt to inflate inline type\");\n+  }\n+\n@@ -1210,1 +1243,1 @@\n-    assert(!mark.has_bias_pattern(), \"invariant\");\n+    assert(!UseBiasedLocking || !mark.has_bias_pattern(), \"invariant\");\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":43,"deletions":10,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -126,1 +126,1 @@\n-  static intptr_t identity_hash_value_for(Handle obj);\n+  static intptr_t identity_hash_value_for(Handle obj);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -1522,0 +1523,1 @@\n+  _return_buffered_value(nullptr),\n@@ -1578,1 +1580,0 @@\n-\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -444,0 +444,1 @@\n+ public:\n@@ -1010,0 +1011,1 @@\n+  friend class VTBuffer;\n@@ -1067,0 +1069,1 @@\n+  oop           _return_buffered_value; \/\/ buffered value being returned\n@@ -1503,0 +1506,3 @@\n+  oop return_buffered_value() const              { return _return_buffered_value; }\n+  void set_return_buffered_value(oop val)        { _return_buffered_value = val; }\n+\n@@ -1567,0 +1573,1 @@\n+  static ByteSize return_buffered_value_offset() { return byte_offset_of(JavaThread, _return_buffered_value); }\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -227,1 +227,1 @@\n-  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ObjArrayKlass*)                        \\\n+  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ArrayKlass*)                        \\\n@@ -242,1 +242,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags,                                   u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags,                                   u4)                                    \\\n@@ -1633,0 +1633,1 @@\n+  declare_c2_type(MachVEPNode, MachIdealNode)                             \\\n@@ -2315,0 +2316,2 @@\n+  declare_constant(InstanceKlass::_misc_invalid_inline_super)             \\\n+  declare_constant(InstanceKlass::_misc_invalid_identity_super)           \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -103,0 +103,1 @@\n+  DCmdFactory::register_DCmdFactory(new DCmdFactoryImpl<PrintClassLayoutDCmd>(full_export, true, false));\n@@ -136,1 +137,0 @@\n-\n@@ -1045,1 +1045,24 @@\n-#endif\n+PrintClassLayoutDCmd::PrintClassLayoutDCmd(outputStream* output, bool heap) :\n+                                       DCmdWithParser(output, heap),\n+  _classname(\"classname\", \"Name of class whose layout should be printed. \",\n+             \"STRING\", true) {\n+  _dcmdparser.add_dcmd_argument(&_classname);\n+}\n+\n+void PrintClassLayoutDCmd::execute(DCmdSource source, TRAPS) {\n+  VM_PrintClassLayout printClassLayoutOp(output(), _classname.value());\n+  VMThread::execute(&printClassLayoutOp);\n+}\n+\n+int PrintClassLayoutDCmd::num_arguments() {\n+  ResourceMark rm;\n+  PrintClassLayoutDCmd* dcmd = new PrintClassLayoutDCmd(NULL, false);\n+  if (dcmd != NULL) {\n+    DCmdMark mark(dcmd);\n+    return dcmd->_dcmdparser.num_arguments();\n+  } else {\n+    return 0;\n+  }\n+}\n+\n+#endif \/\/ INCLUDE_SERVICES\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":25,"deletions":2,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -1046,1 +1046,1 @@\n-  k = ik->array_klass_or_null();\n+  k = k->array_klass_or_null();\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -89,1 +89,1 @@\n-  JVM_ACC_FIELD_INITIALIZED_FINAL_UPDATE  = 0x00000100, \/\/ (static) final field updated outside (class) initializer, same as JVM_ACC_NATIVE\n+  JVM_ACC_FIELD_INITIALIZED_FINAL_UPDATE  = 0x00000200, \/\/ (static) final field updated outside (class) initializer, same as JVM_ACC_NATIVE\n@@ -91,0 +91,1 @@\n+  JVM_ACC_FIELD_INLINED                   = 0x00008000, \/\/ field is inlined\n@@ -96,1 +97,2 @@\n-                                       JVM_ACC_FIELD_HAS_GENERIC_SIGNATURE,\n+                                       JVM_ACC_FIELD_HAS_GENERIC_SIGNATURE |\n+                                       JVM_ACC_FIELD_INLINED,\n@@ -127,0 +129,1 @@\n+  bool is_inline_type () const         { return (_flags & JVM_ACC_INLINE      ) != 0; }\n","filename":"src\/hotspot\/share\/utilities\/accessFlags.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -42,2 +42,2 @@\n-  JVM_CONSTANT_ClassIndex               = 101,  \/\/ Temporary tag while constructing constant pool\n-  JVM_CONSTANT_StringIndex              = 102,  \/\/ Temporary tag while constructing constant pool\n+  JVM_CONSTANT_ClassIndex               = 101,  \/\/ Temporary tag while constructing constant pool, class redefinition\n+  JVM_CONSTANT_StringIndex              = 102,  \/\/ Temporary tag while constructing constant pool, class redefinition\n@@ -48,1 +48,3 @@\n-  JVM_CONSTANT_InternalMax              = 106   \/\/ Last implementation tag\n+  JVM_CONSTANT_InternalMax              = 106,  \/\/ Last implementation tag\n+  \/\/ internal constant tag flags\n+  JVM_CONSTANT_QDescBit                 = (1 << 7) \/\/ Separate bit, encode Q type descriptors\n@@ -51,1 +53,0 @@\n-\n@@ -56,1 +57,1 @@\n-  bool is_klass() const             { return _tag == JVM_CONSTANT_Class; }\n+  bool is_klass() const             { return value() == JVM_CONSTANT_Class; }\n@@ -71,1 +72,1 @@\n-    return _tag == JVM_CONSTANT_UnresolvedClass || _tag == JVM_CONSTANT_UnresolvedClassInError;\n+    return value() == JVM_CONSTANT_UnresolvedClass || value() == JVM_CONSTANT_UnresolvedClassInError;\n@@ -75,1 +76,5 @@\n-    return _tag == JVM_CONSTANT_UnresolvedClassInError;\n+    return value() == JVM_CONSTANT_UnresolvedClassInError;\n+  }\n+\n+  bool is_Qdescriptor_klass() const {\n+    return (_tag & JVM_CONSTANT_QDescBit) != 0;\n@@ -118,3 +123,8 @@\n-    assert((tag >= 0 && tag <= JVM_CONSTANT_NameAndType) ||\n-           (tag >= JVM_CONSTANT_MethodHandle && tag <= JVM_CONSTANT_InvokeDynamic) ||\n-           (tag >= JVM_CONSTANT_InternalMin && tag <= JVM_CONSTANT_InternalMax), \"Invalid constant tag\");\n+    jbyte entry_tag = tag & ~JVM_CONSTANT_QDescBit;\n+    assert((((tag & JVM_CONSTANT_QDescBit) == 0) && (entry_tag >= 0 && entry_tag <= JVM_CONSTANT_NameAndType) ||\n+           (entry_tag >= JVM_CONSTANT_MethodHandle && entry_tag <= JVM_CONSTANT_InvokeDynamic) ||\n+           (entry_tag >= JVM_CONSTANT_InternalMin && entry_tag <= JVM_CONSTANT_InternalMax))\n+           || (((tag & JVM_CONSTANT_QDescBit) != 0) && (entry_tag == JVM_CONSTANT_Class ||\n+               entry_tag == JVM_CONSTANT_UnresolvedClass || entry_tag == JVM_CONSTANT_UnresolvedClassInError\n+               || entry_tag == JVM_CONSTANT_ClassIndex))\n+               , \"Invalid constant tag\");\n@@ -138,1 +148,2 @@\n-  jbyte value() const                { return _tag; }\n+  jbyte value() const                { return _tag & ~JVM_CONSTANT_QDescBit; }\n+  jbyte tag() const                  { return _tag; }\n","filename":"src\/hotspot\/share\/utilities\/constantTag.hpp","additions":22,"deletions":11,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -2596,0 +2596,6 @@\n+            \/\/ resolveOrFail could return a non-static <init> method if present\n+            \/\/ detect and throw NSME before producing a MethodHandle\n+            if (!method.isStatic() && name.equals(\"<init>\")) {\n+                throw new NoSuchMethodException(\"illegal method name: \" + name);\n+            }\n+\n@@ -2741,0 +2747,7 @@\n+         *\n+         * @apiNote\n+         * This method does not find a static {@code <init>} factory method as it is invoked\n+         * via {@code invokestatic} bytecode as opposed to {@code invokespecial} for an\n+         * object constructor.  To look up static {@code <init>} factory method, use\n+         * the {@link #findStatic(Class, String, MethodType) findStatic} method.\n+         *\n@@ -2756,0 +2769,3 @@\n+            if (type.returnType() != void.class) {\n+                throw new NoSuchMethodException(\"Constructors must have void return type: \" + refc.getName());\n+            }\n@@ -3431,1 +3447,1 @@\n-            assert(ctor.isConstructor());\n+            assert(ctor.isObjectConstructorOrStaticInitMethod());\n@@ -3434,1 +3450,9 @@\n-            return lookup.getDirectConstructorNoSecurityManager(ctor.getDeclaringClass(), ctor);\n+            if (ctor.isObjectConstructor()) {\n+                assert(ctor.getReturnType() == void.class);\n+                return lookup.getDirectConstructorNoSecurityManager(ctor.getDeclaringClass(), ctor);\n+            } else {\n+                \/\/ static init factory is a static method\n+                assert(ctor.isMethod() && ctor.getReturnType() == ctor.getDeclaringClass() && ctor.getReferenceKind() == REF_invokeStatic);\n+                assert(!MethodHandleNatives.isCallerSensitive(ctor));  \/\/ must not be caller-sensitive\n+                return lookup.getDirectMethodNoSecurityManager(ctor.getReferenceKind(), ctor.getDeclaringClass(), ctor, lookup);\n+            }\n@@ -3687,2 +3711,5 @@\n-            if (name.startsWith(\"<\") && refKind != REF_newInvokeSpecial)\n-                throw new NoSuchMethodException(\"illegal method name: \"+name);\n+            \/\/ \"<init>\" can only be invoked via invokespecial or it's a static init factory\n+            if (name.startsWith(\"<\") && refKind != REF_newInvokeSpecial &&\n+                    !(refKind == REF_invokeStatic && name.equals(\"<init>\"))) {\n+                    throw new NoSuchMethodException(\"illegal method name: \" + name);\n+            }\n@@ -3691,1 +3718,0 @@\n-\n@@ -3799,1 +3825,1 @@\n-            if (m.isConstructor())\n+            if (m.isObjectConstructor())\n@@ -4100,1 +4126,1 @@\n-            assert(ctor.isConstructor());\n+            assert(ctor.isObjectConstructor());\n@@ -4290,0 +4316,3 @@\n+        if (arrayClass.isInlineClass()) {\n+            throw new UnsupportedOperationException();\n+        }\n@@ -5051,1 +5080,7 @@\n-        return type.isPrimitive() ?  zero(Wrapper.forPrimitiveType(type), type) : zero(Wrapper.OBJECT, type);\n+        if (type.isPrimitive()) {\n+            return zero(Wrapper.forPrimitiveType(type), type);\n+        } else if (type.isInlineClass()) {\n+            throw new UnsupportedOperationException();\n+        } else {\n+            return zero(Wrapper.OBJECT, type);\n+        }\n@@ -5081,1 +5116,1 @@\n-        MethodType mtype = methodType(ptype, ptype);\n+        MethodType mtype = MethodType.methodType(ptype, ptype);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandles.java","additions":44,"deletions":9,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -493,0 +493,4 @@\n+        if (referent != null && referent.getClass().isInlineClass()) {\n+            throw new IllegalArgumentException(\"cannot reference an inline value of type: \" +\n+                    referent.getClass().getName());\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/Reference.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -286,0 +286,14 @@\n+    \/**\n+     * {@inheritDoc} This implementation scans the children in left to right order.\n+     *\n+     * @param node  {@inheritDoc}\n+     * @param p  {@inheritDoc}\n+     * @return the result of scanning\n+     *\/\n+    @Override\n+    public R visitWithField(WithFieldTree node, P p) {\n+        R r = scan(node.getField(), p);\n+        r = scanAndReduce(node.getValue(), p, r);\n+        return r;\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/util\/TreeScanner.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-import java.util.stream.Collectors;\n@@ -171,0 +170,1 @@\n+        allowInlineTypes = Feature.INLINE_TYPES.allowedInSource(source);\n@@ -177,0 +177,1 @@\n+        allowValueMemberCycles = options.isSet(\"allowValueMemberCycles\");\n@@ -203,0 +204,4 @@\n+    \/** Switch: allow inline types?\n+     *\/\n+    boolean allowInlineTypes;\n+\n@@ -221,0 +226,5 @@\n+    \/**\n+     * Switch: Allow value type member cycles?\n+     *\/\n+    boolean allowValueMemberCycles;\n+\n@@ -317,1 +327,11 @@\n-                log.error(pos, Errors.CantAssignValToFinalVar(v));\n+                boolean complain = true;\n+                \/* Allow updates to instance fields of value classes by any method in the same nest via the\n+                   withfield operator -This does not result in mutation of final fields; the code generator\n+                   would implement `copy on write' semantics via the opcode `withfield'.\n+                *\/\n+                if (env.info.inWithField && v.getKind() == ElementKind.FIELD && (v.flags() & STATIC) == 0 && types.isValue(v.owner.type)) {\n+                    if (env.enclClass.sym.outermostClass() == v.owner.outermostClass())\n+                        complain = false;\n+                }\n+                if (complain)\n+                    log.error(pos, Errors.CantAssignValToFinalVar(v));\n@@ -811,1 +831,1 @@\n-                List<Type> bounds = List.of(attribType(tvar.bounds.head, env));\n+                List<Type> bounds = List.of(chk.checkRefType(tvar.bounds.head, attribType(tvar.bounds.head, env), false));\n@@ -813,1 +833,1 @@\n-                    bounds = bounds.prepend(attribType(bound, env));\n+                    bounds = bounds.prepend(chk.checkRefType(bound, attribType(bound, env), false));\n@@ -974,0 +994,3 @@\n+                if (env.tree.hasTag(NEWCLASS) && types.isValue(c.getSuperclass())) {\n+                    c.flags_field |= VALUE; \/\/ avoid further secondary errors.\n+                }\n@@ -1195,1 +1218,1 @@\n-                            TreeInfo.getConstructorInvocationName(body.stats, names) == names.empty) {\n+                            TreeInfo.getConstructorInvocationName(body.stats, names, true) == names.empty) {\n@@ -1226,0 +1249,6 @@\n+                if (m.isConstructor() && m.type.getParameterTypes().size() == 0) {\n+                    if ((owner.type == syms.objectType) ||\n+                            (tree.body.stats.size() == 1 && TreeInfo.getConstructorInvocationName(tree.body.stats, names, false) == names._super)) {\n+                        m.flags_field |= EMPTYNOARGCONSTR;\n+                    }\n+                }\n@@ -1295,0 +1324,3 @@\n+            \/* Don't want constant propagation\/folding for instance fields of value classes,\n+               as these can undergo updates via copy on write.\n+            *\/\n@@ -1296,1 +1328,1 @@\n-                if ((v.flags_field & FINAL) == 0 ||\n+                if ((v.flags_field & FINAL) == 0 || ((v.flags_field & STATIC) == 0 && types.isValue(v.owner.type)) ||\n@@ -1420,1 +1452,5 @@\n-            if ((tree.flags & STATIC) != 0) localEnv.info.staticLevel++;\n+            if ((tree.flags & STATIC) != 0)\n+                localEnv.info.staticLevel++;\n+            else if (tree.stats.size() > 0)\n+                env.info.scope.owner.flags_field |= HASINITBLOCK;\n+\n@@ -1485,0 +1521,33 @@\n+    public void visitWithField(JCWithField tree) {\n+        boolean inWithField = env.info.inWithField;\n+        try {\n+            env.info.inWithField = true;\n+            Type fieldtype = attribTree(tree.field, env.dup(tree), varAssignmentInfo);\n+            attribExpr(tree.value, env, fieldtype);\n+            Type capturedType = syms.errType;\n+            if (tree.field.type != null && !tree.field.type.isErroneous()) {\n+                final Symbol sym = TreeInfo.symbol(tree.field);\n+                if (sym == null || sym.kind != VAR || sym.owner.kind != TYP ||\n+                        (sym.flags() & STATIC) != 0 || !types.isValue(sym.owner.type)) {\n+                    log.error(tree.field.pos(), Errors.ValueInstanceFieldExpectedHere);\n+                } else {\n+                    Type ownType = sym.owner.type;\n+                    switch(tree.field.getTag()) {\n+                        case IDENT:\n+                            JCIdent ident = (JCIdent) tree.field;\n+                            ownType = ident.sym.owner.type;\n+                            break;\n+                        case SELECT:\n+                            JCFieldAccess fieldAccess = (JCFieldAccess) tree.field;\n+                            ownType = fieldAccess.selected.type;\n+                            break;\n+                    }\n+                    capturedType = capture(ownType);\n+                }\n+            }\n+            result = check(tree, capturedType, KindSelector.VAL, resultInfo);\n+        } finally {\n+            env.info.inWithField = inWithField;\n+        }\n+    }\n+\n@@ -1528,1 +1597,1 @@\n-                Type base = types.asSuper(exprType, syms.iterableType.tsym);\n+                Type base = types.asSuper(exprType, syms.iterableType.tsym, true);\n@@ -1754,1 +1823,1 @@\n-        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env));\n+        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env), false);\n@@ -1850,1 +1919,1 @@\n-            types.asSuper(resource, syms.autoCloseableType.tsym) != null &&\n+            types.asSuper(resource, syms.autoCloseableType.tsym, true) != null &&\n@@ -2040,1 +2109,2 @@\n-            \/\/ Those were all the cases that could result in a primitive\n+            \/\/ Those were all the cases that could result in a primitive. See if primitive boxing and inline\n+            \/\/ narrowing conversions bring about a convergence.\n@@ -2042,1 +2112,2 @@\n-                                 .map(t -> t.isPrimitive() ? types.boxedClass(t).type : t)\n+                                 .map(t -> t.isPrimitive() ? types.boxedClass(t).type\n+                                         : t.isReferenceProjection() ? t.valueProjection() : t)\n@@ -2053,1 +2124,1 @@\n-                                 .map(t -> chk.checkNonVoid(posIt.next(), t))\n+                                 .map(t -> chk.checkNonVoid(posIt.next(), t.isValue() ? t.referenceProjection() : t))\n@@ -2056,1 +2127,1 @@\n-            \/\/ both are known to be reference types.  The result is\n+            \/\/ both are known to be reference types (or projections).  The result is\n@@ -2477,0 +2548,36 @@\n+            final Symbol symbol = TreeInfo.symbol(tree.meth);\n+            if (symbol != null) {\n+                \/* Is this an ill conceived attempt to invoke jlO methods not available on value types ??\n+                 *\/\n+                boolean superCallOnValueReceiver = types.isValue(env.enclClass.sym.type)\n+                        && (tree.meth.hasTag(SELECT))\n+                        && ((JCFieldAccess)tree.meth).selected.hasTag(IDENT)\n+                        && TreeInfo.name(((JCFieldAccess)tree.meth).selected) == names._super;\n+                if (types.isValue(qualifier) || superCallOnValueReceiver) {\n+                    int argSize = argtypes.size();\n+                    Name name = symbol.name;\n+                    switch (name.toString()) {\n+                        case \"wait\":\n+                            if (argSize == 0\n+                                    || (types.isConvertible(argtypes.head, syms.longType) &&\n+                                    (argSize == 1 || (argSize == 2 && types.isConvertible(argtypes.tail.head, syms.intType))))) {\n+                                log.error(tree.pos(), Errors.ValueDoesNotSupport(name));\n+                            }\n+                            break;\n+                        case \"notify\":\n+                        case \"notifyAll\":\n+                        case \"clone\":\n+                        case \"finalize\":\n+                            if (argSize == 0)\n+                                log.error(tree.pos(), Errors.ValueDoesNotSupport(name));\n+                            break;\n+                        case \"hashCode\":\n+                        case \"equals\":\n+                        case \"toString\":\n+                            if (superCallOnValueReceiver)\n+                                log.error(tree.pos(), Errors.ValueDoesNotSupport(names.fromString(\"invocation of super.\" + name)));\n+                            break;\n+                    }\n+                }\n+            }\n+\n@@ -2491,0 +2598,9 @@\n+                \/\/ Temporary treatment for inline class: Given an inline class V that implements\n+                \/\/ I1, I2, ... In, v.getClass() is typed to be Class<? extends Object & I1 & I2 .. & In>\n+                Type wcb;\n+                if (qualifierType.isValue()) {\n+                    List<Type> bounds = List.of(syms.objectType).appendList(((ClassSymbol) qualifierType.tsym).getInterfaces());\n+                    wcb = bounds.size() > 1 ? types.makeIntersectionType(bounds) : syms.objectType;\n+                } else {\n+                    wcb = types.erasure(qualifierType);\n+                }\n@@ -2492,1 +2608,1 @@\n-                        List.of(new WildcardType(types.erasure(qualifierType),\n+                        List.of(new WildcardType(wcb,\n@@ -2665,0 +2781,8 @@\n+            \/\/ Check that it is an instantiation of a class and not a projection type\n+            if (clazz.hasTag(SELECT)) {\n+                JCFieldAccess fieldAccess = (JCFieldAccess) clazz;\n+                if (fieldAccess.selected.type.isValue() &&\n+                        (fieldAccess.name == names.ref || fieldAccess.name == names.val)) {\n+                    log.error(tree.pos(), Errors.ProjectionCantBeInstantiated);\n+                }\n+            }\n@@ -2836,0 +2960,1 @@\n+                    chk.checkParameterizationWithValues(tree, clazztype);\n@@ -2908,0 +3033,3 @@\n+        \/\/ Likewise arg can't be null if it is a value.\n+        if (types.isValue(arg.type))\n+            return arg;\n@@ -3922,0 +4050,1 @@\n+                chk.checkForSuspectClassLiteralComparison(tree, left, right);\n@@ -4128,1 +4257,1 @@\n-                tree.name == names._class)\n+                tree.name == names._class || tree.name == names._default)\n@@ -4130,0 +4259,4 @@\n+            if (tree.name == names._default && !allowInlineTypes) {\n+                log.error(DiagnosticFlag.SOURCE_LEVEL, tree.pos(),\n+                        Feature.INLINE_TYPES.error(sourceName));\n+            }\n@@ -4151,4 +4284,9 @@\n-                log.error(tree.pos(), Errors.TypeVarCantBeDeref);\n-                result = tree.type = types.createErrorType(tree.name, site.tsym, site);\n-                tree.sym = tree.type.tsym;\n-                return ;\n+                if (tree.name == names._default) {\n+                    result = check(tree, litType(BOT).constType(null),\n+                            KindSelector.VAL, resultInfo);\n+                } else {\n+                    log.error(tree.pos(), Errors.TypeVarCantBeDeref);\n+                    result = tree.type = types.createErrorType(tree.name, site.tsym, site);\n+                    tree.sym = tree.type.tsym;\n+                    return;\n+                }\n@@ -4162,0 +4300,1 @@\n+\n@@ -4297,0 +4436,6 @@\n+                } else if (name == names._default) {\n+                    return new VarSymbol(STATIC, names._default, site, site.tsym);\n+                } else if (name == names.ref && site.isValue() && resultInfo.pkind.contains(KindSelector.TYP)) {\n+                    return site.tsym.referenceProjection();\n+                } else if (name == names.val && site.isValue() && resultInfo.pkind.contains(KindSelector.TYP)) {\n+                    return site.tsym;\n@@ -4306,0 +4451,4 @@\n+                if (name == names._default) {\n+                    \/\/ Be sure to return the default value before examining bounds\n+                    return new VarSymbol(STATIC, names._default, site, site.tsym);\n+                }\n@@ -4330,1 +4479,1 @@\n-                \/\/ .class is allowed for these.\n+                \/\/ .class and .default is allowed for these.\n@@ -4335,0 +4484,2 @@\n+                } else if (name == names._default) {\n+                    return new VarSymbol(STATIC, names._default, site, site.tsym);\n@@ -4943,1 +5094,1 @@\n-                make.Modifiers(PUBLIC | ABSTRACT),\n+                make.Modifiers(PUBLIC | ABSTRACT | (extending != null && TreeInfo.symbol(extending).isValue() ? VALUE : 0)),\n@@ -4966,1 +5117,1 @@\n-        result = check(tree, new WildcardType(chk.checkRefType(tree.pos(), type),\n+        result = check(tree, new WildcardType(chk.checkRefType(tree.pos(), type, false),\n@@ -5073,0 +5224,7 @@\n+            if (types.isValue(c.type)) {\n+                final Env<AttrContext> env = typeEnvs.get(c);\n+                if (!allowValueMemberCycles) {\n+                    if (env != null && env.tree != null && env.tree.hasTag(CLASSDEF))\n+                        chk.checkNonCyclicMembership((JCClassDecl)env.tree);\n+                }\n+            }\n@@ -5183,1 +5341,1 @@\n-            } else {\n+            } else if ((c.flags_field & Flags.COMPOUND) == 0) {\n@@ -5238,0 +5396,8 @@\n+                if ((c.flags() & (VALUE | ABSTRACT)) == VALUE) { \/\/ for non-intersection, concrete values.\n+                    Assert.check(env.tree.hasTag(CLASSDEF));\n+                    JCClassDecl classDecl = (JCClassDecl) env.tree;\n+                    if (classDecl.extending != null) {\n+                        chk.checkConstraintsOfInlineSuper(env.tree.pos(), c);\n+                    }\n+                }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":190,"deletions":24,"binary":false,"changes":214,"status":"modified"},{"patch":"@@ -55,0 +55,2 @@\n+import static com.sun.tools.javac.comp.Flow.ThisExposability.ALLOWED;\n+import static com.sun.tools.javac.comp.Flow.ThisExposability.BANNED;\n@@ -1238,1 +1240,1 @@\n-                    if (types.asSuper(sup, syms.autoCloseableType.tsym) != null) {\n+                    if (types.asSuper(sup, syms.autoCloseableType.tsym, true) != null) {\n@@ -1657,0 +1659,8 @@\n+    \/** Enum to model whether constructors allowed to \"leak\" this reference before\n+        all instance fields are DA.\n+     *\/\n+    enum ThisExposability {\n+        ALLOWED,     \/\/ Normal Object classes - NOP\n+        BANNED,      \/\/ Value types           - Error\n+    }\n+\n@@ -1745,0 +1755,3 @@\n+        \/\/ Are constructors allowed to leak this reference ?\n+        ThisExposability thisExposability = ALLOWED;\n+\n@@ -1870,0 +1883,22 @@\n+        void checkEmbryonicThisExposure(JCTree node) {\n+            if (this.thisExposability == ALLOWED || classDef == null)\n+                return;\n+\n+            \/\/ Note: for non-initial constructors, firstadr is post all instance fields.\n+            for (int i = firstadr; i < nextadr; i++) {\n+                VarSymbol sym = vardecls[i].sym;\n+                if (sym.owner != classDef.sym)\n+                    continue;\n+                if ((sym.flags() & (FINAL | HASINIT | STATIC | PARAMETER)) != FINAL)\n+                    continue;\n+                if (sym.pos < startPos || sym.adr < firstadr)\n+                    continue;\n+                if (!inits.isMember(sym.adr)) {\n+                    if (this.thisExposability == BANNED) {\n+                        log.error(node, Errors.ThisExposedPrematurely);\n+                    }\n+                    return; \/\/ don't flog a dead horse.\n+                }\n+            }\n+        }\n+\n@@ -2066,0 +2101,1 @@\n+            ThisExposability priorThisExposability = this.thisExposability;\n@@ -2089,0 +2125,6 @@\n+                        this.thisExposability = ALLOWED;\n+                    } else {\n+                        if (types.isValue(tree.sym.owner.type))\n+                            this.thisExposability = BANNED;\n+                        else\n+                            this.thisExposability = ALLOWED;\n@@ -2151,0 +2193,1 @@\n+                this.thisExposability = priorThisExposability;\n@@ -2626,0 +2669,5 @@\n+            if (tree.meth.hasTag(IDENT)) {\n+                JCIdent ident = (JCIdent) tree.meth;\n+                if (ident.name != names._super && !ident.sym.isStatic())\n+                    checkEmbryonicThisExposure(tree);\n+            }\n@@ -2632,0 +2680,6 @@\n+            if (classDef != null && tree.encl == null && tree.clazz.hasTag(IDENT)) {\n+                JCIdent clazz = (JCIdent) tree.clazz;\n+                if (!clazz.sym.isStatic() && clazz.type.getEnclosingType().tsym == classDef.sym) {\n+                    checkEmbryonicThisExposure(tree);\n+                }\n+            }\n@@ -2694,1 +2748,8 @@\n-            super.visitSelect(tree);\n+            ThisExposability priorThisExposability = this.thisExposability;\n+            try {\n+                if (tree.name == names._this && classDef != null && tree.sym.owner == classDef.sym) {\n+                    checkEmbryonicThisExposure(tree);\n+                } else if (tree.sym.kind == VAR || tree.sym.isStatic()) {\n+                    this.thisExposability = ALLOWED;\n+                }\n+                super.visitSelect(tree);\n@@ -2697,1 +2758,4 @@\n-                checkInit(tree.pos(), (VarSymbol)tree.sym);\n+                    checkInit(tree.pos(), (VarSymbol)tree.sym);\n+                }\n+            } finally {\n+                 this.thisExposability = priorThisExposability;\n@@ -2761,0 +2825,3 @@\n+            if (tree.name == names._this) {\n+                checkEmbryonicThisExposure(tree);\n+            }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Flow.java","additions":70,"deletions":3,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -416,37 +416,58 @@\n-        switch ((short)(sym.flags() & AccessFlags)) {\n-        case PRIVATE:\n-            return\n-                (env.enclClass.sym == sym.owner \/\/ fast special case\n-                 ||\n-                 env.enclClass.sym.outermostClass() ==\n-                 sym.owner.outermostClass())\n-                &&\n-                sym.isInheritedIn(site.tsym, types);\n-        case 0:\n-            return\n-                (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n-                 ||\n-                 env.toplevel.packge == sym.packge())\n-                &&\n-                isAccessible(env, site, checkInner)\n-                &&\n-                sym.isInheritedIn(site.tsym, types)\n-                &&\n-                notOverriddenIn(site, sym);\n-        case PROTECTED:\n-            return\n-                (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n-                 ||\n-                 env.toplevel.packge == sym.packge()\n-                 ||\n-                 isProtectedAccessible(sym, env.enclClass.sym, site)\n-                 ||\n-                 \/\/ OK to select instance method or field from 'super' or type name\n-                 \/\/ (but type names should be disallowed elsewhere!)\n-                 env.info.selectSuper && (sym.flags() & STATIC) == 0 && sym.kind != TYP)\n-                &&\n-                isAccessible(env, site, checkInner)\n-                &&\n-                notOverriddenIn(site, sym);\n-        default: \/\/ this case includes erroneous combinations as well\n-            return isAccessible(env, site, checkInner) && notOverriddenIn(site, sym);\n+        ClassSymbol enclosingCsym = env.enclClass.sym;\n+        if (sym.kind == MTH || sym.kind == VAR) {\n+            \/* If any inline types are involved, ask the same question in the reference universe,\n+               where the hierarchy is navigable\n+            *\/\n+            if (site.isValue())\n+                site = site.referenceProjection();\n+            if (sym.owner.isValue())\n+                sym = sym.referenceProjection();\n+            if (env.enclClass.sym.isValue())\n+                env.enclClass.sym = env.enclClass.sym.referenceProjection();\n+        } else if (sym.kind == TYP) {\n+            \/\/ A type is accessible in a reference projection if it was\n+            \/\/ accessible in the value projection.\n+            if (site.isReferenceProjection())\n+                site = site.valueProjection();\n+        }\n+        try {\n+            switch ((short)(sym.flags() & AccessFlags)) {\n+                case PRIVATE:\n+                    return\n+                            (env.enclClass.sym == sym.owner \/\/ fast special case\n+                                    ||\n+                                    env.enclClass.sym.outermostClass() ==\n+                                            sym.owner.outermostClass())\n+                                    &&\n+                                    sym.isInheritedIn(site.tsym, types);\n+                case 0:\n+                    return\n+                            (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n+                                    ||\n+                                    env.toplevel.packge == sym.packge())\n+                                    &&\n+                                    isAccessible(env, site, checkInner)\n+                                    &&\n+                                    sym.isInheritedIn(site.tsym, types)\n+                                    &&\n+                                    notOverriddenIn(site, sym);\n+                case PROTECTED:\n+                    return\n+                            (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n+                                    ||\n+                                    env.toplevel.packge == sym.packge()\n+                                    ||\n+                                    isProtectedAccessible(sym, env.enclClass.sym, site)\n+                                    ||\n+                                    \/\/ OK to select instance method or field from 'super' or type name\n+                                    \/\/ (but type names should be disallowed elsewhere!)\n+                                    env.info.selectSuper && (sym.flags() & STATIC) == 0 && sym.kind != TYP)\n+                                    &&\n+                                    isAccessible(env, site, checkInner)\n+                                    &&\n+                                    notOverriddenIn(site, sym);\n+                default: \/\/ this case includes erroneous combinations as well\n+                    return isAccessible(env, site, checkInner) && notOverriddenIn(site, sym);\n+            }\n+        } finally {\n+            env.enclClass.sym = enclosingCsym;\n@@ -465,5 +486,12 @@\n-        else {\n-            Symbol s2 = ((MethodSymbol)sym).implementation(site.tsym, types, true);\n-            return (s2 == null || s2 == sym || sym.owner == s2.owner ||\n-                    !types.isSubSignature(types.memberType(site, s2), types.memberType(site, sym)));\n-        }\n+\n+        \/* If any inline types are involved, ask the same question in the reference universe,\n+           where the hierarchy is navigable\n+        *\/\n+        if (site.isValue())\n+            site = site.referenceProjection();\n+        if (sym.owner.isValue())\n+            sym = sym.referenceProjection();\n+\n+        Symbol s2 = ((MethodSymbol)sym).implementation(site.tsym, types, true);\n+        return (s2 == null || s2 == sym || sym.owner == s2.owner ||\n+                !types.isSubSignature(types.memberType(site, s2), types.memberType(site, sym)));\n@@ -2225,0 +2253,4 @@\n+        \/\/ ATM, inner\/nested types are members of only the declaring inline class,\n+        \/\/ although accessible via the reference projection.\n+        if (c.isReferenceProjection())\n+            c = (TypeSymbol) c.valueProjection();\n@@ -2282,0 +2314,16 @@\n+        return findMemberTypeInternal(env,site, name, c);\n+    }\n+\n+    \/** Find qualified member type.\n+     *  @param env       The current environment.\n+     *  @param site      The original type from where the selection takes\n+     *                   place.\n+     *  @param name      The type's name.\n+     *  @param c         The class to search for the member type. This is\n+     *                   always a superclass or implemented interface of\n+     *                   site's class.\n+     *\/\n+    Symbol findMemberTypeInternal(Env<AttrContext> env,\n+                          Type site,\n+                          Name name,\n+                          TypeSymbol c) {\n@@ -2343,0 +2391,8 @@\n+        return findTypeInternal(env, name);\n+    }\n+\n+    \/** Find an unqualified type symbol.\n+     *  @param env       The current environment.\n+     *  @param name      The type's name.\n+     *\/\n+    Symbol findTypeInternal(Env<AttrContext> env, Name name) {\n@@ -3010,0 +3066,7 @@\n+                    ClassSymbol refProjection = newConstr.owner.isValue() ?\n+                                                     (ClassSymbol) newConstr.owner.referenceProjection() : null;\n+                    if (refProjection != null) {\n+                        MethodSymbol clone = newConstr.clone(refProjection);\n+                        clone.projection = newConstr;\n+                        newConstr.projection = clone;\n+                    }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Resolve.java","additions":105,"deletions":42,"binary":false,"changes":147,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+import com.sun.tools.javac.code.Flags.Flag;\n@@ -51,0 +52,1 @@\n+import static com.sun.tools.javac.code.Flags.asFlagSet;\n@@ -59,0 +61,1 @@\n+import static com.sun.tools.javac.parser.Tokens.TokenKind.SYNCHRONIZED;\n@@ -187,0 +190,1 @@\n+        this.allowWithFieldOperator = fac.options.isSet(\"allowWithFieldOperator\");\n@@ -206,0 +210,4 @@\n+    \/** Switch: should we allow withField operator at source level ?\n+    *\/\n+    boolean allowWithFieldOperator;\n+\n@@ -305,0 +313,7 @@\n+    protected boolean peekToken(int lookahead, Filter<TokenKind> tk1, Filter<TokenKind> tk2, Filter<TokenKind> tk3, Filter<TokenKind> tk4) {\n+        return tk1.accepts(S.token(lookahead + 1).kind) &&\n+                tk2.accepts(S.token(lookahead + 2).kind) &&\n+                tk3.accepts(S.token(lookahead + 3).kind) &&\n+                tk4.accepts(S.token(lookahead + 4).kind);\n+    }\n+\n@@ -474,0 +489,16 @@\n+    \/** If next input token matches one of the two given tokens, skip it, otherwise report\n+     *  an error.\n+     *\n+     * @return The actual token kind.\n+     *\/\n+    public TokenKind accept2(TokenKind tk1, TokenKind tk2) {\n+        TokenKind returnValue = token.kind;\n+        if (token.kind == tk1 || token.kind == tk2) {\n+            nextToken();\n+        } else {\n+            setErrorEndPos(token.pos);\n+            reportSyntaxError(S.prevToken().endPos, Errors.Expected2(tk1, tk2));\n+        }\n+        return returnValue;\n+    }\n+\n@@ -1143,0 +1174,15 @@\n+        case WITHFIELD:\n+            if (!allowWithFieldOperator) {\n+                log.error(pos, Errors.WithFieldOperatorDisallowed);\n+            }\n+            if (typeArgs == null && (mode & EXPR) != 0) {\n+                nextToken();\n+                accept(LPAREN);\n+                mode = EXPR;\n+                t = term();\n+                accept(COMMA);\n+                mode = EXPR;\n+                JCExpression v = term();\n+                accept(RPAREN);\n+                return F.at(pos).WithField(t, v);\n+            } else return illegal();\n@@ -1311,0 +1357,6 @@\n+                            case DEFAULT:\n+                                if (typeArgs != null) return illegal();\n+                                selectExprMode();\n+                                t = to(F.at(pos).Select(t, names._default));\n+                                nextToken();\n+                                break loop;\n@@ -1368,3 +1420,4 @@\n-                        if ((mode & TYPE) == 0 && isUnboundMemberRef()) {\n-                            \/\/this is an unbound method reference whose qualifier\n-                            \/\/is a generic type i.e. A<S>::m\n+                        if ((mode & TYPE) == 0 && isParameterizedTypePrefix()) {\n+                            \/\/this is either an unbound method reference whose qualifier\n+                            \/\/is a generic type i.e. A<S>::m or a default value creation of\n+                            \/\/the form ValueType<S>.default\n@@ -1383,0 +1436,6 @@\n+                                if (token.kind == DEFAULT) {\n+                                    t =  toP(F.at(token.pos).Select(t, names._default));\n+                                    nextToken();\n+                                    selectExprMode();\n+                                    return term3Rest(t, typeArgs);\n+                                }\n@@ -1547,1 +1606,1 @@\n-                } else if (token.kind == NEW && (mode & EXPR) != 0) {\n+                } else if ((token.kind == NEW) && (mode & EXPR) != 0) {\n@@ -1601,1 +1660,2 @@\n-     * method reference or a binary expression. To disambiguate, look for a\n+     * method reference or a default value creation that uses a parameterized type\n+     * or a binary expression. To disambiguate, look for a\n@@ -1605,1 +1665,1 @@\n-    boolean isUnboundMemberRef() {\n+    boolean isParameterizedTypePrefix() {\n@@ -1727,2 +1787,2 @@\n-                    if (peekToken(lookahead, LAX_IDENTIFIER)) {\n-                        \/\/ Identifier, Identifier\/'_'\/'assert'\/'enum' -> explicit lambda\n+                    if (peekToken(lookahead, LAX_IDENTIFIER) || (peekToken(lookahead, QUES, LAX_IDENTIFIER) && (peekToken(lookahead + 2, RPAREN) || peekToken(lookahead + 2, COMMA)))) {\n+                        \/\/ Identifier[?], Identifier\/'_'\/'assert'\/'enum' -> explicit lambda\n@@ -1804,0 +1864,2 @@\n+                                peekToken(lookahead, QUES, LAX_IDENTIFIER, COMMA) ||\n+                                peekToken(lookahead, QUES, LAX_IDENTIFIER, RPAREN, ARROW) ||\n@@ -2200,1 +2262,1 @@\n-            accept(CLASS);\n+            TokenKind selector = accept2(CLASS, DEFAULT);\n@@ -2218,1 +2280,1 @@\n-                t = toP(F.at(pos).Select(t, names._class));\n+                t = toP(F.at(pos).Select(t, selector == CLASS ? names._class : names._default));\n@@ -2263,2 +2325,5 @@\n-        List<JCAnnotation> newAnnotations = typeAnnotationsOpt();\n-\n+        final JCModifiers mods = modifiersOpt();\n+        List<JCAnnotation> newAnnotations = mods.annotations;\n+        if (!newAnnotations.isEmpty()) {\n+            checkSourceLevel(newAnnotations.head.pos, Feature.TYPE_ANNOTATIONS);\n+        }\n@@ -2268,0 +2333,4 @@\n+            if (mods.flags != 0) {\n+                long badModifiers = (mods.flags & Flags.VALUE) != 0 ? mods.flags & ~Flags.FINAL : mods.flags;\n+                log.error(token.pos, Errors.ModNotAllowedHere(asFlagSet(badModifiers)));\n+            }\n@@ -2336,0 +2405,3 @@\n+            long badModifiers = mods.flags & ~(Flags.VALUE | Flags.FINAL);\n+            if (badModifiers != 0)\n+                log.error(token.pos, Errors.ModNotAllowedHere(asFlagSet(badModifiers)));\n@@ -2340,1 +2412,6 @@\n-            return classCreatorRest(newpos, null, typeArgs, t);\n+            JCNewClass newClass = classCreatorRest(newpos, null, typeArgs, t, mods.flags);\n+            if ((newClass.def == null) && (mods.flags != 0)) {\n+                badModifiers = (mods.flags & Flags.VALUE) != 0 ? mods.flags & ~Flags.FINAL : mods.flags;\n+                log.error(newClass.pos, Errors.ModNotAllowedHere(asFlagSet(badModifiers)));\n+            }\n+            return newClass;\n@@ -2365,1 +2442,1 @@\n-        return classCreatorRest(newpos, encl, typeArgs, t);\n+        return classCreatorRest(newpos, encl, typeArgs, t, 0);\n@@ -2447,1 +2524,2 @@\n-                                  JCExpression t)\n+                                  JCExpression t,\n+                                  long flags)\n@@ -2454,1 +2532,1 @@\n-            JCModifiers mods = F.at(Position.NOPOS).Modifiers(0);\n+            JCModifiers mods = F.at(Position.NOPOS).Modifiers(flags);\n@@ -2457,1 +2535,2 @@\n-        return toP(F.at(newpos).NewClass(encl, typeArgs, t, args, body));\n+        JCNewClass newClass = toP(F.at(newpos).NewClass(encl, typeArgs, t, args, body));\n+        return newClass;\n@@ -2588,0 +2667,1 @@\n+        token = recastToken(token);\n@@ -2598,0 +2678,1 @@\n+        case VALUE:\n@@ -3049,1 +3130,4 @@\n-                return variableDeclarators(modifiersOpt(), t, stats, true).toList();\n+                pos = token.pos;\n+                JCModifiers mods = F.at(Position.NOPOS).Modifiers(0);\n+                F.at(pos);\n+                return variableDeclarators(mods, t, stats, true).toList();\n@@ -3119,0 +3203,1 @@\n+            token = recastToken(token);\n@@ -3128,0 +3213,1 @@\n+            case VALUE       : flag = Flags.VALUE; break;\n@@ -3159,2 +3245,7 @@\n-                    annotations.append(ann);\n-                    flag = 0;\n+                    final Name name = TreeInfo.name(ann.annotationType);\n+                    if (name == names.__inline__ || name == names.java_lang___inline__) {\n+                        flag = Flags.VALUE;\n+                    } else {\n+                        annotations.append(ann);\n+                        flag = 0;\n+                    }\n@@ -3176,0 +3267,5 @@\n+        \/\/ Force value classes to be automatically final.\n+        if ((flags & (Flags.VALUE | Flags.ABSTRACT | Flags.INTERFACE | Flags.ENUM)) == Flags.VALUE) {\n+            flags |= Flags.FINAL;\n+        }\n+\n@@ -3365,0 +3461,36 @@\n+    \/\/ Does the given token signal an inline modifier ? If yes, suitably reclassify token.\n+    Token recastToken(Token token) {\n+        if (token.kind != IDENTIFIER || token.name() != names.inline) {\n+            return token;\n+        }\n+        if (peekToken(t->t == PRIVATE ||\n+                         t == PROTECTED ||\n+                         t == PUBLIC ||\n+                         t == STATIC ||\n+                         t == TRANSIENT ||\n+                         t == FINAL ||\n+                         t == ABSTRACT ||\n+                         t == NATIVE ||\n+                         t == VOLATILE ||\n+                         t == SYNCHRONIZED ||\n+                         t == STRICTFP ||\n+                         t == MONKEYS_AT ||\n+                         t == DEFAULT ||\n+                         t == BYTE ||\n+                         t == SHORT ||\n+                         t == CHAR ||\n+                         t == INT ||\n+                         t == LONG ||\n+                         t == FLOAT ||\n+                         t == DOUBLE ||\n+                         t == BOOLEAN ||\n+                         t == CLASS ||\n+                         t == INTERFACE ||\n+                         t == ENUM ||\n+                         t == IDENTIFIER)) { \/\/ new value Comparable() {}\n+            checkSourceLevel(Feature.INLINE_TYPES);\n+            return new Token(VALUE, token.pos, token.endPos, token.comments);\n+        }\n+        return token;\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":152,"deletions":20,"binary":false,"changes":172,"status":"modified"},{"patch":"@@ -144,0 +144,1 @@\n+        VALUE(), \/\/ a phantom token never returned by the scanner, but can result from a reclassification by the parser.\n@@ -147,0 +148,1 @@\n+        WITHFIELD(\"__WithField\"),\n@@ -247,0 +249,2 @@\n+            case VALUE:\n+                return \"value\";\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/Tokens.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -399,0 +399,4 @@\n+# 0: symbol\n+compiler.err.cyclic.value.type.membership=\\\n+    cyclic inline type membership involving {0}\n+\n@@ -1778,0 +1782,3 @@\n+compiler.warn.get.class.compared.with.interface=\\\n+    return value of getClass() can never equal the class literal of an interface\n+\n@@ -2955,0 +2962,3 @@\n+compiler.misc.feature.inline.type=\\\n+    inline type\n+\n@@ -3785,0 +3795,60 @@\n+# 0: name (of method)\n+compiler.err.inline.class.may.not.override=\\\n+    Inline classes may not override the method {0} from Object\n+\n+# 0: name (of method)\n+compiler.err.value.does.not.support=\\\n+    Inline types do not support {0}\n+\n+compiler.err.value.may.not.extend=\\\n+    Inline type may not extend another inline type or class\n+\n+compiler.err.value.instance.field.expected.here=\\\n+    withfield operator requires an instance field of an inline class here\n+\n+compiler.err.with.field.operator.disallowed=\\\n+    WithField operator is allowed only with -XDallowWithFieldOperator\n+\n+compiler.err.this.exposed.prematurely=\\\n+    Inine type instance should not be passed around before being fully initialized\n+\n+# 0: type\n+compiler.err.generic.parameterization.with.value.type=\\\n+    Inferred type {0} involves generic parameterization by an inline type\n+\n+# 0: type\n+compiler.err.inline.type.must.not.implement.identity.object=\\\n+    The inline type {0} attempts to implement the incompatible interface IdentityObject\n+\n+# 0: symbol, 1: type\n+compiler.err.concrete.supertype.for.inline.class=\\\n+    The concrete class {1} is not allowed to be a super class of the inline class {0} either directly or indirectly\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.method.cannot.be.synchronized=\\\n+    The method {0} in the super class {2} of the inline type {1} is synchronized. This is disallowed\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.constructor.cannot.take.arguments=\\\n+    The super class {2} of the inline type {1} defines a constructor {0} that takes arguments. This is disallowed\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.field.not.allowed=\\\n+    The super class {2} of the inline type {1} defines an instance field {0}. This is disallowed\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.no.arg.constructor.must.be.empty=\\\n+    The super class {2} of the inline type {1} defines a nonempty no-arg constructor {0}. This is disallowed\n+\n+# 0: symbol, 1: type\n+compiler.err.super.class.declares.init.block=\\\n+    The super class {1} of the inline class {0} declares one or more non-empty instance initializer blocks. This is disallowed.\n+\n+# 0: symbol, 1: type\n+compiler.err.super.class.cannot.be.inner=\\\n+    The super class {1} of the inline class {0} is an inner class. This is disallowed.\n+\n+compiler.err.projection.cant.be.instantiated=\\\n+    Illegal attempt to instantiate a projection type\n+\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":70,"deletions":0,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -136,0 +136,4 @@\n+        \/** Withfields, of type WithField.\n+         *\/\n+        WITHFIELD,\n+\n@@ -713,1 +717,0 @@\n-\n@@ -873,0 +876,3 @@\n+        \/** nascent value that evolves into the return value for a value factory *\/\n+        public VarSymbol factoryProduct;\n+\n@@ -1136,0 +1142,30 @@\n+    \/**\n+     * A withfield expression\n+     *\/\n+    public static class JCWithField extends JCExpression implements WithFieldTree {\n+        public JCExpression field;\n+        public JCExpression value;\n+        protected JCWithField(JCExpression field, JCExpression value) {\n+            this.field = field;\n+            this.value = value;\n+        }\n+        @Override\n+        public void accept(Visitor v) { v.visitWithField(this); }\n+\n+        @DefinedBy(Api.COMPILER_TREE)\n+        public Kind getKind() { return Kind.WITH_FIELD; }\n+        @DefinedBy(Api.COMPILER_TREE)\n+        public JCExpression getField() { return field; }\n+        @DefinedBy(Api.COMPILER_TREE)\n+        public JCExpression getValue() { return value; }\n+        @Override @DefinedBy(Api.COMPILER_TREE)\n+        public <R,D> R accept(TreeVisitor<R,D> v, D d) {\n+            return v.visitWithField(this, d);\n+        }\n+\n+        @Override\n+        public Tag getTag() {\n+            return WITHFIELD;\n+        }\n+    }\n+\n@@ -3246,0 +3282,1 @@\n+        public void visitWithField(JCWithField that)         { visitTree(that); }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/JCTree.java","additions":38,"deletions":1,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -782,0 +782,12 @@\n+    public void visitWithField(JCWithField tree) {\n+        try {\n+            print(\"__WithField(\");\n+            printExpr(tree.field);\n+            print(\", \");\n+            printExpr(tree.value);\n+            print(\")\");\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/Pretty.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -113,0 +113,1 @@\n+     *  Optionally, check only for no-arg ctor invocation\n@@ -114,1 +115,1 @@\n-    public static Name getConstructorInvocationName(List<? extends JCTree> trees, Names names) {\n+    public static Name getConstructorInvocationName(List<? extends JCTree> trees, Names names, boolean argsAllowed) {\n@@ -120,4 +121,6 @@\n-                    Name methName = TreeInfo.name(apply.meth);\n-                    if (methName == names._this ||\n-                        methName == names._super) {\n-                        return methName;\n+                    if (argsAllowed || apply.args.size() == 0) {\n+                        Name methName = TreeInfo.name(apply.meth);\n+                        if (methName == names._this ||\n+                                methName == names._super) {\n+                            return methName;\n+                        }\n@@ -626,0 +629,2 @@\n+            case WITHFIELD:\n+                return getEndPos(((JCWithField) tree).value, endPosTable);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeInfo.java","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -168,1 +168,3 @@\n-        } else if (longform) {\n+        }\n+        String s;\n+        if (longform) {\n@@ -176,1 +178,1 @@\n-            return fullClassNameAndPackageToClass.apply(\n+            s = fullClassNameAndPackageToClass.apply(\n@@ -181,1 +183,1 @@\n-            return sym.name.toString();\n+            s = sym.name.toString();\n@@ -183,0 +185,1 @@\n+        return s;\n","filename":"src\/jdk.jshell\/share\/classes\/jdk\/jshell\/TypePrinter.java","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -143,0 +143,117 @@\n+\n+static void assert_unlocked_state(markWord mark) {\n+  EXPECT_FALSE(mark.has_displaced_mark_helper());\n+  EXPECT_FALSE(mark.has_locker());\n+  EXPECT_FALSE(mark.has_monitor());\n+  EXPECT_FALSE(mark.is_being_inflated());\n+  EXPECT_FALSE(mark.is_locked());\n+  EXPECT_TRUE(mark.is_unlocked());\n+}\n+\n+static void assert_copy_set_hash(markWord mark) {\n+  const intptr_t hash = 4711;\n+  EXPECT_TRUE(mark.has_no_hash());\n+  markWord copy = mark.copy_set_hash(hash);\n+  EXPECT_EQ(hash, copy.hash());\n+  EXPECT_FALSE(copy.has_no_hash());\n+}\n+\n+static void assert_type(markWord mark) {\n+  EXPECT_FALSE(mark.is_flat_array());\n+  EXPECT_FALSE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_larval_state());\n+  EXPECT_FALSE(mark.is_nullfree_array());\n+}\n+\n+TEST_VM(markWord, prototype) {\n+  markWord mark = markWord::prototype();\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_type(mark);\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+  EXPECT_TRUE(mark.decode_pointer() == NULL);\n+\n+  assert_copy_set_hash(mark);\n+  assert_type(mark);\n+}\n+\n+static void assert_inline_type(markWord mark) {\n+  EXPECT_FALSE(mark.is_flat_array());\n+  EXPECT_TRUE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_nullfree_array());\n+}\n+\n+TEST_VM(markWord, inline_type_prototype) {\n+  markWord mark = markWord::inline_type_prototype();\n+  assert_unlocked_state(mark);\n+  EXPECT_FALSE(mark.is_neutral());\n+\n+  assert_inline_type(mark);\n+  EXPECT_FALSE(mark.is_larval_state());\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+  EXPECT_TRUE(mark.decode_pointer() == NULL);\n+\n+  markWord larval = mark.enter_larval_state();\n+  EXPECT_TRUE(larval.is_larval_state());\n+  assert_inline_type(larval);\n+  mark = larval.exit_larval_state();\n+  EXPECT_FALSE(mark.is_larval_state());\n+  assert_inline_type(mark);\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+  EXPECT_TRUE(mark.decode_pointer() == NULL);\n+}\n+\n+#if _LP64\n+\n+static void assert_flat_array_type(markWord mark) {\n+  EXPECT_TRUE(mark.is_flat_array());\n+  EXPECT_FALSE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_larval_state());\n+  EXPECT_TRUE(mark.is_nullfree_array());\n+}\n+\n+TEST_VM(markWord, flat_array_prototype) {\n+  markWord mark = markWord::flat_array_prototype();\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_flat_array_type(mark);\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+  EXPECT_TRUE(mark.decode_pointer() == NULL);\n+\n+  assert_copy_set_hash(mark);\n+  assert_flat_array_type(mark);\n+}\n+\n+static void assert_nullfree_array_type(markWord mark) {\n+  EXPECT_FALSE(mark.is_flat_array());\n+  EXPECT_FALSE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_larval_state());\n+  EXPECT_TRUE(mark.is_nullfree_array());\n+}\n+\n+TEST_VM(markWord, nullfree_array_prototype) {\n+  markWord mark = markWord::nullfree_array_prototype();\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_nullfree_array_type(mark);\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+  EXPECT_TRUE(mark.decode_pointer() == NULL);\n+\n+  assert_copy_set_hash(mark);\n+  assert_nullfree_array_type(mark);\n+}\n+#endif \/\/ _LP64\n+\n","filename":"test\/hotspot\/gtest\/oops\/test_markWord.cpp","additions":117,"deletions":0,"binary":false,"changes":117,"status":"modified"},{"patch":"@@ -65,0 +65,77 @@\n+# Valhalla\n+compiler\/arguments\/CheckCICompilerCount.java                        8205030 generic-all\n+compiler\/arguments\/CheckCompileThresholdScaling.java                8205030 generic-all\n+compiler\/codecache\/CheckSegmentedCodeCache.java                     8205030 generic-all\n+compiler\/codecache\/cli\/TestSegmentedCodeCacheOption.java            8205030 generic-all\n+compiler\/codecache\/cli\/codeheapsize\/TestCodeHeapSizeOptions.java    8205030 generic-all\n+compiler\/codecache\/cli\/printcodecache\/TestPrintCodeCacheOption.java 8205030 generic-all\n+compiler\/whitebox\/OSRFailureLevel4Test.java                         8205030 generic-all\n+\n+compiler\/aot\/cli\/DisabledAOTWithLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/SingleAOTOptionTest.java 8226295 generic-all\n+compiler\/aot\/cli\/MultipleAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileClassWithDebugTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileModuleTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/AtFileTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionWrongFileTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ClasspathOptionUnknownClassTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileDirectoryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionNotExistingTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileClassTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileJarTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/IgnoreErrorsTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileAbsoluteDirectoryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/NonExistingAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/SingleAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/IncorrectAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/RecompilationTest.java 8226295 generic-all\n+compiler\/aot\/SharedUsageTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/ClassSearchTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/SearchPathTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/module\/ModuleSourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/ClassSourceTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/directory\/DirectorySourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/jar\/JarSourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/NativeOrderOutputStreamTest.java 8226295 generic-all\n+compiler\/aot\/verification\/vmflags\/TrackedFlagTest.java 8226295 generic-all\n+compiler\/aot\/verification\/vmflags\/NotTrackedFlagTest.java 8226295 generic-all\n+compiler\/aot\/verification\/ClassAndLibraryNotMatchTest.java 8226295 generic-all\n+compiler\/aot\/DeoptimizationTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SelfChanged.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SelfChangedCDS.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SuperChanged.java 8226295 generic-all\n+\n@@ -91,0 +168,22 @@\n+# Valhalla TODO:\n+runtime\/CompressedOops\/CompressedClassPointers.java 8210258 generic-all\n+runtime\/RedefineTests\/RedefineLeak.java 8205032 generic-all\n+runtime\/SharedArchiveFile\/BootAppendTests.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/CdsDifferentCompactStrings.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/CdsDifferentObjectAlignment.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/NonBootLoaderClasses.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/PrintSharedArchiveAndExit.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedArchiveFile.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedStringsDedup.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedStringsRunAuto.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedSymbolTableBucketSize.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SpaceUtilizationCheck.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/TestInterpreterMethodEntries.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformInterfaceAndImplementor.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformSuperAndSubClasses.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformSuperSubTwoPckgs.java 8210258 generic-all\n+runtime\/appcds\/ClassLoaderTest.java 8210258 generic-all\n+runtime\/appcds\/HelloTest.java 8210258 generic-all\n+runtime\/appcds\/sharedStrings\/SharedStringsBasic.java 8210258 generic-all\n+\n+\n@@ -101,1 +200,0 @@\n-\n@@ -104,0 +202,28 @@\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":127,"deletions":1,"binary":false,"changes":128,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-  runtime\n+  runtime \\\n@@ -57,0 +57,7 @@\n+hotspot_valhalla = \\\n+  runtime\/valhalla \\\n+  compiler\/valhalla\n+\n+hotspot_valhalla_runtime = \\\n+  runtime\/valhalla\n+\n@@ -95,1 +102,1 @@\n-  compiler\/codegen\/aes \\\n+  compiler\/codegen\/aes \\\n@@ -144,0 +151,1 @@\n+  compiler\/valhalla\/ \\\n@@ -161,0 +169,7 @@\n+tier1_compiler_no_valhalla = \\\n+  :tier1_compiler_1 \\\n+  :tier1_compiler_2 \\\n+  :tier1_compiler_3 \\\n+  :tier1_compiler_not_xcomp \\\n+  -compiler\/valhalla\n+\n@@ -312,0 +327,4 @@\n+tier1_runtime_no_valhalla = \\\n+  :tier1_runtime \\\n+  -runtime\/valhalla\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":21,"deletions":2,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -438,0 +438,4 @@\n+\n+        public String toString() {\n+            return super.toString() + \", vh:\" + vh;\n+        }\n@@ -458,0 +462,4 @@\n+\n+        public String toString() {\n+            return super.toString() + \", vh:\" + vh + \", f: \" + f;\n+        }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleBaseTest.java","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -71,0 +73,1 @@\n+    VarHandle vhValueTypeField;\n@@ -122,0 +125,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"boolean_v\", boolean.class);\n@@ -277,0 +283,5 @@\n+        cases.add(new VarHandleAccessTestCase(\"Value type field\",\n+                                              vhValueTypeField, vh -> testValueTypeField(Value.getInstance(), vh)));\n+        cases.add(new VarHandleAccessTestCase(\"Value type field unsupported\",\n+                                              vhValueTypeField, vh -> testValueTypeFieldUnsupported(Value.getInstance(), vh),\n+                                              false));\n@@ -354,0 +365,13 @@\n+    static void testValueTypeField(Value recv, VarHandle vh) {\n+        \/\/ Plain\n+        {\n+            boolean x = (boolean) vh.get(recv);\n+            assertEquals(x, true, \"get boolean value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, VarHandle vh) {\n+        checkUOE(() -> {\n+            vh.set(recv, false);\n+        });\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessBoolean.java","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -71,0 +73,1 @@\n+    VarHandle vhValueTypeField;\n@@ -122,0 +125,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"byte_v\", byte.class);\n@@ -277,0 +283,5 @@\n+        cases.add(new VarHandleAccessTestCase(\"Value type field\",\n+                                              vhValueTypeField, vh -> testValueTypeField(Value.getInstance(), vh)));\n+        cases.add(new VarHandleAccessTestCase(\"Value type field unsupported\",\n+                                              vhValueTypeField, vh -> testValueTypeFieldUnsupported(Value.getInstance(), vh),\n+                                              false));\n@@ -343,0 +354,13 @@\n+    static void testValueTypeField(Value recv, VarHandle vh) {\n+        \/\/ Plain\n+        {\n+            byte x = (byte) vh.get(recv);\n+            assertEquals(x, (byte)0x01, \"get byte value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, VarHandle vh) {\n+        checkUOE(() -> {\n+            vh.set(recv, (byte)0x23);\n+        });\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessByte.java","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -71,0 +73,1 @@\n+    VarHandle vhValueTypeField;\n@@ -122,0 +125,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"char_v\", char.class);\n@@ -277,0 +283,5 @@\n+        cases.add(new VarHandleAccessTestCase(\"Value type field\",\n+                                              vhValueTypeField, vh -> testValueTypeField(Value.getInstance(), vh)));\n+        cases.add(new VarHandleAccessTestCase(\"Value type field unsupported\",\n+                                              vhValueTypeField, vh -> testValueTypeFieldUnsupported(Value.getInstance(), vh),\n+                                              false));\n@@ -343,0 +354,13 @@\n+    static void testValueTypeField(Value recv, VarHandle vh) {\n+        \/\/ Plain\n+        {\n+            char x = (char) vh.get(recv);\n+            assertEquals(x, '\\u0123', \"get char value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, VarHandle vh) {\n+        checkUOE(() -> {\n+            vh.set(recv, '\\u4567');\n+        });\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessChar.java","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -71,0 +73,1 @@\n+    VarHandle vhValueTypeField;\n@@ -122,0 +125,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"double_v\", double.class);\n@@ -277,0 +283,5 @@\n+        cases.add(new VarHandleAccessTestCase(\"Value type field\",\n+                                              vhValueTypeField, vh -> testValueTypeField(Value.getInstance(), vh)));\n+        cases.add(new VarHandleAccessTestCase(\"Value type field unsupported\",\n+                                              vhValueTypeField, vh -> testValueTypeFieldUnsupported(Value.getInstance(), vh),\n+                                              false));\n@@ -378,0 +389,13 @@\n+    static void testValueTypeField(Value recv, VarHandle vh) {\n+        \/\/ Plain\n+        {\n+            double x = (double) vh.get(recv);\n+            assertEquals(x, 1.0d, \"get double value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, VarHandle vh) {\n+        checkUOE(() -> {\n+            vh.set(recv, 2.0d);\n+        });\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessDouble.java","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -71,0 +73,1 @@\n+    VarHandle vhValueTypeField;\n@@ -122,0 +125,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"float_v\", float.class);\n@@ -277,0 +283,5 @@\n+        cases.add(new VarHandleAccessTestCase(\"Value type field\",\n+                                              vhValueTypeField, vh -> testValueTypeField(Value.getInstance(), vh)));\n+        cases.add(new VarHandleAccessTestCase(\"Value type field unsupported\",\n+                                              vhValueTypeField, vh -> testValueTypeFieldUnsupported(Value.getInstance(), vh),\n+                                              false));\n@@ -378,0 +389,13 @@\n+    static void testValueTypeField(Value recv, VarHandle vh) {\n+        \/\/ Plain\n+        {\n+            float x = (float) vh.get(recv);\n+            assertEquals(x, 1.0f, \"get float value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, VarHandle vh) {\n+        checkUOE(() -> {\n+            vh.set(recv, 2.0f);\n+        });\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessFloat.java","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -71,0 +73,1 @@\n+    VarHandle vhValueTypeField;\n@@ -122,0 +125,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"int_v\", int.class);\n@@ -277,0 +283,5 @@\n+        cases.add(new VarHandleAccessTestCase(\"Value type field\",\n+                                              vhValueTypeField, vh -> testValueTypeField(Value.getInstance(), vh)));\n+        cases.add(new VarHandleAccessTestCase(\"Value type field unsupported\",\n+                                              vhValueTypeField, vh -> testValueTypeFieldUnsupported(Value.getInstance(), vh),\n+                                              false));\n@@ -343,0 +354,13 @@\n+    static void testValueTypeField(Value recv, VarHandle vh) {\n+        \/\/ Plain\n+        {\n+            int x = (int) vh.get(recv);\n+            assertEquals(x, 0x01234567, \"get int value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, VarHandle vh) {\n+        checkUOE(() -> {\n+            vh.set(recv, 0x89ABCDEF);\n+        });\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessInt.java","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -71,0 +73,1 @@\n+    VarHandle vhValueTypeField;\n@@ -122,0 +125,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"long_v\", long.class);\n@@ -277,0 +283,5 @@\n+        cases.add(new VarHandleAccessTestCase(\"Value type field\",\n+                                              vhValueTypeField, vh -> testValueTypeField(Value.getInstance(), vh)));\n+        cases.add(new VarHandleAccessTestCase(\"Value type field unsupported\",\n+                                              vhValueTypeField, vh -> testValueTypeFieldUnsupported(Value.getInstance(), vh),\n+                                              false));\n@@ -343,0 +354,13 @@\n+    static void testValueTypeField(Value recv, VarHandle vh) {\n+        \/\/ Plain\n+        {\n+            long x = (long) vh.get(recv);\n+            assertEquals(x, 0x0123456789ABCDEFL, \"get long value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, VarHandle vh) {\n+        checkUOE(() -> {\n+            vh.set(recv, 0xCAFEBABECAFEBABEL);\n+        });\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessLong.java","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -71,0 +73,1 @@\n+    VarHandle vhValueTypeField;\n@@ -122,0 +125,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"short_v\", short.class);\n@@ -277,0 +283,5 @@\n+        cases.add(new VarHandleAccessTestCase(\"Value type field\",\n+                                              vhValueTypeField, vh -> testValueTypeField(Value.getInstance(), vh)));\n+        cases.add(new VarHandleAccessTestCase(\"Value type field unsupported\",\n+                                              vhValueTypeField, vh -> testValueTypeFieldUnsupported(Value.getInstance(), vh),\n+                                              false));\n@@ -343,0 +354,13 @@\n+    static void testValueTypeField(Value recv, VarHandle vh) {\n+        \/\/ Plain\n+        {\n+            short x = (short) vh.get(recv);\n+            assertEquals(x, (short)0x0123, \"get short value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, VarHandle vh) {\n+        checkUOE(() -> {\n+            vh.set(recv, (short)0x4567);\n+        });\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessShort.java","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -124,0 +126,1 @@\n+\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessString.java","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -26,1 +28,1 @@\n- * @run testng\/othervm -Diters=20000 VarHandleTestMethodHandleAccessBoolean\n+ * @run testng\/othervm -Diters=2000 VarHandleTestMethodHandleAccessBoolean\n@@ -60,0 +62,2 @@\n+    VarHandle vhValueTypeField;\n+\n@@ -75,0 +79,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"boolean_v\", boolean.class);\n@@ -103,0 +110,5 @@\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeField(Value.getInstance(), hs)));\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field unsupported\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeFieldUnsupported(Value.getInstance(), hs),\n+                                                 false));\n@@ -354,0 +366,23 @@\n+    static void testValueTypeField(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        {\n+            boolean x = (boolean) hs.get(TestAccessMode.GET).invokeExact(recv);\n+            assertEquals(x, true, \"get boolean value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.SET)) {\n+            checkUOE(am, () -> {\n+                hs.get(am).invokeExact(recv, true);\n+            });\n+        }\n+\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.GET_AND_ADD)) {\n+            checkUOE(am, () -> {\n+                boolean r = (boolean) hs.get(am).invokeExact(recv, true);\n+            });\n+        }\n+\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessBoolean.java","additions":36,"deletions":1,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -26,1 +28,1 @@\n- * @run testng\/othervm -Diters=20000 VarHandleTestMethodHandleAccessByte\n+ * @run testng\/othervm -Diters=2000 VarHandleTestMethodHandleAccessByte\n@@ -60,0 +62,2 @@\n+    VarHandle vhValueTypeField;\n+\n@@ -75,0 +79,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"byte_v\", byte.class);\n@@ -103,0 +110,5 @@\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeField(Value.getInstance(), hs)));\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field unsupported\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeFieldUnsupported(Value.getInstance(), hs),\n+                                                 false));\n@@ -376,0 +388,18 @@\n+    static void testValueTypeField(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        {\n+            byte x = (byte) hs.get(TestAccessMode.GET).invokeExact(recv);\n+            assertEquals(x, (byte)0x01, \"get byte value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.SET)) {\n+            checkUOE(am, () -> {\n+                hs.get(am).invokeExact(recv, (byte)0x01);\n+            });\n+        }\n+\n+\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessByte.java","additions":31,"deletions":1,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -26,1 +28,1 @@\n- * @run testng\/othervm -Diters=20000 VarHandleTestMethodHandleAccessChar\n+ * @run testng\/othervm -Diters=2000 VarHandleTestMethodHandleAccessChar\n@@ -60,0 +62,2 @@\n+    VarHandle vhValueTypeField;\n+\n@@ -75,0 +79,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"char_v\", char.class);\n@@ -103,0 +110,5 @@\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeField(Value.getInstance(), hs)));\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field unsupported\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeFieldUnsupported(Value.getInstance(), hs),\n+                                                 false));\n@@ -376,0 +388,18 @@\n+    static void testValueTypeField(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        {\n+            char x = (char) hs.get(TestAccessMode.GET).invokeExact(recv);\n+            assertEquals(x, '\\u0123', \"get char value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.SET)) {\n+            checkUOE(am, () -> {\n+                hs.get(am).invokeExact(recv, '\\u0123');\n+            });\n+        }\n+\n+\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessChar.java","additions":31,"deletions":1,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -26,1 +28,1 @@\n- * @run testng\/othervm -Diters=20000 VarHandleTestMethodHandleAccessDouble\n+ * @run testng\/othervm -Diters=2000 VarHandleTestMethodHandleAccessDouble\n@@ -60,0 +62,2 @@\n+    VarHandle vhValueTypeField;\n+\n@@ -75,0 +79,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"double_v\", double.class);\n@@ -103,0 +110,5 @@\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeField(Value.getInstance(), hs)));\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field unsupported\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeFieldUnsupported(Value.getInstance(), hs),\n+                                                 false));\n@@ -298,0 +310,23 @@\n+    static void testValueTypeField(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        {\n+            double x = (double) hs.get(TestAccessMode.GET).invokeExact(recv);\n+            assertEquals(x, 1.0d, \"get double value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.SET)) {\n+            checkUOE(am, () -> {\n+                hs.get(am).invokeExact(recv, 1.0d);\n+            });\n+        }\n+\n+\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.GET_AND_BITWISE)) {\n+            checkUOE(am, () -> {\n+                double r = (double) hs.get(am).invokeExact(recv, 1.0d);\n+            });\n+        }\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessDouble.java","additions":36,"deletions":1,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -26,1 +28,1 @@\n- * @run testng\/othervm -Diters=20000 VarHandleTestMethodHandleAccessFloat\n+ * @run testng\/othervm -Diters=2000 VarHandleTestMethodHandleAccessFloat\n@@ -60,0 +62,2 @@\n+    VarHandle vhValueTypeField;\n+\n@@ -75,0 +79,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"float_v\", float.class);\n@@ -103,0 +110,5 @@\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeField(Value.getInstance(), hs)));\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field unsupported\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeFieldUnsupported(Value.getInstance(), hs),\n+                                                 false));\n@@ -298,0 +310,23 @@\n+    static void testValueTypeField(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        {\n+            float x = (float) hs.get(TestAccessMode.GET).invokeExact(recv);\n+            assertEquals(x, 1.0f, \"get float value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.SET)) {\n+            checkUOE(am, () -> {\n+                hs.get(am).invokeExact(recv, 1.0f);\n+            });\n+        }\n+\n+\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.GET_AND_BITWISE)) {\n+            checkUOE(am, () -> {\n+                float r = (float) hs.get(am).invokeExact(recv, 1.0f);\n+            });\n+        }\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessFloat.java","additions":36,"deletions":1,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -26,1 +28,1 @@\n- * @run testng\/othervm -Diters=20000 VarHandleTestMethodHandleAccessInt\n+ * @run testng\/othervm -Diters=2000 VarHandleTestMethodHandleAccessInt\n@@ -60,0 +62,2 @@\n+    VarHandle vhValueTypeField;\n+\n@@ -75,0 +79,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"int_v\", int.class);\n@@ -103,0 +110,5 @@\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeField(Value.getInstance(), hs)));\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field unsupported\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeFieldUnsupported(Value.getInstance(), hs),\n+                                                 false));\n@@ -376,0 +388,18 @@\n+    static void testValueTypeField(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        {\n+            int x = (int) hs.get(TestAccessMode.GET).invokeExact(recv);\n+            assertEquals(x, 0x01234567, \"get int value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.SET)) {\n+            checkUOE(am, () -> {\n+                hs.get(am).invokeExact(recv, 0x01234567);\n+            });\n+        }\n+\n+\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessInt.java","additions":31,"deletions":1,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -26,1 +28,1 @@\n- * @run testng\/othervm -Diters=20000 VarHandleTestMethodHandleAccessLong\n+ * @run testng\/othervm -Diters=2000 VarHandleTestMethodHandleAccessLong\n@@ -60,0 +62,2 @@\n+    VarHandle vhValueTypeField;\n+\n@@ -75,0 +79,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"long_v\", long.class);\n@@ -103,0 +110,5 @@\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeField(Value.getInstance(), hs)));\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field unsupported\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeFieldUnsupported(Value.getInstance(), hs),\n+                                                 false));\n@@ -376,0 +388,18 @@\n+    static void testValueTypeField(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        {\n+            long x = (long) hs.get(TestAccessMode.GET).invokeExact(recv);\n+            assertEquals(x, 0x0123456789ABCDEFL, \"get long value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.SET)) {\n+            checkUOE(am, () -> {\n+                hs.get(am).invokeExact(recv, 0x0123456789ABCDEFL);\n+            });\n+        }\n+\n+\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessLong.java","additions":31,"deletions":1,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -26,1 +28,1 @@\n- * @run testng\/othervm -Diters=20000 VarHandleTestMethodHandleAccessShort\n+ * @run testng\/othervm -Diters=2000 VarHandleTestMethodHandleAccessShort\n@@ -60,0 +62,2 @@\n+    VarHandle vhValueTypeField;\n+\n@@ -75,0 +79,3 @@\n+\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"short_v\", short.class);\n@@ -103,0 +110,5 @@\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeField(Value.getInstance(), hs)));\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field unsupported\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeFieldUnsupported(Value.getInstance(), hs),\n+                                                 false));\n@@ -376,0 +388,18 @@\n+    static void testValueTypeField(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        {\n+            short x = (short) hs.get(TestAccessMode.GET).invokeExact(recv);\n+            assertEquals(x, (short)0x0123, \"get short value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.SET)) {\n+            checkUOE(am, () -> {\n+                hs.get(am).invokeExact(recv, (short)0x0123);\n+            });\n+        }\n+\n+\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessShort.java","additions":31,"deletions":1,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\/\/ -- This file was mechanically generated: Do not edit! -- \/\/\n+\n@@ -60,0 +62,1 @@\n+\n@@ -75,0 +78,1 @@\n+\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessString.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -24,0 +24,11 @@\n+#warn\n+\n+#if[Value]\n+\/*\n+ * @test\n+ * @run testng\/othervm -Diters=10    -Xint                   VarHandleTestAccess$Type$\n+ * @run testng\/othervm -Diters=20000 -XX:TieredStopAtLevel=1 VarHandleTestAccess$Type$\n+ * @run testng\/othervm -Diters=20000                         VarHandleTestAccess$Type$\n+ * @run testng\/othervm -Diters=20000 -XX:-TieredCompilation  VarHandleTestAccess$Type$\n+ *\/\n+#else[Value]\n@@ -31,0 +42,1 @@\n+#end[Value]\n@@ -71,1 +83,1 @@\n-#if[String]\n+#if[Object]]\n@@ -73,1 +85,4 @@\n-#end[String]\n+#end[Object]]\n+#if[Value]\n+    VarHandle vhValueTypeField;\n+#end[Value]\n@@ -129,1 +144,1 @@\n-#if[String]\n+#if[Object]]\n@@ -131,1 +146,6 @@\n-#end[String]\n+#end[Object]]\n+\n+#if[Value]\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"$varType$_v\", $type$.class);\n+#end[Value]\n@@ -313,1 +333,1 @@\n-#if[String]\n+#if[Object]]\n@@ -316,1 +336,1 @@\n-#end[String]\n+#end[Object]]\n@@ -323,1 +343,1 @@\n-#if[String]\n+#if[Object]]\n@@ -327,1 +347,8 @@\n-#end[String]\n+#end[Object]]\n+#if[Value]\n+        cases.add(new VarHandleAccessTestCase(\"Value type field\",\n+                                              vhValueTypeField, vh -> testValueTypeField(Value.getInstance(), vh)));\n+        cases.add(new VarHandleAccessTestCase(\"Value type field unsupported\",\n+                                              vhValueTypeField, vh -> testValueTypeFieldUnsupported(Value.getInstance(), vh),\n+                                              false));\n+#end[Value]\n@@ -489,0 +516,15 @@\n+#if[Value]\n+    static void testValueTypeField(Value recv, VarHandle vh) {\n+        \/\/ Plain\n+        {\n+            $type$ x = ($type$) vh.get(recv);\n+            assertEquals(x, $value1$, \"get $type$ value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, VarHandle vh) {\n+        checkUOE(() -> {\n+            vh.set(recv, $value2$);\n+        });\n+    }\n+#end[Value]\n@@ -1909,1 +1951,1 @@\n-#if[String]\n+#if[Object]]\n@@ -1990,1 +2032,1 @@\n-#end[String]\n+#end[Object]]\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/X-VarHandleTestAccess.java.template","additions":52,"deletions":10,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+#warn\n+\n@@ -26,0 +28,3 @@\n+#if[Value]\n+ * @run testng\/othervm -Diters=2000 VarHandleTestMethodHandleAccess$Type$\n+#else[Value]\n@@ -27,0 +32,1 @@\n+#end[Value]\n@@ -60,0 +66,4 @@\n+#if[Value]\n+    VarHandle vhValueTypeField;\n+#end[Value]\n+\n@@ -75,0 +85,5 @@\n+\n+#if[Value]\n+        vhValueTypeField = MethodHandles.lookup().findVarHandle(\n+                    Value.class, \"$varType$_v\", $type$.class);\n+#end[Value]\n@@ -103,0 +118,7 @@\n+#if[Value]\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeField(Value.getInstance(), hs)));\n+        cases.add(new MethodHandleAccessTestCase(\"Value type field unsupported\",\n+                                                 vhValueTypeField, f, hs -> testValueTypeFieldUnsupported(Value.getInstance(), hs),\n+                                                 false));\n+#end[Value]\n@@ -415,0 +437,53 @@\n+#if[Value]\n+    static void testValueTypeField(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        {\n+            $type$ x = ($type$) hs.get(TestAccessMode.GET).invokeExact(recv);\n+            assertEquals(x, $value1$, \"get $type$ value\");\n+        }\n+    }\n+\n+    static void testValueTypeFieldUnsupported(Value recv, Handles hs) throws Throwable {\n+        \/\/ Plain\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.SET)) {\n+            checkUOE(am, () -> {\n+                hs.get(am).invokeExact(recv, $value1$);\n+            });\n+        }\n+#if[!CAS]\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.COMPARE_AND_SET)) {\n+            checkUOE(am, () -> {\n+                boolean r = (boolean) hs.get(am).invokeExact(recv, $value1$, $value2$);\n+            });\n+        }\n+\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.COMPARE_AND_EXCHANGE)) {\n+            checkUOE(am, () -> {\n+                $type$ r = ($type$) hs.get(am).invokeExact(recv, $value1$, $value2$);\n+            });\n+        }\n+\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.GET_AND_SET)) {\n+            checkUOE(am, () -> {\n+                $type$ r = ($type$) hs.get(am).invokeExact(recv, $value1$);\n+            });\n+        }\n+#end[CAS]\n+\n+#if[!AtomicAdd]\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.GET_AND_ADD)) {\n+            checkUOE(am, () -> {\n+                $type$ r = ($type$) hs.get(am).invokeExact(recv, $value1$);\n+            });\n+        }\n+#end[AtomicAdd]\n+\n+#if[!Bitwise]\n+        for (TestAccessMode am : testAccessModesOfType(TestAccessType.GET_AND_BITWISE)) {\n+            checkUOE(am, () -> {\n+                $type$ r = ($type$) hs.get(am).invokeExact(recv, $value1$);\n+            });\n+        }\n+#end[Bitwise]\n+    }\n+#end[Value]\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/X-VarHandleTestMethodHandleAccess.java.template","additions":75,"deletions":0,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -314,0 +314,1 @@\n+        vmOptFinalFlag(map, \"TieredCompilation\");\n@@ -362,0 +363,3 @@\n+        if (WB.getBooleanVMFlag(\"EnableValhalla\").booleanValue()) {\n+            return \"false\";\n+        }\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -151,0 +151,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n","filename":"test\/lib\/sun\/hotspot\/WhiteBox.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"}]}