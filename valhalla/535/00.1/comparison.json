{"files":[{"patch":"@@ -1770,0 +1770,3 @@\n+  } else if (_entry_point == NULL) {\n+    \/\/ See CallLeafNoFPIndirect\n+    return 1 * NativeInstruction::instruction_size;\n@@ -1887,3 +1890,0 @@\n-  \/\/ n.b. frame size includes space for return pc and rfp\n-  const int framesize = C->output()->frame_size_in_bytes();\n-\n@@ -1909,5 +1909,2 @@\n-  int bangsize = C->output()->bang_size_in_bytes();\n-  if (C->output()->need_stack_bang(bangsize))\n-    __ generate_stack_overflow_check(bangsize);\n-\n-  __ build_frame(framesize);\n+  __ verified_entry(C, 0);\n+  __ bind(*_verified_entry);\n@@ -1934,6 +1931,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1979,1 +1970,1 @@\n-  __ remove_frame(framesize);\n+  __ remove_frame(framesize, C->needs_stack_repair(), C->output()->sp_inc_offset());\n@@ -1996,5 +1987,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {\n-  \/\/ Variable size. Determine dynamically.\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2283,1 +2269,23 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n+\n+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+{\n+  MacroAssembler _masm(&cbuf);\n+\n+  if (!_verified) {\n+    Label skip;\n+    __ cmp_klass(j_rarg0, rscratch2, rscratch1);\n+    __ br(Assembler::EQ, skip);\n+      __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+    __ bind(skip);\n@@ -2285,0 +2293,11 @@\n+  } else {\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    __ b(*_verified_entry);\n+  }\n+}\n+\n+\/\/=============================================================================\n@@ -2306,0 +2325,1 @@\n+  Label skip;\n@@ -2307,0 +2327,1 @@\n+  \/\/ UseCompressedClassPointers logic are inside cmp_klass\n@@ -2308,1 +2329,1 @@\n-  Label skip;\n+\n@@ -2316,5 +2337,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_);\n-}\n-\n@@ -3739,0 +3755,11 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic()) {\n+      \/\/ An inline type is returned as fields in multiple registers.\n+      \/\/ R0 either contains an oop if the inline type is buffered or a pointer\n+      \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero r0\n+      \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+      \/\/ r0 &= (r0 & 1) - 1\n+      C2_MacroAssembler _masm(&cbuf);\n+      __ andr(rscratch1, r0, 0x1);\n+      __ sub(rscratch1, rscratch1, 0x1);\n+      __ andr(r0, r0, rscratch1);\n+    }\n@@ -3830,0 +3857,5 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      __ andr(tmp, tmp, ~((int) markWord::inline_type_bit_in_place));\n+    }\n+\n@@ -7492,1 +7524,1 @@\n-    \"mov  $dst, $con\\t# ptr\\n\\t\"\n+    \"mov  $dst, $con\\t# ptr\"\n@@ -8624,0 +8656,15 @@\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -14967,1 +15014,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg_immL0(iRegL_R11 cnt, iRegP_R10 base, immL0 zero, Universe dummy, rFlagsReg cr)\n@@ -14969,1 +15016,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) zero));\n@@ -14986,0 +15033,16 @@\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, KILL cr);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ClearArray $cnt, $base, $val\" %}\n+\n+  ins_encode %{\n+    __ fill_words($base$$Register, $cnt$$Register, $val$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -14989,1 +15052,2 @@\n-            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord));\n+            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord)\n+            && !((ClearArrayNode*)n)->word_copy_only());\n@@ -16319,0 +16383,18 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPIndirect(iRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == NULL);\n+\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(CALL_COST);\n+\n+  format %{ \"CALL, runtime leaf nofp indirect $target\" %}\n+\n+  ins_encode %{\n+    __ blr($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n@@ -16321,0 +16403,2 @@\n+  predicate(n->as_Call()->entry_point() != NULL);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":115,"deletions":31,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -52,0 +53,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -55,0 +57,1 @@\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -774,0 +777,35 @@\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  ldr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  ldr(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  ldr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset);\n+  load_heap_oop(obj, field);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n@@ -1124,1 +1162,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1154,1 +1196,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1233,0 +1279,1 @@\n+  assert_different_registers(arg_1, c_rarg0);\n@@ -1240,0 +1287,2 @@\n+  assert_different_registers(arg_1, c_rarg0);\n+  assert_different_registers(arg_2, c_rarg0, c_rarg1);\n@@ -1246,0 +1295,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -1295,0 +1348,109 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  assert_different_registers(markword, rscratch2);\n+  andr(markword, markword, markWord::inline_type_mask_in_place);\n+  mov(rscratch2, markWord::inline_type_pattern);\n+  cmp(markword, rscratch2);\n+  br(Assembler::EQ, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  andr(temp_reg, temp_reg, JVM_ACC_INLINE);\n+  cbnz(temp_reg, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  cbz(object, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  ldr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  mov(rscratch1, is_inline_type_mask);\n+  andr(tmp, tmp, rscratch1);\n+  cmp(tmp, rscratch1);\n+  br(Assembler::NE, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  ldrw(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  andr(temp_reg, temp_reg, InstanceKlass::misc_flags_is_empty_inline_type());\n+  cbnz(temp_reg, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_null_free_inline_type_shift, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbz(flags, ConstantPoolCacheEntry::is_null_free_inline_type_shift, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_inlined_shift, is_flattened);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  ldr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  tst(temp_reg, markWord::unlocked_value);\n+  br(Assembler::NE, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  load_prototype_header(temp_reg, oop);\n+\n+  bind(test_mark_word);\n+  andr(temp_reg, temp_reg, test_bit);\n+  if (jmp_set) {\n+    cbnz(temp_reg, jmp_label);\n+  } else {\n+    cbz(temp_reg, jmp_label);\n+  }\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label& is_flattened_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flattened_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {\n+  tst(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  br(Assembler::NE, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array) {\n+  tst(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  br(Assembler::EQ, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {\n+  tst(lh, Klass::_lh_null_free_bit_inplace);\n+  br(Assembler::NE, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {\n+  tst(lh, Klass::_lh_null_free_bit_inplace);\n+  br(Assembler::EQ, is_non_null_free_array);\n+}\n+\n@@ -3607,0 +3769,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -3666,0 +3836,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src) {\n+  load_klass(dst, src);\n+  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -3979,1 +4154,2 @@\n-                                     Register tmp1, Register thread_tmp) {\n+                                     Register tmp1, Register thread_tmp, Register tmp3) {\n+\n@@ -3984,1 +4160,1 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -3986,1 +4162,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -3990,0 +4166,40 @@\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  ldr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  ldrw(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    add(data, data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rscratch1, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  ldrw(rscratch1, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  lsr(rscratch1, rscratch1, Klass::_lh_log2_element_size_shift);\n+  andr(rscratch1, rscratch1, Klass::_lh_log2_element_size_mask);\n+  lslv(index, index, rscratch1);\n+\n+  add(data, array, index);\n+  add(data, data, arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE));\n+}\n+\n@@ -4001,2 +4217,2 @@\n-                                    Register thread_tmp, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);\n+                                    Register thread_tmp, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4007,1 +4223,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -4071,0 +4287,119 @@\n+\/\/ Object \/ value buffer allocation...\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == r0, \"needs to be r0, according to barrier asm eden_allocate\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    ldrb(rscratch1, Address(klass, InstanceKlass::init_state_offset()));\n+    cmpw(rscratch1, InstanceKlass::fully_initialized);\n+    br(Assembler::EQ, L);\n+    stop(\"klass not initialized\");\n+    bind(L);\n+  }\n+#endif\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  ldrw(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  tst(layout_size, Klass::_lh_instance_slow_path_bit);\n+  br(Assembler::NE, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+  const bool allow_shared_alloc =\n+    Universe::heap()->supports_inline_contig_alloc();\n+\n+  push(klass);\n+\n+  if (UseTLAB) {\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      b(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      b(initialize_object);\n+    }\n+  } else {\n+    \/\/ Allocation in the shared Eden, if allowed.\n+    \/\/\n+    eden_allocate(new_obj, layout_size, 0, t2, slow_case);\n+  }\n+\n+  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n+  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n+  if (UseTLAB || allow_shared_alloc) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      subs(layout_size, layout_size, sizeof(oopDesc));\n+      br(Assembler::EQ, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      tst(layout_size, 7);\n+      br(Assembler::EQ, L);\n+      stop(\"object size is not multiple of 8 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      lsr(layout_size, layout_size, LogBytesPerLong);\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        Register base = t2;\n+\n+        bind(loop);\n+        add(rscratch1, new_obj, layout_size, Assembler::LSL, LogBytesPerLong);\n+        str(zr, Address(rscratch1, sizeof(oopDesc) - 1*oopSize));\n+        subs(layout_size, layout_size, 1);\n+        br(Assembler::NE, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    ldr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes ()));\n+    store_klass_gap(new_obj, zr);  \/\/ zero klass gap for compressed oops\n+    mov(t2, klass);         \/\/ preserve klass\n+    store_klass(new_obj, t2);  \/\/ src klass reg is potentially compressed\n+\n+    b(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  b(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -4120,0 +4455,13 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  ldr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cbnz(inline_klass, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  ldr(inline_klass, Address(inline_klass, index, Address::lsl(3)));\n+}\n+\n@@ -4243,0 +4591,47 @@\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {\n+  if (needs_stack_repair) {\n+    \/\/ Remove the extension of the caller's frame used for inline type unpacking\n+    \/\/\n+    \/\/ Right now the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Saved LR #1               |\n+    \/\/ | Saved FP #1               |\n+    \/\/ |---------------------------|\n+    \/\/ | Extension space for       |\n+    \/\/ |   inline arg (un)packing  |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | Saved LR #2               |\n+    \/\/ | Saved FP #2               |\n+    \/\/ |---------------------------|  <-- FP\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ There are two copies of FP and LR on the stack. They will be identical\n+    \/\/ unless the caller has been deoptimized, in which case LR #1 will be patched\n+    \/\/ to point at the deopt blob, and LR #2 will still point into the old method.\n+    \/\/\n+    \/\/ The sp_inc stack slot holds the total size of the frame including the\n+    \/\/ extension space minus two words for the saved FP and LR.\n+\n+    ldr(rscratch1, Address(sp, sp_inc_offset));\n+    add(sp, sp, rscratch1);\n+    ldp(rfp, lr, Address(post(sp, 2 * wordSize)));\n+  } else {\n+    remove_frame(initial_framesize);\n+  }\n+}\n+\n+void MacroAssembler::save_stack_increment(int sp_inc, int frame_size, int sp_inc_offset) {\n+  int real_frame_size = frame_size + sp_inc;\n+  assert(sp_inc == 0 || sp_inc > 2*wordSize, \"invalid sp_inc value\");\n+  assert(real_frame_size >= 2*wordSize, \"frame size must include FP\/LR space\");\n+  assert((real_frame_size & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+\n+  \/\/ Subtract two words for the saved FP and LR as these will be popped\n+  \/\/ separately. See remove_frame above.\n+  mov(rscratch1, real_frame_size - 2*wordSize);\n+  str(rscratch1, Address(sp, sp_inc_offset));\n+}\n@@ -5072,0 +5467,352 @@\n+#ifdef COMPILER2\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+\n+  \/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->output()->frame_size_in_bytes();\n+\n+  \/\/ insert a nop at the start of the prolog so we can patch in a\n+  \/\/ branch if we need to invalidate the method later\n+  nop();\n+\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  if (C->output()->need_stack_bang(bangsize))\n+    generate_stack_overflow_check(bangsize);\n+\n+  build_frame(framesize);\n+\n+  if (C->needs_stack_repair()) {\n+    save_stack_increment(sp_inc, framesize, C->output()->sp_inc_offset());\n+  }\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+#endif \/\/ COMPILER2\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  tbz(r0, 0, skip);\n+  int call_offset = -1;\n+\n+  \/\/ Be careful not to clobber r1-7 which hold returned fields\n+  \/\/ Also do not use callee-saved registers as these may be live in the interpreter\n+  Register tmp1 = r13, tmp2 = r14, klass = r15, r0_preserved = r12;\n+\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != NULL, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(r0_preserved, r0); \/\/ save r0 for slow_case since *_allocate may corrupt it when allocation failed\n+\n+  if (vk != NULL) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(klass, (intptr_t)vk->get_InlineKlass());\n+    jint obj_size = vk->layout_helper();\n+    assert(obj_size != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB) {\n+      tlab_allocate(r0, noreg, obj_size, tmp1, tmp2, slow_case);\n+    } else {\n+      eden_allocate(r0, noreg, obj_size, tmp1, slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+    andr(klass, r0, -2);\n+    ldrw(tmp2, Address(klass, Klass::layout_helper_offset()));\n+    if (UseTLAB) {\n+      tlab_allocate(r0, tmp2, 0, tmp1, tmp2, slow_case);\n+    } else {\n+      eden_allocate(r0, tmp2, 0, tmp1, slow_case);\n+    }\n+  }\n+  if (UseTLAB || Universe::heap()->supports_inline_contig_alloc()) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = r0;\n+    mov(rscratch1, (intptr_t)markWord::inline_type_prototype().value());\n+    str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+    store_klass_gap(buffer_obj, zr);\n+    if (vk == NULL) {\n+      \/\/ store_klass corrupts klass, so save it for later use (interpreter case only).\n+      mov(tmp1, klass);\n+    }\n+    store_klass(buffer_obj, klass);\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != NULL) {\n+      far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      \/\/ tmp1 holds klass preserved above\n+      ldr(tmp1, Address(tmp1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      ldr(tmp1, Address(tmp1, InlineKlass::pack_handler_offset()));\n+      blr(tmp1);\n+    }\n+\n+    membar(Assembler::StoreStore);\n+    b(skip);\n+  } else {\n+    \/\/ Must have already branched to slow_case in eden_allocate() above.\n+    DEBUG_ONLY(should_not_reach_here());\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(r0, r0_preserved);\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    far_call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+  membar(Assembler::StoreStore);\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_Register() && to->is_Register()) {\n+          mov(to->as_Register(), from->as_Register());\n+        } else if (from->is_FloatRegister() && to->is_FloatRegister()) {\n+          fmovd(to->as_FloatRegister(), from->as_FloatRegister());\n+        } else {\n+          ShouldNotReachHere();\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+            ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  int sp_inc = args_on_stack * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  assert(sp_inc > 0, \"sanity\");\n+\n+  \/\/ Save a copy of the FP and LR here for deoptimization patching and frame walking\n+  stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+\n+  \/\/ Adjust the stack pointer. This will be repaired on return by MacroAssembler::remove_frame\n+  if (sp_inc < (1 << 9)) {\n+    sub(sp, sp, sp_inc);   \/\/ Fits in an immediate\n+  } else {\n+    mov(rscratch1, sp_inc);\n+    sub(sp, sp, rscratch1);\n+  }\n+\n+  return sp_inc + 2 * wordSize;  \/\/ Account for the FP\/LR space\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  Register tmp1 = r10, tmp2 = r11;\n+  Register fromReg;\n+  if (from->is_reg()) {\n+    fromReg = from->as_Register();\n+  } else {\n+    int st_off = from->reg2stack() * VMRegImpl::stack_slot_size;\n+    ldr(tmp1, Address(sp, st_off));\n+    fromReg = tmp1;\n+  }\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    } else {\n+      assert(reg_state[idx] == reg_writable, \"must be writable\");\n+      reg_state[idx] = reg_written;\n+    }\n+\n+    if (!toReg->is_FloatRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        str(dst, Address(sp, st_off));\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      ldrd(toReg->as_FloatRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      ldrs(toReg->as_FloatRegister(), fromAddr);\n+    }\n+  }\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ The GC barrier expanded by store_heap_oop below may call into the\n+  \/\/ runtime so use callee-saved registers for any values that need to be\n+  \/\/ preserved. The GC barrier assembler should take care of saving the\n+  \/\/ Java argument registers.\n+  Register val_obj_tmp = r21;\n+  Register from_reg_tmp = r22;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r12;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+\n+    if (!fromReg->is_FloatRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        load_sized_value(src, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      strd(fromReg->as_FloatRegister(), dst);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      strs(fromReg->as_FloatRegister(), dst);\n+    }\n+    reg_state[fromReg->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v0->as_VMReg() : r14->as_VMReg();\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":755,"deletions":8,"binary":false,"changes":763,"status":"modified"},{"patch":"@@ -1574,1 +1574,1 @@\n-  if (!UseFastStosb && UseSSE >= 2 && UseUnalignedLoadStores) {\n+  if (UseSSE >= 2 && UseUnalignedLoadStores) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2427,0 +2427,12 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic()) {\n+      \/\/ An inline type is returned as fields in multiple registers.\n+      \/\/ Rax either contains an oop if the inline type is buffered or a pointer\n+      \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero rax\n+      \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+      \/\/ rax &= (rax & 1) - 1\n+      C2_MacroAssembler _masm(&cbuf);\n+      __ movptr(rscratch1, rax);\n+      __ andptr(rscratch1, 0x1);\n+      __ subptr(rscratch1, 0x1);\n+      __ andptr(rax, rscratch1);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -213,0 +215,2 @@\n+  assert(!_gen->in_conditional_code(), \"LIRItem cannot be loaded in conditional code\");\n+\n@@ -608,1 +612,2 @@\n-void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info) {\n+void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no,\n+                                 CodeEmitInfo* info_for_exception, CodeEmitInfo* info, CodeStub* throw_imse_stub) {\n@@ -611,1 +616,1 @@\n-  CodeStub* slow_path = new MonitorEnterStub(object, lock, info);\n+  CodeStub* slow_path = new MonitorEnterStub(object, lock, info, throw_imse_stub, scratch);\n@@ -614,1 +619,1 @@\n-  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception);\n+  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception, throw_imse_stub);\n@@ -638,4 +643,9 @@\n-void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n-  klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n-  \/\/ If klass is not loaded we do not know if the klass has finalizers:\n-  if (UseFastNewInstance && klass->is_loaded()\n+void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, bool allow_inline, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n+  if (allow_inline) {\n+    assert(!is_unresolved && klass->is_loaded(), \"inline type klass should be resolved\");\n+    __ metadata2reg(klass->constant_encoding(), klass_reg);\n+  } else {\n+    klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n+  }\n+  \/\/ If klass is not loaded we do not know if the klass has finalizers or is an unexpected inline klass\n+  if (UseFastNewInstance && klass->is_loaded() && (allow_inline || !klass->is_inlinetype())\n@@ -655,2 +665,2 @@\n-    CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, Runtime1::new_instance_id);\n-    __ branch(lir_cond_always, slow_path);\n+    CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, allow_inline ? Runtime1::new_instance_id : Runtime1::new_instance_no_inline_id);\n+    __ jump(slow_path);\n@@ -756,0 +766,10 @@\n+  if (!src->is_loaded_flattened_array() && !dst->is_loaded_flattened_array()) {\n+    flags &= ~LIR_OpArrayCopy::always_slow_path;\n+  }\n+  if (!src->maybe_flattened_array()) {\n+    flags &= ~LIR_OpArrayCopy::src_inlinetype_check;\n+  }\n+  if (!dst->maybe_flattened_array() && !dst->maybe_null_free_array()) {\n+    flags &= ~LIR_OpArrayCopy::dst_inlinetype_check;\n+  }\n+\n@@ -1528,2 +1548,4 @@\n-  _constants.append(c);\n-  _reg_for_constants.append(result);\n+  if (!in_conditional_code()) {\n+    _constants.append(c);\n+    _reg_for_constants.append(result);\n+  }\n@@ -1533,0 +1555,6 @@\n+void LIRGenerator::set_in_conditional_code(bool v) {\n+  assert(v != _in_conditional_code, \"must change state\");\n+  _in_conditional_code = v;\n+}\n+\n+\n@@ -1624,0 +1652,5 @@\n+  if (!inline_type_field_access_prolog(x, info)) {\n+    \/\/ Field store will always deopt due to unloaded field or holder klass\n+    return;\n+  }\n+\n@@ -1645,0 +1678,171 @@\n+\/\/ FIXME -- I can't find any other way to pass an address to access_load_at().\n+class TempResolvedAddress: public Instruction {\n+ public:\n+  TempResolvedAddress(ValueType* type, LIR_Opr addr) : Instruction(type) {\n+    set_operand(addr);\n+  }\n+  virtual void input_values_do(ValueVisitor*) {}\n+  virtual void visit(InstructionVisitor* v)   {}\n+  virtual const char* name() const  { return \"TempResolvedAddress\"; }\n+};\n+\n+LIR_Opr LIRGenerator::get_and_load_element_address(LIRItem& array, LIRItem& index) {\n+  ciType* array_type = array.value()->declared_type();\n+  ciFlatArrayKlass* flat_array_klass = array_type->as_flat_array_klass();\n+  assert(flat_array_klass->is_loaded(), \"must be\");\n+\n+  int array_header_size = flat_array_klass->array_header_in_bytes();\n+  int shift = flat_array_klass->log2_element_size();\n+\n+#ifndef _LP64\n+  LIR_Opr index_op = new_register(T_INT);\n+  \/\/ FIXME -- on 32-bit, the shift below can overflow, so we need to check that\n+  \/\/ the top (shift+1) bits of index_op must be zero, or\n+  \/\/ else throw ArrayIndexOutOfBoundsException\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::intConst(const_index << shift), index_op);\n+  } else {\n+    __ shift_left(index_op, shift, index.result());\n+  }\n+#else\n+  LIR_Opr index_op = new_register(T_LONG);\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::longConst(const_index << shift), index_op);\n+  } else {\n+    __ convert(Bytecodes::_i2l, index.result(), index_op);\n+    \/\/ Need to shift manually, as LIR_Address can scale only up to 3.\n+    __ shift_left(index_op, shift, index_op);\n+  }\n+#endif\n+\n+  LIR_Opr elm_op = new_pointer_register();\n+  LIR_Address* elm_address = generate_address(array.result(), index_op, 0, array_header_size, T_ADDRESS);\n+  __ leal(LIR_OprFact::address(elm_address), elm_op);\n+  return elm_op;\n+}\n+\n+void LIRGenerator::access_sub_element(LIRItem& array, LIRItem& index, LIR_Opr& result, ciField* field, int sub_offset) {\n+  assert(field != NULL, \"Need a subelement type specified\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  BasicType subelt_type = field->type()->basic_type();\n+  TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(subelt_type), elm_op);\n+  LIRItem elm_item(elm_resolved_addr, this);\n+\n+  DecoratorSet decorators = IN_HEAP;\n+  access_load_at(decorators, subelt_type,\n+                     elm_item, LIR_OprFact::intConst(sub_offset), result,\n+                     NULL, NULL);\n+\n+  if (field->is_null_free()) {\n+    assert(field->type()->as_inline_klass()->is_loaded(), \"Must be\");\n+    LabelObj* L_end = new LabelObj();\n+    __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(NULL));\n+    __ branch(lir_cond_notEqual, L_end->label());\n+    set_in_conditional_code(true);\n+    Constant* default_value = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+    if (default_value->is_pinned()) {\n+      __ move(LIR_OprFact::value_type(default_value->type()), result);\n+    } else {\n+      __ move(load_constant(default_value), result);\n+    }\n+    __ branch_destination(L_end->label());\n+    set_in_conditional_code(false);\n+  }\n+}\n+\n+void LIRGenerator::access_flattened_array(bool is_load, LIRItem& array, LIRItem& index, LIRItem& obj_item,\n+                                          ciField* field, int sub_offset) {\n+  assert(sub_offset == 0 || field != NULL, \"Sanity check\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  ciInlineKlass* elem_klass = NULL;\n+  if (field != NULL) {\n+    elem_klass = field->type()->as_inline_klass();\n+  } else {\n+    elem_klass = array.value()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+  }\n+  for (int i = 0; i < elem_klass->nof_nonstatic_fields(); i++) {\n+    ciField* inner_field = elem_klass->nonstatic_field_at(i);\n+    assert(!inner_field->is_flattened(), \"flattened fields must have been expanded\");\n+    int obj_offset = inner_field->offset();\n+    int elm_offset = obj_offset - elem_klass->first_field_offset() + sub_offset; \/\/ object header is not stored in array.\n+    BasicType field_type = inner_field->type()->basic_type();\n+\n+    \/\/ Types which are smaller than int are still passed in an int register.\n+    BasicType reg_type = field_type;\n+    switch (reg_type) {\n+    case T_BYTE:\n+    case T_BOOLEAN:\n+    case T_SHORT:\n+    case T_CHAR:\n+      reg_type = T_INT;\n+      break;\n+    default:\n+      break;\n+    }\n+\n+    LIR_Opr temp = new_register(reg_type);\n+    TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(field_type), elm_op);\n+    LIRItem elm_item(elm_resolved_addr, this);\n+\n+    DecoratorSet decorators = IN_HEAP;\n+    if (is_load) {\n+      access_load_at(decorators, field_type,\n+                     elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                     NULL, NULL);\n+      access_store_at(decorators, field_type,\n+                      obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                      NULL, NULL);\n+    } else {\n+      access_load_at(decorators, field_type,\n+                     obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                     NULL, NULL);\n+      access_store_at(decorators, field_type,\n+                      elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                      NULL, NULL);\n+    }\n+  }\n+}\n+\n+void LIRGenerator::check_flattened_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path) {\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_flattened_array(array, value, tmp, slow_path);\n+}\n+\n+void LIRGenerator::check_null_free_array(LIRItem& array, LIRItem& value, CodeEmitInfo* info) {\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+  __ branch(lir_cond_equal, L_end->label());\n+  __ null_check(value.result(), info);\n+  __ branch_destination(L_end->label());\n+}\n+\n+bool LIRGenerator::needs_flattened_array_store_check(StoreIndexed* x) {\n+  if (x->elt_type() == T_OBJECT && x->array()->maybe_flattened_array()) {\n+    ciType* type = x->value()->declared_type();\n+    if (type != NULL && type->is_klass()) {\n+      ciKlass* klass = type->as_klass();\n+      if (!klass->can_be_inline_klass() || (klass->is_inlinetype() && !klass->as_inline_klass()->flatten_array())) {\n+        \/\/ This is known to be a non-flattened object. If the array is flattened,\n+        \/\/ it will be caught by the code generated by array_store_check().\n+        return false;\n+      }\n+    }\n+    \/\/ We're not 100% sure, so let's do the flattened_array_store_check.\n+    return true;\n+  }\n+  return false;\n+}\n+\n+bool LIRGenerator::needs_null_free_array_store_check(StoreIndexed* x) {\n+  return x->elt_type() == T_OBJECT && x->array()->maybe_null_free_array();\n+}\n+\n@@ -1647,0 +1851,2 @@\n+  assert(x->elt_type() != T_ARRAY, \"never used\");\n+  bool is_loaded_flattened_array = x->array()->is_loaded_flattened_array();\n@@ -1650,3 +1856,3 @@\n-  bool needs_store_check = obj_store && (x->value()->as_Constant() == NULL ||\n-                                         !get_jobject_constant(x->value())->is_null_object() ||\n-                                         x->should_profile());\n+  bool needs_store_check = obj_store && !(is_loaded_flattened_array && x->is_exact_flattened_array_store()) &&\n+                                        (x->value()->as_Constant() == NULL ||\n+                                         !get_jobject_constant(x->value())->is_null_object());\n@@ -1665,2 +1871,3 @@\n-\n-  if (needs_store_check || x->check_boolean()) {\n+\n+  if (needs_store_check || x->check_boolean()\n+      || is_loaded_flattened_array || needs_flattened_array_store_check(x) || needs_null_free_array_store_check(x)) {\n@@ -1695,0 +1902,16 @@\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flattened_array()) {\n+      \/\/ No need to profile a store to a flattened array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      ciMethodData* md = NULL;\n+      ciArrayLoadStoreData* load_store = NULL;\n+      profile_array_type(x, md, load_store);\n+      if (x->array()->maybe_null_free_array()) {\n+        profile_null_free_array(array, md, load_store);\n+      }\n+      profile_element_type(x->value(), md, load_store);\n+    }\n+  }\n+\n@@ -1697,1 +1920,1 @@\n-    array_store_check(value.result(), array.result(), store_check_info, x->profiled_method(), x->profiled_bci());\n+    array_store_check(value.result(), array.result(), store_check_info, NULL, -1);\n@@ -1700,4 +1923,21 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-  if (x->check_boolean()) {\n-    decorators |= C1_MASK_BOOLEAN;\n-  }\n+  if (is_loaded_flattened_array) {\n+    if (!x->value()->is_null_free()) {\n+      __ null_check(value.result(), new CodeEmitInfo(range_check_info));\n+    }\n+    \/\/ If array element is an empty inline type, no need to copy anything\n+    if (!x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+      access_flattened_array(false, array, index, value);\n+    }\n+  } else {\n+    StoreFlattenedArrayStub* slow_path = NULL;\n+\n+    if (needs_flattened_array_store_check(x)) {\n+      \/\/ Check if we indeed have a flattened array\n+      index.load_item();\n+      slow_path = new StoreFlattenedArrayStub(array.result(), index.result(), value.result(), state_for(x, x->state_before()));\n+      check_flattened_array(array.result(), value.result(), slow_path);\n+      set_in_conditional_code(true);\n+    } else if (needs_null_free_array_store_check(x)) {\n+      CodeEmitInfo* info = new CodeEmitInfo(range_check_info);\n+      check_null_free_array(array, value, info);\n+    }\n@@ -1705,2 +1945,12 @@\n-  access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n-                  NULL, null_check_info);\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    if (x->check_boolean()) {\n+      decorators |= C1_MASK_BOOLEAN;\n+    }\n+\n+    access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n+                    NULL, null_check_info);\n+    if (slow_path != NULL) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+  }\n@@ -1787,0 +2037,25 @@\n+bool LIRGenerator::inline_type_field_access_prolog(AccessField* x, CodeEmitInfo* info) {\n+  ciField* field = x->field();\n+  assert(!field->is_flattened(), \"Flattened field access should have been expanded\");\n+  if (!field->is_null_free()) {\n+    return true; \/\/ Not an inline type field\n+  }\n+  \/\/ Deoptimize if the access is non-static and requires patching (holder not loaded\n+  \/\/ or not accessible) because then we only have partial field information and the\n+  \/\/ field could be flattened (see ciField constructor).\n+  bool could_be_flat = !x->is_static() && x->needs_patching();\n+  \/\/ Deoptimize if we load from a static field with an unloaded type because we need\n+  \/\/ the default value if the field is null.\n+  bool could_be_null = x->is_static() && x->as_LoadField() != NULL && !field->type()->is_loaded();\n+  assert(!could_be_null || !field->holder()->is_loaded(), \"inline type field should be loaded\");\n+  if (could_be_flat || could_be_null) {\n+    assert(x->needs_patching(), \"no deopt required\");\n+    CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),\n+                                        Deoptimization::Reason_unloaded,\n+                                        Deoptimization::Action_make_not_entrant);\n+    __ jump(stub);\n+    return false;\n+  }\n+  return true;\n+}\n+\n@@ -1816,0 +2091,7 @@\n+  if (!inline_type_field_access_prolog(x, info)) {\n+    \/\/ Field load will always deopt due to unloaded field or holder klass\n+    LIR_Opr result = rlock_result(x, field_type);\n+    __ move(LIR_OprFact::oopConst(NULL), result);\n+    return;\n+  }\n+\n@@ -1844,0 +2126,29 @@\n+\n+  ciField* field = x->field();\n+  if (field->is_null_free()) {\n+    \/\/ Load from non-flattened inline type field requires\n+    \/\/ a null check to replace null with the default value.\n+    ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+    assert(inline_klass->is_loaded(), \"field klass must be loaded\");\n+\n+    ciInstanceKlass* holder = field->holder();\n+    if (field->is_static() && holder->is_loaded()) {\n+      ciObject* val = holder->java_mirror()->field_value(field).as_object();\n+      if (!val->is_null_object()) {\n+        \/\/ Static field is initialized, we don need to perform a null check.\n+        return;\n+      }\n+    }\n+    LabelObj* L_end = new LabelObj();\n+    __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(NULL));\n+    __ branch(lir_cond_notEqual, L_end->label());\n+    set_in_conditional_code(true);\n+    Constant* default_value = new Constant(new InstanceConstant(inline_klass->default_instance()));\n+    if (default_value->is_pinned()) {\n+      __ move(LIR_OprFact::value_type(default_value->type()), result);\n+    } else {\n+      __ move(load_constant(default_value), result);\n+    }\n+    __ branch_destination(L_end->label());\n+    set_in_conditional_code(false);\n+  }\n@@ -1988,1 +2299,66 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  ciMethodData* md = NULL;\n+  ciArrayLoadStoreData* load_store = NULL;\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flattened_array()) {\n+      \/\/ No need to profile a load from a flattened array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      profile_array_type(x, md, load_store);\n+    }\n+  }\n+\n+  Value element;\n+  if (x->vt() != NULL) {\n+    assert(x->array()->is_loaded_flattened_array(), \"must be\");\n+    \/\/ Find the destination address (of the NewInlineTypeInstance).\n+    LIRItem obj_item(x->vt(), this);\n+\n+    access_flattened_array(true, array, index, obj_item,\n+                           x->delayed() == NULL ? 0 : x->delayed()->field(),\n+                           x->delayed() == NULL ? 0 : x->delayed()->offset());\n+    set_no_result(x);\n+  } else if (x->delayed() != NULL) {\n+    assert(x->array()->is_loaded_flattened_array(), \"must be\");\n+    LIR_Opr result = rlock_result(x, x->delayed()->field()->type()->basic_type());\n+    access_sub_element(array, index, result, x->delayed()->field(), x->delayed()->offset());\n+  } else if (x->array() != NULL && x->array()->is_loaded_flattened_array() &&\n+             x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+    \/\/ Load the default instance instead of reading the element\n+    ciInlineKlass* elem_klass = x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    Constant* default_value = new Constant(new InstanceConstant(elem_klass->default_instance()));\n+    if (default_value->is_pinned()) {\n+      __ move(LIR_OprFact::value_type(default_value->type()), result);\n+    } else {\n+      __ move(load_constant(default_value), result);\n+    }\n+  } else {\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    LoadFlattenedArrayStub* slow_path = NULL;\n+\n+    if (x->should_profile() && x->array()->maybe_null_free_array()) {\n+      profile_null_free_array(array, md, load_store);\n+    }\n+\n+    if (x->elt_type() == T_OBJECT && x->array()->maybe_flattened_array()) {\n+      assert(x->delayed() == NULL, \"Delayed LoadIndexed only apply to loaded_flattened_arrays\");\n+      index.load_item();\n+      \/\/ if we are loading from flattened array, load it using a runtime call\n+      slow_path = new LoadFlattenedArrayStub(array.result(), index.result(), result, state_for(x, x->state_before()));\n+      check_flattened_array(array.result(), LIR_OprFact::illegalOpr, slow_path);\n+      set_in_conditional_code(true);\n+    }\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    access_load_at(decorators, x->elt_type(),\n+                   array, index.result(), result,\n+                   NULL, null_check_info);\n+\n+    if (slow_path != NULL) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+\n+    element = x;\n+  }\n@@ -1990,4 +2366,3 @@\n-  LIR_Opr result = rlock_result(x, x->elt_type());\n-  access_load_at(decorators, x->elt_type(),\n-                 array, index.result(), result,\n-                 NULL, null_check_info);\n+  if (x->should_profile()) {\n+    profile_element_type(element, md, load_store);\n+  }\n@@ -1996,0 +2371,12 @@\n+void LIRGenerator::do_Deoptimize(Deoptimize* x) {\n+  \/\/ This happens only when a class X uses the withfield\/defaultvalue bytecode\n+  \/\/ to refer to an inline class V, where V has not yet been loaded\/resolved.\n+  \/\/ This is not a common case. Let's just deoptimize.\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+  CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),\n+                                      Deoptimization::Reason_unloaded,\n+                                      Deoptimization::Action_make_not_entrant);\n+  __ jump(stub);\n+  LIR_Opr reg = rlock_result(x, T_OBJECT);\n+  __ move(LIR_OprFact::oopConst(NULL), reg);\n+}\n@@ -2494,1 +2881,1 @@\n-  if (do_update) {\n+  if (do_update && signature_at_call_k != NULL) {\n@@ -2579,0 +2966,46 @@\n+void LIRGenerator::profile_flags(ciMethodData* md, ciProfileData* data, int flag, LIR_Condition condition) {\n+  assert(md != NULL && data != NULL, \"should have been initialized\");\n+  LIR_Opr mdp = new_register(T_METADATA);\n+  __ metadata2reg(md->constant_encoding(), mdp);\n+  LIR_Address* addr = new LIR_Address(mdp, md->byte_offset_of_slot(data, DataLayout::flags_offset()), T_BYTE);\n+  LIR_Opr flags = new_register(T_INT);\n+  __ move(addr, flags);\n+  if (condition != lir_cond_always) {\n+    LIR_Opr update = new_register(T_INT);\n+    __ cmove(condition, LIR_OprFact::intConst(0), LIR_OprFact::intConst(flag), update, T_INT);\n+  } else {\n+    __ logical_or(flags, LIR_OprFact::intConst(flag), flags);\n+  }\n+  __ store(flags, addr);\n+}\n+\n+void LIRGenerator::profile_null_free_array(LIRItem array, ciMethodData* md, ciArrayLoadStoreData* load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+\n+  profile_flags(md, load_store, ArrayLoadStoreData::null_free_array_byte_constant(), lir_cond_equal);\n+}\n+\n+void LIRGenerator::profile_array_type(AccessIndexed* x, ciMethodData*& md, ciArrayLoadStoreData*& load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  int bci = x->profiled_bci();\n+  md = x->profiled_method()->method_data();\n+  assert(md != NULL, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(bci);\n+  assert(data != NULL && data->is_ArrayLoadStoreData(), \"incorrect profiling entry\");\n+  load_store = (ciArrayLoadStoreData*)data;\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_store, ArrayLoadStoreData::array_offset()), 0,\n+               load_store->array()->type(), x->array(), mdp, true, NULL, NULL);\n+}\n+\n+void LIRGenerator::profile_element_type(Value element, ciMethodData* md, ciArrayLoadStoreData* load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  assert(md != NULL && load_store != NULL, \"should have been initialized\");\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_store, ArrayLoadStoreData::element_offset()), 0,\n+               load_store->element()->type(), element, mdp, false, NULL, NULL);\n+}\n+\n@@ -2663,0 +3096,8 @@\n+  if (method()->has_scalarized_args()) {\n+    \/\/ Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized inline type arguments\n+    \/\/ in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.\n+    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, 0), NULL, false);\n+    CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);\n+    __ append(new LIR_Op0(lir_check_orig_pc));\n+    __ branch(lir_cond_notEqual, deopt_stub);\n+  }\n@@ -2678,0 +3119,14 @@\n+void LIRGenerator::invoke_load_one_argument(LIRItem* param, LIR_Opr loc) {\n+  if (loc->is_register()) {\n+    param->load_item_force(loc);\n+  } else {\n+    LIR_Address* addr = loc->as_address_ptr();\n+    param->load_for_store(addr->type());\n+    assert(addr->type() != T_INLINE_TYPE, \"not supported yet\");\n+    if (addr->type() == T_OBJECT) {\n+      __ move_wide(param->result(), addr);\n+    } else {\n+      __ move(param->result(), addr);\n+    }\n+  }\n+}\n@@ -2685,10 +3140,1 @@\n-    if (loc->is_register()) {\n-      param->load_item_force(loc);\n-    } else {\n-      LIR_Address* addr = loc->as_address_ptr();\n-      param->load_for_store(addr->type());\n-      if (addr->type() == T_OBJECT) {\n-        __ move_wide(param->result(), addr);\n-      } else\n-        __ move(param->result(), addr);\n-    }\n+    invoke_load_one_argument(param, loc);\n@@ -2860,1 +3306,1 @@\n-  if (can_inline_as_constant(right.value())) {\n+  if (can_inline_as_constant(right.value()) && !x->substitutability_check()) {\n@@ -2863,0 +3309,1 @@\n+    \/\/ substitutability_check() needs to use right as a base register.\n@@ -2870,3 +3317,60 @@\n-  LIR_Opr reg = rlock_result(x);\n-  __ cmp(lir_cond(x->cond()), left.result(), right.result());\n-  __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  if (x->substitutability_check()) {\n+    substitutability_check(x, left, right, t_val, f_val);\n+  } else {\n+    LIR_Opr reg = rlock_result(x);\n+    __ cmp(lir_cond(x->cond()), left.result(), right.result());\n+    __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  }\n+}\n+\n+void LIRGenerator::substitutability_check(IfOp* x, LIRItem& left, LIRItem& right, LIRItem& t_val, LIRItem& f_val) {\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  bool is_acmpeq = (x->cond() == If::eql);\n+  LIR_Opr equal_result     = is_acmpeq ? t_val.result() : f_val.result();\n+  LIR_Opr not_equal_result = is_acmpeq ? f_val.result() : t_val.result();\n+  LIR_Opr result = rlock_result(x);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+}\n+\n+void LIRGenerator::substitutability_check(If* x, LIRItem& left, LIRItem& right) {\n+  LIR_Opr equal_result     = LIR_OprFact::intConst(1);\n+  LIR_Opr not_equal_result = LIR_OprFact::intConst(0);\n+  LIR_Opr result = new_register(T_INT);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  __ cmp(lir_cond(x->cond()), result, equal_result);\n+}\n+\n+void LIRGenerator::substitutability_check_common(Value left_val, Value right_val, LIRItem& left, LIRItem& right,\n+                                                 LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result,\n+                                                 CodeEmitInfo* info) {\n+  LIR_Opr tmp1 = LIR_OprFact::illegalOpr;\n+  LIR_Opr tmp2 = LIR_OprFact::illegalOpr;\n+  LIR_Opr left_klass_op = LIR_OprFact::illegalOpr;\n+  LIR_Opr right_klass_op = LIR_OprFact::illegalOpr;\n+\n+  ciKlass* left_klass  = left_val ->as_loaded_klass_or_null();\n+  ciKlass* right_klass = right_val->as_loaded_klass_or_null();\n+\n+  if ((left_klass == NULL || right_klass == NULL) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    init_temps_for_substitutability_check(tmp1, tmp2);\n+  }\n+\n+  if (left_klass != NULL && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+  } else {\n+    BasicType t_klass = UseCompressedOops ? T_INT : T_METADATA;\n+    left_klass_op = new_register(t_klass);\n+    right_klass_op = new_register(t_klass);\n+  }\n+\n+  CodeStub* slow_path = new SubstitutabilityCheckStub(left.result(), right.result(), info);\n+  __ substitutability_check(result, left.result(), right.result(), equal_result, not_equal_result,\n+                            tmp1, tmp2,\n+                            left_klass, right_klass, left_klass_op, right_klass_op, info, slow_path);\n@@ -3165,1 +3669,1 @@\n-    ciReturnTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n+    ciSingleTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n@@ -3186,0 +3690,47 @@\n+bool LIRGenerator::profile_inline_klass(ciMethodData* md, ciProfileData* data, Value value, int flag) {\n+  ciKlass* klass = value->as_loaded_klass_or_null();\n+  if (klass != NULL) {\n+    if (klass->is_inlinetype()) {\n+      profile_flags(md, data, flag, lir_cond_always);\n+    } else if (klass->can_be_inline_klass()) {\n+      return false;\n+    }\n+  } else {\n+    return false;\n+  }\n+  return true;\n+}\n+\n+\n+void LIRGenerator::do_ProfileACmpTypes(ProfileACmpTypes* x) {\n+  ciMethod* method = x->method();\n+  assert(method != NULL, \"method should be set if branch is profiled\");\n+  ciMethodData* md = method->method_data_or_null();\n+  assert(md != NULL, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(x->bci());\n+  assert(data != NULL, \"must have profiling data\");\n+  assert(data->is_ACmpData(), \"need BranchData for two-way branches\");\n+  ciACmpData* acmp = (ciACmpData*)data;\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()), 0,\n+               acmp->left()->type(), x->left(), mdp, !x->left_maybe_null(), NULL, NULL);\n+  int flags_offset = md->byte_offset_of_slot(data, DataLayout::flags_offset());\n+  if (!profile_inline_klass(md, acmp, x->left(), ACmpData::left_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->left(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::left_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()),\n+               in_bytes(ACmpData::right_offset()) - in_bytes(ACmpData::left_offset()),\n+               acmp->right()->type(), x->right(), mdp, !x->right_maybe_null(), NULL, NULL);\n+  if (!profile_inline_klass(md, acmp, x->right(), ACmpData::right_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->right(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::right_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":595,"deletions":44,"binary":false,"changes":639,"status":"modified"},{"patch":"@@ -427,1 +427,0 @@\n-    assert(type == _method_entry_ref, \"only special type allowed for now\");\n@@ -430,1 +429,1 @@\n-    _builder->add_special_ref(type, src_obj, field_offset);\n+    _builder->add_special_ref(type, src_obj, field_offset, ref->size() * BytesPerWord);\n@@ -474,4 +473,0 @@\n-void ArchiveBuilder::add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset) {\n-  _special_refs->append(SpecialRefInfo(type, src_obj, field_offset));\n-}\n-\n@@ -668,2 +663,18 @@\n-    assert(s.type() == MetaspaceClosure::_method_entry_ref, \"only special type allowed for now\");\n-    assert(*src_p == *dst_p, \"must be a copy\");\n+\n+    MetaspaceClosure::assert_valid(s.type());\n+    switch (s.type()) {\n+    case MetaspaceClosure::_method_entry_ref:\n+      assert(*src_p == *dst_p, \"must be a copy\");\n+      break;\n+    case MetaspaceClosure::_internal_pointer_ref:\n+      {\n+        \/\/ *src_p points to a location inside src_obj. Let's make *dst_p point to\n+        \/\/ the same location inside dst_obj.\n+        size_t off = pointer_delta(*((address*)src_p), src_obj, sizeof(u1));\n+        assert(off < s.src_obj_size_in_bytes(), \"must point to internal address\");\n+        *((address*)dst_p) = dst_obj + off;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":19,"deletions":8,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -103,0 +103,1 @@\n+    DEBUG_ONLY(size_t _src_obj_size_in_bytes;)\n@@ -106,2 +107,4 @@\n-    SpecialRefInfo(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset)\n-      : _type(type), _src_obj(src_obj), _field_offset(field_offset) {}\n+    SpecialRefInfo(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset, size_t src_obj_size_in_bytes)\n+      : _type(type), _src_obj(src_obj), _field_offset(field_offset) {\n+      DEBUG_ONLY(_src_obj_size_in_bytes = src_obj_size_in_bytes);\n+    }\n@@ -112,0 +115,2 @@\n+\n+    DEBUG_ONLY(size_t src_obj_size_in_bytes() const { return _src_obj_size_in_bytes; })\n@@ -336,1 +341,3 @@\n-  void add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset);\n+  void add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset, size_t src_obj_size_in_bytes) {\n+    _special_refs->append(SpecialRefInfo(type, src_obj, field_offset, src_obj_size_in_bytes));\n+  }\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -146,2 +146,5 @@\n-  unsigned hash = (unsigned)p->identity_hash();\n-  return hash;\n+  \/\/ We are at a safepoint, so the object won't move. It's OK to use its\n+  \/\/ address as the hashcode.\n+  \/\/ We can't use p->identity_hash() as it's not available for primitive oops.\n+  assert_at_safepoint();\n+  return (unsigned)(p2i(p) >> LogBytesPerWord);\n@@ -281,3 +284,4 @@\n-    int hash_original = obj->identity_hash();\n-    archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original));\n-    assert(archived_oop->mark().is_unlocked(), \"sanity\");\n+    if (!(EnableValhalla && obj->mark().is_inline_type())) {\n+      int hash_original = obj->identity_hash();\n+      archived_oop->set_mark(archived_oop->klass()->prototype_header().copy_set_hash(hash_original));\n+      assert(archived_oop->mark().is_unlocked(), \"sanity\");\n@@ -285,2 +289,3 @@\n-    DEBUG_ONLY(int hash_archived = archived_oop->identity_hash());\n-    assert(hash_original == hash_archived, \"Different hash codes: original %x, archived %x\", hash_original, hash_archived);\n+      DEBUG_ONLY(int hash_archived = archived_oop->identity_hash());\n+      assert(hash_original == hash_archived, \"Different hash codes: original %x, archived %x\", hash_original, hash_archived);\n+    }\n@@ -421,1 +426,1 @@\n-    oopDesc::set_mark(mem, markWord::prototype());\n+    oopDesc::set_mark(mem, k->prototype_header());\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":13,"deletions":8,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -61,0 +61,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -657,0 +658,33 @@\n+bool ciMethod::array_access_profiled_type(int bci, ciKlass*& array_type, ciKlass*& element_type, ProfilePtrKind& element_ptr, bool &flat_array, bool &null_free_array) {\n+  if (method_data() != NULL && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != NULL && data->is_ArrayLoadStoreData()) {\n+      ciArrayLoadStoreData* array_access = (ciArrayLoadStoreData*)data->as_ArrayLoadStoreData();\n+      array_type = array_access->array()->valid_type();\n+      element_type = array_access->element()->valid_type();\n+      element_ptr = array_access->element()->ptr_kind();\n+      flat_array = array_access->flat_array();\n+      null_free_array = array_access->null_free_array();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ciMethod::acmp_profiled_type(int bci, ciKlass*& left_type, ciKlass*& right_type, ProfilePtrKind& left_ptr, ProfilePtrKind& right_ptr, bool &left_inline_type, bool &right_inline_type) {\n+  if (method_data() != NULL && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != NULL && data->is_ACmpData()) {\n+      ciACmpData* acmp = (ciACmpData*)data->as_ACmpData();\n+      left_type = acmp->left()->valid_type();\n+      right_type = acmp->right()->valid_type();\n+      left_ptr = acmp->left()->ptr_kind();\n+      right_ptr = acmp->right()->ptr_kind();\n+      left_inline_type = acmp->left_inline_type();\n+      right_inline_type = acmp->right_inline_type();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -941,1 +975,1 @@\n-\/\/ ciMethod::is_object_initializer\n+\/\/ ciMethod::is_object_constructor\n@@ -943,2 +977,15 @@\n-bool ciMethod::is_object_initializer() const {\n-   return name() == ciSymbols::object_initializer_name();\n+bool ciMethod::is_object_constructor() const {\n+   return (name() == ciSymbols::object_initializer_name()\n+           && signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciMethod::is_static_init_factory\n+\/\/\n+bool ciMethod::is_static_init_factory() const {\n+   return (name() == ciSymbols::object_initializer_name()\n+           && !signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n@@ -1223,1 +1270,1 @@\n-bool ciMethod::is_initializer () const {         FETCH_FLAG_FROM_VM(is_initializer); }\n+bool ciMethod::is_object_constructor_or_class_initializer() const { FETCH_FLAG_FROM_VM(is_object_constructor_or_class_initializer); }\n@@ -1375,0 +1422,1 @@\n+  if (bt == T_INLINE_TYPE)   return T_OBJECT;\n@@ -1462,0 +1510,13 @@\n+\n+bool ciMethod::has_scalarized_args() const {\n+  VM_ENTRY_MARK;\n+  return get_Method()->has_scalarized_args();\n+}\n+\n+const GrowableArray<SigEntry>* ciMethod::get_sig_cc() {\n+  VM_ENTRY_MARK;\n+  if (get_Method()->adapter() == NULL) {\n+    return NULL;\n+  }\n+  return get_Method()->adapter()->get_sig_cc();\n+}\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":65,"deletions":4,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -52,0 +52,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -53,1 +55,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -789,0 +791,2 @@\n+int java_lang_Class::_primary_mirror_offset;\n+int java_lang_Class::_secondary_mirror_offset;\n@@ -1019,1 +1023,6 @@\n-      if (k->is_typeArray_klass()) {\n+      if (k->is_flatArray_klass()) {\n+        Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+        assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+        InlineKlass* vk = InlineKlass::cast(element_klass);\n+        comp_mirror = Handle(THREAD, vk->val_mirror());\n+      } else if (k->is_typeArray_klass()) {\n@@ -1026,1 +1035,6 @@\n-        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+        oop comp_oop = element_klass->java_mirror();\n+        if (element_klass->is_inline_klass()) {\n+          InlineKlass* ik = InlineKlass::cast(element_klass);\n+          comp_oop = k->name()->is_Q_array_signature() ? ik->val_mirror() : ik->ref_mirror();\n+        }\n+        comp_mirror = Handle(THREAD, comp_oop);\n@@ -1066,0 +1080,6 @@\n+\n+    if (k->is_inline_klass()) {\n+      oop secondary_mirror = create_secondary_mirror(k, mirror, CHECK);\n+      set_primary_mirror(mirror(), mirror());\n+      set_secondary_mirror(mirror(), secondary_mirror);\n+    }\n@@ -1071,0 +1091,21 @@\n+\/\/ Create the secondary mirror for inline class. Sets all the fields of this java.lang.Class\n+\/\/ instance with the same value as the primary mirror\n+oop java_lang_Class::create_secondary_mirror(Klass* k, Handle mirror, TRAPS) {\n+  assert(k->is_inline_klass(), \"primitive class\");\n+  \/\/ Allocate mirror (java.lang.Class instance)\n+  oop mirror_oop = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(k, CHECK_0);\n+  Handle secondary_mirror(THREAD, mirror_oop);\n+\n+  java_lang_Class::set_klass(secondary_mirror(), k);\n+  java_lang_Class::set_static_oop_field_count(secondary_mirror(), static_oop_field_count(mirror()));\n+  \/\/ ## do we need to set init lock?\n+  java_lang_Class::set_init_lock(secondary_mirror(), init_lock(mirror()));\n+\n+  set_protection_domain(secondary_mirror(), protection_domain(mirror()));\n+  set_class_loader(secondary_mirror(), class_loader(mirror()));\n+  \/\/ ## handle if java.base is not yet defined\n+  set_module(secondary_mirror(), module(mirror()));\n+  set_primary_mirror(secondary_mirror(), mirror());\n+  set_secondary_mirror(secondary_mirror(), secondary_mirror());\n+  return secondary_mirror();\n+}\n@@ -1118,0 +1159,1 @@\n+      case T_INLINE_TYPE:\n@@ -1215,0 +1257,6 @@\n+  if (k->is_inline_klass()) {\n+    \/\/ Inline types have a primary mirror and a secondary mirror. Don't handle this for now. TODO:CDS\n+    k->clear_java_mirror_handle();\n+    return NULL;\n+  }\n+\n@@ -1417,0 +1465,20 @@\n+oop java_lang_Class::primary_mirror(oop java_class) {\n+  assert(_primary_mirror_offset != 0, \"must be set\");\n+  return java_class->obj_field(_primary_mirror_offset);\n+}\n+\n+void java_lang_Class::set_primary_mirror(oop java_class, oop mirror) {\n+  assert(_primary_mirror_offset != 0, \"must be set\");\n+  java_class->obj_field_put(_primary_mirror_offset, mirror);\n+}\n+\n+oop java_lang_Class::secondary_mirror(oop java_class) {\n+  assert(_secondary_mirror_offset != 0, \"must be set\");\n+  return java_class->obj_field(_secondary_mirror_offset);\n+}\n+\n+void java_lang_Class::set_secondary_mirror(oop java_class, oop mirror) {\n+  assert(_secondary_mirror_offset != 0, \"must be set\");\n+  java_class->obj_field_put(_secondary_mirror_offset, mirror);\n+}\n+\n@@ -1510,0 +1578,1 @@\n+  bool is_Q_descriptor = false;\n@@ -1515,0 +1584,1 @@\n+    is_Q_descriptor = k->is_inline_klass() && is_secondary_mirror(java_class);\n@@ -1521,1 +1591,3 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    st->print(is_Q_descriptor ? \"Q\" : \"L\");\n+  }\n@@ -1542,2 +1614,7 @@\n-      const char* sigstr = k->signature_name();\n-      int         siglen = (int) strlen(sigstr);\n+      const char* sigstr;\n+      if (k->is_inline_klass() && is_secondary_mirror(java_class)) {\n+        sigstr = InlineKlass::cast(k)->val_signature_name();\n+      } else {\n+        sigstr = k->signature_name();\n+      }\n+      int siglen = (int) strlen(sigstr);\n@@ -1623,0 +1700,2 @@\n+  macro(_primary_mirror_offset,      k, \"primaryType\",         class_signature,       false); \\\n+  macro(_secondary_mirror_offset,    k, \"secondaryType\",       class_signature,       false); \\\n@@ -2513,2 +2592,2 @@\n-      \/\/ This is simlar to classic VM.\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      \/\/ This is similar to classic VM (before HotSpot).\n+      if (method->is_object_constructor() &&\n@@ -3987,1 +4066,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":88,"deletions":9,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -282,0 +282,3 @@\n+  static int _primary_mirror_offset;\n+  static int _secondary_mirror_offset;\n+\n@@ -296,0 +299,3 @@\n+  static void set_primary_mirror(oop java_class, oop comp_mirror);\n+  static void set_secondary_mirror(oop java_class, oop comp_mirror);\n+\n@@ -308,0 +314,1 @@\n+  static oop  create_secondary_mirror(Klass* k, Handle mirror, TRAPS);\n@@ -342,0 +349,3 @@\n+  static int component_mirror_offset()     { CHECK_INIT(_component_mirror_offset); }\n+  static int primary_mirror_offset()       { CHECK_INIT(_primary_mirror_offset); }\n+  static int secondary_mirror_offset()     { CHECK_INIT(_secondary_mirror_offset); }\n@@ -353,0 +363,5 @@\n+  static oop  primary_mirror(oop java_class);\n+  static oop  secondary_mirror(oop java_class);\n+  static bool is_primary_mirror(oop java_class);\n+  static bool is_secondary_mirror(oop java_class);\n+\n@@ -358,2 +373,0 @@\n-  static int component_mirror_offset() { return _component_mirror_offset; }\n-\n@@ -1170,1 +1183,1 @@\n-    MN_IS_CONSTRUCTOR        = 0x00020000, \/\/ constructor\n+    MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, \/\/ constructor\n@@ -1711,1 +1724,0 @@\n-\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -264,0 +264,18 @@\n+inline bool java_lang_Class::is_primary_mirror(oop java_class) {\n+  Klass* k = as_Klass(java_class);\n+  if (k->is_inline_klass()) {\n+    return java_class == primary_mirror(java_class);\n+  } else {\n+    return true;\n+  }\n+}\n+\n+inline bool java_lang_Class::is_secondary_mirror(oop java_class) {\n+  Klass* k = as_Klass(java_class);\n+  if (k->is_inline_klass()) {\n+    return java_class == secondary_mirror(java_class);\n+  } else {\n+    return false;\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.inline.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -68,0 +69,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -75,0 +77,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -288,1 +291,1 @@\n-    \/\/ Ignore wrapping L and ;.\n+    \/\/ Ignore wrapping L and ;. (and Q and ; for value types);\n@@ -316,1 +319,8 @@\n-      k = k->array_klass(ndims, CHECK_NULL);\n+      if (class_name->is_Q_array_signature()) {\n+        if (!k->is_inline_klass()) {\n+          THROW_MSG_NULL(vmSymbols::java_lang_IncompatibleClassChangeError(), \"L\/Q mismatch on bottom type\");\n+        }\n+        k = InlineKlass::cast(k)->null_free_inline_array_klass(ndims, CHECK_NULL);\n+      } else {\n+        k = k->array_klass(ndims, CHECK_NULL);\n+      }\n@@ -446,0 +456,44 @@\n+Klass* SystemDictionary::resolve_inline_type_field_or_fail(AllFieldStream* fs,\n+                                                           Handle class_loader,\n+                                                           Handle protection_domain,\n+                                                           bool throw_error,\n+                                                           TRAPS) {\n+  Symbol* class_name = fs->signature()->fundamental_name(THREAD);\n+  class_loader = Handle(THREAD, java_lang_ClassLoader::non_reflection_class_loader(class_loader()));\n+  ClassLoaderData* loader_data = class_loader_data(class_loader);\n+  unsigned int p_hash = placeholders()->compute_hash(class_name);\n+  bool throw_circularity_error = false;\n+  PlaceholderEntry* oldprobe;\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    oldprobe = placeholders()->get_entry(p_hash, class_name, loader_data);\n+    if (oldprobe != NULL &&\n+      oldprobe->check_seen_thread(THREAD, PlaceholderTable::INLINE_TYPE_FIELD)) {\n+      throw_circularity_error = true;\n+\n+    } else {\n+      placeholders()->find_and_add(p_hash, class_name, loader_data,\n+                                   PlaceholderTable::INLINE_TYPE_FIELD, NULL, THREAD);\n+    }\n+  }\n+\n+  Klass* klass = NULL;\n+  if (!throw_circularity_error) {\n+    klass = SystemDictionary::resolve_or_fail(class_name, class_loader,\n+                                               protection_domain, true, THREAD);\n+  } else {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG_NULL(vmSymbols::java_lang_ClassCircularityError(), class_name->as_C_string());\n+  }\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    placeholders()->find_and_remove(p_hash, class_name, loader_data,\n+                                    PlaceholderTable::INLINE_TYPE_FIELD, THREAD);\n+  }\n+\n+  class_name->decrement_refcount();\n+  return klass;\n+}\n+\n@@ -803,1 +857,1 @@\n-    if (t != T_OBJECT) {\n+    if (t != T_OBJECT && t != T_INLINE_TYPE) {\n@@ -809,1 +863,5 @@\n-      k = k->array_klass_or_null(ndims);\n+      if (class_name->is_Q_array_signature()) {\n+        k = InlineKlass::cast(k)->null_free_inline_array_klass_or_null(ndims);\n+      } else {\n+        k = k->array_klass_or_null(ndims);\n+      }\n@@ -1151,0 +1209,18 @@\n+\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik->fields(), ik->constants()); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        if (!fs.access_flags().is_static()) {\n+          \/\/ Pre-load inline class\n+          Klass* real_k = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+            class_loader, protection_domain, true, CHECK_NULL);\n+          Klass* k = ik->get_inline_type_field_klass_or_null(fs.index());\n+          if (real_k != k) {\n+            \/\/ oops, the app has substituted a different version of k!\n+            return NULL;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1186,0 +1262,7 @@\n+\n+  if (ik->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    oop val = ik->allocate_instance(CHECK_NULL);\n+    vk->set_default_value(val);\n+  }\n+\n@@ -1772,1 +1855,1 @@\n-    if (t != T_OBJECT) {\n+    if (t != T_OBJECT && t != T_INLINE_TYPE) {\n@@ -1780,1 +1863,5 @@\n-      klass = klass->array_klass_or_null(ndims);\n+      if (class_name->is_Q_array_signature()) {\n+        klass = InlineKlass::cast(klass)->null_free_inline_array_klass_or_null(ndims);\n+      } else {\n+        klass = klass->array_klass_or_null(ndims);\n+      }\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":93,"deletions":6,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -58,0 +58,2 @@\n+  template(java_lang_IdentityObject,                  \"java\/lang\/IdentityObject\")                 \\\n+  template(java_lang_PrimitiveObject,                 \"java\/lang\/PrimitiveObject\")                \\\n@@ -67,0 +69,1 @@\n+  template(java_lang_NonTearable,                     \"java\/lang\/NonTearable\")                    \\\n@@ -481,0 +484,2 @@\n+  template(default_value_name,                        \".default\")                                 \\\n+  template(empty_marker_name,                         \".empty\")                                   \\\n@@ -555,0 +560,1 @@\n+  template(object_object_boolean_signature,           \"(Ljava\/lang\/Object;Ljava\/lang\/Object;)Z\") \\\n@@ -689,0 +695,2 @@\n+  template(primaryType_name,                           \"primaryType\")                                             \\\n+  template(secondaryType_name,                         \"secondaryType\")                                           \\\n@@ -713,0 +721,4 @@\n+  template(java_lang_runtime_PrimitiveObjectMethods,        \"java\/lang\/runtime\/PrimitiveObjectMethods\")           \\\n+  template(isSubstitutable_name,                            \"isSubstitutable\")                                    \\\n+  template(primitiveObjectHashCode_name,                    \"primitiveObjectHashCode\")                            \\\n+                                                                                                                  \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -500,1 +500,2 @@\n-    if (klass->is_array_klass()) {\n+    \/\/ CMH: Valhalla flat arrays can split this work up, but for now, doesn't\n+    if (klass->is_array_klass() && !klass->is_flatArray_klass()) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManager.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -275,1 +275,1 @@\n-    if (is_reference_type(bt)) {\n+    if (is_reference_type(bt) && (!ary_ptr->is_flat())) {\n@@ -301,1 +301,1 @@\n-        length = phase->transform_later(new SubLNode(length, phase->longcon(1))); \/\/ Size is in longs\n+        length = phase->transform_later(new SubXNode(length, phase->longcon(1))); \/\/ Size is in longs\n@@ -346,1 +346,1 @@\n-  phase->igvn().replace_node(ac, call);\n+  phase->replace_node(ac, call);\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -57,0 +57,2 @@\n+  virtual void do_oop_no_buffering(oop* o) { do_oop(o); }\n+  virtual void do_oop_no_buffering(narrowOop* o) { do_oop(o); }\n@@ -117,0 +119,5 @@\n+class BufferedValueClosure : public Closure {\n+public:\n+  virtual void do_buffered_value(oop* p) = 0;\n+};\n+\n","filename":"src\/hotspot\/share\/memory\/iterator.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -82,1 +82,5 @@\n-    _method_entry_ref\n+    \/\/ A field that points to a method entry. E.g., Method::_i2i_entry\n+    _method_entry_ref,\n+\n+    \/\/ A field that points to a location inside the current object.\n+    _internal_pointer_ref,\n@@ -372,1 +376,9 @@\n-    push_special(_method_entry_ref, ref, (intptr_t*)p);\n+    push_special(_method_entry_ref, ref, p);\n+    if (!ref->keep_after_pushing()) {\n+      delete ref;\n+    }\n+  }\n+\n+  template <class T> void push_internal_pointer(T** mpp, intptr_t* p) {\n+    Ref* ref = new MSORef<T>(mpp, _default);\n+    push_special(_internal_pointer_ref, ref, p);\n@@ -381,1 +393,5 @@\n-    assert(type == _method_entry_ref, \"only special type allowed for now\");\n+    assert_valid(type);\n+  }\n+\n+  static void assert_valid(SpecialRef type) {\n+    assert(type == _method_entry_ref || type == _internal_pointer_ref, \"only special types allowed for now\");\n","filename":"src\/hotspot\/share\/memory\/metaspaceClosure.hpp","additions":19,"deletions":3,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -231,1 +232,1 @@\n-      \/\/ All of these should have been reverted back to ClassIndex before calling\n+      \/\/ All of these should have been reverted back to Unresolved before calling\n@@ -250,0 +251,1 @@\n+  assert(!k->name()->is_Q_signature(), \"Q-type without JVM_CONSTANT_QDescBit\");\n@@ -393,0 +395,1 @@\n+    jbyte qdesc_bit = tag_at(index).is_Qdescriptor_klass() ? (jbyte) JVM_CONSTANT_QDescBit : 0;\n@@ -394,1 +397,1 @@\n-      tag_at_put(index, JVM_CONSTANT_UnresolvedClass);\n+      tag_at_put(index, JVM_CONSTANT_UnresolvedClass | qdesc_bit);\n@@ -419,1 +422,1 @@\n-        tag_at_put(index, JVM_CONSTANT_UnresolvedClass);\n+        tag_at_put(index, JVM_CONSTANT_UnresolvedClass | qdesc_bit);\n@@ -469,0 +472,6 @@\n+void check_is_inline_type(Klass* k, TRAPS) {\n+  if (!k->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+}\n+\n@@ -506,0 +515,5 @@\n+  bool inline_type_signature = false;\n+  if (name->is_Q_signature()) {\n+    name = name->fundamental_name(THREAD);\n+    inline_type_signature = true;\n+  }\n@@ -515,0 +529,3 @@\n+  if (inline_type_signature) {\n+    name->decrement_refcount();\n+  }\n@@ -523,0 +540,16 @@\n+  if (!HAS_PENDING_EXCEPTION && inline_type_signature) {\n+    check_is_inline_type(k, THREAD);\n+  }\n+\n+  if (!HAS_PENDING_EXCEPTION) {\n+    Klass* bottom_klass = NULL;\n+    if (k->is_objArray_klass()) {\n+      bottom_klass = ObjArrayKlass::cast(k)->bottom_klass();\n+      assert(bottom_klass != NULL, \"Should be set\");\n+      assert(bottom_klass->is_instance_klass() || bottom_klass->is_typeArray_klass(), \"Sanity check\");\n+    } else if (k->is_flatArray_klass()) {\n+      bottom_klass = FlatArrayKlass::cast(k)->element_klass();\n+      assert(bottom_klass != NULL, \"Should be set\");\n+    }\n+  }\n+\n@@ -526,1 +559,5 @@\n-    save_and_throw_exception(this_cp, which, constantTag(JVM_CONSTANT_UnresolvedClass), CHECK_NULL);\n+    jbyte tag = JVM_CONSTANT_UnresolvedClass;\n+    if (this_cp->tag_at(which).is_Qdescriptor_klass()) {\n+      tag |= JVM_CONSTANT_QDescBit;\n+    }\n+    save_and_throw_exception(this_cp, which, constantTag(tag), CHECK_NULL);\n@@ -545,0 +582,4 @@\n+  jbyte tag = JVM_CONSTANT_Class;\n+  if (this_cp->tag_at(which).is_Qdescriptor_klass()) {\n+    tag |= JVM_CONSTANT_QDescBit;\n+  }\n@@ -549,1 +590,1 @@\n-                                  (jbyte)JVM_CONSTANT_Class);\n+                                  tag);\n@@ -985,1 +1026,3 @@\n-      result_oop = resolved->java_mirror();\n+      result_oop = tag.is_Qdescriptor_klass()\n+                      ? InlineKlass::cast(resolved)->val_mirror()\n+                      : resolved->java_mirror();\n@@ -1903,0 +1946,6 @@\n+      case (JVM_CONSTANT_Class | JVM_CONSTANT_QDescBit): {\n+        idx1 = Bytes::get_Java_u2(bytes);\n+        printf(\"qclass        #%03d\", idx1);\n+        ent_size = 2;\n+        break;\n+      }\n@@ -1945,0 +1994,4 @@\n+      case (JVM_CONSTANT_UnresolvedClass | JVM_CONSTANT_QDescBit): {\n+        printf(\"UnresolvedQClass: %s\", WARN_MSG);\n+        break;\n+      }\n@@ -2116,0 +2169,1 @@\n+        assert(!tag_at(idx).is_Qdescriptor_klass(), \"Failed to encode QDesc\");\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":60,"deletions":6,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -284,1 +284,1 @@\n-  \/\/ For temporary use while constructing constant pool\n+  \/\/ For temporary use while constructing constant pool. Used during a retransform\/class redefinition as well.\n@@ -293,0 +293,9 @@\n+  void unresolved_qdescriptor_at_put(int which, int name_index, int resolved_klass_index) {\n+      release_tag_at_put(which, JVM_CONSTANT_UnresolvedClass | (jbyte) JVM_CONSTANT_QDescBit);\n+\n+      assert((name_index & 0xffff0000) == 0, \"must be\");\n+      assert((resolved_klass_index & 0xffff0000) == 0, \"must be\");\n+      *int_at_addr(which) =\n+        build_int_from_shorts((jushort)resolved_klass_index, (jushort)name_index);\n+    }\n+\n","filename":"src\/hotspot\/share\/oops\/constantPool.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -164,0 +165,2 @@\n+bool InstanceKlass::field_is_null_free_inline_type(int index) const { return Signature::basic_type(field(index)->signature(constants())) == T_INLINE_TYPE; }\n+\n@@ -429,1 +432,3 @@\n-                                       parser.is_interface());\n+                                       parser.is_interface(),\n+                                       parser.has_inline_fields() ? parser.java_fields_count() : 0,\n+                                       parser.is_inline_type());\n@@ -443,2 +448,1 @@\n-    }\n-    else if (is_class_loader(class_name, parser)) {\n+    } else if (is_class_loader(class_name, parser)) {\n@@ -447,0 +451,3 @@\n+    } else if (parser.is_inline_type()) {\n+      \/\/ inline type\n+      ik = new (loader_data, size, THREAD) InlineKlass(parser);\n@@ -462,0 +469,7 @@\n+#ifdef ASSERT\n+  assert(ik->size() == size, \"\");\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -465,0 +479,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = NULL;\n+  address end = NULL;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -499,1 +536,3 @@\n-  _init_thread(NULL)\n+  _init_thread(NULL),\n+  _inline_type_field_klasses(NULL),\n+  _adr_inlineklass_fixed_block(NULL)\n@@ -507,0 +546,4 @@\n+    if (parser.has_inline_fields()) {\n+      set_has_inline_type_fields();\n+    }\n+    _java_fields_count = parser.java_fields_count();\n@@ -511,0 +554,4 @@\n+\n+  if (has_inline_type_fields()) {\n+    _inline_type_field_klasses = (const Klass**) adr_inline_type_field_klasses();\n+  }\n@@ -540,1 +587,3 @@\n-    if (ti != sti && ti != NULL && !ti->is_shared()) {\n+    if (ti != sti && ti != NULL && !ti->is_shared() &&\n+        ti != Universe::the_single_IdentityObject_klass_array() &&\n+        ti != Universe::the_single_PrimitiveObject_klass_array()) {\n@@ -547,1 +596,3 @@\n-      local_interfaces != NULL && !local_interfaces->is_shared()) {\n+      local_interfaces != NULL && !local_interfaces->is_shared() &&\n+      local_interfaces != Universe::the_single_IdentityObject_klass_array() &&\n+      local_interfaces != Universe::the_single_PrimitiveObject_klass_array()) {\n@@ -877,0 +928,56 @@\n+\n+  \/\/ If a class declares a method that uses an inline class as an argument\n+  \/\/ type or return inline type, this inline class must be loaded during the\n+  \/\/ linking of this class because size and properties of the inline class\n+  \/\/ must be known in order to be able to perform inline type optimizations.\n+  \/\/ The implementation below is an approximation of this rule, the code\n+  \/\/ iterates over all methods of the current class (including overridden\n+  \/\/ methods), not only the methods declared by this class. This\n+  \/\/ approximation makes the code simpler, and doesn't change the semantic\n+  \/\/ because classes declaring methods overridden by the current class are\n+  \/\/ linked (and have performed their own pre-loading) before the linking\n+  \/\/ of the current class.\n+\n+\n+  \/\/ Note:\n+  \/\/ Inline class types are loaded during\n+  \/\/ the loading phase (see ClassFileParser::post_process_parsed_stream()).\n+  \/\/ Inline class types used as element types for array creation\n+  \/\/ are not pre-loaded. Their loading is triggered by either anewarray\n+  \/\/ or multianewarray bytecodes.\n+\n+  \/\/ Could it be possible to do the following processing only if the\n+  \/\/ class uses inline types?\n+  if (EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    for (int i = 0; i < methods()->length(); i++) {\n+      Method* m = methods()->at(i);\n+      for (SignatureStream ss(m->signature()); !ss.is_done(); ss.next()) {\n+        if (ss.is_reference()) {\n+          if (ss.is_array()) {\n+            continue;\n+          }\n+          if (ss.type() == T_INLINE_TYPE) {\n+            Symbol* symb = ss.as_symbol();\n+\n+            oop loader = class_loader();\n+            oop protection_domain = this->protection_domain();\n+            Klass* klass = SystemDictionary::resolve_or_fail(symb,\n+                                                             Handle(THREAD, loader), Handle(THREAD, protection_domain), true,\n+                                                             CHECK_false);\n+            if (klass == NULL) {\n+              THROW_(vmSymbols::java_lang_LinkageError(), false);\n+            }\n+            if (!klass->is_inline_klass()) {\n+              Exceptions::fthrow(\n+                THREAD_AND_LOCATION,\n+                vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                \"class %s is not an inline type\",\n+                klass->external_name());\n+            }\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1161,1 +1268,30 @@\n-\n+  \/\/ Initialize classes of inline fields\n+  {\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        Klass* klass = get_inline_type_field_klass_or_null(fs.index());\n+        if (fs.access_flags().is_static() && klass == NULL) {\n+          klass = SystemDictionary::resolve_or_fail(field_signature(fs.index())->fundamental_name(THREAD),\n+              Handle(THREAD, class_loader()),\n+              Handle(THREAD, protection_domain()),\n+              true, CHECK);\n+          if (klass == NULL) {\n+            THROW(vmSymbols::java_lang_NoClassDefFoundError());\n+          }\n+          if (!klass->is_inline_klass()) {\n+            THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+          }\n+          set_inline_type_field_klass(fs.index(), klass);\n+        }\n+        InstanceKlass::cast(klass)->initialize(CHECK);\n+        if (fs.access_flags().is_static()) {\n+          if (java_mirror()->obj_field(fs.offset()) == NULL) {\n+            java_mirror()->obj_field_put(fs.offset(), InlineKlass::cast(klass)->default_value());\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+\n+  \/\/ Step 9\n@@ -1184,1 +1320,1 @@\n-  \/\/ Step 9\n+  \/\/ Step 10\n@@ -1190,1 +1326,1 @@\n-    \/\/ Step 10 and 11\n+    \/\/ Step 11 and 12\n@@ -1454,1 +1590,2 @@\n-        ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, CHECK_NULL);\n+        ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this,\n+                                                                  false, false, CHECK_NULL);\n@@ -1461,2 +1598,2 @@\n-  ObjArrayKlass* oak = array_klasses();\n-  return oak->array_klass(n, THREAD);\n+  ArrayKlass* ak = array_klasses();\n+  return ak->array_klass(n, THREAD);\n@@ -1467,2 +1604,2 @@\n-  ObjArrayKlass* oak = array_klasses_acquire();\n-  if (oak == NULL) {\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == NULL) {\n@@ -1471,1 +1608,1 @@\n-    return oak->array_klass_or_null(n);\n+    return ak->array_klass_or_null(n);\n@@ -1488,1 +1625,1 @@\n-  if (clinit != NULL && clinit->has_valid_initializer_flags()) {\n+  if (clinit != NULL && clinit->is_class_initializer()) {\n@@ -1526,1 +1663,1 @@\n-    MutexLocker x(OopMapCacheAlloc_lock);\n+    MutexLocker x(OopMapCacheAlloc_lock,  Mutex::_no_safepoint_check_flag);\n@@ -1538,5 +1675,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n-\n@@ -1613,0 +1745,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->first_field_offset() && offset < (vk->first_field_offset() + vk->get_exact_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -2014,0 +2155,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited, not even as a static factory\n+    }\n@@ -2459,0 +2603,6 @@\n+\n+  if (has_inline_type_fields()) {\n+    for (int i = 0; i < java_fields_count(); i++) {\n+      it->push(&((Klass**)adr_inline_type_field_klasses())[i]);\n+    }\n+  }\n@@ -2501,0 +2651,8 @@\n+  if (has_inline_type_fields()) {\n+    for (AllFieldStream fs(fields(), constants()); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        reset_inline_type_field_klass(fs.index());\n+      }\n+    }\n+  }\n+\n@@ -2562,0 +2720,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2762,0 +2924,4 @@\n+  return signature_name_of_carrier(JVM_SIGNATURE_CLASS);\n+}\n+\n+const char* InstanceKlass::signature_name_of_carrier(char c) const {\n@@ -2771,1 +2937,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -2773,1 +2939,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = c;\n@@ -3367,1 +3533,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3371,0 +3540,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3374,0 +3548,6 @@\n+    } else if (self != NULL && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3380,1 +3560,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(NULL, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == NULL) { st->print_cr(\"NULL\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == NULL) { st->print_cr(\"NULL\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3390,0 +3591,1 @@\n+  st->print(BULLET\"misc flags:        0x%x\", _misc_flags);                        st->cr();\n@@ -3416,15 +3618,3 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);                  st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);      st->cr();\n-  if (Verbose && default_methods() != NULL) {\n-    Array<Method*>* method_array = default_methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n+  st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3432,1 +3622,1 @@\n-    st->print(BULLET\"default vtable indices:   \"); default_vtable_indices()->print_value_on(st);       st->cr();\n+    st->print(BULLET\"default vtable indices:   \"); print_array_on(st, default_vtable_indices());\n@@ -3434,2 +3624,2 @@\n-  st->print(BULLET\"local interfaces:  \"); local_interfaces()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"trans. interfaces: \"); transitive_interfaces()->print_value_on(st); st->cr();\n+  st->print(BULLET\"local interfaces:  \"); print_array_on(st, local_interfaces());\n+  st->print(BULLET\"trans. interfaces: \"); print_array_on(st, transitive_interfaces());\n@@ -3491,1 +3681,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(NULL, start_of_itable(), itable_length(), st);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":235,"deletions":45,"binary":false,"changes":280,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -53,0 +54,2 @@\n+\/\/    [EMBEDDED inline_type_field_klasses] only if has_inline_fields() == true\n+\/\/    [EMBEDDED InlineKlassFixedBlock] only if is an InlineKlass instance\n@@ -69,0 +72,1 @@\n+class BufferedInlineTypeBlob;\n@@ -129,0 +133,17 @@\n+class SigEntry;\n+\n+class InlineKlassFixedBlock {\n+  Array<SigEntry>** _extended_sig;\n+  Array<VMRegPair>** _return_regs;\n+  address* _pack_handler;\n+  address* _pack_handler_jobject;\n+  address* _unpack_handler;\n+  int* _default_value_offset;\n+  ArrayKlass** _null_free_inline_array_klasses;\n+  int _alignment;\n+  int _first_field_offset;\n+  int _exact_size_in_bytes;\n+\n+  friend class InlineKlass;\n+};\n+\n@@ -134,0 +155,1 @@\n+  friend class TemplateTable;\n@@ -151,1 +173,1 @@\n-    fully_initialized,                  \/\/ initialized (successfull final state)\n+    fully_initialized,                  \/\/ initialized (successful final state)\n@@ -167,1 +189,1 @@\n-  ObjArrayKlass* volatile _array_klasses;\n+  ArrayKlass* volatile _array_klasses;\n@@ -232,1 +254,1 @@\n-  \/\/ This can be used to quickly discriminate among the four kinds of\n+  \/\/ This can be used to quickly discriminate among the five kinds of\n@@ -238,0 +260,1 @@\n+  static const unsigned _kind_inline_type  = 4; \/\/ InlineKlass\n@@ -258,1 +281,9 @@\n-    _misc_has_contended_annotations           = 1 << 15  \/\/ has @Contended annotation\n+    _misc_has_contended_annotations           = 1 << 15,  \/\/ has @Contended annotation\n+    _misc_has_inline_type_fields              = 1 << 16, \/\/ has inline fields and related embedded section is not empty\n+    _misc_is_empty_inline_type                = 1 << 17, \/\/ empty inline type (*)\n+    _misc_is_naturally_atomic                 = 1 << 18, \/\/ loaded\/stored in one instruction\n+    _misc_is_declared_atomic                  = 1 << 19, \/\/ implements jl.NonTearable\n+    _misc_invalid_inline_super                = 1 << 20, \/\/ invalid super type for an inline type\n+    _misc_invalid_identity_super              = 1 << 21, \/\/ invalid super type for an identity type\n+    _misc_has_injected_identityObject         = 1 << 22, \/\/ IdentityObject has been injected by the JVM\n+    _misc_has_injected_primitiveObject        = 1 << 23  \/\/ PrimitiveObject has been injected by the JVM\n@@ -260,0 +291,6 @@\n+\n+  \/\/ (*) An inline type is considered empty if it contains no non-static fields or\n+  \/\/ if it contains only empty inline fields. Note that JITs have a slightly different\n+  \/\/ definition: empty inline fields must be flattened otherwise the container won't\n+  \/\/ be considered empty\n+\n@@ -263,1 +300,1 @@\n-  u2              _misc_flags;           \/\/ There is more space in access_flags for more flags.\n+  u4              _misc_flags;           \/\/ There is more space in access_flags for more flags.\n@@ -315,0 +352,3 @@\n+  const Klass**   _inline_type_field_klasses; \/\/ For \"inline class\" fields, NULL if none present\n+\n+  const InlineKlassFixedBlock* _adr_inlineklass_fixed_block;\n@@ -383,0 +423,73 @@\n+  bool has_inline_type_fields() const          {\n+    return (_misc_flags & _misc_has_inline_type_fields) != 0;\n+  }\n+  void set_has_inline_type_fields()  {\n+    _misc_flags |= _misc_has_inline_type_fields;\n+  }\n+\n+  bool is_empty_inline_type() const {\n+    return (_misc_flags & _misc_is_empty_inline_type) != 0;\n+  }\n+  void set_is_empty_inline_type() {\n+    _misc_flags |= _misc_is_empty_inline_type;\n+  }\n+\n+  \/\/ Note:  The naturally_atomic property only applies to\n+  \/\/ inline classes; it is never true on identity classes.\n+  \/\/ The bit is placed on instanceKlass for convenience.\n+\n+  \/\/ Query if h\/w provides atomic load\/store for instances.\n+  bool is_naturally_atomic() const {\n+    return (_misc_flags & _misc_is_naturally_atomic) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_is_naturally_atomic() {\n+    _misc_flags |= _misc_is_naturally_atomic;\n+  }\n+\n+  \/\/ Query if this class implements jl.NonTearable or was\n+  \/\/ mentioned in the JVM option ForceNonTearable.\n+  \/\/ This bit can occur anywhere, but is only significant\n+  \/\/ for inline classes *and* their super types.\n+  \/\/ It inherits from supers along with NonTearable.\n+  bool is_declared_atomic() const {\n+    return (_misc_flags & _misc_is_declared_atomic) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_is_declared_atomic() {\n+    _misc_flags |= _misc_is_declared_atomic;\n+  }\n+\n+  \/\/ Query if class is an invalid super class for an inline type.\n+  bool invalid_inline_super() const {\n+    return (_misc_flags & _misc_invalid_inline_super) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_invalid_inline_super() {\n+    _misc_flags |= _misc_invalid_inline_super;\n+  }\n+  \/\/ Query if class is an invalid super class for an identity type.\n+  bool invalid_identity_super() const {\n+    return (_misc_flags & _misc_invalid_identity_super) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_invalid_identity_super() {\n+    _misc_flags |= _misc_invalid_identity_super;\n+  }\n+\n+  bool has_injected_identityObject() const {\n+    return (_misc_flags & _misc_has_injected_identityObject);\n+  }\n+\n+  void set_has_injected_identityObject() {\n+    _misc_flags |= _misc_has_injected_identityObject;\n+  }\n+\n+  bool has_injected_primitiveObject() const {\n+    return (_misc_flags & _misc_has_injected_primitiveObject);\n+  }\n+\n+  void set_has_injected_primitiveObject() {\n+    _misc_flags |= _misc_has_injected_primitiveObject;\n+  }\n+\n@@ -398,4 +511,4 @@\n-  ObjArrayKlass* array_klasses() const     { return _array_klasses; }\n-  inline ObjArrayKlass* array_klasses_acquire() const; \/\/ load with acquire semantics\n-  void set_array_klasses(ObjArrayKlass* k) { _array_klasses = k; }\n-  inline void release_set_array_klasses(ObjArrayKlass* k); \/\/ store with release semantics\n+  ArrayKlass* array_klasses() const     { return _array_klasses; }\n+  inline ArrayKlass* array_klasses_acquire() const; \/\/ load with acquire semantics\n+  void set_array_klasses(ArrayKlass* k) { _array_klasses = k; }\n+  inline void release_set_array_klasses(ArrayKlass* k); \/\/ store with release semantics\n@@ -445,0 +558,2 @@\n+  bool    field_is_inlined(int index) const { return field(index)->is_inlined(); }\n+  bool    field_is_null_free_inline_type(int index) const;\n@@ -573,0 +688,4 @@\n+  static ByteSize kind_offset() { return in_ByteSize(offset_of(InstanceKlass, _kind)); }\n+  static ByteSize misc_flags_offset() { return in_ByteSize(offset_of(InstanceKlass, _misc_flags)); }\n+  static u4 misc_flags_is_empty_inline_type() { return _misc_is_empty_inline_type; }\n+\n@@ -741,0 +860,1 @@\n+\n@@ -742,1 +862,1 @@\n-    return ((_misc_flags & _misc_is_being_redefined) != 0);\n+    return (_misc_flags & _misc_is_being_redefined);\n@@ -809,0 +929,1 @@\n+  bool is_inline_type_klass()           const { return is_kind(_kind_inline_type); }\n@@ -978,0 +1099,3 @@\n+  static ByteSize inline_type_field_klasses_offset() { return in_ByteSize(offset_of(InstanceKlass, _inline_type_field_klasses)); }\n+  static ByteSize adr_inlineklass_fixed_block_offset() { return in_ByteSize(offset_of(InstanceKlass, _adr_inlineklass_fixed_block)); }\n+\n@@ -1013,2 +1137,2 @@\n-  void array_klasses_do(void f(Klass* k));\n-  void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);\n+  virtual void array_klasses_do(void f(Klass* k));\n+  virtual void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);\n@@ -1035,1 +1159,2 @@\n-                  bool is_interface) {\n+                  bool is_interface,\n+                  int java_fields, bool is_inline_type) {\n@@ -1040,1 +1165,3 @@\n-           (is_interface ? (int)sizeof(Klass*)\/wordSize : 0));\n+           (is_interface ? (int)sizeof(Klass*)\/wordSize : 0) +\n+           (java_fields * (int)sizeof(Klass*)\/wordSize) +\n+           (is_inline_type ? (int)sizeof(InlineKlassFixedBlock) : 0));\n@@ -1046,1 +1173,3 @@\n-                                               is_interface());\n+                                               is_interface(),\n+                                               has_inline_type_fields() ? java_fields_count() : 0,\n+                                               is_inline_klass());\n@@ -1054,0 +1183,1 @@\n+  bool bounds_check(address addr, bool edge_ok = false, intptr_t size_in_bytes = -1) const PRODUCT_RETURN0;\n@@ -1060,0 +1190,6 @@\n+  inline address adr_inline_type_field_klasses() const;\n+  inline Klass* get_inline_type_field_klass(int idx) const;\n+  inline Klass* get_inline_type_field_klass_or_null(int idx) const;\n+  inline void set_inline_type_field_klass(int idx, Klass* k);\n+  inline void reset_inline_type_field_klass(int idx);\n+\n@@ -1061,1 +1197,1 @@\n-  int size_helper() const {\n+  virtual int size_helper() const {\n@@ -1115,0 +1251,1 @@\n+  const char* signature_name_of_carrier(char c) const;\n@@ -1242,1 +1379,1 @@\n-  void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n+  virtual void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":154,"deletions":17,"binary":false,"changes":171,"status":"modified"},{"patch":"@@ -99,1 +99,1 @@\n-  return ObjectSynchronizer::identity_hash_value_for(object);\n+  return ObjectSynchronizer::FastHashCode(current, object());\n@@ -118,1 +118,1 @@\n-  return !SafepointSynchronize::is_at_safepoint();\n+  return !SafepointSynchronize::is_at_safepoint() ;\n@@ -141,0 +141,2 @@\n+bool oopDesc::is_flatArray_noinline()         const { return is_flatArray();           }\n+bool oopDesc::is_null_free_array_noinline()   const { return is_null_free_array();     }\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -42,0 +42,10 @@\n+\/\/\n+\/\/ oopDesc::_mark - the \"oop mark word\" encoding to be found separately in markWord.hpp\n+\/\/\n+\/\/ oopDesc::_metadata - encodes the object's klass pointer, as a raw pointer in \"_klass\"\n+\/\/                      or compressed pointer in \"_compressed_klass\"\n+\/\/\n+\/\/ The overall size of the _metadata field is dependent on \"UseCompressedClassPointers\",\n+\/\/ hence the terms \"narrow\" (32 bits) vs \"wide\" (64 bits).\n+\/\/\n+\n@@ -107,0 +117,3 @@\n+  inline bool is_inline_type()         const;\n+  inline bool is_flatArray()           const;\n+  inline bool is_null_free_array()     const;\n@@ -113,0 +126,2 @@\n+  bool is_flatArray_noinline()         const;\n+  bool is_null_free_array_noinline()   const;\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -83,1 +83,1 @@\n-  set_mark(markWord::prototype());\n+  set_mark(Klass::default_prototype_header(klass()));\n@@ -214,0 +214,15 @@\n+bool oopDesc::is_inline_type() const { return mark().is_inline_type(); }\n+#ifdef _LP64\n+bool oopDesc::is_flatArray() const {\n+  markWord mrk = mark();\n+  return (mrk.is_unlocked()) ? mrk.is_flat_array() : klass()->is_flatArray_klass();\n+}\n+bool oopDesc::is_null_free_array() const {\n+  markWord mrk = mark();\n+  return (mrk.is_unlocked()) ? mrk.is_null_free_array() : klass()->is_null_free_array_klass();\n+}\n+#else\n+bool oopDesc::is_flatArray()       const { return klass()->is_flatArray_klass(); }\n+bool oopDesc::is_null_free_array() const { return klass()->is_null_free_array_klass(); }\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -203,1 +203,1 @@\n-  bool starts_with(int prefix_char) const {\n+  bool starts_with(char prefix_char) const {\n@@ -233,0 +233,12 @@\n+  \/\/ True if this is a descriptor for a method with void return.\n+  \/\/ (Assumes it is a valid descriptor.)\n+  bool is_void_method_signature() const {\n+    return starts_with('(') && ends_with('V');\n+  }\n+\n+  bool is_Q_signature() const;\n+  bool is_Q_array_signature() const;\n+  bool is_Q_method_signature() const;\n+  Symbol* fundamental_name(TRAPS);\n+  bool is_same_fundamental_type(Symbol*) const;\n+\n@@ -276,0 +288,1 @@\n+  void print_Qvalue_on(outputStream* st) const;  \/\/ Second level print for Q-types.\n","filename":"src\/hotspot\/share\/oops\/symbol.hpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -121,1 +122,1 @@\n-  \/\/ paths to facilitate late inlinig.\n+  \/\/ paths to facilitate late inlining.\n@@ -130,0 +131,1 @@\n+      _call_node(NULL),\n@@ -132,0 +134,8 @@\n+    if (InlineTypeReturnedAsFields && method->is_method_handle_intrinsic()) {\n+      \/\/ If that call has not been optimized by the time optimizations are over,\n+      \/\/ we'll need to add a call to create an inline type instance from the klass\n+      \/\/ returned by the call (see PhaseMacroExpand::expand_mh_intrinsic_return).\n+      \/\/ Separating memory and I\/O projections for exceptions is required to\n+      \/\/ perform that graph transformation.\n+      _separate_io_proj = true;\n+    }\n@@ -146,0 +156,1 @@\n+  PhaseGVN& gvn = kit.gvn();\n@@ -178,1 +189,4 @@\n-  kit.set_arguments_for_java_call(call);\n+  kit.set_arguments_for_java_call(call, is_late_inline());\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -219,1 +233,0 @@\n-\n@@ -231,1 +244,1 @@\n-  if (kit.gvn().type(receiver)->higher_equal(TypePtr::NULL_PTR)) {\n+  if (!receiver->is_InlineType() && kit.gvn().type(receiver)->higher_equal(TypePtr::NULL_PTR)) {\n@@ -279,0 +292,3 @@\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -372,0 +388,4 @@\n+  virtual CallGenerator* inline_cg() {\n+    return _inline_cg;\n+  }\n+\n@@ -426,0 +446,7 @@\n+    \/\/ AlwaysIncrementalInline causes for_method_handle_inline() to\n+    \/\/ return a LateInlineCallGenerator. Extract the\n+    \/\/ InlineCallGenerato from it.\n+    if (AlwaysIncrementalInline && cg->is_late_inline()) {\n+      cg = cg->inline_cg();\n+    }\n+\n@@ -628,3 +655,3 @@\n-  const TypeTuple *r = call->tf()->domain();\n-  for (int i1 = 0; i1 < method()->arg_size(); i1++) {\n-    if (call->in(TypeFunc::Parms + i1)->is_top() && r->field_at(TypeFunc::Parms + i1) != Type::HALF) {\n+  const TypeTuple* r = call->tf()->domain_cc();\n+  for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+    if (call->in(i1)->is_top() && r->field_at(i1) != Type::HALF) {\n@@ -648,10 +675,8 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true);\n-  if ((callprojs.fallthrough_catchproj == call->in(0)) ||\n-      (callprojs.catchall_catchproj    == call->in(0)) ||\n-      (callprojs.fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n-      (callprojs.catchall_memproj      == call->in(TypeFunc::Memory)) ||\n-      (callprojs.fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n-      (callprojs.catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n-      (callprojs.resproj != NULL && call->find_edge(callprojs.resproj) != -1) ||\n-      (callprojs.exobj   != NULL && call->find_edge(callprojs.exobj) != -1)) {\n+  CallProjections* callprojs = call->extract_projections(true);\n+  if ((callprojs->fallthrough_catchproj == call->in(0)) ||\n+      (callprojs->catchall_catchproj    == call->in(0)) ||\n+      (callprojs->fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n+      (callprojs->catchall_memproj      == call->in(TypeFunc::Memory)) ||\n+      (callprojs->fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n+      (callprojs->catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n+      (callprojs->exobj != NULL && call->find_edge(callprojs->exobj) != -1)) {\n@@ -670,1 +695,1 @@\n-    if (is_boxing_late_inline() && callprojs.resproj != nullptr) {\n+    if (is_boxing_late_inline() && callprojs->resproj[0] != nullptr) {\n@@ -673,2 +698,2 @@\n-        if (!has_non_debug_usages(callprojs.resproj) && is_box_cache_valid(call)) {\n-          scalarize_debug_usages(call, callprojs.resproj);\n+        if (!has_non_debug_usages(callprojs->resproj[0]) && is_box_cache_valid(call)) {\n+          scalarize_debug_usages(call, callprojs->resproj[0]);\n@@ -680,1 +705,11 @@\n-    result_not_used = (callprojs.resproj == NULL || callprojs.resproj->outcnt() == 0);\n+    result_not_used = true;\n+    for (uint i = 0; i < callprojs->nb_resproj; i++) {\n+      if (callprojs->resproj[i] != NULL) {\n+        if (callprojs->resproj[i]->outcnt() != 0) {\n+          result_not_used = false;\n+        }\n+        if (call->find_edge(callprojs->resproj[i]) != -1) {\n+          return;\n+        }\n+      }\n+    }\n@@ -696,0 +731,1 @@\n+    PhaseGVN& gvn = *C->initial_gvn();\n@@ -699,1 +735,1 @@\n-      C->initial_gvn()->set_type_bottom(mem);\n+      gvn.set_type_bottom(mem);\n@@ -703,4 +739,2 @@\n-    uint nargs = method()->arg_size();\n-    Node* top = C->top();\n-    for (uint i1 = 0; i1 < nargs; i1++) {\n-      map->set_req(TypeFunc::Parms + i1, top);\n+    for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+      map->set_req(i1, C->top());\n@@ -714,0 +748,5 @@\n+    const TypeTuple* domain_sig = call->_tf->domain_sig();\n+    uint nargs = method()->arg_size();\n+    assert(domain_sig->cnt() - TypeFunc::Parms == nargs, \"inconsistent signature\");\n+\n+    uint j = TypeFunc::Parms;\n@@ -715,1 +754,11 @@\n-      map->set_argument(jvms, i1, call->in(TypeFunc::Parms + i1));\n+      const Type* t = domain_sig->field_at(TypeFunc::Parms + i1);\n+      if (method()->has_scalarized_args() && t->is_inlinetypeptr() && !t->maybe_null() && t->inline_klass()->can_be_passed_as_fields()) {\n+        \/\/ Inline type arguments are not passed by reference: we get an argument per\n+        \/\/ field of the inline type. Build InlineTypeNodes from the inline type arguments.\n+        GraphKit arg_kit(jvms, &gvn);\n+        InlineTypeNode* vt = InlineTypeNode::make_from_multi(&arg_kit, call, t->inline_klass(), j, true);\n+        map->set_control(arg_kit.control());\n+        map->set_argument(jvms, i1, vt);\n+      } else {\n+        map->set_argument(jvms, i1, call->in(j++));\n+      }\n@@ -731,0 +780,18 @@\n+    \/\/ Check if we are late inlining a method handle call that returns an inline type as fields.\n+    Node* buffer_oop = NULL;\n+    ciType* mh_rt = inline_cg()->method()->return_type();\n+    if (is_mh_late_inline() && mh_rt->is_inlinetype() && mh_rt->as_inline_klass()->can_be_returned_as_fields()) {\n+      \/\/ Allocate a buffer for the inline type returned as fields because the caller expects an oop return.\n+      \/\/ Do this before the method handle call in case the buffer allocation triggers deoptimization and\n+      \/\/ we need to \"re-execute\" the call in the interpreter (to make sure the call is only executed once).\n+      GraphKit arg_kit(jvms, &gvn);\n+      {\n+        PreserveReexecuteState preexecs(&arg_kit);\n+        arg_kit.jvms()->set_should_reexecute(true);\n+        arg_kit.inc_sp(nargs);\n+        Node* klass_node = arg_kit.makecon(TypeKlassPtr::make(mh_rt->as_inline_klass()));\n+        buffer_oop = arg_kit.new_instance(klass_node, NULL, NULL, \/* deoptimize_on_exception *\/ true);\n+      }\n+      jvms = arg_kit.transfer_exceptions_into_jvms();\n+    }\n+\n@@ -760,0 +827,29 @@\n+\n+    \/\/ Handle inline type returns\n+    InlineTypeNode* vt = result->isa_InlineType();\n+    if (vt != NULL) {\n+      if (call->tf()->returns_inline_type_as_fields()) {\n+        vt->replace_call_results(&kit, call, C);\n+      } else {\n+        \/\/ Only possible with is_mh_late_inline() when the callee does not \"know\" that the caller expects an oop\n+        assert(is_mh_late_inline(), \"sanity\");\n+        assert(buffer_oop != NULL, \"should have allocated a buffer\");\n+        \/\/ Result might still be allocated (for example, if it has been stored to a non-flattened field)\n+        if (!vt->is_allocated(&kit.gvn())) {\n+          vt->store(&kit, buffer_oop, buffer_oop, vt->type()->inline_klass());\n+          \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+          \/\/ store that would make this buffer accessible by other threads.\n+          AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer_oop, &kit.gvn());\n+          assert(alloc != NULL, \"must have an allocation node\");\n+          kit.insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+          kit.gvn().hash_delete(vt);\n+          vt->set_oop(buffer_oop);\n+          vt = kit.gvn().transform(vt)->as_InlineType();\n+        }\n+        DEBUG_ONLY(buffer_oop = NULL);\n+        \/\/ Convert to InlineTypePtrNode to keep track of field values\n+        result = vt->as_ptr(&kit.gvn());\n+      }\n+    }\n+    assert(buffer_oop == NULL, \"unused buffer allocation\");\n+\n@@ -984,0 +1080,22 @@\n+  \/\/ Allocate inline types if they are merged with objects (similar to Parse::merge_common())\n+  uint tos = kit.jvms()->stkoff() + kit.sp();\n+  uint limit = slow_map->req();\n+  for (uint i = TypeFunc::Parms; i < limit; i++) {\n+    Node* m = kit.map()->in(i);\n+    Node* n = slow_map->in(i);\n+    const Type* t = gvn.type(m)->meet_speculative(gvn.type(n));\n+    if (m->is_InlineType() && !t->isa_inlinetype()) {\n+      \/\/ Allocate inline type in fast path\n+      m = m->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, m);\n+    }\n+    if (n->is_InlineType() && !t->isa_inlinetype()) {\n+      \/\/ Allocate inline type in slow path\n+      PreserveJVMState pjvms(&kit);\n+      kit.set_map(slow_map);\n+      n = n->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, n);\n+      slow_map = kit.stop();\n+    }\n+  }\n+\n@@ -1007,2 +1125,0 @@\n-  uint tos = kit.jvms()->stkoff() + kit.sp();\n-  uint limit = slow_map->req();\n@@ -1044,2 +1160,2 @@\n-  if (IncrementalInlineMH && call_site_count > 0 &&\n-      (input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())) {\n+  if (IncrementalInlineMH && (AlwaysIncrementalInline ||\n+                            (call_site_count > 0 && (input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())))) {\n@@ -1053,0 +1169,20 @@\n+static void cast_argument(int nargs, int arg_nb, ciType* t, GraphKit& kit, bool null_free) {\n+  PhaseGVN& gvn = kit.gvn();\n+  Node* arg = kit.argument(arg_nb);\n+  const Type* arg_type = arg->bottom_type();\n+  const Type* sig_type = TypeOopPtr::make_from_klass(t->as_klass());\n+  if (t->as_klass()->is_inlinetype() && null_free) {\n+    sig_type = sig_type->filter_speculative(TypePtr::NOTNULL);\n+  }\n+  if (arg_type->isa_oopptr() && !arg_type->higher_equal(sig_type)) {\n+    const Type* narrowed_arg_type = arg_type->filter_speculative(sig_type); \/\/ keep speculative part\n+    arg = gvn.transform(new CheckCastPPNode(kit.control(), arg, narrowed_arg_type));\n+    kit.set_argument(arg_nb, arg);\n+  }\n+  if (sig_type->is_inlinetypeptr() && !arg->is_InlineType() &&\n+      !kit.gvn().type(arg)->maybe_null() && t->as_inline_klass()->is_scalarizable()) {\n+    arg = InlineTypeNode::make_from_oop(&kit, arg, t->as_inline_klass());\n+    kit.set_argument(arg_nb, arg);\n+  }\n+}\n+\n@@ -1107,2 +1243,4 @@\n-                                              allow_inline,\n-                                              PROB_ALWAYS);\n+                                              true \/* allow_inline *\/,\n+                                              PROB_ALWAYS,\n+                                              NULL,\n+                                              true);\n@@ -1122,0 +1260,1 @@\n+      int nargs = callee->arg_size();\n@@ -1123,1 +1262,1 @@\n-      Node* member_name = kit.argument(callee->arg_size() - 1);\n+      Node* member_name = kit.argument(nargs - 1);\n@@ -1143,8 +1282,1 @@\n-          Node* arg = kit.argument(0);\n-          const TypeOopPtr* arg_type = arg->bottom_type()->isa_oopptr();\n-          const Type*       sig_type = TypeOopPtr::make_from_klass(signature->accessing_klass());\n-          if (arg_type != NULL && !arg_type->higher_equal(sig_type)) {\n-            const Type* recv_type = arg_type->filter_speculative(sig_type); \/\/ keep speculative part\n-            Node* cast_obj = gvn.transform(new CheckCastPPNode(kit.control(), arg, recv_type));\n-            kit.set_argument(0, cast_obj);\n-          }\n+          cast_argument(nargs, 0, signature->accessing_klass(), kit, false);\n@@ -1156,8 +1288,2 @@\n-            Node* arg = kit.argument(receiver_skip + j);\n-            const TypeOopPtr* arg_type = arg->bottom_type()->isa_oopptr();\n-            const Type*       sig_type = TypeOopPtr::make_from_klass(t->as_klass());\n-            if (arg_type != NULL && !arg_type->higher_equal(sig_type)) {\n-              const Type* narrowed_arg_type = arg_type->filter_speculative(sig_type); \/\/ keep speculative part\n-              Node* cast_obj = gvn.transform(new CheckCastPPNode(kit.control(), arg, narrowed_arg_type));\n-              kit.set_argument(receiver_skip + j, cast_obj);\n-            }\n+            bool null_free = signature->is_null_free_at(i);\n+            cast_argument(nargs, receiver_skip + j, t, kit, null_free);\n@@ -1194,1 +1320,2 @@\n-                                              speculative_receiver_type);\n+                                              speculative_receiver_type,\n+                                              true);\n@@ -1284,1 +1411,1 @@\n-    Node* receiver = kit.null_check_receiver_before_call(method());\n+    kit.null_check_receiver_before_call(method());\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":179,"deletions":52,"binary":false,"changes":231,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -108,3 +109,11 @@\n-    if (!stopped() && result() != NULL) {\n-      BasicType bt = result()->bottom_type()->basic_type();\n-      push_node(bt, result());\n+    Node* res = result();\n+    if (!stopped() && res != NULL) {\n+      BasicType bt = res->bottom_type()->basic_type();\n+      if (C->inlining_incrementally() && res->is_InlineType()) {\n+        \/\/ The caller expects an oop when incrementally inlining an intrinsic that returns an\n+        \/\/ inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        res = res->as_InlineType()->buffer(this);\n+      }\n+      push_node(bt, res);\n@@ -138,1 +147,0 @@\n-  Node* load_mirror_from_klass(Node* klass);\n@@ -160,0 +168,11 @@\n+\n+  enum ArrayKind {\n+    AnyArray,\n+    NonArray,\n+    ObjectArray,\n+    NonObjectArray,\n+    TypeArray,\n+    FlatArray,\n+    NonFlatArray\n+  };\n+\n@@ -161,0 +180,1 @@\n+\n@@ -162,1 +182,1 @@\n-    return generate_array_guard_common(kls, region, false, false);\n+    return generate_array_guard_common(kls, region, AnyArray);\n@@ -165,1 +185,1 @@\n-    return generate_array_guard_common(kls, region, false, true);\n+    return generate_array_guard_common(kls, region, NonArray);\n@@ -168,1 +188,1 @@\n-    return generate_array_guard_common(kls, region, true, false);\n+    return generate_array_guard_common(kls, region, ObjectArray);\n@@ -171,1 +191,12 @@\n-    return generate_array_guard_common(kls, region, true, true);\n+    return generate_array_guard_common(kls, region, NonObjectArray);\n+  }\n+  Node* generate_typeArray_guard(Node* kls, RegionNode* region) {\n+    return generate_array_guard_common(kls, region, TypeArray);\n+  }\n+  Node* generate_flatArray_guard(Node* kls, RegionNode* region) {\n+    assert(UseFlatArray, \"can never be flattened\");\n+    return generate_array_guard_common(kls, region, FlatArray);\n+  }\n+  Node* generate_non_flatArray_guard(Node* kls, RegionNode* region) {\n+    assert(UseFlatArray, \"can never be flattened\");\n+    return generate_array_guard_common(kls, region, NonFlatArray);\n@@ -173,2 +204,1 @@\n-  Node* generate_array_guard_common(Node* kls, RegionNode* region,\n-                                    bool obj_array, bool not_array);\n+  Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);\n@@ -233,0 +263,2 @@\n+  bool inline_unsafe_make_private_buffer();\n+  bool inline_unsafe_finish_private_buffer();\n@@ -241,0 +273,1 @@\n+  bool inline_primitive_Class_conversion(vmIntrinsics::ID id);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":43,"deletions":10,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -64,0 +65,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return NULL;\n+  }\n+\n@@ -979,0 +986,50 @@\n+\/\/ If UseArrayMarkWordCheck is enabled, we can't use immutable memory for the flat array check\n+\/\/ because we are loading the mark word which is mutable. Although the bits we are interested in\n+\/\/ are immutable (we check for markWord::unlocked_value), we need to use raw memory to not break\n+\/\/ anti dependency analysis. Below code will attempt to still move flat array checks out of loops,\n+\/\/ mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::Array)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    ResourceMark rm;\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -991,0 +1048,6 @@\n+\n+  if (UseArrayMarkWordCheck && n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1263,0 +1326,96 @@\n+bool PhaseIdealLoop::flatten_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flattened array and the load of the value\n+  \/\/ happens with a flattened array check then: push the type check\n+  \/\/ through the phi of the flattened array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == NULL) {\n+    return false;\n+  }\n+\n+  assert(obj != NULL && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != NULL, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      _igvn.set_type(cast_clone, cast_clone->Value(&_igvn));\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1269,0 +1428,4 @@\n+  if (flatten_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1420,1 +1583,2 @@\n-  try_sink_out_of_loop(n);\n+  \/\/ TODO Disabled until JDK-8272448 is fixed.\n+  \/\/ try_sink_out_of_loop(n);\n@@ -1424,0 +1588,5 @@\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(&_igvn, this);\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":170,"deletions":1,"binary":false,"changes":171,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -142,1 +143,1 @@\n-inline Node* PhaseMacroExpand::generate_slow_guard(Node** ctrl, Node* test, RegionNode* region) {\n+Node* PhaseMacroExpand::generate_slow_guard(Node** ctrl, Node* test, RegionNode* region) {\n@@ -146,0 +147,4 @@\n+inline Node* PhaseMacroExpand::generate_fair_guard(Node** ctrl, Node* test, RegionNode* region) {\n+  return generate_guard(ctrl, test, region, PROB_FAIR);\n+}\n+\n@@ -286,0 +291,40 @@\n+Node* PhaseMacroExpand::array_lh_test(Node* array, jint mask) {\n+  Node* klass_adr = basic_plus_adr(array, oopDesc::klass_offset_in_bytes());\n+  Node* klass = transform_later(LoadKlassNode::make(_igvn, NULL, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeKlassPtr::OBJECT));\n+  Node* lh_addr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+  Node* lh_val = _igvn.transform(LoadNode::make(_igvn, NULL, C->immutable_memory(), lh_addr, lh_addr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+  Node* masked = transform_later(new AndINode(lh_val, intcon(mask)));\n+  Node* cmp = transform_later(new CmpINode(masked, intcon(0)));\n+  return transform_later(new BoolNode(cmp, BoolTest::ne));\n+}\n+\n+Node* PhaseMacroExpand::generate_flat_array_guard(Node** ctrl, Node* array, RegionNode* region) {\n+  assert(UseFlatArray, \"can never be flattened\");\n+  return generate_fair_guard(ctrl, array_lh_test(array, Klass::_lh_array_tag_vt_value_bit_inplace), region);\n+}\n+\n+Node* PhaseMacroExpand::generate_null_free_array_guard(Node** ctrl, Node* array, RegionNode* region) {\n+  assert(EnableValhalla, \"can never be null free\");\n+  return generate_fair_guard(ctrl, array_lh_test(array, Klass::_lh_null_free_bit_inplace), region);\n+}\n+\n+Node* PhaseMacroExpand::generate_array_guard(Node** ctrl, Node* mem, Node* obj_or_klass, RegionNode* region, jint lh_con) {\n+  if ((*ctrl)->is_top())  return NULL;\n+\n+  Node* kls = NULL;\n+  if (_igvn.type(obj_or_klass)->isa_oopptr()) {\n+    Node* k_adr = basic_plus_adr(obj_or_klass, oopDesc::klass_offset_in_bytes());\n+    kls = transform_later(LoadKlassNode::make(_igvn, NULL, C->immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeKlassPtr::OBJECT));\n+  } else {\n+    assert(_igvn.type(obj_or_klass)->isa_klassptr(), \"what else?\");\n+    kls = obj_or_klass;\n+  }\n+  Node* layout_val = make_load(NULL, mem, kls, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);\n+\n+  layout_val = transform_later(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+  Node* cmp = transform_later(new CmpINode(layout_val, intcon(lh_con)));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+\n+  return generate_fair_guard(ctrl, bol, region);\n+}\n+\n@@ -338,0 +383,19 @@\n+bool PhaseMacroExpand::can_try_zeroing_elimination(AllocateArrayNode* alloc,\n+                                                   Node* src,\n+                                                   Node* dest) const {\n+  const TypeAryPtr* top_dest = _igvn.type(dest)->isa_aryptr();\n+\n+  if (top_dest != NULL) {\n+    if (top_dest->klass() == NULL) {\n+      return false;\n+    }\n+  }\n+\n+  return ReduceBulkZeroing\n+    && !(UseTLAB && ZeroTLAB) \/\/ pointless if already zeroed\n+    && !src->eqv_uncast(dest)\n+    && alloc != NULL\n+    && _igvn.find_int_con(alloc->in(AllocateNode::ALength), 1) > 0\n+    && alloc->maybe_set_complete(&_igvn);\n+}\n+\n@@ -380,0 +444,1 @@\n+                                           Node* dest_length,\n@@ -391,0 +456,2 @@\n+  Node* default_value = NULL;\n+  Node* raw_default_value = NULL;\n@@ -421,0 +488,2 @@\n+      default_value = alloc->in(AllocateNode::DefaultValue);\n+      raw_default_value = alloc->in(AllocateNode::RawDefaultValue);\n@@ -490,1 +559,0 @@\n-      Node* dest_length = alloc->in(AllocateNode::ALength);\n@@ -497,1 +565,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             default_value, raw_default_value,\n+                             basic_elem_type,\n@@ -528,1 +598,0 @@\n-    Node* dest_length = alloc->in(AllocateNode::ALength);\n@@ -534,1 +603,3 @@\n-                           adr_type, dest, basic_elem_type,\n+                           adr_type, dest,\n+                           default_value, raw_default_value,\n+                           basic_elem_type,\n@@ -583,1 +654,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             default_value, raw_default_value,\n+                             basic_elem_type,\n@@ -593,1 +666,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             default_value, raw_default_value,\n+                             basic_elem_type,\n@@ -771,1 +846,3 @@\n-                           adr_type, dest, basic_elem_type,\n+                           adr_type, dest,\n+                           default_value, raw_default_value,\n+                           basic_elem_type,\n@@ -824,0 +901,4 @@\n+    \/\/ Do not let reads from the destination float above the arraycopy.\n+    \/\/ Since we cannot type the arrays, we don't know which slices\n+    \/\/ might be affected.  We could restrict this barrier only to those\n+    \/\/ memory slices which pertain to array elements--but don't bother.\n@@ -833,3 +914,3 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, out_mem);\n-  if (_callprojs.fallthrough_ioproj != NULL) {\n-    _igvn.replace_node(_callprojs.fallthrough_ioproj, *io);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, out_mem);\n+  if (_callprojs->fallthrough_ioproj != NULL) {\n+    _igvn.replace_node(_callprojs->fallthrough_ioproj, *io);\n@@ -837,1 +918,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_catchproj, *ctrl);\n+  _igvn.replace_node(_callprojs->fallthrough_catchproj, *ctrl);\n@@ -877,0 +958,2 @@\n+                                            Node* val,\n+                                            Node* raw_val,\n@@ -892,0 +975,1 @@\n+  assert(basic_elem_type != T_INLINE_TYPE, \"should have been converted to a basic type copy\");\n@@ -915,1 +999,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -920,1 +1004,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -933,1 +1017,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -962,1 +1046,7 @@\n-        mem = StoreNode::make(_igvn, ctrl, mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);\n+        if (val == NULL) {\n+          assert(raw_val == NULL, \"val may not be null\");\n+          mem = StoreNode::make(_igvn, ctrl, mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);\n+        } else {\n+          assert(_igvn.type(val)->isa_narrowoop(), \"should be narrow oop\");\n+          mem = new StoreNNode(ctrl, mem, p1, adr_type, val, MemNode::unordered);\n+        }\n@@ -967,1 +1057,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, raw_val,\n@@ -1083,2 +1173,2 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  *ctrl = _callprojs.fallthrough_catchproj->clone();\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  *ctrl = _callprojs->fallthrough_catchproj->clone();\n@@ -1087,1 +1177,1 @@\n-  Node* m = _callprojs.fallthrough_memproj->clone();\n+  Node* m = _callprojs->fallthrough_memproj->clone();\n@@ -1102,2 +1192,2 @@\n-  if (_callprojs.fallthrough_ioproj != NULL) {\n-    *io = _callprojs.fallthrough_ioproj->clone();\n+  if (_callprojs->fallthrough_ioproj != NULL) {\n+    *io = _callprojs->fallthrough_ioproj->clone();\n@@ -1235,0 +1325,36 @@\n+const TypePtr* PhaseMacroExpand::adjust_for_flat_array(const TypeAryPtr* top_dest, Node*& src_offset,\n+                                                       Node*& dest_offset, Node*& length, BasicType& dest_elem,\n+                                                       Node*& dest_length) {\n+#ifdef ASSERT\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  bool needs_barriers = top_dest->elem()->inline_klass()->contains_oops() &&\n+    bs->array_copy_requires_gc_barriers(dest_length != NULL, T_OBJECT, false, false, BarrierSetC2::Optimization);\n+  assert(!needs_barriers || StressReflectiveCode, \"Flat arracopy would require GC barriers\");\n+#endif\n+  int elem_size = top_dest->klass()->as_flat_array_klass()->element_byte_size();\n+  if (elem_size >= 8) {\n+    if (elem_size > 8) {\n+      \/\/ treat as array of long but scale length, src offset and dest offset\n+      assert((elem_size % 8) == 0, \"not a power of 2?\");\n+      int factor = elem_size \/ 8;\n+      length = transform_later(new MulINode(length, intcon(factor)));\n+      src_offset = transform_later(new MulINode(src_offset, intcon(factor)));\n+      dest_offset = transform_later(new MulINode(dest_offset, intcon(factor)));\n+      if (dest_length != NULL) {\n+        dest_length = transform_later(new MulINode(dest_length, intcon(factor)));\n+      }\n+      elem_size = 8;\n+    }\n+    dest_elem = T_LONG;\n+  } else if (elem_size == 4) {\n+    dest_elem = T_INT;\n+  } else if (elem_size == 2) {\n+    dest_elem = T_CHAR;\n+  } else if (elem_size == 1) {\n+    dest_elem = T_BYTE;\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+  return TypeRawPtr::BOTTOM;\n+}\n+\n@@ -1252,0 +1378,17 @@\n+    const Type* src_type = _igvn.type(src);\n+    const Type* dest_type = _igvn.type(dest);\n+    const TypeAryPtr* top_src = src_type->isa_aryptr();\n+    const TypeAryPtr* top_dest = dest_type->isa_aryptr();\n+    BasicType dest_elem = T_OBJECT;\n+    if (top_dest != NULL && top_dest->klass() != NULL) {\n+      dest_elem = top_dest->klass()->as_array_klass()->element_type()->basic_type();\n+    }\n+    if (dest_elem == T_ARRAY || (dest_elem == T_INLINE_TYPE && top_dest->klass()->is_obj_array_klass())) {\n+      dest_elem = T_OBJECT;\n+    }\n+    if (top_src != NULL && top_src->is_flat()) {\n+      \/\/ If src is flat, dest is guaranteed to be flat as well\n+      dest_elem = T_INLINE_TYPE;\n+      top_dest = top_src;\n+    }\n+\n@@ -1257,0 +1400,1 @@\n+    Node* dest_length = NULL;\n@@ -1260,0 +1404,1 @@\n+      dest_length = alloc->in(AllocateNode::ALength);\n@@ -1262,3 +1407,15 @@\n-    const TypePtr* adr_type = _igvn.type(dest)->is_oopptr()->add_offset(Type::OffsetBot);\n-    if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n-      adr_type = ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr();\n+    const TypePtr* adr_type = NULL;\n+    if (dest_elem == T_INLINE_TYPE) {\n+      assert(dest_length != NULL || StressReflectiveCode, \"must be tightly coupled\");\n+      \/\/ Copy to a flat array modifies multiple memory slices. Conservatively insert a barrier\n+      \/\/ on all slices to prevent writes into the source from floating below the arraycopy.\n+      insert_mem_bar(&ctrl, &mem, Op_MemBarCPUOrder);\n+      adr_type = adjust_for_flat_array(top_dest, src_offset, dest_offset, length, dest_elem, dest_length);\n+    } else {\n+      adr_type = dest_type->is_oopptr()->add_offset(Type::OffsetBot);\n+      if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+        adr_type = ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr();\n+      }\n+      if (ac->_src_type != ac->_dest_type) {\n+        adr_type = TypeRawPtr::BOTTOM;\n+      }\n@@ -1267,1 +1424,1 @@\n-                       adr_type, T_OBJECT,\n+                       adr_type, dest_elem,\n@@ -1269,0 +1426,1 @@\n+                       dest_length,\n@@ -1270,1 +1428,0 @@\n-\n@@ -1301,2 +1458,6 @@\n-  if (is_reference_type(src_elem))  src_elem  = T_OBJECT;\n-  if (is_reference_type(dest_elem)) dest_elem = T_OBJECT;\n+  if (src_elem == T_ARRAY || (src_elem == T_INLINE_TYPE && top_src->klass()->is_obj_array_klass())) {\n+    src_elem = T_OBJECT;\n+  }\n+  if (dest_elem == T_ARRAY || (dest_elem == T_INLINE_TYPE && top_dest->klass()->is_obj_array_klass())) {\n+    dest_elem = T_OBJECT;\n+  }\n@@ -1304,3 +1465,1 @@\n-  if (ac->is_arraycopy_validated() &&\n-      dest_elem != T_CONFLICT &&\n-      src_elem == T_CONFLICT) {\n+  if (ac->is_arraycopy_validated() && dest_elem != T_CONFLICT && src_elem == T_CONFLICT) {\n@@ -1325,0 +1484,1 @@\n+                                   NULL,\n@@ -1331,1 +1491,2 @@\n-  assert(!ac->is_arraycopy_validated() || (src_elem == dest_elem && dest_elem != T_VOID), \"validated but different basic types\");\n+  assert(!ac->is_arraycopy_validated() || (src_elem == dest_elem && dest_elem != T_VOID) ||\n+         (src_elem == T_INLINE_TYPE && StressReflectiveCode), \"validated but different basic types\");\n@@ -1335,1 +1496,8 @@\n-  if (src_elem != dest_elem || dest_elem == T_VOID) {\n+  \/\/\n+  \/\/ We have no stub to copy flattened inline type arrays with oop\n+  \/\/ fields if we need to emit write barriers.\n+  \/\/\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  if (src_elem != dest_elem || dest_elem == T_VOID ||\n+      (dest_elem == T_INLINE_TYPE && top_dest->elem()->inline_klass()->contains_oops() &&\n+       bs->array_copy_requires_gc_barriers(alloc != NULL, T_OBJECT, false, false, BarrierSetC2::Optimization))) {\n@@ -1343,3 +1511,3 @@\n-    _igvn.replace_node(_callprojs.fallthrough_memproj, merge_mem);\n-    if (_callprojs.fallthrough_ioproj != NULL) {\n-      _igvn.replace_node(_callprojs.fallthrough_ioproj, io);\n+    _igvn.replace_node(_callprojs->fallthrough_memproj, merge_mem);\n+    if (_callprojs->fallthrough_ioproj != NULL) {\n+      _igvn.replace_node(_callprojs->fallthrough_ioproj, io);\n@@ -1347,1 +1515,1 @@\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, ctrl);\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, ctrl);\n@@ -1364,5 +1532,3 @@\n-  {\n-    Node* mem = ac->in(TypeFunc::Memory);\n-    merge_mem = MergeMemNode::make(mem);\n-    transform_later(merge_mem);\n-  }\n+  Node* mem = ac->in(TypeFunc::Memory);\n+  merge_mem = MergeMemNode::make(mem);\n+  transform_later(merge_mem);\n@@ -1409,0 +1575,15 @@\n+\n+    \/\/ Handle inline type arrays\n+    if (!top_src->is_flat()) {\n+      if (UseFlatArray && !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        generate_flat_array_guard(&ctrl, src, slow_region);\n+      }\n+      if (EnableValhalla) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        generate_null_free_array_guard(&ctrl, dest, slow_region);\n+      }\n+    } else {\n+      assert(top_dest->is_flat(), \"dest array must be flat\");\n+    }\n@@ -1410,0 +1591,1 @@\n+\n@@ -1412,1 +1594,8 @@\n-  if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+  Node* dest_length = (alloc != NULL) ? alloc->in(AllocateNode::ALength) : NULL;\n+\n+  if (dest_elem == T_INLINE_TYPE) {\n+    \/\/ Copy to a flat array modifies multiple memory slices. Conservatively insert a barrier\n+    \/\/ on all slices to prevent writes into the source from floating below the arraycopy.\n+    insert_mem_bar(&ctrl, &mem, Op_MemBarCPUOrder);\n+    adr_type = adjust_for_flat_array(top_dest, src_offset, dest_offset, length, dest_elem, dest_length);\n+  } else if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n@@ -1421,0 +1610,1 @@\n+                     dest_length,\n@@ -1423,1 +1613,2 @@\n-                     false, ac->has_negative_length_guard(), slow_region);\n+                     false, ac->has_negative_length_guard(),\n+                     slow_region);\n","filename":"src\/hotspot\/share\/opto\/macroArrayCopy.cpp","additions":236,"deletions":45,"binary":false,"changes":281,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -59,0 +60,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -65,0 +67,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -1835,0 +1838,87 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return NULL;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return NULL;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+class CollectOops : public BasicOopIterateClosure {\n+ public:\n+  GrowableArray<Handle>* array;\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < array->length(); i++) {\n+      result_array->obj_at_put(i, array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    \/\/ Value might be oop, but JLS can't see as Object, just iterate through it...\n+    if (oh != NULL && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      array->append(oh);\n+    }\n+  }\n+\n+  void do_oop(oop* o) { add_oop(HeapAccess<>::oop_load(o)); }\n+  void do_oop(narrowOop* v) { add_oop(HeapAccess<>::oop_load(v)); }\n+};\n+\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  StackFrameStream sfs(thread, false \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, NULL, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\n@@ -2617,0 +2707,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":96,"deletions":0,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -119,0 +119,1 @@\n+  VMRegImpl::set_regName();       \/\/ need this before generate_stubs (for printing oop maps).\n@@ -130,1 +131,0 @@\n-  VMRegImpl::set_regName(); \/\/ need this before generate_stubs (for printing oop maps).\n","filename":"src\/hotspot\/share\/runtime\/init.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -986,0 +987,1 @@\n+  _return_buffered_value(nullptr),\n@@ -1053,1 +1055,0 @@\n-\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1046,1 +1046,1 @@\n-  k = ik->array_klass_or_null();\n+  k = k->array_klass_or_null();\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,0 +34,3 @@\n+ * <p>\n+ * The referent must not be an instance of a primitive class; such a value\n+ * can never have another reference to it and cannot be held in a reference type.\n@@ -89,0 +92,2 @@\n+     * @throws IllegalArgumentException if the referent is an instance of an\n+     *         {@link Class#isPrimitiveClass() primitive class}\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/PhantomReference.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -500,0 +500,4 @@\n+        if (referent != null && referent.getClass().isPrimitiveClass()) {\n+            throw new IllegalArgumentException(\"cannot reference a primitive type: \" +\n+                    referent.getClass().getName());\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/Reference.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -174,0 +174,8 @@\n+    \/**\n+     * Visits a {@code DefaultValue} node.\n+     * @param node the node being visited\n+     * @param p a parameter value\n+     * @return a result value\n+     *\/\n+    R visitDefaultValue(DefaultValueTree node, P p);\n+\n@@ -528,0 +536,9 @@\n+     * Visits a {@code WithFieldTree} node.\n+     * @param node the node being visited\n+     * @param p a parameter value\n+     * @return a result value\n+     *\/\n+    R visitWithField(WithFieldTree node, P p);\n+\n+    \/**\n+     * Visits a WildcardTypeTree node.\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/tree\/TreeVisitor.java","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+import com.sun.tools.javac.code.Type.ClassType.Flavor;\n@@ -418,0 +419,12 @@\n+    public boolean isSynthetic() {\n+        return (flags_field & SYNTHETIC) != 0;\n+    }\n+\n+    public boolean isReferenceFavoringPrimitiveClass() {\n+        return (flags() & REFERENCE_FAVORING) != 0;  \/\/ bit set only for primitive classes\n+    }\n+\n+    public boolean isPrimitiveClass() {\n+        return (flags() & PRIMITIVE_CLASS) != 0;\n+    }\n+\n@@ -459,1 +472,7 @@\n-        return name == name.table.names.init;\n+        return name == name.table.names.init && (flags() & STATIC) == 0;\n+    }\n+\n+    \/** Is this symbol a primitive object factory?\n+     *\/\n+    public boolean isPrimitiveObjectFactory() {\n+        return ((name == name.table.names.init && this.type.getReturnType().tsym == this.owner));\n@@ -1319,1 +1338,1 @@\n-                new ClassType(Type.noType, null, null),\n+                new ClassType(Type.noType, null, null, TypeMetadata.EMPTY, Flavor.X_Typeof_X),\n@@ -1356,1 +1375,2 @@\n-                                              type.getMetadata());\n+                                              type.getMetadata(),\n+                                              type.getFlavor());\n@@ -1364,1 +1384,1 @@\n-            else\n+\n@@ -1417,0 +1437,8 @@\n+            } finally {\n+                if (this.type != null && this.type.hasTag(CLASS)) {\n+                    ClassType ct = (ClassType) this.type;\n+                    ct.flavor = ct.flavor.metamorphose(this.flags_field);\n+                    if (!this.type.isIntersection() && this.erasure_field != null && this.erasure_field.hasTag(CLASS)) {\n+                        ((ClassType) this.erasure_field).flavor = ct.flavor;\n+                    }\n+                }\n@@ -1592,0 +1620,1 @@\n+                classType.flavor = Flavor.X_Typeof_X;\n@@ -2008,1 +2037,1 @@\n-                types.asSuper(owner.type, other.owner) != null &&\n+                types.asSuper(owner.type.referenceProjectionOrSelf(), other.owner) != null &&\n@@ -2077,1 +2106,1 @@\n-                types.asSuper(owner.type, other.owner) != null) {\n+                types.asSuper(owner.type.referenceProjectionOrSelf(), other.owner) != null) {\n@@ -2126,0 +2155,1 @@\n+\n@@ -2432,1 +2462,1 @@\n-        \/** Access codes for dereferencing, assignment,\n+        \/** Access codes for dereferencing, assignment, withfield\n@@ -2452,1 +2482,2 @@\n-            FIRSTASGOP(12, Tag.NO_TAG);\n+            WITHFIELD(12, Tag.WITHFIELD),\n+            FIRSTASGOP(14, Tag.NO_TAG);\n@@ -2485,0 +2516,2 @@\n+                    case WITHFIELD:\n+                        return AccessCode.WITHFIELD.code;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Symbol.java","additions":41,"deletions":8,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+import com.sun.tools.javac.code.Type.ClassType.Flavor;\n@@ -239,0 +240,60 @@\n+    public boolean isPrimitiveClass() {\n+        return false;\n+    }\n+\n+    \/**\n+     * Return the `flavor' associated with a ClassType.\n+     * @see ClassType.Flavor\n+     *\/\n+    public Flavor getFlavor() {\n+        throw new AssertionError(\"Unexpected call to getFlavor() on a Type that is not a ClassType: \" + this);\n+    }\n+\n+    \/**\n+     * @return true IFF the receiver is a reference projection type of a *value favoring* primitive class\n+     * and false otherwise.\n+     *\/\n+    public boolean isReferenceProjection() {\n+        return false;\n+    }\n+\n+    \/**\n+     * @return true IFF the receiver is a primitive reference type and false otherwise.\n+     *\/\n+    public boolean isPrimitiveReferenceType() {\n+        return false;\n+    }\n+\n+    \/**\n+     * @return true IFF the receiver is a value projection of a *reference favoring* primitive class type\n+     * and false otherwise.\n+     *\/\n+    public boolean isValueProjection() {\n+        return false;\n+    }\n+\n+    \/**\n+     * Returns the ClassType representing the primitive value type\n+     * of this type, if the class of this type is a primitive class\n+     * null otherwise\n+     *\/\n+    public ClassType asValueType() {\n+        return null;\n+    }\n+\n+    \/**\n+     * @return the reference projection type IFF the receiver is a primitive class type\n+     * and null otherwise\n+     *\/\n+    public Type referenceProjection() {\n+        return null;\n+    }\n+\n+    \/**\n+     * @return the reference projection type IFF the receiver is a primitive class type or self otherwise.\n+     *\/\n+    public Type referenceProjectionOrSelf() {\n+        Type projection = referenceProjection();\n+        return projection != null ? projection : this;\n+    }\n+\n@@ -253,1 +314,1 @@\n-            else return new ClassType(outer1, typarams1, t.tsym, t.metadata) {\n+            else return new ClassType(outer1, typarams1, t.tsym, t.metadata, t.getFlavor()) {\n@@ -947,0 +1008,34 @@\n+    public static class ConstantPoolQType implements PoolConstant {\n+\n+        public final Type type;\n+        final Types types;\n+\n+        public ConstantPoolQType(Type type, Types types) {\n+            this.type = type;\n+            this.types = types;\n+        }\n+\n+        @Override\n+        public Object poolKey(Types types) {\n+            return this;\n+        }\n+\n+        @Override\n+        public int poolTag() {\n+            return ClassFile.CONSTANT_Class;\n+        }\n+\n+        public int hashCode() {\n+            return types.hashCode(type);\n+        }\n+\n+        public boolean equals(Object obj) {\n+            return (obj instanceof ConstantPoolQType) &&\n+                    types.isSameType(type, ((ConstantPoolQType)obj).type);\n+        }\n+\n+        public String toString() {\n+            return type.toString();\n+        }\n+    }\n+\n@@ -950,0 +1045,82 @@\n+        \/**\n+         * The 'flavor' of a ClassType indicates its reference\/primitive projectionness\n+         * viewed against the default nature of the associated class.\n+         *\/\n+        public enum Flavor {\n+\n+            \/**\n+             * Classic reference type. Also reference projection type of a reference-favoring aka\n+             * reference-default primitive class type\n+             *\/\n+            L_TypeOf_L,\n+\n+            \/**\n+             * A primitive reference type:  (Assosiated primitive class could be either\n+             * reference default or value-default)\n+             *\/\n+            L_TypeOf_Q,\n+\n+            \/**\n+             * Value projection type of a primitive-favoring aka primitive-default\n+             * plain vanilla primitive class type,\n+             *\/\n+            Q_TypeOf_Q,\n+\n+            \/**\n+             * Value projection type of a reference-favoring aka\n+             * reference-default primitive class type\n+             *\/\n+            Q_TypeOf_L,\n+\n+            \/**\n+             * Reference projection type of a class type of an as yet unknown default provenance, 'X' will be\n+             * discovered to be 'L' or 'Q' in \"due course\" and mutated suitably.\n+             *\/\n+            L_TypeOf_X,\n+\n+            \/**\n+             * Value projection type of a class type of an as yet unknown default provenance, 'X' will be\n+             * discovered to be 'L' or 'Q' in \"due course\" and mutated suitably.\n+             *\/\n+            Q_TypeOf_X,\n+\n+            \/**\n+             *  As yet unknown projection type of an as yet unknown default provenance class. Is also\n+             *  the terminal flavor for package-info\/module-info files.\n+             *\/\n+            X_Typeof_X,\n+\n+            \/**\n+             *  An error type - we don't care to discriminate them any further.\n+             *\/\n+             E_Typeof_X;\n+\n+            \/\/ We don't seem to need X_Typeof_L or X_Typeof_Q so far.\n+\n+            \/\/ Transform a larval form into a more evolved form\n+            public Flavor metamorphose(long classFlags) {\n+\n+                boolean isPrimtiveClass = (classFlags & PRIMITIVE_CLASS) != 0;\n+                boolean isReferenceFavoring = (classFlags & REFERENCE_FAVORING) != 0;\n+\n+                switch (this) {\n+\n+                    case E_Typeof_X:  \/\/ stunted form\n+                    case L_TypeOf_L:\n+                    case L_TypeOf_Q:\n+                    case Q_TypeOf_L:\n+                    case Q_TypeOf_Q:\n+                            \/\/ These are fully evolved sealed forms or stunted - no futher transformation\n+                            return this;\n+                    case L_TypeOf_X:\n+                            return isPrimtiveClass ? L_TypeOf_Q : L_TypeOf_L;\n+                    case Q_TypeOf_X:\n+                            return isReferenceFavoring ? Q_TypeOf_L : Q_TypeOf_Q;\n+                    case X_Typeof_X:\n+                            return isPrimtiveClass ? (isReferenceFavoring ? L_TypeOf_Q : Q_TypeOf_Q) : L_TypeOf_L;\n+                    default:\n+                            throw new AssertionError(\"Unexpected class type flavor\");\n+                }\n+            }\n+        }\n+\n@@ -978,0 +1155,13 @@\n+        \/** The 'other' projection: If 'this' is type of a primitive class, then 'projection' is the\n+         *  reference projection type and vice versa. Lazily initialized, not to be accessed directly.\n+        *\/\n+        public ClassType projection;\n+\n+        \/** Is this L of default {L, Q, X} or Q of default {L, Q, X} ?\n+         *\/\n+        public Flavor flavor;\n+\n+        \/*\n+         * Use of this constructor is kinda sorta deprecated, use the other constructor\n+         * that forces the call site to consider and include the class type flavor.\n+         *\/\n@@ -979,1 +1169,1 @@\n-            this(outer, typarams, tsym, TypeMetadata.EMPTY);\n+            this(outer, typarams, tsym, TypeMetadata.EMPTY, Flavor.L_TypeOf_L);\n@@ -983,1 +1173,1 @@\n-                         TypeMetadata metadata) {\n+                         TypeMetadata metadata, Flavor flavor) {\n@@ -990,0 +1180,1 @@\n+            this.flavor = flavor;\n@@ -998,1 +1189,1 @@\n-            return new ClassType(outer_field, typarams_field, tsym, md) {\n+            return new ClassType(outer_field, typarams_field, tsym, md, flavor) {\n@@ -1016,1 +1207,1 @@\n-            return new ClassType(getEnclosingType(), typarams_field, tsym, metadata) {\n+            return new ClassType(getEnclosingType(), typarams_field, tsym, metadata, flavor) {\n@@ -1042,1 +1233,11 @@\n-\n+            try {\n+                if (isReferenceProjection()) {\n+                    buf.append('.');\n+                    buf.append(tsym.name.table.names.ref);\n+                } else if (isValueProjection()) {\n+                    buf.append('.');\n+                    buf.append(tsym.name.table.names.val);\n+                }\n+            } catch (CompletionFailure cf) {\n+                \/\/ don't let missing types capsize the boat.\n+            }\n@@ -1074,2 +1275,4 @@\n-                } else if (longform) {\n-                    return sym.getQualifiedName().toString();\n+                }\n+                String s;\n+                if (longform) {\n+                    s =  sym.getQualifiedName().toString();\n@@ -1077,1 +1280,1 @@\n-                    return sym.name.toString();\n+                    s = sym.name.toString();\n@@ -1079,0 +1282,1 @@\n+                return s;\n@@ -1081,0 +1285,4 @@\n+        public Flavor getFlavor() {\n+            return flavor;\n+        }\n+\n@@ -1097,0 +1305,3 @@\n+            if (outer_field != null && outer_field.isReferenceProjection()) {\n+                outer_field = outer_field.asValueType();\n+            }\n@@ -1128,0 +1339,94 @@\n+        @Override\n+        public boolean isPrimitiveClass() {\n+            \/\/ guard against over-eager and\/or inopportune completion\n+            if (tsym != null) {\n+                if (flavor == Flavor.Q_TypeOf_X || tsym.isCompleted()) {\n+                    flavor = flavor.metamorphose(tsym.flags());\n+                }\n+            }\n+            return flavor == Flavor.Q_TypeOf_Q || flavor == Flavor.Q_TypeOf_L;\n+        }\n+\n+        @Override\n+        public boolean isReferenceProjection() {\n+            \/\/ guard against over-eager and\/or inopportune completion\n+            if (tsym != null) {\n+                if (flavor == Flavor.L_TypeOf_X || tsym.isCompleted()) {\n+                    flavor = flavor.metamorphose(tsym.flags());\n+                }\n+            }\n+            return flavor == Flavor.L_TypeOf_Q && tsym.type.getFlavor() == Flavor.Q_TypeOf_Q; \/\/ discount reference favoring primitives.\n+        }\n+\n+        @Override\n+        public boolean isPrimitiveReferenceType() {\n+            \/\/ guard against over-eager and\/or inopportune completion\n+            if (tsym != null) {\n+                if (flavor == Flavor.L_TypeOf_X || tsym.isCompleted()) {\n+                    flavor = flavor.metamorphose(tsym.flags());\n+                }\n+            }\n+            return flavor == Flavor.L_TypeOf_Q;\n+        }\n+\n+        @Override\n+        public boolean isValueProjection() {\n+            \/\/ guard against over-eager and\/or inopportune completion\n+            if (tsym != null) {\n+                if (flavor == Flavor.Q_TypeOf_X || tsym.isCompleted()) {\n+                    flavor = flavor.metamorphose(tsym.flags());\n+\n+                }\n+            }\n+            return flavor == Flavor.Q_TypeOf_L;\n+        }\n+\n+        \/\/ return the primitive value type *preserving parameterizations*\n+        @Override\n+        public ClassType asValueType() {\n+            if (tsym == null || !tsym.isPrimitiveClass())\n+                return null;\n+\n+            switch (flavor) {\n+                case Q_TypeOf_L:\n+                case Q_TypeOf_Q:\n+                    return this;\n+                case L_TypeOf_Q:\n+                    if (projection != null)\n+                        return projection;\n+\n+                    projection = new ClassType(outer_field, typarams_field, tsym, getMetadata(),\n+                            tsym.isReferenceFavoringPrimitiveClass() ? Flavor.Q_TypeOf_L : Flavor.Q_TypeOf_Q);\n+                    projection.allparams_field = allparams_field;\n+                    projection.supertype_field = supertype_field;\n+\n+                    projection.interfaces_field = interfaces_field;\n+                    projection.all_interfaces_field = all_interfaces_field;\n+                    projection.projection = this;\n+                    return projection;\n+                default:\n+                    Assert.check(false, \"Should not get here\");\n+                    return null;\n+            }\n+        }\n+\n+        \/\/ return the reference projection type preserving parameterizations\n+        @Override\n+        public ClassType referenceProjection() {\n+\n+            if (!isPrimitiveClass())\n+                return null;\n+\n+            if (projection != null)\n+                return projection;\n+\n+            projection = new ClassType(outer_field, typarams_field, tsym, getMetadata(), Flavor.L_TypeOf_Q);\n+            projection.allparams_field = allparams_field;\n+            projection.supertype_field = supertype_field;\n+\n+            projection.interfaces_field = interfaces_field;\n+            projection.all_interfaces_field = all_interfaces_field;\n+            projection.projection = this;\n+            return projection;\n+        }\n+\n@@ -1176,1 +1481,1 @@\n-            super(outer, List.nil(), tsym, metadata);\n+            super(outer, List.nil(), tsym, metadata, tsym.type.getFlavor());\n@@ -2343,2 +2648,1 @@\n-            super(noType, List.nil(), null);\n-            this.tsym = tsym;\n+            super(noType, List.nil(), tsym, TypeMetadata.EMPTY, Flavor.E_Typeof_X);\n@@ -2349,2 +2653,2 @@\n-                          TypeMetadata metadata) {\n-            super(noType, List.nil(), null, metadata);\n+                          TypeMetadata metadata, Flavor flavor) {\n+            super(noType, List.nil(), null, metadata, flavor);\n@@ -2357,1 +2661,1 @@\n-            return new ErrorType(originalType, tsym, md) {\n+            return new ErrorType(originalType, tsym, md, getFlavor()) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Type.java","additions":319,"deletions":15,"binary":false,"changes":334,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+import com.sun.tools.javac.code.Type.ClassType.Flavor;\n@@ -58,0 +59,1 @@\n+import com.sun.tools.javac.resources.CompilerProperties.Notes;\n@@ -170,0 +172,1 @@\n+        allowPrimitiveClasses = Feature.PRIMITIVE_CLASSES.allowedInSource(source);\n@@ -202,0 +205,4 @@\n+    \/** Switch: allow primitive classes ?\n+     *\/\n+    boolean allowPrimitiveClasses;\n+\n@@ -320,1 +327,11 @@\n-                log.error(pos, Errors.CantAssignValToFinalVar(v));\n+                boolean complain = true;\n+                \/* Allow updates to instance fields of primitive classes by any method in the same nest via the\n+                   withfield operator -This does not result in mutation of final fields; the code generator\n+                   would implement `copy on write' semantics via the opcode `withfield'.\n+                *\/\n+                if (env.info.inWithField && v.getKind() == ElementKind.FIELD && (v.flags() & STATIC) == 0 && v.owner.isPrimitiveClass()) {\n+                    if (env.enclClass.sym.outermostClass() == v.owner.outermostClass())\n+                        complain = false;\n+                }\n+                if (complain)\n+                    log.error(pos, Errors.CantAssignValToFinalVar(v));\n@@ -814,1 +831,1 @@\n-                List<Type> bounds = List.of(attribType(tvar.bounds.head, env));\n+                List<Type> bounds = List.of(chk.checkRefType(tvar.bounds.head, attribType(tvar.bounds.head, env), false));\n@@ -816,1 +833,1 @@\n-                    bounds = bounds.prepend(attribType(bound, env));\n+                    bounds = bounds.prepend(chk.checkRefType(bound, attribType(bound, env), false));\n@@ -1198,1 +1215,1 @@\n-                            TreeInfo.getConstructorInvocationName(body.stats, names) == names.empty) {\n+                            TreeInfo.getConstructorInvocationName(body.stats, names, true) == names.empty) {\n@@ -1211,0 +1228,6 @@\n+                    } else if ((env.enclClass.sym.flags() & PRIMITIVE_CLASS) != 0 &&\n+                        (tree.mods.flags & GENERATEDCONSTR) == 0 &&\n+                        TreeInfo.isSuperCall(body.stats.head)) {\n+                        \/\/ primitive constructors are not allowed to call super directly,\n+                        \/\/ but tolerate compiler generated ones\n+                        log.error(tree.body.stats.head.pos(), Errors.CallToSuperNotAllowedInPrimitiveCtor);\n@@ -1229,0 +1252,6 @@\n+                if (m.isConstructor() && m.type.getParameterTypes().size() == 0) {\n+                    if ((owner.type == syms.objectType) ||\n+                            (tree.body.stats.size() == 1 && TreeInfo.getConstructorInvocationName(tree.body.stats, names, false) == names._super)) {\n+                        m.flags_field |= EMPTYNOARGCONSTR;\n+                    }\n+                }\n@@ -1298,0 +1327,3 @@\n+            \/* Don't want constant propagation\/folding for instance fields of primitive classes,\n+               as these can undergo updates via copy on write.\n+            *\/\n@@ -1299,1 +1331,1 @@\n-                if ((v.flags_field & FINAL) == 0 ||\n+                if ((v.flags_field & FINAL) == 0 || ((v.flags_field & STATIC) == 0 && v.owner.isPrimitiveClass()) ||\n@@ -1423,1 +1455,5 @@\n-            if ((tree.flags & STATIC) != 0) localEnv.info.staticLevel++;\n+            if ((tree.flags & STATIC) != 0)\n+                localEnv.info.staticLevel++;\n+            else if (tree.stats.size() > 0)\n+                env.info.scope.owner.flags_field |= HASINITBLOCK;\n+\n@@ -1488,0 +1524,34 @@\n+    public void visitWithField(JCWithField tree) {\n+        boolean inWithField = env.info.inWithField;\n+        try {\n+            env.info.inWithField = true;\n+            Type fieldtype = attribTree(tree.field, env.dup(tree), varAssignmentInfo);\n+            attribExpr(tree.value, env, fieldtype);\n+            Type capturedType = syms.errType;\n+            if (tree.field.type != null && !tree.field.type.isErroneous()) {\n+                final Symbol sym = TreeInfo.symbol(tree.field);\n+                if (sym == null || sym.kind != VAR || sym.owner.kind != TYP ||\n+                        (sym.flags() & STATIC) != 0 || !sym.owner.isPrimitiveClass()) {\n+                    log.error(tree.field.pos(), Errors.PrimitiveClassInstanceFieldExpectedHere);\n+                } else {\n+                    Type ownType = sym.owner.type;\n+                    switch(tree.field.getTag()) {\n+                        case IDENT:\n+                            JCIdent ident = (JCIdent) tree.field;\n+                            ownType = ident.sym.owner.type;\n+                            break;\n+                        case SELECT:\n+                            JCFieldAccess fieldAccess = (JCFieldAccess) tree.field;\n+                            ownType = fieldAccess.selected.type;\n+                            break;\n+                    }\n+                    \/\/ withfield always evaluates to the primitive value type.\n+                    capturedType = capture(ownType.asValueType());\n+                }\n+            }\n+            result = check(tree, capturedType, KindSelector.VAL, resultInfo);\n+        } finally {\n+            env.info.inWithField = inWithField;\n+        }\n+    }\n+\n@@ -1531,1 +1601,1 @@\n-                Type base = types.asSuper(exprType, syms.iterableType.tsym);\n+                Type base = types.asSuper(exprType.referenceProjectionOrSelf(), syms.iterableType.tsym);\n@@ -1547,1 +1617,1 @@\n-                    if (types.asSuper(iterSymbol.type.getReturnType(), syms.iteratorType.tsym) == null) {\n+                    if (types.asSuper(iterSymbol.type.getReturnType().referenceProjectionOrSelf(), syms.iteratorType.tsym) == null) {\n@@ -1858,1 +1928,1 @@\n-        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env));\n+        chk.checkIdentityType(tree.pos(), attribExpr(tree.lock, env));\n@@ -1947,1 +2017,1 @@\n-            types.asSuper(resource, syms.autoCloseableType.tsym) != null &&\n+            types.asSuper(resource.referenceProjectionOrSelf(), syms.autoCloseableType.tsym) != null &&\n@@ -2137,1 +2207,2 @@\n-            \/\/ Those were all the cases that could result in a primitive\n+            \/\/ Those were all the cases that could result in a primitive. See if primitive boxing and primitive\n+            \/\/ value conversions bring about a convergence.\n@@ -2139,1 +2210,2 @@\n-                                 .map(t -> t.isPrimitive() ? types.boxedClass(t).type : t)\n+                                 .map(t -> t.isPrimitive() ? types.boxedClass(t).type\n+                                         : t.isPrimitiveReferenceType() ? t.asValueType() : t)\n@@ -2150,1 +2222,1 @@\n-                                 .map(t -> chk.checkNonVoid(posIt.next(), t))\n+                                 .map(t -> chk.checkNonVoid(posIt.next(), t.isPrimitiveClass() ? t.referenceProjection() : t))\n@@ -2153,1 +2225,1 @@\n-            \/\/ both are known to be reference types.  The result is\n+            \/\/ both are known to be reference types (or projections).  The result is\n@@ -2580,0 +2652,30 @@\n+            final Symbol symbol = TreeInfo.symbol(tree.meth);\n+            if (symbol != null) {\n+                \/* Is this an ill conceived attempt to invoke jlO methods not available on primitive class types ??\n+                 *\/\n+                boolean superCallOnPrimitiveReceiver = env.enclClass.sym.isPrimitiveClass()\n+                        && (tree.meth.hasTag(SELECT))\n+                        && ((JCFieldAccess)tree.meth).selected.hasTag(IDENT)\n+                        && TreeInfo.name(((JCFieldAccess)tree.meth).selected) == names._super;\n+                if (qualifier.tsym.isPrimitiveClass() || superCallOnPrimitiveReceiver) {\n+                    int argSize = argtypes.size();\n+                    Name name = symbol.name;\n+                    switch (name.toString()) {\n+                        case \"wait\":\n+                            if (argSize == 0\n+                                    || (types.isConvertible(argtypes.head, syms.longType) &&\n+                                    (argSize == 1 || (argSize == 2 && types.isConvertible(argtypes.tail.head, syms.intType))))) {\n+                                log.error(tree.pos(), Errors.PrimitiveClassDoesNotSupport(name));\n+                            }\n+                            break;\n+                        case \"notify\":\n+                        case \"notifyAll\":\n+                        case \"clone\":\n+                        case \"finalize\":\n+                            if (argSize == 0)\n+                                log.error(tree.pos(), Errors.PrimitiveClassDoesNotSupport(name));\n+                            break;\n+                    }\n+                }\n+            }\n+\n@@ -2594,0 +2696,4 @@\n+                \/\/ Special treatment for primitive classes: Given an expression v of type V where\n+                \/\/ V is a primitive class, v.getClass() is typed to be Class<? extends |V.ref|>\n+                Type wcb = types.erasure(qualifierType.isPrimitiveClass() ?\n+                                                qualifierType.referenceProjection() : qualifierType);\n@@ -2595,1 +2701,1 @@\n-                        List.of(new WildcardType(types.erasure(qualifierType),\n+                        List.of(new WildcardType(wcb,\n@@ -2599,1 +2705,2 @@\n-                        restype.getMetadata());\n+                        restype.getMetadata(),\n+                        restype.getFlavor());\n@@ -2722,0 +2829,3 @@\n+            if (clazztype.tsym == syms.objectType.tsym && cdef == null && !tree.classDeclRemoved()) {\n+                log.note(tree.pos(), Notes.CantInstantiateObjectDirectly);\n+            }\n@@ -2768,0 +2878,11 @@\n+            \/\/ Check that it is an instantiation of a class and not a projection type\n+            JCExpression instantiation = clazz;\n+            if (instantiation.hasTag(TYPEAPPLY))\n+                instantiation = ((JCTypeApply) instantiation).clazz;\n+            if (instantiation.hasTag(SELECT)) {\n+                JCFieldAccess fieldAccess = (JCFieldAccess) instantiation;\n+                if (fieldAccess.selected.type.tsym.isPrimitiveClass() &&\n+                        (fieldAccess.name == names.ref || fieldAccess.name == names.val)) {\n+                    log.error(tree.pos(), Errors.ProjectionCantBeInstantiated);\n+                }\n+            }\n@@ -2792,1 +2913,2 @@\n-                                               clazztype.getMetadata());\n+                                               clazztype.getMetadata(),\n+                                               clazztype.getFlavor());\n@@ -2856,0 +2978,4 @@\n+        \/\/ For primitive classes construction always returns the value type.\n+        if (owntype.tsym.isPrimitiveClass()) {\n+            owntype = owntype.asValueType();\n+        }\n@@ -2939,0 +3065,1 @@\n+                    chk.checkParameterizationByPrimitiveClass(tree, clazztype);\n@@ -3011,0 +3138,3 @@\n+        \/\/ Likewise arg can't be null if it is a primitive class instance.\n+        if (types.isPrimitiveClass(arg.type))\n+            return arg;\n@@ -4025,0 +4155,1 @@\n+                chk.checkForSuspectClassLiteralComparison(tree, left, right);\n@@ -4268,0 +4399,10 @@\n+        Assert.check(site == tree.selected.type);\n+        if (tree.name == names._class && site.isPrimitiveClass()) {\n+            \/* JDK-8269956: Where a reflective (class) literal is needed, the unqualified Point.class is\n+             * always the \"primary\" mirror - representing the primitive reference runtime type - thereby\n+             * always matching the behavior of Object::getClass\n+             *\/\n+             if (!tree.selected.hasTag(SELECT) || ((JCFieldAccess) tree.selected).name != names.val) {\n+                 tree.selected.setType(site = site.referenceProjection());\n+             }\n+        }\n@@ -4280,1 +4421,1 @@\n-                return ;\n+                return;\n@@ -4288,0 +4429,1 @@\n+\n@@ -4383,1 +4525,1 @@\n-                Type site1 = types.asSuper(env.enclClass.sym.type, site.tsym);\n+                Type site1 = types.asSuper(env.enclClass.sym.type.referenceProjectionOrSelf(), site.tsym);\n@@ -4426,0 +4568,2 @@\n+                } else if ((name == names.ref || name == names.val) && site.tsym != null && site.tsym.isPrimitiveClass() && isType(location) && resultInfo.pkind.contains(KindSelector.TYP)) {\n+                    return site.tsym; \/\/ TODO: JDK-8244229: Need more robust handling of .ref and .val reference in source code\n@@ -4529,1 +4673,1 @@\n-                \/\/ except for two situations:\n+                \/\/ except for three situations:\n@@ -4532,0 +4676,1 @@\n+                    Assert.check(owntype.getFlavor() != Flavor.X_Typeof_X);\n@@ -4535,1 +4680,28 @@\n-                    \/\/ (a) If the symbol's type is parameterized, erase it\n+                    \/\/ (a) If symbol is a primitive class and its reference\/value projection\n+                    \/\/ is requested via the .ref\/.val notation, then adjust the computed type to\n+                    \/\/ reflect this.\n+                    if (sym.isPrimitiveClass()) {\n+                        if (sym.isReferenceFavoringPrimitiveClass()) {\n+                            Assert.check(owntype.getFlavor() == Flavor.L_TypeOf_Q);\n+                        } else {\n+                            Assert.check(owntype.getFlavor() == Flavor.Q_TypeOf_Q);\n+                        }\n+                        if (tree.hasTag(SELECT)) {\n+                            Name name = ((JCFieldAccess)tree).name;\n+                            if (name == names.ref) {\n+                                if (sym.isReferenceFavoringPrimitiveClass()) {\n+                                    \/\/ We should already be good to go with owntype\n+                                } else {\n+                                    owntype = new ClassType(ownOuter, owntype.getTypeArguments(), (TypeSymbol)sym, owntype.getMetadata(), Flavor.L_TypeOf_Q);\n+                                }\n+                            } else if (name == names.val) {\n+                                if (sym.isReferenceFavoringPrimitiveClass()) {\n+                                    owntype = new ClassType(ownOuter, owntype.getTypeArguments(), (TypeSymbol)sym, owntype.getMetadata(), Flavor.Q_TypeOf_L);\n+                                } else {\n+                                    \/\/ We should already be good to go with owntype\n+                                }\n+                            }\n+                        }\n+                    }\n+\n+                    \/\/ (b) If the symbol's type is parameterized, erase it\n@@ -4542,1 +4714,1 @@\n-                    \/\/ (b) If the symbol's type is an inner class, then\n+                    \/\/ (c) If the symbol's type is an inner class, then\n@@ -4562,1 +4734,1 @@\n-                                owntype.getMetadata());\n+                                owntype.getMetadata(), owntype.getFlavor());\n@@ -4879,0 +5051,28 @@\n+    public void visitDefaultValue(JCDefaultValue tree) {\n+        if (!allowPrimitiveClasses) {\n+            log.error(DiagnosticFlag.SOURCE_LEVEL, tree.pos(),\n+                    Feature.PRIMITIVE_CLASSES.error(sourceName));\n+        }\n+\n+        \/\/ Attribute the qualifier expression, and determine its symbol (if any).\n+        Type site = attribTree(tree.clazz, env, new ResultInfo(KindSelector.TYP_PCK, Type.noType));\n+        if (!pkind().contains(KindSelector.TYP_PCK))\n+            site = capture(site); \/\/ Capture field access\n+\n+        Symbol sym = switch (site.getTag()) {\n+                case WILDCARD -> throw new AssertionError(tree);\n+                case PACKAGE -> {\n+                    log.error(tree.pos, Errors.CantResolveLocation(Kinds.KindName.CLASS, site.tsym.getQualifiedName(), null, null,\n+                            Fragments.Location(Kinds.typeKindName(env.enclClass.type), env.enclClass.type, null)));\n+                    yield syms.errSymbol;\n+                }\n+                case ERROR -> types.createErrorType(names._default, site.tsym, site).tsym;\n+                default -> new VarSymbol(STATIC, names._default, site, site.tsym);\n+        };\n+\n+        if (site.hasTag(TYPEVAR) && sym.kind != ERR) {\n+            site = types.skipTypeVars(site, true);\n+        }\n+        result = checkId(tree, site, sym, env, resultInfo);\n+    }\n+\n@@ -4945,1 +5145,1 @@\n-                                        clazztype.getMetadata());\n+                                        clazztype.getMetadata(), clazztype.getFlavor());\n@@ -5072,1 +5272,1 @@\n-                make.Modifiers(PUBLIC | ABSTRACT),\n+                make.Modifiers(PUBLIC | ABSTRACT | (extending != null && TreeInfo.symbol(extending).isPrimitiveClass() ? PRIMITIVE_CLASS : 0)),\n@@ -5095,1 +5295,1 @@\n-        result = check(tree, new WildcardType(chk.checkRefType(tree.pos(), type),\n+        result = check(tree, new WildcardType(chk.checkRefType(tree.pos(), type, false),\n@@ -5202,0 +5402,5 @@\n+            if (c.isPrimitiveClass()) {\n+                final Env<AttrContext> env = typeEnvs.get(c);\n+                if (env != null && env.tree != null && env.tree.hasTag(CLASSDEF))\n+                    chk.checkNonCyclicMembership((JCClassDecl)env.tree);\n+            }\n@@ -5315,1 +5520,1 @@\n-            } else {\n+            } else if ((c.flags_field & Flags.COMPOUND) == 0) {\n@@ -5370,0 +5575,8 @@\n+                if ((c.flags() & (PRIMITIVE_CLASS | ABSTRACT)) == PRIMITIVE_CLASS) { \/\/ for non-intersection, concrete primitive classes.\n+                    Assert.check(env.tree.hasTag(CLASSDEF));\n+                    JCClassDecl classDecl = (JCClassDecl) env.tree;\n+                    if (classDecl.extending != null) {\n+                        chk.checkSuperConstraintsOfPrimitiveClass(env.tree.pos(), c);\n+                    }\n+                }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":240,"deletions":27,"binary":false,"changes":267,"status":"modified"},{"patch":"@@ -91,0 +91,5 @@\n+    \/**\n+     *  Is this an attribution environment for a withfield operation ?\n+     *\/\n+    boolean inWithField = false;\n+\n@@ -150,0 +155,1 @@\n+        info.inWithField = inWithField;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/AttrContext.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -114,0 +114,1 @@\n+     *  Optionally, check only for no-arg ctor invocation\n@@ -115,1 +116,1 @@\n-    public static Name getConstructorInvocationName(List<? extends JCTree> trees, Names names) {\n+    public static Name getConstructorInvocationName(List<? extends JCTree> trees, Names names, boolean argsAllowed) {\n@@ -121,4 +122,6 @@\n-                    Name methName = TreeInfo.name(apply.meth);\n-                    if (methName == names._this ||\n-                        methName == names._super) {\n-                        return methName;\n+                    if (argsAllowed || apply.args.size() == 0) {\n+                        Name methName = TreeInfo.name(apply.meth);\n+                        if (methName == names._this ||\n+                                methName == names._super) {\n+                            return methName;\n+                        }\n@@ -488,0 +491,2 @@\n+            case DEFAULT_VALUE:\n+                return getStartPos(((JCDefaultValue) tree).clazz);\n@@ -639,0 +644,2 @@\n+            case WITHFIELD:\n+                return getEndPos(((JCWithField) tree).value, endPosTable);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeInfo.java","additions":12,"deletions":5,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -47,0 +47,2 @@\n+\n+compiler\/valhalla\/inlinetypes\/TestNativeClone.java            8270098   generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList-zgc.txt","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -122,0 +122,27 @@\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -153,0 +153,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n","filename":"test\/lib\/sun\/hotspot\/WhiteBox.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"}]}