{"files":[{"patch":"@@ -1191,1 +1191,1 @@\n-            version: \"7.2\",\n+            version: \"7.3\",\n@@ -1193,1 +1193,1 @@\n-            file: \"bundles\/jtreg-7.2+1.zip\",\n+            file: \"bundles\/jtreg-7.3+1.zip\",\n@@ -1207,1 +1207,1 @@\n-            revision: \"3.0-14-jdk-asm+1.0\",\n+            revision: \"3.0-15-jdk-asm+1.0\",\n","filename":"make\/conf\/jib-profiles.js","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -514,1 +514,1 @@\n-  Method* m = *interpreter_frame_method_addr();\n+  Method* m = safe_interpreter_frame_method();\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -502,1 +502,1 @@\n-  Method* m = *interpreter_frame_method_addr();\n+  Method* m = safe_interpreter_frame_method();\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1808,1 +1808,1 @@\n-  intx cache_line_size = prefetch_data_size();\n+  int cache_line_size = checked_cast<int>(prefetch_data_size());\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2444,3 +2444,4 @@\n-          if (cha_monomorphic_target->holder() != compilation()->env()->Object_klass()) {\n-            ciInstanceKlass* holder = cha_monomorphic_target->holder();\n-            ciInstanceKlass* constraint = (holder->is_subtype_of(singleton) ? holder : singleton); \/\/ avoid upcasts\n+          ciInstanceKlass* holder = cha_monomorphic_target->holder();\n+          ciInstanceKlass* constraint = (holder->is_subtype_of(singleton) ? holder : singleton); \/\/ avoid upcasts\n+          if (holder != compilation()->env()->Object_klass() &&\n+              (!type_is_exact || receiver_klass->is_subtype_of(constraint))) {\n@@ -2459,1 +2460,1 @@\n-            cha_monomorphic_target = nullptr; \/\/ subtype check against Object is useless\n+            cha_monomorphic_target = nullptr;\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1744,0 +1744,1 @@\n+        close(fd);\n@@ -1768,0 +1769,1 @@\n+        close(fd);\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1008,1 +1008,1 @@\n-    _annotations_present |= nth_bit((int)id);\n+    _annotations_present |= (int)nth_bit((int)id);\n@@ -1013,1 +1013,1 @@\n-    _annotations_present &= ~nth_bit((int)id);\n+    _annotations_present &= (int)~nth_bit((int)id);\n@@ -1844,2 +1844,2 @@\n-  const unsigned int size =\n-    (*localvariable_table_length) * sizeof(Classfile_LVT_Element) \/ sizeof(u2);\n+  const unsigned int size = checked_cast<unsigned>(\n+    (*localvariable_table_length) * sizeof(Classfile_LVT_Element) \/ sizeof(u2));\n@@ -2517,1 +2517,1 @@\n-      calculated_attribute_length +=\n+      calculated_attribute_length += checked_cast<unsigned int>(\n@@ -2525,1 +2525,1 @@\n-              sizeof(u2) );  \/\/ catch_type_index\n+              sizeof(u2) )); \/\/ catch_type_index\n@@ -2532,2 +2532,2 @@\n-                                       sizeof(code_attribute_name_index) +\n-                                       sizeof(code_attribute_length);\n+                                       (unsigned)sizeof(code_attribute_name_index) +\n+                                       (unsigned)sizeof(code_attribute_length);\n@@ -2647,1 +2647,1 @@\n-      const u2 real_length = (method_parameters_length * 4u) + 1u;\n+      const u4 real_length = (method_parameters_length * 4u) + 1u;\n@@ -3424,1 +3424,1 @@\n-u2 ClassFileParser::parse_classfile_record_attribute(const ClassFileStream* const cfs,\n+u4 ClassFileParser::parse_classfile_record_attribute(const ClassFileStream* const cfs,\n@@ -3626,1 +3626,1 @@\n-  const int operand_count = (attribute_byte_length - sizeof(u2)) \/ sizeof(u2);\n+  const unsigned int operand_count = (attribute_byte_length - (unsigned)sizeof(u2)) \/ (unsigned)sizeof(u2);\n@@ -5190,1 +5190,1 @@\n-          int newlen = c - (char*) signature;\n+          int newlen = pointer_delta_as_int(c, (char*) signature);\n@@ -5412,1 +5412,1 @@\n-      length -= nextp - p;\n+      length -= pointer_delta_as_int(nextp, p);\n@@ -5644,1 +5644,1 @@\n-  ik->set_initial_method_idnum(ik->methods()->length());\n+  ik->set_initial_method_idnum(checked_cast<u2>(ik->methods()->length()));\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":14,"deletions":14,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -359,1 +359,1 @@\n-  u2 parse_classfile_record_attribute(const ClassFileStream* const cfs,\n+  u4 parse_classfile_record_attribute(const ClassFileStream* const cfs,\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -221,1 +221,1 @@\n-  return SymbolTable::new_symbol(name, start - base, end - base);\n+  return SymbolTable::new_symbol(name, pointer_delta_as_int(start, base), pointer_delta_as_int(end, base));\n@@ -272,0 +272,2 @@\n+        \/\/ We don't verify the length of the classfile stream fits in an int, but this is the\n+        \/\/ bootloader so we have control of this.\n@@ -274,1 +276,1 @@\n-                                   st.st_size,\n+                                   checked_cast<int>(st.st_size),\n@@ -423,1 +425,1 @@\n-                               (int)size,\n+                               checked_cast<int>(size),\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -817,1 +817,1 @@\n-  return vmSymbols::as_SID( (info >> shift) & mask );\n+  return vmSymbols::as_SID( checked_cast<int>((info >> shift) & mask));\n@@ -824,1 +824,1 @@\n-  return vmSymbols::as_SID( (info >> shift) & mask );\n+  return vmSymbols::as_SID( checked_cast<int>((info >> shift) & mask));\n@@ -831,1 +831,1 @@\n-  return vmSymbols::as_SID( (info >> shift) & mask );\n+  return vmSymbols::as_SID( checked_cast<int>((info >> shift) & mask));\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -91,1 +91,2 @@\n-    _evac_failure_regions(evac_failure_regions)\n+    _evac_failure_regions(evac_failure_regions),\n+    _evac_failure_enqueued_cards(0)\n@@ -150,0 +151,4 @@\n+size_t G1ParScanThreadState::evac_failure_enqueued_cards() const {\n+  return _evac_failure_enqueued_cards;\n+}\n+\n@@ -599,0 +604,1 @@\n+    size_t evac_fail_enqueued_cards = pss->evac_failure_enqueued_cards();\n@@ -603,0 +609,1 @@\n+    p->record_or_add_thread_work_item(G1GCPhaseTimes::MergePSS, worker_id, evac_fail_enqueued_cards, G1GCPhaseTimes::MergePSSEvacFailExtra);\n@@ -644,6 +651,3 @@\n-    \/\/ existing closure to scan evacuated objects because:\n-    \/\/ - for objects referring into the collection set we do not need to gather\n-    \/\/ cards at this time. The regions they are in will be unconditionally turned\n-    \/\/ to old regions without remembered sets.\n-    \/\/ - since we are iterating from a collection set region (i.e. never a Survivor\n-    \/\/ region), we always need to gather cards for this case.\n+    \/\/ existing closure to scan evacuated objects; since we are iterating from a\n+    \/\/ collection set region (i.e. never a Survivor region), we always need to\n+    \/\/ gather cards for this case.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":11,"deletions":7,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -983,1 +983,3 @@\n-  entry->fill_in(info.field_holder(), info.offset(), (u2)info.index(), (u1)state, (u1)get_code, (u1)put_code);\n+  entry->fill_in(info.field_holder(), info.offset(),\n+                 checked_cast<u2>(info.index()), checked_cast<u1>(state),\n+                 static_cast<u1>(get_code), static_cast<u1>(put_code));\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -188,1 +188,1 @@\n-    Bytes::put_native_u2(p, field_entry_index);\n+    Bytes::put_native_u2(p, checked_cast<u2>(field_entry_index));\n@@ -192,1 +192,1 @@\n-    Bytes::put_Java_u2(p, pool_index);\n+    Bytes::put_Java_u2(p, checked_cast<u2>(pool_index));\n","filename":"src\/hotspot\/share\/interpreter\/rewriter.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -93,0 +93,1 @@\n+  LOG_TAG(heapdump) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-  u1 _tos_state;                      \/\/ TOS state\n+  u1 _tos_state;                \/\/ TOS state\n@@ -109,5 +109,9 @@\n-  void set_flags(bool is_final, bool is_volatile, bool is_flat, bool is_null_free_inline_type) {\n-    u1 new_flags = (static_cast<u1>(is_final) << static_cast<u1>(is_final_shift)) | static_cast<u1>(is_volatile) |\n-      (static_cast<u1>(is_flat) << static_cast<u1>(is_flat_shift)) |\n-      (static_cast<u1>(is_null_free_inline_type) << static_cast<u1>(is_null_free_inline_type_shift));\n-    _flags = new_flags;\n+  void set_flags(bool is_final_flag, bool is_volatile_flag, bool is_flat_flag, bool is_null_free_inline_type_flag) {\n+    u1 new_flags = (is_final_flag << is_final_shift) | static_cast<int>(is_volatile_flag) |\n+      (is_flat_flag << is_flat_shift) |\n+      (is_null_free_inline_type_flag << is_null_free_inline_type_shift);\n+    _flags = checked_cast<u1>(new_flags);\n+    assert(is_final() == is_final_flag, \"Must be\");\n+    assert(is_volatile() == is_volatile_flag, \"Must be\");\n+    assert(is_flat() == is_flat_flag, \"Must be\");\n+    assert(is_null_free_inline_type() == is_null_free_inline_type_flag, \"Must be\");\n@@ -126,1 +130,1 @@\n-  void fill_in(InstanceKlass* klass, intx offset, int index, int tos_state, u1 b1, u1 b2) {\n+  void fill_in(InstanceKlass* klass, int offset, u2 index, u1 tos_state, u1 b1, u1 b2) {\n","filename":"src\/hotspot\/share\/oops\/resolvedFieldEntry.hpp","additions":11,"deletions":7,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -460,2 +460,3 @@\n-\/\/ create Runtime Predicates above it. They all share the same uncommon trap. The Parse Predicate will follow the\n-\/\/ Runtime Predicates. Together they form a Regular Predicate Block. There are three kinds of Parse Predicates:\n+\/\/ create Regular Predicates (Runtime Predicates with possible Assertion Predicates) above it. Together they form a\n+\/\/ Predicate Block. The Parse Predicate and Regular Predicates share the same uncommon trap.\n+\/\/ There are three kinds of Parse Predicates:\n@@ -476,0 +477,2 @@\n+  Node* uncommon_trap() const;\n+\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"opto\/predicates.hpp\"\n@@ -2011,0 +2012,7 @@\n+Node* ParsePredicateNode::uncommon_trap() const {\n+  ParsePredicateUncommonProj* uncommon_proj = proj_out(0)->as_IfFalse();\n+  Node* uct_region_or_call = uncommon_proj->unique_ctrl_out();\n+  assert(uct_region_or_call->is_Region() || uct_region_or_call->is_Call(), \"must be a region or call uct\");\n+  return uct_region_or_call;\n+}\n+\n@@ -2028,0 +2036,1 @@\n+\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n@@ -3022,0 +3023,7 @@\n+\n+#if INCLUDE_JVMTI\n+  if (too_many_traps(Deoptimization::Reason_intrinsic)) {\n+    return false;\n+  }\n+#endif \/\/INCLUDE_JVMTI\n+\n@@ -3032,0 +3040,18 @@\n+#if INCLUDE_JVMTI\n+    \/\/ Don't try to access new allocated obj in the intrinsic.\n+    \/\/ It causes perfomance issues even when jvmti event VmObjectAlloc is disabled.\n+    \/\/ Deoptimize and allocate in interpreter instead.\n+    Node* addr = makecon(TypeRawPtr::make((address) &JvmtiExport::_should_notify_object_alloc));\n+    Node* should_post_vm_object_alloc = make_load(this->control(), addr, TypeInt::INT, T_INT, MemNode::unordered);\n+    Node* chk = _gvn.transform(new CmpINode(should_post_vm_object_alloc, intcon(0)));\n+    Node* tst = _gvn.transform(new BoolNode(chk, BoolTest::eq));\n+    {\n+      BuildCutout unless(this, tst, PROB_MAX);\n+      uncommon_trap(Deoptimization::Reason_intrinsic,\n+                    Deoptimization::Action_make_not_entrant);\n+    }\n+    if (stopped()) {\n+      return true;\n+    }\n+#endif \/\/INCLUDE_JVMTI\n+\n@@ -3803,3 +3829,4 @@\n-Node* LibraryCallKit::scopedValueCache_helper() {\n-  ciKlass *objects_klass = ciObjArrayKlass::make(env()->Object_klass());\n-  const TypeOopPtr *etype = TypeOopPtr::make_from_klass(env()->Object_klass());\n+const Type* LibraryCallKit::scopedValueCache_type() {\n+  ciKlass* objects_klass = ciObjArrayKlass::make(env()->Object_klass());\n+  const TypeOopPtr* etype = TypeOopPtr::make_from_klass(env()->Object_klass());\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3807,0 +3834,2 @@\n+  \/\/ Because we create the scopedValue cache lazily we have to make the\n+  \/\/ type of the result BotPTR.\n@@ -3808,0 +3837,3 @@\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n+  return objects_type;\n+}\n@@ -3809,0 +3841,1 @@\n+Node* LibraryCallKit::scopedValueCache_helper() {\n@@ -3821,8 +3854,1 @@\n-  ciKlass *objects_klass = ciObjArrayKlass::make(env()->Object_klass());\n-  const TypeOopPtr *etype = TypeOopPtr::make_from_klass(env()->Object_klass());\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n-\n-  \/\/ Because we create the scopedValue cache lazily we have to make the\n-  \/\/ type of the result BotPTR.\n-  bool xk = etype->klass_is_exact();\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n+  const Type* objects_type = scopedValueCache_type();\n@@ -3839,0 +3865,1 @@\n+  const Type* objects_type = scopedValueCache_type();\n@@ -3841,1 +3868,1 @@\n-  access_store_at(nullptr, cache_obj_handle, adr_type, arr, _gvn.type(arr), T_OBJECT, IN_NATIVE | MO_UNORDERED);\n+  access_store_at(nullptr, cache_obj_handle, adr_type, arr, objects_type, T_OBJECT, IN_NATIVE | MO_UNORDERED);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":39,"deletions":12,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -262,0 +262,1 @@\n+  const Type* scopedValueCache_type();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"opto\/predicates.hpp\"\n@@ -147,10 +148,3 @@\n-  LoopNode *head = loop->_head->as_Loop();\n-  Node* entry = head->skip_strip_mined()->in(LoopNode::EntryControl);\n-  if (ParsePredicates::is_success_proj(entry)) {\n-    assert(entry->is_IfProj(), \"sanity - must be ifProj since there is at least one predicate\");\n-    if (entry->outcnt() > 1) {\n-      \/\/ Bailout if there are predicates from which there are additional control dependencies (i.e. from loop\n-      \/\/ entry 'entry') to previously partially peeled statements since this case is not handled and can lead\n-      \/\/ to a wrong execution. Remove this bailout, once this is fixed.\n-      return;\n-    }\n+  LoopNode* head = loop->_head->as_Loop();\n+  if (has_control_dependencies_from_predicates(head)) {\n+    return;\n@@ -158,0 +152,1 @@\n+\n@@ -182,1 +177,1 @@\n-  ProjNode* proj_false = invar_iff->proj_out(0);\n+  verify_fast_loop(head, proj_true);\n@@ -184,16 +179,0 @@\n-#ifdef ASSERT\n-  assert(proj_true->is_IfTrue(), \"must be true projection\");\n-  entry = head->skip_strip_mined()->in(LoopNode::EntryControl);\n-  ParsePredicates parse_predicates(entry);\n-  if (!parse_predicates.has_any()) {\n-    \/\/ No Parse Predicate.\n-    Node* uniqc = proj_true->unique_ctrl_out();\n-    assert((uniqc == head && !head->is_strip_mined()) || (uniqc == head->in(LoopNode::EntryControl)\n-           && head->is_strip_mined()), \"must hold by construction if no predicates\");\n-  } else {\n-    \/\/ There is at least one Parse Predicate. When skipping all predicates\/Regular Predicate Blocks, we should end up\n-    \/\/ at 'proj_true'.\n-    assert(proj_true == Predicates::skip_all_predicates(parse_predicates),\n-           \"must hold by construction if at least one Parse Predicate\");\n-  }\n-#endif\n@@ -273,0 +252,15 @@\n+bool PhaseIdealLoop::has_control_dependencies_from_predicates(LoopNode* head) const {\n+  Node* entry = head->skip_strip_mined()->in(LoopNode::EntryControl);\n+  Predicates predicates(entry);\n+  if (predicates.has_any()) {\n+    assert(entry->is_IfProj(), \"sanity - must be ifProj since there is at least one predicate\");\n+    if (entry->outcnt() > 1) {\n+      \/\/ Bailout if there are predicates from which there are additional control dependencies (i.e. from loop\n+      \/\/ entry 'entry') to previously partially peeled statements since this case is not handled and can lead\n+      \/\/ to a wrong execution. Remove this bailout, once this is fixed.\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -347,0 +341,18 @@\n+#ifdef ASSERT\n+void PhaseIdealLoop::verify_fast_loop(LoopNode* head, const ProjNode* proj_true) const {\n+  assert(proj_true->is_IfTrue(), \"must be true projection\");\n+  Node* entry = head->skip_strip_mined()->in(LoopNode::EntryControl);\n+  Predicates predicates(entry);\n+  if (!predicates.has_any()) {\n+    \/\/ No Parse Predicate.\n+    Node* uniqc = proj_true->unique_ctrl_out();\n+    assert((uniqc == head && !head->is_strip_mined()) || (uniqc == head->in(LoopNode::EntryControl)\n+                                                          && head->is_strip_mined()), \"must hold by construction if no predicates\");\n+  } else {\n+    \/\/ There is at least one Parse Predicate. When skipping all predicates\/predicate blocks, we should end up\n+    \/\/ at 'proj_true'.\n+    assert(proj_true == predicates.entry(), \"must hold by construction if at least one Parse Predicate\");\n+  }\n+}\n+#endif \/\/ ASSERT\n+\n","filename":"src\/hotspot\/share\/opto\/loopUnswitch.cpp","additions":39,"deletions":27,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-class ParsePredicates;\n+class PredicateBlock;\n@@ -328,5 +328,1 @@\n-  \/\/ If this is a main loop in a pre\/main\/post loop nest, walk over\n-  \/\/ the predicates that were inserted by\n-  \/\/ duplicate_predicates()\/add_range_check_predicate()\n-  static Node* skip_predicates_from_entry(Node* ctrl);\n-  Node* skip_predicates();\n+  Node* skip_assertion_predicates_with_halt();\n@@ -343,2 +339,0 @@\n-\n-  static bool is_zero_trip_guard_if(const IfNode* iff);\n@@ -941,5 +935,5 @@\n-  void copy_assertion_predicates_to_main_loop_helper(Node* predicate, Node* init, Node* stride, IdealLoopTree* outer_loop,\n-                                                     LoopNode* outer_main_head, uint dd_main_head,\n-                                                     uint idx_before_pre_post, uint idx_after_post_before_pre,\n-                                                     Node* zero_trip_guard_proj_main, Node* zero_trip_guard_proj_post,\n-                                                     const Node_List &old_new);\n+  void copy_assertion_predicates_to_main_loop_helper(const PredicateBlock* predicate_block, Node* init, Node* stride,\n+                                                     IdealLoopTree* outer_loop, LoopNode* outer_main_head,\n+                                                     uint dd_main_head, uint idx_before_pre_post,\n+                                                     uint idx_after_post_before_pre, Node* zero_trip_guard_proj_main,\n+                                                     Node* zero_trip_guard_proj_post, const Node_List &old_new);\n@@ -962,3 +956,3 @@\n-  void initialize_assertion_predicates_for_peeled_loop(IfProjNode* predicate_proj, LoopNode* outer_loop_head,\n-                                                       const int dd_outer_loop_head, Node* init, Node* stride,\n-                                                       IdealLoopTree* outer_loop, const uint idx_before_clone,\n+  void initialize_assertion_predicates_for_peeled_loop(const PredicateBlock* predicate_block, LoopNode* outer_loop_head,\n+                                                       int dd_outer_loop_head, Node* init, Node* stride,\n+                                                       IdealLoopTree* outer_loop, uint idx_before_clone,\n@@ -1342,2 +1336,1 @@\n-                                          int opcode, bool rewire_uncommon_proj_phi_inputs = false,\n-                                          bool if_cont_is_true_proj = true);\n+                                          int opcode, bool rewire_uncommon_proj_phi_inputs = false);\n@@ -1364,0 +1357,2 @@\n+\n+ private:\n@@ -1367,1 +1362,2 @@\n-  bool loop_predication_should_follow_branches(IdealLoopTree* loop, IfProjNode* predicate_proj, float& loop_trip_cnt);\n+  bool can_create_loop_predicates(const PredicateBlock* profiled_loop_predicate_block) const;\n+  bool loop_predication_should_follow_branches(IdealLoopTree* loop, float& loop_trip_cnt);\n@@ -1382,0 +1378,3 @@\n+  bool has_control_dependencies_from_predicates(LoopNode* head) const;\n+  void verify_fast_loop(LoopNode* head, const ProjNode* proj_true) const NOT_DEBUG_RETURN;\n+ public:\n@@ -1623,0 +1622,6 @@\n+  void clone_loop_predication_predicates_to_unswitched_loop(IdealLoopTree* loop, const Node_List& old_new,\n+                                                            const PredicateBlock* predicate_block,\n+                                                            Deoptimization::DeoptReason reason, IfProjNode*& iffast_pred,\n+                                                            IfProjNode*& ifslow_pred);\n+  void clone_parse_predicate_to_unswitched_loops(const PredicateBlock* predicate_block, Deoptimization::DeoptReason reason,\n+                                                 IfProjNode*& iffast_pred, IfProjNode*& ifslow_pred);\n@@ -1903,59 +1908,0 @@\n-\/\/ Utility class to work on predicates.\n-class Predicates {\n- public:\n-  static Node* skip_all_predicates(Node* node);\n-  static Node* skip_all_predicates(ParsePredicates& parse_predicates);\n-  static Node* skip_predicates_in_block(ParsePredicateSuccessProj* parse_predicate_success_proj);\n-  static IfProjNode* next_predicate_proj_in_block(IfProjNode* proj);\n-  static bool has_profiled_loop_predicates(ParsePredicates& parse_predicates);\n-};\n-\n-\/\/ Class representing the Parse Predicates that are added during parsing with ParsePredicateNodes.\n-class ParsePredicates {\n- private:\n-  ParsePredicateSuccessProj* _loop_predicate_proj = nullptr;\n-  ParsePredicateSuccessProj* _profiled_loop_predicate_proj = nullptr;\n-  ParsePredicateSuccessProj* _loop_limit_check_predicate_proj = nullptr;\n-  \/\/ The success projection of the Parse Predicate that comes first when starting from root.\n-  ParsePredicateSuccessProj* _top_predicate_proj;\n-  ParsePredicateSuccessProj* _starting_proj;\n-\n-  void find_parse_predicate_projections();\n-  static bool is_uct_proj(Node* node, Deoptimization::DeoptReason deopt_reason);\n-  static ParsePredicateNode* get_parse_predicate_or_null(Node* proj);\n-  bool assign_predicate_proj(ParsePredicateSuccessProj* parse_predicate_proj);\n- public:\n-  ParsePredicates(Node* starting_proj);\n-\n-  \/\/ Success projection of Loop Parse Predicate.\n-  ParsePredicateSuccessProj* loop_predicate_proj() {\n-    return _loop_predicate_proj;\n-  }\n-\n-  \/\/ Success proj of Profiled Loop Parse Predicate.\n-  ParsePredicateSuccessProj* profiled_loop_predicate_proj() {\n-    return _profiled_loop_predicate_proj;\n-  }\n-\n-  \/\/ Success proj of Loop Limit Check Parse Predicate.\n-  ParsePredicateSuccessProj* loop_limit_check_predicate_proj() {\n-    return _loop_limit_check_predicate_proj;\n-  }\n-\n-  \/\/ Return the success projection of the Parse Predicate that comes first when starting from root.\n-  ParsePredicateSuccessProj* get_top_predicate_proj() {\n-    return _top_predicate_proj;\n-  }\n-\n-  static bool is_success_proj(Node* node);\n-\n-  \/\/ Are there any Parse Predicates?\n-  bool has_any() const {\n-    return _top_predicate_proj != nullptr;\n-  }\n-\n-  static bool is_loop_limit_check_predicate_proj(Node* node) {\n-    ParsePredicateNode* parse_predicate = get_parse_predicate_or_null(node);\n-    return parse_predicate != nullptr && parse_predicate->deopt_reason() == Deoptimization::DeoptReason::Reason_loop_limit_check;\n-  }\n-};\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":24,"deletions":78,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -193,1 +193,1 @@\n-    intptr_t offset = raw_instance_offset(id);\n+    int offset = raw_instance_offset(id);\n@@ -211,1 +211,1 @@\n-intptr_t jfieldIDWorkaround::encode_klass_hash(Klass* k, intptr_t offset) {\n+intptr_t jfieldIDWorkaround::encode_klass_hash(Klass* k, int offset) {\n@@ -254,1 +254,1 @@\n-  intptr_t offset = raw_instance_offset(id);\n+  int offset = raw_instance_offset(id);\n@@ -418,1 +418,1 @@\n-    intptr_t offset = InstanceKlass::cast(k1)->field_offset( slot );\n+    int offset = InstanceKlass::cast(k1)->field_offset( slot );\n@@ -430,1 +430,1 @@\n-  intptr_t offset = InstanceKlass::cast(k1)->field_offset( slot );\n+  int offset = InstanceKlass::cast(k1)->field_offset( slot );\n@@ -1968,0 +1968,1 @@\n+\/\/ TODO: make this a template\n@@ -1985,1 +1986,0 @@\n-  if (SigType == JVM_SIGNATURE_BOOLEAN) { value = ((jboolean)value) & 1; } \\\n@@ -2178,1 +2178,0 @@\n-  if (SigType == JVM_SIGNATURE_BOOLEAN) { value = ((jboolean)value) & 1; } \\\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":6,"deletions":7,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -611,1 +611,1 @@\n-  \/\/ as implemented in the classic virtual machine; return 0 if object is nullptr\n+  \/\/ as implemented in the classic virtual machine; return 0 if object is null\n@@ -632,1 +632,1 @@\n-    return ObjectSynchronizer::FastHashCode(THREAD, obj);\n+    return checked_cast<jint>(ObjectSynchronizer::FastHashCode(THREAD, obj));\n@@ -1633,1 +1633,1 @@\n-  intptr_t offset = ik->field_offset(slot);\n+  int offset = ik->field_offset(slot);\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1069,0 +1069,3 @@\n+\/\/ This flag is read by C2 during VM internal objects allocation\n+int JvmtiExport::_should_notify_object_alloc = 0;\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiExport.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -698,1 +698,1 @@\n-    DEBUG_ONLY(int vmindex = java_lang_invoke_MemberName::vmindex(mname()));\n+    DEBUG_ONLY(intptr_t vmindex = java_lang_invoke_MemberName::vmindex(mname()));\n@@ -712,1 +712,1 @@\n-  DEBUG_ONLY(int old_vmindex);\n+  DEBUG_ONLY(intptr_t old_vmindex);\n@@ -914,1 +914,1 @@\n-      int vmindex  = java_lang_invoke_MemberName::vmindex(mname());\n+      intptr_t vmindex  = java_lang_invoke_MemberName::vmindex(mname());\n@@ -917,1 +917,1 @@\n-      if (!defc->find_field_from_offset(vmindex, is_static, &fd))\n+      if (!defc->find_field_from_offset(checked_cast<int>(vmindex), is_static, &fd))\n@@ -1169,1 +1169,1 @@\n-      int vmindex = java_lang_invoke_MemberName::vmindex(mname);\n+      intptr_t vmindex = java_lang_invoke_MemberName::vmindex(mname);\n","filename":"src\/hotspot\/share\/prims\/methodHandles.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -117,1 +117,1 @@\n-static inline jlong field_offset_from_byte_offset(jlong byte_offset) {\n+static inline int field_offset_from_byte_offset(int byte_offset) {\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1388,1 +1388,1 @@\n-  int result = value;\n+  int result = checked_cast<int>(value);\n@@ -1393,1 +1393,1 @@\n-  uint result = value;\n+  uint result = checked_cast<uint>(value);\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"runtime\/safefetch.hpp\"\n@@ -308,0 +309,8 @@\n+Method* frame::safe_interpreter_frame_method() const {\n+  Method** m_addr = interpreter_frame_method_addr();\n+  if (m_addr == nullptr) {\n+    return nullptr;\n+  }\n+  return (Method*) SafeFetchN((intptr_t*) m_addr, 0);\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -239,0 +239,2 @@\n+  Method* safe_interpreter_frame_method() const;\n+\n","filename":"src\/hotspot\/share\/runtime\/frame.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -823,1 +823,1 @@\n-  product(intx, ContendedPaddingWidth, 128,                                 \\\n+  product(int, ContendedPaddingWidth, 128,                                  \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -93,1 +93,1 @@\n-  static intptr_t raw_instance_offset(jfieldID id) {\n+  static int raw_instance_offset(jfieldID id) {\n@@ -98,1 +98,3 @@\n-    return result;\n+    \/\/ This gets back the InstanceKlass field offset that\n+    \/\/ the jfieldID is created with.\n+    return checked_cast<int>(result);\n@@ -100,1 +102,1 @@\n-  static intptr_t encode_klass_hash(Klass* k, intptr_t offset);\n+  static intptr_t encode_klass_hash(Klass* k, int offset);\n@@ -142,1 +144,1 @@\n-  static intptr_t from_instance_jfieldID(Klass* k, jfieldID id) {\n+  static int from_instance_jfieldID(Klass* k, jfieldID id) {\n","filename":"src\/hotspot\/share\/runtime\/jfieldIDWorkaround.hpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -104,1 +104,1 @@\n-    event.set_iterations(iterations);\n+    event.set_iterations(checked_cast<u4>(iterations));\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+  template(HeapDumpMerge)                         \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -115,0 +115,1 @@\n+#include \"services\/attachListener.hpp\"\n@@ -242,0 +243,1 @@\n+  nonstatic_field(InstanceKlass,               _nest_members,                                 Array<jushort>*)                       \\\n@@ -249,0 +251,1 @@\n+  nonstatic_field(InstanceKlass,               _nest_host_index,                              u2)                                    \\\n@@ -1267,0 +1270,1 @@\n+        declare_type(AttachListenerThread, JavaThread)                    \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -472,1 +472,4 @@\n-           \"BOOLEAN\", false, \"false\") {\n+           \"BOOLEAN\", false, \"false\"),\n+  _parallel(\"-parallel\", \"Number of parallel threads to use for heap dump. The VM \"\n+                          \"will try to use the specified number of threads, but might use fewer.\",\n+            \"INT\", false, \"1\") {\n@@ -477,0 +480,1 @@\n+  _dcmdparser.add_dcmd_option(&_parallel);\n@@ -481,0 +485,1 @@\n+  jlong parallel = HeapDumper::default_num_of_dump_threads();\n@@ -491,0 +496,12 @@\n+  if (_parallel.is_set()) {\n+    parallel = _parallel.value();\n+\n+    if (parallel < 0) {\n+      output()->print_cr(\"Invalid number of parallel dump threads.\");\n+      return;\n+    } else if (parallel == 0) {\n+      \/\/ 0 implies to disable parallel heap dump, in such case, we use serial dump instead\n+      parallel = 1;\n+    }\n+  }\n+\n@@ -495,1 +512,1 @@\n-  dumper.dump(_filename.value(), output(), (int) level, _overwrite.value());\n+  dumper.dump(_filename.value(), output(), (int) level, _overwrite.value(), (uint)parallel);\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":19,"deletions":2,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -323,0 +323,1 @@\n+  DCmdArgument<jlong> _parallel;\n@@ -324,1 +325,1 @@\n-  static int num_arguments() { return 4; }\n+  static int num_arguments() { return 5; }\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright (c) 2023, Alibaba Group Holding Limited. All rights reserved.\n@@ -60,0 +61,1 @@\n+#include \"runtime\/timerTrace.hpp\"\n@@ -482,1 +484,1 @@\n-class AbstractDumpWriter : public StackObj {\n+class AbstractDumpWriter : public ResourceObj {\n@@ -486,1 +488,0 @@\n-    io_buffer_max_waste = 10*K,\n@@ -499,2 +500,0 @@\n-  virtual void flush(bool force = false) = 0;\n-\n@@ -520,1 +519,1 @@\n-  \/\/ total number of bytes written to the disk\n+  \/\/ Total number of bytes written to the disk\n@@ -522,0 +521,1 @@\n+  \/\/ Return non-null if error occurred\n@@ -542,12 +542,3 @@\n-  void finish_dump_segment(bool force_flush = false);\n-  \/\/ Refresh to get new buffer\n-  void refresh() {\n-    assert (_in_dump_segment ==false, \"Sanity check\");\n-    _buffer = nullptr;\n-    _size = io_buffer_max_size;\n-    _pos = 0;\n-    \/\/ Force flush to guarantee data from parallel dumper are written.\n-    flush(true);\n-  }\n-  \/\/ Called when finished to release the threads.\n-  virtual void deactivate() = 0;\n+  void finish_dump_segment();\n+  \/\/ Flush internal buffer to persistent storage\n+  virtual void flush() = 0;\n@@ -648,1 +639,1 @@\n-void AbstractDumpWriter::finish_dump_segment(bool force_flush) {\n+void AbstractDumpWriter::finish_dump_segment() {\n@@ -666,1 +657,1 @@\n-    flush(force_flush);\n+    flush();\n@@ -711,13 +702,11 @@\n- private:\n-  CompressionBackend _backend; \/\/ Does the actual writing.\n- protected:\n-  void flush(bool force = false) override;\n-\n- public:\n-  \/\/ Takes ownership of the writer and compressor.\n-  DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor);\n-\n-  \/\/ total number of bytes written to the disk\n-  julong bytes_written() const override { return (julong) _backend.get_written(); }\n-\n-  char const* error() const override    { return _backend.error(); }\n+private:\n+  FileWriter* _writer;\n+  AbstractCompressor* _compressor;\n+  size_t _bytes_written;\n+  char* _error;\n+  \/\/ Compression support\n+  char* _out_buffer;\n+  size_t _out_size;\n+  size_t _out_pos;\n+  char* _tmp_buffer;\n+  size_t _tmp_size;\n@@ -725,6 +714,2 @@\n-  \/\/ Called by threads used for parallel writing.\n-  void writer_loop()                    { _backend.thread_loop(); }\n-  \/\/ Called when finish to release the threads.\n-  void deactivate() override            { flush(); _backend.deactivate(); }\n-  \/\/ Get the backend pointer, used by parallel dump writer.\n-  CompressionBackend* backend_ptr()     { return &_backend; }\n+private:\n+  void do_compress();\n@@ -732,0 +717,14 @@\n+public:\n+  DumpWriter(const char* path, bool overwrite, AbstractCompressor* compressor);\n+  ~DumpWriter();\n+  julong bytes_written() const override        { return (julong) _bytes_written; }\n+  void set_bytes_written(julong bytes_written) { _bytes_written = bytes_written; }\n+  char const* error() const override           { return _error; }\n+  void set_error(const char* error)            { _error = (char*)error; }\n+  bool has_error() const                       { return _error != nullptr; }\n+  const char* get_file_path() const            { return _writer->get_file_path(); }\n+  AbstractCompressor* compressor()             { return _compressor; }\n+  void set_compressor(AbstractCompressor* p)   { _compressor = p; }\n+  bool is_overwrite() const                    { return _writer->is_overwrite(); }\n+\n+  void flush() override;\n@@ -734,2 +733,1 @@\n-\/\/ Check for error after constructing the object and destroy it in case of an error.\n-DumpWriter::DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor) :\n+DumpWriter::DumpWriter(const char* path, bool overwrite, AbstractCompressor* compressor) :\n@@ -737,32 +735,22 @@\n-  _backend(writer, compressor, io_buffer_max_size, io_buffer_max_waste) {\n-  flush();\n-}\n-\n-\/\/ flush any buffered bytes to the file\n-void DumpWriter::flush(bool force) {\n-  _backend.get_new_buffer(&_buffer, &_pos, &_size, force);\n-}\n-\n-\/\/ Buffer queue used for parallel dump.\n-struct ParWriterBufferQueueElem {\n-  char* _buffer;\n-  size_t _used;\n-  ParWriterBufferQueueElem* _next;\n-};\n-\n-class ParWriterBufferQueue : public CHeapObj<mtInternal> {\n- private:\n-  ParWriterBufferQueueElem* _head;\n-  ParWriterBufferQueueElem* _tail;\n-  uint _length;\n- public:\n-  ParWriterBufferQueue() : _head(nullptr), _tail(nullptr), _length(0) { }\n-\n-  void enqueue(ParWriterBufferQueueElem* entry) {\n-    if (_head == nullptr) {\n-      assert(is_empty() && _tail == nullptr, \"Sanity check\");\n-      _head = _tail = entry;\n-    } else {\n-      assert ((_tail->_next == nullptr && _tail->_buffer != nullptr), \"Buffer queue is polluted\");\n-      _tail->_next = entry;\n-      _tail = entry;\n+  _writer(new (std::nothrow) FileWriter(path, overwrite)),\n+  _compressor(compressor),\n+  _bytes_written(0),\n+  _error(nullptr),\n+  _out_buffer(nullptr),\n+  _out_size(0),\n+  _out_pos(0),\n+  _tmp_buffer(nullptr),\n+  _tmp_size(0) {\n+  _error = (char*)_writer->open_writer();\n+  if (_error == nullptr) {\n+    _buffer = (char*)os::malloc(io_buffer_max_size, mtInternal);\n+    if (compressor != nullptr) {\n+      _error = (char*)_compressor->init(io_buffer_max_size, &_out_size, &_tmp_size);\n+      if (_error == nullptr) {\n+        if (_out_size > 0) {\n+          _out_buffer = (char*)os::malloc(_out_size, mtInternal);\n+        }\n+        if (_tmp_size > 0) {\n+          _tmp_buffer = (char*)os::malloc(_tmp_size, mtInternal);\n+        }\n+      }\n@@ -770,2 +758,4 @@\n-    _length++;\n-    assert(_tail->_next == nullptr, \"Buffer queue is polluted\");\n+  \/\/ initialize internal buffer\n+  _pos = 0;\n+  _size = io_buffer_max_size;\n+}\n@@ -774,11 +764,3 @@\n-  ParWriterBufferQueueElem* dequeue() {\n-    if (_head == nullptr)  return nullptr;\n-    ParWriterBufferQueueElem* entry = _head;\n-    assert (entry->_buffer != nullptr, \"polluted buffer in writer list\");\n-    _head = entry->_next;\n-    if (_head == nullptr) {\n-      _tail = nullptr;\n-    }\n-    entry->_next = nullptr;\n-    _length--;\n-    return entry;\n+DumpWriter::~DumpWriter(){\n+  if (_buffer != nullptr) {\n+    os::free(_buffer);\n@@ -786,3 +768,2 @@\n-\n-  bool is_empty() {\n-    return _length == 0;\n+  if (_out_buffer != nullptr) {\n+    os::free(_out_buffer);\n@@ -790,32 +771,2 @@\n-\n-  uint length() { return _length; }\n-};\n-\n-\/\/ Support parallel heap dump.\n-class ParDumpWriter : public AbstractDumpWriter {\n- private:\n-  \/\/ Lock used to guarantee the integrity of multiple buffers writing.\n-  static Monitor* _lock;\n-  \/\/ Pointer of backend from global DumpWriter.\n-  CompressionBackend* _backend_ptr;\n-  char const * _err;\n-  ParWriterBufferQueue* _buffer_queue;\n-  size_t _internal_buffer_used;\n-  char* _buffer_base;\n-  bool _split_data;\n-  static const uint BackendFlushThreshold = 2;\n- protected:\n-  void flush(bool force = false) override {\n-    assert(_pos != 0, \"must not be zero\");\n-    if (_pos != 0) {\n-      refresh_buffer();\n-    }\n-\n-    if (_split_data || _is_huge_sub_record) {\n-      return;\n-    }\n-\n-    if (should_flush_buf_list(force)) {\n-      assert(!_in_dump_segment && !_split_data && !_is_huge_sub_record, \"incomplete data send to backend!\\n\");\n-      flush_to_backend(force);\n-    }\n+  if (_tmp_buffer != nullptr) {\n+    os::free(_tmp_buffer);\n@@ -823,60 +774,2 @@\n-\n- public:\n-  \/\/ Check for error after constructing the object and destroy it in case of an error.\n-  ParDumpWriter(DumpWriter* dw) :\n-    AbstractDumpWriter(),\n-    _backend_ptr(dw->backend_ptr()),\n-    _buffer_queue((new (std::nothrow) ParWriterBufferQueue())),\n-    _buffer_base(nullptr),\n-    _split_data(false) {\n-    \/\/ prepare internal buffer\n-    allocate_internal_buffer();\n-  }\n-\n-  ~ParDumpWriter() {\n-     assert(_buffer_queue != nullptr, \"Sanity check\");\n-     assert((_internal_buffer_used == 0) && (_buffer_queue->is_empty()),\n-            \"All data must be send to backend\");\n-     if (_buffer_base != nullptr) {\n-       os::free(_buffer_base);\n-       _buffer_base = nullptr;\n-     }\n-     delete _buffer_queue;\n-     _buffer_queue = nullptr;\n-  }\n-\n-  \/\/ total number of bytes written to the disk\n-  julong bytes_written() const override { return (julong) _backend_ptr->get_written(); }\n-  char const* error() const override    { return _err == nullptr ? _backend_ptr->error() : _err; }\n-\n-  static void before_work() {\n-    assert(_lock == nullptr, \"ParDumpWriter lock must be initialized only once\");\n-    _lock = new (std::nothrow) PaddedMonitor(Mutex::safepoint, \"ParallelHProfWriter_lock\");\n-  }\n-\n-  static void after_work() {\n-    assert(_lock != nullptr, \"ParDumpWriter lock is not initialized\");\n-    delete _lock;\n-    _lock = nullptr;\n-  }\n-\n-  \/\/ write raw bytes\n-  void write_raw(const void* s, size_t len) override {\n-    assert(!_in_dump_segment || (_sub_record_left >= len), \"sub-record too large\");\n-    debug_only(_sub_record_left -= len);\n-    assert(!_split_data, \"Invalid split data\");\n-    _split_data = true;\n-    \/\/ flush buffer to make room.\n-    while (len > buffer_size() - position()) {\n-      assert(!_in_dump_segment || _is_huge_sub_record,\n-             \"Cannot overflow in non-huge sub-record.\");\n-      size_t to_write = buffer_size() - position();\n-      memcpy(buffer() + position(), s, to_write);\n-      s = (void*) ((char*) s + to_write);\n-      len -= to_write;\n-      set_position(position() + to_write);\n-      flush();\n-    }\n-    _split_data = false;\n-    memcpy(buffer() + position(), s, len);\n-    set_position(position() + len);\n+  if (_writer != NULL) {\n+    delete _writer;\n@@ -884,0 +777,2 @@\n+  _bytes_written = -1;\n+}\n@@ -885,14 +780,4 @@\n-  void deactivate() override { flush(true); _backend_ptr->deactivate(); }\n-\n- private:\n-  void allocate_internal_buffer() {\n-    assert(_buffer_queue != nullptr, \"Internal buffer queue is not ready when allocate internal buffer\");\n-    assert(_buffer == nullptr && _buffer_base == nullptr, \"current buffer must be null before allocate\");\n-    _buffer_base = _buffer = (char*)os::malloc(io_buffer_max_size, mtInternal);\n-    if (_buffer == nullptr) {\n-      set_error(\"Could not allocate buffer for writer\");\n-      return;\n-    }\n-    _pos = 0;\n-    _internal_buffer_used = 0;\n-    _size = io_buffer_max_size;\n+\/\/ flush any buffered bytes to the file\n+void DumpWriter::flush() {\n+  if (_pos <= 0) {\n+    return;\n@@ -900,5 +785,3 @@\n-\n-  void set_error(char const* new_error) {\n-    if ((new_error != nullptr) && (_err == nullptr)) {\n-      _err = new_error;\n-    }\n+  if (has_error()) {\n+    _pos = 0;\n+    return;\n@@ -906,23 +789,9 @@\n-\n-  \/\/ Add buffer to internal list\n-  void refresh_buffer() {\n-    size_t expected_total = _internal_buffer_used + _pos;\n-    if (expected_total < io_buffer_max_size - io_buffer_max_waste) {\n-      \/\/ reuse current buffer.\n-      _internal_buffer_used = expected_total;\n-      assert(_size - _pos == io_buffer_max_size - expected_total, \"illegal resize of buffer\");\n-      _size -= _pos;\n-      _buffer += _pos;\n-      _pos = 0;\n-\n-      return;\n-    }\n-    \/\/ It is not possible here that expected_total is larger than io_buffer_max_size because\n-    \/\/ of limitation in write_xxx().\n-    assert(expected_total <= io_buffer_max_size, \"buffer overflow\");\n-    assert(_buffer - _buffer_base <= io_buffer_max_size, \"internal buffer overflow\");\n-    ParWriterBufferQueueElem* entry =\n-        (ParWriterBufferQueueElem*)os::malloc(sizeof(ParWriterBufferQueueElem), mtInternal);\n-    if (entry == nullptr) {\n-      set_error(\"Heap dumper can allocate memory\");\n-      return;\n+  char* result = nullptr;\n+  if (_compressor == nullptr) {\n+    result = (char*)_writer->write_buf(_buffer, _pos);\n+    _bytes_written += _pos;\n+  } else {\n+    do_compress();\n+    if (!has_error()) {\n+      result = (char*)_writer->write_buf(_out_buffer, _out_pos);\n+      _bytes_written += _out_pos;\n@@ -930,14 +799,1 @@\n-    entry->_buffer = _buffer_base;\n-    entry->_used = expected_total;\n-    entry->_next = nullptr;\n-    \/\/ add to internal buffer queue\n-    _buffer_queue->enqueue(entry);\n-    _buffer_base =_buffer = nullptr;\n-    allocate_internal_buffer();\n-  }\n-\n-  void reclaim_entry(ParWriterBufferQueueElem* entry) {\n-    assert(entry != nullptr && entry->_buffer != nullptr, \"Invalid entry to reclaim\");\n-    os::free(entry->_buffer);\n-    entry->_buffer = nullptr;\n-    os::free(entry);\n+  _pos = 0; \/\/ reset pos to make internal buffer available\n@@ -946,5 +802,2 @@\n-  void flush_buffer(char* buffer, size_t used) {\n-    assert(_lock->owner() == Thread::current(), \"flush buffer must hold lock\");\n-    size_t max = io_buffer_max_size;\n-    \/\/ get_new_buffer\n-    _backend_ptr->flush_external_buffer(buffer, used, max);\n+  if (result != nullptr) {\n+    set_error(result);\n@@ -952,0 +805,1 @@\n+}\n@@ -953,3 +807,3 @@\n-  bool should_flush_buf_list(bool force) {\n-    return force || _buffer_queue->length() > BackendFlushThreshold;\n-  }\n+void DumpWriter::do_compress() {\n+  const char* msg = _compressor->compress(_buffer, _pos, _out_buffer, _out_size,\n+                                          _tmp_buffer, _tmp_size, &_out_pos);\n@@ -957,21 +811,2 @@\n-  void flush_to_backend(bool force) {\n-    \/\/ Guarantee there is only one writer updating the backend buffers.\n-    MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-    while (!_buffer_queue->is_empty()) {\n-      ParWriterBufferQueueElem* entry = _buffer_queue->dequeue();\n-      flush_buffer(entry->_buffer, entry->_used);\n-      \/\/ Delete buffer and entry.\n-      reclaim_entry(entry);\n-      entry = nullptr;\n-    }\n-    assert(_pos == 0, \"available buffer must be empty before flush\");\n-    \/\/ Flush internal buffer.\n-    if (_internal_buffer_used > 0) {\n-      flush_buffer(_buffer_base, _internal_buffer_used);\n-      os::free(_buffer_base);\n-      _pos = 0;\n-      _internal_buffer_used = 0;\n-      _buffer_base = _buffer = nullptr;\n-      \/\/ Allocate internal buffer for future use.\n-      allocate_internal_buffer();\n-    }\n+  if (msg != nullptr) {\n+    set_error(msg);\n@@ -979,3 +814,1 @@\n-};\n-\n-Monitor* ParDumpWriter::_lock = nullptr;\n+}\n@@ -2101,67 +1934,0 @@\n-\/\/ Large object heap dump support.\n-\/\/ To avoid memory consumption, when dumping large objects such as huge array and\n-\/\/ large objects whose size are larger than LARGE_OBJECT_DUMP_THRESHOLD, the scanned\n-\/\/ partial object\/array data will be sent to the backend directly instead of caching\n-\/\/ the whole object\/array in the internal buffer.\n-\/\/ The HeapDumpLargeObjectList is used to save the large object when dumper scans\n-\/\/ the heap. The large objects could be added (push) parallelly by multiple dumpers,\n-\/\/ But they will be removed (popped) serially only by the VM thread.\n-class HeapDumpLargeObjectList : public CHeapObj<mtInternal> {\n- private:\n-  class HeapDumpLargeObjectListElem : public CHeapObj<mtInternal> {\n-   public:\n-    HeapDumpLargeObjectListElem(oop obj) : _obj(obj), _next(nullptr) { }\n-    oop _obj;\n-    HeapDumpLargeObjectListElem* _next;\n-  };\n-\n-  volatile HeapDumpLargeObjectListElem* _head;\n-\n- public:\n-  HeapDumpLargeObjectList() : _head(nullptr) { }\n-\n-  void atomic_push(oop obj) {\n-    assert (obj != nullptr, \"sanity check\");\n-    HeapDumpLargeObjectListElem* entry = new HeapDumpLargeObjectListElem(obj);\n-    if (entry == nullptr) {\n-      warning(\"failed to allocate element for large object list\");\n-      return;\n-    }\n-    assert (entry->_obj != nullptr, \"sanity check\");\n-    while (true) {\n-      volatile HeapDumpLargeObjectListElem* old_head = Atomic::load_acquire(&_head);\n-      HeapDumpLargeObjectListElem* new_head = entry;\n-      if (Atomic::cmpxchg(&_head, old_head, new_head) == old_head) {\n-        \/\/ successfully push\n-        new_head->_next = (HeapDumpLargeObjectListElem*)old_head;\n-        return;\n-      }\n-    }\n-  }\n-\n-  oop pop() {\n-    if (_head == nullptr) {\n-      return nullptr;\n-    }\n-    HeapDumpLargeObjectListElem* entry = (HeapDumpLargeObjectListElem*)_head;\n-    _head = _head->_next;\n-    assert (entry != nullptr, \"illegal larger object list entry\");\n-    oop ret = entry->_obj;\n-    delete entry;\n-    assert (ret != nullptr, \"illegal oop pointer\");\n-    return ret;\n-  }\n-\n-  void drain(ObjectClosure* cl) {\n-    while (_head !=  nullptr) {\n-      cl->do_object(pop());\n-    }\n-  }\n-\n-  bool is_empty() {\n-    return _head == nullptr;\n-  }\n-\n-  static const size_t LargeObjectSizeThreshold = 1 << 20; \/\/ 1 MB\n-};\n-\n@@ -2174,3 +1940,1 @@\n-  HeapDumpLargeObjectList* _list;\n-\n-  bool is_large(oop o);\n+\n@@ -2179,1 +1943,1 @@\n-  HeapObjectDumper(AbstractDumpWriter* writer, HeapDumpLargeObjectList* list = nullptr) {\n+  HeapObjectDumper(AbstractDumpWriter* writer) {\n@@ -2181,1 +1945,0 @@\n-    _list = list;\n@@ -2201,7 +1964,0 @@\n-  \/\/ If large object list exists and it is large object\/array,\n-  \/\/ add oop into the list and skip scan. VM thread will process it later.\n-  if (_list != nullptr && is_large(o)) {\n-    _list->atomic_push(o);\n-    return;\n-  }\n-\n@@ -2222,31 +1978,0 @@\n-bool HeapObjectDumper::is_large(oop o) {\n-  size_t size = 0;\n-  if (o->is_instance()) {\n-    \/\/ Use o->size() * 8 as the upper limit of instance size to avoid iterating static fields\n-    size = o->size() * 8;\n-  } else if (o->is_objArray()) {\n-    objArrayOop array = objArrayOop(o);\n-    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n-    assert(type >= T_BOOLEAN && type <= T_OBJECT, \"invalid array element type\");\n-    int length = array->length();\n-    int type_size = sizeof(address);\n-    size = (size_t)length * type_size;\n-  } else if (o->is_typeArray()) {\n-    flatArrayOop array = flatArrayOop(o);\n-    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n-    assert(type == T_PRIMITIVE_OBJECT, \"invalid array element type\");\n-    int length = array->length();\n-    \/\/TODO: FIXME\n-    \/\/int type_size = type2aelembytes(type);\n-    \/\/size = (size_t)length * type_size;\n-  } else if (o->is_typeArray()) {\n-    typeArrayOop array = typeArrayOop(o);\n-    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n-    assert(type >= T_BOOLEAN && type <= T_OBJECT, \"invalid array element type\");\n-    int length = array->length();\n-    int type_size = type2aelembytes(type);\n-    size = (size_t)length * type_size;\n-  }\n-  return size > HeapDumpLargeObjectList::LargeObjectSizeThreshold;\n-}\n-\n@@ -2256,2 +1981,1 @@\n-   bool     _started;\n-   uint   _dumper_number;\n+   const uint   _dumper_number;\n@@ -2263,1 +1987,0 @@\n-     _started(false),\n@@ -2270,17 +1993,1 @@\n-   void wait_for_start_signal() {\n-     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-     while (_started == false) {\n-       ml.wait();\n-     }\n-     assert(_started == true,  \"dumper woke up with wrong state\");\n-   }\n-\n-   void start_dump() {\n-     assert (_started == false, \"start dump with wrong state\");\n-     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-     _started = true;\n-     ml.notify_all();\n-   }\n-\n-   void dumper_complete() {\n-     assert (_started == true, \"dumper complete with wrong state\");\n+   void dumper_complete(DumpWriter* local_writer, DumpWriter* global_writer) {\n@@ -2289,0 +1996,4 @@\n+     \/\/ propagate local error to global if any\n+     if (local_writer->has_error()) {\n+       global_writer->set_error(local_writer->error());\n+     }\n@@ -2293,1 +2004,0 @@\n-     assert (_started == true, \"wrong state when wait for dumper complete\");\n@@ -2298,1 +2008,0 @@\n-     _started = false;\n@@ -2302,0 +2011,103 @@\n+\/\/ DumpMerger merges separate dump files into a complete one\n+class DumpMerger : public StackObj {\n+private:\n+  DumpWriter* _writer;\n+  InlinedObjects*  _inlined_objects;\n+  const char* _path;\n+  bool _has_error;\n+  int _dump_seq;\n+\n+private:\n+  void merge_file(char* path);\n+  void merge_done();\n+\n+public:\n+  DumpMerger(const char* path, DumpWriter* writer, InlinedObjects* inlined_objects, int dump_seq) :\n+    _writer(writer),\n+    _inlined_objects(inlined_objects),\n+    _path(path),\n+    _has_error(_writer->has_error()),\n+    _dump_seq(dump_seq) {}\n+\n+  void do_merge();\n+};\n+\n+void DumpMerger::merge_done() {\n+  \/\/ Writes the HPROF_HEAP_DUMP_END record.\n+  if (!_has_error) {\n+    DumperSupport::end_of_dump(_writer);\n+    _inlined_objects->dump_flat_arrays(_writer);\n+    _writer->flush();\n+    _inlined_objects->release();\n+  }\n+  _dump_seq = 0; \/\/reset\n+}\n+\n+void DumpMerger::merge_file(char* path) {\n+  assert(!SafepointSynchronize::is_at_safepoint(), \"merging happens outside safepoint\");\n+  TraceTime timer(\"Merge segmented heap file\", TRACETIME_LOG(Info, heapdump));\n+\n+  fileStream segment_fs(path, \"rb\");\n+  if (!segment_fs.is_open()) {\n+    log_error(heapdump)(\"Can not open segmented heap file %s during merging\", path);\n+    _writer->set_error(\"Can not open segmented heap file during merging\");\n+    _has_error = true;\n+    return;\n+  }\n+\n+  jlong total = 0;\n+  size_t cnt = 0;\n+  char read_buf[4096];\n+  while ((cnt = segment_fs.read(read_buf, 1, 4096)) != 0) {\n+    _writer->write_raw(read_buf, cnt);\n+    total += cnt;\n+  }\n+\n+  _writer->flush();\n+  if (segment_fs.fileSize() != total) {\n+    log_error(heapdump)(\"Merged heap dump %s is incomplete, expect %ld but read \" JLONG_FORMAT \" bytes\",\n+                        path, segment_fs.fileSize(), total);\n+    _writer->set_error(\"Merged heap dump is incomplete\");\n+    _has_error = true;\n+  }\n+}\n+\n+void DumpMerger::do_merge() {\n+  assert(!SafepointSynchronize::is_at_safepoint(), \"merging happens outside safepoint\");\n+  TraceTime timer(\"Merge heap files complete\", TRACETIME_LOG(Info, heapdump));\n+\n+  \/\/ Since contents in segmented heap file were already zipped, we don't need to zip\n+  \/\/ them again during merging.\n+  AbstractCompressor* saved_compressor = _writer->compressor();\n+  _writer->set_compressor(nullptr);\n+\n+  \/\/ merge segmented heap file and remove it anyway\n+  char path[JVM_MAXPATHLEN];\n+  for (int i = 0; i < _dump_seq; i++) {\n+    memset(path, 0, JVM_MAXPATHLEN);\n+    os::snprintf(path, JVM_MAXPATHLEN, \"%s.p%d\", _path, i);\n+    if (!_has_error) {\n+      merge_file(path);\n+    }\n+    remove(path);\n+  }\n+\n+  \/\/ restore compressor for further use\n+  _writer->set_compressor(saved_compressor);\n+  merge_done();\n+}\n+\n+\/\/ The VM operation wraps DumpMerger so that it could be performed by VM thread\n+class VM_HeapDumpMerge : public VM_Operation {\n+private:\n+  DumpMerger* _merger;\n+public:\n+  VM_HeapDumpMerge(DumpMerger* merger) : _merger(merger) {}\n+  VMOp_Type type() const { return VMOp_HeapDumpMerge; }\n+  \/\/ heap dump merge could happen outside safepoint\n+  virtual bool evaluate_at_safepoint() const { return false; }\n+  void doit() {\n+    _merger->do_merge();\n+  }\n+};\n+\n@@ -2314,0 +2126,1 @@\n+  volatile int            _dump_seq;\n@@ -2317,1 +2130,0 @@\n-  InlinedObjects* inlined_objects() { return &_inlined_objects; }\n@@ -2321,1 +2133,0 @@\n-  uint                    _num_writer_threads;\n@@ -2324,6 +2135,0 @@\n-  HeapDumpLargeObjectList* _large_object_list;\n-\n-  \/\/ VMDumperType is for thread that dumps both heap and non-heap data.\n-  static const size_t VMDumperType = 0;\n-  static const size_t WriterType = 1;\n-  static const size_t DumperType = 2;\n@@ -2332,46 +2137,2 @@\n-\n-  size_t get_worker_type(uint worker_id) {\n-    assert(_num_writer_threads >= 1, \"Must be at least one writer\");\n-    \/\/ worker id of VMDumper that dump heap and non-heap data\n-    if (worker_id == VMDumperWorkerId) {\n-      return VMDumperType;\n-    }\n-\n-    \/\/ worker id of dumper starts from 1, which only dump heap datar\n-    if (worker_id < _num_dumper_threads) {\n-      return DumperType;\n-    }\n-\n-    \/\/ worker id of writer starts from _num_dumper_threads\n-    return WriterType;\n-  }\n-\n-  void prepare_parallel_dump(uint num_total) {\n-    assert (_dumper_controller == nullptr, \"dumper controller must be null\");\n-    assert (num_total > 0, \"active workers number must >= 1\");\n-    \/\/ Dumper threads number must not be larger than active workers number.\n-    if (num_total < _num_dumper_threads) {\n-      _num_dumper_threads = num_total - 1;\n-    }\n-    \/\/ Calculate dumper and writer threads number.\n-    _num_writer_threads = num_total - _num_dumper_threads;\n-    \/\/ If dumper threads number is 1, only the VMThread works as a dumper.\n-    \/\/ If dumper threads number is equal to active workers, need at lest one worker thread as writer.\n-    if (_num_dumper_threads > 0 && _num_writer_threads == 0) {\n-      _num_writer_threads = 1;\n-      _num_dumper_threads = num_total - _num_writer_threads;\n-    }\n-    \/\/ Prepare parallel writer.\n-    if (_num_dumper_threads > 1) {\n-      ParDumpWriter::before_work();\n-      \/\/ Number of dumper threads that only iterate heap.\n-      uint _heap_only_dumper_threads = _num_dumper_threads - 1 \/* VMDumper thread *\/;\n-      _dumper_controller = new (std::nothrow) DumperController(_heap_only_dumper_threads);\n-    }\n-  }\n-\n-  void finish_parallel_dump() {\n-    if (_num_dumper_threads > 1) {\n-      ParDumpWriter::after_work();\n-    }\n-  }\n+  \/\/ VM dumper dumps both heap and non-heap data, other dumpers dump heap-only data.\n+  static bool is_vm_dumper(uint worker_id) { return worker_id == VMDumperWorkerId; }\n@@ -2382,0 +2143,1 @@\n+\n@@ -2395,0 +2157,3 @@\n+  \/\/ create dump writer for every parallel dump thread\n+  DumpWriter* create_local_writer();\n+\n@@ -2412,3 +2177,0 @@\n-  \/\/ large objects\n-  void dump_large_objects(ObjectClosure* writer);\n-\n@@ -2427,0 +2189,1 @@\n+    _dump_seq = 0;\n@@ -2430,1 +2193,0 @@\n-    _large_object_list = new (std::nothrow) HeapDumpLargeObjectList();\n@@ -2457,1 +2219,5 @@\n-    delete _large_object_list;\n+  int dump_seq()           { return _dump_seq; }\n+  bool is_parallel_dump()  { return _num_dumper_threads > 1; }\n+  bool can_parallel_dump(WorkerThreads* workers);\n+\n+  InlinedObjects* inlined_objects() { return &_inlined_objects; }\n@@ -2650,0 +2416,26 @@\n+bool VM_HeapDumper::can_parallel_dump(WorkerThreads* workers) {\n+  bool can_parallel = true;\n+  uint num_active_workers = workers != nullptr ? workers->active_workers() : 0;\n+  uint num_requested_dump_threads = _num_dumper_threads;\n+  \/\/ check if we can dump in parallel based on requested and active threads\n+  if (num_active_workers <= 1 || num_requested_dump_threads <= 1) {\n+    _num_dumper_threads = 1;\n+    can_parallel = false;\n+  } else {\n+    \/\/ check if we have extra path room to accommodate segmented heap files\n+    const char* base_path = writer()->get_file_path();\n+    assert(base_path != nullptr, \"sanity check\");\n+    if ((strlen(base_path) + 7\/*.p\\d\\d\\d\\d\\0*\/) >= JVM_MAXPATHLEN) {\n+      _num_dumper_threads = 1;\n+      can_parallel = false;\n+    } else {\n+      _num_dumper_threads = clamp(num_requested_dump_threads, 2U, num_active_workers);\n+    }\n+  }\n+\n+  log_info(heapdump)(\"Requested dump threads %u, active dump threads %u, \"\n+                     \"actual dump threads %u, parallelism %s\",\n+                     num_requested_dump_threads, num_active_workers,\n+                     _num_dumper_threads, can_parallel ? \"true\" : \"false\");\n+  return can_parallel;\n+}\n@@ -2696,6 +2488,2 @@\n-\n-  if (workers == nullptr) {\n-    \/\/ Use serial dump, set dumper threads and writer threads number to 1.\n-    _num_dumper_threads=1;\n-    _num_writer_threads=1;\n-    work(0);\n+  if (!can_parallel_dump(workers)) {\n+    work(VMDumperWorkerId);\n@@ -2703,10 +2491,6 @@\n-    prepare_parallel_dump(workers->active_workers());\n-    if (_num_dumper_threads > 1) {\n-      ParallelObjectIterator poi(_num_dumper_threads);\n-      _poi = &poi;\n-      workers->run_task(this);\n-      _poi = nullptr;\n-    } else {\n-      workers->run_task(this);\n-    }\n-    finish_parallel_dump();\n+    uint heap_only_dumper_threads = _num_dumper_threads - 1 \/* VMDumper thread *\/;\n+    _dumper_controller = new (std::nothrow) DumperController(heap_only_dumper_threads);\n+    ParallelObjectIterator poi(_num_dumper_threads);\n+    _poi = &poi;\n+    workers->run_task(this, _num_dumper_threads);\n+    _poi = nullptr;\n@@ -2720,0 +2504,16 @@\n+\/\/ prepare DumpWriter for every parallel dump thread\n+DumpWriter* VM_HeapDumper::create_local_writer() {\n+  char* path = NEW_RESOURCE_ARRAY(char, JVM_MAXPATHLEN);\n+  memset(path, 0, JVM_MAXPATHLEN);\n+\n+  \/\/ generate segmented heap file path\n+  const char* base_path = writer()->get_file_path();\n+  AbstractCompressor* compressor = writer()->compressor();\n+  int seq = Atomic::fetch_then_add(&_dump_seq, 1);\n+  os::snprintf(path, JVM_MAXPATHLEN, \"%s.p%d\", base_path, seq);\n+\n+  \/\/ create corresponding writer for that\n+  DumpWriter* local_writer = new DumpWriter(path, writer()->is_overwrite(), compressor);\n+  return local_writer;\n+}\n+\n@@ -2721,10 +2521,3 @@\n-  if (worker_id != 0) {\n-    if (get_worker_type(worker_id) == WriterType) {\n-      writer()->writer_loop();\n-      return;\n-    }\n-    if (_num_dumper_threads > 1 && get_worker_type(worker_id) == DumperType) {\n-      _dumper_controller->wait_for_start_signal();\n-    }\n-  } else {\n-    \/\/ The worker 0 on all non-heap data dumping and part of heap iteration.\n+  \/\/ VM Dumper works on all non-heap data dumping and part of heap iteration.\n+  if (is_vm_dumper(worker_id)) {\n+    TraceTime timer(\"Dump non-objects\", TRACETIME_LOG(Info, heapdump));\n@@ -2781,0 +2574,2 @@\n+\n+  \/\/ Heap iteration.\n@@ -2787,1 +2582,4 @@\n-  if (_num_dumper_threads <= 1) {\n+  if (!is_parallel_dump()) {\n+    assert(is_vm_dumper(worker_id), \"must be\");\n+    \/\/ == Serial dump\n+    TraceTime timer(\"Dump heap objects\", TRACETIME_LOG(Info, heapdump));\n@@ -2790,0 +2588,6 @@\n+    writer()->finish_dump_segment();\n+    \/\/ Writes the HPROF_HEAP_DUMP_END record because merge does not happen in serial dump\n+    DumperSupport::end_of_dump(writer());\n+    inlined_objects()->dump_flat_arrays(writer());\n+    writer()->flush();\n+    inlined_objects()->release();\n@@ -2791,44 +2595,19 @@\n-    assert(get_worker_type(worker_id) == DumperType\n-          || get_worker_type(worker_id) == VMDumperType,\n-          \"must be dumper thread to do heap iteration\");\n-    if (get_worker_type(worker_id) == VMDumperType) {\n-      \/\/ Clear global writer's buffer.\n-      writer()->finish_dump_segment(true);\n-      \/\/ Notify dumpers to start heap iteration.\n-      _dumper_controller->start_dump();\n-    }\n-    \/\/ Heap iteration.\n-    {\n-       ParDumpWriter pw(writer());\n-       {\n-         HeapObjectDumper obj_dumper(&pw, _large_object_list);\n-         _poi->object_iterate(&obj_dumper, worker_id);\n-       }\n-\n-       if (get_worker_type(worker_id) == VMDumperType) {\n-         _dumper_controller->wait_all_dumpers_complete();\n-         \/\/ clear internal buffer;\n-         pw.finish_dump_segment(true);\n-         \/\/ refresh the global_writer's buffer and position;\n-         writer()->refresh();\n-       } else {\n-         pw.finish_dump_segment(true);\n-         _dumper_controller->dumper_complete();\n-         return;\n-       }\n-    }\n-  }\n-\n-  assert(get_worker_type(worker_id) == VMDumperType, \"Heap dumper must be VMDumper\");\n-  \/\/ Use writer() rather than ParDumpWriter to avoid memory consumption.\n-  HeapObjectDumper obj_dumper(writer());\n-  dump_large_objects(&obj_dumper);\n-  \/\/ Writes the HPROF_HEAP_DUMP_END record.\n-  DumperSupport::end_of_dump(writer());\n-\n-  inlined_objects()->dump_flat_arrays(writer());\n-\n-  \/\/ We are done with writing. Release the worker threads.\n-  writer()->deactivate();\n-\n-  inlined_objects()->release();\n+    \/\/ == Parallel dump\n+    ResourceMark rm;\n+    TraceTime timer(\"Dump heap objects in parallel\", TRACETIME_LOG(Info, heapdump));\n+    DumpWriter* local_writer = is_vm_dumper(worker_id) ? writer() : create_local_writer();\n+    if (!local_writer->has_error()) {\n+      HeapObjectDumper obj_dumper(local_writer);\n+      _poi->object_iterate(&obj_dumper, worker_id);\n+      local_writer->finish_dump_segment();\n+      local_writer->flush();\n+    }\n+    if (is_vm_dumper(worker_id)) {\n+      _dumper_controller->wait_all_dumpers_complete();\n+    } else {\n+      _dumper_controller->dumper_complete(local_writer, writer());\n+      return;\n+    }\n+  }\n+  \/\/ At this point, all fragments of the heapdump have been written to separate files.\n+  \/\/ We need to merge them into a complete heapdump and write HPROF_HEAP_DUMP_END at that time.\n@@ -2895,5 +2674,0 @@\n-\/\/ dump the large objects.\n-void VM_HeapDumper::dump_large_objects(ObjectClosure* cl) {\n-  _large_object_list->drain(cl);\n-}\n-\n@@ -2923,1 +2697,1 @@\n-  DumpWriter writer(new (std::nothrow) FileWriter(path, overwrite), compressor);\n+  DumpWriter writer(path, overwrite, compressor);\n@@ -2934,1 +2708,1 @@\n-  \/\/ generate the dump\n+  \/\/ generate the segmented heap dump into separate files\n@@ -2936,6 +2710,1 @@\n-  if (Thread::current()->is_VM_thread()) {\n-    assert(SafepointSynchronize::is_at_safepoint(), \"Expected to be called at a safepoint\");\n-    dumper.doit();\n-  } else {\n-    VMThread::execute(&dumper);\n-  }\n+  VMThread::execute(&dumper);\n@@ -2946,0 +2715,25 @@\n+  \/\/ For serial dump, once VM_HeapDumper completes, the whole heap dump process\n+  \/\/ is done, no further phases needed. For parallel dump, the whole heap dump\n+  \/\/ process is done in two phases\n+  \/\/\n+  \/\/ Phase 1: Concurrent threads directly write heap data to multiple heap files.\n+  \/\/          This is done by VM_HeapDumper, which is performed within safepoint.\n+  \/\/\n+  \/\/ Phase 2: Merge multiple heap files into one complete heap dump file.\n+  \/\/          This is done by DumpMerger, which is performed outside safepoint\n+  if (dumper.is_parallel_dump()) {\n+    DumpMerger merger(path, &writer, dumper.inlined_objects(), dumper.dump_seq());\n+    Thread* current_thread = Thread::current();\n+    if (current_thread->is_AttachListener_thread()) {\n+      \/\/ perform heapdump file merge operation in the current thread prevents us\n+      \/\/ from occupying the VM Thread, which in turn affects the occurrence of\n+      \/\/ GC and other VM operations.\n+      merger.do_merge();\n+    } else {\n+      \/\/ otherwise, performs it by VM thread\n+      VM_HeapDumpMerge op(&merger);\n+      VMThread::execute(&op);\n+    }\n+    set_error(writer.error());\n+  }\n+\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":337,"deletions":543,"binary":false,"changes":880,"status":"modified"},{"patch":"@@ -3550,1 +3550,3 @@\n-                nestedDescriptions[i] = makePatternDescription(types.erasure(componentTypes[i]), it.head);\n+                Type componentType = i < componentTypes.length ? componentTypes[i]\n+                                                               : syms.errType;\n+                nestedDescriptions[i] = makePatternDescription(types.erasure(componentType), it.head);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Flow.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -55,0 +55,2 @@\n+    protected short  _nestHostIndex;\n+    protected short  _nestMembersIndex;\n@@ -146,0 +148,8 @@\n+        Short nestHostIndex = utf8ToIndex.get(\"NestHost\");\n+        _nestHostIndex = (nestHostIndex != null)? nestHostIndex.shortValue() : 0;\n+        if (DEBUG) debugMessage(\"NestHost index = \" + _nestHostIndex);\n+\n+        Short nestMembersIndex = utf8ToIndex.get(\"NestMembers\");\n+        _nestMembersIndex = (nestMembersIndex != null)? nestMembersIndex.shortValue() : 0;\n+        if (DEBUG) debugMessage(\"NestMembers index = \" + _nestMembersIndex);\n+\n@@ -784,0 +794,11 @@\n+        short nestHost = klass.getNestHostIndex();\n+        if (nestHost != 0) {\n+            classAttributeCount++;\n+        }\n+\n+        U2Array nestMembers = klass.getNestMembers();\n+        final int numNestMembers = nestMembers.length();\n+        if (numNestMembers != 0) {\n+            classAttributeCount++;\n+        }\n+\n@@ -839,0 +860,17 @@\n+        if (nestHost != 0) {\n+            writeIndex(_nestHostIndex);\n+            final int nestHostAttrLen = 2;\n+            dos.writeInt(nestHostAttrLen);\n+            dos.writeShort(nestHost);\n+        }\n+\n+        if (numNestMembers != 0) {\n+           writeIndex(_nestMembersIndex);\n+           final int nestMembersAttrLen = 2 + numNestMembers * 2;\n+           dos.writeInt(nestMembersAttrLen);\n+           dos.writeShort(numNestMembers);\n+           for (int index = 0; index < numNestMembers; index++) {\n+               dos.writeShort(nestMembers.at(index));\n+           }\n+        }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/tools\/jcore\/ClassWriter.java","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -78,1 +78,2 @@\n-compiler\/valhalla\/inlinetypes\/TestCallingConventionC1.java 8317143 linux-x64\n+compiler\/valhalla\/inlinetypes\/TestCallingConventionC1.java 8317143 generic-all\n+compiler\/valhalla\/inlinetypes\/TestC1.java 8317143 generic-all\n@@ -151,0 +152,2 @@\n+serviceability\/jvmti\/Valhalla\/HeapDump\/HeapDump.java 8317416 generic-all\n+\n@@ -197,1 +200,0 @@\n-vmTestbase\/nsk\/jvmti\/AttachOnDemand\/attach002a\/TestDescription.java 8307462 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -741,0 +741,1 @@\n+sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java              8313798 generic-aarch64\n@@ -782,0 +783,3 @@\n+javax\/swing\/JFileChooser\/FileSystemView\/SystemIconTest.java 8313902 windows-all\n+sanity\/client\/SwingSet\/src\/FileChooserDemoTest.java 8313903 windows-all\n+\n","filename":"test\/jdk\/ProblemList.txt","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,0 +26,17 @@\n+#############################################################################\n+#\n+# List of quarantined tests -- tests that should not be run by default, because\n+# they may fail due to known reason. The reason (CR#) must be mandatory specified.\n+#\n+# List items are testnames followed by labels, all MUST BE commented\n+#   as to why they are here and use a label:\n+#     generic-all   Problems on all platforms\n+#     generic-ARCH  Where ARCH is one of: x64, i586, ppc64, ppc64le, s390x etc.\n+#     OSNAME-all    Where OSNAME is one of: linux, windows, macosx, aix\n+#     OSNAME-ARCH   Specific on to one OSNAME and ARCH, e.g. macosx-x64\n+#     OSNAME-REV    Specific on to one OSNAME and REV, e.g. macosx-10.7.4\n+#\n+# More than one label is allowed but must be on the same line.\n+#\n+#############################################################################\n+\n@@ -59,0 +76,4 @@\n+tools\/javac\/lambda\/bytecode\/TestLambdaBytecodeTargetRelease14.java              8312534    linux-i586     fails with assert \"g1ConcurrentMark.cpp: Overflow during reference processing\"\n+tools\/javac\/varargs\/warning\/Warn5.java                                          8312534    linux-i586     fails with assert \"g1ConcurrentMark.cpp: Overflow during reference processing\"\n+\n+# javac Valhalla\n@@ -60,0 +81,3 @@\n+tools\/javac\/annotations\/SyntheticParameters.java                                8317415    generic-all\n+tools\/javac\/annotations\/typeAnnotations\/classfile\/SyntheticParameters.java      8317415    generic-all\n+\n","filename":"test\/langtools\/ProblemList.txt","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+import java.util.ArrayList;\n+import java.util.List;\n@@ -29,3 +31,2 @@\n-import com.sun.tools.classfile.*;\n-import com.sun.tools.classfile.ConstantPool.InvalidIndex;\n-import com.sun.tools.classfile.ConstantPool.UnexpectedEntry;\n+import jdk.internal.classfile.*;\n+import jdk.internal.classfile.attribute.*;\n@@ -329,2 +330,2 @@\n-         * @param Whether or not the annotation is visible at runtime.\n-         * @return Whether or not this template matches the visibility.\n+         * @param visibility Whether the annotation is visible at runtime.\n+         * @return Whether this template matches the visibility.\n@@ -343,3 +344,2 @@\n-        public void matchAnnotation(ConstantPool cpool,\n-                                    Annotation anno) {\n-            if (checkMatch(cpool, anno)) {\n+        public void matchAnnotation(Annotation anno) {\n+            if (checkMatch(anno)) {\n@@ -354,1 +354,0 @@\n-         * @param ConstantPool The constant pool to use.\n@@ -358,7 +357,2 @@\n-        protected boolean checkMatch(ConstantPool cpool,\n-                                     Annotation anno) {\n-            try {\n-                return cpool.getUTF8Info(anno.type_index).value.equals(\"L\" + expectedName + \";\");\n-            } catch (InvalidIndex | UnexpectedEntry e) {\n-                return false;\n-            }\n+        protected boolean checkMatch(Annotation anno) {\n+            return anno.classSymbol().descriptorString().equals(\"L\" + expectedName + \";\");\n@@ -535,1 +529,1 @@\n-        protected final TypeAnnotation.Position.TypePathEntry[] typePath;\n+        protected final List<TypeAnnotation.TypePathComponent> typePath;\n@@ -564,1 +558,1 @@\n-                                      TypeAnnotation.Position.TypePathEntry... typePath) {\n+                                      List<TypeAnnotation.TypePathComponent> typePath) {\n@@ -592,1 +586,1 @@\n-            for(int i = 0; i < typePath.length; i++) {\n+            for(int i = 0; i < typePath.size(); i++) {\n@@ -596,1 +590,1 @@\n-                sb.append(typePath[i]);\n+                sb.append(typePath.get(i));\n@@ -603,2 +597,1 @@\n-        public void matchAnnotation(ConstantPool cpool,\n-                                    Annotation anno) {}\n+        public void matchAnnotation(Annotation anno) {}\n@@ -613,8 +606,27 @@\n-            boolean matches = checkMatch(anno.constant_pool, anno.annotation);\n-\n-            matches = matches && anno.position.type == targetType;\n-            matches = matches && anno.position.bound_index == bound_index;\n-            matches = matches && anno.position.parameter_index == parameter_index;\n-            matches = matches && anno.position.type_index == type_index;\n-            matches = matches && anno.position.exception_index == exception_index;\n-            matches = matches && anno.position.location.size() == typePath.length;\n+            boolean matches = checkMatch((Annotation) anno);\n+            int boundIdx = Integer.MIN_VALUE, paraIdx = Integer.MIN_VALUE, tIdx = Integer.MIN_VALUE, exIdx = Integer.MIN_VALUE;\n+            switch (anno.targetInfo()) {\n+                case TypeAnnotation.TypeParameterBoundTarget binfo -> {\n+                    boundIdx = binfo.boundIndex();\n+                    paraIdx = binfo.typeParameterIndex();\n+                }\n+                case TypeAnnotation.FormalParameterTarget fpinfo -> {\n+                    paraIdx = fpinfo.formalParameterIndex();\n+                }\n+                case TypeAnnotation.TypeParameterTarget pinfo -> {\n+                    paraIdx = pinfo.typeParameterIndex();\n+                }\n+                case TypeAnnotation.TypeArgumentTarget ainfo -> {\n+                    tIdx = ainfo.typeArgumentIndex();\n+                }\n+                case TypeAnnotation.CatchTarget cinfo -> {\n+                    exIdx = cinfo.exceptionTableIndex();\n+                }\n+                default -> {}\n+            }\n+            matches = matches && anno.targetInfo().targetType() == targetType;\n+            matches = matches && boundIdx == bound_index;\n+            matches = matches && paraIdx == parameter_index;\n+            matches = matches && tIdx == type_index;\n+            matches = matches && exIdx == exception_index;\n+            matches = matches && anno.targetPath().size() == typePath.size();\n@@ -624,3 +636,3 @@\n-                for(TypeAnnotation.Position.TypePathEntry entry :\n-                         anno.position.location) {\n-                    matches = matches && typePath[i++].equals(entry);\n+                for(TypeAnnotation.TypePathComponent entry :\n+                        anno.targetPath()) {\n+                    matches = matches && typePath.get(i++).equals(entry);\n@@ -650,2 +662,2 @@\n-            protected TypeAnnotation.Position.TypePathEntry[] typePath =\n-                new TypeAnnotation.Position.TypePathEntry[0];\n+            protected List<TypeAnnotation.TypePathComponent> typePath =\n+                new ArrayList<TypeAnnotation.TypePathComponent>();\n@@ -699,1 +711,1 @@\n-             * @param bound_index The parameter_index value.\n+             * @param parameter_index The parameter_index value.\n@@ -731,1 +743,1 @@\n-            public Builder setTypePath(TypeAnnotation.Position.TypePathEntry[] typePath) {\n+            public Builder setTypePath(List<TypeAnnotation.TypePathComponent> typePath) {\n@@ -771,1 +783,1 @@\n-                                            TypeAnnotation.Position.TypePathEntry... typePath) {\n+                                            List<TypeAnnotation.TypePathComponent> typePath) {\n@@ -795,1 +807,1 @@\n-            for(int i = 0; i < typePath.length; i++) {\n+            for(int i = 0; i < typePath.size(); i++) {\n@@ -799,1 +811,1 @@\n-                sb.append(typePath[i]);\n+                sb.append(typePath.get(i));\n@@ -897,1 +909,1 @@\n-                                           TypeAnnotation.Position.TypePathEntry... typePath) {\n+                                           List<TypeAnnotation.TypePathComponent> typePath) {\n@@ -916,1 +928,1 @@\n-            for(int i = 0; i < typePath.length; i++) {\n+            for(int i = 0; i < typePath.size(); i++) {\n@@ -920,1 +932,1 @@\n-                sb.append(typePath[i]);\n+                sb.append(typePath.get(i));\n@@ -984,5 +996,4 @@\n-    private void matchClassAnnotation(ClassFile classfile,\n-                                      ExpectedAnnotation expected)\n-        throws ConstantPoolException {\n-        for(Attribute attr : classfile.attributes) {\n-            attr.accept(annoMatcher(classfile.constant_pool), expected);\n+    private void matchClassAnnotation(ClassModel classfile,\n+                                      ExpectedAnnotation expected) {\n+        for(Attribute<?> attr : classfile.attributes()) {\n+            annoMatcher(attr, expected);\n@@ -992,7 +1003,6 @@\n-    private void matchMethodAnnotation(ClassFile classfile,\n-                                       ExpectedMethodAnnotation expected)\n-        throws ConstantPoolException {\n-        for(Method meth : classfile.methods) {\n-            if (expected.matchMethodName(meth.getName(classfile.constant_pool))) {\n-                for(Attribute attr : meth.attributes) {\n-                    attr.accept(annoMatcher(classfile.constant_pool), expected);\n+    private void matchMethodAnnotation(ClassModel classfile,\n+                                       ExpectedMethodAnnotation expected) {\n+        for(MethodModel meth : classfile.methods()) {\n+            if (expected.matchMethodName(meth.methodName().stringValue())) {\n+                for(Attribute<?> attr : meth.attributes()) {\n+                    annoMatcher(attr, expected);\n@@ -1004,7 +1014,6 @@\n-    private void matchParameterAnnotation(ClassFile classfile,\n-                                          ExpectedParameterAnnotation expected)\n-        throws ConstantPoolException {\n-        for(Method meth : classfile.methods) {\n-            if (expected.matchMethodName(meth.getName(classfile.constant_pool))) {\n-                for(Attribute attr : meth.attributes) {\n-                    attr.accept(paramMatcher(classfile.constant_pool), expected);\n+    private void matchParameterAnnotation(ClassModel classfile,\n+                                          ExpectedParameterAnnotation expected) {\n+        for(MethodModel meth : classfile.methods()) {\n+            if (expected.matchMethodName(meth.methodName().stringValue())) {\n+                for(Attribute<?> attr : meth.attributes()) {\n+                    paramMatcher(attr, expected);\n@@ -1016,7 +1025,6 @@\n-    private void matchFieldAnnotation(ClassFile classfile,\n-                                      ExpectedFieldAnnotation expected)\n-        throws ConstantPoolException {\n-        for(Field field : classfile.fields) {\n-            if (expected.matchFieldName(field.getName(classfile.constant_pool))) {\n-                for(Attribute attr : field.attributes) {\n-                    attr.accept(annoMatcher(classfile.constant_pool), expected);\n+    private void matchFieldAnnotation(ClassModel classfile,\n+                                      ExpectedFieldAnnotation expected) {\n+        for(FieldModel field : classfile.fields()) {\n+            if (expected.matchFieldName(field.fieldName().stringValue())) {\n+                for(Attribute<?> attr : field.attributes()) {\n+                    annoMatcher(attr, expected);\n@@ -1028,5 +1036,4 @@\n-    private void matchClassTypeAnnotation(ClassFile classfile,\n-                                          ExpectedTypeAnnotation expected)\n-        throws ConstantPoolException {\n-        for(Attribute attr : classfile.attributes) {\n-            attr.accept(typeAnnoMatcher, expected);\n+    private void matchClassTypeAnnotation(ClassModel classfile,\n+                                          ExpectedTypeAnnotation expected) {\n+        for(Attribute<?> attr : classfile.attributes()) {\n+            typeAnnoMatcher(attr, expected);\n@@ -1036,7 +1043,6 @@\n-    private void matchMethodTypeAnnotation(ClassFile classfile,\n-                                           ExpectedMethodTypeAnnotation expected)\n-        throws ConstantPoolException {\n-        for(Method meth : classfile.methods) {\n-            if (expected.matchMethodName(meth.getName(classfile.constant_pool))) {\n-                for(Attribute attr : meth.attributes) {\n-                    attr.accept(typeAnnoMatcher, expected);\n+    private void matchMethodTypeAnnotation(ClassModel classfile,\n+                                           ExpectedMethodTypeAnnotation expected) {\n+        for(MethodModel meth : classfile.methods()) {\n+            if (expected.matchMethodName(meth.methodName().stringValue())) {\n+                for(Attribute<?> attr : meth.attributes()) {\n+                    typeAnnoMatcher(attr, expected);\n@@ -1048,7 +1054,6 @@\n-    private void matchFieldTypeAnnotation(ClassFile classfile,\n-                                          ExpectedFieldTypeAnnotation expected)\n-        throws ConstantPoolException {\n-        for(Field field : classfile.fields) {\n-            if (expected.matchFieldName(field.getName(classfile.constant_pool))) {\n-                for(Attribute attr : field.attributes) {\n-                    attr.accept(typeAnnoMatcher, expected);\n+    private void matchFieldTypeAnnotation(ClassModel classfile,\n+                                          ExpectedFieldTypeAnnotation expected) {\n+        for(FieldModel field : classfile.fields()) {\n+            if (expected.matchFieldName(field.fieldName().stringValue())) {\n+                for(Attribute<?> attr : field.attributes()) {\n+                    typeAnnoMatcher(attr, expected);\n@@ -1060,3 +1065,2 @@\n-    private void matchClassAnnotations(ClassFile classfile,\n-                                       ExpectedAnnotation[] expected)\n-        throws ConstantPoolException {\n+    private void matchClassAnnotations(ClassModel classfile,\n+                                       ExpectedAnnotation[] expected) {\n@@ -1068,3 +1072,2 @@\n-    private void matchMethodAnnotations(ClassFile classfile,\n-                                        ExpectedMethodAnnotation[] expected)\n-        throws ConstantPoolException {\n+    private void matchMethodAnnotations(ClassModel classfile,\n+                                        ExpectedMethodAnnotation[] expected) {\n@@ -1076,3 +1079,2 @@\n-    private void matchParameterAnnotations(ClassFile classfile,\n-                                           ExpectedParameterAnnotation[] expected)\n-        throws ConstantPoolException {\n+    private void matchParameterAnnotations(ClassModel classfile,\n+                                           ExpectedParameterAnnotation[] expected) {\n@@ -1084,3 +1086,2 @@\n-    private void matchFieldAnnotations(ClassFile classfile,\n-                                       ExpectedFieldAnnotation[] expected)\n-        throws ConstantPoolException {\n+    private void matchFieldAnnotations(ClassModel classfile,\n+                                       ExpectedFieldAnnotation[] expected) {\n@@ -1092,3 +1093,2 @@\n-    private void matchClassTypeAnnotations(ClassFile classfile,\n-                                           ExpectedTypeAnnotation[] expected)\n-        throws ConstantPoolException {\n+    private void matchClassTypeAnnotations(ClassModel classfile,\n+                                           ExpectedTypeAnnotation[] expected) {\n@@ -1100,3 +1100,2 @@\n-    private void matchMethodTypeAnnotations(ClassFile classfile,\n-                                            ExpectedMethodTypeAnnotation[] expected)\n-        throws ConstantPoolException {\n+    private void matchMethodTypeAnnotations(ClassModel classfile,\n+                                            ExpectedMethodTypeAnnotation[] expected) {\n@@ -1108,3 +1107,2 @@\n-    private void matchFieldTypeAnnotations(ClassFile classfile,\n-                                           ExpectedFieldTypeAnnotation[] expected)\n-        throws ConstantPoolException {\n+    private void matchFieldTypeAnnotations(ClassModel classfile,\n+                                           ExpectedFieldTypeAnnotation[] expected) {\n@@ -1117,1 +1115,1 @@\n-     * Run a template on a single {@code ClassFile}.\n+     * Run a template on a single {@code ClassModel}.\n@@ -1119,1 +1117,1 @@\n-     * @param classfile The {@code ClassFile} on which to run tests.\n+     * @param classfile The {@code ClassModel} on which to run tests.\n@@ -1122,4 +1120,3 @@\n-    public void run(ClassFile classfile,\n-                    Expected... expected)\n-            throws ConstantPoolException {\n-        run(new ClassFile[] { classfile }, expected);\n+    public void run(ClassModel classfile,\n+                    Expected... expected) {\n+        run(new ClassModel[] { classfile }, expected);\n@@ -1129,1 +1126,1 @@\n-     * Run a template on multiple {@code ClassFile}s.\n+     * Run a template on multiple {@code ClassModel}s.\n@@ -1131,1 +1128,1 @@\n-     * @param classfile The {@code ClassFile}s on which to run tests.\n+     * @param classfiles The {@code ClassModel}s on which to run tests.\n@@ -1134,4 +1131,3 @@\n-    public void run(ClassFile[] classfiles,\n-                    Expected... expected)\n-            throws ConstantPoolException {\n-        for(ClassFile classfile : classfiles) {\n+    public void run(ClassModel[] classfiles,\n+                    Expected... expected) {\n+        for(ClassModel classfile : classfiles) {\n@@ -1139,1 +1135,1 @@\n-                if (one.matchClassName(classfile.getName())) {\n+                if (one.matchClassName(classfile.thisClass().name().stringValue())) {\n@@ -1168,1 +1164,1 @@\n-     * Get a {@code ClassFile} from its file name.\n+     * Get a {@code ClassModel} from its file name.\n@@ -1172,1 +1168,1 @@\n-     * @return The {@code ClassFile}\n+     * @return The {@code ClassModel}\n@@ -1174,1 +1170,1 @@\n-    public static ClassFile getClassFile(String name,\n+    public static ClassModel getClassFile(String name,\n@@ -1176,1 +1172,1 @@\n-        throws IOException, ConstantPoolException {\n+        throws IOException {\n@@ -1178,0 +1174,1 @@\n+        assert url != null;\n@@ -1179,1 +1176,1 @@\n-            return ClassFile.read(in);\n+            return Classfile.of().parse(in.readAllBytes());\n@@ -1183,211 +1180,5 @@\n-    private static class AbstractAttributeVisitor<T> implements Attribute.Visitor<Void, T> {\n-\n-        @Override\n-        public Void visitDefault(DefaultAttribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitAnnotationDefault(AnnotationDefault_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitBootstrapMethods(BootstrapMethods_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitCharacterRangeTable(CharacterRangeTable_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitCode(Code_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitCompilationID(CompilationID_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitConstantValue(ConstantValue_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitDeprecated(Deprecated_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitEnclosingMethod(EnclosingMethod_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitExceptions(Exceptions_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitInnerClasses(InnerClasses_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitLineNumberTable(LineNumberTable_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitLocalVariableTable(LocalVariableTable_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitLocalVariableTypeTable(LocalVariableTypeTable_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-          public Void visitNestHost(NestHost_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitMethodParameters(MethodParameters_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitModule(Module_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitModuleHashes(ModuleHashes_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitModuleMainClass(ModuleMainClass_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitModulePackages(ModulePackages_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitModuleResolution(ModuleResolution_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitModuleTarget(ModuleTarget_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitNestMembers(NestMembers_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitRuntimeInvisibleAnnotations(RuntimeInvisibleAnnotations_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitRuntimeInvisibleParameterAnnotations(RuntimeInvisibleParameterAnnotations_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitRuntimeInvisibleTypeAnnotations(RuntimeInvisibleTypeAnnotations_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitRuntimeVisibleAnnotations(RuntimeVisibleAnnotations_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitRuntimeVisibleParameterAnnotations(RuntimeVisibleParameterAnnotations_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitRuntimeVisibleTypeAnnotations(RuntimeVisibleTypeAnnotations_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitSignature(Signature_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitSourceDebugExtension(SourceDebugExtension_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitSourceFile(SourceFile_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitSourceID(SourceID_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitStackMap(StackMap_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitStackMapTable(StackMapTable_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitSynthetic(Synthetic_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitPermittedSubclasses(PermittedSubclasses_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitRecord(Record_attribute attr, T p) {\n-            return null;\n-        }\n-\n-        @Override\n-        public Void visitPreload(Preload_attribute attr, T p) {\n-            return null;\n-        }\n-    }\n-\n-    private static final Attribute.Visitor<Void, ExpectedTypeAnnotation> typeAnnoMatcher\n-            = new AbstractAttributeVisitor<ExpectedTypeAnnotation>() {\n-\n-                @Override\n-                public Void visitRuntimeVisibleTypeAnnotations(RuntimeVisibleTypeAnnotations_attribute attr,\n-                        ExpectedTypeAnnotation expected) {\n-                    if (expected.matchVisibility(true)) {\n-                        for (TypeAnnotation anno : attr.annotations) {\n-                            expected.matchAnnotation(anno);\n-                        }\n-                    }\n-\n-                    return null;\n+    public void typeAnnoMatcher(Attribute<?> attr, ExpectedTypeAnnotation expected) {\n+        switch (attr) {\n+            case RuntimeVisibleTypeAnnotationsAttribute vattr -> {\n+                if (expected.matchVisibility(true)) {\n+                    for (TypeAnnotation anno : vattr.annotations()) expected.matchAnnotation(anno);\n@@ -1395,11 +1186,4 @@\n-\n-                @Override\n-                public Void visitRuntimeInvisibleTypeAnnotations(RuntimeInvisibleTypeAnnotations_attribute attr,\n-                        ExpectedTypeAnnotation expected) {\n-                    if (expected.matchVisibility(false)) {\n-                        for (TypeAnnotation anno : attr.annotations) {\n-                            expected.matchAnnotation(anno);\n-                        }\n-                    }\n-\n-                    return null;\n+            }\n+            case RuntimeInvisibleTypeAnnotationsAttribute ivattr -> {\n+                if (expected.matchVisibility(false)) {\n+                    ivattr.annotations().forEach(expected::matchAnnotation);\n@@ -1407,4 +1191,4 @@\n-            };\n-\n-    private static Attribute.Visitor<Void, ExpectedAnnotation> annoMatcher(ConstantPool cpool) {\n-        return new AbstractAttributeVisitor<ExpectedAnnotation>() {\n+            }\n+            default -> {}\n+        }\n+    };\n@@ -1412,3 +1196,3 @@\n-            @Override\n-            public Void visitRuntimeVisibleAnnotations(RuntimeVisibleAnnotations_attribute attr,\n-                                                       ExpectedAnnotation expected) {\n+    public void annoMatcher(Attribute<?> attr, ExpectedAnnotation expected) {\n+        switch (attr) {\n+            case RuntimeVisibleTypeAnnotationsAttribute rvattr -> {\n@@ -1416,2 +1200,2 @@\n-                    for(Annotation anno : attr.annotations) {\n-                        expected.matchAnnotation(cpool, anno);\n+                    for(Annotation anno : rvattr.annotations()) {\n+                        expected.matchAnnotation(anno);\n@@ -1420,6 +1204,1 @@\n-\n-                return null;\n-\n-            @Override\n-            public Void visitRuntimeInvisibleAnnotations(RuntimeInvisibleAnnotations_attribute attr,\n-                                                         ExpectedAnnotation expected) {\n+            case RuntimeInvisibleAnnotationsAttribute rivattr -> {\n@@ -1428,2 +1207,2 @@\n-                    for(Annotation anno : attr.annotations) {\n-                        expected.matchAnnotation(cpool, anno);\n+                    for(Annotation anno : rivattr.annotations()) {\n+                        expected.matchAnnotation(anno);\n@@ -1432,4 +1211,2 @@\n-\n-                return null;\n-        };\n-    }\n+            default -> {}\n+        }\n@@ -1437,2 +1214,5 @@\n-    private static Attribute.Visitor<Void, ExpectedParameterAnnotation> paramMatcher(ConstantPool cpool) {\n-        return new AbstractAttributeVisitor<ExpectedParameterAnnotation>() {\n+        @Override\n+        public Void visitPreload(Preload_attribute attr, T p) {\n+            return null;\n+        }\n+    }\n@@ -1441,3 +1221,3 @@\n-            @Override\n-            public Void visitRuntimeVisibleParameterAnnotations(RuntimeVisibleParameterAnnotations_attribute attr,\n-                                                                ExpectedParameterAnnotation expected) {\n+    private void paramMatcher(Attribute<?> attr, ExpectedParameterAnnotation expected) {\n+        switch (attr) {\n+            case RuntimeVisibleParameterAnnotationsAttribute vattr -> {\n@@ -1445,1 +1225,1 @@\n-                    if (expected.index < attr.parameter_annotations.length) {\n+                    if (expected.index < vattr.parameterAnnotations().size()) {\n@@ -1447,2 +1227,2 @@\n-                                attr.parameter_annotations[expected.index]) {\n-                            expected.matchAnnotation(cpool, anno);\n+                                vattr.parameterAnnotations().get(expected.index)) {\n+                            expected.matchAnnotation(anno);\n@@ -1452,6 +1232,1 @@\n-\n-                return null;\n-\n-            @Override\n-            public Void visitRuntimeInvisibleParameterAnnotations(RuntimeInvisibleParameterAnnotations_attribute attr,\n-                                                                  ExpectedParameterAnnotation expected) {\n+            case RuntimeInvisibleParameterAnnotationsAttribute ivattr -> {\n@@ -1460,1 +1235,1 @@\n-                    if (expected.index < attr.parameter_annotations.length) {\n+                    if (expected.index < ivattr.parameterAnnotations().size()) {\n@@ -1462,2 +1237,2 @@\n-                                attr.parameter_annotations[expected.index]) {\n-                            expected.matchAnnotation(cpool, anno);\n+                                ivattr.parameterAnnotations().get(expected.index)) {\n+                            expected.matchAnnotation(anno);\n@@ -1467,3 +1242,2 @@\n-\n-                return null;\n-        };\n+            default -> {}\n+        }\n","filename":"test\/langtools\/lib\/annotations\/annotations\/classfile\/ClassfileInspector.java","additions":164,"deletions":390,"binary":false,"changes":554,"status":"modified"},{"patch":"@@ -31,1 +31,6 @@\n- *          jdk.jdeps\/com.sun.tools.classfile\n+ *          java.base\/jdk.internal.classfile\n+ *          java.base\/jdk.internal.classfile.attribute\n+ *          java.base\/jdk.internal.classfile.constantpool\n+ *          java.base\/jdk.internal.classfile.instruction\n+ *          java.base\/jdk.internal.classfile.components\n+ *          java.base\/jdk.internal.classfile.impl\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerClassesInAnonymousClassTest.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -29,1 +29,6 @@\n- * @modules jdk.jdeps\/com.sun.tools.classfile\n+ * @modules java.base\/jdk.internal.classfile\n+ *          java.base\/jdk.internal.classfile.attribute\n+ *          java.base\/jdk.internal.classfile.constantpool\n+ *          java.base\/jdk.internal.classfile.instruction\n+ *          java.base\/jdk.internal.classfile.components\n+ *          java.base\/jdk.internal.classfile.impl\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerClassesInLocalClassTest.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -24,4 +24,4 @@\n-import com.sun.tools.classfile.Attribute;\n-import com.sun.tools.classfile.ClassFile;\n-import com.sun.tools.classfile.InnerClasses_attribute;\n-import com.sun.tools.classfile.InnerClasses_attribute.Info;\n+import jdk.internal.classfile.*;\n+import jdk.internal.classfile.attribute.*;\n+import jdk.internal.classfile.constantpool.*;\n+import jdk.internal.classfile.impl.BoundAttribute;\n@@ -30,8 +30,1 @@\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n+import java.util.*;\n@@ -39,0 +32,1 @@\n+import java.lang.reflect.AccessFlag;\n@@ -202,1 +196,1 @@\n-            ClassFile cf = readClassFile(compile(getCompileOptions(), test.getSource())\n+            ClassModel cm = readClassFile(compile(getCompileOptions(), test.getSource())\n@@ -204,2 +198,1 @@\n-            InnerClasses_attribute innerClasses = (InnerClasses_attribute)\n-                    cf.getAttribute(Attribute.InnerClasses);\n+            InnerClassesAttribute innerClasses = cm.findAttribute(Attributes.INNER_CLASSES).orElse(null);\n@@ -207,2 +200,2 @@\n-            for (Attribute a : cf.attributes.attrs) {\n-                if (a instanceof InnerClasses_attribute) {\n+            for (Attribute<?> a : cm.attributes()) {\n+                if (a instanceof InnerClassesAttribute) {\n@@ -216,2 +209,1 @@\n-            checkEquals(cf.constant_pool.\n-                    getUTF8Info(innerClasses.attribute_name_index).value, \"InnerClasses\",\n+            checkEquals(innerClasses.attributeName(), \"InnerClasses\",\n@@ -221,1 +213,1 @@\n-            checkEquals(innerClasses.attribute_length,\n+            checkEquals(((BoundAttribute<?>)innerClasses).payloadLen(),\n@@ -223,1 +215,1 @@\n-            checkEquals(innerClasses.number_of_classes,\n+            checkEquals(innerClasses.classes().size(),\n@@ -226,5 +218,6 @@\n-            for (Info e : innerClasses.classes) {\n-                String baseName = cf.constant_pool.getClassInfo(\n-                        e.inner_class_info_index).getBaseName();\n-                if (cf.major_version >= 51 && e.inner_name_index == 0) {\n-                    checkEquals(e.outer_class_info_index, 0,\n+            for (InnerClassInfo e : innerClasses.classes()) {\n+                String baseName = e.innerClass().asInternalName();\n+                if (cm.majorVersion() >= 51 && e.innerClass().index() == 0) {\n+                    ClassEntry out = e.outerClass().orElse(null);\n+                    \/\/ The outer_class_info_index of sun.tools.classfile will return 0 if it is not a member of a class or interface.\n+                    checkEquals(out == null? 0: out.index(), 0,\n@@ -240,2 +233,6 @@\n-                checkEquals(e.inner_class_access_flags.getInnerClassFlags(),\n-                        class2Flags.get(className),\n+                \/\/Convert the Set<string> to Set<AccessFlag>\n+                Set<AccessFlag> accFlags = class2Flags.get(className).stream()\n+                        .map(str -> AccessFlag.valueOf(str.substring(str.indexOf(\"_\") + 1)))\n+                        .collect(Collectors.toSet());\n+                checkEquals(e.flags(),\n+                        accFlags,\n@@ -244,0 +241,5 @@\n+                    checkEquals(\n+                            e.innerClass().asInternalName(),\n+                            classToTest + \"$\" + className,\n+                            \"inner_class_info_index of \" + className);\n+                    if (e.outerClass().orElse(null) != null && e.outerClass().get().index() > 0) {\n@@ -245,6 +247,1 @@\n-                                cf.constant_pool.getClassInfo(e.inner_class_info_index).getBaseName(),\n-                                classToTest + \"$\" + className,\n-                                \"inner_class_info_index of \" + className);\n-                    if (e.outer_class_info_index > 0) {\n-                        checkEquals(\n-                                cf.constant_pool.getClassInfo(e.outer_class_info_index).getName(),\n+                                e.outerClass().get().name().stringValue(),\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerClassesTestBase.java","additions":31,"deletions":34,"binary":false,"changes":65,"status":"modified"}]}