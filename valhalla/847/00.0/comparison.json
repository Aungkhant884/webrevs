{"files":[{"patch":"@@ -623,1 +623,1 @@\n-SPECS_TOP := $(if $(filter true, $(IS_DRAFT)), <header class=\"draft-header\">$(DRAFT_TEXT)<\/header>)\n+SPECS_TOP := $(if $(filter true, $(IS_DRAFT)), <header class=\"draft-header\" role=\"banner\">$(DRAFT_TEXT)<\/header>)\n","filename":"make\/Docs.gmk","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1203,2 +1203,2 @@\n-    if (StressInlineTypeReturnedAsFields) {\n-      \/\/ TODO 8284443 Enable this for value class returns (L-type descriptor)\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n@@ -1207,4 +1207,3 @@\n-      movptr(rscratch1, Address(rscratch1, Method::const_offset()));\n-      load_unsigned_byte(rscratch1, Address(rscratch1, ConstMethod::result_type_offset()));\n-      cmpl(rscratch1, T_PRIMITIVE_OBJECT);\n-      jcc(Assembler::notEqual, skip_stress);\n+      movl(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      testl(rcx, Method::scalarized_return_flag());\n+      jcc(Assembler::zero, skip_stress);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":5,"deletions":6,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+  _transitive_interfaces = NULL;\n@@ -877,0 +878,26 @@\n+GrowableArray<ciInstanceKlass*>* ciInstanceKlass::transitive_interfaces() const{\n+  if (_transitive_interfaces == NULL) {\n+    const_cast<ciInstanceKlass*>(this)->compute_transitive_interfaces();\n+  }\n+  return _transitive_interfaces;\n+}\n+\n+void ciInstanceKlass::compute_transitive_interfaces() {\n+  GUARDED_VM_ENTRY(\n+          InstanceKlass* ik = get_instanceKlass();\n+          Array<InstanceKlass*>* interfaces = ik->transitive_interfaces();\n+          int orig_length = interfaces->length();\n+          Arena* arena = CURRENT_ENV->arena();\n+          int transitive_interfaces_len = orig_length + (is_interface() ? 1 : 0);\n+          GrowableArray<ciInstanceKlass*>* transitive_interfaces = new(arena)GrowableArray<ciInstanceKlass*>(arena, transitive_interfaces_len,\n+                                                                                                             0, NULL);\n+          for (int i = 0; i < orig_length; i++) {\n+            transitive_interfaces->append(CURRENT_ENV->get_instance_klass(interfaces->at(i)));\n+          }\n+          if (is_interface()) {\n+            transitive_interfaces->append(this);\n+          }\n+          _transitive_interfaces = transitive_interfaces;\n+  );\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.cpp","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -81,0 +81,1 @@\n+  GrowableArray<ciInstanceKlass*>* _transitive_interfaces;\n@@ -84,0 +85,1 @@\n+  void compute_transitive_interfaces();\n@@ -301,0 +303,1 @@\n+  GrowableArray<ciInstanceKlass*>* transitive_interfaces() const;\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2447,3 +2447,1 @@\n-\/\/ The promoted_flags parameter is used to pass relevant access_flags\n-\/\/ from the method back up to the containing klass. These flag values\n-\/\/ are added to klass's access_flags.\n+\/\/ The has_localvariable_table parameter is used to pass up the value to InstanceKlass.\n@@ -2456,1 +2454,1 @@\n-                                      AccessFlags* const promoted_flags,\n+                                      bool* const has_localvariable_table,\n@@ -2460,1 +2458,1 @@\n-  assert(promoted_flags != NULL, \"invariant\");\n+  assert(has_localvariable_table != NULL, \"invariant\");\n@@ -3063,1 +3061,1 @@\n-    promoted_flags->set_has_localvariable_table();\n+    *has_localvariable_table = true;\n@@ -3119,3 +3117,0 @@\n-\/\/ The promoted_flags parameter is used to pass relevant access_flags\n-\/\/ from the methods back up to the containing klass. These flag values\n-\/\/ are added to klass's access_flags.\n@@ -3127,1 +3122,1 @@\n-                                    AccessFlags* promoted_flags,\n+                                    bool* const has_localvariable_table,\n@@ -3132,1 +3127,1 @@\n-  assert(promoted_flags != NULL, \"invariant\");\n+  assert(has_localvariable_table != NULL, \"invariant\");\n@@ -3154,1 +3149,1 @@\n-                                    promoted_flags,\n+                                    has_localvariable_table,\n@@ -5828,0 +5823,4 @@\n+  if (_has_localvariable_table) {\n+    ik->set_has_localvariable_table(true);\n+  }\n+\n@@ -6125,0 +6124,1 @@\n+  _has_localvariable_table(false),\n@@ -6476,1 +6476,0 @@\n-  AccessFlags promoted_flags;\n@@ -6481,1 +6480,1 @@\n-                &promoted_flags,\n+                &_has_localvariable_table,\n@@ -6488,3 +6487,0 @@\n-  \/\/ promote flags from parse_methods() to the klass' flags\n-  _access_flags.add_promoted_flags(promoted_flags.as_int());\n-\n@@ -6589,1 +6585,4 @@\n-    _super_klass = (const InstanceKlass*)\n+    if (loader.is_null() && super_class_name == vmSymbols::java_lang_Object()) {\n+      _super_klass = vmClasses::Object_klass();\n+    } else {\n+      _super_klass = (const InstanceKlass*)\n@@ -6596,0 +6595,1 @@\n+    }\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":18,"deletions":18,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -200,0 +200,1 @@\n+  bool _has_localvariable_table;\n@@ -291,1 +292,1 @@\n-                       AccessFlags* const promoted_flags,\n+                       bool* const has_localvariable_table,\n@@ -298,1 +299,1 @@\n-                     AccessFlags* const promoted_flags,\n+                     bool* const has_localvariable_table,\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1794,1 +1794,1 @@\n-  return (JavaThread*)java_thread->address_field(_eetop_offset);\n+  return reinterpret_cast<JavaThread*>(java_thread->address_field(_eetop_offset));\n@@ -2572,1 +2572,2 @@\n-  char* buf = NEW_RESOURCE_ARRAY(char, buf_len + 64);\n+  const size_t buf_size = buf_len + 64;\n+  char* buf = NEW_RESOURCE_ARRAY(char, buf_size);\n@@ -2575,1 +2576,1 @@\n-  sprintf(buf, \"\\tat %s.%s(\", klass_name, method_name);\n+  size_t buf_off = os::snprintf_checked(buf, buf_size, \"\\tat %s.%s(\", klass_name, method_name);\n@@ -2580,1 +2581,1 @@\n-      sprintf(buf + (int)strlen(buf), \"%s@%s\/\", module_name, module_version);\n+      buf_off += os::snprintf_checked(buf + buf_off, buf_size - buf_off, \"%s@%s\/\", module_name, module_version);\n@@ -2582,1 +2583,1 @@\n-      sprintf(buf + (int)strlen(buf), \"%s\/\", module_name);\n+      buf_off += os::snprintf_checked(buf + buf_off, buf_size - buf_off, \"%s\/\", module_name);\n@@ -2597,1 +2598,1 @@\n-        sprintf(buf + (int)strlen(buf), \"%s:%d)\", source_file_name, line_number);\n+        buf_off += os::snprintf_checked(buf + buf_off, buf_size - buf_off, \"%s:%d)\", source_file_name, line_number);\n@@ -2600,1 +2601,1 @@\n-        sprintf(buf + (int)strlen(buf), \"%s)\", source_file_name);\n+        buf_off += os::snprintf_checked(buf + buf_off, buf_size - buf_off, \"%s)\", source_file_name);\n@@ -2603,1 +2604,1 @@\n-        sprintf(buf + (int)strlen(buf), \"Unknown Source)\");\n+        buf_off += os::snprintf_checked(buf + buf_off, buf_size - buf_off, \"Unknown Source)\");\n@@ -2607,1 +2608,1 @@\n-        sprintf(buf + (int)strlen(buf), \"(nmethod \" INTPTR_FORMAT \")\", (intptr_t)nm);\n+        os::snprintf_checked(buf + buf_off, buf_size - buf_off, \"(nmethod \" INTPTR_FORMAT \")\", (intptr_t)nm);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":10,"deletions":9,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -186,0 +186,5 @@\n+\/\/ Helper function\n+inline ClassLoaderData* class_loader_data(Handle class_loader) {\n+  return ClassLoaderData::class_loader_data(class_loader());\n+}\n+\n@@ -1124,1 +1129,1 @@\n-      pkg_entry = ClassLoaderData::class_loader_data(class_loader())->packages()->lookup_only(pkg_name);\n+      pkg_entry = class_loader_data(class_loader)->packages()->lookup_only(pkg_name);\n@@ -1240,1 +1245,1 @@\n-  assert(shared_nest_host->class_loader_data() == ClassLoaderData::class_loader_data(class_loader()), \"mismatched class loader data\");\n+  assert(shared_nest_host->class_loader_data() == class_loader_data(class_loader), \"mismatched class loader data\");\n@@ -1306,1 +1311,1 @@\n-  ClassLoaderData* loader_data = ClassLoaderData::class_loader_data(class_loader());\n+  ClassLoaderData* loader_data = class_loader_data(class_loader);\n@@ -1343,1 +1348,0 @@\n-    ClassLoaderData *loader_data = class_loader_data(class_loader);\n@@ -1348,1 +1352,1 @@\n-      pkg_entry = loader_data->packages()->lookup_only(pkg_name);\n+      pkg_entry = class_loader_data(class_loader)->packages()->lookup_only(pkg_name);\n@@ -1489,1 +1493,1 @@\n-    loaded_class->class_loader() != class_loader()) {\n+      loaded_class->class_loader() != class_loader()) {\n@@ -1491,1 +1495,2 @@\n-    check_constraints(loaded_class, class_loader, false, CHECK_NULL);\n+    ClassLoaderData* loader_data = class_loader_data(class_loader);\n+    check_constraints(loaded_class, loader_data, false, CHECK_NULL);\n@@ -1498,1 +1503,0 @@\n-    ClassLoaderData* loader_data = class_loader_data(class_loader);\n@@ -1504,1 +1508,1 @@\n-      update_dictionary(THREAD, loaded_class, class_loader);\n+      update_dictionary(THREAD, loaded_class, loader_data);\n@@ -1548,3 +1552,1 @@\n-  Symbol*  name_h = k->name();\n-  Dictionary* dictionary = loader_data->dictionary();\n-  check_constraints(k, class_loader, true, CHECK);\n+  check_constraints(k, loader_data, true, CHECK);\n@@ -1574,1 +1576,1 @@\n-    update_dictionary(THREAD, k, class_loader);\n+    update_dictionary(THREAD, k, loader_data);\n@@ -1607,1 +1609,1 @@\n-  Symbol*  name_h = k->name(); \/\/ passed in class_name may be null\n+  Symbol* name_h = k->name();\n@@ -1804,1 +1806,1 @@\n-                                         Handle class_loader,\n+                                         ClassLoaderData* loader_data,\n@@ -1812,2 +1814,1 @@\n-    Symbol *name = k->name();\n-    ClassLoaderData *loader_data = class_loader_data(class_loader);\n+    Symbol* name = k->name();\n@@ -1834,1 +1835,1 @@\n-      if (LoaderConstraintTable::check_or_update(k, class_loader, name) == false) {\n+      if (LoaderConstraintTable::check_or_update(k, loader_data, name) == false) {\n@@ -1839,2 +1840,2 @@\n-        Klass *existing_klass = LoaderConstraintTable::find_constrained_klass(name, class_loader);\n-        if (existing_klass != NULL && existing_klass->class_loader() != class_loader()) {\n+        Klass *existing_klass = LoaderConstraintTable::find_constrained_klass(name, loader_data);\n+        if (existing_klass != NULL && existing_klass->class_loader_data() != loader_data) {\n@@ -1863,1 +1864,1 @@\n-                                         Handle class_loader) {\n+                                         ClassLoaderData* loader_data) {\n@@ -1866,2 +1867,1 @@\n-  Symbol*  name  = k->name();\n-  ClassLoaderData *loader_data = class_loader_data(class_loader);\n+  Symbol* name  = k->name();\n@@ -1869,2 +1869,1 @@\n-  {\n-    MutexLocker mu1(SystemDictionary_lock);\n+  MutexLocker mu1(SystemDictionary_lock);\n@@ -1872,7 +1871,5 @@\n-    \/\/ Make a new dictionary entry.\n-    Dictionary* dictionary = loader_data->dictionary();\n-    InstanceKlass* sd_check = dictionary->find_class(current, name);\n-    if (sd_check == NULL) {\n-      dictionary->add_klass(current, name, k);\n-    }\n-    SystemDictionary_lock->notify_all();\n+  \/\/ Make a new dictionary entry.\n+  Dictionary* dictionary = loader_data->dictionary();\n+  InstanceKlass* sd_check = dictionary->find_class(current, name);\n+  if (sd_check == NULL) {\n+    dictionary->add_klass(current, name, k);\n@@ -1880,0 +1877,1 @@\n+  SystemDictionary_lock->notify_all();\n@@ -1910,1 +1908,1 @@\n-      klass = LoaderConstraintTable::find_constrained_klass(ss.as_symbol(), class_loader);\n+      klass = LoaderConstraintTable::find_constrained_klass(ss.as_symbol(), class_loader_data(class_loader));\n@@ -1923,1 +1921,1 @@\n-    klass = LoaderConstraintTable::find_constrained_klass(class_name, class_loader);\n+    klass = LoaderConstraintTable::find_constrained_klass(class_name, class_loader_data(class_loader));\n@@ -1963,2 +1961,2 @@\n-    bool result = LoaderConstraintTable::add_entry(constraint_name, klass1, class_loader1,\n-                                                   klass2, class_loader2);\n+    bool result = LoaderConstraintTable::add_entry(constraint_name, klass1, loader_data1,\n+                                                   klass2, loader_data2);\n@@ -2526,4 +2524,0 @@\n-ClassLoaderData* SystemDictionary::class_loader_data(Handle class_loader) {\n-  return ClassLoaderData::class_loader_data(class_loader());\n-}\n-\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":34,"deletions":40,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -239,0 +239,3 @@\n+  oop resolved_reference_at(int obj_index) const;\n+  oop set_resolved_reference_at(int index, oop new_value);\n+\n@@ -486,1 +489,1 @@\n-    return resolved_references()->obj_at(obj_index);\n+    return resolved_reference_at(obj_index);\n","filename":"src\/hotspot\/share\/oops\/constantPool.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -491,1 +491,1 @@\n-    \/\/ Oop is tagged, must be an InlineKlass oop\n+    \/\/ Return value is tagged, must be an InlineKlass pointer\n@@ -498,6 +498,3 @@\n-#ifdef ASSERT\n-  \/\/ Oop is not tagged, must be a valid oop\n-  if (VerifyOops) {\n-    oopDesc::verify(cast_to_oop(ptr));\n-  }\n-#endif\n+  \/\/ Return value is not tagged, must be a valid oop\n+  assert(oopDesc::is_oop_or_null(cast_to_oop(ptr), true),\n+         \"Bad oop return: \" PTR_FORMAT, ptr);\n","filename":"src\/hotspot\/share\/oops\/inlineKlass.cpp","additions":4,"deletions":7,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -656,4 +656,4 @@\n-  \/\/ Can't release the constant pool here because the constant pool can be\n-  \/\/ deallocated separately from the InstanceKlass for default methods and\n-  \/\/ redefine classes.\n-  release_C_heap_structures(\/* release_constant_pool *\/ false);\n+  \/\/ Can't release the constant pool or MethodData C heap data here because the constant\n+  \/\/ pool can be deallocated separately from the InstanceKlass for default methods and\n+  \/\/ redefine classes.  MethodData can also be released separately.\n+  release_C_heap_structures(\/* release_sub_metadata *\/ false);\n@@ -2365,9 +2365,2 @@\n-      \/\/ cache can grow so we have to be more careful\n-      if (Threads::number_of_threads() == 0 ||\n-          SafepointSynchronize::is_at_safepoint()) {\n-        \/\/ we're single threaded or at a safepoint - no locking needed\n-        get_jmethod_id_length_value(jmeths, idnum, &length, &id);\n-      } else {\n-        MutexLocker ml(JmethodIdCreation_lock, Mutex::_no_safepoint_check_flag);\n-        get_jmethod_id_length_value(jmeths, idnum, &length, &id);\n-      }\n+      MutexLocker ml(JmethodIdCreation_lock, Mutex::_no_safepoint_check_flag);\n+      get_jmethod_id_length_value(jmeths, idnum, &length, &id);\n@@ -2383,2 +2376,2 @@\n-    \/\/ This function can be called by the VMThread so we have to do all\n-    \/\/ things that might block on a safepoint before grabbing the lock.\n+    \/\/ This function can be called by the VMThread or GC worker threads so we\n+    \/\/ have to do all things that might block on a safepoint before grabbing the lock.\n@@ -2403,18 +2396,1 @@\n-    jmethodID new_id = NULL;\n-    if (method_h->is_old() && !method_h->is_obsolete()) {\n-      \/\/ The method passed in is old (but not obsolete), we need to use the current version\n-      Method* current_method = method_with_idnum((int)idnum);\n-      assert(current_method != NULL, \"old and but not obsolete, so should exist\");\n-      new_id = Method::make_jmethod_id(class_loader_data(), current_method);\n-    } else {\n-      \/\/ It is the current version of the method or an obsolete method,\n-      \/\/ use the version passed in\n-      new_id = Method::make_jmethod_id(class_loader_data(), method_h());\n-    }\n-\n-    if (Threads::number_of_threads() == 0 ||\n-        SafepointSynchronize::is_at_safepoint()) {\n-      \/\/ we're single threaded or at a safepoint - no locking needed\n-      id = get_jmethod_id_fetch_or_update(idnum, new_id, new_jmeths,\n-                                          &to_dealloc_id, &to_dealloc_jmeths);\n-    } else {\n+    {\n@@ -2422,0 +2398,12 @@\n+      jmethodID new_id = NULL;\n+      if (method_h->is_old() && !method_h->is_obsolete()) {\n+        \/\/ The method passed in is old (but not obsolete), we need to use the current version\n+        Method* current_method = method_with_idnum((int)idnum);\n+        assert(current_method != NULL, \"old and but not obsolete, so should exist\");\n+        new_id = Method::make_jmethod_id(class_loader_data(), current_method);\n+      } else {\n+        \/\/ It is the current version of the method or an obsolete method,\n+        \/\/ use the version passed in\n+        new_id = Method::make_jmethod_id(class_loader_data(), method_h());\n+      }\n+\n@@ -2471,3 +2459,1 @@\n-  assert(Threads::number_of_threads() == 0 ||\n-         SafepointSynchronize::is_at_safepoint() ||\n-         JmethodIdCreation_lock->owned_by_self(), \"sanity check\");\n+  assert(JmethodIdCreation_lock->owned_by_self(), \"sanity check\");\n@@ -2903,2 +2889,2 @@\n-\/\/ Called also by InstanceKlass::deallocate_contents, with false for release_constant_pool.\n-void InstanceKlass::release_C_heap_structures(bool release_constant_pool) {\n+\/\/ Called also by InstanceKlass::deallocate_contents, with false for release_sub_metadata.\n+void InstanceKlass::release_C_heap_structures(bool release_sub_metadata) {\n@@ -2909,1 +2895,3 @@\n-  methods_do(method_release_C_heap_structures);\n+  if (release_sub_metadata) {\n+    methods_do(method_release_C_heap_structures);\n+  }\n@@ -2949,1 +2937,1 @@\n-  if (release_constant_pool) {\n+  if (release_sub_metadata) {\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":28,"deletions":40,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -383,0 +383,3 @@\n+  bool has_localvariable_table() const     { return _misc_status.has_localvariable_table(); }\n+  void set_has_localvariable_table(bool b) { _misc_status.set_has_localvariable_table(b); }\n+\n@@ -1122,1 +1125,1 @@\n-  virtual void release_C_heap_structures(bool release_constant_pool = true);\n+  virtual void release_C_heap_structures(bool release_sub_metadata = true);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -713,1 +713,1 @@\n-      if (t->is_inlinetypeptr() && method()->is_scalarized_arg(arg_num)) {\n+      if (t->is_inlinetypeptr() && !method()->get_Method()->mismatch() && method()->is_scalarized_arg(arg_num)) {\n@@ -828,1 +828,1 @@\n-          vt->set_is_buffered();\n+          vt->set_is_buffered(kit.gvn());\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -398,0 +398,41 @@\n+#ifdef ASSERT\n+\/\/ Is this region in an infinite subgraph?\n+\/\/ (no path to root except through false NeverBranch exit)\n+bool RegionNode::is_in_infinite_subgraph() {\n+  ResourceMark rm;\n+  Unique_Node_List worklist;\n+  worklist.push(this);\n+  return RegionNode::are_all_nodes_in_infinite_subgraph(worklist);\n+}\n+\n+\/\/ Are all nodes in worklist in infinite subgraph?\n+\/\/ (no path to root except through false NeverBranch exit)\n+\/\/ worklist is directly used for the traversal\n+bool RegionNode::are_all_nodes_in_infinite_subgraph(Unique_Node_List& worklist) {\n+  \/\/ BFS traversal down the CFG, except through NeverBranch exits\n+  for (uint i = 0; i < worklist.size(); ++i) {\n+    Node* n = worklist.at(i);\n+    assert(n->is_CFG(), \"only traverse CFG\");\n+    if (n->is_Root()) {\n+      \/\/ Found root -> there was an exit!\n+      return false;\n+    } else if (n->is_NeverBranch()) {\n+      \/\/ Only follow the loop-internal projection, not the NeverBranch exit\n+      ProjNode* proj = n->as_NeverBranch()->proj_out_or_null(0);\n+      assert(proj != nullptr, \"must find loop-internal projection of NeverBranch\");\n+      worklist.push(proj);\n+    } else {\n+      \/\/ Traverse all CFG outputs\n+      for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+        Node* use = n->fast_out(i);\n+        if (use->is_CFG()) {\n+          worklist.push(use);\n+        }\n+      }\n+    }\n+  }\n+  \/\/ No exit found for any loop -> all are infinite\n+  return true;\n+}\n+#endif \/\/ASSERT\n+\n@@ -993,8 +1034,0 @@\n-  const TypePtr *t = adr_type();\n-  assert(type() == Type::MEMORY &&\n-         (t == TypePtr::BOTTOM || t == TypeRawPtr::BOTTOM ||\n-          t->isa_oopptr() && !t->is_oopptr()->is_known_instance() &&\n-          t->is_oopptr()->cast_to_exactness(true)\n-           ->is_oopptr()->cast_to_ptr_type(t_oop->ptr())\n-           ->is_oopptr()->cast_to_instance_id(t_oop->instance_id()) == t_oop),\n-         \"bottom or raw memory required\");\n@@ -1200,16 +1233,0 @@\n-  \/\/ Until we have harmony between classes and interfaces in the type\n-  \/\/ lattice, we must tread carefully around phis which implicitly\n-  \/\/ convert the one to the other.\n-  const TypePtr* ttp = _type->make_ptr();\n-  const TypeInstPtr* ttip = (ttp != NULL) ? ttp->isa_instptr() : NULL;\n-  const TypeInstKlassPtr* ttkp = (ttp != NULL) ? ttp->isa_instklassptr() : NULL;\n-  bool is_intf = false;\n-  if (ttip != NULL) {\n-    if (ttip->is_interface())\n-      is_intf = true;\n-  }\n-  if (ttkp != NULL) {\n-    if (ttkp->is_interface())\n-      is_intf = true;\n-  }\n-\n@@ -1222,14 +1239,0 @@\n-      \/\/ We assume that each input of an interface-valued Phi is a true\n-      \/\/ subtype of that interface.  This might not be true of the meet\n-      \/\/ of all the input types.  The lattice is not distributive in\n-      \/\/ such cases.  Ward off asserts in type.cpp by refusing to do\n-      \/\/ meets between interfaces and proper classes.\n-      const TypePtr* tip = ti->make_ptr();\n-      const TypeInstPtr* tiip = (tip != NULL) ? tip->isa_instptr() : NULL;\n-      if (tiip) {\n-        bool ti_is_intf = false;\n-        if (tiip->is_interface())\n-          ti_is_intf = true;\n-        if (is_intf != ti_is_intf)\n-          { t = _type; break; }\n-      }\n@@ -1259,25 +1262,3 @@\n-\n-    \/\/ Check for evil case of 't' being a class and '_type' expecting an\n-    \/\/ interface.  This can happen because the bytecodes do not contain\n-    \/\/ enough type info to distinguish a Java-level interface variable\n-    \/\/ from a Java-level object variable.  If we meet 2 classes which\n-    \/\/ both implement interface I, but their meet is at 'j\/l\/O' which\n-    \/\/ doesn't implement I, we have no way to tell if the result should\n-    \/\/ be 'I' or 'j\/l\/O'.  Thus we'll pick 'j\/l\/O'.  If this then flows\n-    \/\/ into a Phi which \"knows\" it's an Interface type we'll have to\n-    \/\/ uplift the type.\n-    if (!t->empty() && ttip && ttip->is_interface()) {\n-      assert(ft == _type, \"\"); \/\/ Uplift to interface\n-    } else if (!t->empty() && ttkp && ttkp->is_interface()) {\n-      assert(ft == _type, \"\"); \/\/ Uplift to interface\n-    } else {\n-      \/\/ We also have to handle 'evil cases' of interface- vs. class-arrays\n-      Type::get_arrays_base_elements(jt, _type, NULL, &ttip);\n-      if (!t->empty() && ttip != NULL && ttip->is_interface()) {\n-          assert(ft == _type, \"\");   \/\/ Uplift to array of interface\n-      } else {\n-        \/\/ Otherwise it's something stupid like non-overlapping int ranges\n-        \/\/ found on dying counted loops.\n-        assert(ft == Type::TOP, \"\"); \/\/ Canonical empty value\n-      }\n-    }\n+    \/\/ Otherwise it's something stupid like non-overlapping int ranges\n+    \/\/ found on dying counted loops.\n+    assert(ft == Type::TOP, \"\"); \/\/ Canonical empty value\n@@ -1288,25 +1269,0 @@\n-    \/\/ If we have an interface-typed Phi and we narrow to a class type, the join\n-    \/\/ should report back the class.  However, if we have a J\/L\/Object\n-    \/\/ class-typed Phi and an interface flows in, it's possible that the meet &\n-    \/\/ join report an interface back out.  This isn't possible but happens\n-    \/\/ because the type system doesn't interact well with interfaces.\n-    const TypePtr *jtp = jt->make_ptr();\n-    const TypeInstPtr *jtip = (jtp != NULL) ? jtp->isa_instptr() : NULL;\n-    const TypeInstKlassPtr *jtkp = (jtp != NULL) ? jtp->isa_instklassptr() : NULL;\n-    if (jtip && ttip) {\n-      if (jtip->is_interface() &&\n-          !ttip->is_interface()) {\n-        assert(ft == ttip->cast_to_ptr_type(jtip->ptr()) ||\n-               ft->isa_narrowoop() && ft->make_ptr() == ttip->cast_to_ptr_type(jtip->ptr()), \"\");\n-        jt = ft;\n-      }\n-    }\n-    if (jtkp && ttkp) {\n-      if (jtkp->is_interface() &&\n-          !jtkp->klass_is_exact() && \/\/ Keep exact interface klass (6894807)\n-          ttkp->is_loaded() && !ttkp->is_interface()) {\n-        assert(ft == ttkp->cast_to_ptr_type(jtkp->ptr()) ||\n-               ft->isa_narrowklass() && ft->make_ptr() == ttkp->cast_to_ptr_type(jtkp->ptr()), \"\");\n-        jt = ft;\n-      }\n-    }\n@@ -2528,1 +2484,1 @@\n-  if (EnableValhalla && (_type->isa_ptr() || _type->isa_inlinetype()) && req() > 2) {\n+  if (EnableValhalla && _type->isa_ptr() && req() > 2) {\n@@ -2538,4 +2494,1 @@\n-    \/\/ TODO 8284443 We need to prevent endless pushing through\n-    \/\/ TODO 8284443 We could revisit the same node over and over again, right?\n-    \/\/ TestLWorld -XX:+UseZGC -DScenarios=0 -DTest=test69\n-    \/\/ TestLWorld -XX:-TieredCompilation -XX:-DoEscapeAnalysis -XX:+AlwaysIncrementalInline\n+    \/\/ TODO 8302217 We need to prevent endless pushing through\n@@ -2579,1 +2532,1 @@\n-        } else if (n->is_Phi() && can_reshape && (n->bottom_type()->isa_ptr() || n->bottom_type()->isa_inlinetype())) {\n+        } else if (n->is_Phi() && can_reshape && n->bottom_type()->isa_ptr()) {\n@@ -2597,1 +2550,1 @@\n-\/\/ TODO 8275400\n+\/\/ TODO 8302217\n@@ -2700,0 +2653,1 @@\n+  const Type*        ptype = cached_vbox->field_value(0)->bottom_type();\n@@ -2701,3 +2655,3 @@\n-  Node* new_payload_phi = clone_through_phi(root_phi, cached_vbox->field_value(0)->bottom_type(), 3, igvn);\n-  Node* new_vector_phi = clone_through_phi(new_payload_phi, vtype, 3, igvn);\n-  Node* new_vbox_phi = clone_through_phi(root_phi, btype, 1, igvn);\n+  Node* new_payload_phi = clone_through_phi(root_phi, ptype, InlineTypeNode::get_Values_idx(), igvn);\n+  Node* new_vector_phi = clone_through_phi(new_payload_phi, vtype, InlineTypeNode::get_Values_idx(), igvn);\n+  Node* new_vbox_phi = clone_through_phi(root_phi, btype, InlineTypeNode::get_Oop_idx(), igvn);\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":53,"deletions":99,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -94,0 +94,4 @@\n+#ifdef ASSERT\n+  bool is_in_infinite_subgraph();\n+  static bool are_all_nodes_in_infinite_subgraph(Unique_Node_List& worklist);\n+#endif \/\/ASSERT\n@@ -598,1 +602,4 @@\n-  NeverBranchNode( Node *ctrl ) : MultiBranchNode(1) { init_req(0,ctrl); }\n+  NeverBranchNode(Node* ctrl) : MultiBranchNode(1) {\n+    init_req(0, ctrl);\n+    init_class_id(Class_NeverBranch);\n+  }\n@@ -619,0 +626,1 @@\n+    init_class_id(Class_Blackhole);\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -617,0 +617,1 @@\n+                  _has_circular_inline_type(false),\n@@ -911,0 +912,1 @@\n+    _has_circular_inline_type(false),\n@@ -1413,2 +1415,2 @@\n-    if (ta->is_flat() && ta->elem() != TypeInlineType::BOTTOM && _flattened_accesses_share_alias) {\n-      const TypeAry *tary = TypeAry::make(TypeInlineType::BOTTOM, ta->size());\n+    if (ta->is_flat() && ta->elem() != TypeInstPtr::BOTTOM && _flattened_accesses_share_alias) {\n+      const TypeAry* tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size(), \/* stable= *\/ false, \/* flat= *\/ true);\n@@ -1487,1 +1489,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, Type::Offset(offset), canonical_holder->flatten_array(), to->instance_id());\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, Type::Offset(offset), to->instance_id());\n@@ -1516,1 +1518,0 @@\n-\n@@ -1718,1 +1719,1 @@\n-      if (elemtype->isa_inlinetype() &&\n+      if (flat->is_flat() &&\n@@ -1771,1 +1772,1 @@\n-        assert(flat->is_aryptr()->is_flat(), \"must be a flat array\");\n+        assert(flat->is_flat(), \"must be a flat array\");\n@@ -1926,1 +1927,0 @@\n-    assert(n == ret_val || !n->is_InlineType(), \"chain of inline type nodes\");\n@@ -2023,0 +2023,2 @@\n+        } else if (u->is_Phi()) {\n+          \/\/ TODO 8302217 Remove this once InlineTypeNodes are reliably pushed through\n@@ -2027,1 +2029,2 @@\n-          vt->dump(-3);\n+          vt->dump(0);\n+          u->dump(0);\n@@ -2095,2 +2098,1 @@\n-          ace->_adr_type->isa_aryptr() &&\n-          ace->_adr_type->is_aryptr()->is_flat()) {\n+          ace->_adr_type->is_flat()) {\n@@ -2221,2 +2223,2 @@\n-                  const Type* adr_type = get_adr_type(j);\n-                  if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat() || j == (uint)index) {\n+                  const TypePtr* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n@@ -2251,2 +2253,2 @@\n-                const Type* adr_type = get_adr_type(j);\n-                if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat() || j == (uint)index) {\n+                const TypePtr* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n@@ -2294,2 +2296,2 @@\n-        const Type* adr_type = get_adr_type(j);\n-        if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat()) {\n+        const TypePtr* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_flat()) {\n@@ -3712,2 +3714,2 @@\n-          const Type* adr_type = get_adr_type(i);\n-          if (adr_type->isa_aryptr() && adr_type->is_aryptr()->is_flat()) {\n+          const TypePtr* adr_type = get_adr_type(i);\n+          if (adr_type->is_flat()) {\n@@ -4887,2 +4889,2 @@\n-Compile::SubTypeCheckResult Compile::static_subtype_check(const TypeKlassPtr* superk, const TypeKlassPtr* subk) {\n-  if (StressReflectiveCode) {\n+Compile::SubTypeCheckResult Compile::static_subtype_check(const TypeKlassPtr* superk, const TypeKlassPtr* subk, bool skip) {\n+  if (skip) {\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":22,"deletions":20,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -1034,3 +1034,0 @@\n-  BasicType rtype = T_ILLEGAL;\n-  int       rsize = 0;\n-\n@@ -1039,3 +1036,0 @@\n-    rtype = Bytecodes::result_type(code); \/\/ checkcast=P, athrow=V\n-    if (rtype < T_CONFLICT)\n-      rsize = type2size[rtype];\n@@ -1044,0 +1038,6 @@\n+  auto rsize = [&]() {\n+    assert(code != Bytecodes::_illegal, \"code is illegal!\");\n+    BasicType rtype = Bytecodes::result_type(code); \/\/ checkcast=P, athrow=V\n+    return (rtype < T_CONFLICT) ? type2size[rtype] : 0;\n+  };\n+\n@@ -1070,3 +1070,1 @@\n-      int size = field->is_multifield_base() ?\n-                   (InlineTypeNode::is_multifield_scalarized(field) ? field->type()->elem_word_count() : 1)\n-                   : field->type()->size();\n+      int size = InlineTypeNode::stack_size_for_field(field);\n@@ -1106,2 +1104,2 @@\n-      assert(rsize == 1, \"\");\n-      depth = rsize - inputs;\n+      assert(rsize() == 1, \"\");\n+      depth = 1 - inputs;\n@@ -1114,3 +1112,1 @@\n-    int size = field->is_multifield_base() ?\n-                 (InlineTypeNode::is_multifield_scalarized(field) ? field->type()->elem_word_count() : 1)\n-                   : field->type()->size();\n+    int size = InlineTypeNode::stack_size_for_field(field);\n@@ -1118,1 +1114,1 @@\n-    depth = rsize - inputs;\n+    depth = rsize() - inputs;\n@@ -1127,2 +1123,2 @@\n-    assert(rsize == -depth, \"\");\n-    inputs = rsize;\n+    assert(rsize() == -depth, \"\");\n+    inputs = -depth;\n@@ -1139,1 +1135,1 @@\n-    inputs = rsize - depth;\n+    inputs = rsize() - depth;\n@@ -1611,1 +1607,2 @@\n-                                bool unsafe) {\n+                                bool unsafe,\n+                                int barrier_data) {\n@@ -1626,0 +1623,1 @@\n+  st->as_Store()->set_barrier_data(barrier_data);\n@@ -1841,1 +1839,2 @@\n-    if (t->is_inlinetypeptr() && call->method()->is_scalarized_arg(arg_num)) {\n+    \/\/ TODO 8284443 A static call to a mismatched method should still be scalarized\n+    if (t->is_inlinetypeptr() && !call->method()->get_Method()->mismatch() && call->method()->is_scalarized_arg(arg_num)) {\n@@ -1853,0 +1852,3 @@\n+      \/\/ Register an evol dependency on the callee method to make sure that this method is deoptimized and\n+      \/\/ re-compiled with a non-scalarized calling convention if the callee method is later marked as mismatched.\n+      C->dependencies()->assert_evol_method(call->method());\n@@ -1928,0 +1930,3 @@\n+    if (call->method()->return_type()->is_inlinetype()) {\n+      ret = InlineTypeNode::make_from_oop(this, ret, call->method()->return_type()->as_inline_klass(), call->method()->signature()->returns_null_free_inline_type());\n+    }\n@@ -2300,1 +2305,1 @@\n-    const TypeKlassPtr* tklass = TypeKlassPtr::make(exact_kls);\n+    const TypeKlassPtr* tklass = TypeKlassPtr::make(exact_kls, Type::trust_interfaces);\n@@ -2766,1 +2771,1 @@\n-  gvn.transform(cmp);\n+  cmp = gvn.transform(cmp);\n@@ -2988,1 +2993,1 @@\n-  const TypeKlassPtr* tklass = TypeKlassPtr::make(klass);\n+  const TypeKlassPtr* tklass = TypeKlassPtr::make(klass, Type::trust_interfaces);\n@@ -3028,1 +3033,1 @@\n-  const TypeKlassPtr* tklass = TypeKlassPtr::make(klass);\n+  const TypeKlassPtr* tklass = TypeKlassPtr::make(klass, Type::trust_interfaces)->try_improve();\n@@ -3170,1 +3175,1 @@\n-        C->static_subtype_check(require_klass, TypeKlassPtr::make(exact_kls)) == Compile::SSC_always_true) {\n+        C->static_subtype_check(require_klass, TypeKlassPtr::make(exact_kls, Type::trust_interfaces)) == Compile::SSC_always_true) {\n@@ -3355,2 +3360,2 @@\n-  const TypeKlassPtr* tk = _gvn.type(superklass)->is_klassptr();\n-  const TypeOopPtr* toop = tk->cast_to_exactness(false)->as_instance_type();\n+  const TypeKlassPtr *tk = _gvn.type(superklass)->is_klassptr()->try_improve();\n+  const TypeOopPtr *toop = tk->cast_to_exactness(false)->as_instance_type();\n@@ -3373,1 +3378,1 @@\n-      kptr = TypeInstKlassPtr::make(TypePtr::NotNull, vk, Type::Offset(0), vk->flatten_array());\n+      kptr = TypeInstKlassPtr::make(TypePtr::NotNull, vk, Type::Offset(0));\n@@ -4247,1 +4252,1 @@\n-      ciInlineKlass* vk = ary_ptr->elem()->make_oopptr()->inline_klass();\n+      ciInlineKlass* vk = ary_ptr->elem()->inline_klass();\n@@ -4464,1 +4469,1 @@\n-                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, true, true),\n+                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, false, true, true),\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":36,"deletions":31,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,10 +37,6 @@\n-uint InlineTypeNode::size_of() const {\n-  return sizeof(*this);\n-}\n-\n-uint InlineTypeNode::hash() const {\n-  return TypeNode::hash() + _is_buffered;\n-}\n-\n-bool InlineTypeNode::cmp(const Node& n) const {\n-  return TypeNode::cmp(n) && ((InlineTypeNode&)n)._is_buffered == _is_buffered;\n+int  InlineTypeNode::stack_size_for_field(ciField* field) {\n+  return field->is_multifield_base()\n+             ? (InlineTypeNode::is_multifield_scalarized(field)\n+                    ? field->type()->elem_word_count()\n+                    : 1)\n+             : field->type()->size();\n@@ -81,6 +77,3 @@\n-  if (vt->is_InlineType()) {\n-    \/\/ Use nullable type\n-    const Type* t = Type::get_const_type(inline_klass());\n-    gvn->set_type(vt, t);\n-    vt->as_InlineType()->set_type(t);\n-  }\n+  const Type* t = Type::get_const_type(inline_klass());\n+  gvn->set_type(vt, t);\n+  vt->as_InlineType()->set_type(t);\n@@ -89,3 +82,2 @@\n-  const Type* phi_type = Type::get_const_type(inline_klass());\n-  PhiNode* oop = PhiNode::make(region, vt->get_oop(), phi_type);\n-  gvn->set_type(oop, phi_type);\n+  PhiNode* oop = PhiNode::make(region, vt->get_oop(), t);\n+  gvn->set_type(oop, t);\n@@ -95,0 +87,7 @@\n+  \/\/ Create a PhiNode for merging the is_buffered values\n+  t = Type::get_const_basic_type(T_BOOLEAN);\n+  Node* is_buffered_node = PhiNode::make(region, vt->get_is_buffered(), t);\n+  gvn->set_type(is_buffered_node, t);\n+  gvn->record_for_igvn(is_buffered_node);\n+  vt->set_req(IsBuffered, is_buffered_node);\n+\n@@ -100,3 +99,3 @@\n-    phi_type = Type::get_const_basic_type(T_BOOLEAN);\n-    is_init_node = PhiNode::make(region, vt->get_is_init(), phi_type);\n-    gvn->set_type(is_init_node, phi_type);\n+    t = Type::get_const_basic_type(T_BOOLEAN);\n+    is_init_node = PhiNode::make(region, vt->get_is_init(), t);\n+    gvn->set_type(is_init_node, t);\n@@ -111,1 +110,5 @@\n-    if (value->is_InlineType()) {\n+    \/\/ We limit scalarization for inline types with circular fields and can therefore observe nodes\n+    \/\/ of the same type but with different scalarization depth during IGVN. To avoid inconsistencies\n+    \/\/ during merging, make sure that we only create Phis for fields that are guaranteed to be scalarized.\n+    bool no_circularity = !gvn->C->has_circular_inline_type() || !gvn->is_IterGVN() || field_is_flattened(i);\n+    if (value->is_InlineType() && no_circularity) {\n@@ -115,1 +118,1 @@\n-      phi_type = Type::get_const_type(type);\n+      t = Type::get_const_type(type);\n@@ -118,1 +121,1 @@\n-        phi_type = TypeVect::make(phi_type, vt->secondary_fields_count(i));\n+        t = TypeVect::make(t, vt->secondary_fields_count(i));\n@@ -120,2 +123,2 @@\n-      value = PhiNode::make(region, value, phi_type);\n-      gvn->set_type(value, phi_type);\n+      value = PhiNode::make(region, value, t);\n+      gvn->set_type(value, t);\n@@ -126,1 +129,0 @@\n-  gvn->set_type(vt, vt->bottom_type());\n@@ -154,1 +156,0 @@\n-  _is_buffered = _is_buffered && other->_is_buffered;\n@@ -162,0 +163,8 @@\n+  \/\/ Merge is_buffered inputs\n+  phi = get_is_buffered()->as_Phi();\n+  phi->set_req(pnum, other->get_is_buffered());\n+  if (transform) {\n+    set_req(IsBuffered, gvn->transform(phi));\n+  }\n+\n+  \/\/ Merge is_init inputs\n@@ -198,0 +207,4 @@\n+  phi = get_is_buffered()->as_Phi();\n+  phi->add_req(NULL);\n+  assert(phi->req() == region->req(), \"must be same size as region\");\n+\n@@ -410,1 +423,44 @@\n-void InlineTypeNode::load(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, int holder_offset, DecoratorSet decorators) {\n+\/\/ We limit scalarization for inline types with circular fields and can therefore observe\n+\/\/ nodes of same type but with different scalarization depth during GVN. This method adjusts\n+\/\/ the scalarization depth to avoid inconsistencies during merging.\n+InlineTypeNode* InlineTypeNode::adjust_scalarization_depth(GraphKit* kit) {\n+  if (!kit->C->has_circular_inline_type()) {\n+    return this;\n+  }\n+  GrowableArray<ciType*> visited;\n+  visited.push(inline_klass());\n+  return adjust_scalarization_depth_impl(kit, visited);\n+}\n+\n+InlineTypeNode* InlineTypeNode::adjust_scalarization_depth_impl(GraphKit* kit, GrowableArray<ciType*>& visited) {\n+  InlineTypeNode* val = this;\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* value = field_value(i);\n+    Node* new_value = value;\n+    ciType* ft = field_type(i);\n+    if (value->is_InlineType()) {\n+      if (!field_is_flattened(i) && visited.contains(ft)) {\n+        new_value = value->as_InlineType()->buffer(kit)->get_oop();\n+      } else {\n+        int old_len = visited.length();\n+        visited.push(ft);\n+        new_value = value->as_InlineType()->adjust_scalarization_depth_impl(kit, visited);\n+        visited.trunc_to(old_len);\n+      }\n+    } else if (ft->is_inlinetype() && !visited.contains(ft)) {\n+      int old_len = visited.length();\n+      visited.push(ft);\n+      new_value = make_from_oop_impl(kit, value, ft->as_inline_klass(), field_is_null_free(i), visited);\n+      visited.trunc_to(old_len);\n+    }\n+    if (value != new_value) {\n+      if (val == this) {\n+        val = clone()->as_InlineType();\n+      }\n+      val->set_field_value(i, new_value);\n+    }\n+  }\n+  return (val == this) ? this : kit->gvn().transform(val)->as_InlineType();\n+}\n+\n+void InlineTypeNode::load(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, GrowableArray<ciType*>& visited, int holder_offset, DecoratorSet decorators) {\n@@ -420,1 +476,1 @@\n-      value = InlineTypeNode::make_default(kit->gvn(), ft->as_inline_klass());\n+      value = make_default_impl(kit->gvn(), ft->as_inline_klass(), visited);\n@@ -423,1 +479,1 @@\n-      value = InlineTypeNode::make_from_flattened(kit, ft->as_inline_klass(), base, ptr, holder, offset, decorators);\n+      value = make_from_flattened_impl(kit, ft->as_inline_klass(), base, ptr, holder, offset, decorators, visited);\n@@ -456,3 +512,0 @@\n-        if (is_array) {\n-          decorators |= IS_ARRAY;\n-        }\n@@ -464,1 +517,1 @@\n-          value = kit->access_load_at(base, adr, adr_type, val_type, bt, decorators);\n+          value = kit->access_load_at(base, adr, adr_type, val_type, bt, is_array ? (decorators | IS_ARRAY) : decorators);\n@@ -468,2 +521,7 @@\n-      if (ft->is_inlinetype()) {\n-        value = InlineTypeNode::make_from_oop(kit, value, ft->as_inline_klass(), null_free);\n+      if (visited.contains(ft)) {\n+        kit->C->set_has_circular_inline_type(true);\n+      } else if (ft->is_inlinetype()) {\n+        int old_len = visited.length();\n+        visited.push(ft);\n+        value = make_from_oop_impl(kit, value, ft->as_inline_klass(), null_free, visited);\n+        visited.trunc_to(old_len);\n@@ -497,4 +555,1 @@\n-      if (!value->is_InlineType()) {\n-        \/\/ Recursively store the flattened inline type field\n-        value = InlineTypeNode::make_from_oop(kit, value, ft->as_inline_klass());\n-      }\n+      \/\/ Recursively store the flattened inline type field\n@@ -513,5 +568,2 @@\n-        const TypeAryPtr* ary_type = kit->gvn().type(base)->isa_aryptr();\n-        if (ary_type != NULL) {\n-          decorators |= IS_ARRAY;\n-        }\n-        kit->access_store_at(base, adr, adr_type, value, val_type, bt, decorators);\n+        bool is_array = (kit->gvn().type(base)->isa_aryptr() != NULL);\n+        kit->access_store_at(base, adr, adr_type, value, val_type, bt, is_array ? (decorators | IS_ARRAY) : decorators);\n@@ -525,1 +577,1 @@\n-  if (_is_buffered) {\n+  if (kit->gvn().find_int_con(get_is_buffered(), 0) == 1) {\n@@ -536,1 +588,1 @@\n-    vt->_is_buffered = true;\n+    vt->set_is_buffered(kit->gvn());\n@@ -607,1 +659,1 @@\n-  vt->_is_buffered = true;\n+  vt->set_is_buffered(kit->gvn());\n@@ -620,1 +672,1 @@\n-  if (_is_buffered) {\n+  if (phase->find_int_con(get_is_buffered(), 0) == 1) {\n@@ -714,3 +766,3 @@\n-  Node* alloc = AllocateNode::Ideal_allocation(oop, phase);\n-  bool is_larval_alloc = alloc && alloc->as_Allocate()->_larval == true;\n-  if (!is_larval_alloc && is_default(phase) && inline_klass()->is_initialized() &&\n+  if (!is_larval(phase) &&\n+      is_default(phase) &&\n+      inline_klass()->is_initialized() &&\n@@ -726,0 +778,1 @@\n+    set_is_buffered(*phase);\n@@ -772,1 +825,2 @@\n-  InlineTypeNode* vt = new InlineTypeNode(vk, oop, null_free, vk->is_empty() && vk->is_initialized());\n+  InlineTypeNode* vt = new InlineTypeNode(vk, oop, null_free);\n+  vt->set_is_buffered(gvn, vk->is_empty() && vk->is_initialized());\n@@ -794,0 +848,6 @@\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_default_impl(gvn, vk, visited);\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_default_impl(PhaseGVN& gvn, ciInlineKlass* vk, GrowableArray<ciType*>& visited) {\n@@ -796,1 +856,2 @@\n-  InlineTypeNode* vt = new InlineTypeNode(vk, oop, true, vk->is_initialized());\n+  InlineTypeNode* vt = new InlineTypeNode(vk, oop, \/* null_free= *\/ true);\n+  vt->set_is_buffered(gvn, vk->is_initialized());\n@@ -799,4 +860,8 @@\n-    ciType* field_type = vt->field_type(i);\n-    Node* value = default_value(gvn, field_type);\n-    if (field_type->is_inlinetype()) {\n-      ciInlineKlass* vk = field_type->as_inline_klass();\n+    ciType* ft = vt->field_type(i);\n+    Node* value = default_value(gvn, ft);\n+    if (!vt->field_is_flattened(i) && visited.contains(ft)) {\n+      gvn.C->set_has_circular_inline_type(true);\n+    } else if (ft->is_inlinetype()) {\n+      int old_len = visited.length();\n+      visited.push(ft);\n+      ciInlineKlass* vk = ft->as_inline_klass();\n@@ -804,1 +869,1 @@\n-        value = make_default(gvn, vk);\n+        value = make_default_impl(gvn, vk, visited);\n@@ -806,1 +871,1 @@\n-        value = InlineTypeNode::make_null(gvn, vk);\n+        value = make_null_impl(gvn, vk, visited);\n@@ -808,0 +873,1 @@\n+      visited.trunc_to(old_len);\n@@ -841,0 +907,6 @@\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_from_oop_impl(kit, oop, vk, null_free, visited);\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_from_oop_impl(GraphKit* kit, Node* oop, ciInlineKlass* vk, bool null_free, GrowableArray<ciType*>& visited) {\n@@ -844,1 +916,1 @@\n-    InlineTypeNode* def = make_default(gvn, vk);\n+    InlineTypeNode* def = make_default_impl(gvn, vk, visited);\n@@ -862,1 +934,1 @@\n-        vt = make_default(gvn, vk);\n+        vt = make_default_impl(gvn, vk, visited);\n@@ -864,1 +936,1 @@\n-        vt = InlineTypeNode::make_null(gvn, vk);\n+        vt = make_null_impl(gvn, vk, visited);\n@@ -869,1 +941,2 @@\n-    vt = new InlineTypeNode(vk, not_null_oop, null_free, true);\n+    vt = new InlineTypeNode(vk, not_null_oop, null_free);\n+    vt->set_is_buffered(gvn);\n@@ -871,1 +944,1 @@\n-    vt->load(kit, not_null_oop, not_null_oop, vk, \/* holder_offset *\/ 0);\n+    vt->load(kit, not_null_oop, not_null_oop, vk, visited);\n@@ -876,1 +949,1 @@\n-        null_vt = make_default(gvn, vk);\n+        null_vt = make_default_impl(gvn, vk, visited);\n@@ -878,1 +951,1 @@\n-        null_vt = InlineTypeNode::make_null(gvn, vk);\n+        null_vt = make_null_impl(gvn, vk, visited);\n@@ -893,1 +966,1 @@\n-    vt = new InlineTypeNode(vk, oop, \/* null_free= *\/ true, true);\n+    vt = new InlineTypeNode(vk, oop, \/* null_free= *\/ true);\n@@ -895,0 +968,1 @@\n+    vt->set_is_buffered(gvn);\n@@ -896,1 +970,1 @@\n-    vt->load(kit, oop, oop, vk, \/* holder_offset *\/ 0);\n+    vt->load(kit, oop, oop, vk, visited);\n@@ -906,1 +980,7 @@\n-\/\/ GraphKit wrapper for the 'make_from_flattened' method\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_from_flattened_impl(kit, vk, obj, ptr, holder, holder_offset, decorators, visited);\n+}\n+\n+\/\/ GraphKit wrapper for the 'make_from_flattened' method\n+InlineTypeNode* InlineTypeNode::make_from_flattened_impl(GraphKit* kit, ciInlineKlass* vk, Node* obj, Node* ptr, ciInstanceKlass* holder, int holder_offset, DecoratorSet decorators, GrowableArray<ciType*>& visited) {\n@@ -917,1 +997,1 @@\n-  vt->load(kit, obj, ptr, holder, holder_offset, decorators);\n+  vt->load(kit, obj, ptr, holder, visited, holder_offset, decorators);\n@@ -929,1 +1009,3 @@\n-  vt->initialize_fields(kit, multi, base_input, in, null_free);\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  vt->initialize_fields(kit, multi, base_input, in, null_free, NULL, visited);\n@@ -935,1 +1017,1 @@\n-  InlineTypeNode* res = InlineTypeNode::make_uninitialized(kit->gvn(), vk);\n+  InlineTypeNode* res = make_uninitialized(kit->gvn(), vk);\n@@ -973,1 +1055,1 @@\n-  InlineTypeNode* res = InlineTypeNode::make_uninitialized(kit->gvn(), vk);\n+  InlineTypeNode* res = make_uninitialized(kit->gvn(), vk);\n@@ -983,0 +1065,10 @@\n+bool InlineTypeNode::is_larval(PhaseGVN* gvn) const {\n+  if (!is_allocated(gvn)) {\n+    return false;\n+  }\n+\n+  Node* oop = get_oop();\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(oop, gvn);\n+  return alloc != NULL && alloc->_larval;\n+}\n+\n@@ -1080,1 +1172,1 @@\n-void InlineTypeNode::initialize_fields(GraphKit* kit, MultiNode* multi, uint& base_input, bool in, bool null_free, Node* null_check_region) {\n+void InlineTypeNode::initialize_fields(GraphKit* kit, MultiNode* multi, uint& base_input, bool in, bool null_free, Node* null_check_region, GrowableArray<ciType*>& visited) {\n@@ -1118,1 +1210,1 @@\n-      vt->initialize_fields(kit, multi, base_input, in, true, null_check_region);\n+      vt->initialize_fields(kit, multi, base_input, in, true, null_check_region, visited);\n@@ -1131,1 +1223,3 @@\n-          load(kit, not_null_oop, not_null_oop, ik, \/* holder_offset *\/ 0);\n+          GrowableArray<ciType*> visited;\n+          visited.push(ik);\n+          load(kit, not_null_oop, not_null_oop, ik, visited, \/* holder_offset *\/ 0);\n@@ -1150,0 +1244,3 @@\n+          if (parm->is_InlineType() && kit->C->has_circular_inline_type()) {\n+            parm = parm->as_InlineType()->get_oop();\n+          }\n@@ -1155,1 +1252,8 @@\n-        parm = make_from_oop(kit, parm, type->as_inline_klass(), field_is_null_free(i));\n+        if (visited.contains(type)) {\n+          kit->C->set_has_circular_inline_type(true);\n+        } else if (!parm->is_InlineType()) {\n+          int old_len = visited.length();\n+          visited.push(type);\n+          parm = make_from_oop_impl(kit, parm, type->as_inline_klass(), field_is_null_free(i), visited);\n+          visited.trunc_to(old_len);\n+        }\n@@ -1167,1 +1271,1 @@\n-    is_init= gvn.transform(new ProjNode(multi->as_Call(), base_input));\n+    is_init = gvn.transform(new ProjNode(multi->as_Call(), base_input));\n@@ -1214,7 +1318,19 @@\n-  InlineTypeNode* ptr = new InlineTypeNode(vk, gvn.zerocon(T_OBJECT), \/* null_free= *\/ false, true);\n-  ptr->set_req(IsInit, gvn.intcon(0));\n-  for (uint i = 0; i < ptr->field_count(); i++) {\n-    ciType* field_type = ptr->field_type(i);\n-    Node* value = default_value(gvn, field_type);\n-    if (field_type->is_inlinetype()) {\n-      value = InlineTypeNode::make_null(gvn, field_type->as_inline_klass());\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_null_impl(gvn, vk, visited);\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_null_impl(PhaseGVN& gvn, ciInlineKlass* vk, GrowableArray<ciType*>& visited) {\n+  InlineTypeNode* vt = new InlineTypeNode(vk, gvn.zerocon(T_OBJECT), \/* null_free= *\/ false);\n+  vt->set_is_buffered(gvn);\n+  vt->set_is_init(gvn, false);\n+  for (uint i = 0; i < vt->field_count(); i++) {\n+    ciType* ft = vt->field_type(i);\n+    Node* value = default_value(gvn, ft);\n+    if (!vt->field_is_flattened(i) && visited.contains(ft)) {\n+      gvn.C->set_has_circular_inline_type(true);\n+    } else if (ft->is_inlinetype()) {\n+      int old_len = visited.length();\n+      visited.push(ft);\n+      value = make_null_impl(gvn, ft->as_inline_klass(), visited);\n+      visited.trunc_to(old_len);\n@@ -1222,1 +1338,1 @@\n-    ptr->set_field_value(i, value);\n+    vt->set_field_value(i, value);\n@@ -1224,1 +1340,1 @@\n-  return gvn.transform(ptr)->as_InlineType();\n+  return gvn.transform(vt)->as_InlineType();\n@@ -1250,0 +1366,3 @@\n+  if (tinit == Type::TOP) {\n+    return Type::TOP;\n+  }\n","filename":"src\/hotspot\/share\/opto\/inlinetypenode.cpp","additions":210,"deletions":91,"binary":false,"changes":301,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,7 +39,2 @@\n-  virtual uint hash() const;\n-  virtual bool cmp(const Node &n) const;\n-  virtual uint size_of() const;\n-  bool _is_buffered;\n-\n-  InlineTypeNode(ciInlineKlass* vk, Node* oop, bool null_free, bool is_buffered)\n-      : TypeNode(TypeInstPtr::make(null_free ? TypePtr::NotNull : TypePtr::BotPTR, vk), Values + vk->nof_declared_nonstatic_fields()), _is_buffered(is_buffered) {\n+  InlineTypeNode(ciInlineKlass* vk, Node* oop, bool null_free)\n+      : TypeNode(TypeInstPtr::make(null_free ? TypePtr::NotNull : TypePtr::BotPTR, vk), Values + vk->nof_declared_nonstatic_fields()) {\n@@ -52,5 +47,6 @@\n-  enum { Control,   \/\/ Control input.\n-         Oop,       \/\/ Oop to heap allocated buffer (NULL if not buffered).\n-         IsInit,    \/\/ Needs to be checked for NULL before using the field values.\n-         Values     \/\/ Nodes corresponding to values of the inline type's fields.\n-                    \/\/ Nodes are connected in increasing order of the index of the field they correspond to.\n+  enum { Control,    \/\/ Control input.\n+         Oop,        \/\/ Oop to heap allocated buffer.\n+         IsBuffered, \/\/ True if inline type is heap allocated (or NULL), false otherwise.\n+         IsInit,     \/\/ Needs to be checked for NULL before using the field values.\n+         Values      \/\/ Nodes corresponding to values of the inline type's fields.\n+                     \/\/ Nodes are connected in increasing order of the index of the field they correspond to.\n@@ -68,0 +64,3 @@\n+  \/\/ Checks if the inline type oop is an allocated buffer with larval state\n+  bool is_larval(PhaseGVN* gvn) const;\n+\n@@ -72,1 +71,8 @@\n-  void initialize_fields(GraphKit* kit, MultiNode* multi, uint& base_input, bool in, bool null_free = true, Node* null_check_region = NULL);\n+  void initialize_fields(GraphKit* kit, MultiNode* multi, uint& base_input, bool in, bool null_free, Node* null_check_region, GrowableArray<ciType*>& visited);\n+\n+  InlineTypeNode* adjust_scalarization_depth_impl(GraphKit* kit, GrowableArray<ciType*>& visited);\n+\n+  static InlineTypeNode* make_default_impl(PhaseGVN& gvn, ciInlineKlass* vk, GrowableArray<ciType*>& visited);\n+  static InlineTypeNode* make_from_oop_impl(GraphKit* kit, Node* oop, ciInlineKlass* vk, bool null_free, GrowableArray<ciType*>& visited);\n+  static InlineTypeNode* make_null_impl(PhaseGVN& gvn, ciInlineKlass* vk, GrowableArray<ciType*>& visited);\n+  static InlineTypeNode* make_from_flattened_impl(GraphKit* kit, ciInlineKlass* vk, Node* obj, Node* ptr, ciInstanceKlass* holder, int holder_offset, DecoratorSet decorators, GrowableArray<ciType*>& visited);\n@@ -88,1 +94,1 @@\n-\n+  \/\/ Create with null field values\n@@ -91,0 +97,2 @@\n+  static int stack_size_for_field(ciField* field);\n+\n@@ -108,3 +116,7 @@\n-  void  set_is_init(PhaseGVN& gvn) { set_req(IsInit, gvn.intcon(1)); }\n-  void  set_is_buffered() { _is_buffered = true; }\n-  bool  is_buffered() { return _is_buffered; }\n+  void  set_is_init(PhaseGVN& gvn, bool init = true) { set_req(IsInit, gvn.intcon(init ? 1 : 0)); }\n+  Node* get_is_buffered() const { return in(IsBuffered); }\n+  void  set_is_buffered(PhaseGVN& gvn, bool buffered = true) { set_req(IsBuffered, gvn.intcon(buffered ? 1 : 0)); }\n+\n+  \/\/ Get indices for inputs.\n+  static int   get_Oop_idx()    { return InlineTypeNode::Oop; }\n+  static int   get_Values_idx() { return InlineTypeNode::Values; }\n@@ -137,1 +149,3 @@\n-  void load(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, int holder_offset = 0, DecoratorSet decorators = IN_HEAP | MO_UNORDERED);\n+  void load(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, GrowableArray<ciType*>& visited, int holder_offset = 0, DecoratorSet decorators = IN_HEAP | MO_UNORDERED);\n+  \/\/ Make sure that inline type is fully scalarized\n+  InlineTypeNode* adjust_scalarization_depth(GraphKit* kit);\n","filename":"src\/hotspot\/share\/opto\/inlinetypenode.hpp","additions":33,"deletions":19,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -2002,1 +2002,1 @@\n-    } else if (type == T_OBJECT) {\n+    } else if (type == T_OBJECT || type == T_PRIMITIVE_OBJECT) {\n@@ -2340,1 +2340,5 @@\n-            set_result(vt->field_value_by_offset(off, false));\n+            Node* value = vt->field_value_by_offset(off, false);\n+            if (value->is_InlineType()) {\n+              value = value->as_InlineType()->adjust_scalarization_depth(this);\n+            }\n+            set_result(value);\n@@ -2431,0 +2435,3 @@\n+    if (adr_type->is_flat()) {\n+      bt = T_PRIMITIVE_OBJECT;\n+    }\n@@ -2458,3 +2465,1 @@\n-      if (!elem->isa_inlinetype()) {\n-        mismatched = true;\n-      } else if (elem->inline_klass() != inline_klass) {\n+      if (!adr_type->is_flat() || elem->inline_klass() != inline_klass) {\n@@ -2468,1 +2473,1 @@\n-      if (!(val_t->isa_inlinetype() || val_t->is_inlinetypeptr()) || val_t->inline_klass() != inline_klass) {\n+      if (!val_t->is_inlinetypeptr() || val_t->inline_klass() != inline_klass) {\n@@ -3908,1 +3913,1 @@\n-      int static_res = C->static_subtype_check(TypeKlassPtr::make(tm->as_klass()), tp->as_klass_type());\n+      int static_res = C->static_subtype_check(TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces), tp->as_klass_type());\n@@ -5528,1 +5533,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -7395,1 +7400,1 @@\n-  assert(UsePolyIntrinsics, \"need Poly intrinsics support\");\n+  assert(UsePoly1305Intrinsics, \"need Poly intrinsics support\");\n@@ -7627,1 +7632,1 @@\n-  const TypeOopPtr* xtype = aklass->as_instance_type()->cast_to_ptr_type(TypePtr::NotNull);\n+  const TypeOopPtr* xtype = aklass->cast_to_exactness(false)->as_instance_type()->cast_to_ptr_type(TypePtr::NotNull);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":15,"deletions":10,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -777,1 +777,1 @@\n-    _max_classes  = ClassMask_Move\n+    _max_classes  = ClassMask_LShift\n@@ -940,0 +940,1 @@\n+  DEFINE_CLASS_QUERY(NeverBranch)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -222,106 +222,0 @@\n-volatile int C2SafepointPollStubTable::_stub_size = 0;\n-\n-Label& C2SafepointPollStubTable::add_safepoint(uintptr_t safepoint_offset) {\n-  C2SafepointPollStub* entry = new (Compile::current()->comp_arena()) C2SafepointPollStub(safepoint_offset);\n-  _safepoints.append(entry);\n-  return entry->_stub_label;\n-}\n-\n-void C2SafepointPollStubTable::emit(CodeBuffer& cb) {\n-  MacroAssembler masm(&cb);\n-  for (int i = _safepoints.length() - 1; i >= 0; i--) {\n-    \/\/ Make sure there is enough space in the code buffer\n-    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == NULL) {\n-      ciEnv::current()->record_failure(\"CodeCache is full\");\n-      return;\n-    }\n-\n-    C2SafepointPollStub* entry = _safepoints.at(i);\n-    emit_stub(masm, entry);\n-  }\n-}\n-\n-int C2SafepointPollStubTable::stub_size_lazy() const {\n-  int size = Atomic::load(&_stub_size);\n-\n-  if (size != 0) {\n-    return size;\n-  }\n-\n-  Compile* const C = Compile::current();\n-  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n-  CodeBuffer cb(blob->content_begin(), C->output()->scratch_buffer_code_size());\n-  MacroAssembler masm(&cb);\n-  C2SafepointPollStub* entry = _safepoints.at(0);\n-  emit_stub(masm, entry);\n-  size += cb.insts_size();\n-\n-  Atomic::store(&_stub_size, size);\n-\n-  return size;\n-}\n-\n-int C2SafepointPollStubTable::estimate_stub_size() const {\n-  if (_safepoints.length() == 0) {\n-    return 0;\n-  }\n-\n-  int result = stub_size_lazy() * _safepoints.length();\n-\n-#ifdef ASSERT\n-  Compile* const C = Compile::current();\n-  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n-  int size = 0;\n-\n-  for (int i = _safepoints.length() - 1; i >= 0; i--) {\n-    CodeBuffer cb(blob->content_begin(), C->output()->scratch_buffer_code_size());\n-    MacroAssembler masm(&cb);\n-    C2SafepointPollStub* entry = _safepoints.at(i);\n-    emit_stub(masm, entry);\n-    size += cb.insts_size();\n-  }\n-  assert(size == result, \"stubs should not have variable size\");\n-#endif\n-\n-  return result;\n-}\n-\n-\/\/ Nmethod entry barrier stubs\n-C2EntryBarrierStub* C2EntryBarrierStubTable::add_entry_barrier() {\n-  C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-  _stubs.append(stub);\n-  return stub;\n-}\n-\n-void C2EntryBarrierStubTable::emit(CodeBuffer& cb) {\n-  if (_stubs.is_empty()) {\n-    \/\/ No stub - nothing to do\n-    return;\n-  }\n-\n-  C2_MacroAssembler masm(&cb);\n-  for (C2EntryBarrierStub* stub : _stubs) {\n-    \/\/ Make sure there is enough space in the code buffer\n-    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == NULL) {\n-      ciEnv::current()->record_failure(\"CodeCache is full\");\n-      return;\n-    }\n-\n-    intptr_t before = masm.offset();\n-    masm.emit_entry_barrier_stub(stub);\n-    intptr_t after = masm.offset();\n-    int actual_size = (int)(after - before);\n-    int expected_size = masm.entry_barrier_stub_size();\n-    assert(actual_size == expected_size, \"Estimated size is wrong, expected %d, was %d\", expected_size, actual_size);\n-  }\n-}\n-\n-int C2EntryBarrierStubTable::estimate_stub_size() const {\n-  if (BarrierSet::barrier_set()->barrier_set_nmethod() == NULL) {\n-    \/\/ No nmethod entry barrier?\n-    return 0;\n-  }\n-\n-  return C2_MacroAssembler::entry_barrier_stub_size();\n-}\n-\n@@ -334,2 +228,1 @@\n-    _safepoint_poll_table(),\n-    _entry_barrier_table(),\n+    _stub_list(),\n@@ -1428,2 +1321,0 @@\n-  stub_req += safepoint_poll_table()->estimate_stub_size();\n-  stub_req += entry_barrier_table()->estimate_stub_size();\n@@ -1937,6 +1828,2 @@\n-  \/\/ Fill in stubs for calling the runtime from safepoint polls.\n-  safepoint_poll_table()->emit(*cb);\n-  if (C->failing())  return;\n-\n-  \/\/ Fill in stubs for calling the runtime from nmethod entries.\n-  entry_barrier_table()->emit(*cb);\n+  \/\/ Fill in stubs.\n+  _stub_list.emit(*cb);\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":3,"deletions":116,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -612,2 +612,2 @@\n-    } else if (UseTypeSpeculation && (i == (arg_size - 1)) && !is_osr_parse() &&\n-               method()->has_vararg() && t->isa_aryptr() != NULL && !t->is_aryptr()->is_not_null_free()) {\n+    } else if (UseTypeSpeculation && (i == (arg_size - 1)) && !is_osr_parse() && method()->has_vararg() &&\n+               t->isa_aryptr() != NULL && !t->is_aryptr()->is_null_free() && !t->is_aryptr()->is_not_null_free()) {\n@@ -825,5 +825,0 @@\n-    \/\/ Scalarize inline type when returning as fields or inlining non-incrementally\n-    if ((tf()->returns_inline_type_as_fields() || (_caller->has_method() && !Compile::current()->inlining_incrementally())) &&\n-        ret_type->is_inlinetypeptr()) {\n-      ret_type = TypeInlineType::make(ret_type->inline_klass());\n-    }\n@@ -926,1 +921,1 @@\n-      assert(vt->is_buffered(), \"\");\n+      assert(vt->get_is_buffered(), \"\");\n@@ -933,1 +928,1 @@\n-      if (vt->is_allocated(&kit.gvn()) && !StressInlineTypeReturnedAsFields) {\n+      if (vt->is_allocated(&kit.gvn()) && !StressCallingConvention) {\n@@ -1278,1 +1273,1 @@\n-    const Type* holder_type = TypeInstPtr::make(TypePtr::BotPTR, callee_holder);\n+    const Type* holder_type = TypeInstPtr::make(TypePtr::BotPTR, callee_holder, Type::trust_interfaces);\n@@ -1291,1 +1286,1 @@\n-      Node* holder_klass = _gvn.makecon(TypeKlassPtr::make(callee_holder));\n+      Node* holder_klass = _gvn.makecon(TypeKlassPtr::make(callee_holder, Type::trust_interfaces));\n@@ -2290,1 +2285,1 @@\n-  Node* holder = makecon(TypeKlassPtr::make(method()->holder()));\n+  Node* holder = makecon(TypeKlassPtr::make(method()->holder(), Type::trust_interfaces));\n@@ -2354,2 +2349,2 @@\n-    \/\/ The return_type is set in Parse::build_exits().\n-    if (return_type->isa_inlinetype()) {\n+    if ((tf()->returns_inline_type_as_fields() || (_caller->has_method() && !Compile::current()->inlining_incrementally())) &&\n+        return_type->is_inlinetypeptr()) {\n@@ -2383,20 +2378,3 @@\n-    } else if (tr && tr->isa_instptr() && tr->is_loaded() && tr->is_interface()) {\n-      \/\/ If returning oops to an interface-return, there is a silent free\n-      \/\/ cast from oop to interface allowed by the Verifier. Make it explicit here.\n-      const TypeInstPtr* tp = value->bottom_type()->isa_instptr();\n-      if (tp && tp->is_loaded() && !tp->is_interface()) {\n-        \/\/ sharpen the type eagerly; this eases certain assert checking\n-        if (tp->higher_equal(TypeInstPtr::NOTNULL)) {\n-          tr = tr->join_speculative(TypeInstPtr::NOTNULL)->is_instptr();\n-        }\n-        value = _gvn.transform(new CheckCastPPNode(0, value, tr));\n-      }\n-    } else {\n-      \/\/ Handle returns of oop-arrays to an arrays-of-interface return\n-      const TypeInstPtr* phi_tip;\n-      const TypeInstPtr* val_tip;\n-      Type::get_arrays_base_elements(return_type, value->bottom_type(), &phi_tip, &val_tip);\n-      if (phi_tip != NULL && phi_tip->is_loaded() && phi_tip->is_interface() &&\n-          val_tip != NULL && val_tip->is_loaded() && !val_tip->is_interface()) {\n-        value = _gvn.transform(new CheckCastPPNode(0, value, return_type));\n-      }\n+    \/\/ ...else\n+    \/\/ If returning oops to an interface-return, there is a silent free\n+    \/\/ cast from oop to interface allowed by the Verifier. Make it explicit here.\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":12,"deletions":34,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -99,1 +99,1 @@\n-  Node* res = gen_checkcast(obj, makecon(TypeKlassPtr::make(klass)), NULL, null_free);\n+  Node* res = gen_checkcast(obj, makecon(TypeKlassPtr::make(klass, Type::trust_interfaces)), NULL, null_free);\n@@ -138,1 +138,1 @@\n-  Node* res = gen_instanceof(peek(), makecon(TypeKlassPtr::make(klass)), true);\n+  Node* res = gen_instanceof(peek(), makecon(TypeKlassPtr::make(klass, Type::trust_interfaces)), true);\n@@ -267,3 +267,1 @@\n-  if (!elemtype->isa_inlinetype()) {\n-    elemtype = elemtype->make_oopptr();\n-  }\n+  const TypeAryPtr* arytype = _gvn.type(ary)->is_aryptr();\n@@ -271,1 +269,1 @@\n-  if (elemtype->isa_inlinetype() != NULL || elemtype->is_inlinetypeptr()) {\n+  if (elemtype->make_ptr()->is_inlinetypeptr()) {\n@@ -273,1 +271,1 @@\n-    null_free = elemtype->isa_inlinetype() || !elemtype->maybe_null();\n+    null_free = arytype->is_flat() || !elemtype->make_ptr()->maybe_null();\n@@ -368,4 +366,3 @@\n-  InlineTypeNode* new_vt = InlineTypeNode::make_uninitialized(gvn(), gvn().type(holder)->inline_klass());\n-  for (uint i = 2; i < holder->req(); ++i) {\n-    new_vt->set_req(i, holder->in(i));\n-  }\n+  InlineTypeNode* new_vt = holder->clone()->as_InlineType();\n+  new_vt->set_oop(gvn().zerocon(T_PRIMITIVE_OBJECT));\n+  new_vt->set_is_buffered(gvn(), false);\n@@ -384,0 +381,8 @@\n+\n+  {\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    int nargs = InlineTypeNode::stack_size_for_field(field);\n+    inc_sp(nargs);\n+    new_vt = new_vt->adjust_scalarization_depth(this);\n+  }\n","filename":"src\/hotspot\/share\/opto\/parseHelper.cpp","additions":16,"deletions":11,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -131,2 +131,1 @@\n-    default:\n-      assert(false, \"Unexpected type\");\n+    default: fatal(\"Unexpected type: %s\", type2name(elem_bt));\n@@ -1803,0 +1802,2 @@\n+\n+\n@@ -1808,1 +1809,4 @@\n-\n+  \/\/ TODO[valhalla] Limiting support to only vectors cases untill mask and shuffle becomes inline types.\n+  if (is_vector_mask(vbox_type->instance_klass())) {\n+    return false;\n+  }\n@@ -1819,1 +1823,7 @@\n-  Node* opd2 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+  Node* opd2;\n+  if (Matcher::vectortest_needs_second_argument(booltest == BoolTest::overflow,\n+                                                opd1->bottom_type()->isa_vectmask())) {\n+    opd2 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+  } else {\n+    opd2 = opd1;\n+  }\n@@ -1823,3 +1833,7 @@\n-  Node* test = new VectorTestNode(opd1, opd2, booltest);\n-  test = gvn().transform(test);\n-  set_result(test);\n+  Node* cmp = gvn().transform(new VectorTestNode(opd1, opd2, booltest));\n+  BoolTest::mask test = Matcher::vectortest_mask(booltest == BoolTest::overflow,\n+                                                 opd1->bottom_type()->isa_vectmask(), num_elem);\n+  Node* bol = gvn().transform(new BoolNode(cmp, test));\n+  Node* res = gvn().transform(new CMoveINode(bol, gvn().intcon(0), gvn().intcon(1), TypeInt::BOOL));\n+\n+  set_result(res);\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":22,"deletions":8,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -1426,1 +1426,1 @@\n-class VectorTestNode : public Node {\n+class VectorTestNode : public CmpNode {\n@@ -1434,1 +1434,1 @@\n-  VectorTestNode(Node* in1, Node* in2, BoolTest::mask predicate) : Node(NULL, in1, in2), _predicate(predicate) {\n+  VectorTestNode(Node* in1, Node* in2, BoolTest::mask predicate) : CmpNode(in1, in2), _predicate(predicate) {\n@@ -1439,0 +1439,4 @@\n+  virtual const Type* Value(PhaseGVN* phase) const { return TypeInt::CC; }\n+  virtual const Type* sub(const Type*, const Type*) const { return TypeInt::CC; }\n+  BoolTest::mask get_predicate() const { return _predicate; }\n+\n@@ -1442,4 +1446,0 @@\n-  virtual const Type *bottom_type() const { return TypeInt::BOOL; }\n-  virtual uint ideal_reg() const { return Op_RegI; }  \/\/ TODO Should be RegFlags but due to missing comparison flags for BoolTest\n-                                                      \/\/ in middle-end, we make it boolean result directly.\n-  BoolTest::mask get_predicate() const { return _predicate; }\n@@ -1678,2 +1678,2 @@\n-  VectorBoxNode(Compile* C, ciInlineKlass* vk, Node* oop, const TypeInstPtr* box_type, const TypeVect* vt, bool null_free, bool is_buffered) :\n-    InlineTypeNode(vk, oop, null_free, is_buffered) {\n+  VectorBoxNode(Compile* C, ciInlineKlass* vk, Node* oop, const TypeInstPtr* box_type, const TypeVect* vt, bool null_free) :\n+    InlineTypeNode(vk, oop, null_free) {\n@@ -1697,1 +1697,1 @@\n-    VectorBoxNode* box_node = new VectorBoxNode(C, vk, box, box_type, vt, false, vk->is_empty() && vk->is_initialized());\n+    VectorBoxNode* box_node = new VectorBoxNode(C, vk, box, box_type, vt, false);\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -72,0 +72,1 @@\n+#include <string.h>\n@@ -551,7 +552,0 @@\n-  { \"ExtendedDTraceProbes\",         JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n-  { \"UseContainerCpuShares\",        JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n-  { \"PreferContainerQuotaForCPUCount\", JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n-  { \"AliasLevel\",                   JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n-  { \"UseCodeAging\",                 JDK_Version::undefined(), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n-  { \"PrintSharedDictionary\",          JDK_Version::undefined(), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n-\n@@ -1933,1 +1927,0 @@\n-unsigned int patch_mod_count = 0;\n@@ -2110,2 +2103,0 @@\n-  bool patch_mod_javabase = false;\n-\n@@ -2125,1 +2116,1 @@\n-  jint result = parse_each_vm_init_arg(vm_options_args, &patch_mod_javabase, JVMFlagOrigin::JIMAGE_RESOURCE);\n+  jint result = parse_each_vm_init_arg(vm_options_args, JVMFlagOrigin::JIMAGE_RESOURCE);\n@@ -2132,1 +2123,1 @@\n-  result = parse_each_vm_init_arg(java_tool_options_args, &patch_mod_javabase, JVMFlagOrigin::ENVIRON_VAR);\n+  result = parse_each_vm_init_arg(java_tool_options_args, JVMFlagOrigin::ENVIRON_VAR);\n@@ -2138,1 +2129,1 @@\n-  result = parse_each_vm_init_arg(cmd_line_args, &patch_mod_javabase, JVMFlagOrigin::COMMAND_LINE);\n+  result = parse_each_vm_init_arg(cmd_line_args, JVMFlagOrigin::COMMAND_LINE);\n@@ -2145,1 +2136,1 @@\n-  result = parse_each_vm_init_arg(java_options_args, &patch_mod_javabase, JVMFlagOrigin::ENVIRON_VAR);\n+  result = parse_each_vm_init_arg(java_options_args, JVMFlagOrigin::ENVIRON_VAR);\n@@ -2159,1 +2150,1 @@\n-  result = finalize_vm_init_args(patch_mod_javabase);\n+  result = finalize_vm_init_args();\n@@ -2212,1 +2203,1 @@\n-int Arguments::process_patch_mod_option(const char* patch_mod_tail, bool* patch_mod_javabase) {\n+int Arguments::process_patch_mod_option(const char* patch_mod_tail) {\n@@ -2228,1 +2219,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1, patch_mod_javabase);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/);\n@@ -2230,3 +2221,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2240,0 +2228,64 @@\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, each module may have value classes that\n+  \/\/ are to be patched into the module.\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (enable_preview() && EnableValhalla) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module or --enable-preview\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2298,1 +2350,1 @@\n-jint Arguments::parse_each_vm_init_arg(const JavaVMInitArgs* args, bool* patch_mod_javabase, JVMFlagOrigin origin) {\n+jint Arguments::parse_each_vm_init_arg(const JavaVMInitArgs* args, JVMFlagOrigin origin) {\n@@ -2426,1 +2478,1 @@\n-      int res = process_patch_mod_option(tail, patch_mod_javabase);\n+      int res = process_patch_mod_option(tail);\n@@ -2955,12 +3007,7 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool* patch_mod_javabase) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (*patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      *patch_mod_javabase = true;\n-    }\n-  }\n+bool match_module(void *module_name, ModulePatchPath *patch) {\n+  return (strcmp((char *)module_name, patch->module_name()) == 0);\n+}\n+\n+bool Arguments::patch_mod_javabase() {\n+    return _patch_mod_prefix != nullptr && _patch_mod_prefix->find((void*)JAVA_BASE_NAME, match_module) >= 0;\n+}\n@@ -2968,0 +3015,1 @@\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append) {\n@@ -2969,1 +3017,1 @@\n-  if (_patch_mod_prefix == NULL) {\n+  if (_patch_mod_prefix == nullptr) {\n@@ -2973,1 +3021,16 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find((void*)module_name, match_module);\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -3016,1 +3079,1 @@\n-jint Arguments::finalize_vm_init_args(bool patch_mod_javabase) {\n+jint Arguments::finalize_vm_init_args() {\n@@ -3099,0 +3162,5 @@\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n+    return JNI_ERR;\n+  }\n+\n@@ -3135,1 +3203,1 @@\n-  if (UseSharedSpaces && patch_mod_javabase) {\n+  if (UseSharedSpaces && patch_mod_javabase()) {\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":107,"deletions":39,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -91,0 +91,1 @@\n+  inline void append_path(const char* path) { _path->append_value(path); }\n@@ -398,1 +399,1 @@\n-  static int process_patch_mod_option(const char* patch_mod_tail, bool* patch_mod_javabase);\n+  static int process_patch_mod_option(const char* patch_mod_tail);\n@@ -436,2 +437,2 @@\n-  static jint parse_each_vm_init_arg(const JavaVMInitArgs* args, bool* patch_mod_javabase, JVMFlagOrigin origin);\n-  static jint finalize_vm_init_args(bool patch_mod_javabase);\n+  static jint parse_each_vm_init_arg(const JavaVMInitArgs* args, JVMFlagOrigin origin);\n+  static jint finalize_vm_init_args();\n@@ -605,1 +606,3 @@\n-  static void add_patch_mod_prefix(const char *module_name, const char *path, bool* patch_mod_javabase);\n+  static void add_patch_mod_prefix(const char *module_name, const char *path, bool allow_append);\n+  static bool patch_mod_javabase();\n+  static int finalize_patch_module();\n","filename":"src\/hotspot\/share\/runtime\/arguments.hpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2682,1 +2682,1 @@\n-  sprintf(buf, \"reason%d\", reason);\n+  os::snprintf_checked(buf, sizeof(buf), \"reason%d\", reason);\n@@ -2692,1 +2692,1 @@\n-  sprintf(buf, \"action%d\", action);\n+  os::snprintf_checked(buf, sizeof(buf), \"action%d\", action);\n@@ -2815,1 +2815,1 @@\n-            sprintf(name, \"%s\/%s\/%s\",\n+            os::snprintf_checked(name, sizeof(name), \"%s\/%s\/%s\",\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -106,1 +106,3 @@\n-    assert(field_holder() == ik, \"must be already initialized to this class\");\n+    \/\/ If the class is a scratch class, the constant pool points to the original class,\n+    \/\/ but that's ok because of constant pool merging.\n+    assert(field_holder() == ik || ik->is_scratch_class(), \"must be already initialized to this class\");\n","filename":"src\/hotspot\/share\/runtime\/fieldDescriptor.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1274,5 +1274,0 @@\n-    bool caller_is_c1 = false;\n-\n-    if (callerFrame.is_compiled_frame()) {\n-      caller_is_c1 = callerFrame.cb()->is_compiled_by_c1();\n-    }\n@@ -1287,0 +1282,1 @@\n+    bool caller_is_c1 = callerFrame.is_compiled_frame() && callerFrame.cb()->is_compiled_by_c1();\n@@ -1290,0 +1286,1 @@\n+      assert(!callee->mismatch(), \"calls with inline type receivers should never mismatch\");\n@@ -1345,1 +1342,1 @@\n-methodHandle SharedRuntime::find_callee_method(TRAPS) {\n+methodHandle SharedRuntime::find_callee_method(bool is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1371,0 +1368,4 @@\n+    \/\/ Calls via mismatching methods are always non-scalarized\n+    if (callinfo.resolved_method()->mismatch() && !is_optimized) {\n+      caller_is_c1 = true;\n+    }\n@@ -1378,1 +1379,1 @@\n-methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, bool* caller_is_c1, TRAPS) {\n+methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1405,1 +1406,1 @@\n-                                                CompiledMethod* caller_nm, bool is_virtual, bool is_optimized,\n+                                                CompiledMethod* caller_nm, bool is_virtual, bool is_optimized, bool& caller_is_c1,\n@@ -1427,1 +1428,0 @@\n-  bool caller_is_c1 = caller_nm->is_compiled_by_c1();\n@@ -1497,1 +1497,1 @@\n-methodHandle SharedRuntime::resolve_sub_helper(bool is_virtual, bool is_optimized, bool* caller_is_c1, TRAPS) {\n+methodHandle SharedRuntime::resolve_sub_helper(bool is_virtual, bool is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1509,1 +1509,0 @@\n-  *caller_is_c1 = caller_nm->is_compiled_by_c1();\n@@ -1518,0 +1517,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || (call_info.resolved_method()->mismatch() && !is_optimized)) {\n+    caller_is_c1 = true;\n+  }\n@@ -1582,1 +1585,1 @@\n-                                                  is_virtual, is_optimized, receiver,\n+                                                  is_virtual, is_optimized, caller_is_c1, receiver,\n@@ -1717,1 +1720,1 @@\n-  bool caller_is_c1;\n+  bool caller_is_c1 = false;\n@@ -1720,1 +1723,1 @@\n-    callee_method = SharedRuntime::resolve_helper(false, false, &caller_is_c1, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(false, false, caller_is_c1, CHECK_NULL);\n@@ -1758,1 +1761,1 @@\n-  bool caller_is_c1;\n+  bool caller_is_c1 = false;\n@@ -1760,1 +1763,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, false, &caller_is_c1, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, false, caller_is_c1, CHECK_NULL);\n@@ -1775,1 +1778,1 @@\n-  bool caller_is_c1;\n+  bool caller_is_c1 = false;\n@@ -1777,1 +1780,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, true, &caller_is_c1, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, true, caller_is_c1, CHECK_NULL);\n@@ -1844,1 +1847,1 @@\n-                                            caller_nm->is_compiled_by_c1(),\n+                                            caller_is_c1,\n@@ -1950,1 +1953,4 @@\n-  caller_is_c1 = caller_nm->is_compiled_by_c1();\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_is_c1 = true;\n+  }\n@@ -2080,1 +2086,1 @@\n-  methodHandle callee_method = find_callee_method(CHECK_(methodHandle()));\n+  methodHandle callee_method = find_callee_method(is_optimized, caller_is_c1, CHECK_(methodHandle()));\n@@ -2877,1 +2883,1 @@\n-    SigEntry::add_entry(&obj_args.sig(), T_OBJECT, NULL);\n+    SigEntry::add_entry(obj_args.sig(), T_OBJECT, NULL);\n@@ -2882,1 +2888,1 @@\n-    SigEntry::add_entry(&int_args.sig(), T_INT, NULL);\n+    SigEntry::add_entry(int_args.sig(), T_INT, NULL);\n@@ -2887,2 +2893,2 @@\n-    SigEntry::add_entry(&obj_int_args.sig(), T_OBJECT, NULL);\n-    SigEntry::add_entry(&obj_int_args.sig(), T_INT, NULL);\n+    SigEntry::add_entry(obj_int_args.sig(), T_OBJECT, NULL);\n+    SigEntry::add_entry(obj_int_args.sig(), T_INT, NULL);\n@@ -2893,2 +2899,2 @@\n-    SigEntry::add_entry(&obj_obj_args.sig(), T_OBJECT, NULL);\n-    SigEntry::add_entry(&obj_obj_args.sig(), T_OBJECT, NULL);\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT, NULL);\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT, NULL);\n@@ -2990,1 +2996,1 @@\n-  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false) {\n+  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _supers(nullptr) {\n@@ -3035,0 +3041,44 @@\n+\/\/ Returns all super methods (transitive) in classes and interfaces that are overridden by the current method.\n+GrowableArray<Method*>* CompiledEntrySignature::get_supers() {\n+  if (_supers != nullptr) {\n+    return _supers;\n+  }\n+  _supers = new GrowableArray<Method*>();\n+  \/\/ Skip private, static, and <init> methods\n+  if (_method->is_private() || _method->is_static() || _method->is_object_constructor()) {\n+    return _supers;\n+  }\n+  Symbol* name = _method->name();\n+  Symbol* signature = _method->signature();\n+  const Klass* holder = _method->method_holder()->super();\n+  Symbol* holder_name = holder->name();\n+  ThreadInVMfromUnknown tiv;\n+  JavaThread* current = JavaThread::current();\n+  HandleMark hm(current);\n+  Handle loader(current, _method->method_holder()->class_loader());\n+\n+  \/\/ Walk up the class hierarchy and search for super methods\n+  while (holder != NULL) {\n+    Method* super_method = holder->lookup_method(name, signature);\n+    if (super_method == NULL) {\n+      break;\n+    }\n+    if (!super_method->is_static() && !super_method->is_private() &&\n+        (!super_method->is_package_private() ||\n+         super_method->method_holder()->is_same_class_package(loader(), holder_name))) {\n+      _supers->push(super_method);\n+    }\n+    holder = super_method->method_holder()->super();\n+  }\n+  \/\/ Search interfaces for super methods\n+  Array<InstanceKlass*>* interfaces = _method->method_holder()->transitive_interfaces();\n+  for (int i = 0; i < interfaces->length(); ++i) {\n+    Method* m = interfaces->at(i)->lookup_method(name, signature);\n+    if (m != NULL && !m->is_static() && m->is_public()) {\n+      _supers->push(m);\n+    }\n+  }\n+  return _supers;\n+}\n+\n+\/\/ Iterate over arguments and compute scalarized and non-scalarized signatures\n@@ -3036,1 +3086,0 @@\n-  \/\/ Iterate over arguments and compute scalarized and non-scalarized signatures\n@@ -3059,11 +3108,54 @@\n-        \/\/ TODO 8284443 Mismatch handling, we need to check parent method args (look at klassVtable::needs_new_vtable_entry)\n-          _num_inline_args++;\n-          has_scalarized = true;\n-          int last = _sig_cc->length();\n-          int last_ro = _sig_cc_ro->length();\n-          _sig_cc->appendAll(vk->extended_sig());\n-          _sig_cc_ro->appendAll(vk->extended_sig());\n-          if (bt == T_OBJECT) {\n-            \/\/ Nullable inline type argument, insert InlineTypeNode::IsInit field right after T_PRIMITIVE_OBJECT\n-            _sig_cc->insert_before(last+1, SigEntry(T_BOOLEAN, -1, NULL));\n-            _sig_cc_ro->insert_before(last_ro+1, SigEntry(T_BOOLEAN, -1, NULL));\n+          \/\/ Check for a calling convention mismatch with super method(s)\n+          bool scalar_super = false;\n+          bool non_scalar_super = false;\n+          GrowableArray<Method*>* supers = get_supers();\n+          for (int i = 0; i < supers->length(); ++i) {\n+            Method* super_method = supers->at(i);\n+            if (super_method->is_scalarized_arg(arg_num)) {\n+              scalar_super = true;\n+            } else {\n+              non_scalar_super = true;\n+            }\n+          }\n+#ifdef ASSERT\n+          \/\/ Randomly enable below code paths for stress testing\n+          bool stress = init && StressCallingConvention;\n+          if (stress && (os::random() & 1) == 1) {\n+            non_scalar_super = true;\n+            if ((os::random() & 1) == 1) {\n+              scalar_super = true;\n+            }\n+          }\n+#endif\n+          if (non_scalar_super) {\n+            \/\/ Found a super method with a non-scalarized argument. Fall back to the non-scalarized calling convention.\n+            if (scalar_super) {\n+              \/\/ Found non-scalar *and* scalar super methods. We can't handle both.\n+              \/\/ Mark the scalar method as mismatch and re-compile call sites to use non-scalarized calling convention.\n+              for (int i = 0; i < supers->length(); ++i) {\n+                Method* super_method = supers->at(i);\n+                if (super_method->is_scalarized_arg(arg_num) debug_only(|| (stress && (os::random() & 1) == 1))) {\n+                  super_method->set_mismatch(true);\n+                  MutexLocker ml(Compile_lock, Mutex::_safepoint_check_flag);\n+                  JavaThread* thread = JavaThread::current();\n+                  HandleMark hm(thread);\n+                  methodHandle mh(thread, super_method);\n+                  CodeCache::flush_dependents_on_method(mh);\n+                }\n+              }\n+            }\n+            \/\/ Fall back to non-scalarized calling convention\n+            SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+            SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+          } else {\n+            _num_inline_args++;\n+            has_scalarized = true;\n+            int last = _sig_cc->length();\n+            int last_ro = _sig_cc_ro->length();\n+            _sig_cc->appendAll(vk->extended_sig());\n+            _sig_cc_ro->appendAll(vk->extended_sig());\n+            if (bt == T_OBJECT) {\n+              \/\/ Nullable inline type argument, insert InlineTypeNode::IsInit field right after T_PRIMITIVE_OBJECT\n+              _sig_cc->insert_before(last+1, SigEntry(T_BOOLEAN, -1, NULL));\n+              _sig_cc_ro->insert_before(last_ro+1, SigEntry(T_BOOLEAN, -1, NULL));\n+            }\n@@ -3156,2 +3248,2 @@\n-      GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc_ro().length(), mtInternal);\n-      heap_sig->appendAll(&ces.sig_cc_ro());\n+      GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc_ro()->length(), mtInternal);\n+      heap_sig->appendAll(ces.sig_cc_ro());\n@@ -3163,1 +3255,1 @@\n-    entry = lookup(&ces.sig_cc(), ces.has_inline_recv());\n+    entry = lookup(ces.sig_cc(), ces.has_inline_recv());\n@@ -3206,1 +3298,1 @@\n-  AdapterFingerPrint* fingerprint = new AdapterFingerPrint(&ces.sig_cc(), ces.has_inline_recv());\n+  AdapterFingerPrint* fingerprint = new AdapterFingerPrint(ces.sig_cc(), ces.has_inline_recv());\n@@ -3210,1 +3302,1 @@\n-                                                &ces.sig(),\n+                                                ces.sig(),\n@@ -3212,1 +3304,1 @@\n-                                                &ces.sig_cc(),\n+                                                ces.sig_cc(),\n@@ -3214,1 +3306,1 @@\n-                                                &ces.sig_cc_ro(),\n+                                                ces.sig_cc_ro(),\n@@ -3222,2 +3314,2 @@\n-    GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc().length(), mtInternal);\n-    heap_sig->appendAll(&ces.sig_cc());\n+    GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc()->length(), mtInternal);\n+    heap_sig->appendAll(ces.sig_cc());\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":141,"deletions":49,"binary":false,"changes":190,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-                                          CompiledMethod* caller_nm, bool is_virtual, bool is_optimized,\n+                                          CompiledMethod* caller_nm, bool is_virtual, bool is_optimized, bool& caller_is_c1,\n@@ -55,1 +55,1 @@\n-  static methodHandle resolve_sub_helper(bool is_virtual, bool is_optimized, bool* caller_is_c1, TRAPS);\n+  static methodHandle resolve_sub_helper(bool is_virtual, bool is_optimized, bool& caller_is_c1, TRAPS);\n@@ -324,1 +324,1 @@\n-  static methodHandle resolve_helper(bool is_virtual, bool is_optimized, bool* caller_is_c1, TRAPS);\n+  static methodHandle resolve_helper(bool is_virtual, bool is_optimized, bool& caller_is_c1, TRAPS);\n@@ -345,1 +345,1 @@\n-  static methodHandle find_callee_method(TRAPS);\n+  static methodHandle find_callee_method(bool is_optimized, bool& caller_is_c1, TRAPS);\n@@ -761,3 +761,3 @@\n-  GrowableArray<SigEntry> *_sig;\n-  GrowableArray<SigEntry> *_sig_cc;\n-  GrowableArray<SigEntry> *_sig_cc_ro;\n+  GrowableArray<SigEntry>* _sig;\n+  GrowableArray<SigEntry>* _sig_cc;\n+  GrowableArray<SigEntry>* _sig_cc_ro;\n@@ -775,0 +775,2 @@\n+  GrowableArray<Method*>* _supers;\n+\n@@ -779,1 +781,1 @@\n-  GrowableArray<SigEntry>& sig()       const { return *_sig; }\n+  GrowableArray<SigEntry>* sig()       const { return _sig; }\n@@ -782,1 +784,1 @@\n-  GrowableArray<SigEntry>& sig_cc()    const { return *_sig_cc; }\n+  GrowableArray<SigEntry>* sig_cc()    const { return _sig_cc; }\n@@ -785,1 +787,1 @@\n-  GrowableArray<SigEntry>& sig_cc_ro() const { return *_sig_cc_ro; }\n+  GrowableArray<SigEntry>* sig_cc_ro() const { return _sig_cc_ro; }\n@@ -803,0 +805,2 @@\n+  GrowableArray<Method*>* get_supers();\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":14,"deletions":10,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -158,0 +158,3 @@\n+    \/**\n+     * @hidden\n+     *\/\n@@ -164,0 +167,3 @@\n+    \/**\n+     * @hidden\n+     *\/\n@@ -175,0 +181,3 @@\n+    \/**\n+     * @hidden\n+     *\/\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/vm\/vector\/VectorSupport.java","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"}]}