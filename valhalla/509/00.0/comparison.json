{"files":[{"patch":"@@ -181,1 +181,1 @@\n-          sudo apt-get install gcc-10=10.2.0-5ubuntu1~20.04 g++-10=10.2.0-5ubuntu1~20.04 libxrandr-dev libxtst-dev libcups2-dev libasound2-dev\n+          sudo apt-get install gcc-10=10.3.0-1ubuntu1~20.04 g++-10=10.3.0-1ubuntu1~20.04 libxrandr-dev libxtst-dev libcups2-dev libasound2-dev\n@@ -496,1 +496,1 @@\n-          sudo apt-get install gcc-10=10.2.0-5ubuntu1~20.04 g++-10=10.2.0-5ubuntu1~20.04 libxrandr-dev libxtst-dev libcups2-dev libasound2-dev\n+          sudo apt-get install gcc-10=10.3.0-1ubuntu1~20.04 g++-10=10.3.0-1ubuntu1~20.04 libxrandr-dev libxtst-dev libcups2-dev libasound2-dev\n@@ -501,1 +501,1 @@\n-        run: sudo apt-get install gcc-10-${{ matrix.gnu-arch }}-linux-gnu${{ matrix.gnu-flavor}}=10.2.0-5ubuntu1~20.04cross1 g++-10-${{ matrix.gnu-arch }}-linux-gnu${{ matrix.gnu-flavor}}=10.2.0-5ubuntu1~20.04cross1\n+        run: sudo apt-get install gcc-10-${{ matrix.gnu-arch }}-linux-gnu${{ matrix.gnu-flavor}}=10.3.0-1ubuntu1~20.04cross1 g++-10-${{ matrix.gnu-arch }}-linux-gnu${{ matrix.gnu-flavor}}=10.3.0-1ubuntu1~20.04cross1\n@@ -919,1 +919,1 @@\n-          --add Microsoft.VisualStudio.Component.VC.14.28.arm64'\n+          --add Microsoft.VisualStudio.Component.VC.14.29.arm64'\n@@ -928,1 +928,1 @@\n-          --with-msvc-toolset-version=14.28\n+          --with-msvc-toolset-version=14.29\n","filename":".github\/workflows\/submit.yml","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1167,1 +1167,1 @@\n-            revision: \"3.0-7-jdk-asm+1.0\",\n+            revision: \"3.0-9-jdk-asm+1.0\",\n","filename":"make\/conf\/jib-profiles.js","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -119,0 +119,1 @@\n+      cdsProtectionDomain.cpp \\\n@@ -121,0 +122,3 @@\n+      dumpTimeSharedClassInfo.cpp \\\n+      lambdaProxyClassDictionary.cpp \\\n+      runTimeSharedClassInfo.cpp \\\n@@ -186,1 +190,0 @@\n-      biasedLocking.cpp \\\n","filename":"make\/hotspot\/lib\/JvmFeatures.gmk","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1298,1 +1298,25 @@\n- bool is_CAS(int opcode, bool maybe_volatile);\n+  static inline BasicType vector_element_basic_type(const MachNode* n) {\n+    const TypeVect* vt = n->bottom_type()->is_vect();\n+    return vt->element_basic_type();\n+  }\n+\n+  static inline BasicType vector_element_basic_type(const MachNode* use, const MachOper* opnd) {\n+    int def_idx = use->operand_index(opnd);\n+    Node* def = use->in(def_idx);\n+    const TypeVect* vt = def->bottom_type()->is_vect();\n+    return vt->element_basic_type();\n+  }\n+\n+  static inline uint vector_length(const MachNode* n) {\n+    const TypeVect* vt = n->bottom_type()->is_vect();\n+    return vt->length();\n+  }\n+\n+  static inline uint vector_length(const MachNode* use, const MachOper* opnd) {\n+    int def_idx = use->operand_index(opnd);\n+    Node* def = use->in(def_idx);\n+    const TypeVect* vt = def->bottom_type()->is_vect();\n+    return vt->length();\n+  }\n+\n+  bool is_CAS(int opcode, bool maybe_volatile);\n@@ -2419,0 +2443,8 @@\n+    case Op_VectorMaskCmp:\n+    \/\/ We don't have VectorReinterpret with bit_size less than 64 support for\n+    \/\/ now, even for byte type. To be refined with fully VectorCast support.\n+    case Op_VectorReinterpret:\n+      if (vlen < 2 || bit_size < 64) {\n+        return false;\n+      }\n+      break;\n@@ -2432,0 +2464,17 @@\n+    \/\/ Some types of VectorCast are not implemented for now.\n+    case Op_VectorCastI2X:\n+      if (bt == T_BYTE) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastS2X:\n+      if (vlen < 4 || bit_size < 64) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastF2X:\n+    case Op_VectorCastD2X:\n+      if (bt == T_INT || bt == T_SHORT || bt == T_BYTE || bt == T_LONG) {\n+        return false;\n+      }\n+      break;\n@@ -2457,4 +2506,0 @@\n-const int Matcher::float_pressure(int default_pressure_threshold) {\n-  return default_pressure_threshold;\n-}\n-\n@@ -2532,1 +2577,1 @@\n-bool Matcher::is_generic_reg2reg_move(MachNode* m) {\n+bool Matcher::is_reg2reg_move(MachNode* m) {\n@@ -2573,0 +2618,33 @@\n+uint Matcher::int_pressure_limit()\n+{\n+  \/\/ JDK-8183543: When taking the number of available registers as int\n+  \/\/ register pressure threshold, the jtreg test:\n+  \/\/ test\/hotspot\/jtreg\/compiler\/regalloc\/TestC2IntPressure.java\n+  \/\/ failed due to C2 compilation failure with\n+  \/\/ \"COMPILE SKIPPED: failed spill-split-recycle sanity check\".\n+  \/\/\n+  \/\/ A derived pointer is live at CallNode and then is flagged by RA\n+  \/\/ as a spilled LRG. Spilling heuristics(Spill-USE) explicitly skip\n+  \/\/ derived pointers and lastly fail to spill after reaching maximum\n+  \/\/ number of iterations. Lowering the default pressure threshold to\n+  \/\/ (_NO_SPECIAL_REG32_mask.Size() minus 1) forces CallNode to become\n+  \/\/ a high register pressure area of the code so that split_DEF can\n+  \/\/ generate DefinitionSpillCopy for the derived pointer.\n+  uint default_int_pressure_threshold = _NO_SPECIAL_REG32_mask.Size() - 1;\n+  if (!PreserveFramePointer) {\n+    \/\/ When PreserveFramePointer is off, frame pointer is allocatable,\n+    \/\/ but different from other SOC registers, it is excluded from\n+    \/\/ fatproj's mask because its save type is No-Save. Decrease 1 to\n+    \/\/ ensure high pressure at fatproj when PreserveFramePointer is off.\n+    \/\/ See check_pressure_at_fatproj().\n+    default_int_pressure_threshold--;\n+  }\n+  return (INTPRESSURE == -1) ? default_int_pressure_threshold : INTPRESSURE;\n+}\n+\n+uint Matcher::float_pressure_limit()\n+{\n+  \/\/ _FLOAT_REG_mask is generated by adlc from the float_reg register class.\n+  return (FLOATPRESSURE == -1) ? _FLOAT_REG_mask.Size() : FLOATPRESSURE;\n+}\n+\n@@ -3797,4 +3875,0 @@\n-    if (UseBiasedLocking && !UseOptoBiasInlining) {\n-      __ biased_locking_enter(box, oop, disp_hdr, tmp, true, cont);\n-    }\n-\n@@ -3808,1 +3882,0 @@\n-      assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n@@ -3877,4 +3950,0 @@\n-    if (UseBiasedLocking && !UseOptoBiasInlining) {\n-      __ biased_locking_exit(oop, tmp, cont);\n-    }\n-\n@@ -5493,0 +5562,9 @@\n+operand pReg()\n+%{\n+  constraint(ALLOC_IN_RC(pr_reg));\n+  match(RegVectMask);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n@@ -8917,5 +8995,0 @@\n-\n-\/\/ storeLConditional is used by PhaseMacroExpand::expand_lock_node\n-\/\/ when attempting to rebias a lock towards the current thread.  We\n-\/\/ must use the acquire form of cmpxchg in order to guarantee acquire\n-\/\/ semantics in this case.\n@@ -16715,2 +16788,2 @@\n-                              iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2,\n-                              iRegINoSp tmp3, rFlagsReg cr)\n+                             iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2,\n+                             iRegINoSp tmp3, rFlagsReg cr)\n@@ -16719,1 +16792,1 @@\n-  predicate(((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::U);\n+  predicate((UseSVE == 0) && (((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::U));\n@@ -16738,1 +16811,1 @@\n-  predicate(((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::L);\n+  predicate((UseSVE == 0) && (((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::L));\n@@ -16746,2 +16819,2 @@\n-                           $result$$Register, $tmp1$$Register, $tmp2$$Register,\n-                           $tmp3$$Register);\n+                            $result$$Register, $tmp1$$Register, $tmp2$$Register,\n+                            $tmp3$$Register);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":99,"deletions":26,"binary":false,"changes":125,"status":"modified"},{"patch":"@@ -452,2 +452,4 @@\n-  __ lea(rscratch2, ExternalAddress((address)&Runtime1::_arraycopy_slowcase_cnt));\n-  __ incrementw(Address(rscratch2));\n+  if (PrintC1Statistics) {\n+    __ lea(rscratch2, ExternalAddress((address)&Runtime1::_arraycopy_slowcase_cnt));\n+    __ incrementw(Address(rscratch2));\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -814,1 +814,1 @@\n-void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide, bool \/* unaligned *\/) {\n+void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide) {\n@@ -972,1 +972,1 @@\n-void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool \/* unaligned *\/) {\n+void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide) {\n@@ -2792,4 +2792,0 @@\n-    Register scratch = noreg;\n-    if (UseBiasedLocking) {\n-      scratch = op->scratch_opr()->as_register();\n-    }\n@@ -2798,1 +2794,1 @@\n-    int null_check_offset = __ lock_object(hdr, obj, lock, scratch, *op->stub()->entry());\n+    int null_check_offset = __ lock_object(hdr, obj, lock, *op->stub()->entry());\n@@ -3052,1 +3048,1 @@\n-        __ ldr(tmp, mdo_addr);\n+        __ str(tmp, mdo_addr);\n@@ -3146,1 +3142,1 @@\n-            \/*pop_fpu_stack*\/false, \/*unaligned*\/false, \/*wide*\/false);\n+            \/*pop_fpu_stack*\/false, \/*wide*\/false);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":5,"deletions":9,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -341,1 +341,1 @@\n-  \/\/ Need a scratch register for biased locking\n+  \/\/ Need a scratch register for inline type\n@@ -343,1 +343,1 @@\n-  if (UseBiasedLocking || x->maybe_inlinetype()) {\n+  if (EnableValhalla && x->maybe_inlinetype()) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -38,1 +38,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -66,1 +65,1 @@\n-int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Register scratch, Label& slow_case) {\n+int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Label& slow_case) {\n@@ -87,5 +86,0 @@\n-  if (UseBiasedLocking) {\n-    assert(scratch != noreg, \"should have scratch register at this point\");\n-    biased_locking_enter(disp_hdr, obj, hdr, scratch, false, done, &slow_case);\n-  }\n-\n@@ -98,1 +92,0 @@\n-    assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n@@ -134,4 +127,0 @@\n-  if (PrintBiasedLockingStatistics) {\n-    lea(rscratch2, ExternalAddress((address)BiasedLocking::fast_path_entry_count_addr()));\n-    addmw(Address(rscratch2, 0), 1, rscratch1);\n-  }\n@@ -148,6 +137,0 @@\n-  if (UseBiasedLocking) {\n-    \/\/ load object\n-    ldr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n-    biased_locking_exit(obj, hdr, done);\n-  }\n-\n@@ -159,4 +142,2 @@\n-  if (!UseBiasedLocking) {\n-    \/\/ load object\n-    ldr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n-  }\n+  \/\/ load object\n+  ldr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":3,"deletions":22,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -61,2 +61,1 @@\n-  \/\/ scratch : scratch register, contents destroyed\n-  int lock_object  (Register swap, Register obj, Register disp_hdr, Register scratch, Label& slow_case);\n+  int lock_object  (Register swap, Register obj, Register disp_hdr, Label& slow_case);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -368,1 +368,1 @@\n-JavaFrameAnchor* OptimizedEntryBlob::jfa_for_frame(const frame& frame) const {\n+OptimizedEntryBlob::FrameData* OptimizedEntryBlob::frame_data_for_frame(const frame& frame) const {\n@@ -373,0 +373,5 @@\n+bool frame::optimized_entry_frame_is_first() const {\n+  ShouldNotCallThis();\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -368,1 +368,1 @@\n-    ce->mem2reg(stub->addr(), stub->pre_val(), T_OBJECT, stub->patch_code(), stub->info(), false \/*wide*\/, false \/*unaligned*\/);\n+    ce->mem2reg(stub->addr(), stub->pre_val(), T_OBJECT, stub->patch_code(), stub->info(), false \/*wide*\/);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -616,1 +616,1 @@\n-    ce->mem2reg(stub->addr(), stub->pre_val(), T_OBJECT, stub->patch_code(), stub->info(), false \/*wide*\/, false \/*unaligned*\/);\n+    ce->mem2reg(stub->addr(), stub->pre_val(), T_OBJECT, stub->patch_code(), stub->info(), false \/*wide*\/);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -43,1 +43,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -852,4 +851,0 @@\n-    if (UseBiasedLocking) {\n-      biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, done, &slow_case);\n-    }\n-\n@@ -860,1 +855,0 @@\n-      assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n@@ -872,11 +866,1 @@\n-    if (PrintBiasedLockingStatistics) {\n-      Label fast;\n-      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, fast, &fail);\n-      bind(fast);\n-      atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n-                  rscratch2, rscratch1, tmp);\n-      b(done);\n-      bind(fail);\n-    } else {\n-      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n-    }\n+    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n@@ -919,6 +903,0 @@\n-\n-    if (PrintBiasedLockingStatistics) {\n-      br(Assembler::NE, slow_case);\n-      atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n-                  rscratch2, rscratch1, tmp);\n-    }\n@@ -975,4 +953,0 @@\n-    if (UseBiasedLocking) {\n-      biased_locking_exit(obj_reg, header_reg, done);\n-    }\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":1,"deletions":27,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -48,1 +48,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -448,172 +447,0 @@\n-void MacroAssembler::biased_locking_enter(Register lock_reg,\n-                                          Register obj_reg,\n-                                          Register swap_reg,\n-                                          Register tmp_reg,\n-                                          bool swap_reg_contains_mark,\n-                                          Label& done,\n-                                          Label* slow_case,\n-                                          BiasedLockingCounters* counters) {\n-  assert(UseBiasedLocking, \"why call this otherwise?\");\n-  assert_different_registers(lock_reg, obj_reg, swap_reg);\n-\n-  if (PrintBiasedLockingStatistics && counters == NULL)\n-    counters = BiasedLocking::counters();\n-\n-  assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg, rscratch1, rscratch2, noreg);\n-  assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, \"biased locking makes assumptions about bit layout\");\n-  Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());\n-  Address klass_addr     (obj_reg, oopDesc::klass_offset_in_bytes());\n-  Address saved_mark_addr(lock_reg, 0);\n-\n-  \/\/ Biased locking\n-  \/\/ See whether the lock is currently biased toward our thread and\n-  \/\/ whether the epoch is still valid\n-  \/\/ Note that the runtime guarantees sufficient alignment of JavaThread\n-  \/\/ pointers to allow age to be placed into low bits\n-  \/\/ First check to see whether biasing is even enabled for this object\n-  Label cas_label;\n-  if (!swap_reg_contains_mark) {\n-    ldr(swap_reg, mark_addr);\n-  }\n-  andr(tmp_reg, swap_reg, markWord::biased_lock_mask_in_place);\n-  cmp(tmp_reg, (u1)markWord::biased_lock_pattern);\n-  br(Assembler::NE, cas_label);\n-  \/\/ The bias pattern is present in the object's header. Need to check\n-  \/\/ whether the bias owner and the epoch are both still current.\n-  load_prototype_header(tmp_reg, obj_reg);\n-  orr(tmp_reg, tmp_reg, rthread);\n-  eor(tmp_reg, swap_reg, tmp_reg);\n-  andr(tmp_reg, tmp_reg, ~((int) markWord::age_mask_in_place));\n-  if (counters != NULL) {\n-    Label around;\n-    cbnz(tmp_reg, around);\n-    atomic_incw(Address((address)counters->biased_lock_entry_count_addr()), tmp_reg, rscratch1, rscratch2);\n-    b(done);\n-    bind(around);\n-  } else {\n-    cbz(tmp_reg, done);\n-  }\n-\n-  Label try_revoke_bias;\n-  Label try_rebias;\n-\n-  \/\/ At this point we know that the header has the bias pattern and\n-  \/\/ that we are not the bias owner in the current epoch. We need to\n-  \/\/ figure out more details about the state of the header in order to\n-  \/\/ know what operations can be legally performed on the object's\n-  \/\/ header.\n-\n-  \/\/ If the low three bits in the xor result aren't clear, that means\n-  \/\/ the prototype header is no longer biased and we have to revoke\n-  \/\/ the bias on this object.\n-  andr(rscratch1, tmp_reg, markWord::biased_lock_mask_in_place);\n-  cbnz(rscratch1, try_revoke_bias);\n-\n-  \/\/ Biasing is still enabled for this data type. See whether the\n-  \/\/ epoch of the current bias is still valid, meaning that the epoch\n-  \/\/ bits of the mark word are equal to the epoch bits of the\n-  \/\/ prototype header. (Note that the prototype header's epoch bits\n-  \/\/ only change at a safepoint.) If not, attempt to rebias the object\n-  \/\/ toward the current thread. Note that we must be absolutely sure\n-  \/\/ that the current epoch is invalid in order to do this because\n-  \/\/ otherwise the manipulations it performs on the mark word are\n-  \/\/ illegal.\n-  andr(rscratch1, tmp_reg, markWord::epoch_mask_in_place);\n-  cbnz(rscratch1, try_rebias);\n-\n-  \/\/ The epoch of the current bias is still valid but we know nothing\n-  \/\/ about the owner; it might be set or it might be clear. Try to\n-  \/\/ acquire the bias of the object using an atomic operation. If this\n-  \/\/ fails we will go in to the runtime to revoke the object's bias.\n-  \/\/ Note that we first construct the presumed unbiased header so we\n-  \/\/ don't accidentally blow away another thread's valid bias.\n-  {\n-    Label here;\n-    mov(rscratch1, markWord::biased_lock_mask_in_place | markWord::age_mask_in_place | markWord::epoch_mask_in_place);\n-    andr(swap_reg, swap_reg, rscratch1);\n-    orr(tmp_reg, swap_reg, rthread);\n-    cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, rscratch1, here, slow_case);\n-    \/\/ If the biasing toward our thread failed, this means that\n-    \/\/ another thread succeeded in biasing it toward itself and we\n-    \/\/ need to revoke that bias. The revocation will occur in the\n-    \/\/ interpreter runtime in the slow case.\n-    bind(here);\n-    if (counters != NULL) {\n-      atomic_incw(Address((address)counters->anonymously_biased_lock_entry_count_addr()),\n-                  tmp_reg, rscratch1, rscratch2);\n-    }\n-  }\n-  b(done);\n-\n-  bind(try_rebias);\n-  \/\/ At this point we know the epoch has expired, meaning that the\n-  \/\/ current \"bias owner\", if any, is actually invalid. Under these\n-  \/\/ circumstances _only_, we are allowed to use the current header's\n-  \/\/ value as the comparison value when doing the cas to acquire the\n-  \/\/ bias in the current epoch. In other words, we allow transfer of\n-  \/\/ the bias from one thread to another directly in this situation.\n-  \/\/\n-  \/\/ FIXME: due to a lack of registers we currently blow away the age\n-  \/\/ bits in this situation. Should attempt to preserve them.\n-  {\n-    Label here;\n-    load_prototype_header(tmp_reg, obj_reg);\n-    orr(tmp_reg, rthread, tmp_reg);\n-    cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, rscratch1, here, slow_case);\n-    \/\/ If the biasing toward our thread failed, then another thread\n-    \/\/ succeeded in biasing it toward itself and we need to revoke that\n-    \/\/ bias. The revocation will occur in the runtime in the slow case.\n-    bind(here);\n-    if (counters != NULL) {\n-      atomic_incw(Address((address)counters->rebiased_lock_entry_count_addr()),\n-                  tmp_reg, rscratch1, rscratch2);\n-    }\n-  }\n-  b(done);\n-\n-  bind(try_revoke_bias);\n-  \/\/ The prototype mark in the klass doesn't have the bias bit set any\n-  \/\/ more, indicating that objects of this data type are not supposed\n-  \/\/ to be biased any more. We are going to try to reset the mark of\n-  \/\/ this object to the prototype value and fall through to the\n-  \/\/ CAS-based locking scheme. Note that if our CAS fails, it means\n-  \/\/ that another thread raced us for the privilege of revoking the\n-  \/\/ bias of this particular object, so it's okay to continue in the\n-  \/\/ normal locking code.\n-  \/\/\n-  \/\/ FIXME: due to a lack of registers we currently blow away the age\n-  \/\/ bits in this situation. Should attempt to preserve them.\n-  {\n-    Label here, nope;\n-    load_prototype_header(tmp_reg, obj_reg);\n-    cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, rscratch1, here, &nope);\n-    bind(here);\n-\n-    \/\/ Fall through to the normal CAS-based lock, because no matter what\n-    \/\/ the result of the above CAS, some thread must have succeeded in\n-    \/\/ removing the bias bit from the object's header.\n-    if (counters != NULL) {\n-      atomic_incw(Address((address)counters->revoked_lock_entry_count_addr()), tmp_reg,\n-                  rscratch1, rscratch2);\n-    }\n-    bind(nope);\n-  }\n-\n-  bind(cas_label);\n-}\n-\n-void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label& done) {\n-  assert(UseBiasedLocking, \"why call this otherwise?\");\n-\n-  \/\/ Check for biased locking unlock case, which is a no-op\n-  \/\/ Note: we do not have to check the thread ID for two reasons.\n-  \/\/ First, the interpreter checks for IllegalMonitorStateException at\n-  \/\/ a higher level. Second, if the bias was revoked while we held the\n-  \/\/ lock, the object could not be rebiased toward another thread, so\n-  \/\/ the bias bit would be clear.\n-  ldr(temp_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-  andr(temp_reg, temp_reg, markWord::biased_lock_mask_in_place);\n-  cmp(temp_reg, (u1)markWord::biased_lock_pattern);\n-  br(Assembler::EQ, done);\n-}\n-\n@@ -6080,43 +5907,0 @@\n-void MacroAssembler::neon_compare(FloatRegister dst, BasicType bt, FloatRegister src1,\n-                                  FloatRegister src2, int cond, bool isQ) {\n-  SIMD_Arrangement size = esize2arrangement(type2aelembytes(bt), isQ);\n-  if (bt == T_FLOAT || bt == T_DOUBLE) {\n-    switch (cond) {\n-      case BoolTest::eq: fcmeq(dst, size, src1, src2); break;\n-      case BoolTest::ne: {\n-        fcmeq(dst, size, src1, src2);\n-        notr(dst, T16B, dst);\n-        break;\n-      }\n-      case BoolTest::ge: fcmge(dst, size, src1, src2); break;\n-      case BoolTest::gt: fcmgt(dst, size, src1, src2); break;\n-      case BoolTest::le: fcmge(dst, size, src2, src1); break;\n-      case BoolTest::lt: fcmgt(dst, size, src2, src1); break;\n-      default:\n-        assert(false, \"unsupported\");\n-        ShouldNotReachHere();\n-    }\n-  } else {\n-    switch (cond) {\n-      case BoolTest::eq: cmeq(dst, size, src1, src2); break;\n-      case BoolTest::ne: {\n-        cmeq(dst, size, src1, src2);\n-        notr(dst, T16B, dst);\n-        break;\n-      }\n-      case BoolTest::ge: cmge(dst, size, src1, src2); break;\n-      case BoolTest::gt: cmgt(dst, size, src1, src2); break;\n-      case BoolTest::le: cmge(dst, size, src2, src1); break;\n-      case BoolTest::lt: cmgt(dst, size, src2, src1); break;\n-      case BoolTest::uge: cmhs(dst, size, src1, src2); break;\n-      case BoolTest::ugt: cmhi(dst, size, src1, src2); break;\n-      case BoolTest::ult: cmhi(dst, size, src2, src1); break;\n-      case BoolTest::ule: cmhs(dst, size, src2, src1); break;\n-      default:\n-        assert(false, \"unsupported\");\n-        ShouldNotReachHere();\n-    }\n-  }\n-}\n-\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":0,"deletions":216,"binary":false,"changes":216,"status":"modified"},{"patch":"@@ -113,14 +113,0 @@\n-  \/\/ Biased locking support\n-  \/\/ lock_reg and obj_reg must be loaded up with the appropriate values.\n-  \/\/ swap_reg is killed.\n-  \/\/ tmp_reg must be supplied and must not be rscratch1 or rscratch2\n-  \/\/ Optional slow case is for implementations (interpreter and C1) which branch to\n-  \/\/ slow case directly. Leaves condition codes set for C2's Fast_Lock node.\n-  void biased_locking_enter(Register lock_reg, Register obj_reg,\n-                            Register swap_reg, Register tmp_reg,\n-                            bool swap_reg_contains_mark,\n-                            Label& done, Label* slow_case = NULL,\n-                            BiasedLockingCounters* counters = NULL);\n-  void biased_locking_exit (Register obj_reg, Register temp_reg, Label& done);\n-\n-\n@@ -1120,2 +1106,0 @@\n-  \/\/ SIMD&FP comparison\n-  void neon_compare(FloatRegister dst, BasicType bt, FloatRegister src1, FloatRegister src2, int cond, bool isQ);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":0,"deletions":16,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2047,4 +2047,0 @@\n-    if (UseBiasedLocking) {\n-      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, lock_done, &slow_path_lock);\n-    }\n-\n@@ -2055,1 +2051,0 @@\n-      assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n@@ -2204,5 +2199,0 @@\n-\n-    if (UseBiasedLocking) {\n-      __ biased_locking_exit(obj_reg, old_hdr, done);\n-    }\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -5712,0 +5712,1 @@\n+   *  c_rarg6   - isMIME\n@@ -5794,6 +5795,7 @@\n-    Register src   = c_rarg0;  \/\/ source array\n-    Register soff  = c_rarg1;  \/\/ source start offset\n-    Register send  = c_rarg2;  \/\/ source end offset\n-    Register dst   = c_rarg3;  \/\/ dest array\n-    Register doff  = c_rarg4;  \/\/ position for writing to dest array\n-    Register isURL = c_rarg5;  \/\/ Base64 or URL character set\n+    Register src    = c_rarg0;  \/\/ source array\n+    Register soff   = c_rarg1;  \/\/ source start offset\n+    Register send   = c_rarg2;  \/\/ source end offset\n+    Register dst    = c_rarg3;  \/\/ dest array\n+    Register doff   = c_rarg4;  \/\/ position for writing to dest array\n+    Register isURL  = c_rarg5;  \/\/ Base64 or URL character set\n+    Register isMIME = c_rarg6;  \/\/ Decoding MIME block - unused in this implementation\n@@ -5983,0 +5985,4 @@\n+      case memory_order_release:\n+        acquire = false;\n+        release = true;\n+        break;\n@@ -6064,0 +6070,14 @@\n+    AtomicStubMark mark_cmpxchg_4_release\n+      (_masm, &aarch64_atomic_cmpxchg_4_release_impl);\n+    gen_cas_entry(MacroAssembler::word, memory_order_release);\n+    AtomicStubMark mark_cmpxchg_8_release\n+      (_masm, &aarch64_atomic_cmpxchg_8_release_impl);\n+    gen_cas_entry(MacroAssembler::xword, memory_order_release);\n+\n+    AtomicStubMark mark_cmpxchg_4_seq_cst\n+      (_masm, &aarch64_atomic_cmpxchg_4_seq_cst_impl);\n+    gen_cas_entry(MacroAssembler::word, memory_order_seq_cst);\n+    AtomicStubMark mark_cmpxchg_8_seq_cst\n+      (_masm, &aarch64_atomic_cmpxchg_8_seq_cst_impl);\n+    gen_cas_entry(MacroAssembler::xword, memory_order_seq_cst);\n+\n@@ -7365,0 +7385,4 @@\n+DEFAULT_ATOMIC_OP(cmpxchg, 4, _release)\n+DEFAULT_ATOMIC_OP(cmpxchg, 8, _release)\n+DEFAULT_ATOMIC_OP(cmpxchg, 4, _seq_cst)\n+DEFAULT_ATOMIC_OP(cmpxchg, 8, _seq_cst)\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":30,"deletions":6,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -250,1 +250,1 @@\n-    __ unlock_object(R2, R1, R0, Rtemp, *stub->entry());\n+    __ unlock_object(R2, R1, R0, *stub->entry());\n@@ -497,2 +497,1 @@\n-                            bool pop_fpu_stack, bool wide,\n-                            bool unaligned) {\n+                            bool pop_fpu_stack, bool wide) {\n@@ -698,1 +697,1 @@\n-                            bool wide, bool unaligned) {\n+                            bool wide) {\n@@ -2432,2 +2431,0 @@\n-  Register tmp = op->scratch_opr()->is_illegal() ? noreg :\n-                 op->scratch_opr()->as_pointer_register();\n@@ -2439,1 +2436,1 @@\n-    int null_check_offset = __ lock_object(hdr, obj, lock, tmp, *op->stub()->entry());\n+    int null_check_offset = __ lock_object(hdr, obj, lock, *op->stub()->entry());\n@@ -2444,1 +2441,1 @@\n-    __ unlock_object(hdr, obj, lock, tmp, *op->stub()->entry());\n+    __ unlock_object(hdr, obj, lock, *op->stub()->entry());\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":5,"deletions":8,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -717,1 +717,1 @@\n-int LIR_Assembler::store(LIR_Opr from_reg, Register base, int offset, BasicType type, bool wide, bool unaligned) {\n+int LIR_Assembler::store(LIR_Opr from_reg, Register base, int offset, BasicType type, bool wide) {\n@@ -797,1 +797,1 @@\n-int LIR_Assembler::load(Register base, int offset, LIR_Opr to_reg, BasicType type, bool wide, bool unaligned) {\n+int LIR_Assembler::load(Register base, int offset, LIR_Opr to_reg, BasicType type, bool wide) {\n@@ -968,1 +968,1 @@\n-    offset = store(tmp, base, addr->disp(), type, wide, false);\n+    offset = store(tmp, base, addr->disp(), type, wide);\n@@ -1123,1 +1123,1 @@\n-                            LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool unaligned) {\n+                            LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide) {\n@@ -1173,1 +1173,1 @@\n-    offset = load(src, disp_value, to_reg, type, wide, unaligned);\n+    offset = load(src, disp_value, to_reg, type, wide);\n@@ -1175,1 +1175,0 @@\n-    assert(!unaligned, \"unexpected\");\n@@ -1196,2 +1195,1 @@\n-  bool unaligned = addr.disp() % 8 != 0;\n-  load(addr.base(), addr.disp(), dest, dest->type(), true \/*wide*\/, unaligned);\n+  load(addr.base(), addr.disp(), dest, dest->type(), true \/*wide*\/);\n@@ -1208,2 +1206,2 @@\n-  bool unaligned = addr.disp() % 8 != 0;\n-  store(from_reg, addr.base(), addr.disp(), from_reg->type(), true \/*wide*\/, unaligned);\n+\n+  store(from_reg, addr.base(), addr.disp(), from_reg->type(), true \/*wide*\/);\n@@ -1245,1 +1243,1 @@\n-                            bool wide, bool unaligned) {\n+                            bool wide) {\n@@ -1302,1 +1300,1 @@\n-    offset = store(from_reg, src, disp_value, type, wide, unaligned);\n+    offset = store(from_reg, src, disp_value, type, wide);\n@@ -1304,1 +1302,0 @@\n-    assert(!unaligned, \"unexpected\");\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":10,"deletions":13,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -946,4 +946,0 @@\n-    if (UseBiasedLocking) {\n-      biased_locking_enter(CCR0, object, displaced_header, tmp, current_header, done, &slow_case);\n-    }\n-\n@@ -1051,7 +1047,0 @@\n-    if (UseBiasedLocking) {\n-      \/\/ The object address from the monitor is in object.\n-      ld(object, BasicObjectLock::obj_offset_in_bytes(), monitor);\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-      biased_locking_exit(CCR0, object, displaced_header, free_slot);\n-    }\n-\n@@ -1073,1 +1062,1 @@\n-    if (!UseBiasedLocking) { ld(object, BasicObjectLock::obj_offset_in_bytes(), monitor); }\n+    ld(object, BasicObjectLock::obj_offset_in_bytes(), monitor);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":1,"deletions":12,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -882,1 +882,1 @@\n-                            CodeEmitInfo* info, bool wide, bool unaligned) {\n+                            CodeEmitInfo* info, bool wide) {\n@@ -1082,1 +1082,1 @@\n-                            bool wide, bool unaligned) {\n+                            bool wide) {\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -41,1 +41,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -1008,4 +1007,0 @@\n-  if (UseBiasedLocking) {\n-    biased_locking_enter(object, displaced_header, Z_R1, Z_R0, done, &slow_case);\n-  }\n-\n@@ -1119,6 +1114,0 @@\n-  if (UseBiasedLocking) {\n-    \/\/ The object address from the monitor is in object.\n-    assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-    biased_locking_exit(object, displaced_header, done);\n-  }\n-\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -641,1 +641,3 @@\n-  __ incrementl(ExternalAddress((address)&Runtime1::_arraycopy_slowcase_cnt));\n+  if (PrintC1Statistics) {\n+    __ incrementl(ExternalAddress((address)&Runtime1::_arraycopy_slowcase_cnt));\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -983,1 +983,1 @@\n-void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide, bool \/* unaligned *\/) {\n+void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide) {\n@@ -1209,1 +1209,1 @@\n-void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool \/* unaligned *\/) {\n+void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide) {\n@@ -3725,4 +3725,0 @@\n-    Register scratch = noreg;\n-    if (UseBiasedLocking) {\n-      scratch = op->scratch_opr()->as_register();\n-    }\n@@ -3731,1 +3727,1 @@\n-    int null_check_offset = __ lock_object(hdr, obj, lock, scratch, *op->stub()->entry());\n+    int null_check_offset = __ lock_object(hdr, obj, lock, *op->stub()->entry());\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -305,1 +305,1 @@\n-  \/\/ Need a scratch register for biased locking on x86\n+  \/\/ Need a scratch register for inline types on x86\n@@ -307,1 +307,1 @@\n-  if (UseBiasedLocking || x->maybe_inlinetype()) {\n+  if (EnableValhalla && x->maybe_inlinetype()) {\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -42,1 +41,1 @@\n-int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Register scratch, Label& slow_case) {\n+int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Label& slow_case) {\n@@ -65,5 +64,0 @@\n-  if (UseBiasedLocking) {\n-    assert(scratch != noreg, \"should have scratch register at this point\");\n-    biased_locking_enter(disp_hdr, obj, hdr, scratch, rklass_decode_tmp, false, done, &slow_case);\n-  }\n-\n@@ -75,1 +69,0 @@\n-    assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n@@ -87,4 +80,0 @@\n-  if (PrintBiasedLockingStatistics) {\n-    cond_inc32(Assembler::equal,\n-               ExternalAddress((address)BiasedLocking::fast_path_entry_count_addr()));\n-  }\n@@ -125,6 +114,0 @@\n-  if (UseBiasedLocking) {\n-    \/\/ load object\n-    movptr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n-    biased_locking_exit(obj, hdr, done);\n-  }\n-\n@@ -137,4 +120,3 @@\n-  if (!UseBiasedLocking) {\n-    \/\/ load object\n-    movptr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n-  }\n+  \/\/ load object\n+  movptr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":4,"deletions":22,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -321,1 +321,5 @@\n-\n+\/\/\n+\/\/ Register is a class, but it would be assigned numerical value.\n+\/\/ \"0\" is assigned for rax. Thus we need to ignore -Wnonnull.\n+PRAGMA_DIAG_PUSH\n+PRAGMA_NONNULL_IGNORED\n@@ -421,0 +425,1 @@\n+PRAGMA_DIAG_POP\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -33,1 +33,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -237,1 +236,0 @@\n-  assert(!UseBiasedLocking, \"Biased locking is not supported with RTM locking\");\n@@ -247,1 +245,1 @@\n-  testptr(tmpReg, markWord::monitor_value);  \/\/ inflated vs stack-locked|neutral|biased\n+  testptr(tmpReg, markWord::monitor_value);  \/\/ inflated vs stack-locked|neutral\n@@ -262,2 +260,2 @@\n-  andptr(tmpReg, markWord::biased_lock_mask_in_place); \/\/ look at 3 lock bits\n-  cmpptr(tmpReg, markWord::unlocked_value);            \/\/ bits = 001 unlocked\n+  andptr(tmpReg, markWord::lock_mask_in_place);     \/\/ look at 2 lock bits\n+  cmpptr(tmpReg, markWord::unlocked_value);         \/\/ bits = 01 unlocked\n@@ -450,1 +448,0 @@\n-                                 BiasedLockingCounters* counters,\n@@ -465,4 +462,0 @@\n-  if (counters != NULL) {\n-    atomic_incl(ExternalAddress((address)counters->total_entry_count_addr()), scrReg);\n-  }\n-\n@@ -476,3 +469,0 @@\n-  \/\/ * biased\n-  \/\/    -- by Self\n-  \/\/    -- by other\n@@ -496,10 +486,0 @@\n-  \/\/ it's stack-locked, biased or neutral\n-  \/\/ TODO: optimize away redundant LDs of obj->mark and improve the markword triage\n-  \/\/ order to reduce the number of conditional branches in the most common cases.\n-  \/\/ Beware -- there's a subtle invariant that fetch of the markword\n-  \/\/ at [FETCH], below, will never observe a biased encoding (*101b).\n-  \/\/ If this invariant is not held we risk exclusion (safety) failure.\n-  if (UseBiasedLocking && !UseOptoBiasInlining) {\n-    biased_locking_enter(boxReg, objReg, tmpReg, scrReg, cx1Reg, false, DONE_LABEL, NULL, counters);\n-  }\n-\n@@ -515,1 +495,1 @@\n-  testptr(tmpReg, markWord::monitor_value); \/\/ inflated vs stack-locked|neutral|biased\n+  testptr(tmpReg, markWord::monitor_value); \/\/ inflated vs stack-locked|neutral\n@@ -521,1 +501,0 @@\n-    assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n@@ -528,4 +507,0 @@\n-  if (counters != NULL) {\n-    cond_inc32(Assembler::equal,\n-               ExternalAddress((address)counters->fast_path_entry_count_addr()));\n-  }\n@@ -541,4 +516,0 @@\n-  if (counters != NULL) {\n-    cond_inc32(Assembler::equal,\n-               ExternalAddress((address)counters->fast_path_entry_count_addr()));\n-  }\n@@ -667,6 +638,0 @@\n-  \/\/ Critically, the biased locking test must have precedence over\n-  \/\/ and appear before the (box->dhw == 0) recursive stack-lock test.\n-  if (UseBiasedLocking && !UseOptoBiasInlining) {\n-    biased_locking_exit(objReg, tmpReg, DONE_LABEL);\n-  }\n-\n@@ -675,1 +640,0 @@\n-    assert(!UseBiasedLocking, \"Biased locking is not supported with RTM locking\");\n@@ -678,2 +642,2 @@\n-    andptr(tmpReg, markWord::biased_lock_mask_in_place);              \/\/ look at 3 lock bits\n-    cmpptr(tmpReg, markWord::unlocked_value);                         \/\/ bits = 001 unlocked\n+    andptr(tmpReg, markWord::lock_mask_in_place);                     \/\/ look at 2 lock bits\n+    cmpptr(tmpReg, markWord::unlocked_value);                         \/\/ bits = 01 unlocked\n@@ -746,1 +710,1 @@\n-  \/\/ It's not inflated and it's not recursively stack-locked and it's not biased.\n+  \/\/ It's not inflated and it's not recursively stack-locked.\n@@ -1470,1 +1434,1 @@\n-void C2_MacroAssembler::load_vector_mask(XMMRegister dst, XMMRegister src, int vlen_in_bytes, BasicType elem_bt) {\n+void C2_MacroAssembler::load_vector_mask(XMMRegister dst, XMMRegister src, int vlen_in_bytes, BasicType elem_bt, bool is_legacy) {\n@@ -1485,0 +1449,1 @@\n+    assert(!is_legacy || !is_subword_type(elem_bt) || vlen_in_bytes < 64, \"\");\n@@ -1488,1 +1453,2 @@\n-    vpsubb(dst, dst, src, vlen_enc);\n+    vpsubb(dst, dst, src, is_legacy ? AVX_256bit : vlen_enc);\n+\n@@ -1504,1 +1470,5 @@\n-  if (vlen_in_bytes <= 16) {\n+  if (vlen_in_bytes == 4) {\n+    movdl(dst, addr);\n+  } else if (vlen_in_bytes == 8) {\n+    movq(dst, addr);\n+  } else if (vlen_in_bytes == 16) {\n@@ -1513,0 +1483,1 @@\n+\n@@ -3895,0 +3866,3 @@\n+  if (masklen < 64) {\n+    andq(tmp, (((jlong)1 << masklen) - 1));\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":20,"deletions":46,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -359,1 +359,2 @@\n-JavaFrameAnchor* OptimizedEntryBlob::jfa_for_frame(const frame& frame) const {\n+OptimizedEntryBlob::FrameData* OptimizedEntryBlob::frame_data_for_frame(const frame& frame) const {\n+  assert(frame.is_optimized_entry_frame(), \"wrong frame\");\n@@ -361,1 +362,9 @@\n-  return reinterpret_cast<JavaFrameAnchor*>(reinterpret_cast<char*>(frame.unextended_sp()) + in_bytes(jfa_sp_offset()));\n+  return reinterpret_cast<OptimizedEntryBlob::FrameData*>(\n+    reinterpret_cast<char*>(frame.unextended_sp()) + in_bytes(_frame_data_offset));\n+}\n+\n+bool frame::optimized_entry_frame_is_first() const {\n+  assert(is_optimized_entry_frame(), \"must be optimzed entry frame\");\n+  OptimizedEntryBlob* blob = _cb->as_optimized_entry_blob();\n+  JavaFrameAnchor* jfa = blob->jfa_for_frame(*this);\n+  return jfa->last_Java_sp() == NULL;\n@@ -370,0 +379,1 @@\n+  assert(!optimized_entry_frame_is_first(), \"must have a frame anchor to go back to\");\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -449,1 +449,1 @@\n-    ce->mem2reg(stub->addr(), stub->pre_val(), T_OBJECT, stub->patch_code(), stub->info(), false \/*wide*\/, false \/*unaligned*\/);\n+    ce->mem2reg(stub->addr(), stub->pre_val(), T_OBJECT, stub->patch_code(), stub->info(), false \/*wide*\/);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -847,1 +847,1 @@\n-    ce->mem2reg(stub->addr(), stub->pre_val(), T_OBJECT, stub->patch_code(), stub->info(), false \/*wide*\/, false \/*unaligned*\/);\n+    ce->mem2reg(stub->addr(), stub->pre_val(), T_OBJECT, stub->patch_code(), stub->info(), false \/*wide*\/);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -470,0 +470,4 @@\n+\/\/ Register is a class, but it would be assigned numerical value.\n+\/\/ \"0\" is assigned for rax. Thus we need to ignore -Wnonnull.\n+PRAGMA_DIAG_PUSH\n+PRAGMA_NONNULL_IGNORED\n@@ -545,0 +549,1 @@\n+PRAGMA_DIAG_POP\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zBarrierSetAssembler_x86.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -115,0 +115,3 @@\n+  product(bool, UseKNLSetting, false, DIAGNOSTIC,                           \\\n+          \"Control whether Knights platform setting should be used\")        \\\n+                                                                            \\\n","filename":"src\/hotspot\/cpu\/x86\/globals_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -39,1 +39,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -1352,2 +1351,1 @@\n-    const Register tmp_reg = rbx; \/\/ Will be passed to biased_locking_enter to avoid a\n-                                  \/\/ problematic case where tmp_reg = no_reg.\n+    const Register tmp_reg = rbx;\n@@ -1374,4 +1372,0 @@\n-    if (UseBiasedLocking) {\n-      biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp_reg, rklass_decode_tmp, false, done, &slow_case);\n-    }\n-\n@@ -1384,1 +1378,0 @@\n-      assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n@@ -1397,4 +1390,0 @@\n-    if (PrintBiasedLockingStatistics) {\n-      cond_inc32(Assembler::zero,\n-                 ExternalAddress((address) BiasedLocking::fast_path_entry_count_addr()));\n-    }\n@@ -1437,5 +1426,0 @@\n-\n-    if (PrintBiasedLockingStatistics) {\n-      cond_inc32(Assembler::zero,\n-                 ExternalAddress((address) BiasedLocking::fast_path_entry_count_addr()));\n-    }\n@@ -1493,4 +1477,0 @@\n-    if (UseBiasedLocking) {\n-      biased_locking_exit(obj_reg, header_reg, done);\n-    }\n-\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":1,"deletions":21,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -44,1 +44,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -1289,194 +1288,0 @@\n-void MacroAssembler::biased_locking_enter(Register lock_reg,\n-                                          Register obj_reg,\n-                                          Register swap_reg,\n-                                          Register tmp_reg,\n-                                          Register tmp_reg2,\n-                                          bool swap_reg_contains_mark,\n-                                          Label& done,\n-                                          Label* slow_case,\n-                                          BiasedLockingCounters* counters) {\n-  assert(UseBiasedLocking, \"why call this otherwise?\");\n-  assert(swap_reg == rax, \"swap_reg must be rax for cmpxchgq\");\n-  assert(tmp_reg != noreg, \"tmp_reg must be supplied\");\n-  assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg);\n-  assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, \"biased locking makes assumptions about bit layout\");\n-  Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());\n-  NOT_LP64( Address saved_mark_addr(lock_reg, 0); )\n-\n-  if (PrintBiasedLockingStatistics && counters == NULL) {\n-    counters = BiasedLocking::counters();\n-  }\n-  \/\/ Biased locking\n-  \/\/ See whether the lock is currently biased toward our thread and\n-  \/\/ whether the epoch is still valid\n-  \/\/ Note that the runtime guarantees sufficient alignment of JavaThread\n-  \/\/ pointers to allow age to be placed into low bits\n-  \/\/ First check to see whether biasing is even enabled for this object\n-  Label cas_label;\n-  if (!swap_reg_contains_mark) {\n-    movptr(swap_reg, mark_addr);\n-  }\n-  movptr(tmp_reg, swap_reg);\n-  andptr(tmp_reg, markWord::biased_lock_mask_in_place);\n-  cmpptr(tmp_reg, markWord::biased_lock_pattern);\n-  jcc(Assembler::notEqual, cas_label);\n-  \/\/ The bias pattern is present in the object's header. Need to check\n-  \/\/ whether the bias owner and the epoch are both still current.\n-#ifndef _LP64\n-  \/\/ Note that because there is no current thread register on x86_32 we\n-  \/\/ need to store off the mark word we read out of the object to\n-  \/\/ avoid reloading it and needing to recheck invariants below. This\n-  \/\/ store is unfortunate but it makes the overall code shorter and\n-  \/\/ simpler.\n-  movptr(saved_mark_addr, swap_reg);\n-#endif\n-  load_prototype_header(tmp_reg, obj_reg, tmp_reg2);\n-#ifdef _LP64\n-  orptr(tmp_reg, r15_thread);\n-  xorptr(tmp_reg, swap_reg);\n-  Register header_reg = tmp_reg;\n-#else\n-  xorptr(tmp_reg, swap_reg);\n-  get_thread(swap_reg);\n-  xorptr(swap_reg, tmp_reg);\n-  Register header_reg = swap_reg;\n-#endif\n-  andptr(header_reg, ~((int) markWord::age_mask_in_place));\n-  if (counters != NULL) {\n-    cond_inc32(Assembler::zero,\n-               ExternalAddress((address) counters->biased_lock_entry_count_addr()));\n-  }\n-  jcc(Assembler::equal, done);\n-\n-  Label try_revoke_bias;\n-  Label try_rebias;\n-\n-  \/\/ At this point we know that the header has the bias pattern and\n-  \/\/ that we are not the bias owner in the current epoch. We need to\n-  \/\/ figure out more details about the state of the header in order to\n-  \/\/ know what operations can be legally performed on the object's\n-  \/\/ header.\n-\n-  \/\/ If the low three bits in the xor result aren't clear, that means\n-  \/\/ the prototype header is no longer biased and we have to revoke\n-  \/\/ the bias on this object.\n-  testptr(header_reg, markWord::biased_lock_mask_in_place);\n-  jcc(Assembler::notZero, try_revoke_bias);\n-\n-  \/\/ Biasing is still enabled for this data type. See whether the\n-  \/\/ epoch of the current bias is still valid, meaning that the epoch\n-  \/\/ bits of the mark word are equal to the epoch bits of the\n-  \/\/ prototype header. (Note that the prototype header's epoch bits\n-  \/\/ only change at a safepoint.) If not, attempt to rebias the object\n-  \/\/ toward the current thread. Note that we must be absolutely sure\n-  \/\/ that the current epoch is invalid in order to do this because\n-  \/\/ otherwise the manipulations it performs on the mark word are\n-  \/\/ illegal.\n-  testptr(header_reg, markWord::epoch_mask_in_place);\n-  jccb(Assembler::notZero, try_rebias);\n-\n-  \/\/ The epoch of the current bias is still valid but we know nothing\n-  \/\/ about the owner; it might be set or it might be clear. Try to\n-  \/\/ acquire the bias of the object using an atomic operation. If this\n-  \/\/ fails we will go in to the runtime to revoke the object's bias.\n-  \/\/ Note that we first construct the presumed unbiased header so we\n-  \/\/ don't accidentally blow away another thread's valid bias.\n-  NOT_LP64( movptr(swap_reg, saved_mark_addr); )\n-  andptr(swap_reg,\n-         markWord::biased_lock_mask_in_place | markWord::age_mask_in_place | markWord::epoch_mask_in_place);\n-#ifdef _LP64\n-  movptr(tmp_reg, swap_reg);\n-  orptr(tmp_reg, r15_thread);\n-#else\n-  get_thread(tmp_reg);\n-  orptr(tmp_reg, swap_reg);\n-#endif\n-  lock();\n-  cmpxchgptr(tmp_reg, mark_addr); \/\/ compare tmp_reg and swap_reg\n-  \/\/ If the biasing toward our thread failed, this means that\n-  \/\/ another thread succeeded in biasing it toward itself and we\n-  \/\/ need to revoke that bias. The revocation will occur in the\n-  \/\/ interpreter runtime in the slow case.\n-  if (counters != NULL) {\n-    cond_inc32(Assembler::zero,\n-               ExternalAddress((address) counters->anonymously_biased_lock_entry_count_addr()));\n-  }\n-  if (slow_case != NULL) {\n-    jcc(Assembler::notZero, *slow_case);\n-  }\n-  jmp(done);\n-\n-  bind(try_rebias);\n-  \/\/ At this point we know the epoch has expired, meaning that the\n-  \/\/ current \"bias owner\", if any, is actually invalid. Under these\n-  \/\/ circumstances _only_, we are allowed to use the current header's\n-  \/\/ value as the comparison value when doing the cas to acquire the\n-  \/\/ bias in the current epoch. In other words, we allow transfer of\n-  \/\/ the bias from one thread to another directly in this situation.\n-  \/\/\n-  \/\/ FIXME: due to a lack of registers we currently blow away the age\n-  \/\/ bits in this situation. Should attempt to preserve them.\n-  load_prototype_header(tmp_reg, obj_reg, tmp_reg2);\n-#ifdef _LP64\n-  orptr(tmp_reg, r15_thread);\n-#else\n-  get_thread(swap_reg);\n-  orptr(tmp_reg, swap_reg);\n-  movptr(swap_reg, saved_mark_addr);\n-#endif\n-  lock();\n-  cmpxchgptr(tmp_reg, mark_addr); \/\/ compare tmp_reg and swap_reg\n-  \/\/ If the biasing toward our thread failed, then another thread\n-  \/\/ succeeded in biasing it toward itself and we need to revoke that\n-  \/\/ bias. The revocation will occur in the runtime in the slow case.\n-  if (counters != NULL) {\n-    cond_inc32(Assembler::zero,\n-               ExternalAddress((address) counters->rebiased_lock_entry_count_addr()));\n-  }\n-  if (slow_case != NULL) {\n-    jcc(Assembler::notZero, *slow_case);\n-  }\n-  jmp(done);\n-\n-  bind(try_revoke_bias);\n-  \/\/ The prototype mark in the klass doesn't have the bias bit set any\n-  \/\/ more, indicating that objects of this data type are not supposed\n-  \/\/ to be biased any more. We are going to try to reset the mark of\n-  \/\/ this object to the prototype value and fall through to the\n-  \/\/ CAS-based locking scheme. Note that if our CAS fails, it means\n-  \/\/ that another thread raced us for the privilege of revoking the\n-  \/\/ bias of this particular object, so it's okay to continue in the\n-  \/\/ normal locking code.\n-  \/\/\n-  \/\/ FIXME: due to a lack of registers we currently blow away the age\n-  \/\/ bits in this situation. Should attempt to preserve them.\n-  NOT_LP64( movptr(swap_reg, saved_mark_addr); )\n-  load_prototype_header(tmp_reg, obj_reg, tmp_reg2);\n-  lock();\n-  cmpxchgptr(tmp_reg, mark_addr); \/\/ compare tmp_reg and swap_reg\n-  \/\/ Fall through to the normal CAS-based lock, because no matter what\n-  \/\/ the result of the above CAS, some thread must have succeeded in\n-  \/\/ removing the bias bit from the object's header.\n-  if (counters != NULL) {\n-    cond_inc32(Assembler::zero,\n-               ExternalAddress((address) counters->revoked_lock_entry_count_addr()));\n-  }\n-\n-  bind(cas_label);\n-}\n-\n-void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label& done) {\n-  assert(UseBiasedLocking, \"why call this otherwise?\");\n-\n-  \/\/ Check for biased locking unlock case, which is a no-op\n-  \/\/ Note: we do not have to check the thread ID for two reasons.\n-  \/\/ First, the interpreter checks for IllegalMonitorStateException at\n-  \/\/ a higher level. Second, if the bias was revoked while we held the\n-  \/\/ lock, the object could not be rebiased toward another thread, so\n-  \/\/ the bias bit would be clear.\n-  movptr(temp_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-  andptr(temp_reg, markWord::biased_lock_mask_in_place);\n-  cmpptr(temp_reg, markWord::biased_lock_pattern);\n-  jcc(Assembler::equal, done);\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":0,"deletions":195,"binary":false,"changes":195,"status":"modified"},{"patch":"@@ -733,15 +733,0 @@\n-  \/\/ Biased locking support\n-  \/\/ lock_reg and obj_reg must be loaded up with the appropriate values.\n-  \/\/ swap_reg must be rax, and is killed.\n-  \/\/ tmp_reg is optional. If it is supplied (i.e., != noreg) it will\n-  \/\/ be killed; if not supplied, push\/pop will be used internally to\n-  \/\/ allocate a temporary (inefficient, avoid if possible).\n-  \/\/ Optional slow case is for implementations (interpreter and C1) which branch to\n-  \/\/ slow case directly. Leaves condition codes set for C2's Fast_Lock node.\n-  void biased_locking_enter(Register lock_reg, Register obj_reg,\n-                            Register swap_reg, Register tmp_reg,\n-                            Register tmp_reg2, bool swap_reg_contains_mark,\n-                            Label& done, Label* slow_case = NULL,\n-                            BiasedLockingCounters* counters = NULL);\n-  void biased_locking_exit (Register obj_reg, Register temp_reg, Label& done);\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1844,5 +1844,0 @@\n-    if (UseBiasedLocking) {\n-      \/\/ Note that oop_handle_reg is trashed during this call\n-      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, oop_handle_reg, noreg, false, lock_done, &slow_path_lock);\n-    }\n-\n@@ -1881,5 +1876,0 @@\n-\n-    if (UseBiasedLocking) {\n-      \/\/ Re-fetch oop_handle_reg as we trashed it above\n-      __ movptr(oop_handle_reg, Address(rsp, wordSize));\n-    }\n@@ -2015,4 +2005,0 @@\n-    if (UseBiasedLocking) {\n-      __ biased_locking_exit(obj_reg, rbx, done);\n-    }\n-\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -172,0 +172,4 @@\n+\/\/ Register is a class, but it would be assigned numerical value.\n+\/\/ \"0\" is assigned for rax. Thus we need to ignore -Wnonnull.\n+PRAGMA_DIAG_PUSH\n+PRAGMA_NONNULL_IGNORED\n@@ -364,0 +368,1 @@\n+PRAGMA_DIAG_POP\n@@ -2332,4 +2337,0 @@\n-    if (UseBiasedLocking) {\n-      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, rscratch2, false, lock_done, &slow_path_lock);\n-    }\n-\n@@ -2342,1 +2343,0 @@\n-      assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n@@ -2493,5 +2493,0 @@\n-\n-    if (UseBiasedLocking) {\n-      __ biased_locking_exit(obj_reg, old_hdr, done);\n-    }\n-\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":5,"deletions":10,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -5328,4 +5329,4 @@\n-  \/\/base64 character set\n-  address base64_charset_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"base64_charset\");\n+  address base64_shuffle_addr()\n+  {\n+    __ align(64, (unsigned long long)__ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"shuffle_base64\");\n@@ -5333,32 +5334,10 @@\n-    __ emit_data64(0x0000004200000041, relocInfo::none);\n-    __ emit_data64(0x0000004400000043, relocInfo::none);\n-    __ emit_data64(0x0000004600000045, relocInfo::none);\n-    __ emit_data64(0x0000004800000047, relocInfo::none);\n-    __ emit_data64(0x0000004a00000049, relocInfo::none);\n-    __ emit_data64(0x0000004c0000004b, relocInfo::none);\n-    __ emit_data64(0x0000004e0000004d, relocInfo::none);\n-    __ emit_data64(0x000000500000004f, relocInfo::none);\n-    __ emit_data64(0x0000005200000051, relocInfo::none);\n-    __ emit_data64(0x0000005400000053, relocInfo::none);\n-    __ emit_data64(0x0000005600000055, relocInfo::none);\n-    __ emit_data64(0x0000005800000057, relocInfo::none);\n-    __ emit_data64(0x0000005a00000059, relocInfo::none);\n-    __ emit_data64(0x0000006200000061, relocInfo::none);\n-    __ emit_data64(0x0000006400000063, relocInfo::none);\n-    __ emit_data64(0x0000006600000065, relocInfo::none);\n-    __ emit_data64(0x0000006800000067, relocInfo::none);\n-    __ emit_data64(0x0000006a00000069, relocInfo::none);\n-    __ emit_data64(0x0000006c0000006b, relocInfo::none);\n-    __ emit_data64(0x0000006e0000006d, relocInfo::none);\n-    __ emit_data64(0x000000700000006f, relocInfo::none);\n-    __ emit_data64(0x0000007200000071, relocInfo::none);\n-    __ emit_data64(0x0000007400000073, relocInfo::none);\n-    __ emit_data64(0x0000007600000075, relocInfo::none);\n-    __ emit_data64(0x0000007800000077, relocInfo::none);\n-    __ emit_data64(0x0000007a00000079, relocInfo::none);\n-    __ emit_data64(0x0000003100000030, relocInfo::none);\n-    __ emit_data64(0x0000003300000032, relocInfo::none);\n-    __ emit_data64(0x0000003500000034, relocInfo::none);\n-    __ emit_data64(0x0000003700000036, relocInfo::none);\n-    __ emit_data64(0x0000003900000038, relocInfo::none);\n-    __ emit_data64(0x0000002f0000002b, relocInfo::none);\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x0405030401020001, relocInfo::none);\n+    __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n+    __ emit_data64(0x10110f100d0e0c0d, relocInfo::none);\n+    __ emit_data64(0x1617151613141213, relocInfo::none);\n+    __ emit_data64(0x1c1d1b1c191a1819, relocInfo::none);\n+    __ emit_data64(0x222321221f201e1f, relocInfo::none);\n+    __ emit_data64(0x2829272825262425, relocInfo::none);\n+    __ emit_data64(0x2e2f2d2e2b2c2a2b, relocInfo::none);\n@@ -5368,4 +5347,4 @@\n-  \/\/base64 url character set\n-  address base64url_charset_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"base64url_charset\");\n+  address base64_avx2_shuffle_addr()\n+  {\n+    __ align(32);\n+    StubCodeMark mark(this, \"StubRoutines\", \"avx2_shuffle_base64\");\n@@ -5373,32 +5352,6 @@\n-    __ emit_data64(0x0000004200000041, relocInfo::none);\n-    __ emit_data64(0x0000004400000043, relocInfo::none);\n-    __ emit_data64(0x0000004600000045, relocInfo::none);\n-    __ emit_data64(0x0000004800000047, relocInfo::none);\n-    __ emit_data64(0x0000004a00000049, relocInfo::none);\n-    __ emit_data64(0x0000004c0000004b, relocInfo::none);\n-    __ emit_data64(0x0000004e0000004d, relocInfo::none);\n-    __ emit_data64(0x000000500000004f, relocInfo::none);\n-    __ emit_data64(0x0000005200000051, relocInfo::none);\n-    __ emit_data64(0x0000005400000053, relocInfo::none);\n-    __ emit_data64(0x0000005600000055, relocInfo::none);\n-    __ emit_data64(0x0000005800000057, relocInfo::none);\n-    __ emit_data64(0x0000005a00000059, relocInfo::none);\n-    __ emit_data64(0x0000006200000061, relocInfo::none);\n-    __ emit_data64(0x0000006400000063, relocInfo::none);\n-    __ emit_data64(0x0000006600000065, relocInfo::none);\n-    __ emit_data64(0x0000006800000067, relocInfo::none);\n-    __ emit_data64(0x0000006a00000069, relocInfo::none);\n-    __ emit_data64(0x0000006c0000006b, relocInfo::none);\n-    __ emit_data64(0x0000006e0000006d, relocInfo::none);\n-    __ emit_data64(0x000000700000006f, relocInfo::none);\n-    __ emit_data64(0x0000007200000071, relocInfo::none);\n-    __ emit_data64(0x0000007400000073, relocInfo::none);\n-    __ emit_data64(0x0000007600000075, relocInfo::none);\n-    __ emit_data64(0x0000007800000077, relocInfo::none);\n-    __ emit_data64(0x0000007a00000079, relocInfo::none);\n-    __ emit_data64(0x0000003100000030, relocInfo::none);\n-    __ emit_data64(0x0000003300000032, relocInfo::none);\n-    __ emit_data64(0x0000003500000034, relocInfo::none);\n-    __ emit_data64(0x0000003700000036, relocInfo::none);\n-    __ emit_data64(0x0000003900000038, relocInfo::none);\n-    __ emit_data64(0x0000005f0000002d, relocInfo::none);\n+    __ emit_data64(0x0809070805060405, relocInfo::none);\n+    __ emit_data64(0x0e0f0d0e0b0c0a0b, relocInfo::none);\n+    __ emit_data64(0x0405030401020001, relocInfo::none);\n+    __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n+    return start;\n+  }\n@@ -5406,0 +5359,9 @@\n+  address base64_avx2_input_mask_addr()\n+  {\n+    __ align(32);\n+    StubCodeMark mark(this, \"StubRoutines\", \"avx2_input_mask_base64\");\n+    address start = __ pc();\n+    __ emit_data64(0x8000000000000000, relocInfo::none);\n+    __ emit_data64(0x8000000080000000, relocInfo::none);\n+    __ emit_data64(0x8000000080000000, relocInfo::none);\n+    __ emit_data64(0x8000000080000000, relocInfo::none);\n@@ -5409,3 +5371,4 @@\n-  address base64_bswap_mask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"bswap_mask_base64\");\n+  address base64_avx2_lut_addr()\n+  {\n+    __ align(32);\n+    StubCodeMark mark(this, \"StubRoutines\", \"avx2_lut_base64\");\n@@ -5413,8 +5376,12 @@\n-    __ emit_data64(0x0504038002010080, relocInfo::none);\n-    __ emit_data64(0x0b0a098008070680, relocInfo::none);\n-    __ emit_data64(0x0908078006050480, relocInfo::none);\n-    __ emit_data64(0x0f0e0d800c0b0a80, relocInfo::none);\n-    __ emit_data64(0x0605048003020180, relocInfo::none);\n-    __ emit_data64(0x0c0b0a8009080780, relocInfo::none);\n-    __ emit_data64(0x0504038002010080, relocInfo::none);\n-    __ emit_data64(0x0b0a098008070680, relocInfo::none);\n+    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+    __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n+    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+    __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n+\n+    \/\/ URL LUT\n+    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+    __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n+    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+    __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n+    return start;\n+  }\n@@ -5422,0 +5389,24 @@\n+  address base64_encoding_table_addr()\n+  {\n+    __ align(64, (unsigned long long)__ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"encoding_table_base64\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0, \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x4847464544434241, relocInfo::none);\n+    __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n+    __ emit_data64(0x5857565554535251, relocInfo::none);\n+    __ emit_data64(0x6665646362615a59, relocInfo::none);\n+    __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n+    __ emit_data64(0x767574737271706f, relocInfo::none);\n+    __ emit_data64(0x333231307a797877, relocInfo::none);\n+    __ emit_data64(0x2f2b393837363534, relocInfo::none);\n+\n+    \/\/ URL table\n+    __ emit_data64(0x4847464544434241, relocInfo::none);\n+    __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n+    __ emit_data64(0x5857565554535251, relocInfo::none);\n+    __ emit_data64(0x6665646362615a59, relocInfo::none);\n+    __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n+    __ emit_data64(0x767574737271706f, relocInfo::none);\n+    __ emit_data64(0x333231307a797877, relocInfo::none);\n+    __ emit_data64(0x5f2d393837363534, relocInfo::none);\n@@ -5425,1 +5416,6 @@\n-  address base64_right_shift_mask_addr() {\n+  \/\/ Code for generating Base64 encoding.\n+  \/\/ Intrinsic function prototype in Base64.java:\n+  \/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp,\n+  \/\/ boolean isURL) {\n+  address generate_base64_encodeBlock()\n+  {\n@@ -5427,1 +5423,1 @@\n-    StubCodeMark mark(this, \"StubRoutines\", \"right_shift_mask\");\n+    StubCodeMark mark(this, \"StubRoutines\", \"implEncode\");\n@@ -5429,8 +5425,360 @@\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n+    __ enter();\n+\n+    \/\/ Save callee-saved registers before using them\n+    __ push(r12);\n+    __ push(r13);\n+    __ push(r14);\n+    __ push(r15);\n+\n+    \/\/ arguments\n+    const Register source = c_rarg0;       \/\/ Source Array\n+    const Register start_offset = c_rarg1; \/\/ start offset\n+    const Register end_offset = c_rarg2;   \/\/ end offset\n+    const Register dest = c_rarg3;   \/\/ destination array\n+\n+#ifndef _WIN64\n+    const Register dp = c_rarg4;    \/\/ Position for writing to dest array\n+    const Register isURL = c_rarg5; \/\/ Base64 or URL character set\n+#else\n+    const Address dp_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n+    const Address isURL_mem(rbp, 7 * wordSize);\n+    const Register isURL = r10; \/\/ pick the volatile windows register\n+    const Register dp = r12;\n+    __ movl(dp, dp_mem);\n+    __ movl(isURL, isURL_mem);\n+#endif\n+\n+    const Register length = r14;\n+    const Register encode_table = r13;\n+    Label L_process3, L_exit, L_processdata, L_vbmiLoop, L_not512, L_32byteLoop;\n+\n+    \/\/ calculate length from offsets\n+    __ movl(length, end_offset);\n+    __ subl(length, start_offset);\n+    __ cmpl(length, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n+\n+    \/\/ Code for 512-bit VBMI encoding.  Encodes 48 input bytes into 64\n+    \/\/ output bytes. We read 64 input bytes and ignore the last 16, so be\n+    \/\/ sure not to read past the end of the input buffer.\n+    if (VM_Version::supports_avx512_vbmi()) {\n+      __ cmpl(length, 64); \/\/ Do not overrun input buffer.\n+      __ jcc(Assembler::below, L_not512);\n+\n+      __ shll(isURL, 6); \/\/ index into decode table based on isURL\n+      __ lea(encode_table, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n+      __ addptr(encode_table, isURL);\n+      __ shrl(isURL, 6); \/\/ restore isURL\n+\n+      __ mov64(rax, 0x3036242a1016040aull); \/\/ Shifts\n+      __ evmovdquq(xmm3, ExternalAddress(StubRoutines::x86::base64_shuffle_addr()), Assembler::AVX_512bit, r15);\n+      __ evmovdquq(xmm2, Address(encode_table, 0), Assembler::AVX_512bit);\n+      __ evpbroadcastq(xmm1, rax, Assembler::AVX_512bit);\n+\n+      __ align(32);\n+      __ BIND(L_vbmiLoop);\n+\n+      __ vpermb(xmm0, xmm3, Address(source, start_offset), Assembler::AVX_512bit);\n+      __ subl(length, 48);\n+\n+      \/\/ Put the input bytes into the proper lanes for writing, then\n+      \/\/ encode them.\n+      __ evpmultishiftqb(xmm0, xmm1, xmm0, Assembler::AVX_512bit);\n+      __ vpermb(xmm0, xmm0, xmm2, Assembler::AVX_512bit);\n+\n+      \/\/ Write to destination\n+      __ evmovdquq(Address(dest, dp), xmm0, Assembler::AVX_512bit);\n+\n+      __ addptr(dest, 64);\n+      __ addptr(source, 48);\n+      __ cmpl(length, 64);\n+      __ jcc(Assembler::aboveEqual, L_vbmiLoop);\n+\n+      __ vzeroupper();\n+    }\n+\n+    __ BIND(L_not512);\n+    if (VM_Version::supports_avx2()\n+        && VM_Version::supports_avx512vlbw()) {\n+      \/*\n+      ** This AVX2 encoder is based off the paper at:\n+      **      https:\/\/dl.acm.org\/doi\/10.1145\/3132709\n+      **\n+      ** We use AVX2 SIMD instructions to encode 24 bytes into 32\n+      ** output bytes.\n+      **\n+      *\/\n+      \/\/ Lengths under 32 bytes are done with scalar routine\n+      __ cmpl(length, 31);\n+      __ jcc(Assembler::belowEqual, L_process3);\n+\n+      \/\/ Set up supporting constant table data\n+      __ vmovdqu(xmm9, ExternalAddress(StubRoutines::x86::base64_avx2_shuffle_addr()), rax);\n+      \/\/ 6-bit mask for 2nd and 4th (and multiples) 6-bit values\n+      __ movl(rax, 0x0fc0fc00);\n+      __ vmovdqu(xmm1, ExternalAddress(StubRoutines::x86::base64_avx2_input_mask_addr()), rax);\n+      __ evpbroadcastd(xmm8, rax, Assembler::AVX_256bit);\n+\n+      \/\/ Multiplication constant for \"shifting\" right by 6 and 10\n+      \/\/ bits\n+      __ movl(rax, 0x04000040);\n+\n+      __ subl(length, 24);\n+      __ evpbroadcastd(xmm7, rax, Assembler::AVX_256bit);\n+\n+      \/\/ For the first load, we mask off reading of the first 4\n+      \/\/ bytes into the register. This is so we can get 4 3-byte\n+      \/\/ chunks into each lane of the register, avoiding having to\n+      \/\/ handle end conditions.  We then shuffle these bytes into a\n+      \/\/ specific order so that manipulation is easier.\n+      \/\/\n+      \/\/ The initial read loads the XMM register like this:\n+      \/\/\n+      \/\/ Lower 128-bit lane:\n+      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+      \/\/ | XX | XX | XX | XX | A0 | A1 | A2 | B0 | B1 | B2 | C0 | C1\n+      \/\/ | C2 | D0 | D1 | D2 |\n+      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+      \/\/\n+      \/\/ Upper 128-bit lane:\n+      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+      \/\/ | E0 | E1 | E2 | F0 | F1 | F2 | G0 | G1 | G2 | H0 | H1 | H2\n+      \/\/ | XX | XX | XX | XX |\n+      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+      \/\/\n+      \/\/ Where A0 is the first input byte, B0 is the fourth, etc.\n+      \/\/ The alphabetical significance denotes the 3 bytes to be\n+      \/\/ consumed and encoded into 4 bytes.\n+      \/\/\n+      \/\/ We then shuffle the register so each 32-bit word contains\n+      \/\/ the sequence:\n+      \/\/    A1 A0 A2 A1, B1, B0, B2, B1, etc.\n+      \/\/ Each of these byte sequences are then manipulated into 4\n+      \/\/ 6-bit values ready for encoding.\n+      \/\/\n+      \/\/ If we focus on one set of 3-byte chunks, changing the\n+      \/\/ nomenclature such that A0 => a, A1 => b, and A2 => c, we\n+      \/\/ shuffle such that each 24-bit chunk contains:\n+      \/\/\n+      \/\/ b7 b6 b5 b4 b3 b2 b1 b0 | a7 a6 a5 a4 a3 a2 a1 a0 | c7 c6\n+      \/\/ c5 c4 c3 c2 c1 c0 | b7 b6 b5 b4 b3 b2 b1 b0\n+      \/\/ Explain this step.\n+      \/\/ b3 b2 b1 b0 c5 c4 c3 c2 | c1 c0 d5 d4 d3 d2 d1 d0 | a5 a4\n+      \/\/ a3 a2 a1 a0 b5 b4 | b3 b2 b1 b0 c5 c4 c3 c2\n+      \/\/\n+      \/\/ W first and off all but bits 4-9 and 16-21 (c5..c0 and\n+      \/\/ a5..a0) and shift them using a vector multiplication\n+      \/\/ operation (vpmulhuw) which effectively shifts c right by 6\n+      \/\/ bits and a right by 10 bits.  We similarly mask bits 10-15\n+      \/\/ (d5..d0) and 22-27 (b5..b0) and shift them left by 8 and 4\n+      \/\/ bits respecively.  This is done using vpmullw.  We end up\n+      \/\/ with 4 6-bit values, thus splitting the 3 input bytes,\n+      \/\/ ready for encoding:\n+      \/\/    0 0 d5..d0 0 0 c5..c0 0 0 b5..b0 0 0 a5..a0\n+      \/\/\n+      \/\/ For translation, we recognize that there are 5 distinct\n+      \/\/ ranges of legal Base64 characters as below:\n+      \/\/\n+      \/\/   +-------------+-------------+------------+\n+      \/\/   | 6-bit value | ASCII range |   offset   |\n+      \/\/   +-------------+-------------+------------+\n+      \/\/   |    0..25    |    A..Z     |     65     |\n+      \/\/   |   26..51    |    a..z     |     71     |\n+      \/\/   |   52..61    |    0..9     |     -4     |\n+      \/\/   |     62      |   + or -    | -19 or -17 |\n+      \/\/   |     63      |   \/ or _    | -16 or 32  |\n+      \/\/   +-------------+-------------+------------+\n+      \/\/\n+      \/\/ We note that vpshufb does a parallel lookup in a\n+      \/\/ destination register using the lower 4 bits of bytes from a\n+      \/\/ source register.  If we use a saturated subtraction and\n+      \/\/ subtract 51 from each 6-bit value, bytes from [0,51]\n+      \/\/ saturate to 0, and [52,63] map to a range of [1,12].  We\n+      \/\/ distinguish the [0,25] and [26,51] ranges by assigning a\n+      \/\/ value of 13 for all 6-bit values less than 26.  We end up\n+      \/\/ with:\n+      \/\/\n+      \/\/   +-------------+-------------+------------+\n+      \/\/   | 6-bit value |   Reduced   |   offset   |\n+      \/\/   +-------------+-------------+------------+\n+      \/\/   |    0..25    |     13      |     65     |\n+      \/\/   |   26..51    |      0      |     71     |\n+      \/\/   |   52..61    |    0..9     |     -4     |\n+      \/\/   |     62      |     11      | -19 or -17 |\n+      \/\/   |     63      |     12      | -16 or 32  |\n+      \/\/   +-------------+-------------+------------+\n+      \/\/\n+      \/\/ We then use a final vpshufb to add the appropriate offset,\n+      \/\/ translating the bytes.\n+      \/\/\n+      \/\/ Load input bytes - only 28 bytes.  Mask the first load to\n+      \/\/ not load into the full register.\n+      __ vpmaskmovd(xmm1, xmm1, Address(source, start_offset, Address::times_1, -4), Assembler::AVX_256bit);\n+\n+      \/\/ Move 3-byte chunks of input (12 bytes) into 16 bytes,\n+      \/\/ ordering by:\n+      \/\/   1, 0, 2, 1; 4, 3, 5, 4; etc.  This groups 6-bit chunks\n+      \/\/   for easy masking\n+      __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n+\n+      __ addl(start_offset, 24);\n+\n+      \/\/ Load masking register for first and third (and multiples)\n+      \/\/ 6-bit values.\n+      __ movl(rax, 0x003f03f0);\n+      __ evpbroadcastd(xmm6, rax, Assembler::AVX_256bit);\n+      \/\/ Multiplication constant for \"shifting\" left by 4 and 8 bits\n+      __ movl(rax, 0x01000010);\n+      __ evpbroadcastd(xmm5, rax, Assembler::AVX_256bit);\n+\n+      \/\/ Isolate 6-bit chunks of interest\n+      __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n+\n+      \/\/ Load constants for encoding\n+      __ movl(rax, 0x19191919);\n+      __ evpbroadcastd(xmm3, rax, Assembler::AVX_256bit);\n+      __ movl(rax, 0x33333333);\n+      __ evpbroadcastd(xmm4, rax, Assembler::AVX_256bit);\n+\n+      \/\/ Shift output bytes 0 and 2 into proper lanes\n+      __ vpmulhuw(xmm2, xmm0, xmm7, Assembler::AVX_256bit);\n+\n+      \/\/ Mask and shift output bytes 1 and 3 into proper lanes and\n+      \/\/ combine\n+      __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n+      __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n+      __ vpor(xmm0, xmm0, xmm2, Assembler::AVX_256bit);\n+\n+      \/\/ Find out which are 0..25.  This indicates which input\n+      \/\/ values fall in the range of 'A'-'Z', which require an\n+      \/\/ additional offset (see comments above)\n+      __ vpcmpgtb(xmm2, xmm0, xmm3, Assembler::AVX_256bit);\n+      __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n+      __ vpsubb(xmm1, xmm1, xmm2, Assembler::AVX_256bit);\n+\n+      \/\/ Load the proper lookup table\n+      __ lea(r11, ExternalAddress(StubRoutines::x86::base64_avx2_lut_addr()));\n+      __ movl(r15, isURL);\n+      __ shll(r15, 5);\n+      __ vmovdqu(xmm2, Address(r11, r15));\n+\n+      \/\/ Shuffle the offsets based on the range calculation done\n+      \/\/ above. This allows us to add the correct offset to the\n+      \/\/ 6-bit value corresponding to the range documented above.\n+      __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n+      __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n+\n+      \/\/ Store the encoded bytes\n+      __ vmovdqu(Address(dest, dp), xmm0);\n+      __ addl(dp, 32);\n+\n+      __ cmpl(length, 31);\n+      __ jcc(Assembler::belowEqual, L_process3);\n+\n+      __ align(32);\n+      __ BIND(L_32byteLoop);\n+\n+      \/\/ Get next 32 bytes\n+      __ vmovdqu(xmm1, Address(source, start_offset, Address::times_1, -4));\n+\n+      __ subl(length, 24);\n+      __ addl(start_offset, 24);\n+\n+      \/\/ This logic is identical to the above, with only constant\n+      \/\/ register loads removed.  Shuffle the input, mask off 6-bit\n+      \/\/ chunks, shift them into place, then add the offset to\n+      \/\/ encode.\n+      __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n+\n+      __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n+      __ vpmulhuw(xmm10, xmm0, xmm7, Assembler::AVX_256bit);\n+      __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n+      __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n+      __ vpor(xmm0, xmm0, xmm10, Assembler::AVX_256bit);\n+      __ vpcmpgtb(xmm10, xmm0, xmm3, Assembler::AVX_256bit);\n+      __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n+      __ vpsubb(xmm1, xmm1, xmm10, Assembler::AVX_256bit);\n+      __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n+      __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n+\n+      \/\/ Store the encoded bytes\n+      __ vmovdqu(Address(dest, dp), xmm0);\n+      __ addl(dp, 32);\n+\n+      __ cmpl(length, 31);\n+      __ jcc(Assembler::above, L_32byteLoop);\n+\n+      __ BIND(L_process3);\n+      __ vzeroupper();\n+    } else {\n+      __ BIND(L_process3);\n+    }\n+\n+    __ cmpl(length, 3);\n+    __ jcc(Assembler::below, L_exit);\n+\n+    \/\/ Load the encoding table based on isURL\n+    __ lea(r11, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n+    __ movl(r15, isURL);\n+    __ shll(r15, 6);\n+    __ addptr(r11, r15);\n+\n+    __ BIND(L_processdata);\n+\n+    \/\/ Load 3 bytes\n+    __ load_unsigned_byte(r15, Address(source, start_offset));\n+    __ load_unsigned_byte(r10, Address(source, start_offset, Address::times_1, 1));\n+    __ load_unsigned_byte(r13, Address(source, start_offset, Address::times_1, 2));\n+\n+    \/\/ Build a 32-bit word with bytes 1, 2, 0, 1\n+    __ movl(rax, r10);\n+    __ shll(r10, 24);\n+    __ orl(rax, r10);\n+\n+    __ subl(length, 3);\n+\n+    __ shll(r15, 8);\n+    __ shll(r13, 16);\n+    __ orl(rax, r15);\n+\n+    __ addl(start_offset, 3);\n+\n+    __ orl(rax, r13);\n+    \/\/ At this point, rax contains | byte1 | byte2 | byte0 | byte1\n+    \/\/ r13 has byte2 << 16 - need low-order 6 bits to translate.\n+    \/\/ This translated byte is the fourth output byte.\n+    __ shrl(r13, 16);\n+    __ andl(r13, 0x3f);\n+\n+    \/\/ The high-order 6 bits of r15 (byte0) is translated.\n+    \/\/ The translated byte is the first output byte.\n+    __ shrl(r15, 10);\n+\n+    __ load_unsigned_byte(r13, Address(r11, r13));\n+    __ load_unsigned_byte(r15, Address(r11, r15));\n+\n+    __ movb(Address(dest, dp, Address::times_1, 3), r13);\n+\n+    \/\/ Extract high-order 4 bits of byte1 and low-order 2 bits of byte0.\n+    \/\/ This translated byte is the second output byte.\n+    __ shrl(rax, 4);\n+    __ movl(r10, rax);\n+    __ andl(rax, 0x3f);\n+\n+    __ movb(Address(dest, dp, Address::times_1, 0), r15);\n+\n+    __ load_unsigned_byte(rax, Address(r11, rax));\n+\n+    \/\/ Extract low-order 2 bits of byte1 and high-order 4 bits of byte2.\n+    \/\/ This translated byte is the third output byte.\n+    __ shrl(r10, 18);\n+    __ andl(r10, 0x3f);\n+\n+    __ load_unsigned_byte(r10, Address(r11, r10));\n+\n+    __ movb(Address(dest, dp, Address::times_1, 1), rax);\n+    __ movb(Address(dest, dp, Address::times_1, 2), r10);\n+\n+    __ addl(dp, 4);\n+    __ cmpl(length, 3);\n+    __ jcc(Assembler::aboveEqual, L_processdata);\n@@ -5438,0 +5786,7 @@\n+    __ BIND(L_exit);\n+    __ pop(r15);\n+    __ pop(r14);\n+    __ pop(r13);\n+    __ pop(r12);\n+    __ leave();\n+    __ ret(0);\n@@ -5441,3 +5796,4 @@\n-  address base64_left_shift_mask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"left_shift_mask\");\n+  \/\/ base64 AVX512vbmi tables\n+  address base64_vbmi_lookup_lo_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64\");\n@@ -5445,8 +5801,12 @@\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x3f8080803e808080, relocInfo::none);\n+    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+    __ emit_data64(0x8080808080803d3c, relocInfo::none);\n+    return start;\n+  }\n@@ -5454,0 +5814,30 @@\n+  address base64_vbmi_lookup_hi_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x0605040302010080, relocInfo::none);\n+    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+    __ emit_data64(0x161514131211100f, relocInfo::none);\n+    __ emit_data64(0x8080808080191817, relocInfo::none);\n+    __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n+    __ emit_data64(0x2827262524232221, relocInfo::none);\n+    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+    __ emit_data64(0x8080808080333231, relocInfo::none);\n+    return start;\n+  }\n+  address base64_vbmi_lookup_lo_url_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64url\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x80803e8080808080, relocInfo::none);\n+    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+    __ emit_data64(0x8080808080803d3c, relocInfo::none);\n@@ -5457,3 +5847,3 @@\n-  address base64_and_mask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"and_mask\");\n+  address base64_vbmi_lookup_hi_url_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64url\");\n@@ -5461,8 +5851,10 @@\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x0605040302010080, relocInfo::none);\n+    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+    __ emit_data64(0x161514131211100f, relocInfo::none);\n+    __ emit_data64(0x3f80808080191817, relocInfo::none);\n+    __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n+    __ emit_data64(0x2827262524232221, relocInfo::none);\n+    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+    __ emit_data64(0x8080808080333231, relocInfo::none);\n@@ -5472,3 +5864,70 @@\n-  address base64_gather_mask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"gather_mask\");\n+  address base64_vbmi_pack_vec_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"pack_vec_base64\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x090a040506000102, relocInfo::none);\n+    __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n+    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+    __ emit_data64(0x292a242526202122, relocInfo::none);\n+    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    return start;\n+  }\n+\n+  address base64_vbmi_join_0_1_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"join_0_1_base64\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x090a040506000102, relocInfo::none);\n+    __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n+    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+    __ emit_data64(0x292a242526202122, relocInfo::none);\n+    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+    __ emit_data64(0x494a444546404142, relocInfo::none);\n+    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n+    return start;\n+  }\n+\n+  address base64_vbmi_join_1_2_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"join_1_2_base64\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+    __ emit_data64(0x292a242526202122, relocInfo::none);\n+    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+    __ emit_data64(0x494a444546404142, relocInfo::none);\n+    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n+    __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n+    __ emit_data64(0x696a646566606162, relocInfo::none);\n+    return start;\n+  }\n+\n+  address base64_vbmi_join_2_3_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"join_2_3_base64\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+    __ emit_data64(0x494a444546404142, relocInfo::none);\n+    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n+    __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n+    __ emit_data64(0x696a646566606162, relocInfo::none);\n+    __ emit_data64(0x767071726c6d6e68, relocInfo::none);\n+    __ emit_data64(0x7c7d7e78797a7475, relocInfo::none);\n+    return start;\n+  }\n+\n+  address base64_decoding_table_addr() {\n+    StubCodeMark mark(this, \"StubRoutines\", \"decoding_table_base64\");\n@@ -5477,0 +5936,65 @@\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0x3fffffff3effffff, relocInfo::none);\n+    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+    __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n+    __ emit_data64(0x06050403020100ff, relocInfo::none);\n+    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+    __ emit_data64(0x161514131211100f, relocInfo::none);\n+    __ emit_data64(0xffffffffff191817, relocInfo::none);\n+    __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n+    __ emit_data64(0x2827262524232221, relocInfo::none);\n+    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+    __ emit_data64(0xffffffffff333231, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+\n+    \/\/ URL table\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffff3effffffffff, relocInfo::none);\n+    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+    __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n+    __ emit_data64(0x06050403020100ff, relocInfo::none);\n+    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+    __ emit_data64(0x161514131211100f, relocInfo::none);\n+    __ emit_data64(0x3fffffffff191817, relocInfo::none);\n+    __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n+    __ emit_data64(0x2827262524232221, relocInfo::none);\n+    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+    __ emit_data64(0xffffffffff333231, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n@@ -5480,1 +6004,5 @@\n-\/\/ Code for generating Base64 encoding.\n+\n+\/\/ Code for generating Base64 decoding.\n+\/\/\n+\/\/ Based on the article (and associated code) from https:\/\/arxiv.org\/abs\/1910.05109.\n+\/\/\n@@ -5482,2 +6010,2 @@\n-\/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL) {\n-  address generate_base64_encodeBlock() {\n+\/\/ private void decodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL, isMIME) {\n+  address generate_base64_decodeBlock() {\n@@ -5485,1 +6013,1 @@\n-    StubCodeMark mark(this, \"StubRoutines\", \"implEncode\");\n+    StubCodeMark mark(this, \"StubRoutines\", \"implDecode\");\n@@ -5494,0 +6022,1 @@\n+    __ push(rbx);\n@@ -5500,0 +6029,1 @@\n+    const Register isMIME = rbx;\n@@ -5504,0 +6034,1 @@\n+    __ movl(isMIME, Address(rbp, 2 * wordSize));\n@@ -5511,0 +6042,1 @@\n+    __ movl(isMIME, Address(rbp, 8 * wordSize));\n@@ -5513,0 +6045,28 @@\n+    const XMMRegister lookup_lo = xmm5;\n+    const XMMRegister lookup_hi = xmm6;\n+    const XMMRegister errorvec = xmm7;\n+    const XMMRegister pack16_op = xmm9;\n+    const XMMRegister pack32_op = xmm8;\n+    const XMMRegister input0 = xmm3;\n+    const XMMRegister input1 = xmm20;\n+    const XMMRegister input2 = xmm21;\n+    const XMMRegister input3 = xmm19;\n+    const XMMRegister join01 = xmm12;\n+    const XMMRegister join12 = xmm11;\n+    const XMMRegister join23 = xmm10;\n+    const XMMRegister translated0 = xmm2;\n+    const XMMRegister translated1 = xmm1;\n+    const XMMRegister translated2 = xmm0;\n+    const XMMRegister translated3 = xmm4;\n+\n+    const XMMRegister merged0 = xmm2;\n+    const XMMRegister merged1 = xmm1;\n+    const XMMRegister merged2 = xmm0;\n+    const XMMRegister merged3 = xmm4;\n+    const XMMRegister merge_ab_bc0 = xmm2;\n+    const XMMRegister merge_ab_bc1 = xmm1;\n+    const XMMRegister merge_ab_bc2 = xmm0;\n+    const XMMRegister merge_ab_bc3 = xmm4;\n+\n+    const XMMRegister pack24bits = xmm4;\n+\n@@ -5514,1 +6074,12 @@\n-    Label L_process80, L_process32, L_process3, L_exit, L_processdata;\n+    const Register output_size = r13;\n+    const Register output_mask = r15;\n+    const KRegister input_mask = k1;\n+\n+    const XMMRegister input_initial_valid_b64 = xmm0;\n+    const XMMRegister tmp = xmm10;\n+    const XMMRegister mask = xmm0;\n+    const XMMRegister invalid_b64 = xmm1;\n+\n+    Label L_process256, L_process64, L_process64Loop, L_exit, L_processdata, L_loadURL;\n+    Label L_continue, L_finalBit, L_padding, L_donePadding, L_bruteForce;\n+    Label L_forceLoop, L_bottomLoop, L_checkMIME, L_exit_no_vzero;\n@@ -5519,0 +6090,288 @@\n+    __ push(dest);          \/\/ Save for return value calc\n+\n+    \/\/ If AVX512 VBMI not supported, just compile non-AVX code\n+    if(VM_Version::supports_avx512_vbmi() &&\n+       VM_Version::supports_avx512bw()) {\n+      __ cmpl(length, 128);     \/\/ 128-bytes is break-even for AVX-512\n+      __ jcc(Assembler::lessEqual, L_bruteForce);\n+\n+      __ cmpl(isMIME, 0);\n+      __ jcc(Assembler::notEqual, L_bruteForce);\n+\n+      \/\/ Load lookup tables based on isURL\n+      __ cmpl(isURL, 0);\n+      __ jcc(Assembler::notZero, L_loadURL);\n+\n+      __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_addr()), Assembler::AVX_512bit, r13);\n+      __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_addr()), Assembler::AVX_512bit, r13);\n+\n+      __ BIND(L_continue);\n+\n+      __ movl(r15, 0x01400140);\n+      __ evpbroadcastd(pack16_op, r15, Assembler::AVX_512bit);\n+\n+      __ movl(r15, 0x00011000);\n+      __ evpbroadcastd(pack32_op, r15, Assembler::AVX_512bit);\n+\n+      __ cmpl(length, 0xff);\n+      __ jcc(Assembler::lessEqual, L_process64);\n+\n+      \/\/ load masks required for decoding data\n+      __ BIND(L_processdata);\n+      __ evmovdquq(join01, ExternalAddress(StubRoutines::x86::base64_vbmi_join_0_1_addr()), Assembler::AVX_512bit,r13);\n+      __ evmovdquq(join12, ExternalAddress(StubRoutines::x86::base64_vbmi_join_1_2_addr()), Assembler::AVX_512bit, r13);\n+      __ evmovdquq(join23, ExternalAddress(StubRoutines::x86::base64_vbmi_join_2_3_addr()), Assembler::AVX_512bit, r13);\n+\n+      __ align(32);\n+      __ BIND(L_process256);\n+      \/\/ Grab input data\n+      __ evmovdquq(input0, Address(source, start_offset, Address::times_1, 0x00), Assembler::AVX_512bit);\n+      __ evmovdquq(input1, Address(source, start_offset, Address::times_1, 0x40), Assembler::AVX_512bit);\n+      __ evmovdquq(input2, Address(source, start_offset, Address::times_1, 0x80), Assembler::AVX_512bit);\n+      __ evmovdquq(input3, Address(source, start_offset, Address::times_1, 0xc0), Assembler::AVX_512bit);\n+\n+      \/\/ Copy the low part of the lookup table into the destination of the permutation\n+      __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n+      __ evmovdquq(translated1, lookup_lo, Assembler::AVX_512bit);\n+      __ evmovdquq(translated2, lookup_lo, Assembler::AVX_512bit);\n+      __ evmovdquq(translated3, lookup_lo, Assembler::AVX_512bit);\n+\n+      \/\/ Translate the base64 input into \"decoded\" bytes\n+      __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n+      __ evpermt2b(translated1, input1, lookup_hi, Assembler::AVX_512bit);\n+      __ evpermt2b(translated2, input2, lookup_hi, Assembler::AVX_512bit);\n+      __ evpermt2b(translated3, input3, lookup_hi, Assembler::AVX_512bit);\n+\n+      \/\/ OR all of the translations together to check for errors (high-order bit of byte set)\n+      __ vpternlogd(input0, 0xfe, input1, input2, Assembler::AVX_512bit);\n+\n+      __ vpternlogd(input3, 0xfe, translated0, translated1, Assembler::AVX_512bit);\n+      __ vpternlogd(input0, 0xfe, translated2, translated3, Assembler::AVX_512bit);\n+      __ vpor(errorvec, input3, input0, Assembler::AVX_512bit);\n+\n+      \/\/ Check if there was an error - if so, try 64-byte chunks\n+      __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n+      __ kortestql(k3, k3);\n+      __ jcc(Assembler::notZero, L_process64);\n+\n+      \/\/ The merging and shuffling happens here\n+      \/\/ We multiply each byte pair [00dddddd | 00cccccc | 00bbbbbb | 00aaaaaa]\n+      \/\/ Multiply [00cccccc] by 2^6 added to [00dddddd] to get [0000cccc | ccdddddd]\n+      \/\/ The pack16_op is a vector of 0x01400140, so multiply D by 1 and C by 0x40\n+      __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n+      __ vpmaddubsw(merge_ab_bc1, translated1, pack16_op, Assembler::AVX_512bit);\n+      __ vpmaddubsw(merge_ab_bc2, translated2, pack16_op, Assembler::AVX_512bit);\n+      __ vpmaddubsw(merge_ab_bc3, translated3, pack16_op, Assembler::AVX_512bit);\n+\n+      \/\/ Now do the same with packed 16-bit values.\n+      \/\/ We start with [0000cccc | ccdddddd | 0000aaaa | aabbbbbb]\n+      \/\/ pack32_op is 0x00011000 (2^12, 1), so this multiplies [0000aaaa | aabbbbbb] by 2^12\n+      \/\/ and adds [0000cccc | ccdddddd] to yield [00000000 | aaaaaabb | bbbbcccc | ccdddddd]\n+      __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n+      __ vpmaddwd(merged1, merge_ab_bc1, pack32_op, Assembler::AVX_512bit);\n+      __ vpmaddwd(merged2, merge_ab_bc2, pack32_op, Assembler::AVX_512bit);\n+      __ vpmaddwd(merged3, merge_ab_bc3, pack32_op, Assembler::AVX_512bit);\n+\n+      \/\/ The join vectors specify which byte from which vector goes into the outputs\n+      \/\/ One of every 4 bytes in the extended vector is zero, so we pack them into their\n+      \/\/ final positions in the register for storing (256 bytes in, 192 bytes out)\n+      __ evpermt2b(merged0, join01, merged1, Assembler::AVX_512bit);\n+      __ evpermt2b(merged1, join12, merged2, Assembler::AVX_512bit);\n+      __ evpermt2b(merged2, join23, merged3, Assembler::AVX_512bit);\n+\n+      \/\/ Store result\n+      __ evmovdquq(Address(dest, dp, Address::times_1, 0x00), merged0, Assembler::AVX_512bit);\n+      __ evmovdquq(Address(dest, dp, Address::times_1, 0x40), merged1, Assembler::AVX_512bit);\n+      __ evmovdquq(Address(dest, dp, Address::times_1, 0x80), merged2, Assembler::AVX_512bit);\n+\n+      __ addptr(source, 0x100);\n+      __ addptr(dest, 0xc0);\n+      __ subl(length, 0x100);\n+      __ cmpl(length, 64 * 4);\n+      __ jcc(Assembler::greaterEqual, L_process256);\n+\n+      \/\/ At this point, we've decoded 64 * 4 * n bytes.\n+      \/\/ The remaining length will be <= 64 * 4 - 1.\n+      \/\/ UNLESS there was an error decoding the first 256-byte chunk.  In this\n+      \/\/ case, the length will be arbitrarily long.\n+      \/\/\n+      \/\/ Note that this will be the path for MIME-encoded strings.\n+\n+      __ BIND(L_process64);\n+\n+      __ evmovdquq(pack24bits, ExternalAddress(StubRoutines::x86::base64_vbmi_pack_vec_addr()), Assembler::AVX_512bit, r13);\n+\n+      __ cmpl(length, 63);\n+      __ jcc(Assembler::lessEqual, L_finalBit);\n+\n+      __ align(32);\n+      __ BIND(L_process64Loop);\n+\n+      \/\/ Handle first 64-byte block\n+\n+      __ evmovdquq(input0, Address(source, start_offset), Assembler::AVX_512bit);\n+      __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n+      __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n+\n+      __ vpor(errorvec, translated0, input0, Assembler::AVX_512bit);\n+\n+      \/\/ Check for error and bomb out before updating dest\n+      __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n+      __ kortestql(k3, k3);\n+      __ jcc(Assembler::notZero, L_exit);\n+\n+      \/\/ Pack output register, selecting correct byte ordering\n+      __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n+      __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n+      __ vpermb(merged0, pack24bits, merged0, Assembler::AVX_512bit);\n+\n+      __ evmovdquq(Address(dest, dp), merged0, Assembler::AVX_512bit);\n+\n+      __ subl(length, 64);\n+      __ addptr(source, 64);\n+      __ addptr(dest, 48);\n+\n+      __ cmpl(length, 64);\n+      __ jcc(Assembler::greaterEqual, L_process64Loop);\n+\n+      __ cmpl(length, 0);\n+      __ jcc(Assembler::lessEqual, L_exit);\n+\n+      __ BIND(L_finalBit);\n+      \/\/ Now have 1 to 63 bytes left to decode\n+\n+      \/\/ I was going to let Java take care of the final fragment\n+      \/\/ however it will repeatedly call this routine for every 4 bytes\n+      \/\/ of input data, so handle the rest here.\n+      __ movq(rax, -1);\n+      __ bzhiq(rax, rax, length);    \/\/ Input mask in rax\n+\n+      __ movl(output_size, length);\n+      __ shrl(output_size, 2);   \/\/ Find (len \/ 4) * 3 (output length)\n+      __ lea(output_size, Address(output_size, output_size, Address::times_2, 0));\n+      \/\/ output_size in r13\n+\n+      \/\/ Strip pad characters, if any, and adjust length and mask\n+      __ cmpb(Address(source, length, Address::times_1, -1), '=');\n+      __ jcc(Assembler::equal, L_padding);\n+\n+      __ BIND(L_donePadding);\n+\n+      \/\/ Output size is (64 - output_size), output mask is (all 1s >> output_size).\n+      __ kmovql(input_mask, rax);\n+      __ movq(output_mask, -1);\n+      __ bzhiq(output_mask, output_mask, output_size);\n+\n+      \/\/ Load initial input with all valid base64 characters.  Will be used\n+      \/\/ in merging source bytes to avoid masking when determining if an error occurred.\n+      __ movl(rax, 0x61616161);\n+      __ evpbroadcastd(input_initial_valid_b64, rax, Assembler::AVX_512bit);\n+\n+      \/\/ A register containing all invalid base64 decoded values\n+      __ movl(rax, 0x80808080);\n+      __ evpbroadcastd(invalid_b64, rax, Assembler::AVX_512bit);\n+\n+      \/\/ input_mask is in k1\n+      \/\/ output_size is in r13\n+      \/\/ output_mask is in r15\n+      \/\/ zmm0 - free\n+      \/\/ zmm1 - 0x00011000\n+      \/\/ zmm2 - 0x01400140\n+      \/\/ zmm3 - errorvec\n+      \/\/ zmm4 - pack vector\n+      \/\/ zmm5 - lookup_lo\n+      \/\/ zmm6 - lookup_hi\n+      \/\/ zmm7 - errorvec\n+      \/\/ zmm8 - 0x61616161\n+      \/\/ zmm9 - 0x80808080\n+\n+      \/\/ Load only the bytes from source, merging into our \"fully-valid\" register\n+      __ evmovdqub(input_initial_valid_b64, input_mask, Address(source, start_offset, Address::times_1, 0x0), true, Assembler::AVX_512bit);\n+\n+      \/\/ Decode all bytes within our merged input\n+      __ evmovdquq(tmp, lookup_lo, Assembler::AVX_512bit);\n+      __ evpermt2b(tmp, input_initial_valid_b64, lookup_hi, Assembler::AVX_512bit);\n+      __ vporq(mask, tmp, input_initial_valid_b64, Assembler::AVX_512bit);\n+\n+      \/\/ Check for error.  Compare (decoded | initial) to all invalid.\n+      \/\/ If any bytes have their high-order bit set, then we have an error.\n+      __ evptestmb(k2, mask, invalid_b64, Assembler::AVX_512bit);\n+      __ kortestql(k2, k2);\n+\n+      \/\/ If we have an error, use the brute force loop to decode what we can (4-byte chunks).\n+      __ jcc(Assembler::notZero, L_bruteForce);\n+\n+      \/\/ Shuffle output bytes\n+      __ vpmaddubsw(tmp, tmp, pack16_op, Assembler::AVX_512bit);\n+      __ vpmaddwd(tmp, tmp, pack32_op, Assembler::AVX_512bit);\n+\n+      __ vpermb(tmp, pack24bits, tmp, Assembler::AVX_512bit);\n+      __ kmovql(k1, output_mask);\n+      __ evmovdqub(Address(dest, dp), k1, tmp, true, Assembler::AVX_512bit);\n+\n+      __ addptr(dest, output_size);\n+\n+      __ BIND(L_exit);\n+      __ vzeroupper();\n+      __ pop(rax);             \/\/ Get original dest value\n+      __ subptr(dest, rax);      \/\/ Number of bytes converted\n+      __ movptr(rax, dest);\n+      __ pop(rbx);\n+      __ pop(r15);\n+      __ pop(r14);\n+      __ pop(r13);\n+      __ pop(r12);\n+      __ leave();\n+      __ ret(0);\n+\n+      __ BIND(L_loadURL);\n+      __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_url_addr()), Assembler::AVX_512bit, r13);\n+      __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_url_addr()), Assembler::AVX_512bit, r13);\n+      __ jmp(L_continue);\n+\n+      __ BIND(L_padding);\n+      __ decrementq(output_size, 1);\n+      __ shrq(rax, 1);\n+\n+      __ cmpb(Address(source, length, Address::times_1, -2), '=');\n+      __ jcc(Assembler::notEqual, L_donePadding);\n+\n+      __ decrementq(output_size, 1);\n+      __ shrq(rax, 1);\n+      __ jmp(L_donePadding);\n+\n+      __ align(32);\n+      __ BIND(L_bruteForce);\n+    }   \/\/ End of if(avx512_vbmi)\n+\n+    \/\/ Use non-AVX code to decode 4-byte chunks into 3 bytes of output\n+\n+    \/\/ Register state (Linux):\n+    \/\/ r12-15 - saved on stack\n+    \/\/ rdi - src\n+    \/\/ rsi - sp\n+    \/\/ rdx - sl\n+    \/\/ rcx - dst\n+    \/\/ r8 - dp\n+    \/\/ r9 - isURL\n+\n+    \/\/ Register state (Windows):\n+    \/\/ r12-15 - saved on stack\n+    \/\/ rcx - src\n+    \/\/ rdx - sp\n+    \/\/ r8 - sl\n+    \/\/ r9 - dst\n+    \/\/ r12 - dp\n+    \/\/ r10 - isURL\n+\n+    \/\/ Registers (common):\n+    \/\/ length (r14) - bytes in src\n+\n+    const Register decode_table = r11;\n+    const Register out_byte_count = rbx;\n+    const Register byte1 = r13;\n+    const Register byte2 = r15;\n+    const Register byte3 = WINDOWS_ONLY(r8) NOT_WINDOWS(rdx);\n+    const Register byte4 = WINDOWS_ONLY(r10) NOT_WINDOWS(r9);\n+\n+    __ shrl(length, 2);    \/\/ Multiple of 4 bytes only - length is # 4-byte chunks\n@@ -5520,1 +6379,1 @@\n-    __ jcc(Assembler::lessEqual, L_exit);\n+    __ jcc(Assembler::lessEqual, L_exit_no_vzero);\n@@ -5522,5 +6381,3 @@\n-    __ lea(r11, ExternalAddress(StubRoutines::x86::base64_charset_addr()));\n-    \/\/ check if base64 charset(isURL=0) or base64 url charset(isURL=1) needs to be loaded\n-    __ cmpl(isURL, 0);\n-    __ jcc(Assembler::equal, L_processdata);\n-    __ lea(r11, ExternalAddress(StubRoutines::x86::base64url_charset_addr()));\n+    __ shll(isURL, 8);    \/\/ index into decode table based on isURL\n+    __ lea(decode_table, ExternalAddress(StubRoutines::x86::base64_decoding_table_addr()));\n+    __ addptr(decode_table, isURL);\n@@ -5528,187 +6385,44 @@\n-    \/\/ load masks required for encoding data\n-    __ BIND(L_processdata);\n-    __ movdqu(xmm16, ExternalAddress(StubRoutines::x86::base64_gather_mask_addr()));\n-    \/\/ Set 64 bits of K register.\n-    __ evpcmpeqb(k3, xmm16, xmm16, Assembler::AVX_512bit);\n-    __ evmovdquq(xmm12, ExternalAddress(StubRoutines::x86::base64_bswap_mask_addr()), Assembler::AVX_256bit, r13);\n-    __ evmovdquq(xmm13, ExternalAddress(StubRoutines::x86::base64_right_shift_mask_addr()), Assembler::AVX_512bit, r13);\n-    __ evmovdquq(xmm14, ExternalAddress(StubRoutines::x86::base64_left_shift_mask_addr()), Assembler::AVX_512bit, r13);\n-    __ evmovdquq(xmm15, ExternalAddress(StubRoutines::x86::base64_and_mask_addr()), Assembler::AVX_512bit, r13);\n-\n-    \/\/ Vector Base64 implementation, producing 96 bytes of encoded data\n-    __ BIND(L_process80);\n-    __ cmpl(length, 80);\n-    __ jcc(Assembler::below, L_process32);\n-    __ evmovdquq(xmm0, Address(source, start_offset, Address::times_1, 0), Assembler::AVX_256bit);\n-    __ evmovdquq(xmm1, Address(source, start_offset, Address::times_1, 24), Assembler::AVX_256bit);\n-    __ evmovdquq(xmm2, Address(source, start_offset, Address::times_1, 48), Assembler::AVX_256bit);\n-\n-    \/\/permute the input data in such a manner that we have continuity of the source\n-    __ vpermq(xmm3, xmm0, 148, Assembler::AVX_256bit);\n-    __ vpermq(xmm4, xmm1, 148, Assembler::AVX_256bit);\n-    __ vpermq(xmm5, xmm2, 148, Assembler::AVX_256bit);\n-\n-    \/\/shuffle input and group 3 bytes of data and to it add 0 as the 4th byte.\n-    \/\/we can deal with 12 bytes at a time in a 128 bit register\n-    __ vpshufb(xmm3, xmm3, xmm12, Assembler::AVX_256bit);\n-    __ vpshufb(xmm4, xmm4, xmm12, Assembler::AVX_256bit);\n-    __ vpshufb(xmm5, xmm5, xmm12, Assembler::AVX_256bit);\n-\n-    \/\/convert byte to word. Each 128 bit register will have 6 bytes for processing\n-    __ vpmovzxbw(xmm3, xmm3, Assembler::AVX_512bit);\n-    __ vpmovzxbw(xmm4, xmm4, Assembler::AVX_512bit);\n-    __ vpmovzxbw(xmm5, xmm5, Assembler::AVX_512bit);\n-\n-    \/\/ Extract bits in the following pattern 6, 4+2, 2+4, 6 to convert 3, 8 bit numbers to 4, 6 bit numbers\n-    __ evpsrlvw(xmm0, xmm3, xmm13,  Assembler::AVX_512bit);\n-    __ evpsrlvw(xmm1, xmm4, xmm13, Assembler::AVX_512bit);\n-    __ evpsrlvw(xmm2, xmm5, xmm13, Assembler::AVX_512bit);\n-\n-    __ evpsllvw(xmm3, xmm3, xmm14, Assembler::AVX_512bit);\n-    __ evpsllvw(xmm4, xmm4, xmm14, Assembler::AVX_512bit);\n-    __ evpsllvw(xmm5, xmm5, xmm14, Assembler::AVX_512bit);\n-\n-    __ vpsrlq(xmm0, xmm0, 8, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm1, xmm1, 8, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm2, xmm2, 8, Assembler::AVX_512bit);\n-\n-    __ vpsllq(xmm3, xmm3, 8, Assembler::AVX_512bit);\n-    __ vpsllq(xmm4, xmm4, 8, Assembler::AVX_512bit);\n-    __ vpsllq(xmm5, xmm5, 8, Assembler::AVX_512bit);\n-\n-    __ vpandq(xmm3, xmm3, xmm15, Assembler::AVX_512bit);\n-    __ vpandq(xmm4, xmm4, xmm15, Assembler::AVX_512bit);\n-    __ vpandq(xmm5, xmm5, xmm15, Assembler::AVX_512bit);\n-\n-    \/\/ Get the final 4*6 bits base64 encoding\n-    __ vporq(xmm3, xmm3, xmm0, Assembler::AVX_512bit);\n-    __ vporq(xmm4, xmm4, xmm1, Assembler::AVX_512bit);\n-    __ vporq(xmm5, xmm5, xmm2, Assembler::AVX_512bit);\n-\n-    \/\/ Shift\n-    __ vpsrlq(xmm3, xmm3, 8, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm4, xmm4, 8, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm5, xmm5, 8, Assembler::AVX_512bit);\n-\n-    \/\/ look up 6 bits in the base64 character set to fetch the encoding\n-    \/\/ we are converting word to dword as gather instructions need dword indices for looking up encoding\n-    __ vextracti64x4(xmm6, xmm3, 0);\n-    __ vpmovzxwd(xmm0, xmm6, Assembler::AVX_512bit);\n-    __ vextracti64x4(xmm6, xmm3, 1);\n-    __ vpmovzxwd(xmm1, xmm6, Assembler::AVX_512bit);\n-\n-    __ vextracti64x4(xmm6, xmm4, 0);\n-    __ vpmovzxwd(xmm2, xmm6, Assembler::AVX_512bit);\n-    __ vextracti64x4(xmm6, xmm4, 1);\n-    __ vpmovzxwd(xmm3, xmm6, Assembler::AVX_512bit);\n-\n-    __ vextracti64x4(xmm4, xmm5, 0);\n-    __ vpmovzxwd(xmm6, xmm4, Assembler::AVX_512bit);\n-\n-    __ vextracti64x4(xmm4, xmm5, 1);\n-    __ vpmovzxwd(xmm7, xmm4, Assembler::AVX_512bit);\n-\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm4, k2, Address(r11, xmm0, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm5, k2, Address(r11, xmm1, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm8, k2, Address(r11, xmm2, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm9, k2, Address(r11, xmm3, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm10, k2, Address(r11, xmm6, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm11, k2, Address(r11, xmm7, Address::times_4, 0), Assembler::AVX_512bit);\n-\n-    \/\/Down convert dword to byte. Final output is 16*6 = 96 bytes long\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 0), xmm4, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 16), xmm5, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 32), xmm8, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 48), xmm9, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 64), xmm10, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 80), xmm11, Assembler::AVX_512bit);\n-\n-    __ addq(dest, 96);\n-    __ addq(source, 72);\n-    __ subq(length, 72);\n-    __ jmp(L_process80);\n-\n-    \/\/ Vector Base64 implementation generating 32 bytes of encoded data\n-    __ BIND(L_process32);\n-    __ cmpl(length, 32);\n-    __ jcc(Assembler::below, L_process3);\n-    __ evmovdquq(xmm0, Address(source, start_offset), Assembler::AVX_256bit);\n-    __ vpermq(xmm0, xmm0, 148, Assembler::AVX_256bit);\n-    __ vpshufb(xmm6, xmm0, xmm12, Assembler::AVX_256bit);\n-    __ vpmovzxbw(xmm6, xmm6, Assembler::AVX_512bit);\n-    __ evpsrlvw(xmm2, xmm6, xmm13, Assembler::AVX_512bit);\n-    __ evpsllvw(xmm3, xmm6, xmm14, Assembler::AVX_512bit);\n-\n-    __ vpsrlq(xmm2, xmm2, 8, Assembler::AVX_512bit);\n-    __ vpsllq(xmm3, xmm3, 8, Assembler::AVX_512bit);\n-    __ vpandq(xmm3, xmm3, xmm15, Assembler::AVX_512bit);\n-    __ vporq(xmm1, xmm2, xmm3, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm1, xmm1, 8, Assembler::AVX_512bit);\n-    __ vextracti64x4(xmm9, xmm1, 0);\n-    __ vpmovzxwd(xmm6, xmm9, Assembler::AVX_512bit);\n-    __ vextracti64x4(xmm9, xmm1, 1);\n-    __ vpmovzxwd(xmm5, xmm9,  Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm8, k2, Address(r11, xmm6, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm10, k2, Address(r11, xmm5, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 0), xmm8, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 16), xmm10, Assembler::AVX_512bit);\n-    __ subq(length, 24);\n-    __ addq(dest, 32);\n-    __ addq(source, 24);\n-    __ jmp(L_process32);\n-\n-    \/\/ Scalar data processing takes 3 bytes at a time and produces 4 bytes of encoded data\n-    \/* This code corresponds to the scalar version of the following snippet in Base64.java\n-    ** int bits = (src[sp0++] & 0xff) << 16 |(src[sp0++] & 0xff) << 8 |(src[sp0++] & 0xff);\n-    ** dst[dp0++] = (byte)base64[(bits >> > 18) & 0x3f];\n-    ** dst[dp0++] = (byte)base64[(bits >> > 12) & 0x3f];\n-    ** dst[dp0++] = (byte)base64[(bits >> > 6) & 0x3f];\n-    ** dst[dp0++] = (byte)base64[bits & 0x3f];*\/\n-    __ BIND(L_process3);\n-    __ cmpl(length, 3);\n-    __ jcc(Assembler::below, L_exit);\n-    \/\/ Read 1 byte at a time\n-    __ movzbl(rax, Address(source, start_offset));\n-    __ shll(rax, 0x10);\n-    __ movl(r15, rax);\n-    __ movzbl(rax, Address(source, start_offset, Address::times_1, 1));\n-    __ shll(rax, 0x8);\n-    __ movzwl(rax, rax);\n-    __ orl(r15, rax);\n-    __ movzbl(rax, Address(source, start_offset, Address::times_1, 2));\n-    __ orl(rax, r15);\n-    \/\/ Save 3 bytes read in r15\n-    __ movl(r15, rax);\n-    __ shrl(rax, 0x12);\n-    __ andl(rax, 0x3f);\n-    \/\/ rax contains the index, r11 contains base64 lookup table\n-    __ movb(rax, Address(r11, rax, Address::times_4));\n-    \/\/ Write the encoded byte to destination\n-    __ movb(Address(dest, dp, Address::times_1, 0), rax);\n-    __ movl(rax, r15);\n-    __ shrl(rax, 0xc);\n-    __ andl(rax, 0x3f);\n-    __ movb(rax, Address(r11, rax, Address::times_4));\n-    __ movb(Address(dest, dp, Address::times_1, 1), rax);\n-    __ movl(rax, r15);\n-    __ shrl(rax, 0x6);\n-    __ andl(rax, 0x3f);\n-    __ movb(rax, Address(r11, rax, Address::times_4));\n-    __ movb(Address(dest, dp, Address::times_1, 2), rax);\n-    __ movl(rax, r15);\n-    __ andl(rax, 0x3f);\n-    __ movb(rax, Address(r11, rax, Address::times_4));\n-    __ movb(Address(dest, dp, Address::times_1, 3), rax);\n-    __ subl(length, 3);\n-    __ addq(dest, 4);\n-    __ addq(source, 3);\n-    __ jmp(L_process3);\n-    __ BIND(L_exit);\n+    __ jmp(L_bottomLoop);\n+\n+    __ align(32);\n+    __ BIND(L_forceLoop);\n+    __ shll(byte1, 18);\n+    __ shll(byte2, 12);\n+    __ shll(byte3, 6);\n+    __ orl(byte1, byte2);\n+    __ orl(byte1, byte3);\n+    __ orl(byte1, byte4);\n+\n+    __ addptr(source, 4);\n+\n+    __ movb(Address(dest, dp, Address::times_1, 2), byte1);\n+    __ shrl(byte1, 8);\n+    __ movb(Address(dest, dp, Address::times_1, 1), byte1);\n+    __ shrl(byte1, 8);\n+    __ movb(Address(dest, dp, Address::times_1, 0), byte1);\n+\n+    __ addptr(dest, 3);\n+    __ decrementl(length, 1);\n+    __ jcc(Assembler::zero, L_exit_no_vzero);\n+\n+    __ BIND(L_bottomLoop);\n+    __ load_unsigned_byte(byte1, Address(source, start_offset, Address::times_1, 0x00));\n+    __ load_unsigned_byte(byte2, Address(source, start_offset, Address::times_1, 0x01));\n+    __ load_signed_byte(byte1, Address(decode_table, byte1));\n+    __ load_signed_byte(byte2, Address(decode_table, byte2));\n+    __ load_unsigned_byte(byte3, Address(source, start_offset, Address::times_1, 0x02));\n+    __ load_unsigned_byte(byte4, Address(source, start_offset, Address::times_1, 0x03));\n+    __ load_signed_byte(byte3, Address(decode_table, byte3));\n+    __ load_signed_byte(byte4, Address(decode_table, byte4));\n+\n+    __ mov(rax, byte1);\n+    __ orl(rax, byte2);\n+    __ orl(rax, byte3);\n+    __ orl(rax, byte4);\n+    __ jcc(Assembler::positive, L_forceLoop);\n+\n+    __ BIND(L_exit_no_vzero);\n+    __ pop(rax);             \/\/ Get original dest value\n+    __ subptr(dest, rax);      \/\/ Number of bytes converted\n+    __ movptr(rax, dest);\n+    __ pop(rbx);\n@@ -5721,0 +6435,1 @@\n+\n@@ -5724,0 +6439,1 @@\n+\n@@ -7147,0 +7863,1 @@\n+\n@@ -7148,7 +7865,20 @@\n-      StubRoutines::x86::_and_mask = base64_and_mask_addr();\n-      StubRoutines::x86::_bswap_mask = base64_bswap_mask_addr();\n-      StubRoutines::x86::_base64_charset = base64_charset_addr();\n-      StubRoutines::x86::_url_charset = base64url_charset_addr();\n-      StubRoutines::x86::_gather_mask = base64_gather_mask_addr();\n-      StubRoutines::x86::_left_shift_mask = base64_left_shift_mask_addr();\n-      StubRoutines::x86::_right_shift_mask = base64_right_shift_mask_addr();\n+      if(VM_Version::supports_avx2() &&\n+         VM_Version::supports_avx512bw() &&\n+         VM_Version::supports_avx512vl()) {\n+        StubRoutines::x86::_avx2_shuffle_base64 = base64_avx2_shuffle_addr();\n+        StubRoutines::x86::_avx2_input_mask_base64 = base64_avx2_input_mask_addr();\n+        StubRoutines::x86::_avx2_lut_base64 = base64_avx2_lut_addr();\n+      }\n+      StubRoutines::x86::_encoding_table_base64 = base64_encoding_table_addr();\n+      if (VM_Version::supports_avx512_vbmi()) {\n+        StubRoutines::x86::_shuffle_base64 = base64_shuffle_addr();\n+        StubRoutines::x86::_lookup_lo_base64 = base64_vbmi_lookup_lo_addr();\n+        StubRoutines::x86::_lookup_hi_base64 = base64_vbmi_lookup_hi_addr();\n+        StubRoutines::x86::_lookup_lo_base64url = base64_vbmi_lookup_lo_url_addr();\n+        StubRoutines::x86::_lookup_hi_base64url = base64_vbmi_lookup_hi_url_addr();\n+        StubRoutines::x86::_pack_vec_base64 = base64_vbmi_pack_vec_addr();\n+        StubRoutines::x86::_join_0_1_base64 = base64_vbmi_join_0_1_addr();\n+        StubRoutines::x86::_join_1_2_base64 = base64_vbmi_join_1_2_addr();\n+        StubRoutines::x86::_join_2_3_base64 = base64_vbmi_join_2_3_addr();\n+      }\n+      StubRoutines::x86::_decoding_table_base64 = base64_decoding_table_addr();\n@@ -7156,0 +7886,1 @@\n+      StubRoutines::_base64_decodeBlock = generate_base64_decodeBlock();\n@@ -7188,1 +7919,4 @@\n-    libsvml = os::dll_load(JNI_LIB_PREFIX \"svml\" JNI_LIB_SUFFIX, ebuf, sizeof ebuf);\n+    char dll_name[JVM_MAXPATHLEN];\n+    if (os::dll_locate_lib(dll_name, sizeof(dll_name), Arguments::get_dll_dir(), \"svml\")) {\n+      libsvml = os::dll_load(dll_name, ebuf, sizeof ebuf);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":1058,"deletions":324,"binary":false,"changes":1382,"status":"modified"},{"patch":"@@ -772,0 +772,9 @@\n+      _features &= ~CPU_AVX512DQ;\n+      _features &= ~CPU_AVX512_VNNI;\n+      _features &= ~CPU_AVX512_VAES;\n+      _features &= ~CPU_AVX512_VPOPCNTDQ;\n+      _features &= ~CPU_AVX512_VPCLMULQDQ;\n+      _features &= ~CPU_AVX512_VBMI;\n+      _features &= ~CPU_AVX512_VBMI2;\n+      _features &= ~CPU_CLWB;\n+      _features &= ~CPU_FLUSHOPT;\n@@ -1015,4 +1024,0 @@\n-    \/\/ Can't continue because UseRTMLocking affects UseBiasedLocking flag\n-    \/\/ setting during arguments processing. See use_biased_locking().\n-    \/\/ VM_Version_init() is executed after UseBiasedLocking is used\n-    \/\/ in Thread::allocate().\n@@ -1026,2 +1031,0 @@\n-      \/\/ Can't continue because UseRTMLocking affects UseBiasedLocking flag\n-      \/\/ setting during arguments processing. See use_biased_locking().\n@@ -1065,2 +1068,0 @@\n-    \/\/ Can't continue because UseRTMLocking affects UseBiasedLocking flag\n-    \/\/ setting during arguments processing. See use_biased_locking().\n@@ -1739,21 +1740,0 @@\n-bool VM_Version::use_biased_locking() {\n-#if INCLUDE_RTM_OPT\n-  \/\/ RTM locking is most useful when there is high lock contention and\n-  \/\/ low data contention.  With high lock contention the lock is usually\n-  \/\/ inflated and biased locking is not suitable for that case.\n-  \/\/ RTM locking code requires that biased locking is off.\n-  \/\/ Note: we can't switch off UseBiasedLocking in get_processor_features()\n-  \/\/ because it is used by Thread::allocate() which is called before\n-  \/\/ VM_Version::initialize().\n-  if (UseRTMLocking && UseBiasedLocking) {\n-    if (FLAG_IS_DEFAULT(UseBiasedLocking)) {\n-      FLAG_SET_DEFAULT(UseBiasedLocking, false);\n-    } else {\n-      warning(\"Biased locking is not supported with RTM locking; ignoring UseBiasedLocking flag.\" );\n-      UseBiasedLocking = false;\n-    }\n-  }\n-#endif\n-  return UseBiasedLocking;\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":9,"deletions":29,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -1838,0 +1838,5 @@\n+    case Op_VectorMaskCmp:\n+      if (vlen < 2 || size_in_bits < 32) {\n+        return false;\n+      }\n+      break;\n@@ -1871,1 +1876,1 @@\n-bool Matcher::is_generic_reg2reg_move(MachNode* m) {\n+bool Matcher::is_reg2reg_move(MachNode* m) {\n@@ -1875,0 +1880,8 @@\n+    case MoveF2VL_rule:\n+    case MoveF2LEG_rule:\n+    case MoveVL2F_rule:\n+    case MoveLEG2F_rule:\n+    case MoveD2VL_rule:\n+    case MoveD2LEG_rule:\n+    case MoveVL2D_rule:\n+    case MoveLEG2D_rule:\n@@ -1901,12 +1914,0 @@\n-const int Matcher::float_pressure(int default_pressure_threshold) {\n-  int float_pressure_threshold = default_pressure_threshold;\n-#ifdef _LP64\n-  if (UseAVX > 2) {\n-    \/\/ Increase pressure threshold on machines with AVX3 which have\n-    \/\/ 2x more XMM registers.\n-    float_pressure_threshold = default_pressure_threshold * 2;\n-  }\n-#endif\n-  return float_pressure_threshold;\n-}\n-\n@@ -4859,1 +4860,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -4892,1 +4894,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -4926,1 +4929,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -4959,1 +4963,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -4992,1 +4997,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5025,1 +5031,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5060,1 +5067,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5094,1 +5102,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5127,1 +5136,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5161,1 +5171,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5194,1 +5205,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5227,1 +5239,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5375,1 +5388,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5409,1 +5423,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5433,1 +5448,2 @@\n-  predicate(VM_Version::supports_avx512dq());\n+  predicate(VM_Version::supports_avx512dq() &&\n+              (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5518,1 +5534,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5551,1 +5568,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5622,1 +5640,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5655,1 +5674,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -5839,0 +5859,1 @@\n+  predicate(vector_length_in_bytes(n->in(1)) > 8);\n@@ -5862,0 +5883,1 @@\n+  predicate(vector_length_in_bytes(n->in(1)) > 8);\n@@ -6474,1 +6496,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -6508,1 +6531,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -6542,1 +6566,2 @@\n-  predicate(UseAVX > 0);\n+  predicate((UseAVX > 0) &&\n+            (vector_length_in_bytes(n->in(1)) > 8));\n@@ -6588,1 +6613,1 @@\n-  predicate(UseAVX <= 2 &&\n+  predicate((UseAVX <= 2 || !VM_Version::supports_avx512vlbw()) &&\n@@ -6604,1 +6629,1 @@\n-  predicate(UseAVX <= 2 &&\n+  predicate((UseAVX <= 2 || !VM_Version::supports_avx512vlbw()) &&\n@@ -6622,1 +6647,1 @@\n-  predicate(UseAVX > 2 ||\n+  predicate((UseAVX > 2 && VM_Version::supports_avx512vlbw()) ||\n@@ -6737,1 +6762,1 @@\n-        __ vcvtdq2ps($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n+        __ vcvtdq2ps($dst$$XMMRegister, $src$$XMMRegister, dst_vlen_enc);\n@@ -6743,1 +6768,1 @@\n-        __ vcvtdq2pd($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n+        __ vcvtdq2pd($dst$$XMMRegister, $src$$XMMRegister, dst_vlen_enc);\n@@ -6909,1 +6934,1 @@\n-            vector_length_in_bytes(n->in(1)->in(1)) >=  8 && \/\/ src1\n+            vector_length_in_bytes(n->in(1)->in(1)) >=  4 && \/\/ src1\n@@ -7445,1 +7470,16 @@\n-instruct loadMask(vec dst, vec src) %{\n+instruct loadMask(legVec dst, legVec src) %{\n+  predicate(!VM_Version::supports_avx512vlbw());\n+  match(Set dst (VectorLoadMask src));\n+  effect(TEMP dst);\n+  format %{ \"vector_loadmask_byte $dst,$src\\n\\t\" %}\n+  ins_encode %{\n+    int vlen_in_bytes = vector_length_in_bytes(this);\n+    BasicType elem_bt = vector_element_basic_type(this);\n+\n+    __ load_vector_mask($dst$$XMMRegister, $src$$XMMRegister, vlen_in_bytes, elem_bt, true);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct loadMask_evex(vec dst, vec src) %{\n+  predicate(VM_Version::supports_avx512vlbw());\n@@ -7453,1 +7493,1 @@\n-    __ load_vector_mask($dst$$XMMRegister, $src$$XMMRegister, vlen_in_bytes, elem_bt);\n+    __ load_vector_mask($dst$$XMMRegister, $src$$XMMRegister, vlen_in_bytes, elem_bt, false);\n@@ -7962,0 +8002,1 @@\n+  predicate(vector_length_in_bytes(n->in(1)) > 8);\n@@ -7986,0 +8027,1 @@\n+  predicate(vector_length_in_bytes(n->in(1)) > 8);\n@@ -8063,0 +8105,1 @@\n+  predicate(vector_length_in_bytes(n->in(1)->in(1)) > 8);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":86,"deletions":43,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -1440,0 +1440,10 @@\n+uint Matcher::int_pressure_limit()\n+{\n+  return (INTPRESSURE == -1) ? 6 : INTPRESSURE;\n+}\n+\n+uint Matcher::float_pressure_limit()\n+{\n+  return (FLOATPRESSURE == -1) ? 6 : FLOATPRESSURE;\n+}\n+\n@@ -5011,0 +5021,83 @@\n+\/\/ Dummy reg-to-reg vector moves. Removed during post-selection cleanup.\n+\/\/ Load Float\n+instruct MoveF2LEG(legRegF dst, regF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveLEG2F(regF dst, legRegF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveF2VL(vlRegF dst, regF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveVL2F(regF dst, vlRegF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\n+\n+\/\/ Load Double\n+instruct MoveD2LEG(legRegD dst, regD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveLEG2D(regD dst, legRegD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveD2VL(vlRegD dst, regD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveVL2D(regD dst, vlRegD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n@@ -5756,40 +5849,0 @@\n-\/\/ Load Float\n-instruct MoveF2LEG(legRegF dst, regF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Float\n-instruct MoveLEG2F(regF dst, legRegF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Double\n-instruct MoveD2LEG(legRegD dst, regD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Double\n-instruct MoveLEG2D(regD dst, legRegD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n@@ -6425,20 +6478,0 @@\n-\/\/ Load Double\n-instruct MoveD2VL(vlRegD dst, regD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Double\n-instruct MoveVL2D(regD dst, vlRegD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n@@ -6458,19 +6491,0 @@\n-\/\/ Load Float\n-instruct MoveF2VL(vlRegF dst, regF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Float\n-instruct MoveVL2F(regF dst, vlRegF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n@@ -13665,1 +13679,1 @@\n-                 _counters, _rtm_counters, _stack_rtm_counters,\n+                 _rtm_counters, _stack_rtm_counters,\n@@ -13680,1 +13694,1 @@\n-                 $scr$$Register, noreg, noreg, _counters, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, NULL, NULL, NULL, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":95,"deletions":81,"binary":false,"changes":176,"status":"modified"},{"patch":"@@ -325,0 +325,1 @@\n+extern RegMask _FLOAT_REG_mask;\n@@ -353,0 +354,1 @@\n+RegMask _FLOAT_REG_mask;\n@@ -428,0 +430,4 @@\n+  \/\/ _FLOAT_REG_LEGACY_mask\/_FLOAT_REG_EVEX_mask is generated by adlc\n+  \/\/ from the float_reg_legacy\/float_reg_evex register class.\n+  _FLOAT_REG_mask = VM_Version::supports_evex() ? _FLOAT_REG_EVEX_mask : _FLOAT_REG_LEGACY_mask;\n+\n@@ -1760,0 +1766,14 @@\n+uint Matcher::int_pressure_limit()\n+{\n+  return (INTPRESSURE == -1) ? _INT_REG_mask.Size() : INTPRESSURE;\n+}\n+\n+uint Matcher::float_pressure_limit()\n+{\n+  \/\/ After experiment around with different values, the following default threshold\n+  \/\/ works best for LCM's register pressure scheduling on x64.\n+  uint dec_count  = VM_Version::supports_evex() ? 4 : 2;\n+  uint default_float_pressure_threshold = _FLOAT_REG_mask.Size() - dec_count;\n+  return (FLOATPRESSURE == -1) ? default_float_pressure_threshold : FLOATPRESSURE;\n+}\n+\n@@ -4818,0 +4838,81 @@\n+\/\/ Dummy reg-to-reg vector moves. Removed during post-selection cleanup.\n+\/\/ Load Float\n+instruct MoveF2VL(vlRegF dst, regF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveF2LEG(legRegF dst, regF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveVL2F(regF dst, vlRegF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveLEG2F(regF dst, legRegF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveD2VL(vlRegD dst, regD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveD2LEG(legRegD dst, regD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveVL2D(regD dst, vlRegD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveLEG2D(regD dst, legRegD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n@@ -5231,40 +5332,0 @@\n-\/\/ Load Float\n-instruct MoveF2VL(vlRegF dst, regF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Float\n-instruct MoveF2LEG(legRegF dst, regF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Float\n-instruct MoveVL2F(regF dst, vlRegF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Float\n-instruct MoveLEG2F(regF dst, legRegF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n@@ -5298,39 +5359,0 @@\n-\/\/ Load Double\n-instruct MoveD2VL(vlRegD dst, regD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Double\n-instruct MoveD2LEG(legRegD dst, regD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Double\n-instruct MoveVL2D(regD dst, vlRegD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Double\n-instruct MoveLEG2D(regD dst, legRegD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n@@ -13148,1 +13170,1 @@\n-                 _counters, _rtm_counters, _stack_rtm_counters,\n+                 _rtm_counters, _stack_rtm_counters,\n@@ -13163,1 +13185,1 @@\n-                 $scr$$Register, $cx1$$Register, noreg, _counters, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, $cx1$$Register, noreg, NULL, NULL, NULL, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":103,"deletions":81,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -233,1 +233,0 @@\n-  AD.addInclude(AD._CPP_file, \"runtime\/biasedLocking.hpp\");\n","filename":"src\/hotspot\/share\/adlc\/main.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -232,1 +232,5 @@\n-  void emit_int32(int32_t x) { *((int32_t*) end()) = x; set_end(end() + sizeof(int32_t)); }\n+  void emit_int32(int32_t x) {\n+    address curr = end();\n+    *((int32_t*) curr) = x;\n+    set_end(curr + sizeof(int32_t));\n+  }\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -800,1 +800,1 @@\n-          if (compilation()->profile_branches() || compilation()->count_backedges()) {\n+          if (compilation()->profile_branches() || compilation()->is_profiling()) {\n@@ -863,160 +863,5 @@\n-\n-static bool match_index_and_scale(Instruction*  instr,\n-                                  Instruction** index,\n-                                  int*          log2_scale) {\n-  \/\/ Skip conversion ops. This works only on 32bit because of the implicit l2i that the\n-  \/\/ unsafe performs.\n-#ifndef _LP64\n-  Convert* convert = instr->as_Convert();\n-  if (convert != NULL && convert->op() == Bytecodes::_i2l) {\n-    assert(convert->value()->type() == intType, \"invalid input type\");\n-    instr = convert->value();\n-  }\n-#endif\n-\n-  ShiftOp* shift = instr->as_ShiftOp();\n-  if (shift != NULL) {\n-    if (shift->op() == Bytecodes::_lshl) {\n-      assert(shift->x()->type() == longType, \"invalid input type\");\n-    } else {\n-#ifndef _LP64\n-      if (shift->op() == Bytecodes::_ishl) {\n-        assert(shift->x()->type() == intType, \"invalid input type\");\n-      } else {\n-        return false;\n-      }\n-#else\n-      return false;\n-#endif\n-    }\n-\n-\n-    \/\/ Constant shift value?\n-    Constant* con = shift->y()->as_Constant();\n-    if (con == NULL) return false;\n-    \/\/ Well-known type and value?\n-    IntConstant* val = con->type()->as_IntConstant();\n-    assert(val != NULL, \"Should be an int constant\");\n-\n-    *index = shift->x();\n-    int tmp_scale = val->value();\n-    if (tmp_scale >= 0 && tmp_scale < 4) {\n-      *log2_scale = tmp_scale;\n-      return true;\n-    } else {\n-      return false;\n-    }\n-  }\n-\n-  ArithmeticOp* arith = instr->as_ArithmeticOp();\n-  if (arith != NULL) {\n-    \/\/ See if either arg is a known constant\n-    Constant* con = arith->x()->as_Constant();\n-    if (con != NULL) {\n-      *index = arith->y();\n-    } else {\n-      con = arith->y()->as_Constant();\n-      if (con == NULL) return false;\n-      *index = arith->x();\n-    }\n-    long const_value;\n-    \/\/ Check for integer multiply\n-    if (arith->op() == Bytecodes::_lmul) {\n-      assert((*index)->type() == longType, \"invalid input type\");\n-      LongConstant* val = con->type()->as_LongConstant();\n-      assert(val != NULL, \"expecting a long constant\");\n-      const_value = val->value();\n-    } else {\n-#ifndef _LP64\n-      if (arith->op() == Bytecodes::_imul) {\n-        assert((*index)->type() == intType, \"invalid input type\");\n-        IntConstant* val = con->type()->as_IntConstant();\n-        assert(val != NULL, \"expecting an int constant\");\n-        const_value = val->value();\n-      } else {\n-        return false;\n-      }\n-#else\n-      return false;\n-#endif\n-    }\n-    switch (const_value) {\n-    case 1: *log2_scale = 0; return true;\n-    case 2: *log2_scale = 1; return true;\n-    case 4: *log2_scale = 2; return true;\n-    case 8: *log2_scale = 3; return true;\n-    default:            return false;\n-    }\n-  }\n-\n-  \/\/ Unknown instruction sequence; don't touch it\n-  return false;\n-}\n-\n-\n-static bool match(UnsafeRawOp* x,\n-                  Instruction** base,\n-                  Instruction** index,\n-                  int*          log2_scale) {\n-  ArithmeticOp* root = x->base()->as_ArithmeticOp();\n-  if (root == NULL) return false;\n-  \/\/ Limit ourselves to addition for now\n-  if (root->op() != Bytecodes::_ladd) return false;\n-\n-  bool match_found = false;\n-  \/\/ Try to find shift or scale op\n-  if (match_index_and_scale(root->y(), index, log2_scale)) {\n-    *base = root->x();\n-    match_found = true;\n-  } else if (match_index_and_scale(root->x(), index, log2_scale)) {\n-    *base = root->y();\n-    match_found = true;\n-  } else if (NOT_LP64(root->y()->as_Convert() != NULL) LP64_ONLY(false)) {\n-    \/\/ Skipping i2l works only on 32bit because of the implicit l2i that the unsafe performs.\n-    \/\/ 64bit needs a real sign-extending conversion.\n-    Convert* convert = root->y()->as_Convert();\n-    if (convert->op() == Bytecodes::_i2l) {\n-      assert(convert->value()->type() == intType, \"should be an int\");\n-      \/\/ pick base and index, setting scale at 1\n-      *base  = root->x();\n-      *index = convert->value();\n-      *log2_scale = 0;\n-      match_found = true;\n-    }\n-  }\n-  \/\/ The default solution\n-  if (!match_found) {\n-    *base = root->x();\n-    *index = root->y();\n-    *log2_scale = 0;\n-  }\n-\n-  \/\/ If the value is pinned then it will be always be computed so\n-  \/\/ there's no profit to reshaping the expression.\n-  return !root->is_pinned();\n-}\n-\n-\n-void Canonicalizer::do_UnsafeRawOp(UnsafeRawOp* x) {\n-  Instruction* base = NULL;\n-  Instruction* index = NULL;\n-  int          log2_scale;\n-\n-  if (match(x, &base, &index, &log2_scale)) {\n-    x->set_base(base);\n-    x->set_index(index);\n-    x->set_log2_scale(log2_scale);\n-    if (PrintUnsafeOptimization) {\n-      tty->print_cr(\"Canonicalizer: UnsafeRawOp id %d: base = id %d, index = id %d, log2_scale = %d\",\n-                    x->id(), x->base()->id(), x->index()->id(), x->log2_scale());\n-    }\n-  }\n-}\n-\n-void Canonicalizer::do_RoundFP(RoundFP* x) {}\n-void Canonicalizer::do_UnsafeGetRaw(UnsafeGetRaw* x) { if (OptimizeUnsafes) do_UnsafeRawOp(x); }\n-void Canonicalizer::do_UnsafePutRaw(UnsafePutRaw* x) { if (OptimizeUnsafes) do_UnsafeRawOp(x); }\n-void Canonicalizer::do_UnsafeGetObject(UnsafeGetObject* x) {}\n-void Canonicalizer::do_UnsafePutObject(UnsafePutObject* x) {}\n-void Canonicalizer::do_UnsafeGetAndSetObject(UnsafeGetAndSetObject* x) {}\n-void Canonicalizer::do_ProfileCall(ProfileCall* x) {}\n+void Canonicalizer::do_RoundFP        (RoundFP*         x) {}\n+void Canonicalizer::do_UnsafeGet      (UnsafeGet*       x) {}\n+void Canonicalizer::do_UnsafePut      (UnsafePut*       x) {}\n+void Canonicalizer::do_UnsafeGetAndSet(UnsafeGetAndSet* x) {}\n+void Canonicalizer::do_ProfileCall    (ProfileCall*     x) {}\n@@ -1024,3 +869,3 @@\n-void Canonicalizer::do_ProfileInvoke(ProfileInvoke* x) {}\n-void Canonicalizer::do_ProfileACmpTypes(ProfileACmpTypes* x) {}\n-void Canonicalizer::do_RuntimeCall(RuntimeCall* x) {}\n+void Canonicalizer::do_ProfileInvoke    (ProfileInvoke* x) {}\n+void Canonicalizer::do_ProfileACmpTypes (ProfileACmpTypes* x) {}\n+void Canonicalizer::do_RuntimeCall      (RuntimeCall* x) {}\n@@ -1029,1 +874,1 @@\n-void Canonicalizer::do_Assert(Assert* x) {}\n+void Canonicalizer::do_Assert         (Assert*          x) {}\n@@ -1031,1 +876,1 @@\n-void Canonicalizer::do_MemBar(MemBar* x) {}\n+void Canonicalizer::do_MemBar         (MemBar*          x) {}\n","filename":"src\/hotspot\/share\/c1\/c1_Canonicalizer.cpp","additions":11,"deletions":166,"binary":false,"changes":177,"status":"modified"},{"patch":"@@ -49,6 +49,0 @@\n-  void do_UnsafeRawOp(UnsafeRawOp* x);\n-\n-  void unsafe_raw_match(UnsafeRawOp* x,\n-                        Instruction** base,\n-                        Instruction** index,\n-                        int* scale);\n@@ -104,5 +98,3 @@\n-  virtual void do_UnsafeGetRaw   (UnsafeGetRaw*    x);\n-  virtual void do_UnsafePutRaw   (UnsafePutRaw*    x);\n-  virtual void do_UnsafeGetObject(UnsafeGetObject* x);\n-  virtual void do_UnsafePutObject(UnsafePutObject* x);\n-  virtual void do_UnsafeGetAndSetObject(UnsafeGetAndSetObject* x);\n+  virtual void do_UnsafeGet      (UnsafeGet*       x);\n+  virtual void do_UnsafePut      (UnsafePut*       x);\n+  virtual void do_UnsafeGetAndSet(UnsafeGetAndSet* x);\n","filename":"src\/hotspot\/share\/c1\/c1_Canonicalizer.hpp","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -59,2 +59,0 @@\n-  virtual bool is_range_check_stub() const       { return false; }\n-  virtual bool is_divbyzero_stub() const         { return false; }\n@@ -70,9 +68,1 @@\n-  virtual void visit(LIR_OpVisitState* visit) {\n-#ifndef PRODUCT\n-    if (LIRTracePeephole && Verbose) {\n-      tty->print(\"no visitor for \");\n-      print_name(tty);\n-      tty->cr();\n-    }\n-#endif\n-  }\n+  virtual void visit(LIR_OpVisitState* visit) = 0;\n@@ -184,1 +174,0 @@\n-  virtual bool is_range_check_stub() const       { return true; }\n@@ -227,1 +216,0 @@\n-  virtual bool is_divbyzero_stub() const         { return true; }\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":1,"deletions":13,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -233,2 +233,0 @@\n-  bool count_invocations() { return is_profiling(); }\n-  bool count_backedges()   { return is_profiling(); }\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3444,1 +3444,1 @@\n-  if (std_entry->number_of_preds() > 0 || count_invocations() || count_backedges() || RangeCheckElimination) {\n+  if (std_entry->number_of_preds() > 0 || is_profiling() || RangeCheckElimination) {\n@@ -3531,4 +3531,5 @@\n-      get = append(new UnsafeGetRaw(as_BasicType(local->type()), e,\n-                                    append(new Constant(new IntConstant(offset))),\n-                                    0,\n-                                    true \/*unaligned*\/, true \/*wide*\/));\n+      Value off_val = append(new Constant(new IntConstant(offset)));\n+      get = append(new UnsafeGet(as_BasicType(local->type()), e,\n+                                 off_val,\n+                                 false\/*is_volatile*\/,\n+                                 true\/*is_raw*\/));\n@@ -3864,44 +3865,44 @@\n-  case vmIntrinsics::_getReference       : append_unsafe_get_obj(callee, T_OBJECT,  false); return;\n-  case vmIntrinsics::_getBoolean         : append_unsafe_get_obj(callee, T_BOOLEAN, false); return;\n-  case vmIntrinsics::_getByte            : append_unsafe_get_obj(callee, T_BYTE,    false); return;\n-  case vmIntrinsics::_getShort           : append_unsafe_get_obj(callee, T_SHORT,   false); return;\n-  case vmIntrinsics::_getChar            : append_unsafe_get_obj(callee, T_CHAR,    false); return;\n-  case vmIntrinsics::_getInt             : append_unsafe_get_obj(callee, T_INT,     false); return;\n-  case vmIntrinsics::_getLong            : append_unsafe_get_obj(callee, T_LONG,    false); return;\n-  case vmIntrinsics::_getFloat           : append_unsafe_get_obj(callee, T_FLOAT,   false); return;\n-  case vmIntrinsics::_getDouble          : append_unsafe_get_obj(callee, T_DOUBLE,  false); return;\n-  case vmIntrinsics::_putReference       : append_unsafe_put_obj(callee, T_OBJECT,  false); return;\n-  case vmIntrinsics::_putBoolean         : append_unsafe_put_obj(callee, T_BOOLEAN, false); return;\n-  case vmIntrinsics::_putByte            : append_unsafe_put_obj(callee, T_BYTE,    false); return;\n-  case vmIntrinsics::_putShort           : append_unsafe_put_obj(callee, T_SHORT,   false); return;\n-  case vmIntrinsics::_putChar            : append_unsafe_put_obj(callee, T_CHAR,    false); return;\n-  case vmIntrinsics::_putInt             : append_unsafe_put_obj(callee, T_INT,     false); return;\n-  case vmIntrinsics::_putLong            : append_unsafe_put_obj(callee, T_LONG,    false); return;\n-  case vmIntrinsics::_putFloat           : append_unsafe_put_obj(callee, T_FLOAT,   false); return;\n-  case vmIntrinsics::_putDouble          : append_unsafe_put_obj(callee, T_DOUBLE,  false); return;\n-  case vmIntrinsics::_getShortUnaligned  : append_unsafe_get_obj(callee, T_SHORT,   false); return;\n-  case vmIntrinsics::_getCharUnaligned   : append_unsafe_get_obj(callee, T_CHAR,    false); return;\n-  case vmIntrinsics::_getIntUnaligned    : append_unsafe_get_obj(callee, T_INT,     false); return;\n-  case vmIntrinsics::_getLongUnaligned   : append_unsafe_get_obj(callee, T_LONG,    false); return;\n-  case vmIntrinsics::_putShortUnaligned  : append_unsafe_put_obj(callee, T_SHORT,   false); return;\n-  case vmIntrinsics::_putCharUnaligned   : append_unsafe_put_obj(callee, T_CHAR,    false); return;\n-  case vmIntrinsics::_putIntUnaligned    : append_unsafe_put_obj(callee, T_INT,     false); return;\n-  case vmIntrinsics::_putLongUnaligned   : append_unsafe_put_obj(callee, T_LONG,    false); return;\n-  case vmIntrinsics::_getReferenceVolatile  : append_unsafe_get_obj(callee, T_OBJECT,  true); return;\n-  case vmIntrinsics::_getBooleanVolatile : append_unsafe_get_obj(callee, T_BOOLEAN, true); return;\n-  case vmIntrinsics::_getByteVolatile    : append_unsafe_get_obj(callee, T_BYTE,    true); return;\n-  case vmIntrinsics::_getShortVolatile   : append_unsafe_get_obj(callee, T_SHORT,   true); return;\n-  case vmIntrinsics::_getCharVolatile    : append_unsafe_get_obj(callee, T_CHAR,    true); return;\n-  case vmIntrinsics::_getIntVolatile     : append_unsafe_get_obj(callee, T_INT,     true); return;\n-  case vmIntrinsics::_getLongVolatile    : append_unsafe_get_obj(callee, T_LONG,    true); return;\n-  case vmIntrinsics::_getFloatVolatile   : append_unsafe_get_obj(callee, T_FLOAT,   true); return;\n-  case vmIntrinsics::_getDoubleVolatile  : append_unsafe_get_obj(callee, T_DOUBLE,  true); return;\n-  case vmIntrinsics::_putReferenceVolatile : append_unsafe_put_obj(callee, T_OBJECT,  true); return;\n-  case vmIntrinsics::_putBooleanVolatile : append_unsafe_put_obj(callee, T_BOOLEAN, true); return;\n-  case vmIntrinsics::_putByteVolatile    : append_unsafe_put_obj(callee, T_BYTE,    true); return;\n-  case vmIntrinsics::_putShortVolatile   : append_unsafe_put_obj(callee, T_SHORT,   true); return;\n-  case vmIntrinsics::_putCharVolatile    : append_unsafe_put_obj(callee, T_CHAR,    true); return;\n-  case vmIntrinsics::_putIntVolatile     : append_unsafe_put_obj(callee, T_INT,     true); return;\n-  case vmIntrinsics::_putLongVolatile    : append_unsafe_put_obj(callee, T_LONG,    true); return;\n-  case vmIntrinsics::_putFloatVolatile   : append_unsafe_put_obj(callee, T_FLOAT,   true); return;\n-  case vmIntrinsics::_putDoubleVolatile  : append_unsafe_put_obj(callee, T_DOUBLE,  true); return;\n+  case vmIntrinsics::_getReference           : append_unsafe_get(callee, T_OBJECT,  false); return;\n+  case vmIntrinsics::_getBoolean             : append_unsafe_get(callee, T_BOOLEAN, false); return;\n+  case vmIntrinsics::_getByte                : append_unsafe_get(callee, T_BYTE,    false); return;\n+  case vmIntrinsics::_getShort               : append_unsafe_get(callee, T_SHORT,   false); return;\n+  case vmIntrinsics::_getChar                : append_unsafe_get(callee, T_CHAR,    false); return;\n+  case vmIntrinsics::_getInt                 : append_unsafe_get(callee, T_INT,     false); return;\n+  case vmIntrinsics::_getLong                : append_unsafe_get(callee, T_LONG,    false); return;\n+  case vmIntrinsics::_getFloat               : append_unsafe_get(callee, T_FLOAT,   false); return;\n+  case vmIntrinsics::_getDouble              : append_unsafe_get(callee, T_DOUBLE,  false); return;\n+  case vmIntrinsics::_putReference           : append_unsafe_put(callee, T_OBJECT,  false); return;\n+  case vmIntrinsics::_putBoolean             : append_unsafe_put(callee, T_BOOLEAN, false); return;\n+  case vmIntrinsics::_putByte                : append_unsafe_put(callee, T_BYTE,    false); return;\n+  case vmIntrinsics::_putShort               : append_unsafe_put(callee, T_SHORT,   false); return;\n+  case vmIntrinsics::_putChar                : append_unsafe_put(callee, T_CHAR,    false); return;\n+  case vmIntrinsics::_putInt                 : append_unsafe_put(callee, T_INT,     false); return;\n+  case vmIntrinsics::_putLong                : append_unsafe_put(callee, T_LONG,    false); return;\n+  case vmIntrinsics::_putFloat               : append_unsafe_put(callee, T_FLOAT,   false); return;\n+  case vmIntrinsics::_putDouble              : append_unsafe_put(callee, T_DOUBLE,  false); return;\n+  case vmIntrinsics::_getShortUnaligned      : append_unsafe_get(callee, T_SHORT,   false); return;\n+  case vmIntrinsics::_getCharUnaligned       : append_unsafe_get(callee, T_CHAR,    false); return;\n+  case vmIntrinsics::_getIntUnaligned        : append_unsafe_get(callee, T_INT,     false); return;\n+  case vmIntrinsics::_getLongUnaligned       : append_unsafe_get(callee, T_LONG,    false); return;\n+  case vmIntrinsics::_putShortUnaligned      : append_unsafe_put(callee, T_SHORT,   false); return;\n+  case vmIntrinsics::_putCharUnaligned       : append_unsafe_put(callee, T_CHAR,    false); return;\n+  case vmIntrinsics::_putIntUnaligned        : append_unsafe_put(callee, T_INT,     false); return;\n+  case vmIntrinsics::_putLongUnaligned       : append_unsafe_put(callee, T_LONG,    false); return;\n+  case vmIntrinsics::_getReferenceVolatile   : append_unsafe_get(callee, T_OBJECT,  true); return;\n+  case vmIntrinsics::_getBooleanVolatile     : append_unsafe_get(callee, T_BOOLEAN, true); return;\n+  case vmIntrinsics::_getByteVolatile        : append_unsafe_get(callee, T_BYTE,    true); return;\n+  case vmIntrinsics::_getShortVolatile       : append_unsafe_get(callee, T_SHORT,   true); return;\n+  case vmIntrinsics::_getCharVolatile        : append_unsafe_get(callee, T_CHAR,    true); return;\n+  case vmIntrinsics::_getIntVolatile         : append_unsafe_get(callee, T_INT,     true); return;\n+  case vmIntrinsics::_getLongVolatile        : append_unsafe_get(callee, T_LONG,    true); return;\n+  case vmIntrinsics::_getFloatVolatile       : append_unsafe_get(callee, T_FLOAT,   true); return;\n+  case vmIntrinsics::_getDoubleVolatile      : append_unsafe_get(callee, T_DOUBLE,  true); return;\n+  case vmIntrinsics::_putReferenceVolatile   : append_unsafe_put(callee, T_OBJECT,  true); return;\n+  case vmIntrinsics::_putBooleanVolatile     : append_unsafe_put(callee, T_BOOLEAN, true); return;\n+  case vmIntrinsics::_putByteVolatile        : append_unsafe_put(callee, T_BYTE,    true); return;\n+  case vmIntrinsics::_putShortVolatile       : append_unsafe_put(callee, T_SHORT,   true); return;\n+  case vmIntrinsics::_putCharVolatile        : append_unsafe_put(callee, T_CHAR,    true); return;\n+  case vmIntrinsics::_putIntVolatile         : append_unsafe_put(callee, T_INT,     true); return;\n+  case vmIntrinsics::_putLongVolatile        : append_unsafe_put(callee, T_LONG,    true); return;\n+  case vmIntrinsics::_putFloatVolatile       : append_unsafe_put(callee, T_FLOAT,   true); return;\n+  case vmIntrinsics::_putDoubleVolatile      : append_unsafe_put(callee, T_DOUBLE,  true); return;\n@@ -3912,6 +3913,6 @@\n-  case vmIntrinsics::_getAndAddLong      : append_unsafe_get_and_set_obj(callee, true); return;\n-  case vmIntrinsics::_getAndSetInt       :\n-  case vmIntrinsics::_getAndSetLong      :\n-  case vmIntrinsics::_getAndSetReference : append_unsafe_get_and_set_obj(callee, false); return;\n-  case vmIntrinsics::_getCharStringU     : append_char_access(callee, false); return;\n-  case vmIntrinsics::_putCharStringU     : append_char_access(callee, true); return;\n+  case vmIntrinsics::_getAndAddLong          : append_unsafe_get_and_set(callee, true); return;\n+  case vmIntrinsics::_getAndSetInt           :\n+  case vmIntrinsics::_getAndSetLong          :\n+  case vmIntrinsics::_getAndSetReference     : append_unsafe_get_and_set(callee, false); return;\n+  case vmIntrinsics::_getCharStringU         : append_char_access(callee, false); return;\n+  case vmIntrinsics::_putCharStringU         : append_char_access(callee, true); return;\n@@ -4595,1 +4596,1 @@\n-void GraphBuilder::append_unsafe_get_obj(ciMethod* callee, BasicType t, bool is_volatile) {\n+void GraphBuilder::append_unsafe_get(ciMethod* callee, BasicType t, bool is_volatile) {\n@@ -4602,1 +4603,1 @@\n-  Instruction* op = append(new UnsafeGetObject(t, args->at(1), offset, is_volatile));\n+  Instruction* op = append(new UnsafeGet(t, args->at(1), offset, is_volatile));\n@@ -4608,1 +4609,1 @@\n-void GraphBuilder::append_unsafe_put_obj(ciMethod* callee, BasicType t, bool is_volatile) {\n+void GraphBuilder::append_unsafe_put(ciMethod* callee, BasicType t, bool is_volatile) {\n@@ -4620,1 +4621,1 @@\n-  Instruction* op = append(new UnsafePutObject(t, args->at(1), offset, val, is_volatile));\n+  Instruction* op = append(new UnsafePut(t, args->at(1), offset, val, is_volatile));\n@@ -4625,18 +4626,0 @@\n-\n-void GraphBuilder::append_unsafe_get_raw(ciMethod* callee, BasicType t) {\n-  Values* args = state()->pop_arguments(callee->arg_size());\n-  null_check(args->at(0));\n-  Instruction* op = append(new UnsafeGetRaw(t, args->at(1), false));\n-  push(op->type(), op);\n-  compilation()->set_has_unsafe_access(true);\n-}\n-\n-\n-void GraphBuilder::append_unsafe_put_raw(ciMethod* callee, BasicType t) {\n-  Values* args = state()->pop_arguments(callee->arg_size());\n-  null_check(args->at(0));\n-  Instruction* op = append(new UnsafePutRaw(t, args->at(1), args->at(2)));\n-  compilation()->set_has_unsafe_access(true);\n-}\n-\n-\n@@ -4730,1 +4713,1 @@\n-void GraphBuilder::append_unsafe_get_and_set_obj(ciMethod* callee, bool is_add) {\n+void GraphBuilder::append_unsafe_get_and_set(ciMethod* callee, bool is_add) {\n@@ -4738,1 +4721,1 @@\n-  Instruction* op = append(new UnsafeGetAndSetObject(t, args->at(1), offset, args->at(3), is_add));\n+  Instruction* op = append(new UnsafeGetAndSet(t, args->at(1), offset, args->at(3), is_add));\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":62,"deletions":79,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -419,4 +419,2 @@\n-  void append_unsafe_get_obj(ciMethod* callee, BasicType t, bool is_volatile);\n-  void append_unsafe_put_obj(ciMethod* callee, BasicType t, bool is_volatile);\n-  void append_unsafe_get_raw(ciMethod* callee, BasicType t);\n-  void append_unsafe_put_raw(ciMethod* callee, BasicType t);\n+  void append_unsafe_get(ciMethod* callee, BasicType t, bool is_volatile);\n+  void append_unsafe_put(ciMethod* callee, BasicType t, bool is_volatile);\n@@ -424,1 +422,1 @@\n-  void append_unsafe_get_and_set_obj(ciMethod* callee, bool is_add);\n+  void append_unsafe_get_and_set(ciMethod* callee, bool is_add);\n@@ -435,2 +433,0 @@\n-  bool count_invocations()     { return _compilation->count_invocations();     }\n-  bool count_backedges()       { return _compilation->count_backedges();       }\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.hpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -100,7 +100,3 @@\n-class     UnsafeRawOp;\n-class       UnsafeGetRaw;\n-class       UnsafePutRaw;\n-class     UnsafeObjectOp;\n-class       UnsafeGetObject;\n-class       UnsafePutObject;\n-class         UnsafeGetAndSetObject;\n+class     UnsafeGet;\n+class     UnsafePut;\n+class     UnsafeGetAndSet;\n@@ -203,5 +199,3 @@\n-  virtual void do_UnsafeGetRaw   (UnsafeGetRaw*    x) = 0;\n-  virtual void do_UnsafePutRaw   (UnsafePutRaw*    x) = 0;\n-  virtual void do_UnsafeGetObject(UnsafeGetObject* x) = 0;\n-  virtual void do_UnsafePutObject(UnsafePutObject* x) = 0;\n-  virtual void do_UnsafeGetAndSetObject(UnsafeGetAndSetObject* x) = 0;\n+  virtual void do_UnsafeGet      (UnsafeGet*       x) = 0;\n+  virtual void do_UnsafePut      (UnsafePut*       x) = 0;\n+  virtual void do_UnsafeGetAndSet(UnsafeGetAndSet* x) = 0;\n@@ -2340,1 +2334,4 @@\n-  BasicType _basic_type;    \/\/ ValueType can not express byte-sized integers\n+  Value _object;                                 \/\/ Object to be fetched from or mutated\n+  Value _offset;                                 \/\/ Offset within object\n+  bool  _is_volatile;                            \/\/ true if volatile - dl\/JSR166\n+  BasicType _basic_type;                         \/\/ ValueType can not express byte-sized integers\n@@ -2344,3 +2341,3 @@\n-  UnsafeOp(BasicType basic_type, bool is_put)\n-  : Instruction(is_put ? voidType : as_ValueType(basic_type))\n-  , _basic_type(basic_type)\n+  UnsafeOp(BasicType basic_type, Value object, Value offset, bool is_put, bool is_volatile)\n+    : Instruction(is_put ? voidType : as_ValueType(basic_type)),\n+    _object(object), _offset(offset), _is_volatile(is_volatile), _basic_type(basic_type)\n@@ -2357,0 +2354,3 @@\n+  Value object()                                 { return _object; }\n+  Value offset()                                 { return _offset; }\n+  bool  is_volatile()                            { return _is_volatile; }\n@@ -2359,47 +2359,2 @@\n-  virtual void input_values_do(ValueVisitor* f)   { }\n-};\n-\n-\n-BASE(UnsafeRawOp, UnsafeOp)\n- private:\n-  Value _base;                                   \/\/ Base address (a Java long)\n-  Value _index;                                  \/\/ Index if computed by optimizer; initialized to NULL\n-  int   _log2_scale;                             \/\/ Scale factor: 0, 1, 2, or 3.\n-                                                 \/\/ Indicates log2 of number of bytes (1, 2, 4, or 8)\n-                                                 \/\/ to scale index by.\n-\n- protected:\n-  UnsafeRawOp(BasicType basic_type, Value addr, bool is_put)\n-  : UnsafeOp(basic_type, is_put)\n-  , _base(addr)\n-  , _index(NULL)\n-  , _log2_scale(0)\n-  {\n-    \/\/ Can not use ASSERT_VALUES because index may be NULL\n-    assert(addr != NULL && addr->type()->is_long(), \"just checking\");\n-  }\n-\n-  UnsafeRawOp(BasicType basic_type, Value base, Value index, int log2_scale, bool is_put)\n-  : UnsafeOp(basic_type, is_put)\n-  , _base(base)\n-  , _index(index)\n-  , _log2_scale(log2_scale)\n-  {\n-  }\n-\n- public:\n-  \/\/ accessors\n-  Value base()                                   { return _base; }\n-  Value index()                                  { return _index; }\n-  bool  has_index()                              { return (_index != NULL); }\n-  int   log2_scale()                             { return _log2_scale; }\n-\n-  \/\/ setters\n-  void set_base (Value base)                     { _base  = base; }\n-  void set_index(Value index)                    { _index = index; }\n-  void set_log2_scale(int log2_scale)            { _log2_scale = log2_scale; }\n-\n-  \/\/ generic\n-  virtual void input_values_do(ValueVisitor* f)   { UnsafeOp::input_values_do(f);\n-                                                   f->visit(&_base);\n-                                                   if (has_index()) f->visit(&_index); }\n+  virtual void input_values_do(ValueVisitor* f)   { f->visit(&_object);\n+                                                    f->visit(&_offset); }\n@@ -2408,2 +2363,1 @@\n-\n-LEAF(UnsafeGetRaw, UnsafeRawOp)\n+LEAF(UnsafeGet, UnsafeOp)\n@@ -2411,2 +2365,1 @@\n- bool _may_be_unaligned, _is_wide;  \/\/ For OSREntry\n-\n+  bool _is_raw;\n@@ -2414,25 +2367,2 @@\n- UnsafeGetRaw(BasicType basic_type, Value addr, bool may_be_unaligned, bool is_wide = false)\n-  : UnsafeRawOp(basic_type, addr, false) {\n-    _may_be_unaligned = may_be_unaligned;\n-    _is_wide = is_wide;\n-  }\n-\n- UnsafeGetRaw(BasicType basic_type, Value base, Value index, int log2_scale, bool may_be_unaligned, bool is_wide = false)\n-  : UnsafeRawOp(basic_type, base, index, log2_scale, false) {\n-    _may_be_unaligned = may_be_unaligned;\n-    _is_wide = is_wide;\n-  }\n-\n-  bool may_be_unaligned()                         { return _may_be_unaligned; }\n-  bool is_wide()                                  { return _is_wide; }\n-};\n-\n-\n-LEAF(UnsafePutRaw, UnsafeRawOp)\n- private:\n-  Value _value;                                  \/\/ Value to be stored\n-\n- public:\n-  UnsafePutRaw(BasicType basic_type, Value addr, Value value)\n-  : UnsafeRawOp(basic_type, addr, true)\n-  , _value(value)\n+  UnsafeGet(BasicType basic_type, Value object, Value offset, bool is_volatile)\n+  : UnsafeOp(basic_type, object, offset, false, is_volatile)\n@@ -2440,1 +2370,1 @@\n-    assert(value != NULL, \"just checking\");\n+    _is_raw = false;\n@@ -2443,4 +2373,2 @@\n-\n-  UnsafePutRaw(BasicType basic_type, Value base, Value index, int log2_scale, Value value)\n-  : UnsafeRawOp(basic_type, base, index, log2_scale, true)\n-  , _value(value)\n+  UnsafeGet(BasicType basic_type, Value object, Value offset, bool is_volatile, bool is_raw)\n+  : UnsafeOp(basic_type, object, offset, false, is_volatile), _is_raw(is_raw)\n@@ -2448,1 +2376,0 @@\n-    assert(value != NULL, \"just checking\");\n@@ -2453,5 +2380,1 @@\n-  Value value()                                  { return _value; }\n-\n-  \/\/ generic\n-  virtual void input_values_do(ValueVisitor* f)   { UnsafeRawOp::input_values_do(f);\n-                                                   f->visit(&_value); }\n+  bool is_raw()                             { return _is_raw; }\n@@ -2461,33 +2384,1 @@\n-BASE(UnsafeObjectOp, UnsafeOp)\n- private:\n-  Value _object;                                 \/\/ Object to be fetched from or mutated\n-  Value _offset;                                 \/\/ Offset within object\n-  bool  _is_volatile;                            \/\/ true if volatile - dl\/JSR166\n- public:\n-  UnsafeObjectOp(BasicType basic_type, Value object, Value offset, bool is_put, bool is_volatile)\n-    : UnsafeOp(basic_type, is_put), _object(object), _offset(offset), _is_volatile(is_volatile)\n-  {\n-  }\n-\n-  \/\/ accessors\n-  Value object()                                 { return _object; }\n-  Value offset()                                 { return _offset; }\n-  bool  is_volatile()                            { return _is_volatile; }\n-  \/\/ generic\n-  virtual void input_values_do(ValueVisitor* f)   { UnsafeOp::input_values_do(f);\n-                                                   f->visit(&_object);\n-                                                   f->visit(&_offset); }\n-};\n-\n-\n-LEAF(UnsafeGetObject, UnsafeObjectOp)\n- public:\n-  UnsafeGetObject(BasicType basic_type, Value object, Value offset, bool is_volatile)\n-  : UnsafeObjectOp(basic_type, object, offset, false, is_volatile)\n-  {\n-    ASSERT_VALUES\n-  }\n-};\n-\n-\n-LEAF(UnsafePutObject, UnsafeObjectOp)\n+LEAF(UnsafePut, UnsafeOp)\n@@ -2497,2 +2388,2 @@\n-  UnsafePutObject(BasicType basic_type, Value object, Value offset, Value value, bool is_volatile)\n-  : UnsafeObjectOp(basic_type, object, offset, true, is_volatile)\n+  UnsafePut(BasicType basic_type, Value object, Value offset, Value value, bool is_volatile)\n+  : UnsafeOp(basic_type, object, offset, true, is_volatile)\n@@ -2508,1 +2399,1 @@\n-  virtual void input_values_do(ValueVisitor* f)   { UnsafeObjectOp::input_values_do(f);\n+  virtual void input_values_do(ValueVisitor* f)   { UnsafeOp::input_values_do(f);\n@@ -2512,1 +2403,1 @@\n-LEAF(UnsafeGetAndSetObject, UnsafeObjectOp)\n+LEAF(UnsafeGetAndSet, UnsafeOp)\n@@ -2517,2 +2408,2 @@\n-  UnsafeGetAndSetObject(BasicType basic_type, Value object, Value offset, Value value, bool is_add)\n-  : UnsafeObjectOp(basic_type, object, offset, false, false)\n+  UnsafeGetAndSet(BasicType basic_type, Value object, Value offset, Value value, bool is_add)\n+  : UnsafeOp(basic_type, object, offset, false, false)\n@@ -2530,1 +2421,1 @@\n-  virtual void input_values_do(ValueVisitor* f)   { UnsafeObjectOp::input_values_do(f);\n+  virtual void input_values_do(ValueVisitor* f)   { UnsafeOp::input_values_do(f);\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.hpp","additions":34,"deletions":143,"binary":false,"changes":177,"status":"modified"},{"patch":"@@ -267,16 +267,0 @@\n-  output()->print(\".(\");\n-}\n-\n-void InstructionPrinter::print_unsafe_raw_op(UnsafeRawOp* op, const char* name) {\n-  print_unsafe_op(op, name);\n-  output()->print(\"base \");\n-  print_value(op->base());\n-  if (op->has_index()) {\n-    output()->print(\", index \"); print_value(op->index());\n-    output()->print(\", log2_scale %d\", op->log2_scale());\n-  }\n-}\n-\n-\n-void InstructionPrinter::print_unsafe_object_op(UnsafeObjectOp* op, const char* name) {\n-  print_unsafe_op(op, name);\n@@ -827,11 +811,2 @@\n-\n-void InstructionPrinter::do_UnsafeGetRaw(UnsafeGetRaw* x) {\n-  print_unsafe_raw_op(x, \"UnsafeGetRaw\");\n-  output()->put(')');\n-}\n-\n-\n-void InstructionPrinter::do_UnsafePutRaw(UnsafePutRaw* x) {\n-  print_unsafe_raw_op(x, \"UnsafePutRaw\");\n-  output()->print(\", value \");\n-  print_value(x->value());\n+void InstructionPrinter::do_UnsafeGet(UnsafeGet* x) {\n+  print_unsafe_op(x, x->is_raw() ? \"UnsafeGet (raw)\" : \"UnsafeGet\");\n@@ -841,9 +816,2 @@\n-\n-void InstructionPrinter::do_UnsafeGetObject(UnsafeGetObject* x) {\n-  print_unsafe_object_op(x, \"UnsafeGetObject\");\n-  output()->put(')');\n-}\n-\n-\n-void InstructionPrinter::do_UnsafePutObject(UnsafePutObject* x) {\n-  print_unsafe_object_op(x, \"UnsafePutObject\");\n+void InstructionPrinter::do_UnsafePut(UnsafePut* x) {\n+  print_unsafe_op(x, \"UnsafePut\");\n@@ -855,2 +823,2 @@\n-void InstructionPrinter::do_UnsafeGetAndSetObject(UnsafeGetAndSetObject* x) {\n-  print_unsafe_object_op(x, x->is_add()?\"UnsafeGetAndSetObject (add)\":\"UnsafeGetAndSetObject\");\n+void InstructionPrinter::do_UnsafeGetAndSet(UnsafeGetAndSet* x) {\n+  print_unsafe_op(x, x->is_add()?\"UnsafeGetAndSet (add)\":\"UnsafeGetAndSet\");\n","filename":"src\/hotspot\/share\/c1\/c1_InstructionPrinter.cpp","additions":6,"deletions":38,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -77,2 +77,0 @@\n-  void print_unsafe_raw_op(UnsafeRawOp* op, const char* name);\n-  void print_unsafe_object_op(UnsafeObjectOp* op, const char* name);\n@@ -128,5 +126,3 @@\n-  virtual void do_UnsafeGetRaw   (UnsafeGetRaw*    x);\n-  virtual void do_UnsafePutRaw   (UnsafePutRaw*    x);\n-  virtual void do_UnsafeGetObject(UnsafeGetObject* x);\n-  virtual void do_UnsafePutObject(UnsafePutObject* x);\n-  virtual void do_UnsafeGetAndSetObject(UnsafeGetAndSetObject* x);\n+  virtual void do_UnsafeGet      (UnsafeGet*       x);\n+  virtual void do_UnsafePut      (UnsafePut*       x);\n+  virtual void do_UnsafeGetAndSet(UnsafeGetAndSet* x);\n","filename":"src\/hotspot\/share\/c1\/c1_InstructionPrinter.hpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -442,1 +442,0 @@\n-    case lir_backwardbranch_target:    \/\/ result and info always invalid\n@@ -1814,1 +1813,0 @@\n-     case lir_backwardbranch_target: s = \"backbranch\";    break;\n@@ -1967,2 +1965,0 @@\n-    case lir_move_unaligned:\n-      return \"unaligned move\";\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -899,1 +899,0 @@\n-      , lir_backwardbranch_target\n@@ -1041,1 +1040,0 @@\n-  lir_move_unaligned,\n@@ -2217,3 +2215,0 @@\n-  void unaligned_move(LIR_Address* src, LIR_Opr dst) { append(new LIR_Op1(lir_move, LIR_OprFact::address(src), dst, dst->type(), lir_patch_none, NULL, lir_move_unaligned)); }\n-  void unaligned_move(LIR_Opr src, LIR_Address* dst) { append(new LIR_Op1(lir_move, src, LIR_OprFact::address(dst), src->type(), lir_patch_none, NULL, lir_move_unaligned)); }\n-  void unaligned_move(LIR_Opr src, LIR_Opr dst) { append(new LIR_Op1(lir_move, src, dst, dst->type(), lir_patch_none, NULL, lir_move_unaligned)); }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -521,1 +521,0 @@\n-                op->move_kind() == lir_move_unaligned,\n@@ -911,1 +910,1 @@\n-void LIR_Assembler::move_op(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool unaligned, bool wide) {\n+void LIR_Assembler::move_op(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide) {\n@@ -920,1 +919,1 @@\n-      reg2mem(src, dest, type, patch_code, info, pop_fpu_stack, wide, unaligned);\n+      reg2mem(src, dest, type, patch_code, info, pop_fpu_stack, wide);\n@@ -949,2 +948,1 @@\n-    mem2reg(src, dest, type, patch_code, info, wide, unaligned);\n-\n+    mem2reg(src, dest, type, patch_code, info, wide);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -178,1 +178,1 @@\n-                   bool pop_fpu_stack, bool wide, bool unaligned);\n+                   bool pop_fpu_stack, bool wide);\n@@ -183,1 +183,1 @@\n-                   CodeEmitInfo* info, bool wide, bool unaligned);\n+                   CodeEmitInfo* info, bool wide);\n@@ -230,1 +230,1 @@\n-               LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool unaligned, bool wide);\n+               LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2478,182 +2478,1 @@\n-\/\/ Here UnsafeGetRaw may have x->base() and x->index() be int or long\n-\/\/ on both 64 and 32 bits. Expecting x->base() to be always long on 64bit.\n-void LIRGenerator::do_UnsafeGetRaw(UnsafeGetRaw* x) {\n-  LIRItem base(x->base(), this);\n-  LIRItem idx(this);\n-  base.load_item();\n-  if (x->has_index()) {\n-    idx.set_instruction(x->index());\n-    idx.load_nonconstant();\n-  }\n-\n-  LIR_Opr reg = rlock_result(x, x->basic_type());\n-\n-  int   log2_scale = 0;\n-  if (x->has_index()) {\n-    log2_scale = x->log2_scale();\n-  }\n-\n-  assert(!x->has_index() || idx.value() == x->index(), \"should match\");\n-\n-  LIR_Opr base_op = base.result();\n-  LIR_Opr index_op = idx.result();\n-#ifndef _LP64\n-  if (base_op->type() == T_LONG) {\n-    base_op = new_register(T_INT);\n-    __ convert(Bytecodes::_l2i, base.result(), base_op);\n-  }\n-  if (x->has_index()) {\n-    if (index_op->type() == T_LONG) {\n-      LIR_Opr long_index_op = index_op;\n-      if (index_op->is_constant()) {\n-        long_index_op = new_register(T_LONG);\n-        __ move(index_op, long_index_op);\n-      }\n-      index_op = new_register(T_INT);\n-      __ convert(Bytecodes::_l2i, long_index_op, index_op);\n-    } else {\n-      assert(x->index()->type()->tag() == intTag, \"must be\");\n-    }\n-  }\n-  \/\/ At this point base and index should be all ints.\n-  assert(base_op->type() == T_INT && !base_op->is_constant(), \"base should be an non-constant int\");\n-  assert(!x->has_index() || index_op->type() == T_INT, \"index should be an int\");\n-#else\n-  if (x->has_index()) {\n-    if (index_op->type() == T_INT) {\n-      if (!index_op->is_constant()) {\n-        index_op = new_register(T_LONG);\n-        __ convert(Bytecodes::_i2l, idx.result(), index_op);\n-      }\n-    } else {\n-      assert(index_op->type() == T_LONG, \"must be\");\n-      if (index_op->is_constant()) {\n-        index_op = new_register(T_LONG);\n-        __ move(idx.result(), index_op);\n-      }\n-    }\n-  }\n-  \/\/ At this point base is a long non-constant\n-  \/\/ Index is a long register or a int constant.\n-  \/\/ We allow the constant to stay an int because that would allow us a more compact encoding by\n-  \/\/ embedding an immediate offset in the address expression. If we have a long constant, we have to\n-  \/\/ move it into a register first.\n-  assert(base_op->type() == T_LONG && !base_op->is_constant(), \"base must be a long non-constant\");\n-  assert(!x->has_index() || (index_op->type() == T_INT && index_op->is_constant()) ||\n-                            (index_op->type() == T_LONG && !index_op->is_constant()), \"unexpected index type\");\n-#endif\n-\n-  BasicType dst_type = x->basic_type();\n-\n-  LIR_Address* addr;\n-  if (index_op->is_constant()) {\n-    assert(log2_scale == 0, \"must not have a scale\");\n-    assert(index_op->type() == T_INT, \"only int constants supported\");\n-    addr = new LIR_Address(base_op, index_op->as_jint(), dst_type);\n-  } else {\n-#ifdef X86\n-    addr = new LIR_Address(base_op, index_op, LIR_Address::Scale(log2_scale), 0, dst_type);\n-#elif defined(GENERATE_ADDRESS_IS_PREFERRED)\n-    addr = generate_address(base_op, index_op, log2_scale, 0, dst_type);\n-#else\n-    if (index_op->is_illegal() || log2_scale == 0) {\n-      addr = new LIR_Address(base_op, index_op, dst_type);\n-    } else {\n-      LIR_Opr tmp = new_pointer_register();\n-      __ shift_left(index_op, log2_scale, tmp);\n-      addr = new LIR_Address(base_op, tmp, dst_type);\n-    }\n-#endif\n-  }\n-\n-  if (x->may_be_unaligned() && (dst_type == T_LONG || dst_type == T_DOUBLE)) {\n-    __ unaligned_move(addr, reg);\n-  } else {\n-    if (dst_type == T_OBJECT && x->is_wide()) {\n-      __ move_wide(addr, reg);\n-    } else {\n-      __ move(addr, reg);\n-    }\n-  }\n-}\n-\n-\n-void LIRGenerator::do_UnsafePutRaw(UnsafePutRaw* x) {\n-  int  log2_scale = 0;\n-  BasicType type = x->basic_type();\n-\n-  if (x->has_index()) {\n-    log2_scale = x->log2_scale();\n-  }\n-\n-  LIRItem base(x->base(), this);\n-  LIRItem value(x->value(), this);\n-  LIRItem idx(this);\n-\n-  base.load_item();\n-  if (x->has_index()) {\n-    idx.set_instruction(x->index());\n-    idx.load_item();\n-  }\n-\n-  if (type == T_BYTE || type == T_BOOLEAN) {\n-    value.load_byte_item();\n-  } else {\n-    value.load_item();\n-  }\n-\n-  set_no_result(x);\n-\n-  LIR_Opr base_op = base.result();\n-  LIR_Opr index_op = idx.result();\n-\n-#ifdef GENERATE_ADDRESS_IS_PREFERRED\n-  LIR_Address* addr = generate_address(base_op, index_op, log2_scale, 0, x->basic_type());\n-#else\n-#ifndef _LP64\n-  if (base_op->type() == T_LONG) {\n-    base_op = new_register(T_INT);\n-    __ convert(Bytecodes::_l2i, base.result(), base_op);\n-  }\n-  if (x->has_index()) {\n-    if (index_op->type() == T_LONG) {\n-      index_op = new_register(T_INT);\n-      __ convert(Bytecodes::_l2i, idx.result(), index_op);\n-    }\n-  }\n-  \/\/ At this point base and index should be all ints and not constants\n-  assert(base_op->type() == T_INT && !base_op->is_constant(), \"base should be an non-constant int\");\n-  assert(!x->has_index() || (index_op->type() == T_INT && !index_op->is_constant()), \"index should be an non-constant int\");\n-#else\n-  if (x->has_index()) {\n-    if (index_op->type() == T_INT) {\n-      index_op = new_register(T_LONG);\n-      __ convert(Bytecodes::_i2l, idx.result(), index_op);\n-    }\n-  }\n-  \/\/ At this point base and index are long and non-constant\n-  assert(base_op->type() == T_LONG && !base_op->is_constant(), \"base must be a non-constant long\");\n-  assert(!x->has_index() || (index_op->type() == T_LONG && !index_op->is_constant()), \"index must be a non-constant long\");\n-#endif\n-\n-  if (log2_scale != 0) {\n-    \/\/ temporary fix (platform dependent code without shift on Intel would be better)\n-    \/\/ TODO: ARM also allows embedded shift in the address\n-    LIR_Opr tmp = new_pointer_register();\n-    if (TwoOperandLIRForm) {\n-      __ move(index_op, tmp);\n-      index_op = tmp;\n-    }\n-    __ shift_left(index_op, log2_scale, tmp);\n-    if (!TwoOperandLIRForm) {\n-      index_op = tmp;\n-    }\n-  }\n-\n-  LIR_Address* addr = new LIR_Address(base_op, index_op, x->basic_type());\n-#endif \/\/ !GENERATE_ADDRESS_IS_PREFERRED\n-  __ move(value.result(), addr);\n-}\n-\n-\n-void LIRGenerator::do_UnsafeGetObject(UnsafeGetObject* x) {\n+void LIRGenerator::do_UnsafeGet(UnsafeGet* x) {\n@@ -2681,2 +2500,18 @@\n-  access_load_at(decorators, type,\n-                 src, off.result(), result);\n+  if (!x->is_raw()) {\n+    access_load_at(decorators, type, src, off.result(), result);\n+  } else {\n+    \/\/ Currently it is only used in GraphBuilder::setup_osr_entry_block.\n+    \/\/ It reads the value from [src + offset] directly.\n+#ifdef _LP64\n+    LIR_Opr offset = new_register(T_LONG);\n+    __ convert(Bytecodes::_i2l, off.result(), offset);\n+#else\n+    LIR_Opr offset = off.result();\n+#endif\n+    LIR_Address* addr = new LIR_Address(src.result(), offset, type);\n+    if (type == T_LONG || type == T_DOUBLE) {\n+      __ move(addr, result);\n+    } else {\n+      access_load(IN_NATIVE, type, LIR_OprFact::address(addr), result);\n+    }\n+  }\n@@ -2686,1 +2521,1 @@\n-void LIRGenerator::do_UnsafePutObject(UnsafePutObject* x) {\n+void LIRGenerator::do_UnsafePut(UnsafePut* x) {\n@@ -2712,1 +2547,1 @@\n-void LIRGenerator::do_UnsafeGetAndSetObject(UnsafeGetAndSetObject* x) {\n+void LIRGenerator::do_UnsafeGetAndSet(UnsafeGetAndSet* x) {\n@@ -3294,5 +3129,1 @@\n-      if (addr->type() == T_LONG || addr->type() == T_DOUBLE) {\n-        __ unaligned_move(param->result(), addr);\n-      } else {\n-        __ move(param->result(), addr);\n-      }\n+      __ move(param->result(), addr);\n@@ -3922,1 +3753,1 @@\n-  if (compilation()->count_backedges()) {\n+  if (compilation()->is_profiling()) {\n@@ -4215,5 +4046,1 @@\n-      if (addr->type() == T_LONG || addr->type() == T_DOUBLE) {\n-        __ unaligned_move(arg, addr);\n-      } else {\n-        __ move(arg, addr);\n-      }\n+      __ move(arg, addr);\n@@ -4257,5 +4084,1 @@\n-      if (addr->type() == T_LONG || addr->type() == T_DOUBLE) {\n-        __ unaligned_move(arg->result(), addr);\n-      } else {\n-        __ move(arg->result(), addr);\n-      }\n+      __ move(arg->result(), addr);\n@@ -4310,7 +4133,0 @@\n-\n-LIR_Opr LIRGenerator::maybe_mask_boolean(StoreIndexed* x, LIR_Opr array, LIR_Opr value, CodeEmitInfo*& null_check_info) {\n-  if (x->check_boolean()) {\n-    value = mask_boolean(array, value, null_check_info);\n-  }\n-  return value;\n-}\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":25,"deletions":209,"binary":false,"changes":234,"status":"modified"},{"patch":"@@ -417,1 +417,1 @@\n-    if (compilation()->count_invocations()) {\n+    if (compilation()->is_profiling()) {\n@@ -422,1 +422,1 @@\n-    if (compilation()->count_backedges()) {\n+    if (compilation()->is_profiling()) {\n@@ -428,1 +428,1 @@\n-    if (compilation()->count_backedges()) {\n+    if (compilation()->is_profiling()) {\n@@ -448,3 +448,0 @@\n-  void bind_block_entry(BlockBegin* block);\n-  void start_block(BlockBegin* block);\n-\n@@ -489,1 +486,0 @@\n-  void do_ClassIDIntrinsic(Intrinsic* x);\n@@ -507,1 +503,0 @@\n-  LIR_Opr maybe_mask_boolean(StoreIndexed* x, LIR_Opr array, LIR_Opr value, CodeEmitInfo*& null_check_info);\n@@ -609,5 +604,3 @@\n-  virtual void do_UnsafeGetRaw   (UnsafeGetRaw*    x);\n-  virtual void do_UnsafePutRaw   (UnsafePutRaw*    x);\n-  virtual void do_UnsafeGetObject(UnsafeGetObject* x);\n-  virtual void do_UnsafePutObject(UnsafePutObject* x);\n-  virtual void do_UnsafeGetAndSetObject(UnsafeGetAndSetObject* x);\n+  virtual void do_UnsafeGet      (UnsafeGet*       x);\n+  virtual void do_UnsafePut      (UnsafePut*       x);\n+  virtual void do_UnsafeGetAndSet(UnsafeGetAndSet* x);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":6,"deletions":13,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"c1\/c1_ValueSet.inline.hpp\"\n+#include \"c1\/c1_ValueSet.hpp\"\n@@ -537,5 +537,3 @@\n-  void do_UnsafeGetRaw   (UnsafeGetRaw*    x);\n-  void do_UnsafePutRaw   (UnsafePutRaw*    x);\n-  void do_UnsafeGetObject(UnsafeGetObject* x);\n-  void do_UnsafePutObject(UnsafePutObject* x);\n-  void do_UnsafeGetAndSetObject(UnsafeGetAndSetObject* x);\n+  void do_UnsafeGet      (UnsafeGet*       x);\n+  void do_UnsafePut      (UnsafePut*       x);\n+  void do_UnsafeGetAndSet(UnsafeGetAndSet* x);\n@@ -727,5 +725,3 @@\n-void NullCheckVisitor::do_UnsafeGetRaw   (UnsafeGetRaw*    x) {}\n-void NullCheckVisitor::do_UnsafePutRaw   (UnsafePutRaw*    x) {}\n-void NullCheckVisitor::do_UnsafeGetObject(UnsafeGetObject* x) {}\n-void NullCheckVisitor::do_UnsafePutObject(UnsafePutObject* x) {}\n-void NullCheckVisitor::do_UnsafeGetAndSetObject(UnsafeGetAndSetObject* x) {}\n+void NullCheckVisitor::do_UnsafeGet      (UnsafeGet*       x) {}\n+void NullCheckVisitor::do_UnsafePut      (UnsafePut*       x) {}\n+void NullCheckVisitor::do_UnsafeGetAndSet(UnsafeGetAndSet* x) {}\n","filename":"src\/hotspot\/share\/c1\/c1_Optimizer.cpp","additions":7,"deletions":11,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -133,2 +133,0 @@\n-    void do_UnsafePutRaw   (UnsafePutRaw*    x) { \/* nothing to do *\/ };\n-    void do_UnsafePutObject(UnsafePutObject* x) { \/* nothing to do *\/ };\n@@ -165,3 +163,3 @@\n-    void do_UnsafeGetRaw   (UnsafeGetRaw*    x) { \/* nothing to do *\/ };\n-    void do_UnsafeGetObject(UnsafeGetObject* x) { \/* nothing to do *\/ };\n-    void do_UnsafeGetAndSetObject(UnsafeGetAndSetObject* x) { \/* nothing to do *\/ };\n+    void do_UnsafePut      (UnsafePut*       x) { \/* nothing to do *\/ };\n+    void do_UnsafeGet      (UnsafeGet*       x) { \/* nothing to do *\/ };\n+    void do_UnsafeGetAndSet(UnsafeGetAndSet* x) { \/* nothing to do *\/ };\n","filename":"src\/hotspot\/share\/c1\/c1_RangeCheckElimination.hpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -64,1 +64,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -359,0 +358,5 @@\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    Runtime1::_new_instance_slowcase_cnt++;\n+  }\n+#endif\n@@ -371,1 +375,0 @@\n-  NOT_PRODUCT(_new_instance_slowcase_cnt++;)\n@@ -377,1 +380,0 @@\n-  NOT_PRODUCT(_new_instance_slowcase_cnt++;)\n@@ -386,1 +388,5 @@\n-  NOT_PRODUCT(_new_type_array_slowcase_cnt++;)\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    _new_type_array_slowcase_cnt++;\n+  }\n+#endif\n@@ -404,2 +410,5 @@\n-  NOT_PRODUCT(_new_object_array_slowcase_cnt++;)\n-\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    _new_object_array_slowcase_cnt++;\n+  }\n+#endif\n@@ -445,2 +454,5 @@\n-  NOT_PRODUCT(_new_multi_array_slowcase_cnt++;)\n-\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    _new_multi_array_slowcase_cnt++;\n+  }\n+#endif\n@@ -779,1 +791,5 @@\n-  NOT_PRODUCT(_throw_range_check_exception_count++;)\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    _throw_range_check_exception_count++;\n+  }\n+#endif\n@@ -789,1 +805,5 @@\n-  NOT_PRODUCT(_throw_index_exception_count++;)\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    _throw_index_exception_count++;\n+  }\n+#endif\n@@ -797,1 +817,5 @@\n-  NOT_PRODUCT(_throw_div0_exception_count++;)\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    _throw_div0_exception_count++;\n+  }\n+#endif\n@@ -803,1 +827,5 @@\n-  NOT_PRODUCT(_throw_null_pointer_exception_count++;)\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    _throw_null_pointer_exception_count++;\n+  }\n+#endif\n@@ -809,1 +837,5 @@\n-  NOT_PRODUCT(_throw_class_cast_exception_count++;)\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    _throw_class_cast_exception_count++;\n+  }\n+#endif\n@@ -817,1 +849,5 @@\n-  NOT_PRODUCT(_throw_incompatible_class_change_error_count++;)\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    _throw_incompatible_class_change_error_count++;\n+  }\n+#endif\n@@ -830,1 +866,5 @@\n-  NOT_PRODUCT(_monitorenter_slowcase_cnt++;)\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    _monitorenter_slowcase_cnt++;\n+  }\n+#endif\n@@ -840,1 +880,5 @@\n-  NOT_PRODUCT(_monitorexit_slowcase_cnt++;)\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    _monitorexit_slowcase_cnt++;\n+  }\n+#endif\n@@ -992,1 +1036,5 @@\n-  NOT_PRODUCT(_patch_code_slowcase_cnt++;)\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    _patch_code_slowcase_cnt++;\n+  }\n+#endif\n@@ -1397,1 +1445,5 @@\n-  NOT_PRODUCT(_patch_code_slowcase_cnt++);\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    _patch_code_slowcase_cnt++;\n+  }\n+#endif\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":70,"deletions":18,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"c1\/c1_ValueSet.inline.hpp\"\n+#include \"c1\/c1_ValueSet.hpp\"\n","filename":"src\/hotspot\/share\/c1\/c1_ValueMap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -160,5 +160,3 @@\n-  void do_UnsafePutRaw   (UnsafePutRaw*    x) { kill_memory(); }\n-  void do_UnsafePutObject(UnsafePutObject* x) { kill_memory(); }\n-  void do_UnsafeGetAndSetObject(UnsafeGetAndSetObject* x) { kill_memory(); }\n-  void do_UnsafeGetRaw   (UnsafeGetRaw*    x) { \/* nothing to do *\/ }\n-  void do_UnsafeGetObject(UnsafeGetObject* x) {\n+  void do_UnsafePut      (UnsafePut*       x) { kill_memory(); }\n+  void do_UnsafeGetAndSet(UnsafeGetAndSet* x) { kill_memory(); }\n+  void do_UnsafeGet      (UnsafeGet*       x) {\n","filename":"src\/hotspot\/share\/c1\/c1_ValueMap.hpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -83,6 +83,0 @@\n-ValueType* ValueType::join(ValueType* y) const {\n-  Unimplemented();\n-  return NULL;\n-}\n-\n-\n","filename":"src\/hotspot\/share\/c1\/c1_ValueType.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -200,3 +200,0 @@\n-  develop(bool, LIRTracePeephole, false,                                    \\\n-          \"Trace peephole optimizer\")                                       \\\n-                                                                            \\\n@@ -297,6 +294,0 @@\n-  develop(bool, OptimizeUnsafes, true,                                      \\\n-          \"Optimize raw unsafe ops\")                                        \\\n-                                                                            \\\n-  develop(bool, PrintUnsafeOptimization, false,                             \\\n-          \"Print optimization of raw unsafe ops\")                           \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/c1\/c1_globals.hpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -163,1 +163,1 @@\n-  _src_obj_table(INITIAL_TABLE_SIZE),\n+  _src_obj_table(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE),\n@@ -193,0 +193,3 @@\n+  if (_shared_rs.is_reserved()) {\n+    _shared_rs.release();\n+  }\n@@ -231,1 +234,1 @@\n-    \/\/ See RunTimeSharedClassInfo::get_for()\n+    \/\/ See RunTimeClassInfo::get_for()\n@@ -322,1 +325,1 @@\n-  \/\/ size of the symbol table and two dictionaries, plus the RunTimeSharedClassInfo's\n+  \/\/ size of the symbol table and two dictionaries, plus the RunTimeClassInfo's\n@@ -465,1 +468,1 @@\n-  SourceObjInfo* p = _src_obj_table.add_if_absent(src_obj, src_info, &created);\n+  SourceObjInfo* p = _src_obj_table.put_if_absent(src_obj, src_info, &created);\n@@ -467,1 +470,1 @@\n-    if (_src_obj_table.maybe_grow(MAX_TABLE_SIZE)) {\n+    if (_src_obj_table.maybe_grow()) {\n@@ -630,2 +633,2 @@\n-    \/\/ we can do a quick lookup from InstanceKlass* -> RunTimeSharedClassInfo*\n-    \/\/ without building another hashtable. See RunTimeSharedClassInfo::get_for()\n+    \/\/ we can do a quick lookup from InstanceKlass* -> RunTimeClassInfo*\n+    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\n@@ -657,1 +660,1 @@\n-  SourceObjInfo* p = _src_obj_table.lookup(src_obj);\n+  SourceObjInfo* p = _src_obj_table.get(src_obj);\n@@ -1091,1 +1094,1 @@\n-    _total_closed_heap_region_size = mapinfo->write_archive_heap_regions(\n+    _total_closed_heap_region_size = mapinfo->write_heap_regions(\n@@ -1094,3 +1097,3 @@\n-                                        MetaspaceShared::first_closed_archive_heap_region,\n-                                        MetaspaceShared::max_closed_archive_heap_region);\n-    _total_open_heap_region_size = mapinfo->write_archive_heap_regions(\n+                                        MetaspaceShared::first_closed_heap_region,\n+                                        MetaspaceShared::max_closed_heap_region);\n+    _total_open_heap_region_size = mapinfo->write_heap_regions(\n@@ -1099,2 +1102,2 @@\n-                                        MetaspaceShared::first_open_archive_heap_region,\n-                                        MetaspaceShared::max_open_archive_heap_region);\n+                                        MetaspaceShared::first_open_heap_region,\n+                                        MetaspaceShared::max_open_heap_region);\n@@ -1166,1 +1169,1 @@\n-void ArchiveBuilder::print_heap_region_stats(GrowableArray<MemRegion> *heap_mem,\n+void ArchiveBuilder::print_heap_region_stats(GrowableArray<MemRegion>* regions,\n@@ -1168,1 +1171,1 @@\n-  int arr_len = heap_mem == NULL ? 0 : heap_mem->length();\n+  int arr_len = regions == NULL ? 0 : regions->length();\n@@ -1170,2 +1173,2 @@\n-      char* start = (char*)heap_mem->at(i).start();\n-      size_t size = heap_mem->at(i).byte_size();\n+      char* start = (char*)regions->at(i).start();\n+      size_t size = regions->at(i).byte_size();\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":21,"deletions":18,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"utilities\/hashtable.hpp\"\n+#include \"utilities\/resizeableResourceHash.hpp\"\n@@ -187,2 +187,2 @@\n-    bool do_entry(address key, const SourceObjInfo* value) {\n-      delete value->ref();\n+    bool do_entry(address key, const SourceObjInfo& value) {\n+      delete value.ref();\n@@ -207,1 +207,1 @@\n-  KVHashtable<address, SourceObjInfo, mtClassShared> _src_obj_table;\n+  ResizeableResourceHashtable<address, SourceObjInfo, ResourceObj::C_HEAP, mtClassShared> _src_obj_table;\n@@ -224,1 +224,1 @@\n-  void print_heap_region_stats(GrowableArray<MemRegion> *heap_mem,\n+  void print_heap_region_stats(GrowableArray<MemRegion>* regions,\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -57,1 +57,1 @@\n-ClassListParser::ClassListParser(const char* file) : _id2klass_table(INITIAL_TABLE_SIZE) {\n+ClassListParser::ClassListParser(const char* file) : _id2klass_table(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE) {\n@@ -513,1 +513,1 @@\n-    bool added = SystemDictionaryShared::add_unregistered_class(THREAD, k);\n+    bool added = SystemDictionaryShared::add_unregistered_class_for_static_archive(THREAD, k);\n@@ -559,1 +559,1 @@\n-bool ClassListParser::is_matching_cp_entry(constantPoolHandle &pool, int cp_index, TRAPS) {\n+bool ClassListParser::is_matching_cp_entry(const constantPoolHandle &pool, int cp_index, TRAPS) {\n@@ -695,2 +695,3 @@\n-    InstanceKlass** old_ptr = table()->lookup(id);\n-    if (old_ptr != NULL) {\n+    bool created;\n+    id2klass_table()->put_if_absent(id, ik, &created);\n+    if (!created) {\n@@ -699,1 +700,3 @@\n-    table()->add(id, ik);\n+    if (id2klass_table()->maybe_grow()) {\n+      log_info(cds, hashtables)(\"Expanded id2klass_table() to %d\", id2klass_table()->table_size());\n+    }\n@@ -710,1 +713,1 @@\n-  InstanceKlass** klass_ptr = table()->lookup(id);\n+  InstanceKlass** klass_ptr = id2klass_table()->get(id);\n","filename":"src\/hotspot\/share\/cds\/classListParser.cpp","additions":10,"deletions":7,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -70,3 +70,2 @@\n-bool HeapShared::_closed_archive_heap_region_mapped = false;\n-bool HeapShared::_open_archive_heap_region_mapped = false;\n-bool HeapShared::_archive_heap_region_fixed = false;\n+bool HeapShared::_closed_regions_mapped = false;\n+bool HeapShared::_open_regions_mapped = false;\n@@ -120,0 +119,8 @@\n+#ifdef ASSERT\n+bool HeapShared::is_archived_object_during_dumptime(oop p) {\n+  assert(HeapShared::is_heap_object_archiving_allowed(), \"must be\");\n+  assert(DumpSharedSpaces, \"this function is only used with -Xshare:dump\");\n+  return Universe::heap()->is_archived_object(p);\n+}\n+#endif\n+\n@@ -125,1 +132,1 @@\n-void HeapShared::fixup_mapped_heap_regions() {\n+void HeapShared::fixup_mapped_regions() {\n@@ -128,1 +135,0 @@\n-  set_archive_heap_region_fixed();\n@@ -140,2 +146,2 @@\n-  assert(!UseBiasedLocking || !p->mark().has_bias_pattern(),\n-         \"this object should never have been locked\");  \/\/ so identity_hash won't safepoin\n+  assert(!EnableValhalla || !p->mark().is_inline_type(),\n+         \"this object should never have been locked\");\n@@ -218,1 +224,1 @@\n-  assert(open_archive_heap_region_mapped(), \"must be\");\n+  assert(open_regions_mapped(), \"must be\");\n@@ -243,1 +249,1 @@\n-  if (open_archive_heap_region_mapped()) {\n+  if (open_regions_mapped()) {\n@@ -252,1 +258,1 @@\n-oop HeapShared::archive_heap_object(oop obj) {\n+oop HeapShared::archive_object(oop obj) {\n@@ -278,1 +284,1 @@\n-    archived_oop->set_mark(markWord::prototype_for_klass(archived_oop->klass()).copy_set_hash(hash_original));\n+    archived_oop->set_mark(archived_oop->klass()->prototype_header().copy_set_hash(hash_original));\n@@ -338,2 +344,2 @@\n-void HeapShared::archive_java_heap_objects(GrowableArray<MemRegion>* closed,\n-                                           GrowableArray<MemRegion>* open) {\n+void HeapShared::archive_objects(GrowableArray<MemRegion>* closed_regions,\n+                                 GrowableArray<MemRegion>* open_regions) {\n@@ -352,1 +358,1 @@\n-    copy_closed_archive_heap_objects(closed);\n+    copy_closed_objects(closed_regions);\n@@ -355,1 +361,1 @@\n-    copy_open_archive_heap_objects(open);\n+    copy_open_objects(open_regions);\n@@ -363,2 +369,1 @@\n-void HeapShared::copy_closed_archive_heap_objects(\n-                                    GrowableArray<MemRegion> * closed_archive) {\n+void HeapShared::copy_closed_objects(GrowableArray<MemRegion>* closed_regions) {\n@@ -377,1 +382,1 @@\n-  G1CollectedHeap::heap()->end_archive_alloc_range(closed_archive,\n+  G1CollectedHeap::heap()->end_archive_alloc_range(closed_regions,\n@@ -381,2 +386,1 @@\n-void HeapShared::copy_open_archive_heap_objects(\n-                                    GrowableArray<MemRegion> * open_archive) {\n+void HeapShared::copy_open_objects(GrowableArray<MemRegion>* open_regions) {\n@@ -405,1 +409,1 @@\n-  G1CollectedHeap::heap()->end_archive_alloc_range(open_archive,\n+  G1CollectedHeap::heap()->end_archive_alloc_range(open_regions,\n@@ -419,5 +423,1 @@\n-    if (UseBiasedLocking) {\n-      oopDesc::set_mark(mem, k->prototype_header());\n-    } else {\n-      oopDesc::set_mark(mem, markWord::prototype());\n-    }\n+    oopDesc::set_mark(mem, k->prototype_header());\n@@ -886,1 +886,1 @@\n-      assert(!HeapShared::is_archived_object(obj),\n+      assert(!HeapShared::is_archived_object_during_dumptime(obj),\n@@ -905,1 +905,1 @@\n-      assert(HeapShared::is_archived_object(archived), \"must be\");\n+      assert(HeapShared::is_archived_object_during_dumptime(archived), \"must be\");\n@@ -917,1 +917,1 @@\n-void HeapShared::check_closed_archive_heap_region_object(InstanceKlass* k) {\n+void HeapShared::check_closed_region_object(InstanceKlass* k) {\n@@ -961,1 +961,1 @@\n-  assert(!is_archived_object(orig_obj), \"sanity\");\n+  assert(!is_archived_object_during_dumptime(orig_obj), \"sanity\");\n@@ -999,1 +999,1 @@\n-    archived_obj = archive_heap_object(orig_obj);\n+    archived_obj = archive_object(orig_obj);\n@@ -1040,1 +1040,1 @@\n-    check_closed_archive_heap_region_object(InstanceKlass::cast(orig_k));\n+    check_closed_region_object(InstanceKlass::cast(orig_k));\n@@ -1179,1 +1179,1 @@\n-      assert(is_archived_object(obj), \"must be\");\n+      assert(is_archived_object_during_dumptime(obj), \"must be\");\n@@ -1182,1 +1182,1 @@\n-      assert(!is_archived_object(obj), \"must be\");\n+      assert(!is_archived_object_during_dumptime(obj), \"must be\");\n@@ -1442,2 +1442,4 @@\n-void HeapShared::patch_archived_heap_embedded_pointers(MemRegion region, address oopmap,\n-                                                       size_t oopmap_size_in_bits) {\n+\/\/ Patch all the non-null pointers that are embedded in the archived heap objects\n+\/\/ in this region\n+void HeapShared::patch_embedded_pointers(MemRegion region, address oopmap,\n+                                         size_t oopmap_size_in_bits) {\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":38,"deletions":36,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cds\/cdsProtectionDomain.hpp\"\n@@ -74,0 +75,1 @@\n+#include \"services\/memTracker.hpp\"\n@@ -117,1 +119,1 @@\n-\/\/ The ca0\/ca1 and oa0\/oa1 regions are populated inside HeapShared::archive_java_heap_objects.\n+\/\/ The ca0\/ca1 and oa0\/oa1 regions are populated inside HeapShared::archive_objects.\n@@ -251,1 +253,1 @@\n-      SystemDictionaryShared::allocate_shared_data_arrays(size, CHECK);\n+      CDSProtectionDomain::allocate_shared_data_arrays(size, CHECK);\n@@ -395,1 +397,1 @@\n-    if (ik->can_be_verified_at_dumptime()) {\n+    if (ik->can_be_verified_at_dumptime() && ik->is_linked()) {\n@@ -406,2 +408,2 @@\n-  GrowableArray<MemRegion> *_closed_archive_heap_regions;\n-  GrowableArray<MemRegion> *_open_archive_heap_regions;\n+  GrowableArray<MemRegion> *_closed_heap_regions;\n+  GrowableArray<MemRegion> *_open_heap_regions;\n@@ -409,2 +411,2 @@\n-  GrowableArray<ArchiveHeapOopmapInfo> *_closed_archive_heap_oopmaps;\n-  GrowableArray<ArchiveHeapOopmapInfo> *_open_archive_heap_oopmaps;\n+  GrowableArray<ArchiveHeapOopmapInfo> *_closed_heap_oopmaps;\n+  GrowableArray<ArchiveHeapOopmapInfo> *_open_heap_oopmaps;\n@@ -413,2 +415,2 @@\n-  void dump_archive_heap_oopmaps() NOT_CDS_JAVA_HEAP_RETURN;\n-  void dump_archive_heap_oopmaps(GrowableArray<MemRegion>* regions,\n+  void dump_heap_oopmaps() NOT_CDS_JAVA_HEAP_RETURN;\n+  void dump_heap_oopmaps(GrowableArray<MemRegion>* regions,\n@@ -426,4 +428,4 @@\n-    _closed_archive_heap_regions(NULL),\n-    _open_archive_heap_regions(NULL),\n-    _closed_archive_heap_oopmaps(NULL),\n-    _open_archive_heap_oopmaps(NULL) {}\n+    _closed_heap_regions(NULL),\n+    _open_heap_regions(NULL),\n+    _closed_heap_oopmaps(NULL),\n+    _open_heap_oopmaps(NULL) {}\n@@ -475,1 +477,1 @@\n-  dump_archive_heap_oopmaps();\n+  dump_heap_oopmaps();\n@@ -489,3 +491,2 @@\n-  \/\/ At this point, many classes have been loaded.\n-  \/\/ Gather systemDictionary classes in a global array and do everything to\n-  \/\/ that so we don't have to walk the SystemDictionary again.\n+  \/\/ Block concurrent class unloading from changing the _dumptime_table\n+  MutexLocker ml(DumpTimeTable_lock, Mutex::_no_safepoint_check_flag);\n@@ -494,1 +495,0 @@\n-  MutexLocker ml(DumpTimeTable_lock, Mutex::_no_safepoint_check_flag);\n@@ -535,4 +535,4 @@\n-                        _closed_archive_heap_regions,\n-                        _open_archive_heap_regions,\n-                        _closed_archive_heap_oopmaps,\n-                        _open_archive_heap_oopmaps);\n+                        _closed_heap_regions,\n+                        _open_heap_regions,\n+                        _closed_heap_oopmaps,\n+                        _open_heap_oopmaps);\n@@ -576,4 +576,20 @@\n-bool MetaspaceShared::linking_required(InstanceKlass* ik) {\n-  \/\/ For static CDS dump, do not link old classes.\n-  \/\/ For dynamic CDS dump, only link classes loaded by the builtin class loaders.\n-  return DumpSharedSpaces ? ik->can_be_verified_at_dumptime() : !ik->is_shared_unregistered_class();\n+\/\/ Check if we can eagerly link this class at dump time, so we can avoid the\n+\/\/ runtime linking overhead (especially verification)\n+bool MetaspaceShared::may_be_eagerly_linked(InstanceKlass* ik) {\n+  if (!ik->can_be_verified_at_dumptime()) {\n+    \/\/ For old classes, try to leave them in the unlinked state, so\n+    \/\/ we can still store them in the archive. They must be\n+    \/\/ linked\/verified at runtime.\n+    return false;\n+  }\n+  if (DynamicDumpSharedSpaces && ik->is_shared_unregistered_class()) {\n+    \/\/ Linking of unregistered classes at this stage may cause more\n+    \/\/ classes to be resolved, resulting in calls to ClassLoader.loadClass()\n+    \/\/ that may not be expected by custom class loaders.\n+    \/\/\n+    \/\/ It's OK to do this for the built-in loaders as we know they can\n+    \/\/ tolerate this. (Note that unregistered classes are loaded by the NULL\n+    \/\/ loader during DumpSharedSpaces).\n+    return false;\n+  }\n+  return true;\n@@ -597,1 +613,1 @@\n-void MetaspaceShared::link_and_cleanup_shared_classes(TRAPS) {\n+void MetaspaceShared::link_shared_classes(TRAPS) {\n@@ -619,1 +635,1 @@\n-          if (linking_required(ik)) {\n+          if (may_be_eagerly_linked(ik)) {\n@@ -707,1 +723,1 @@\n-  SystemDictionaryShared::create_jar_manifest(dummy, strlen(dummy), CHECK);\n+  CDSProtectionDomain::create_jar_manifest(dummy, strlen(dummy), CHECK);\n@@ -731,1 +747,1 @@\n-  link_and_cleanup_shared_classes(CHECK);\n+  link_shared_classes(CHECK);\n@@ -814,5 +830,4 @@\n-  \/\/ See FileMapInfo::write_archive_heap_regions() for details.\n-  _closed_archive_heap_regions = new GrowableArray<MemRegion>(2);\n-  _open_archive_heap_regions = new GrowableArray<MemRegion>(2);\n-  HeapShared::archive_java_heap_objects(_closed_archive_heap_regions,\n-                                        _open_archive_heap_regions);\n+  \/\/ See FileMapInfo::write_heap_regions() for details.\n+  _closed_heap_regions = new GrowableArray<MemRegion>(2);\n+  _open_heap_regions = new GrowableArray<MemRegion>(2);\n+  HeapShared::archive_objects(_closed_heap_regions, _open_heap_regions);\n@@ -823,1 +838,1 @@\n-void VM_PopulateDumpSharedSpace::dump_archive_heap_oopmaps() {\n+void VM_PopulateDumpSharedSpace::dump_heap_oopmaps() {\n@@ -825,2 +840,2 @@\n-    _closed_archive_heap_oopmaps = new GrowableArray<ArchiveHeapOopmapInfo>(2);\n-    dump_archive_heap_oopmaps(_closed_archive_heap_regions, _closed_archive_heap_oopmaps);\n+    _closed_heap_oopmaps = new GrowableArray<ArchiveHeapOopmapInfo>(2);\n+    dump_heap_oopmaps(_closed_heap_regions, _closed_heap_oopmaps);\n@@ -828,2 +843,2 @@\n-    _open_archive_heap_oopmaps = new GrowableArray<ArchiveHeapOopmapInfo>(2);\n-    dump_archive_heap_oopmaps(_open_archive_heap_regions, _open_archive_heap_oopmaps);\n+    _open_heap_oopmaps = new GrowableArray<ArchiveHeapOopmapInfo>(2);\n+    dump_heap_oopmaps(_open_heap_regions, _open_heap_oopmaps);\n@@ -833,2 +848,2 @@\n-void VM_PopulateDumpSharedSpace::dump_archive_heap_oopmaps(GrowableArray<MemRegion>* regions,\n-                                                           GrowableArray<ArchiveHeapOopmapInfo>* oopmaps) {\n+void VM_PopulateDumpSharedSpace::dump_heap_oopmaps(GrowableArray<MemRegion>* regions,\n+                                                   GrowableArray<ArchiveHeapOopmapInfo>* oopmaps) {\n@@ -1389,1 +1404,1 @@\n-  static_mapinfo->patch_archived_heap_embedded_pointers();\n+  static_mapinfo->patch_heap_embedded_pointers();\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":58,"deletions":43,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -844,1 +844,1 @@\n-        JavaThread* THREAD = Thread::current()->as_Java_thread();\n+        JavaThread* THREAD = JavaThread::current();\n@@ -857,1 +857,1 @@\n-          oop value = vk->allocate_instance(Thread::current()->as_Java_thread());\n+          oop value = vk->allocate_instance(JavaThread::current());\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -708,2 +708,3 @@\n-            \/\/ Method name and signature are verified above, when iterating NameAndType_info.\n-            \/\/ Need only to be sure signature is non-zero length and the right type.\n+            \/\/ Method name and signature are individually verified above, when iterating\n+            \/\/ NameAndType_info.  Need to check here that signature is non-zero length and\n+            \/\/ the right type.\n@@ -714,1 +715,1 @@\n-          \/\/ 4509014: If a class method name begins with '<', it must be \"<init>\"\n+          \/\/ If a class method name begins with '<', it must be \"<init>\" and have void signature.\n@@ -716,8 +717,10 @@\n-          if (tag == JVM_CONSTANT_Methodref &&\n-              name_len != 0 &&\n-              name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n-              name != vmSymbols::object_initializer_name()) {\n-            classfile_parse_error(\n-              \"Bad method name at constant pool index %u in class file %s\",\n-              name_ref_index, THREAD);\n-            return;\n+          if (tag == JVM_CONSTANT_Methodref && name_len != 0 &&\n+              name->char_at(0) == JVM_SIGNATURE_SPECIAL) {\n+            if (name != vmSymbols::object_initializer_name()) {\n+              classfile_parse_error(\n+                \"Bad method name at constant pool index %u in class file %s\",\n+                name_ref_index, THREAD);\n+              return;\n+            } else if (!Signature::is_void_method(signature) && !EnableValhalla) { \/\/ must have void signature (unless inline type).\n+              throwIllegalSignature(\"Method\", name, signature, CHECK);\n+            }\n@@ -2186,1 +2189,0 @@\n-      ik->set_prototype_header(markWord::prototype());\n@@ -2445,0 +2447,1 @@\n+    verify_legal_name_with_signature(name, signature, CHECK_NULL);\n@@ -4355,1 +4358,1 @@\n-        JavaThread *THREAD = Thread::current()->as_Java_thread();\n+        JavaThread *THREAD = JavaThread::current();\n@@ -5294,0 +5297,28 @@\n+\/\/ Check that the signature is compatible with the method name.  For example,\n+\/\/ check that <init> has a void signature.\n+void ClassFileParser::verify_legal_name_with_signature(const Symbol* name,\n+                                                       const Symbol* signature,\n+                                                       TRAPS) const {\n+  if (!_need_verify) {\n+    return;\n+  }\n+\n+  \/\/ Class initializers cannot have args for class format version >= 51.\n+  if (name == vmSymbols::class_initializer_name() &&\n+      signature != vmSymbols::void_method_signature() &&\n+      _major_version >= JAVA_7_VERSION) {\n+    throwIllegalSignature(\"Method\", name, signature, THREAD);\n+    return;\n+  }\n+\n+  if (!is_inline_type()) {\n+    int sig_length = signature->utf8_length();\n+    if (name->utf8_length() > 0 &&\n+      name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n+      sig_length > 0 &&\n+      signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n+      throwIllegalSignature(\"Method\", name, signature, THREAD);\n+    }\n+  }\n+}\n+\n@@ -5305,8 +5336,0 @@\n-  \/\/ Class initializers cannot have args for class format version >= 51.\n-  if (name == vmSymbols::class_initializer_name() &&\n-      signature != vmSymbols::void_method_signature() &&\n-      _major_version >= JAVA_7_VERSION) {\n-    throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n-    return 0;\n-  }\n-\n@@ -5335,27 +5358,4 @@\n-      if (name->utf8_length() > 0 && name->char_at(0) == JVM_SIGNATURE_SPECIAL) {\n-        \/\/ All constructor methods must return void\n-        if ((length == 1) && (p[0] == JVM_SIGNATURE_VOID)) {\n-          return args_size;\n-        }\n-        \/\/ All static init methods must return the current class\n-        if ((length >= 3) && (p[length-1] == JVM_SIGNATURE_ENDCLASS)\n-            && name == vmSymbols::object_initializer_name()) {\n-          nextp = skip_over_field_signature(p, true, length, CHECK_0);\n-          if (nextp && ((int)length == (nextp - p))) {\n-            \/\/ The actual class will be checked against current class\n-            \/\/ when the method is defined (see parse_method).\n-            \/\/ A reference to a static init with a bad return type\n-            \/\/ will load and verify OK, but will fail to link.\n-            return args_size;\n-          }\n-        }\n-        \/\/ The distinction between static factory methods and\n-        \/\/ constructors depends on the JVM_ACC_STATIC modifier.\n-        \/\/ This distinction must be reflected in a void or non-void\n-        \/\/ return. For declared methods, the check is in parse_method.\n-      } else {\n-        \/\/ Now we better just have a return value\n-        nextp = skip_over_field_signature(p, true, length, CHECK_0);\n-        if (nextp && ((int)length == (nextp - p))) {\n-          return args_size;\n-        }\n+      \/\/ Now we better just have a return value\n+      nextp = skip_over_field_signature(p, true, length, CHECK_0);\n+      if (nextp && ((int)length == (nextp - p))) {\n+        return args_size;\n@@ -5366,1 +5366,1 @@\n-  throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+  throwIllegalSignature(\"Method\", name, signature, THREAD);\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":49,"deletions":49,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -493,0 +493,3 @@\n+  void verify_legal_name_with_signature(const Symbol* name,\n+                                        const Symbol* signature,\n+                                        TRAPS) const;\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -566,0 +566,15 @@\n+  \/\/ Method::clear_jmethod_ids only sets the jmethod_ids to NULL without\n+  \/\/ releasing the memory for related JNIMethodBlocks and JNIMethodBlockNodes.\n+  \/\/ This is done intentionally because native code (e.g. JVMTI agent) holding\n+  \/\/ jmethod_ids may access them after the associated classes and class loader\n+  \/\/ are unloaded. The Java Native Interface Specification says \"method ID\n+  \/\/ does not prevent the VM from unloading the class from which the ID has\n+  \/\/ been derived. After the class is unloaded, the method or field ID becomes\n+  \/\/ invalid\". In real world usages, the native code may rely on jmethod_ids\n+  \/\/ being NULL after class unloading. Hence, it is unsafe to free the memory\n+  \/\/ from the VM side without knowing when native code is going to stop using\n+  \/\/ them.\n+  if (_jmethod_ids != NULL) {\n+    Method::clear_jmethod_ids(this);\n+  }\n+\n@@ -710,14 +725,1 @@\n-  \/\/ Method::clear_jmethod_ids only sets the jmethod_ids to NULL without\n-  \/\/ releasing the memory for related JNIMethodBlocks and JNIMethodBlockNodes.\n-  \/\/ This is done intentionally because native code (e.g. JVMTI agent) holding\n-  \/\/ jmethod_ids may access them after the associated classes and class loader\n-  \/\/ are unloaded. The Java Native Interface Specification says \"method ID\n-  \/\/ does not prevent the VM from unloading the class from which the ID has\n-  \/\/ been derived. After the class is unloaded, the method or field ID becomes\n-  \/\/ invalid\". In real world usages, the native code may rely on jmethod_ids\n-  \/\/ being NULL after class unloading. Hence, it is unsafe to free the memory\n-  \/\/ from the VM side without knowing when native code is going to stop using\n-  \/\/ them.\n-  if (_jmethod_ids != NULL) {\n-    Method::clear_jmethod_ids(this);\n-  }\n+\n@@ -820,0 +822,1 @@\n+    record_modified_oops();\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.cpp","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -327,1 +327,1 @@\n-    return (unsigned)((uintptr_t)this >> 3);\n+    return (unsigned)((uintptr_t)this >> LogBytesPerWord);\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -646,1 +646,1 @@\n-        JavaThread* THREAD = Thread::current()->as_Java_thread();\n+        JavaThread* THREAD = JavaThread::current();\n@@ -747,1 +747,1 @@\n-        JavaThread* THREAD = Thread::current()->as_Java_thread();\n+        JavaThread* THREAD = JavaThread::current();\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n@@ -507,1 +508,1 @@\n-unsigned int java_lang_String::hash_code(oop java_string) {\n+inline unsigned int java_lang_String::hash_code_impl(oop java_string, bool update) {\n@@ -534,4 +535,6 @@\n-  if (hash != 0) {\n-    java_string->int_field_put(_hash_offset, hash);\n-  } else {\n-    java_string->bool_field_put(_hashIsZero_offset, true);\n+  if (update) {\n+    if (hash != 0) {\n+      java_string->int_field_put(_hash_offset, hash);\n+    } else {\n+      java_string->bool_field_put(_hashIsZero_offset, true);\n+    }\n@@ -542,0 +545,9 @@\n+unsigned int java_lang_String::hash_code(oop java_string) {\n+  return hash_code_impl(java_string, \/*update=*\/true);\n+}\n+\n+unsigned int java_lang_String::hash_code_noupdate(oop java_string) {\n+  return hash_code_impl(java_string, \/*update=*\/false);\n+}\n+\n+\n@@ -812,8 +824,5 @@\n-  if (HeapShared::is_archived_object(mirror())) {\n-    \/\/ Archive the String field and update the pointer.\n-    oop s = mirror()->obj_field(fd->offset());\n-    oop archived_s = StringTable::create_archived_string(s);\n-    mirror()->obj_field_put(fd->offset(), archived_s);\n-  } else {\n-    guarantee(false, \"Unexpected\");\n-  }\n+  assert(HeapShared::is_archived_object_during_dumptime(mirror()), \"must be\");\n+  \/\/ Archive the String field and update the pointer.\n+  oop s = mirror()->obj_field(fd->offset());\n+  oop archived_s = StringTable::create_archived_string(s);\n+  mirror()->obj_field_put(fd->offset(), archived_s);\n@@ -901,1 +910,1 @@\n-    if (HeapShared::open_archive_heap_region_mapped()) {\n+    if (HeapShared::open_regions_mapped()) {\n@@ -1190,1 +1199,1 @@\n-      oop archived_m = HeapShared::archive_heap_object(m);\n+      oop archived_m = HeapShared::archive_object(m);\n@@ -1255,1 +1264,1 @@\n-  oop archived_mirror = HeapShared::archive_heap_object(mirror);\n+  oop archived_mirror = HeapShared::archive_object(mirror);\n@@ -1382,1 +1391,1 @@\n-  assert(HeapShared::is_archived_object(m), \"must be archived mirror object\");\n+  assert(Universe::heap()->is_archived_object(m), \"must be archived mirror object\");\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":26,"deletions":17,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -130,0 +130,2 @@\n+  static inline unsigned int hash_code_impl(oop java_string, bool update);\n+\n@@ -225,0 +227,1 @@\n+  static unsigned int hash_code_noupdate(oop java_string);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-   Thread *_thread;\n+   JavaThread* _thread;\n@@ -52,1 +52,1 @@\n-   SeenThread(Thread *thread) {\n+   SeenThread(JavaThread* thread) {\n@@ -57,2 +57,2 @@\n-   Thread* thread()                const { return _thread;}\n-   void set_thread(Thread *thread) { _thread = thread; }\n+   JavaThread* thread()          const { return _thread;}\n+   void set_thread(JavaThread* thread) { _thread = thread; }\n@@ -60,3 +60,3 @@\n-   SeenThread* next()              const { return _stnext;}\n-   void set_next(SeenThread *seen) { _stnext = seen; }\n-   void set_prev(SeenThread *seen) { _stprev = seen; }\n+   SeenThread* next()        const { return _stnext;}\n+   void set_next(SeenThread* seen) { _stnext = seen; }\n+   void set_prev(SeenThread* seen) { _stprev = seen; }\n@@ -118,1 +118,1 @@\n-void PlaceholderEntry::add_seen_thread(Thread* thread, PlaceholderTable::classloadAction action) {\n+void PlaceholderEntry::add_seen_thread(JavaThread* thread, PlaceholderTable::classloadAction action) {\n@@ -139,1 +139,1 @@\n-bool PlaceholderEntry::check_seen_thread(Thread* thread, PlaceholderTable::classloadAction action) {\n+bool PlaceholderEntry::check_seen_thread(JavaThread* thread, PlaceholderTable::classloadAction action) {\n@@ -157,1 +157,1 @@\n-bool PlaceholderEntry::remove_seen_thread(Thread* thread, PlaceholderTable::classloadAction action) {\n+bool PlaceholderEntry::remove_seen_thread(JavaThread* thread, PlaceholderTable::classloadAction action) {\n@@ -301,1 +301,1 @@\n-                                                 Thread* thread) {\n+                                                 JavaThread* thread) {\n@@ -334,1 +334,1 @@\n-                                       Thread* thread) {\n+                                       JavaThread* thread) {\n","filename":"src\/hotspot\/share\/classfile\/placeholders.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -90,1 +90,1 @@\n-                                 Thread* thread);\n+                                 JavaThread* thread);\n@@ -99,1 +99,1 @@\n-                       classloadAction action, Thread* thread);\n+                       classloadAction action, JavaThread* thread);\n@@ -121,1 +121,1 @@\n-  Thread*           _definer;       \/\/ owner of define token\n+  JavaThread*       _definer;       \/\/ owner of define token\n@@ -136,2 +136,2 @@\n-  void add_seen_thread(Thread* thread, PlaceholderTable::classloadAction action);\n-  bool remove_seen_thread(Thread* thread, PlaceholderTable::classloadAction action);\n+  void add_seen_thread(JavaThread* thread, PlaceholderTable::classloadAction action);\n+  bool remove_seen_thread(JavaThread* thread, PlaceholderTable::classloadAction action);\n@@ -152,2 +152,2 @@\n-  Thread*            definer()             const {return _definer; }\n-  void               set_definer(Thread* definer) { _definer = definer; }\n+  JavaThread*        definer()             const {return _definer; }\n+  void               set_definer(JavaThread* definer) { _definer = definer; }\n@@ -164,1 +164,1 @@\n-  SeenThread*        defineThreadQ()        const { return _defineThreadQ; }\n+  SeenThread*        defineThreadQ()       const { return _defineThreadQ; }\n@@ -201,1 +201,1 @@\n-  bool check_seen_thread(Thread* thread, PlaceholderTable::classloadAction action);\n+  bool check_seen_thread(JavaThread* thread, PlaceholderTable::classloadAction action);\n","filename":"src\/hotspot\/share\/classfile\/placeholders.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -73,1 +73,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -161,1 +160,0 @@\n-  if (AlwaysLockClassLoader) return false;\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -212,5 +212,0 @@\n-protected:\n-  \/\/ Returns the class loader data to be used when looking up\/updating the\n-  \/\/ system dictionary.\n-  static ClassLoaderData *class_loader_data(Handle class_loader);\n-\n@@ -221,0 +216,4 @@\n+  \/\/ Returns the class loader data to be used when looking up\/updating the\n+  \/\/ system dictionary.\n+  static ClassLoaderData *class_loader_data(Handle class_loader);\n+\n@@ -230,1 +229,0 @@\n-public:\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -460,1 +460,1 @@\n-   do_signature(decodeBlock_signature, \"([BII[BIZ)I\")                                                                   \\\n+   do_signature(decodeBlock_signature, \"([BII[BIZZ)I\")                                                                   \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,0 +44,2 @@\n+#include \"runtime\/javaFrameAnchor.hpp\"\n+#include \"runtime\/jniHandles.hpp\"\n@@ -745,1 +747,1 @@\n-                     jobject receiver, ByteSize jfa_sp_offset) :\n+                                       jobject receiver, ByteSize frame_data_offset) :\n@@ -749,1 +751,1 @@\n-  _jfa_sp_offset(jfa_sp_offset) {\n+  _frame_data_offset(frame_data_offset) {\n@@ -754,1 +756,1 @@\n-                             jobject receiver, ByteSize jfa_sp_offset) {\n+                                               jobject receiver, ByteSize frame_data_offset) {\n@@ -761,1 +763,1 @@\n-    blob = new (size) OptimizedEntryBlob(name, size, cb, exception_handler_offset, receiver, jfa_sp_offset);\n+    blob = new (size) OptimizedEntryBlob(name, size, cb, exception_handler_offset, receiver, frame_data_offset);\n@@ -768,0 +770,8 @@\n+\n+void OptimizedEntryBlob::oops_do(OopClosure* f, const frame& frame) {\n+  frame_data_for_frame(frame)->old_handles->oops_do(f);\n+}\n+\n+JavaFrameAnchor* OptimizedEntryBlob::jfa_for_frame(const frame& frame) const {\n+  return &frame_data_for_frame(frame)->jfa;\n+}\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":14,"deletions":4,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"runtime\/javaFrameAnchor.hpp\"\n@@ -37,0 +38,1 @@\n+class JNIHandleBlock;\n@@ -80,1 +82,1 @@\n-class JavaFrameAnchor; \/\/ for EntryBlob::jfa_for_frame\n+class JavaFrameAnchor; \/\/ for OptimizedEntryBlob::jfa_for_frame\n@@ -760,1 +762,2 @@\n-\/\/ For optimized upcall stubs\n+class ProgrammableUpcallHandler;\n+\n@@ -762,0 +765,1 @@\n+  friend class ProgrammableUpcallHandler;\n@@ -765,1 +769,1 @@\n-  ByteSize _jfa_sp_offset;\n+  ByteSize _frame_data_offset;\n@@ -768,1 +772,9 @@\n-            jobject receiver, ByteSize jfa_sp_offset);\n+                     jobject receiver, ByteSize frame_data_offset);\n+\n+  struct FrameData {\n+    JavaFrameAnchor jfa;\n+    JavaThread* thread;\n+    JNIHandleBlock* old_handles;\n+    JNIHandleBlock* new_handles;\n+    bool should_detach;\n+  };\n@@ -770,0 +782,2 @@\n+  \/\/ defined in frame_ARCH.cpp\n+  FrameData* frame_data_for_frame(const frame& frame) const;\n@@ -773,2 +787,2 @@\n-                           intptr_t exception_handler_offset, jobject receiver,\n-                           ByteSize jfa_sp_offset);\n+                                    intptr_t exception_handler_offset, jobject receiver,\n+                                    ByteSize frame_data_offset);\n@@ -778,2 +792,0 @@\n-  ByteSize jfa_sp_offset() const { return _jfa_sp_offset; }\n-  \/\/ defined in frame_ARCH.cpp\n@@ -783,0 +795,2 @@\n+  void oops_do(OopClosure* f, const frame& frame);\n+\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":22,"deletions":8,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -1623,2 +1623,14 @@\n-  if (is_not_entrant() && can_convert_to_zombie()) {\n-    return;\n+  {\n+    MutexLocker ml(CompiledMethod_lock, Mutex::_no_safepoint_check_flag);\n+    \/\/ When the nmethod is acquired from the CodeCache iterator, it can racingly become zombie\n+    \/\/ before this code is called. Filter them out here under the CompiledMethod_lock.\n+    if (!is_alive()) {\n+      return;\n+    }\n+    \/\/ As for is_alive() nmethods, we also don't want them to racingly become zombie once we\n+    \/\/ release this lock, so we check that this is not going to be the case.\n+    if (is_not_entrant() && can_convert_to_zombie()) {\n+      return;\n+    }\n+    \/\/ Ensure the sweeper can't collect this nmethod until it become \"active\" with JvmtiThreadState::nmethods_do.\n+    mark_as_seen_on_stack();\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":14,"deletions":2,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -276,2 +276,1 @@\n-  \/\/ locate the owner and stack slot for the BasicLock so that we can\n-  \/\/ properly revoke the bias of the owner if necessary. They are\n+  \/\/ locate the owner and stack slot for the BasicLock. They are\n@@ -282,5 +281,2 @@\n-  \/\/ sharing between platforms. Note that currently biased locking\n-  \/\/ will never cause Class instances to be biased but this code\n-  \/\/ handles the static synchronized case as well.\n-  \/\/ JVMTI's GetLocalInstance() also uses these offsets to find the receiver\n-  \/\/ for non-static native wrapper frames.\n+  \/\/ sharing between platforms. JVMTI's GetLocalInstance() uses these\n+  \/\/ offsets to find the receiver for non-static native wrapper frames.\n@@ -746,1 +742,1 @@\n-  \/\/ UseBiasedLocking support\n+  \/\/ JVMTI's GetLocalInstance() support\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":5,"deletions":9,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -798,8 +798,2 @@\n-  Handle string = java_lang_String::create_from_str(name, CHECK_NH);\n-  Handle thread_group(THREAD, Universe::system_thread_group());\n-  return JavaCalls::construct_new_instance(\n-                       vmClasses::Thread_klass(),\n-                       vmSymbols::threadgroup_string_void_signature(),\n-                       thread_group,\n-                       string,\n-                       CHECK_NH);\n+  Handle thread_oop = JavaThread::create_system_thread_object(name, false \/* not visible *\/, CHECK_NH);\n+  return thread_oop;\n@@ -878,13 +872,12 @@\n-  {\n-    MutexLocker mu(THREAD, Threads_lock);\n-    switch (type) {\n-      case compiler_t:\n-        assert(comp != NULL, \"Compiler instance missing.\");\n-        if (!InjectCompilerCreationFailure || comp->num_compiler_threads() == 0) {\n-          CompilerCounters* counters = new CompilerCounters();\n-          new_thread = new CompilerThread(queue, counters);\n-        }\n-        break;\n-      case sweeper_t:\n-        new_thread = new CodeCacheSweeperThread();\n-        break;\n+\n+  switch (type) {\n+    case compiler_t:\n+      assert(comp != NULL, \"Compiler instance missing.\");\n+      if (!InjectCompilerCreationFailure || comp->num_compiler_threads() == 0) {\n+        CompilerCounters* counters = new CompilerCounters();\n+        new_thread = new CompilerThread(queue, counters);\n+      }\n+      break;\n+    case sweeper_t:\n+      new_thread = new CodeCacheSweeperThread();\n+      break;\n@@ -892,3 +885,3 @@\n-      case deoptimizer_t:\n-        new_thread = new DeoptimizeObjectsALotThread();\n-        break;\n+    case deoptimizer_t:\n+      new_thread = new DeoptimizeObjectsALotThread();\n+      break;\n@@ -896,20 +889,3 @@\n-      default:\n-        ShouldNotReachHere();\n-    }\n-\n-    \/\/ At this point the new CompilerThread data-races with this startup\n-    \/\/ thread (which I believe is the primoridal thread and NOT the VM\n-    \/\/ thread).  This means Java bytecodes being executed at startup can\n-    \/\/ queue compile jobs which will run at whatever default priority the\n-    \/\/ newly created CompilerThread runs at.\n-\n-\n-    \/\/ At this point it may be possible that no osthread was created for the\n-    \/\/ JavaThread due to lack of memory. We would have to throw an exception\n-    \/\/ in that case. However, since this must work and we do not allow\n-    \/\/ exceptions anyway, check and abort if this fails. But first release the\n-    \/\/ lock.\n-\n-    if (new_thread != NULL && new_thread->osthread() != NULL) {\n-\n-      java_lang_Thread::set_thread(JNIHandles::resolve_non_null(thread_handle), new_thread);\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -917,3 +893,5 @@\n-      \/\/ Note that this only sets the JavaThread _priority field, which by\n-      \/\/ definition is limited to Java priorities and not OS priorities.\n-      \/\/ The os-priority is set in the CompilerThread startup code itself\n+  \/\/ At this point the new CompilerThread data-races with this startup\n+  \/\/ thread (which is the main thread and NOT the VM thread).\n+  \/\/ This means Java bytecodes being executed at startup can\n+  \/\/ queue compile jobs which will run at whatever default priority the\n+  \/\/ newly created CompilerThread runs at.\n@@ -921,5 +899,5 @@\n-      java_lang_Thread::set_priority(JNIHandles::resolve_non_null(thread_handle), NearMaxPriority);\n-      \/\/ Note that we cannot call os::set_priority because it expects Java\n-      \/\/ priorities and we are *explicitly* using OS priorities so that it's\n-      \/\/ possible to set the compiler thread priority higher than any Java\n-      \/\/ thread.\n+  \/\/ At this point it may be possible that no osthread was created for the\n+  \/\/ JavaThread due to lack of resources. We will handle that failure below.\n+  \/\/ Also check new_thread so that static analysis is happy.\n+  if (new_thread != NULL && new_thread->osthread() != NULL) {\n+    Handle thread_oop(THREAD, JNIHandles::resolve_non_null(thread_handle));\n@@ -928,9 +906,3 @@\n-      int native_prio = CompilerThreadPriority;\n-      if (native_prio == -1) {\n-        if (UseCriticalCompilerThreadPriority) {\n-          native_prio = os::java_to_os_priority[CriticalPriority];\n-        } else {\n-          native_prio = os::java_to_os_priority[NearMaxPriority];\n-        }\n-      }\n-      os::set_native_priority(new_thread, native_prio);\n+    if (type == compiler_t) {\n+      CompilerThread::cast(new_thread)->set_compiler(comp);\n+    }\n@@ -938,1 +910,4 @@\n-      java_lang_Thread::set_daemon(JNIHandles::resolve_non_null(thread_handle));\n+    \/\/ Note that we cannot call os::set_priority because it expects Java\n+    \/\/ priorities and we are *explicitly* using OS priorities so that it's\n+    \/\/ possible to set the compiler thread priority higher than any Java\n+    \/\/ thread.\n@@ -940,3 +915,6 @@\n-      new_thread->set_threadObj(JNIHandles::resolve_non_null(thread_handle));\n-      if (type == compiler_t) {\n-        CompilerThread::cast(new_thread)->set_compiler(comp);\n+    int native_prio = CompilerThreadPriority;\n+    if (native_prio == -1) {\n+      if (UseCriticalCompilerThreadPriority) {\n+        native_prio = os::java_to_os_priority[CriticalPriority];\n+      } else {\n+        native_prio = os::java_to_os_priority[NearMaxPriority];\n@@ -944,3 +922,1 @@\n-      Threads::add(new_thread);\n-      Thread::start(new_thread);\n-  }\n+    os::set_native_priority(new_thread, native_prio);\n@@ -949,6 +925,9 @@\n-  \/\/ First release lock before aborting VM.\n-  if (new_thread == NULL || new_thread->osthread() == NULL) {\n-    if (UseDynamicNumberOfCompilerThreads && type == compiler_t && comp->num_compiler_threads() > 0) {\n-      if (new_thread != NULL) {\n-        new_thread->smr_delete();\n-      }\n+    \/\/ Note that this only sets the JavaThread _priority field, which by\n+    \/\/ definition is limited to Java priorities and not OS priorities.\n+    JavaThread::start_internal_daemon(THREAD, new_thread, thread_oop, NearMaxPriority);\n+\n+  } else { \/\/ osthread initialization failure\n+    if (UseDynamicNumberOfCompilerThreads && type == compiler_t\n+        && comp->num_compiler_threads() > 0) {\n+      \/\/ The new thread is not known to Thread-SMR yet so we can just delete.\n+      delete new_thread;\n@@ -956,0 +935,3 @@\n+    } else {\n+      vm_exit_during_initialization(\"java.lang.OutOfMemoryError\",\n+                                    os::native_thread_creation_failed_msg());\n@@ -957,2 +939,0 @@\n-    vm_exit_during_initialization(\"java.lang.OutOfMemoryError\",\n-                                  os::native_thread_creation_failed_msg());\n@@ -961,1 +941,0 @@\n-  \/\/ Let go of Threads_lock before yielding\n@@ -1011,1 +990,1 @@\n-        ThreadsListHandle tlh;  \/\/ get_thread_name() depends on the TLH.\n+        ThreadsListHandle tlh;  \/\/ name() depends on the TLH.\n@@ -1013,1 +992,1 @@\n-        tty->print_cr(\"Added initial compiler thread %s\", ct->get_thread_name());\n+        tty->print_cr(\"Added initial compiler thread %s\", ct->name());\n@@ -1032,1 +1011,1 @@\n-        ThreadsListHandle tlh;  \/\/ get_thread_name() depends on the TLH.\n+        ThreadsListHandle tlh;  \/\/ name() depends on the TLH.\n@@ -1034,1 +1013,1 @@\n-        tty->print_cr(\"Added initial compiler thread %s\", ct->get_thread_name());\n+        tty->print_cr(\"Added initial compiler thread %s\", ct->name());\n@@ -1119,1 +1098,1 @@\n-        ThreadsListHandle tlh;  \/\/ get_thread_name() depends on the TLH.\n+        ThreadsListHandle tlh;  \/\/ name() depends on the TLH.\n@@ -1122,1 +1101,1 @@\n-                      ct->get_thread_name(), (int)(available_memory\/M), (int)(available_cc_np\/M));\n+                      ct->name(), (int)(available_memory\/M), (int)(available_cc_np\/M));\n@@ -1140,1 +1119,1 @@\n-        ThreadsListHandle tlh;  \/\/ get_thread_name() depends on the TLH.\n+        ThreadsListHandle tlh;  \/\/ name() depends on the TLH.\n@@ -1143,1 +1122,1 @@\n-                      ct->get_thread_name(), (int)(available_memory\/M), (int)(available_cc_p\/M));\n+                      ct->name(), (int)(available_memory\/M), (int)(available_cc_p\/M));\n@@ -1279,1 +1258,1 @@\n-        vframeStream vfst(thread->as_Java_thread());\n+        vframeStream vfst(JavaThread::cast(thread));\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":65,"deletions":86,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -111,4 +111,1 @@\n-      \/\/ since it will be restored by preserved marks. There is an exception\n-      \/\/ with BiasedLocking, in this case forwardee() will return NULL\n-      \/\/ even if the mark-word is used. This is no problem since\n-      \/\/ forwardee() will return NULL in the compaction phase as well.\n+      \/\/ since it will be restored by preserved marks.\n@@ -119,3 +116,2 @@\n-      assert(object->mark() == markWord::prototype_for_klass(object->klass()) || \/\/ Correct mark\n-             object->mark_must_be_preserved() || \/\/ Will be restored by PreservedMarksSet\n-             (UseBiasedLocking && object->has_bias_pattern()), \/\/ Will be restored by BiasedLocking\n+      assert(object->mark() == object->klass()->prototype_header() || \/\/ Correct mark\n+             object->mark_must_be_preserved(), \/\/ Will be restored by PreservedMarksSet\n@@ -123,1 +119,1 @@\n-             p2i(object), object->mark().value(), markWord::prototype_for_klass(object->klass()).value());\n+             p2i(object), object->mark().value(), object->klass()->prototype_header().value());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":5,"deletions":9,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-#include \"gc\/g1\/heapRegionRemSet.hpp\"\n+#include \"gc\/g1\/heapRegionRemSet.inline.hpp\"\n@@ -83,3 +83,2 @@\n-    assert(obj->mark() == markWord::prototype_for_klass(obj->klass()) || \/\/ Correct mark\n-           obj->mark_must_be_preserved() || \/\/ Will be restored by PreservedMarksSet\n-           (UseBiasedLocking && obj->has_bias_pattern()), \/\/ Will be restored by BiasedLocking\n+    assert(obj->mark() == obj->klass()->prototype_header() || \/\/ Correct mark\n+           obj->mark_must_be_preserved(), \/\/ Will be restored by PreservedMarksSet\n@@ -87,1 +86,1 @@\n-           p2i(obj), obj->mark().value(), markWord::prototype_for_klass(obj->klass()).value());\n+           p2i(obj), obj->mark().value(), obj->klass()->prototype_header().value());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.inline.hpp","additions":5,"deletions":6,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1790,4 +1790,1 @@\n-    ref_processor()->enable_discovery();\n-    ref_processor()->setup_policy(maximum_heap_compaction);\n-\n-    bool marked_for_unloading = false;\n+    ref_processor()->start_discovery(maximum_heap_compaction);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,1 @@\n-#include \"oops\/markWord.inline.hpp\"\n+#include \"oops\/markWord.hpp\"\n@@ -85,4 +85,2 @@\n-    assert(new_obj != NULL ||                          \/\/ is forwarding ptr?\n-           obj->mark() == markWord::prototype_for_klass(obj->klass()) || \/\/ not gc marked?\n-           (UseBiasedLocking && obj->mark().has_bias_pattern()),\n-           \/\/ not gc marked?\n+    assert(new_obj != NULL ||                                 \/\/ is forwarding ptr?\n+           obj->mark() == obj->klass()->prototype_header(),   \/\/ not gc marked?\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.inline.hpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n-      throw_array_null_pointer_store_exception(src_obj, dst_obj, Thread::current()->as_Java_thread());\n+      throw_array_null_pointer_store_exception(src_obj, dst_obj, JavaThread::current());\n@@ -61,1 +61,1 @@\n-      throw_array_store_exception(src_obj, dst_obj, Thread::current()->as_Java_thread());\n+      throw_array_store_exception(src_obj, dst_obj, JavaThread::current());\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSet.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -175,1 +175,1 @@\n-  _thread->as_Java_thread()->check_for_valid_safepoint_state();\n+  JavaThread::cast(_thread)->check_for_valid_safepoint_state();\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -133,1 +133,1 @@\n-        throw_array_null_pointer_store_exception(src_obj, dst_obj, Thread::current()->as_Java_thread());\n+        throw_array_null_pointer_store_exception(src_obj, dst_obj, JavaThread::current());\n@@ -139,1 +139,1 @@\n-        throw_array_store_exception(src_obj, dst_obj, Thread::current()->as_Java_thread());\n+        throw_array_store_exception(src_obj, dst_obj, JavaThread::current());\n","filename":"src\/hotspot\/share\/gc\/shared\/modRefBarrierSet.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1406,1 +1406,1 @@\n-      Node* base = new CheckCastPPNode(ctrl, orig_base, orig_base->bottom_type(), true);\n+      Node* base = new CheckCastPPNode(ctrl, orig_base, orig_base->bottom_type(), ConstraintCastNode::StrongDependency);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -57,1 +57,1 @@\n-    return (decorators & (ON_WEAK_OOP_REF | ON_PHANTOM_OOP_REF | ON_UNKNOWN_OOP_REF)) == 0;\n+    return (decorators & (ON_WEAK_OOP_REF | ON_PHANTOM_OOP_REF)) == 0;\n@@ -61,1 +61,1 @@\n-    return (decorators & (ON_WEAK_OOP_REF | ON_UNKNOWN_OOP_REF)) != 0;\n+    return (decorators & ON_WEAK_OOP_REF) != 0;\n@@ -93,2 +93,0 @@\n-  template <DecoratorSet decorators>\n-  inline void keep_alive_if_weak(oop value);\n@@ -104,2 +102,11 @@\n-  template <DecoratorSet decorators, class T>\n-  inline oop load_reference_barrier(oop obj, T* load_addr);\n+  template <class T>\n+  inline oop load_reference_barrier(DecoratorSet decorators, oop obj, T* load_addr);\n+\n+  template <typename T>\n+  inline oop oop_load(DecoratorSet decorators, T* addr);\n+\n+  template <typename T>\n+  inline oop oop_cmpxchg(DecoratorSet decorators, T* addr, oop compare_value, oop new_value);\n+\n+  template <typename T>\n+  inline oop oop_xchg(DecoratorSet decorators, T* addr, oop new_value);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":13,"deletions":6,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -73,1 +73,1 @@\n-    ShenandoahHeap::cas_oop(fwd, load_addr, obj);\n+    ShenandoahHeap::atomic_update_oop(fwd, load_addr, obj);\n@@ -102,2 +102,2 @@\n-template <DecoratorSet decorators, class T>\n-inline oop ShenandoahBarrierSet::load_reference_barrier(oop obj, T* load_addr) {\n+template <class T>\n+inline oop ShenandoahBarrierSet::load_reference_barrier(DecoratorSet decorators, oop obj, T* load_addr) {\n@@ -109,1 +109,1 @@\n-  if (HasDecorator<decorators, ON_PHANTOM_OOP_REF>::value &&\n+  if ((decorators & ON_PHANTOM_OOP_REF) != 0 &&\n@@ -116,1 +116,1 @@\n-  if ((HasDecorator<decorators, ON_WEAK_OOP_REF>::value || HasDecorator<decorators, ON_UNKNOWN_OOP_REF>::value) &&\n+  if ((decorators & ON_WEAK_OOP_REF) != 0 &&\n@@ -124,1 +124,1 @@\n-  if (HasDecorator<decorators, AS_NO_KEEPALIVE>::value &&\n+  if ((decorators & AS_NO_KEEPALIVE) != 0 &&\n@@ -133,1 +133,1 @@\n-    ShenandoahHeap::cas_oop(fwd, load_addr, obj);\n+    ShenandoahHeap::atomic_update_oop(fwd, load_addr, obj);\n@@ -187,7 +187,35 @@\n-template <DecoratorSet decorators>\n-inline void ShenandoahBarrierSet::keep_alive_if_weak(oop value) {\n-  assert((decorators & ON_UNKNOWN_OOP_REF) == 0, \"Reference strength must be known\");\n-  if (!HasDecorator<decorators, ON_STRONG_OOP_REF>::value &&\n-      !HasDecorator<decorators, AS_NO_KEEPALIVE>::value) {\n-    satb_enqueue(value);\n-  }\n+template <typename T>\n+inline oop ShenandoahBarrierSet::oop_load(DecoratorSet decorators, T* addr) {\n+  oop value = RawAccess<>::oop_load(addr);\n+  value = load_reference_barrier(decorators, value, addr);\n+  keep_alive_if_weak(decorators, value);\n+  return value;\n+}\n+\n+template <typename T>\n+inline oop ShenandoahBarrierSet::oop_cmpxchg(DecoratorSet decorators, T* addr, oop compare_value, oop new_value) {\n+  iu_barrier(new_value);\n+  oop res;\n+  oop expected = compare_value;\n+  do {\n+    compare_value = expected;\n+    res = RawAccess<>::oop_atomic_cmpxchg(addr, compare_value, new_value);\n+    expected = res;\n+  } while ((compare_value != expected) && (resolve_forwarded(compare_value) == resolve_forwarded(expected)));\n+\n+  \/\/ Note: We don't need a keep-alive-barrier here. We already enqueue any loaded reference for SATB anyway,\n+  \/\/ because it must be the previous value.\n+  res = load_reference_barrier(decorators, res, reinterpret_cast<T*>(NULL));\n+  satb_enqueue(res);\n+  return res;\n+}\n+\n+template <typename T>\n+inline oop ShenandoahBarrierSet::oop_xchg(DecoratorSet decorators, T* addr, oop new_value) {\n+  iu_barrier(new_value);\n+  oop previous = RawAccess<>::oop_atomic_xchg(addr, new_value);\n+  \/\/ Note: We don't need a keep-alive-barrier here. We already enqueue any loaded reference for SATB anyway,\n+  \/\/ because it must be the previous value.\n+  previous = load_reference_barrier<T>(decorators, previous, reinterpret_cast<T*>(NULL));\n+  satb_enqueue(previous);\n+  return previous;\n@@ -199,7 +227,3 @@\n-  oop value = Raw::oop_load_not_in_heap(addr);\n-  if (value != NULL) {\n-    ShenandoahBarrierSet *const bs = ShenandoahBarrierSet::barrier_set();\n-    value = bs->load_reference_barrier<decorators, T>(value, addr);\n-    bs->keep_alive_if_weak<decorators>(value);\n-  }\n-  return value;\n+  assert((decorators & ON_UNKNOWN_OOP_REF) == 0, \"must be absent\");\n+  ShenandoahBarrierSet* const bs = ShenandoahBarrierSet::barrier_set();\n+  return bs->oop_load(decorators, addr);\n@@ -211,5 +235,3 @@\n-  oop value = Raw::oop_load_in_heap(addr);\n-  ShenandoahBarrierSet *const bs = ShenandoahBarrierSet::barrier_set();\n-  value = bs->load_reference_barrier<decorators, T>(value, addr);\n-  bs->keep_alive_if_weak<decorators>(value);\n-  return value;\n+  assert((decorators & ON_UNKNOWN_OOP_REF) == 0, \"must be absent\");\n+  ShenandoahBarrierSet* const bs = ShenandoahBarrierSet::barrier_set();\n+  return bs->oop_load(decorators, addr);\n@@ -220,2 +242,1 @@\n-  oop value = Raw::oop_load_in_heap_at(base, offset);\n-  ShenandoahBarrierSet *const bs = ShenandoahBarrierSet::barrier_set();\n+  ShenandoahBarrierSet* const bs = ShenandoahBarrierSet::barrier_set();\n@@ -223,3 +244,1 @@\n-  value = bs->load_reference_barrier<decorators>(value, AccessInternal::oop_field_addr<decorators>(base, offset));\n-  bs->keep_alive_if_weak(resolved_decorators, value);\n-  return value;\n+  return bs->oop_load(resolved_decorators, AccessInternal::oop_field_addr<decorators>(base, offset));\n@@ -257,0 +276,1 @@\n+  assert((decorators & (AS_NO_KEEPALIVE | ON_UNKNOWN_OOP_REF)) == 0, \"must be absent\");\n@@ -258,15 +278,1 @@\n-  bs->iu_barrier(new_value);\n-\n-  oop res;\n-  oop expected = compare_value;\n-  do {\n-    compare_value = expected;\n-    res = Raw::oop_atomic_cmpxchg(addr, compare_value, new_value);\n-    expected = res;\n-  } while ((compare_value != expected) && (resolve_forwarded(compare_value) == resolve_forwarded(expected)));\n-\n-  \/\/ Note: We don't need a keep-alive-barrier here. We already enqueue any loaded reference for SATB anyway,\n-  \/\/ because it must be the previous value.\n-  res = ShenandoahBarrierSet::barrier_set()->load_reference_barrier<decorators, T>(res, NULL);\n-  bs->satb_enqueue(res);\n-  return res;\n+  return bs->oop_cmpxchg(decorators, addr, compare_value, new_value);\n@@ -278,1 +284,3 @@\n-  return oop_atomic_cmpxchg_not_in_heap(addr, compare_value, new_value);\n+  assert((decorators & (AS_NO_KEEPALIVE | ON_UNKNOWN_OOP_REF)) == 0, \"must be absent\");\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  return bs->oop_cmpxchg(decorators, addr, compare_value, new_value);\n@@ -283,1 +291,4 @@\n-  return oop_atomic_cmpxchg_in_heap(AccessInternal::oop_field_addr<decorators>(base, offset), compare_value, new_value);\n+  assert((decorators & AS_NO_KEEPALIVE) == 0, \"must be absent\");\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  DecoratorSet resolved_decorators = AccessBarrierSupport::resolve_possibly_unknown_oop_ref_strength<decorators>(base, offset);\n+  return bs->oop_cmpxchg(resolved_decorators, AccessInternal::oop_field_addr<decorators>(base, offset), compare_value, new_value);\n@@ -289,0 +300,1 @@\n+  assert((decorators & (AS_NO_KEEPALIVE | ON_UNKNOWN_OOP_REF)) == 0, \"must be absent\");\n@@ -290,9 +302,1 @@\n-  bs->iu_barrier(new_value);\n-\n-  oop previous = Raw::oop_atomic_xchg(addr, new_value);\n-\n-  \/\/ Note: We don't need a keep-alive-barrier here. We already enqueue any loaded reference for SATB anyway,\n-  \/\/ because it must be the previous value.\n-  previous = ShenandoahBarrierSet::barrier_set()->load_reference_barrier<decorators, T>(previous, NULL);\n-  bs->satb_enqueue(previous);\n-  return previous;\n+  return bs->oop_xchg(decorators, addr, new_value);\n@@ -304,1 +308,3 @@\n-  return oop_atomic_xchg_not_in_heap(addr, new_value);\n+  assert((decorators & (AS_NO_KEEPALIVE | ON_UNKNOWN_OOP_REF)) == 0, \"must be absent\");\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  return bs->oop_xchg(decorators, addr, new_value);\n@@ -309,1 +315,4 @@\n-  return oop_atomic_xchg_in_heap(AccessInternal::oop_field_addr<decorators>(base, offset), new_value);\n+  assert((decorators & AS_NO_KEEPALIVE) == 0, \"must be absent\");\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  DecoratorSet resolved_decorators = AccessBarrierSupport::resolve_possibly_unknown_oop_ref_strength<decorators>(base, offset);\n+  return bs->oop_xchg(resolved_decorators, AccessInternal::oop_field_addr<decorators>(base, offset), new_value);\n@@ -352,1 +361,1 @@\n-        oop witness = ShenandoahHeap::cas_oop(fwd, elem_ptr, o);\n+        ShenandoahHeap::atomic_update_oop(fwd, elem_ptr, o);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":69,"deletions":60,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -75,1 +75,1 @@\n-      live = new (Compile::current()->comp_arena()->Amalloc_D(sizeof(RegMask))) RegMask();\n+      live = new (Compile::current()->comp_arena()->AmallocWords(sizeof(RegMask))) RegMask();\n@@ -197,2 +197,6 @@\n-    if (access.decorators() & ON_WEAK_OOP_REF) {\n-      access.set_barrier_data(ZLoadBarrierWeak);\n+    uint8_t barrier_data = 0;\n+\n+    if (access.decorators() & ON_PHANTOM_OOP_REF) {\n+      barrier_data |= ZLoadBarrierPhantom;\n+    } else if (access.decorators() & ON_WEAK_OOP_REF) {\n+      barrier_data |= ZLoadBarrierWeak;\n@@ -200,1 +204,5 @@\n-      access.set_barrier_data(ZLoadBarrierStrong);\n+      barrier_data |= ZLoadBarrierStrong;\n+    }\n+\n+    if (access.decorators() & AS_NO_KEEPALIVE) {\n+      barrier_data |= ZLoadBarrierNoKeepalive;\n@@ -202,0 +210,2 @@\n+\n+    access.set_barrier_data(barrier_data);\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":15,"deletions":5,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -101,1 +101,0 @@\n-  static oop weak_load_barrier_on_weak_oop_field(volatile oop* p);\n@@ -104,1 +103,0 @@\n-  static oop weak_load_barrier_on_phantom_oop_field(volatile oop* p);\n@@ -120,1 +118,0 @@\n-  static void mark_barrier_on_invisible_root_oop_field(oop* p);\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -198,1 +198,1 @@\n-      throw_array_null_pointer_store_exception(src_obj, dst_obj, Thread::current()->as_Java_thread());\n+      throw_array_null_pointer_store_exception(src_obj, dst_obj, JavaThread::current());\n@@ -204,1 +204,1 @@\n-      throw_array_store_exception(src_obj, dst_obj, Thread::current()->as_Java_thread());\n+      throw_array_store_exception(src_obj, dst_obj, JavaThread::current());\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSet.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -62,1 +62,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -953,3 +952,0 @@\n-  if (PrintBiasedLockingStatistics) {\n-    Atomic::inc(BiasedLocking::slow_path_entry_count_addr());\n-  }\n@@ -1261,21 +1257,0 @@\n-\n-  if (osr_nm != NULL) {\n-    \/\/ We may need to do on-stack replacement which requires that no\n-    \/\/ monitors in the activation are biased because their\n-    \/\/ BasicObjectLocks will need to migrate during OSR. Force\n-    \/\/ unbiasing of all monitors in the activation now (even though\n-    \/\/ the OSR nmethod might be invalidated) because we don't have a\n-    \/\/ safepoint opportunity later once the migration begins.\n-    if (UseBiasedLocking) {\n-      ResourceMark rm;\n-      GrowableArray<Handle>* objects_to_revoke = new GrowableArray<Handle>();\n-      for( BasicObjectLock *kptr = last_frame.monitor_end();\n-           kptr < last_frame.monitor_begin();\n-           kptr = last_frame.next_monitor(kptr) ) {\n-        if( kptr->obj() != NULL ) {\n-          objects_to_revoke->append(Handle(current, kptr->obj()));\n-        }\n-      }\n-      BiasedLocking::revoke(objects_to_revoke, current);\n-    }\n-  }\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":0,"deletions":25,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -574,1 +574,0 @@\n-    assert(!klass->can_be_verified_at_dumptime(), \"only shared old classes aren't rewritten\");\n","filename":"src\/hotspot\/share\/interpreter\/rewriter.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+#include \"runtime\/vframe.inline.hpp\"\n@@ -151,1 +152,1 @@\n-  return thread->as_Java_thread();\n+  return JavaThread::cast(thread);\n@@ -1150,1 +1151,4 @@\n-bool matches(jobjectArray methods, Method* method, JVMCIEnv* JVMCIENV) {\n+\/*\n+ * Used by matches() to convert a ResolvedJavaMethod[] to an array of Method*.\n+ *\/\n+GrowableArray<Method*>* init_resolved_methods(jobjectArray methods, JVMCIEnv* JVMCIENV) {\n@@ -1152,1 +1156,1 @@\n-\n+  GrowableArray<Method*>* resolved_methods = new GrowableArray<Method*>(methods_oop->length());\n@@ -1155,1 +1159,26 @@\n-    if ((resolved->klass() == HotSpotJVMCI::HotSpotResolvedJavaMethodImpl::klass()) && HotSpotJVMCI::asMethod(JVMCIENV, resolved) == method) {\n+    Method* resolved_method = NULL;\n+    if (resolved->klass() == HotSpotJVMCI::HotSpotResolvedJavaMethodImpl::klass()) {\n+      resolved_method = HotSpotJVMCI::asMethod(JVMCIENV, resolved);\n+    }\n+    resolved_methods->append(resolved_method);\n+  }\n+  return resolved_methods;\n+}\n+\n+\/*\n+ * Used by c2v_iterateFrames to check if `method` matches one of the ResolvedJavaMethods in the `methods` array.\n+ * The ResolvedJavaMethod[] array is converted to a Method* array that is then cached in the resolved_methods_ref in\/out parameter.\n+ * In case of a match, the matching ResolvedJavaMethod is returned in matched_jvmci_method_ref.\n+ *\/\n+bool matches(jobjectArray methods, Method* method, GrowableArray<Method*>** resolved_methods_ref, Handle* matched_jvmci_method_ref, Thread* THREAD, JVMCIEnv* JVMCIENV) {\n+  GrowableArray<Method*>* resolved_methods = *resolved_methods_ref;\n+  if (resolved_methods == NULL) {\n+    resolved_methods = init_resolved_methods(methods, JVMCIENV);\n+    *resolved_methods_ref = resolved_methods;\n+  }\n+  assert(method != NULL, \"method should not be NULL\");\n+  assert(resolved_methods->length() == ((objArrayOop) JNIHandles::resolve(methods))->length(), \"arrays must have the same length\");\n+  for (int i = 0; i < resolved_methods->length(); i++) {\n+    Method* m = resolved_methods->at(i);\n+    if (m == method) {\n+      *matched_jvmci_method_ref = Handle(THREAD, ((objArrayOop) JNIHandles::resolve(methods))->obj_at(i));\n@@ -1162,1 +1191,4 @@\n-void call_interface(JavaValue* result, Klass* spec_klass, Symbol* name, Symbol* signature, JavaCallArguments* args, TRAPS) {\n+\/*\n+ * Resolves an interface call to a concrete method handle.\n+ *\/\n+methodHandle resolve_interface_call(Klass* spec_klass, Symbol* name, Symbol* signature, JavaCallArguments* args, TRAPS) {\n@@ -1168,1 +1200,1 @@\n-          callinfo, receiver, recvrKlass, link_info, true, CHECK);\n+          callinfo, receiver, recvrKlass, link_info, true, CHECK_(methodHandle()));\n@@ -1171,0 +1203,25 @@\n+  return method;\n+}\n+\n+\/*\n+ * Used by c2v_iterateFrames to make a new vframeStream at the given compiled frame id (stack pointer) and vframe id.\n+ *\/\n+void resync_vframestream_to_compiled_frame(vframeStream& vfst, intptr_t* stack_pointer, int vframe_id, JavaThread* thread, TRAPS) {\n+  vfst = vframeStream(thread);\n+  while (vfst.frame_id() != stack_pointer && !vfst.at_end()) {\n+    vfst.next();\n+  }\n+  if (vfst.frame_id() != stack_pointer) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalStateException(), \"stack frame not found after deopt\")\n+  }\n+  if (vfst.is_interpreted_frame()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalStateException(), \"compiled stack frame expected\")\n+  }\n+  while (vfst.vframe_id() != vframe_id) {\n+    if (vfst.at_end()) {\n+      THROW_MSG(vmSymbols::java_lang_IllegalStateException(), \"vframe not found after deopt\")\n+    }\n+    vfst.next();\n+    assert(!vfst.is_interpreted_frame(), \"Wrong frame type\");\n+  }\n+}\n@@ -1172,2 +1229,15 @@\n-  \/\/ Invoke the method\n-  JavaCalls::call(result, method, args, CHECK);\n+\/*\n+ * Used by c2v_iterateFrames. Returns an array of any unallocated scope objects or NULL if none.\n+ *\/\n+GrowableArray<ScopeValue*>* get_unallocated_objects_or_null(GrowableArray<ScopeValue*>* scope_objects) {\n+  GrowableArray<ScopeValue*>* unallocated = NULL;\n+  for (int i = 0; i < scope_objects->length(); i++) {\n+    ObjectValue* sv = (ObjectValue*) scope_objects->at(i);\n+    if (sv->value().is_null()) {\n+      if (unallocated == NULL) {\n+        unallocated = new GrowableArray<ScopeValue*>(scope_objects->length());\n+      }\n+      unallocated->append(sv);\n+    }\n+  }\n+  return unallocated;\n@@ -1186,2 +1256,1 @@\n-  Handle frame_reference = HotSpotJVMCI::HotSpotStackFrameReference::klass()->allocate_instance_handle(CHECK_NULL);\n-  StackFrameStream fst(thread, true \/* update *\/, true \/* process_frames *\/);\n+  vframeStream vfst(thread);\n@@ -1190,0 +1259,2 @@\n+  methodHandle visitor_method;\n+  GrowableArray<Method*>* resolved_methods = NULL;\n@@ -1191,5 +1262,1 @@\n-  int frame_number = 0;\n-  vframe* vf = vframe::new_vframe(fst, thread);\n-\n-  while (true) {\n-    \/\/ look for the given method\n+  while (!vfst.at_end()) { \/\/ frame loop\n@@ -1197,27 +1264,47 @@\n-    while (true) {\n-      StackValueCollection* locals = NULL;\n-      if (vf->is_compiled_frame()) {\n-        \/\/ compiled method frame\n-        compiledVFrame* cvf = compiledVFrame::cast(vf);\n-        if (methods == NULL || matches(methods, cvf->method(), JVMCIENV)) {\n-          if (initialSkip > 0) {\n-            initialSkip--;\n-          } else {\n-            ScopeDesc* scope = cvf->scope();\n-            \/\/ native wrappers do not have a scope\n-            if (scope != NULL && scope->objects() != NULL) {\n-              GrowableArray<ScopeValue*>* objects;\n-              if (!realloc_called) {\n-                objects = scope->objects();\n-              } else {\n-                \/\/ some object might already have been re-allocated, only reallocate the non-allocated ones\n-                objects = new GrowableArray<ScopeValue*>(scope->objects()->length());\n-                for (int i = 0; i < scope->objects()->length(); i++) {\n-                  ObjectValue* sv = (ObjectValue*) scope->objects()->at(i);\n-                  if (sv->value().is_null()) {\n-                    objects->append(sv);\n-                  }\n-                }\n-              }\n-              bool realloc_failures = Deoptimization::realloc_objects(thread, fst.current(), fst.register_map(), objects, CHECK_NULL);\n-              Deoptimization::reassign_fields(fst.current(), fst.register_map(), objects, realloc_failures, false, CHECK_NULL);\n+    intptr_t* frame_id = vfst.frame_id();\n+\n+    \/\/ Previous compiledVFrame of this frame; use with at_scope() to reuse scope object pool.\n+    compiledVFrame* prev_cvf = NULL;\n+\n+    for (; !vfst.at_end() && vfst.frame_id() == frame_id; vfst.next()) { \/\/ vframe loop\n+      int frame_number = 0;\n+      Method *method = vfst.method();\n+      int bci = vfst.bci();\n+\n+      Handle matched_jvmci_method;\n+      if (methods == NULL || matches(methods, method, &resolved_methods, &matched_jvmci_method, THREAD, JVMCIENV)) {\n+        if (initialSkip > 0) {\n+          initialSkip--;\n+          continue;\n+        }\n+        javaVFrame* vf;\n+        if (prev_cvf != NULL && prev_cvf->frame_pointer()->id() == frame_id) {\n+          assert(prev_cvf->is_compiled_frame(), \"expected compiled Java frame\");\n+          vf = prev_cvf->at_scope(vfst.decode_offset(), vfst.vframe_id());\n+        } else {\n+          vf = vfst.asJavaVFrame();\n+        }\n+\n+        StackValueCollection* locals = NULL;\n+        typeArrayHandle localIsVirtual_h;\n+        if (vf->is_compiled_frame()) {\n+          \/\/ compiled method frame\n+          compiledVFrame* cvf = compiledVFrame::cast(vf);\n+\n+          ScopeDesc* scope = cvf->scope();\n+          \/\/ native wrappers do not have a scope\n+          if (scope != NULL && scope->objects() != NULL) {\n+            prev_cvf = cvf;\n+\n+            GrowableArray<ScopeValue*>* objects = NULL;\n+            if (!realloc_called) {\n+              objects = scope->objects();\n+            } else {\n+              \/\/ some object might already have been re-allocated, only reallocate the non-allocated ones\n+              objects = get_unallocated_objects_or_null(scope->objects());\n+            }\n+\n+            if (objects != NULL) {\n+              RegisterMap reg_map(vf->register_map());\n+              bool realloc_failures = Deoptimization::realloc_objects(thread, vf->frame_pointer(), &reg_map, objects, CHECK_NULL);\n+              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false, CHECK_NULL);\n@@ -1225,0 +1312,1 @@\n+            }\n@@ -1226,8 +1314,7 @@\n-              GrowableArray<ScopeValue*>* local_values = scope->locals();\n-              assert(local_values != NULL, \"NULL locals\");\n-              typeArrayOop array_oop = oopFactory::new_boolArray(local_values->length(), CHECK_NULL);\n-              typeArrayHandle array(THREAD, array_oop);\n-              for (int i = 0; i < local_values->length(); i++) {\n-                ScopeValue* value = local_values->at(i);\n-                if (value->is_object()) {\n-                  array->bool_at_put(i, true);\n+            GrowableArray<ScopeValue*>* local_values = scope->locals();\n+            for (int i = 0; i < local_values->length(); i++) {\n+              ScopeValue* value = local_values->at(i);\n+              if (value->is_object()) {\n+                if (localIsVirtual_h.is_null()) {\n+                  typeArrayOop array_oop = oopFactory::new_boolArray(local_values->length(), CHECK_NULL);\n+                  localIsVirtual_h = typeArrayHandle(THREAD, array_oop);\n@@ -1235,0 +1322,1 @@\n+                localIsVirtual_h->bool_at_put(i, true);\n@@ -1236,9 +1324,8 @@\n-              HotSpotJVMCI::HotSpotStackFrameReference::set_localIsVirtual(JVMCIENV, frame_reference(), array());\n-            } else {\n-              HotSpotJVMCI::HotSpotStackFrameReference::set_localIsVirtual(JVMCIENV, frame_reference(), NULL);\n-\n-            locals = cvf->locals();\n-            HotSpotJVMCI::HotSpotStackFrameReference::set_bci(JVMCIENV, frame_reference(), cvf->bci());\n-            methodHandle mh(THREAD, cvf->method());\n-            JVMCIObject method = JVMCIENV->get_jvmci_method(mh, JVMCI_CHECK_NULL);\n-            HotSpotJVMCI::HotSpotStackFrameReference::set_method(JVMCIENV, frame_reference(), JNIHandles::resolve(method.as_jobject()));\n+\n+          locals = cvf->locals();\n+          frame_number = cvf->vframe_id();\n+        } else {\n+          \/\/ interpreted method frame\n+          interpretedVFrame* ivf = interpretedVFrame::cast(vf);\n+\n+          locals = ivf->locals();\n@@ -1248,14 +1335,9 @@\n-      } else if (vf->is_interpreted_frame()) {\n-        \/\/ interpreted method frame\n-        interpretedVFrame* ivf = interpretedVFrame::cast(vf);\n-        if (methods == NULL || matches(methods, ivf->method(), JVMCIENV)) {\n-          if (initialSkip > 0) {\n-            initialSkip--;\n-          } else {\n-            locals = ivf->locals();\n-            HotSpotJVMCI::HotSpotStackFrameReference::set_bci(JVMCIENV, frame_reference(), ivf->bci());\n-            methodHandle mh(THREAD, ivf->method());\n-            JVMCIObject method = JVMCIENV->get_jvmci_method(mh, JVMCI_CHECK_NULL);\n-            HotSpotJVMCI::HotSpotStackFrameReference::set_method(JVMCIENV, frame_reference(), JNIHandles::resolve(method.as_jobject()));\n-            HotSpotJVMCI::HotSpotStackFrameReference::set_localIsVirtual(JVMCIENV, frame_reference(), NULL);\n-          }\n+        assert(bci == vf->bci(), \"wrong bci\");\n+        assert(method == vf->method(), \"wrong method\");\n+\n+        Handle frame_reference = HotSpotJVMCI::HotSpotStackFrameReference::klass()->allocate_instance_handle(CHECK_NULL);\n+        HotSpotJVMCI::HotSpotStackFrameReference::set_bci(JVMCIENV, frame_reference(), bci);\n+        if (matched_jvmci_method.is_null()) {\n+          methodHandle mh(THREAD, method);\n+          JVMCIObject jvmci_method = JVMCIENV->get_jvmci_method(mh, JVMCI_CHECK_NULL);\n+          matched_jvmci_method = Handle(THREAD, JNIHandles::resolve(jvmci_method.as_jobject()));\n@@ -1263,1 +1345,2 @@\n-      }\n+        HotSpotJVMCI::HotSpotStackFrameReference::set_method(JVMCIENV, frame_reference(), matched_jvmci_method());\n+        HotSpotJVMCI::HotSpotStackFrameReference::set_localIsVirtual(JVMCIENV, frame_reference(), localIsVirtual_h());\n@@ -1265,4 +1348,1 @@\n-      \/\/ locals != NULL means that we found a matching frame and result is already partially initialized\n-      if (locals != NULL) {\n-        methods = match_methods;\n-        HotSpotJVMCI::HotSpotStackFrameReference::set_stackPointer(JVMCIENV, frame_reference(), (jlong) fst.current()->sp());\n+        HotSpotJVMCI::HotSpotStackFrameReference::set_stackPointer(JVMCIENV, frame_reference(), (jlong) frame_id);\n@@ -1286,0 +1366,4 @@\n+        if (visitor_method.is_null()) {\n+          visitor_method = resolve_interface_call(HotSpotJVMCI::InspectedFrameVisitor::klass(), vmSymbols::visitFrame_name(), vmSymbols::visitFrame_signature(), &args, CHECK_NULL);\n+        }\n+\n@@ -1287,1 +1371,1 @@\n-        call_interface(&result, HotSpotJVMCI::InspectedFrameVisitor::klass(), vmSymbols::visitFrame_name(), vmSymbols::visitFrame_signature(), &args, CHECK_NULL);\n+        JavaCalls::call(&result, visitor_method, &args, CHECK_NULL);\n@@ -1291,0 +1375,6 @@\n+        if (methods == initial_methods) {\n+          methods = match_methods;\n+          if (resolved_methods != NULL && JNIHandles::resolve(match_methods) != JNIHandles::resolve(initial_methods)) {\n+            resolved_methods = NULL;\n+          }\n+        }\n@@ -1294,0 +1384,1 @@\n+          prev_cvf = NULL;\n@@ -1295,18 +1386,1 @@\n-          fst = StackFrameStream(thread, true \/* update *\/, true \/* process_frames *\/);\n-          while (fst.current()->sp() != stack_pointer && !fst.is_done()) {\n-            fst.next();\n-          }\n-          if (fst.current()->sp() != stack_pointer) {\n-            THROW_MSG_NULL(vmSymbols::java_lang_IllegalStateException(), \"stack frame not found after deopt\")\n-          }\n-          vf = vframe::new_vframe(fst, thread);\n-          if (!vf->is_compiled_frame()) {\n-            THROW_MSG_NULL(vmSymbols::java_lang_IllegalStateException(), \"compiled stack frame expected\")\n-          }\n-          for (int i = 0; i < frame_number; i++) {\n-            if (vf->is_top()) {\n-              THROW_MSG_NULL(vmSymbols::java_lang_IllegalStateException(), \"vframe not found after deopt\")\n-            }\n-            vf = vf->sender();\n-            assert(vf->is_compiled_frame(), \"Wrong frame type\");\n-          }\n+          resync_vframestream_to_compiled_frame(vfst, stack_pointer, frame_number, thread, CHECK_NULL);\n@@ -1314,15 +1388,0 @@\n-        frame_reference = HotSpotJVMCI::HotSpotStackFrameReference::klass()->allocate_instance_handle(CHECK_NULL);\n-        HotSpotJVMCI::HotSpotStackFrameReference::klass()->initialize(CHECK_NULL);\n-      }\n-\n-      if (vf->is_top()) {\n-        break;\n-      frame_number++;\n-      vf = vf->sender();\n-\n-    if (fst.is_done()) {\n-      break;\n-    }\n-    fst.next();\n-    vf = vframe::new_vframe(fst, thread);\n-    frame_number = 0;\n@@ -1429,1 +1488,1 @@\n-  while (fst.current()->sp() != stack_pointer && !fst.is_done()) {\n+  while (fst.current()->id() != stack_pointer && !fst.is_done()) {\n@@ -1432,1 +1491,1 @@\n-  if (fst.current()->sp() != stack_pointer) {\n+  if (fst.current()->id() != stack_pointer) {\n@@ -1446,1 +1505,1 @@\n-  while (fstAfterDeopt.current()->sp() != stack_pointer && !fstAfterDeopt.is_done()) {\n+  while (fstAfterDeopt.current()->id() != stack_pointer && !fstAfterDeopt.is_done()) {\n@@ -1449,1 +1508,1 @@\n-  if (fstAfterDeopt.current()->sp() != stack_pointer) {\n+  if (fstAfterDeopt.current()->id() != stack_pointer) {\n@@ -2282,1 +2341,1 @@\n-    attach_args.name = thread->name();\n+    attach_args.name = const_cast<char*>(thread->name());\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":176,"deletions":117,"binary":false,"changes":293,"status":"modified"},{"patch":"@@ -206,1 +206,0 @@\n-  nonstatic_field(Klass,                       _prototype_header,                             markWord)                              \\\n@@ -500,7 +499,0 @@\n-  declare_constant(CollectedHeap::Serial)                                 \\\n-  declare_constant(CollectedHeap::Parallel)                               \\\n-  declare_constant(CollectedHeap::G1)                                     \\\n-  declare_constant(CollectedHeap::Epsilon)                                \\\n-  declare_constant(CollectedHeap::Z)                                      \\\n-  declare_constant(CollectedHeap::Shenandoah)                             \\\n-                                                                          \\\n@@ -653,2 +645,0 @@\n-  declare_constant(markWord::biased_lock_mask_in_place)                   \\\n-  declare_constant(markWord::epoch_mask_in_place)                         \\\n@@ -660,1 +650,0 @@\n-  declare_constant(markWord::biased_lock_pattern)                         \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-  LOG_TAG(biasedlocking) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -127,0 +127,1 @@\n+  f(mtGCCardSet,      \"GCCardSet\")   \/* G1 card set remembered set                *\/ \\\n","filename":"src\/hotspot\/share\/memory\/allocation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"utilities\/hashtable.inline.hpp\"\n+#include \"utilities\/resizeableResourceHash.hpp\"\n@@ -412,1 +412,1 @@\n-  UniqueMetaspaceClosure() : _has_been_visited(INITIAL_TABLE_SIZE) {}\n+  UniqueMetaspaceClosure() : _has_been_visited(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE) {}\n@@ -415,1 +415,2 @@\n-  KVHashtable<address, bool, mtInternal> _has_been_visited;\n+  ResizeableResourceHashtable<address, bool, ResourceObj::C_HEAP,\n+                              mtClassShared> _has_been_visited;\n","filename":"src\/hotspot\/share\/memory\/metaspaceClosure.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -456,1 +456,1 @@\n-        HeapShared::open_archive_heap_region_mapped() &&\n+        HeapShared::open_regions_mapped() &&\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -292,1 +292,1 @@\n-    oop archived = HeapShared::archive_heap_object(rr);\n+    oop archived = HeapShared::archive_object(rr);\n@@ -295,1 +295,1 @@\n-    \/\/ the return value of archive_heap_object here. At runtime, the\n+    \/\/ the return value of archive_object() here. At runtime, the\n@@ -352,1 +352,1 @@\n-    if (HeapShared::open_archive_heap_region_mapped() &&\n+    if (HeapShared::open_regions_mapped() &&\n@@ -375,0 +375,6 @@\n+  \/\/ Shared ConstantPools are in the RO region, so the _flags cannot be modified.\n+  \/\/ The _on_stack flag is used to prevent ConstantPools from deallocation during\n+  \/\/ class redefinition. Since shared ConstantPools cannot be deallocated anyway,\n+  \/\/ we always set _on_stack to true to avoid having to change _flags during runtime.\n+  _flags |= (_on_stack | _is_shared);\n+\n@@ -387,5 +393,0 @@\n-  \/\/ Shared ConstantPools are in the RO region, so the _flags cannot be modified.\n-  \/\/ The _on_stack flag is used to prevent ConstantPools from deallocation during\n-  \/\/ class redefinition. Since shared ConstantPools cannot be deallocated anyway,\n-  \/\/ we always set _on_stack to true to avoid having to change _flags during runtime.\n-  _flags |= (_on_stack | _is_shared);\n@@ -630,1 +631,1 @@\n-      JavaThread* THREAD = current->as_Java_thread(); \/\/ For exception macros.\n+      JavaThread* THREAD = JavaThread::cast(current); \/\/ For exception macros.\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":10,"deletions":9,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,2 +37,0 @@\n-class PSPromotionManager;\n-\n","filename":"src\/hotspot\/share\/oops\/cpCache.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -373,1 +373,1 @@\n-  THREAD->as_Java_thread()->check_possible_safepoint();\n+  JavaThread::cast(THREAD)->check_possible_safepoint();\n","filename":"src\/hotspot\/share\/oops\/flatArrayKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2189,1 +2189,1 @@\n-    _exception = Exceptions::new_exception(current->as_Java_thread(),\n+    _exception = Exceptions::new_exception(JavaThread::cast(current),\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -171,1 +171,1 @@\n-    JavaThread *jt = THREAD->as_Java_thread();\n+    JavaThread *jt = JavaThread::cast(THREAD);\n","filename":"src\/hotspot\/share\/oops\/inlineKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -79,1 +79,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -551,3 +550,3 @@\n-    assert(NULL == _methods, \"underlying memory not zeroed?\");\n-    assert(is_instance_klass(), \"is layout incorrect?\");\n-    assert(size_helper() == parser.layout_size(), \"incorrect size_helper?\");\n+  assert(NULL == _methods, \"underlying memory not zeroed?\");\n+  assert(is_instance_klass(), \"is layout incorrect?\");\n+  assert(size_helper() == parser.layout_size(), \"incorrect size_helper?\");\n@@ -555,5 +554,0 @@\n-  \/\/ Set biased locking bit for all instances of this class; it will be\n-  \/\/ cleared if revocation occurs too often for this type\n-  if (UseBiasedLocking && BiasedLocking::enabled()) {\n-    set_prototype_header(markWord::biased_locking_prototype());\n-  }\n@@ -736,1 +730,1 @@\n-    SystemDictionaryShared::remove_dumptime_info(this);\n+    SystemDictionaryShared::handle_class_unloading(this);\n@@ -1017,0 +1011,3 @@\n+        if (is_shared()) {\n+          assert(!verified_at_dump_time(), \"must be\");\n+        }\n@@ -1048,1 +1045,2 @@\n-      \/\/ 2) the class is loaded by built-in class loader but failed to add archived loader constraints\n+      \/\/ 2) the class is loaded by built-in class loader but failed to add archived loader constraints or\n+      \/\/ 3) the class was not verified during dump time\n@@ -1050,1 +1048,2 @@\n-      if (is_shared() && SystemDictionaryShared::check_linking_constraints(THREAD, this)) {\n+      if (is_shared() && verified_at_dump_time() &&\n+          SystemDictionaryShared::check_linking_constraints(THREAD, this)) {\n@@ -2554,1 +2553,2 @@\n-  if (can_be_verified_at_dumptime()) {\n+  if (is_linked()) {\n+    assert(can_be_verified_at_dumptime(), \"must be\");\n@@ -2694,5 +2694,0 @@\n-  \/\/ Initialize current biased locking state.\n-  if (UseBiasedLocking && BiasedLocking::enabled() && !is_inline_klass()) {\n-    set_prototype_header(markWord::biased_locking_prototype());\n-  }\n-\n@@ -2702,1 +2697,0 @@\n-    set_prototype_header(markWord::prototype());\n@@ -2777,1 +2771,1 @@\n-    SystemDictionaryShared::remove_dumptime_info(ik);\n+    SystemDictionaryShared::handle_class_unloading(ik);\n@@ -3816,1 +3810,1 @@\n-        current->as_Java_thread()->security_get_caller_class(1):\n+        JavaThread::cast(current)->security_get_caller_class(1):\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":15,"deletions":21,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -205,1 +205,1 @@\n-                           _prototype_header(markWord::prototype()),\n+                            _prototype_header(markWord::prototype()),\n@@ -526,3 +526,8 @@\n-  it->push((Klass**)&_subklass);\n-  it->push((Klass**)&_next_sibling);\n-  it->push(&_next_link);\n+  if (!Arguments::is_dumping_archive()) {\n+    \/\/ If dumping archive, these may point to excluded classes. There's no need\n+    \/\/ to follow these pointers anyway, as they will be set to NULL in\n+    \/\/ remove_unshareable_info().\n+    it->push((Klass**)&_subklass);\n+    it->push((Klass**)&_next_sibling);\n+    it->push(&_next_link);\n+  }\n@@ -603,1 +608,1 @@\n-    if (HeapShared::open_archive_heap_region_mapped()) {\n+    if (HeapShared::open_regions_mapped()) {\n@@ -715,4 +720,0 @@\n-int Klass::atomic_incr_biased_lock_revocation_count() {\n-  return (int) Atomic::add(&_biased_lock_revocation_count, 1);\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":10,"deletions":9,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -163,6 +163,1 @@\n-  \/\/ Biased locking implementation and statistics\n-  \/\/ (the 64-bit chunk goes first, to avoid some fragmentation)\n-  jlong    _last_biased_lock_bulk_revocation_time;\n-  markWord _prototype_header;   \/\/ Used when biased locking is both enabled and disabled for this type\n-  jint     _biased_lock_revocation_count;\n-\n+  markWord _prototype_header;  \/\/ inline type and inline array mark patterns\n@@ -678,5 +673,4 @@\n-  \/\/ Biased locking support\n-  \/\/ Note: the prototype header is always set up to be at least the\n-  \/\/ prototype markWord. If biased locking is enabled it may further be\n-  \/\/ biasable and have an epoch.\n-  markWord prototype_header() const     { return _prototype_header; }\n+  \/\/ inline types and inline type array patterns\n+  markWord prototype_header() const {\n+    return _prototype_header;\n+  }\n@@ -687,8 +681,0 @@\n-  \/\/ NOTE: once instances of this klass are floating around in the\n-  \/\/ system, this header must only be updated at a safepoint.\n-  \/\/ NOTE 2: currently we only ever set the prototype header to the\n-  \/\/ biasable prototype for instanceKlasses. There is no technical\n-  \/\/ reason why it could not be done for arrayKlasses aside from\n-  \/\/ wanting to reduce the initial scope of this optimization. There\n-  \/\/ are potential problems in setting the bias pattern for\n-  \/\/ JVM-internal oops.\n@@ -698,7 +684,0 @@\n-  int  biased_lock_revocation_count() const { return (int) _biased_lock_revocation_count; }\n-  \/\/ Atomically increments biased_lock_revocation_count and returns updated value\n-  int atomic_incr_biased_lock_revocation_count();\n-  void set_biased_lock_revocation_count(int val) { _biased_lock_revocation_count = (jint) val; }\n-  jlong last_biased_lock_bulk_revocation_time() { return _last_biased_lock_bulk_revocation_time; }\n-  void  set_last_biased_lock_bulk_revocation_time(jlong cur_time) { _last_biased_lock_bulk_revocation_time = cur_time; }\n-\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":5,"deletions":26,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -56,1 +56,13 @@\n-  assert(!UseBiasedLocking || !header.has_bias_pattern() || is_instance_klass(), \"biased locking currently only supported for Java instances\");\n+  assert(_prototype_header.value() == 0 || _prototype_header == markWord::prototype(),\n+         \"Prototype already set\");\n+#ifdef _LP64\n+    assert(header == markWord::prototype() ||\n+           header.is_inline_type() ||\n+           header.is_flat_array() ||\n+           header.is_null_free_array(),\n+           \"unknown prototype header\");\n+#else\n+    assert(header == markWord::prototype() ||\n+           header.is_inline_type(),\n+           \"unknown prototype header\");\n+#endif\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,2 +39,1 @@\n-\/\/             hash:25 ------------>| age:4    biased_lock:1 lock:2 (normal object)\n-\/\/             JavaThread*:23 epoch:2 age:4    biased_lock:1 lock:2 (biased object)\n+\/\/             hash:25 ------------>| age:4  unused_gap:1  lock:2 (normal object)\n@@ -44,2 +43,1 @@\n-\/\/  unused:25 hash:31 -->| unused_gap:1   age:4    biased_lock:1 lock:2 (normal object)\n-\/\/  JavaThread*:54 epoch:2 unused_gap:1   age:4    biased_lock:1 lock:2 (biased object)\n+\/\/  unused:25 hash:31 -->| unused_gap:1  age:4  unused_gap:1  lock:2 (normal object)\n@@ -52,28 +50,0 @@\n-\/\/  - the biased lock pattern is used to bias a lock toward a given\n-\/\/    thread. When this pattern is set in the low three bits, the lock\n-\/\/    is either biased toward a given thread or \"anonymously\" biased,\n-\/\/    indicating that it is possible for it to be biased. When the\n-\/\/    lock is biased toward a given thread, locking and unlocking can\n-\/\/    be performed by that thread without using atomic operations.\n-\/\/    When a lock's bias is revoked, it reverts back to the normal\n-\/\/    locking scheme described below.\n-\/\/\n-\/\/    Note that we are overloading the meaning of the \"unlocked\" state\n-\/\/    of the header. Because we steal a bit from the age we can\n-\/\/    guarantee that the bias pattern will never be seen for a truly\n-\/\/    unlocked object.\n-\/\/\n-\/\/    Note also that the biased state contains the age bits normally\n-\/\/    contained in the object header. Large increases in scavenge\n-\/\/    times were seen when these bits were absent and an arbitrary age\n-\/\/    assigned to all biased objects, because they tended to consume a\n-\/\/    significant fraction of the eden semispaces and were not\n-\/\/    promoted promptly, causing an increase in the amount of copying\n-\/\/    performed. The runtime system aligns all JavaThread* pointers to\n-\/\/    a very large value (currently 128 bytes (32bVM) or 256 bytes (64bVM))\n-\/\/    to make room for the age bits & the epoch bits (used in support of\n-\/\/    biased locking).\n-\/\/\n-\/\/    [JavaThread* | epoch | age | 1 | 01]       lock is biased toward given thread\n-\/\/    [0           | epoch | age | 1 | 01]       lock is anonymously biased\n-\/\/\n@@ -83,1 +53,1 @@\n-\/\/    [header      | 0 | 01]  unlocked           regular object header\n+\/\/    [header          | 01]  unlocked           regular object header\n@@ -111,5 +81,1 @@\n-\/\/  UseBiasedLocking \/ EnableValhalla\n-\/\/\n-\/\/  Making the assumption that \"biased locking\" will be removed before inline types\n-\/\/  are introduced to mainline. However, to ease future merge conflict work, left\n-\/\/  bias locking code in place for now. UseBiasedLocking cannot be enabled.\n+\/\/  EnableValhalla\n@@ -117,4 +83,4 @@\n-\/\/  \"biased lock bit\" is free to use: using this bit to indicate inline type,\n-\/\/  combined with \"unlocked\" lock bits, means we will not interfere with lock\n-\/\/  encodings (displaced, inflating, and monitor), since inline types can't be\n-\/\/  locked.\n+\/\/  Formerly known as \"biased lock bit\", \"unused_gap\" is free to use: using this\n+\/\/  bit to indicate inline type, combined with \"unlocked\" lock bits, means we\n+\/\/  will not interfere with lock encodings (displaced, inflating, and monitor),\n+\/\/  since inline types can't be locked.\n@@ -199,2 +165,2 @@\n-  static const int biased_lock_bits               = 1; \/\/ Valhalla: unused\n-  \/\/ static prototype header bits (fast path instead of klass layout_helper)\n+  static const int first_unused_gap_bits          = 1; \/\/ When !EnableValhalla\n+  \/\/ EnableValhalla: static prototype header bits (fast path instead of klass layout_helper)\n@@ -209,2 +175,1 @@\n-  static const int unused_gap_bits                = LP64_ONLY(1) NOT_LP64(0); \/\/ Valhalla: unused\n-  static const int epoch_bits                     = 2; \/\/ Valhalla: unused\n+  static const int second_unused_gap_bits         = LP64_ONLY(1) NOT_LP64(0); \/\/ !EnableValhalla: unused\n@@ -212,3 +177,0 @@\n-  \/\/ The biased locking code currently requires that the age bits be\n-  \/\/ contiguous to the lock bits.\n-  static const int biased_lock_shift              = lock_bits;\n@@ -219,1 +181,1 @@\n-  static const int unused_gap_shift               = age_shift + age_bits; \/\/ Valhalla: unused\n+  static const int unused_gap_shift               = age_shift + age_bits; \/\/ !EnableValhalla: unused\n@@ -222,1 +184,0 @@\n-  static const int epoch_shift                    = unused_gap_shift + unused_gap_bits \/*hash_shift*\/; \/\/ Valhalla: unused\n@@ -227,3 +188,0 @@\n-  static const uintptr_t biased_lock_mask         = right_n_bits(lock_bits + biased_lock_bits); \/\/ Valhalla: unused\n-  static const uintptr_t biased_lock_mask_in_place= biased_lock_mask << lock_shift; \/\/ Valhalla: unused\n-  static const uintptr_t biased_lock_bit_in_place = 1 << biased_lock_shift; \/\/ Valhalla: unused\n@@ -247,3 +205,0 @@\n-  static const uintptr_t epoch_mask               = right_n_bits(epoch_bits); \/\/ Valhalla: unused\n-  static const uintptr_t epoch_mask_in_place      = epoch_mask << epoch_shift;\/\/ Valhalla: unused\n-\n@@ -253,3 +208,0 @@\n-  \/\/ Alignment of JavaThread pointers encoded in object header required by biased locking\n-  static const size_t biased_lock_alignment       = 2 << (epoch_shift + epoch_bits); \/\/ Valhalla: unused\n-\n@@ -260,1 +212,0 @@\n-  static const uintptr_t biased_lock_pattern      = 5; \/\/ Valhalla: unused\n@@ -264,0 +215,1 @@\n+  \/\/ Has static klass prototype, used for decode\/encode pointer\n@@ -277,2 +229,0 @@\n-  static const int max_bias_epoch                 = epoch_mask;\n-\n@@ -286,45 +236,0 @@\n-  \/\/ Biased Locking accessors.\n-  \/\/ These must be checked by all code which calls into the\n-  \/\/ ObjectSynchronizer and other code. The biasing is not understood\n-  \/\/ by the lower-level CAS-based locking code, although the runtime\n-  \/\/ fixes up biased locks to be compatible with it when a bias is\n-  \/\/ revoked.\n-  bool has_bias_pattern() const {\n-    ShouldNotReachHere(); \/\/ Valhalla: unused\n-    return (mask_bits(value(), biased_lock_mask_in_place) == biased_lock_pattern);\n-  }\n-  JavaThread* biased_locker() const {\n-    ShouldNotReachHere(); \/\/ Valhalla: unused\n-    assert(has_bias_pattern(), \"should not call this otherwise\");\n-    return (JavaThread*) mask_bits(value(), ~(biased_lock_mask_in_place | age_mask_in_place | epoch_mask_in_place));\n-  }\n-  \/\/ Indicates that the mark has the bias bit set but that it has not\n-  \/\/ yet been biased toward a particular thread\n-  bool is_biased_anonymously() const {\n-    ShouldNotReachHere(); \/\/ Valhalla: unused\n-    return (has_bias_pattern() && (biased_locker() == NULL));\n-  }\n-  \/\/ Indicates epoch in which this bias was acquired. If the epoch\n-  \/\/ changes due to too many bias revocations occurring, the biases\n-  \/\/ from the previous epochs are all considered invalid.\n-  int bias_epoch() const {\n-    ShouldNotReachHere(); \/\/ Valhalla: unused\n-    assert(has_bias_pattern(), \"should not call this otherwise\");\n-    return (mask_bits(value(), epoch_mask_in_place) >> epoch_shift);\n-  }\n-  markWord set_bias_epoch(int epoch) {\n-    ShouldNotReachHere(); \/\/ Valhalla: unused\n-    assert(has_bias_pattern(), \"should not call this otherwise\");\n-    assert((epoch & (~epoch_mask)) == 0, \"epoch overflow\");\n-    return markWord(mask_bits(value(), ~epoch_mask_in_place) | (epoch << epoch_shift));\n-  }\n-  markWord incr_bias_epoch() {\n-    ShouldNotReachHere(); \/\/ Valhalla: unused\n-    return set_bias_epoch((1 + bias_epoch()) & epoch_mask);\n-  }\n-  \/\/ Prototype mark for initialization\n-  static markWord biased_locking_prototype() {\n-    ShouldNotReachHere(); \/\/ Valhalla: unused\n-    return markWord( biased_lock_pattern );\n-  }\n-\n@@ -344,1 +249,3 @@\n-  bool is_neutral()  const { return (mask_bits(value(), inline_type_mask_in_place) == unlocked_value); }\n+  bool is_neutral()  const {\n+    return (mask_bits(value(), inline_type_mask_in_place) == unlocked_value);\n+  }\n@@ -359,1 +266,3 @@\n-  inline bool must_be_preserved(const oopDesc* obj) const;\n+  bool must_be_preserved(const oopDesc* obj) const {\n+    return (!is_unlocked() || !has_no_hash() || (EnableValhalla && is_larval_state()));\n+  }\n@@ -363,16 +272,3 @@\n-  \/\/ Note that we special case this situation. We want to avoid\n-  \/\/ calling BiasedLocking::preserve_marks()\/restore_marks() (which\n-  \/\/ decrease the number of mark words that need to be preserved\n-  \/\/ during GC) during each scavenge. During scavenges in which there\n-  \/\/ is no promotion failure, we actually don't need to call the above\n-  \/\/ routines at all, since we don't mutate and re-initialize the\n-  \/\/ marks of promoted objects using init_mark(). However, during\n-  \/\/ scavenges which result in promotion failure, we do re-initialize\n-  \/\/ the mark words of objects, meaning that we should have called\n-  \/\/ these mark word preservation routines. Currently there's no good\n-  \/\/ place in which to call them in any of the scavengers (although\n-  \/\/ guarded by appropriate locks we could make one), but the\n-  \/\/ observation is that promotion failures are quite rare and\n-  \/\/ reducing the number of mark words preserved during them isn't a\n-  \/\/ high priority.\n-  inline bool must_be_preserved_for_promotion_failure(const oopDesc* obj) const;\n+  bool must_be_preserved_for_promotion_failure(const oopDesc* obj) const {\n+    return (!is_unlocked() || !has_no_hash() || (EnableValhalla && is_larval_state()));\n+  }\n@@ -425,7 +321,0 @@\n-  static markWord encode(JavaThread* thread, uint age, int bias_epoch) {\n-    uintptr_t tmp = (uintptr_t) thread;\n-    assert(UseBiasedLocking && ((tmp & (epoch_mask_in_place | age_mask_in_place | biased_lock_mask_in_place)) == 0), \"misaligned JavaThread pointer\");\n-    assert(age <= max_age, \"age too large\");\n-    assert(bias_epoch <= max_bias_epoch, \"bias epoch too large\");\n-    return markWord(tmp | (bias_epoch << epoch_shift) | (age << age_shift) | biased_lock_pattern);\n-  }\n@@ -504,2 +393,0 @@\n-    \/\/ Helper function for restoration of unmarked mark oops during GC\n-  static inline markWord prototype_for_klass(const Klass* klass);\n@@ -514,1 +401,4 @@\n-  inline void* decode_pointer() { return (EnableValhalla && _value < static_prototype_value_max) ? NULL : (void*) (clear_lock_bits().value()); }\n+  inline void* decode_pointer() {\n+    return (EnableValhalla && _value < static_prototype_value_max) ? NULL :\n+      (void*) (clear_lock_bits().value());\n+  }\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":27,"deletions":137,"binary":false,"changes":164,"status":"modified"},{"patch":"@@ -626,1 +626,1 @@\n-    JavaThread* THREAD = current->as_Java_thread(); \/\/ For exception macros.\n+    JavaThread* THREAD = JavaThread::cast(current); \/\/ For exception macros.\n@@ -691,1 +691,1 @@\n-    k = ss.as_klass(class_loader, protection_domain, SignatureStream::ReturnNull, thread->as_Java_thread());\n+    k = ss.as_klass(class_loader, protection_domain, SignatureStream::ReturnNull, JavaThread::cast(thread));\n@@ -2323,1 +2323,1 @@\n-  if (o == NULL || o == JNIMethodBlock::_free_method || !((Metadata*)o)->is_method()) {\n+  if (o == NULL || o == JNIMethodBlock::_free_method) {\n@@ -2326,1 +2326,6 @@\n-  return o;\n+  \/\/ Method should otherwise be valid. Assert for testing.\n+  assert(is_valid_method(o), \"should be valid jmethodid\");\n+  \/\/ If the method's class holder object is unreferenced, but not yet marked as\n+  \/\/ unloaded, we need to return NULL here too because after a safepoint, its memory\n+  \/\/ will be reclaimed.\n+  return o->method_holder()->is_loader_alive() ? o : NULL;\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -26,1 +26,0 @@\n-#include \"cds\/heapShared.inline.hpp\"\n@@ -29,0 +28,2 @@\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -223,1 +224,1 @@\n-  assert(!HeapShared::is_archived_object(forwardee) && !HeapShared::is_archived_object(this),\n+  assert(!Universe::heap()->is_archived_object(forwardee) && !Universe::heap()->is_archived_object(this),\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -251,1 +251,0 @@\n-  inline bool has_bias_pattern() const;\n@@ -301,2 +300,0 @@\n-  \/\/ NOTE with the introduction of UseBiasedLocking that identity_hash() might reach a\n-  \/\/ safepoint if called on a biased object. Calling code must be aware of that.\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-#include \"oops\/markWord.inline.hpp\"\n+#include \"oops\/markWord.hpp\"\n@@ -78,1 +78,1 @@\n-  set_mark(markWord::prototype_for_klass(klass()));\n+  set_mark(Klass::default_prototype_header(klass()));\n@@ -272,4 +272,0 @@\n-bool oopDesc::has_bias_pattern() const {\n-  return mark().has_bias_pattern();\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -92,1 +92,1 @@\n-  address res = (address)arena->Amalloc_4(alloc_size);\n+  address res = (address)arena->AmallocWords(alloc_size);\n","filename":"src\/hotspot\/share\/oops\/symbol.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -158,1 +158,1 @@\n-    unsigned addr_bits = (unsigned)((uintptr_t)this >> (LogMinObjAlignmentInBytes + 3));\n+    unsigned addr_bits = (unsigned)((uintptr_t)this >> (LogBytesPerWord + 3));\n","filename":"src\/hotspot\/share\/oops\/symbol.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -324,0 +324,34 @@\n+  \/\/ Associative\n+  if (op1 == Op_MulI && op2 == Op_MulI) {\n+    Node* add_in1 = NULL;\n+    Node* add_in2 = NULL;\n+    Node* mul_in = NULL;\n+\n+    if (in1->in(1) == in2->in(1)) {\n+      \/\/ Convert \"a*b+a*c into a*(b+c)\n+      add_in1 = in1->in(2);\n+      add_in2 = in2->in(2);\n+      mul_in = in1->in(1);\n+    } else if (in1->in(2) == in2->in(1)) {\n+      \/\/ Convert a*b+b*c into b*(a+c)\n+      add_in1 = in1->in(1);\n+      add_in2 = in2->in(2);\n+      mul_in = in1->in(2);\n+    } else if (in1->in(2) == in2->in(2)) {\n+      \/\/ Convert a*c+b*c into (a+b)*c\n+      add_in1 = in1->in(1);\n+      add_in2 = in2->in(1);\n+      mul_in = in1->in(2);\n+    } else if (in1->in(1) == in2->in(2)) {\n+      \/\/ Convert a*b+c*a into a*(b+c)\n+      add_in1 = in1->in(2);\n+      add_in2 = in2->in(1);\n+      mul_in = in1->in(1);\n+    }\n+\n+    if (mul_in != NULL) {\n+      Node* add = phase->transform(new AddINode(add_in1, add_in2));\n+      return new MulINode(mul_in, add);\n+    }\n+  }\n+\n@@ -472,0 +506,34 @@\n+  \/\/ Associative\n+  if (op1 == Op_MulL && op2 == Op_MulL) {\n+    Node* add_in1 = NULL;\n+    Node* add_in2 = NULL;\n+    Node* mul_in = NULL;\n+\n+    if (in1->in(1) == in2->in(1)) {\n+      \/\/ Convert \"a*b+a*c into a*(b+c)\n+      add_in1 = in1->in(2);\n+      add_in2 = in2->in(2);\n+      mul_in = in1->in(1);\n+    } else if (in1->in(2) == in2->in(1)) {\n+      \/\/ Convert a*b+b*c into b*(a+c)\n+      add_in1 = in1->in(1);\n+      add_in2 = in2->in(2);\n+      mul_in = in1->in(2);\n+    } else if (in1->in(2) == in2->in(2)) {\n+      \/\/ Convert a*c+b*c into (a+b)*c\n+      add_in1 = in1->in(1);\n+      add_in2 = in2->in(1);\n+      mul_in = in1->in(2);\n+    } else if (in1->in(1) == in2->in(2)) {\n+      \/\/ Convert a*b+c*a into a*(b+c)\n+      add_in1 = in1->in(2);\n+      add_in2 = in2->in(1);\n+      mul_in = in1->in(1);\n+    }\n+\n+    if (mul_in != NULL) {\n+      Node* add = phase->transform(new AddLNode(add_in1, add_in2));\n+      return new MulLNode(mul_in, add);\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":68,"deletions":0,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -207,2 +207,8 @@\n-    assert(!ik->is_interface() && !ik->has_subklass(), \"inconsistent klass hierarchy\");\n-    phase->C->dependencies()->assert_leaf_type(ik);\n+    assert(!ik->is_interface(), \"inconsistent klass hierarchy\");\n+    if (ik->has_subklass()) {\n+      \/\/ Concurrent class loading.\n+      \/\/ Fail fast and return NodeSentinel to indicate that the transform failed.\n+      return NodeSentinel;\n+    } else {\n+      phase->C->dependencies()->assert_leaf_type(ik);\n+    }\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -469,2 +469,5 @@\n-          if( OptoReg::is_valid(first) ) set_live_bit(tmp_live,first);\n-          if( OptoReg::is_valid(second) ) set_live_bit(tmp_live,second);\n+          \/\/If peephole had removed the node,do not set live bit for it.\n+          if (!(def->is_Mach() && def->as_Mach()->get_removed())) {\n+            if (OptoReg::is_valid(first)) set_live_bit(tmp_live,first);\n+            if (OptoReg::is_valid(second)) set_live_bit(tmp_live,second);\n+          }\n","filename":"src\/hotspot\/share\/opto\/buildOopMap.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -290,7 +290,11 @@\n-  develop_pd(intx, FLOATPRESSURE,                                           \\\n-          \"Number of float LRG's that constitute high register pressure\")   \\\n-          range(0, max_jint)                                                \\\n-                                                                            \\\n-  develop_pd(intx, INTPRESSURE,                                             \\\n-          \"Number of integer LRG's that constitute high register pressure\") \\\n-          range(0, max_jint)                                                \\\n+  develop(intx, FLOATPRESSURE, -1,                                          \\\n+          \"Number of float LRG's that constitute high register pressure.\"   \\\n+          \"-1: means the threshold is determined by number of available \"   \\\n+          \"float register for allocation\")                                  \\\n+          range(-1, max_jint)                                               \\\n+                                                                            \\\n+  develop(intx, INTPRESSURE, -1,                                            \\\n+          \"Number of integer LRG's that constitute high register pressure.\" \\\n+          \"-1: means the threshold is determined by number of available \"   \\\n+          \"integer register for allocation\")                                \\\n+          range(-1, max_jint)                                               \\\n@@ -451,4 +455,0 @@\n-  product(bool, PrintPreciseBiasedLockingStatistics, false, DIAGNOSTIC,     \\\n-          \"(Deprecated) Print per-lock-site statistics of biased locking \"  \\\n-          \"in JVM\")                                                         \\\n-                                                                            \\\n@@ -510,3 +510,0 @@\n-  product(bool, UseOptoBiasInlining, true,                                  \\\n-          \"(Deprecated) Generate biased locking code in C2 ideal graph\")    \\\n-                                                                            \\\n@@ -776,0 +773,3 @@\n+                                                                            \\\n+  product(bool, VerifyReceiverTypes, trueInDebug, DIAGNOSTIC,               \\\n+          \"Verify receiver types at runtime\")                               \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":14,"deletions":14,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -545,0 +545,4 @@\n+  if (!allow_inline && _callee->holder()->is_interface()) {\n+    \/\/ Don't convert the interface call to a direct call guarded by an interface subtype check.\n+    return false;\n+  }\n@@ -1052,1 +1056,1 @@\n-    \/\/ Instance exactly does not matches the desired type.\n+    \/\/ Instance does not match the predicted type.\n@@ -1057,1 +1061,1 @@\n-  \/\/ fall through if the instance exactly matches the desired type\n+  \/\/ Fall through if the instance matches the desired type.\n@@ -1178,1 +1182,1 @@\n-    sig_type = sig_type->join_speculative(TypePtr::NOTNULL);\n+    sig_type = sig_type->filter_speculative(TypePtr::NOTNULL);\n@@ -1181,1 +1185,1 @@\n-    const Type* narrowed_arg_type = arg_type->join_speculative(sig_type); \/\/ keep speculative part\n+    const Type* narrowed_arg_type = arg_type->filter_speculative(sig_type); \/\/ keep speculative part\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1615,2 +1615,8 @@\n-  if( in(TypeFunc::Control)->is_SafePoint() )\n-    return in(TypeFunc::Control);\n+  if (in(TypeFunc::Control)->is_SafePoint()) {\n+    Node* out_c = unique_ctrl_out();\n+    \/\/ This can be the safepoint of an outer strip mined loop if the inner loop's backedge was removed. Replacing the\n+    \/\/ outer loop's safepoint could confuse removal of the outer loop.\n+    if (out_c != NULL && !out_c->is_OuterStripMinedLoopEnd()) {\n+      return in(TypeFunc::Control);\n+    }\n+  }\n@@ -1973,1 +1979,1 @@\n-      length->set_req(0, init->proj_out_or_null(0));\n+      length->set_req(TypeFunc::Control, init->proj_out_or_null(TypeFunc::Control));\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-  if (_carry_dependency) {\n+  if (_dependency != RegularDependency) {\n@@ -88,1 +88,1 @@\n-  return TypeNode::cmp(n) && ((ConstraintCastNode&)n)._carry_dependency == _carry_dependency;\n+  return TypeNode::cmp(n) && ((ConstraintCastNode&)n)._dependency == _dependency;\n@@ -95,1 +95,1 @@\n-Node* ConstraintCastNode::make_cast(int opcode, Node* c, Node *n, const Type *t, bool carry_dependency) {\n+Node* ConstraintCastNode::make_cast(int opcode, Node* c, Node *n, const Type *t, DependencyType dependency) {\n@@ -98,1 +98,1 @@\n-    Node* cast = new CastIINode(n, t, carry_dependency);\n+    Node* cast = new CastIINode(n, t, dependency);\n@@ -103,1 +103,1 @@\n-    Node* cast = new CastLLNode(n, t, carry_dependency);\n+    Node* cast = new CastLLNode(n, t, dependency);\n@@ -108,1 +108,1 @@\n-    Node* cast = new CastPPNode(n, t, carry_dependency);\n+    Node* cast = new CastPPNode(n, t, dependency);\n@@ -113,1 +113,1 @@\n-    Node* cast = new CastFFNode(n, t, carry_dependency);\n+    Node* cast = new CastFFNode(n, t, dependency);\n@@ -118,1 +118,1 @@\n-    Node* cast = new CastDDNode(n, t, carry_dependency);\n+    Node* cast = new CastDDNode(n, t, dependency);\n@@ -123,1 +123,1 @@\n-    Node* cast = new CastVVNode(n, t, carry_dependency);\n+    Node* cast = new CastVVNode(n, t, dependency);\n@@ -127,1 +127,1 @@\n-  case Op_CheckCastPP: return new CheckCastPPNode(c, n, t, carry_dependency);\n+  case Op_CheckCastPP: return new CheckCastPPNode(c, n, t, dependency);\n@@ -137,1 +137,1 @@\n-    return make_cast(Op_CastII, c, n, t, false);\n+    return make_cast(Op_CastII, c, n, t, RegularDependency);\n@@ -140,1 +140,1 @@\n-    return make_cast(Op_CastLL, c, n, t, false);\n+    return make_cast(Op_CastLL, c, n, t, RegularDependency);\n@@ -149,0 +149,3 @@\n+  if (_dependency == UnconditionalDependency) {\n+    return NULL;\n+  }\n@@ -189,2 +192,2 @@\n-  if (_carry_dependency) {\n-    st->print(\" carry dependency\");\n+  if (_dependency != RegularDependency) {\n+    st->print(\" %s dependency\", _dependency == StrongDependency ? \"strong\" : \"unconditional\");\n@@ -200,1 +203,1 @@\n-  if (_carry_dependency) {\n+  if (_dependency != RegularDependency) {\n@@ -261,2 +264,2 @@\n-static Node* find_or_make_CastII(PhaseIterGVN* igvn, Node* parent, Node* control, const TypeInt* type, bool carry_dependency) {\n-  Node* n = new CastIINode(parent, type, carry_dependency);\n+static Node* find_or_make_CastII(PhaseIterGVN* igvn, Node* parent, Node* control, const TypeInt* type, ConstraintCastNode::DependencyType dependency) {\n+  Node* n = new CastIINode(parent, type, dependency);\n@@ -295,2 +298,2 @@\n-    Node* cx = find_or_make_CastII(igvn, x, in(0), rx->is_int(), _carry_dependency);\n-    Node* cy = find_or_make_CastII(igvn, y, in(0), ry->is_int(), _carry_dependency);\n+    Node* cx = find_or_make_CastII(igvn, x, in(0), rx->is_int(), _dependency);\n+    Node* cy = find_or_make_CastII(igvn, y, in(0), ry->is_int(), _dependency);\n@@ -386,1 +389,1 @@\n-  if (_carry_dependency) {\n+  if (_dependency != RegularDependency) {\n@@ -607,1 +610,1 @@\n-Node* ConstraintCastNode::make_cast_for_type(Node* c, Node* in, const Type* type) {\n+Node* ConstraintCastNode::make_cast_for_type(Node* c, Node* in, const Type* type, DependencyType dependency) {\n@@ -610,1 +613,1 @@\n-    cast = make_cast(Op_CastII, c, in, type, true);\n+    cast = make_cast(Op_CastII, c, in, type, dependency);\n@@ -612,1 +615,1 @@\n-    cast = make_cast(Op_CastLL, c, in, type, true);\n+    cast = make_cast(Op_CastLL, c, in, type, dependency);\n@@ -614,1 +617,1 @@\n-    cast = make_cast(Op_CastFF, c, in, type, true);\n+    cast = make_cast(Op_CastFF, c, in, type, dependency);\n@@ -616,1 +619,1 @@\n-    cast = make_cast(Op_CastDD, c, in, type, true);\n+    cast = make_cast(Op_CastDD, c, in, type, dependency);\n@@ -618,1 +621,1 @@\n-    cast = make_cast(Op_CastVV, c, in, type, true);\n+    cast = make_cast(Op_CastVV, c, in, type, dependency);\n@@ -620,1 +623,1 @@\n-    cast = make_cast(Op_CastPP, c, in, type, true);\n+    cast = make_cast(Op_CastPP, c, in, type, dependency);\n","filename":"src\/hotspot\/share\/opto\/castnode.cpp","additions":30,"deletions":27,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -35,0 +35,7 @@\n+public:\n+  enum DependencyType {\n+    RegularDependency, \/\/ if cast doesn't improve input type, cast can be removed\n+    StrongDependency,  \/\/ leave cast in even if _type doesn't improve input type, can be replaced by stricter dominating cast if one exist\n+    UnconditionalDependency \/\/ leave cast in unconditionally\n+  };\n+\n@@ -36,2 +43,1 @@\n-  \/\/ Can this node be removed post CCP or does it carry a required dependency?\n-  const bool _carry_dependency;\n+  const DependencyType _dependency;\n@@ -42,2 +48,2 @@\n-  ConstraintCastNode(Node *n, const Type *t, bool carry_dependency)\n-    : TypeNode(t,2), _carry_dependency(carry_dependency) {\n+  ConstraintCastNode(Node *n, const Type *t, DependencyType dependency)\n+    : TypeNode(t,2), _dependency(dependency) {\n@@ -52,2 +58,2 @@\n-  virtual bool depends_only_on_test() const { return !_carry_dependency; }\n-  bool carry_dependency() const { return _carry_dependency; }\n+  virtual bool depends_only_on_test() const { return _dependency == RegularDependency; }\n+  bool carry_dependency() const { return _dependency != RegularDependency; }\n@@ -55,1 +61,1 @@\n-  static Node* make_cast(int opcode,  Node* c, Node *n, const Type *t, bool carry_dependency);\n+  static Node* make_cast(int opcode, Node* c, Node *n, const Type *t, DependencyType dependency);\n@@ -66,1 +72,1 @@\n-  static Node* make_cast_for_type(Node* c, Node* in, const Type* type);\n+  static Node* make_cast_for_type(Node* c, Node* in, const Type* type, DependencyType dependency);\n@@ -79,2 +85,2 @@\n-  CastIINode(Node* n, const Type* t, bool carry_dependency = false, bool range_check_dependency = false)\n-    : ConstraintCastNode(n, t, carry_dependency), _range_check_dependency(range_check_dependency) {\n+  CastIINode(Node* n, const Type* t, DependencyType dependency = RegularDependency, bool range_check_dependency = false)\n+    : ConstraintCastNode(n, t, dependency), _range_check_dependency(range_check_dependency) {\n@@ -83,2 +89,2 @@\n-  CastIINode(Node* ctrl, Node* n, const Type* t, bool carry_dependency = false, bool range_check_dependency = false)\n-    : ConstraintCastNode(n, t, carry_dependency), _range_check_dependency(range_check_dependency) {\n+  CastIINode(Node* ctrl, Node* n, const Type* t, DependencyType dependency = RegularDependency, bool range_check_dependency = false)\n+    : ConstraintCastNode(n, t, dependency), _range_check_dependency(range_check_dependency) {\n@@ -113,2 +119,2 @@\n-  CastLLNode(Node* ctrl, Node* n, const Type* t, bool carry_dependency = false)\n-    : ConstraintCastNode(n, t, carry_dependency) {\n+  CastLLNode(Node* ctrl, Node* n, const Type* t, DependencyType dependency = RegularDependency)\n+    : ConstraintCastNode(n, t, dependency) {\n@@ -118,2 +124,2 @@\n-  CastLLNode(Node* n, const Type* t, bool carry_dependency = false)\n-          : ConstraintCastNode(n, t, carry_dependency){\n+  CastLLNode(Node* n, const Type* t, DependencyType dependency = RegularDependency)\n+          : ConstraintCastNode(n, t, dependency){\n@@ -132,2 +138,2 @@\n-  CastFFNode(Node* n, const Type* t, bool carry_dependency = false)\n-          : ConstraintCastNode(n, t, carry_dependency){\n+  CastFFNode(Node* n, const Type* t, DependencyType dependency = RegularDependency)\n+          : ConstraintCastNode(n, t, dependency){\n@@ -142,2 +148,2 @@\n-  CastDDNode(Node* n, const Type* t, bool carry_dependency = false)\n-          : ConstraintCastNode(n, t, carry_dependency){\n+  CastDDNode(Node* n, const Type* t, DependencyType dependency = RegularDependency)\n+          : ConstraintCastNode(n, t, dependency){\n@@ -152,2 +158,2 @@\n-  CastVVNode(Node* n, const Type* t, bool carry_dependency = false)\n-          : ConstraintCastNode(n, t, carry_dependency){\n+  CastVVNode(Node* n, const Type* t, DependencyType dependency = RegularDependency)\n+          : ConstraintCastNode(n, t, dependency){\n@@ -165,2 +171,2 @@\n-  CastPPNode (Node *n, const Type *t, bool carry_dependency = false)\n-    : ConstraintCastNode(n, t, carry_dependency) {\n+  CastPPNode (Node *n, const Type *t, DependencyType dependency = RegularDependency)\n+    : ConstraintCastNode(n, t, dependency) {\n@@ -176,2 +182,2 @@\n-  CheckCastPPNode(Node *c, Node *n, const Type *t, bool carry_dependency = false)\n-    : ConstraintCastNode(n, t, carry_dependency) {\n+  CheckCastPPNode(Node *c, Node *n, const Type *t, DependencyType dependency = RegularDependency)\n+    : ConstraintCastNode(n, t, dependency) {\n","filename":"src\/hotspot\/share\/opto\/castnode.hpp","additions":32,"deletions":26,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -616,0 +616,9 @@\n+      if (is_CountedLoop()) {\n+        Node* opaq = as_CountedLoop()->is_canonical_loop_entry();\n+        if (opaq != NULL) {\n+          \/\/ This is not a loop anymore. No need to keep the Opaque1 node on the test that guards the loop as it won't be\n+          \/\/ subject to further loop opts.\n+          assert(opaq->Opcode() == Op_Opaque1, \"\");\n+          igvn->replace_node(opaq, opaq->in(1));\n+        }\n+      }\n@@ -2015,1 +2024,1 @@\n-          cast = ConstraintCastNode::make_cast(Op_CastPP, r, uin, phi_type, true);\n+          cast = ConstraintCastNode::make_cast(Op_CastPP, r, uin, phi_type, ConstraintCastNode::StrongDependency);\n@@ -2025,1 +2034,1 @@\n-            cast = ConstraintCastNode::make_cast(Op_CastPP, r, uin, TypePtr::NOTNULL, true);\n+            cast = ConstraintCastNode::make_cast(Op_CastPP, r, uin, TypePtr::NOTNULL, ConstraintCastNode::StrongDependency);\n@@ -2037,1 +2046,1 @@\n-            cast = ConstraintCastNode::make_cast(Op_CheckCastPP, r, n, phi_type, true);\n+            cast = ConstraintCastNode::make_cast(Op_CheckCastPP, r, n, phi_type, ConstraintCastNode::StrongDependency);\n@@ -2040,1 +2049,1 @@\n-            cast = ConstraintCastNode::make_cast(Op_CastPP, r, uin, phi_type, true);\n+            cast = ConstraintCastNode::make_cast(Op_CastPP, r, uin, phi_type, ConstraintCastNode::StrongDependency);\n@@ -2044,1 +2053,1 @@\n-        cast = ConstraintCastNode::make_cast_for_type(r, uin, phi_type);\n+        cast = ConstraintCastNode::make_cast_for_type(r, uin, phi_type, ConstraintCastNode::StrongDependency);\n@@ -2074,1 +2083,5 @@\n-  if( true_path != 0 ) {\n+  if (true_path != 0 &&\n+      \/\/ If one of the diamond's branch is in the process of dying then, the Phi's input for that branch might transform\n+      \/\/ to top. If that happens replacing the Phi with an operation that consumes the Phi's inputs will cause the Phi\n+      \/\/ to be replaced by top. To prevent that, delay the transformation until the branch has a chance to be removed.\n+      !(can_reshape && wait_for_region_igvn(phase))) {\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":19,"deletions":6,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -213,4 +213,4 @@\n-  , _sched_int_pressure(0, INTPRESSURE)\n-  , _sched_float_pressure(0, FLOATPRESSURE)\n-  , _scratch_int_pressure(0, INTPRESSURE)\n-  , _scratch_float_pressure(0, FLOATPRESSURE)\n+  , _sched_int_pressure(0, Matcher::int_pressure_limit())\n+  , _sched_float_pressure(0, Matcher::float_pressure_limit())\n+  , _scratch_int_pressure(0, Matcher::int_pressure_limit())\n+  , _scratch_float_pressure(0, Matcher::float_pressure_limit())\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -4646,1 +4646,1 @@\n-    value = new CastIINode(value, itype, carry_dependency, true \/* range check dependency *\/);\n+    value = new CastIINode(value, itype, carry_dependency ? ConstraintCastNode::StrongDependency : ConstraintCastNode::RegularDependency, true \/* range check dependency *\/);\n@@ -5025,1 +5025,3 @@\n-    if (size != locks_list->origin_cnt()) {\n+    if (size == 0) {\n+      unbalanced = false; \/\/ All locks were eliminated - good\n+    } else if (size != locks_list->origin_cnt()) {\n@@ -5090,4 +5092,6 @@\n-      uint max = n->len();\n-      for( uint i = 0; i < max; ++i ) {\n-        Node *m = n->in(i);\n-        if (not_a_node(m))  continue;\n+      \/\/ Iterate over outs - endless loops is unreachable from below\n+      for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+        Node *m = n->fast_out(i);\n+        if (not_a_node(m)) {\n+          continue;\n+        }\n@@ -5114,4 +5118,6 @@\n-      uint max = n->len();\n-      for( uint i = 0; i < max; ++i ) {\n-        Node *m = n->in(i);\n-        if (not_a_node(m))  continue;\n+      \/\/ Iterate over outs - endless loops is unreachable from below\n+      for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+        Node *m = n->fast_out(i);\n+        if (not_a_node(m)) {\n+          continue;\n+        }\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":16,"deletions":10,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -76,0 +76,3 @@\n+  const bool is_virtual_or_interface = (bytecode == Bytecodes::_invokevirtual) ||\n+                                       (bytecode == Bytecodes::_invokeinterface);\n+\n@@ -168,0 +171,12 @@\n+        \/\/ For optimized virtual calls assert at runtime that receiver object\n+        \/\/ is a subtype of the inlined method holder. CHA can report a method\n+        \/\/ as a unique target under an abstract method, but receiver type\n+        \/\/ sometimes has a broader type. Similar scenario is possible with\n+        \/\/ default methods when type system loses information about implemented\n+        \/\/ interfaces.\n+        if (cg != NULL && is_virtual_or_interface && !callee->is_static()) {\n+          CallGenerator* trap_cg = CallGenerator::for_uncommon_trap(callee,\n+              Deoptimization::Reason_receiver_constraint, Deoptimization::Action_none);\n+\n+          cg = CallGenerator::for_guarded_call(callee->holder(), trap_cg, cg);\n+        }\n@@ -348,3 +363,10 @@\n-    \/\/ Class Hierarchy Analysis or Type Profile reveals a unique target,\n-    \/\/ or it is a static or special call.\n-    return CallGenerator::for_direct_call(callee, should_delay_inlining(callee, jvms));\n+    \/\/ Class Hierarchy Analysis or Type Profile reveals a unique target, or it is a static or special call.\n+    CallGenerator* cg = CallGenerator::for_direct_call(callee, should_delay_inlining(callee, jvms));\n+    \/\/ For optimized virtual calls assert at runtime that receiver object\n+    \/\/ is a subtype of the method holder.\n+    if (cg != NULL && is_virtual_or_interface && !callee->is_static()) {\n+      CallGenerator* trap_cg = CallGenerator::for_uncommon_trap(callee,\n+          Deoptimization::Reason_receiver_constraint, Deoptimization::Action_none);\n+      cg = CallGenerator::for_guarded_call(callee->holder(), trap_cg, cg);\n+    }\n+    return cg;\n@@ -1124,3 +1146,3 @@\n-  ciInstanceKlass *ikl = receiver_type->klass()->as_instance_klass();\n-  if (ikl->is_loaded() && ikl->is_initialized() && !ikl->is_interface() &&\n-      (ikl == actual_receiver || ikl->is_subtype_of(actual_receiver))) {\n+  ciInstanceKlass* receiver_klass = receiver_type->klass()->as_instance_klass();\n+  if (receiver_klass->is_loaded() && receiver_klass->is_initialized() && !receiver_klass->is_interface() &&\n+      (receiver_klass == actual_receiver || receiver_klass->is_subtype_of(actual_receiver))) {\n@@ -1129,1 +1151,1 @@\n-    actual_receiver = ikl;\n+    actual_receiver = receiver_klass;\n@@ -1137,11 +1159,0 @@\n-  \/\/ Validate receiver info against target method.\n-  if (cha_monomorphic_target != NULL) {\n-    bool has_receiver = !cha_monomorphic_target->is_static();\n-    bool is_interface_holder = cha_monomorphic_target->holder()->is_interface();\n-    if (has_receiver && !is_interface_holder) {\n-      if (!cha_monomorphic_target->holder()->is_subtype_of(receiver_type->klass())) {\n-        cha_monomorphic_target = NULL; \/\/ not a subtype\n-      }\n-    }\n-  }\n-\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":29,"deletions":18,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -543,0 +543,22 @@\n+\/\/ This function is used by insert_anti_dependences to find unrelated loads for stores in implicit null checks.\n+bool PhaseCFG::unrelated_load_in_store_null_block(Node* store, Node* load) {\n+  \/\/ We expect an anti-dependence edge from 'load' to 'store', except when\n+  \/\/ implicit_null_check() has hoisted 'store' above its early block to\n+  \/\/ perform an implicit null check, and 'load' is placed in the null\n+  \/\/ block. In this case it is safe to ignore the anti-dependence, as the\n+  \/\/ null block is only reached if 'store' tries to write to null object and\n+  \/\/ 'load' read from non-null object (there is preceding check for that)\n+  \/\/ These objects can't be the same.\n+  Block* store_block = get_block_for_node(store);\n+  Block* load_block = get_block_for_node(load);\n+  Node* end = store_block->end();\n+  if (end->is_MachNullCheck() && (end->in(1) == store) && store_block->dominates(load_block)) {\n+    Node* if_true = end->find_out_with(Op_IfTrue);\n+    assert(if_true != NULL, \"null check without null projection\");\n+    Node* null_block_region = if_true->find_out_with(Op_Region);\n+    assert(null_block_region != NULL, \"null check without null region\");\n+    return get_block_for_node(null_block_region) == load_block;\n+  }\n+  return false;\n+}\n+\n@@ -762,1 +784,1 @@\n-      if (LCA != early) {\n+      if (LCA != early && !unrelated_load_in_store_null_block(store, load)) {\n@@ -773,17 +795,1 @@\n-#ifdef ASSERT\n-        \/\/ We expect an anti-dependence edge from 'load' to 'store', except when\n-        \/\/ implicit_null_check() has hoisted 'store' above its early block to\n-        \/\/ perform an implicit null check, and 'load' is placed in the null\n-        \/\/ block. In this case it is safe to ignore the anti-dependence, as the\n-        \/\/ null block is only reached if 'store' tries to write to null.\n-        Block* store_null_block = NULL;\n-        Node* store_null_check = store->find_out_with(Op_MachNullCheck);\n-        if (store_null_check != NULL) {\n-          Node* if_true = store_null_check->find_out_with(Op_IfTrue);\n-          assert(if_true != NULL, \"null check without null projection\");\n-          Node* null_block_region = if_true->find_out_with(Op_Region);\n-          assert(null_block_region != NULL, \"null check without null region\");\n-          store_null_block = get_block_for_node(null_block_region);\n-        }\n-#endif\n-        assert(LCA == store_null_block || store->find_edge(load) != -1,\n+        assert(store->find_edge(load) != -1 || unrelated_load_in_store_null_block(store, load),\n","filename":"src\/hotspot\/share\/opto\/gcm.cpp","additions":24,"deletions":18,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -544,7 +544,0 @@\n-  if (env()->jvmti_can_post_on_exceptions()) {\n-    \/\/ check if we must post exception events, take uncommon trap if so\n-    uncommon_trap_if_should_post_on_exceptions(reason, must_throw);\n-    \/\/ here if should_post_on_exceptions is false\n-    \/\/ continue on with the normal codegen\n-  }\n-\n@@ -613,0 +606,7 @@\n+      if (env()->jvmti_can_post_on_exceptions()) {\n+        \/\/ check if we must post exception events, take uncommon trap if so\n+        uncommon_trap_if_should_post_on_exceptions(reason, must_throw);\n+        \/\/ here if should_post_on_exceptions is false\n+        \/\/ continue on with the normal codegen\n+      }\n+\n@@ -1208,5 +1208,1 @@\n-    alen = alloc->Ideal_length();\n-    Node* ccast = alloc->make_ideal_length(_gvn.type(array)->is_oopptr(), &_gvn);\n-    if (ccast != alen) {\n-      alen = _gvn.transform(ccast);\n-    }\n+    alen = array_ideal_length(alloc, _gvn.type(array)->is_oopptr(), false);\n@@ -1217,0 +1213,20 @@\n+Node* GraphKit::array_ideal_length(AllocateArrayNode* alloc,\n+                                   const TypeOopPtr* oop_type,\n+                                   bool replace_length_in_map) {\n+  Node* length = alloc->Ideal_length();\n+  if (replace_length_in_map == false || map()->find_edge(length) >= 0) {\n+    Node* ccast = alloc->make_ideal_length(oop_type, &_gvn);\n+    if (ccast != length) {\n+      \/\/ do not transfrom ccast here, it might convert to top node for\n+      \/\/ negative array length and break assumptions in parsing stage.\n+      _gvn.set_type_bottom(ccast);\n+      record_for_igvn(ccast);\n+      if (replace_length_in_map) {\n+        replace_in_map(length, ccast);\n+      }\n+      return ccast;\n+    }\n+  }\n+  return length;\n+}\n+\n@@ -3050,1 +3066,3 @@\n-  if (ExpandSubTypeCheckAtParseTime) {\n+  bool expand_subtype_check = C->post_loop_opts_phase() ||   \/\/ macro node expansion is over\n+                              ExpandSubTypeCheckAtParseTime; \/\/ forced expansion\n+  if (expand_subtype_check) {\n@@ -3072,0 +3090,1 @@\n+  assert(!klass->is_interface(), \"no exact type check on interfaces\");\n@@ -3087,9 +3106,17 @@\n-  const TypeOopPtr* recv_xtype = tklass->as_instance_type();\n-  assert(recv_xtype->klass_is_exact(), \"\");\n-  \/\/ Subsume downstream occurrences of receiver with a cast to\n-  \/\/ recv_xtype, since now we know what the type will be.\n-  Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);\n-  Node* res = _gvn.transform(cast);\n-  if (recv_xtype->is_inlinetypeptr() && recv_xtype->inline_klass()->is_scalarizable()) {\n-    assert(!gvn().type(res)->maybe_null(), \"receiver should never be null\");\n-    res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass())->as_ptr(&gvn());\n+  if (!stopped()) {\n+    const TypeOopPtr* receiver_type = _gvn.type(receiver)->isa_oopptr();\n+    const TypeOopPtr* recv_xtype = tklass->as_instance_type();\n+    assert(recv_xtype->klass_is_exact(), \"\");\n+\n+    if (!receiver_type->higher_equal(recv_xtype)) { \/\/ ignore redundant casts\n+      \/\/ Subsume downstream occurrences of receiver with a cast to\n+      \/\/ recv_xtype, since now we know what the type will be.\n+      Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);\n+      Node* res = _gvn.transform(cast);\n+      if (recv_xtype->is_inlinetypeptr() && recv_xtype->inline_klass()->is_scalarizable()) {\n+        assert(!gvn().type(res)->maybe_null(), \"receiver should never be null\");\n+        res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass())->as_ptr(&gvn());\n+      }\n+      (*casted_receiver) = res;\n+      \/\/ (User must make the replace_in_map call.)\n+    }\n@@ -3099,3 +3126,0 @@\n-  (*casted_receiver) = res;\n-  \/\/ (User must make the replace_in_map call.)\n-\n@@ -3108,2 +3132,2 @@\n-  Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass));\n-  Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );\n+  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n+  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n@@ -3111,2 +3135,2 @@\n-  set_control(  _gvn.transform( new IfTrueNode (iff)));\n-  Node* fail = _gvn.transform( new IfFalseNode(iff));\n+  set_control(_gvn.transform(new IfTrueNode (iff)));\n+  Node* fail = _gvn.transform(new IfFalseNode(iff));\n@@ -3124,4 +3148,9 @@\n-  \/\/ Cast receiver after successful check\n-  const TypeOopPtr* recv_type = tklass->cast_to_exactness(false)->is_klassptr()->as_instance_type();\n-  Node* cast = new CheckCastPPNode(control(), receiver, recv_type);\n-  (*casted_receiver) = _gvn.transform(cast);\n+  \/\/ Ignore interface type information until interface types are properly tracked.\n+  if (!stopped() && !klass->is_interface()) {\n+    const TypeOopPtr* receiver_type = _gvn.type(receiver)->isa_oopptr();\n+    const TypeOopPtr* recv_type = tklass->cast_to_exactness(false)->is_klassptr()->as_instance_type();\n+    if (receiver_type != NULL && !receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n+      Node* cast = new CheckCastPPNode(control(), receiver, recv_type);\n+      (*casted_receiver) = _gvn.transform(cast);\n+    }\n+  }\n@@ -3826,4 +3855,0 @@\n-  if (UseBiasedLocking && PrintPreciseBiasedLockingStatistics) {\n-    \/\/ Create the counters for this fast lock.\n-    flock->create_lock_counter(sync_jvms()); \/\/ sync_jvms used to get current bci\n-  }\n@@ -4398,10 +4423,1 @@\n-  \/\/ Cast length on remaining path to be as narrow as possible\n-  if (map()->find_edge(length) >= 0) {\n-    Node* ccast = alloc->make_ideal_length(ary_type, &_gvn);\n-    if (ccast != length) {\n-      _gvn.set_type_bottom(ccast);\n-      record_for_igvn(ccast);\n-      replace_in_map(length, ccast);\n-    }\n-  }\n-\n+  array_ideal_length(alloc, ary_type, true);\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":63,"deletions":47,"binary":false,"changes":110,"status":"modified"},{"patch":"@@ -355,0 +355,6 @@\n+  \/\/ Cast array allocation's length as narrow as possible.\n+  \/\/ If replace_length_in_map is true, replace length with CastIINode in map.\n+  \/\/ This method is invoked after creating\/moving ArrayAllocationNode or in load_array_length\n+  Node* array_ideal_length(AllocateArrayNode* alloc,\n+                           const TypeOopPtr* oop_type,\n+                           bool replace_length_in_map);\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1014,0 +1014,27 @@\n+\n+    int lo = igvn->type(adjusted_lim)->is_int()->_lo;\n+    if (lo < 0) {\n+      \/\/ If range check elimination applies to this comparison, it includes code to protect from overflows that may\n+      \/\/ cause the main loop to be skipped entirely. Delay this transformation.\n+      \/\/ Example:\n+      \/\/ for (int i = 0; i < limit; i++) {\n+      \/\/   if (i < max_jint && i > min_jint) {...\n+      \/\/ }\n+      \/\/ Comparisons folded as:\n+      \/\/ i - min_jint - 1 <u -2\n+      \/\/ when RC applies, main loop limit becomes:\n+      \/\/ min(limit, max(-2 + min_jint + 1, min_jint))\n+      \/\/ = min(limit, min_jint)\n+      \/\/ = min_jint\n+      if (!igvn->C->post_loop_opts_phase()) {\n+        if (adjusted_val->outcnt() == 0) {\n+          igvn->remove_dead_node(adjusted_val);\n+        }\n+        if (adjusted_lim->outcnt() == 0) {\n+          igvn->remove_dead_node(adjusted_lim);\n+        }\n+        igvn->C->record_for_post_loop_opts_igvn(this);\n+        return false;\n+      }\n+    }\n+\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -441,1 +441,1 @@\n-  if (get_block_for_node(ctrl) == not_null_block) {\n+  if (ctrl != NULL && get_block_for_node(ctrl) == not_null_block) {\n@@ -1104,5 +1104,4 @@\n-    uint float_pressure = Matcher::float_pressure(FLOATPRESSURE);\n-    _regalloc->_sched_int_pressure.init(INTPRESSURE);\n-    _regalloc->_sched_float_pressure.init(float_pressure);\n-    _regalloc->_scratch_int_pressure.init(INTPRESSURE);\n-    _regalloc->_scratch_float_pressure.init(float_pressure);\n+    _regalloc->_sched_int_pressure.init(Matcher::int_pressure_limit());\n+    _regalloc->_sched_float_pressure.init(Matcher::float_pressure_limit());\n+    _regalloc->_scratch_int_pressure.init(Matcher::int_pressure_limit());\n+    _regalloc->_scratch_float_pressure.init(Matcher::float_pressure_limit());\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":5,"deletions":6,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1043,0 +1043,5 @@\n+  if (stopped()) {\n+    \/\/ Length is known to be always negative during compilation and the IR graph so far constructed is good so return success\n+    return true;\n+  }\n+\n@@ -1050,4 +1055,0 @@\n-  if (stopped()) {\n-    return false;\n-  }\n-\n@@ -1072,1 +1073,2 @@\n-    return false;\n+    \/\/ Range check is known to always fail during compilation and the IR graph so far constructed is good so return success\n+    return true;\n@@ -4127,2 +4129,2 @@\n-  \/\/ This also serves as guard against inline types (they have the always_locked_pattern set).\n-  Node *lock_mask      = _gvn.MakeConX(markWord::biased_lock_mask_in_place);\n+  \/\/ This also serves as guard against inline types\n+  Node *lock_mask      = _gvn.MakeConX(markWord::inline_type_mask_in_place);\n@@ -4788,0 +4790,30 @@\n+\n+    \/\/ The CastIINode created in GraphKit::new_array (in AllocateArrayNode::make_ideal_length) must stay below\n+    \/\/ the allocation (i.e. is only valid if the allocation succeeds):\n+    \/\/ 1) replace CastIINode with AllocateArrayNode's length here\n+    \/\/ 2) Create CastIINode again once allocation has moved (see below) at the end of this method\n+    \/\/\n+    \/\/ Multiple identical CastIINodes might exist here. Each GraphKit::load_array_length() call will generate\n+    \/\/ new separate CastIINode (arraycopy guard checks or any array length use between array allocation and ararycopy)\n+    Node* init_control = init->proj_out(TypeFunc::Control);\n+    Node* alloc_length = alloc->Ideal_length();\n+#ifdef ASSERT\n+    Node* prev_cast = NULL;\n+#endif\n+    for (uint i = 0; i < init_control->outcnt(); i++) {\n+      Node* init_out = init_control->raw_out(i);\n+      if (init_out->is_CastII() && init_out->in(TypeFunc::Control) == init_control && init_out->in(1) == alloc_length) {\n+#ifdef ASSERT\n+        if (prev_cast == NULL) {\n+          prev_cast = init_out;\n+        } else {\n+          if (prev_cast->cmp(*init_out) == false) {\n+            prev_cast->dump();\n+            init_out->dump();\n+            assert(false, \"not equal CastIINode\");\n+          }\n+        }\n+#endif\n+        C->gvn_replace_by(init_out, alloc_length);\n+      }\n+    }\n@@ -4819,0 +4851,2 @@\n+\n+    array_ideal_length(alloc, ary_type, true);\n@@ -5678,2 +5712,0 @@\n-    const TypeVect* vt = TypeVect::make(elem_bt, inline_limit);\n-\n@@ -5684,0 +5716,1 @@\n+      const TypeVect* vt = TypeVect::make(elem_bt, inline_limit);\n@@ -6749,1 +6782,1 @@\n-  assert(callee()->signature()->size() == 6, \"base64_decodeBlock has 6 parameters\");\n+  assert(callee()->signature()->size() == 7, \"base64_decodeBlock has 7 parameters\");\n@@ -6761,0 +6794,1 @@\n+  Node* isMIME = argument(7);\n@@ -6773,1 +6807,1 @@\n-                                 src_start, src_offset, len, dest_start, dest_offset, isURL);\n+                                 src_start, src_offset, len, dest_start, dest_offset, isURL, isMIME);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":45,"deletions":11,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -161,9 +161,0 @@\n-\/\/\n-\/\/ Create a counter which counts the number of times this lock is acquired\n-\/\/\n-void FastLockNode::create_lock_counter(JVMState* state) {\n-  BiasedLockingNamedCounter* blnc = (BiasedLockingNamedCounter*)\n-           OptoRuntime::new_named_counter(state, NamedCounter::BiasedLockingCounter);\n-  _counters = blnc->counters();\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/locknode.cpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -356,0 +356,2 @@\n+  Node* is_canonical_loop_entry();\n+\n@@ -937,2 +939,0 @@\n-  static bool is_canonical_loop_entry(CountedLoopNode* cl);\n-\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -959,0 +959,3 @@\n+            lca = place_outside_loop(lca, n_loop);\n+            assert(!n_loop->is_member(get_loop(lca)), \"control must not be back in the loop\");\n+            assert(get_loop(lca)->_nest < n_loop->_nest || lca->in(0)->Opcode() == Op_NeverBranch, \"must not be moved into inner loop\");\n@@ -1213,1 +1216,3 @@\n-    if (loop->is_member(get_loop(dom))) {\n+    if (loop->is_member(get_loop(dom)) ||\n+        \/\/ NeverBranch nodes are not assigned to the loop when constructed\n+        (dom->Opcode() == Op_NeverBranch && loop->is_member(get_loop(dom->in(0))))) {\n@@ -1674,1 +1679,9 @@\n-          if (x->in(0) == NULL && !x->is_DecodeNarrowPtr()) {\n+          \/\/ Chain of AddP: (AddP base (AddP base )) must keep the same base after sinking so:\n+          \/\/ 1- We don't add a CastPP here when the first one is sunk so if the second one is not, their bases remain\n+          \/\/ the same.\n+          \/\/ (see 2- below)\n+          assert(!x->is_AddP() || !x->in(AddPNode::Address)->is_AddP() ||\n+                 x->in(AddPNode::Address)->in(AddPNode::Base) == x->in(AddPNode::Base) ||\n+                 !x->in(AddPNode::Address)->in(AddPNode::Base)->eqv_uncast(x->in(AddPNode::Base)), \"unexpected AddP shape\");\n+          if (x->in(0) == NULL && !x->is_DecodeNarrowPtr() &&\n+              !(x->is_AddP() && x->in(AddPNode::Address)->is_AddP() && x->in(AddPNode::Address)->in(AddPNode::Base) == x->in(AddPNode::Base))) {\n@@ -1682,1 +1695,1 @@\n-                cast = ConstraintCastNode::make_cast_for_type(x_ctrl, in, in_t);\n+                cast = ConstraintCastNode::make_cast_for_type(x_ctrl, in, in_t, ConstraintCastNode::UnconditionalDependency);\n@@ -1687,0 +1700,11 @@\n+                \/\/ Chain of AddP:\n+                \/\/ 2- A CastPP of the base is only added now that both AddP nodes are sunk\n+                if (x->is_AddP() && k == AddPNode::Base) {\n+                  for (DUIterator_Fast imax, i = x->fast_outs(imax); i < imax; i++) {\n+                    Node* u = x->fast_out(i);\n+                    if (u->is_AddP() && u->in(AddPNode::Base) == n->in(AddPNode::Base)) {\n+                      _igvn.replace_input_of(u, AddPNode::Base, cast);\n+                      assert(u->find_out_with(Op_AddP) == NULL, \"more than 2 chained AddP nodes?\");\n+                    }\n+                  }\n+                }\n@@ -2148,1 +2172,1 @@\n-      Node* c = u->in(0) != NULL ? u->in(0) : phase->get_ctrl(u);\n+      Node* c = phase->get_ctrl(u);\n@@ -2151,1 +2175,4 @@\n-      if (outer_loop->is_member(u_loop)) {\n+      if (outer_loop->is_member(u_loop) ||\n+          \/\/ nodes pinned with control in the outer loop but not referenced from the safepoint must be moved out of\n+          \/\/ the outer loop too\n+          (u->in(0) != NULL && outer_loop->is_member(phase->get_loop(u->in(0))))) {\n@@ -2347,4 +2374,8 @@\n-  for( i = 0; i < loop->_body.size(); i++ ) {\n-    Node *old = loop->_body.at(i);\n-    Node *nnn = old->clone();\n-    old_new.map( old->_idx, nnn );\n+  for (i = 0; i < loop->_body.size(); i++) {\n+    Node* old = loop->_body.at(i);\n+    Node* nnn = old->clone();\n+    old_new.map(old->_idx, nnn);\n+    if (old->is_reduction()) {\n+      \/\/ Reduction flag is not copied by default. Copy it here when cloning the entire loop body.\n+      nnn->add_flag(Node::Flag_is_reduction);\n+    }\n@@ -2971,0 +3002,6 @@\n+\n+  if (C->check_node_count(worklist.size() + NodeLimitFudgeFactor,\n+                          \"Too many clones required in clone_for_use_outside_loop in partial peeling\")) {\n+    return -1;\n+  }\n+\n@@ -3523,0 +3560,1 @@\n+  bool too_many_clones = false;\n@@ -3539,1 +3577,6 @@\n-            cloned_for_outside_use += clone_for_use_outside_loop(loop, n, worklist);\n+            int new_clones = clone_for_use_outside_loop(loop, n, worklist);\n+            if (new_clones == -1) {\n+              too_many_clones = true;\n+              break;\n+            }\n+            cloned_for_outside_use += new_clones;\n@@ -3567,1 +3610,1 @@\n-  if (exceed_node_budget || exceed_phi_limit) {\n+  if (too_many_clones || exceed_node_budget || exceed_phi_limit) {\n@@ -3569,1 +3612,1 @@\n-    if (TracePartialPeeling) {\n+    if (TracePartialPeeling && exceed_phi_limit) {\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":55,"deletions":12,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-class BiasedLockingCounters;\n@@ -69,1 +68,1 @@\n-    return C->node_arena()->Amalloc_D(x);\n+    return C->node_arena()->AmallocWords(x);\n@@ -209,0 +208,3 @@\n+private:\n+  bool _removed = false;\n+\n@@ -377,0 +379,2 @@\n+  void set_removed() { _removed = true; }\n+  bool get_removed() { return _removed; }\n@@ -834,1 +838,0 @@\n-  BiasedLockingCounters*        _counters;\n","filename":"src\/hotspot\/share\/opto\/machnode.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2345,80 +2345,3 @@\n-  if (UseOptoBiasInlining) {\n-    \/*\n-     *  See the full description in MacroAssembler::biased_locking_enter().\n-     *\n-     *  if( (mark_word & biased_lock_mask) == biased_lock_pattern ) {\n-     *    \/\/ The object is biased.\n-     *    proto_node = klass->prototype_header;\n-     *    o_node = thread | proto_node;\n-     *    x_node = o_node ^ mark_word;\n-     *    if( (x_node & ~age_mask) == 0 ) { \/\/ Biased to the current thread ?\n-     *      \/\/ Done.\n-     *    } else {\n-     *      if( (x_node & biased_lock_mask) != 0 ) {\n-     *        \/\/ The klass's prototype header is no longer biased.\n-     *        cas(&mark_word, mark_word, proto_node)\n-     *        goto cas_lock;\n-     *      } else {\n-     *        \/\/ The klass's prototype header is still biased.\n-     *        if( (x_node & epoch_mask) != 0 ) { \/\/ Expired epoch?\n-     *          old = mark_word;\n-     *          new = o_node;\n-     *        } else {\n-     *          \/\/ Different thread or anonymous biased.\n-     *          old = mark_word & (epoch_mask | age_mask | biased_lock_mask);\n-     *          new = thread | old;\n-     *        }\n-     *        \/\/ Try to rebias.\n-     *        if( cas(&mark_word, old, new) == 0 ) {\n-     *          \/\/ Done.\n-     *        } else {\n-     *          goto slow_path; \/\/ Failed.\n-     *        }\n-     *      }\n-     *    }\n-     *  } else {\n-     *    \/\/ The object is not biased.\n-     *    cas_lock:\n-     *    if( FastLock(obj) == 0 ) {\n-     *      \/\/ Done.\n-     *    } else {\n-     *      slow_path:\n-     *      OptoRuntime::complete_monitor_locking_Java(obj);\n-     *    }\n-     *  }\n-     *\/\n-\n-    region  = new RegionNode(5);\n-    \/\/ create a Phi for the memory state\n-    mem_phi = new PhiNode( region, Type::MEMORY, TypeRawPtr::BOTTOM);\n-\n-    Node* fast_lock_region  = new RegionNode(3);\n-    Node* fast_lock_mem_phi = new PhiNode( fast_lock_region, Type::MEMORY, TypeRawPtr::BOTTOM);\n-\n-    \/\/ First, check mark word for the biased lock pattern.\n-    Node* mark_node = make_load(ctrl, mem, obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X->basic_type());\n-\n-    \/\/ Get fast path - mark word has the biased lock pattern.\n-    ctrl = opt_bits_test(ctrl, fast_lock_region, 1, mark_node,\n-                         markWord::biased_lock_mask_in_place,\n-                         markWord::biased_lock_pattern, true);\n-    \/\/ fast_lock_region->in(1) is set to slow path.\n-    fast_lock_mem_phi->init_req(1, mem);\n-\n-    \/\/ Now check that the lock is biased to the current thread and has\n-    \/\/ the same epoch and bias as Klass::_prototype_header.\n-\n-    \/\/ Special-case a fresh allocation to avoid building nodes:\n-    Node* klass_node = AllocateNode::Ideal_klass(obj, &_igvn);\n-    if (klass_node == NULL) {\n-      Node* k_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n-      klass_node = transform_later(LoadKlassNode::make(_igvn, NULL, mem, k_adr, _igvn.type(k_adr)->is_ptr()));\n-#ifdef _LP64\n-      if (UseCompressedClassPointers && klass_node->is_DecodeNKlass()) {\n-        assert(klass_node->in(1)->Opcode() == Op_LoadNKlass, \"sanity\");\n-        klass_node->in(1)->init_req(0, ctrl);\n-      } else\n-#endif\n-      klass_node->init_req(0, ctrl);\n-    }\n-    Node *proto_node = make_load(ctrl, mem, klass_node, in_bytes(Klass::prototype_header_offset()), TypeX_X, TypeX_X->basic_type());\n+  region  = new RegionNode(3);\n+  \/\/ create a Phi for the memory state\n+  mem_phi = new PhiNode( region, Type::MEMORY, TypeRawPtr::BOTTOM);\n@@ -2426,109 +2349,3 @@\n-    Node* thread = transform_later(new ThreadLocalNode());\n-    Node* cast_thread = transform_later(new CastP2XNode(ctrl, thread));\n-    Node* o_node = transform_later(new OrXNode(cast_thread, proto_node));\n-    Node* x_node = transform_later(new XorXNode(o_node, mark_node));\n-\n-    \/\/ Get slow path - mark word does NOT match the value.\n-    STATIC_ASSERT(markWord::age_mask_in_place <= INT_MAX);\n-    Node* not_biased_ctrl =  opt_bits_test(ctrl, region, 3, x_node,\n-                                      (~(int)markWord::age_mask_in_place), 0);\n-    \/\/ region->in(3) is set to fast path - the object is biased to the current thread.\n-    mem_phi->init_req(3, mem);\n-\n-\n-    \/\/ Mark word does NOT match the value (thread | Klass::_prototype_header).\n-\n-\n-    \/\/ First, check biased pattern.\n-    \/\/ Get fast path - _prototype_header has the same biased lock pattern.\n-    ctrl =  opt_bits_test(not_biased_ctrl, fast_lock_region, 2, x_node,\n-                          markWord::biased_lock_mask_in_place, 0, true);\n-\n-    not_biased_ctrl = fast_lock_region->in(2); \/\/ Slow path\n-    \/\/ fast_lock_region->in(2) - the prototype header is no longer biased\n-    \/\/ and we have to revoke the bias on this object.\n-    \/\/ We are going to try to reset the mark of this object to the prototype\n-    \/\/ value and fall through to the CAS-based locking scheme.\n-    Node* adr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n-    Node* cas = new StoreXConditionalNode(not_biased_ctrl, mem, adr,\n-                                          proto_node, mark_node);\n-    transform_later(cas);\n-    Node* proj = transform_later(new SCMemProjNode(cas));\n-    fast_lock_mem_phi->init_req(2, proj);\n-\n-\n-    \/\/ Second, check epoch bits.\n-    Node* rebiased_region  = new RegionNode(3);\n-    Node* old_phi = new PhiNode( rebiased_region, TypeX_X);\n-    Node* new_phi = new PhiNode( rebiased_region, TypeX_X);\n-\n-    \/\/ Get slow path - mark word does NOT match epoch bits.\n-    Node* epoch_ctrl =  opt_bits_test(ctrl, rebiased_region, 1, x_node,\n-                                      markWord::epoch_mask_in_place, 0);\n-    \/\/ The epoch of the current bias is not valid, attempt to rebias the object\n-    \/\/ toward the current thread.\n-    rebiased_region->init_req(2, epoch_ctrl);\n-    old_phi->init_req(2, mark_node);\n-    new_phi->init_req(2, o_node);\n-\n-    \/\/ rebiased_region->in(1) is set to fast path.\n-    \/\/ The epoch of the current bias is still valid but we know\n-    \/\/ nothing about the owner; it might be set or it might be clear.\n-    Node* cmask   = MakeConX(markWord::biased_lock_mask_in_place |\n-                             markWord::age_mask_in_place |\n-                             markWord::epoch_mask_in_place);\n-    Node* old = transform_later(new AndXNode(mark_node, cmask));\n-    cast_thread = transform_later(new CastP2XNode(ctrl, thread));\n-    Node* new_mark = transform_later(new OrXNode(cast_thread, old));\n-    old_phi->init_req(1, old);\n-    new_phi->init_req(1, new_mark);\n-\n-    transform_later(rebiased_region);\n-    transform_later(old_phi);\n-    transform_later(new_phi);\n-\n-    \/\/ Try to acquire the bias of the object using an atomic operation.\n-    \/\/ If this fails we will go in to the runtime to revoke the object's bias.\n-    cas = new StoreXConditionalNode(rebiased_region, mem, adr, new_phi, old_phi);\n-    transform_later(cas);\n-    proj = transform_later(new SCMemProjNode(cas));\n-\n-    \/\/ Get slow path - Failed to CAS.\n-    not_biased_ctrl = opt_bits_test(rebiased_region, region, 4, cas, 0, 0);\n-    mem_phi->init_req(4, proj);\n-    \/\/ region->in(4) is set to fast path - the object is rebiased to the current thread.\n-\n-    \/\/ Failed to CAS.\n-    slow_path  = new RegionNode(3);\n-    Node *slow_mem = new PhiNode( slow_path, Type::MEMORY, TypeRawPtr::BOTTOM);\n-\n-    slow_path->init_req(1, not_biased_ctrl); \/\/ Capture slow-control\n-    slow_mem->init_req(1, proj);\n-\n-    \/\/ Call CAS-based locking scheme (FastLock node).\n-\n-    transform_later(fast_lock_region);\n-    transform_later(fast_lock_mem_phi);\n-\n-    \/\/ Get slow path - FastLock failed to lock the object.\n-    ctrl = opt_bits_test(fast_lock_region, region, 2, flock, 0, 0);\n-    mem_phi->init_req(2, fast_lock_mem_phi);\n-    \/\/ region->in(2) is set to fast path - the object is locked to the current thread.\n-\n-    slow_path->init_req(2, ctrl); \/\/ Capture slow-control\n-    slow_mem->init_req(2, fast_lock_mem_phi);\n-\n-    transform_later(slow_path);\n-    transform_later(slow_mem);\n-    \/\/ Reset lock's memory edge.\n-    lock->set_req(TypeFunc::Memory, slow_mem);\n-\n-  } else {\n-    region  = new RegionNode(3);\n-    \/\/ create a Phi for the memory state\n-    mem_phi = new PhiNode( region, Type::MEMORY, TypeRawPtr::BOTTOM);\n-\n-    \/\/ Optimize test; set region slot 2\n-    slow_path = opt_bits_test(ctrl, region, 2, flock, 0, 0);\n-    mem_phi->init_req(2, mem);\n-  }\n+  \/\/ Optimize test; set region slot 2\n+  slow_path = opt_bits_test(ctrl, region, 2, flock, 0, 0);\n+  mem_phi->init_req(2, mem);\n@@ -2587,17 +2404,3 @@\n-  if (UseOptoBiasInlining) {\n-    \/\/ Check for biased locking unlock case, which is a no-op.\n-    \/\/ See the full description in MacroAssembler::biased_locking_exit().\n-    region  = new RegionNode(4);\n-    \/\/ create a Phi for the memory state\n-    mem_phi = new PhiNode( region, Type::MEMORY, TypeRawPtr::BOTTOM);\n-    mem_phi->init_req(3, mem);\n-\n-    Node* mark_node = make_load(ctrl, mem, obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X->basic_type());\n-    ctrl = opt_bits_test(ctrl, region, 3, mark_node,\n-                         markWord::biased_lock_mask_in_place,\n-                         markWord::biased_lock_pattern);\n-  } else {\n-    region  = new RegionNode(3);\n-    \/\/ create a Phi for the memory state\n-    mem_phi = new PhiNode( region, Type::MEMORY, TypeRawPtr::BOTTOM);\n-  }\n+  region  = new RegionNode(3);\n+  \/\/ create a Phi for the memory state\n+  mem_phi = new PhiNode( region, Type::MEMORY, TypeRawPtr::BOTTOM);\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":9,"deletions":206,"binary":false,"changes":215,"status":"modified"},{"patch":"@@ -466,1 +466,1 @@\n-  RegMask *rms = (RegMask*)C->comp_arena()->Amalloc_D(sizeof(RegMask) * NOF_STACK_MASKS);\n+  RegMask *rms = (RegMask*)C->comp_arena()->AmallocWords(sizeof(RegMask) * NOF_STACK_MASKS);\n@@ -2678,1 +2678,1 @@\n-      } else if (is_generic_reg2reg_move(def->as_Mach())) {\n+      } else if (is_reg2reg_move(def->as_Mach())) {\n@@ -2704,3 +2704,0 @@\n-  if (C->max_vector_size() == 0) {\n-    return; \/\/ no vector instructions or operands\n-  }\n@@ -2715,1 +2712,1 @@\n-      if (Matcher::is_generic_reg2reg_move(m)) {\n+      if (Matcher::is_reg2reg_move(m)) {\n@@ -2738,1 +2735,1 @@\n-        assert(!Matcher::is_generic_reg2reg_move(m), \"no MoveVec nodes allowed\");\n+        assert(!Matcher::is_reg2reg_move(m), \"no MoveVec nodes allowed\");\n@@ -2807,3 +2804,1 @@\n-    \/\/ With biased locking we're no longer guaranteed that a monitor\n-    \/\/ enter operation contains a serializing instruction.\n-    if ((xop == Op_FastLock) && !UseBiasedLocking) {\n+    if (xop == Op_FastLock) {\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":5,"deletions":10,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -236,0 +236,4 @@\n+  \/\/ Number of integer live ranges that constitute high register pressure\n+  static uint int_pressure_limit();\n+  \/\/ Number of float live ranges that constitute high register pressure\n+  static uint float_pressure_limit();\n@@ -333,3 +337,0 @@\n-  \/\/ Some uarchs have different sized float register resources\n-  static const int float_pressure(int default_pressure_threshold);\n-\n@@ -474,1 +475,1 @@\n-  static bool is_generic_reg2reg_move(MachNode* m);\n+  static bool is_reg2reg_move(MachNode* m);\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1878,11 +1878,0 @@\n-  AllocateNode* alloc = AllocateNode::Ideal_allocation(address, phase);\n-  if (alloc != NULL && mem->is_Proj() &&\n-      mem->in(0) != NULL &&\n-      mem->in(0) == alloc->initialization() &&\n-      Opcode() == Op_LoadX &&\n-      alloc->initialization()->proj_out_or_null(0) != NULL) {\n-    InitializeNode* init = alloc->initialization();\n-    Node* control = init->proj_out(0);\n-    return alloc->make_ideal_mark(phase, control, mem);\n-  }\n-\n@@ -2188,1 +2177,1 @@\n-  if (alloc != NULL && !(alloc->Opcode() == Op_Allocate && UseBiasedLocking)) {\n+  if (alloc != NULL) {\n@@ -3444,1 +3433,2 @@\n-    return;\n+    assert(Opcode() == Op_Initialize, \"Only seen when there are no use of init memory\");\n+    assert(outcnt() == 1, \"Only control then\");\n@@ -3453,2 +3443,6 @@\n-  igvn->replace_node(proj_out(TypeFunc::Memory), in(TypeFunc::Memory));\n-  igvn->replace_node(proj_out(TypeFunc::Control), in(TypeFunc::Control));\n+  if (proj_out_or_null(TypeFunc::Memory) != NULL) {\n+    igvn->replace_node(proj_out(TypeFunc::Memory), in(TypeFunc::Memory));\n+  }\n+  if (proj_out_or_null(TypeFunc::Control) != NULL) {\n+    igvn->replace_node(proj_out(TypeFunc::Control), in(TypeFunc::Control));\n+  }\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":9,"deletions":15,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -49,1 +49,0 @@\n-  assert((Opcode() != Op_If && Opcode() != Op_RangeCheck) || outcnt() == 2, \"bad if #1\");\n@@ -78,0 +77,1 @@\n+  assert((Opcode() != Op_If && Opcode() != Op_RangeCheck) || outcnt() == 2, \"bad if #1\");\n","filename":"src\/hotspot\/share\/opto\/multnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -311,1 +311,1 @@\n-    _in = (Node **) ((char *) (C->node_arena()->Amalloc_D(req * sizeof(void*))));\n+    _in = (Node **) ((char *) (C->node_arena()->AmallocWords(req * sizeof(void*))));\n@@ -502,1 +502,1 @@\n-  Node *n = (Node*)C->node_arena()->Amalloc_D(size_of() + _max*sizeof(Node*));\n+  Node *n = (Node*)C->node_arena()->AmallocWords(size_of() + _max*sizeof(Node*));\n@@ -531,0 +531,4 @@\n+  if (n->is_reduction()) {\n+    \/\/ Do not copy reduction information. This must be explicitly set by the calling code.\n+    n->remove_flag(Node::Flag_is_reduction);\n+  }\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -229,2 +229,1 @@\n-  Node(const Node&);            \/\/ not defined; linker error to use these\n-  Node &operator=(const Node &rhs);\n+  NONCOPYABLE(Node);\n@@ -248,1 +247,1 @@\n-    Node* n = (Node*)C->node_arena()->Amalloc_D(x);\n+    Node* n = (Node*)C->node_arena()->AmallocWords(x);\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1259,0 +1259,1 @@\n+#ifdef ASSERT\n@@ -1260,1 +1261,1 @@\n-  ciInstanceKlass* callee_holder = method()->holder();\n+    ciInstanceKlass* callee_holder = method()->holder();\n@@ -1268,0 +1269,5 @@\n+      \/\/ Receiver should always be a subtype of callee holder.\n+      \/\/ But, since C2 type system doesn't properly track interfaces,\n+      \/\/ the invariant can't be expressed in the type system for default methods.\n+      \/\/ Example: for unrelated C <: I and D <: I, (C `meet` D) = Object <\/: I.\n+      assert(callee_holder->is_interface(), \"missing subtype check\");\n@@ -1269,1 +1275,0 @@\n-#ifdef ASSERT\n@@ -1277,15 +1282,0 @@\n-#endif \/\/ ASSERT\n-\n-      \/\/ Receiver should always be a subtype of callee holder.\n-      \/\/ But, since C2 type system doesn't properly track interfaces,\n-      \/\/ the invariant on default methods can't be expressed in the type system.\n-      \/\/ Example: for unrelated C <: I and D <: I, (C `meet` D) = Object <\/: I.\n-      \/\/ (Downcasting interface receiver type to concrete class is fine, though it doesn't happen in practice.)\n-      if (!callee_holder->is_interface()) {\n-        assert(callee_holder->is_subtype_of(receiver_type->klass()), \"sanity\");\n-        assert(!receiver_type->klass()->is_interface(), \"interface receiver type\");\n-        receiver_type = receiver_type->join_speculative(holder_type)->is_instptr(); \/\/ keep speculative part\n-        Node* casted_receiver_obj = _gvn.transform(new CheckCastPPNode(control(), receiver_obj, receiver_type));\n-        set_local(0, casted_receiver_obj);\n-      }\n-\n@@ -1294,0 +1284,1 @@\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":8,"deletions":17,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -188,1 +188,1 @@\n-        alloc_obj = new CastPPNode(alloc_obj, _gvn.type(alloc_obj), true);\n+        alloc_obj = new CastPPNode(alloc_obj, _gvn.type(alloc_obj), ConstraintCastNode::StrongDependency);\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1202,1 +1202,1 @@\n-  int argcnt = 6;\n+  int argcnt = 7;\n@@ -1212,0 +1212,1 @@\n+  fields[argp++] = TypeInt::BOOL;       \/\/ isMIME\n@@ -1607,6 +1608,0 @@\n-    } else if (c->tag() == NamedCounter::BiasedLockingCounter) {\n-      BiasedLockingCounters* blc = ((BiasedLockingNamedCounter*)c)->counters();\n-      if (blc->nonzero()) {\n-        tty->print_cr(\"%s\", c->name());\n-        blc->print_on(tty);\n-      }\n@@ -1663,3 +1658,1 @@\n-  if (tag == NamedCounter::BiasedLockingCounter) {\n-    c = new BiasedLockingNamedCounter(st.as_string());\n-  } else if (tag == NamedCounter::RTMLockingCounter) {\n+  if (tag == NamedCounter::RTMLockingCounter) {\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":3,"deletions":10,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -66,1 +65,0 @@\n-    BiasedLockingCounter,\n@@ -103,12 +101,0 @@\n-class BiasedLockingNamedCounter : public NamedCounter {\n- private:\n-  BiasedLockingCounters _counters;\n-\n- public:\n-  BiasedLockingNamedCounter(const char *n) :\n-    NamedCounter(n, BiasedLockingCounter), _counters() {}\n-\n-  BiasedLockingCounters* counters() { return &_counters; }\n-};\n-\n-\n@@ -173,4 +159,0 @@\n-  \/\/ Slow-path Locking and Unlocking\n-  static void complete_monitor_locking_C(oopDesc* obj, BasicLock* lock, JavaThread* thread);\n-  static void complete_monitor_unlocking_C(oopDesc* obj, BasicLock* lock, JavaThread* thread);\n-\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":0,"deletions":18,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -272,0 +272,34 @@\n+  \/\/ Associative\n+  if (op1 == Op_MulI && op2 == Op_MulI) {\n+    Node* sub_in1 = NULL;\n+    Node* sub_in2 = NULL;\n+    Node* mul_in = NULL;\n+\n+    if (in1->in(1) == in2->in(1)) {\n+      \/\/ Convert \"a*b-a*c into a*(b-c)\n+      sub_in1 = in1->in(2);\n+      sub_in2 = in2->in(2);\n+      mul_in = in1->in(1);\n+    } else if (in1->in(2) == in2->in(1)) {\n+      \/\/ Convert a*b-b*c into b*(a-c)\n+      sub_in1 = in1->in(1);\n+      sub_in2 = in2->in(2);\n+      mul_in = in1->in(2);\n+    } else if (in1->in(2) == in2->in(2)) {\n+      \/\/ Convert a*c-b*c into (a-b)*c\n+      sub_in1 = in1->in(1);\n+      sub_in2 = in2->in(1);\n+      mul_in = in1->in(2);\n+    } else if (in1->in(1) == in2->in(2)) {\n+      \/\/ Convert a*b-c*a into a*(b-c)\n+      sub_in1 = in1->in(2);\n+      sub_in2 = in2->in(1);\n+      mul_in = in1->in(1);\n+    }\n+\n+    if (mul_in != NULL) {\n+      Node* sub = phase->transform(new SubINode(sub_in1, sub_in2));\n+      return new MulINode(mul_in, sub);\n+    }\n+  }\n+\n@@ -396,0 +430,34 @@\n+  \/\/ Associative\n+  if (op1 == Op_MulL && op2 == Op_MulL) {\n+    Node* sub_in1 = NULL;\n+    Node* sub_in2 = NULL;\n+    Node* mul_in = NULL;\n+\n+    if (in1->in(1) == in2->in(1)) {\n+      \/\/ Convert \"a*b-a*c into a*(b+c)\n+      sub_in1 = in1->in(2);\n+      sub_in2 = in2->in(2);\n+      mul_in = in1->in(1);\n+    } else if (in1->in(2) == in2->in(1)) {\n+      \/\/ Convert a*b-b*c into b*(a-c)\n+      sub_in1 = in1->in(1);\n+      sub_in2 = in2->in(2);\n+      mul_in = in1->in(2);\n+    } else if (in1->in(2) == in2->in(2)) {\n+      \/\/ Convert a*c-b*c into (a-b)*c\n+      sub_in1 = in1->in(1);\n+      sub_in2 = in2->in(1);\n+      mul_in = in1->in(2);\n+    } else if (in1->in(1) == in2->in(2)) {\n+      \/\/ Convert a*b-c*a into a*(b-c)\n+      sub_in1 = in1->in(2);\n+      sub_in2 = in2->in(1);\n+      mul_in = in1->in(1);\n+    }\n+\n+    if (mul_in != NULL) {\n+      Node* sub = phase->transform(new SubLNode(sub_in1, sub_in2));\n+      return new MulLNode(mul_in, sub);\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":68,"deletions":0,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -564,1 +564,1 @@\n-  const Type **fboth =(const Type**)shared_type_arena->Amalloc_4(2*sizeof(Type*));\n+  const Type **fboth =(const Type**)shared_type_arena->AmallocWords(2*sizeof(Type*));\n@@ -569,1 +569,1 @@\n-  const Type **ffalse =(const Type**)shared_type_arena->Amalloc_4(2*sizeof(Type*));\n+  const Type **ffalse =(const Type**)shared_type_arena->AmallocWords(2*sizeof(Type*));\n@@ -574,1 +574,1 @@\n-  const Type **fneither =(const Type**)shared_type_arena->Amalloc_4(2*sizeof(Type*));\n+  const Type **fneither =(const Type**)shared_type_arena->AmallocWords(2*sizeof(Type*));\n@@ -579,1 +579,1 @@\n-  const Type **ftrue =(const Type**)shared_type_arena->Amalloc_4(2*sizeof(Type*));\n+  const Type **ftrue =(const Type**)shared_type_arena->AmallocWords(2*sizeof(Type*));\n@@ -584,1 +584,1 @@\n-  const Type **floop =(const Type**)shared_type_arena->Amalloc_4(2*sizeof(Type*));\n+  const Type **floop =(const Type**)shared_type_arena->AmallocWords(2*sizeof(Type*));\n@@ -599,1 +599,1 @@\n-  const Type **fsc = (const Type**)shared_type_arena->Amalloc_4(2*sizeof(Type*));\n+  const Type **fsc = (const Type**)shared_type_arena->AmallocWords(2*sizeof(Type*));\n@@ -2176,1 +2176,1 @@\n-  const Type **flds = (const Type **)(Compile::current()->type_arena()->Amalloc_4((TypeFunc::Parms+arg_cnt)*sizeof(Type*) ));\n+  const Type **flds = (const Type **)(Compile::current()->type_arena()->AmallocWords((TypeFunc::Parms+arg_cnt)*sizeof(Type*) ));\n@@ -2204,1 +2204,1 @@\n-    const Type **fields = (const Type **)(Compile::current()->type_arena()->Amalloc_4( _cnt*sizeof(Type*) ));\n+    const Type **fields = (const Type **)(Compile::current()->type_arena()->AmallocWords( _cnt*sizeof(Type*) ));\n@@ -2218,1 +2218,1 @@\n-  const Type **fields = (const Type **)(Compile::current()->type_arena()->Amalloc_4( _cnt*sizeof(Type*) ));\n+  const Type **fields = (const Type **)(Compile::current()->type_arena()->AmallocWords( _cnt*sizeof(Type*) ));\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -220,1 +220,1 @@\n-    return compile->type_arena()->Amalloc_D(x);\n+    return compile->type_arena()->AmallocWords(x);\n@@ -413,1 +413,1 @@\n-#endif\n+#endif \/\/ !PRODUCT\n@@ -2013,3 +2013,1 @@\n-\/\/ UseOptoBiasInlining\n-#define XorXNode     XorLNode\n-#define StoreXConditionalNode StoreLConditionalNode\n+\/\/ For shenandoahSupport\n@@ -2063,3 +2061,1 @@\n-\/\/ UseOptoBiasInlining\n-#define XorXNode     XorINode\n-#define StoreXConditionalNode StoreIConditionalNode\n+\/\/ For shenandoahSupport\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":4,"deletions":8,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"gc\/shared\/stringdedup\/stringDedup.hpp\"\n@@ -592,1 +593,1 @@\n-        \"in thread \\\"%s\\\" \", thread->get_thread_name());\n+        \"in thread \\\"%s\\\" \", thread->name());\n@@ -994,1 +995,1 @@\n-  DT_RETURN_MARK(NewObjectA, jobject, (const jobject)obj);\n+  DT_RETURN_MARK(NewObjectA, jobject, (const jobject&)obj);\n@@ -2945,7 +2946,2 @@\n-  assert(a->is_array(), \"just checking\");\n-  BasicType type;\n-  if (a->is_objArray()) {\n-    type = T_OBJECT;\n-  } else {\n-    type = TypeArrayKlass::cast(a->klass())->element_type();\n-  }\n+  assert(a->is_typeArray(), \"Primitive array only\");\n+  BasicType type = TypeArrayKlass::cast(a->klass())->element_type();\n@@ -2965,0 +2961,28 @@\n+static typeArrayOop lock_gc_or_pin_string_value(JavaThread* thread, oop str) {\n+  if (Universe::heap()->supports_object_pinning()) {\n+    \/\/ Forbid deduplication before obtaining the value array, to prevent\n+    \/\/ deduplication from replacing the value array while setting up or in\n+    \/\/ the critical section.  That would lead to the release operation\n+    \/\/ unpinning the wrong object.\n+    if (StringDedup::is_enabled()) {\n+      NoSafepointVerifier nsv;\n+      StringDedup::forbid_deduplication(str);\n+    }\n+    typeArrayOop s_value = java_lang_String::value(str);\n+    return (typeArrayOop) Universe::heap()->pin_object(thread, s_value);\n+  } else {\n+    Handle h(thread, str);      \/\/ Handlize across potential safepoint.\n+    GCLocker::lock_critical(thread);\n+    return java_lang_String::value(h());\n+  }\n+}\n+\n+static void unlock_gc_or_unpin_string_value(JavaThread* thread, oop str) {\n+  if (Universe::heap()->supports_object_pinning()) {\n+    typeArrayOop s_value = java_lang_String::value(str);\n+    Universe::heap()->unpin_object(thread, s_value);\n+  } else {\n+    GCLocker::unlock_critical(thread);\n+  }\n+}\n+\n@@ -2967,6 +2991,1 @@\n-  oop s = lock_gc_or_pin_object(thread, string);\n-  typeArrayOop s_value = java_lang_String::value(s);\n-  bool is_latin1 = java_lang_String::is_latin1(s);\n-  if (isCopy != NULL) {\n-    *isCopy = is_latin1 ? JNI_TRUE : JNI_FALSE;\n-  }\n+  oop s = JNIHandles::resolve_non_null(string);\n@@ -2974,1 +2993,2 @@\n-  if (!is_latin1) {\n+  if (!java_lang_String::is_latin1(s)) {\n+    typeArrayOop s_value = lock_gc_or_pin_string_value(thread, s);\n@@ -2976,0 +2996,1 @@\n+    if (isCopy != NULL) *isCopy = JNI_FALSE;\n@@ -2978,0 +2999,1 @@\n+    typeArrayOop s_value = java_lang_String::value(s);\n@@ -2987,0 +3009,1 @@\n+    if (isCopy != NULL) *isCopy = JNI_TRUE;\n@@ -2995,1 +3018,0 @@\n-  \/\/ The str and chars arguments are ignored for UTF16 strings\n@@ -3002,0 +3024,3 @@\n+  } else {\n+    \/\/ For non-latin1 string, drop the associated gc-locker\/pin.\n+    unlock_gc_or_unpin_string_value(thread, s);\n@@ -3003,1 +3028,0 @@\n-  unlock_gc_or_unpin_object(thread, str);\n@@ -3890,1 +3914,1 @@\n-      *(JNIEnv**)penv = t->as_Java_thread()->jni_environment();\n+      *(JNIEnv**)penv = JavaThread::cast(t)->jni_environment();\n@@ -4027,1 +4051,1 @@\n-  JavaThread* thread = current->as_Java_thread();\n+  JavaThread* thread = JavaThread::cast(current);\n@@ -4089,1 +4113,1 @@\n-      *(JNIEnv**)penv = thread->as_Java_thread()->jni_environment();\n+      *(JNIEnv**)penv = JavaThread::cast(thread)->jni_environment();\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":45,"deletions":21,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -102,1 +102,1 @@\n-    JavaThread* thr = cur->as_Java_thread();                             \\\n+    JavaThread* thr = JavaThread::cast(cur);                             \\\n","filename":"src\/hotspot\/share\/prims\/jniCheck.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -428,1 +428,1 @@\n-    DynamicArchive::prepare_for_dynamic_dumping_at_exit();\n+    DynamicArchive::prepare_for_dynamic_dumping();\n@@ -2981,0 +2981,3 @@\n+    ResourceMark rm(thread);\n+    log_warning(os, thread)(\"Failed to start the native thread for java.lang.Thread \\\"%s\\\"\",\n+                            JavaThread::name_for(JNIHandles::resolve_non_null(jthread)));\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1329,12 +1329,1 @@\n-  {\n-    MutexLocker mu(current_thread, Threads_lock); \/\/ grab Threads_lock\n-\n-    JvmtiAgentThread *new_thread = new JvmtiAgentThread(this, proc, arg);\n-    \/\/ At this point it may be possible that no osthread was created for the\n-    \/\/ JavaThread due to lack of memory.\n-    if (new_thread == NULL || new_thread->osthread() == NULL) {\n-      if (new_thread != NULL) {\n-        new_thread->smr_delete();\n-      }\n-      return JVMTI_ERROR_OUT_OF_MEMORY;\n-    }\n+  JvmtiAgentThread* new_thread = new JvmtiAgentThread(this, proc, arg);\n@@ -1343,3 +1332,7 @@\n-    java_lang_Thread::set_thread(thread_hndl(), new_thread);\n-    java_lang_Thread::set_priority(thread_hndl(), (ThreadPriority)priority);\n-    java_lang_Thread::set_daemon(thread_hndl());\n+  \/\/ At this point it may be possible that no osthread was created for the\n+  \/\/ JavaThread due to lack of resources.\n+  if (new_thread->osthread() == NULL) {\n+    \/\/ The new thread is not known to Thread-SMR yet so we can just delete.\n+    delete new_thread;\n+    return JVMTI_ERROR_OUT_OF_MEMORY;\n+  }\n@@ -1347,4 +1340,2 @@\n-    new_thread->set_threadObj(thread_hndl());\n-    Threads::add(new_thread);\n-    Thread::start(new_thread);\n-  } \/\/ unlock Threads_lock\n+  JavaThread::start_internal_daemon(current_thread, new_thread, thread_hndl,\n+                                    (ThreadPriority)priority);\n@@ -3149,1 +3140,4 @@\n-    rmonitor->raw_enter(Thread::current());\n+    Thread* thread = Thread::current();\n+    \/\/ 8266889: raw_enter changes Java thread state, needs WXWrite\n+    MACOS_AARCH64_ONLY(ThreadWXEnable __wx(WXWrite, thread));\n+    rmonitor->raw_enter(thread);\n@@ -3181,0 +3175,2 @@\n+  \/\/ 8266889: raw_wait changes Java thread state, needs WXWrite\n+  MACOS_AARCH64_ONLY(ThreadWXEnable __wx(WXWrite, thread));\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":16,"deletions":20,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -115,1 +115,1 @@\n-       _jthread = thread->as_Java_thread();\n+       _jthread = JavaThread::cast(thread);\n@@ -2209,0 +2209,1 @@\n+  assert(!nm->is_zombie(), \"nmethod zombie in post_compiled_method_load\");\n@@ -2311,1 +2312,1 @@\n-    JvmtiThreadState *state = thread->as_Java_thread()->jvmti_thread_state();\n+    JvmtiThreadState *state = JavaThread::cast(thread)->jvmti_thread_state();\n@@ -2335,1 +2336,1 @@\n-    JvmtiThreadState *state = thread->as_Java_thread()->jvmti_thread_state();\n+    JvmtiThreadState *state = JavaThread::cast(thread)->jvmti_thread_state();\n@@ -2871,1 +2872,1 @@\n-    JavaThread* current_thread = thread->as_Java_thread();\n+    JavaThread* current_thread = JavaThread::cast(thread);\n","filename":"src\/hotspot\/share\/prims\/jvmtiExport.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -969,1 +969,1 @@\n-  \/\/ Post and destroy queue nodes\n+  \/\/ Post events while nmethods are still in the queue and can't be unloaded or made zombie\n@@ -971,2 +971,2 @@\n-     JvmtiDeferredEvent event = dequeue();\n-     event.post_compiled_method_load_event(env);\n+    _queue_head->event().post_compiled_method_load_event(env);\n+    dequeue();\n","filename":"src\/hotspot\/share\/prims\/jvmtiImpl.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -4079,1 +4079,1 @@\n-\/\/ Deoptimize all compiled code that depends on this class.\n+\/\/ Deoptimize all compiled code that depends on the classes redefined.\n@@ -4095,12 +4095,0 @@\n-\/\/ First step is to walk the code cache for each class redefined and mark\n-\/\/ dependent methods.  Wait until all classes are processed to deoptimize everything.\n-void VM_RedefineClasses::mark_dependent_code(InstanceKlass* ik) {\n-  assert_locked_or_safepoint(Compile_lock);\n-\n-  \/\/ All dependencies have been recorded from startup or this is a second or\n-  \/\/ subsequent use of RedefineClasses\n-  if (JvmtiExport::all_dependencies_are_recorded()) {\n-    CodeCache::mark_for_evol_deoptimization(ik);\n-  }\n-}\n-\n@@ -4231,3 +4219,0 @@\n-\n-  \/\/ Mark all compiled code that depends on this class\n-  mark_dependent_code(the_class);\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":1,"deletions":16,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -105,1 +105,1 @@\n-#include \"gc\/g1\/heapRegionRemSet.hpp\"\n+#include \"gc\/g1\/heapRegionRemSet.inline.hpp\"\n@@ -2026,1 +2026,1 @@\n-  return HeapShared::is_archived_object(obj_oop);\n+  return Universe::heap()->is_archived_object(obj_oop);\n@@ -2034,1 +2034,1 @@\n-  return !HeapShared::closed_archive_heap_region_mapped();\n+  return !HeapShared::closed_regions_mapped();\n@@ -2059,1 +2059,1 @@\n-  return HeapShared::open_archive_heap_region_mapped();\n+  return HeapShared::open_regions_mapped();\n@@ -2110,0 +2110,46 @@\n+WB_ENTRY(jboolean, WB_HandshakeReadMonitors(JNIEnv* env, jobject wb, jobject thread_handle))\n+  class ReadMonitorsClosure : public HandshakeClosure {\n+    jboolean _executed;\n+\n+    void do_thread(Thread* th) {\n+      JavaThread* jt = JavaThread::cast(th);\n+      ResourceMark rm;\n+\n+      GrowableArray<MonitorInfo*>* info = new GrowableArray<MonitorInfo*>();\n+\n+      if (!jt->has_last_Java_frame()) {\n+        return;\n+      }\n+      RegisterMap rmap(jt);\n+      for (javaVFrame* vf = jt->last_java_vframe(&rmap); vf != NULL; vf = vf->java_sender()) {\n+        GrowableArray<MonitorInfo*> *monitors = vf->monitors();\n+        if (monitors != NULL) {\n+          int len = monitors->length();\n+          \/\/ Walk monitors youngest to oldest\n+          for (int i = len - 1; i >= 0; i--) {\n+            MonitorInfo* mon_info = monitors->at(i);\n+            if (mon_info->eliminated()) continue;\n+            oop owner = mon_info->owner();\n+            if (owner != NULL) {\n+              info->append(mon_info);\n+            }\n+          }\n+        }\n+      }\n+      _executed = true;\n+    }\n+\n+   public:\n+    ReadMonitorsClosure() : HandshakeClosure(\"WB_HandshakeReadMonitors\"), _executed(false) {}\n+    jboolean executed() const { return _executed; }\n+  };\n+\n+  ReadMonitorsClosure rmc;\n+  oop thread_oop = JNIHandles::resolve(thread_handle);\n+  if (thread_oop != NULL) {\n+    JavaThread* target = java_lang_Thread::thread(thread_oop);\n+    Handshake::execute(&rmc, target);\n+  }\n+  return rmc.executed();\n+WB_END\n+\n@@ -2115,1 +2161,1 @@\n-      JavaThread* jt = th->as_Java_thread();\n+      JavaThread* jt = JavaThread::cast(th);\n@@ -2151,1 +2197,1 @@\n-      JavaThread* jt = th->as_Java_thread();\n+      JavaThread* jt = JavaThread::cast(th);\n@@ -2399,0 +2445,1 @@\n+  ResourceMark rm; \/\/ for verify\n@@ -2662,0 +2709,1 @@\n+  {CC\"handshakeReadMonitors\", CC\"(Ljava\/lang\/Thread;)Z\", (void*)&WB_HandshakeReadMonitors },\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":54,"deletions":6,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -528,0 +528,1 @@\n+  { \"FilterSpuriousWakeups\",        JDK_Version::jdk(18), JDK_Version::jdk(19), JDK_Version::jdk(20) },\n@@ -536,0 +537,2 @@\n+  { \"G1RSetRegionEntries\",          JDK_Version::undefined(), JDK_Version::jdk(18), JDK_Version::jdk(19) },\n+  { \"G1RSetSparseRegionEntries\",    JDK_Version::undefined(), JDK_Version::jdk(18), JDK_Version::jdk(19) },\n@@ -1111,1 +1114,8 @@\n-    st->print_cr(\"java_class_path (initial): %s\", strlen(path) == 0 ? \"<not set>\" : path );\n+    size_t len = strlen(path);\n+    st->print(\"java_class_path (initial): \");\n+    \/\/ Avoid using st->print_cr() because path length maybe longer than O_BUFLEN.\n+    if (len == 0) {\n+      st->print_raw_cr(\"<not set>\");\n+    } else {\n+      st->print_raw_cr(path, len);\n+    }\n@@ -3084,5 +3094,0 @@\n-  if (UseBiasedLocking) {\n-    jio_fprintf(defaultStream::error_stream(), \"Valhalla does not support use with UseBiasedLocking\");\n-    return JNI_ERR;\n-  }\n-\n@@ -3139,4 +3144,0 @@\n-    \/\/ Disable biased locking now as it interferes with the clean up of\n-    \/\/ the archived Klasses and Java string objects (at dump time only).\n-    UseBiasedLocking = false;\n-\n@@ -4020,0 +4021,5 @@\n+  if (log_is_enabled(Info, arguments)) {\n+    LogStream st(Log(arguments)::info());\n+    Arguments::print_on(&st);\n+  }\n+\n@@ -4056,20 +4062,0 @@\n-  \/\/ Turn off biased locking for locking debug mode flags,\n-  \/\/ which are subtly different from each other but neither works with\n-  \/\/ biased locking\n-  if (UseHeavyMonitors\n-#ifdef COMPILER1\n-      || !UseFastLocking\n-#endif \/\/ COMPILER1\n-#if INCLUDE_JVMCI\n-      || !JVMCIUseFastLocking\n-#endif\n-    ) {\n-    if (!FLAG_IS_DEFAULT(UseBiasedLocking) && UseBiasedLocking) {\n-      \/\/ flag set to true on command line; warn the user that they\n-      \/\/ can't enable biased locking here\n-      warning(\"Biased Locking is not supported with locking debug flags\"\n-              \"; ignoring UseBiasedLocking flag.\" );\n-    }\n-    UseBiasedLocking = false;\n-  }\n-\n@@ -4079,1 +4065,0 @@\n-  FLAG_SET_DEFAULT(UseBiasedLocking, false);\n@@ -4115,11 +4100,0 @@\n-  \/\/ Apply CPU specific policy for the BiasedLocking\n-  if (UseBiasedLocking) {\n-    if (!VM_Version::use_biased_locking() &&\n-        !(FLAG_IS_CMDLINE(UseBiasedLocking))) {\n-      UseBiasedLocking = false;\n-    }\n-  }\n-  if (!UseBiasedLocking) {\n-    UseOptoBiasInlining = false;\n-  }\n-\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":16,"deletions":42,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -62,1 +62,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -341,3 +340,0 @@\n-  \/\/ Revoke biases of objects with eliminated locks in the given frame.\n-  Deoptimization::revoke_for_object_deoptimization(deoptee_thread, deoptee, &map, thread);\n-\n@@ -413,4 +409,0 @@\n-  \/\/ Revoke biases, done with in java state.\n-  \/\/ No safepoints allowed after this\n-  revoke_from_deopt_handler(current, deoptee, &map);\n-\n@@ -899,1 +891,1 @@\n-    JavaThread* jt = thread->as_Java_thread();\n+    JavaThread* jt = JavaThread::cast(thread);\n@@ -1528,10 +1520,1 @@\n-        if (UseBiasedLocking && mark.has_bias_pattern()) {\n-          \/\/ New allocated objects may have the mark set to anonymously biased.\n-          \/\/ Also the deoptimized method may called methods with synchronization\n-          \/\/ where the thread-local object is bias locked to the current thread.\n-          assert(mark.is_biased_anonymously() ||\n-                 mark.biased_locker() == deoptee_thread, \"should be locked to current thread\");\n-          \/\/ Reset mark word to unbiased prototype.\n-          markWord unbiased_prototype = markWord::prototype().set_age(mark.age());\n-          obj->set_mark(unbiased_prototype);\n-        } else if (exec_mode == Unpack_none) {\n+        if (exec_mode == Unpack_none) {\n@@ -1540,2 +1523,1 @@\n-            \/\/ a callee frame. In this case the bias was revoked before in revoke_for_object_deoptimization().\n-            \/\/ Make the lock in the callee a recursive lock and restore the displaced header.\n+            \/\/ a callee frame. Make the lock in the callee a recursive lock and restore the displaced header.\n@@ -1682,92 +1664,0 @@\n-static void collect_monitors(compiledVFrame* cvf, GrowableArray<Handle>* objects_to_revoke,\n-                             bool only_eliminated) {\n-  GrowableArray<MonitorInfo*>* monitors = cvf->monitors();\n-  Thread* thread = Thread::current();\n-  for (int i = 0; i < monitors->length(); i++) {\n-    MonitorInfo* mon_info = monitors->at(i);\n-    if (mon_info->eliminated() == only_eliminated &&\n-        !mon_info->owner_is_scalar_replaced() &&\n-        mon_info->owner() != NULL) {\n-      objects_to_revoke->append(Handle(thread, mon_info->owner()));\n-    }\n-  }\n-}\n-\n-static void get_monitors_from_stack(GrowableArray<Handle>* objects_to_revoke, JavaThread* thread,\n-                                    frame fr, RegisterMap* map, bool only_eliminated) {\n-  \/\/ Unfortunately we don't have a RegisterMap available in most of\n-  \/\/ the places we want to call this routine so we need to walk the\n-  \/\/ stack again to update the register map.\n-  if (map == NULL || !map->update_map()) {\n-    StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n-    bool found = false;\n-    while (!found && !sfs.is_done()) {\n-      frame* cur = sfs.current();\n-      sfs.next();\n-      found = cur->id() == fr.id();\n-    }\n-    assert(found, \"frame to be deoptimized not found on target thread's stack\");\n-    map = sfs.register_map();\n-  }\n-\n-  vframe* vf = vframe::new_vframe(&fr, map, thread);\n-  compiledVFrame* cvf = compiledVFrame::cast(vf);\n-  \/\/ Revoke monitors' biases in all scopes\n-  while (!cvf->is_top()) {\n-    collect_monitors(cvf, objects_to_revoke, only_eliminated);\n-    cvf = compiledVFrame::cast(cvf->sender());\n-  }\n-  collect_monitors(cvf, objects_to_revoke, only_eliminated);\n-}\n-\n-void Deoptimization::revoke_from_deopt_handler(JavaThread* thread, frame fr, RegisterMap* map) {\n-  if (!UseBiasedLocking) {\n-    return;\n-  }\n-  assert(thread == Thread::current(), \"should be\");\n-  ResourceMark rm(thread);\n-  HandleMark hm(thread);\n-  GrowableArray<Handle>* objects_to_revoke = new GrowableArray<Handle>();\n-  get_monitors_from_stack(objects_to_revoke, thread, fr, map, false);\n-\n-  int len = objects_to_revoke->length();\n-  for (int i = 0; i < len; i++) {\n-    oop obj = (objects_to_revoke->at(i))();\n-    BiasedLocking::revoke_own_lock(thread, objects_to_revoke->at(i));\n-    assert(!obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n-  }\n-}\n-\n-\/\/ Revoke the bias of objects with eliminated locking to prepare subsequent relocking.\n-void Deoptimization::revoke_for_object_deoptimization(JavaThread* deoptee_thread, frame fr,\n-                                                      RegisterMap* map, JavaThread* thread) {\n-  if (!UseBiasedLocking) {\n-    return;\n-  }\n-  GrowableArray<Handle>* objects_to_revoke = new GrowableArray<Handle>();\n-  assert(KeepStackGCProcessedMark::stack_is_kept_gc_processed(deoptee_thread), \"must be\");\n-  \/\/ Collect monitors but only those with eliminated locking.\n-  get_monitors_from_stack(objects_to_revoke, deoptee_thread, fr, map, true);\n-\n-  int len = objects_to_revoke->length();\n-  for (int i = 0; i < len; i++) {\n-    oop obj = (objects_to_revoke->at(i))();\n-    markWord mark = obj->mark();\n-    if (!mark.has_bias_pattern() ||\n-        mark.is_biased_anonymously() || \/\/ eliminated locking does not bias an object if it wasn't before\n-        !obj->klass()->prototype_header().has_bias_pattern() || \/\/ bulk revoke ignores eliminated monitors\n-        (obj->klass()->prototype_header().bias_epoch() != mark.bias_epoch())) { \/\/ bulk rebias ignores eliminated monitors\n-      \/\/ We reach here regularly if there's just eliminated locking on obj.\n-      \/\/ We must not call BiasedLocking::revoke_own_lock() in this case, as we\n-      \/\/ would hit assertions because it is a prerequisite that there has to be\n-      \/\/ non-eliminated locking on obj by deoptee_thread.\n-      \/\/ Luckily we don't have to revoke here because obj has to be a\n-      \/\/ non-escaping obj and can be relocked without revoking the bias. See\n-      \/\/ Deoptimization::relock_objects().\n-      continue;\n-    }\n-    BiasedLocking::revoke(thread, objects_to_revoke->at(i));\n-    assert(!objects_to_revoke->at(i)->mark().has_bias_pattern(), \"biases should be revoked by now\");\n-  }\n-}\n-\n@@ -2010,1 +1900,0 @@\n-  \/\/ We need to update the map if we have biased locking.\n@@ -2015,1 +1904,1 @@\n-  RegisterMap reg_map(current, UseBiasedLocking);\n+  RegisterMap reg_map(current, false);\n@@ -2044,1 +1933,3 @@\n-    if (TraceDeoptimization) {\n+    bool is_receiver_constraint_failure = COMPILER2_PRESENT(VerifyReceiverTypes &&) (reason == Deoptimization::Reason_receiver_constraint);\n+\n+    if (TraceDeoptimization || is_receiver_constraint_failure) {\n@@ -2103,1 +1994,1 @@\n-    if (TraceDeoptimization || LogCompilation) {\n+    if (TraceDeoptimization || LogCompilation || is_receiver_constraint_failure) {\n@@ -2196,0 +2087,4 @@\n+    if (is_receiver_constraint_failure) {\n+      fatal(\"missing receiver type check\");\n+    }\n+\n@@ -2685,0 +2580,1 @@\n+  \"receiver_constraint\",\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":13,"deletions":117,"binary":false,"changes":130,"status":"modified"},{"patch":"@@ -92,0 +92,1 @@\n+    Reason_receiver_constraint,   \/\/ receiver subtype check failed\n@@ -154,7 +155,0 @@\n- private:\n-  \/\/ Revoke biased locks at deopt.\n-  static void revoke_from_deopt_handler(JavaThread* thread, frame fr, RegisterMap* map);\n-\n-  static void revoke_for_object_deoptimization(JavaThread* deoptee_thread, frame fr,\n-                                               RegisterMap* map, JavaThread* thread);\n-\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.hpp","additions":1,"deletions":7,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1112,3 +1112,1 @@\n-   \/\/ Nothing to do\n-   \/\/ receiver is a global ref\n-   \/\/ handle block is for JNI\n+    _cb->as_optimized_entry_blob()->oops_do(f, *this);\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -264,2 +264,1 @@\n-  \/\/ native methods so that biased locking can revoke the receiver's\n-  \/\/ bias if necessary.  This is also used by JVMTI's GetLocalInstance method\n+  \/\/ native methods. Used by JVMTI's GetLocalInstance method\n@@ -345,0 +344,1 @@\n+  bool optimized_entry_frame_is_first() const;\n","filename":"src\/hotspot\/share\/runtime\/frame.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -456,1 +456,2 @@\n-          \"Use only malloc\/free for allocation (no resource area\/arena)\")   \\\n+          \"Use only malloc\/free for allocation (no resource area\/arena). \"  \\\n+          \"Used to help diagnose memory stomping bugs.\")                    \\\n@@ -680,5 +681,0 @@\n-  product(bool, AlwaysLockClassLoader, false,                               \\\n-          \"(Deprecated) Require the VM to acquire the class loader lock \"   \\\n-          \"before calling loadClass() even for class loaders registering \"  \\\n-          \"as parallel capable\")                                            \\\n-                                                                            \\\n@@ -730,2 +726,2 @@\n-          \"When true prevents OS-level spurious, or premature, wakeups \"    \\\n-          \"from Object.wait (Ignored for Windows)\")                         \\\n+          \"(Deprecated) When true prevents OS-level spurious, or premature,\"\\\n+          \" wakeups from Object.wait (Ignored for Windows)\")                \\\n@@ -820,31 +816,0 @@\n-  product(bool, UseBiasedLocking, false,                                     \\\n-          \"(Deprecated) Enable biased locking in JVM (completely disabled by Valhalla)\") \\\n-                                                                            \\\n-  product(intx, BiasedLockingStartupDelay, 0,                               \\\n-          \"(Deprecated) Number of milliseconds to wait before enabling \"    \\\n-          \"biased locking\")                                                 \\\n-          range(0, (intx)(max_jint-(max_jint%PeriodicTask::interval_gran))) \\\n-          constraint(BiasedLockingStartupDelayFunc,AfterErgo)               \\\n-                                                                            \\\n-  product(bool, PrintBiasedLockingStatistics, false, DIAGNOSTIC,            \\\n-          \"(Deprecated) Print statistics of biased locking in JVM\")         \\\n-                                                                            \\\n-  product(intx, BiasedLockingBulkRebiasThreshold, 20,                       \\\n-          \"(Deprecated) Threshold of number of revocations per type to \"    \\\n-          \"try to rebias all objects in the heap of that type\")             \\\n-          range(0, max_intx)                                                \\\n-          constraint(BiasedLockingBulkRebiasThresholdFunc,AfterErgo)        \\\n-                                                                            \\\n-  product(intx, BiasedLockingBulkRevokeThreshold, 40,                       \\\n-          \"(Deprecated) Threshold of number of revocations per type to \"    \\\n-          \"permanently revoke biases of all objects in the heap of that \"   \\\n-          \"type\")                                                           \\\n-          range(0, max_intx)                                                \\\n-          constraint(BiasedLockingBulkRevokeThresholdFunc,AfterErgo)        \\\n-                                                                            \\\n-  product(intx, BiasedLockingDecayTime, 25000,                              \\\n-          \"(Deprecated) Decay time (in milliseconds) to re-enable bulk \"    \\\n-          \"rebiasing of a type after previous bulk rebias\")                 \\\n-          range(500, max_intx)                                              \\\n-          constraint(BiasedLockingDecayTimeFunc,AfterErgo)                  \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":4,"deletions":39,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -197,5 +197,2 @@\n-#ifdef ASSERT\n-    oop* handle = (oop*) (UseMallocOnly ? internal_malloc_4(oopSize) : Amalloc_4(oopSize));\n-#else\n-    oop* handle = (oop*) Amalloc_4(oopSize);\n-#endif\n+    \/\/ Ignore UseMallocOnly by allocating only in arena.\n+    oop* handle = (oop*)internal_amalloc(oopSize);\n","filename":"src\/hotspot\/share\/runtime\/handles.hpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -212,1 +212,1 @@\n-           is_frame_handle(thread->as_Java_thread(), handle))) {\n+           is_frame_handle(JavaThread::cast(thread), handle))) {\n@@ -307,1 +307,1 @@\n-          thread->as_Java_thread()->thread_state() == _thread_in_native);\n+          JavaThread::cast(thread)->thread_state() == _thread_in_native);\n@@ -321,1 +321,1 @@\n-      JavaThread* THREAD = Thread::current()->as_Java_thread();\n+      JavaThread* THREAD = JavaThread::current();\n","filename":"src\/hotspot\/share\/runtime\/jniHandles.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -514,1 +514,1 @@\n-      StackWatermarkSet::start_processing(thread->as_Java_thread(), StackWatermarkKind::gc);\n+      StackWatermarkSet::start_processing(JavaThread::cast(thread), StackWatermarkKind::gc);\n@@ -934,0 +934,1 @@\n+    HandleMark hm(self);\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -64,1 +64,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -147,1 +146,0 @@\n-int SharedRuntime::_throw_null_ctr = 0;\n@@ -162,1 +160,0 @@\n-int SharedRuntime::_multi1_ctr=0;\n@@ -999,1 +996,1 @@\n-      oop obj = thread->as_Java_thread()->threadObj();\n+      oop obj = JavaThread::cast(thread)->threadObj();\n@@ -2190,3 +2187,0 @@\n-  if (PrintBiasedLockingStatistics) {\n-    Atomic::inc(BiasedLocking::slow_path_entry_count_addr());\n-  }\n@@ -2230,2 +2224,0 @@\n-  if (_throw_null_ctr) tty->print_cr(\"%5d implicit null throw\", _throw_null_ctr);\n-\n@@ -2237,1 +2229,0 @@\n-  if (_multi1_ctr) tty->print_cr(\"%5d multianewarray 1 dim\", _multi1_ctr);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":1,"deletions":10,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -569,1 +569,0 @@\n-  static int _throw_null_ctr;                    \/\/ throwing a null-pointer exception\n@@ -590,1 +589,1 @@\n-  static int _multi1_ctr, _multi2_ctr, _multi3_ctr, _multi4_ctr, _multi5_ctr;\n+  static int _multi2_ctr, _multi3_ctr, _multi4_ctr, _multi5_ctr;\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -392,1 +392,1 @@\n-  JavaThread* THREAD = Thread::current()->as_Java_thread();\n+  JavaThread* THREAD = JavaThread::current();\n","filename":"src\/hotspot\/share\/runtime\/signature.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,1 +37,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -130,1 +129,1 @@\n-      ObjectSynchronizer::chk_for_block_req(current->as_Java_thread(), \"unlinking\",\n+      ObjectSynchronizer::chk_for_block_req(JavaThread::cast(current), \"unlinking\",\n@@ -318,1 +317,1 @@\n-  \/\/ biased locking and any other IMS exception states take the slow-path\n+  \/\/ other IMS exception states take the slow-path\n@@ -368,2 +367,1 @@\n-    \/\/ Biased Locking in the object's header, the second check is for\n-    \/\/ stack-locking in the object's header, the third check is for\n+    \/\/ stack-locking in the object's header, the second check is for\n@@ -383,1 +381,0 @@\n-  \/\/ -- perform bias revocation, or\n@@ -451,6 +448,0 @@\n-  if (UseBiasedLocking) {\n-    BiasedLocking::revoke(current, obj);\n-  }\n-\n-  assert(!UseBiasedLocking || !mark.has_bias_pattern(), \"should not see bias pattern here\");\n-\n@@ -496,4 +487,0 @@\n-  \/\/ We cannot check for Biased Locking if we are racing an inflation.\n-  assert(mark == markWord::INFLATING() ||\n-         !UseBiasedLocking ||\n-         !mark.has_bias_pattern(), \"should not see bias pattern here\");\n@@ -561,4 +548,0 @@\n-  if (UseBiasedLocking) {\n-    BiasedLocking::revoke(current, obj);\n-    assert(!obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n-  }\n@@ -576,4 +559,0 @@\n-  if (UseBiasedLocking) {\n-    BiasedLocking::revoke(current, obj);\n-    assert(!obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n-  }\n@@ -600,0 +579,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -602,5 +582,0 @@\n-  CHECK_THROW_NOSYNC_IMSE(obj);\n-  if (UseBiasedLocking) {\n-    BiasedLocking::revoke(current, obj);\n-    assert(!obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n-  }\n@@ -624,6 +599,0 @@\n-  if (UseBiasedLocking) {\n-    Handle h_obj(current, obj);\n-    BiasedLocking::revoke(current, h_obj);\n-    obj = h_obj();\n-    assert(!obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n-  }\n@@ -668,4 +637,0 @@\n-  if (UseBiasedLocking) {\n-    BiasedLocking::revoke(current, obj);\n-    assert(!obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n-  }\n@@ -695,4 +660,0 @@\n-  if (UseBiasedLocking) {\n-    BiasedLocking::revoke(current, obj);\n-    assert(!obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n-  }\n@@ -709,4 +670,0 @@\n-  if (UseBiasedLocking) {\n-    BiasedLocking::revoke(current, obj);\n-    assert(!obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n-  }\n@@ -729,4 +686,0 @@\n-  if (UseBiasedLocking) {\n-    BiasedLocking::revoke(current, obj);\n-    assert(!obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n-  }\n@@ -883,19 +836,0 @@\n-  if (UseBiasedLocking) {\n-    \/\/ NOTE: many places throughout the JVM do not expect a safepoint\n-    \/\/ to be taken here. However, we only ever bias Java instances and all\n-    \/\/ of the call sites of identity_hash that might revoke biases have\n-    \/\/ been checked to make sure they can handle a safepoint. The\n-    \/\/ added check of the bias pattern is to avoid useless calls to\n-    \/\/ thread-local storage.\n-    if (obj->mark().has_bias_pattern()) {\n-      \/\/ Handle for oop obj in case of STW safepoint\n-      Handle hobj(current, obj);\n-      if (SafepointSynchronize::is_at_safepoint()) {\n-        BiasedLocking::revoke_at_safepoint(hobj);\n-      } else {\n-        BiasedLocking::revoke(current->as_Java_thread(), hobj);\n-      }\n-      obj = hobj();\n-      assert(!obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n-    }\n-  }\n@@ -909,3 +843,0 @@\n-    \/\/ object should remain ineligible for biased locking\n-    assert(!UseBiasedLocking || !mark.has_bias_pattern(), \"invariant\");\n-\n@@ -1019,5 +950,0 @@\n-  if (UseBiasedLocking) {\n-    BiasedLocking::revoke(current, h_obj);\n-    assert(!h_obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n-  }\n-\n@@ -1047,9 +973,0 @@\n-  if (UseBiasedLocking) {\n-    if (SafepointSynchronize::is_at_safepoint()) {\n-      BiasedLocking::revoke_at_safepoint(h_obj);\n-    } else {\n-      BiasedLocking::revoke(JavaThread::current(), h_obj);\n-    }\n-    assert(!h_obj->mark().has_bias_pattern(), \"biases should be revoked by now\");\n-  }\n-\n@@ -1240,1 +1157,0 @@\n-    assert(!UseBiasedLocking || !mark.has_bias_pattern(), \"invariant\");\n@@ -1247,1 +1163,0 @@\n-    \/\/ *  BIASED       - Illegal.  We should never see this\n@@ -1455,1 +1370,1 @@\n-      chk_for_block_req(current->as_Java_thread(), \"deflation\", \"deflated_count\",\n+      chk_for_block_req(JavaThread::cast(current), \"deflation\", \"deflated_count\",\n@@ -1545,1 +1460,1 @@\n-        chk_for_block_req(current->as_Java_thread(), \"deletion\", \"deleted_count\",\n+        chk_for_block_req(JavaThread::cast(current), \"deletion\", \"deleted_count\",\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":6,"deletions":91,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -80,1 +80,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -167,1 +166,1 @@\n-      const char* name = (javathread)->get_thread_name();                  \\\n+      const char* name = (javathread)->name();                             \\\n@@ -188,20 +187,1 @@\n-\/\/ Support for forcing alignment of thread objects for biased locking\n-  if (UseBiasedLocking) {\n-    const size_t alignment = markWord::biased_lock_alignment;\n-    size_t aligned_size = size + (alignment - sizeof(intptr_t));\n-    void* real_malloc_addr = throw_excpt? AllocateHeap(aligned_size, flags, CURRENT_PC)\n-                                          : AllocateHeap(aligned_size, flags, CURRENT_PC,\n-                                                         AllocFailStrategy::RETURN_NULL);\n-    void* aligned_addr     = align_up(real_malloc_addr, alignment);\n-    assert(((uintptr_t) aligned_addr + (uintptr_t) size) <=\n-           ((uintptr_t) real_malloc_addr + (uintptr_t) aligned_size),\n-           \"JavaThread alignment code overflowed allocated storage\");\n-    if (aligned_addr != real_malloc_addr) {\n-      log_info(biasedlocking)(\"Aligned thread \" INTPTR_FORMAT \" to \" INTPTR_FORMAT,\n-                              p2i(real_malloc_addr),\n-                              p2i(aligned_addr));\n-    }\n-    ((Thread*) aligned_addr)->_real_malloc_address = real_malloc_addr;\n-    return aligned_addr;\n-  } else {\n-    return throw_excpt? AllocateHeap(size, flags, CURRENT_PC)\n+  return throw_excpt ? AllocateHeap(size, flags, CURRENT_PC)\n@@ -210,1 +190,0 @@\n-  }\n@@ -214,5 +193,1 @@\n-  if (UseBiasedLocking) {\n-    FreeHeap(((Thread*) p)->_real_malloc_address);\n-  } else {\n-    FreeHeap(p);\n-  }\n+  FreeHeap(p);\n@@ -292,8 +267,0 @@\n-#ifdef ASSERT\n-  if (UseBiasedLocking) {\n-    assert(is_aligned(this, markWord::biased_lock_alignment), \"forced alignment of thread object failed\");\n-    assert(this == _real_malloc_address ||\n-           this == align_up(_real_malloc_address, markWord::biased_lock_alignment),\n-           \"bug in forced alignment of thread objects\");\n-  }\n-#endif \/\/ ASSERT\n@@ -353,1 +320,1 @@\n-    as_Java_thread()->stack_overflow_state()->initialize(stack_base(), stack_end());\n+    JavaThread::cast(this)->stack_overflow_state()->initialize(stack_base(), stack_end());\n@@ -465,2 +432,2 @@\n-         thread->as_Java_thread()->is_handshake_safe_for(Thread::current()) ||\n-         !thread->as_Java_thread()->on_thread_list() ||\n+         JavaThread::cast(thread)->is_handshake_safe_for(Thread::current()) ||\n+         !JavaThread::cast(thread)->on_thread_list() ||\n@@ -468,1 +435,1 @@\n-         ThreadsSMRSupport::is_a_protected_JavaThread_with_lock(thread->as_Java_thread()),\n+         ThreadsSMRSupport::is_a_protected_JavaThread_with_lock(JavaThread::cast(thread)),\n@@ -483,0 +450,7 @@\n+  \/\/ If the target hasn't been started yet then it is trivially\n+  \/\/ \"protected\". We assume the caller is the thread that will do\n+  \/\/ the starting.\n+  if (p->osthread() == NULL || p->osthread()->get_state() <= INITIALIZED) {\n+    return true;\n+  }\n+\n@@ -540,1 +514,1 @@\n-    java_lang_Thread::set_thread_status(thread->as_Java_thread()->threadObj(),\n+    java_lang_Thread::set_thread_status(JavaThread::cast(thread)->threadObj(),\n@@ -646,9 +620,1 @@\n-  if (is_VM_thread())                 { st->print(\"VMThread\"); }\n-  else if (is_GC_task_thread())       { st->print(\"GCTaskThread\"); }\n-  else if (is_Watcher_thread())       { st->print(\"WatcherThread\"); }\n-  else if (is_ConcurrentGC_thread())  { st->print(\"ConcurrentGCThread\"); }\n-  else                                { st->print(\"Thread\"); }\n-\n-  if (is_Named_thread()) {\n-    st->print(\" \\\"%s\\\"\", name());\n-  }\n+  st->print(\"%s \\\"%s\\\"\", type_name(), name());\n@@ -707,1 +673,1 @@\n-  return os::create_main_thread(this->as_Java_thread());\n+  return os::create_main_thread(JavaThread::cast(this));\n@@ -1084,1 +1050,0 @@\n-  _cached_monitor_info(nullptr),\n@@ -1243,5 +1208,0 @@\n-    if (jvmci_counters_include(this)) {\n-      for (int i = 0; i < JVMCICounterSize; i++) {\n-        _jvmci_old_thread_counters[i] += _jvmci_counters[i];\n-      }\n-    }\n@@ -1308,1 +1268,1 @@\n-      this->set_native_thread_name(this->get_thread_name());\n+      this->set_native_thread_name(this->name());\n@@ -1387,1 +1347,1 @@\n-                    get_thread_name());\n+                    name());\n@@ -1492,1 +1452,1 @@\n-    thread_name = os::strdup(get_thread_name());\n+    thread_name = os::strdup(name());\n@@ -1503,0 +1463,11 @@\n+\n+#if INCLUDE_JVMCI\n+  if (JVMCICounterSize > 0) {\n+    if (jvmci_counters_include(this)) {\n+      for (int i = 0; i < JVMCICounterSize; i++) {\n+        _jvmci_old_thread_counters[i] += _jvmci_counters[i];\n+      }\n+    }\n+  }\n+#endif \/\/ INCLUDE_JVMCI\n+\n@@ -1549,1 +1520,1 @@\n-    return thread->as_Java_thread();\n+    return JavaThread::cast(thread);\n@@ -1553,1 +1524,1 @@\n-    JavaThread *ret = op == NULL ? NULL : op->calling_thread()->as_Java_thread();\n+    JavaThread *ret = op == NULL ? NULL : JavaThread::cast(op->calling_thread());\n@@ -1711,1 +1682,1 @@\n-    JavaThread* target = thr->as_Java_thread();\n+    JavaThread* target = JavaThread::cast(thr);\n@@ -1962,0 +1933,9 @@\n+#ifdef ASSERT\n+void JavaThread::verify_frame_info() {\n+  assert((!has_last_Java_frame() && java_call_counter() == 0) ||\n+         (has_last_Java_frame() && java_call_counter() > 0),\n+         \"unexpected frame info: has_last_frame=%s, java_call_counter=%d\",\n+         has_last_Java_frame() ? \"true\" : \"false\", java_call_counter());\n+}\n+#endif\n+\n@@ -1969,2 +1949,1 @@\n-  assert((!has_last_Java_frame() && java_call_counter() == 0) ||\n-         (has_last_Java_frame() && java_call_counter() > 0), \"wrong java_sp info!\");\n+  DEBUG_ONLY(verify_frame_info();)\n@@ -2018,4 +1997,1 @@\n-  assert((!has_last_Java_frame() && java_call_counter() == 0) ||\n-         (has_last_Java_frame() && java_call_counter() > 0),\n-         \"unexpected frame info: has_last_frame=%d, java_call_counter=%d\",\n-         has_last_Java_frame(), java_call_counter());\n+  verify_frame_info();\n@@ -2026,4 +2002,1 @@\n-  assert((!has_last_Java_frame() && java_call_counter() == 0) ||\n-         (has_last_Java_frame() && java_call_counter() > 0),\n-         \"unexpected frame info: has_last_frame=%d, java_call_counter=%d\",\n-         has_last_Java_frame(), java_call_counter());\n+  DEBUG_ONLY(verify_frame_info();)\n@@ -2089,1 +2062,1 @@\n-  st->print_raw(get_thread_name());\n+  st->print_raw(name());\n@@ -2127,1 +2100,1 @@\n-  st->print(\"JavaThread \\\"%s\\\"\", get_thread_name_string(buf, buflen));\n+  st->print(\"%s \\\"%s\\\"\", type_name(), get_thread_name_string(buf, buflen));\n@@ -2175,1 +2148,1 @@\n-const char* JavaThread::get_thread_name() const {\n+const char* JavaThread::name() const  {\n@@ -2186,1 +2159,1 @@\n-\/\/ descriptive string if there is no set name\n+\/\/ descriptive string if there is no set name.\n@@ -2201,1 +2174,1 @@\n-      name_str = Thread::name();\n+      name_str = \"<un-named>\";\n@@ -2210,0 +2183,13 @@\n+\/\/ Helper to extract the name from the thread oop for logging.\n+const char* JavaThread::name_for(oop thread_obj) {\n+  assert(thread_obj != NULL, \"precondition\");\n+  oop name = java_lang_Thread::name(thread_obj);\n+  const char* name_str;\n+  if (name != NULL) {\n+    name_str = java_lang_String::as_utf8_string(name);\n+  } else {\n+    name_str = \"<un-named>\";\n+  }\n+  return name_str;\n+}\n+\n@@ -3052,2 +3038,0 @@\n-  BiasedLocking::init();\n-\n@@ -3300,1 +3284,1 @@\n-  \/\/ it will cause MetaspaceShared::link_and_cleanup_shared_classes to\n+  \/\/ it will cause MetaspaceShared::link_shared_classes to\n@@ -3311,1 +3295,1 @@\n-    DynamicArchive::prepare_for_dynamic_dumping_at_exit();\n+    DynamicArchive::prepare_for_dynamic_dumping();\n@@ -3942,0 +3926,78 @@\n+\n+\/\/ Helper function to create the java.lang.Thread object for a\n+\/\/ VM-internal thread. The thread will have the given name, be\n+\/\/ part of the System ThreadGroup and if is_visible is true will be\n+\/\/ discoverable via the system ThreadGroup.\n+Handle JavaThread::create_system_thread_object(const char* name,\n+                                               bool is_visible, TRAPS) {\n+  Handle string = java_lang_String::create_from_str(name, CHECK_NH);\n+\n+  \/\/ Initialize thread_oop to put it into the system threadGroup.\n+  \/\/ This is done by calling the Thread(ThreadGroup tg, String name)\n+  \/\/ constructor, which adds the new thread to the group as an unstarted\n+  \/\/ thread.\n+  Handle thread_group(THREAD, Universe::system_thread_group());\n+  Handle thread_oop =\n+    JavaCalls::construct_new_instance(vmClasses::Thread_klass(),\n+                                      vmSymbols::threadgroup_string_void_signature(),\n+                                      thread_group,\n+                                      string,\n+                                      CHECK_NH);\n+\n+  \/\/ If the Thread is intended to be visible then we have to mimic what\n+  \/\/ Thread.start() would do, by adding it to its ThreadGroup: tg.add(t).\n+  if (is_visible) {\n+    Klass* group = vmClasses::ThreadGroup_klass();\n+    JavaValue result(T_VOID);\n+    JavaCalls::call_special(&result,\n+                            thread_group,\n+                            group,\n+                            vmSymbols::add_method_name(),\n+                            vmSymbols::thread_void_signature(),\n+                            thread_oop,\n+                            CHECK_NH);\n+  }\n+\n+  return thread_oop;\n+}\n+\n+\/\/ Starts the target JavaThread as a daemon of the given priority, and\n+\/\/ bound to the given java.lang.Thread instance.\n+\/\/ The Threads_lock is held for the duration.\n+void JavaThread::start_internal_daemon(JavaThread* current, JavaThread* target,\n+                                       Handle thread_oop, ThreadPriority prio) {\n+\n+  assert(target->osthread() != NULL, \"target thread is not properly initialized\");\n+\n+  MutexLocker mu(current, Threads_lock);\n+\n+  \/\/ Initialize the fields of the thread_oop first.\n+\n+  java_lang_Thread::set_thread(thread_oop(), target); \/\/ isAlive == true now\n+\n+  if (prio != NoPriority) {\n+    java_lang_Thread::set_priority(thread_oop(), prio);\n+    \/\/ Note: we don't call os::set_priority here. Possibly we should,\n+    \/\/ else all threads should call it themselves when they first run.\n+  }\n+\n+  java_lang_Thread::set_daemon(thread_oop());\n+\n+  \/\/ Now bind the thread_oop to the target JavaThread.\n+  target->set_threadObj(thread_oop());\n+\n+  Threads::add(target); \/\/ target is now visible for safepoint\/handshake\n+  Thread::start(target);\n+}\n+\n+void JavaThread::vm_exit_on_osthread_failure(JavaThread* thread) {\n+  \/\/ At this point it may be possible that no osthread was created for the\n+  \/\/ JavaThread due to lack of resources. However, since this must work\n+  \/\/ for critical system threads just check and abort if this fails.\n+  if (thread->osthread() == nullptr) {\n+    \/\/ This isn't really an OOM condition, but historically this is what\n+    \/\/ we report.\n+    vm_exit_during_initialization(\"java.lang.OutOfMemoryError\",\n+                                  os::native_thread_creation_failed_msg());\n+  }\n+}\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":144,"deletions":82,"binary":false,"changes":226,"status":"modified"},{"patch":"@@ -57,1 +57,0 @@\n-\n@@ -167,3 +166,0 @@\n-  \/\/ Support for forcing alignment of thread objects for biased locking\n-  void*       _real_malloc_address;\n-\n@@ -358,3 +354,5 @@\n-  \/\/ Casts\n-  inline JavaThread* as_Java_thread();\n-  inline const JavaThread* as_Java_thread() const;\n+  \/\/ All threads are given names. For singleton subclasses we can\n+  \/\/ just hard-wire the known name of the instance. JavaThreads and\n+  \/\/ NamedThreads support multiple named instances, and dynamic\n+  \/\/ changing of the name of an instance.\n+  virtual const char* name() const { return \"Unknown thread\"; }\n@@ -362,1 +360,3 @@\n-  virtual char* name() const { return (char*)\"Unknown thread\"; }\n+  \/\/ A thread's type name is also made available for debugging\n+  \/\/ and logging.\n+  virtual const char* type_name() const { return \"Thread\"; }\n@@ -578,0 +578,1 @@\n+  \/\/ Basic, non-virtual, printing support that is simple and always safe.\n@@ -1130,0 +1131,2 @@\n+  DEBUG_ONLY(void verify_frame_info();)\n+\n@@ -1363,0 +1366,3 @@\n+  \/\/ factor out low-level mechanics for use in both normal and error cases\n+  const char* get_thread_name_string(char* buf = NULL, int buflen = 0) const;\n+\n@@ -1382,1 +1388,4 @@\n-  char* name() const { return (char*)get_thread_name(); }\n+  const char* name() const;\n+  const char* type_name() const { return \"JavaThread\"; }\n+  static const char* name_for(oop thread_obj);\n+\n@@ -1390,5 +1399,1 @@\n-  const char* get_thread_name() const;\n- protected:\n-  \/\/ factor out low-level mechanics for use in both normal and error cases\n-  virtual const char* get_thread_name_string(char* buf = NULL, int buflen = 0) const;\n- public:\n+\n@@ -1429,1 +1434,17 @@\n-  static inline JavaThread* current();\n+  static JavaThread* current() {\n+    return JavaThread::cast(Thread::current());\n+  }\n+\n+  \/\/ Returns the current thread as a JavaThread, or NULL if not attached\n+  static inline JavaThread* current_or_null();\n+\n+  \/\/ Casts\n+  static JavaThread* cast(Thread* t) {\n+    assert(t->is_Java_thread(), \"incorrect cast to JavaThread\");\n+    return static_cast<JavaThread*>(t);\n+  }\n+\n+  static const JavaThread* cast(const Thread* t) {\n+    assert(t->is_Java_thread(), \"incorrect cast to const JavaThread\");\n+    return static_cast<const JavaThread*>(t);\n+  }\n@@ -1560,6 +1581,0 @@\n-  \/\/ Biased locking support\n- private:\n-  GrowableArray<MonitorInfo*>* _cached_monitor_info;\n-  GrowableArray<MonitorInfo*>* cached_monitor_info() { return _cached_monitor_info; }\n-  void set_cached_monitor_info(GrowableArray<MonitorInfo*>* info) { _cached_monitor_info = info; }\n-\n@@ -1593,5 +1608,5 @@\n-};\n-\/\/ Inline implementation of JavaThread::current\n-inline JavaThread* JavaThread::current() {\n-  return Thread::current()->as_Java_thread();\n-}\n+  \/\/ Helper function to create the java.lang.Thread object for a\n+  \/\/ VM-internal thread. The thread will have the given name, be\n+  \/\/ part of the System ThreadGroup and if is_visible is true will be\n+  \/\/ discoverable via the system ThreadGroup.\n+  static Handle create_system_thread_object(const char* name, bool is_visible, TRAPS);\n@@ -1600,4 +1615,9 @@\n-inline JavaThread* Thread::as_Java_thread() {\n-  assert(is_Java_thread(), \"incorrect cast to JavaThread\");\n-  return static_cast<JavaThread*>(this);\n-}\n+  \/\/ Helper function to start a VM-internal daemon thread.\n+  \/\/ E.g. ServiceThread, NotificationThread, CompilerThread etc.\n+  static void start_internal_daemon(JavaThread* current, JavaThread* target,\n+                                    Handle thread_oop, ThreadPriority prio);\n+\n+  \/\/ Helper function to do vm_exit_on_initialization for osthread\n+  \/\/ resource allocation failure.\n+  static void vm_exit_on_osthread_failure(JavaThread* thread);\n+};\n@@ -1605,3 +1625,3 @@\n-inline const JavaThread* Thread::as_Java_thread() const {\n-  assert(is_Java_thread(), \"incorrect cast to const JavaThread\");\n-  return static_cast<const JavaThread*>(this);\n+inline JavaThread* JavaThread::current_or_null() {\n+  Thread* current = Thread::current_or_null();\n+  return current != nullptr ? JavaThread::cast(current) : nullptr;\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":53,"deletions":33,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -235,3 +235,2 @@\n-    \/\/ This monitor is really only needed for UseBiasedLocking, but\n-    \/\/ return it in all cases for now as it might be useful for stack\n-    \/\/ traces and tools as well\n+    \/\/ This monitor is not really needed but return it for now as it might be\n+    \/\/ useful for stack traces and tools\n","filename":"src\/hotspot\/share\/runtime\/vframe_hp.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -50,1 +50,0 @@\n-  template(PrintJNI)                              \\\n@@ -71,2 +70,0 @@\n-  template(EnableBiasedLocking)                   \\\n-  template(BulkRevokeBias)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -156,1 +156,1 @@\n-  calling_thread()->as_Java_thread()->make_zombies();\n+  JavaThread::cast(calling_thread())->make_zombies();\n@@ -171,0 +171,3 @@\n+  if (_print_jni_handle_info) {\n+    JNIHandles::print_on(_out);\n+  }\n@@ -180,4 +183,0 @@\n-void VM_PrintJNI::doit() {\n-  JNIHandles::print_on(_out);\n-}\n-\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -146,0 +146,1 @@\n+  bool _print_jni_handle_info;\n@@ -148,1 +149,1 @@\n-    : _out(tty), _print_concurrent_locks(PrintConcurrentLocks), _print_extended_info(false)\n+    : _out(tty), _print_concurrent_locks(PrintConcurrentLocks), _print_extended_info(false), _print_jni_handle_info(false)\n@@ -150,2 +151,3 @@\n-  VM_PrintThreads(outputStream* out, bool print_concurrent_locks, bool print_extended_info)\n-    : _out(out), _print_concurrent_locks(print_concurrent_locks), _print_extended_info(print_extended_info)\n+  VM_PrintThreads(outputStream* out, bool print_concurrent_locks, bool print_extended_info, bool print_jni_handle_info)\n+    : _out(out), _print_concurrent_locks(print_concurrent_locks), _print_extended_info(print_extended_info),\n+      _print_jni_handle_info(print_jni_handle_info)\n@@ -161,10 +163,0 @@\n-class VM_PrintJNI: public VM_Operation {\n- private:\n-  outputStream* _out;\n- public:\n-  VM_PrintJNI()                         { _out = tty; }\n-  VM_PrintJNI(outputStream* out)        { _out = out; }\n-  VMOp_Type type() const                { return VMOp_PrintJNI; }\n-  void doit();\n-};\n-\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.hpp","additions":5,"deletions":13,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -169,5 +169,0 @@\n-typedef HashtableEntry<intptr_t, mtInternal>  IntptrHashtableEntry;\n-typedef Hashtable<intptr_t, mtInternal>       IntptrHashtable;\n-typedef Hashtable<InstanceKlass*, mtClass>       KlassHashtable;\n-typedef HashtableEntry<InstanceKlass*, mtClass>  KlassHashtableEntry;\n-\n@@ -269,1 +264,0 @@\n-  nonstatic_field(Klass,                       _prototype_header,                             markWord)                              \\\n@@ -474,21 +468,0 @@\n-  \/* HashtableBucket *\/                                                                                                              \\\n-  \/*******************\/                                                                                                              \\\n-                                                                                                                                     \\\n-  nonstatic_field(HashtableBucket<mtInternal>, _entry,                                        BasicHashtableEntry<mtInternal>*)      \\\n-                                                                                                                                     \\\n-  \/******************\/                                                                                                               \\\n-  \/* HashtableEntry *\/                                                                                                               \\\n-  \/******************\/                                                                                                               \\\n-                                                                                                                                     \\\n-  nonstatic_field(BasicHashtableEntry<mtInternal>, _next,                                     BasicHashtableEntry<mtInternal>*)      \\\n-  nonstatic_field(BasicHashtableEntry<mtInternal>, _hash,                                     unsigned int)                          \\\n-  nonstatic_field(IntptrHashtableEntry,            _literal,                                  intptr_t)                              \\\n-                                                                                                                                     \\\n-  \/*************\/                                                                                                                    \\\n-  \/* Hashtable *\/                                                                                                                    \\\n-  \/*************\/                                                                                                                    \\\n-                                                                                                                                     \\\n-  nonstatic_field(BasicHashtable<mtInternal>,  _table_size,                                   int)                                   \\\n-  nonstatic_field(BasicHashtable<mtInternal>,  _buckets,                                      HashtableBucket<mtInternal>*)          \\\n-                                                                                                                                     \\\n-  \/*******************\/                                                                                                              \\\n@@ -501,1 +474,0 @@\n-  volatile_nonstatic_field(ClassLoaderData,    _dictionary,                                   Dictionary*)                           \\\n@@ -884,2 +856,2 @@\n-  nonstatic_field(ObjectMonitor,               _contentions,                                  jint)                                  \\\n-  volatile_nonstatic_field(ObjectMonitor,      _waiters,                                      jint)                                  \\\n+  nonstatic_field(ObjectMonitor,               _contentions,                                  int)                                   \\\n+  volatile_nonstatic_field(ObjectMonitor,      _waiters,                                      int)                                   \\\n@@ -1313,8 +1285,0 @@\n-  declare_toplevel_type(BasicHashtable<mtInternal>)                       \\\n-    declare_type(IntptrHashtable, BasicHashtable<mtInternal>)             \\\n-  declare_toplevel_type(BasicHashtable<mtSymbol>)                         \\\n-    declare_type(Dictionary, KlassHashtable)                              \\\n-  declare_toplevel_type(BasicHashtableEntry<mtInternal>)                  \\\n-  declare_type(IntptrHashtableEntry, BasicHashtableEntry<mtInternal>)     \\\n-    declare_type(DictionaryEntry, KlassHashtableEntry)                    \\\n-  declare_toplevel_type(HashtableBucket<mtInternal>)                      \\\n@@ -2410,0 +2374,1 @@\n+  declare_constant(Deoptimization::Reason_receiver_constraint)            \\\n@@ -2637,1 +2602,0 @@\n-  declare_constant(markWord::biased_lock_bits)                            \\\n@@ -2642,1 +2606,0 @@\n-  declare_constant(markWord::biased_lock_shift)                           \\\n@@ -2648,3 +2611,0 @@\n-  declare_constant(markWord::biased_lock_mask)                            \\\n-  declare_constant(markWord::biased_lock_mask_in_place)                   \\\n-  declare_constant(markWord::biased_lock_bit_in_place)                    \\\n@@ -2653,2 +2613,0 @@\n-  declare_constant(markWord::epoch_mask)                                  \\\n-  declare_constant(markWord::epoch_mask_in_place)                         \\\n@@ -2657,1 +2615,0 @@\n-  declare_constant(markWord::biased_lock_alignment)                       \\\n@@ -2663,1 +2620,0 @@\n-  declare_constant(markWord::biased_lock_pattern)                         \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":3,"deletions":47,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -471,1 +471,3 @@\n-               \"9 the strongest compression.\", \"INT\", false, \"1\") {\n+               \"9 the strongest compression.\", \"INT\", false, \"1\"),\n+  _overwrite(\"-overwrite\", \"If specified, the dump file will be overwritten if it exists\",\n+           \"BOOLEAN\", false, \"false\") {\n@@ -475,0 +477,1 @@\n+  _dcmdparser.add_dcmd_option(&_overwrite);\n@@ -493,1 +496,1 @@\n-  dumper.dump(_filename.value(), output(), (int) level);\n+  dumper.dump(_filename.value(), output(), (int) level, _overwrite.value());\n@@ -537,2 +540,2 @@\n-  \/\/ thread stacks\n-  VM_PrintThreads op1(output(), _locks.value(), _extended.value());\n+  \/\/ thread stacks and JNI global handles\n+  VM_PrintThreads op1(output(), _locks.value(), _extended.value(), true \/* print JNI handle info *\/);\n@@ -541,6 +544,2 @@\n-  \/\/ JNI global handles\n-  VM_PrintJNI op2(output());\n-  VMThread::execute(&op2);\n-\n-  VM_FindDeadlocks op3(output());\n-  VMThread::execute(&op3);\n+  VM_FindDeadlocks op2(output());\n+  VMThread::execute(&op2);\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":9,"deletions":10,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -317,0 +317,1 @@\n+  DCmdArgument<bool> _overwrite;\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1908,1 +1908,1 @@\n-int HeapDumper::dump(const char* path, outputStream* out, int compression) {\n+int HeapDumper::dump(const char* path, outputStream* out, int compression, bool overwrite) {\n@@ -1931,1 +1931,1 @@\n-  DumpWriter writer(new (std::nothrow) FileWriter(path), compressor);\n+  DumpWriter writer(new (std::nothrow) FileWriter(path, overwrite), compressor);\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1216,1 +1216,1 @@\n-\/\/ Default hash\/equals functions used by ResourceHashtable and KVHashtable\n+\/\/ Default hash\/equals functions used by ResourceHashtable\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -626,1 +626,1 @@\n-            cw.visit(V1_6, NOT_ACC_PUBLIC + ACC_FINAL + ACC_SUPER, className, null, superClassName, null);\n+            cw.visit(CLASSFILE_VERSION, NOT_ACC_PUBLIC + ACC_FINAL + ACC_SUPER, className, null, superClassName, null);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/ClassSpecializer.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+import static java.lang.invoke.MethodHandleStatics.CLASSFILE_VERSION;\n@@ -507,1 +508,1 @@\n-        cw.visit(Opcodes.V1_8, Opcodes.ACC_PRIVATE + Opcodes.ACC_FINAL + Opcodes.ACC_SUPER,\n+        cw.visit(CLASSFILE_VERSION, Opcodes.ACC_PRIVATE + Opcodes.ACC_FINAL + Opcodes.ACC_SUPER,\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/GenerateJLIClassesHelper.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -343,1 +343,1 @@\n-        cw.visit(Opcodes.V1_8, NOT_ACC_PUBLIC + Opcodes.ACC_FINAL + Opcodes.ACC_SUPER,\n+        cw.visit(CLASSFILE_VERSION, NOT_ACC_PUBLIC + Opcodes.ACC_FINAL + Opcodes.ACC_SUPER,\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/InvokerBytecodeGenerator.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2788,0 +2788,1 @@\n+         * @throws NullPointerException if {@code targetName} is null\n@@ -2857,0 +2858,4 @@\n+         * If {@code targetClass} is an array class, {@code targetClass} is accessible\n+         * if the element type of the array class is accessible.  Otherwise,\n+         * {@code targetClass} is determined as accessible as follows.\n+         *\n@@ -2858,1 +2863,1 @@\n-         * If the {@code targetClass} is in the same module as the lookup class,\n+         * If {@code targetClass} is in the same module as the lookup class,\n@@ -2881,1 +2886,1 @@\n-         * Otherwise, the target class is in a different module from {@code lookupClass},\n+         * Otherwise, {@code targetClass} is in a different module from {@code lookupClass},\n@@ -2917,2 +2922,3 @@\n-         * @throws    SecurityException if a security manager is present and it\n-         *                              <a href=\"MethodHandles.Lookup.html#secmgr\">refuses access<\/a>\n+         * @throws SecurityException if a security manager is present and it\n+         *                           <a href=\"MethodHandles.Lookup.html#secmgr\">refuses access<\/a>\n+         * @throws NullPointerException if {@code targetClass} is {@code null}\n@@ -2923,1 +2929,1 @@\n-            if (!VerifyAccess.isClassAccessible(targetClass, lookupClass, prevLookupClass, allowedModes)) {\n+            if (!isClassAccessible(targetClass)) {\n@@ -3713,1 +3719,5 @@\n-            return caller == null || VerifyAccess.isClassAccessible(refc, caller, prevLookupClass, allowedModes);\n+            Class<?> type = refc;\n+            while (type.isArray()) {\n+                type = type.getComponentType();\n+            }\n+            return caller == null || VerifyAccess.isClassAccessible(type, caller, prevLookupClass, allowedModes);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandles.java","additions":16,"deletions":6,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2188,9 +2188,0 @@\n-    static final BiFunction<String, List<Number>, ArrayIndexOutOfBoundsException>\n-            AIOOBE_SUPPLIER = Preconditions.outOfBoundsExceptionFormatter(\n-            new Function<String, ArrayIndexOutOfBoundsException>() {\n-                @Override\n-                public ArrayIndexOutOfBoundsException apply(String s) {\n-                    return new ArrayIndexOutOfBoundsException(s);\n-                }\n-            });\n-\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/VarHandle.java","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -874,1 +874,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType});\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType});\n@@ -894,1 +894,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -914,1 +914,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType});\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType});\n@@ -934,1 +934,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -954,1 +954,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType});\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType});\n@@ -974,1 +974,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -995,1 +995,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -1016,1 +1016,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -1037,1 +1037,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -1058,1 +1058,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -1079,1 +1079,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -1100,1 +1100,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -1121,1 +1121,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -1142,1 +1142,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Object]?, handle.componentType},\n@@ -1163,1 +1163,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -1183,1 +1183,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -1203,1 +1203,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase{#if[Value]?, handle.componentType},\n@@ -1214,1 +1214,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n@@ -1223,1 +1223,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n@@ -1232,1 +1232,1 @@\n-                    (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase,\n+                    (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n@@ -1243,1 +1243,1 @@\n-                                       (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase,\n+                                       (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n@@ -1252,1 +1252,1 @@\n-                                       (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase,\n+                                       (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n@@ -1261,1 +1261,1 @@\n-                                       (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase,\n+                                       (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n@@ -1270,1 +1270,1 @@\n-                                       (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase,\n+                                       (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n@@ -1279,1 +1279,1 @@\n-                                       (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase,\n+                                       (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n@@ -1288,1 +1288,1 @@\n-                                       (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase,\n+                                       (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n@@ -1297,1 +1297,1 @@\n-                                       (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase,\n+                                       (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n@@ -1306,1 +1306,1 @@\n-                                       (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase,\n+                                       (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n@@ -1315,1 +1315,1 @@\n-                                       (((long) Preconditions.checkIndex(index, array.length, AIOOBE_SUPPLIER)) << handle.ashift) + handle.abase,\n+                                       (((long) Preconditions.checkIndex(index, array.length, Preconditions.AIOOBE_FORMATTER)) << handle.ashift) + handle.abase,\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/X-VarHandle.java.template","additions":29,"deletions":29,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -49,0 +49,2 @@\n+ * The {@link #refersTo(Object) refersTo} method can be used to test\n+ * whether some object is the referent of a phantom reference.\n@@ -81,3 +83,1 @@\n-     * queue, but such a reference is completely useless: Its {@code get}\n-     * method will always return {@code null} and, since it does not have a queue,\n-     * it will never be enqueued.\n+     * queue.  Such a reference will never be enqueued.\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/PhantomReference.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -713,0 +713,4 @@\n+                if (intf.isSealed()) {\n+                    throw new IllegalArgumentException(intf.getName() + \" is a sealed interface\");\n+                }\n+\n@@ -933,1 +937,2 @@\n-     * must represent {@linkplain Class#isHidden() non-hidden} interfaces,\n+     * must represent {@linkplain Class#isHidden() non-hidden} and\n+     * {@linkplain Class#isSealed() non-sealed} interfaces,\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Proxy.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import jdk.internal.misc.VM;\n@@ -58,1 +59,1 @@\n-\n+    private static final int CLASSFILE_VERSION = VM.classFileVersion();\n@@ -458,1 +459,1 @@\n-        visit(V18, accessFlags, dotToSlash(className), null,\n+        visit(CLASSFILE_VERSION, accessFlags, dotToSlash(className), null,\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/ProxyGenerator.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -201,0 +201,1 @@\n+        jdk.crypto.cryptoki,\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+import com.sun.tools.javac.code.Symbol;\n@@ -88,1 +89,0 @@\n-import com.sun.tools.javac.code.Type.ErrorType;\n@@ -91,1 +91,0 @@\n-import com.sun.tools.javac.code.Types.TypeRelation;\n@@ -100,1 +99,0 @@\n-import com.sun.tools.javac.code.Symbol;\n@@ -154,1 +152,0 @@\n-import static com.sun.tools.javac.code.TypeTag.*;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/api\/JavacTrees.java","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -336,1 +336,6 @@\n-    public static final long SYSTEM_MODULE = 1L<<53;\n+    public static final long SYSTEM_MODULE = 1L<<53; \/\/ModuleSymbols only\n+\n+    \/**\n+     * Flag to indicate the given ClassSymbol is a value based.\n+     *\/\n+    public static final long VALUE_BASED = 1L<<53; \/\/ClassSymbols only\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Flags.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -229,0 +229,1 @@\n+    public final Type valueBasedInternalType;\n@@ -606,0 +607,1 @@\n+        valueBasedInternalType = enterSyntheticAnnotation(\"jdk.internal.ValueBased+Annotation\");\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Symtab.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -175,0 +175,2 @@\n+        allowPatternSwitch = (preview.isEnabled() || !preview.isPreview(Feature.PATTERN_SWITCH)) &&\n+                             Feature.PATTERN_SWITCH.allowedInSource(source);\n@@ -219,0 +221,4 @@\n+    \/** Are patterns in switch allowed\n+     *\/\n+    private final boolean allowPatternSwitch;\n+\n@@ -1751,1 +1757,0 @@\n-            boolean prevCompletedNormally = false;\n@@ -1769,1 +1774,1 @@\n-                                log.error(c.pos(), Errors.DuplicateCaseLabel);\n+                                log.error(pat.pos(), Errors.DuplicateCaseLabel);\n@@ -1771,1 +1776,1 @@\n-                                log.error(c.pos(), Errors.PatternDominated);\n+                                log.error(pat.pos(), Errors.PatternDominated);\n@@ -1781,1 +1786,1 @@\n-                                log.error(c.pos(), Errors.DuplicateCaseLabel);\n+                                log.error(pat.pos(), Errors.DuplicateCaseLabel);\n@@ -1797,1 +1802,4 @@\n-                            Type pattype = attribExpr(expr, switchEnv, seltype);\n+                            ResultInfo valTypInfo = new ResultInfo(KindSelector.VAL_TYP,\n+                                                                   !seltype.hasTag(ERROR) ? seltype\n+                                                                                          : Type.noType);\n+                            Type pattype = attribTree(expr, switchEnv, valTypInfo);\n@@ -1799,5 +1807,10 @@\n-                                if (!stringSwitch && !types.isAssignable(seltype, syms.intType)) {\n-                                    log.error(pat.pos(), Errors.ConstantLabelNotCompatible(pattype, seltype));\n-                                }\n-                                    log.error(expr.pos(),\n-                                              (stringSwitch ? Errors.StringConstReq : Errors.ConstExprReq));\n+                                    Symbol s = TreeInfo.symbol(expr);\n+                                    if (s != null && s.kind == TYP && allowPatternSwitch) {\n+                                        log.error(expr.pos(),\n+                                                  Errors.PatternExpected);\n+                                    } else {\n+                                        log.error(expr.pos(),\n+                                                  (stringSwitch ? Errors.StringConstReq : Errors.ConstExprReq));\n+                                    }\n+                                } else if (!stringSwitch && !types.isAssignable(seltype, syms.intType)) {\n+                                    log.error(pat.pos(), Errors.ConstantLabelNotCompatible(pattype, seltype));\n@@ -1817,3 +1830,0 @@\n-                        } else if (matchBindings.bindingsWhenTrue.nonEmpty()) {\n-                            \/\/there was a pattern, and the execution flows into a default:\n-                            log.error(pat.pos(), Errors.FlowsThroughFromPattern);\n@@ -1824,3 +1834,0 @@\n-                        if (prevCompletedNormally) {\n-                            log.error(pat.pos(), Errors.FlowsThroughToPattern);\n-                        }\n@@ -1853,1 +1860,0 @@\n-                    prevCompletedNormally = !TreeInfo.isNull(pat);\n@@ -1864,6 +1870,7 @@\n-                boolean completesNormally = c.caseKind == CaseTree.CaseKind.STATEMENT ? flow.aliveAfter(caseEnv, c, make) : false;\n-                prevBindings = completesNormally ? currentBindings : null;\n-                prevCompletedNormally =\n-                        completesNormally &&\n-                        !(c.labels.size() == 1 &&\n-                          TreeInfo.isNull(c.labels.head) && c.stats.isEmpty());\n+                c.completesNormally = flow.aliveAfter(caseEnv, c, make);\n+\n+                prevBindings = c.caseKind == CaseTree.CaseKind.STATEMENT && c.completesNormally ? currentBindings\n+                                                                                                : null;\n+            }\n+            if (patternSwitch) {\n+                chk.checkSwitchCaseStructure(cases);\n@@ -1930,8 +1937,1 @@\n-            if (t != null && t.tsym != null) {\n-                for (Attribute.Compound a: t.tsym.getDeclarationAttributes()) {\n-                    if (a.type.tsym == syms.valueBasedType.tsym) {\n-                        return true;\n-                    }\n-                }\n-            }\n-            return false;\n+            return t != null && t.tsym != null && (t.tsym.flags() & VALUE_BASED) != 0;\n@@ -4198,2 +4198,2 @@\n-        boolean checkRawTypes;\n-        if (tree.pattern.getTag() == BINDINGPATTERN) {\n+        if (tree.pattern.getTag() == BINDINGPATTERN ||\n+           tree.pattern.getTag() == PARENTHESIZEDPATTERN) {\n@@ -4206,6 +4206,1 @@\n-            JCBindingPattern pattern = (JCBindingPattern) tree.pattern;\n-            typeTree = pattern.var.vartype;\n-            if (!clazztype.hasTag(TYPEVAR)) {\n-                clazztype = chk.checkClassOrArrayType(pattern.var.vartype.pos(), clazztype);\n-            }\n-            checkRawTypes = true;\n+            typeTree = TreeInfo.primaryPatternTree((JCPattern) tree.pattern).var.vartype;\n@@ -4215,1 +4210,1 @@\n-            checkRawTypes = false;\n+            chk.validate(typeTree, env, false);\n@@ -4233,1 +4228,0 @@\n-        chk.validate(typeTree, env, checkRawTypes);\n@@ -4267,0 +4261,1 @@\n+        chk.validate(tree.var.vartype, env, true);\n@@ -4274,0 +4269,1 @@\n+        result = tree.type = tree.pattern.type;\n@@ -4488,1 +4484,1 @@\n-            if (sym.name == names._this) {\n+            if (sym.name == names._this || sym.name == names._super) {\n@@ -4490,1 +4486,2 @@\n-                \/\/ C.this' does not appear in a call to a super(...)\n+                \/\/ `C.this' does not appear in an explicit call to a constructor\n+                \/\/ also make sure that `super` is not used in constructor invocations\n@@ -4492,1 +4489,3 @@\n-                    site.tsym == env.enclClass.sym) {\n+                        ((sym.name == names._this &&\n+                        site.tsym == env.enclClass.sym) ||\n+                        sym.name == names._super)) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":43,"deletions":44,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+import com.sun.source.tree.CaseTree;\n@@ -4522,0 +4523,62 @@\n+    \/**\n+     * Verify the case labels conform to the constraints. Checks constraints related\n+     * combinations of patterns and other labels.\n+     *\n+     * @param cases the cases that should be checked.\n+     *\/\n+    void checkSwitchCaseStructure(List<JCCase> cases) {\n+        boolean wasConstant = false;          \/\/ Seen a constant in the same case label\n+        boolean wasDefault = false;           \/\/ Seen a default in the same case label\n+        boolean wasNullPattern = false;       \/\/ Seen a null pattern in the same case label,\n+                                              \/\/or fall through from a null pattern\n+        boolean wasPattern = false;           \/\/ Seen a pattern in the same case label\n+                                              \/\/or fall through from a pattern\n+        boolean wasTypePattern = false;       \/\/ Seen a pattern in the same case label\n+                                              \/\/or fall through from a type pattern\n+        boolean wasNonEmptyFallThrough = false;\n+        for (List<JCCase> l = cases; l.nonEmpty(); l = l.tail) {\n+            JCCase c = l.head;\n+            for (JCCaseLabel pat : c.labels) {\n+                if (pat.isExpression()) {\n+                    JCExpression expr = (JCExpression) pat;\n+                    if (TreeInfo.isNull(expr)) {\n+                        if (wasPattern && !wasTypePattern && !wasNonEmptyFallThrough) {\n+                            log.error(pat.pos(), Errors.FlowsThroughFromPattern);\n+                        }\n+                        wasNullPattern = true;\n+                    } else {\n+                        if (wasPattern && !wasNonEmptyFallThrough) {\n+                            log.error(pat.pos(), Errors.FlowsThroughFromPattern);\n+                        }\n+                        wasConstant = true;\n+                    }\n+                } else if (pat.hasTag(DEFAULTCASELABEL)) {\n+                    if (wasPattern && !wasNonEmptyFallThrough) {\n+                        log.error(pat.pos(), Errors.FlowsThroughFromPattern);\n+                    }\n+                    wasDefault = true;\n+                } else {\n+                    boolean isTypePattern = pat.hasTag(BINDINGPATTERN);\n+                    if (wasPattern || wasConstant || wasDefault ||\n+                        (wasNullPattern && (!isTypePattern || wasNonEmptyFallThrough))) {\n+                        log.error(pat.pos(), Errors.FlowsThroughToPattern);\n+                    }\n+                    wasPattern = true;\n+                    wasTypePattern = isTypePattern;\n+                }\n+            }\n+\n+            boolean completesNormally = c.caseKind == CaseTree.CaseKind.STATEMENT ? c.completesNormally\n+                                                                                  : false;\n+\n+            if (c.stats.nonEmpty()) {\n+                wasConstant = false;\n+                wasDefault = false;\n+                wasNullPattern &= completesNormally;\n+                wasPattern &= completesNormally;\n+                wasTypePattern &= completesNormally;\n+            }\n+\n+            wasNonEmptyFallThrough = c.stats.nonEmpty() && completesNormally;\n+        }\n+    }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Check.java","additions":63,"deletions":0,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-import java.util.function.Predicate;\n+import java.util.stream.StreamSupport;\n@@ -48,1 +48,0 @@\n-import com.sun.tools.javac.code.Kinds.Kind;\n@@ -55,0 +54,1 @@\n+import com.sun.tools.javac.code.Type.TypeVar;\n@@ -60,1 +60,0 @@\n-import com.sun.tools.javac.tree.JCTree.JCParenthesizedPattern;\n@@ -670,1 +669,5 @@\n-            Set<Object> constants = tree.patternSwitch ? allSwitchConstants(tree.selector) : null;\n+            boolean exhaustiveSwitch = tree.patternSwitch ||\n+                                       tree.cases.stream()\n+                                                 .flatMap(c -> c.labels.stream())\n+                                                 .anyMatch(l -> TreeInfo.isNull(l));\n+            Set<Symbol> constants = exhaustiveSwitch ? new HashSet<>() : null;\n@@ -679,1 +682,0 @@\n-                c.completesNormally = alive != Liveness.DEAD;\n@@ -692,2 +694,3 @@\n-            if ((constants == null || !constants.isEmpty()) && !tree.hasTotalPattern &&\n-                tree.patternSwitch && !TreeInfo.isErrorEnumSwitch(tree.selector, tree.cases)) {\n+            if (!tree.hasTotalPattern && exhaustiveSwitch &&\n+                !TreeInfo.isErrorEnumSwitch(tree.selector, tree.cases) &&\n+                (constants == null || !isExhaustive(tree.selector.type, constants))) {\n@@ -707,1 +710,1 @@\n-            Set<Object> constants = allSwitchConstants(tree.selector);\n+            Set<Symbol> constants = new HashSet<>();\n@@ -726,3 +729,2 @@\n-                c.completesNormally = alive != Liveness.DEAD;\n-            if ((constants == null || !constants.isEmpty()) && !tree.hasTotalPattern &&\n-                !TreeInfo.isErrorEnumSwitch(tree.selector, tree.cases)) {\n+            if (!tree.hasTotalPattern && !TreeInfo.isErrorEnumSwitch(tree.selector, tree.cases) &&\n+                !isExhaustive(tree.selector.type, constants)) {\n@@ -736,18 +738,1 @@\n-        private Set<Object> allSwitchConstants(JCExpression selector) {\n-            Set<Object> constants = null;\n-            TypeSymbol selectorSym = selector.type.tsym;\n-            if ((selectorSym.flags() & ENUM) != 0) {\n-                constants = new HashSet<>();\n-                Predicate<Symbol> enumConstantFilter =\n-                        s -> (s.flags() & ENUM) != 0 && s.kind == Kind.VAR;\n-                for (Symbol s : selectorSym.members().getSymbols(enumConstantFilter)) {\n-                    constants.add(s.name);\n-                }\n-            } else if (selectorSym.isAbstract() && selectorSym.isSealed() && selectorSym.kind == Kind.TYP) {\n-                constants = new HashSet<>();\n-                constants.addAll(((ClassSymbol) selectorSym).permitted);\n-            }\n-            return constants;\n-        }\n-\n-        private void handleConstantCaseLabel(Set<Object> constants, JCCaseLabel pat) {\n+        private void handleConstantCaseLabel(Set<Symbol> constants, JCCaseLabel pat) {\n@@ -757,2 +742,2 @@\n-                    if (expr.hasTag(IDENT))\n-                        constants.remove(((JCIdent) expr).name);\n+                    if (expr.hasTag(IDENT) && ((JCIdent) expr).sym.isEnum())\n+                        constants.add(((JCIdent) expr).sym);\n@@ -763,1 +748,1 @@\n-                        constants.remove(patternType.type().tsym);\n+                        constants.add(patternType.type().tsym);\n@@ -769,0 +754,53 @@\n+        private void transitiveCovers(Set<Symbol> covered) {\n+            List<Symbol> todo = List.from(covered);\n+            while (todo.nonEmpty()) {\n+                Symbol sym = todo.head;\n+                todo = todo.tail;\n+                switch (sym.kind) {\n+                    case VAR -> {\n+                        Iterable<Symbol> constants = sym.owner\n+                                                        .members()\n+                                                        .getSymbols(s -> s.isEnum() &&\n+                                                                         s.kind == VAR);\n+                        boolean hasAll = StreamSupport.stream(constants.spliterator(), false)\n+                                                      .allMatch(covered::contains);\n+\n+                        if (hasAll && covered.add(sym.owner)) {\n+                            todo = todo.prepend(sym.owner);\n+                        }\n+                    }\n+\n+                    case TYP -> {\n+                        for (Type sup : types.directSupertypes(sym.type)) {\n+                            if (sup.tsym.kind == TYP && sup.tsym.isAbstract() && sup.tsym.isSealed()) {\n+                                boolean hasAll = ((ClassSymbol) sup.tsym).permitted\n+                                                                         .stream()\n+                                                                         .allMatch(covered::contains);\n+\n+                                if (hasAll && covered.add(sup.tsym)) {\n+                                    todo = todo.prepend(sup.tsym);\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        private boolean isExhaustive(Type seltype, Set<Symbol> covered) {\n+            transitiveCovers(covered);\n+            return switch (seltype.getTag()) {\n+                case CLASS -> {\n+                    if (seltype.isCompound()) {\n+                        if (seltype.isIntersection()) {\n+                            yield ((Type.IntersectionClassType) seltype).getComponents().stream().anyMatch(t -> isExhaustive(t, covered));\n+                        }\n+                        yield false;\n+                    }\n+                    yield covered.contains(seltype.tsym);\n+                }\n+                case TYPEVAR -> isExhaustive(((TypeVar) seltype).getUpperBound(), covered);\n+                default -> false;\n+            };\n+        }\n+\n@@ -1546,0 +1584,4 @@\n+        @Override\n+        public void visitLambda(JCLambda tree) {\n+            \/\/skip\n+        }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Flow.java","additions":74,"deletions":32,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -3662,1 +3662,1 @@\n-            cases = cases.append(c);\n+            cases = cases.prepend(c);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Lower.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1498,0 +1498,3 @@\n+            } else if (proxy.type.tsym.flatName() == syms.valueBasedInternalType.tsym.flatName()) {\n+                Assert.check(sym.kind == TYP);\n+                sym.flags_field |= VALUE_BASED;\n@@ -1509,0 +1512,2 @@\n+                }  else if (proxy.type.tsym == syms.valueBasedType.tsym && sym.kind == TYP) {\n+                    sym.flags_field |= VALUE_BASED;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassReader.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1429,2 +1429,1 @@\n-                code.resolve(switchEnv.info.cont);\n-                code.resolve(code.branch(goto_), switchStart);\n+                code.resolve(switchEnv.info.cont, switchStart);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/Gen.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -804,0 +804,1 @@\n+        JCPattern pattern;\n@@ -809,1 +810,1 @@\n-            return toP(F.at(startPos).ParenthesizedPattern(p));\n+            pattern = toP(F.at(startPos).ParenthesizedPattern(p));\n@@ -811,3 +812,2 @@\n-            JCPattern pattern;\n-            JCExpression e = parsedType == null ? term(EXPR | TYPE | NOLAMBDA) : parsedType;\n-            mods = mods != null ? mods : F.at(token.pos).Modifiers(0);\n+            mods = mods != null ? mods : optFinal(0);\n+            JCExpression e = parsedType == null ? term(TYPE | NOLAMBDA) : parsedType;\n@@ -816,7 +816,7 @@\n-            if (!inInstanceOf && token.kind == AMPAMP) {\n-                checkSourceLevel(Feature.PATTERN_SWITCH);\n-                nextToken();\n-                JCExpression guard = term(EXPR | NOLAMBDA);\n-                pattern = F.at(pos).GuardPattern(pattern, guard);\n-            }\n-            return pattern;\n+        if (!inInstanceOf && token.kind == AMPAMP) {\n+            checkSourceLevel(Feature.PATTERN_SWITCH);\n+            nextToken();\n+            JCExpression guard = term(EXPR | NOLAMBDA);\n+            pattern = F.at(pos).GuardPattern(pattern, guard);\n+        }\n+        return pattern;\n@@ -1845,25 +1845,1 @@\n-                    lookahead += 1; \/\/skip '@'\n-                    while (peekToken(lookahead, DOT)) {\n-                        lookahead += 2;\n-                    }\n-                    if (peekToken(lookahead, LPAREN)) {\n-                        lookahead++;\n-                        \/\/skip annotation values\n-                        int nesting = 0;\n-                        for (; ; lookahead++) {\n-                            TokenKind tk2 = S.token(lookahead).kind;\n-                            switch (tk2) {\n-                                case EOF:\n-                                    return ParensResult.PARENS;\n-                                case LPAREN:\n-                                    nesting++;\n-                                    break;\n-                                case RPAREN:\n-                                    nesting--;\n-                                    if (nesting == 0) {\n-                                        continue outer;\n-                                    }\n-                                break;\n-                            }\n-                        }\n-                    }\n+                    lookahead = skipAnnotation(lookahead);\n@@ -1928,0 +1904,29 @@\n+    private int skipAnnotation(int lookahead) {\n+        lookahead += 1; \/\/skip '@'\n+        while (peekToken(lookahead, DOT)) {\n+            lookahead += 2;\n+        }\n+        if (peekToken(lookahead, LPAREN)) {\n+            lookahead++;\n+            \/\/skip annotation values\n+            int nesting = 0;\n+            for (; ; lookahead++) {\n+                TokenKind tk2 = S.token(lookahead).kind;\n+                switch (tk2) {\n+                    case EOF:\n+                        return lookahead;\n+                    case LPAREN:\n+                        nesting++;\n+                        break;\n+                    case RPAREN:\n+                        nesting--;\n+                        if (nesting == 0) {\n+                            return lookahead;\n+                        }\n+                    break;\n+                }\n+            }\n+        }\n+        return lookahead;\n+    }\n+\n@@ -3159,15 +3164,10 @@\n-            if (token.kind == LPAREN) {\n-                int lookahead = 0;\n-                Token ahead;\n-                while ((ahead = S.token(lookahead)).kind != EOF && ahead.kind != RPAREN && ahead.kind != AMPAMP) {\n-                    lookahead++;\n-                }\n-                Token twoBack;\n-                boolean pattern = S.token(lookahead - 1).kind == IDENTIFIER &&\n-                                  ((twoBack = S.token(lookahead - 2)).kind == IDENTIFIER ||\n-                                   twoBack.kind == GT || twoBack.kind == GTGT || twoBack.kind == GTGTGT);\n-                if (pattern) {\n-                    return parsePattern(token.pos, null, null, false);\n-                } else {\n-                    return term(EXPR | TYPE | NOLAMBDA);\n-                }\n+            int lookahead = 0;\n+            while (S.token(lookahead).kind == LPAREN) {\n+                lookahead++;\n+            }\n+            JCModifiers mods = optFinal(0);\n+            boolean pattern = mods.flags != 0 || mods.annotations.nonEmpty() ||\n+                              analyzePattern(lookahead) == PatternResult.PATTERN;\n+            if (pattern) {\n+                checkSourceLevel(token.pos, Feature.PATTERN_SWITCH);\n+                return parsePattern(patternPos, mods, null, false);\n@@ -3175,9 +3175,1 @@\n-                JCModifiers mods = optFinal(0);\n-                JCExpression e = term(EXPR | TYPE | NOLAMBDA);\n-\n-                if (token.kind == IDENTIFIER || mods.flags != 0 || mods.annotations.nonEmpty()) {\n-                    checkSourceLevel(token.pos, Feature.PATTERN_SWITCH);\n-                    return parsePattern(patternPos, null, e, false);\n-                } else {\n-                    return e;\n-                }\n+                return term(EXPR | NOLAMBDA);\n@@ -3190,0 +3182,45 @@\n+    @SuppressWarnings(\"fallthrough\")\n+    PatternResult analyzePattern(int lookahead) {\n+        int depth = 0;\n+        while (true) {\n+            TokenKind token = S.token(lookahead).kind;\n+            switch (token) {\n+                case BYTE: case SHORT: case INT: case LONG: case FLOAT:\n+                case DOUBLE: case BOOLEAN: case CHAR: case VOID:\n+                case ASSERT, ENUM, IDENTIFIER, UNDERSCORE:\n+                    if (depth == 0 && peekToken(lookahead, LAX_IDENTIFIER)) return PatternResult.PATTERN;\n+                    break;\n+                case DOT, QUES, EXTENDS, SUPER, COMMA: break;\n+                case LT: depth++; break;\n+                case GTGTGT: depth--;\n+                case GTGT: depth--;\n+                case GT:\n+                    depth--;\n+                    if (depth == 0) {\n+                         return peekToken(lookahead, LAX_IDENTIFIER) ? PatternResult.PATTERN\n+                                                          : PatternResult.EXPRESSION;\n+                    } else if (depth < 0) return PatternResult.EXPRESSION;\n+                    break;\n+                case MONKEYS_AT:\n+                    lookahead = skipAnnotation(lookahead);\n+                    break;\n+                case LBRACKET:\n+                    if (peekToken(lookahead, RBRACKET, LAX_IDENTIFIER)) {\n+                        return PatternResult.PATTERN;\n+                    } else if (peekToken(lookahead, RBRACKET)) {\n+                        lookahead++;\n+                        break;\n+                    } else {\n+                        return PatternResult.EXPRESSION;\n+                    }\n+                default: return PatternResult.EXPRESSION;\n+            }\n+            lookahead++;\n+        }\n+    }\n+\n+    private enum PatternResult {\n+        EXPRESSION,\n+        PATTERN;\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":97,"deletions":60,"binary":false,"changes":157,"status":"modified"},{"patch":"@@ -1192,0 +1192,3 @@\n+compiler.err.pattern.expected=\\\n+    type pattern expected\n+\n@@ -3272,0 +3275,3 @@\n+compiler.err.dc.ref.annotations.not.allowed=\\\n+    annotations not allowed\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1377,0 +1377,9 @@\n+    public static JCBindingPattern primaryPatternTree(JCPattern pat) {\n+        return switch (pat.getTag()) {\n+            case BINDINGPATTERN -> (JCBindingPattern) pat;\n+            case GUARDPATTERN -> primaryPatternTree(((JCGuardPattern) pat).patt);\n+            case PARENTHESIZEDPATTERN -> primaryPatternTree(((JCParenthesizedPattern) pat).pattern);\n+            default -> throw new AssertionError();\n+        };\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeInfo.java","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -232,0 +232,4 @@\n+    \/\/ pattern switches\n+    public final Name typeSwitch;\n+    public final Name enumSwitch;\n+\n@@ -414,0 +418,4 @@\n+\n+        \/\/ pattern switches\n+        typeSwitch = fromString(\"typeSwitch\");\n+        enumSwitch = fromString(\"enumSwitch\");\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/util\/Names.java","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import sun.jvm.hotspot.memory.Dictionary;\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/InstanceKlass.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -927,5 +927,1 @@\n-        if (isArray()) {\n-            return config.arrayPrototypeMarkWord();\n-        } else {\n-            return UNSAFE.getAddress(getMetaspaceKlass() + config.prototypeMarkWordOffset);\n-        }\n+        return config.prototypeMarkWord();\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotResolvedObjectTypeImpl.java","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -78,1 +78,0 @@\n-    final int prototypeMarkWordOffset = getFieldOffset(\"Klass::_prototype_header\", Integer.class, \"markWord\");\n@@ -155,1 +154,1 @@\n-    long arrayPrototypeMarkWord() {\n+    long prototypeMarkWord() {\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_preservedMarks.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"runtime\/biasedLocking.hpp\"\n@@ -54,6 +53,0 @@\n-static void assert_not_test_pattern(Handle object, const char* pattern) {\n-  stringStream st;\n-  object->print_on(&st);\n-  ASSERT_FALSE(test_pattern(&st, pattern)) << pattern << \" found in \" << st.as_string();\n-}\n-\n@@ -96,21 +89,1 @@\n-  if (UseBiasedLocking && BiasedLocking::enabled()) {\n-    \/\/ Can't test this with biased locking disabled.\n-    \/\/ Biased locking is initially enabled for this java.lang.Byte object.\n-    assert_test_pattern(h_obj, \"is_biased\");\n-\n-    \/\/ Lock using biased locking.\n-    BasicObjectLock lock;\n-    lock.set_obj(obj);\n-    markWord prototype_header = obj->klass()->prototype_header();\n-    markWord mark = obj->mark();\n-    markWord biased_mark = markWord::encode((JavaThread*) THREAD, mark.age(), prototype_header.bias_epoch());\n-    obj->set_mark(biased_mark);\n-    \/\/ Look for the biased_locker in markWord, not prototype_header.\n-#ifdef _LP64\n-    assert_not_test_pattern(h_obj, \"mark(is_biased biased_locker=0x0000000000000000\");\n-#else\n-    assert_not_test_pattern(h_obj, \"mark(is_biased biased_locker=0x00000000\");\n-#endif\n-  }\n-\n-  \/\/ Same thread tries to lock it again.\n+  \/\/ Thread tries to lock it.\n@@ -121,2 +94,0 @@\n-\n-  \/\/ This is no longer biased, because ObjectLocker revokes the bias.\n","filename":"test\/hotspot\/gtest\/oops\/test_markWord.cpp","additions":1,"deletions":30,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -73,1 +73,0 @@\n-compiler\/intrinsics\/VectorizedMismatchTest.java 8268482 windows-x64\n@@ -107,5 +106,1 @@\n-serviceability\/sa\/sadebugd\/DebugdConnectTest.java 8239062,8268570 generic-all\n-serviceability\/attach\/RemovingUnixDomainSocketTest.java 8268570 generic-all\n-serviceability\/sa\/TestJhsdbJstackLock.java 8268570 generic-all\n-serviceability\/sa\/JhsdbThreadInfoTest.java 8268570 generic-all\n-\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java 8239062 macosx-x64\n@@ -116,0 +111,1 @@\n+serviceability\/jvmti\/HeapMonitor\/MyPackage\/HeapMonitorInterpreterObjectTest.java 8225313 linux-i586,linux-x64,windows-x64\n@@ -117,0 +113,1 @@\n+serviceability\/jvmti\/HeapMonitor\/MyPackage\/HeapMonitorStatObjectCorrectnessTest.java 8225313 linux-i586,linux-x64,windows-x64\n@@ -118,2 +115,6 @@\n-serviceability\/dcmd\/gc\/RunFinalizationTest.java 8227120 linux-x64\n-serviceability\/jvmti\/CompiledMethodLoad\/Zombie.java 8245877 linux-aarch64\n+serviceability\/dcmd\/gc\/RunFinalizationTest.java 8227120 linux-all,windows-x64\n+\n+serviceability\/sa\/ClhsdbCDSCore.java 8269982 macosx-aarch64\n+serviceability\/sa\/ClhsdbFindPC.java#id1 8269982 macosx-aarch64\n+serviceability\/sa\/ClhsdbFindPC.java#id3 8269982 macosx-aarch64\n+serviceability\/sa\/ClhsdbPstack.java#id1 8269982 macosx-aarch64\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -81,0 +81,23 @@\n+hotspot_vector_1 = \\\n+  compiler\/c2\/cr6340864 \\\n+  compiler\/codegen \\\n+  compiler\/loopopts\/superword \\\n+  compiler\/vectorapi \\\n+  compiler\/vectorization \\\n+  -compiler\/codegen\/aes \\\n+  -compiler\/codegen\/Test6875866.java \\\n+  -compiler\/codegen\/Test6935535.java \\\n+  -compiler\/codegen\/TestGCMStorePlacement.java \\\n+  -compiler\/codegen\/TestTrichotomyExpressions.java \\\n+  -compiler\/loopopts\/superword\/Vec_MulAddS2I.java \\\n+  -compiler\/vectorapi\/VectorRebracket128Test.java\n+\n+hotspot_vector_2 = \\\n+  compiler\/intrinsics \\\n+  compiler\/codegen\/aes \\\n+  compiler\/codegen\/Test6875866.java \\\n+  compiler\/codegen\/Test6935535.java \\\n+  compiler\/loopopts\/superword\/Vec_MulAddS2I.java \\\n+  compiler\/vectorapi\/VectorRebracket128Test.java \\\n+  -compiler\/intrinsics\/string\/TestStringLatin1IndexOfChar.java\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -165,0 +165,13 @@\n+    @Test\n+    public void internalNameTest() {\n+        \/\/ Verify that the last slash in lambda types are not replaced with a '.' as they\n+        \/\/ are part of the type name.\n+        Supplier<Runnable> lambda = () -> () -> System.out.println(\"run\");\n+        ResolvedJavaType lambdaType = metaAccess.lookupJavaType(lambda.getClass());\n+        String typeName = lambdaType.getName();\n+        int typeNameLen = TestResolvedJavaType.class.getSimpleName().length();\n+        int index = typeName.indexOf(TestResolvedJavaType.class.getSimpleName());\n+        String suffix = typeName.substring(index + typeNameLen, typeName.length() - 1);\n+        assertEquals(TestResolvedJavaType.class.getName() + suffix, lambdaType.toJavaName());\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/jvmci\/jdk.vm.ci.runtime.test\/src\/jdk\/vm\/ci\/runtime\/test\/TestResolvedJavaType.java","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -62,1 +62,1 @@\n-            \/\/ i = 1 -- dump with agent = disable BiasedLocking\n+            \/\/ i = 1 -- dump with agent\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/javaldr\/LockDuringDump.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -109,1 +109,1 @@\n-    valhalla \n+    valhalla\n@@ -284,2 +284,1 @@\n-    jdk\/jfr \\\n-    -jdk\/jfr\/event\/runtime\/TestBiasedLockRevocationEvents.java\n+    jdk\/jfr\n","filename":"test\/jdk\/TEST.groups","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -327,1 +327,1 @@\n-    return getMethodCompilationLevel(method, false \/*not ost*\/);\n+    return getMethodCompilationLevel(method, false \/*not osr*\/);\n@@ -622,0 +622,1 @@\n+  public native boolean handshakeReadMonitors(Thread t);\n","filename":"test\/lib\/sun\/hotspot\/WhiteBox.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"}]}