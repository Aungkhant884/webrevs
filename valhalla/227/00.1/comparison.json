{"files":[{"patch":"@@ -1889,0 +1889,2 @@\n+  __ verified_entry(C, 0);\n+  __ bind(*_verified_entry);\n@@ -1937,6 +1939,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1999,5 +1995,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {\n-  \/\/ Variable size. Determine dynamically.\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2286,1 +2277,12 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n@@ -2288,0 +2290,22 @@\n+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+{\n+  MacroAssembler _masm(&cbuf);\n+\n+  if (!_verified) {\n+    Label skip;\n+    __ cmp_klass(j_rarg0, rscratch2, rscratch1);\n+    __ br(Assembler::EQ, skip);\n+      __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+    __ bind(skip);\n+\n+  } else {\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    __ b(*_verified_entry);\n+  }\n+}\n+\n+\/\/=============================================================================\n@@ -2309,0 +2333,1 @@\n+  Label skip;\n@@ -2310,0 +2335,1 @@\n+  \/\/ UseCompressedClassPointers logic are inside cmp_klass\n@@ -2311,1 +2337,1 @@\n-  Label skip;\n+\n@@ -2319,5 +2345,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2740,1 +2761,0 @@\n-\n@@ -8687,0 +8707,15 @@\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -8702,0 +8737,31 @@\n+instruct castN2I(iRegINoSp dst, iRegN src) %{\n+  match(Set dst (CastN2I src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"movw $dst, $src\\t# compressed ptr -> int\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct castI2N(iRegNNoSp dst, iRegI src) %{\n+  match(Set dst (CastI2N src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"movw $dst, $src\\t# int -> compressed ptr\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\n@@ -14686,1 +14752,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n@@ -14688,1 +14754,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":87,"deletions":21,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -135,0 +135,66 @@\n+\/\/ Implementation of LoadFlattenedArrayStub\n+\n+LoadFlattenedArrayStub::LoadFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _result = result;\n+  _scratch_reg = FrameMap::r0_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void LoadFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 1);\n+  ce->store_parameter(_index->as_register(), 0);\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::load_flattened_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  if (_result->as_register() != r0) {\n+    __ mov(_result->as_register(), r0);\n+  }\n+  __ b(_continuation);\n+}\n+\n+\n+\/\/ Implementation of StoreFlattenedArrayStub\n+\n+StoreFlattenedArrayStub::StoreFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr value, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _value = value;\n+  _scratch_reg = FrameMap::r0_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+\n+void StoreFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 2);\n+  ce->store_parameter(_index->as_register(), 1);\n+  ce->store_parameter(_value->as_register(), 0);\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::store_flattened_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ b(_continuation);\n+}\n+\n+\/\/ Implementation of SubstitutabilityCheckStub\n+SubstitutabilityCheckStub::SubstitutabilityCheckStub(LIR_Opr left, LIR_Opr right, CodeEmitInfo* info) {\n+  _left = left;\n+  _right = right;\n+  _scratch_reg = FrameMap::r0_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void SubstitutabilityCheckStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_left->as_register(), 1);\n+  ce->store_parameter(_right->as_register(), 0);\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::substitutability_check_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ b(_continuation);\n+}\n@@ -151,2 +217,0 @@\n-\n-\n@@ -192,1 +256,1 @@\n-NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info) {\n+NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info, bool is_inline_type) {\n@@ -197,0 +261,1 @@\n+  _is_inline_type = is_inline_type;\n@@ -205,1 +270,7 @@\n-  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::new_object_array_id)));\n+\n+  if (_is_inline_type) {\n+    __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::new_flat_array_id)));\n+  } else {\n+    __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::new_object_array_id)));\n+  }\n+\n@@ -213,1 +284,1 @@\n-MonitorEnterStub::MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)\n+MonitorEnterStub::MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info,  CodeStub* throw_imse_stub, LIR_Opr scratch_reg)\n@@ -217,0 +288,5 @@\n+  _scratch_reg = scratch_reg;\n+  _throw_imse_stub = throw_imse_stub;\n+  if (_throw_imse_stub != NULL) {\n+    assert(_scratch_reg != LIR_OprFact::illegalOpr, \"must be\");\n+  }\n@@ -223,0 +299,10 @@\n+  if (_throw_imse_stub != NULL) {\n+    \/\/ When we come here, _obj_reg has already been checked to be non-null.\n+    __ ldr(rscratch1, Address(_obj_reg->as_register(), oopDesc::mark_offset_in_bytes()));\n+    __ mov(rscratch2, markWord::always_locked_pattern);\n+    __ andr(rscratch1, rscratch1, rscratch2);\n+\n+    __ cmp(rscratch1, rscratch2);\n+    __ br(Assembler::NE, *_throw_imse_stub->entry());\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":91,"deletions":5,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -244,1 +246,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), needs_stack_repair(), NULL);\n@@ -459,1 +461,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -510,0 +512,15 @@\n+  ciMethod* method = compilation()->method();\n+\n+  ciType* return_type = method->return_type();\n+  if (InlineTypeReturnedAsFields && return_type->is_inlinetype()) {\n+    ciInlineKlass* vk = return_type->as_inline_klass();\n+    if (vk->can_be_returned_as_fields()) {\n+      address unpack_handler = vk->unpack_handler();\n+      assert(unpack_handler != NULL, \"must be\");\n+      __ far_call(RuntimeAddress(unpack_handler));\n+      \/\/ At this point, rax points to the value object (for interpreter or C1 caller).\n+      \/\/ The fields of the object are copied into registers (for C2 caller).\n+    }\n+  }\n+\n+\n@@ -511,1 +528,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -523,0 +540,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -568,0 +589,1 @@\n+    case T_INLINE_TYPE:\n@@ -569,3 +591,1 @@\n-        if (patch_code == lir_patch_none) {\n-          jobject2reg(c->as_jobject(), dest->as_register());\n-        } else {\n+        if (patch_code != lir_patch_none) {\n@@ -573,0 +593,2 @@\n+        } else {\n+          jobject2reg(c->as_jobject(), dest->as_register());\n@@ -614,0 +636,1 @@\n+  case T_INLINE_TYPE:\n@@ -680,0 +703,1 @@\n+  case T_INLINE_TYPE:\n@@ -682,0 +706,2 @@\n+    \/\/ Non-null case is not handled on aarch64 but handled on x86\n+    \/\/ FIXME: do we need to add it here?\n@@ -720,1 +746,1 @@\n-    if (src->type() == T_OBJECT) {\n+    if (src->type() == T_OBJECT || src->type() == T_INLINE_TYPE) {\n@@ -820,0 +846,1 @@\n+    case T_INLINE_TYPE: \/\/ fall through\n@@ -949,1 +976,1 @@\n-  if (addr->base()->type() == T_OBJECT) {\n+  if (addr->base()->type() == T_OBJECT || addr->base()->type() == T_INLINE_TYPE) {\n@@ -973,0 +1000,1 @@\n+    case T_INLINE_TYPE: \/\/ fall through\n@@ -1038,0 +1066,1 @@\n+      __ andr(dest->as_register(), dest->as_register(), oopDesc::compressed_klass_mask());\n@@ -1039,0 +1068,2 @@\n+    } else {\n+      __   ubfm(dest->as_register(), dest->as_register(), 0, 63 - oopDesc::storage_props_nof_bits);\n@@ -1043,0 +1074,14 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, NULL);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n@@ -1234,1 +1279,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->type() == T_INLINE_TYPE ||\n@@ -1546,0 +1591,122 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing an array that *may* be a flattened array (the declared type\n+  \/\/ Object[], interface[], or VT?[]). If this array is flattened, take slow path.\n+\n+  __ load_storage_props(op->tmp()->as_register(), op->array()->as_register());\n+  __ tst(op->tmp()->as_register(), ArrayStorageProperties::flattened_value);\n+  __ br(Assembler::NE, *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ We are storing into the array.\n+    Label skip;\n+    __ tst(op->tmp()->as_register(), ArrayStorageProperties::null_free_value);\n+    __ br(Assembler::EQ, skip);\n+    \/\/ The array is not flattened, but it is null_free. If we are storing\n+    \/\/ a null, take the slow path (which will throw NPE).\n+    __ cbz(op->value()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ This is called when we use aastore into a an array declared as \"[LVT;\",\n+  \/\/ where we know VT is not flattened (due to FlatArrayElementMaxSize, etc).\n+  \/\/ However, we need to do a NULL check if the actual array is a \"[QVT;\".\n+\n+  __ load_storage_props(op->tmp()->as_register(), op->array()->as_register());\n+  __ mov(rscratch1, (uint64_t) ArrayStorageProperties::null_free_value);\n+  __ cmp(op->tmp()->as_register(), rscratch1);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmp(left, right);\n+  __ br(Assembler::EQ, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  {\n+    __ cbz(left, L_oops_not_equal);\n+    __ cbz(right, L_oops_not_equal);\n+  }\n+\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Value object check -- if either of the operands is not a value object,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are value objects\n+  if ((left_klass == NULL || right_klass == NULL) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = rscratch1; \/* op->tmp1()->as_register(); *\/\n+    Register tmp2  = rscratch2; \/* op->tmp2()->as_register(); *\/\n+\n+    __ mov(tmp1, (intptr_t)markWord::always_locked_pattern);\n+\n+    __ ldr(tmp2, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, tmp2);\n+\n+    __ ldr(tmp2, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, tmp2);\n+\n+    __ mov(tmp2, (intptr_t)markWord::always_locked_pattern);\n+    __ cmp(tmp1, tmp2);\n+    __ br(Assembler::NE, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != NULL && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ b(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedOops) {\n+      __ ldrw(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldrw(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpw(left_klass_op, right_klass_op);\n+    } else {\n+      __ ldr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmp(left_klass_op, right_klass_op);\n+    }\n+\n+    __ br(Assembler::EQ, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  \/\/ We've returned from the stub. op->result_opr() contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+\n+  if (op->result_opr()->type() == T_LONG) {\n+    __ cbzw(op->result_opr()->as_register(), L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  } else {\n+    __ cbz(op->result_opr()->as_register(), L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  }\n+\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+\n+}\n+\n+\n@@ -1989,0 +2156,1 @@\n+      case T_INLINE_TYPE:\n@@ -2162,0 +2330,1 @@\n+    case T_INLINE_TYPE:\n@@ -2198,0 +2367,1 @@\n+    case T_INLINE_TYPE:\n@@ -2242,0 +2412,13 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest) {\n+  __ load_storage_props(tmp, obj);\n+  if (is_dest) {\n+    \/\/ We also take slow path if it's a null_free destination array, just in case the source array\n+    \/\/ contains NULLs.\n+    __ tst(tmp, ArrayStorageProperties::flattened_value | ArrayStorageProperties::null_free_value);\n+  } else {\n+    __ tst(tmp, ArrayStorageProperties::flattened_value);\n+  }\n+  __ br(Assembler::NE, *slow_path->entry());\n+}\n+\n+\n@@ -2263,0 +2446,16 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ b(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false);\n+  }\n+\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true);\n+  }\n+\n+\n+\n@@ -2875,0 +3074,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n@@ -3162,0 +3364,1 @@\n+  case T_INLINE_TYPE:\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":212,"deletions":9,"binary":false,"changes":221,"status":"modified"},{"patch":"@@ -588,0 +588,1 @@\n+    case T_INLINE_TYPE :\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -684,0 +685,1 @@\n+\n@@ -700,0 +702,27 @@\n+\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    Label skip;\n+    \/\/ Test if the return type is an inline type\n+    ldr(rscratch1, Address(rfp, frame::interpreter_frame_method_offset * wordSize));\n+    ldr(rscratch1, Address(rscratch1, Method::const_offset()));\n+    ldrb(rscratch1, Address(rscratch1, ConstMethod::result_type_offset()));\n+    cmpw(rscratch1, (u1) T_INLINE_TYPE);\n+    br(Assembler::NE, skip);\n+\n+    \/\/ We are returning an inline type, load its fields into registers\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+\n+    load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+    cbz(rscratch1, skip);\n+\n+    blr(rscratch1);\n+\n+    \/\/ call above kills the value in r1. Reload it.\n+    ldr(r1, Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n+\n+\n@@ -760,0 +789,5 @@\n+    if (EnableValhalla && !UseBiasedLocking) {\n+      \/\/ For slow path is_always_locked, using biased, which is never natural for !UseBiasLocking\n+      andr(swap_reg, swap_reg, ~((int) markWord::biased_lock_bit_in_place));\n+    }\n+\n@@ -1721,1 +1755,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1767,1 +1801,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":36,"deletions":2,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -1302,1 +1303,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1332,1 +1337,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1425,0 +1434,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -1474,0 +1487,33 @@\n+void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label& is_value) {\n+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  andr(temp_reg, temp_reg, JVM_ACC_INLINE);\n+  cbnz(temp_reg, is_value);\n+}\n+\n+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_inline_field_shift, is_inline);\n+}\n+\n+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbz(flags, ConstantPoolCacheEntry::is_inline_field_shift, not_inline);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label& is_flattened_array) {\n+  load_storage_props(temp_reg, oop);\n+  andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);\n+  cbnz(temp_reg, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  load_storage_props(temp_reg, oop);\n+  andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);\n+  cbnz(temp_reg, is_null_free_array);\n+}\n+\n@@ -3778,1 +3824,1 @@\n-void MacroAssembler::load_klass(Register dst, Register src) {\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n@@ -3781,1 +3827,0 @@\n-    decode_klass_not_null(dst);\n@@ -3787,0 +3832,10 @@\n+void MacroAssembler::load_klass(Register dst, Register src) {\n+  load_metadata(dst, src);\n+  if (UseCompressedClassPointers) {\n+    andr(dst, dst, oopDesc::compressed_klass_mask());\n+    decode_klass_not_null(dst);\n+  } else {\n+    ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);\n+  }\n+}\n+\n@@ -3818,0 +3873,9 @@\n+void MacroAssembler::load_storage_props(Register dst, Register src) {\n+  load_metadata(dst, src);\n+  if (UseCompressedClassPointers) {\n+    asrw(dst, dst, oopDesc::narrow_storage_props_shift);\n+  } else {\n+    asr(dst, dst, oopDesc::wide_storage_props_shift);\n+  }\n+}\n+\n@@ -4155,1 +4219,2 @@\n-                                     Register tmp1, Register thread_tmp) {\n+                                     Register tmp1, Register thread_tmp, Register tmp3) {\n+\n@@ -4160,1 +4225,1 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4162,1 +4227,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4186,2 +4251,2 @@\n-                                    Register thread_tmp, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);\n+                                    Register thread_tmp, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4192,1 +4257,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -5258,0 +5323,339 @@\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+\n+\/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->frame_size_in_bytes();\n+  assert(framesize % (2 * wordSize) == 0, \"must preserve 2 * wordSize alignment\");\n+\n+  \/\/ insert a nop at the start of the prolog so we can patch in a\n+  \/\/ branch if we need to invalidate the method later\n+  nop();\n+\n+  int bangsize = C->bang_size_in_bytes();\n+  if (C->need_stack_bang(bangsize) && UseStackBanging)\n+     generate_stack_overflow_check(bangsize);\n+\n+  build_frame(framesize);\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  cmp(r0, (u1) 1);\n+  br(Assembler::EQ, skip);\n+  int call_offset = -1;\n+\n+  Label slow_case;\n+\n+  \/\/ Try to allocate a new buffered inline type (from the heap)\n+  if (UseTLAB) {\n+\n+    if (vk != NULL) {\n+      \/\/ Called from C1, where the return type is statically known.\n+      mov(r1, (intptr_t)vk->get_InlineKlass());\n+      jint lh = vk->layout_helper();\n+      assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+      mov(r14, lh);\n+    } else {\n+       \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+       andr(r1, r0, -2);\n+       \/\/ get obj size\n+       ldrw(r14, Address(rscratch1 \/*klass*\/, Klass::layout_helper_offset()));\n+    }\n+\n+     ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+\n+     \/\/ check whether we have space in TLAB,\n+     \/\/ rscratch1 contains pointer to just allocated obj\n+      lea(r14, Address(r13, r14));\n+      ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));\n+\n+      cmp(r14, rscratch1);\n+      br(Assembler::GT, slow_case);\n+\n+      \/\/ OK we have room in TLAB,\n+      \/\/ Set new TLAB top\n+      str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+\n+      \/\/ Set new class always locked\n+      mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());\n+      str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));\n+\n+      store_klass_gap(r13, zr);  \/\/ zero klass gap for compressed oops\n+      if (vk == NULL) {\n+        \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n+         mov(r0, r1);\n+      }\n+\n+      store_klass(r13, r1);  \/\/ klass\n+\n+      if (vk != NULL) {\n+        \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+        mov(r0, r13);\n+        far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+      } else {\n+\n+        \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+        ldr(r1, Address(r0, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+        ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));\n+\n+        \/\/ Mov new class to r0 and call pack_handler\n+        mov(r0, r13);\n+        blr(r1);\n+      }\n+      b(skip);\n+  }\n+\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    ldr(rscratch1, RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    blr(rscratch1);\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        mov(to->as_Register(), from->as_Register());\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,\n+                                          int& to_index, RegState reg_state[]) {\n+  Register fromReg = from->is_reg() ? from->as_Register() : noreg;\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+\n+\n+  int vt = 1;\n+  bool done = true;\n+  bool mark_done = true;\n+  do {\n+    sig_index--;\n+    BasicType bt = sig->at(sig_index)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      vt--;\n+    } else if (bt == T_VOID &&\n+               sig->at(sig_index-1)._bt != T_LONG &&\n+               sig->at(sig_index-1)._bt != T_DOUBLE) {\n+      vt++;\n+    } else {\n+      assert(to_index >= 0, \"invalid to_index\");\n+      VMRegPair pair_to = regs_to[to_index--];\n+      VMReg to = pair_to.first();\n+\n+      if (bt == T_VOID) continue;\n+\n+      int idx = (int) to->value();\n+      if (reg_state[idx] == reg_readonly) {\n+         if (idx != from->value()) {\n+           mark_done = false;\n+         }\n+         done = false;\n+         continue;\n+      } else if (reg_state[idx] == reg_written) {\n+        continue;\n+      } else {\n+        assert(reg_state[idx] == reg_writable, \"must be writable\");\n+        reg_state[idx] = reg_written;\n+      }\n+\n+      if (fromReg == noreg) {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        ldr(rscratch2, Address(sp, st_off));\n+        fromReg = rscratch2;\n+      }\n+\n+      int off = sig->at(sig_index)._offset;\n+      assert(off > 0, \"offset in object should be positive\");\n+      bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+\n+      Address fromAddr = Address(fromReg, off);\n+      bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+\n+      if (!to->is_FloatRegister()) {\n+\n+        Register dst = to->is_stack() ? rscratch1 : to->as_Register();\n+\n+        if (is_oop) {\n+          load_heap_oop(dst, fromAddr);\n+        } else {\n+          load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+        }\n+        if (to->is_stack()) {\n+          int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+          str(dst, Address(sp, st_off));\n+        }\n+      } else {\n+        if (bt == T_DOUBLE) {\n+          ldrd(to->as_FloatRegister(), fromAddr);\n+        } else {\n+          assert(bt == T_FLOAT, \"must be float\");\n+          ldrs(to->as_FloatRegister(), fromAddr);\n+        }\n+     }\n+\n+    }\n+\n+  } while (vt != 0);\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"must be\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  Register val_array = r0;\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r10;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r1;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);\n+  VMRegPair from_pair;\n+  BasicType bt;\n+\n+  while (stream.next(from_pair, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    VMReg from_r1 = from_pair.first();\n+    VMReg from_r2 = from_pair.second();\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+\n+    if (!from_r1->is_FloatRegister()) {\n+      Register from_reg;\n+      if (from_r1->is_stack()) {\n+        from_reg = from_reg_tmp;\n+        int ld_off = from_r1->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        from_reg = from_r1->as_Register();\n+      }\n+\n+      if (is_oop) {\n+        DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;\n+        store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);\n+      } else {\n+        store_sized_value(dst, from_reg, size_in_bytes);\n+      }\n+    } else {\n+      if (from_r2->is_valid()) {\n+        strd(from_r1->as_FloatRegister(), dst);\n+      } else {\n+        strs(from_r1->as_FloatRegister(), dst);\n+      }\n+    }\n+\n+    reg_state[from_r1->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v0->as_VMReg() : r14->as_VMReg();\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":414,"deletions":10,"binary":false,"changes":424,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -32,0 +33,4 @@\n+#include \"runtime\/signature.hpp\"\n+\n+\n+class ciInlineKlass;\n@@ -614,0 +619,10 @@\n+  void test_klass_is_value(Register klass, Register temp_reg, Label& is_value);\n+\n+  void test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline);\n+  void test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline);\n+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened);\n+\n+  \/\/ Check klass\/oops is flat inline type array (oop->_klass->_layout_helper & vt_bit)\n+  void test_flattened_array_oop(Register klass, Register temp_reg, Label& is_flattened_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+\n@@ -821,0 +836,3 @@\n+  void load_metadata(Register dst, Register src);\n+  void load_storage_props(Register dst, Register src);\n+\n@@ -833,1 +851,1 @@\n-                       Register tmp1, Register tmp_thread);\n+                       Register tmp1, Register tmp_thread, Register tmp3 = noreg);\n@@ -845,1 +863,1 @@\n-                      Register tmp_thread = noreg, DecoratorSet decorators = 0);\n+                      Register tmp_thread = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);\n@@ -1185,0 +1203,14 @@\n+  void verified_entry(Compile* C, int sp_inc);\n+\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[]);\n+  void restore_stack(Compile* C);\n+  VMReg spill_reg_for(VMReg reg);\n+\n@@ -1249,0 +1281,2 @@\n+  void fill_words(Register base, uint64_t cnt, Register value);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":36,"deletions":2,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -322,0 +323,1 @@\n+    case T_INLINE_TYPE:\n@@ -355,0 +357,84 @@\n+\n+\/\/ const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_int = 6;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt, VMRegPair *regs, int total_args_passed) {\n+\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  \/\/ r1, r2 used to address klasses and states, exclude it from return convention to avoid colision\n+\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+     r0 \/* j_rarg7 *\/, j_rarg6, j_rarg5, j_rarg4, j_rarg3, j_rarg2\n+  };\n+\n+  static const FloatRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3, j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        \/\/ Should we have gurantee here?\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+      \/\/ Should T_METADATA be added to java_calling_convention as well ?\n+    case T_METADATA:\n+    case T_INLINE_TYPE:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -385,19 +471,40 @@\n-static void gen_c2i_adapter(MacroAssembler *masm,\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n-                            const VMRegPair *regs,\n-                            Label& skip_fixup) {\n-  \/\/ Before we get into the guts of the C2I adapter, see if we should be here\n-  \/\/ at all.  We've come from compiled code and are attempting to jump to the\n-  \/\/ interpreter, which means the caller made a static call to get here\n-  \/\/ (vcalls always get a compiled target if there is one).  Check for a\n-  \/\/ compiled target.  If there is one, we need to patch the caller's call.\n-  patch_callers_callsite(masm);\n-\n-  __ bind(skip_fixup);\n-\n-  int words_pushed = 0;\n-\n-  \/\/ Since all args are passed on the stack, total_args_passed *\n-  \/\/ Interpreter::stackElementSize is the space we need.\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+     for (int i = 0; i < sig_extended->length(); i++) {\n+       BasicType bt = sig_extended->at(i)._bt;\n+       if (bt == T_INLINE_TYPE) {\n+         \/\/ In sig_extended, an inline type argument starts with:\n+         \/\/ T_INLINE_TYPE, followed by the types of the fields of the\n+         \/\/ inline type and T_VOID to mark the end of the value\n+         \/\/ type. Inline types are flattened so, for instance, in the\n+         \/\/ case of an inline type with an int field and an inline type\n+         \/\/ field that itself has 2 fields, an int and a long:\n+         \/\/ T_INLINE_TYPE T_INT T_INLINE_TYPE T_INT T_LONG T_VOID (second\n+         \/\/ slot for the T_LONG) T_VOID (inner T_INLINE_TYPE) T_VOID\n+         \/\/ (outer T_INLINE_TYPE)\n+         total_args_passed++;\n+         int vt = 1;\n+         do {\n+           i++;\n+           BasicType bt = sig_extended->at(i)._bt;\n+           BasicType prev_bt = sig_extended->at(i-1)._bt;\n+           if (bt == T_INLINE_TYPE) {\n+             vt++;\n+           } else if (bt == T_VOID &&\n+                      prev_bt != T_LONG &&\n+                      prev_bt != T_DOUBLE) {\n+             vt--;\n+           }\n+         } while (vt != 0);\n+       } else {\n+         total_args_passed++;\n+       }\n+     }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n@@ -405,1 +512,2 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+  return total_args_passed;\n+}\n@@ -407,3 +515,1 @@\n-  __ mov(r13, sp);\n-  \/\/ stack is aligned, keep it that way\n-  extraspace = align_up(extraspace, 2*wordSize);\n+static void gen_c2i_adapter_helper(MacroAssembler* masm, BasicType bt, const VMRegPair& reg_pair, int extraspace, const Address& to) {\n@@ -412,13 +518,1 @@\n-  if (extraspace)\n-    __ sub(sp, sp, extraspace);\n-\n-  \/\/ Now write the args into the outgoing interpreter space\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n+    assert(bt != T_INLINE_TYPE || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n@@ -439,2 +533,5 @@\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n+    \/\/ int next_off = st_off - Interpreter::stackElementSize;\n+\n+    VMReg r_1 = reg_pair.first();\n+    VMReg r_2 = reg_pair.second();\n+\n@@ -443,1 +540,1 @@\n-      continue;\n+      return;\n@@ -445,0 +542,1 @@\n+\n@@ -447,3 +545,2 @@\n-      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size\n-                    + extraspace\n-                    + words_pushed * wordSize);\n+      \/\/ words_pushed is always 0 so we don't use it.\n+      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace \/* + word_pushed * wordSize *\/);\n@@ -453,1 +550,1 @@\n-        __ str(rscratch1, Address(sp, st_off));\n+        __ str(rscratch1, to);\n@@ -456,16 +553,1 @@\n-\n-\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ str(rscratch1, Address(sp, next_off));\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaaaull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ str(rscratch1, Address(sp, st_off));\n-        }\n+        __ str(rscratch1, to);\n@@ -476,19 +558,1 @@\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ str(r, Address(sp, st_off));\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ jlong\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaabull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-          __ str(r, Address(sp, next_off));\n-        } else {\n-          __ str(r, Address(sp, st_off));\n-        }\n-      }\n+      __ str(r, to);\n@@ -499,1 +563,1 @@\n-        __ strs(r_1->as_FloatRegister(), Address(sp, st_off));\n+        __ strs(r_1->as_FloatRegister(), to);\n@@ -501,6 +565,1 @@\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaacull);\n-        __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        __ strd(r_1->as_FloatRegister(), Address(sp, next_off));\n+        __ strd(r_1->as_FloatRegister(), to);\n@@ -508,0 +567,153 @@\n+   }\n+}\n+\n+static void gen_c2i_adapter(MacroAssembler *masm,\n+                            const GrowableArray<SigEntry>* sig_extended,\n+                            const VMRegPair *regs,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+\n+  \/\/ Before we get into the guts of the C2I adapter, see if we should be here\n+  \/\/ at all.  We've come from compiled code and are attempting to jump to the\n+  \/\/ interpreter, which means the caller made a static call to get here\n+  \/\/ (vcalls always get a compiled target if there is one).  Check for a\n+  \/\/ compiled target.  If there is one, we need to patch the caller's call.\n+  patch_callers_callsite(masm);\n+\n+  __ bind(skip_fixup);\n+\n+  bool has_inline_argument = false;\n+\n+  if (InlineTypePassFieldsAsArgs) {\n+      \/\/ Is there an inline type argument?\n+     for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+       has_inline_argument = (sig_extended->at(i)._bt == T_INLINE_TYPE);\n+     }\n+     if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);\n+\n+      frame_complete = __ offset();\n+      address the_pc = __ pc();\n+\n+      __ set_last_Java_frame(noreg, noreg, the_pc, rscratch1);\n+\n+      __ mov(c_rarg0, rthread);\n+      __ mov(c_rarg1, r1);\n+      __ mov(c_rarg2, (int64_t)alloc_inline_receiver);\n+\n+      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+      __ blr(rscratch1);\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ cbz(r0, no_exception);\n+\n+      __ str(zr, Address(rthread, JavaThread::vm_result_offset()));\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ b(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(r10, rthread);\n+      __ get_vm_result_2(r1, rthread); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n+  int words_pushed = 0;\n+\n+  \/\/ Since all args are passed on the stack, total_args_passed *\n+  \/\/ Interpreter::stackElementSize is the space we need.\n+\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n+  int extraspace = (total_args_passed * Interpreter::stackElementSize) + wordSize;\n+\n+  \/\/ stack is aligned, keep it that way\n+  extraspace = align_up(extraspace, 2 * wordSize);\n+\n+  __ mov(r13, sp);\n+\n+  if (extraspace)\n+    __ sub(sp, sp, extraspace);\n+\n+  \/\/ Now write the args into the outgoing interpreter space\n+\n+  int ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+  bool has_oop_field = false;\n+\n+  for (int next_arg_comp = 0; next_arg_comp < total_args_passed; next_arg_comp++) {\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    \/\/ offset to start parameters\n+    int st_off   = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;\n+\n+    if (!InlineTypePassFieldsAsArgs || bt != T_INLINE_TYPE) {\n+      if (bt == T_VOID) {\n+         assert(next_arg_comp > 0 && (sig_extended->at(next_arg_comp - 1)._bt == T_LONG || sig_extended->at(next_arg_comp - 1)._bt == T_DOUBLE), \"missing half\");\n+         next_arg_int ++;\n+         continue;\n+       }\n+\n+       int next_off = st_off - Interpreter::stackElementSize;\n+       int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+\n+       gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp], extraspace, Address(sp, offset));\n+       next_arg_int ++;\n+   } else {\n+       ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_INLINE_TYPE);\n+      __ load_heap_oop(rscratch1, Address(r10, index));\n+      next_vt_arg++;\n+      next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of value\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp - 1)._bt;\n+        if (bt == T_INLINE_TYPE) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n+        } else {\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          assert(off > 0, \"offset in object should be positive\");\n+\n+          bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+          has_oop_field = has_oop_field || is_oop;\n+\n+          gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp - ignored], extraspace, Address(r11, off));\n+        }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ str(rscratch1, Address(sp, st_off));\n+   }\n+\n+  }\n+\n+\/\/ If an inline type was allocated and initialized, apply post barrier to all oop fields\n+  if (has_inline_argument && has_oop_field) {\n+    __ push(r13); \/\/ save senderSP\n+    __ push(r1); \/\/ save callee\n+    \/\/ Allocate argument register save area\n+    if (frame::arg_reg_save_area_bytes != 0) {\n+      __ sub(sp, sp, frame::arg_reg_save_area_bytes);\n@@ -509,0 +721,7 @@\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::apply_post_barriers), rthread, r10);\n+    \/\/ De-allocate argument register save area\n+    if (frame::arg_reg_save_area_bytes != 0) {\n+      __ add(sp, sp, frame::arg_reg_save_area_bytes);\n+    }\n+    __ pop(r1); \/\/ restore callee\n+    __ pop(r13); \/\/ restore sender SP\n@@ -517,0 +736,1 @@\n+void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm, int comp_args_on_stack, const GrowableArray<SigEntry>* sig, const VMRegPair *regs) {\n@@ -518,5 +738,0 @@\n-void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,\n-                                    int total_args_passed,\n-                                    int comp_args_on_stack,\n-                                    const BasicType *sig_bt,\n-                                    const VMRegPair *regs) {\n@@ -582,1 +797,1 @@\n-  int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)>>LogBytesPerWord;\n+  int comp_words_on_stack = 0;\n@@ -584,2 +799,3 @@\n-    __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n-    __ andr(sp, rscratch1, -16);\n+     comp_words_on_stack = align_up(comp_args_on_stack * VMRegImpl::stack_slot_size, wordSize) >> LogBytesPerWord;\n+     __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n+     __ andr(sp, rscratch1, -16);\n@@ -604,0 +820,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -606,2 +824,5 @@\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+    BasicType bt = sig->at(i)._bt;\n+\n+    assert(bt != T_INLINE_TYPE, \"i2c adapter doesn't unpack inline typ args\");\n+    if (bt == T_VOID) {\n+      assert(i > 0 && (sig->at(i - 1)._bt == T_LONG || sig->at(i - 1)._bt == T_DOUBLE), \"missing half\");\n@@ -612,0 +833,1 @@\n+    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(), \"scrambled load targets?\");\n@@ -613,3 +835,1 @@\n-    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),\n-            \"scrambled load targets?\");\n-    int ld_off = (total_args_passed - i - 1)*Interpreter::stackElementSize;\n+    int ld_off = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n@@ -630,1 +850,1 @@\n-      int st_off = regs[i].first()->reg2stack()*VMRegImpl::stack_slot_size;\n+      int st_off = regs[i].first()->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -647,2 +867,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n@@ -651,11 +870,28 @@\n-        __ str(rscratch2, Address(sp, st_off));\n-      }\n-    } else if (r_1->is_Register()) {  \/\/ Register argument\n-      Register r = r_1->as_Register();\n-      if (r_2->is_valid()) {\n-        \/\/\n-        \/\/ We are using two VMRegs. This can be either T_OBJECT,\n-        \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n-        \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n-        \/\/ So we must adjust where to pick up the data to match the\n-        \/\/ interpreter.\n+         __ str(rscratch2, Address(sp, st_off));\n+       }\n+     } else if (r_1->is_Register()) {  \/\/ Register argument\n+       Register r = r_1->as_Register();\n+       if (r_2->is_valid()) {\n+         \/\/\n+         \/\/ We are using two VMRegs. This can be either T_OBJECT,\n+         \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n+         \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n+         \/\/ So we must adjust where to pick up the data to match the\n+         \/\/ interpreter.\n+\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n+\n+         \/\/ this can be a misaligned move\n+         __ ldr(r, Address(esp, offset));\n+       } else {\n+         \/\/ sign extend and use a full word?\n+         __ ldrw(r, Address(esp, ld_off));\n+       }\n+     } else {\n+       if (!r_2->is_valid()) {\n+         __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n+       } else {\n+         __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n+       }\n+     }\n+   }\n@@ -663,17 +899,0 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n-\n-        \/\/ this can be a misaligned move\n-        __ ldr(r, Address(esp, offset));\n-      } else {\n-        \/\/ sign extend and use a full word?\n-        __ ldrw(r, Address(esp, ld_off));\n-      }\n-    } else {\n-      if (!r_2->is_valid()) {\n-        __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n-      } else {\n-        __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n-      }\n-    }\n-  }\n@@ -692,1 +911,0 @@\n-\n@@ -696,13 +914,1 @@\n-\/\/ ---------------------------------------------------------------\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                                            int total_args_passed,\n-                                                            int comp_args_on_stack,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n-  address i2c_entry = __ pc();\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n-\n-  address c2i_unverified_entry = __ pc();\n-  Label skip_fixup;\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n@@ -743,0 +949,34 @@\n+}\n+\n+\n+\/\/ ---------------------------------------------------------------\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n+                                                            int comp_args_on_stack,\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n+\n+  address i2c_entry = __ pc();\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n+\n+  address c2i_unverified_entry = __ pc();\n+  Label skip_fixup;\n+\n+  gen_inline_cache_check(masm, skip_fixup);\n+\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    Label unused;\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+    skip_fixup = unused;\n+  }\n@@ -744,0 +984,1 @@\n+  \/\/ Scalarized c2i adapter\n@@ -748,0 +989,1 @@\n+\n@@ -750,4 +992,4 @@\n-\n-      __ ldrw(rscratch1, Address(rmethod, Method::access_flags_offset()));\n-      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n-      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n+        Register flags  = rscratch1;\n+      __ ldrw(flags, Address(rmethod, Method::access_flags_offset()));\n+      __ tst(flags, JVM_ACC_STATIC);\n+      __ br(Assembler::NE, L_skip_barrier); \/\/ non-static\n@@ -757,3 +999,6 @@\n-    __ load_method_holder(rscratch2, rmethod);\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    Register klass = rscratch1;\n+    __ load_method_holder(klass, rmethod);\n+    \/\/ We pass rthread to this function on x86\n+    __ clinit_barrier(klass, rscratch2, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n@@ -770,0 +1015,14 @@\n+  address c2i_unverified_inline_entry = c2i_unverified_entry;\n+\n+ \/\/ Non-scalarized c2i adapter\n+  address c2i_inline_entry = c2i_entry;\n+  if (regs != regs_cc) {\n+    Label inline_entry_skip_fixup;\n+    c2i_unverified_inline_entry = __ pc();\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    Label unused;\n+    gen_c2i_adapter(masm, sig, regs, inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+  }\n+\n@@ -771,1 +1030,8 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+\n+  \/\/ The c2i adapter might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+\n+  bool caller_must_gc_arguments = (regs != regs_cc);\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words + 10, oop_maps, caller_must_gc_arguments);\n+\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -814,0 +1080,1 @@\n+      case T_INLINE_TYPE:\n@@ -1665,0 +1932,1 @@\n+      case T_INLINE_TYPE:\n@@ -1852,0 +2120,1 @@\n+  case T_INLINE_TYPE:\n@@ -3097,0 +3366,105 @@\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler _masm(&buffer);\n+  MacroAssembler* masm = &_masm;\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ strs(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_DOUBLE) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(r0, val);\n+      \/\/ We don't need barriers because the destination is a newly allocated object.\n+      \/\/ Also, we cannot use store_heap_oop(to, val) because it uses r8 as tmp.\n+      if (UseCompressedOops) {\n+        __ encode_heap_oop(val);\n+        __ str(val, to);\n+      } else {\n+        __ str(val, to);\n+      }\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ store_sized_value(to, r_1->as_Register(), size_in_bytes);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(lr);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ ldrs(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ ldrd(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+       assert_different_registers(r0, r_1->as_Register());\n+       __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(lr);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, unpack_fields_off);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":525,"deletions":151,"binary":false,"changes":676,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -438,0 +439,5 @@\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(NULL, true);\n+  }\n+\n@@ -556,0 +562,1 @@\n+  case T_INLINE_TYPE: \/\/ fall through (value types are handled with oops)\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -145,1 +145,1 @@\n-  __ store_heap_oop(dst, val, r10, r1, decorators);\n+  __ store_heap_oop(dst, val, r10, r1, noreg, decorators);\n@@ -168,0 +168,1 @@\n+  case Bytecodes::_fast_qputfield:\n@@ -744,4 +745,4 @@\n-    \/\/ ??? convention: move array into r3 for exception message\n-  __ mov(r3, array);\n-  __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n-  __ br(rscratch1);\n+  \/\/ ??? convention: move array into r3 for exception message\n+   __ mov(r3, array);\n+   __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n+   __ br(rscratch1);\n@@ -807,5 +808,15 @@\n-  __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n-  do_oop_load(_masm,\n-              Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)),\n-              r0,\n-              IS_ARRAY);\n+  if (UseFlatArray) {\n+    Label is_flat_array, done;\n+\n+    __ test_flattened_array_oop(r0, r8 \/*temp*\/, is_flat_array);\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+\n+    __ b(done);\n+    __ bind(is_flat_array);\n+    __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_load), r0, r1);\n+    __ bind(done);\n+  } else {\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+  }\n@@ -1108,0 +1119,2 @@\n+\n+  \/\/ FIXME: Could we remove the line below?\n@@ -1113,0 +1126,5 @@\n+  Label  is_flat_array;\n+  if (UseFlatArray) {\n+    __ test_flattened_array_oop(r3, r8 \/*temp*\/, is_flat_array);\n+  }\n+\n@@ -1115,0 +1133,1 @@\n+\n@@ -1117,2 +1136,1 @@\n-  __ ldr(r0, Address(r0,\n-                     ObjArrayKlass::element_klass_offset()));\n+  __ ldr(r0, Address(r0, ObjArrayKlass::element_klass_offset()));\n@@ -1123,0 +1141,1 @@\n+\n@@ -1129,0 +1148,1 @@\n+\n@@ -1132,0 +1152,1 @@\n+\n@@ -1142,0 +1163,13 @@\n+  if (EnableValhalla) {\n+    Label is_null_into_value_array_npe, store_null;\n+\n+    \/\/ No way to store null in flat array\n+    __ test_null_free_array_oop(r3, r8, is_null_into_value_array_npe);\n+    __ b(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n+\n@@ -1144,0 +1178,40 @@\n+  __ b(done);\n+\n+  if (EnableValhalla) {\n+     Label is_type_ok;\n+\n+    \/\/ store non-null value\n+    __ bind(is_flat_array);\n+\n+    \/\/ Simplistic type check...\n+    \/\/ r0 - value, r2 - index, r3 - array.\n+\n+    \/\/ Profile the not-null value's klass.\n+    \/\/ Load value class\n+     __ load_klass(r1, r0);\n+     __ profile_typecheck(r2, r1, r0); \/\/ blows r2, and r0\n+\n+    \/\/ flat value array needs exact type match\n+    \/\/ is \"r8 == r0\" (value subclass == array element superclass)\n+\n+    \/\/ Move element klass into r0\n+\n+     __ load_klass(r0, r3);\n+\n+     __ ldr(r0, Address(r0, ArrayKlass::element_klass_offset()));\n+     __ cmp(r0, r1);\n+     __ br(Assembler::EQ, is_type_ok);\n+\n+     __ profile_typecheck_failed(r2);\n+     __ b(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));\n+\n+     __ bind(is_type_ok);\n+\n+    \/\/ Reload from TOS to be safe, because of profile_typecheck that blows r2 and r0.\n+    \/\/ FIXME: Should we really do it?\n+     __ ldr(r1, at_tos());  \/\/ value\n+     __ mov(r2, r3); \/\/ array, ldr(r2, at_tos_p2());\n+     __ ldr(r3, at_tos_p1()); \/\/ index\n+     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_store), r1, r2, r3);\n+  }\n+\n@@ -2014,2 +2088,1 @@\n-void TemplateTable::if_acmp(Condition cc)\n-{\n+void TemplateTable::if_acmp(Condition cc) {\n@@ -2018,1 +2091,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -2020,0 +2093,36 @@\n+\n+  Register is_value_mask = rscratch1;\n+  __ mov(is_value_mask, markWord::always_locked_pattern);\n+\n+  if (EnableValhalla) {\n+    __ cmp(r1, r0);\n+    __ br(Assembler::EQ, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either r0 or r1 is null\n+    __ andr(r2, r0, r1);\n+    __ cbz(r2, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ ldr(r2, Address(r1, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r2, r2, is_value_mask);\n+    __ ldr(r4, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r4, r4, is_value_mask);\n+    __ andr(r2, r2, r4);\n+    __ cmp(r2,  is_value_mask);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(r2, r1);\n+    __ load_metadata(r4, r0);\n+    __ cmp(r2, r4);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(r0, r1, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(r0, r1, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -2022,0 +2131,1 @@\n+  __ bind(taken);\n@@ -2027,0 +2137,10 @@\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored... r0 answer, jmp to outcome...\n+  __ cbz(r0, not_subst);\n+  __ b(is_subst);\n+}\n+\n+\n@@ -2499,2 +2619,1 @@\n-  __ ubfxw(flags, raw_flags, ConstantPoolCacheEntry::tos_state_shift,\n-           ConstantPoolCacheEntry::tos_state_bits);\n+  __ ubfxw(flags, raw_flags, ConstantPoolCacheEntry::tos_state_shift, ConstantPoolCacheEntry::tos_state_bits);\n@@ -2535,4 +2654,61 @@\n-  do_oop_load(_masm, field, r0, IN_HEAP);\n-  __ push(atos);\n-  if (rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+  if (!EnableValhalla) {\n+    do_oop_load(_masm, field, r0, IN_HEAP);\n+    __ push(atos);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+    }\n+    __ b(Done);\n+  } else { \/\/ Valhalla\n+\n+    if (is_static) {\n+      __ load_heap_oop(r0, field);\n+      Label is_inline, isUninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_inline_type(raw_flags, r8 \/*temp*\/, is_inline);\n+        \/\/ Not inline case\n+        __ push(atos);\n+        __ b(Done);\n+      \/\/ Inline case, must not return null even if uninitialized\n+      __ bind(is_inline);\n+        __ cbz(r0, isUninitialized);\n+          __ push(atos);\n+          __ b(Done);\n+        __ bind(isUninitialized);\n+          __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);\n+          __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field), obj, raw_flags);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(Done);\n+    } else {\n+      Label isFlattened, isInitialized, is_inline, rewrite_inline;\n+        __ test_field_is_inline_type(raw_flags, r8 \/*temp*\/, is_inline);\n+        \/\/ Non-inline field case\n+        __ load_heap_oop(r0, field);\n+        __ push(atos);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+        }\n+        __ b(Done);\n+      __ bind(is_inline);\n+        __ test_field_is_inlined(raw_flags, r8 \/* temp *\/, isFlattened);\n+         \/\/ Non-inline field case\n+          __ load_heap_oop(r0, field);\n+          __ cbnz(r0, isInitialized);\n+            __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);\n+            __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_inline_type_field), obj, raw_flags);\n+          __ bind(isInitialized);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(rewrite_inline);\n+        __ bind(isFlattened);\n+          __ ldr(r10, Address(cache, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));\n+          __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);\n+          call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), obj, raw_flags, r10);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+         patch_bytecode(Bytecodes::_fast_qgetfield, bc, r1);\n+      }\n+      __ b(Done);\n+    }\n@@ -2540,1 +2716,0 @@\n-  __ b(Done);\n@@ -2710,0 +2885,1 @@\n+  const Register flags2 = r6;\n@@ -2732,0 +2908,2 @@\n+  __ mov(flags2, flags);\n+\n@@ -2774,8 +2952,50 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, r0, IN_HEAP);\n-    if (rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n-    }\n-    __ b(Done);\n+     if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n+      }\n+      __ b(Done);\n+     } else { \/\/ Valhalla\n+\n+      __ pop(atos);\n+      if (is_static) {\n+        Label not_inline;\n+         __ test_field_is_not_inline_type(flags2, r8 \/* temp *\/, not_inline);\n+         __ null_check(r0);\n+         __ bind(not_inline);\n+         do_oop_store(_masm, field, r0, IN_HEAP);\n+         __ b(Done);\n+      } else {\n+        Label is_inline, isFlattened, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_inline_type(flags2, r8 \/*temp*\/, is_inline);\n+        \/\/ Not inline case\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ bind(rewrite_not_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+        \/\/ Implementation of the inline semantic\n+        __ bind(is_inline);\n+        __ null_check(r0);\n+        __ test_field_is_inlined(flags2, r8 \/*temp*\/, isFlattened);\n+        \/\/ Not inline case\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ b(rewrite_inline);\n+        __ bind(isFlattened);\n+        pop_and_check_object(obj);\n+        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, off, obj);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_qputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+      }\n+     }  \/\/ Valhalla\n@@ -2921,0 +3141,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -2947,0 +3168,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -3000,0 +3222,13 @@\n+  case Bytecodes::_fast_qputfield: \/\/fall through\n+   {\n+      Label isFlattened, done;\n+      __ null_check(r0);\n+      __ test_field_is_flattened(r3, r8 \/* temp *\/, isFlattened);\n+      \/\/ No Flattened case\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      __ b(done);\n+      __ bind(isFlattened);\n+      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, r1, r2);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3097,0 +3332,26 @@\n+  case Bytecodes::_fast_qgetfield:\n+    {\n+       Label isFlattened, isInitialized, Done;\n+       \/\/ FIXME: We don't need to reload registers multiple times, but stay close to x86 code\n+       __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n+       __ test_field_is_inlined(r9, r8 \/* temp *\/, isFlattened);\n+        \/\/ Non-flattened field case\n+        __ mov(r9, r0);\n+        __ load_heap_oop(r0, field);\n+        __ cbnz(r0, isInitialized);\n+          __ mov(r0, r9);\n+          __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n+          __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);\n+          __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_inline_type_field), r0, r9);\n+        __ bind(isInitialized);\n+        __ verify_oop(r0);\n+        __ b(Done);\n+      __ bind(isFlattened);\n+        __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n+        __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);\n+        __ ldr(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));\n+        call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), r0, r9, r3);\n+        __ verify_oop(r0);\n+      __ bind(Done);\n+    }\n+    break;\n@@ -3647,0 +3908,24 @@\n+void TemplateTable::defaultvalue() {\n+  transition(vtos, atos);\n+  __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);\n+  __ get_constant_pool(c_rarg1);\n+  call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::defaultvalue),\n+          c_rarg1, c_rarg2);\n+  __ verify_oop(r0);\n+  \/\/ Must prevent reordering of stores for object initialization with stores that publish the new object.\n+  __ membar(Assembler::StoreStore);\n+}\n+\n+void TemplateTable::withfield() {\n+  transition(vtos, atos);\n+  resolve_cache_and_index(f2_byte, c_rarg1 \/*cache*\/, c_rarg2 \/*index*\/, sizeof(u2));\n+\n+  \/\/ n.b. unlike x86 cache is now rcpool plus the indexed offset\n+  \/\/ so using rcpool to meet shared code expectations\n+\n+  call_VM(r1, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), rcpool);\n+  __ verify_oop(r1);\n+  __ add(esp, esp, r0);\n+  __ mov(r0, r1);\n+}\n+\n@@ -3718,0 +4003,3 @@\n+  __ b(done);\n+  __ bind(is_null);\n+\n@@ -3720,4 +4008,16 @@\n-    __ b(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n+  if (EnableValhalla) {\n+    \/\/ Get cpool & tags index\n+    __ get_cpool_and_tags(r2, r3); \/\/ r2=cpool, r3=tags array\n+    __ get_unsigned_2_byte_index_at_bcp(r19, 1); \/\/ r19=index\n+     \/\/ See if bytecode has already been quicked\n+    __ add(rscratch1, r3, Array<u1>::base_offset_in_bytes());\n+    __ lea(r1, Address(rscratch1, r19));\n+    __ ldarb(r1, r1);\n+    \/\/ See if CP entry is a Q-descriptor\n+    __ andr (r1, r1, JVM_CONSTANT_QDescBit);\n+    __ cmp(r1, (u1) JVM_CONSTANT_QDescBit);\n+    __ br(Assembler::NE, done);\n+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":334,"deletions":34,"binary":false,"changes":368,"status":"modified"},{"patch":"@@ -2566,0 +2566,4 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3202,0 +3202,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1885,1 +1885,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n@@ -1926,1 +1926,1 @@\n-    profile_obj_type(ret, R28_mdx, -in_bytes(ReturnTypeEntry::size()), tmp1, tmp2);\n+    profile_obj_type(ret, R28_mdx, -in_bytes(SingleTypeEntry::size()), tmp1, tmp2);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3091,0 +3091,4 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1771,1 +1771,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1820,1 +1820,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/objArrayKlass.hpp\"\n@@ -175,0 +176,73 @@\n+\/\/ Implementation of LoadFlattenedArrayStub\n+\n+LoadFlattenedArrayStub::LoadFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _result = result;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void LoadFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 1);\n+  ce->store_parameter(_index->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::load_flattened_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  if (_result->as_register() != rax) {\n+    __ movptr(_result->as_register(), rax);\n+  }\n+  __ jmp(_continuation);\n+}\n+\n+\n+\/\/ Implementation of StoreFlattenedArrayStub\n+\n+StoreFlattenedArrayStub::StoreFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr value, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _value = value;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+\n+void StoreFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 2);\n+  ce->store_parameter(_index->as_register(), 1);\n+  ce->store_parameter(_value->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::store_flattened_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ jmp(_continuation);\n+}\n+\n+\n+\/\/ Implementation of SubstitutabilityCheckStub\n+\n+SubstitutabilityCheckStub::SubstitutabilityCheckStub(LIR_Opr left, LIR_Opr right, CodeEmitInfo* info) {\n+  _left = left;\n+  _right = right;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void SubstitutabilityCheckStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_left->as_register(), 1);\n+  ce->store_parameter(_right->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::substitutability_check_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ jmp(_continuation);\n+}\n+\n+\n@@ -183,0 +257,1 @@\n+         stub_id == Runtime1::new_instance_no_inline_id       ||\n@@ -227,1 +302,2 @@\n-NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info) {\n+NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result,\n+                                       CodeEmitInfo* info, bool is_inline_type) {\n@@ -232,0 +308,1 @@\n+  _is_inline_type = is_inline_type;\n@@ -240,1 +317,5 @@\n-  __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::new_object_array_id)));\n+  if (_is_inline_type) {\n+    __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::new_flat_array_id)));\n+  } else {\n+    __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::new_object_array_id)));\n+  }\n@@ -250,1 +331,1 @@\n-MonitorEnterStub::MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)\n+MonitorEnterStub::MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info, CodeStub* throw_imse_stub, LIR_Opr scratch_reg)\n@@ -254,0 +335,5 @@\n+  _throw_imse_stub = throw_imse_stub;\n+  _scratch_reg = scratch_reg;\n+  if (_throw_imse_stub != NULL) {\n+    assert(_scratch_reg != LIR_OprFact::illegalOpr, \"must be\");\n+  }\n@@ -260,0 +346,9 @@\n+  if (_throw_imse_stub != NULL) {\n+    \/\/ When we come here, _obj_reg has already been checked to be non-null.\n+    const int is_value_mask = markWord::always_locked_pattern;\n+    Register mark = _scratch_reg->as_register();\n+    __ movptr(mark, Address(_obj_reg->as_register(), oopDesc::mark_offset_in_bytes()));\n+    __ andptr(mark, is_value_mask);\n+    __ cmpl(mark, is_value_mask);\n+    __ jcc(Assembler::equal, *_throw_imse_stub->entry());\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":98,"deletions":3,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -195,1 +197,1 @@\n-    if (const_opr->type() == T_OBJECT) {\n+    if (const_opr->type() == T_OBJECT || const_opr->type() == T_INLINE_TYPE) {\n@@ -482,1 +484,2 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  int initial_framesize = initial_frame_size_in_bytes();\n+  __ remove_frame(initial_framesize, needs_stack_repair(), initial_framesize - wordSize);\n@@ -527,0 +530,17 @@\n+  ciMethod* method = compilation()->method();\n+  ciType* return_type = method->return_type();\n+  if (InlineTypeReturnedAsFields && return_type->is_inlinetype()) {\n+    ciInlineKlass* vk = return_type->as_inline_klass();\n+    if (vk->can_be_returned_as_fields()) {\n+#ifndef _LP64\n+      Unimplemented();\n+#else\n+      address unpack_handler = vk->unpack_handler();\n+      assert(unpack_handler != NULL, \"must be\");\n+      __ call(RuntimeAddress(unpack_handler));\n+      \/\/ At this point, rax points to the value object (for interpreter or C1 caller).\n+      \/\/ The fields of the object are copied into registers (for C2 caller).\n+#endif\n+    }\n+  }\n+\n@@ -528,1 +548,2 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  int initial_framesize = initial_frame_size_in_bytes();\n+  __ remove_frame(initial_framesize, needs_stack_repair(), initial_framesize - wordSize);\n@@ -553,0 +574,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -613,0 +638,1 @@\n+    case T_INLINE_TYPE: \/\/ Fall through\n@@ -703,0 +729,1 @@\n+    case T_INLINE_TYPE: \/\/ Fall through\n@@ -742,0 +769,1 @@\n+    case T_INLINE_TYPE: \/\/ fall through\n@@ -830,1 +858,1 @@\n-    if (src->type() == T_OBJECT) {\n+    if (src->type() == T_OBJECT || src->type() == T_INLINE_TYPE) {\n@@ -1016,0 +1044,1 @@\n+    case T_INLINE_TYPE: \/\/ fall through\n@@ -1189,1 +1218,1 @@\n-  if (addr->base()->type() == T_OBJECT) {\n+  if (addr->base()->type() == T_OBJECT || addr->base()->type() == T_INLINE_TYPE) {\n@@ -1250,0 +1279,1 @@\n+    case T_INLINE_TYPE: \/\/ fall through\n@@ -1636,1 +1666,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->type() == T_INLINE_TYPE ||\n@@ -1735,14 +1765,16 @@\n-  __ cmpptr(obj, (int32_t)NULL_WORD);\n-  if (op->should_profile()) {\n-    Label not_null;\n-    __ jccb(Assembler::notEqual, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n-    int header_bits = BitData::null_seen_byte_constant();\n-    __ orb(data_addr, header_bits);\n-    __ jmp(*obj_is_null);\n-    __ bind(not_null);\n-  } else {\n-    __ jcc(Assembler::equal, *obj_is_null);\n+  if (op->need_null_check()) {\n+    __ cmpptr(obj, (int32_t)NULL_WORD);\n+    if (op->should_profile()) {\n+      Label not_null;\n+      __ jccb(Assembler::notEqual, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n+      int header_bits = BitData::null_seen_byte_constant();\n+      __ orb(data_addr, header_bits);\n+      __ jmp(*obj_is_null);\n+      __ bind(not_null);\n+    } else {\n+      __ jcc(Assembler::equal, *obj_is_null);\n+    }\n@@ -1960,0 +1992,105 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be flattened (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is flattened, take the slow path.\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  Register klass = op->tmp()->as_register();\n+  __ load_klass(klass, op->array()->as_register(), tmp_load_klass);\n+  __ movl(klass, Address(klass, Klass::layout_helper_offset()));\n+  __ testl(klass, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  __ jcc(Assembler::notZero, *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ The array is not flattened, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cmpptr(op->value()->as_register(), (int32_t)NULL_WORD);\n+    __ jcc(Assembler::notEqual, skip);\n+    __ testl(klass, Klass::_lh_null_free_bit_inplace);\n+    __ jcc(Assembler::notZero, *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  Register klass = op->tmp()->as_register();\n+  __ load_klass(klass, op->array()->as_register(), tmp_load_klass);\n+  __ movl(klass, Address(klass, Klass::layout_helper_offset()));\n+  __ testl(klass, Klass::_lh_null_free_bit_inplace);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmpptr(left, right);\n+  __ jcc(Assembler::equal, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  __ testptr(left, right);\n+  __ jcc(Assembler::zero, L_oops_not_equal);\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Value object check -- if either of the operands is not a value object,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are value objects\n+  if ((left_klass == NULL || right_klass == NULL) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ movptr(tmp1, (intptr_t)markWord::always_locked_pattern);\n+    __ andptr(tmp1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(tmp1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ cmpptr(tmp1, (intptr_t)markWord::always_locked_pattern);\n+    __ jcc(Assembler::notEqual, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != NULL && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ jmp(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedOops) {\n+      __ movl(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movl(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpl(left_klass_op, right_klass_op);\n+    } else {\n+      __ movptr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movptr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpptr(left_klass_op, right_klass_op);\n+    }\n+\n+    __ jcc(Assembler::equal, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  \/\/ We've returned from the stub. RAX contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cmpl(rax, 0);\n+  __ jcc(Assembler::equal, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n@@ -2020,0 +2157,15 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, NULL);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n@@ -2899,1 +3051,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2905,1 +3057,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -3101,0 +3253,19 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ testptr(obj, obj);\n+    __ jcc(Assembler::zero, *slow_path->entry());\n+  }\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  __ load_klass(tmp, obj, tmp_load_klass);\n+  __ movl(tmp, Address(tmp, Klass::layout_helper_offset()));\n+  if (is_dest) {\n+    \/\/ We also take slow path if it's a null_free destination array, just in case the source array\n+    \/\/ contains NULLs.\n+    __ testl(tmp, Klass::_lh_null_free_bit_inplace);\n+  } else {\n+    __ testl(tmp, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  }\n+  __ jcc(Assembler::notZero, *slow_path->entry());\n+}\n+\n+\n@@ -3122,0 +3293,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ jmp(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -3215,0 +3392,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -3799,0 +3984,20 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr());\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ testptr(obj, obj);\n+    __ jccb(Assembler::zero, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  __ orb(mdo_addr, flag);\n+\n+  __ bind(not_inline_type);\n+}\n+\n@@ -4059,0 +4264,3 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ cmpptr(frame_map()->address_for_orig_pc_addr(), (int32_t)NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":230,"deletions":22,"binary":false,"changes":252,"status":"modified"},{"patch":"@@ -504,0 +504,4 @@\n+  if (EnableValhalla && !UseBiasedLocking) {\n+    \/\/ Mask always_locked bit such that we go to the slow path if object is an inline type\n+    andptr(tmpReg, ~((int) markWord::biased_lock_bit_in_place));\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -139,1 +139,0 @@\n-      sender_unextended_sp = sender_sp;\n@@ -143,2 +142,2 @@\n-      saved_fp = (intptr_t*) *(sender_sp - frame::sender_sp_offset);\n-    }\n+      intptr_t** saved_fp_addr = (intptr_t**) (sender_sp - frame::sender_sp_offset);\n+      saved_fp = *saved_fp_addr;\n@@ -146,0 +145,4 @@\n+      \/\/ Repair the sender sp if this is a method with scalarized inline type args\n+      sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+      sender_unextended_sp = sender_sp;\n+    }\n@@ -443,3 +446,3 @@\n-  intptr_t* unextended_sp = sender_sp;\n-  \/\/ On Intel the return_address is always the word on the stack\n-  address sender_pc = (address) *(sender_sp-1);\n+#ifdef ASSERT\n+  address sender_pc_copy = (address) *(sender_sp-1);\n+#endif\n@@ -452,0 +455,16 @@\n+  \/\/ Repair the sender sp if the frame has been extended\n+  sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+\n+  \/\/ On Intel the return_address is always the word on the stack\n+  address sender_pc = (address) *(sender_sp-1);\n+\n+#ifdef ASSERT\n+  if (sender_pc != sender_pc_copy) {\n+    \/\/ When extending the stack in the callee method entry to make room for unpacking of value\n+    \/\/ type args, we keep a copy of the sender pc at the expected location in the callee frame.\n+    \/\/ If the sender pc is patched due to deoptimization, the copy is not consistent anymore.\n+    nmethod* nm = CodeCache::find_blob(sender_pc)->as_nmethod();\n+    assert(sender_pc == nm->deopt_mh_handler_begin() || sender_pc == nm->deopt_handler_begin(), \"unexpected sender pc\");\n+  }\n+#endif\n+\n@@ -456,1 +475,20 @@\n-    map->set_include_argument_oops(_cb->caller_must_gc_arguments(map->thread()));\n+    bool caller_args = _cb->caller_must_gc_arguments(map->thread());\n+#ifdef COMPILER1\n+    if (!caller_args) {\n+      nmethod* nm = _cb->as_nmethod_or_null();\n+      if (nm != NULL && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+          pc() < nm->verified_inline_entry_point()) {\n+        \/\/ The VEP and VIEP(RO) of C1-compiled methods call buffer_inline_args_xxx\n+        \/\/ before doing any argument shuffling, so we need to scan the oops\n+        \/\/ as the caller passes them.\n+        caller_args = true;\n+#ifdef ASSERT\n+        NativeCall* call = nativeCall_before(pc());\n+        address dest = call->destination();\n+        assert(dest == Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id) ||\n+               dest == Runtime1::entry_for(Runtime1::buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+      }\n+    }\n+#endif\n+    map->set_include_argument_oops(caller_args);\n@@ -468,1 +506,1 @@\n-  return frame(sender_sp, unextended_sp, *saved_fp_addr, sender_pc);\n+  return frame(sender_sp, sender_sp, *saved_fp_addr, sender_pc);\n@@ -475,1 +513,1 @@\n-  \/\/ Default is we done have to follow them. The sender_for_xxx will\n+  \/\/ Default is we don't have to follow them. The sender_for_xxx will\n@@ -580,0 +618,1 @@\n+    case T_INLINE_TYPE:\n@@ -681,0 +720,15 @@\n+\/\/ Check for a method with scalarized inline type arguments that needs\n+\/\/ a stack repair and return the repaired sender stack pointer.\n+intptr_t* frame::repair_sender_sp(intptr_t* sender_sp, intptr_t** saved_fp_addr) const {\n+  CompiledMethod* cm = _cb->as_compiled_method_or_null();\n+  if (cm != NULL && cm->needs_stack_repair()) {\n+    \/\/ The stack increment resides just below the saved rbp on the stack\n+    \/\/ and does not account for the return address.\n+    intptr_t* real_frame_size_addr = (intptr_t*) (saved_fp_addr - 1);\n+    int real_frame_size = ((*real_frame_size_addr) + wordSize) \/ wordSize;\n+    assert(real_frame_size >= _cb->frame_size() && real_frame_size <= 1000000, \"invalid frame size\");\n+    sender_sp = unextended_sp() + real_frame_size;\n+  }\n+  return sender_sp;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":63,"deletions":9,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -126,0 +126,3 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  intptr_t* repair_sender_sp(intptr_t* sender_sp, intptr_t** saved_fp_addr) const;\n+\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -569,1 +569,1 @@\n-              Address dst, Register val, Register tmp1, Register tmp2) {\n+              Address dst, Register val, Register tmp1, Register tmp2, Register tmp3) {\n@@ -607,1 +607,1 @@\n-      BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg);\n+      BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg, noreg);\n@@ -610,1 +610,1 @@\n-      BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg);\n+      BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg, noreg);\n@@ -614,1 +614,1 @@\n-    BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2);\n+    BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2, tmp3);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -154,1 +155,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -199,1 +200,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n@@ -559,1 +560,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -567,1 +569,3 @@\n-  profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  if (profile) {\n+    profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  }\n@@ -573,1 +577,3 @@\n-  profile_typecheck_failed(rcx); \/\/ blows rcx\n+  if (profile) {\n+    profile_typecheck_failed(rcx); \/\/ blows rcx\n+  }\n@@ -1013,1 +1019,1 @@\n- \/\/ get method access flags\n+  \/\/ get method access flags\n@@ -1137,4 +1143,2 @@\n-  \/\/ remove activation\n-  \/\/ get sender sp\n-  movptr(rbx,\n-         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    movptr(rbx,\n+               Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n@@ -1162,0 +1166,34 @@\n+\n+  \/\/ remove activation\n+  \/\/ get sender sp\n+  movptr(rbx,\n+         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    Label skip;\n+    \/\/ Test if the return type is an inline type\n+    movptr(rdi, Address(rbp, frame::interpreter_frame_method_offset * wordSize));\n+    movptr(rdi, Address(rdi, Method::const_offset()));\n+    load_unsigned_byte(rdi, Address(rdi, ConstMethod::result_type_offset()));\n+    cmpl(rdi, T_INLINE_TYPE);\n+    jcc(Assembler::notEqual, skip);\n+\n+    \/\/ We are returning an inline type, load its fields into registers\n+#ifndef _LP64\n+    super_call_VM_leaf(StubRoutines::load_inline_type_fields_in_regs());\n+#else\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    load_klass(rdi, rax, tmp_load_klass);\n+    movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+\n+    testptr(rdi, rdi);\n+    jcc(Assembler::equal, skip);\n+\n+    call(rdi);\n+#endif\n+    \/\/ call above kills the value in rbx. Reload it.\n+    movptr(rbx, Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n@@ -1181,0 +1219,106 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  {\n+    SkipIfEqual skip_if(this, &DTraceAllocProbes, 0);\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::read_inlined_field(Register holder_klass,\n+                                                     Register field_index, Register field_offset,\n+                                                     Register obj) {\n+  Label alloc_failed, empty_value, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+  assert_different_registers(obj, holder_klass, field_index, field_offset, dst_temp);\n+\n+  \/\/ Grap the inline field klass\n+  push(holder_klass);\n+  const Register field_klass = holder_klass;\n+  get_inline_type_field_klass(holder_klass, field_index, field_klass);\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(field_klass, dst_temp, empty_value);\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  data_for_oop(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, field_klass);\n+  pop(obj);\n+  pop(holder_klass);\n+  jmp(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(field_klass, dst_temp, obj);\n+  pop(holder_klass);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  pop(holder_klass);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_inlined_field),\n+          obj, field_index, holder_klass);\n+\n+  bind(done);\n+}\n+\n+void InterpreterMacroAssembler::read_flattened_element(Register array, Register index,\n+                                                       Register t1, Register t2,\n+                                                       Register obj) {\n+  assert_different_registers(array, index, t1, t2);\n+  Label alloc_failed, empty_value, done;\n+  const Register array_klass = t2;\n+  const Register elem_klass = t1;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+\n+  \/\/ load in array->klass()->element_klass()\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(array_klass, array, tmp_load_klass);\n+  movptr(elem_klass, Address(array_klass, ArrayKlass::element_klass_offset()));\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(elem_klass, dst_temp, empty_value);\n+\n+  \/\/ calc source into \"array_klass\" and free up some regs\n+  const Register src = array_klass;\n+  push(index); \/\/ preserve index reg in case alloc_failed\n+  data_for_value_array_index(array, array_klass, index, src);\n+\n+  allocate_instance(elem_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+  \/\/ Have an oop instance buffer, copy into it\n+  store_ptr(0, obj); \/\/ preserve obj (overwrite index, no longer needed)\n+  data_for_oop(obj, dst_temp, elem_klass);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, elem_klass);\n+  pop(obj);\n+  jmp(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(elem_klass, dst_temp, obj);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(index);\n+  if (array == c_rarg2) {\n+    mov(elem_klass, array);\n+    array = elem_klass;\n+  }\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_load), array, index);\n+\n+  bind(done);\n+}\n+\n@@ -1232,0 +1376,4 @@\n+    if (EnableValhalla && !UseBiasedLocking) {\n+      \/\/ For slow path is_always_locked, using biased, which is never natural for !UseBiasLocking\n+      andptr(swap_reg, ~((int) markWord::biased_lock_bit_in_place));\n+    }\n@@ -1565,1 +1713,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1577,1 +1725,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()): in_bytes(BranchData::branch_data_size()));\n@@ -1952,0 +2100,78 @@\n+void InterpreterMacroAssembler::profile_array(Register mdp,\n+                                              Register array,\n+                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flattened_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element(Register mdp,\n+                                                Register element,\n+                                                Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadStoreData::array_load_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":238,"deletions":12,"binary":false,"changes":250,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -51,0 +52,1 @@\n+#include \"vmreg_x86.inline.hpp\"\n@@ -52,0 +54,3 @@\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1636,0 +1641,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2604,0 +2613,104 @@\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  testl(temp_reg, JVM_ACC_INLINE);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  testptr(object, object);\n+  jcc(Assembler::equal, not_inline_type);\n+  const int is_inline_type_mask = markWord::always_locked_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  movl(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  testl(temp_reg, InstanceKlass::misc_flags_is_empty_inline_type());\n+  jcc(Assembler::notZero, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::zero, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inlined_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inlined);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,\n+                                              Label&is_flattened_array) {\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(temp_reg, oop, tmp_load_klass);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flattened_array_layout(temp_reg, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flattened_array) {\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(temp_reg, oop, tmp_load_klass);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flattened_array_layout(temp_reg, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(temp_reg, oop, tmp_load_klass);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_null_free_array_layout(temp_reg, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(temp_reg, oop, tmp_load_klass);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_null_free_array_layout(temp_reg, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flattened_array);\n+}\n+void MacroAssembler::test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_bit_inplace);\n+  jcc(Assembler::notZero, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_bit_inplace);\n+  jcc(Assembler::zero, is_non_null_free_array);\n+}\n+\n+\n@@ -3303,0 +3416,129 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax, according to barrier asm eden_allocate\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+    jcc(Assembler::equal, L);\n+    stop(\"klass not initialized\");\n+    bind(L);\n+  }\n+#endif\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+  const bool allow_shared_alloc =\n+    Universe::heap()->supports_inline_contig_alloc();\n+\n+  push(klass);\n+  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(klass);\n+#ifndef _LP64\n+  if (UseTLAB || allow_shared_alloc) {\n+    get_thread(thread);\n+  }\n+#endif \/\/ _LP64\n+\n+  if (UseTLAB) {\n+    tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    \/\/ Allocation in the shared Eden, if allowed.\n+    \/\/\n+    eden_allocate(thread, new_obj, layout_size, 0, t2, slow_case);\n+  }\n+\n+  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n+  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n+  if (UseTLAB || allow_shared_alloc) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      decrement(layout_size, sizeof(oopDesc));\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 1*oopSize), zero);\n+        NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 2*oopSize), zero));\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+#ifdef _LP64\n+    xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+#endif\n+    movptr(t2, klass);         \/\/ preserve klass\n+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    store_klass(new_obj, t2, tmp_store_klass);  \/\/ src klass reg is potentially compressed\n+\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -3380,0 +3622,50 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  movptr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(inline_klass, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  movptr(inline_klass, Address(inline_klass, index, Address::times_ptr));\n+}\n+\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  movptr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  movl(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  movptr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset, Address::times_1);\n+  load_heap_oop(obj, field);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n+\n@@ -3728,1 +4020,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -3826,1 +4122,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4322,0 +4622,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -4331,1 +4639,1 @@\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -4364,1 +4672,1 @@\n-                                     Register tmp1, Register tmp2) {\n+                                     Register tmp1, Register tmp2, Register tmp3) {\n@@ -4369,1 +4677,23 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+  } else {\n+    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+  }\n+}\n+\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n@@ -4371,1 +4701,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2);\n+    lea(data, Address(oop, offset));\n@@ -4375,0 +4705,18 @@\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE)));\n+}\n+\n@@ -4396,2 +4744,2 @@\n-                                    Register tmp2, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2);\n+                                    Register tmp2, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2, tmp3);\n@@ -4402,1 +4750,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -4717,1 +5065,5 @@\n-void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n@@ -4770,0 +5122,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, C->output()->sp_inc_offset()), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -4798,5 +5156,0 @@\n-\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->nmethod_entry_barrier(this);\n-  }\n@@ -4806,1 +5159,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, XMMRegister xtmp) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp) {\n@@ -4810,0 +5163,1 @@\n+  movdq(xtmp, val);\n@@ -4811,1 +5165,2 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -4813,1 +5168,1 @@\n-    pxor(xtmp, xtmp);\n+    punpcklqdq(xtmp, xtmp);\n@@ -4857,1 +5212,297 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp, bool is_large) {\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  Label slow_case;\n+\n+  \/\/ Try to allocate a new buffered inline type (from the heap)\n+  if (UseTLAB) {\n+    \/\/ FIXME -- for smaller code, the inline allocation (and the slow case) should be moved inside the pack handler.\n+    if (vk != NULL) {\n+      \/\/ Called from C1, where the return type is statically known.\n+      movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+      jint lh = vk->layout_helper();\n+      assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+      movl(r14, lh);\n+    } else {\n+      \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+      mov(rbx, rax);\n+      andptr(rbx, -2);\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+    }\n+\n+    movptr(r13, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));\n+    lea(r14, Address(r13, r14, Address::times_1));\n+    cmpptr(r14, Address(r15_thread, in_bytes(JavaThread::tlab_end_offset())));\n+    jcc(Assembler::above, slow_case);\n+    movptr(Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())), r14);\n+    movptr(Address(r13, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::always_locked_prototype().value());\n+\n+    xorl(rax, rax); \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(r13, rax);  \/\/ zero klass gap for compressed oops\n+\n+    if (vk == NULL) {\n+      \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n+      mov(rax, rbx);\n+    }\n+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    store_klass(r13, rbx, tmp_store_klass);  \/\/ klass\n+\n+    \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+    if (vk != NULL) {\n+      \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+      mov(rax, r13);\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      mov(rax, r13);\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must bevalid\");\n+  Register fromReg;\n+  if (from->is_reg()) {\n+    fromReg = from->as_Register();\n+  } else {\n+    int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+    movq(r10, Address(rsp, st_off));\n+    fromReg = r10;\n+  }\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  while (stream.next(toReg, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+     if (idx != from->value()) {\n+       mark_done = false;\n+     }\n+     done = false;\n+     continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    } else {\n+      assert(reg_state[idx] == reg_writable, \"must be writable\");\n+      reg_state[idx] = reg_written;\n+    }\n+\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? r13 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  Register val_array = rax;\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14; \/\/ Be careful with r14 because it's used for spilling\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  while (stream.next(fromReg, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+    reg_state[fromReg->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    movq(rbp, Address(rsp, initial_framesize));\n+    addq(rsp, Address(rsp, sp_inc_offset));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only) {\n@@ -4862,1 +5513,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"tmp register must be eax for rep stos\");\n@@ -4869,4 +5520,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n-\n@@ -4885,1 +5532,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -4894,1 +5541,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -4898,2 +5545,1 @@\n-    movptr(tmp, base);\n-    xmm_clear_mem(tmp, cnt, xtmp);\n+    xmm_clear_mem(base, cnt, val, xtmp);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":674,"deletions":28,"binary":false,"changes":702,"status":"modified"},{"patch":"@@ -31,0 +31,3 @@\n+#include \"runtime\/signature.hpp\"\n+\n+class ciInlineKlass;\n@@ -101,0 +104,27 @@\n+  \/\/ valueKlass queries, kills temp_reg\n+  void test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type);\n+  void test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type);\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type);\n+\n+  \/\/ Get the default value oop for the given InlineKlass\n+  void get_default_value_oop(Register inline_klass, Register temp_reg, Register obj);\n+  \/\/ The empty value oop, for the given InlineKlass (\"empty\" as in no instance fields)\n+  \/\/ get_default_value_oop with extra assertion for empty inline klass\n+  void get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj);\n+\n+  void test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline);\n+  void test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline);\n+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined);\n+\n+  \/\/ Check oops array storage properties, i.e. flattened and\/or null-free\n+  void test_flattened_array_oop(Register oop, Register temp_reg, Label&is_flattened_array);\n+  void test_non_flattened_array_oop(Register oop, Register temp_reg, Label&is_non_flattened_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flatten or null-free arrays...\n+  void test_flattened_array_layout(Register lh, Label& is_flattened_array);\n+  void test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array);\n+  void test_null_free_array_layout(Register lh, Label& is_null_free_array);\n+  void test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array);\n+\n@@ -318,0 +348,1 @@\n+  void load_metadata(Register dst, Register src);\n@@ -324,1 +355,11 @@\n-                       Register tmp1, Register tmp2);\n+                       Register tmp1, Register tmp2, Register tmp3 = noreg);\n+\n+  void access_value_copy(DecoratorSet decorators, Register src, Register dst, Register inline_klass);\n+\n+  \/\/ inline type data payload offsets...\n+  void first_field_offset(Register inline_klass, Register offset);\n+  void data_for_oop(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -335,1 +376,1 @@\n-                      Register tmp2 = noreg, DecoratorSet decorators = 0);\n+                      Register tmp2 = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);\n@@ -511,0 +552,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -530,0 +580,3 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n@@ -691,1 +744,2 @@\n-  void andptr(Register src1, Register src2) { LP64_ONLY(andq(src1, src2)) NOT_LP64(andl(src1, src2)) ; }\n+  void andptr(Register dst, Register src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n+  void andptr(Register dst, Address src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n@@ -1594,1 +1648,15 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(Compile* C, int sp_inc = 0);\n+\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[]);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset);\n+  VMReg spill_reg_for(VMReg reg);\n@@ -1598,1 +1666,1 @@\n-  void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large);\n+  void clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only);\n@@ -1601,1 +1669,1 @@\n-  void xmm_clear_mem(Register base, Register cnt, XMMRegister xtmp);\n+  void xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":74,"deletions":6,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -470,0 +470,1 @@\n+    case T_INLINE_TYPE:\n@@ -520,0 +521,9 @@\n+const uint SharedRuntime::java_return_convention_max_int = 1;\n+const uint SharedRuntime::java_return_convention_max_float = 1;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  Unimplemented();\n+  return 0;\n+}\n+\n@@ -581,3 +591,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>& sig_extended,\n@@ -585,1 +593,5 @@\n-                            Label& skip_fixup) {\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet*& oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words) {\n@@ -607,1 +619,1 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+  int extraspace = sig_extended.length() * Interpreter::stackElementSize;\n@@ -618,3 +630,3 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+  for (int i = 0; i < sig_extended.length(); i++) {\n+    if (sig_extended.at(i)._bt == T_VOID) {\n+      assert(i > 0 && (sig_extended.at(i-1)._bt == T_LONG || sig_extended.at(i-1)._bt == T_DOUBLE), \"missing half\");\n@@ -625,1 +637,1 @@\n-    int st_off = ((total_args_passed - 1) - i) * Interpreter::stackElementSize;\n+    int st_off = ((sig_extended.length() - 1) - i) * Interpreter::stackElementSize;\n@@ -675,1 +687,1 @@\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n+        if (sig_extended.at(i)._bt == T_LONG || sig_extended.at(i)._bt == T_DOUBLE) {\n@@ -692,1 +704,1 @@\n-        assert(sig_bt[i] == T_DOUBLE || sig_bt[i] == T_LONG, \"wrong type\");\n+        assert(sig_extended.at(i)._bt == T_DOUBLE || sig_extended.at(i)._bt == T_LONG, \"wrong type\");\n@@ -725,2 +737,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>& sig_extended,\n@@ -729,0 +740,1 @@\n+\n@@ -817,2 +829,2 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n+  for (int i = 0; i < sig_extended.length(); i++) {\n+    if (sig_extended.at(i)._bt == T_VOID) {\n@@ -821,1 +833,1 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      assert(i > 0 && (sig_extended.at(i-1)._bt == T_LONG || sig_extended.at(i-1)._bt == T_DOUBLE), \"missing half\");\n@@ -830,1 +842,1 @@\n-    int ld_off = (total_args_passed - i) * Interpreter::stackElementSize;\n+    int ld_off = (sig_extended.length() - i) * Interpreter::stackElementSize;\n@@ -871,1 +883,1 @@\n-        const int offset = (NOT_LP64(true ||) sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (NOT_LP64(true ||) sig_extended.at(i)._bt==T_LONG||sig_extended.at(i)._bt==T_DOUBLE)?\n@@ -889,1 +901,1 @@\n-        const int offset = (NOT_LP64(true ||) sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (NOT_LP64(true ||) sig_extended.at(i)._bt==T_LONG||sig_extended.at(i)._bt==T_DOUBLE)?\n@@ -937,2 +949,1 @@\n-                                                            int total_args_passed,\n-                                                            const BasicType *sig_bt,\n+                                                            const GrowableArray<SigEntry>& sig_extended,\n@@ -941,1 +952,2 @@\n-                                                            AdapterFingerPrint* fingerprint) {\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n@@ -944,1 +956,1 @@\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig_extended, regs);\n@@ -984,1 +996,4 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  OopMapSet* oop_maps = NULL;\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+  gen_c2i_adapter(masm, sig_extended, regs, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words);\n@@ -987,0 +1002,1 @@\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps);\n@@ -1010,0 +1026,1 @@\n+    case T_INLINE_TYPE:\n@@ -1291,0 +1308,1 @@\n+        case T_INLINE_TYPE:\n@@ -2008,0 +2026,1 @@\n+      case T_INLINE_TYPE:\n@@ -2190,0 +2209,1 @@\n+  case T_INLINE_TYPE:           \/\/ Really a handle\n@@ -3308,0 +3328,5 @@\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  Unimplemented();\n+  return NULL;\n+}\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":49,"deletions":24,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -497,0 +498,1 @@\n+    case T_INLINE_TYPE:\n@@ -530,0 +532,82 @@\n+\/\/ Same as java_calling_convention() but for multiple return\n+\/\/ values. There's no way to store them on the stack so if we don't\n+\/\/ have enough registers, multiple values can't be returned.\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    rax, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+  static const XMMRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3,\n+    j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_INLINE_TYPE:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+    case T_METADATA:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -572,0 +656,106 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+    for (int i = 0; i < sig_extended->length(); i++) {\n+      BasicType bt = sig_extended->at(i)._bt;\n+      if (bt == T_INLINE_TYPE) {\n+        \/\/ In sig_extended, an inline type argument starts with:\n+        \/\/ T_INLINE_TYPE, followed by the types of the fields of the\n+        \/\/ inline type and T_VOID to mark the end of the value\n+        \/\/ type. Inline types are flattened so, for instance, in the\n+        \/\/ case of an inline type with an int field and an inline type\n+        \/\/ field that itself has 2 fields, an int and a long:\n+        \/\/ T_INLINE_TYPE T_INT T_INLINE_TYPE T_INT T_LONG T_VOID (second\n+        \/\/ slot for the T_LONG) T_VOID (inner T_INLINE_TYPE) T_VOID\n+        \/\/ (outer T_INLINE_TYPE)\n+        total_args_passed++;\n+        int vt = 1;\n+        do {\n+          i++;\n+          BasicType bt = sig_extended->at(i)._bt;\n+          BasicType prev_bt = sig_extended->at(i-1)._bt;\n+          if (bt == T_INLINE_TYPE) {\n+            vt++;\n+          } else if (bt == T_VOID &&\n+                     prev_bt != T_LONG &&\n+                     prev_bt != T_DOUBLE) {\n+            vt--;\n+          }\n+        } while (vt != 0);\n+      } else {\n+        total_args_passed++;\n+      }\n+    }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  assert(bt != T_INLINE_TYPE || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"must be invalid\");\n+    return;\n+  }\n+\n+  if (!r_1->is_XMMRegister()) {\n+    Register val = rax;\n+    if (r_1->is_stack()) {\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, rscratch1);\n+    if (is_oop) {\n+      __ push(r13);\n+      __ push(rbx);\n+      __ store_heap_oop(to, val, rscratch1, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      __ pop(rbx);\n+      __ pop(r13);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    }\n+  }\n+}\n@@ -574,3 +764,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -578,1 +766,6 @@\n-                            Label& skip_fixup) {\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n@@ -588,0 +781,42 @@\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_INLINE_TYPE);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types.\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);\n+\n+      frame_complete = __ offset();\n+\n+      __ set_last_Java_frame(noreg, noreg, NULL);\n+\n+      __ mov(c_rarg0, r15_thread);\n+      __ mov(c_rarg1, rbx);\n+      __ mov64(c_rarg2, (int64_t)alloc_inline_receiver);\n+      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+      __ jcc(Assembler::equal, no_exception);\n+\n+      __ movptr(Address(r15_thread, JavaThread::vm_result_offset()), (int)NULL_WORD);\n+      __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+      __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(rscratch2, r15_thread); \/\/ Use rscratch2 (r11) as temporary because rscratch1 (r10) is trashed by movptr()\n+      __ get_vm_result_2(rbx, r15_thread); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n@@ -592,1 +827,1 @@\n-\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n@@ -610,46 +845,24 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n-\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n-    }\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rax\n-      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ movl(rax, Address(rsp, ld_off));\n-        __ movptr(Address(rsp, st_off), rax);\n-\n-      } else {\n-\n-        __ movq(rax, Address(rsp, ld_off));\n-\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ movq(Address(rsp, next_off), rax);\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_INLINE_TYPE,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_INLINE_TYPE\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_INLINE_TYPE) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(rsp, offset), extraspace, false);\n+      next_arg_int++;\n@@ -658,7 +871,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n-          __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ movq(Address(rsp, st_off), rax);\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n+        __ movptr(Address(rsp, st_off), rax);\n@@ -666,16 +876,25 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ movl(Address(rsp, st_off), r);\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ long\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaab));\n-          __ movptr(Address(rsp, st_off), rax);\n-          __ movq(Address(rsp, next_off), r);\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_INLINE_TYPE);\n+      __ load_heap_oop(r14, Address(rscratch2, index));\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp-1)._bt;\n+        if (bt == T_INLINE_TYPE) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID &&\n+                   prev_bt != T_LONG &&\n+                   prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -684,1 +903,6 @@\n-          __ movptr(Address(rsp, st_off), r);\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(r14, off), extraspace, is_oop);\n@@ -686,14 +910,3 @@\n-      }\n-    } else {\n-      assert(r_1->is_XMMRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ movflt(Address(rsp, st_off), r_1->as_XMMRegister());\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov64(rax, CONST64(0xdeadffffdeadaaac));\n-        __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        __ movdbl(Address(rsp, next_off), r_1->as_XMMRegister());\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ movptr(Address(rsp, st_off), r14);\n@@ -722,2 +935,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>* sig,\n@@ -816,1 +1028,1 @@\n-  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_offset())));\n+  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_inline_offset())));\n@@ -830,0 +1042,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -833,1 +1047,3 @@\n-    if (sig_bt[i] == T_VOID) {\n+    BasicType bt = sig->at(i)._bt;\n+    assert(bt != T_INLINE_TYPE, \"i2c adapter doesn't unpack inline type args\");\n+    if (bt == T_VOID) {\n@@ -836,1 +1052,2 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      BasicType prev_bt = (i > 0) ? sig->at(i-1)._bt : T_ILLEGAL;\n+      assert(i > 0 && (prev_bt == T_LONG || prev_bt == T_DOUBLE), \"missing half\");\n@@ -878,1 +1095,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -893,1 +1110,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -924,1 +1141,1 @@\n-  \/\/ only needed becaus eof c2 resolve stubs return Method* as a result in\n+  \/\/ only needed because of c2 resolve stubs return Method* as a result in\n@@ -930,0 +1147,22 @@\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Label ok;\n+\n+  Register holder = rax;\n+  Register receiver = j_rarg0;\n+  Register temp = rbx;\n+\n+  __ load_klass(temp, receiver, rscratch1);\n+  __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n+  __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n+  __ jcc(Assembler::equal, ok);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+\n+  __ bind(ok);\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);\n+  __ jcc(Assembler::equal, skip_fixup);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n+\n@@ -932,4 +1171,8 @@\n-                                                            int total_args_passed,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n@@ -938,2 +1181,1 @@\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -952,11 +1194,1 @@\n-  Label ok;\n-  Register holder = rax;\n-  Register receiver = j_rarg0;\n-  Register temp = rbx;\n-\n-  {\n-    __ load_klass(temp, receiver, rscratch1);\n-    __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ jcc(Assembler::equal, ok);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -965,7 +1197,9 @@\n-    __ bind(ok);\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);\n-    __ jcc(Assembler::equal, skip_fixup);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+    skip_fixup.reset();\n@@ -974,0 +1208,1 @@\n+  \/\/ Scalarized c2i adapter\n@@ -1002,1 +1237,14 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, true);\n+\n+  address c2i_unverified_inline_entry = c2i_unverified_entry;\n+\n+  \/\/ Non-scalarized c2i adapter\n+  address c2i_inline_entry = c2i_entry;\n+  if (regs != regs_cc) {\n+    Label inline_entry_skip_fixup;\n+    c2i_unverified_inline_entry = __ pc();\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+  }\n@@ -1005,1 +1253,7 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+\n+  \/\/ The c2i adapters might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  bool caller_must_gc_arguments = (regs != regs_cc);\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n+\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -1063,0 +1317,1 @@\n+      case T_INLINE_TYPE:\n@@ -1410,1 +1665,1 @@\n-          map->set_oop(VMRegImpl::stack2reg(slot));;\n+          map->set_oop(VMRegImpl::stack2reg(slot));\n@@ -1444,0 +1699,1 @@\n+        case T_INLINE_TYPE:\n@@ -2357,0 +2613,1 @@\n+      case T_INLINE_TYPE:\n@@ -2492,0 +2749,4 @@\n+    if (EnableValhalla && !UseBiasedLocking) {\n+      \/\/ For slow path is_always_locked, using biased, which is never natural for !UseBiasLocking\n+      __ andptr(swap_reg, ~((int) markWord::biased_lock_bit_in_place));\n+    }\n@@ -2553,0 +2814,1 @@\n+  case T_INLINE_TYPE:           \/\/ Really a handle\n@@ -4071,0 +4333,111 @@\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  __ movptr(rax, Address(r13, 0));\n+  __ resolve_jobject(rax \/* value *\/,\n+                     r15_thread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ movptr(Address(r13, 0), rax);\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r14, r13, rbx, rscratch1);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r14, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(0);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(rax, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(rax, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  if (StressInlineTypeReturnedAsFields) {\n+    __ load_klass(rax, rax, rscratch1);\n+    __ orptr(rax, 1);\n+  }\n+\n+  __ ret(0);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":497,"deletions":124,"binary":false,"changes":621,"status":"modified"},{"patch":"@@ -339,5 +339,5 @@\n-    \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n-    __ movptr(c_rarg0, result);\n-    Label is_long, is_float, is_double, exit;\n-    __ movl(c_rarg1, result_type);\n-    __ cmpl(c_rarg1, T_OBJECT);\n+    \/\/ T_OBJECT, T_INLINE_TYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+    __ movptr(r13, result);\n+    Label is_long, is_float, is_double, is_value, exit;\n+    __ movl(rbx, result_type);\n+    __ cmpl(rbx, T_OBJECT);\n@@ -345,1 +345,3 @@\n-    __ cmpl(c_rarg1, T_LONG);\n+    __ cmpl(rbx, T_INLINE_TYPE);\n+    __ jcc(Assembler::equal, is_value);\n+    __ cmpl(rbx, T_LONG);\n@@ -347,1 +349,1 @@\n-    __ cmpl(c_rarg1, T_FLOAT);\n+    __ cmpl(rbx, T_FLOAT);\n@@ -349,1 +351,1 @@\n-    __ cmpl(c_rarg1, T_DOUBLE);\n+    __ cmpl(rbx, T_DOUBLE);\n@@ -353,1 +355,1 @@\n-    __ movl(Address(c_rarg0, 0), rax);\n+    __ movl(Address(r13, 0), rax);\n@@ -415,0 +417,13 @@\n+    __ BIND(is_value);\n+    if (InlineTypeReturnedAsFields) {\n+      \/\/ Check for flattened return value\n+      __ testptr(rax, 1);\n+      __ jcc(Assembler::zero, is_long);\n+      \/\/ Load pack handler address\n+      __ andptr(rax, -2);\n+      __ movptr(rax, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ movptr(rbx, Address(rax, InlineKlass::pack_handler_jobject_offset()));\n+      \/\/ Call pack handler to initialize the buffer\n+      __ call(rbx);\n+      __ jmp(exit);\n+    }\n@@ -416,1 +431,1 @@\n-    __ movq(Address(c_rarg0, 0), rax);\n+    __ movq(Address(r13, 0), rax);\n@@ -420,1 +435,1 @@\n-    __ movflt(Address(c_rarg0, 0), xmm0);\n+    __ movflt(Address(r13, 0), xmm0);\n@@ -424,1 +439,1 @@\n-    __ movdbl(Address(c_rarg0, 0), xmm0);\n+    __ movdbl(Address(r13, 0), xmm0);\n@@ -2777,1 +2792,1 @@\n-    __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, AS_RAW);  \/\/ store the oop\n+    __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n@@ -3072,0 +3087,8 @@\n+    \/\/ Check for flat inline type array -> return -1\n+    __ testl(rax_lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+    __ jcc(Assembler::notZero, L_failed);\n+\n+    \/\/ Check for null-free (non-flat) inline type array -> handle as object array\n+    __ testl(rax_lh, Klass::_lh_null_free_bit_inplace);\n+    __ jcc(Assembler::notZero, L_objArray);\n+\n@@ -3081,2 +3104,4 @@\n-      __ cmpl(rax_lh, (Klass::_lh_array_tag_type_value << Klass::_lh_array_tag_shift));\n-      __ jcc(Assembler::greaterEqual, L);\n+      __ movl(rklass_tmp, rax_lh);\n+      __ sarl(rklass_tmp, Klass::_lh_array_tag_shift);\n+      __ cmpl(rklass_tmp, Klass::_lh_array_tag_type_value);\n+      __ jcc(Assembler::equal, L);\n@@ -3190,0 +3215,1 @@\n+      \/\/ This check also fails for flat\/null-free arrays which are not supported.\n@@ -3193,0 +3219,13 @@\n+#ifdef ASSERT\n+      {\n+        BLOCK_COMMENT(\"assert not null-free array {\");\n+        Label L;\n+        __ movl(rklass_tmp, Address(rax, lh_offset));\n+        __ testl(rklass_tmp, Klass::_lh_null_free_bit_inplace);\n+        __ jcc(Assembler::zero, L);\n+        __ stop(\"unexpected null-free array\");\n+        __ bind(L);\n+        BLOCK_COMMENT(\"} assert not null-free array\");\n+      }\n+#endif\n+\n@@ -6629,0 +6668,140 @@\n+  \/\/ Call here from the interpreter or compiled code to either load\n+  \/\/ multiple returned values from the inline type instance being\n+  \/\/ returned to registers or to store returned values to a newly\n+  \/\/ allocated inline type instance.\n+  address generate_return_value_stub(address destination, const char* name, bool has_res) {\n+    \/\/ We need to save all registers the calling convention may use so\n+    \/\/ the runtime calls read or update those registers. This needs to\n+    \/\/ be in sync with SharedRuntime::java_return_convention().\n+    enum layout {\n+      pad_off = frame::arg_reg_save_area_bytes\/BytesPerInt, pad_off_2,\n+      rax_off, rax_off_2,\n+      j_rarg5_off, j_rarg5_2,\n+      j_rarg4_off, j_rarg4_2,\n+      j_rarg3_off, j_rarg3_2,\n+      j_rarg2_off, j_rarg2_2,\n+      j_rarg1_off, j_rarg1_2,\n+      j_rarg0_off, j_rarg0_2,\n+      j_farg0_off, j_farg0_2,\n+      j_farg1_off, j_farg1_2,\n+      j_farg2_off, j_farg2_2,\n+      j_farg3_off, j_farg3_2,\n+      j_farg4_off, j_farg4_2,\n+      j_farg5_off, j_farg5_2,\n+      j_farg6_off, j_farg6_2,\n+      j_farg7_off, j_farg7_2,\n+      rbp_off, rbp_off_2,\n+      return_off, return_off_2,\n+\n+      framesize\n+    };\n+\n+    CodeBuffer buffer(name, 1000, 512);\n+    MacroAssembler* masm = new MacroAssembler(&buffer);\n+\n+    int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+    assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+    int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+    int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+    OopMapSet *oop_maps = new OopMapSet();\n+    OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(rax_off), rax->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+    int start = __ offset();\n+\n+    __ subptr(rsp, frame_size_in_bytes - 8 \/* return address*\/);\n+\n+    __ movptr(Address(rsp, rbp_off * BytesPerInt), rbp);\n+    __ movdbl(Address(rsp, j_farg7_off * BytesPerInt), j_farg7);\n+    __ movdbl(Address(rsp, j_farg6_off * BytesPerInt), j_farg6);\n+    __ movdbl(Address(rsp, j_farg5_off * BytesPerInt), j_farg5);\n+    __ movdbl(Address(rsp, j_farg4_off * BytesPerInt), j_farg4);\n+    __ movdbl(Address(rsp, j_farg3_off * BytesPerInt), j_farg3);\n+    __ movdbl(Address(rsp, j_farg2_off * BytesPerInt), j_farg2);\n+    __ movdbl(Address(rsp, j_farg1_off * BytesPerInt), j_farg1);\n+    __ movdbl(Address(rsp, j_farg0_off * BytesPerInt), j_farg0);\n+\n+    __ movptr(Address(rsp, j_rarg0_off * BytesPerInt), j_rarg0);\n+    __ movptr(Address(rsp, j_rarg1_off * BytesPerInt), j_rarg1);\n+    __ movptr(Address(rsp, j_rarg2_off * BytesPerInt), j_rarg2);\n+    __ movptr(Address(rsp, j_rarg3_off * BytesPerInt), j_rarg3);\n+    __ movptr(Address(rsp, j_rarg4_off * BytesPerInt), j_rarg4);\n+    __ movptr(Address(rsp, j_rarg5_off * BytesPerInt), j_rarg5);\n+    __ movptr(Address(rsp, rax_off * BytesPerInt), rax);\n+\n+    int frame_complete = __ offset();\n+\n+    __ set_last_Java_frame(noreg, noreg, NULL);\n+\n+    __ mov(c_rarg0, r15_thread);\n+    __ mov(c_rarg1, rax);\n+\n+    __ call(RuntimeAddress(destination));\n+\n+    \/\/ Set an oopmap for the call site.\n+\n+    oop_maps->add_gc_map( __ offset() - start, map);\n+\n+    \/\/ clear last_Java_sp\n+    __ reset_last_Java_frame(false);\n+\n+    __ movptr(rbp, Address(rsp, rbp_off * BytesPerInt));\n+    __ movdbl(j_farg7, Address(rsp, j_farg7_off * BytesPerInt));\n+    __ movdbl(j_farg6, Address(rsp, j_farg6_off * BytesPerInt));\n+    __ movdbl(j_farg5, Address(rsp, j_farg5_off * BytesPerInt));\n+    __ movdbl(j_farg4, Address(rsp, j_farg4_off * BytesPerInt));\n+    __ movdbl(j_farg3, Address(rsp, j_farg3_off * BytesPerInt));\n+    __ movdbl(j_farg2, Address(rsp, j_farg2_off * BytesPerInt));\n+    __ movdbl(j_farg1, Address(rsp, j_farg1_off * BytesPerInt));\n+    __ movdbl(j_farg0, Address(rsp, j_farg0_off * BytesPerInt));\n+\n+    __ movptr(j_rarg0, Address(rsp, j_rarg0_off * BytesPerInt));\n+    __ movptr(j_rarg1, Address(rsp, j_rarg1_off * BytesPerInt));\n+    __ movptr(j_rarg2, Address(rsp, j_rarg2_off * BytesPerInt));\n+    __ movptr(j_rarg3, Address(rsp, j_rarg3_off * BytesPerInt));\n+    __ movptr(j_rarg4, Address(rsp, j_rarg4_off * BytesPerInt));\n+    __ movptr(j_rarg5, Address(rsp, j_rarg5_off * BytesPerInt));\n+    __ movptr(rax, Address(rsp, rax_off * BytesPerInt));\n+\n+    __ addptr(rsp, frame_size_in_bytes-8);\n+\n+    \/\/ check for pending exceptions\n+    Label pending;\n+    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+    __ jcc(Assembler::notEqual, pending);\n+\n+    if (has_res) {\n+      __ get_vm_result(rax, r15_thread);\n+    }\n+\n+    __ ret(0);\n+\n+    __ bind(pending);\n+\n+    __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+    \/\/ -------------\n+    \/\/ make sure all code is generated\n+    masm->flush();\n+\n+    RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_in_words, oop_maps, false);\n+    return stub->entry_point();\n+  }\n+\n@@ -6644,2 +6823,5 @@\n-    StubRoutines::_call_stub_entry =\n-      generate_call_stub(StubRoutines::_call_stub_return_address);\n+    \/\/ Generate these first because they are called from other stubs\n+    StubRoutines::_load_inline_type_fields_in_regs = generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf = generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), \"store_inline_type_fields_to_buf\", true);\n+\n+    StubRoutines::_call_stub_entry = generate_call_stub(StubRoutines::_call_stub_return_address);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":199,"deletions":17,"binary":false,"changes":216,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -60,1 +61,1 @@\n-int TemplateInterpreter::InterpreterCodeSize = JVMCI_ONLY(268) NOT_JVMCI(256) * 1024;\n+int TemplateInterpreter::InterpreterCodeSize = JVMCI_ONLY(280) NOT_JVMCI(268) * 1024;\n@@ -208,0 +209,4 @@\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(NULL);\n+  }\n+\n@@ -350,0 +355,1 @@\n+  case T_INLINE_TYPE: \/\/ fall through (inline types are handled with oops)\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -152,1 +153,1 @@\n-  __ store_heap_oop(dst, val, rdx, rbx, decorators);\n+  __ store_heap_oop(dst, val, rdx, rbx, noreg, decorators);\n@@ -175,0 +176,1 @@\n+  case Bytecodes::_fast_qputfield:\n@@ -367,0 +369,1 @@\n+  __ andl(rdx, ~JVM_CONSTANT_QDescBit);\n@@ -818,9 +821,27 @@\n-  \/\/ rax: index\n-  \/\/ rdx: array\n-  index_check(rdx, rax); \/\/ kills rbx\n-  do_oop_load(_masm,\n-              Address(rdx, rax,\n-                      UseCompressedOops ? Address::times_4 : Address::times_ptr,\n-                      arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n-              rax,\n-              IS_ARRAY);\n+  Register array = rdx;\n+  Register index = rax;\n+\n+  index_check(array, index); \/\/ kills rbx\n+  __ profile_array(rbx, array, rcx);\n+  if (UseFlatArray) {\n+    Label is_flat_array, done;\n+    __ test_flattened_array_oop(array, rbx, is_flat_array);\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+    __ jmp(done);\n+    __ bind(is_flat_array);\n+    __ read_flattened_element(array, index, rbx, rcx, rax);\n+    __ bind(done);\n+  } else {\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+  }\n+  __ profile_element(rbx, rax, rcx);\n@@ -1112,1 +1133,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1124,0 +1145,4 @@\n+\n+  __ profile_array(rdi, rdx, rbx);\n+  __ profile_element(rdi, rax, rbx);\n+\n@@ -1127,0 +1152,1 @@\n+  \/\/ Move array class to rdi\n@@ -1128,0 +1154,6 @@\n+  __ load_klass(rdi, rdx, tmp_load_klass);\n+  if (UseFlatArray) {\n+    __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+    __ test_flattened_array_layout(rbx, is_flat_array);\n+  }\n+\n@@ -1130,3 +1162,2 @@\n-  \/\/ Move superklass into rax\n-  __ load_klass(rax, rdx, tmp_load_klass);\n-  __ movptr(rax, Address(rax,\n+  \/\/ Move array element superklass into rax\n+  __ movptr(rax, Address(rdi,\n@@ -1137,1 +1168,2 @@\n-  __ gen_subtype_check(rbx, ok_is_subtype);\n+  \/\/ is \"rbx <: rax\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(rbx, ok_is_subtype, false);\n@@ -1155,1 +1187,2 @@\n-  __ profile_null_seen(rbx);\n+  if (EnableValhalla) {\n+    Label is_null_into_value_array_npe, store_null;\n@@ -1157,0 +1190,9 @@\n+    \/\/ No way to store null in null-free array\n+    __ test_null_free_array_oop(rdx, rbx, is_null_into_value_array_npe);\n+    __ jmp(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ jump(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n@@ -1159,0 +1201,7 @@\n+  __ jmp(done);\n+\n+  if (EnableValhalla) {\n+    Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n+\n+    \/\/ Simplistic type check...\n@@ -1160,0 +1209,27 @@\n+    \/\/ Profile the not-null value's klass.\n+    __ load_klass(rbx, rax, tmp_load_klass);\n+    \/\/ Move element klass into rax\n+    __ movptr(rax, Address(rdi, ArrayKlass::element_klass_offset()));\n+    \/\/ flat value array needs exact type match\n+    \/\/ is \"rax == rbx\" (value subclass == array element superclass)\n+    __ cmpptr(rax, rbx);\n+    __ jccb(Assembler::equal, is_type_ok);\n+\n+    __ jump(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));\n+\n+    __ bind(is_type_ok);\n+    \/\/ rbx: value's klass\n+    \/\/ rdx: array\n+    \/\/ rdi: array klass\n+    __ test_klass_is_empty_inline_type(rbx, rax, done);\n+\n+    \/\/ calc dst for copy\n+    __ movl(rax, at_tos_p1()); \/\/ index\n+    __ data_for_value_array_index(rdx, rdi, rax, rax);\n+\n+    \/\/ ...and src for copy\n+    __ movptr(rcx, at_tos());  \/\/ value\n+    __ data_for_oop(rcx, rcx, rbx);\n+\n+    __ access_value_copy(IN_HEAP, rcx, rax, rbx);\n+  }\n@@ -2406,1 +2482,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -2408,0 +2484,34 @@\n+\n+  __ profile_acmp(rbx, rdx, rax, rcx);\n+\n+  const int is_inline_type_mask = markWord::always_locked_pattern;\n+  if (EnableValhalla) {\n+    __ cmpoop(rdx, rax);\n+    __ jcc(Assembler::equal, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either rax or rdx is null\n+    __ testptr(rdx, rax);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ movptr(rbx, Address(rdx, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, is_inline_type_mask);\n+    __ cmpptr(rbx, is_inline_type_mask);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(rbx, rdx);\n+    __ load_metadata(rcx, rax);\n+    __ cmpptr(rbx, rcx);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(rax, rdx, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(rax, rdx, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -2410,0 +2520,1 @@\n+  __ bind(taken);\n@@ -2412,1 +2523,10 @@\n-  __ profile_not_taken_branch(rax);\n+  __ profile_not_taken_branch(rax, true);\n+}\n+\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored...rax answer, jmp to outcome...\n+  __ testl(rax, rax);\n+  __ jcc(Assembler::zero, not_subst);\n+  __ jmp(is_subst);\n@@ -2681,1 +2801,2 @@\n-  __ remove_activation(state, rbcp);\n+\n+  __ remove_activation(state, rbcp, true, true, true);\n@@ -2879,0 +3000,1 @@\n+  const Register flags2 = rdx;\n@@ -2884,2 +3006,0 @@\n-  if (!is_static) pop_and_check_object(obj);\n-\n@@ -2888,1 +3008,9 @@\n-  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj;\n+  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj, notInlineType;\n+\n+  if (!is_static) {\n+    __ movptr(rcx, Address(cache, index, Address::times_ptr,\n+                           in_bytes(ConstantPoolCache::base_offset() +\n+                                    ConstantPoolCacheEntry::f1_offset())));\n+  }\n+\n+  __ movl(flags2, flags);\n@@ -2898,0 +3026,1 @@\n+  if (!is_static) pop_and_check_object(obj);\n@@ -2907,0 +3036,1 @@\n+\n@@ -2909,1 +3039,1 @@\n-\n+   if (!is_static) pop_and_check_object(obj);\n@@ -2924,4 +3054,82 @@\n-  do_oop_load(_masm, field, rax);\n-  __ push(atos);\n-  if (!is_static && rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+  if (!EnableValhalla) {\n+    if (!is_static) pop_and_check_object(obj);\n+    do_oop_load(_masm, field, rax);\n+    __ push(atos);\n+    if (!is_static && rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+    }\n+    __ jmp(Done);\n+  } else {\n+    if (is_static) {\n+      __ load_heap_oop(rax, field);\n+      Label is_inline_type, uninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);\n+        \/\/ field is not an inline type\n+        __ push(atos);\n+        __ jmp(Done);\n+      \/\/ field is an inline type, must not return null even if uninitialized\n+      __ bind(is_inline_type);\n+        __ testptr(rax, rax);\n+        __ jcc(Assembler::zero, uninitialized);\n+          __ push(atos);\n+          __ jmp(Done);\n+        __ bind(uninitialized);\n+          __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+#ifdef _LP64\n+          Label slow_case, finish;\n+          __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+          __ jcc(Assembler::notEqual, slow_case);\n+        __ get_default_value_oop(rcx, off, rax);\n+        __ jmp(finish);\n+        __ bind(slow_case);\n+#endif \/\/ LP64\n+          __ call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field),\n+                 obj, flags2);\n+#ifdef _LP64\n+          __ bind(finish);\n+#endif \/\/ _LP64\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(Done);\n+    } else {\n+      Label is_inlined, nonnull, is_inline_type, rewrite_inline;\n+      __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);\n+        \/\/ field is not an inline type\n+        pop_and_check_object(obj);\n+        __ load_heap_oop(rax, field);\n+        __ push(atos);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+        }\n+        __ jmp(Done);\n+      __ bind(is_inline_type);\n+        __ test_field_is_inlined(flags2, rscratch1, is_inlined);\n+          \/\/ field is not inlined\n+          __ movptr(rax, rcx);  \/\/ small dance required to preserve the klass_holder somewhere\n+          pop_and_check_object(obj);\n+          __ push(rax);\n+          __ load_heap_oop(rax, field);\n+          __ pop(rcx);\n+          __ testptr(rax, rax);\n+          __ jcc(Assembler::notZero, nonnull);\n+            __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+            __ get_inline_type_field_klass(rcx, flags2, rbx);\n+            __ get_default_value_oop(rbx, rcx, rax);\n+          __ bind(nonnull);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(rewrite_inline);\n+        __ bind(is_inlined);\n+        \/\/ field is inlined\n+          __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);\n+          pop_and_check_object(rax);\n+          __ read_inlined_field(rcx, flags2, rbx, rax);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_qgetfield, bc, rbx);\n+      }\n+      __ jmp(Done);\n+    }\n@@ -2929,1 +3137,0 @@\n-  __ jmp(Done);\n@@ -2932,0 +3139,3 @@\n+\n+  if (!is_static) pop_and_check_object(obj);\n+\n@@ -3031,0 +3241,15 @@\n+void TemplateTable::withfield() {\n+  transition(vtos, atos);\n+\n+  Register cache = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n+  Register index = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n+\n+  resolve_cache_and_index(f2_byte, cache, index, sizeof(u2));\n+\n+  call_VM(rbx, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), cache);\n+  \/\/ new value type is returned in rbx\n+  \/\/ stack adjustement is returned in rax\n+  __ verify_oop(rbx);\n+  __ addptr(rsp, rax);\n+  __ movptr(rax, rbx);\n+}\n@@ -3126,0 +3351,1 @@\n+  const Register flags2 = rdx;\n@@ -3142,0 +3368,1 @@\n+  __ movl(flags2, flags);\n@@ -3144,1 +3371,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags, flags2);\n@@ -3150,1 +3377,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags, flags2);\n@@ -3156,1 +3383,1 @@\n-                                              Register obj, Register off, Register flags) {\n+                                              Register obj, Register off, Register flags, Register flags2) {\n@@ -3163,1 +3390,1 @@\n-        notLong, notFloat, notObj;\n+        notLong, notFloat, notObj, notInlineType;\n@@ -3206,6 +3433,53 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, rax);\n-    if (!is_static && rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+    if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, rax);\n+      if (!is_static && rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+      }\n+      __ jmp(Done);\n+    } else {\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_inline_type;\n+        __ test_field_is_not_inline_type(flags2, rscratch1, is_inline_type);\n+        __ null_check(rax);\n+        __ bind(is_inline_type);\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(Done);\n+      } else {\n+        Label is_inline_type, is_inlined, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);\n+        \/\/ Not an inline type\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        __ bind(rewrite_not_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(is_inline_type);\n+        __ null_check(rax);\n+        __ test_field_is_inlined(flags2, rscratch1, is_inlined);\n+        \/\/ field is not inlined\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(rewrite_inline);\n+        __ bind(is_inlined);\n+        \/\/ field is inlined\n+        pop_and_check_object(obj);\n+        assert_different_registers(rax, rdx, obj, off);\n+        __ load_klass(rdx, rax, rscratch1);\n+        __ data_for_oop(rax, rax, rdx);\n+        __ addptr(obj, off);\n+        __ access_value_copy(IN_HEAP, rax, obj, rdx);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_qputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+      }\n@@ -3213,1 +3487,0 @@\n-    __ jmp(Done);\n@@ -3352,0 +3625,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -3377,0 +3651,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/ fall through\n@@ -3416,0 +3691,4 @@\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rscratch2, rdx);  \/\/ saving flags for is_inlined test\n+  }\n+\n@@ -3429,1 +3708,4 @@\n-  fast_storefield_helper(field, rax);\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rdx, rscratch2);  \/\/ restoring flags for is_inlined test\n+  }\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3435,1 +3717,4 @@\n-  fast_storefield_helper(field, rax);\n+  if (bytecode() == Bytecodes::_fast_qputfield) {\n+    __ movl(rdx, rscratch2);  \/\/ restoring flags for is_inlined test\n+  }\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3440,1 +3725,1 @@\n-void TemplateTable::fast_storefield_helper(Address field, Register rax) {\n+void TemplateTable::fast_storefield_helper(Address field, Register rax, Register flags) {\n@@ -3444,0 +3729,17 @@\n+  case Bytecodes::_fast_qputfield:\n+    {\n+      Label is_inlined, done;\n+      __ null_check(rax);\n+      __ test_field_is_inlined(flags, rscratch1, is_inlined);\n+      \/\/ field is not inlined\n+      do_oop_store(_masm, field, rax);\n+      __ jmp(done);\n+      __ bind(is_inlined);\n+      \/\/ field is inlined\n+      __ load_klass(rdx, rax, rscratch1);\n+      __ data_for_oop(rax, rax, rdx);\n+      __ lea(rcx, field);\n+      __ access_value_copy(IN_HEAP, rax, rcx, rdx);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3445,1 +3747,3 @@\n-    do_oop_store(_masm, field, rax);\n+    {\n+      do_oop_store(_masm, field, rax);\n+    }\n@@ -3515,1 +3819,1 @@\n-  __ movptr(rbx, Address(rcx, rbx, Address::times_ptr,\n+  __ movptr(rdx, Address(rcx, rbx, Address::times_ptr,\n@@ -3522,1 +3826,1 @@\n-  Address field(rax, rbx, Address::times_1);\n+  Address field(rax, rdx, Address::times_1);\n@@ -3526,0 +3830,39 @@\n+  case Bytecodes::_fast_qgetfield:\n+    {\n+      Label is_inlined, nonnull, Done;\n+      __ movptr(rscratch1, Address(rcx, rbx, Address::times_ptr,\n+                                   in_bytes(ConstantPoolCache::base_offset() +\n+                                            ConstantPoolCacheEntry::flags_offset())));\n+      __ test_field_is_inlined(rscratch1, rscratch2, is_inlined);\n+        \/\/ field is not inlined\n+        __ load_heap_oop(rax, field);\n+        __ testptr(rax, rax);\n+        __ jcc(Assembler::notZero, nonnull);\n+          __ movl(rdx, Address(rcx, rbx, Address::times_ptr,\n+                             in_bytes(ConstantPoolCache::base_offset() +\n+                                      ConstantPoolCacheEntry::flags_offset())));\n+          __ andl(rdx, ConstantPoolCacheEntry::field_index_mask);\n+          __ movptr(rcx, Address(rcx, rbx, Address::times_ptr,\n+                                       in_bytes(ConstantPoolCache::base_offset() +\n+                                                ConstantPoolCacheEntry::f1_offset())));\n+          __ get_inline_type_field_klass(rcx, rdx, rbx);\n+          __ get_default_value_oop(rbx, rcx, rax);\n+        __ bind(nonnull);\n+        __ verify_oop(rax);\n+        __ jmp(Done);\n+      __ bind(is_inlined);\n+      \/\/ field is inlined\n+        __ push(rdx); \/\/ save offset\n+        __ movl(rdx, Address(rcx, rbx, Address::times_ptr,\n+                           in_bytes(ConstantPoolCache::base_offset() +\n+                                    ConstantPoolCacheEntry::flags_offset())));\n+        __ andl(rdx, ConstantPoolCacheEntry::field_index_mask);\n+        __ movptr(rcx, Address(rcx, rbx, Address::times_ptr,\n+                                     in_bytes(ConstantPoolCache::base_offset() +\n+                                              ConstantPoolCacheEntry::f1_offset())));\n+        __ pop(rbx); \/\/ restore offset\n+        __ read_inlined_field(rcx, rdx, rbx, rax);\n+      __ bind(Done);\n+      __ verify_oop(rax);\n+    }\n+    break;\n@@ -3994,3 +4337,1 @@\n-  Label slow_case_no_pop;\n-  Label initialize_header;\n-  Label initialize_object;  \/\/ including clearing the fields\n+  Label is_not_value;\n@@ -4006,1 +4347,1 @@\n-  __ jcc(Assembler::notEqual, slow_case_no_pop);\n+  __ jcc(Assembler::notEqual, slow_case);\n@@ -4010,1 +4351,7 @@\n-  __ push(rcx);  \/\/ save the contexts of klass for initializing the header\n+\n+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InstanceKlass::_kind_inline_type);\n+  __ jcc(Assembler::notEqual, is_not_value);\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_InstantiationError));\n+\n+  __ bind(is_not_value);\n@@ -4013,1 +4360,0 @@\n-  \/\/ make sure klass is fully initialized\n@@ -4017,19 +4363,2 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ movl(rdx, Address(rcx, Klass::layout_helper_offset()));\n-  \/\/ test to see if it has a finalizer or is malformed in some way\n-  __ testl(rdx, Klass::_lh_instance_slow_path_bit);\n-  __ jcc(Assembler::notZero, slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/  Else If inline contiguous allocations are enabled:\n-  \/\/    Try to allocate in eden.\n-  \/\/    If fails due to heap end, go to slow path.\n-  \/\/\n-  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n+  __ allocate_instance(rcx, rax, rdx, rbx, true, slow_case);\n+  __ jmp(done);\n@@ -4037,2 +4366,2 @@\n-  const bool allow_shared_alloc =\n-    Universe::heap()->supports_inline_contig_alloc();\n+  \/\/ slow case\n+  __ bind(slow_case);\n@@ -4040,6 +4369,2 @@\n-  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n-#ifndef _LP64\n-  if (UseTLAB || allow_shared_alloc) {\n-    __ get_thread(thread);\n-  }\n-#endif \/\/ _LP64\n+  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n+  Register rarg2 = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n@@ -4047,15 +4372,4 @@\n-  if (UseTLAB) {\n-    __ tlab_allocate(thread, rax, rdx, 0, rcx, rbx, slow_case);\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ jmp(initialize_header);\n-    } else {\n-      \/\/ initialize both the header and fields\n-      __ jmp(initialize_object);\n-    }\n-  } else {\n-    \/\/ Allocation in the shared Eden, if allowed.\n-    \/\/\n-    \/\/ rdx: instance size in bytes\n-    __ eden_allocate(thread, rax, rdx, 0, rbx, slow_case);\n-  }\n+  __ get_constant_pool(rarg1);\n+  __ get_unsigned_2_byte_index_at_bcp(rarg2, 1);\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);\n+   __ verify_oop(rax);\n@@ -4063,24 +4377,3 @@\n-  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n-  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n-  if (UseTLAB || allow_shared_alloc) {\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    __ bind(initialize_object);\n-    __ decrement(rdx, sizeof(oopDesc));\n-    __ jcc(Assembler::zero, initialize_header);\n-\n-    \/\/ Initialize topmost object field, divide rdx by 8, check if odd and\n-    \/\/ test if zero.\n-    __ xorl(rcx, rcx);    \/\/ use zero reg to clear memory (shorter code)\n-    __ shrl(rdx, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n-\n-    \/\/ rdx must have been multiple of 8\n-#ifdef ASSERT\n-    \/\/ make sure rdx was multiple of 8\n-    Label L;\n-    \/\/ Ignore partial flag stall after shrl() since it is debug VM\n-    __ jcc(Assembler::carryClear, L);\n-    __ stop(\"object size is not multiple of 2 - adjust this code\");\n-    __ bind(L);\n-    \/\/ rdx must be > 0, no extra check needed here\n-#endif\n+  \/\/ continue\n+  __ bind(done);\n+}\n@@ -4088,8 +4381,2 @@\n-    \/\/ initialize remaining object fields: rdx was a multiple of 8\n-    { Label loop;\n-    __ bind(loop);\n-    __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n-    __ decrement(rdx);\n-    __ jcc(Assembler::notZero, loop);\n-    }\n+void TemplateTable::defaultvalue() {\n+  transition(vtos, atos);\n@@ -4097,17 +4384,3 @@\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    if (UseBiasedLocking) {\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);\n-    } else {\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()),\n-                (intptr_t)markWord::prototype().value()); \/\/ header\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-    }\n-#ifdef _LP64\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n-#endif\n-    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-    __ store_klass(rax, rcx, tmp_store_klass);  \/\/ klass\n+  Label slow_case;\n+  Label done;\n+  Label is_value;\n@@ -4115,8 +4388,2 @@\n-    {\n-      SkipIfEqual skip_if(_masm, &DTraceAllocProbes, 0);\n-      \/\/ Trigger dtrace event for fastpath\n-      __ push(atos);\n-      __ call_VM_leaf(\n-           CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), rax);\n-      __ pop(atos);\n-    }\n+  __ get_unsigned_2_byte_index_at_bcp(rdx, 1);\n+  __ get_cpool_and_tags(rcx, rax);\n@@ -4124,2 +4391,25 @@\n-    __ jmp(done);\n-  }\n+  \/\/ Make sure the class we're about to instantiate has been resolved.\n+  \/\/ This is done before loading InstanceKlass to be consistent with the order\n+  \/\/ how Constant Pool is updated (see ConstantPool::klass_at_put)\n+  const int tags_offset = Array<u1>::base_offset_in_bytes();\n+  __ cmpb(Address(rax, rdx, Address::times_1, tags_offset), JVM_CONSTANT_Class);\n+  __ jcc(Assembler::notEqual, slow_case);\n+\n+  \/\/ get InstanceKlass\n+  __ load_resolved_klass_at_index(rcx, rcx, rdx);\n+\n+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InstanceKlass::_kind_inline_type);\n+  __ jcc(Assembler::equal, is_value);\n+\n+  \/\/ in the future, defaultvalue will just return null instead of throwing an exception\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_IncompatibleClassChangeError));\n+\n+  __ bind(is_value);\n+\n+  \/\/ make sure klass is fully initialized\n+  __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+  __ jcc(Assembler::notEqual, slow_case);\n+\n+  \/\/ have a resolved InlineKlass in rcx, return the default value oop from it\n+  __ get_default_value_oop(rcx, rdx, rax);\n+  __ jmp(done);\n@@ -4127,4 +4417,1 @@\n-  \/\/ slow case\n-  __ pop(rcx);   \/\/ restore stack pointer to what it was when we came in.\n-  __ bind(slow_case_no_pop);\n-  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n+  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n@@ -4135,3 +4422,4 @@\n-  __ get_constant_pool(rarg1);\n-  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);\n-   __ verify_oop(rax);\n+  __ get_constant_pool(rarg1);\n+\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::defaultvalue),\n+      rarg1, rarg2);\n@@ -4140,1 +4428,1 @@\n-  \/\/ continue\n+  __ verify_oop(rax);\n@@ -4180,4 +4468,5 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+      Address::times_1,\n+      Array<u1>::base_offset_in_bytes()));\n+  __ andl (rdx, ~JVM_CONSTANT_QDescBit);\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4222,0 +4511,3 @@\n+  __ jmp(done);\n+\n+  __ bind(is_null);\n@@ -4225,4 +4517,15 @@\n-    __ jmp(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n+  if (EnableValhalla) {\n+    \/\/ Get cpool & tags index\n+    __ get_cpool_and_tags(rcx, rdx); \/\/ rcx=cpool, rdx=tags array\n+    __ get_unsigned_2_byte_index_at_bcp(rbx, 1); \/\/ rbx=index\n+    \/\/ See if CP entry is a Q-descriptor\n+    __ movzbl(rcx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+    __ andl (rcx, JVM_CONSTANT_QDescBit);\n+    __ cmpl(rcx, JVM_CONSTANT_QDescBit);\n+    __ jcc(Assembler::notEqual, done);\n+    __ jump(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+  }\n+\n@@ -4244,4 +4547,5 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+  __ andl (rdx, ~JVM_CONSTANT_QDescBit);\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4300,1 +4604,0 @@\n-\n@@ -4364,0 +4667,11 @@\n+  const int is_inline_type_mask = markWord::always_locked_pattern;\n+  Label has_identity;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ andptr(rbx, is_inline_type_mask);\n+  __ cmpl(rbx, is_inline_type_mask);\n+  __ jcc(Assembler::notEqual, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n@@ -4463,0 +4777,11 @@\n+  const int is_inline_type_mask = markWord::always_locked_pattern;\n+  Label has_identity;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ andptr(rbx, is_inline_type_mask);\n+  __ cmpl(rbx, is_inline_type_mask);\n+  __ jcc(Assembler::notEqual, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":498,"deletions":173,"binary":false,"changes":671,"status":"modified"},{"patch":"@@ -1485,1 +1485,1 @@\n-  if (!UseFastStosb && UseSSE >= 2 && UseUnalignedLoadStores) {\n+  if (UseSSE >= 2 && UseUnalignedLoadStores) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -613,4 +613,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->in_24_bit_fp_mode(), C->stub_function() != NULL);\n+  __ verified_entry(C);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -870,3 +870,0 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n@@ -888,1 +885,7 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL);\n+  __ verified_entry(C);\n+  __ bind(*_verified_entry);\n+\n+  if (C->stub_function() == NULL) {\n+    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->nmethod_entry_barrier(&_masm);\n+  }\n@@ -900,6 +903,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -953,23 +950,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    emit_opcode(cbuf, Assembler::REX_W);\n-    if (framesize < 0x80) {\n-      emit_opcode(cbuf, 0x83); \/\/ addq rsp, #framesize\n-      emit_rm(cbuf, 0x3, 0x00, RSP_enc);\n-      emit_d8(cbuf, framesize);\n-    } else {\n-      emit_opcode(cbuf, 0x81); \/\/ addq rsp, #framesize\n-      emit_rm(cbuf, 0x3, 0x00, RSP_enc);\n-      emit_d32(cbuf, framesize);\n-    }\n-  }\n-\n-  \/\/ popq rbp\n-  emit_opcode(cbuf, 0x58 | RBP_enc);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair(), C->output()->sp_inc_offset());\n@@ -993,6 +970,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1534,0 +1505,30 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+{\n+  MacroAssembler _masm(&cbuf);\n+  if (!_verified) {  \n+    uint insts_size = cbuf.insts_size();\n+    if (UseCompressedClassPointers) {\n+      __ load_klass(rscratch1, j_rarg0, rscratch2);\n+      __ cmpptr(rax, rscratch1);\n+    } else {\n+      __ cmpptr(rax, Address(j_rarg0, oopDesc::klass_offset_in_bytes()));\n+    }\n+    __ jump_cc(Assembler::notEqual, RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  } else {\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    __ jmp(*_verified_entry);\n+  }\n+}\n+\n@@ -1576,7 +1577,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n-\n@@ -3844,0 +3838,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -4186,1 +4196,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -6640,0 +6650,13 @@\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -6653,0 +6676,27 @@\n+instruct castN2I(rRegI dst, rRegN src)\n+%{\n+  match(Set dst (CastN2I src));\n+\n+  format %{ \"movl    $dst, $src\\t# compressed ptr -> int\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castI2N(rRegN dst, rRegI src)\n+%{\n+  match(Set dst (CastI2N src));\n+\n+  format %{ \"movl    $dst, $src\\t# int -> compressed ptr\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+\n@@ -10709,1 +10759,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -10712,3 +10762,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);\n@@ -10717,1 +10767,0 @@\n-    $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n@@ -10731,2 +10780,3 @@\n-       $$emit$$\"mov     rdi,rax\\n\\t\"\n-       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n@@ -10735,2 +10785,2 @@\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n@@ -10743,1 +10793,1 @@\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n@@ -10762,2 +10812,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n@@ -10768,1 +10818,57 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -10771,3 +10877,3 @@\n-  predicate(((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);\n@@ -10777,1 +10883,0 @@\n-       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n@@ -10781,2 +10886,3 @@\n-       $$emit$$\"mov     rdi,rax\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n@@ -10785,2 +10891,2 @@\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n@@ -10793,1 +10899,1 @@\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n@@ -10807,1 +10913,0 @@\n-       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n@@ -10812,2 +10917,48 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val, \n+                        Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register, \n+                 $tmp$$XMMRegister, true, true);\n@@ -11392,0 +11543,11 @@\n+\/\/ Fold array properties check\n+instruct testI_mem_imm(rFlagsReg cr, memory mem, immI con, immI0 zero)\n+%{\n+  match(Set cr (CmpI (AndI (CastN2I (LoadNKlass mem)) con) zero));\n+\n+  format %{ \"testl   $mem, $con\" %}\n+  opcode(0xF7, 0x00);\n+  ins_encode(REX_mem(mem), OpcP, RM_opc_mem(0x00, mem), Con32(con));\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n@@ -11704,0 +11866,11 @@\n+\/\/ Fold array properties check\n+instruct testL_reg_mem3(rFlagsReg cr, memory mem, rRegL src, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL (CastP2X (LoadKlass mem)) src) zero));\n+\n+  format %{ \"testq   $src, $mem\\t# test array properties\" %}\n+  opcode(0x85);\n+  ins_encode(REX_reg_mem_wide(src, mem), OpcP, reg_mem(src, mem));\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n@@ -12371,0 +12544,15 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == NULL);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -12373,0 +12561,1 @@\n+  predicate(n->as_Call()->entry_point() != NULL);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":261,"deletions":72,"binary":false,"changes":333,"status":"modified"},{"patch":"@@ -258,0 +258,77 @@\n+class LoadFlattenedArrayStub: public CodeStub {\n+ private:\n+  LIR_Opr          _array;\n+  LIR_Opr          _index;\n+  LIR_Opr          _result;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+\n+ public:\n+  LoadFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_array);\n+    visitor->do_input(_index);\n+    visitor->do_output(_result);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"LoadFlattenedArrayStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n+\n+class StoreFlattenedArrayStub: public CodeStub {\n+ private:\n+  LIR_Opr          _array;\n+  LIR_Opr          _index;\n+  LIR_Opr          _value;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+\n+ public:\n+  StoreFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr value, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_array);\n+    visitor->do_input(_index);\n+    visitor->do_input(_value);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"StoreFlattenedArrayStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n+class SubstitutabilityCheckStub: public CodeStub {\n+ private:\n+  LIR_Opr          _left;\n+  LIR_Opr          _right;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+ public:\n+  SubstitutabilityCheckStub(LIR_Opr left, LIR_Opr right, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_left);\n+    visitor->do_input(_right);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"SubstitutabilityCheckStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n@@ -310,1 +387,1 @@\n-\n+  bool           _is_inline_type;\n@@ -312,1 +389,1 @@\n-  NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info);\n+  NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info, bool is_inline_type);\n@@ -347,0 +424,2 @@\n+  CodeStub* _throw_imse_stub;\n+  LIR_Opr _scratch_reg;\n@@ -349,1 +428,1 @@\n-  MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info);\n+  MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info, CodeStub* throw_imse_stub = NULL, LIR_Opr scratch_reg = LIR_OprFact::illegalOpr);\n@@ -356,0 +435,3 @@\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":85,"deletions":3,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -97,0 +98,1 @@\n+    case T_INLINE_TYPE:\n@@ -155,0 +157,1 @@\n+    case T_INLINE_TYPE:\n@@ -298,1 +301,1 @@\n-                                 CodeStub* stub)\n+                                 CodeStub* stub, bool need_null_check)\n@@ -314,0 +317,1 @@\n+  , _need_null_check(need_null_check)\n@@ -341,0 +345,1 @@\n+  , _need_null_check(true)\n@@ -350,0 +355,31 @@\n+LIR_OpFlattenedArrayCheck::LIR_OpFlattenedArrayCheck(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub)\n+  : LIR_Op(lir_flattened_array_check, LIR_OprFact::illegalOpr, NULL)\n+  , _array(array)\n+  , _value(value)\n+  , _tmp(tmp)\n+  , _stub(stub) {}\n+\n+\n+LIR_OpNullFreeArrayCheck::LIR_OpNullFreeArrayCheck(LIR_Opr array, LIR_Opr tmp)\n+  : LIR_Op(lir_null_free_array_check, LIR_OprFact::illegalOpr, NULL)\n+  , _array(array)\n+  , _tmp(tmp) {}\n+\n+\n+LIR_OpSubstitutabilityCheck::LIR_OpSubstitutabilityCheck(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                                                         LIR_Opr tmp1, LIR_Opr tmp2,\n+                                                         ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                                                         CodeEmitInfo* info, CodeStub* stub)\n+  : LIR_Op(lir_substitutability_check, result, info)\n+  , _left(left)\n+  , _right(right)\n+  , _equal_result(equal_result)\n+  , _not_equal_result(not_equal_result)\n+  , _tmp1(tmp1)\n+  , _tmp2(tmp2)\n+  , _left_klass(left_klass)\n+  , _right_klass(right_klass)\n+  , _left_klass_op(left_klass_op)\n+  , _right_klass_op(right_klass_op)\n+  , _stub(stub) {}\n+\n@@ -417,0 +453,1 @@\n+    case lir_check_orig_pc:            \/\/ result and info always invalid\n@@ -809,0 +846,1 @@\n+      do_stub(opLock->_throw_imse_stub);\n@@ -845,0 +883,45 @@\n+\/\/ LIR_OpFlattenedArrayCheck\n+    case lir_flattened_array_check: {\n+      assert(op->as_OpFlattenedArrayCheck() != NULL, \"must be\");\n+      LIR_OpFlattenedArrayCheck* opFlattenedArrayCheck = (LIR_OpFlattenedArrayCheck*)op;\n+\n+      if (opFlattenedArrayCheck->_array->is_valid()) do_input(opFlattenedArrayCheck->_array);\n+      if (opFlattenedArrayCheck->_value->is_valid()) do_input(opFlattenedArrayCheck->_value);\n+      if (opFlattenedArrayCheck->_tmp->is_valid())   do_temp(opFlattenedArrayCheck->_tmp);\n+                                                     do_stub(opFlattenedArrayCheck->_stub);\n+\n+      break;\n+    }\n+\n+\/\/ LIR_OpNullFreeArrayCheck\n+    case lir_null_free_array_check: {\n+      assert(op->as_OpNullFreeArrayCheck() != NULL, \"must be\");\n+      LIR_OpNullFreeArrayCheck* opNullFreeArrayCheck = (LIR_OpNullFreeArrayCheck*)op;\n+\n+      if (opNullFreeArrayCheck->_array->is_valid()) do_input(opNullFreeArrayCheck->_array);\n+      if (opNullFreeArrayCheck->_tmp->is_valid())   do_temp(opNullFreeArrayCheck->_tmp);\n+      break;\n+    }\n+\n+\/\/ LIR_OpSubstitutabilityCheck\n+    case lir_substitutability_check: {\n+      assert(op->as_OpSubstitutabilityCheck() != NULL, \"must be\");\n+      LIR_OpSubstitutabilityCheck* opSubstitutabilityCheck = (LIR_OpSubstitutabilityCheck*)op;\n+                                                                do_input(opSubstitutabilityCheck->_left);\n+                                                                do_temp (opSubstitutabilityCheck->_left);\n+                                                                do_input(opSubstitutabilityCheck->_right);\n+                                                                do_temp (opSubstitutabilityCheck->_right);\n+                                                                do_input(opSubstitutabilityCheck->_equal_result);\n+                                                                do_temp (opSubstitutabilityCheck->_equal_result);\n+                                                                do_input(opSubstitutabilityCheck->_not_equal_result);\n+                                                                do_temp (opSubstitutabilityCheck->_not_equal_result);\n+      if (opSubstitutabilityCheck->_tmp1->is_valid())           do_temp(opSubstitutabilityCheck->_tmp1);\n+      if (opSubstitutabilityCheck->_tmp2->is_valid())           do_temp(opSubstitutabilityCheck->_tmp2);\n+      if (opSubstitutabilityCheck->_left_klass_op->is_valid())  do_temp(opSubstitutabilityCheck->_left_klass_op);\n+      if (opSubstitutabilityCheck->_right_klass_op->is_valid()) do_temp(opSubstitutabilityCheck->_right_klass_op);\n+      if (opSubstitutabilityCheck->_result->is_valid())         do_output(opSubstitutabilityCheck->_result);\n+                                                                do_info(opSubstitutabilityCheck->_info);\n+                                                                do_stub(opSubstitutabilityCheck->_stub);\n+      break;\n+    }\n+\n@@ -908,1 +991,12 @@\n-  default:\n+\n+    \/\/ LIR_OpProfileInlineType:\n+    case lir_profile_inline_type: {\n+      assert(op->as_OpProfileInlineType() != NULL, \"must be\");\n+      LIR_OpProfileInlineType* opProfileInlineType = (LIR_OpProfileInlineType*)op;\n+\n+      do_input(opProfileInlineType->_mdp); do_temp(opProfileInlineType->_mdp);\n+      do_input(opProfileInlineType->_obj);\n+      do_temp(opProfileInlineType->_tmp);\n+      break;\n+    }\n+default:\n@@ -981,0 +1075,39 @@\n+bool LIR_OpJavaCall::maybe_return_as_fields(ciInlineKlass** vk_ret) const {\n+  if (InlineTypeReturnedAsFields) {\n+    if (method()->signature()->maybe_returns_inline_type()) {\n+      ciType* return_type = method()->return_type();\n+      if (return_type->is_inlinetype()) {\n+        ciInlineKlass* vk = return_type->as_inline_klass();\n+        if (vk->can_be_returned_as_fields()) {\n+          if (vk_ret != NULL) {\n+            *vk_ret = vk;\n+          }\n+          return true;\n+        }\n+      } else {\n+        assert(return_type->is_instance_klass() && !return_type->as_instance_klass()->is_loaded(), \"must be\");\n+        if (vk_ret != NULL) {\n+          *vk_ret = NULL;\n+        }\n+        return true;\n+      }\n+    } else if (is_method_handle_invoke()) {\n+      BasicType bt = method()->return_type()->basic_type();\n+      if (bt == T_OBJECT || bt == T_INLINE_TYPE) {\n+        \/\/ An inline type might be returned from the call but we don't know its\n+        \/\/ type. Either we get a buffered inline type (and nothing needs to be done)\n+        \/\/ or one of the inlines being returned is the klass of the inline type\n+        \/\/ (RAX on x64, with LSB set to 1) and we need to allocate an inline\n+        \/\/ type instance of that type and initialize it with other values being\n+        \/\/ returned (in other registers).\n+        \/\/ type.\n+        if (vk_ret != NULL) {\n+          *vk_ret = NULL;\n+        }\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1041,0 +1174,18 @@\n+void LIR_OpFlattenedArrayCheck::emit_code(LIR_Assembler* masm) {\n+  masm->emit_opFlattenedArrayCheck(this);\n+  if (stub() != NULL) {\n+    masm->append_code_stub(stub());\n+  }\n+}\n+\n+void LIR_OpNullFreeArrayCheck::emit_code(LIR_Assembler* masm) {\n+  masm->emit_opNullFreeArrayCheck(this);\n+}\n+\n+void LIR_OpSubstitutabilityCheck::emit_code(LIR_Assembler* masm) {\n+  masm->emit_opSubstitutabilityCheck(this);\n+  if (stub() != NULL) {\n+    masm->append_code_stub(stub());\n+  }\n+}\n+\n@@ -1054,0 +1205,3 @@\n+  if (throw_imse_stub()) {\n+    masm->append_code_stub(throw_imse_stub());\n+  }\n@@ -1074,0 +1228,4 @@\n+void LIR_OpProfileInlineType::emit_code(LIR_Assembler* masm) {\n+  masm->emit_profile_inline_type(this);\n+}\n+\n@@ -1355,1 +1513,1 @@\n-void LIR_List::lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info) {\n+void LIR_List::lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_imse_stub) {\n@@ -1363,1 +1521,2 @@\n-                    info));\n+                    info,\n+                    throw_imse_stub));\n@@ -1388,1 +1547,4 @@\n-                          ciMethod* profiled_method, int profiled_bci) {\n+                          ciMethod* profiled_method, int profiled_bci, bool is_null_free) {\n+  \/\/ If klass is non-nullable,  LIRGenerator::do_CheckCast has already performed null-check\n+  \/\/ on the object.\n+  bool need_null_check = !is_null_free;\n@@ -1390,1 +1552,2 @@\n-                                           tmp1, tmp2, tmp3, fast_check, info_for_exception, info_for_patch, stub);\n+                                           tmp1, tmp2, tmp3, fast_check, info_for_exception, info_for_patch, stub,\n+                                           need_null_check);\n@@ -1412,0 +1575,1 @@\n+  \/\/ FIXME -- if the types of the array and\/or the object are known statically, we can avoid loading the klass\n@@ -1433,0 +1597,21 @@\n+void LIR_List::check_flattened_array(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub) {\n+  LIR_OpFlattenedArrayCheck* c = new LIR_OpFlattenedArrayCheck(array, value, tmp, stub);\n+  append(c);\n+}\n+\n+void LIR_List::check_null_free_array(LIR_Opr array, LIR_Opr tmp) {\n+  LIR_OpNullFreeArrayCheck* c = new LIR_OpNullFreeArrayCheck(array, tmp);\n+  append(c);\n+}\n+\n+void LIR_List::substitutability_check(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                                      LIR_Opr tmp1, LIR_Opr tmp2,\n+                                      ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                                      CodeEmitInfo* info, CodeStub* stub) {\n+  LIR_OpSubstitutabilityCheck* c = new LIR_OpSubstitutabilityCheck(result, left, right, equal_result, not_equal_result,\n+                                                                   tmp1, tmp2,\n+                                                                   left_klass, right_klass, left_klass_op, right_klass_op,\n+                                                                   info, stub);\n+  append(c);\n+}\n+\n@@ -1650,0 +1835,1 @@\n+     case lir_check_orig_pc:         s = \"check_orig_pc\"; break;\n@@ -1718,0 +1904,6 @@\n+     \/\/ LIR_OpFlattenedArrayCheck\n+     case lir_flattened_array_check: s = \"flattened_array_check\"; break;\n+     \/\/ LIR_OpNullFreeArrayCheck\n+     case lir_null_free_array_check: s = \"null_free_array_check\"; break;\n+     \/\/ LIR_OpSubstitutabilityCheck\n+     case lir_substitutability_check: s = \"substitutability_check\"; break;\n@@ -1726,0 +1918,2 @@\n+     \/\/ LIR_OpProfileInlineType\n+     case lir_profile_inline_type:   s = \"profile_inline_type\"; break;\n@@ -1963,0 +2157,38 @@\n+void LIR_OpFlattenedArrayCheck::print_instr(outputStream* out) const {\n+  array()->print(out);                   out->print(\" \");\n+  value()->print(out);                   out->print(\" \");\n+  tmp()->print(out);                     out->print(\" \");\n+  if (stub() != NULL) {\n+    out->print(\"[label:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n+  }\n+}\n+\n+void LIR_OpNullFreeArrayCheck::print_instr(outputStream* out) const {\n+  array()->print(out);                   out->print(\" \");\n+  tmp()->print(out);                     out->print(\" \");\n+}\n+\n+void LIR_OpSubstitutabilityCheck::print_instr(outputStream* out) const {\n+  result_opr()->print(out);              out->print(\" \");\n+  left()->print(out);                    out->print(\" \");\n+  right()->print(out);                   out->print(\" \");\n+  equal_result()->print(out);            out->print(\" \");\n+  not_equal_result()->print(out);        out->print(\" \");\n+  tmp1()->print(out);                    out->print(\" \");\n+  tmp2()->print(out);                    out->print(\" \");\n+  if (left_klass() == NULL) {\n+    out->print(\"unknown \");\n+  } else {\n+    left_klass()->print(out);            out->print(\" \");\n+  }\n+  if (right_klass() == NULL) {\n+    out->print(\"unknown \");\n+  } else {\n+    right_klass()->print(out);           out->print(\" \");\n+  }\n+  left_klass_op()->print(out);           out->print(\" \");\n+  right_klass_op()->print(out);          out->print(\" \");\n+  if (stub() != NULL) {\n+    out->print(\"[label:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n+  }\n+}\n@@ -2024,0 +2256,8 @@\n+\/\/ LIR_OpProfileInlineType\n+void LIR_OpProfileInlineType::print_instr(outputStream* out) const {\n+  out->print(\" flag = %x \", flag());\n+  mdp()->print(out);          out->print(\" \");\n+  obj()->print(out);          out->print(\" \");\n+  tmp()->print(out);          out->print(\" \");\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":246,"deletions":6,"binary":false,"changes":252,"status":"modified"},{"patch":"@@ -320,0 +320,1 @@\n+      case T_INLINE_TYPE:\n@@ -470,0 +471,1 @@\n+  case T_INLINE_TYPE:\n@@ -655,0 +657,1 @@\n+      case T_INLINE_TYPE: \/\/ fall through\n@@ -760,0 +763,1 @@\n+      case T_INLINE_TYPE: \/\/ fall through\n@@ -873,0 +877,3 @@\n+class    LIR_OpFlattenedArrayCheck;\n+class    LIR_OpNullFreeArrayCheck;\n+class    LIR_OpSubstitutabilityCheck;\n@@ -876,0 +883,1 @@\n+class    LIR_OpProfileInlineType;\n@@ -901,0 +909,1 @@\n+      , lir_check_orig_pc\n@@ -980,0 +989,9 @@\n+  , begin_opFlattenedArrayCheck\n+    , lir_flattened_array_check\n+  , end_opFlattenedArrayCheck\n+  , begin_opNullFreeArrayCheck\n+    , lir_null_free_array_check\n+  , end_opNullFreeArrayCheck\n+  , begin_opSubstitutabilityCheck\n+    , lir_substitutability_check\n+  , end_opSubstitutabilityCheck\n@@ -988,0 +1006,1 @@\n+    , lir_profile_inline_type\n@@ -1131,0 +1150,3 @@\n+  virtual LIR_OpFlattenedArrayCheck* as_OpFlattenedArrayCheck() { return NULL; }\n+  virtual LIR_OpNullFreeArrayCheck* as_OpNullFreeArrayCheck() { return NULL; }\n+  virtual LIR_OpSubstitutabilityCheck* as_OpSubstitutabilityCheck() { return NULL; }\n@@ -1134,0 +1156,1 @@\n+  virtual LIR_OpProfileInlineType* as_OpProfileInlineType() { return NULL; }\n@@ -1211,0 +1234,2 @@\n+\n+  bool maybe_return_as_fields(ciInlineKlass** vk = NULL) const;\n@@ -1262,1 +1287,4 @@\n-    all_flags              = (1 << 12) - 1\n+    always_slow_path       = 1 << 12,\n+    src_inlinetype_check   = 1 << 13,\n+    dst_inlinetype_check   = 1 << 14,\n+    all_flags              = (1 << 15) - 1\n@@ -1564,0 +1592,1 @@\n+  bool          _need_null_check;\n@@ -1568,1 +1597,1 @@\n-                  CodeEmitInfo* info_for_exception, CodeEmitInfo* info_for_patch, CodeStub* stub);\n+                  CodeEmitInfo* info_for_exception, CodeEmitInfo* info_for_patch, CodeStub* stub, bool need_null_check = true);\n@@ -1590,1 +1619,1 @@\n-\n+  bool      need_null_check() const              { return _need_null_check;   }\n@@ -1597,0 +1626,76 @@\n+\/\/ LIR_OpFlattenedArrayCheck\n+class LIR_OpFlattenedArrayCheck: public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr       _array;\n+  LIR_Opr       _value;\n+  LIR_Opr       _tmp;\n+  CodeStub*     _stub;\n+public:\n+  LIR_OpFlattenedArrayCheck(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub);\n+  LIR_Opr array() const                          { return _array;         }\n+  LIR_Opr value() const                          { return _value;         }\n+  LIR_Opr tmp() const                            { return _tmp;           }\n+  CodeStub* stub() const                         { return _stub;          }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpFlattenedArrayCheck* as_OpFlattenedArrayCheck() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n+\/\/ LIR_OpNullFreeArrayCheck\n+class LIR_OpNullFreeArrayCheck: public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr       _array;\n+  LIR_Opr       _tmp;\n+public:\n+  LIR_OpNullFreeArrayCheck(LIR_Opr array, LIR_Opr tmp);\n+  LIR_Opr array() const                          { return _array;         }\n+  LIR_Opr tmp() const                            { return _tmp;           }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpNullFreeArrayCheck* as_OpNullFreeArrayCheck() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n+class LIR_OpSubstitutabilityCheck: public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr       _left;\n+  LIR_Opr       _right;\n+  LIR_Opr       _equal_result;\n+  LIR_Opr       _not_equal_result;\n+  LIR_Opr       _tmp1;\n+  LIR_Opr       _tmp2;\n+  ciKlass*      _left_klass;\n+  ciKlass*      _right_klass;\n+  LIR_Opr       _left_klass_op;\n+  LIR_Opr       _right_klass_op;\n+  CodeStub*     _stub;\n+public:\n+  LIR_OpSubstitutabilityCheck(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                              LIR_Opr tmp1, LIR_Opr tmp2,\n+                              ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                              CodeEmitInfo* info, CodeStub* stub);\n+\n+  LIR_Opr left() const             { return _left; }\n+  LIR_Opr right() const            { return _right; }\n+  LIR_Opr equal_result() const     { return _equal_result; }\n+  LIR_Opr not_equal_result() const { return _not_equal_result; }\n+  LIR_Opr tmp1() const             { return _tmp1; }\n+  LIR_Opr tmp2() const             { return _tmp2; }\n+  ciKlass* left_klass() const      { return _left_klass; }\n+  ciKlass* right_klass() const     { return _right_klass; }\n+  LIR_Opr left_klass_op() const    { return _left_klass_op; }\n+  LIR_Opr right_klass_op() const   { return _right_klass_op; }\n+  CodeStub* stub() const           { return _stub; }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpSubstitutabilityCheck* as_OpSubstitutabilityCheck() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n@@ -1789,0 +1894,1 @@\n+  CodeStub* _throw_imse_stub;\n@@ -1790,1 +1896,1 @@\n-  LIR_OpLock(LIR_Code code, LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info)\n+  LIR_OpLock(LIR_Code code, LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_imse_stub=NULL)\n@@ -1796,1 +1902,2 @@\n-    , _stub(stub)                      {}\n+    , _stub(stub)\n+    , _throw_imse_stub(throw_imse_stub)                    {}\n@@ -1803,0 +1910,1 @@\n+  CodeStub* throw_imse_stub() const              { return _throw_imse_stub; }\n@@ -1969,0 +2077,32 @@\n+\/\/ LIR_OpProfileInlineType\n+class LIR_OpProfileInlineType : public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr      _mdp;\n+  LIR_Opr      _obj;\n+  int          _flag;\n+  LIR_Opr      _tmp;\n+  bool         _not_null;      \/\/ true if we know statically that _obj cannot be null\n+\n+ public:\n+  \/\/ Destroys recv\n+  LIR_OpProfileInlineType(LIR_Opr mdp, LIR_Opr obj, int flag, LIR_Opr tmp, bool not_null)\n+    : LIR_Op(lir_profile_inline_type, LIR_OprFact::illegalOpr, NULL)  \/\/ no result, no info\n+    , _mdp(mdp)\n+    , _obj(obj)\n+    , _flag(flag)\n+    , _tmp(tmp)\n+    , _not_null(not_null) { }\n+\n+  LIR_Opr      mdp()              const             { return _mdp;              }\n+  LIR_Opr      obj()              const             { return _obj;              }\n+  int          flag()             const             { return _flag;             }\n+  LIR_Opr      tmp()              const             { return _tmp;              }\n+  bool         not_null()         const             { return _not_null;         }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpProfileInlineType* as_OpProfileInlineType() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n@@ -2233,1 +2373,1 @@\n-  void lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info);\n+  void lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_imse_stub=NULL);\n@@ -2243,0 +2383,6 @@\n+  void check_flattened_array(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub);\n+  void check_null_free_array(LIR_Opr array, LIR_Opr tmp);\n+  void substitutability_check(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                              LIR_Opr tmp1, LIR_Opr tmp2,\n+                              ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                              CodeEmitInfo* info, CodeStub* stub);\n@@ -2247,1 +2393,1 @@\n-                  ciMethod* profiled_method, int profiled_bci);\n+                  ciMethod* profiled_method, int profiled_bci, bool is_null_free);\n@@ -2255,0 +2401,3 @@\n+  void profile_inline_type(LIR_Address* mdp, LIR_Opr obj, int flag, LIR_Opr tmp, bool not_null) {\n+    append(new LIR_OpProfileInlineType(LIR_OprFact::address(mdp), obj, flag, tmp, not_null));\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":156,"deletions":7,"binary":false,"changes":163,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -62,0 +64,1 @@\n+      case Bytecodes::_defaultvalue:\n@@ -118,0 +121,1 @@\n+  _verified_inline_entry.reset();\n@@ -339,1 +343,1 @@\n-void LIR_Assembler::add_call_info(int pc_offset, CodeEmitInfo* cinfo) {\n+void LIR_Assembler::add_call_info(int pc_offset, CodeEmitInfo* cinfo, bool maybe_return_as_fields) {\n@@ -341,1 +345,1 @@\n-  cinfo->record_debug_info(compilation()->debug_info_recorder(), pc_offset);\n+  cinfo->record_debug_info(compilation()->debug_info_recorder(), pc_offset, maybe_return_as_fields);\n@@ -484,0 +488,6 @@\n+  ciInlineKlass* vk;\n+  if (op->maybe_return_as_fields(&vk)) {\n+    int offset = store_inline_type_fields_to_buf(vk);\n+    add_call_info(offset, op->info(), true);\n+  }\n+\n@@ -591,0 +601,135 @@\n+void LIR_Assembler::add_scalarized_entry_info(int pc_offset) {\n+  flush_debug_info(pc_offset);\n+  DebugInformationRecorder* debug_info = compilation()->debug_info_recorder();\n+  \/\/ The VEP and VIEP(RO) of a C1-compiled method call buffer_inline_args_xxx()\n+  \/\/ before doing any argument shuffling. This call may cause GC. When GC happens,\n+  \/\/ all the parameters are still as passed by the caller, so we just use\n+  \/\/ map->set_include_argument_oops() inside frame::sender_for_compiled_frame(RegisterMap* map).\n+  \/\/ There's no need to build a GC map here.\n+  OopMap* oop_map = new OopMap(0, 0);\n+  debug_info->add_safepoint(pc_offset, oop_map);\n+  DebugToken* locvals = debug_info->create_scope_values(NULL); \/\/ FIXME is this needed (for Java debugging to work properly??)\n+  DebugToken* expvals = debug_info->create_scope_values(NULL); \/\/ FIXME is this needed (for Java debugging to work properly??)\n+  DebugToken* monvals = debug_info->create_monitor_values(NULL); \/\/ FIXME: need testing with synchronized method\n+  bool reexecute = false;\n+  bool return_oop = false; \/\/ This flag will be ignored since it used only for C2 with escape analysis.\n+  bool rethrow_exception = false;\n+  bool is_method_handle_invoke = false;\n+  debug_info->describe_scope(pc_offset, methodHandle(), method(), 0, reexecute, rethrow_exception, is_method_handle_invoke, return_oop, false, locvals, expvals, monvals);\n+  debug_info->end_safepoint(pc_offset);\n+}\n+\n+\/\/ The entries points of C1-compiled methods can have the following types:\n+\/\/ (1) Methods with no inline type args\n+\/\/ (2) Methods with inline type receiver but no inline type args\n+\/\/     VIEP_RO is the same as VIEP\n+\/\/ (3) Methods with non-inline type receiver and some inline type args\n+\/\/     VIEP_RO is the same as VEP\n+\/\/ (4) Methods with inline type receiver and other inline type args\n+\/\/     Separate VEP, VIEP and VIEP_RO\n+\/\/\n+\/\/ (1)               (2)                 (3)                    (4)\n+\/\/ UEP\/UIEP:         VEP:                UEP:                   UEP:\n+\/\/   check_icache      pack receiver       check_icache           check_icache\n+\/\/ VEP\/VIEP\/VIEP_RO    jump to VIEP      VEP\/VIEP_RO:           VIEP_RO:\n+\/\/   body            UEP\/UIEP:             pack inline args       pack inline args (except receiver)\n+\/\/                     check_icache        jump to VIEP           jump to VIEP\n+\/\/                   VIEP\/VIEP_RO        UIEP:                  VEP:\n+\/\/                     body                check_icache           pack all inline args\n+\/\/                                       VIEP:                    jump to VIEP\n+\/\/                                         body                 UIEP:\n+\/\/                                                                check_icache\n+\/\/                                                              VIEP:\n+\/\/                                                                body\n+void LIR_Assembler::emit_std_entries() {\n+  offsets()->set_value(CodeOffsets::OSR_Entry, _masm->offset());\n+\n+  _masm->align(CodeEntryAlignment);\n+  const CompiledEntrySignature* ces = compilation()->compiled_entry_signature();\n+  if (ces->has_scalarized_args()) {\n+    assert(InlineTypePassFieldsAsArgs && method()->get_Method()->has_scalarized_args(), \"must be\");\n+    CodeOffsets::Entries ro_entry_type = ces->c1_inline_ro_entry_type();\n+\n+    \/\/ UEP: check icache and fall-through\n+    if (ro_entry_type != CodeOffsets::Verified_Inline_Entry) {\n+      offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+      if (needs_icache(method())) {\n+        check_icache();\n+      }\n+    }\n+\n+    \/\/ VIEP_RO: pack all value parameters, except the receiver\n+    if (ro_entry_type == CodeOffsets::Verified_Inline_Entry_RO) {\n+      emit_std_entry(CodeOffsets::Verified_Inline_Entry_RO, ces);\n+    }\n+\n+    \/\/ VEP: pack all value parameters\n+    _masm->align(CodeEntryAlignment);\n+    emit_std_entry(CodeOffsets::Verified_Entry, ces);\n+\n+    \/\/ UIEP: check icache and fall-through\n+    _masm->align(CodeEntryAlignment);\n+    offsets()->set_value(CodeOffsets::Inline_Entry, _masm->offset());\n+    if (ro_entry_type == CodeOffsets::Verified_Inline_Entry) {\n+      \/\/ Special case if we have VIEP == VIEP(RO):\n+      \/\/ this means UIEP (called by C1) == UEP (called by C2).\n+      offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+    }\n+    if (needs_icache(method())) {\n+      check_icache();\n+    }\n+\n+    \/\/ VIEP: all value parameters are passed as refs - no packing.\n+    emit_std_entry(CodeOffsets::Verified_Inline_Entry, NULL);\n+\n+    if (ro_entry_type != CodeOffsets::Verified_Inline_Entry_RO) {\n+      \/\/ The VIEP(RO) is the same as VEP or VIEP\n+      assert(ro_entry_type == CodeOffsets::Verified_Entry ||\n+             ro_entry_type == CodeOffsets::Verified_Inline_Entry, \"must be\");\n+      offsets()->set_value(CodeOffsets::Verified_Inline_Entry_RO,\n+                           offsets()->value(ro_entry_type));\n+    }\n+  } else {\n+    \/\/ All 3 entries are the same (no value-type packing)\n+    offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+    offsets()->set_value(CodeOffsets::Inline_Entry, _masm->offset());\n+    if (needs_icache(method())) {\n+      check_icache();\n+    }\n+    emit_std_entry(CodeOffsets::Verified_Inline_Entry, NULL);\n+    offsets()->set_value(CodeOffsets::Verified_Entry, offsets()->value(CodeOffsets::Verified_Inline_Entry));\n+    offsets()->set_value(CodeOffsets::Verified_Inline_Entry_RO, offsets()->value(CodeOffsets::Verified_Inline_Entry));\n+  }\n+}\n+\n+void LIR_Assembler::emit_std_entry(CodeOffsets::Entries entry, const CompiledEntrySignature* ces) {\n+  offsets()->set_value(entry, _masm->offset());\n+  _masm->verified_entry();\n+  switch (entry) {\n+  case CodeOffsets::Verified_Entry: {\n+    if (needs_clinit_barrier_on_entry(method())) {\n+      clinit_barrier(method());\n+    }\n+    int rt_call_offset = _masm->verified_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()), _verified_inline_entry);\n+    add_scalarized_entry_info(rt_call_offset);\n+    break;\n+  }\n+  case CodeOffsets::Verified_Inline_Entry_RO: {\n+    assert(!needs_clinit_barrier_on_entry(method()), \"can't be static\");\n+    int rt_call_offset = _masm->verified_inline_ro_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()), _verified_inline_entry);\n+    add_scalarized_entry_info(rt_call_offset);\n+    break;\n+  }\n+  case CodeOffsets::Verified_Inline_Entry: {\n+    if (needs_clinit_barrier_on_entry(method())) {\n+      clinit_barrier(method());\n+    }\n+    build_frame();\n+    offsets()->set_value(CodeOffsets::Frame_Complete, _masm->offset());\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n+    break;\n+  }\n+}\n@@ -604,13 +749,1 @@\n-      \/\/ init offsets\n-      offsets()->set_value(CodeOffsets::OSR_Entry, _masm->offset());\n-      _masm->align(CodeEntryAlignment);\n-      if (needs_icache(compilation()->method())) {\n-        check_icache();\n-      }\n-      offsets()->set_value(CodeOffsets::Verified_Entry, _masm->offset());\n-      _masm->verified_entry();\n-      if (needs_clinit_barrier_on_entry(compilation()->method())) {\n-        clinit_barrier(compilation()->method());\n-      }\n-      build_frame();\n-      offsets()->set_value(CodeOffsets::Frame_Complete, _masm->offset());\n+      emit_std_entries();\n@@ -670,0 +803,4 @@\n+    case lir_check_orig_pc:\n+      check_orig_pc();\n+      break;\n+\n@@ -763,1 +900,2 @@\n-  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()),\n+                     needs_stack_repair(), method()->has_scalarized_args(), &_verified_inline_entry);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":154,"deletions":16,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+class CompiledEntrySignature;\n@@ -51,0 +52,1 @@\n+  Label              _verified_inline_entry;\n@@ -95,0 +97,4 @@\n+  bool needs_stack_repair() const {\n+    return compilation()->needs_stack_repair();\n+  }\n+\n@@ -101,1 +107,1 @@\n-  void add_call_info(int pc_offset, CodeEmitInfo* cinfo);\n+  void add_call_info(int pc_offset, CodeEmitInfo* cinfo, bool maybe_return_as_fields = false);\n@@ -197,0 +203,3 @@\n+  void emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op);\n+  void emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op);\n+  void emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op);\n@@ -204,0 +213,1 @@\n+  void emit_profile_inline_type(LIR_OpProfileInlineType* op);\n@@ -205,0 +215,3 @@\n+  void emit_std_entries();\n+  void emit_std_entry(CodeOffsets::Entries entry, const CompiledEntrySignature* ces);\n+  void add_scalarized_entry_info(int call_offset);\n@@ -226,0 +239,1 @@\n+  int  store_inline_type_fields_to_buf(ciInlineKlass* vk);\n@@ -252,0 +266,1 @@\n+  void check_orig_pc();\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.hpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -66,1 +66,1 @@\n-static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 0, 2,  1, 2, 1, -1};\n+static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 0, 2,  1, 2, 1, 2, -1};\n@@ -68,1 +68,1 @@\n-static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, -1, 1, 1, -1};\n+static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, -1, 1, 1, 1, -1};\n@@ -263,1 +263,1 @@\n-  if (!frame_map()->finalize_frame(max_spills())) {\n+  if (!frame_map()->finalize_frame(max_spills(), compilation()->needs_stack_repair())) {\n@@ -2946,1 +2946,1 @@\n-  return new IRScopeDebugInfo(cur_scope, cur_state->bci(), locals, expressions, monitors, caller_debug_info);\n+  return new IRScopeDebugInfo(cur_scope, cur_state->bci(), locals, expressions, monitors, caller_debug_info, cur_state->should_reexecute());\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -55,0 +55,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -123,0 +125,1 @@\n+int Runtime1::_new_flat_array_slowcase_cnt = 0;\n@@ -125,0 +128,5 @@\n+int Runtime1::_load_flattened_array_slowcase_cnt = 0;\n+int Runtime1::_store_flattened_array_slowcase_cnt = 0;\n+int Runtime1::_substitutability_check_slowcase_cnt = 0;\n+int Runtime1::_buffer_inline_args_slowcase_cnt = 0;\n+int Runtime1::_buffer_inline_args_no_receiver_slowcase_cnt = 0;\n@@ -134,0 +142,1 @@\n+int Runtime1::_throw_illegal_monitor_state_exception_count = 0;\n@@ -355,4 +364,1 @@\n-\n-JRT_ENTRY(void, Runtime1::new_instance(JavaThread* thread, Klass* klass))\n-  NOT_PRODUCT(_new_instance_slowcase_cnt++;)\n-\n+static void allocate_instance(JavaThread* thread, Klass* klass, TRAPS) {\n@@ -368,0 +374,5 @@\n+}\n+\n+JRT_ENTRY(void, Runtime1::new_instance(JavaThread* thread, Klass* klass))\n+  NOT_PRODUCT(_new_instance_slowcase_cnt++;)\n+  allocate_instance(thread, klass, CHECK);\n@@ -370,0 +381,9 @@\n+\/\/ Same as new_instance but throws error for inline klasses\n+JRT_ENTRY(void, Runtime1::new_instance_no_inline(JavaThread* thread, Klass* klass))\n+  NOT_PRODUCT(_new_instance_slowcase_cnt++;)\n+  if (klass->is_inline_klass()) {\n+    SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_InstantiationError());\n+  } else {\n+    allocate_instance(thread, klass, CHECK);\n+  }\n+JRT_END\n@@ -397,1 +417,1 @@\n-  Klass* elem_klass = ObjArrayKlass::cast(array_klass)->element_klass();\n+  Klass* elem_klass = ArrayKlass::cast(array_klass)->element_klass();\n@@ -408,0 +428,22 @@\n+JRT_ENTRY(void, Runtime1::new_flat_array(JavaThread* thread, Klass* array_klass, jint length))\n+  NOT_PRODUCT(_new_flat_array_slowcase_cnt++;)\n+\n+  \/\/ Note: no handle for klass needed since they are not used\n+  \/\/       anymore after new_objArray() and no GC can happen before.\n+  \/\/       (This may have to change if this code changes!)\n+  assert(array_klass->is_klass(), \"not a class\");\n+  Handle holder(THREAD, array_klass->klass_holder()); \/\/ keep the klass alive\n+  Klass* elem_klass = ArrayKlass::cast(array_klass)->element_klass();\n+  assert(elem_klass->is_inline_klass(), \"must be\");\n+  \/\/ Logically creates elements, ensure klass init\n+  elem_klass->initialize(CHECK);\n+  arrayOop obj = oopFactory::new_flatArray(elem_klass, length, CHECK);\n+  thread->set_vm_result(obj);\n+  \/\/ This is pretty rare but this runtime patch is stressful to deoptimization\n+  \/\/ if we deoptimize here so force a deopt to stress the path.\n+  if (DeoptimizeALot) {\n+    deopt_caller();\n+  }\n+JRT_END\n+\n+\n@@ -419,0 +461,77 @@\n+static void profile_flat_array(JavaThread* thread) {\n+  ResourceMark rm(thread);\n+  vframeStream vfst(thread, true);\n+  assert(!vfst.at_end(), \"Java frame must exist\");\n+  int bci = vfst.bci();\n+  Method* method = vfst.method();\n+  MethodData* md = method->method_data();\n+  if (md != NULL) {\n+    ProfileData* data = md->bci_to_data(bci);\n+    assert(data != NULL && data->is_ArrayLoadStoreData(), \"incorrect profiling entry\");\n+    ArrayLoadStoreData* load_store = (ArrayLoadStoreData*)data;\n+    load_store->set_flat_array();\n+  }\n+}\n+\n+JRT_ENTRY(void, Runtime1::load_flattened_array(JavaThread* thread, flatArrayOopDesc* array, int index))\n+  assert(array->klass()->is_flatArray_klass(), \"should not be called\");\n+  profile_flat_array(thread);\n+\n+  NOT_PRODUCT(_load_flattened_array_slowcase_cnt++;)\n+  assert(array->length() > 0 && index < array->length(), \"already checked\");\n+  flatArrayHandle vah(thread, array);\n+  oop obj = flatArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK);\n+  thread->set_vm_result(obj);\n+JRT_END\n+\n+\n+JRT_ENTRY(void, Runtime1::store_flattened_array(JavaThread* thread, flatArrayOopDesc* array, int index, oopDesc* value))\n+  if (array->klass()->is_flatArray_klass()) {\n+    profile_flat_array(thread);\n+  }\n+\n+  NOT_PRODUCT(_store_flattened_array_slowcase_cnt++;)\n+  if (value == NULL) {\n+    assert(array->klass()->is_flatArray_klass() || array->klass()->is_null_free_array_klass(), \"should not be called\");\n+    SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_NullPointerException());\n+  } else {\n+    assert(array->klass()->is_flatArray_klass(), \"should not be called\");\n+    array->value_copy_to_index(value, index);\n+  }\n+JRT_END\n+\n+\n+JRT_ENTRY(int, Runtime1::substitutability_check(JavaThread* thread, oopDesc* left, oopDesc* right))\n+  NOT_PRODUCT(_substitutability_check_slowcase_cnt++;)\n+  JavaCallArguments args;\n+  args.push_oop(Handle(THREAD, left));\n+  args.push_oop(Handle(THREAD, right));\n+  JavaValue result(T_BOOLEAN);\n+  JavaCalls::call_static(&result,\n+                         SystemDictionary::ValueBootstrapMethods_klass(),\n+                         vmSymbols::isSubstitutable_name(),\n+                         vmSymbols::object_object_boolean_signature(),\n+                         &args, CHECK_0);\n+  return result.get_jboolean() ? 1 : 0;\n+JRT_END\n+\n+\n+extern \"C\" void ps();\n+\n+void Runtime1::buffer_inline_args_impl(JavaThread* thread, Method* m, bool allocate_receiver) {\n+  Thread* THREAD = thread;\n+  methodHandle method(thread, m); \/\/ We are inside the verified_entry or verified_inline_ro_entry of this method.\n+  oop obj = SharedRuntime::allocate_inline_types_impl(thread, method, allocate_receiver, CHECK);\n+  thread->set_vm_result(obj);\n+}\n+\n+JRT_ENTRY(void, Runtime1::buffer_inline_args(JavaThread* thread, Method* method))\n+  NOT_PRODUCT(_buffer_inline_args_slowcase_cnt++;)\n+  buffer_inline_args_impl(thread, method, true);\n+JRT_END\n+\n+JRT_ENTRY(void, Runtime1::buffer_inline_args_no_receiver(JavaThread* thread, Method* method))\n+  NOT_PRODUCT(_buffer_inline_args_no_receiver_slowcase_cnt++;)\n+  buffer_inline_args_impl(thread, method, false);\n+JRT_END\n+\n@@ -712,0 +831,7 @@\n+JRT_ENTRY(void, Runtime1::throw_illegal_monitor_state_exception(JavaThread* thread))\n+  NOT_PRODUCT(_throw_illegal_monitor_state_exception_count++;)\n+  ResourceMark rm(thread);\n+  SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_IllegalMonitorStateException());\n+JRT_END\n+\n+\n@@ -914,0 +1040,1 @@\n+    assert(!result.is_inlined(), \"Can not patch access to flattened field\");\n@@ -956,0 +1083,5 @@\n+      case Bytecodes::_defaultvalue:\n+        { Bytecode_defaultvalue bdefaultvalue(caller_method(), caller_method->bcp_from(bci));\n+          k = caller_method->constants()->klass_at(bdefaultvalue.index(), CHECK);\n+        }\n+        break;\n@@ -959,0 +1091,4 @@\n+          if (k->name()->is_Q_array_signature()) {\n+            \/\/ Logically creates elements, ensure klass init\n+            k->initialize(CHECK);\n+          }\n@@ -1473,0 +1609,1 @@\n+  tty->print_cr(\" _new_flat_array_slowcase_cnt:    %d\", _new_flat_array_slowcase_cnt);\n@@ -1475,0 +1612,6 @@\n+  tty->print_cr(\" _load_flattened_array_slowcase_cnt:   %d\", _load_flattened_array_slowcase_cnt);\n+  tty->print_cr(\" _store_flattened_array_slowcase_cnt:  %d\", _store_flattened_array_slowcase_cnt);\n+  tty->print_cr(\" _substitutability_check_slowcase_cnt: %d\", _substitutability_check_slowcase_cnt);\n+  tty->print_cr(\" _buffer_inline_args_slowcase_cnt:%d\", _buffer_inline_args_slowcase_cnt);\n+  tty->print_cr(\" _buffer_inline_args_no_receiver_slowcase_cnt:%d\", _buffer_inline_args_no_receiver_slowcase_cnt);\n+\n@@ -1485,0 +1628,1 @@\n+  tty->print_cr(\" _throw_illegal_monitor_state_exception_count:  %d:\", _throw_illegal_monitor_state_exception_count);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":149,"deletions":5,"binary":false,"changes":154,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -472,1 +473,3 @@\n-      (sym->char_at(1) == JVM_SIGNATURE_ARRAY || sym->char_at(1) == JVM_SIGNATURE_CLASS)) {\n+      (sym->char_at(1) == JVM_SIGNATURE_ARRAY ||\n+       sym->char_at(1) == JVM_SIGNATURE_CLASS ||\n+       sym->char_at(1) == JVM_SIGNATURE_INLINE_TYPE )) {\n@@ -485,1 +488,1 @@\n-      return ciObjArrayKlass::make_impl(elem_klass);\n+      return ciArrayKlass::make(elem_klass);\n@@ -511,0 +514,15 @@\n+  int i = 0;\n+  while (sym->char_at(i) == JVM_SIGNATURE_ARRAY) {\n+    i++;\n+  }\n+  if (i > 0 && sym->char_at(i) == JVM_SIGNATURE_INLINE_TYPE) {\n+    \/\/ An unloaded array class of inline types is an ObjArrayKlass, an\n+    \/\/ unloaded inline type class is an InstanceKlass. For consistency,\n+    \/\/ make the signature of the unloaded array of inline type use L\n+    \/\/ rather than Q.\n+    char *new_name = CURRENT_THREAD_ENV->name_buffer(sym->utf8_length()+1);\n+    strncpy(new_name, (char*)sym->base(), sym->utf8_length());\n+    new_name[i] = JVM_SIGNATURE_CLASS;\n+    new_name[sym->utf8_length()] = '\\0';\n+    return get_unloaded_klass(accessing_klass, ciSymbol::make(new_name));\n+  }\n@@ -541,1 +559,1 @@\n-    klass =  ConstantPool::klass_at_if_loaded(cpool, index);\n+    klass = ConstantPool::klass_at_if_loaded(cpool, index);\n@@ -593,0 +611,8 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciEnv::is_inline_klass\n+\/\/\n+\/\/ Check if the klass is an inline klass.\n+bool ciEnv::is_inline_klass(const constantPoolHandle& cpool, int index) {\n+  GUARDED_VM_ENTRY(return cpool->klass_name_at(index)->is_Q_signature();)\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":29,"deletions":3,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -137,0 +137,2 @@\n+  bool       is_inline_klass(const constantPoolHandle& cpool,\n+                             int klass_index);\n@@ -201,0 +203,4 @@\n+  ciFlatArrayKlass* get_flat_array_klass(Klass* o) {\n+    if (o == NULL) return NULL;\n+    return get_metadata(o)->as_flat_array_klass();\n+  }\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -693,0 +694,33 @@\n+bool ciMethod::array_access_profiled_type(int bci, ciKlass*& array_type, ciKlass*& element_type, ProfilePtrKind& element_ptr, bool &flat_array, bool &null_free_array) {\n+  if (method_data() != NULL && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != NULL && data->is_ArrayLoadStoreData()) {\n+      ciArrayLoadStoreData* array_access = (ciArrayLoadStoreData*)data->as_ArrayLoadStoreData();\n+      array_type = array_access->array()->valid_type();\n+      element_type = array_access->element()->valid_type();\n+      element_ptr = array_access->element()->ptr_kind();\n+      flat_array = array_access->flat_array();\n+      null_free_array = array_access->null_free_array();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ciMethod::acmp_profiled_type(int bci, ciKlass*& left_type, ciKlass*& right_type, ProfilePtrKind& left_ptr, ProfilePtrKind& right_ptr, bool &left_inline_type, bool &right_inline_type) {\n+  if (method_data() != NULL && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != NULL && data->is_ACmpData()) {\n+      ciACmpData* acmp = (ciACmpData*)data->as_ACmpData();\n+      left_type = acmp->left()->valid_type();\n+      right_type = acmp->right()->valid_type();\n+      left_ptr = acmp->left()->ptr_kind();\n+      right_ptr = acmp->right()->ptr_kind();\n+      left_inline_type = acmp->left_inline_type();\n+      right_inline_type = acmp->right_inline_type();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -992,1 +1026,1 @@\n-\/\/ ciMethod::is_object_initializer\n+\/\/ ciMethod::is_object_constructor\n@@ -994,2 +1028,15 @@\n-bool ciMethod::is_object_initializer() const {\n-   return name() == ciSymbol::object_initializer_name();\n+bool ciMethod::is_object_constructor() const {\n+   return (name() == ciSymbol::object_initializer_name()\n+           && signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciMethod::is_static_init_factory\n+\/\/\n+bool ciMethod::is_static_init_factory() const {\n+   return (name() == ciSymbol::object_initializer_name()\n+           && !signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n@@ -1316,1 +1363,1 @@\n-bool ciMethod::is_initializer () const {         FETCH_FLAG_FROM_VM(is_initializer); }\n+bool ciMethod::is_object_constructor_or_class_initializer() const { FETCH_FLAG_FROM_VM(is_object_constructor_or_class_initializer); }\n@@ -1462,0 +1509,1 @@\n+  if (bt == T_INLINE_TYPE)   return T_OBJECT;\n@@ -1549,0 +1597,13 @@\n+\n+bool ciMethod::has_scalarized_args() const {\n+  VM_ENTRY_MARK;\n+  return get_Method()->has_scalarized_args();\n+}\n+\n+const GrowableArray<SigEntry>* ciMethod::get_sig_cc() {\n+  VM_ENTRY_MARK;\n+  if (get_Method()->adapter() == NULL) {\n+    return NULL;\n+  }\n+  return get_Method()->adapter()->get_sig_cc();\n+}\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":65,"deletions":4,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+  ProfileUnknownNull,\n@@ -202,1 +203,1 @@\n-  bool is_static_initializer() const { return get_Method()->is_static_initializer(); }\n+  bool is_class_initializer()  const { return get_Method()->is_class_initializer(); }\n@@ -267,1 +268,4 @@\n-\n+  bool          array_access_profiled_type(int bci, ciKlass*& array_type, ciKlass*& element_type, ProfilePtrKind& element_ptr, bool &flat_array, bool &null_free);\n+  bool          acmp_profiled_type(int bci, ciKlass*& left_type, ciKlass*& right_type,\n+                                   ProfilePtrKind& left_ptr, ProfilePtrKind& right_ptr,\n+                                   bool &left_inline_type, bool &right_inline_type);\n@@ -341,0 +345,1 @@\n+  bool has_vararg     () const                   { return flags().has_vararg(); }\n@@ -354,1 +359,0 @@\n-  bool is_initializer () const;\n@@ -359,1 +363,3 @@\n-  bool is_object_initializer() const;\n+  bool is_object_constructor() const;\n+  bool is_static_init_factory() const;\n+  bool is_object_constructor_or_class_initializer() const;\n@@ -379,0 +385,4 @@\n+\n+  \/\/ Support for the inline type calling convention\n+  bool has_scalarized_args() const;\n+  const GrowableArray<SigEntry>* get_sig_cc();\n","filename":"src\/hotspot\/share\/ci\/ciMethod.hpp","additions":14,"deletions":4,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -90,0 +90,14 @@\n+bool ciSymbol::starts_with(char prefix_char) const {\n+  GUARDED_VM_ENTRY(return get_symbol()->starts_with(prefix_char);)\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciSymbol::ends_with\n+\/\/\n+\/\/ Tests if the symbol ends with the given suffix.\n+bool ciSymbol::ends_with(const char* suffix, int len) const {\n+  GUARDED_VM_ENTRY(return get_symbol()->ends_with(suffix, len);)\n+}\n+bool ciSymbol::ends_with(char suffix_char) const {\n+  GUARDED_VM_ENTRY(return get_symbol()->ends_with(suffix_char);)\n+}\n@@ -109,0 +123,6 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciSymbol::is_Q_signature\n+bool ciSymbol::is_Q_signature() {\n+  GUARDED_VM_ENTRY(return get_symbol()->is_Q_signature();)\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciSymbol.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+\n@@ -53,0 +54,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -85,0 +87,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -138,0 +141,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        60\n+\n@@ -177,1 +182,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -510,1 +515,8 @@\n-        cp->unresolved_klass_at_put(index, class_index, num_klasses++);\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n+        if (name->is_Q_signature()) {\n+          cp->unresolved_qdescriptor_at_put(index, class_index, num_klasses++);\n+        } else {\n+          cp->unresolved_klass_at_put(index, class_index, num_klasses++);\n+        }\n@@ -766,2 +778,2 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+            if (name != vmSymbols::object_initializer_name()) {\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -774,1 +786,12 @@\n-              if (name == vmSymbols::object_initializer_name()) {\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else if (!signature->is_void_method_signature()\n+                         && ref_kind == JVM_REF_invokeStatic) {\n+                \/\/ also OK, could be a static factory call\n+              } else {\n@@ -934,3 +957,4 @@\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n+                                       bool is_inline_type,\n@@ -938,0 +962,7 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n+                                       bool* const is_declared_atomic,\n@@ -944,1 +975,1 @@\n-    _local_interfaces = Universe::the_empty_instance_klass_array();\n+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(0);\n@@ -947,3 +978,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);\n-\n-    int index;\n+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(itfs_len);\n+    int index = 0;\n@@ -967,1 +997,1 @@\n-        \/\/ Call resolve_super so classcircularity is checked\n+        \/\/ Call resolve_super so class circularity is checked\n@@ -985,1 +1015,14 @@\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+      InstanceKlass* ik = InstanceKlass::cast(interf);\n+      if (is_inline_type && ik->invalid_inline_super()) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(\n+          THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_IncompatibleClassChangeError(),\n+          \"Inline type %s attempts to implement interface java.lang.IdentityObject\",\n+          _class_name->as_klass_external_name());\n+        return;\n+      }\n+      if (ik->invalid_inline_super()) {\n+        set_invalid_inline_super();\n+      }\n+      if (ik->has_nonstatic_concrete_methods()) {\n@@ -988,1 +1031,7 @@\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      if (ik->is_declared_atomic()) {\n+        *is_declared_atomic = true;\n+      }\n+      if (ik->name() == vmSymbols::java_lang_IdentityObject()) {\n+        _implements_identityObject = true;\n+      }\n+      _temp_local_interfaces->append(ik);\n@@ -1006,1 +1055,1 @@\n-        const InstanceKlass* const k = _local_interfaces->at(index);\n+        const InstanceKlass* const k = _temp_local_interfaces->at(index);\n@@ -1491,0 +1540,1 @@\n+  STATIC_INLINE,        \/\/ inline type field\n@@ -1496,0 +1546,1 @@\n+  NONSTATIC_INLINE,\n@@ -1515,6 +1566,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  NONSTATIC_OOP,       \/\/ T_INLINE_TYPE = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20,\n@@ -1535,6 +1587,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  STATIC_OOP,          \/\/ T_INLINE_TYPE = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20\n@@ -1543,1 +1596,1 @@\n-static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type) {\n+static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type, bool is_inline_type) {\n@@ -1547,0 +1600,3 @@\n+  if (is_inline_type) {\n+    result = is_static ? STATIC_INLINE : NONSTATIC_INLINE;\n+  }\n@@ -1560,2 +1616,2 @@\n-  FieldAllocationType update(bool is_static, BasicType type) {\n-    FieldAllocationType atype = basic_type_to_atype(is_static, type);\n+  FieldAllocationType update(bool is_static, BasicType type, bool is_inline_type) {\n+    FieldAllocationType atype = basic_type_to_atype(is_static, type, is_inline_type);\n@@ -1575,0 +1631,1 @@\n+                                   bool is_inline_type,\n@@ -1597,1 +1654,5 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);\n@@ -1627,0 +1688,1 @@\n+  int instance_fields_count = 0;\n@@ -1631,0 +1693,4 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, is_interface, is_inline_type, CHECK);\n@@ -1632,2 +1698,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1649,0 +1713,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1708,1 +1773,1 @@\n-    const FieldAllocationType atype = fac->update(is_static, type);\n+    const FieldAllocationType atype = fac->update(is_static, type, type == T_INLINE_TYPE);\n@@ -1753,1 +1818,1 @@\n-      const FieldAllocationType atype = fac->update(false, type);\n+      const FieldAllocationType atype = fac->update(false, type, false);\n@@ -1759,0 +1824,29 @@\n+  if (is_inline_type) {\n+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);\n+    field->initialize(JVM_ACC_FIELD_INTERNAL | JVM_ACC_STATIC,\n+                      vmSymbols::default_value_name_enum,\n+                      vmSymbols::object_signature_enum,\n+                      0);\n+    const BasicType type = Signature::basic_type(vmSymbols::object_signature());\n+    const FieldAllocationType atype = fac->update(true, type, false);\n+    field->set_allocation_type(atype);\n+    index++;\n+  }\n+\n+  if (is_inline_type && instance_fields_count == 0) {\n+    _is_empty_inline_type = true;\n+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);\n+    field->initialize(JVM_ACC_FIELD_INTERNAL,\n+        vmSymbols::empty_marker_name_enum,\n+        vmSymbols::byte_signature_enum,\n+        0);\n+    const BasicType type = Signature::basic_type(vmSymbols::byte_signature());\n+    const FieldAllocationType atype = fac->update(false, type, false);\n+    field->set_allocation_type(atype);\n+    index++;\n+  }\n+\n+  if (instance_fields_count > 0) {\n+    _has_nonstatic_fields = true;\n+  }\n+\n@@ -2076,0 +2170,5 @@\n+  const char* class_note = \"\";\n+  if (is_inline_type() && name == vmSymbols::object_initializer_name()) {\n+    class_note = \" (an inline class)\";\n+  }\n+\n@@ -2079,2 +2178,2 @@\n-      \"%s \\\"%s\\\" in class %s has illegal signature \\\"%s\\\"\", type,\n-      name->as_C_string(), _class_name->as_C_string(), sig->as_C_string());\n+      \"%s \\\"%s\\\" in class %s%s has illegal signature \\\"%s\\\"\", type,\n+      name->as_C_string(), _class_name->as_C_string(), class_note, sig->as_C_string());\n@@ -2348,0 +2447,1 @@\n+                                      bool is_inline_type,\n@@ -2389,1 +2489,1 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n+    verify_legal_method_modifiers(flags, is_interface, is_inline_type, name, CHECK_NULL);\n@@ -2392,3 +2492,48 @@\n-  if (name == vmSymbols::object_initializer_name() && is_interface) {\n-    classfile_parse_error(\"Interface cannot have a method named <init>, class file %s\", THREAD);\n-    return NULL;\n+  if (name == vmSymbols::object_initializer_name()) {\n+    if (is_interface) {\n+      classfile_parse_error(\"Interface cannot have a method named <init>, class file %s\", THREAD);\n+      return NULL;\n+    } else if (!is_inline_type && signature->is_void_method_signature()) {\n+      \/\/ OK, a constructor\n+    } else if (is_inline_type && !signature->is_void_method_signature()) {\n+      \/\/ also OK, a static factory, as long as the return value is good\n+      bool ok = false;\n+      SignatureStream ss((Symbol*) signature, true);\n+      while (!ss.at_return_type())  ss.next();\n+      if (ss.is_reference()) {\n+        Symbol* ret = ss.as_symbol();\n+        const Symbol* required = class_name();\n+        if (is_hidden()) {\n+          \/\/ The original class name in hidden classes gets changed.  So using\n+          \/\/ the original name in the return type is no longer valid.\n+          \/\/ Note that expecting the return type for inline hidden class factory\n+          \/\/ methods to be java.lang.Object works around a JVM Spec issue for\n+          \/\/ hidden classes.\n+          required = vmSymbols::java_lang_Object();\n+        }\n+        ok = (ret == required);\n+      }\n+      if (!ok) {\n+        throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+      }\n+    } else {\n+      \/\/ not OK, so throw the same error as in verify_legal_method_signature.\n+      throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+    }\n+    \/\/ A declared <init> method must always be either a non-static\n+    \/\/ object constructor, with a void return, or else it must be a\n+    \/\/ static factory method, with a non-void return.  No other\n+    \/\/ definition of <init> is possible.\n+    \/\/\n+    \/\/ The verifier (in verify_invoke_instructions) will inspect the\n+    \/\/ signature of any attempt to invoke <init>, and ensures that it\n+    \/\/ returns non-void if and only if it is being invoked by\n+    \/\/ invokestatic, and void if and only if it is being invoked by\n+    \/\/ invokespecial.\n+    \/\/\n+    \/\/ When a symbolic reference to <init> is resolved for a\n+    \/\/ particular invocation mode (special or static), the mode is\n+    \/\/ matched to the JVM_ACC_STATIC modifier of the <init> method.\n+    \/\/ Thus, it is impossible to statically invoke a constructor, and\n+    \/\/ impossible to \"new + invokespecial\" a static factory, either\n+    \/\/ through bytecode or through reflection.\n@@ -2962,0 +3107,1 @@\n+                                    bool is_inline_type,\n@@ -2986,0 +3132,1 @@\n+                                    is_inline_type,\n@@ -3178,2 +3325,2 @@\n-    \/\/ Access flags\n-    jint flags;\n+\n+    jint recognized_modifiers = RECOGNIZED_INNER_CLASS_MODIFIERS;\n@@ -3182,3 +3329,1 @@\n-      flags = cfs->get_u2_fast() & (RECOGNIZED_INNER_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-    } else {\n-      flags = cfs->get_u2_fast() & RECOGNIZED_INNER_CLASS_MODIFIERS;\n+      recognized_modifiers |= JVM_ACC_MODULE;\n@@ -3186,0 +3331,8 @@\n+    \/\/ JVM_ACC_INLINE is defined for class file version 55 and later\n+    if (supports_inline_types()) {\n+      recognized_modifiers |= JVM_ACC_INLINE;\n+    }\n+\n+    \/\/ Access flags\n+    jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+\n@@ -3565,3 +3718,2 @@\n-  return _major_version == JVM_CLASSFILE_MAJOR_VERSION &&\n-         _minor_version == JAVA_PREVIEW_MINOR_VERSION &&\n-         Arguments::enable_preview();\n+  \/\/ temporarily disable the sealed type preview feature check\n+  return _major_version == JVM_CLASSFILE_MAJOR_VERSION;\n@@ -4075,1 +4227,2 @@\n-    check_property(_class_name == vmSymbols::java_lang_Object(),\n+    check_property(_class_name == vmSymbols::java_lang_Object()\n+                   || (_access_flags.get_flags() & JVM_ACC_INLINE),\n@@ -4218,0 +4371,19 @@\n+void ClassFileParser::throwInlineTypeLimitation(THREAD_AND_LOCATION_DECL,\n+                                                const char* msg,\n+                                                const Symbol* name,\n+                                                const Symbol* sig) const {\n+\n+  ResourceMark rm(THREAD);\n+  if (name == NULL || sig == NULL) {\n+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"class: %s - %s\", _class_name->as_C_string(), msg);\n+  }\n+  else {\n+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"\\\"%s\\\" sig: \\\"%s\\\" class: %s - %s\", name->as_C_string(), sig->as_C_string(),\n+        _class_name->as_C_string(), msg);\n+  }\n+}\n+\n@@ -4251,0 +4423,5 @@\n+      if (ik->is_inline_klass()) {\n+        Thread *THREAD = Thread::current();\n+        throwInlineTypeLimitation(THREAD_AND_LOCATION, \"Inline Types do not support Cloneable\");\n+        return;\n+      }\n@@ -4291,0 +4468,5 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 55 and later\n+  return _major_version >= JAVA_11_VERSION;\n+}\n+\n@@ -4334,3 +4516,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4357,0 +4540,5 @@\n+\n+    if (length == 1 && result->at(0) == SystemDictionary::IdentityObject_klass()) {\n+      return Universe::the_single_IdentityObject_klass_array();\n+    }\n+\n@@ -4579,0 +4767,1 @@\n+  const bool is_inline_type = (flags & JVM_ACC_INLINE) != 0;\n@@ -4580,0 +4769,1 @@\n+  assert(supports_inline_types() || !is_inline_type, \"JVM_ACC_INLINE should not be set\");\n@@ -4590,0 +4780,11 @@\n+  if (is_inline_type && !EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    Exceptions::fthrow(\n+      THREAD_AND_LOCATION,\n+      vmSymbols::java_lang_ClassFormatError(),\n+      \"Class modifier ACC_INLINE in class %s requires option -XX:+EnableValhalla\",\n+      _class_name->as_C_string()\n+    );\n+    return;\n+  }\n+\n@@ -4604,1 +4805,2 @@\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (is_inline_type && (is_interface || is_abstract || is_enum || !is_final))) {\n@@ -4606,0 +4808,2 @@\n+    const char* class_note = \"\";\n+    if (is_inline_type)  class_note = \" (an inline class)\";\n@@ -4609,2 +4813,2 @@\n-      \"Illegal class modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags\n+      \"Illegal class modifiers in class %s%s: 0x%X\",\n+      _class_name->as_C_string(), class_note, flags\n@@ -4680,0 +4884,1 @@\n+                                                   bool is_inline_type,\n@@ -4704,0 +4909,4 @@\n+    } else {\n+      if (is_inline_type && !is_static && !is_final) {\n+        is_illegal = true;\n+      }\n@@ -4720,0 +4929,1 @@\n+                                                    bool is_inline_type,\n@@ -4740,0 +4950,1 @@\n+  const char* class_note = \"\";\n@@ -4774,1 +4985,1 @@\n-        if (is_static || is_final || is_synchronized || is_native ||\n+        if (is_final || is_synchronized || is_native ||\n@@ -4778,0 +4989,9 @@\n+        if (!is_static && !is_inline_type) {\n+          \/\/ OK, an object constructor in a regular class\n+        } else if (is_static && is_inline_type) {\n+          \/\/ OK, a static init factory in an inline class\n+        } else {\n+          \/\/ but no other combinations are allowed\n+          is_illegal = true;\n+          class_note = (is_inline_type ? \" (an inline class)\" : \" (not an inline class)\");\n+        }\n@@ -4779,4 +4999,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || is_strict)))) {\n-            is_illegal = true;\n+        if (is_inline_type && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (an inline class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || is_strict)))) {\n+              is_illegal = true;\n+            }\n@@ -4794,2 +5019,2 @@\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+      \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+      name->as_C_string(), _class_name->as_C_string(), class_note, flags);\n@@ -4953,1 +5178,10 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_INLINE_TYPE:\n+      \/\/ Can't enable this check until JDK upgrades the bytecode generators\n+      \/\/ if (_major_version < CONSTANT_CLASS_DESCRIPTORS ) {\n+      \/\/   classfile_parse_error(\"Class name contains illegal Q-signature \"\n+      \/\/                                    \"in descriptor in class file %s\",\n+      \/\/                                    CHECK_0);\n+      \/\/ }\n+      \/\/ fall through\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -4964,1 +5198,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -5020,0 +5254,3 @@\n+    } else if (_major_version >= CONSTANT_CLASS_DESCRIPTORS && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; and Q...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -5117,0 +5354,3 @@\n+  if (!supports_inline_types() && (signature->is_Q_signature() || signature->is_Q_array_signature())) {\n+    throwIllegalSignature(\"Field\", name, signature, CHECK);\n+  }\n@@ -5169,1 +5409,1 @@\n-        \/\/ All internal methods must return void\n+        \/\/ All constructor methods must return void\n@@ -5173,0 +5413,16 @@\n+        \/\/ All static init methods must return the current class\n+        if ((length >= 3) && (p[length-1] == JVM_SIGNATURE_ENDCLASS)\n+            && name == vmSymbols::object_initializer_name()) {\n+          nextp = skip_over_field_signature(p, true, length, CHECK_0);\n+          if (nextp && ((int)length == (nextp - p))) {\n+            \/\/ The actual class will be checked against current class\n+            \/\/ when the method is defined (see parse_method).\n+            \/\/ A reference to a static init with a bad return type\n+            \/\/ will load and verify OK, but will fail to link.\n+            return args_size;\n+          }\n+        }\n+        \/\/ The distinction between static factory methods and\n+        \/\/ constructors depends on the JVM_ACC_STATIC modifier.\n+        \/\/ This distinction must be reflected in a void or non-void\n+        \/\/ return. For declared methods, the check is in parse_method.\n@@ -5330,0 +5586,6 @@\n+  if (ik->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    oop val = ik->allocate_instance(CHECK_NULL);\n+    vk->set_default_value(val);\n+  }\n+\n@@ -5333,0 +5595,34 @@\n+\/\/ Return true if the specified class is not a valid super class for an inline type.\n+\/\/ A valid super class for an inline type is abstract, has no instance fields,\n+\/\/ does not implement interface java.lang.IdentityObject (checked elsewhere), has\n+\/\/ an empty body-less no-arg constructor, and no synchronized instance methods.\n+\/\/ This function doesn't check if the class's super types are invalid.  Those checks\n+\/\/ are done elsewhere.  The final determination of whether or not a class is an\n+\/\/ invalid super type for an inline class is done in fill_instance_klass().\n+bool ClassFileParser::is_invalid_super_for_inline_type() {\n+  if (class_name() == vmSymbols::java_lang_IdentityObject()) {\n+    return true;\n+  }\n+  if (is_interface() || class_name() == vmSymbols::java_lang_Object()) {\n+    return false;\n+  }\n+  if (!access_flags().is_abstract() || _has_nonstatic_fields) {\n+    return true;\n+  } else {\n+    \/\/ Look at each method\n+    for (int x = 0; x < _methods->length(); x++) {\n+      const Method* const method = _methods->at(x);\n+      if (method->is_synchronized() && !method->is_static()) {\n+        return true;\n+\n+      } else if (method->name() == vmSymbols::object_initializer_name()) {\n+        if (method->signature() != vmSymbols::void_method_signature() ||\n+            !method->is_vanilla_constructor()) {\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -5365,0 +5661,12 @@\n+  if (_field_info->_is_naturally_atomic && ik->is_inline_klass()) {\n+    ik->set_is_naturally_atomic();\n+  }\n+\n+  if (this->_invalid_inline_super) {\n+    ik->set_invalid_inline_super();\n+  }\n+\n+  if (_has_injected_identityObject) {\n+    ik->set_has_injected_identityObject();\n+  }\n+\n@@ -5366,1 +5674,1 @@\n-  ik->set_static_oop_field_count(_fac->count[STATIC_OOP]);\n+  ik->set_static_oop_field_count(_fac->count[STATIC_OOP] + _fac->count[STATIC_INLINE]);\n@@ -5416,0 +5724,3 @@\n+  if (_is_declared_atomic) {\n+    ik->set_is_declared_atomic();\n+  }\n@@ -5527,0 +5838,37 @@\n+  bool all_fields_empty = true;\n+  int nfields = ik->java_fields_count();\n+  if (ik->is_inline_klass()) nfields++;\n+  for (int i = 0; i < nfields; i++) {\n+    if (((ik->field_access_flags(i) & JVM_ACC_STATIC) == 0)) {\n+      if (ik->field_is_inline_type(i)) {\n+        Symbol* klass_name = ik->field_signature(i)->fundamental_name(CHECK);\n+        \/\/ Inline classes for instance fields must have been pre-loaded\n+        \/\/ Inline classes for static fields might not have been loaded yet\n+        Klass* klass = SystemDictionary::find(klass_name,\n+            Handle(THREAD, ik->class_loader()),\n+            Handle(THREAD, ik->protection_domain()), CHECK);\n+        assert(klass != NULL, \"Just checking\");\n+        assert(klass->access_flags().is_inline_type(), \"Inline type expected\");\n+        ik->set_inline_type_field_klass(i, klass);\n+        klass_name->decrement_refcount();\n+        if (!InlineKlass::cast(klass)->is_empty_inline_type()) { all_fields_empty = false; }\n+      } else {\n+        all_fields_empty = false;\n+      }\n+    } else if (is_inline_type() && ((ik->field_access_flags(i) & JVM_ACC_FIELD_INTERNAL) != 0)) {\n+      InlineKlass::cast(ik)->set_default_value_offset(ik->field_offset(i));\n+    }\n+  }\n+\n+  if (_is_empty_inline_type || (is_inline_type() && all_fields_empty)) {\n+    ik->set_is_empty_inline_type();\n+  }\n+\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_alignment(_alignment);\n+    vk->set_first_field_offset(_first_field_offset);\n+    vk->set_exact_size_in_bytes(_exact_size_in_bytes);\n+    InlineKlass::cast(ik)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -5572,0 +5920,4 @@\n+  if (ik->name() == vmSymbols::java_lang_IdentityObject()) {\n+    Universe::initialize_the_single_IdentityObject_klass_array(ik, CHECK);\n+  }\n+\n@@ -5674,0 +6026,1 @@\n+  _temp_local_interfaces(NULL),\n@@ -5713,0 +6066,9 @@\n+  _has_inline_type_fields(false),\n+  _has_nonstatic_fields(false),\n+  _is_empty_inline_type(false),\n+  _is_naturally_atomic(false),\n+  _is_declared_atomic(false),\n+  _invalid_inline_super(false),\n+  _invalid_identity_super(false),\n+  _implements_identityObject(false),\n+  _has_injected_identityObject(false),\n@@ -5923,2 +6285,1 @@\n-  \/\/ Access flags\n-  jint flags;\n+  jint recognized_modifiers = JVM_RECOGNIZED_CLASS_MODIFIERS;\n@@ -5927,3 +6288,5 @@\n-    flags = stream->get_u2_fast() & (JVM_RECOGNIZED_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-  } else {\n-    flags = stream->get_u2_fast() & JVM_RECOGNIZED_CLASS_MODIFIERS;\n+    recognized_modifiers |= JVM_ACC_MODULE;\n+  }\n+  \/\/ JVM_ACC_INLINE is defined for class file version 55 and later\n+  if (supports_inline_types()) {\n+    recognized_modifiers |= JVM_ACC_INLINE;\n@@ -5932,0 +6295,3 @@\n+  \/\/ Access flags\n+  jint flags = stream->get_u2_fast() & recognized_modifiers;\n+\n@@ -6052,0 +6418,1 @@\n+                   is_inline_type(),\n@@ -6053,0 +6420,1 @@\n+                   &_is_declared_atomic,\n@@ -6055,1 +6423,1 @@\n-  assert(_local_interfaces != NULL, \"invariant\");\n+  assert(_temp_local_interfaces != NULL, \"invariant\");\n@@ -6060,1 +6428,2 @@\n-               _access_flags.is_interface(),\n+               is_interface(),\n+               is_inline_type(),\n@@ -6072,1 +6441,2 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                is_inline_type(),\n@@ -6154,3 +6524,3 @@\n-    check_property(_local_interfaces == Universe::the_empty_instance_klass_array(),\n-                   \"java.lang.Object cannot implement an interface in class file %s\",\n-                   CHECK);\n+    check_property(_temp_local_interfaces->length() == 0,\n+        \"java.lang.Object cannot implement an interface in class file %s\",\n+        CHECK);\n@@ -6161,1 +6531,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -6182,0 +6552,3 @@\n+    if (_super_klass->is_declared_atomic()) {\n+      _is_declared_atomic = true;\n+    }\n@@ -6187,0 +6560,54 @@\n+\n+    \/\/ For an inline class, only java\/lang\/Object or special abstract classes\n+    \/\/ are acceptable super classes.\n+    if (is_inline_type()) {\n+      const InstanceKlass* super_ik = _super_klass;\n+      if (super_ik->invalid_inline_super()) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(\n+          THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_IncompatibleClassChangeError(),\n+          \"inline class %s has an invalid super class %s\",\n+          _class_name->as_klass_external_name(),\n+          _super_klass->external_name());\n+        return;\n+      }\n+    }\n+  }\n+\n+  if (_class_name == vmSymbols::java_lang_NonTearable() && _loader_data->class_loader() == NULL) {\n+    \/\/ This is the original source of this condition.\n+    \/\/ It propagates by inheritance, as if testing \"instanceof NonTearable\".\n+    _is_declared_atomic = true;\n+  } else if (*ForceNonTearable != '\\0') {\n+    \/\/ Allow a command line switch to force the same atomicity property:\n+    const char* class_name_str = _class_name->as_C_string();\n+    if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+      _is_declared_atomic = true;\n+    }\n+  }\n+\n+  \/\/ Set ik->invalid_inline_super field to TRUE if already marked as invalid,\n+  \/\/ if super is marked invalid, or if is_invalid_super_for_inline_type()\n+  \/\/ returns true\n+  if (invalid_inline_super() ||\n+      (_super_klass != NULL && _super_klass->invalid_inline_super()) ||\n+      is_invalid_super_for_inline_type()) {\n+    set_invalid_inline_super();\n+  }\n+\n+  if (!is_inline_type() && invalid_inline_super() && (_super_klass == NULL || !_super_klass->invalid_inline_super())\n+      && !_implements_identityObject && class_name() != vmSymbols::java_lang_IdentityObject()) {\n+    _temp_local_interfaces->append(SystemDictionary::IdentityObject_klass());\n+    _has_injected_identityObject = true;\n+  }\n+  int itfs_len = _temp_local_interfaces->length();\n+  if (itfs_len == 0) {\n+    _local_interfaces = Universe::the_empty_instance_klass_array();\n+  } else if (itfs_len == 1 && _temp_local_interfaces->at(0) == SystemDictionary::IdentityObject_klass()) {\n+    _local_interfaces = Universe::the_single_IdentityObject_klass_array();\n+  } else {\n+    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);\n+    for (int i = 0; i < itfs_len; i++) {\n+      _local_interfaces->at_put(i, _temp_local_interfaces->at(i));\n+    }\n@@ -6188,0 +6615,2 @@\n+  _temp_local_interfaces = NULL;\n+  assert(_local_interfaces != NULL, \"invariant\");\n@@ -6217,1 +6646,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -6223,0 +6652,19 @@\n+\n+  for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {\n+    if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE  && !fs.access_flags().is_static()) {\n+      \/\/ Pre-load inline class\n+      Klass* klass = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+          Handle(THREAD, _loader_data->class_loader()),\n+          _protection_domain, true, CHECK);\n+      assert(klass != NULL, \"Sanity check\");\n+      if (!klass->access_flags().is_inline_type()) {\n+        assert(klass->is_instance_klass(), \"Sanity check\");\n+        ResourceMark rm(THREAD);\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"Class %s expects class %s to be an inline type, but it is not\",\n+                    _class_name->as_C_string(),\n+                    InstanceKlass::cast(klass)->external_name()));\n+      }\n+    }\n+  }\n+\n@@ -6225,2 +6673,9 @@\n-                        _parsed_annotations->is_contended(), _field_info);\n-  lb.build_layout();\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      loader_data(), _protection_domain, _field_info);\n+  lb.build_layout(CHECK);\n+  if (is_inline_type()) {\n+    _alignment = lb.get_alignment();\n+    _first_field_offset = lb.get_first_field_offset();\n+    _exact_size_in_bytes = lb.get_exact_size_in_byte();\n+  }\n+  _has_inline_type_fields = _field_info->_has_inline_fields;\n@@ -6228,1 +6683,1 @@\n-  \/\/ Compute reference typ\n+  \/\/ Compute reference type\n@@ -6230,1 +6685,0 @@\n-\n@@ -6262,0 +6716,1 @@\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":540,"deletions":85,"binary":false,"changes":625,"status":"modified"},{"patch":"@@ -78,0 +78,2 @@\n+  bool  _is_naturally_atomic;\n+  bool _has_inline_fields;\n@@ -137,0 +139,1 @@\n+  GrowableArray<InstanceKlass*>* _temp_local_interfaces;\n@@ -162,0 +165,4 @@\n+  int _alignment;\n+  int _first_field_offset;\n+  int _exact_size_in_bytes;\n+\n@@ -200,0 +207,10 @@\n+  bool _has_inline_type_fields;\n+  bool _has_nonstatic_fields;\n+  bool _is_empty_inline_type;\n+  bool _is_naturally_atomic;\n+  bool _is_declared_atomic;\n+  bool _invalid_inline_super;   \/\/ if true, invalid super type for an inline type.\n+  bool _invalid_identity_super; \/\/ if true, invalid super type for an identity type.\n+  bool _implements_identityObject;\n+  bool _has_injected_identityObject;\n+\n@@ -249,0 +266,1 @@\n+                        bool is_inline_type,\n@@ -250,0 +268,1 @@\n+                        bool* is_declared_atomic,\n@@ -270,0 +289,1 @@\n+                    bool is_inline_type,\n@@ -279,0 +299,1 @@\n+                       bool is_inline_type,\n@@ -285,0 +306,1 @@\n+                     bool is_inline_type,\n@@ -456,0 +478,5 @@\n+  void throwInlineTypeLimitation(THREAD_AND_LOCATION_DECL,\n+                                 const char* msg,\n+                                 const Symbol* name = NULL,\n+                                 const Symbol* sig  = NULL) const;\n+\n@@ -476,1 +503,4 @@\n-  void verify_legal_field_modifiers(jint flags, bool is_interface, TRAPS) const;\n+  void verify_legal_field_modifiers(jint flags,\n+                                    bool is_interface,\n+                                    bool is_inline_type,\n+                                    TRAPS) const;\n@@ -479,0 +509,1 @@\n+                                     bool is_inline_type,\n@@ -556,0 +587,3 @@\n+  \/\/ Check if the class file supports inline types\n+  bool supports_inline_types() const;\n+\n@@ -584,0 +618,10 @@\n+  bool is_inline_type() const { return _access_flags.is_inline_type(); }\n+  bool is_value_capable_class() const;\n+  bool has_inline_fields() const { return _has_inline_type_fields; }\n+  bool invalid_inline_super() const { return _invalid_inline_super; }\n+  void set_invalid_inline_super() { _invalid_inline_super = true; }\n+  bool invalid_identity_super() const { return _invalid_identity_super; }\n+  void set_invalid_identity_super() { _invalid_identity_super = true; }\n+  bool is_invalid_super_for_inline_type();\n+\n+  u2 java_fields_count() const { return _java_fields_count; }\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":45,"deletions":1,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"classfile\/vmSymbols.hpp\"\n@@ -315,1 +316,27 @@\n-    if (k->local_interfaces()->length() != _interfaces->length()) {\n+    int actual_num_interfaces = k->local_interfaces()->length();\n+    int specified_num_interfaces = _interfaces->length();\n+    int expected_num_interfaces, i;\n+\n+    bool identity_object_implemented = false;\n+    bool identity_object_specified = false;\n+    for (i = 0; i < actual_num_interfaces; i++) {\n+      if (k->local_interfaces()->at(i) == SystemDictionary::IdentityObject_klass()) {\n+        identity_object_implemented = true;\n+        break;\n+      }\n+    }\n+    for (i = 0; i < specified_num_interfaces; i++) {\n+      if (lookup_class_by_id(_interfaces->at(i)) == SystemDictionary::IdentityObject_klass()) {\n+        identity_object_specified = true;\n+        break;\n+      }\n+    }\n+\n+    expected_num_interfaces = actual_num_interfaces;\n+    if (identity_object_implemented  && !identity_object_specified) {\n+      \/\/ Backwards compatibility -- older classlists do not know about\n+      \/\/ java.lang.IdentityObject.\n+      expected_num_interfaces--;\n+    }\n+\n+    if (specified_num_interfaces != expected_num_interfaces) {\n@@ -319,1 +346,1 @@\n-            _interfaces->length(), k->local_interfaces()->length());\n+            specified_num_interfaces, expected_num_interfaces);\n@@ -450,0 +477,6 @@\n+  if (interface_name == vmSymbols::java_lang_IdentityObject()) {\n+    \/\/ Backwards compatibility -- older classlists do not know about\n+    \/\/ java.lang.IdentityObject.\n+    return SystemDictionary::IdentityObject_klass();\n+  }\n+\n","filename":"src\/hotspot\/share\/classfile\/classListParser.cpp","additions":35,"deletions":2,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -380,0 +381,10 @@\n+void ClassLoaderData::inline_classes_do(void f(InlineKlass*)) {\n+  \/\/ Lock-free access requires load_acquire\n+  for (Klass* k = Atomic::load_acquire(&_klasses); k != NULL; k = k->next_link()) {\n+    if (k->is_inline_klass()) {\n+      f(InlineKlass::cast(k));\n+    }\n+    assert(k != k->next_link(), \"no loops!\");\n+  }\n+}\n+\n@@ -546,0 +557,2 @@\n+  inline_classes_do(InlineKlass::cleanup);\n+\n@@ -840,1 +853,5 @@\n-        MetadataFactory::free_metadata(this, (InstanceKlass*)m);\n+        if (!((Klass*)m)->is_inline_klass()) {\n+          MetadataFactory::free_metadata(this, (InstanceKlass*)m);\n+        } else {\n+          MetadataFactory::free_metadata(this, (InlineKlass*)m);\n+        }\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.cpp","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -188,0 +188,1 @@\n+  void inline_classes_do(void f(InlineKlass*));\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -46,0 +46,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -47,1 +49,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -1006,1 +1008,6 @@\n-      if (k->is_typeArray_klass()) {\n+      if (k->is_flatArray_klass()) {\n+        Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+        assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+        InlineKlass* vk = InlineKlass::cast(InstanceKlass::cast(element_klass));\n+        comp_mirror = Handle(THREAD, vk->java_mirror());\n+      } else if (k->is_typeArray_klass()) {\n@@ -1105,0 +1112,1 @@\n+      case T_INLINE_TYPE:\n@@ -1188,0 +1196,6 @@\n+  if (k->is_inline_klass()) {\n+    \/\/ Inline types have a val type mirror and a ref type mirror. Don't handle this for now. TODO:CDS\n+    k->clear_java_mirror_handle();\n+    return NULL;\n+  }\n+\n@@ -1517,0 +1531,1 @@\n+  bool is_value = false;\n@@ -1522,0 +1537,1 @@\n+    is_value = k->is_inline_klass();\n@@ -1528,1 +1544,7 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    if (is_value) {\n+      st->print(\"Q\");\n+    } else {\n+      st->print(\"L\");\n+    }\n+  }\n@@ -1550,1 +1572,1 @@\n-      int         siglen = (int) strlen(sigstr);\n+      int siglen = (int) strlen(sigstr);\n@@ -2515,2 +2537,2 @@\n-      \/\/ This is simlar to classic VM.\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      \/\/ This is similar to classic VM (before HotSpot).\n+      if (method->is_object_constructor() &&\n@@ -3890,1 +3912,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n@@ -4692,0 +4714,71 @@\n+\/\/ jdk_internal_vm_jni_SubElementSelector\n+\n+int jdk_internal_vm_jni_SubElementSelector::_arrayElementType_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_subElementType_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_offset_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_isInlined_offset;\n+int jdk_internal_vm_jni_SubElementSelector::_isInlineType_offset;\n+\n+#define SUBELEMENT_SELECTOR_FIELDS_DO(macro) \\\n+  macro(_arrayElementType_offset,  k, \"arrayElementType\", class_signature, false); \\\n+  macro(_subElementType_offset,    k, \"subElementType\",   class_signature, false); \\\n+  macro(_offset_offset,            k, \"offset\",           int_signature,   false); \\\n+  macro(_isInlined_offset,         k, \"isInlined\",        bool_signature,  false); \\\n+  macro(_isInlineType_offset,      k, \"isInlineType\",     bool_signature,  false);\n+\n+void jdk_internal_vm_jni_SubElementSelector::compute_offsets() {\n+  InstanceKlass* k = SystemDictionary::jdk_internal_vm_jni_SubElementSelector_klass();\n+  SUBELEMENT_SELECTOR_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+}\n+\n+#if INCLUDE_CDS\n+void jdk_internal_vm_jni_SubElementSelector::serialize_offsets(SerializeClosure* f) {\n+  SUBELEMENT_SELECTOR_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+#undef SUBELEMENT_SELECTOR_FIELDS_DO\n+\n+Symbol* jdk_internal_vm_jni_SubElementSelector::symbol() {\n+  return vmSymbols::jdk_internal_vm_jni_SubElementSelector();\n+}\n+\n+oop jdk_internal_vm_jni_SubElementSelector::getArrayElementType(oop obj) {\n+  return obj->obj_field(_arrayElementType_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setArrayElementType(oop obj, oop type) {\n+  obj->obj_field_put(_arrayElementType_offset, type);\n+}\n+\n+oop jdk_internal_vm_jni_SubElementSelector::getSubElementType(oop obj) {\n+  return obj->obj_field(_subElementType_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setSubElementType(oop obj, oop type) {\n+  obj->obj_field_put(_subElementType_offset, type);\n+}\n+\n+int jdk_internal_vm_jni_SubElementSelector::getOffset(oop obj) {\n+  return obj->int_field(_offset_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setOffset(oop obj, int offset) {\n+  obj->int_field_put(_offset_offset, offset);\n+}\n+\n+bool jdk_internal_vm_jni_SubElementSelector::getIsInlined(oop obj) {\n+  return obj->bool_field(_isInlined_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setIsInlined(oop obj, bool b) {\n+  obj->bool_field_put(_isInlined_offset, b);\n+}\n+\n+bool jdk_internal_vm_jni_SubElementSelector::getIsInlineType(oop obj) {\n+  return obj->bool_field(_isInlineType_offset);\n+}\n+\n+void jdk_internal_vm_jni_SubElementSelector::setIsInlineType(oop obj, bool b) {\n+  obj->bool_field_put(_isInlineType_offset, b);\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":100,"deletions":7,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -76,0 +76,1 @@\n+class AllFieldStream;\n@@ -106,0 +107,1 @@\n+  do_klass(IdentityObject_klass,                        java_lang_IdentityObject                              ) \\\n@@ -177,0 +179,1 @@\n+  do_klass(ValueBootstrapMethods_klass,                 java_lang_invoke_ValueBootstrapMethods                ) \\\n@@ -226,0 +229,1 @@\n+  do_klass(jdk_internal_vm_jni_SubElementSelector_klass, jdk_internal_vm_jni_SubElementSelector               ) \\\n@@ -279,0 +283,6 @@\n+  static Klass* resolve_inline_type_field_or_fail(AllFieldStream* fs,\n+                                                  Handle class_loader,\n+                                                  Handle protection_domain,\n+                                                  bool throw_error,\n+                                                  TRAPS);\n+\n@@ -368,0 +378,1 @@\n+  static InstanceKlass* check_klass_ValhallaClasses(InstanceKlass* k) { return k; }\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -339,0 +339,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -348,0 +350,1 @@\n+  case vmIntrinsics::_getValue:\n@@ -357,0 +360,1 @@\n+  case vmIntrinsics::_putValue:\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -528,0 +528,2 @@\n+  do_signature(getValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;)Ljava\/lang\/Object;\")                   \\\n+  do_signature(putValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;Ljava\/lang\/Object;)V\")                  \\\n@@ -538,0 +540,3 @@\n+  do_name(getValue_name,\"getValue\")             do_name(putValue_name,\"putValue\")                                       \\\n+  do_name(makePrivateBuffer_name,\"makePrivateBuffer\")                                                                   \\\n+  do_name(finishPrivateBuffer_name,\"finishPrivateBuffer\")                                                               \\\n@@ -548,0 +553,1 @@\n+  do_intrinsic(_getValue,           jdk_internal_misc_Unsafe,     getValue_name, getValue_signature,             F_RN)  \\\n@@ -557,0 +563,4 @@\n+  do_intrinsic(_putValue,           jdk_internal_misc_Unsafe,     putValue_name, putValue_signature,             F_RN)  \\\n+                                                                                                                        \\\n+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \\\n+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+  template(java_lang_IdentityObject,                  \"java\/lang\/IdentityObject\")                 \\\n@@ -67,0 +68,1 @@\n+  template(java_lang_NonTearable,                     \"java\/lang\/NonTearable\")                    \\\n@@ -462,0 +464,2 @@\n+  template(default_value_name,                        \".default\")                                 \\\n+  template(empty_marker_name,                         \".empty\")                                   \\\n@@ -532,0 +536,1 @@\n+  template(object_object_boolean_signature,           \"(Ljava\/lang\/Object;Ljava\/lang\/Object;)Z\") \\\n@@ -679,0 +684,5 @@\n+  template(java_lang_invoke_ValueBootstrapMethods, \"java\/lang\/invoke\/ValueBootstrapMethods\")                      \\\n+  template(isSubstitutable_name,                   \"isSubstitutable\")                                             \\\n+  template(inlineObjectHashCode_name,              \"inlineObjectHashCode\")                                        \\\n+                                                                                                                  \\\n+  template(jdk_internal_vm_jni_SubElementSelector, \"jdk\/internal\/vm\/jni\/SubElementSelector\")                      \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -246,2 +246,2 @@\n-BufferBlob::BufferBlob(const char* name, int size, CodeBuffer* cb)\n-  : RuntimeBlob(name, cb, sizeof(BufferBlob), size, CodeOffsets::frame_never_safe, 0, NULL)\n+BufferBlob::BufferBlob(const char* name, int header_size, int size, CodeBuffer* cb)\n+  : RuntimeBlob(name, cb, header_size, size, CodeOffsets::frame_never_safe, 0, NULL)\n@@ -258,1 +258,1 @@\n-    blob = new (size) BufferBlob(name, size, cb);\n+    blob = new (size) BufferBlob(name, sizeof(BufferBlob), size, cb);\n@@ -282,0 +282,4 @@\n+BufferBlob::BufferBlob(const char* name, int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments)\n+  : RuntimeBlob(name, cb, sizeof(BufferBlob), size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n+{}\n+\n@@ -286,2 +290,2 @@\n-AdapterBlob::AdapterBlob(int size, CodeBuffer* cb) :\n-  BufferBlob(\"I2C\/C2I adapters\", size, cb) {\n+AdapterBlob::AdapterBlob(int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n+  BufferBlob(\"I2C\/C2I adapters\", size, cb, frame_complete, frame_size, oop_maps, caller_must_gc_arguments) {\n@@ -291,1 +295,1 @@\n-AdapterBlob* AdapterBlob::create(CodeBuffer* cb) {\n+AdapterBlob* AdapterBlob::create(CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) {\n@@ -298,1 +302,1 @@\n-    blob = new (size) AdapterBlob(size, cb);\n+    blob = new (size) AdapterBlob(size, cb, frame_complete, frame_size, oop_maps, caller_must_gc_arguments);\n@@ -347,0 +351,25 @@\n+  \/\/ Track memory usage statistic after releasing CodeCache_lock\n+  MemoryService::track_code_cache_memory_usage();\n+\n+  return blob;\n+}\n+\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Implementation of BufferedInlineTypeBlob\n+BufferedInlineTypeBlob::BufferedInlineTypeBlob(int size, CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) :\n+  BufferBlob(\"buffered inline type\", sizeof(BufferedInlineTypeBlob), size, cb),\n+  _pack_fields_off(pack_fields_off),\n+  _pack_fields_jobject_off(pack_fields_jobject_off),\n+  _unpack_fields_off(unpack_fields_off) {\n+  CodeCache::commit(this);\n+}\n+\n+BufferedInlineTypeBlob* BufferedInlineTypeBlob::create(CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) {\n+  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n+\n+  BufferedInlineTypeBlob* blob = NULL;\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(BufferedInlineTypeBlob));\n+  {\n+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+    blob = new (size) BufferedInlineTypeBlob(size, cb, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+  }\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":36,"deletions":7,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -637,0 +637,6 @@\n+\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\"); \/\/ for the next 3 fields\n+    _inline_entry_point       = _entry_point;\n+    _verified_inline_entry_point = _verified_entry_point;\n+    _verified_inline_ro_entry_point = _verified_entry_point;\n+\n@@ -808,0 +814,3 @@\n+    _inline_entry_point       = code_begin()         + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point = code_begin()      + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin()   + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n@@ -923,0 +932,3 @@\n+static nmethod* _nmethod_to_print = NULL;\n+static const CompiledEntrySignature* _nmethod_to_print_ces = NULL;\n+\n@@ -924,0 +936,6 @@\n+  ResourceMark rm;\n+  CompiledEntrySignature ces(method());\n+  ces.compute_calling_conventions();\n+  \/\/ ces.compute_calling_conventions() needs to grab the ProtectionDomainSet_lock, so we\n+  \/\/ can't do that (inside nmethod::print_entry_parameters) while holding the ttyLocker.\n+  \/\/ Hence we have do compute it here and pass via a global. Yuck.\n@@ -925,0 +943,3 @@\n+  assert(_nmethod_to_print == NULL && _nmethod_to_print_ces == NULL, \"no nesting\");\n+  _nmethod_to_print = this;\n+  _nmethod_to_print_ces = &ces;\n@@ -1000,0 +1021,3 @@\n+\n+  _nmethod_to_print = NULL;\n+  _nmethod_to_print_ces = NULL;\n@@ -2424,1 +2448,1 @@\n-                                     pd->return_oop());\n+                                     pd->return_oop(), pd->return_vt());\n@@ -3061,1 +3085,1 @@\n-                         p->return_oop());\n+                         p->return_oop(), p->return_vt());\n@@ -3070,0 +3094,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -3071,0 +3096,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -3080,0 +3107,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -3082,4 +3119,13 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != NULL) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != NULL) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n+      }\n@@ -3089,6 +3135,34 @@\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != NULL) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n+  if (_nmethod_to_print != this) {\n+    return;\n+  }\n+  Method* m = method();\n+  if (m == NULL || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN4(entry_point(), verified_entry_point(), verified_inline_entry_point(), verified_inline_ro_entry_point());\n+  low = MIN2(low, inline_entry_point());\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  {\n+    const CompiledEntrySignature* ces = _nmethod_to_print_ces;\n+    const GrowableArray<SigEntry>* sig_cc;\n+    const VMRegPair* regs;\n+    if (block_begin == verified_entry_point()) {\n+      sig_cc = &ces->sig_cc();\n+      regs = ces->regs_cc();\n+    } else if (block_begin == verified_inline_entry_point()) {\n+      sig_cc = &ces->sig();\n+      regs = ces->regs();\n+    } else if (block_begin == verified_inline_ro_entry_point()) {\n+      sig_cc = &ces->sig_cc_ro();\n+      regs = ces->regs_cc_ro();\n+    } else {\n+      return;\n@@ -3096,19 +3170,12 @@\n-    if (m != NULL && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+\n+    ResourceMark rm;\n+    int sizeargs = 0;\n+    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, 256);\n+    TempNewSymbol sig = SigEntry::create_symbol(sig_cc);\n+    for (SignatureStream ss(sig); !ss.at_return_type(); ss.next()) {\n+      BasicType t = ss.type();\n+      sig_bt[sizeargs++] = t;\n+      if (type2size[t] == 2) {\n+        sig_bt[sizeargs++] = T_VOID;\n+      } else {\n+        assert(type2size[t] == 1, \"size is 1 or 2\");\n@@ -3116,45 +3183,29 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      intptr_t out_preserve = SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs, false);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n+    }\n+    bool has_this = !m->is_static();\n+    if (ces->has_inline_recv() && block_begin == verified_entry_point()) {\n+      \/\/ <this> argument is scalarized for verified_entry_point()\n+      has_this = false;\n+    }\n+    const char* spname = \"sp\"; \/\/ make arch-specific?\n+    int stack_slot_offset = this->frame_size() * wordSize;\n+    int tab1 = 14, tab2 = 24;\n+    int sig_index = 0;\n+    int arg_index = has_this ? -1 : 0;\n+    bool did_old_sp = false;\n+    for (SignatureStream ss(sig); !ss.at_return_type(); ) {\n+      bool at_this = (arg_index == -1);\n+      bool at_old_sp = false;\n+      BasicType t = ss.type();\n+      assert(t == sig_bt[sig_index], \"sigs in sync\");\n+      if (at_this) {\n+        stream->print(\"  # this: \");\n+      } else {\n+        stream->print(\"  # parm%d: \", arg_index);\n+      }\n+      stream->move_to(tab1);\n+      VMReg fst = regs[sig_index].first();\n+      VMReg snd = regs[sig_index].second();\n+      if (fst->is_reg()) {\n+        stream->print(\"%s\", fst->name());\n+        if (snd->is_valid())  {\n+          stream->print(\":%s\", snd->name());\n@@ -3162,3 +3213,18 @@\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n+      } else if (fst->is_stack()) {\n+        int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+        if (offset == stack_slot_offset)  at_old_sp = true;\n+        stream->print(\"[%s+0x%x]\", spname, offset);\n+      } else {\n+        stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+      }\n+      stream->print(\" \");\n+      stream->move_to(tab2);\n+      stream->print(\"= \");\n+      if (at_this) {\n+        m->method_holder()->print_value_on(stream);\n+      } else {\n+        bool did_name = false;\n+        if (ss.is_reference()) {\n+          Symbol* name = ss.as_symbol();\n+          name->print_value_on(stream);\n+          did_name = true;\n@@ -3166,4 +3232,2 @@\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+        if (!did_name)\n+          stream->print(\"%s\", type2name(t));\n@@ -3171,4 +3235,1 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+      if (at_old_sp) {\n@@ -3176,1 +3237,1 @@\n-        stream->cr();\n+        did_old_sp = true;\n@@ -3178,0 +3239,11 @@\n+      stream->cr();\n+      sig_index += type2size[t];\n+      arg_index += 1;\n+      ss.next();\n+    }\n+    if (!did_old_sp) {\n+      stream->print(\"  # \");\n+      stream->move_to(tab1);\n+      stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+      stream->print(\"  (%s of caller)\", spname);\n+      stream->cr();\n@@ -3302,1 +3374,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_vt=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_vt());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":161,"deletions":89,"binary":false,"changes":250,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/compiler\/oopMap.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -501,1 +501,2 @@\n-    if (klass->is_array_klass()) {\n+    \/\/ CMH: Valhalla flat arrays can split this work up, but for now, doesn't\n+    if (klass->is_array_klass() && !klass->is_flatArray_klass()) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -285,0 +285,1 @@\n+  oop obj_buffer_allocate(Klass* klass, int size, TRAPS); \/\/ doesn't clear memory\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -833,1 +833,1 @@\n-    Node* offset = phase->igvn().MakeConX(in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n+    Node* offset = phase->MakeConX(in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n@@ -885,1 +885,1 @@\n-    phase->igvn().replace_node(ac, call);\n+    phase->replace_node(ac, call);\n@@ -911,1 +911,1 @@\n-void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* n) const {\n+void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseIterGVN* igvn, Node* n) const {\n@@ -913,1 +913,1 @@\n-    shenandoah_eliminate_wb_pre(n, &macro->igvn());\n+    shenandoah_eliminate_wb_pre(n, igvn);\n@@ -1046,1 +1046,1 @@\n-    uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain()->cnt();\n+    uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain_sig()->cnt();\n@@ -1135,1 +1135,1 @@\n-        uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain()->cnt();\n+        uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain_sig()->cnt();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -463,1 +463,1 @@\n-        const TypeTuple* args = n->as_Call()->_tf->domain();\n+        const TypeTuple* args = n->as_Call()->_tf->domain_sig();\n@@ -582,1 +582,1 @@\n-      uint stop = n->is_Call() ? n->as_Call()->tf()->domain()->cnt() : n->req();\n+      uint stop = n->is_Call() ? n->as_Call()->tf()->domain_sig()->cnt() : n->req();\n@@ -802,6 +802,5 @@\n-        CallProjections projs;\n-        c->as_Call()->extract_projections(&projs, true, false);\n-        if (projs.fallthrough_memproj != NULL) {\n-          if (projs.fallthrough_memproj->adr_type() == TypePtr::BOTTOM) {\n-            if (projs.catchall_memproj == NULL) {\n-              mem = projs.fallthrough_memproj;\n+        CallProjections* projs = c->as_Call()->extract_projections(true, false);\n+        if (projs->fallthrough_memproj != NULL) {\n+          if (projs->fallthrough_memproj->adr_type() == TypePtr::BOTTOM) {\n+            if (projs->catchall_memproj == NULL) {\n+              mem = projs->fallthrough_memproj;\n@@ -809,2 +808,2 @@\n-              if (phase->is_dominator(projs.fallthrough_catchproj, ctrl)) {\n-                mem = projs.fallthrough_memproj;\n+              if (phase->is_dominator(projs->fallthrough_catchproj, ctrl)) {\n+                mem = projs->fallthrough_memproj;\n@@ -812,2 +811,2 @@\n-                assert(phase->is_dominator(projs.catchall_catchproj, ctrl), \"one proj must dominate barrier\");\n-                mem = projs.catchall_memproj;\n+                assert(phase->is_dominator(projs->catchall_catchproj, ctrl), \"one proj must dominate barrier\");\n+                mem = projs->catchall_memproj;\n@@ -1055,1 +1054,1 @@\n-static Node* create_phis_on_call_return(Node* ctrl, Node* c, Node* n, Node* n_clone, const CallProjections& projs, PhaseIdealLoop* phase) {\n+static Node* create_phis_on_call_return(Node* ctrl, Node* c, Node* n, Node* n_clone, const CallProjections* projs, PhaseIdealLoop* phase) {\n@@ -1067,1 +1066,1 @@\n-    if (phase->is_dominator(projs.fallthrough_catchproj, in)) {\n+    if (phase->is_dominator(projs->fallthrough_catchproj, in)) {\n@@ -1069,1 +1068,1 @@\n-    } else if (phase->is_dominator(projs.catchall_catchproj, in)) {\n+    } else if (phase->is_dominator(projs->catchall_catchproj, in)) {\n@@ -1185,3 +1184,1 @@\n-      CallProjections projs;\n-      call->extract_projections(&projs, false, false);\n-\n+      CallProjections* projs = call->extract_projections(false, false);\n@@ -1192,2 +1189,2 @@\n-      phase->register_new_node(lrb_clone, projs.catchall_catchproj);\n-      phase->set_ctrl(lrb, projs.fallthrough_catchproj);\n+      phase->register_new_node(lrb_clone, projs->catchall_catchproj);\n+      phase->set_ctrl(lrb, projs->fallthrough_catchproj);\n@@ -1215,1 +1212,1 @@\n-          if (phase->is_dominator(call, c) && phase->is_dominator(c, projs.fallthrough_proj)) {\n+          if (phase->is_dominator(call, c) && phase->is_dominator(c, projs->fallthrough_proj)) {\n@@ -1223,1 +1220,1 @@\n-            phase->register_new_node(u_clone, projs.catchall_catchproj);\n+            phase->register_new_node(u_clone, projs->catchall_catchproj);\n@@ -1225,1 +1222,1 @@\n-            phase->set_ctrl(u, projs.fallthrough_catchproj);\n+            phase->set_ctrl(u, projs->fallthrough_catchproj);\n@@ -1231,1 +1228,1 @@\n-                  if (phase->is_dominator(projs.catchall_catchproj, u->in(0)->in(k))) {\n+                  if (phase->is_dominator(projs->catchall_catchproj, u->in(0)->in(k))) {\n@@ -1234,1 +1231,1 @@\n-                  } else if (!phase->is_dominator(projs.fallthrough_catchproj, u->in(0)->in(k))) {\n+                  } else if (!phase->is_dominator(projs->fallthrough_catchproj, u->in(0)->in(k))) {\n@@ -1241,1 +1238,1 @@\n-              if (phase->is_dominator(projs.catchall_catchproj, c)) {\n+              if (phase->is_dominator(projs->catchall_catchproj, c)) {\n@@ -1246,1 +1243,1 @@\n-              } else if (!phase->is_dominator(projs.fallthrough_catchproj, c)) {\n+              } else if (!phase->is_dominator(projs->fallthrough_catchproj, c)) {\n@@ -2402,5 +2399,4 @@\n-    CallProjections projs;\n-    call->extract_projections(&projs, true, false);\n-    if (projs.catchall_memproj != NULL) {\n-      if (projs.fallthrough_memproj == n) {\n-        c = projs.fallthrough_catchproj;\n+    CallProjections* projs = call->extract_projections(true, false);\n+    if (projs->catchall_memproj != NULL) {\n+      if (projs->fallthrough_memproj == n) {\n+        c = projs->fallthrough_catchproj;\n@@ -2408,2 +2404,2 @@\n-        assert(projs.catchall_memproj == n, \"\");\n-        c = projs.catchall_catchproj;\n+        assert(projs->catchall_memproj == n, \"\");\n+        c = projs->catchall_catchproj;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":30,"deletions":34,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -144,1 +144,1 @@\n-    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -299,1 +299,1 @@\n-bool ShenandoahBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+void ShenandoahBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -306,1 +306,1 @@\n-  return Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n+  Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -46,0 +46,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -75,0 +78,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -220,0 +224,4 @@\n+  if (klass->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_InstantiationError());\n+  }\n+\n@@ -244,0 +252,179 @@\n+void copy_primitive_argument(intptr_t* addr, Handle instance, int offset, BasicType type) {\n+  switch (type) {\n+  case T_BOOLEAN:\n+    instance()->bool_field_put(offset, (jboolean)*((int*)addr));\n+    break;\n+  case T_CHAR:\n+    instance()->char_field_put(offset, (jchar) *((int*)addr));\n+    break;\n+  case T_FLOAT:\n+    instance()->float_field_put(offset, (jfloat)*((float*)addr));\n+    break;\n+  case T_DOUBLE:\n+    instance()->double_field_put(offset, (jdouble)*((double*)addr));\n+    break;\n+  case T_BYTE:\n+    instance()->byte_field_put(offset, (jbyte)*((int*)addr));\n+    break;\n+  case T_SHORT:\n+    instance()->short_field_put(offset, (jshort)*((int*)addr));\n+    break;\n+  case T_INT:\n+    instance()->int_field_put(offset, (jint)*((int*)addr));\n+    break;\n+  case T_LONG:\n+    instance()->long_field_put(offset, (jlong)*((long long*)addr));\n+    break;\n+  case T_OBJECT:\n+  case T_ARRAY:\n+  case T_INLINE_TYPE:\n+    fatal(\"Should not be handled with this method\");\n+    break;\n+  default:\n+    fatal(\"Unsupported BasicType\");\n+  }\n+}\n+\n+JRT_ENTRY(void, InterpreterRuntime::defaultvalue(JavaThread* thread, ConstantPool* pool, int index))\n+  \/\/ Getting the InlineKlass\n+  Klass* k = pool->klass_at(index, CHECK);\n+  if (!k->is_inline_klass()) {\n+    \/\/ inconsistency with 'new' which throws an InstantiationError\n+    \/\/ in the future, defaultvalue will just return null instead of throwing an exception\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+  assert(k->is_inline_klass(), \"defaultvalue argument must be the inline type class\");\n+  InlineKlass* vklass = InlineKlass::cast(k);\n+\n+  vklass->initialize(THREAD);\n+  oop res = vklass->default_value();\n+  thread->set_vm_result(res);\n+JRT_END\n+\n+JRT_ENTRY(int, InterpreterRuntime::withfield(JavaThread* thread, ConstantPoolCache* cp_cache))\n+  LastFrameAccessor last_frame(thread);\n+  \/\/ Getting the InlineKlass\n+  int index = ConstantPool::decode_cpcache_index(last_frame.get_index_u2_cpcache(Bytecodes::_withfield));\n+  ConstantPoolCacheEntry* cp_entry = cp_cache->entry_at(index);\n+  assert(cp_entry->is_resolved(Bytecodes::_withfield), \"Should have been resolved\");\n+  Klass* klass = cp_entry->f1_as_klass();\n+  assert(klass->is_inline_klass(), \"withfield only applies to inline types\");\n+  InlineKlass* vklass = InlineKlass::cast(klass);\n+\n+  \/\/ Getting Field information\n+  int offset = cp_entry->f2_as_index();\n+  int field_index = cp_entry->field_index();\n+  int field_offset = cp_entry->f2_as_offset();\n+  Symbol* field_signature = vklass->field_signature(field_index);\n+  BasicType field_type = Signature::basic_type(field_signature);\n+  int return_offset = (type2size[field_type] + type2size[T_OBJECT]) * AbstractInterpreter::stackElementSize;\n+\n+  \/\/ Getting old value\n+  frame& f = last_frame.get_frame();\n+  jint tos_idx = f.interpreter_frame_expression_stack_size() - 1;\n+  int vt_offset = type2size[field_type];\n+  oop old_value = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx - vt_offset);\n+  assert(old_value != NULL && oopDesc::is_oop(old_value) && old_value->is_inline_type(),\"Verifying receiver\");\n+  Handle old_value_h(THREAD, old_value);\n+\n+  \/\/ Creating new value by copying the one passed in argument\n+  instanceOop new_value = vklass->allocate_instance_buffer(\n+      CHECK_((type2size[field_type]) * AbstractInterpreter::stackElementSize));\n+  Handle new_value_h = Handle(THREAD, new_value);\n+  vklass->inline_copy_oop_to_new_oop(old_value_h(), new_value_h());\n+\n+  \/\/ Updating the field specified in arguments\n+  if (field_type == T_ARRAY || field_type == T_OBJECT) {\n+    oop aoop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);\n+    assert(aoop == NULL || oopDesc::is_oop(aoop),\"argument must be a reference type\");\n+    new_value_h()->obj_field_put(field_offset, aoop);\n+  } else if (field_type == T_INLINE_TYPE) {\n+    if (cp_entry->is_inlined()) {\n+      oop vt_oop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);\n+      assert(vt_oop != NULL && oopDesc::is_oop(vt_oop) && vt_oop->is_inline_type(),\"argument must be an inline type\");\n+      InlineKlass* field_vk = InlineKlass::cast(vklass->get_inline_type_field_klass(field_index));\n+      assert(vt_oop != NULL && field_vk == vt_oop->klass(), \"Must match\");\n+      field_vk->write_inlined_field(new_value_h(), offset, vt_oop, CHECK_(return_offset));\n+    } else { \/\/ not inlined\n+      oop voop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);\n+      if (voop == NULL && cp_entry->is_inline_type()) {\n+        THROW_(vmSymbols::java_lang_NullPointerException(), return_offset);\n+      }\n+      assert(voop == NULL || oopDesc::is_oop(voop),\"checking argument\");\n+      new_value_h()->obj_field_put(field_offset, voop);\n+    }\n+  } else { \/\/ not T_OBJECT nor T_ARRAY nor T_INLINE_TYPE\n+    intptr_t* addr = f.interpreter_frame_expression_stack_at(tos_idx);\n+    copy_primitive_argument(addr, new_value_h, field_offset, field_type);\n+  }\n+\n+  \/\/ returning result\n+  thread->set_vm_result(new_value_h());\n+  return return_offset;\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::uninitialized_static_inline_type_field(JavaThread* thread, oopDesc* mirror, int index))\n+  \/\/ The interpreter tries to access an inline static field that has not been initialized.\n+  \/\/ This situation can happen in different scenarios:\n+  \/\/   1 - if the load or initialization of the field failed during step 8 of\n+  \/\/       the initialization of the holder of the field, in this case the access to the field\n+  \/\/       must fail\n+  \/\/   2 - it can also happen when the initialization of the holder class triggered the initialization of\n+  \/\/       another class which accesses this field in its static initializer, in this case the\n+  \/\/       access must succeed to allow circularity\n+  \/\/ The code below tries to load and initialize the field's class again before returning the default value.\n+  \/\/ If the field was not initialized because of an error, a exception should be thrown.\n+  \/\/ If the class is being initialized, the default value is returned.\n+  instanceHandle mirror_h(THREAD, (instanceOop)mirror);\n+  InstanceKlass* klass = InstanceKlass::cast(java_lang_Class::as_Klass(mirror));\n+  if (klass->is_being_initialized() && klass->is_reentrant_initialization(THREAD)) {\n+    int offset = klass->field_offset(index);\n+    Klass* field_k = klass->get_inline_type_field_klass_or_null(index);\n+    if (field_k == NULL) {\n+      field_k = SystemDictionary::resolve_or_fail(klass->field_signature(index)->fundamental_name(THREAD),\n+          Handle(THREAD, klass->class_loader()),\n+          Handle(THREAD, klass->protection_domain()),\n+          true, CHECK);\n+      assert(field_k != NULL, \"Should have been loaded or an exception thrown above\");\n+      klass->set_inline_type_field_klass(index, field_k);\n+    }\n+    field_k->initialize(CHECK);\n+    oop defaultvalue = InlineKlass::cast(field_k)->default_value();\n+    \/\/ It is safe to initialized the static field because 1) the current thread is the initializing thread\n+    \/\/ and is the only one that can access it, and 2) the field is actually not initialized (i.e. null)\n+    \/\/ otherwise the JVM should not be executing this code.\n+    mirror->obj_field_put(offset, defaultvalue);\n+    thread->set_vm_result(defaultvalue);\n+  } else {\n+    assert(klass->is_in_error_state(), \"If not initializing, initialization must have failed to get there\");\n+    ResourceMark rm(THREAD);\n+    const char* desc = \"Could not initialize class \";\n+    const char* className = klass->external_name();\n+    size_t msglen = strlen(desc) + strlen(className) + 1;\n+    char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+    if (NULL == message) {\n+      \/\/ Out of memory: can't create detailed error message\n+      THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), className);\n+    } else {\n+      jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+      THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), message);\n+    }\n+  }\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::read_inlined_field(JavaThread* thread, oopDesc* obj, int index, Klass* field_holder))\n+  Handle obj_h(THREAD, obj);\n+\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+\n+  assert(field_holder->is_instance_klass(), \"Sanity check\");\n+  InstanceKlass* klass = InstanceKlass::cast(field_holder);\n+\n+  assert(klass->field_is_inlined(index), \"Sanity check\");\n+\n+  InlineKlass* field_vklass = InlineKlass::cast(klass->get_inline_type_field_klass(index));\n+  assert(field_vklass->is_initialized(), \"Must be initialized at this point\");\n+\n+  oop res = field_vklass->read_inlined_field(obj_h(), klass->field_offset(index), CHECK);\n+  thread->set_vm_result(res);\n+JRT_END\n@@ -253,1 +440,8 @@\n-  objArrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n+  bool      is_qtype_desc = pool->tag_at(index).is_Qdescriptor_klass();\n+  arrayOop obj;\n+  if ((!klass->is_array_klass()) && is_qtype_desc) { \/\/ Logically creates elements, ensure klass init\n+    klass->initialize(CHECK);\n+    obj = oopFactory::new_flatArray(klass, size, CHECK);\n+  } else {\n+    obj = oopFactory::new_objArray(klass, size, CHECK);\n+  }\n@@ -257,0 +451,10 @@\n+JRT_ENTRY(void, InterpreterRuntime::value_array_load(JavaThread* thread, arrayOopDesc* array, int index))\n+  flatArrayHandle vah(thread, (flatArrayOop)array);\n+  oop value_holder = flatArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK);\n+  thread->set_vm_result(value_holder);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::value_array_store(JavaThread* thread, void* val, arrayOopDesc* array, int index))\n+  assert(val != NULL, \"can't store null into flat array\");\n+  ((flatArrayOop)array)->value_copy_to_index((oop)val, index);\n+JRT_END\n@@ -262,2 +466,3 @@\n-  int          i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n-  Klass* klass   = constants->klass_at(i, CHECK);\n+  int i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n+  Klass* klass = constants->klass_at(i, CHECK);\n+  bool is_qtype = klass->name()->is_Q_array_signature();\n@@ -268,0 +473,4 @@\n+  if (is_qtype) { \/\/ Logically creates elements, ensure klass init\n+    klass->initialize(CHECK);\n+  }\n+\n@@ -292,0 +501,23 @@\n+JRT_ENTRY(jboolean, InterpreterRuntime::is_substitutable(JavaThread* thread, oopDesc* aobj, oopDesc* bobj))\n+  assert(oopDesc::is_oop(aobj) && oopDesc::is_oop(bobj), \"must be valid oops\");\n+\n+  Handle ha(THREAD, aobj);\n+  Handle hb(THREAD, bobj);\n+  JavaValue result(T_BOOLEAN);\n+  JavaCallArguments args;\n+  args.push_oop(ha);\n+  args.push_oop(hb);\n+  methodHandle method(thread, Universe::is_substitutable_method());\n+  JavaCalls::call(&result, method, &args, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    \/\/ Something really bad happened because isSubstitutable() should not throw exceptions\n+    \/\/ If it is an error, just let it propagate\n+    \/\/ If it is an exception, wrap it into an InternalError\n+    if (!PENDING_EXCEPTION->is_a(SystemDictionary::Error_klass())) {\n+      Handle e(THREAD, PENDING_EXCEPTION);\n+      CLEAR_PENDING_EXCEPTION;\n+      THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in substitutability test\", e, false);\n+    }\n+  }\n+  return result.get_jboolean();\n+JRT_END\n@@ -620,0 +852,4 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_InstantiationError(JavaThread* thread))\n+  THROW(vmSymbols::java_lang_InstantiationError());\n+JRT_END\n+\n@@ -650,1 +886,1 @@\n-                    bytecode == Bytecodes::_putstatic);\n+                    bytecode == Bytecodes::_putstatic || bytecode == Bytecodes::_withfield);\n@@ -652,0 +888,1 @@\n+  bool is_inline_type  = bytecode == Bytecodes::_withfield;\n@@ -695,3 +932,9 @@\n-    get_code = ((is_static) ? Bytecodes::_getstatic : Bytecodes::_getfield);\n-    if ((is_put && !has_initialized_final_update) || !info.access_flags().is_final()) {\n-      put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_putfield);\n+    if (is_static) {\n+      get_code = Bytecodes::_getstatic;\n+    } else {\n+      get_code = Bytecodes::_getfield;\n+    }\n+    if (is_put && is_inline_type) {\n+        put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_withfield);\n+    } else if ((is_put && !has_initialized_final_update) || !info.access_flags().is_final()) {\n+        put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_putfield);\n@@ -710,0 +953,2 @@\n+    info.is_inlined(),\n+    info.is_inline_type(),\n@@ -956,0 +1201,1 @@\n+  case Bytecodes::_withfield:\n@@ -1199,0 +1445,1 @@\n+  bool is_inlined = cp_entry->is_inlined();\n@@ -1207,1 +1454,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(cp_entry_f1, cp_entry->f2_as_index(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(cp_entry_f1, cp_entry->f2_as_index(), is_static, is_inlined);\n@@ -1237,0 +1484,6 @@\n+\n+  \/\/ Both Q-signatures and L-signatures are mapped to atos\n+  if (cp_entry->flag_state() == atos && ik->field_signature(index)->is_Q_signature()) {\n+    sig_type = JVM_SIGNATURE_INLINE_TYPE;\n+  }\n+\n@@ -1238,0 +1491,1 @@\n+  bool is_inlined = cp_entry->is_inlined();\n@@ -1240,1 +1494,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, cp_entry->f2_as_index(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, cp_entry->f2_as_index(), is_static, is_inlined);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":263,"deletions":9,"binary":false,"changes":272,"status":"modified"},{"patch":"@@ -64,0 +64,10 @@\n+  static void    defaultvalue  (JavaThread* thread, ConstantPool* pool, int index);\n+  static int     withfield     (JavaThread* thread, ConstantPoolCache* cp_cache);\n+  static void    uninitialized_static_inline_type_field(JavaThread* thread, oopDesc* mirror, int offset);\n+  static void    write_heap_copy (JavaThread* thread, oopDesc* value, int offset, oopDesc* rcv);\n+  static void    read_inlined_field(JavaThread* thread, oopDesc* value, int index, Klass* field_holder);\n+\n+  static void value_array_load(JavaThread* thread, arrayOopDesc* array, int index);\n+  static void value_array_store(JavaThread* thread, void* val, arrayOopDesc* array, int index);\n+\n+  static jboolean is_substitutable(JavaThread* thread, oopDesc* aobj, oopDesc* bobj);\n@@ -75,0 +85,1 @@\n+  static void    throw_InstantiationError(JavaThread* thread);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -295,0 +295,1 @@\n+  static void withfield();\n@@ -297,0 +298,1 @@\n+  static void defaultvalue();\n","filename":"src\/hotspot\/share\/interpreter\/templateTable.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1191,1 +1191,1 @@\n-  _debug_recorder->describe_scope(pc_offset, method, NULL, bci, reexecute, throw_exception, is_mh_invoke, return_oop,\n+  _debug_recorder->describe_scope(pc_offset, method, NULL, bci, reexecute, throw_exception, is_mh_invoke, return_oop, false,\n@@ -1357,0 +1357,2 @@\n+        _offsets.set_value(CodeOffsets::Verified_Inline_Entry, pc_offset);\n+        _offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, pc_offset);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1262,1 +1262,1 @@\n-              Deoptimization::reassign_fields(fst.current(), fst.register_map(), objects, realloc_failures, false);\n+              Deoptimization::reassign_fields(fst.current(), fst.register_map(), objects, realloc_failures, false, CHECK_NULL);\n@@ -1522,1 +1522,1 @@\n-  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false);\n+  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false, THREAD);\n@@ -1921,1 +1921,1 @@\n-    if (m->is_initializer() && !m->is_static()) {\n+    if (m->is_object_constructor()) {\n@@ -1951,1 +1951,1 @@\n-    if (!m->is_initializer() && !m->is_overpass()) {\n+    if (!(m->is_object_constructor() || m->is_class_initializer()) && !m->is_overpass()) {\n@@ -2543,2 +2543,1 @@\n-  if (m->is_initializer()) {\n-    if (m->is_static_initializer()) {\n+  if (m->is_class_initializer()) {\n@@ -2547,1 +2546,2 @@\n-    }\n+  }\n+  else if (m->is_object_constructor() || m->is_static_init_factory()) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -157,1 +157,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags,                                   u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags,                                   u4)                                    \\\n@@ -519,0 +519,2 @@\n+  declare_constant(DataLayout::array_load_store_data_tag)                 \\\n+  declare_constant(DataLayout::acmp_data_tag)                             \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -184,0 +184,1 @@\n+  LOG_TAG(valuetypes) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -323,1 +323,0 @@\n-    assert(type == _method_entry_ref, \"only special type allowed for now\");\n@@ -326,1 +325,1 @@\n-    _builder->add_special_ref(type, src_obj, field_offset);\n+    _builder->add_special_ref(type, src_obj, field_offset, ref->size() * BytesPerWord);\n@@ -370,4 +369,0 @@\n-void ArchiveBuilder::add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset) {\n-  _special_refs->append(SpecialRefInfo(type, src_obj, field_offset));\n-}\n-\n@@ -519,2 +514,18 @@\n-    assert(s.type() == MetaspaceClosure::_method_entry_ref, \"only special type allowed for now\");\n-    assert(*src_p == *dst_p, \"must be a copy\");\n+\n+    MetaspaceClosure::assert_valid(s.type());\n+    switch (s.type()) {\n+    case MetaspaceClosure::_method_entry_ref:\n+      assert(*src_p == *dst_p, \"must be a copy\");\n+      break;\n+    case MetaspaceClosure::_internal_pointer_ref:\n+      {\n+        \/\/ *src_p points to a location inside src_obj. Let's make *dst_p point to\n+        \/\/ the same location inside dst_obj.\n+        size_t off = pointer_delta(*((address*)src_p), src_obj, sizeof(u1));\n+        assert(off < s.src_obj_size_in_bytes(), \"must point to internal address\");\n+        *((address*)dst_p) = dst_obj + off;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n","filename":"src\/hotspot\/share\/memory\/archiveBuilder.cpp","additions":19,"deletions":8,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+    DEBUG_ONLY(size_t _src_obj_size_in_bytes;)\n@@ -59,2 +60,4 @@\n-    SpecialRefInfo(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset)\n-      : _type(type), _src_obj(src_obj), _field_offset(field_offset) {}\n+    SpecialRefInfo(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset, size_t src_obj_size_in_bytes)\n+      : _type(type), _src_obj(src_obj), _field_offset(field_offset) {\n+      DEBUG_ONLY(_src_obj_size_in_bytes = src_obj_size_in_bytes);\n+    }\n@@ -65,0 +68,2 @@\n+\n+    DEBUG_ONLY(size_t src_obj_size_in_bytes() const { return _src_obj_size_in_bytes; })\n@@ -242,1 +247,3 @@\n-  void add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset);\n+  void add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset, size_t src_obj_size_in_bytes) {\n+    _special_refs->append(SpecialRefInfo(type, src_obj, field_offset, src_obj_size_in_bytes));\n+  }\n","filename":"src\/hotspot\/share\/memory\/archiveBuilder.hpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -337,1 +337,1 @@\n-    align_up(SharedRuntime::trampoline_size(), BytesPerWord) +\n+    align_up(SharedRuntime::trampoline_size(), BytesPerWord) * 3 +\n@@ -366,0 +366,2 @@\n+\n+      \/\/ TODO:CDS - JDK-8234693 will consolidate this with Method::unlink()\n@@ -371,0 +373,10 @@\n+      address c2i_inline_ro_entry_trampoline = (address)p;\n+      p += SharedRuntime::trampoline_size();\n+      assert(p >= mc_space->base() && p <= mc_space->top(), \"must be\");\n+      m->set_from_compiled_inline_ro_entry(to_target(c2i_inline_ro_entry_trampoline));\n+\n+      address c2i_inline_entry_trampoline = (address)p;\n+      p +=  SharedRuntime::trampoline_size();\n+      assert(p >= mc_space->base() && p <= mc_space->top(), \"must be\");\n+      m->set_from_compiled_inline_entry(to_target(c2i_inline_entry_trampoline));\n+\n","filename":"src\/hotspot\/share\/memory\/dynamicArchive.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -56,0 +56,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/memory\/metaspaceShared.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -141,1 +141,1 @@\n-    if (!is_static)\n+    if (!is_static) {\n@@ -143,0 +143,1 @@\n+    }\n@@ -839,1 +840,1 @@\n-  assert(cts.is_reference() || cts.is_value() || cts.is_address(),\n+  assert(cts.is_reference() || cts.is_inline_type() || cts.is_address(),\n@@ -1380,0 +1381,3 @@\n+    case Bytecodes::_defaultvalue:      ppush1(CellTypeState::make_line_ref(itr->bci())); break;\n+    case Bytecodes::_withfield:         do_withfield(itr->get_index_u2_cpcache(), itr->bci()); break;\n+\n@@ -1593,2 +1597,2 @@\n-    case Bytecodes::_getstatic:         do_field(true,  true,  itr->get_index_u2_cpcache(), itr->bci()); break;\n-    case Bytecodes::_putstatic:         do_field(false, true,  itr->get_index_u2_cpcache(), itr->bci()); break;\n+    case Bytecodes::_getstatic:         do_field(true,  true, itr->get_index_u2_cpcache(), itr->bci()); break;\n+    case Bytecodes::_putstatic:         do_field(false, true, itr->get_index_u2_cpcache(), itr->bci()); break;\n@@ -1598,0 +1602,1 @@\n+    case Bytecodes::_invokeinterface:\n@@ -1599,4 +1604,3 @@\n-    case Bytecodes::_invokespecial:     do_method(false, false, itr->get_index_u2_cpcache(), itr->bci()); break;\n-    case Bytecodes::_invokestatic:      do_method(true,  false, itr->get_index_u2_cpcache(), itr->bci()); break;\n-    case Bytecodes::_invokedynamic:     do_method(true,  false, itr->get_index_u4(),         itr->bci()); break;\n-    case Bytecodes::_invokeinterface:   do_method(false, true,  itr->get_index_u2_cpcache(), itr->bci()); break;\n+    case Bytecodes::_invokespecial:     do_method(false, itr->get_index_u2_cpcache(), itr->bci()); break;\n+    case Bytecodes::_invokestatic:      do_method(true , itr->get_index_u2_cpcache(), itr->bci()); break;\n+    case Bytecodes::_invokedynamic:     do_method(true , itr->get_index_u4(),         itr->bci()); break;\n@@ -1622,0 +1626,1 @@\n+\n@@ -1728,1 +1733,1 @@\n-  assert(in.is_reference() | in.is_value(), \"sanity check\");\n+  assert(in.is_reference() || in.is_inline_type(), \"sanity check\");\n@@ -1947,1 +1952,3 @@\n-  if (!is_static) in[i++] = CellTypeState::ref;\n+  if (!is_static) {\n+    in[i++] = CellTypeState::ref;\n+  }\n@@ -1953,1 +1960,1 @@\n-void GenerateOopMap::do_method(int is_static, int is_interface, int idx, int bci) {\n+void GenerateOopMap::do_method(int is_static, int idx, int bci) {\n@@ -1990,0 +1997,27 @@\n+void GenerateOopMap::do_withfield(int idx, int bci) {\n+  \/\/ Dig up signature for field in constant pool\n+  ConstantPool* cp = method()->constants();\n+  int nameAndTypeIdx = cp->name_and_type_ref_index_at(idx);\n+  int signatureIdx = cp->signature_ref_index_at(nameAndTypeIdx);\n+  Symbol* signature = cp->symbol_at(signatureIdx);\n+\n+  \/\/ Parse signature (especially simple for fields)\n+  assert(signature->utf8_length() > 0,\n+      \"field signatures cannot have zero length\");\n+  \/\/ The signature is UFT8 encoded, but the first char is always ASCII for signatures.\n+  CellTypeState temp[4];\n+  CellTypeState *eff = signature_to_effect(signature, bci, temp);\n+\n+  CellTypeState in[4];\n+  int i = copy_cts(in, eff);\n+  in[i++] = CellTypeState::ref;\n+  in[i] = CellTypeState::bottom;\n+  assert(i <= 3, \"sanity check\");\n+\n+  CellTypeState out[2];\n+  out[0] = CellTypeState::ref;\n+  out[1] = CellTypeState::bottom;\n+\n+  pp(in, out);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.cpp","additions":45,"deletions":11,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -118,1 +119,0 @@\n-\n@@ -155,0 +155,5 @@\n+address Method::get_c2i_inline_entry() {\n+  assert(adapter() != NULL, \"must have\");\n+  return adapter()->get_c2i_inline_entry();\n+}\n+\n@@ -160,0 +165,5 @@\n+address Method::get_c2i_unverified_inline_entry() {\n+  assert(adapter() != NULL, \"must have\");\n+  return adapter()->get_c2i_unverified_inline_entry();\n+}\n+\n@@ -352,0 +362,2 @@\n+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_inline_ro_entry);\n+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_inline_entry);\n@@ -596,0 +608,18 @@\n+\/\/ InlineKlass the method is declared to return. This must not\n+\/\/ safepoint as it is called with references live on the stack at\n+\/\/ locations the GC is unaware of.\n+InlineKlass* Method::returned_inline_type(Thread* thread) const {\n+  SignatureStream ss(signature());\n+  while (!ss.at_return_type()) {\n+    ss.next();\n+  }\n+  Handle class_loader(thread, method_holder()->class_loader());\n+  Handle protection_domain(thread, method_holder()->protection_domain());\n+  Klass* k = NULL;\n+  {\n+    NoSafepointVerifier nsv;\n+    k = ss.as_klass(class_loader, protection_domain, SignatureStream::ReturnNull, thread);\n+  }\n+  assert(k != NULL && !thread->has_pending_exception(), \"can't resolve klass\");\n+  return InlineKlass::cast(k);\n+}\n@@ -606,1 +636,1 @@\n-  \/\/   aload_0\n+  \/\/   aload_0, _fast_aload_0, or _nofast_aload_0\n@@ -630,1 +660,2 @@\n-  if (cb[0] != Bytecodes::_aload_0 || cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {\n+  if ((cb[0] != Bytecodes::_aload_0 && cb[0] != Bytecodes::_fast_aload_0 && cb[0] != Bytecodes::_nofast_aload_0) ||\n+       cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {\n@@ -810,7 +841,2 @@\n-bool Method::is_initializer() const {\n-  return is_object_initializer() || is_static_initializer();\n-}\n-\n-bool Method::has_valid_initializer_flags() const {\n-  return (is_static() ||\n-          method_holder()->major_version() < 51);\n+bool Method::is_object_constructor_or_class_initializer() const {\n+  return (is_object_constructor() || is_class_initializer());\n@@ -819,1 +845,1 @@\n-bool Method::is_static_initializer() const {\n+bool Method::is_class_initializer() const {\n@@ -823,2 +849,8 @@\n-  return name() == vmSymbols::class_initializer_name() &&\n-         has_valid_initializer_flags();\n+  return (name() == vmSymbols::class_initializer_name() &&\n+          (is_static() ||\n+           method_holder()->major_version() < 51));\n+}\n+\n+\/\/ A method named <init>, if non-static, is a classic object constructor.\n+bool Method::is_object_constructor() const {\n+   return name() == vmSymbols::object_initializer_name() && !is_static();\n@@ -827,2 +859,3 @@\n-bool Method::is_object_initializer() const {\n-   return name() == vmSymbols::object_initializer_name();\n+\/\/ A static method named <init> is a factory for an inline class.\n+bool Method::is_static_init_factory() const {\n+   return name() == vmSymbols::object_initializer_name() && is_static();\n@@ -886,1 +919,1 @@\n-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {\n+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {\n@@ -902,1 +935,3 @@\n-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;\n+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {\n+      return false;\n+    }\n@@ -1071,0 +1106,2 @@\n+    _from_compiled_inline_entry = NULL;\n+    _from_compiled_inline_ro_entry = NULL;\n@@ -1073,0 +1110,2 @@\n+    _from_compiled_inline_entry = adapter()->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = adapter()->get_c2i_inline_ro_entry();\n@@ -1117,0 +1156,1 @@\n+\n@@ -1119,1 +1159,9 @@\n-           \"must be NULL during dump time, to be initialized at run time\");\n+           \"instructions must be zeros during dump time, to be initialized at run time\");\n+\n+    _from_compiled_inline_ro_entry = cds_adapter->get_c2i_inline_ro_entry_trampoline();\n+    assert(*((int*)_from_compiled_inline_ro_entry) == 0,\n+           \"instructions must be zeros during dump time, to be initialized at run time\");\n+\n+    _from_compiled_inline_entry = cds_adapter->get_c2i_inline_entry_trampoline();\n+    assert(*((int*)_from_compiled_inline_entry) == 0,\n+           \"instructions must be zeros during dump time, to be initialized at run time\");\n@@ -1272,0 +1320,2 @@\n+    assert(mh->_from_compiled_inline_entry != NULL, \"must be\");\n+    assert(mh->_from_compiled_inline_ro_entry != NULL, \"must be\");\n@@ -1275,0 +1325,2 @@\n+    mh->_from_compiled_inline_entry = adapter->get_c2i_inline_entry();\n+    mh->_from_compiled_inline_ro_entry = adapter->get_c2i_inline_ro_entry();\n@@ -1282,0 +1334,12 @@\n+#if 0\n+  \/*\n+   * CDS:TODO --\n+   * \"Q\" classes in the method signature must be resolved during link_method.\n+   * However, at this point we are still inside method_holder()->restore_unshareable_info.\n+   * If we try to resolve method_holder(), or multually dependent classes, it will\n+   * cause deadlock and other ill effects.\n+   *\n+   * For now, lets do method linking inside InstanceKlass::link_class(). Optimization\n+   * may be possible if we know that resolution will never happen.\n+   *\/\n+\n@@ -1288,0 +1352,1 @@\n+#endif\n@@ -1290,1 +1355,1 @@\n-address Method::from_compiled_entry_no_trampoline() const {\n+address Method::from_compiled_entry_no_trampoline(bool caller_is_c1) const {\n@@ -1292,2 +1357,7 @@\n-  if (code) {\n-    return code->verified_entry_point();\n+  if (caller_is_c1) {\n+    \/\/ C1 - inline type arguments are passed as objects\n+    if (code) {\n+      return code->verified_inline_entry_point();\n+    } else {\n+      return adapter()->get_c2i_inline_entry();\n+    }\n@@ -1295,1 +1365,6 @@\n-    return adapter()->get_c2i_entry();\n+    \/\/ C2 - inline type arguments may be passed as fields\n+    if (code) {\n+      return code->verified_entry_point();\n+    } else {\n+      return adapter()->get_c2i_entry();\n+    }\n@@ -1312,0 +1387,12 @@\n+address Method::verified_inline_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_entry != NULL, \"must be set\");\n+  return _from_compiled_inline_entry;\n+}\n+\n+address Method::verified_inline_ro_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_ro_entry != NULL, \"must be set\");\n+  return _from_compiled_inline_ro_entry;\n+}\n+\n@@ -1343,0 +1430,2 @@\n+  mh->_from_compiled_inline_entry = code->verified_inline_entry_point();\n+  mh->_from_compiled_inline_ro_entry = code->verified_inline_ro_entry_point();\n@@ -2374,0 +2463,2 @@\n+  if (valid_itable_index())\n+    st->print_cr(\" - itable index:      %d\",   itable_index());\n@@ -2451,0 +2542,1 @@\n+  if (WizardMode) access_flags().print_on(st);\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":114,"deletions":22,"binary":false,"changes":136,"status":"modified"},{"patch":"@@ -42,0 +42,10 @@\n+\/\/\n+\/\/ oopDesc::_mark - the \"oop mark word\" encoding to be found separately in markWord.hpp\n+\/\/\n+\/\/ oopDesc::_metadata - encodes the object's klass pointer, as a raw pointer in \"_klass\"\n+\/\/                      or compressed pointer in \"_compressed_klass\"\n+\/\/\n+\/\/ The overall size of the _metadata field is dependent on \"UseCompressedClassPointers\",\n+\/\/ hence the terms \"narrow\" (32 bits) vs \"wide\" (64 bits).\n+\/\/\n+\n@@ -105,0 +115,2 @@\n+  inline bool is_inline_type()         const;\n+  inline bool is_flatArray()           const;\n@@ -111,0 +123,2 @@\n+  bool is_value_noinline()             const;\n+  bool is_flatArray_noinline()         const;\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -206,0 +206,2 @@\n+bool oopDesc::is_inline_type() const { return klass()->is_inline_klass(); }\n+bool oopDesc::is_flatArray() const { return klass()->is_flatArray_klass(); }\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -780,0 +780,6 @@\n+  product(bool, UseArrayLoadStoreProfile, true,                             \\\n+          \"Take advantage of profiling at array load\/store\")                \\\n+                                                                            \\\n+  product(bool, UseACmpProfile, true,                                       \\\n+          \"Take advantage of profiling at acmp\")                            \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -504,0 +504,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -513,0 +515,1 @@\n+  case vmIntrinsics::_getValue:\n@@ -522,0 +525,1 @@\n+  case vmIntrinsics::_putValue:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -119,1 +120,1 @@\n-  \/\/ paths to facilitate late inlinig.\n+  \/\/ paths to facilitate late inlining.\n@@ -125,0 +126,1 @@\n+      _call_node(NULL),\n@@ -127,0 +129,8 @@\n+    if (InlineTypeReturnedAsFields && method->is_method_handle_intrinsic()) {\n+      \/\/ If that call has not been optimized by the time optimizations are over,\n+      \/\/ we'll need to add a call to create an inline type instance from the klass\n+      \/\/ returned by the call (see PhaseMacroExpand::expand_mh_intrinsic_return).\n+      \/\/ Separating memory and I\/O projections for exceptions is required to\n+      \/\/ perform that graph transformation.\n+      _separate_io_proj = true;\n+    }\n@@ -136,0 +146,1 @@\n+  PhaseGVN& gvn = kit.gvn();\n@@ -168,1 +179,4 @@\n-  kit.set_arguments_for_java_call(call);\n+  kit.set_arguments_for_java_call(call, is_late_inline());\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -194,1 +208,0 @@\n-\n@@ -206,1 +219,1 @@\n-  if (kit.gvn().type(receiver)->higher_equal(TypePtr::NULL_PTR)) {\n+  if (!receiver->is_InlineType() && kit.gvn().type(receiver)->higher_equal(TypePtr::NULL_PTR)) {\n@@ -252,0 +265,3 @@\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -342,0 +358,4 @@\n+\n+  virtual CallGenerator* inline_cg() {\n+    return _inline_cg;\n+  }\n@@ -352,3 +372,3 @@\n-  const TypeTuple *r = call->tf()->domain();\n-  for (int i1 = 0; i1 < method()->arg_size(); i1++) {\n-    if (call->in(TypeFunc::Parms + i1)->is_top() && r->field_at(TypeFunc::Parms + i1) != Type::HALF) {\n+  const TypeTuple* r = call->tf()->domain_cc();\n+  for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+    if (call->in(i1)->is_top() && r->field_at(i1) != Type::HALF) {\n@@ -366,10 +386,8 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true);\n-  if (callprojs.fallthrough_catchproj == call->in(0) ||\n-      callprojs.catchall_catchproj == call->in(0) ||\n-      callprojs.fallthrough_memproj == call->in(TypeFunc::Memory) ||\n-      callprojs.catchall_memproj == call->in(TypeFunc::Memory) ||\n-      callprojs.fallthrough_ioproj == call->in(TypeFunc::I_O) ||\n-      callprojs.catchall_ioproj == call->in(TypeFunc::I_O) ||\n-      (callprojs.resproj != NULL && call->find_edge(callprojs.resproj) != -1) ||\n-      (callprojs.exobj != NULL && call->find_edge(callprojs.exobj) != -1)) {\n+  CallProjections* callprojs = call->extract_projections(true);\n+  if (callprojs->fallthrough_catchproj == call->in(0) ||\n+      callprojs->catchall_catchproj == call->in(0) ||\n+      callprojs->fallthrough_memproj == call->in(TypeFunc::Memory) ||\n+      callprojs->catchall_memproj == call->in(TypeFunc::Memory) ||\n+      callprojs->fallthrough_ioproj == call->in(TypeFunc::I_O) ||\n+      callprojs->catchall_ioproj == call->in(TypeFunc::I_O) ||\n+      (callprojs->exobj != NULL && call->find_edge(callprojs->exobj) != -1)) {\n@@ -378,0 +396,11 @@\n+  bool result_not_used = true;\n+  for (uint i = 0; i < callprojs->nb_resproj; i++) {\n+    if (callprojs->resproj[i] != NULL) {\n+      if (callprojs->resproj[i]->outcnt() != 0) {\n+        result_not_used = false;\n+      }\n+      if (call->find_edge(callprojs->resproj[i]) != -1) {\n+        return;\n+      }\n+    }\n+  }\n@@ -385,1 +414,0 @@\n-  bool result_not_used = (callprojs.resproj == NULL || callprojs.resproj->outcnt() == 0);\n@@ -401,0 +429,1 @@\n+    PhaseGVN& gvn = *C->initial_gvn();\n@@ -404,1 +433,1 @@\n-      C->initial_gvn()->set_type_bottom(mem);\n+      gvn.set_type_bottom(mem);\n@@ -408,4 +437,2 @@\n-    uint nargs = method()->arg_size();\n-    Node* top = C->top();\n-    for (uint i1 = 0; i1 < nargs; i1++) {\n-      map->set_req(TypeFunc::Parms + i1, top);\n+    for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+      map->set_req(i1, C->top());\n@@ -419,0 +446,6 @@\n+    const TypeTuple *domain_sig = call->_tf->domain_sig();\n+    ExtendedSignature sig_cc = ExtendedSignature(method()->get_sig_cc(), SigEntryFilter());\n+    uint nargs = method()->arg_size();\n+    assert(domain_sig->cnt() - TypeFunc::Parms == nargs, \"inconsistent signature\");\n+\n+    uint j = TypeFunc::Parms;\n@@ -420,1 +453,11 @@\n-      map->set_argument(jvms, i1, call->in(TypeFunc::Parms + i1));\n+      const Type* t = domain_sig->field_at(TypeFunc::Parms + i1);\n+      if (method()->has_scalarized_args() && t->is_inlinetypeptr() && !t->maybe_null() && t->inline_klass()->can_be_passed_as_fields()) {\n+        \/\/ Inline type arguments are not passed by reference: we get an argument per\n+        \/\/ field of the inline type. Build InlineTypeNodes from the inline type arguments.\n+        GraphKit arg_kit(jvms, &gvn);\n+        InlineTypeNode* vt = InlineTypeNode::make_from_multi(&arg_kit, call, sig_cc, t->inline_klass(), j, true);\n+        map->set_control(arg_kit.control());\n+        map->set_argument(jvms, i1, vt);\n+      } else {\n+        map->set_argument(jvms, i1, call->in(j++));\n+      }\n@@ -436,0 +479,15 @@\n+    \/\/ Allocate a buffer for the returned InlineTypeNode because the caller expects an oop return.\n+    \/\/ Do this before the method handle call in case the buffer allocation triggers deoptimization.\n+    Node* buffer_oop = NULL;\n+    if (is_mh_late_inline() && _inline_cg->method()->return_type()->is_inlinetype()) {\n+      GraphKit arg_kit(jvms, &gvn);\n+      {\n+        PreserveReexecuteState preexecs(&arg_kit);\n+        arg_kit.jvms()->set_should_reexecute(true);\n+        arg_kit.inc_sp(nargs);\n+        Node* klass_node = arg_kit.makecon(TypeKlassPtr::make(_inline_cg->method()->return_type()->as_inline_klass()));\n+        buffer_oop = arg_kit.new_instance(klass_node, NULL, NULL, \/* deoptimize_on_exception *\/ true);\n+      }\n+      jvms = arg_kit.transfer_exceptions_into_jvms();\n+    }\n+\n@@ -462,0 +520,19 @@\n+\n+    \/\/ Handle inline type returns\n+    bool returned_as_fields = call->tf()->returns_inline_type_as_fields();\n+    if (result->is_InlineType()) {\n+      \/\/ Only possible if is_mh_late_inline() when the callee does not \"know\" that the caller expects an oop\n+      assert(is_mh_late_inline() && !returned_as_fields, \"sanity\");\n+      assert(buffer_oop != NULL, \"should have allocated a buffer\");\n+      InlineTypeNode* vt = result->as_InlineType();\n+      vt->store(&kit, buffer_oop, buffer_oop, vt->type()->inline_klass(), 0);\n+      \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+      \/\/ store that would make this buffer accessible by other threads.\n+      AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer_oop, &kit.gvn());\n+      assert(alloc != NULL, \"must have an allocation node\");\n+      kit.insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+      result = buffer_oop;\n+    } else if (result->is_InlineTypePtr() && returned_as_fields) {\n+      result->as_InlineTypePtr()->replace_call_results(&kit, call, C);\n+    }\n+\n@@ -503,0 +580,7 @@\n+  \/\/ AlwaysIncrementalInline causes for_method_handle_inline() to\n+  \/\/ return a LateInlineCallGenerator. Extract the\n+  \/\/ InlineCallGenerato from it.\n+  if (AlwaysIncrementalInline && cg != NULL && cg->is_late_inline()) {\n+    cg = cg->inline_cg();\n+  }\n+\n@@ -509,1 +593,1 @@\n-  if (cg != NULL && cg->is_inline()) {\n+  if (cg != NULL && (cg->is_inline() || cg->is_inlined_method_handle_intrinsic(jvms, cg->method()))) {\n@@ -779,0 +863,22 @@\n+  \/\/ Allocate inline types if they are merged with objects (similar to Parse::merge_common())\n+  uint tos = kit.jvms()->stkoff() + kit.sp();\n+  uint limit = slow_map->req();\n+  for (uint i = TypeFunc::Parms; i < limit; i++) {\n+    Node* m = kit.map()->in(i);\n+    Node* n = slow_map->in(i);\n+    const Type* t = gvn.type(m)->meet_speculative(gvn.type(n));\n+    if (m->is_InlineType() && !t->isa_inlinetype()) {\n+      \/\/ Allocate inline type in fast path\n+      m = m->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, m);\n+    }\n+    if (n->is_InlineType() && !t->isa_inlinetype()) {\n+      \/\/ Allocate inline type in slow path\n+      PreserveJVMState pjvms(&kit);\n+      kit.set_map(slow_map);\n+      n = n->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, n);\n+      slow_map = kit.stop();\n+    }\n+  }\n+\n@@ -802,2 +908,0 @@\n-  uint tos = kit.jvms()->stkoff() + kit.sp();\n-  uint limit = slow_map->req();\n@@ -839,2 +943,2 @@\n-  if (IncrementalInline && call_site_count > 0 &&\n-      (input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())) {\n+  if (IncrementalInline && (AlwaysIncrementalInline ||\n+                            (call_site_count > 0 && (input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())))) {\n@@ -848,0 +952,17 @@\n+static void cast_argument(int nargs, int arg_nb, ciType* t, GraphKit& kit) {\n+  PhaseGVN& gvn = kit.gvn();\n+  Node* arg = kit.argument(arg_nb);\n+  const Type* arg_type = arg->bottom_type();\n+  const Type* sig_type = TypeOopPtr::make_from_klass(t->as_klass());\n+  if (arg_type->isa_oopptr() && !arg_type->higher_equal(sig_type)) {\n+    const Type* narrowed_arg_type = arg_type->join_speculative(sig_type); \/\/ keep speculative part\n+    arg = gvn.transform(new CheckCastPPNode(kit.control(), arg, narrowed_arg_type));\n+    kit.set_argument(arg_nb, arg);\n+  }\n+  if (sig_type->is_inlinetypeptr() && !arg->is_InlineType() &&\n+      !kit.gvn().type(arg)->maybe_null() && t->as_inline_klass()->is_scalarizable()) {\n+    arg = InlineTypeNode::make_from_oop(&kit, arg, t->as_inline_klass());\n+    kit.set_argument(arg_nb, arg);\n+  }\n+}\n+\n@@ -875,1 +996,3 @@\n-                                              PROB_ALWAYS);\n+                                              PROB_ALWAYS,\n+                                              NULL,\n+                                              true);\n@@ -889,0 +1012,1 @@\n+      int nargs = callee->arg_size();\n@@ -890,1 +1014,1 @@\n-      Node* member_name = kit.argument(callee->arg_size() - 1);\n+      Node* member_name = kit.argument(nargs - 1);\n@@ -910,8 +1034,1 @@\n-          Node* arg = kit.argument(0);\n-          const TypeOopPtr* arg_type = arg->bottom_type()->isa_oopptr();\n-          const Type*       sig_type = TypeOopPtr::make_from_klass(signature->accessing_klass());\n-          if (arg_type != NULL && !arg_type->higher_equal(sig_type)) {\n-            const Type* recv_type = arg_type->join_speculative(sig_type); \/\/ keep speculative part\n-            Node* cast_obj = gvn.transform(new CheckCastPPNode(kit.control(), arg, recv_type));\n-            kit.set_argument(0, cast_obj);\n-          }\n+          cast_argument(nargs, 0, signature->accessing_klass(), kit);\n@@ -923,8 +1040,1 @@\n-            Node* arg = kit.argument(receiver_skip + j);\n-            const TypeOopPtr* arg_type = arg->bottom_type()->isa_oopptr();\n-            const Type*       sig_type = TypeOopPtr::make_from_klass(t->as_klass());\n-            if (arg_type != NULL && !arg_type->higher_equal(sig_type)) {\n-              const Type* narrowed_arg_type = arg_type->join_speculative(sig_type); \/\/ keep speculative part\n-              Node* cast_obj = gvn.transform(new CheckCastPPNode(kit.control(), arg, narrowed_arg_type));\n-              kit.set_argument(receiver_skip + j, cast_obj);\n-            }\n+            cast_argument(nargs, receiver_skip + j, t, kit);\n@@ -961,1 +1071,2 @@\n-                                              speculative_receiver_type);\n+                                              speculative_receiver_type,\n+                                              true);\n@@ -1032,1 +1143,1 @@\n-    Node* receiver = kit.null_check_receiver_before_call(method());\n+    kit.null_check_receiver_before_call(method());\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":160,"deletions":49,"binary":false,"changes":209,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -396,1 +397,1 @@\n-bool RegionNode::try_clean_mem_phi(PhaseGVN *phase) {\n+Node* PhiNode::try_clean_mem_phi(PhaseGVN *phase) {\n@@ -416,2 +417,1 @@\n-  PhiNode* phi = has_unique_phi();\n-  if (phi && phi->type() == Type::MEMORY && req() == 3 && phi->is_diamond_phi(true)) {\n+  if (type() == Type::MEMORY && is_diamond_phi(true)) {\n@@ -419,1 +419,2 @@\n-    assert(phi->req() == 3, \"same as region\");\n+    assert(req() == 3, \"same as region\");\n+    Node* r = in(0);\n@@ -421,2 +422,2 @@\n-      Node *mem = phi->in(i);\n-      if (mem && mem->is_MergeMem() && in(i)->outcnt() == 1) {\n+      Node *mem = in(i);\n+      if (mem && mem->is_MergeMem() && r->in(i)->outcnt() == 1) {\n@@ -426,1 +427,1 @@\n-        Node* other = phi->in(j);\n+        Node* other = in(j);\n@@ -430,2 +431,1 @@\n-          phase->is_IterGVN()->replace_node(phi, m);\n-          return true;\n+          return m;\n@@ -436,1 +436,1 @@\n-  return false;\n+  return NULL;\n@@ -451,2 +451,9 @@\n-    if (has_phis && try_clean_mem_phi(phase)) {\n-      has_phis = false;\n+    if (has_phis) {\n+      PhiNode* phi = has_unique_phi();\n+      if (phi != NULL) {\n+        Node* m = phi->try_clean_mem_phi(phase);\n+        if (m != NULL) {\n+          phase->is_IterGVN()->replace_node(phi, m);\n+          has_phis = false;\n+        }\n+      }\n@@ -922,1 +929,1 @@\n-  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at), \"flatten at\");\n+  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at) || (flatten_phi_adr_type(at) == TypeAryPtr::INLINES && Compile::current()->flattened_accesses_share_alias()), \"flatten at\");\n@@ -1132,9 +1139,4 @@\n-  if (ttip != NULL) {\n-    ciKlass* k = ttip->klass();\n-    if (k->is_loaded() && k->is_interface())\n-      is_intf = true;\n-  }\n-  if (ttkp != NULL) {\n-    ciKlass* k = ttkp->klass();\n-    if (k->is_loaded() && k->is_interface())\n-      is_intf = true;\n+  if (ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {\n+    is_intf = true;\n+  } else if (ttkp != NULL && ttkp->is_loaded() && ttkp->klass()->is_interface()) {\n+    is_intf = true;\n@@ -1197,1 +1199,1 @@\n-    if (!t->empty() && ttip && ttip->is_loaded() && ttip->klass()->is_interface()) {\n+    if (!t->empty() && ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {\n@@ -1199,1 +1201,1 @@\n-    } else if (!t->empty() && ttkp && ttkp->is_loaded() && ttkp->klass()->is_interface()) {\n+    } else if (!t->empty() && ttkp != NULL && ttkp->is_loaded() && ttkp->klass()->is_interface()) {\n@@ -1361,0 +1363,8 @@\n+  if (phase->is_IterGVN()) {\n+    Node* m = try_clean_mem_phi(phase);\n+    if (m != NULL) {\n+      return m;\n+    }\n+  }\n+\n+\n@@ -1896,0 +1906,18 @@\n+  \/\/ If all inputs are inline types of the same type, push the inline type node down\n+  \/\/ through the phi because inline type nodes should be merged through their input values.\n+  if (req() > 2 && in(1) != NULL && in(1)->is_InlineTypeBase() && (can_reshape || in(1)->is_InlineType())) {\n+    int opcode = in(1)->Opcode();\n+    uint i = 2;\n+    \/\/ Check if inputs are values of the same type\n+    for (; i < req() && in(i) && in(i)->is_InlineTypeBase() && in(i)->cmp(*in(1)); i++) {\n+      assert(in(i)->Opcode() == opcode, \"mixing pointers and values?\");\n+    }\n+    if (i == req()) {\n+      InlineTypeBaseNode* vt = in(1)->as_InlineTypeBase()->clone_with_phis(phase, in(0));\n+      for (uint i = 2; i < req(); ++i) {\n+        vt->merge_with(phase, in(i)->as_InlineTypeBase(), i, i == (req()-1));\n+      }\n+      return vt;\n+    }\n+  }\n+\n@@ -2185,0 +2213,2 @@\n+    \/\/ TODO revisit this with JDK-8247216\n+    bool mergemem_only = true;\n@@ -2197,0 +2227,2 @@\n+      } else {\n+        mergemem_only = false;\n@@ -2201,1 +2233,1 @@\n-    if (!saw_self && adr_type() == TypePtr::BOTTOM)  merge_width = 0;\n+    if (!mergemem_only && !saw_self && adr_type() == TypePtr::BOTTOM)  merge_width = 0;\n@@ -2653,0 +2685,6 @@\n+\n+  \/\/ CheckCastPPNode::Ideal() for inline types reuses the exception\n+  \/\/ paths of a call to perform an allocation: we can see a Phi here.\n+  if (in(1)->is_Phi()) {\n+    return this;\n+  }\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":63,"deletions":25,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -104,1 +104,0 @@\n-  bool try_clean_mem_phi(PhaseGVN* phase);\n@@ -217,0 +216,1 @@\n+  Node* try_clean_mem_phi(PhaseGVN *phase);\n@@ -404,0 +404,2 @@\n+  bool is_non_flattened_array_check(PhaseTransform* phase, Node** array = NULL);\n+\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -409,0 +410,7 @@\n+  \/\/ Remove useless inline type nodes\n+  for (int i = _inline_type_nodes->length() - 1; i >= 0; i--) {\n+    Node* vt = _inline_type_nodes->at(i);\n+    if (!useful.member(vt)) {\n+      _inline_type_nodes->remove(vt);\n+    }\n+  }\n@@ -636,4 +644,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -646,1 +652,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -783,0 +789,4 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n@@ -938,0 +948,3 @@\n+  _has_flattened_accesses = false;\n+  _flattened_accesses_share_alias = true;\n+\n@@ -1021,0 +1034,1 @@\n+  _inline_type_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);\n@@ -1250,1 +1264,2 @@\n-    assert(InlineUnsafeOps, \"indeterminate pointers come only from unsafe ops\");\n+    bool default_value_load = EnableValhalla && tj->is_instptr()->klass() == ciEnv::current()->Class_klass();\n+    assert(InlineUnsafeOps || default_value_load, \"indeterminate pointers come only from unsafe ops\");\n@@ -1263,0 +1278,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1267,1 +1291,1 @@\n-      tj = ta = TypeAryPtr::make(ptr, ta->ary(), ta->klass(), true, offset, ta->instance_id());\n+      tj = ta = TypeAryPtr::make(ptr, ta->ary(), ta->klass(), true, Type::Offset(offset), ta->field_offset(), ta->instance_id());\n@@ -1273,0 +1297,2 @@\n+    \/\/ For flattened inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1276,1 +1302,1 @@\n-        tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),ta->ary(),ta->klass(),false,offset);\n+        tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1290,1 +1316,1 @@\n-        tj = ta = TypeAryPtr::make(ptr,ta->ary(),ta->klass(),false,offset);\n+        tj = ta = TypeAryPtr::make(ptr,ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1296,1 +1322,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,ta->klass(),false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1301,1 +1327,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());\n@@ -1305,1 +1331,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInlineType::BOTTOM && _flattened_accesses_share_alias) {\n+      const TypeAry *tary = TypeAry::make(TypeInlineType::BOTTOM, ta->size());\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1312,1 +1343,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1318,1 +1349,1 @@\n-      tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta->ary(),ta->klass(),false,offset);\n+      tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1332,1 +1363,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,Type::Offset(offset));\n@@ -1340,1 +1371,1 @@\n-      tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,offset);\n+      tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,Type::Offset(offset));\n@@ -1343,1 +1374,1 @@\n-      tj = to = TypeInstPtr::make(to->ptr(),to->klass(),to->klass_is_exact(),to->const_oop(),to->offset(), to->instance_id());\n+      tj = to = TypeInstPtr::make(to->ptr(),to->klass(),to->klass_is_exact(),to->const_oop(),Type::Offset(to->offset()), to->klass()->flatten_array(), to->instance_id());\n@@ -1350,1 +1381,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, Type::Offset(offset));\n@@ -1364,1 +1395,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, offset, to->instance_id());\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, Type::Offset(offset), canonical_holder->flatten_array(), to->instance_id());\n@@ -1366,1 +1397,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, offset);\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, Type::Offset(offset));\n@@ -1383,1 +1414,1 @@\n-                                   offset);\n+                                   Type::Offset(offset));\n@@ -1387,1 +1418,1 @@\n-    if( klass->is_obj_array_klass() ) {\n+    if (klass != NULL && klass->is_obj_array_klass()) {\n@@ -1391,1 +1422,1 @@\n-      tj = tk = TypeKlassPtr::make( TypePtr::NotNull, k, offset );\n+      tj = tk = TypeKlassPtr::make(TypePtr::NotNull, k, Type::Offset(offset));\n@@ -1407,1 +1438,1 @@\n-      tj = tk = TypeKlassPtr::make( TypePtr::NotNull, tk->klass(), offset );\n+      tj = tk = TypeKlassPtr::make(TypePtr::NotNull, tk->klass(), Type::Offset(offset));\n@@ -1546,1 +1577,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1550,3 +1581,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = NULL;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1602,0 +1636,1 @@\n+    ciField* field = NULL;\n@@ -1608,0 +1643,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1609,1 +1645,9 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (elemtype->isa_inlinetype() &&\n+          elemtype->inline_klass() != NULL &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->first_field_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1621,0 +1665,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1631,1 +1677,0 @@\n-      ciField* field;\n@@ -1638,0 +1683,4 @@\n+      } else if (tinst->klass()->is_inlinetype()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1639,1 +1688,1 @@\n-        ciInstanceKlass *k = tinst->klass()->as_instance_klass();\n+        ciInstanceKlass* k = tinst->klass()->as_instance_klass();\n@@ -1642,7 +1691,14 @@\n-      assert(field == NULL ||\n-             original_field == NULL ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset() == original_field->offset() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != NULL)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == NULL ||\n+           original_field == NULL ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset() == original_field->offset() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != NULL) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_aryptr()->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1653,3 +1709,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1657,6 +1714,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == NULL) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == NULL) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1834,0 +1892,348 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineTypeBase(), \"unexpected node\");\n+  if (_inline_type_nodes != NULL) {\n+    _inline_type_nodes->push(n);\n+  }\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineTypeBase(), \"unexpected node\");\n+  if (_inline_type_nodes != NULL && _inline_type_nodes->contains(n)) {\n+    _inline_type_nodes->remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    assert(!n->is_InlineType(), \"chain of inline type nodes\");\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineTypePtr()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool post_ea) {\n+  \/\/ Make inline types scalar in safepoints\n+  for (int i = _inline_type_nodes->length()-1; i >= 0; i--) {\n+    InlineTypeBaseNode* vt = _inline_type_nodes->at(i)->as_InlineTypeBase();\n+    vt->make_scalar_in_safepoints(&igvn);\n+  }\n+  \/\/ Remove InlineTypePtr nodes only after EA to give scalar replacement a chance\n+  \/\/ to remove buffer allocations. InlineType nodes are kept until loop opts and\n+  \/\/ removed via InlineTypeNode::remove_redundant_allocations.\n+  if (post_ea) {\n+    while (_inline_type_nodes->length() > 0) {\n+      InlineTypeBaseNode* vt = _inline_type_nodes->pop()->as_InlineTypeBase();\n+      if (vt->is_InlineTypePtr()) {\n+        igvn.replace_node(vt, vt->get_oop());\n+      }\n+    }\n+  }\n+  \/\/ Make sure that the return value does not keep an unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = NULL;\n+    for (uint i = 1; i < root()->req(); i++){\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == NULL, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != NULL) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flattened_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flattened_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flattened array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flattened array) correct. We're done with parsing so we\n+  \/\/ now know all flattened array accesses in this compile\n+  \/\/ unit. Let's move flattened array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flattened memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flattened array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = NULL;\n+      if (n->Opcode() == Op_StoreCM) {\n+        adr_type = get_adr_type(get_alias_index(n->in(MemNode::OopStore)->adr_type()));\n+      } else {\n+        adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      }\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != NULL) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flattened_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flattened array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != NULL &&\n+          ace->_adr_type->isa_aryptr() &&\n+          ace->_adr_type->is_aryptr()->is_flat()) {\n+        ace->_adr_type = NULL;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the NULL adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = NULL;\n+      if (m->Opcode() == Op_StoreCM) {\n+        adr_type = m->in(MemNode::OopStore)->adr_type();\n+        Node* clone = new StoreCMNode(m->in(MemNode::Control), m->in(MemNode::Memory), m->in(MemNode::Address),\n+                                      m->adr_type(), m->in(MemNode::ValueIn), m->in(MemNode::OopStore),\n+                                      get_alias_index(adr_type));\n+        igvn.register_new_node_with_optimizer(clone);\n+        igvn.replace_node(m, clone);\n+      } else {\n+        adr_type = m->adr_type();\n+#ifdef ASSERT\n+        m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      }\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flattened array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = NULL;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = NULL;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == NULL) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const Type* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat()) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flattened arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flattened array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const Type* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat()) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != NULL) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct();\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const Type* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+}\n+\n+\n@@ -2113,0 +2519,7 @@\n+  if (_inline_type_nodes->length() > 0) {\n+    \/\/ Do this once all inlining is over to avoid getting inconsistent debug info\n+    process_inline_types(igvn);\n+  }\n+\n+  adjust_flattened_array_access_aliases(igvn);\n+\n@@ -2145,0 +2558,5 @@\n+  if (_inline_type_nodes->length() > 0) {\n+    \/\/ Process inline types again now that EA might have simplified the graph\n+    process_inline_types(igvn, \/* post_ea= *\/ true);\n+  }\n+\n@@ -2784,0 +3202,1 @@\n+\n@@ -3507,0 +3926,8 @@\n+#ifdef ASSERT\n+  case Op_InlineTypePtr:\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n+#endif\n@@ -3854,2 +4281,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -3863,1 +4290,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -3984,1 +4411,1 @@\n-  if (StressReflectiveCode) {\n+  if (StressReflectiveCode || superk == NULL || subk == NULL) {\n@@ -3993,1 +4420,2 @@\n-  if (superelem->is_array_klass())\n+  if (superelem->is_array_klass()) {\n+    ciArrayKlass* ak = superelem->as_array_klass();\n@@ -3995,0 +4423,1 @@\n+  }\n@@ -4455,0 +4884,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == NULL || tb == NULL ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are NULL.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(NULL, a));\n+    b = phase->transform(new CastP2XNode(NULL, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":502,"deletions":52,"binary":false,"changes":554,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+class CallNode;\n@@ -87,0 +88,1 @@\n+class InlineTypeBaseNode;\n@@ -303,0 +305,2 @@\n+  bool                  _has_flattened_accesses; \/\/ Any known flattened array accesses?\n+  bool                  _flattened_accesses_share_alias; \/\/ Initially all flattened array share a single slice\n@@ -318,0 +322,1 @@\n+  GrowableArray<Node*>* _inline_type_nodes;     \/\/ List of InlineType nodes\n@@ -598,0 +603,7 @@\n+  void          set_flattened_accesses()         { _has_flattened_accesses = true; }\n+  bool          flattened_accesses_share_alias() const { return _flattened_accesses_share_alias; }\n+  void          set_flattened_accesses_share_alias(bool z) { _flattened_accesses_share_alias = z; }\n+\n+  \/\/ Support for scalarized inline type calling convention\n+  bool              has_scalarized_args() const  { return _method != NULL && _method->has_scalarized_args(); }\n+  bool              needs_stack_repair()  const  { return _method != NULL && _method->get_Method()->c2_needs_stack_repair(); }\n@@ -708,0 +720,7 @@\n+  \/\/ Keep track of inline type nodes for later processing\n+  void add_inline_type(Node* n);\n+  void remove_inline_type(Node* n);\n+  void process_inline_types(PhaseIterGVN &igvn, bool post_ea = false);\n+\n+  void adjust_flattened_array_access_aliases(PhaseIterGVN& igvn);\n+\n@@ -848,1 +867,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = NULL) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = NULL, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }\n@@ -852,1 +871,1 @@\n-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }\n+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, NULL, uncached)->index(); }\n@@ -1068,1 +1087,1 @@\n-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);\n+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);\n@@ -1141,1 +1160,3 @@\n-  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  Node* optimize_acmp(PhaseGVN* phase, Node* a, Node* b);\n+\n+  \/\/ Auxiliary method for randomized fuzzing\/stressing\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":25,"deletions":4,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -188,1 +189,1 @@\n-          } else if ((should_delay || AlwaysIncrementalInline)) {\n+          } else if (should_delay || AlwaysIncrementalInline) {\n@@ -541,1 +542,6 @@\n-    const TypeOopPtr* receiver_type = _gvn.type(receiver_node)->isa_oopptr();\n+    const TypeOopPtr* receiver_type = NULL;\n+    if (receiver_node->is_InlineType()) {\n+      receiver_type = TypeInstPtr::make(TypePtr::NotNull, _gvn.type(receiver_node)->inline_klass());\n+    } else {\n+      receiver_type = _gvn.type(receiver_node)->isa_oopptr();\n+    }\n@@ -557,1 +563,1 @@\n-  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_initializer()) {\n+  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_constructor()) {\n@@ -628,1 +634,1 @@\n-  if (receiver != NULL && !call_does_dispatch && !cg->is_string_late_inline()) {\n+  if (receiver != NULL && !receiver->is_InlineType() && !call_does_dispatch && !cg->is_string_late_inline()) {\n@@ -696,1 +702,1 @@\n-          \/\/ It's OK for a method  to return a value that is discarded.\n+          \/\/ It's OK for a method to return a value that is discarded.\n@@ -708,1 +714,4 @@\n-            if (arg_type != NULL && !arg_type->higher_equal(sig_type)) {\n+            if (ct == T_INLINE_TYPE) {\n+              sig_type = sig_type->join_speculative(TypePtr::NOTNULL);\n+            }\n+            if (arg_type != NULL && !arg_type->higher_equal(sig_type) && !peek()->is_InlineType()) {\n@@ -734,0 +743,9 @@\n+    if (rtype->basic_type() == T_INLINE_TYPE && !peek()->is_InlineType()) {\n+      Node* retnode = pop();\n+      assert(!gvn().type(retnode)->maybe_null(), \"should never be null\");\n+      if (rtype->as_inline_klass()->is_scalarizable()) {\n+        retnode = InlineTypeNode::make_from_oop(this, retnode, rtype->as_inline_klass());\n+      }\n+      push_node(T_INLINE_TYPE, retnode);\n+    }\n+\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":24,"deletions":6,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -38,0 +40,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -41,0 +44,1 @@\n+#include \"opto\/narrowptrnode.hpp\"\n@@ -53,1 +57,1 @@\n-GraphKit::GraphKit(JVMState* jvms)\n+GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)\n@@ -56,1 +60,1 @@\n-    _gvn(*C->initial_gvn()),\n+    _gvn((gvn != NULL) ? *gvn : *C->initial_gvn()),\n@@ -59,0 +63,1 @@\n+  assert(gvn == NULL || !gvn->is_IterGVN() || gvn->is_IterGVN()->delay_transform(), \"delay transform should be enabled\");\n@@ -62,0 +67,7 @@\n+#ifdef ASSERT\n+  if (_gvn.is_IterGVN() != NULL) {\n+    assert(_gvn.is_IterGVN()->delay_transform(), \"Transformation must be delayed if IterGVN is used\");\n+    \/\/ Save the initial size of _for_igvn worklist for verification (see ~GraphKit)\n+    _worklist_size = _gvn.C->for_igvn()->size();\n+  }\n+#endif\n@@ -830,1 +842,1 @@\n-           (is_anewarray && code == Bytecodes::_multianewarray);\n+           (is_anewarray && (code == Bytecodes::_multianewarray));\n@@ -838,1 +850,1 @@\n-  } else\n+  } else {\n@@ -840,0 +852,1 @@\n+  }\n@@ -1083,0 +1096,9 @@\n+  case Bytecodes::_withfield: {\n+    bool ignored_will_link;\n+    ciField* field = method()->get_field_at_bci(bci(), ignored_will_link);\n+    int      size  = field->type()->size();\n+    inputs = size+1;\n+    depth = rsize - inputs;\n+    break;\n+  }\n+\n@@ -1165,1 +1187,1 @@\n-  return _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+  return _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeKlassPtr::OBJECT));\n@@ -1208,0 +1230,1 @@\n+    case T_INLINE_TYPE : \/\/ fall through\n@@ -1379,0 +1402,16 @@\n+Node* GraphKit::null2default(Node* value, ciInlineKlass* vk) {\n+  assert(!vk->is_scalarizable(), \"Should only be used for non scalarizable inline klasses\");\n+  Node* null_ctl = top();\n+  value = null_check_oop(value, &null_ctl);\n+  if (!null_ctl->is_top()) {\n+    \/\/ Return default value if oop is null\n+    Node* region = new RegionNode(3);\n+    region->init_req(1, control());\n+    region->init_req(2, null_ctl);\n+    value = PhiNode::make(region, value, TypeInstPtr::make(TypePtr::BotPTR, vk));\n+    value->set_req(2, InlineTypeNode::default_oop(gvn(), vk));\n+    set_control(gvn().transform(region));\n+    value = gvn().transform(value);\n+  }\n+  return value;\n+}\n@@ -1383,0 +1422,3 @@\n+  if (obj->is_InlineType()) {\n+    return obj;\n+  }\n@@ -1392,0 +1434,5 @@\n+  if (t->is_inlinetypeptr() && t->inline_klass()->is_scalarizable()) {\n+    \/\/ Scalarize inline type now that we know it's non-null\n+    cast = InlineTypeNode::make_from_oop(this, cast, t->inline_klass())->as_ptr(&gvn());\n+  }\n+\n@@ -1515,1 +1562,2 @@\n-  if (((bt == T_OBJECT) && C->do_escape_analysis()) || C->eliminate_boxing()) {\n+\n+  if (((bt == T_OBJECT || bt == T_INLINE_TYPE) && C->do_escape_analysis()) || C->eliminate_boxing()) {\n@@ -1566,1 +1614,2 @@\n-                                DecoratorSet decorators) {\n+                                DecoratorSet decorators,\n+                                bool safe_for_replace) {\n@@ -1579,0 +1628,7 @@\n+  if (val->is_InlineType()) {\n+    \/\/ Store to non-flattened field. Buffer the inline type and make sure\n+    \/\/ the store is re-executed if the allocation triggers deoptimization.\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    val = val->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n@@ -1595,1 +1651,2 @@\n-                               DecoratorSet decorators) {\n+                               DecoratorSet decorators,\n+                               Node* ctl) {\n@@ -1601,1 +1658,1 @@\n-  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr, ctl);\n@@ -1699,2 +1756,2 @@\n-void GraphKit::access_clone(Node* src, Node* dst, Node* size, bool is_array) {\n-  return _barrier_set->clone(this, src, dst, size, is_array);\n+void GraphKit::access_clone(Node* src_base, Node* dst_base, Node* countx, bool is_array) {\n+  return _barrier_set->clone(this, src_base, dst_base, countx, is_array);\n@@ -1707,0 +1764,5 @@\n+  ciKlass* arytype_klass = _gvn.type(ary)->is_aryptr()->klass();\n+  if (arytype_klass != NULL && arytype_klass->is_flat_array_klass()) {\n+    ciFlatArrayKlass* vak = arytype_klass->as_flat_array_klass();\n+    shift = vak->log2_element_size();\n+  }\n@@ -1727,0 +1789,1 @@\n+  assert(elembt != T_INLINE_TYPE, \"inline types are not supported by this method\");\n@@ -1737,6 +1800,32 @@\n-void GraphKit::set_arguments_for_java_call(CallJavaNode* call) {\n-  \/\/ Add the call arguments:\n-  uint nargs = call->method()->arg_size();\n-  for (uint i = 0; i < nargs; i++) {\n-    Node* arg = argument(i);\n-    call->init_req(i + TypeFunc::Parms, arg);\n+void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {\n+  PreserveReexecuteState preexecs(this);\n+  if (EnableValhalla) {\n+    \/\/ Make sure the call is re-executed, if buffering of inline type arguments triggers deoptimization\n+    jvms()->set_should_reexecute(true);\n+    int arg_size = method()->get_declared_signature_at_bci(bci())->arg_size_for_bc(java_bc());\n+    inc_sp(arg_size);\n+  }\n+  \/\/ Add the call arguments\n+  const TypeTuple* domain = call->tf()->domain_sig();\n+  ExtendedSignature sig_cc = ExtendedSignature(call->method()->get_sig_cc(), SigEntryFilter());\n+  uint nargs = domain->cnt();\n+  for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i < nargs; i++) {\n+    Node* arg = argument(i-TypeFunc::Parms);\n+    const Type* t = domain->field_at(i);\n+    if (call->method()->has_scalarized_args() && t->is_inlinetypeptr() && !t->maybe_null() && t->inline_klass()->can_be_passed_as_fields()) {\n+      \/\/ We don't pass inline type arguments by reference but instead pass each field of the inline type\n+      InlineTypeNode* vt = arg->as_InlineType();\n+      vt->pass_fields(this, call, sig_cc, idx);\n+      \/\/ If an inline type argument is passed as fields, attach the Method* to the call site\n+      \/\/ to be able to access the extended signature later via attached_method_before_pc().\n+      \/\/ For example, see CompiledMethod::preserve_callee_argument_oops().\n+      call->set_override_symbolic_info(true);\n+      continue;\n+    } else if (arg->is_InlineType()) {\n+      \/\/ Pass inline type argument via oop to callee\n+      arg = arg->as_InlineType()->buffer(this);\n+      if (!is_late_inline) {\n+        arg = arg->as_InlineTypePtr()->get_oop();\n+      }\n+    }\n+    call->init_req(idx++, arg);\n@@ -1780,7 +1869,0 @@\n-  \/\/ Capture the return value, if any.\n-  Node* ret;\n-  if (call->method() == NULL ||\n-      call->method()->return_type()->basic_type() == T_VOID)\n-        ret = top();\n-  else  ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n-\n@@ -1799,0 +1881,19 @@\n+\n+  \/\/ Capture the return value, if any.\n+  Node* ret;\n+  if (call->method() == NULL || call->method()->return_type()->basic_type() == T_VOID) {\n+    ret = top();\n+  } else if (call->tf()->returns_inline_type_as_fields()) {\n+    \/\/ Return of multiple values (inline type fields): we create a\n+    \/\/ InlineType node, each field is a projection from the call.\n+    ciInlineKlass* vk = call->method()->return_type()->as_inline_klass();\n+    const Array<SigEntry>* sig_array = vk->extended_sig();\n+    GrowableArray<SigEntry> sig = GrowableArray<SigEntry>(sig_array->length());\n+    sig.appendAll(sig_array);\n+    ExtendedSignature sig_cc = ExtendedSignature(&sig, SigEntryFilter());\n+    uint base_input = TypeFunc::Parms + 1;\n+    ret = InlineTypeNode::make_from_multi(this, call, sig_cc, vk, base_input, false);\n+  } else {\n+    ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+  }\n+\n@@ -1889,2 +1990,1 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true);\n+  CallProjections* callprojs = call->extract_projections(true);\n@@ -1899,2 +1999,2 @@\n-  if (callprojs.fallthrough_catchproj != NULL) {\n-    C->gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);\n+  if (callprojs->fallthrough_catchproj != NULL) {\n+    C->gvn_replace_by(callprojs->fallthrough_catchproj, final_ctl);\n@@ -1902,1 +2002,1 @@\n-  if (callprojs.fallthrough_memproj != NULL) {\n+  if (callprojs->fallthrough_memproj != NULL) {\n@@ -1907,1 +2007,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);\n+    C->gvn_replace_by(callprojs->fallthrough_memproj,   final_mem);\n@@ -1910,2 +2010,2 @@\n-  if (callprojs.fallthrough_ioproj != NULL) {\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);\n+  if (callprojs->fallthrough_ioproj != NULL) {\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj,    final_io);\n@@ -1915,2 +2015,3 @@\n-  if (callprojs.resproj != NULL && result != NULL) {\n-    C->gvn_replace_by(callprojs.resproj, result);\n+  if (callprojs->resproj[0] != NULL && result != NULL) {\n+    assert(callprojs->nb_resproj == 1, \"unexpected number of results\");\n+    C->gvn_replace_by(callprojs->resproj[0], result);\n@@ -1921,2 +2022,2 @@\n-    if (callprojs.catchall_catchproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, C->top());\n+    if (callprojs->catchall_catchproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, C->top());\n@@ -1924,2 +2025,2 @@\n-    if (callprojs.catchall_memproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_memproj,   C->top());\n+    if (callprojs->catchall_memproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_memproj,   C->top());\n@@ -1927,2 +2028,2 @@\n-    if (callprojs.catchall_ioproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    C->top());\n+    if (callprojs->catchall_ioproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    C->top());\n@@ -1931,2 +2032,2 @@\n-    if (callprojs.exobj != NULL) {\n-      C->gvn_replace_by(callprojs.exobj, C->top());\n+    if (callprojs->exobj != NULL) {\n+      C->gvn_replace_by(callprojs->exobj, C->top());\n@@ -1943,2 +2044,2 @@\n-    if (callprojs.catchall_catchproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, ekit.control());\n+    if (callprojs->catchall_catchproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, ekit.control());\n@@ -1947,1 +2048,1 @@\n-    if (callprojs.catchall_memproj != NULL) {\n+    if (callprojs->catchall_memproj != NULL) {\n@@ -1949,1 +2050,1 @@\n-      C->gvn_replace_by(callprojs.catchall_memproj,   ex_mem);\n+      C->gvn_replace_by(callprojs->catchall_memproj,   ex_mem);\n@@ -1952,2 +2053,2 @@\n-    if (callprojs.catchall_ioproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());\n+    if (callprojs->catchall_ioproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    ekit.i_o());\n@@ -1957,2 +2058,2 @@\n-    if (callprojs.exobj != NULL) {\n-      C->gvn_replace_by(callprojs.exobj, ex_oop);\n+    if (callprojs->exobj != NULL) {\n+      C->gvn_replace_by(callprojs->exobj, ex_oop);\n@@ -1972,1 +2073,1 @@\n-  if (callprojs.fallthrough_catchproj != NULL && !final_ctl->is_top() && do_replaced_nodes) {\n+  if (callprojs->fallthrough_catchproj != NULL && !final_ctl->is_top() && do_replaced_nodes) {\n@@ -2170,1 +2271,1 @@\n-    const TypePtr* ptr = (ptr_kind == ProfileMaybeNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n+    const TypePtr* ptr = (ptr_kind != ProfileNeverNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n@@ -2193,1 +2294,1 @@\n-    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::OffsetBot, TypeOopPtr::InstanceBot, speculative);\n+    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, speculative);\n@@ -2227,2 +2328,9 @@\n-      if (!data->as_BitData()->null_seen()) {\n-        ptr_kind = ProfileNeverNull;\n+      if (java_bc() == Bytecodes::_aastore) {\n+        ciKlass* array_type = NULL;\n+        ciKlass* element_type = NULL;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+        exact_kls = element_type;\n+        ptr_kind = element_ptr;\n@@ -2230,7 +2338,11 @@\n-        assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n-        ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n-        uint i = 0;\n-        for (; i < call->row_limit(); i++) {\n-          ciKlass* receiver = call->receiver(i);\n-          if (receiver != NULL) {\n-            break;\n+        if (!data->as_BitData()->null_seen()) {\n+          ptr_kind = ProfileNeverNull;\n+        } else {\n+          assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n+          ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n+          uint i = 0;\n+          for (; i < call->row_limit(); i++) {\n+            ciKlass* receiver = call->receiver(i);\n+            if (receiver != NULL) {\n+              break;\n+            }\n@@ -2238,0 +2350,1 @@\n+          ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2239,1 +2352,0 @@\n-        ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2258,1 +2370,1 @@\n-  int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+  int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2261,1 +2373,1 @@\n-    const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+    const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2332,1 +2444,1 @@\n-    int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+    int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2334,1 +2446,1 @@\n-      const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+      const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2814,0 +2926,4 @@\n+  const Type* sub_t = _gvn.type(obj_or_subklass);\n+  if (sub_t->isa_inlinetype()) {\n+    obj_or_subklass = makecon(TypeKlassPtr::make(sub_t->inline_klass()));\n+  }\n@@ -2818,1 +2934,1 @@\n-    if (!_gvn.type(obj_or_subklass)->isa_klassptr()) {\n+    if (!sub_t->isa_klassptr()) {\n@@ -2821,1 +2937,0 @@\n-\n@@ -2827,1 +2942,0 @@\n-  const TypePtr* adr_type = TypeKlassPtr::make(TypePtr::NotNull, C->env()->Object_klass(), Type::OffsetBot);\n@@ -2837,2 +2951,13 @@\n-                                    float prob,\n-                                    Node* *casted_receiver) {\n+                                    float prob, Node* *casted_receiver) {\n+  Node* fail = top();\n+  const Type* rec_t = _gvn.type(receiver);\n+  if (rec_t->isa_inlinetype()) {\n+    if (klass->equals(rec_t->inline_klass())) {\n+      (*casted_receiver) = receiver; \/\/ Always passes\n+    } else {\n+      (*casted_receiver) = top();    \/\/ Always fails\n+      fail = control();\n+      set_control(top());\n+    }\n+    return fail;\n+  }\n@@ -2841,7 +2966,1 @@\n-  Node* want_klass = makecon(tklass);\n-  Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass) );\n-  Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );\n-  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n-  set_control( _gvn.transform( new IfTrueNode (iff) ));\n-  Node* fail = _gvn.transform( new IfFalseNode(iff) );\n-\n+  fail = type_check(recv_klass, tklass, prob);\n@@ -2854,1 +2973,7 @@\n-  (*casted_receiver) = _gvn.transform(cast);\n+  Node* res = _gvn.transform(cast);\n+  if (recv_xtype->is_inlinetypeptr() && recv_xtype->inline_klass()->is_scalarizable()) {\n+    assert(!gvn().type(res)->maybe_null(), \"receiver should never be null\");\n+    res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass());\n+  }\n+\n+  (*casted_receiver) = res;\n@@ -2860,0 +2985,11 @@\n+Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,\n+                           float prob) {\n+  Node* want_klass = makecon(tklass);\n+  Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass));\n+  Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );\n+  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n+  set_control(  _gvn.transform( new IfTrueNode (iff)));\n+  Node* fail = _gvn.transform( new IfFalseNode(iff));\n+  return fail;\n+}\n+\n@@ -2899,0 +3035,3 @@\n+    if (java_bc() == Bytecodes::_aastore) {\n+      return ((ciArrayLoadStoreData*)data->as_ArrayLoadStoreData())->element()->ptr_kind() == ProfileNeverNull;\n+    }\n@@ -2978,1 +3117,14 @@\n-  ciKlass* exact_kls = spec_klass == NULL ? profile_has_unique_klass() : spec_klass;\n+  ciKlass* exact_kls = spec_klass;\n+  if (exact_kls == NULL) {\n+    if (java_bc() == Bytecodes::_aastore) {\n+      ciKlass* array_type = NULL;\n+      ciKlass* element_type = NULL;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      exact_kls = element_type;\n+    } else {\n+      exact_kls = profile_has_unique_klass();\n+    }\n+  }\n@@ -3083,0 +3235,1 @@\n+  bool is_value = obj->is_InlineType();\n@@ -3086,1 +3239,1 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = is_value ? obj : null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n@@ -3104,7 +3257,9 @@\n-  bool known_statically = false;\n-  if (_gvn.type(superklass)->singleton()) {\n-    ciKlass* superk = _gvn.type(superklass)->is_klassptr()->klass();\n-    ciKlass* subk = _gvn.type(obj)->is_oopptr()->klass();\n-    if (subk != NULL && subk->is_loaded()) {\n-      int static_res = C->static_subtype_check(superk, subk);\n-      known_statically = (static_res == Compile::SSC_always_true || static_res == Compile::SSC_always_false);\n+  if (!is_value) {\n+    bool known_statically = false;\n+    if (_gvn.type(superklass)->singleton()) {\n+      ciKlass* superk = _gvn.type(superklass)->is_klassptr()->klass();\n+      ciKlass* subk = _gvn.type(obj)->is_oopptr()->klass();\n+      if (subk != NULL && subk->is_loaded()) {\n+        int static_res = C->static_subtype_check(superk, subk);\n+        known_statically = (static_res == Compile::SSC_always_true || static_res == Compile::SSC_always_false);\n+      }\n@@ -3112,14 +3267,17 @@\n-  }\n-  if (!known_statically) {\n-    const TypeOopPtr* obj_type = _gvn.type(obj)->is_oopptr();\n-    \/\/ We may not have profiling here or it may not help us. If we\n-    \/\/ have a speculative type use it to perform an exact cast.\n-    ciKlass* spec_obj_type = obj_type->speculative_type();\n-    if (spec_obj_type != NULL || (ProfileDynamicTypes && data != NULL)) {\n-      Node* cast_obj = maybe_cast_profiled_receiver(not_null_obj, NULL, spec_obj_type, safe_for_replace);\n-      if (stopped()) {            \/\/ Profile disagrees with this path.\n-        set_control(null_ctl);    \/\/ Null is the only remaining possibility.\n-        return intcon(0);\n-      }\n-      if (cast_obj != NULL) {\n-        not_null_obj = cast_obj;\n+    if (!known_statically) {\n+      const TypeOopPtr* obj_type = _gvn.type(obj)->is_oopptr();\n+      \/\/ We may not have profiling here or it may not help us. If we\n+      \/\/ have a speculative type use it to perform an exact cast.\n+      ciKlass* spec_obj_type = obj_type->speculative_type();\n+      if (spec_obj_type != NULL || (ProfileDynamicTypes && data != NULL)) {\n+        Node* cast_obj = maybe_cast_profiled_receiver(not_null_obj, NULL, spec_obj_type, safe_for_replace);\n+        if (stopped()) {            \/\/ Profile disagrees with this path.\n+          set_control(null_ctl);    \/\/ Null is the only remaining possibility.\n+          return intcon(0);\n+        }\n+        if (cast_obj != NULL &&\n+            \/\/ A value that's sometimes null is not something we can optimize well\n+            !(cast_obj->is_InlineType() && null_ctl != top())) {\n+          not_null_obj = cast_obj;\n+          is_value = not_null_obj->is_InlineType();\n+        }\n@@ -3149,1 +3307,1 @@\n-  if (safe_for_replace) {\n+  if (safe_for_replace && !is_value) {\n@@ -3164,2 +3322,1 @@\n-Node* GraphKit::gen_checkcast(Node *obj, Node* superklass,\n-                              Node* *failure_control) {\n+Node* GraphKit::gen_checkcast(Node *obj, Node* superklass, Node* *failure_control) {\n@@ -3167,2 +3324,6 @@\n-  const TypeKlassPtr *tk = _gvn.type(superklass)->is_klassptr();\n-  const Type *toop = TypeOopPtr::make_from_klass(tk->klass());\n+  const TypeKlassPtr* tk = _gvn.type(superklass)->is_klassptr();\n+  const TypeOopPtr* toop = TypeOopPtr::make_from_klass(tk->klass());\n+\n+  \/\/ Check if inline types are involved\n+  bool from_inline = obj->is_InlineType();\n+  bool to_inline = tk->klass()->is_inlinetype();\n@@ -3177,3 +3338,11 @@\n-    const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n-    if (objtp != NULL && objtp->klass() != NULL) {\n-      switch (C->static_subtype_check(tk->klass(), objtp->klass())) {\n+    ciKlass* klass = NULL;\n+    if (from_inline) {\n+      klass = _gvn.type(obj)->inline_klass();\n+    } else {\n+      const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n+      if (objtp != NULL) {\n+        klass = objtp->klass();\n+      }\n+    }\n+    if (klass != NULL) {\n+      switch (C->static_subtype_check(tk->klass(), klass)) {\n@@ -3184,1 +3353,10 @@\n-        return record_profiled_receiver_for_speculation(obj);\n+        if (!from_inline) {\n+          obj = record_profiled_receiver_for_speculation(obj);\n+          if (to_inline) {\n+            obj = null_check(obj);\n+            if (toop->inline_klass()->is_scalarizable()) {\n+              obj = InlineTypeNode::make_from_oop(this, obj, toop->inline_klass());\n+            }\n+          }\n+        }\n+        return obj;\n@@ -3186,3 +3364,11 @@\n-        \/\/ It needs a null check because a null will *pass* the cast check.\n-        \/\/ A non-null value will always produce an exception.\n-        return null_assert(obj);\n+        if (from_inline || to_inline) {\n+          if (!from_inline) {\n+            null_check(obj);\n+          }\n+          \/\/ Inline type is never null. Always throw an exception.\n+          builtin_throw(Deoptimization::Reason_class_check, makecon(TypeKlassPtr::make(klass)));\n+          return top();\n+        } else {\n+          \/\/ It needs a null check because a null will *pass* the cast check.\n+          return null_assert(obj);\n+        }\n@@ -3199,1 +3385,3 @@\n-    data = method()->method_data()->bci_to_data(bci());\n+    if (method()->method_data()->is_mature()) {\n+      data = method()->method_data()->bci_to_data(bci());\n+    }\n@@ -3207,0 +3395,3 @@\n+  _gvn.set_type(region, Type::CONTROL);\n+  _gvn.set_type(phi, toop);\n+\n@@ -3216,1 +3407,8 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = NULL;\n+  if (from_inline) {\n+    not_null_obj = obj;\n+  } else if (to_inline) {\n+    not_null_obj = null_check(obj);\n+  } else {\n+    not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  }\n@@ -3234,1 +3432,1 @@\n-  if (tk->klass_is_exact()) {\n+  if (!from_inline && tk->klass_is_exact()) {\n@@ -3245,0 +3443,7 @@\n+      if (cast_obj != NULL && cast_obj->is_InlineType()) {\n+        if (null_ctl != top()) {\n+          cast_obj = NULL; \/\/ A value that's sometimes null is not something we can optimize well\n+        } else {\n+          return cast_obj;\n+        }\n+      }\n@@ -3256,1 +3461,1 @@\n-    Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass );\n+    Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass);\n@@ -3259,1 +3464,1 @@\n-    cast_obj = _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));\n+    cast_obj = from_inline ? not_null_obj : _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));\n@@ -3265,1 +3470,7 @@\n-        builtin_throw(Deoptimization::Reason_class_check, load_object_klass(not_null_obj));\n+        Node* obj_klass = NULL;\n+        if (from_inline) {\n+          obj_klass = makecon(TypeKlassPtr::make(_gvn.type(not_null_obj)->inline_klass()));\n+        } else {\n+          obj_klass = load_object_klass(not_null_obj);\n+        }\n+        builtin_throw(Deoptimization::Reason_class_check, obj_klass);\n@@ -3292,1 +3503,134 @@\n-  return record_profiled_receiver_for_speculation(res);\n+  bool not_inline = !toop->can_be_inline_type();\n+  bool not_flattened = !UseFlatArray || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->flatten_array());\n+  if (EnableValhalla && not_flattened) {\n+    \/\/ Check if obj has been loaded from an array\n+    obj = obj->isa_DecodeN() ? obj->in(1) : obj;\n+    Node* array = NULL;\n+    if (obj->isa_Load()) {\n+      Node* address = obj->in(MemNode::Address);\n+      if (address->isa_AddP()) {\n+        array = address->as_AddP()->in(AddPNode::Base);\n+      }\n+    } else if (obj->is_Phi()) {\n+      Node* region = obj->in(0);\n+      \/\/ TODO make this more robust (see JDK-8231346)\n+      if (region->req() == 3 && region->in(2) != NULL && region->in(2)->in(0) != NULL) {\n+        IfNode* iff = region->in(2)->in(0)->isa_If();\n+        if (iff != NULL) {\n+          iff->is_non_flattened_array_check(&_gvn, &array);\n+        }\n+      }\n+    }\n+    if (array != NULL) {\n+      const TypeAryPtr* ary_t = _gvn.type(array)->isa_aryptr();\n+      if (ary_t != NULL) {\n+        if (!ary_t->is_not_null_free() && not_inline) {\n+          \/\/ Casting array element to a non-inline-type, mark array as not null-free.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_null_free()));\n+          replace_in_map(array, cast);\n+        } else if (!ary_t->is_not_flat()) {\n+          \/\/ Casting array element to a non-flattened type, mark array as not flat.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_flat()));\n+          replace_in_map(array, cast);\n+        }\n+      }\n+    }\n+  }\n+\n+  if (!from_inline) {\n+    res = record_profiled_receiver_for_speculation(res);\n+    if (to_inline && toop->inline_klass()->is_scalarizable()) {\n+      assert(!gvn().type(res)->maybe_null(), \"Inline types are null-free\");\n+      res = InlineTypeNode::make_from_oop(this, res, toop->inline_klass());\n+    }\n+  }\n+  return res;\n+}\n+\n+\/\/ Check if 'obj' is an inline type by checking if it has the always_locked markWord pattern set.\n+Node* GraphKit::inline_type_test(Node* obj) {\n+  Node* mark_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+  Node* mark = make_load(NULL, mark_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  Node* mask = _gvn.MakeConX(markWord::always_locked_pattern);\n+  Node* andx = _gvn.transform(new AndXNode(mark, mask));\n+  return _gvn.transform(new CmpXNode(andx, mask));\n+}\n+\n+Node* GraphKit::is_inline_type(Node* obj) {\n+  return _gvn.transform(new BoolNode(inline_type_test(obj), BoolTest::eq));\n+}\n+\n+Node* GraphKit::is_not_inline_type(Node* obj) {\n+  return _gvn.transform(new BoolNode(inline_type_test(obj), BoolTest::ne));\n+}\n+\n+\/\/ Check if 'ary' is a non-flattened array\n+Node* GraphKit::is_non_flattened_array(Node* ary) {\n+  Node* kls = load_object_klass(ary);\n+  Node* cmp = gen_lh_array_test(kls, Klass::_lh_array_tag_vt_value);\n+  return _gvn.transform(new BoolNode(cmp, BoolTest::ne));\n+}\n+\n+\/\/ Check bit that determines if an array is null-free\n+Node* GraphKit::check_null_free_bit(Node* klass, bool null_free) {\n+  Node* lhp = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+  Node* layout_val = _gvn.transform(LoadNode::make(_gvn, NULL, immutable_memory(), lhp, lhp->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+  Node* bit = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_null_free_shift)));\n+  bit = _gvn.transform(new AndINode(bit, intcon(Klass::_lh_null_free_mask)));\n+  Node* cmp = _gvn.transform(new CmpINode(bit, intcon(0)));\n+  return _gvn.transform(new BoolNode(cmp, null_free ? BoolTest::ne : BoolTest::eq));\n+}\n+\n+\/\/ Check if 'ary' is a nullable array\n+Node* GraphKit::is_nullable_array(Node* ary) {\n+  Node* kls = load_object_klass(ary);\n+  return check_null_free_bit(kls, false);\n+}\n+\n+\/\/ Deoptimize if 'ary' is a null-free inline type array and 'val' is null\n+Node* GraphKit::gen_inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {\n+  const Type* val_t = _gvn.type(val);\n+  if (val->is_InlineType() || !TypePtr::NULL_PTR->higher_equal(val_t)) {\n+    return ary; \/\/ Never null\n+  }\n+  RegionNode* region = new RegionNode(3);\n+  Node* null_ctl = top();\n+  null_check_oop(val, &null_ctl);\n+  if (null_ctl != top()) {\n+    PreserveJVMState pjvms(this);\n+    set_control(null_ctl);\n+    {\n+      \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, is_nullable_array(ary), PROB_MAX);\n+      inc_sp(nargs);\n+      uncommon_trap(Deoptimization::Reason_null_check,\n+                    Deoptimization::Action_none);\n+    }\n+    region->init_req(1, control());\n+  }\n+  region->init_req(2, control());\n+  set_control(_gvn.transform(region));\n+  record_for_igvn(region);\n+  const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+  if (val_t == TypePtr::NULL_PTR && !ary_t->is_not_null_free()) {\n+    \/\/ Since we were just successfully storing null, the array can't be null free.\n+    ary_t = ary_t->cast_to_not_null_free();\n+    Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+    if (safe_for_replace) {\n+      replace_in_map(ary, cast);\n+    }\n+    ary = cast;\n+  }\n+  return ary;\n+}\n+\n+Node* GraphKit::load_lh_array_tag(Node* kls) {\n+  Node* lhp = basic_plus_adr(kls, in_bytes(Klass::layout_helper_offset()));\n+  Node* layout_val = _gvn.transform(LoadNode::make(_gvn, NULL, immutable_memory(), lhp, lhp->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+  return _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+}\n+\n+Node* GraphKit::gen_lh_array_test(Node* kls, unsigned int lh_value) {\n+  Node* layout_val = load_lh_array_tag(kls);\n+  Node* cmp = _gvn.transform(new CmpINode(layout_val, intcon(lh_value)));\n+  return cmp;\n@@ -3360,0 +3704,1 @@\n+\n@@ -3432,0 +3777,1 @@\n+  assert(!obj->is_InlineTypeBase(), \"should not unlock on inline type\");\n@@ -3472,0 +3818,1 @@\n+    assert(klass != NULL, \"klass should not be NULL\");\n@@ -3473,1 +3820,6 @@\n-    if (xklass || klass->is_array_klass()) {\n+    bool can_be_flattened = false;\n+    if (UseFlatArray && klass->is_obj_array_klass()) {\n+      ciKlass* elem = klass->as_obj_array_klass()->element_klass();\n+      can_be_flattened = elem->can_be_inline_klass() && (!elem->is_inlinetype() || elem->flatten_array());\n+    }\n+    if (xklass || (klass->is_array_klass() && !can_be_flattened)) {\n@@ -3535,0 +3887,1 @@\n+    _gvn.set_type(minit_in, Type::MEMORY);\n@@ -3542,3 +3895,26 @@\n-      const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n-      int            elemidx  = C->get_alias_index(telemref);\n-      hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      const TypeAryPtr* arytype = oop_type->is_aryptr();\n+      if (arytype->klass()->is_flat_array_klass()) {\n+        \/\/ Initially all flattened array accesses share a single slice\n+        \/\/ but that changes after parsing. Prepare the memory graph so\n+        \/\/ it can optimize flattened array accesses properly once they\n+        \/\/ don't share a single slice.\n+        assert(C->flattened_accesses_share_alias(), \"should be set at parse time\");\n+        C->set_flattened_accesses_share_alias(false);\n+        ciFlatArrayKlass* vak = arytype->klass()->as_flat_array_klass();\n+        ciInlineKlass* vk = vak->element_klass()->as_inline_klass();\n+        for (int i = 0, len = vk->nof_nonstatic_fields(); i < len; i++) {\n+          ciField* field = vk->nonstatic_field_at(i);\n+          if (field->offset() >= TrackedInitializationLimit * HeapWordSize)\n+            continue;  \/\/ do not bother to track really large numbers of fields\n+          int off_in_vt = field->offset() - vk->first_field_offset();\n+          const TypePtr* adr_type = arytype->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+          int fieldidx = C->get_alias_index(adr_type, true);\n+          hook_memory_on_init(*this, fieldidx, minit_in, minit_out);\n+        }\n+        C->set_flattened_accesses_share_alias(true);\n+        hook_memory_on_init(*this, C->get_alias_index(TypeAryPtr::INLINES), minit_in, minit_out);\n+      } else {\n+        const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n+        int            elemidx  = C->get_alias_index(telemref);\n+        hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      }\n@@ -3546,0 +3922,1 @@\n+      set_memory(minit_out, C->get_alias_index(oop_type)); \/\/ mark word\n@@ -3596,1 +3973,2 @@\n-                             bool deoptimize_on_exception) {\n+                             bool deoptimize_on_exception,\n+                             InlineTypeBaseNode* inline_type_node) {\n@@ -3603,1 +3981,1 @@\n-  int   layout_is_con = (layout_val == NULL);\n+  bool  layout_is_con = (layout_val == NULL);\n@@ -3654,1 +4032,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3661,1 +4039,1 @@\n-                                         initial_slow_test);\n+                                         initial_slow_test, inline_type_node);\n@@ -3666,0 +4044,8 @@\n+\/\/ With compressed oops, the 64 bit init value for non flattened value\n+\/\/ arrays is built from 2 32 bit compressed oops\n+static Node* raw_default_for_coops(Node* default_value, GraphKit& kit) {\n+  Node* lower = kit.gvn().transform(new CastP2XNode(kit.control(), default_value));\n+  Node* upper = kit.gvn().transform(new LShiftLNode(lower, kit.intcon(32)));\n+  return kit.gvn().transform(new OrLNode(lower, upper));\n+}\n+\n@@ -3667,1 +4053,1 @@\n-\/\/ helper for both newarray and anewarray\n+\/\/ helper for newarray and anewarray\n@@ -3677,1 +4063,1 @@\n-  int   layout_is_con = (layout_val == NULL);\n+  bool  layout_is_con = (layout_val == NULL);\n@@ -3707,1 +4093,1 @@\n-    fast_size_limit <<= (LogBytesPerLong - log2_esize);\n+    fast_size_limit <<= MAX2(LogBytesPerLong - log2_esize, 0);\n@@ -3725,1 +4111,1 @@\n-    BasicType etype  = Klass::layout_helper_element_type(layout_con);\n+    bool is_flat_array = Klass::layout_helper_is_flatArray(layout_con);\n@@ -3728,1 +4114,1 @@\n-    assert((hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n+    assert(is_flat_array || (hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n@@ -3812,1 +4198,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3821,0 +4207,74 @@\n+  const TypeKlassPtr* ary_klass = _gvn.type(klass_node)->isa_klassptr();\n+  const TypeOopPtr* ary_type = ary_klass->as_instance_type();\n+  const TypeAryPtr* ary_ptr = ary_type->isa_aryptr();\n+\n+  \/\/ Inline type array variants:\n+  \/\/ - null-ok:              MyValue.ref[] (ciObjArrayKlass \"[LMyValue$ref\")\n+  \/\/ - null-free:            MyValue.val[] (ciObjArrayKlass \"[QMyValue$val\")\n+  \/\/ - null-free, flattened: MyValue.val[] (ciFlatArrayKlass \"[QMyValue$val\")\n+  \/\/ Check if array is a null-free, non-flattened inline type array\n+  \/\/ that needs to be initialized with the default inline type.\n+  Node* default_value = NULL;\n+  Node* raw_default_value = NULL;\n+  if (ary_ptr != NULL && ary_ptr->klass_is_exact()) {\n+    \/\/ Array type is known\n+    ciKlass* elem_klass = ary_ptr->klass()->as_array_klass()->element_klass();\n+    if (elem_klass != NULL && elem_klass->is_inlinetype()) {\n+      ciInlineKlass* vk = elem_klass->as_inline_klass();\n+      if (!vk->flatten_array()) {\n+        default_value = InlineTypeNode::default_oop(gvn(), vk);\n+        if (UseCompressedOops) {\n+          default_value = _gvn.transform(new EncodePNode(default_value, default_value->bottom_type()->make_narrowoop()));\n+          raw_default_value = raw_default_for_coops(default_value, *this);\n+        } else {\n+          raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));\n+        }\n+      }\n+    }\n+  } else if (ary_klass->klass()->can_be_inline_array_klass()) {\n+    \/\/ Array type is not known, add runtime checks\n+    assert(!ary_klass->klass_is_exact(), \"unexpected exact type\");\n+    Node* r = new RegionNode(4);\n+    default_value = new PhiNode(r, TypeInstPtr::BOTTOM);\n+\n+    \/\/ Check if array is an object array\n+    Node* cmp = gen_lh_array_test(klass_node, Klass::_lh_array_tag_obj_value);\n+    Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+    IfNode* iff = create_and_map_if(control(), bol, PROB_FAIR, COUNT_UNKNOWN);\n+\n+    \/\/ Not an object array, initialize with all zero\n+    r->init_req(1, _gvn.transform(new IfFalseNode(iff)));\n+    default_value->init_req(1, null());\n+\n+    \/\/ Object array, check if null-free\n+    set_control(_gvn.transform(new IfTrueNode(iff)));\n+    iff = create_and_map_if(control(), check_null_free_bit(klass_node, true), PROB_FAIR, COUNT_UNKNOWN);\n+\n+    \/\/ Not null-free, initialize with all zero\n+    r->init_req(2, _gvn.transform(new IfFalseNode(iff)));\n+    default_value->init_req(2, null());\n+\n+    \/\/ Null-free, non-flattened inline type array, initialize with the default value\n+    set_control(_gvn.transform(new IfTrueNode(iff)));\n+    Node* p = basic_plus_adr(klass_node, in_bytes(ArrayKlass::element_klass_offset()));\n+    Node* eklass = _gvn.transform(LoadKlassNode::make(_gvn, control(), immutable_memory(), p, TypeInstPtr::KLASS));\n+    Node* adr_fixed_block_addr = basic_plus_adr(eklass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    Node* adr_fixed_block = make_load(control(), adr_fixed_block_addr, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+    Node* default_value_offset_addr = basic_plus_adr(adr_fixed_block, in_bytes(InlineKlass::default_value_offset_offset()));\n+    Node* default_value_offset = make_load(control(), default_value_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+    Node* elem_mirror = load_mirror_from_klass(eklass);\n+    Node* default_value_addr = basic_plus_adr(elem_mirror, ConvI2X(default_value_offset));\n+    Node* val = access_load_at(elem_mirror, default_value_addr, _gvn.type(default_value_addr)->is_ptr(), TypeInstPtr::BOTTOM, T_OBJECT, IN_HEAP);\n+    r->init_req(3, control());\n+    default_value->init_req(3, val);\n+\n+    set_control(_gvn.transform(r));\n+    default_value = _gvn.transform(default_value);\n+    if (UseCompressedOops) {\n+      default_value = _gvn.transform(new EncodePNode(default_value, default_value->bottom_type()->make_narrowoop()));\n+      raw_default_value = raw_default_for_coops(default_value, *this);\n+    } else {\n+      raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));\n+    }\n+  }\n+\n@@ -3822,6 +4282,6 @@\n-  AllocateArrayNode* alloc\n-    = new AllocateArrayNode(C, AllocateArrayNode::alloc_type(TypeInt::INT),\n-                            control(), mem, i_o(),\n-                            size, klass_node,\n-                            initial_slow_test,\n-                            length);\n+  AllocateArrayNode* alloc = new AllocateArrayNode(C, AllocateArrayNode::alloc_type(TypeInt::INT),\n+                                                   control(), mem, i_o(),\n+                                                   size, klass_node,\n+                                                   initial_slow_test,\n+                                                   length, default_value,\n+                                                   raw_default_value);\n@@ -3834,1 +4294,0 @@\n-  const TypeOopPtr* ary_type = _gvn.type(klass_node)->is_klassptr()->as_instance_type();\n@@ -3992,1 +4451,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -3995,2 +4454,2 @@\n-                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS),\n-                                                  ciTypeArrayKlass::make(T_BYTE), true, 0);\n+                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, true, true),\n+                                                  ciTypeArrayKlass::make(T_BYTE), true, Type::Offset(0));\n@@ -4009,1 +4468,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4021,1 +4480,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4031,1 +4490,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4142,1 +4601,8 @@\n-    return makecon(con_type);\n+    Node* con = makecon(con_type);\n+    assert(!field->type()->is_inlinetype() || (field->is_static() && !con_type->is_zero_type()), \"sanity\");\n+    \/\/ Check type of constant which might be more precise\n+    if (con_type->is_inlinetypeptr() && con_type->inline_klass()->is_scalarizable()) {\n+      \/\/ Load inline type from constant oop\n+      con = InlineTypeNode::make_from_oop(this, con, con_type->inline_klass());\n+    }\n+    return con;\n@@ -4146,0 +4612,9 @@\n+\n+\/\/---------------------------load_mirror_from_klass----------------------------\n+\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n+Node* GraphKit::load_mirror_from_klass(Node* klass) {\n+  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n+  Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+  \/\/ mirror = ((OopHandle)mirror)->resolve();\n+  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n+}\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":625,"deletions":150,"binary":false,"changes":775,"status":"modified"},{"patch":"@@ -276,1 +276,1 @@\n-        if (offset == Type::OffsetBot || tptr->_offset == Type::OffsetBot)\n+        if (offset == Type::OffsetBot || tptr->offset() == Type::OffsetBot)\n@@ -278,1 +278,1 @@\n-        offset += tptr->_offset; \/\/ correct if base is offseted\n+        offset += tptr->offset(); \/\/ correct if base is offseted\n@@ -315,1 +315,5 @@\n-      Block *inb = get_block_for_node(mach->in(j));\n+      Block* inb = get_block_for_node(mach->in(j));\n+      if (mach->in(j)->is_Con() && inb == get_block_for_node(mach)) {\n+        \/\/ Ignore constant loads scheduled in the same block (we can simply hoist them as well)\n+        continue;\n+      }\n@@ -391,0 +395,20 @@\n+  } else {\n+    \/\/ Hoist constant load inputs as well.\n+    for (uint i = 1; i < best->req(); ++i) {\n+      Node* n = best->in(i);\n+      if (n->is_Con() && get_block_for_node(n) == get_block_for_node(best)) {\n+        get_block_for_node(n)->find_remove(n);\n+        block->add_inst(n);\n+        map_node_to_block(n, block);\n+        \/\/ Constant loads may kill flags (for example, when XORing a register).\n+        \/\/ Check for flag-killing projections that also need to be hoisted.\n+        for (DUIterator_Fast jmax, j = n->fast_outs(jmax); j < jmax; j++) {\n+          Node* proj = n->fast_out(j);\n+          if (proj->is_MachProj()) {\n+            get_block_for_node(proj)->find_remove(proj);\n+            block->add_inst(proj);\n+            map_node_to_block(proj, block);\n+          }\n+        }\n+      }\n+    }\n@@ -392,0 +416,1 @@\n+\n@@ -843,1 +868,1 @@\n-  uint r_cnt = mcall->tf()->range()->cnt();\n+  uint r_cnt = mcall->tf()->range_cc()->cnt();\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":29,"deletions":4,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -45,0 +46,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -137,3 +139,11 @@\n-    if (!stopped() && result() != NULL) {\n-      BasicType bt = result()->bottom_type()->basic_type();\n-      push_node(bt, result());\n+    Node* res = result();\n+    if (!stopped() && res != NULL) {\n+      BasicType bt = res->bottom_type()->basic_type();\n+      if (C->inlining_incrementally() && res->is_InlineType()) {\n+        \/\/ The caller expects an oop when incrementally inlining an intrinsic that returns an\n+        \/\/ inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        res = res->as_InlineType()->buffer(this);\n+      }\n+      push_node(bt, res);\n@@ -167,1 +177,0 @@\n-  Node* load_mirror_from_klass(Node* klass);\n@@ -189,0 +198,12 @@\n+  Node* generate_value_guard(Node* kls, RegionNode* region);\n+\n+  enum ArrayKind {\n+    AnyArray,\n+    NonArray,\n+    ObjectArray,\n+    NonObjectArray,\n+    TypeArray,\n+    FlatArray,\n+    NonFlatArray\n+  };\n+\n@@ -190,0 +211,1 @@\n+\n@@ -191,1 +213,1 @@\n-    return generate_array_guard_common(kls, region, false, false);\n+    return generate_array_guard_common(kls, region, AnyArray);\n@@ -194,1 +216,1 @@\n-    return generate_array_guard_common(kls, region, false, true);\n+    return generate_array_guard_common(kls, region, NonArray);\n@@ -197,1 +219,1 @@\n-    return generate_array_guard_common(kls, region, true, false);\n+    return generate_array_guard_common(kls, region, ObjectArray);\n@@ -200,1 +222,8 @@\n-    return generate_array_guard_common(kls, region, true, true);\n+    return generate_array_guard_common(kls, region, NonObjectArray);\n+  }\n+  Node* generate_typeArray_guard(Node* kls, RegionNode* region) {\n+    return generate_array_guard_common(kls, region, TypeArray);\n+  }\n+  Node* generate_flatArray_guard(Node* kls, RegionNode* region) {\n+    assert(UseFlatArray, \"can never be flattened\");\n+    return generate_array_guard_common(kls, region, FlatArray);\n@@ -202,2 +231,5 @@\n-  Node* generate_array_guard_common(Node* kls, RegionNode* region,\n-                                    bool obj_array, bool not_array);\n+  Node* generate_non_flatArray_guard(Node* kls, RegionNode* region) {\n+    assert(UseFlatArray, \"can never be flattened\");\n+    return generate_array_guard_common(kls, region, NonFlatArray);\n+  }\n+  Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);\n@@ -261,0 +293,2 @@\n+  bool inline_unsafe_make_private_buffer();\n+  bool inline_unsafe_finish_private_buffer();\n@@ -610,0 +644,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -619,0 +655,1 @@\n+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_INLINE_TYPE,Relaxed, false);\n@@ -629,0 +666,1 @@\n+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_INLINE_TYPE,Relaxed, false);\n@@ -2415,2 +2453,2 @@\n-      assert(rtype == type, \"getter must return the expected value\");\n-      assert(sig->count() == 2, \"oop getter has 2 arguments\");\n+      assert(rtype == type || (rtype == T_OBJECT && type == T_INLINE_TYPE), \"getter must return the expected value\");\n+      assert(sig->count() == 2 || (type == T_INLINE_TYPE && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n@@ -2422,1 +2460,1 @@\n-      assert(sig->count() == 3, \"oop putter has 3 arguments\");\n+      assert(sig->count() == 3 || (type == T_INLINE_TYPE && sig->count() == 4), \"oop putter has 3 arguments\");\n@@ -2426,1 +2464,1 @@\n-      assert(vtype == type, \"putter must accept the expected value\");\n+      assert(vtype == type || (type == T_INLINE_TYPE && vtype == T_OBJECT), \"putter must accept the expected value\");\n@@ -2451,0 +2489,58 @@\n+\n+  ciInlineKlass* inline_klass = NULL;\n+  if (type == T_INLINE_TYPE) {\n+    Node* cls = null_check(argument(4));\n+    if (stopped()) {\n+      return true;\n+    }\n+    Node* kls = load_klass_from_mirror(cls, false, NULL, 0);\n+    const TypeKlassPtr* kls_t = _gvn.type(kls)->isa_klassptr();\n+    if (!kls_t->klass_is_exact()) {\n+      return false;\n+    }\n+    ciKlass* klass = kls_t->klass();\n+    if (!klass->is_inlinetype()) {\n+      return false;\n+    }\n+    inline_klass = klass->as_inline_klass();\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  if (base->is_InlineType()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (is_store) {\n+      if (!vt->is_allocated(&_gvn) || !_gvn.type(vt)->is_inlinetype()->larval()) {\n+        return false;\n+      }\n+      base = vt->get_oop();\n+    } else {\n+      if (offset->is_Con()) {\n+        long off = find_long_con(offset, 0);\n+        ciInlineKlass* vk = vt->type()->inline_klass();\n+        if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+          return false;\n+        }\n+\n+        ciField* field = vk->get_non_flattened_field_by_offset(off);\n+        if (field != NULL) {\n+          BasicType bt = field->layout_type();\n+          if (bt == T_ARRAY || bt == T_NARROWOOP || (bt == T_INLINE_TYPE && !field->is_flattened())) {\n+            bt = T_OBJECT;\n+          }\n+          if (bt == type && (bt != T_INLINE_TYPE || field->type() == inline_klass)) {\n+            set_result(vt->field_value_by_offset(off, false));\n+            return true;\n+          }\n+        }\n+      }\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      base = vt->buffer(this)->get_oop();\n+    }\n+  }\n+\n@@ -2456,1 +2552,1 @@\n-    if (type != T_OBJECT) {\n+    if (type != T_OBJECT && (inline_klass == NULL || !inline_klass->has_object_fields())) {\n@@ -2472,1 +2568,1 @@\n-  val = is_store ? argument(4) : NULL;\n+  val = is_store ? argument(4 + (type == T_INLINE_TYPE ? 1 : 0)) : NULL;\n@@ -2489,1 +2585,25 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = NULL;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->klass()->as_instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != NULL &&\n+        instptr->klass() == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (instptr->klass()->as_instance_klass()->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flattened_field_by_offset(off);\n+    }\n+    if (field != NULL) {\n+      bt = field->layout_type();\n+    }\n+    assert(bt == alias_type->basic_type() || bt == T_INLINE_TYPE, \"should match\");\n+    if (field != NULL && bt == T_INLINE_TYPE && !field->is_flattened()) {\n+      bt = T_OBJECT;\n+    }\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2510,0 +2630,21 @@\n+  if (type == T_INLINE_TYPE) {\n+    if (adr_type->isa_instptr()) {\n+      if (field == NULL || field->type() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else if (adr_type->isa_aryptr()) {\n+      const Type* elem = adr_type->is_aryptr()->elem();\n+      if (!elem->isa_inlinetype()) {\n+        mismatched = true;\n+      } else if (elem->inline_klass() != inline_klass) {\n+        mismatched = true;\n+      }\n+    }\n+    if (is_store) {\n+      const Type* val_t = _gvn.type(val);\n+      if (!val_t->isa_inlinetype() || val_t->inline_klass() != inline_klass) {\n+        return false;\n+      }\n+    }\n+  }\n+\n@@ -2522,4 +2663,8 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != NULL) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != NULL) {\n+        value_type = tjp;\n+      }\n+    } else if (type == T_INLINE_TYPE) {\n+      value_type = NULL;\n@@ -2529,4 +2674,0 @@\n-  receiver = null_check(receiver);\n-  if (stopped()) {\n-    return true;\n-  }\n@@ -2541,1 +2682,1 @@\n-    ciField* field = alias_type->field();\n+\n@@ -2548,1 +2689,11 @@\n-      p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      if (type == T_INLINE_TYPE) {\n+        if (adr_type->isa_instptr() && !mismatched) {\n+          ciInstanceKlass* holder = adr_type->is_instptr()->klass()->as_instance_klass();\n+          int offset = adr_type->is_instptr()->offset();\n+          p = InlineTypeNode::make_from_flattened(this, inline_klass, base, base, holder, offset, decorators);\n+        } else {\n+          p = InlineTypeNode::make_from_flattened(this, inline_klass, base, adr, NULL, 0, decorators);\n+        }\n+      } else {\n+        p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      }\n@@ -2575,0 +2726,8 @@\n+    if (field != NULL && field->type()->is_inlinetype() && !field->is_flattened()) {\n+      \/\/ Load a non-flattened inline type from memory\n+      if (value_type->inline_klass()->is_scalarizable()) {\n+        p = InlineTypeNode::make_from_oop(this, p, value_type->inline_klass());\n+      } else {\n+        p = null2default(p, value_type->inline_klass());\n+      }\n+    }\n@@ -2586,1 +2745,56 @@\n-    access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    if (type == T_INLINE_TYPE) {\n+      if (adr_type->isa_instptr() && !mismatched) {\n+        ciInstanceKlass* holder = adr_type->is_instptr()->klass()->as_instance_klass();\n+        int offset = adr_type->is_instptr()->offset();\n+        val->as_InlineType()->store_flattened(this, base, base, holder, offset, decorators);\n+      } else {\n+        val->as_InlineType()->store_flattened(this, base, adr, NULL, 0, decorators);\n+      }\n+    } else {\n+      access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    }\n+  }\n+\n+  if (argument(1)->is_InlineType() && is_store) {\n+    Node* value = InlineTypeNode::make_from_oop(this, base, _gvn.type(base)->inline_klass());\n+    value = value->as_InlineType()->make_larval(this, false);\n+    replace_in_map(argument(1), value);\n+  }\n+\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  if (!value->is_InlineType()) {\n+    return false;\n+  }\n+\n+  set_result(value->as_InlineType()->make_larval(this, true));\n+\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  if (!buffer->is_InlineType()) {\n+    return false;\n+  }\n+\n+  InlineTypeNode* vt = buffer->as_InlineType();\n+  if (!vt->is_allocated(&_gvn) || !_gvn.type(vt)->is_inlinetype()->larval()) {\n+    return false;\n@@ -2589,0 +2803,2 @@\n+  set_result(vt->finish_larval(this));\n+\n@@ -3051,9 +3267,0 @@\n-\/\/---------------------------load_mirror_from_klass----------------------------\n-\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {\n-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n-  Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n-  \/\/ mirror = ((OopHandle)mirror)->resolve();\n-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n-}\n-\n@@ -3102,0 +3309,1 @@\n+\n@@ -3109,0 +3317,4 @@\n+Node* LibraryCallKit::generate_value_guard(Node* kls, RegionNode* region) {\n+  return generate_access_flags_guard(kls, JVM_ACC_INLINE, 0, region);\n+}\n+\n@@ -3306,1 +3518,7 @@\n-  const TypeOopPtr* tp = _gvn.type(obj)->isa_oopptr();\n+  ciKlass* obj_klass = NULL;\n+  const Type* obj_t = _gvn.type(obj);\n+  if (obj->is_InlineType()) {\n+    obj_klass = obj_t->inline_klass();\n+  } else if (obj_t->isa_oopptr()) {\n+    obj_klass = obj_t->is_oopptr()->klass();\n+  }\n@@ -3311,3 +3529,2 @@\n-  if (tm != NULL && tm->is_klass() &&\n-      tp != NULL && tp->klass() != NULL) {\n-    if (!tp->klass()->is_loaded()) {\n+  if (tm != NULL && tm->is_klass() && obj_klass != NULL) {\n+    if (!obj_klass->is_loaded()) {\n@@ -3317,1 +3534,8 @@\n-      int static_res = C->static_subtype_check(tm->as_klass(), tp->klass());\n+      if (!obj->is_InlineType() && tm->as_klass()->is_inlinetype()) {\n+        \/\/ Casting to .val, check for null\n+        obj = null_check(obj);\n+        if (stopped()) {\n+          return true;\n+        }\n+      }\n+      int static_res = C->static_subtype_check(tm->as_klass(), obj_klass);\n@@ -3347,1 +3571,1 @@\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -3358,0 +3582,19 @@\n+    if (EnableValhalla && !obj->is_InlineType()) {\n+      \/\/ Check if we are casting to .val\n+      Node* is_val_kls = generate_value_guard(kls, NULL);\n+      if (is_val_kls != NULL) {\n+        RegionNode* r = new RegionNode(3);\n+        record_for_igvn(r);\n+        r->init_req(1, control());\n+\n+        \/\/ Casting to .val, check for null\n+        set_control(is_val_kls);\n+        Node* null_ctr = top();\n+        null_check_oop(obj, &null_ctr);\n+        region->init_req(_npe_path, null_ctr);\n+        r->init_req(2, control());\n+\n+        set_control(_gvn.transform(r));\n+      }\n+    }\n+\n@@ -3364,1 +3607,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -3401,0 +3645,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -3403,0 +3648,1 @@\n+  record_for_igvn(prim_region);\n@@ -3427,2 +3673,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -3445,1 +3694,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -3450,1 +3700,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -3481,2 +3731,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind) {\n@@ -3488,9 +3737,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -3501,4 +3741,13 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;\n+      case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case FlatArray:      query = Klass::layout_helper_is_flatArray(layout_con); break;\n+      case NonFlatArray:   query = !Klass::layout_helper_is_flatArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -3514,0 +3763,28 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case ObjectArray:\n+    case NonObjectArray: {\n+      value = Klass::_lh_array_tag_obj_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == ObjectArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case FlatArray:\n+    case NonFlatArray: {\n+      value = Klass::_lh_array_tag_vt_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == FlatArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -3515,4 +3792,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -3520,3 +3794,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -3529,1 +3800,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -3668,1 +3939,13 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    ciKlass* klass = _gvn.type(klass_node)->is_klassptr()->klass();\n+    bool exclude_flat = UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == NULL || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        klass->can_be_inline_array_klass() && (!klass->is_flat_array_klass() || klass->as_flat_array_klass()->element_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_objArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n@@ -3672,1 +3955,1 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n@@ -3678,0 +3961,22 @@\n+    Node* original_kls = load_object_klass(original);\n+    \/\/ ArrayCopyNode:Ideal may transform the ArrayCopyNode to\n+    \/\/ loads\/stores but it is legal only if we're sure the\n+    \/\/ Arrays.copyOf would succeed. So we need all input arguments\n+    \/\/ to the copyOf to be validated, including that the copy to the\n+    \/\/ new array won't trigger an ArrayStoreException. That subtype\n+    \/\/ check can be optimized if we know something on the type of\n+    \/\/ the input array from type speculation.\n+    if (_gvn.type(klass_node)->singleton() && !stopped()) {\n+      ciKlass* subk   = _gvn.type(original_kls)->is_klassptr()->klass();\n+      ciKlass* superk = _gvn.type(klass_node)->is_klassptr()->klass();\n+\n+      int test = C->static_subtype_check(superk, subk);\n+      if (test != Compile::SSC_always_true && test != Compile::SSC_always_false) {\n+        const TypeOopPtr* t_original = _gvn.type(original)->is_oopptr();\n+        if (t_original->speculative_type() != NULL) {\n+          original = maybe_cast_profiled_obj(original, t_original->speculative_type(), true);\n+          original_kls = load_object_klass(original);\n+        }\n+      }\n+    }\n+\n@@ -3693,0 +3998,32 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != NULL && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_non_flatArray_guard(klass_node, bailout);\n+        }\n+      } else if (UseFlatArray && (orig_t == NULL || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!klass->is_flat_array_klass() && klass->can_be_inline_array_klass()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_flatArray_guard(original_kls, bailout);\n+        if (orig_t != NULL) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(check_null_free_bit(klass_node, true), bailout);\n+      }\n+    }\n+\n@@ -3712,20 +4049,0 @@\n-      \/\/ ArrayCopyNode:Ideal may transform the ArrayCopyNode to\n-      \/\/ loads\/stores but it is legal only if we're sure the\n-      \/\/ Arrays.copyOf would succeed. So we need all input arguments\n-      \/\/ to the copyOf to be validated, including that the copy to the\n-      \/\/ new array won't trigger an ArrayStoreException. That subtype\n-      \/\/ check can be optimized if we know something on the type of\n-      \/\/ the input array from type speculation.\n-      if (_gvn.type(klass_node)->singleton()) {\n-        ciKlass* subk   = _gvn.type(load_object_klass(original))->is_klassptr()->klass();\n-        ciKlass* superk = _gvn.type(klass_node)->is_klassptr()->klass();\n-\n-        int test = C->static_subtype_check(superk, subk);\n-        if (test != Compile::SSC_always_true && test != Compile::SSC_always_false) {\n-          const TypeOopPtr* t_original = _gvn.type(original)->is_oopptr();\n-          if (t_original->speculative_type() != NULL) {\n-            original = maybe_cast_profiled_obj(original, t_original->speculative_type(), true);\n-          }\n-        }\n-      }\n-\n@@ -3735,1 +4052,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -3752,1 +4069,1 @@\n-                                                load_object_klass(original), klass_node);\n+                                                original_kls, klass_node);\n@@ -3876,1 +4193,6 @@\n-  Node* obj = NULL;\n+  Node* obj = argument(0);\n+\n+  if (obj->is_InlineType() || gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -3886,1 +4208,0 @@\n-    obj = argument(0);\n@@ -3926,0 +4247,1 @@\n+  \/\/ This also serves as guard against inline types (they have the always_locked_pattern set).\n@@ -3992,1 +4314,7 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    ciKlass* vk = _gvn.type(obj)->inline_klass();\n+    set_result(makecon(TypeInstPtr::make(vk->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -4254,1 +4582,8 @@\n-  access_clone(obj, alloc_obj, size, is_array);\n+  \/\/ Exclude the header but include array length to copy by 8 bytes words.\n+  \/\/ Can't use base_offset_in_bytes(bt) since basic type is unknown.\n+  int base_off = BarrierSetC2::arraycopy_payload_base_offset(is_array);\n+  Node* countx = size;\n+  countx = _gvn.transform(new SubXNode(countx, MakeConX(base_off)));\n+  countx = _gvn.transform(new URShiftXNode(countx, intcon(LogBytesPerLong)));\n+\n+  access_clone(obj, alloc_obj, countx, is_array);\n@@ -4297,1 +4632,6 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    if (obj->is_InlineType()) {\n+      return false;\n+    }\n+\n+    obj = null_check_receiver();\n@@ -4307,1 +4647,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -4339,0 +4680,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -4344,3 +4690,0 @@\n-      Node* obj_length = load_array_length(obj);\n-      Node* obj_size  = NULL;\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size, \/*deoptimize_on_exception=*\/true);\n@@ -4349,20 +4692,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);\n-        if (is_obja != NULL) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing) &&\n+          obj_type->klass()->can_be_inline_array_klass() &&\n+          (ary_ptr == NULL || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flattened inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_flatArray_guard(obj_klass, slow_region);\n@@ -4370,7 +4700,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -4379,7 +4702,43 @@\n-        copy_to_clone(obj, alloc_obj, obj_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(obj);\n+        Node* obj_size  = NULL;\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);\n+          if (is_obja != NULL) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, obj_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -4389,4 +4748,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -4553,2 +4908,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -4557,1 +4911,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -4569,1 +4923,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -4743,0 +5097,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -4746,0 +5102,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -4761,2 +5119,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -4805,0 +5162,2 @@\n+      slow_region->add_req(not_subtype_ctrl);\n+    }\n@@ -4806,6 +5165,28 @@\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n+    const Type* toop = TypeOopPtr::make_from_klass(dest_klass_t->klass());\n+    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n+    src_type = _gvn.type(src);\n+    top_src  = src_type->isa_aryptr();\n+\n+    \/\/ Handle flat inline type arrays (null-free arrays are handled by the subtype check above)\n+    if (!stopped() && UseFlatArray) {\n+      \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check). Handle flat src here.\n+      assert(top_dest == NULL || !top_dest->is_flat() || top_src->is_flat(), \"src array must be flat\");\n+      if (top_src != NULL && top_src->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (top_dest != NULL && !top_dest->is_flat()) {\n+          generate_non_flatArray_guard(dest_klass, slow_region);\n+          \/\/ Since dest is flat and src <: dest, dest must have the same type as src.\n+          top_dest = TypeOopPtr::make_from_klass(top_src->klass())->isa_aryptr();\n+          assert(top_dest->is_flat(), \"dest must be flat\");\n+          dest = _gvn.transform(new CheckCastPPNode(control(), dest, top_dest));\n+        }\n+      } else if (top_src == NULL || !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        assert(top_dest == NULL || !top_dest->is_flat(), \"dest array must not be flat\");\n+        generate_flatArray_guard(load_object_klass(src), slow_region);\n+        if (top_src != NULL) {\n+          top_src = top_src->cast_to_not_flat();\n+          src = _gvn.transform(new CheckCastPPNode(control(), src, top_src));\n+        }\n@@ -4814,0 +5195,1 @@\n+\n@@ -4821,4 +5203,0 @@\n-\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = TypeOopPtr::make_from_klass(dest_klass_t->klass());\n-    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":536,"deletions":158,"binary":false,"changes":694,"status":"modified"},{"patch":"@@ -79,1 +79,2 @@\n-         ProfileTripFailed=131072};\n+         ProfileTripFailed=131072,\n+         FlattenedArrays=262144};\n@@ -104,0 +105,1 @@\n+  bool is_flattened_arrays() const { return _loop_flags & FlattenedArrays; }\n@@ -118,0 +120,1 @@\n+  void mark_flattened_arrays() { _loop_flags |= FlattenedArrays; }\n@@ -1241,1 +1244,1 @@\n-  IfNode* find_unswitching_candidate(const IdealLoopTree *loop) const;\n+  IfNode* find_unswitching_candidate(const IdealLoopTree *loop, Node_List& unswitch_iffs) const;\n@@ -1363,0 +1366,1 @@\n+  bool flatten_array_element_type_check(Node *n);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -85,50 +87,0 @@\n-void PhaseMacroExpand::migrate_outs(Node *old, Node *target) {\n-  assert(old != NULL, \"sanity\");\n-  for (DUIterator_Fast imax, i = old->fast_outs(imax); i < imax; i++) {\n-    Node* use = old->fast_out(i);\n-    _igvn.rehash_node_delayed(use);\n-    imax -= replace_input(use, old, target);\n-    \/\/ back up iterator\n-    --i;\n-  }\n-  assert(old->outcnt() == 0, \"all uses must be deleted\");\n-}\n-\n-void PhaseMacroExpand::copy_call_debug_info(CallNode *oldcall, CallNode * newcall) {\n-  \/\/ Copy debug information and adjust JVMState information\n-  uint old_dbg_start = oldcall->tf()->domain()->cnt();\n-  uint new_dbg_start = newcall->tf()->domain()->cnt();\n-  int jvms_adj  = new_dbg_start - old_dbg_start;\n-  assert (new_dbg_start == newcall->req(), \"argument count mismatch\");\n-\n-  \/\/ SafePointScalarObject node could be referenced several times in debug info.\n-  \/\/ Use Dict to record cloned nodes.\n-  Dict* sosn_map = new Dict(cmpkey,hashkey);\n-  for (uint i = old_dbg_start; i < oldcall->req(); i++) {\n-    Node* old_in = oldcall->in(i);\n-    \/\/ Clone old SafePointScalarObjectNodes, adjusting their field contents.\n-    if (old_in != NULL && old_in->is_SafePointScalarObject()) {\n-      SafePointScalarObjectNode* old_sosn = old_in->as_SafePointScalarObject();\n-      uint old_unique = C->unique();\n-      Node* new_in = old_sosn->clone(sosn_map);\n-      if (old_unique != C->unique()) { \/\/ New node?\n-        new_in->set_req(0, C->root()); \/\/ reset control edge\n-        new_in = transform_later(new_in); \/\/ Register new node.\n-      }\n-      old_in = new_in;\n-    }\n-    newcall->add_req(old_in);\n-  }\n-\n-  \/\/ JVMS may be shared so clone it before we modify it\n-  newcall->set_jvms(oldcall->jvms() != NULL ? oldcall->jvms()->clone_deep(C) : NULL);\n-  for (JVMState *jvms = newcall->jvms(); jvms != NULL; jvms = jvms->caller()) {\n-    jvms->set_map(newcall);\n-    jvms->set_locoff(jvms->locoff()+jvms_adj);\n-    jvms->set_stkoff(jvms->stkoff()+jvms_adj);\n-    jvms->set_monoff(jvms->monoff()+jvms_adj);\n-    jvms->set_scloff(jvms->scloff()+jvms_adj);\n-    jvms->set_endoff(jvms->endoff()+jvms_adj);\n-  }\n-}\n-\n@@ -187,1 +139,1 @@\n-  copy_call_debug_info(oldcall, call);\n+  call->copy_call_debug_info(&_igvn, oldcall);\n@@ -252,1 +204,1 @@\n-  bs->eliminate_gc_barrier(this, p2x);\n+  bs->eliminate_gc_barrier(&_igvn, p2x);\n@@ -295,1 +247,1 @@\n-        int adr_offset = atype->offset();\n+        int adr_offset = atype->flattened_offset();\n@@ -338,1 +290,1 @@\n-   } else if (mem->Opcode() == Op_StrInflatedCopy) {\n+    } else if (mem->Opcode() == Op_StrInflatedCopy) {\n@@ -383,1 +335,7 @@\n-      const TypePtr* adr_type = NULL;\n+      Node* base = ac->in(ArrayCopyNode::Src);\n+      const TypePtr* adr_type = _igvn.type(base)->is_ptr();\n+      assert(adr_type->isa_aryptr(), \"only arrays here\");\n+      if (adr_type->is_aryptr()->is_flat()) {\n+        ciFlatArrayKlass* vak = adr_type->is_aryptr()->klass()->as_flat_array_klass();\n+        shift = vak->log2_element_size();\n+      }\n@@ -386,2 +344,2 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(off);\n+        adr_type = _igvn.type(adr)->is_ptr();\n+        assert(adr_type == _igvn.type(base)->is_aryptr()->add_field_offset_and_offset(off), \"incorrect address type\");\n@@ -394,0 +352,5 @@\n+        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n+          \/\/ Non constant offset in the array: we can't statically\n+          \/\/ determine the value\n+          return NULL;\n+        }\n@@ -401,7 +364,5 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(Type::OffsetBot);\n-        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n-          \/\/ Non constant offset in the array: we can't statically\n-          \/\/ determine the value\n-          return NULL;\n-        }\n+        \/\/ In the case of a flattened inline type array, each field has its\n+        \/\/ own slice so we need to extract the field being accessed from\n+        \/\/ the address computation\n+        adr_type = adr_type->is_aryptr()->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot);\n+        adr = _igvn.transform(new CastPPNode(adr, adr_type));\n@@ -418,0 +379,1 @@\n+      assert(res->isa_DecodeN(), \"should be narrow oop\");\n@@ -433,1 +395,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flattened_offset();\n@@ -472,1 +434,7 @@\n-        values.at_put(j, _igvn.zerocon(ft));\n+        Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+        if (default_value != NULL) {\n+          values.at_put(j, default_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawDefaultValue) == NULL, \"default value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -489,1 +457,7 @@\n-        values.at_put(j, _igvn.zerocon(ft));\n+        Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+        if (default_value != NULL) {\n+          values.at_put(j, default_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawDefaultValue) == NULL, \"default value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -535,1 +509,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flattened_offset();\n@@ -537,1 +511,0 @@\n-  Node *alloc_ctrl = alloc->in(TypeFunc::Control);\n@@ -553,1 +526,1 @@\n-        done = true; \/\/ Something go wrong.\n+        done = true; \/\/ Something went wrong.\n@@ -563,1 +536,1 @@\n-             atype->is_known_instance_field() && atype->offset() == offset &&\n+             atype->is_known_instance_field() && atype->flattened_offset() == offset &&\n@@ -595,0 +568,5 @@\n+      Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+      if (default_value != NULL) {\n+        return default_value;\n+      }\n+      assert(alloc->in(AllocateNode::RawDefaultValue) == NULL, \"default value may not be null\");\n@@ -626,1 +604,1 @@\n-  \/\/ Something go wrong.\n+  \/\/ Something went wrong.\n@@ -630,0 +608,37 @@\n+\/\/ Search the last value stored into the inline type's fields.\n+Node* PhaseMacroExpand::inline_type_from_mem(Node* mem, Node* ctl, ciInlineKlass* vk, const TypeAryPtr* adr_type, int offset, AllocateNode* alloc) {\n+  \/\/ Subtract the offset of the first field to account for the missing oop header\n+  offset -= vk->first_field_offset();\n+  \/\/ Create a new InlineTypeNode and retrieve the field values from memory\n+  InlineTypeNode* vt = InlineTypeNode::make_uninitialized(_igvn, vk)->as_InlineType();\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); ++i) {\n+    ciType* field_type = vt->field_type(i);\n+    int field_offset = offset + vt->field_offset(i);\n+    Node* value = NULL;\n+    if (vt->field_is_flattened(i)) {\n+      value = inline_type_from_mem(mem, ctl, field_type->as_inline_klass(), adr_type, field_offset, alloc);\n+    } else {\n+      const Type* ft = Type::get_const_type(field_type);\n+      BasicType bt = field_type->basic_type();\n+      if (UseCompressedOops && !is_java_primitive(bt)) {\n+        ft = ft->make_narrowoop();\n+        bt = T_NARROWOOP;\n+      }\n+      \/\/ Each inline type field has its own memory slice\n+      adr_type = adr_type->with_field_offset(field_offset);\n+      value = value_from_mem(mem, ctl, bt, ft, adr_type, alloc);\n+      if (value != NULL && ft->isa_narrowoop()) {\n+        assert(UseCompressedOops, \"unexpected narrow oop\");\n+        value = transform_later(new DecodeNNode(value, value->get_ptr_type()));\n+      }\n+    }\n+    if (value != NULL) {\n+      vt->set_field_value(i, value);\n+    } else {\n+      \/\/ We might have reached the TrackedInitializationLimit\n+      return NULL;\n+    }\n+  }\n+  return transform_later(vt);\n+}\n+\n@@ -682,1 +697,1 @@\n-              NOT_PRODUCT(fail_eliminate = \"Not store field referrence\";)\n+              NOT_PRODUCT(fail_eliminate = \"Not store field reference\";)\n@@ -710,0 +725,5 @@\n+      } else if (use->is_InlineType() && use->isa_InlineType()->get_oop() == res) {\n+        \/\/ ok to eliminate\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(res_type->is_inlinetypeptr(), \"Unexpected store to mark word\");\n@@ -721,1 +741,1 @@\n-          }else {\n+          } else {\n@@ -727,0 +747,3 @@\n+      } else {\n+        assert(use->Opcode() == Op_CastP2X, \"should be\");\n+        assert(!use->has_out_with(Op_OrL), \"should have been removed because oop is never null\");\n@@ -789,0 +812,4 @@\n+      if (elem_type->is_inlinetype() && !klass->is_flat_array_klass()) {\n+        assert(basic_elem_type == T_INLINE_TYPE, \"unexpected element basic type\");\n+        basic_elem_type = T_OBJECT;\n+      }\n@@ -791,0 +818,4 @@\n+      if (klass->is_flat_array_klass()) {\n+        \/\/ Flattened inline type array\n+        element_size = klass->as_flat_array_klass()->element_byte_size();\n+      }\n@@ -796,0 +827,1 @@\n+  Unique_Node_List value_worklist;\n@@ -822,0 +854,1 @@\n+        assert(!field->is_flattened(), \"flattened inline type fields should not have safepoint uses\");\n@@ -849,3 +882,9 @@\n-      const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n-\n-      Node *field_val = value_from_mem(mem, ctl, basic_elem_type, field_type, field_addr_type, alloc);\n+      Node* field_val = NULL;\n+      const TypeOopPtr* field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n+      if (klass->is_flat_array_klass()) {\n+        ciInlineKlass* vk = elem_type->as_inline_klass();\n+        assert(vk->flatten_array(), \"must be flattened\");\n+        field_val = inline_type_from_mem(mem, ctl, vk, field_addr_type->isa_aryptr(), 0, alloc);\n+      } else {\n+        field_val = value_from_mem(mem, ctl, basic_elem_type, field_type, field_addr_type, alloc);\n+      }\n@@ -909,1 +948,4 @@\n-      if (UseCompressedOops && field_type->isa_narrowoop()) {\n+      if (field_val->is_InlineType()) {\n+        \/\/ Keep track of inline types to scalarize them later\n+        value_worklist.push(field_val);\n+      } else if (UseCompressedOops && field_type->isa_narrowoop()) {\n@@ -930,0 +972,8 @@\n+  \/\/ Scalarize inline types that were added to the safepoint.\n+  \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+  \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+  bool allow_oop = (klass == NULL) || !klass->is_flat_array_klass();\n+  for (uint i = 0; i < value_worklist.size(); ++i) {\n+    Node* vt = value_worklist.at(i);\n+    vt->as_InlineType()->make_scalar_in_safepoints(&_igvn, allow_oop);\n+  }\n@@ -945,1 +995,1 @@\n-void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc) {\n+void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc, bool inline_alloc) {\n@@ -957,10 +1007,7 @@\n-#ifdef ASSERT\n-            \/\/ Verify that there is no dependent MemBarVolatile nodes,\n-            \/\/ they should be removed during IGVN, see MemBarNode::Ideal().\n-            for (DUIterator_Fast pmax, p = n->fast_outs(pmax);\n-                                       p < pmax; p++) {\n-              Node* mb = n->fast_out(p);\n-              assert(mb->is_Initialize() || !mb->is_MemBar() ||\n-                     mb->req() <= MemBarNode::Precedent ||\n-                     mb->in(MemBarNode::Precedent) != n,\n-                     \"MemBarVolatile should be eliminated for non-escaping object\");\n+            for (DUIterator_Fast pmax, p = n->fast_outs(pmax); p < pmax; p++) {\n+              MemBarNode* mb = n->fast_out(p)->isa_MemBar();\n+              if (mb != NULL && mb->req() <= MemBarNode::Precedent && mb->in(MemBarNode::Precedent) == n) {\n+                \/\/ MemBarVolatiles should have been removed by MemBarNode::Ideal() for non-inline allocations\n+                assert(inline_alloc, \"MemBarVolatile should be eliminated for non-escaping object\");\n+                mb->remove(&_igvn);\n+              }\n@@ -968,1 +1015,0 @@\n-#endif\n@@ -992,2 +1038,1 @@\n-          CallProjections callprojs;\n-          ac->extract_projections(&callprojs, true);\n+          CallProjections* callprojs = ac->extract_projections(true);\n@@ -995,3 +1040,3 @@\n-          _igvn.replace_node(callprojs.fallthrough_ioproj, ac->in(TypeFunc::I_O));\n-          _igvn.replace_node(callprojs.fallthrough_memproj, ac->in(TypeFunc::Memory));\n-          _igvn.replace_node(callprojs.fallthrough_catchproj, ac->in(TypeFunc::Control));\n+          _igvn.replace_node(callprojs->fallthrough_ioproj, ac->in(TypeFunc::I_O));\n+          _igvn.replace_node(callprojs->fallthrough_memproj, ac->in(TypeFunc::Memory));\n+          _igvn.replace_node(callprojs->fallthrough_catchproj, ac->in(TypeFunc::Control));\n@@ -1014,0 +1059,8 @@\n+      } else if (use->is_InlineType()) {\n+        assert(use->isa_InlineType()->get_oop() == res, \"unexpected inline type use\");\n+        _igvn.rehash_node_delayed(use);\n+        use->isa_InlineType()->set_oop(_igvn.zerocon(T_INLINE_TYPE));\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(inline_alloc, \"Unexpected store to mark word\");\n+        _igvn.replace_node(use, use->in(MemNode::Memory));\n@@ -1047,0 +1100,5 @@\n+          \/\/ Inline type buffer allocations are followed by a membar\n+          Node* membar_after = ctrl_proj->unique_ctrl_out();\n+          if (inline_alloc && membar_after->Opcode() == Op_MemBarCPUOrder) {\n+            membar_after->as_MemBar()->remove(&_igvn);\n+          }\n@@ -1065,0 +1123,4 @@\n+      } else if (use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarStoreStore\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1096,1 +1158,1 @@\n-  if (!EliminateAllocations || JvmtiExport::can_pop_frame() || !alloc->_is_non_escaping) {\n+  if (!EliminateAllocations || JvmtiExport::can_pop_frame()) {\n@@ -1101,1 +1163,7 @@\n-  Node* res = alloc->result_cast();\n+\n+  \/\/ Attempt to eliminate inline type buffer allocations\n+  \/\/ regardless of usage and escape\/replaceable status.\n+  bool inline_alloc = tklass->klass()->is_inlinetype();\n+  if (!alloc->_is_non_escaping && !inline_alloc) {\n+    return false;\n+  }\n@@ -1103,3 +1171,4 @@\n-  \/\/ regardless scalar replacable status.\n-  bool boxing_alloc = C->eliminate_boxing() &&\n-                      tklass->klass()->is_instance_klass()  &&\n+  \/\/ regardless of scalar replaceable status.\n+  Node* res = alloc->result_cast();\n+  bool boxing_alloc = (res == NULL) && C->eliminate_boxing() &&\n+                      tklass->klass()->is_instance_klass() &&\n@@ -1107,1 +1176,1 @@\n-  if (!alloc->_is_scalar_replaceable && (!boxing_alloc || (res != NULL))) {\n+  if (!alloc->_is_scalar_replaceable && !boxing_alloc && !inline_alloc) {\n@@ -1119,1 +1188,1 @@\n-    assert(res == NULL, \"sanity\");\n+    assert(res == NULL || inline_alloc, \"sanity\");\n@@ -1124,0 +1193,1 @@\n+      assert(!inline_alloc, \"Inline type allocations should not have safepoint uses\");\n@@ -1144,1 +1214,1 @@\n-  process_users_of_allocation(alloc);\n+  process_users_of_allocation(alloc, inline_alloc);\n@@ -1168,1 +1238,1 @@\n-  const TypeTuple* r = boxing->tf()->range();\n+  const TypeTuple* r = boxing->tf()->range_sig();\n@@ -1369,1 +1439,1 @@\n-    IfNode *toobig_iff = new IfNode(ctrl, initial_slow_test, PROB_MIN, COUNT_UNKNOWN);\n+    IfNode* toobig_iff = new IfNode(ctrl, initial_slow_test, PROB_MIN, COUNT_UNKNOWN);\n@@ -1372,1 +1442,1 @@\n-    Node *toobig_true = new IfTrueNode( toobig_iff );\n+    Node* toobig_true = new IfTrueNode(toobig_iff);\n@@ -1375,1 +1445,1 @@\n-    toobig_false = new IfFalseNode( toobig_iff );\n+    toobig_false = new IfFalseNode(toobig_iff);\n@@ -1414,0 +1484,1 @@\n+\n@@ -1472,0 +1543,3 @@\n+  } else {\n+    \/\/ Let the runtime know if this is a larval allocation\n+    call->init_req(TypeFunc::Parms+1, _igvn.intcon(alloc->_larval));\n@@ -1476,1 +1550,1 @@\n-  copy_call_debug_info((CallNode *) alloc,  call);\n+  call->copy_call_debug_info(&_igvn, alloc);\n@@ -1504,1 +1578,1 @@\n-    migrate_outs(_memproj_fallthrough, result_phi_rawmem);\n+    _igvn.replace_in_uses(_memproj_fallthrough, result_phi_rawmem);\n@@ -1508,1 +1582,1 @@\n-  if (_memproj_catchall != NULL ) {\n+  if (_memproj_catchall != NULL) {\n@@ -1513,1 +1587,1 @@\n-    migrate_outs(_memproj_catchall, _memproj_fallthrough);\n+    _igvn.replace_in_uses(_memproj_catchall, _memproj_fallthrough);\n@@ -1523,1 +1597,1 @@\n-    migrate_outs(_ioproj_fallthrough, result_phi_i_o);\n+    _igvn.replace_in_uses(_ioproj_fallthrough, result_phi_i_o);\n@@ -1527,1 +1601,1 @@\n-  if (_ioproj_catchall != NULL ) {\n+  if (_ioproj_catchall != NULL) {\n@@ -1532,1 +1606,1 @@\n-    migrate_outs(_ioproj_catchall, _ioproj_fallthrough);\n+    _igvn.replace_in_uses(_ioproj_catchall, _ioproj_fallthrough);\n@@ -1600,1 +1674,1 @@\n-    migrate_outs(_fallthroughcatchproj, ctrl);\n+    _igvn.replace_in_uses(_fallthroughcatchproj, ctrl);\n@@ -1613,1 +1687,1 @@\n-    migrate_outs(_memproj_fallthrough, mem);\n+    _igvn.replace_in_uses(_memproj_fallthrough, mem);\n@@ -1617,1 +1691,1 @@\n-    migrate_outs(_ioproj_fallthrough, i_o);\n+    _igvn.replace_in_uses(_ioproj_fallthrough, i_o);\n@@ -1743,5 +1817,4 @@\n-Node*\n-PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n-                                    Node* control, Node* rawmem, Node* object,\n-                                    Node* klass_node, Node* length,\n-                                    Node* size_in_bytes) {\n+Node* PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n+                                          Node* control, Node* rawmem, Node* object,\n+                                          Node* klass_node, Node* length,\n+                                          Node* size_in_bytes) {\n@@ -1750,1 +1823,1 @@\n-  Node* mark_node = alloc->make_ideal_mark(&_igvn, object, control, rawmem);\n+  Node* mark_node = alloc->make_ideal_mark(&_igvn, control, rawmem);\n@@ -1782,0 +1855,2 @@\n+                                            alloc->in(AllocateNode::DefaultValue),\n+                                            alloc->in(AllocateNode::RawDefaultValue),\n@@ -2162,0 +2237,2 @@\n+  const Type* obj_type = _igvn.type(alock->obj_node());\n+  assert(!obj_type->isa_inlinetype() && !obj_type->is_inlinetypeptr(), \"Eliminating lock on inline type\");\n@@ -2443,0 +2520,42 @@\n+  const TypeOopPtr* objptr = _igvn.type(obj)->make_oopptr();\n+  if (objptr->can_be_inline_type()) {\n+    \/\/ Deoptimize and re-execute if a value\n+    assert(EnableValhalla, \"should only be used if inline types are enabled\");\n+    Node* mark = make_load(slow_path, mem, obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X->basic_type());\n+    Node* value_mask = _igvn.MakeConX(markWord::always_locked_pattern);\n+    Node* is_value = _igvn.transform(new AndXNode(mark, value_mask));\n+    Node* cmp = _igvn.transform(new CmpXNode(is_value, value_mask));\n+    Node* bol = _igvn.transform(new BoolNode(cmp, BoolTest::eq));\n+    Node* unc_ctrl = generate_slow_guard(&slow_path, bol, NULL);\n+\n+    int trap_request = Deoptimization::make_trap_request(Deoptimization::Reason_class_check, Deoptimization::Action_none);\n+    address call_addr = SharedRuntime::uncommon_trap_blob()->entry_point();\n+    const TypePtr* no_memory_effects = NULL;\n+    JVMState* jvms = lock->jvms();\n+    CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\",\n+                                           jvms->bci(), no_memory_effects);\n+\n+    unc->init_req(TypeFunc::Control, unc_ctrl);\n+    unc->init_req(TypeFunc::I_O, lock->i_o());\n+    unc->init_req(TypeFunc::Memory, mem); \/\/ may gc ptrs\n+    unc->init_req(TypeFunc::FramePtr,  lock->in(TypeFunc::FramePtr));\n+    unc->init_req(TypeFunc::ReturnAdr, lock->in(TypeFunc::ReturnAdr));\n+    unc->init_req(TypeFunc::Parms+0, _igvn.intcon(trap_request));\n+    unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+    unc->copy_call_debug_info(&_igvn, lock);\n+\n+    assert(unc->peek_monitor_box() == box, \"wrong monitor\");\n+    assert(unc->peek_monitor_obj() == obj, \"wrong monitor\");\n+\n+    \/\/ pop monitor and push obj back on stack: we trap before the monitorenter\n+    unc->pop_monitor();\n+    unc->grow_stack(unc->jvms(), 1);\n+    unc->set_stack(unc->jvms(), unc->jvms()->stk_size()-1, obj);\n+\n+    _igvn.register_new_node_with_optimizer(unc);\n+\n+    Node* ctrl = _igvn.transform(new ProjNode(unc, TypeFunc::Control));\n+    Node* halt = _igvn.transform(new HaltNode(ctrl, lock->in(TypeFunc::FramePtr), \"monitor enter on value-type\"));\n+    C->root()->add_req(halt);\n+  }\n+\n@@ -2544,0 +2663,205 @@\n+\/\/ An inline type might be returned from the call but we don't know its\n+\/\/ type. Either we get a buffered inline type (and nothing needs to be done)\n+\/\/ or one of the inlines being returned is the klass of the inline type\n+\/\/ and we need to allocate an inline type instance of that type and\n+\/\/ initialize it with other values being returned. In that case, we\n+\/\/ first try a fast path allocation and initialize the value with the\n+\/\/ inline klass's pack handler or we fall back to a runtime call.\n+void PhaseMacroExpand::expand_mh_intrinsic_return(CallStaticJavaNode* call) {\n+  assert(call->method()->is_method_handle_intrinsic(), \"must be a method handle intrinsic call\");\n+  Node* ret = call->proj_out_or_null(TypeFunc::Parms);\n+  if (ret == NULL) {\n+    return;\n+  }\n+  const TypeFunc* tf = call->_tf;\n+  const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+  const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+  call->_tf = new_tf;\n+  \/\/ Make sure the change of type is applied before projections are processed by igvn\n+  _igvn.set_type(call, call->Value(&_igvn));\n+  _igvn.set_type(ret, ret->Value(&_igvn));\n+\n+  \/\/ Before any new projection is added:\n+  CallProjections* projs = call->extract_projections(true, true);\n+\n+  Node* ctl = new Node(1);\n+  Node* mem = new Node(1);\n+  Node* io = new Node(1);\n+  Node* ex_ctl = new Node(1);\n+  Node* ex_mem = new Node(1);\n+  Node* ex_io = new Node(1);\n+  Node* res = new Node(1);\n+\n+  Node* cast = transform_later(new CastP2XNode(ctl, res));\n+  Node* mask = MakeConX(0x1);\n+  Node* masked = transform_later(new AndXNode(cast, mask));\n+  Node* cmp = transform_later(new CmpXNode(masked, mask));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* allocation_iff = new IfNode(ctl, bol, PROB_MAX, COUNT_UNKNOWN);\n+  transform_later(allocation_iff);\n+  Node* allocation_ctl = transform_later(new IfTrueNode(allocation_iff));\n+  Node* no_allocation_ctl = transform_later(new IfFalseNode(allocation_iff));\n+\n+  Node* no_allocation_res = transform_later(new CheckCastPPNode(no_allocation_ctl, res, TypeInstPtr::BOTTOM));\n+\n+  Node* mask2 = MakeConX(-2);\n+  Node* masked2 = transform_later(new AndXNode(cast, mask2));\n+  Node* rawklassptr = transform_later(new CastX2PNode(masked2));\n+  Node* klass_node = transform_later(new CheckCastPPNode(allocation_ctl, rawklassptr, TypeKlassPtr::OBJECT_OR_NULL));\n+\n+  Node* slowpath_bol = NULL;\n+  Node* top_adr = NULL;\n+  Node* old_top = NULL;\n+  Node* new_top = NULL;\n+  if (UseTLAB) {\n+    Node* end_adr = NULL;\n+    set_eden_pointers(top_adr, end_adr);\n+    Node* end = make_load(ctl, mem, end_adr, 0, TypeRawPtr::BOTTOM, T_ADDRESS);\n+    old_top = new LoadPNode(ctl, mem, top_adr, TypeRawPtr::BOTTOM, TypeRawPtr::BOTTOM, MemNode::unordered);\n+    transform_later(old_top);\n+    Node* layout_val = make_load(NULL, mem, klass_node, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);\n+    Node* size_in_bytes = ConvI2X(layout_val);\n+    new_top = new AddPNode(top(), old_top, size_in_bytes);\n+    transform_later(new_top);\n+    Node* slowpath_cmp = new CmpPNode(new_top, end);\n+    transform_later(slowpath_cmp);\n+    slowpath_bol = new BoolNode(slowpath_cmp, BoolTest::ge);\n+    transform_later(slowpath_bol);\n+  } else {\n+    slowpath_bol = intcon(1);\n+    top_adr = top();\n+    old_top = top();\n+    new_top = top();\n+  }\n+  IfNode* slowpath_iff = new IfNode(allocation_ctl, slowpath_bol, PROB_UNLIKELY_MAG(4), COUNT_UNKNOWN);\n+  transform_later(slowpath_iff);\n+\n+  Node* slowpath_true = new IfTrueNode(slowpath_iff);\n+  transform_later(slowpath_true);\n+\n+  CallStaticJavaNode* slow_call = new CallStaticJavaNode(OptoRuntime::store_inline_type_fields_Type(),\n+                                                         StubRoutines::store_inline_type_fields_to_buf(),\n+                                                         \"store_inline_type_fields\",\n+                                                         call->jvms()->bci(),\n+                                                         TypePtr::BOTTOM);\n+  slow_call->init_req(TypeFunc::Control, slowpath_true);\n+  slow_call->init_req(TypeFunc::Memory, mem);\n+  slow_call->init_req(TypeFunc::I_O, io);\n+  slow_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+  slow_call->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  slow_call->init_req(TypeFunc::Parms, res);\n+\n+  Node* slow_ctl = transform_later(new ProjNode(slow_call, TypeFunc::Control));\n+  Node* slow_mem = transform_later(new ProjNode(slow_call, TypeFunc::Memory));\n+  Node* slow_io = transform_later(new ProjNode(slow_call, TypeFunc::I_O));\n+  Node* slow_res = transform_later(new ProjNode(slow_call, TypeFunc::Parms));\n+  Node* slow_catc = transform_later(new CatchNode(slow_ctl, slow_io, 2));\n+  Node* slow_norm = transform_later(new CatchProjNode(slow_catc, CatchProjNode::fall_through_index, CatchProjNode::no_handler_bci));\n+  Node* slow_excp = transform_later(new CatchProjNode(slow_catc, CatchProjNode::catch_all_index,    CatchProjNode::no_handler_bci));\n+\n+  Node* ex_r = new RegionNode(3);\n+  Node* ex_mem_phi = new PhiNode(ex_r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* ex_io_phi = new PhiNode(ex_r, Type::ABIO);\n+  ex_r->init_req(1, slow_excp);\n+  ex_mem_phi->init_req(1, slow_mem);\n+  ex_io_phi->init_req(1, slow_io);\n+  ex_r->init_req(2, ex_ctl);\n+  ex_mem_phi->init_req(2, ex_mem);\n+  ex_io_phi->init_req(2, ex_io);\n+\n+  transform_later(ex_r);\n+  transform_later(ex_mem_phi);\n+  transform_later(ex_io_phi);\n+\n+  Node* slowpath_false = new IfFalseNode(slowpath_iff);\n+  transform_later(slowpath_false);\n+  Node* rawmem = new StorePNode(slowpath_false, mem, top_adr, TypeRawPtr::BOTTOM, new_top, MemNode::unordered);\n+  transform_later(rawmem);\n+  Node* mark_node = makecon(TypeRawPtr::make((address)markWord::always_locked_prototype().value()));\n+  rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::mark_offset_in_bytes(), mark_node, T_ADDRESS);\n+  rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  if (UseCompressedClassPointers) {\n+    rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::klass_gap_offset_in_bytes(), intcon(0), T_INT);\n+  }\n+  Node* fixed_block  = make_load(slowpath_false, rawmem, klass_node, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+  Node* pack_handler = make_load(slowpath_false, rawmem, fixed_block, in_bytes(InlineKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+\n+  CallLeafNoFPNode* handler_call = new CallLeafNoFPNode(OptoRuntime::pack_inline_type_Type(),\n+                                                        NULL,\n+                                                        \"pack handler\",\n+                                                        TypeRawPtr::BOTTOM);\n+  handler_call->init_req(TypeFunc::Control, slowpath_false);\n+  handler_call->init_req(TypeFunc::Memory, rawmem);\n+  handler_call->init_req(TypeFunc::I_O, top());\n+  handler_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+  handler_call->init_req(TypeFunc::ReturnAdr, top());\n+  handler_call->init_req(TypeFunc::Parms, pack_handler);\n+  handler_call->init_req(TypeFunc::Parms+1, old_top);\n+\n+  \/\/ We don't know how many values are returned. This assumes the\n+  \/\/ worst case, that all available registers are used.\n+  for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+    if (domain->field_at(i) == Type::HALF) {\n+      slow_call->init_req(i, top());\n+      handler_call->init_req(i+1, top());\n+      continue;\n+    }\n+    Node* proj = transform_later(new ProjNode(call, i));\n+    slow_call->init_req(i, proj);\n+    handler_call->init_req(i+1, proj);\n+  }\n+\n+  \/\/ We can safepoint at that new call\n+  slow_call->copy_call_debug_info(&_igvn, call);\n+  transform_later(slow_call);\n+  transform_later(handler_call);\n+\n+  Node* handler_ctl = transform_later(new ProjNode(handler_call, TypeFunc::Control));\n+  rawmem = transform_later(new ProjNode(handler_call, TypeFunc::Memory));\n+  Node* slowpath_false_res = transform_later(new ProjNode(handler_call, TypeFunc::Parms));\n+\n+  MergeMemNode* slowpath_false_mem = MergeMemNode::make(mem);\n+  slowpath_false_mem->set_memory_at(Compile::AliasIdxRaw, rawmem);\n+  transform_later(slowpath_false_mem);\n+\n+  Node* r = new RegionNode(4);\n+  Node* mem_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* io_phi = new PhiNode(r, Type::ABIO);\n+  Node* res_phi = new PhiNode(r, TypeInstPtr::BOTTOM);\n+\n+  r->init_req(1, no_allocation_ctl);\n+  mem_phi->init_req(1, mem);\n+  io_phi->init_req(1, io);\n+  res_phi->init_req(1, no_allocation_res);\n+  r->init_req(2, slow_norm);\n+  mem_phi->init_req(2, slow_mem);\n+  io_phi->init_req(2, slow_io);\n+  res_phi->init_req(2, slow_res);\n+  r->init_req(3, handler_ctl);\n+  mem_phi->init_req(3, slowpath_false_mem);\n+  io_phi->init_req(3, io);\n+  res_phi->init_req(3, slowpath_false_res);\n+\n+  transform_later(r);\n+  transform_later(mem_phi);\n+  transform_later(io_phi);\n+  transform_later(res_phi);\n+\n+  assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+  _igvn.replace_in_uses(projs->fallthrough_catchproj, r);\n+  _igvn.replace_in_uses(projs->fallthrough_memproj, mem_phi);\n+  _igvn.replace_in_uses(projs->fallthrough_ioproj, io_phi);\n+  _igvn.replace_in_uses(projs->resproj[0], res_phi);\n+  _igvn.replace_in_uses(projs->catchall_catchproj, ex_r);\n+  _igvn.replace_in_uses(projs->catchall_memproj, ex_mem_phi);\n+  _igvn.replace_in_uses(projs->catchall_ioproj, ex_io_phi);\n+\n+  _igvn.replace_node(ctl, projs->fallthrough_catchproj);\n+  _igvn.replace_node(mem, projs->fallthrough_memproj);\n+  _igvn.replace_node(io, projs->fallthrough_ioproj);\n+  _igvn.replace_node(res, projs->resproj[0]);\n+  _igvn.replace_node(ex_ctl, projs->catchall_catchproj);\n+  _igvn.replace_node(ex_mem, projs->catchall_memproj);\n+  _igvn.replace_node(ex_io, projs->catchall_ioproj);\n+ }\n+\n@@ -2569,1 +2893,1 @@\n-      subklass = _igvn.transform(LoadKlassNode::make(_igvn, NULL, C->immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+      subklass = _igvn.transform(LoadKlassNode::make(_igvn, NULL, C->immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeKlassPtr::OBJECT));\n@@ -2625,2 +2949,5 @@\n-      case Node::Class_CallStaticJava:\n-        success = eliminate_boxing_node(n->as_CallStaticJava());\n+      case Node::Class_CallStaticJava: {\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          success = eliminate_boxing_node(n->as_CallStaticJava());\n+        }\n@@ -2628,0 +2955,1 @@\n+      }\n@@ -2674,4 +3002,7 @@\n-        \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n-        C->remove_macro_node(n);\n-        _igvn._worklist.push(n);\n-        success = true;\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n+          C->remove_macro_node(n);\n+          _igvn._worklist.push(n);\n+          success = true;\n+        }\n@@ -2771,0 +3102,5 @@\n+    case Node::Class_CallStaticJava:\n+      expand_mh_intrinsic_return(n->as_CallStaticJava());\n+      C->remove_macro_node(n);\n+      assert(C->macro_count() == (old_macro_count - 1), \"expansion must have deleted one node from macro list\");\n+      break;\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":466,"deletions":130,"binary":false,"changes":596,"status":"modified"},{"patch":"@@ -565,0 +565,3 @@\n+  if (n->is_InlineTypeBase()) {\n+    C->add_inline_type(n);\n+  }\n@@ -645,0 +648,3 @@\n+  if (is_InlineTypeBase()) {\n+    compile->remove_inline_type(this);\n+  }\n@@ -1407,0 +1413,3 @@\n+      if (dead->is_InlineTypeBase()) {\n+        igvn->C->remove_inline_type(dead);\n+      }\n@@ -2169,1 +2178,3 @@\n-      assert(i >= req() || i == 0 || is_Region() || is_Phi() || is_ArrayCopy() || (is_Unlock() && i == req()-1)\n+      assert(i >= req() || i == 0 || is_Region() || is_Phi() || is_ArrayCopy() ||\n+             (is_Allocate() && i >= AllocateNode::InlineTypeNode) ||\n+             (is_Unlock() && i == req()-1)\n@@ -2171,1 +2182,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+             \"only region, phi, arraycopy, allocate or unlock nodes have null data edges\");\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -104,0 +104,1 @@\n+class MachPrologNode;\n@@ -110,0 +111,1 @@\n+class MachVEPNode;\n@@ -153,0 +155,3 @@\n+class InlineTypeBaseNode;\n+class InlineTypeNode;\n+class InlineTypePtrNode;\n@@ -664,0 +669,2 @@\n+      DEFINE_CLASS_ID(MachProlog,       Mach, 8)\n+      DEFINE_CLASS_ID(MachVEP,          Mach, 9)\n@@ -678,0 +685,3 @@\n+      DEFINE_CLASS_ID(InlineTypeBase, Type, 8)\n+        DEFINE_CLASS_ID(InlineType, InlineTypeBase, 0)\n+        DEFINE_CLASS_ID(InlineTypePtr, InlineTypeBase, 1)\n@@ -856,0 +866,1 @@\n+  DEFINE_CLASS_QUERY(MachProlog)\n@@ -862,0 +873,1 @@\n+  DEFINE_CLASS_QUERY(MachVEP)\n@@ -885,0 +897,3 @@\n+  DEFINE_CLASS_QUERY(InlineType)\n+  DEFINE_CLASS_QUERY(InlineTypeBase)\n+  DEFINE_CLASS_QUERY(InlineTypePtr)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -311,0 +311,2 @@\n+    _sp_inc_slot(0),\n+    _sp_inc_slot_offset_in_bytes(0),\n@@ -316,1 +318,6 @@\n-    _orig_pc_slot = C->fixed_slots() - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n+    int fixed_slots = C->fixed_slots();\n+    if (C->needs_stack_repair()) {\n+      fixed_slots -= 2;\n+      _sp_inc_slot = fixed_slots;\n+    }\n+    _orig_pc_slot = fixed_slots - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n@@ -355,1 +362,2 @@\n-  MachPrologNode *prolog = new MachPrologNode();\n+  Label verified_entry;\n+  MachPrologNode* prolog = new MachPrologNode(&verified_entry);\n@@ -361,3 +369,2 @@\n-\n-  if( C->is_osr_compilation() ) {\n-    if( PoisonOSREntry ) {\n+  if (C->is_osr_compilation()) {\n+    if (PoisonOSREntry) {\n@@ -368,3 +375,14 @@\n-    if( C->method() && !C->method()->flags().is_static() ) {\n-      \/\/ Insert unvalidated entry point\n-      C->cfg()->insert( broot, 0, new MachUEPNode() );\n+    if (C->method()) {\n+      if (C->method()->has_scalarized_args()) {\n+        \/\/ Add entry point to unpack all inline type arguments\n+        C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true, \/* receiver_only *\/ false));\n+        if (!C->method()->is_static()) {\n+          \/\/ Add verified\/unverified entry points to only unpack inline type receiver at interface calls\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ false));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true,  \/* receiver_only *\/ true));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ true));\n+        }\n+      } else if (!C->method()->is_static()) {\n+        \/\/ Insert unvalidated entry point\n+        C->cfg()->insert(broot, 0, new MachUEPNode());\n+      }\n@@ -372,1 +390,0 @@\n-\n@@ -412,0 +429,25 @@\n+  if (!C->is_osr_compilation() && C->has_scalarized_args()) {\n+    \/\/ Compute the offsets of the entry points required by the inline type calling convention\n+    if (!C->method()->is_static()) {\n+      \/\/ We have entries at the beginning of the method, implemented by the first 4 nodes.\n+      \/\/ Entry                     (unverified) @ offset 0\n+      \/\/ Verified_Inline_Entry_RO\n+      \/\/ Inline_Entry              (unverified)\n+      \/\/ Verified_Inline_Entry\n+      uint offset = 0;\n+      _code_offsets.set_value(CodeOffsets::Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(0))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(1))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Inline_Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(2))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, offset);\n+    } else {\n+      _code_offsets.set_value(CodeOffsets::Entry, -1); \/\/ will be patched later\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, 0);\n+    }\n+  }\n+\n@@ -569,1 +611,3 @@\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != NULL) {\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -1002,0 +1046,1 @@\n+  bool return_vt = false;\n@@ -1019,1 +1064,1 @@\n-    if (mcall->returns_pointer()) {\n+    if (mcall->returns_pointer() || mcall->returns_vt()) {\n@@ -1022,0 +1067,3 @@\n+    if (mcall->returns_vt()) {\n+      return_vt = true;\n+    }\n@@ -1136,1 +1184,1 @@\n-    C->debug_info()->describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms->bci(), jvms->should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, locvals, expvals, monvals);\n+    C->debug_info()->describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms->bci(), jvms->should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, return_vt, locvals, expvals, monvals);\n@@ -1241,0 +1289,4 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Compute the byte offset of the stack increment value\n+    _sp_inc_slot_offset_in_bytes = C->regalloc()->reg2offset(OptoReg::stack2reg(_sp_inc_slot));\n+  }\n@@ -1518,2 +1570,4 @@\n-          \/\/ This destination address is NOT PC-relative\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != NULL) {\n+            \/\/ This destination address is NOT PC-relative\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -3254,0 +3308,10 @@\n+    if (C->has_scalarized_args()) {\n+      \/\/ Inline type entry points (MachVEPNodes) require lots of space for GC barriers and oop verification\n+      \/\/ when loading object fields from the buffered argument. Increase scratch buffer size accordingly.\n+      int barrier_size = UseZGC ? 200 : (7 DEBUG_ONLY(+ 37));\n+      for (ciSignatureStream str(C->method()->signature()); !str.at_return_type(); str.next()) {\n+        if (str.type()->is_inlinetype() && str.type()->as_inline_klass()->can_be_passed_as_fields()) {\n+          size += str.type()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+      }\n+    }\n@@ -3318,0 +3382,6 @@\n+  } else if (n->is_MachProlog()) {\n+    saveL = ((MachPrologNode*)n)->_verified_entry;\n+    ((MachPrologNode*)n)->_verified_entry = &fakeL;\n+  } else if (n->is_MachVEP()) {\n+    saveL = ((MachVEPNode*)n)->_verified_entry;\n+    ((MachVEPNode*)n)->_verified_entry = &fakeL;\n@@ -3324,1 +3394,2 @@\n-  if (is_branch) \/\/ Restore label.\n+  \/\/ Restore label.\n+  if (is_branch) {\n@@ -3326,0 +3397,5 @@\n+  } else if (n->is_MachProlog()) {\n+    ((MachPrologNode*)n)->_verified_entry = saveL;\n+  } else if (n->is_MachVEP()) {\n+    ((MachVEPNode*)n)->_verified_entry = saveL;\n+  }\n@@ -3370,0 +3446,9 @@\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry_RO) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);\n+      }\n@@ -3374,12 +3459,12 @@\n-                                     entry_bci,\n-                                     &_code_offsets,\n-                                     _orig_pc_slot_offset_in_bytes,\n-                                     code_buffer(),\n-                                     frame_size_in_words(),\n-                                     oop_map_set(),\n-                                     &_handler_table,\n-                                     inc_table(),\n-                                     compiler,\n-                                     has_unsafe_access,\n-                                     SharedRuntime::is_wide_vector(C->max_vector_size()),\n-                                     C->rtm_state());\n+                              entry_bci,\n+                              &_code_offsets,\n+                              _orig_pc_slot_offset_in_bytes,\n+                              code_buffer(),\n+                              frame_size_in_words(),\n+                              _oop_map_set,\n+                              &_handler_table,\n+                              &_inc_table,\n+                              compiler,\n+                              has_unsafe_access,\n+                              SharedRuntime::is_wide_vector(C->max_vector_size()),\n+                              C->rtm_state());\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":112,"deletions":27,"binary":false,"changes":139,"status":"modified"},{"patch":"@@ -140,0 +140,3 @@\n+  \/\/ For the inline type calling convention\n+  int                    _sp_inc_slot;\n+  int                    _sp_inc_slot_offset_in_bytes;\n@@ -243,0 +246,2 @@\n+  int               sp_inc_offset()      const  { return _sp_inc_slot_offset_in_bytes; }\n+\n","filename":"src\/hotspot\/share\/opto\/output.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -105,4 +106,10 @@\n-Node *Parse::fetch_interpreter_state(int index,\n-                                     BasicType bt,\n-                                     Node *local_addrs,\n-                                     Node *local_addrs_base) {\n+Node* Parse::fetch_interpreter_state(int index,\n+                                     const Type* type,\n+                                     Node* local_addrs,\n+                                     Node* local_addrs_base) {\n+  BasicType bt = type->basic_type();\n+  if (type == TypePtr::NULL_PTR) {\n+    \/\/ Ptr types are mixed together with T_ADDRESS but NULL is\n+    \/\/ really for T_OBJECT types so correct it.\n+    bt = T_OBJECT;\n+  }\n@@ -120,0 +127,1 @@\n+  case T_INLINE_TYPE:\n@@ -150,1 +158,5 @@\n-\n+  if (type->isa_inlinetype() != NULL) {\n+    \/\/ The interpreter passes inline types as oops\n+    tp = TypeOopPtr::make_from_klass(type->inline_klass());\n+    tp = tp->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+  }\n@@ -174,0 +186,6 @@\n+    if (tp->klass()->is_inlinetype()) {\n+      \/\/ Check inline types for null here to prevent checkcast from adding an\n+      \/\/ exception state before the bytecode entry (use 'bad_type_ctrl' instead).\n+      l = null_check_oop(l, &bad_type_ctrl);\n+      bad_type_exit->control()->add_req(bad_type_ctrl);\n+    }\n@@ -177,3 +195,0 @@\n-\n-  BasicType bt_l = _gvn.type(l)->basic_type();\n-  BasicType bt_t = type->basic_type();\n@@ -192,1 +207,0 @@\n-\n@@ -230,1 +244,0 @@\n-\n@@ -234,1 +247,1 @@\n-    Node *lock_object = fetch_interpreter_state(index*2, T_OBJECT, monitors_addr, osr_buf);\n+    Node* lock_object = fetch_interpreter_state(index*2, Type::get_const_basic_type(T_OBJECT), monitors_addr, osr_buf);\n@@ -236,2 +249,1 @@\n-    Node *displaced_hdr = fetch_interpreter_state((index*2) + 1, T_ADDRESS, monitors_addr, osr_buf);\n-\n+    Node* displaced_hdr = fetch_interpreter_state((index*2) + 1, Type::get_const_basic_type(T_ADDRESS), monitors_addr, osr_buf);\n@@ -304,7 +316,1 @@\n-    BasicType bt = type->basic_type();\n-    if (type == TypePtr::NULL_PTR) {\n-      \/\/ Ptr types are mixed together with T_ADDRESS but NULL is\n-      \/\/ really for T_OBJECT types so correct it.\n-      bt = T_OBJECT;\n-    }\n-    Node *value = fetch_interpreter_state(index, bt, locals_addr, osr_buf);\n+    Node* value = fetch_interpreter_state(index, type, locals_addr, osr_buf);\n@@ -601,0 +607,21 @@\n+  \/\/ Handle inline type arguments\n+  int arg_size_sig = tf()->domain_sig()->cnt();\n+  for (uint i = 0; i < (uint)arg_size_sig; i++) {\n+    Node* parm = map()->in(i);\n+    const Type* t = _gvn.type(parm);\n+    if (t->is_inlinetypeptr() && t->inline_klass()->is_scalarizable() && !t->maybe_null()) {\n+      \/\/ Create InlineTypeNode from the oop and replace the parameter\n+      Node* vt = InlineTypeNode::make_from_oop(this, parm, t->inline_klass());\n+      map()->replace_edge(parm, vt);\n+    } else if (UseTypeSpeculation && (i == (uint)(arg_size_sig - 1)) && !is_osr_parse() &&\n+               method()->has_vararg() && t->isa_aryptr() != NULL && !t->is_aryptr()->is_not_null_free()) {\n+      \/\/ Speculate on varargs Object array being not null-free (and therefore also not flattened)\n+      const TypePtr* spec_type = t->speculative();\n+      spec_type = (spec_type != NULL && spec_type->isa_aryptr() != NULL) ? spec_type : t->is_aryptr();\n+      spec_type = spec_type->remove_speculative()->is_aryptr()->cast_to_not_null_free();\n+      spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, spec_type);\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), parm, t->join_speculative(spec_type)));\n+      replace_in_map(parm, cast);\n+    }\n+  }\n+\n@@ -783,2 +810,2 @@\n-  if (tf()->range()->cnt() > TypeFunc::Parms) {\n-    const Type* ret_type = tf()->range()->field_at(TypeFunc::Parms);\n+  if (tf()->range_sig()->cnt() > TypeFunc::Parms) {\n+    const Type* ret_type = tf()->range_sig()->field_at(TypeFunc::Parms);\n@@ -802,0 +829,5 @@\n+    if ((_caller->has_method() || tf()->returns_inline_type_as_fields()) &&\n+        ret_type->is_inlinetypeptr() && ret_type->inline_klass()->is_scalarizable() && !ret_type->maybe_null()) {\n+      \/\/ Scalarize inline type return when inlining or with multiple return values\n+      ret_type = TypeInlineType::make(ret_type->inline_klass());\n+    }\n@@ -806,1 +838,1 @@\n-    assert((int)(tf()->range()->cnt() - TypeFunc::Parms) == ret_size, \"good tf range\");\n+    assert((int)(tf()->range_sig()->cnt() - TypeFunc::Parms) == ret_size, \"good tf range\");\n@@ -813,1 +845,0 @@\n-\n@@ -818,2 +849,2 @@\n-  int        arg_size = tf->domain()->cnt();\n-  int        max_size = MAX2(arg_size, (int)tf->range()->cnt());\n+  int        arg_size = tf->domain_sig()->cnt();\n+  int        max_size = MAX2(arg_size, (int)tf->range_cc()->cnt());\n@@ -822,0 +853,2 @@\n+  map->set_jvms(jvms);\n+  jvms->set_map(map);\n@@ -833,3 +866,20 @@\n-  uint i;\n-  for (i = 0; i < (uint)arg_size; i++) {\n-    Node* parm = initial_gvn()->transform(new ParmNode(start, i));\n+  PhaseGVN& gvn = *initial_gvn();\n+  uint i = 0;\n+  ExtendedSignature sig_cc = ExtendedSignature(method()->get_sig_cc(), SigEntryFilter());\n+  for (uint j = 0; i < (uint)arg_size; i++) {\n+    const Type* t = tf->domain_sig()->field_at(i);\n+    Node* parm = NULL;\n+    if (has_scalarized_args() && t->is_inlinetypeptr() && !t->maybe_null() && t->inline_klass()->can_be_passed_as_fields()) {\n+      \/\/ Inline type arguments are not passed by reference: we get an argument per\n+      \/\/ field of the inline type. Build InlineTypeNodes from the inline type arguments.\n+      GraphKit kit(jvms, &gvn);\n+      kit.set_control(map->control());\n+      Node* old_mem = map->memory();\n+      \/\/ Use immutable memory for inline type loads and restore it below\n+      kit.set_all_memory(C->immutable_memory());\n+      parm = InlineTypeNode::make_from_multi(&kit, start, sig_cc, t->inline_klass(), j, true);\n+      map->set_control(kit.control());\n+      map->set_memory(old_mem);\n+    } else {\n+      parm = gvn.transform(new ParmNode(start, j++));\n+    }\n@@ -845,2 +895,0 @@\n-  map->set_jvms(jvms);\n-  jvms->set_map(map);\n@@ -873,1 +921,1 @@\n-  int ret_size = tf()->range()->cnt() - TypeFunc::Parms;\n+  int ret_size = tf()->range_sig()->cnt() - TypeFunc::Parms;\n@@ -877,2 +925,22 @@\n-    ret->add_req(kit.argument(0));\n-    \/\/ Note:  The second dummy edge is not needed by a ReturnNode.\n+    Node* res = kit.argument(0);\n+    if (tf()->returns_inline_type_as_fields()) {\n+      \/\/ Multiple return values (inline type fields): add as many edges\n+      \/\/ to the Return node as returned values.\n+      assert(res->is_InlineType(), \"what else supports multi value return?\");\n+      InlineTypeNode* vt = res->as_InlineType();\n+      ret->add_req_batch(NULL, tf()->range_cc()->cnt() - TypeFunc::Parms);\n+      if (vt->is_allocated(&kit.gvn()) && !StressInlineTypeReturnedAsFields) {\n+        ret->init_req(TypeFunc::Parms, vt->get_oop());\n+      } else {\n+        ret->init_req(TypeFunc::Parms, vt->tagged_klass(kit.gvn()));\n+      }\n+      const Array<SigEntry>* sig_array = vt->type()->inline_klass()->extended_sig();\n+      GrowableArray<SigEntry> sig = GrowableArray<SigEntry>(sig_array->length());\n+      sig.appendAll(sig_array);\n+      ExtendedSignature sig_cc = ExtendedSignature(&sig, SigEntryFilter());\n+      uint idx = TypeFunc::Parms+1;\n+      vt->pass_fields(&kit, ret, sig_cc, idx);\n+    } else {\n+      ret->add_req(res);\n+      \/\/ Note:  The second dummy edge is not needed by a ReturnNode.\n+    }\n@@ -1002,1 +1070,1 @@\n-  if (method()->is_initializer() &&\n+  if (method()->is_object_constructor_or_class_initializer() &&\n@@ -1040,2 +1108,2 @@\n-  if (tf()->range()->cnt() > TypeFunc::Parms) {\n-    const Type* ret_type = tf()->range()->field_at(TypeFunc::Parms);\n+  if (tf()->range_sig()->cnt() > TypeFunc::Parms) {\n+    const Type* ret_type = tf()->range_sig()->field_at(TypeFunc::Parms);\n@@ -1136,1 +1204,1 @@\n-    kit.null_check_receiver_before_call(method());\n+    kit.null_check_receiver_before_call(method(), false);\n@@ -1174,1 +1242,1 @@\n-  uint arg_size = tf()->domain()->cnt();\n+  uint arg_size = tf()->domain_sig()->cnt();\n@@ -1221,0 +1289,1 @@\n+      assert(!_gvn.type(lock_obj)->make_oopptr()->can_be_inline_type(), \"can't be an inline type\");\n@@ -1636,0 +1705,33 @@\n+  \/\/ Check for merge conflicts involving inline types\n+  JVMState* old_jvms = map()->jvms();\n+  int old_bci = bci();\n+  JVMState* tmp_jvms = old_jvms->clone_shallow(C);\n+  tmp_jvms->set_should_reexecute(true);\n+  map()->set_jvms(tmp_jvms);\n+  \/\/ Execution needs to restart a the next bytecode (entry of next\n+  \/\/ block)\n+  if (target->is_merged() ||\n+      pnum > PhiNode::Input ||\n+      target->is_handler() ||\n+      target->is_loop_head()) {\n+    set_parse_bci(target->start());\n+    for (uint j = TypeFunc::Parms; j < map()->req(); j++) {\n+      Node* n = map()->in(j);                 \/\/ Incoming change to target state.\n+      const Type* t = NULL;\n+      if (tmp_jvms->is_loc(j)) {\n+        t = target->local_type_at(j - tmp_jvms->locoff());\n+      } else if (tmp_jvms->is_stk(j) && j < (uint)sp() + tmp_jvms->stkoff()) {\n+        t = target->stack_type_at(j - tmp_jvms->stkoff());\n+      }\n+      if (t != NULL && t != Type::BOTTOM) {\n+        if (n->is_InlineType() && !t->isa_inlinetype()) {\n+          \/\/ Allocate inline type in src block to be able to merge it with oop in target block\n+          map()->set_req(j, n->as_InlineType()->buffer(this));\n+        }\n+        assert(!t->isa_inlinetype() || n->is_InlineType(), \"inconsistent typeflow info\");\n+      }\n+    }\n+  }\n+  map()->set_jvms(old_jvms);\n+  set_parse_bci(old_bci);\n+\n@@ -1689,0 +1791,1 @@\n+\n@@ -1724,0 +1827,1 @@\n+    bool last_merge = (pnum == PhiNode::Input);\n@@ -1728,1 +1832,1 @@\n-      if (m->is_Phi() && m->as_Phi()->region() == r)\n+      if (m->is_Phi() && m->as_Phi()->region() == r) {\n@@ -1730,1 +1834,3 @@\n-      else\n+      } else if (m->is_InlineType() && m->as_InlineType()->has_phi_inputs(r)){\n+        phi = m->as_InlineType()->get_oop()->as_Phi();\n+      } else {\n@@ -1732,0 +1838,1 @@\n+      }\n@@ -1765,1 +1872,24 @@\n-      if (phi != NULL) {\n+      \/\/ Merging two inline types?\n+      if (phi != NULL && n->is_InlineType()) {\n+        \/\/ Reload current state because it may have been updated by ensure_phi\n+        m = map()->in(j);\n+        InlineTypeNode* vtm = m->as_InlineType(); \/\/ Current inline type\n+        InlineTypeNode* vtn = n->as_InlineType(); \/\/ Incoming inline type\n+        assert(vtm->get_oop() == phi, \"Inline type should have Phi input\");\n+        if (TraceOptoParse) {\n+#ifdef ASSERT\n+          tty->print_cr(\"\\nMerging inline types\");\n+          tty->print_cr(\"Current:\");\n+          vtm->dump(2);\n+          tty->print_cr(\"Incoming:\");\n+          vtn->dump(2);\n+          tty->cr();\n+#endif\n+        }\n+        \/\/ Do the merge\n+        vtm->merge_with(&_gvn, vtn, pnum, last_merge);\n+        if (last_merge) {\n+          map()->set_req(j, _gvn.transform_no_reclaim(vtm));\n+          record_for_igvn(vtm);\n+        }\n+      } else if (phi != NULL) {\n@@ -1769,1 +1899,1 @@\n-        if (pnum == PhiNode::Input) {\n+        if (last_merge) {\n@@ -1785,2 +1915,1 @@\n-    if (pnum == PhiNode::Input &&\n-        !r->in(0)) {         \/\/ The occasional useless Region\n+    if (last_merge && !r->in(0)) {         \/\/ The occasional useless Region\n@@ -1938,0 +2067,2 @@\n+      } else if (n->is_InlineType() && n->as_InlineType()->has_phi_inputs(r)) {\n+        n->as_InlineType()->add_new_path(r);\n@@ -1960,0 +2091,4 @@\n+  InlineTypeBaseNode* vt = o->isa_InlineType();\n+  if (vt != NULL && vt->has_phi_inputs(region)) {\n+    return vt->get_oop()->as_Phi();\n+  }\n@@ -1979,2 +2114,2 @@\n-  \/\/ is mixing ints and oops or some such.  Forcing it to top\n-  \/\/ makes it go dead.\n+  \/\/ is already dead or is mixing ints and oops or some such.\n+  \/\/ Forcing it to top makes it go dead.\n@@ -1993,5 +2128,14 @@\n-  PhiNode* phi = PhiNode::make(region, o, t);\n-  gvn().set_type(phi, t);\n-  if (C->do_escape_analysis()) record_for_igvn(phi);\n-  map->set_req(idx, phi);\n-  return phi;\n+  if (vt != NULL) {\n+    \/\/ Inline types are merged by merging their field values.\n+    \/\/ Create a cloned InlineTypeNode with phi inputs that\n+    \/\/ represents the merged inline type and update the map.\n+    vt = vt->clone_with_phis(&_gvn, region);\n+    map->set_req(idx, vt);\n+    return vt->get_oop()->as_Phi();\n+  } else {\n+    PhiNode* phi = PhiNode::make(region, o, t);\n+    gvn().set_type(phi, t);\n+    if (C->do_escape_analysis()) record_for_igvn(phi);\n+    map->set_req(idx, phi);\n+    return phi;\n+  }\n@@ -2190,1 +2334,4 @@\n-  set_bci(InvocationEntryBci);\n+  \/\/ vreturn can trigger an allocation so vreturn can throw. Setting\n+  \/\/ the bci here breaks exception handling. Commenting this out\n+  \/\/ doesn't seem to break anything.\n+  \/\/  set_bci(InvocationEntryBci);\n@@ -2197,17 +2344,0 @@\n-  SafePointNode* exit_return = _exits.map();\n-  exit_return->in( TypeFunc::Control  )->add_req( control() );\n-  exit_return->in( TypeFunc::I_O      )->add_req( i_o    () );\n-  Node *mem = exit_return->in( TypeFunc::Memory   );\n-  for (MergeMemStream mms(mem->as_MergeMem(), merged_memory()); mms.next_non_empty2(); ) {\n-    if (mms.is_empty()) {\n-      \/\/ get a copy of the base memory, and patch just this one input\n-      const TypePtr* adr_type = mms.adr_type(C);\n-      Node* phi = mms.force_memory()->as_Phi()->slice_memory(adr_type);\n-      assert(phi->as_Phi()->region() == mms.base_memory()->in(0), \"\");\n-      gvn().set_type_bottom(phi);\n-      phi->del_req(phi->req()-1);  \/\/ prepare to re-patch\n-      mms.set_memory(phi);\n-    }\n-    mms.memory()->add_req(mms.memory2());\n-  }\n-\n@@ -2216,9 +2346,31 @@\n-    \/\/ If returning oops to an interface-return, there is a silent free\n-    \/\/ cast from oop to interface allowed by the Verifier.  Make it explicit\n-    \/\/ here.\n-    const TypeInstPtr *tr = phi->bottom_type()->isa_instptr();\n-    if (tr && tr->klass()->is_loaded() &&\n-        tr->klass()->is_interface()) {\n-      const TypeInstPtr *tp = value->bottom_type()->isa_instptr();\n-      if (tp && tp->klass()->is_loaded() &&\n-          !tp->klass()->is_interface()) {\n+    const Type* return_type = phi->bottom_type();\n+    const TypeOopPtr* tr = return_type->isa_oopptr();\n+    if (return_type->isa_inlinetype() && !Compile::current()->inlining_incrementally()) {\n+      \/\/ Inline type is returned as fields, make sure it is scalarized\n+      if (!value->is_InlineType()) {\n+        value = InlineTypeNode::make_from_oop(this, value, return_type->inline_klass());\n+      }\n+      if (!_caller->has_method()) {\n+        \/\/ Inline type is returned as fields from root method, make sure all non-flattened\n+        \/\/ fields are buffered and re-execute if allocation triggers deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        assert(tf()->returns_inline_type_as_fields(), \"must be returned as fields\");\n+        jvms()->set_should_reexecute(true);\n+        inc_sp(1);\n+        value = value->as_InlineType()->allocate_fields(this);\n+      }\n+    } else if (value->is_InlineType()) {\n+      \/\/ Inline type is returned as oop, make sure it is buffered and re-execute\n+      \/\/ if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      inc_sp(1);\n+      value = value->as_InlineType()->buffer(this);\n+      if (Compile::current()->inlining_incrementally()) {\n+        value = value->as_InlineTypeBase()->allocate_fields(this);\n+      }\n+    } else if (tr && tr->isa_instptr() && tr->klass()->is_loaded() && tr->klass()->is_interface()) {\n+      \/\/ If returning oops to an interface-return, there is a silent free\n+      \/\/ cast from oop to interface allowed by the Verifier. Make it explicit here.\n+      const TypeInstPtr* tp = value->bottom_type()->isa_instptr();\n+      if (tp && tp->klass()->is_loaded() && !tp->klass()->is_interface()) {\n@@ -2227,1 +2379,1 @@\n-        if (tp->higher_equal(TypeInstPtr::NOTNULL))\n+        if (tp->higher_equal(TypeInstPtr::NOTNULL)) {\n@@ -2229,0 +2381,1 @@\n+        }\n@@ -2232,1 +2385,1 @@\n-      \/\/ Also handle returns of oop-arrays to an arrays-of-interface return\n+      \/\/ Handle returns of oop-arrays to an arrays-of-interface return\n@@ -2235,1 +2388,1 @@\n-      Type::get_arrays_base_elements(phi->bottom_type(), value->bottom_type(), &phi_tip, &val_tip);\n+      Type::get_arrays_base_elements(return_type, value->bottom_type(), &phi_tip, &val_tip);\n@@ -2238,1 +2391,1 @@\n-        value = _gvn.transform(new CheckCastPPNode(0, value, phi->bottom_type()));\n+        value = _gvn.transform(new CheckCastPPNode(0, value, return_type));\n@@ -2244,0 +2397,17 @@\n+  SafePointNode* exit_return = _exits.map();\n+  exit_return->in( TypeFunc::Control  )->add_req( control() );\n+  exit_return->in( TypeFunc::I_O      )->add_req( i_o    () );\n+  Node *mem = exit_return->in( TypeFunc::Memory   );\n+  for (MergeMemStream mms(mem->as_MergeMem(), merged_memory()); mms.next_non_empty2(); ) {\n+    if (mms.is_empty()) {\n+      \/\/ get a copy of the base memory, and patch just this one input\n+      const TypePtr* adr_type = mms.adr_type(C);\n+      Node* phi = mms.force_memory()->as_Phi()->slice_memory(adr_type);\n+      assert(phi->as_Phi()->region() == mms.base_memory()->in(0), \"\");\n+      gvn().set_type_bottom(phi);\n+      phi->del_req(phi->req()-1);  \/\/ prepare to re-patch\n+      mms.set_memory(phi);\n+    }\n+    mms.memory()->add_req(mms.memory2());\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":253,"deletions":83,"binary":false,"changes":336,"status":"modified"},{"patch":"@@ -49,0 +49,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -197,1 +199,1 @@\n-JRT_BLOCK_ENTRY(void, OptoRuntime::new_instance_C(Klass* klass, JavaThread* thread))\n+JRT_BLOCK_ENTRY(void, OptoRuntime::new_instance_C(Klass* klass, bool is_larval, JavaThread* thread))\n@@ -217,1 +219,5 @@\n-    oop result = InstanceKlass::cast(klass)->allocate_instance(THREAD);\n+    instanceOop result = InstanceKlass::cast(klass)->allocate_instance(THREAD);\n+    if (is_larval) {\n+      \/\/ Check if this is a larval buffer allocation\n+      result->set_mark(result->mark().enter_larval_state());\n+    }\n@@ -245,1 +251,4 @@\n-  if (array_type->is_typeArray_klass()) {\n+  if (array_type->is_flatArray_klass()) {\n+    Klass* elem_type = FlatArrayKlass::cast(array_type)->element_klass();\n+    result = oopFactory::new_flatArray(elem_type, len, THREAD);\n+  } else if (array_type->is_typeArray_klass()) {\n@@ -251,5 +260,1 @@\n-    \/\/ Although the oopFactory likes to work with the elem_type,\n-    \/\/ the compiler prefers the array_type, since it must already have\n-    \/\/ that latter value in hand for the fast path.\n-    Klass* elem_type = ObjArrayKlass::cast(array_type)->element_klass();\n-    result = oopFactory::new_objArray(elem_type, len, THREAD);\n+    result = ObjArrayKlass::cast(array_type)->allocate(len, THREAD);\n@@ -450,1 +455,1 @@\n-  const Type **fields = TypeTuple::fields(1);\n+  const Type **fields = TypeTuple::fields(2);\n@@ -452,1 +457,2 @@\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+1, fields);\n+  fields[TypeFunc::Parms+1] = TypeInt::BOOL;        \/\/ is_larval\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n@@ -570,1 +576,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1224,1 +1230,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1547,1 +1553,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1565,1 +1571,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1581,1 +1587,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1713,0 +1719,106 @@\n+\n+const TypeFunc *OptoRuntime::store_inline_type_fields_Type() {\n+  \/\/ create input type (domain)\n+  uint total = SharedRuntime::java_return_convention_max_int + SharedRuntime::java_return_convention_max_float*2;\n+  const Type **fields = TypeTuple::fields(total);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypePtr::BOTTOM;\n+  uint i = 1;\n+  for (; i < SharedRuntime::java_return_convention_max_int; i++) {\n+    fields[TypeFunc::Parms+i] = TypeInt::INT;\n+  }\n+  for (; i < total; i+=2) {\n+    fields[TypeFunc::Parms+i] = Type::DOUBLE;\n+    fields[TypeFunc::Parms+i+1] = Type::HALF;\n+  }\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + total, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1,fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+const TypeFunc *OptoRuntime::pack_inline_type_Type() {\n+  \/\/ create input type (domain)\n+  uint total = 1 + SharedRuntime::java_return_convention_max_int + SharedRuntime::java_return_convention_max_float*2;\n+  const Type **fields = TypeTuple::fields(total);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypeRawPtr::BOTTOM;\n+  fields[TypeFunc::Parms+1] = TypeRawPtr::BOTTOM;\n+  uint i = 2;\n+  for (; i < SharedRuntime::java_return_convention_max_int+1; i++) {\n+    fields[TypeFunc::Parms+i] = TypeInt::INT;\n+  }\n+  for (; i < total; i+=2) {\n+    fields[TypeFunc::Parms+i] = Type::DOUBLE;\n+    fields[TypeFunc::Parms+i+1] = Type::HALF;\n+  }\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + total, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1,fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+JRT_LEAF(void, OptoRuntime::load_unknown_inline(flatArrayOopDesc* array, int index, instanceOopDesc* buffer))\n+{\n+  array->value_copy_from_index(index, buffer);\n+}\n+JRT_END\n+\n+const TypeFunc* OptoRuntime::load_unknown_inline_type() {\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(3);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypeOopPtr::NOTNULL;\n+  fields[TypeFunc::Parms+1] = TypeInt::POS;\n+  fields[TypeFunc::Parms+2] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+3, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(0);\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms+0, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+JRT_LEAF(void, OptoRuntime::store_unknown_inline(instanceOopDesc* buffer, flatArrayOopDesc* array, int index))\n+{\n+  assert(buffer != NULL, \"can't store null into flat array\");\n+  array->value_copy_to_index(buffer, index);\n+}\n+JRT_END\n+\n+const TypeFunc* OptoRuntime::store_unknown_inline_type() {\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(3);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypeInstPtr::NOTNULL;\n+  fields[TypeFunc::Parms+1] = TypeOopPtr::NOTNULL;\n+  fields[TypeFunc::Parms+2] = TypeInt::POS;\n+\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+3, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(0);\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms+0, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":127,"deletions":15,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -325,4 +325,3 @@\n-  CallProjections projs;\n-  call->extract_projections(&projs, false);\n-  if (projs.fallthrough_catchproj != NULL) {\n-    C->gvn_replace_by(projs.fallthrough_catchproj, call->in(TypeFunc::Control));\n+  CallProjections* projs = call->extract_projections(false);\n+  if (projs->fallthrough_catchproj != NULL) {\n+    C->gvn_replace_by(projs->fallthrough_catchproj, call->in(TypeFunc::Control));\n@@ -330,2 +329,2 @@\n-  if (projs.fallthrough_memproj != NULL) {\n-    C->gvn_replace_by(projs.fallthrough_memproj, call->in(TypeFunc::Memory));\n+  if (projs->fallthrough_memproj != NULL) {\n+    C->gvn_replace_by(projs->fallthrough_memproj, call->in(TypeFunc::Memory));\n@@ -333,2 +332,2 @@\n-  if (projs.catchall_memproj != NULL) {\n-    C->gvn_replace_by(projs.catchall_memproj, C->top());\n+  if (projs->catchall_memproj != NULL) {\n+    C->gvn_replace_by(projs->catchall_memproj, C->top());\n@@ -336,2 +335,2 @@\n-  if (projs.fallthrough_ioproj != NULL) {\n-    C->gvn_replace_by(projs.fallthrough_ioproj, call->in(TypeFunc::I_O));\n+  if (projs->fallthrough_ioproj != NULL) {\n+    C->gvn_replace_by(projs->fallthrough_ioproj, call->in(TypeFunc::I_O));\n@@ -339,2 +338,2 @@\n-  if (projs.catchall_ioproj != NULL) {\n-    C->gvn_replace_by(projs.catchall_ioproj, C->top());\n+  if (projs->catchall_ioproj != NULL) {\n+    C->gvn_replace_by(projs->catchall_ioproj, C->top());\n@@ -342,1 +341,1 @@\n-  if (projs.catchall_catchproj != NULL) {\n+  if (projs->catchall_catchproj != NULL) {\n@@ -345,1 +344,1 @@\n-    for (SimpleDUIterator i(projs.catchall_catchproj); i.has_next(); i.next()) {\n+    for (SimpleDUIterator i(projs->catchall_catchproj); i.has_next(); i.next()) {\n@@ -352,1 +351,1 @@\n-    C->gvn_replace_by(projs.catchall_catchproj, C->top());\n+    C->gvn_replace_by(projs->catchall_catchproj, C->top());\n@@ -354,2 +353,3 @@\n-  if (projs.resproj != NULL) {\n-    C->gvn_replace_by(projs.resproj, C->top());\n+  if (projs->resproj[0] != NULL) {\n+    assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+    C->gvn_replace_by(projs->resproj[0], C->top());\n","filename":"src\/hotspot\/share\/opto\/stringopts.cpp","additions":17,"deletions":17,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -52,0 +52,2 @@\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -478,0 +480,1 @@\n+  bool is_inlined = InstanceKlass::cast(k1)->field_is_inlined(slot);\n@@ -479,1 +482,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset);\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset, is_inlined);\n@@ -498,1 +501,1 @@\n-  if (m->is_initializer()) {\n+  if (m->is_object_constructor() || m->is_static_init_factory()) {\n@@ -560,1 +563,0 @@\n-\n@@ -896,1 +898,2 @@\n-    case T_OBJECT:      push_object(va_arg(_ap, jobject)); break;\n+    case T_OBJECT:\n+    case T_INLINE_TYPE: push_object(va_arg(_ap, jobject)); break;\n@@ -932,1 +935,2 @@\n-    case T_OBJECT:      push_object((_ap++)->l); break;\n+    case T_OBJECT:\n+    case T_INLINE_TYPE: push_object((_ap++)->l); break;\n@@ -1072,5 +1076,19 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherArray ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == NULL) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), NULL);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherArray ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  } else {\n+    JavaValue jvalue(T_INLINE_TYPE);\n+    JNI_ArgumentPusherArray ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, NULL, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -1078,1 +1096,1 @@\n-JNI_END\n+  JNI_END\n@@ -1092,5 +1110,19 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherVaArg ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == NULL) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), NULL);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+  } else {\n+    JavaValue jvalue(T_INLINE_TYPE);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, NULL, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -1112,8 +1144,25 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n-  obj = JNIHandles::make_local(THREAD, i);\n-  va_list args;\n-  va_start(args, methodID);\n-  JavaValue jvalue(T_VOID);\n-  JNI_ArgumentPusherVaArg ap(methodID, args);\n-  jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n-  va_end(args);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == NULL) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), NULL);\n+  }\n+\n+  if (!k->is_inline_klass()) {\n+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n+    obj = JNIHandles::make_local(THREAD, i);\n+    va_list args;\n+    va_start(args, methodID);\n+    JavaValue jvalue(T_VOID);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);\n+    va_end(args);\n+  } else {\n+    va_list args;\n+    va_start(args, methodID);\n+    JavaValue jvalue(T_INLINE_TYPE);\n+    JNI_ArgumentPusherVaArg ap(methodID, args);\n+    jni_invoke_static(env, &jvalue, NULL, JNI_STATIC, methodID, &ap, CHECK_NULL);\n+    va_end(args);\n+    obj = jvalue.get_jobject();\n+  }\n@@ -1897,1 +1946,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset());\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset(), fd.is_inlined());\n@@ -1908,0 +1957,1 @@\n+  oop res = NULL;\n@@ -1913,2 +1963,12 @@\n-  oop loaded_obj = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n-  jobject ret = JNIHandles::make_local(THREAD, loaded_obj);\n+  if (!jfieldIDWorkaround::is_inlined_jfieldID(fieldID)) {\n+    res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instance can have inlined fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);  \/\/ performance bottleneck\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineKlass* field_vklass = InlineKlass::cast(holder->get_inline_type_field_klass(fd.index()));\n+    res = field_vklass->read_inlined_field(o, ik->field_offset(fd.index()), CHECK_NULL);\n+  }\n+  jobject ret = JNIHandles::make_local(THREAD, res);\n@@ -2012,1 +2072,12 @@\n-  HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  if (!jfieldIDWorkaround::is_inlined_jfieldID(fieldID)) {\n+    HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instances can have inlined fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineKlass* vklass = InlineKlass::cast(holder->get_inline_type_field_klass(fd.index()));\n+    oop v = JNIHandles::resolve_non_null(value);\n+    vklass->write_inlined_field(o, offset, v, CHECK);\n+  }\n@@ -2449,4 +2520,13 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  if (a->is_within_bounds(index)) {\n-    ret = JNIHandles::make_local(THREAD, a->obj_at(index));\n-    return ret;\n+  oop res = NULL;\n+  arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+  if (arr->is_within_bounds(index)) {\n+    if (arr->is_flatArray()) {\n+      flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+      flatArrayHandle vah(thread, a);\n+      res = flatArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK_NULL);\n+      assert(res != NULL, \"Must be set in one of two paths above\");\n+    } else {\n+      assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+      objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+      res = a->obj_at(index);\n+    }\n@@ -2456,1 +2536,1 @@\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n+    ss.print(\"Index %d out of bounds for length %d\", index,arr->length());\n@@ -2459,0 +2539,2 @@\n+  ret = JNIHandles::make_local(THREAD, res);\n+  return ret;\n@@ -2466,1 +2548,1 @@\n- HOTSPOT_JNI_SETOBJECTARRAYELEMENT_ENTRY(env, array, index, value);\n+  HOTSPOT_JNI_SETOBJECTARRAYELEMENT_ENTRY(env, array, index, value);\n@@ -2469,24 +2551,51 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  oop v = JNIHandles::resolve(value);\n-  if (a->is_within_bounds(index)) {\n-    if (v == NULL || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n-      a->obj_at_put(index, v);\n-    } else {\n-      ResourceMark rm(THREAD);\n-      stringStream ss;\n-      Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n-      ss.print(\"type mismatch: can not store %s to %s[%d]\",\n-               v->klass()->external_name(),\n-               bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n-               index);\n-      for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n-        ss.print(\"[]\");\n-      }\n-      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n-    }\n-  } else {\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n-    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n-  }\n+   bool oob = false;\n+   int length = -1;\n+   oop res = NULL;\n+   arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+   if (arr->is_within_bounds(index)) {\n+     if (arr->is_flatArray()) {\n+       flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       FlatArrayKlass* vaklass = FlatArrayKlass::cast(a->klass());\n+       InlineKlass* element_vklass = vaklass->element_klass();\n+       if (v != NULL && v->is_a(element_vklass)) {\n+         a->value_copy_to_index(v, index);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *kl = FlatArrayKlass::cast(a->klass());\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             kl->external_name(),\n+             index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     } else {\n+       assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+       objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       if (v == NULL || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n+         a->obj_at_put(index, v);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n+                 index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     }\n+   } else {\n+     ResourceMark rm(THREAD);\n+     stringStream ss;\n+     ss.print(\"Index %d out of bounds for length %d\", index, arr->length());\n+     THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n+   }\n@@ -3288,0 +3397,277 @@\n+JNI_ENTRY(void*, jni_GetFlattenedArrayElements(JNIEnv* env, jarray array, jboolean* isCopy))\n+  JNIWrapper(\"jni_GetFlattenedArrayElements\");\n+  if (isCopy != NULL) {\n+    *isCopy = JNI_FALSE;\n+  }\n+  arrayOop ar = arrayOop(JNIHandles::resolve_non_null(array));\n+  if (!ar->is_array()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!ar->is_flatArray()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass());\n+  if (vak->contains_oops()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Flattened array contains oops\");\n+  }\n+  oop a = lock_gc_or_pin_object(thread, array);\n+  flatArrayOop vap = flatArrayOop(a);\n+  void* ret = vap->value_at_addr(0, vak->layout_helper());\n+  return ret;\n+JNI_END\n+\n+JNI_ENTRY(void, jni_ReleaseFlattenedArrayElements(JNIEnv* env, jarray array, void* elem, jint mode))\n+  JNIWrapper(\"jni_ReleaseFlattenedArrayElements\");\n+  unlock_gc_or_unpin_object(thread, array);\n+JNI_END\n+\n+JNI_ENTRY(jsize, jni_GetFlattenedArrayElementSize(JNIEnv* env, jarray array)) {\n+  JNIWrapper(\"jni_GetFlattenedElementSize\");\n+  arrayOop a = arrayOop(JNIHandles::resolve_non_null(array));\n+  if (!a->is_array()) {\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!a->is_flatArray()) {\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(a->klass());\n+  jsize ret = vak->element_byte_size();\n+  return ret;\n+}\n+JNI_END\n+\n+JNI_ENTRY(jclass, jni_GetFlattenedArrayElementClass(JNIEnv* env, jarray array))\n+  JNIWrapper(\"jni_GetArrayElementClass\");\n+  arrayOop a = arrayOop(JNIHandles::resolve_non_null(array));\n+  if (!a->is_array()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!a->is_flatArray()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(a->klass());\n+  InlineKlass* vk = vak->element_klass();\n+  return (jclass) JNIHandles::make_local(vk->java_mirror());\n+JNI_END\n+\n+JNI_ENTRY(jsize, jni_GetFieldOffsetInFlattenedLayout(JNIEnv* env, jclass clazz, const char *name, const char *signature, jboolean* is_inlined))\n+  JNIWrapper(\"jni_GetFieldOffsetInFlattenedLayout\");\n+\n+  oop mirror = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(mirror);\n+  if (!k->is_inline_klass()) {\n+    ResourceMark rm;\n+        THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), err_msg(\"%s has not flattened layout\", k->external_name()));\n+  }\n+  InlineKlass* vk = InlineKlass::cast(k);\n+\n+  TempNewSymbol fieldname = SymbolTable::probe(name, (int)strlen(name));\n+  TempNewSymbol signame = SymbolTable::probe(signature, (int)strlen(signature));\n+  if (fieldname == NULL || signame == NULL) {\n+    ResourceMark rm;\n+    THROW_MSG_0(vmSymbols::java_lang_NoSuchFieldError(), err_msg(\"%s.%s %s\", vk->external_name(), name, signature));\n+  }\n+\n+  assert(vk->is_initialized(), \"If a flattened array has been created, the element klass must have been initialized\");\n+\n+  fieldDescriptor fd;\n+  if (!vk->is_instance_klass() ||\n+      !InstanceKlass::cast(vk)->find_field(fieldname, signame, false, &fd)) {\n+    ResourceMark rm;\n+    THROW_MSG_0(vmSymbols::java_lang_NoSuchFieldError(), err_msg(\"%s.%s %s\", vk->external_name(), name, signature));\n+  }\n+\n+  int offset = fd.offset() - vk->first_field_offset();\n+  if (is_inlined != NULL) {\n+    *is_inlined = fd.is_inlined();\n+  }\n+  return (jsize)offset;\n+JNI_END\n+\n+JNI_ENTRY(jobject, jni_CreateSubElementSelector(JNIEnv* env, jarray array))\n+  JNIWrapper(\"jni_CreateSubElementSelector\");\n+\n+  oop ar = JNIHandles::resolve_non_null(array);\n+  if (!ar->is_array()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not an array\");\n+  }\n+  if (!ar->is_flatArray()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a flattened array\");\n+  }\n+  flatArrayHandle ar_h(THREAD, flatArrayOop(ar));\n+  Klass* ses_k = SystemDictionary::resolve_or_null(vmSymbols::jdk_internal_vm_jni_SubElementSelector(),\n+        Handle(THREAD, SystemDictionary::java_system_loader()), Handle(), CHECK_NULL);\n+  InstanceKlass* ses_ik = InstanceKlass::cast(ses_k);\n+  ses_ik->initialize(CHECK_NULL);\n+  Klass* elementKlass = ArrayKlass::cast(ar_h()->klass())->element_klass();\n+  oop ses = ses_ik->allocate_instance(CHECK_NULL);\n+  Handle ses_h(THREAD, ses);\n+  jdk_internal_vm_jni_SubElementSelector::setArrayElementType(ses_h(), elementKlass->java_mirror());\n+  jdk_internal_vm_jni_SubElementSelector::setSubElementType(ses_h(), elementKlass->java_mirror());\n+  jdk_internal_vm_jni_SubElementSelector::setOffset(ses_h(), 0);\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlined(ses_h(), true);   \/\/ by definition, top element of a flattened array is inlined\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlineType(ses_h(), true); \/\/ by definition, top element of a flattened array is an inline type\n+  return JNIHandles::make_local(ses_h());\n+JNI_END\n+\n+JNI_ENTRY(jobject, jni_GetSubElementSelector(JNIEnv* env, jobject selector, jfieldID fieldID))\n+  JNIWrapper(\"jni_GetSubElementSelector\");\n+\n+  oop slct = JNIHandles::resolve_non_null(selector);\n+  if (slct->klass()->name() != vmSymbols::jdk_internal_vm_jni_SubElementSelector()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Not a SubElementSelector\");\n+  }\n+  jboolean is_inlined = jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct);\n+  if (!is_inlined) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"SubElement is not inlined\");\n+  }\n+  oop semirror = jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct);\n+  Klass* k = java_lang_Class::as_Klass(semirror);\n+  if (!k->is_inline_klass()) {\n+    ResourceMark rm;\n+        THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), err_msg(\"%s is not an inline type\", k->external_name()));\n+  }\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  assert(vk->is_initialized(), \"If a flattened array has been created, the element klass must have been initialized\");\n+  int field_offset = jfieldIDWorkaround::from_instance_jfieldID(vk, fieldID);\n+  fieldDescriptor fd;\n+  if (!vk->find_field_from_offset(field_offset, false, &fd)) {\n+    THROW_NULL(vmSymbols::java_lang_NoSuchFieldError());\n+  }\n+  Handle arrayElementMirror(THREAD, jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct));\n+  \/\/ offset of the SubElement is offset of the original SubElement plus the offset of the field inside the element\n+  int offset = fd.offset() - vk->first_field_offset() + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+  InstanceKlass* sesklass = InstanceKlass::cast(JNIHandles::resolve_non_null(selector)->klass());\n+  oop res = sesklass->allocate_instance(CHECK_NULL);\n+  Handle res_h(THREAD, res);\n+  jdk_internal_vm_jni_SubElementSelector::setArrayElementType(res_h(), arrayElementMirror());\n+  InstanceKlass* holder = fd.field_holder();\n+  BasicType bt = Signature::basic_type(fd.signature());\n+  if (is_java_primitive(bt)) {\n+    jdk_internal_vm_jni_SubElementSelector::setSubElementType(res_h(), java_lang_Class::primitive_mirror(bt));\n+  } else {\n+    Klass* fieldKlass = SystemDictionary::resolve_or_fail(fd.signature(), Handle(THREAD, holder->class_loader()),\n+        Handle(THREAD, holder->protection_domain()), true, CHECK_NULL);\n+    jdk_internal_vm_jni_SubElementSelector::setSubElementType(res_h(),fieldKlass->java_mirror());\n+  }\n+  jdk_internal_vm_jni_SubElementSelector::setOffset(res_h(), offset);\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlined(res_h(), fd.is_inlined());\n+  jdk_internal_vm_jni_SubElementSelector::setIsInlineType(res_h(), fd.is_inline_type());\n+  return JNIHandles::make_local(res_h());\n+JNI_END\n+\n+JNI_ENTRY(jobject, jni_GetObjectSubElement(JNIEnv* env, jarray array, jobject selector, int index))\n+  JNIWrapper(\"jni_GetObjectSubElement\");\n+\n+  flatArrayOop ar =  (flatArrayOop)JNIHandles::resolve_non_null(array);\n+  oop slct = JNIHandles::resolve_non_null(selector);\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass());\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\");\n+  }\n+  oop res = NULL;\n+  if (!jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct)) {\n+    int offset = (address)ar->base() - cast_from_oop<address>(ar) + index * vak->element_byte_size()\n+                      + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+    res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(ar, offset);\n+  } else {\n+    Handle slct_h(THREAD, slct);\n+    InlineKlass* fieldKlass = InlineKlass::cast(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)));\n+    res = fieldKlass->allocate_instance_buffer(CHECK_NULL);\n+    \/\/ The array might have been moved by the GC, refreshing the arrayOop\n+    ar =  (flatArrayOop)JNIHandles::resolve_non_null(array);\n+    address addr = (address)ar->value_at_addr(index, vak->layout_helper())\n+              + jdk_internal_vm_jni_SubElementSelector::getOffset(slct_h());\n+    fieldKlass->inline_copy_payload_to_new_oop(addr, res);\n+  }\n+  return JNIHandles::make_local(res);\n+JNI_END\n+\n+JNI_ENTRY(void, jni_SetObjectSubElement(JNIEnv* env, jarray array, jobject selector, int index, jobject value))\n+  JNIWrapper(\"jni_SetObjectSubElement\");\n+\n+  flatArrayOop ar =  (flatArrayOop)JNIHandles::resolve_non_null(array);\n+  oop slct = JNIHandles::resolve_non_null(selector);\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass());\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\");\n+  }\n+  oop val = JNIHandles::resolve(value);\n+  if (val == NULL) {\n+    if (jdk_internal_vm_jni_SubElementSelector::getIsInlineType(slct)) {\n+      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), \"null cannot be stored in a flattened array\");\n+    }\n+  } else {\n+    if (!val->is_a(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)))) {\n+      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), \"type mismatch\");\n+    }\n+  }\n+  if (!jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct)) {\n+    int offset = (address)ar->base() - cast_from_oop<address>(ar) + index * vak->element_byte_size()\n+                  + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+    HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(ar, offset, JNIHandles::resolve(value));\n+  } else {\n+    InlineKlass* fieldKlass = InlineKlass::cast(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)));\n+    address addr = (address)ar->value_at_addr(index, vak->layout_helper())\n+                  + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);\n+    fieldKlass->inline_copy_oop_to_payload(JNIHandles::resolve_non_null(value), addr);\n+  }\n+JNI_END\n+\n+#define DEFINE_GETSUBELEMENT(ElementType,Result,ElementBasicType) \\\n+\\\n+JNI_ENTRY(ElementType, \\\n+          jni_Get##Result##SubElement(JNIEnv *env, jarray array, jobject selector, int index)) \\\n+  JNIWrapper(\"Get\" XSTR(Result) \"SubElement\"); \\\n+  flatArrayOop ar = (flatArrayOop)JNIHandles::resolve_non_null(array); \\\n+  oop slct = JNIHandles::resolve_non_null(selector); \\\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass()); \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) { \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\"); \\\n+  } \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct) != java_lang_Class::primitive_mirror(ElementBasicType)) { \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Wrong SubElement type\"); \\\n+  } \\\n+  address addr = (address)ar->value_at_addr(index, vak->layout_helper()) \\\n+               + jdk_internal_vm_jni_SubElementSelector::getOffset(slct); \\\n+  ElementType result = *(ElementType*)addr; \\\n+  return result; \\\n+JNI_END\n+\n+DEFINE_GETSUBELEMENT(jboolean, Boolean,T_BOOLEAN)\n+DEFINE_GETSUBELEMENT(jbyte, Byte, T_BYTE)\n+DEFINE_GETSUBELEMENT(jshort, Short,T_SHORT)\n+DEFINE_GETSUBELEMENT(jchar, Char,T_CHAR)\n+DEFINE_GETSUBELEMENT(jint, Int,T_INT)\n+DEFINE_GETSUBELEMENT(jlong, Long,T_LONG)\n+DEFINE_GETSUBELEMENT(jfloat, Float,T_FLOAT)\n+DEFINE_GETSUBELEMENT(jdouble, Double,T_DOUBLE)\n+\n+#define DEFINE_SETSUBELEMENT(ElementType,Result,ElementBasicType) \\\n+\\\n+JNI_ENTRY(void, \\\n+          jni_Set##Result##SubElement(JNIEnv *env, jarray array, jobject selector, int index, ElementType value)) \\\n+  JNIWrapper(\"Get\" XSTR(Result) \"SubElement\"); \\\n+  flatArrayOop ar = (flatArrayOop)JNIHandles::resolve_non_null(array); \\\n+  oop slct = JNIHandles::resolve_non_null(selector); \\\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(ar->klass()); \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) { \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array\/Selector mismatch\"); \\\n+  } \\\n+  if (jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct) != java_lang_Class::primitive_mirror(ElementBasicType)) { \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Wrong SubElement type\"); \\\n+  } \\\n+  address addr = (address)ar->value_at_addr(index, vak->layout_helper()) \\\n+               + jdk_internal_vm_jni_SubElementSelector::getOffset(slct); \\\n+  *(ElementType*)addr = value; \\\n+JNI_END\n+\n+DEFINE_SETSUBELEMENT(jboolean, Boolean,T_BOOLEAN)\n+DEFINE_SETSUBELEMENT(jbyte, Byte, T_BYTE)\n+DEFINE_SETSUBELEMENT(jshort, Short,T_SHORT)\n+DEFINE_SETSUBELEMENT(jchar, Char,T_CHAR)\n+DEFINE_SETSUBELEMENT(jint, Int,T_INT)\n+DEFINE_SETSUBELEMENT(jlong, Long,T_LONG)\n+DEFINE_SETSUBELEMENT(jfloat, Float,T_FLOAT)\n+DEFINE_SETSUBELEMENT(jdouble, Double,T_DOUBLE)\n+\n@@ -3571,1 +3957,32 @@\n-    jni_GetModule\n+    jni_GetModule,\n+\n+    \/\/ Flattened arrays features\n+\n+    jni_GetFlattenedArrayElements,\n+    jni_ReleaseFlattenedArrayElements,\n+    jni_GetFlattenedArrayElementClass,\n+    jni_GetFlattenedArrayElementSize,\n+    jni_GetFieldOffsetInFlattenedLayout,\n+\n+    jni_CreateSubElementSelector,\n+    jni_GetSubElementSelector,\n+    jni_GetObjectSubElement,\n+    jni_SetObjectSubElement,\n+\n+    jni_GetBooleanSubElement,\n+    jni_GetByteSubElement,\n+    jni_GetShortSubElement,\n+    jni_GetCharSubElement,\n+    jni_GetIntSubElement,\n+    jni_GetLongSubElement,\n+    jni_GetFloatSubElement,\n+    jni_GetDoubleSubElement,\n+\n+    jni_SetBooleanSubElement,\n+    jni_SetByteSubElement,\n+    jni_SetShortSubElement,\n+    jni_SetCharSubElement,\n+    jni_SetIntSubElement,\n+    jni_SetLongSubElement,\n+    jni_SetFloatSubElement,\n+    jni_SetDoubleSubElement\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":476,"deletions":59,"binary":false,"changes":535,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -666,1 +667,22 @@\n-  return handle == NULL ? 0 : ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)) ;\n+  if (handle == NULL) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, Universe::inline_type_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(SystemDictionary::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return ObjectSynchronizer::FastHashCode(THREAD, obj);\n+  }\n@@ -722,0 +744,1 @@\n+       klass->is_inline_klass() ||\n@@ -1267,1 +1290,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1270,1 +1294,1 @@\n-    size = 2;\n+    size = 3;\n@@ -1280,1 +1304,2 @@\n-      Klass* k = InstanceKlass::cast(klass)->local_interfaces()->at(index);\n+      InstanceKlass* ik = InstanceKlass::cast(klass);\n+      Klass* k = ik->local_interfaces()->at(index);\n@@ -1284,1 +1309,1 @@\n-    \/\/ All arrays implement java.lang.Cloneable and java.io.Serializable\n+    \/\/ All arrays implement java.lang.Cloneable, java.io.Serializable and java.lang.IdentityObject\n@@ -1287,0 +1312,1 @@\n+    result->obj_at_put(2, SystemDictionary::IdentityObject_klass()->java_mirror());\n@@ -1901,0 +1927,2 @@\n+  bool is_ctor = (method->is_object_constructor() ||\n+                  method->is_static_init_factory());\n@@ -1902,1 +1930,1 @@\n-    return (method->is_initializer() && !method->is_static());\n+    return is_ctor;\n@@ -1904,1 +1932,3 @@\n-    return  (!method->is_initializer() && !method->is_overpass());\n+    return (!is_ctor &&\n+            !method->is_class_initializer() &&\n+            !method->is_overpass());\n@@ -1967,0 +1997,2 @@\n+        assert(method->is_object_constructor() ||\n+               method->is_static_init_factory(), \"must be\");\n@@ -2226,3 +2258,1 @@\n-  if (!m->is_initializer() || m->is_static()) {\n-    method = Reflection::new_method(m, true, CHECK_NULL);\n-  } else {\n+  if (m->is_object_constructor() || m->is_static_init_factory()) {\n@@ -2230,0 +2260,2 @@\n+  } else {\n+    method = Reflection::new_method(m, true, CHECK_NULL);\n@@ -2517,0 +2549,39 @@\n+\/\/ Arrays support \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+JVM_ENTRY(jboolean, JVM_ArrayIsAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  JVMWrapper(\"JVM_ArrayIsAccessAtomic\");\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == NULL) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  return ArrayKlass::cast(k)->element_access_is_atomic();\n+JVM_END\n+\n+JVM_ENTRY(jobject, JVM_ArrayEnsureAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  JVMWrapper(\"JVM_ArrayEnsureAccessAtomic\");\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == NULL) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vk = FlatArrayKlass::cast(k);\n+    if (!vk->element_access_is_atomic()) {\n+      \/**\n+       * Need to decide how to implement:\n+       *\n+       * 1) Change to objArrayOop layout, therefore oop->klass() differs so\n+       * then \"<atomic>[Qfoo;\" klass needs to subclass \"[Qfoo;\" to pass through\n+       * \"checkcast\" & \"instanceof\"\n+       *\n+       * 2) Use extra header in the flatArrayOop to flag atomicity required and\n+       * possibly per instance lock structure. Said info, could be placed in\n+       * \"trailer\" rather than disturb the current arrayOop\n+       *\/\n+      Unimplemented();\n+    }\n+  }\n+  return array;\n+JVM_END\n+\n@@ -2696,1 +2767,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3690,1 +3761,1 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3711,0 +3782,1 @@\n+  objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3712,1 +3784,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":84,"deletions":13,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -2617,1 +2617,2 @@\n-                                            src_st.access_flags().is_static());\n+                                            src_st.access_flags().is_static(),\n+                                            src_st.field_descriptor().is_inlined());\n@@ -2654,2 +2655,3 @@\n-    Array<InstanceKlass*>* interface_list = InstanceKlass::cast(k)->local_interfaces();\n-    const int result_length = (interface_list == NULL ? 0 : interface_list->length());\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    Array<InstanceKlass*>* interface_list = ik->local_interfaces();\n+    int result_length = (interface_list == NULL ? 0 : interface_list->length());\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -971,2 +971,7 @@\n-    \/\/ Revoke any biases before querying the mark word\n-    BiasedLocking::revoke_at_safepoint(hobj);\n+    \/\/ Inline types instances don't support synchronization operations\n+    \/\/ they are marked as always locked and no attempt to remove a\n+    \/\/ potential bias (which cannot exist) should be made\n+    if (!hobj()->mark().is_always_locked()) {\n+      \/\/ Revoke any biases before querying the mark word\n+      BiasedLocking::revoke_at_safepoint(hobj);\n+    }\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnvBase.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-#include \"memory\/iterator.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n@@ -55,0 +55,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -60,0 +61,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -1826,0 +1828,92 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return NULL;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(aoop->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return NULL;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayOop result_array =\n+      oopFactory::new_objArray(SystemDictionary::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  instanceOop ioop = ih();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ioop->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array);\n+WB_END\n+\n+class CollectOops : public BasicOopIterateClosure {\n+ public:\n+  GrowableArray<Handle>* array;\n+\n+  objArrayOop create_results(TRAPS) {\n+    objArrayOop result_array =\n+        oopFactory::new_objArray(SystemDictionary::Object_klass(), array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < array->length(); i++) {\n+      result_array->obj_at_put(i, array->at(i)());\n+    }\n+    return result_array;\n+  }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return (jobjectArray)JNIHandles::make_local(THREAD, create_results(THREAD));\n+  }\n+\n+  void add_oop(oop o) {\n+    \/\/ Value might be oop, but JLS can't see as Object, just iterate through it...\n+    if (o != NULL && o->is_inline_type()) {\n+      o->oop_iterate(this);\n+    } else {\n+      array->append(Handle(Thread::current(), o));\n+    }\n+  }\n+\n+  void do_oop(oop* o) { add_oop(*o); }\n+  void do_oop(narrowOop* v) { add_oop(CompressedOops::decode(*v)); }\n+};\n+\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+\n+  JNIHandles::resolve(thing)->oop_iterate(&collectOops);\n+\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  StackFrameStream sfs(thread, false \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, NULL, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\n@@ -2496,0 +2590,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":101,"deletions":1,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -2167,0 +2167,10 @@\n+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypePassFieldsAsArgs)) {\n+    FLAG_SET_CMDLINE(InlineTypePassFieldsAsArgs, false);\n+    warning(\"InlineTypePassFieldsAsArgs is not supported on this platform\");\n+  }\n+\n+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypeReturnedAsFields)) {\n+    FLAG_SET_CMDLINE(InlineTypeReturnedAsFields, false);\n+    warning(\"InlineTypeReturnedAsFields is not supported on this platform\");\n+  }\n+\n@@ -3089,0 +3099,18 @@\n+  if (EnableValhalla) {\n+    \/\/ create_property(\"valhalla.enableValhalla\", \"true\", InternalProperty)\n+    const char* prop_name = \"valhalla.enableValhalla\";\n+    const char* prop_value = \"true\";\n+    const size_t prop_len = strlen(prop_name) + strlen(prop_value) + 2;\n+    char* property = AllocateHeap(prop_len, mtArguments);\n+    int ret = jio_snprintf(property, prop_len, \"%s=%s\", prop_name, prop_value);\n+    if (ret < 0 || ret >= (int)prop_len) {\n+      FreeHeap(property);\n+      return JNI_ENOMEM;\n+    }\n+    bool added = add_property(property, UnwriteableProperty, InternalProperty);\n+    FreeHeap(property);\n+    if (!added) {\n+      return JNI_ENOMEM;\n+    }\n+  }\n+\n@@ -4174,0 +4202,5 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !is_dumping_archive())) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported\n+    InlineTypePassFieldsAsArgs = false;\n+    InlineTypeReturnedAsFields = false;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":33,"deletions":0,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -44,0 +44,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n@@ -49,0 +51,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -192,2 +195,13 @@\n-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n-  Handle return_value;\n+  ScopeDesc* scope = chunk->at(0)->scope();\n+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n+  \/\/ In case of the return of multiple values, we must take care\n+  \/\/ of all oop return values.\n+  GrowableArray<Handle> return_oops;\n+  InlineKlass* vk = NULL;\n+  if (save_oop_result && scope->return_vt()) {\n+    vk = InlineKlass::returned_inline_klass(map);\n+    if (vk != NULL) {\n+      vk->save_oop_fields(map, return_oops);\n+      save_oop_result = false;\n+    }\n+  }\n@@ -199,1 +213,1 @@\n-    return_value = Handle(thread, result);\n+    return_oops.push(Handle(thread, result));\n@@ -206,1 +220,2 @@\n-  if (objects != NULL) {\n+  if (objects != NULL || vk != NULL) {\n+    bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();\n@@ -208,1 +223,7 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+      if (vk != NULL) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);\n+      }\n+      if (objects != NULL) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, THREAD);\n+      }\n@@ -210,2 +231,0 @@\n-    bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();\n-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal);\n@@ -216,1 +235,6 @@\n-      Deoptimization::print_objects(objects, realloc_failures);\n+      if (objects != NULL) {\n+        Deoptimization::print_objects(objects, realloc_failures);\n+      } else {\n+        Handle obj = realloc_failures ? Handle() : return_oops.first();\n+        Deoptimization::print_object(vk, obj, realloc_failures);\n+      }\n@@ -220,1 +244,1 @@\n-  if (save_oop_result) {\n+  if (save_oop_result || vk != NULL) {\n@@ -222,1 +246,2 @@\n-    deoptee.set_saved_oop_result(&map, return_value());\n+    assert(return_oops.length() == 1, \"no inline type\");\n+    deoptee.set_saved_oop_result(&map, return_oops.pop()());\n@@ -529,1 +554,1 @@\n-  \/\/ If the sender is deoptimized the we must retrieve the address of the handler\n+  \/\/ If the sender is deoptimized we must retrieve the address of the handler\n@@ -1020,0 +1045,4 @@\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* ak = FlatArrayKlass::cast(k);\n+      \/\/ Inline type array must be zeroed because not all memory is reassigned\n+      obj = ak->allocate(sv->field_size(), THREAD);\n@@ -1049,0 +1078,15 @@\n+\/\/ We're deoptimizing at the return of a call, inline type fields are\n+\/\/ in registers. When we go back to the interpreter, it will expect a\n+\/\/ reference to an inline type instance. Allocate and initialize it from\n+\/\/ the register values here.\n+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {\n+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);\n+  if (new_vt == NULL) {\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);\n+  }\n+  return_oops.clear();\n+  return_oops.push(Handle(THREAD, new_vt));\n+  return false;\n+}\n+\n@@ -1221,0 +1265,1 @@\n+  InstanceKlass* _klass;\n@@ -1225,0 +1270,1 @@\n+    _klass = NULL;\n@@ -1234,1 +1280,5 @@\n-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal) {\n+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal, int base_offset, TRAPS) {\n+  if (svIndex >= sv->field_size()) {\n+    \/\/ No fields left to re-assign.\n+    return svIndex;\n+  }\n@@ -1243,0 +1293,9 @@\n+        if (field._type == T_INLINE_TYPE) {\n+          field._type = T_OBJECT;\n+        }\n+        if (fs.is_inlined()) {\n+          \/\/ Resolve klass of flattened inline type field\n+          Klass* vk = klass->get_inline_type_field_klass(fs.index());\n+          field._klass = InlineKlass::cast(vk);\n+          field._type = T_INLINE_TYPE;\n+        }\n@@ -1253,1 +1312,1 @@\n-    int offset = fields->at(i)._offset;\n+    int offset = base_offset + fields->at(i)._offset;\n@@ -1256,1 +1315,2 @@\n-      case T_OBJECT: case T_ARRAY:\n+      case T_OBJECT:\n+      case T_ARRAY:\n@@ -1261,0 +1321,9 @@\n+      case T_INLINE_TYPE: {\n+        \/\/ Recursively re-assign flattened inline type fields\n+        InstanceKlass* vk = fields->at(i)._klass;\n+        assert(vk != NULL, \"must be resolved\");\n+        offset -= InlineKlass::cast(vk)->first_field_offset(); \/\/ Adjust offset to omit oop header\n+        svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, skip_internal, offset, CHECK_0);\n+        continue; \/\/ Continue because we don't need to increment svIndex\n+      }\n+\n@@ -1336,0 +1405,14 @@\n+\/\/ restore fields of an eliminated inline type array\n+void Deoptimization::reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool skip_internal, TRAPS) {\n+  InlineKlass* vk = vak->element_klass();\n+  assert(vk->flatten_array(), \"should only be used for flattened inline type arrays\");\n+  \/\/ Adjust offset to omit oop header\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE) - InlineKlass::cast(vk)->first_field_offset();\n+  \/\/ Initialize all elements of the flattened inline type array\n+  for (int i = 0; i < sv->field_size(); i++) {\n+    ScopeValue* val = sv->field_at(i);\n+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));\n+    reassign_fields_by_klass(vk, fr, reg_map, val->as_ObjectValue(), 0, (oop)obj, skip_internal, offset, CHECK);\n+  }\n+}\n+\n@@ -1337,1 +1420,1 @@\n-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal) {\n+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal, TRAPS) {\n@@ -1357,1 +1440,4 @@\n-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal);\n+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal, 0, CHECK);\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      reassign_flat_array_elements(fr, reg_map, sv, (flatArrayOop) obj(), vak, skip_internal, CHECK);\n@@ -1400,1 +1486,0 @@\n-\n@@ -1404,1 +1489,3 @@\n-    Handle obj = sv->value();\n+    print_object(k, sv->value(), realloc_failures);\n+  }\n+}\n@@ -1406,9 +1493,10 @@\n-    tty->print(\"     object <\" INTPTR_FORMAT \"> of type \", p2i(sv->value()()));\n-    k->print_value();\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n-    if (obj.is_null()) {\n-      tty->print(\" allocation failed\");\n-    } else {\n-      tty->print(\" allocated (%d bytes)\", obj->size() * HeapWordSize);\n-    }\n-    tty->cr();\n+void Deoptimization::print_object(Klass* k, Handle obj, bool realloc_failures) {\n+  tty->print(\"     object <\" INTPTR_FORMAT \"> of type \", p2i(obj()));\n+  k->print_value();\n+  assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+  if (obj.is_null()) {\n+    tty->print(\" allocation failed\");\n+  } else {\n+    tty->print(\" allocated (%d bytes)\", obj->size() * HeapWordSize);\n+  }\n+  tty->cr();\n@@ -1416,3 +1504,2 @@\n-    if (Verbose && !obj.is_null()) {\n-      k->oop_print_on(obj(), tty);\n-    }\n+  if (Verbose && !obj.is_null()) {\n+    k->oop_print_on(obj(), tty);\n@@ -1594,1 +1681,1 @@\n-  \/\/ Deoptimize only if the frame comes from compile code.\n+  \/\/ Deoptimize only if the frame comes from compiled code.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":118,"deletions":31,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -55,0 +56,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -289,0 +293,19 @@\n+\n+#ifdef COMPILER1\n+  if (cm->is_compiled_by_c1() && cm->method()->has_scalarized_args() &&\n+      pc() < cm->verified_inline_entry_point()) {\n+    \/\/ The VEP and VIEP(RO) of C1-compiled methods call into the runtime to buffer scalarized value\n+    \/\/ type args. We can't deoptimize at that point because the buffers have not yet been initialized.\n+    \/\/ Also, if the method is synchronized, we first need to acquire the lock.\n+    \/\/ Don't patch the return pc to delay deoptimization until we enter the method body (the check\n+    \/\/ addedin LIRGenerator::do_Base will detect the pending deoptimization by checking the original_pc).\n+#ifdef ASSERT\n+    NativeCall* call = nativeCall_before(this->pc());\n+    address dest = call->destination();\n+    assert(dest == Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id) ||\n+           dest == Runtime1::entry_for(Runtime1::buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+    return;\n+  }\n+#endif\n+\n@@ -684,1 +707,1 @@\n-                          OopClosure* f) {\n+                          OopClosure* f, BufferedValueClosure* bvt_f) {\n@@ -696,1 +719,3 @@\n-      _f->do_oop(addr);\n+      if (_f != NULL) {\n+        _f->do_oop(addr);\n+      }\n@@ -708,1 +733,3 @@\n-        _f->do_oop(addr);\n+        if (_f != NULL) {\n+          _f->do_oop(addr);\n+        }\n@@ -883,1 +910,1 @@\n-  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f);\n+  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f, NULL);\n@@ -895,0 +922,17 @@\n+void frame::buffered_values_interpreted_do(BufferedValueClosure* f) {\n+  assert(is_interpreted_frame(), \"Not an interpreted frame\");\n+  Thread *thread = Thread::current();\n+  methodHandle m (thread, interpreter_frame_method());\n+  jint      bci = interpreter_frame_bci();\n+\n+  assert(m->is_method(), \"checking frame value\");\n+  assert(!m->is_native() && bci >= 0 && bci < m->code_size(),\n+         \"invalid bci value\");\n+\n+  InterpreterFrameClosure blk(this, m->max_locals(), m->max_stack(), NULL, f);\n+\n+  \/\/ process locals & expression stack\n+  InterpreterOopMap mask;\n+  m->mask_for(bci, &mask);\n+  mask.iterate_oop(&blk);\n+}\n@@ -942,0 +986,1 @@\n+    assert(_offset < _arg_size, \"out of bounds\");\n@@ -958,5 +1003,1 @@\n-    _arg_size  = ArgumentSizeComputer(signature).size() + (has_receiver ? 1 : 0) + (has_appendix ? 1 : 0);\n-\n-    int arg_size;\n-    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &arg_size);\n-    assert(arg_size == _arg_size, \"wrong arg size\");\n+    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &_arg_size);\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":50,"deletions":9,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -369,0 +369,1 @@\n+  void buffered_values_interpreted_do(BufferedValueClosure* f);\n","filename":"src\/hotspot\/share\/runtime\/frame.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -777,0 +777,18 @@\n+  notproduct(bool, PrintInlineLayout, false,                                \\\n+          \"Print field layout for each inline type\")                        \\\n+                                                                            \\\n+  notproduct(bool, PrintFlatArrayLayout, false,                             \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxSize, -1,                                \\\n+          \"Max size for flattening inline array elements, <0 no limit\")     \\\n+                                                                            \\\n+  product(intx, InlineFieldMaxFlatSize, 128,                                \\\n+          \"Max size for flattening inline type fields, <0 no limit\")        \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  product(bool, InlineArrayAtomicAccess, false,                             \\\n+          \"Atomic inline array accesses by-default, for all inline arrays\") \\\n+                                                                            \\\n@@ -792,1 +810,1 @@\n-  product(bool, UseBiasedLocking, false,                                    \\\n+  product(bool, UseBiasedLocking, true,                                     \\\n@@ -2459,0 +2477,20 @@\n+  product(bool, EnableValhalla, true,                                       \\\n+          \"Enable experimental Valhalla features\")                          \\\n+                                                                            \\\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressInlineTypeReturnedAsFields, false,                    \\\n+          \"Stress return of fields instead of an inline type reference\")    \\\n+                                                                            \\\n+  develop(bool, ScalarizeInlineTypes, true,                                 \\\n+          \"Scalarize inline types in compiled code\")                        \\\n+                                                                            \\\n+  product(ccstrlist, ForceNonTearable, \"\", DIAGNOSTIC,                      \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n@@ -2467,0 +2505,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":40,"deletions":1,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -159,4 +160,4 @@\n-    case T_BOOLEAN: \/\/ fall through\n-    case T_CHAR   : \/\/ fall through\n-    case T_SHORT  : \/\/ fall through\n-    case T_INT    : \/\/ fall through\n+    case T_BOOLEAN  : \/\/ fall through\n+    case T_CHAR     : \/\/ fall through\n+    case T_SHORT    : \/\/ fall through\n+    case T_INT      : \/\/ fall through\n@@ -164,2 +165,3 @@\n-    case T_OBJECT : \/\/ fall through\n-    case T_ARRAY  : \/\/ fall through\n+    case T_OBJECT   : \/\/ fall through\n+    case T_ARRAY    : \/\/ fall through\n+    case T_INLINE_TYPE: \/\/ fall through\n@@ -167,5 +169,5 @@\n-    case T_BYTE   : \/\/ fall through\n-    case T_VOID   : return T_INT;\n-    case T_LONG   : return T_LONG;\n-    case T_FLOAT  : return T_FLOAT;\n-    case T_DOUBLE : return T_DOUBLE;\n+    case T_BYTE     : \/\/ fall through\n+    case T_VOID     : return T_INT;\n+    case T_LONG     : return T_LONG;\n+    case T_FLOAT    : return T_FLOAT;\n+    case T_DOUBLE   : return T_DOUBLE;\n@@ -173,2 +175,3 @@\n-    case T_ARRAY  : \/\/ fall through\n-    case T_OBJECT:  return T_OBJECT;\n+    case T_ARRAY    : \/\/ fall through\n+    case T_OBJECT   : return T_OBJECT;\n+    case T_INLINE_TYPE: return T_INLINE_TYPE;\n@@ -302,0 +305,13 @@\n+\n+  \/\/ Special case for factory methods\n+  if (!constructor_signature->is_void_method_signature()) {\n+    assert(klass->is_inline_klass(), \"inline classes must use factory methods\");\n+    JavaValue factory_result(T_OBJECT);\n+    JavaCalls::call_static(&factory_result, klass,\n+                           vmSymbols::object_initializer_name(),\n+                           constructor_signature, args, CHECK_NH);\n+    return Handle(THREAD, (oop)factory_result.get_jobject());\n+  }\n+\n+  \/\/ main branch of code creates a non-inline object:\n+  assert(!klass->is_inline_klass(), \"classic constructors are only for non-inline classes\");\n@@ -404,0 +420,12 @@\n+  jobject value_buffer = NULL;\n+  if (InlineTypeReturnedAsFields && result->get_type() == T_INLINE_TYPE) {\n+    \/\/ Pre allocate a buffered inline type in case the result is returned\n+    \/\/ flattened by compiled code\n+    InlineKlass* vk = method->returned_inline_type(thread);\n+    if (vk->can_be_returned_as_fields()) {\n+      oop instance = vk->allocate_instance(CHECK);\n+      value_buffer = JNIHandles::make_local(thread, instance);\n+      result->set_jobject(value_buffer);\n+    }\n+  }\n+\n@@ -454,0 +482,1 @@\n+    JNIHandles::destroy_local(value_buffer);\n@@ -590,0 +619,1 @@\n+    case T_INLINE_TYPE:\n@@ -602,1 +632,1 @@\n-  if (is_reference_type(return_type)) return_type = T_OBJECT;\n+  if (return_type == T_ARRAY) return_type = T_OBJECT;\n","filename":"src\/hotspot\/share\/runtime\/javaCalls.cpp","additions":44,"deletions":14,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -970,0 +971,1 @@\n+    ResourceMark rm;\n@@ -971,2 +973,25 @@\n-    bool return_oop = nm->method()->is_returning_oop();\n-    Handle return_value;\n+    Method* method = nm->method();\n+    bool return_oop = method->is_returning_oop();\n+\n+    GrowableArray<Handle> return_values;\n+    InlineKlass* vk = NULL;\n+\n+    if (return_oop && InlineTypeReturnedAsFields) {\n+      SignatureStream ss(method->signature());\n+      while (!ss.at_return_type()) {\n+        ss.next();\n+      }\n+      if (ss.type() == T_INLINE_TYPE) {\n+        \/\/ Check if inline type is returned as fields\n+        vk = InlineKlass::returned_inline_klass(map);\n+        if (vk != NULL) {\n+          \/\/ We're at a safepoint at the return of a method that returns\n+          \/\/ multiple values. We must make sure we preserve the oop values\n+          \/\/ across the safepoint.\n+          assert(vk == method->returned_inline_type(thread()), \"bad inline klass\");\n+          vk->save_oop_fields(map, return_values);\n+          return_oop = false;\n+        }\n+      }\n+    }\n+\n@@ -979,1 +1004,1 @@\n-      return_value = Handle(thread(), result);\n+      return_values.push(Handle(thread(), result));\n@@ -993,1 +1018,4 @@\n-      caller_fr.set_saved_oop_result(&map, return_value());\n+      assert(return_values.length() == 1, \"only one return value\");\n+      caller_fr.set_saved_oop_result(&map, return_values.pop()());\n+    } else if (vk != NULL) {\n+      vk->restore_oop_results(map, return_values);\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -49,0 +50,2 @@\n+#include \"oops\/access.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -52,0 +55,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -53,0 +57,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -89,1 +94,0 @@\n-address             SharedRuntime::_resolve_static_call_entry;\n@@ -109,1 +113,0 @@\n-  _resolve_static_call_entry           = _resolve_static_call_blob->entry_point();\n@@ -1066,0 +1069,15 @@\n+  \/\/ Substitutability test implementation piggy backs on static call resolution\n+  Bytecodes::Code code = caller->java_code_at(bci);\n+  if (code == Bytecodes::_if_acmpeq || code == Bytecodes::_if_acmpne) {\n+    bc = Bytecodes::_invokestatic;\n+    methodHandle attached_method(THREAD, extract_attached_method(vfst));\n+    assert(attached_method.not_null(), \"must have attached method\");\n+    SystemDictionary::ValueBootstrapMethods_klass()->initialize(CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, false, CHECK_NH);\n+#ifdef ASSERT\n+    Method* is_subst = SystemDictionary::ValueBootstrapMethods_klass()->find_method(vmSymbols::isSubstitutable_name(), vmSymbols::object_object_boolean_signature());\n+    assert(callinfo.selected_method() == is_subst, \"must be isSubstitutable method\");\n+#endif\n+    return receiver;\n+  }\n+\n@@ -1101,0 +1119,6 @@\n+    } else {\n+      assert(attached_method->has_scalarized_args(), \"invalid use of attached method\");\n+      if (!attached_method->method_holder()->is_inline_klass()) {\n+        \/\/ Ignore the attached method in this case to not confuse below code\n+        attached_method = methodHandle(thread, NULL);\n+      }\n@@ -1109,0 +1133,1 @@\n+  bool check_null_and_abstract = true;\n@@ -1118,0 +1143,1 @@\n+    bool caller_is_c1 = false;\n@@ -1119,2 +1145,7 @@\n-    if (attached_method.is_null()) {\n-      Method* callee = bytecode.static_target(CHECK_NH);\n+    if (callerFrame.is_compiled_frame() && !callerFrame.is_deoptimized_frame()) {\n+      caller_is_c1 = callerFrame.cb()->is_compiled_by_c1();\n+    }\n+\n+    Method* callee = attached_method();\n+    if (callee == NULL) {\n+      callee = bytecode.static_target(CHECK_NH);\n@@ -1125,6 +1156,15 @@\n-\n-    \/\/ Retrieve from a compiled argument list\n-    receiver = Handle(THREAD, callerFrame.retrieve_receiver(&reg_map2));\n-\n-    if (receiver.is_null()) {\n-      THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+    if (!caller_is_c1 && callee->has_scalarized_args() && callee->method_holder()->is_inline_klass() &&\n+        InlineKlass::cast(callee->method_holder())->can_be_passed_as_fields()) {\n+      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n+      \/\/ Resolve the call without receiver null checking.\n+      assert(attached_method.not_null() && !attached_method->is_abstract(), \"must have non-abstract attached method\");\n+      if (bc == Bytecodes::_invokeinterface) {\n+        bc = Bytecodes::_invokevirtual; \/\/ C2 optimistically replaces interface calls by virtual calls\n+      }\n+      check_null_and_abstract = false;\n+    } else {\n+      \/\/ Retrieve from a compiled argument list\n+      receiver = Handle(THREAD, callerFrame.retrieve_receiver(&reg_map2));\n+      if (receiver.is_null()) {\n+        THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+      }\n@@ -1137,1 +1177,1 @@\n-    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);\n@@ -1146,1 +1186,1 @@\n-  if (has_receiver) {\n+  if (has_receiver && check_null_and_abstract) {\n@@ -1205,1 +1245,2 @@\n-                                           bool is_optimized, TRAPS) {\n+                                           bool is_optimized,\n+                                           bool* caller_is_c1, TRAPS) {\n@@ -1207,1 +1248,1 @@\n-  callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, THREAD);\n+  callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, caller_is_c1, THREAD);\n@@ -1224,1 +1265,1 @@\n-      callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, THREAD);\n+      callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, caller_is_c1, THREAD);\n@@ -1255,0 +1296,1 @@\n+  bool caller_is_c1 = caller_nm->is_compiled_by_c1();\n@@ -1257,1 +1299,9 @@\n-    assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, \"sanity check\");\n+    Klass* receiver_klass = NULL;\n+    if (!caller_is_c1 && callee_method->has_scalarized_args() && callee_method->method_holder()->is_inline_klass() &&\n+        InlineKlass::cast(callee_method->method_holder())->can_be_passed_as_fields()) {\n+      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n+      receiver_klass = callee_method->method_holder();\n+    } else {\n+      assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, \"sanity check\");\n+      receiver_klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver->klass();\n+    }\n@@ -1259,3 +1309,2 @@\n-    Klass* klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver->klass();\n-    CompiledIC::compute_monomorphic_entry(callee_method, klass,\n-                     is_optimized, static_bound, is_nmethod, virtual_call_info,\n+    CompiledIC::compute_monomorphic_entry(callee_method, receiver_klass,\n+                     is_optimized, static_bound, is_nmethod, caller_is_c1, virtual_call_info,\n@@ -1265,1 +1314,1 @@\n-    CompiledStaticCall::compute_entry(callee_method, is_nmethod, static_call_info);\n+    CompiledStaticCall::compute_entry(callee_method, caller_nm, static_call_info);\n@@ -1317,1 +1366,2 @@\n-                                               bool is_optimized, TRAPS) {\n+                                               bool is_optimized,\n+                                               bool* caller_is_c1, TRAPS) {\n@@ -1326,0 +1376,1 @@\n+  *caller_is_c1 = caller_nm->is_compiled_by_c1();\n@@ -1427,0 +1478,2 @@\n+  bool is_optimized = false;\n+  bool caller_is_c1 = false;\n@@ -1428,1 +1481,1 @@\n-    callee_method = SharedRuntime::handle_ic_miss_helper(thread, CHECK_NULL);\n+    callee_method = SharedRuntime::handle_ic_miss_helper(thread, is_optimized, caller_is_c1, CHECK_NULL);\n@@ -1433,2 +1486,1 @@\n-  assert(callee_method->verified_code_entry() != NULL, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  return entry_for_handle_wrong_method(callee_method, false, is_optimized, caller_is_c1);\n@@ -1477,0 +1529,3 @@\n+  bool is_static_call = false;\n+  bool is_optimized = false;\n+  bool caller_is_c1 = false;\n@@ -1479,1 +1534,1 @@\n-    callee_method = SharedRuntime::reresolve_call_site(thread, CHECK_NULL);\n+    callee_method = SharedRuntime::reresolve_call_site(thread, is_static_call, is_optimized, caller_is_c1, CHECK_NULL);\n@@ -1483,2 +1538,1 @@\n-  assert(callee_method->verified_code_entry() != NULL, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  return entry_for_handle_wrong_method(callee_method, is_static_call, is_optimized, caller_is_c1);\n@@ -1522,0 +1576,1 @@\n+  bool caller_is_c1;\n@@ -1523,1 +1578,1 @@\n-    callee_method = SharedRuntime::resolve_helper(thread, false, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(thread, false, false, &caller_is_c1, CHECK_NULL);\n@@ -1527,2 +1582,4 @@\n-  assert(callee_method->verified_code_entry() != NULL, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  address entry = caller_is_c1 ?\n+    callee_method->verified_inline_code_entry() : callee_method->verified_code_entry();\n+  assert(entry != NULL, \"Jump to zero!\");\n+  return entry;\n@@ -1535,0 +1592,1 @@\n+  bool caller_is_c1;\n@@ -1536,1 +1594,1 @@\n-    callee_method = SharedRuntime::resolve_helper(thread, true, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(thread, true, false, &caller_is_c1, CHECK_NULL);\n@@ -1540,2 +1598,4 @@\n-  assert(callee_method->verified_code_entry() != NULL, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  address entry = caller_is_c1 ?\n+    callee_method->verified_inline_code_entry() : callee_method->verified_inline_ro_code_entry();\n+  assert(entry != NULL, \"Jump to zero!\");\n+  return entry;\n@@ -1549,0 +1609,1 @@\n+  bool caller_is_c1;\n@@ -1550,1 +1611,1 @@\n-    callee_method = SharedRuntime::resolve_helper(thread, true, true, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(thread, true, true, &caller_is_c1, CHECK_NULL);\n@@ -1554,2 +1615,4 @@\n-  assert(callee_method->verified_code_entry() != NULL, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+  address entry = caller_is_c1 ?\n+    callee_method->verified_inline_code_entry() : callee_method->verified_code_entry();\n+  assert(entry != NULL, \"Jump to zero!\");\n+  return entry;\n@@ -1566,1 +1629,1 @@\n-                                                   bool& needs_ic_stub_refill, TRAPS) {\n+                                                   bool& needs_ic_stub_refill, bool& is_optimized, bool caller_is_c1, TRAPS) {\n@@ -1577,0 +1640,1 @@\n+    is_optimized = true;\n@@ -1614,0 +1678,1 @@\n+                                            caller_nm->is_compiled_by_c1(),\n@@ -1622,1 +1687,1 @@\n-    bool successful = inline_cache->set_to_megamorphic(&call_info, bc, needs_ic_stub_refill, CHECK_false);\n+    bool successful = inline_cache->set_to_megamorphic(&call_info, bc, needs_ic_stub_refill, caller_is_c1, CHECK_false);\n@@ -1638,1 +1703,1 @@\n-methodHandle SharedRuntime::handle_ic_miss_helper(JavaThread *thread, TRAPS) {\n+methodHandle SharedRuntime::handle_ic_miss_helper(JavaThread *thread, bool& is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1658,1 +1723,3 @@\n-    methodHandle callee_method = SharedRuntime::reresolve_call_site(thread, CHECK_(methodHandle()));\n+    bool is_static_call = false;\n+    methodHandle callee_method = SharedRuntime::reresolve_call_site(thread, is_static_call, is_optimized, caller_is_c1, CHECK_(methodHandle()));\n+    assert(!is_static_call, \"IC miss at static call?\");\n@@ -1708,0 +1775,1 @@\n+  caller_is_c1 = caller_nm->is_compiled_by_c1();\n@@ -1713,1 +1781,1 @@\n-                                                     bc, call_info, needs_ic_stub_refill, CHECK_(methodHandle()));\n+                                                     bc, call_info, needs_ic_stub_refill, is_optimized, caller_is_c1, CHECK_(methodHandle()));\n@@ -1745,1 +1813,1 @@\n-methodHandle SharedRuntime::reresolve_call_site(JavaThread *thread, TRAPS) {\n+methodHandle SharedRuntime::reresolve_call_site(JavaThread *thread, bool& is_static_call, bool& is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1761,1 +1829,1 @@\n-    bool is_static_call = false;\n+    caller_is_c1 = caller_nm->is_compiled_by_c1();\n@@ -1806,0 +1874,1 @@\n+          is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);\n@@ -1831,1 +1900,0 @@\n-\n@@ -1926,2 +1994,0 @@\n-  address entry_point = moop->from_compiled_entry_no_trampoline();\n-\n@@ -1939,1 +2005,5 @@\n-  if (cb == NULL || !cb->is_compiled() || entry_point == moop->get_c2i_entry()) {\n+  if (cb == NULL || !cb->is_compiled()) {\n+    return;\n+  }\n+  address entry_point = moop->from_compiled_entry_no_trampoline(cb->is_compiled_by_c1());\n+  if (entry_point == moop->get_c2i_entry()) {\n@@ -2311,1 +2381,1 @@\n-  static int adapter_encoding(BasicType in) {\n+  static int adapter_encoding(BasicType in, bool is_inlinetype) {\n@@ -2316,3 +2386,16 @@\n-      case T_CHAR:\n-        \/\/ There are all promoted to T_INT in the calling convention\n-        return T_INT;\n+      case T_CHAR: {\n+        if (is_inlinetype) {\n+          \/\/ Do not widen inline type field types\n+          assert(InlineTypePassFieldsAsArgs, \"must be enabled\");\n+          return in;\n+        } else {\n+          \/\/ They are all promoted to T_INT in the calling convention\n+          return T_INT;\n+        }\n+      }\n+\n+      case T_INLINE_TYPE: {\n+        \/\/ If inline types are passed as fields, return 'in' to differentiate\n+        \/\/ between a T_INLINE_TYPE and a T_OBJECT in the signature.\n+        return InlineTypePassFieldsAsArgs ? in : adapter_encoding(T_OBJECT, false);\n+      }\n@@ -2344,1 +2427,1 @@\n-  AdapterFingerPrint(int total_args_passed, BasicType* sig_bt) {\n+  AdapterFingerPrint(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n@@ -2347,0 +2430,1 @@\n+    int total_args_passed = (sig != NULL) ? sig->length() : 0;\n@@ -2364,0 +2448,2 @@\n+    BasicType prev_sbt = T_ILLEGAL;\n+    int vt_count = 0;\n@@ -2367,3 +2453,20 @@\n-        int bt = ((sig_index < total_args_passed)\n-                  ? adapter_encoding(sig_bt[sig_index++])\n-                  : 0);\n+        int bt = 0;\n+        if (sig_index < total_args_passed) {\n+          BasicType sbt = sig->at(sig_index++)._bt;\n+          if (InlineTypePassFieldsAsArgs && sbt == T_INLINE_TYPE) {\n+            \/\/ Found start of inline type in signature\n+            vt_count++;\n+            if (sig_index == 1 && has_ro_adapter) {\n+              \/\/ With a ro_adapter, replace receiver inline type delimiter by T_VOID to prevent matching\n+              \/\/ with other adapters that have the same inline type as first argument and no receiver.\n+              sbt = T_VOID;\n+            }\n+          } else if (InlineTypePassFieldsAsArgs && sbt == T_VOID &&\n+                     prev_sbt != T_LONG && prev_sbt != T_DOUBLE) {\n+            \/\/ Found end of inline type in signature\n+            vt_count--;\n+            assert(vt_count >= 0, \"invalid vt_count\");\n+          }\n+          bt = adapter_encoding(sbt, vt_count > 0);\n+          prev_sbt = sbt;\n+        }\n@@ -2375,0 +2478,1 @@\n+    assert(vt_count == 0, \"invalid vt_count\");\n@@ -2460,1 +2564,3 @@\n-  AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry, address c2i_unverified_entry, address c2i_no_clinit_check_entry) {\n+  AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry,\n+                                 address c2i_inline_entry, address c2i_inline_ro_entry,\n+                                 address c2i_unverified_entry, address c2i_unverified_inline_entry, address c2i_no_clinit_check_entry) {\n@@ -2462,1 +2568,2 @@\n-    entry->init(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+    entry->init(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry,\n+                c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -2481,1 +2588,1 @@\n-  AdapterHandlerEntry* lookup(int total_args_passed, BasicType* sig_bt) {\n+  AdapterHandlerEntry* lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n@@ -2483,1 +2590,1 @@\n-    AdapterFingerPrint fp(total_args_passed, sig_bt);\n+    AdapterFingerPrint fp(sig, has_ro_adapter);\n@@ -2579,1 +2686,1 @@\n-const int AdapterHandlerLibrary_size = 16*K;\n+const int AdapterHandlerLibrary_size = 32*K;\n@@ -2603,1 +2710,1 @@\n-  _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(0, NULL),\n+  _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),\n@@ -2605,0 +2712,1 @@\n+                                                              wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,\n@@ -2611,0 +2719,2 @@\n+                                                      address c2i_inline_entry,\n+                                                      address c2i_inline_ro_entry,\n@@ -2612,0 +2722,1 @@\n+                                                      address c2i_unverified_inline_entry,\n@@ -2613,1 +2724,16 @@\n-  return _adapters->new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+  return _adapters->new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry,\n+                              c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n+}\n+\n+static void generate_trampoline(address trampoline, address destination) {\n+  if (*(int*)trampoline == 0) {\n+    CodeBuffer buffer(trampoline, (int)SharedRuntime::trampoline_size());\n+    MacroAssembler _masm(&buffer);\n+    SharedRuntime::generate_trampoline(&_masm, destination);\n+    assert(*(int*)trampoline != 0, \"Instruction(s) for trampoline must not be encoded as zeros.\");\n+      _masm.flush();\n+\n+    if (PrintInterpreter) {\n+      Disassembler::decode(buffer.insts_begin(), buffer.insts_end());\n+    }\n+  }\n@@ -2624,7 +2750,4 @@\n-    address trampoline = method->from_compiled_entry();\n-    if (*(int*)trampoline == 0) {\n-      CodeBuffer buffer(trampoline, (int)SharedRuntime::trampoline_size());\n-      MacroAssembler _masm(&buffer);\n-      SharedRuntime::generate_trampoline(&_masm, entry->get_c2i_entry());\n-      assert(*(int*)trampoline != 0, \"Instruction(s) for trampoline must not be encoded as zeros.\");\n-      _masm.flush();\n+    generate_trampoline(method->from_compiled_entry(),           entry->get_c2i_entry());\n+    generate_trampoline(method->from_compiled_inline_ro_entry(), entry->get_c2i_inline_ro_entry());\n+    generate_trampoline(method->from_compiled_inline_entry(),    entry->get_c2i_inline_entry());\n+  }\n@@ -2632,2 +2755,31 @@\n-      if (PrintInterpreter) {\n-        Disassembler::decode(buffer.insts_begin(), buffer.insts_end());\n+  return entry;\n+}\n+\n+\n+CompiledEntrySignature::CompiledEntrySignature(Method* method) :\n+  _method(method), _num_inline_args(0), _has_inline_recv(false),\n+  _sig_cc(NULL), _sig_cc_ro(NULL), _regs(NULL), _regs_cc(NULL), _regs_cc_ro(NULL),\n+  _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),\n+  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _has_scalarized_args(false) {\n+  _sig = new GrowableArray<SigEntry>(method->size_of_parameters());\n+\n+}\n+\n+int CompiledEntrySignature::compute_scalarized_cc(GrowableArray<SigEntry>*& sig_cc, VMRegPair*& regs_cc, bool scalar_receiver) {\n+  InstanceKlass* holder = _method->method_holder();\n+  sig_cc = new GrowableArray<SigEntry>(_method->size_of_parameters());\n+  if (!_method->is_static()) {\n+    if (holder->is_inline_klass() && scalar_receiver && InlineKlass::cast(holder)->can_be_passed_as_fields()) {\n+      sig_cc->appendAll(InlineKlass::cast(holder)->extended_sig());\n+    } else {\n+      SigEntry::add_entry(sig_cc, T_OBJECT);\n+    }\n+  }\n+  Thread* THREAD = Thread::current();\n+  for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {\n+    if (ss.type() == T_INLINE_TYPE) {\n+      InlineKlass* vk = ss.as_inline_klass(holder);\n+      if (vk->can_be_passed_as_fields()) {\n+        sig_cc->appendAll(vk->extended_sig());\n+      } else {\n+        SigEntry::add_entry(sig_cc, T_OBJECT);\n@@ -2635,0 +2787,2 @@\n+    } else {\n+      SigEntry::add_entry(sig_cc, ss.type());\n@@ -2637,0 +2791,3 @@\n+  regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, sig_cc->length() + 2);\n+  return SharedRuntime::java_calling_convention(sig_cc, regs_cc);\n+}\n@@ -2638,1 +2795,103 @@\n-  return entry;\n+\/\/ See if we can save space by sharing the same entry for VIEP and VIEP(RO),\n+\/\/ or the same entry for VEP and VIEP(RO).\n+CodeOffsets::Entries CompiledEntrySignature::c1_inline_ro_entry_type() const {\n+  if (!has_scalarized_args()) {\n+    \/\/ VEP\/VIEP\/VIEP(RO) all share the same entry. There's no packing.\n+    return CodeOffsets::Verified_Entry;\n+  }\n+  if (_method->is_static()) {\n+    \/\/ Static methods don't need VIEP(RO)\n+    return CodeOffsets::Verified_Entry;\n+  }\n+\n+  if (has_inline_recv()) {\n+    if (num_inline_args() == 1) {\n+      \/\/ Share same entry for VIEP and VIEP(RO).\n+      \/\/ This is quite common: we have an instance method in an InlineKlass that has\n+      \/\/ no inline type args other than <this>.\n+      return CodeOffsets::Verified_Inline_Entry;\n+    } else {\n+      assert(num_inline_args() > 1, \"must be\");\n+      \/\/ No sharing:\n+      \/\/   VIEP(RO) -- <this> is passed as object\n+      \/\/   VEP      -- <this> is passed as fields\n+      return CodeOffsets::Verified_Inline_Entry_RO;\n+    }\n+  }\n+\n+  \/\/ Either a static method, or <this> is not an inline type\n+  if (args_on_stack_cc() != args_on_stack_cc_ro()) {\n+    \/\/ No sharing:\n+    \/\/ Some arguments are passed on the stack, and we have inserted reserved entries\n+    \/\/ into the VEP, but we never insert reserved entries into the VIEP(RO).\n+    return CodeOffsets::Verified_Inline_Entry_RO;\n+  } else {\n+    \/\/ Share same entry for VEP and VIEP(RO).\n+    return CodeOffsets::Verified_Entry;\n+  }\n+}\n+\n+\n+void CompiledEntrySignature::compute_calling_conventions() {\n+  \/\/ Get the (non-scalarized) signature and check for inline type arguments\n+  if (!_method->is_static()) {\n+    if (_method->method_holder()->is_inline_klass() && InlineKlass::cast(_method->method_holder())->can_be_passed_as_fields()) {\n+      _has_inline_recv = true;\n+      _num_inline_args++;\n+    }\n+    SigEntry::add_entry(_sig, T_OBJECT);\n+  }\n+  for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_INLINE_TYPE) {\n+      if (ss.as_inline_klass(_method->method_holder())->can_be_passed_as_fields()) {\n+        _num_inline_args++;\n+      }\n+      bt = T_OBJECT;\n+    }\n+    SigEntry::add_entry(_sig, bt);\n+  }\n+  if (_method->is_abstract() && !has_inline_arg()) {\n+    return;\n+  }\n+\n+  \/\/ Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n+\n+  \/\/ Now compute the scalarized calling convention if there are inline types in the signature\n+  _sig_cc = _sig;\n+  _sig_cc_ro = _sig;\n+  _regs_cc = _regs;\n+  _regs_cc_ro = _regs;\n+  _args_on_stack_cc = _args_on_stack;\n+  _args_on_stack_cc_ro = _args_on_stack;\n+\n+  if (has_inline_arg() && !_method->is_native()) {\n+    _args_on_stack_cc = compute_scalarized_cc(_sig_cc, _regs_cc, \/* scalar_receiver = *\/ true);\n+\n+    _sig_cc_ro = _sig_cc;\n+    _regs_cc_ro = _regs_cc;\n+    _args_on_stack_cc_ro = _args_on_stack_cc;\n+    if (_has_inline_recv || _args_on_stack_cc > _args_on_stack) {\n+      \/\/ For interface calls, we need another entry point \/ adapter to unpack the receiver\n+      _args_on_stack_cc_ro = compute_scalarized_cc(_sig_cc_ro, _regs_cc_ro, \/* scalar_receiver = *\/ false);\n+    }\n+\n+    \/\/ Upper bound on stack arguments to avoid hitting the argument limit and\n+    \/\/ bailing out of compilation (\"unsupported incoming calling sequence\").\n+    \/\/ TODO we need a reasonable limit (flag?) here\n+    if (_args_on_stack_cc > 50) {\n+      \/\/ Don't scalarize inline type arguments\n+      _sig_cc = _sig;\n+      _sig_cc_ro = _sig;\n+      _regs_cc = _regs;\n+      _regs_cc_ro = _regs;\n+      _args_on_stack_cc = _args_on_stack;\n+      _args_on_stack_cc_ro = _args_on_stack;\n+    } else {\n+      _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+      _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+      _has_scalarized_args = true;\n+    }\n+  }\n@@ -2649,1 +2908,1 @@\n-  NOT_PRODUCT(int insts_size);\n+  NOT_PRODUCT(int insts_size = 0);\n@@ -2653,0 +2912,1 @@\n+\n@@ -2658,2 +2918,4 @@\n-    if (method->is_abstract()) {\n-      return _abstract_method_handler;\n+    CompiledEntrySignature ces(method());\n+    {\n+       MutexUnlocker mul(AdapterHandlerLibrary_lock);\n+       ces.compute_calling_conventions();\n@@ -2661,0 +2923,6 @@\n+    GrowableArray<SigEntry>& sig       = ces.sig();\n+    GrowableArray<SigEntry>& sig_cc    = ces.sig_cc();\n+    GrowableArray<SigEntry>& sig_cc_ro = ces.sig_cc_ro();\n+    VMRegPair* regs         = ces.regs();\n+    VMRegPair* regs_cc      = ces.regs_cc();\n+    VMRegPair* regs_cc_ro   = ces.regs_cc_ro();\n@@ -2662,2 +2930,5 @@\n-    \/\/ Fill in the signature array, for the calling-convention call.\n-    int total_args_passed = method->size_of_parameters(); \/\/ All args on stack\n+    if (ces.has_scalarized_args()) {\n+      method->set_has_scalarized_args(true);\n+      method->set_c1_needs_stack_repair(ces.c1_needs_stack_repair());\n+      method->set_c2_needs_stack_repair(ces.c2_needs_stack_repair());\n+    }\n@@ -2665,9 +2936,15 @@\n-    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n-    VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);\n-    int i = 0;\n-    if (!method->is_static())  \/\/ Pass in receiver first\n-      sig_bt[i++] = T_OBJECT;\n-    for (SignatureStream ss(method->signature()); !ss.at_return_type(); ss.next()) {\n-      sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n-      if (ss.type() == T_LONG || ss.type() == T_DOUBLE)\n-        sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n+    if (method->is_abstract()) {\n+      if (ces.has_scalarized_args()) {\n+        \/\/ Save a C heap allocated version of the signature for abstract methods with scalarized inline type arguments\n+        address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();\n+        entry = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),\n+                                                 StubRoutines::throw_AbstractMethodError_entry(),\n+                                                 wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,\n+                                                 wrong_method_abstract, wrong_method_abstract);\n+        GrowableArray<SigEntry>* heap_sig = new (ResourceObj::C_HEAP, mtInternal) GrowableArray<SigEntry>(sig_cc_ro.length(), mtInternal);\n+        heap_sig->appendAll(&sig_cc_ro);\n+        entry->set_sig_cc(heap_sig);\n+        return entry;\n+      } else {\n+        return _abstract_method_handler;\n+      }\n@@ -2675,1 +2952,0 @@\n-    assert(i == total_args_passed, \"\");\n@@ -2678,1 +2954,1 @@\n-    entry = _adapters->lookup(total_args_passed, sig_bt);\n+    entry = _adapters->lookup(&sig_cc, regs_cc != regs_cc_ro);\n@@ -2693,4 +2969,1 @@\n-    \/\/ Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage\n-    int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed, false);\n-\n-    fingerprint = new AdapterFingerPrint(total_args_passed, sig_bt);\n+    fingerprint = new AdapterFingerPrint(&sig_cc, regs_cc != regs_cc_ro);\n@@ -2715,3 +2988,2 @@\n-                                                     total_args_passed,\n-                                                     comp_args_on_stack,\n-                                                     sig_bt,\n+                                                     ces.args_on_stack(),\n+                                                     &sig,\n@@ -2719,1 +2991,14 @@\n-                                                     fingerprint);\n+                                                     &sig_cc,\n+                                                     regs_cc,\n+                                                     &sig_cc_ro,\n+                                                     regs_cc_ro,\n+                                                     fingerprint,\n+                                                     new_adapter);\n+\n+      if (ces.has_scalarized_args()) {\n+        \/\/ Save a C heap allocated version of the scalarized signature and store it in the adapter\n+        GrowableArray<SigEntry>* heap_sig = new (ResourceObj::C_HEAP, mtInternal) GrowableArray<SigEntry>(sig_cc.length(), mtInternal);\n+        heap_sig->appendAll(&sig_cc);\n+        entry->set_sig_cc(heap_sig);\n+      }\n+\n@@ -2723,0 +3008,3 @@\n+          if (!shared_entry->compare_code(buf->code_begin(), buffer.insts_size())) {\n+            method->print();\n+          }\n@@ -2733,1 +3021,0 @@\n-      new_adapter = AdapterBlob::create(&buffer);\n@@ -2789,0 +3076,2 @@\n+  assert(base <= _c2i_inline_entry || _c2i_inline_entry == NULL, \"\");\n+  assert(base <= _c2i_inline_ro_entry || _c2i_inline_ro_entry == NULL, \"\");\n@@ -2790,0 +3079,1 @@\n+  assert(base <= _c2i_unverified_inline_entry || _c2i_unverified_inline_entry == NULL, \"\");\n@@ -2802,0 +3092,4 @@\n+  if (_c2i_inline_entry != NULL)\n+    _c2i_inline_entry += delta;\n+  if (_c2i_inline_ro_entry != NULL)\n+    _c2i_inline_ro_entry += delta;\n@@ -2804,0 +3098,2 @@\n+  if (_c2i_unverified_inline_entry != NULL)\n+    _c2i_unverified_inline_entry += delta;\n@@ -2812,0 +3108,3 @@\n+  if (_sig_cc != NULL) {\n+    delete _sig_cc;\n+  }\n@@ -2895,1 +3194,2 @@\n-        sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n+        BasicType bt = ss.type();\n+        sig_bt[i++] = bt;  \/\/ Collect remaining bits of signature\n@@ -3154,0 +3454,9 @@\n+  if (get_c2i_entry() != NULL) {\n+    st->print(\" c2iVE: \" INTPTR_FORMAT, p2i(get_c2i_inline_entry()));\n+  }\n+  if (get_c2i_entry() != NULL) {\n+    st->print(\" c2iVROE: \" INTPTR_FORMAT, p2i(get_c2i_inline_ro_entry()));\n+  }\n+  if (get_c2i_unverified_entry() != NULL) {\n+    st->print(\" c2iUE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+  }\n@@ -3155,1 +3464,1 @@\n-    st->print(\" c2iUV: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iUVE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_inline_entry()));\n@@ -3168,0 +3477,2 @@\n+  _c2i_inline_ro_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());\n+  _c2i_inline_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());\n@@ -3251,0 +3562,206 @@\n+\n+\/\/ We are at a compiled code to interpreter call. We need backing\n+\/\/ buffers for all inline type arguments. Allocate an object array to\n+\/\/ hold them (convenient because once we're done with it we don't have\n+\/\/ to worry about freeing it).\n+oop SharedRuntime::allocate_inline_types_impl(JavaThread* thread, methodHandle callee, bool allocate_receiver, TRAPS) {\n+  assert(InlineTypePassFieldsAsArgs, \"no reason to call this\");\n+  ResourceMark rm;\n+\n+  int nb_slots = 0;\n+  InstanceKlass* holder = callee->method_holder();\n+  allocate_receiver &= !callee->is_static() && holder->is_inline_klass();\n+  if (allocate_receiver) {\n+    nb_slots++;\n+  }\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    if (ss.type() == T_INLINE_TYPE) {\n+      nb_slots++;\n+    }\n+  }\n+  objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);\n+  objArrayHandle array(THREAD, array_oop);\n+  int i = 0;\n+  if (allocate_receiver) {\n+    InlineKlass* vk = InlineKlass::cast(holder);\n+    oop res = vk->allocate_instance(CHECK_NULL);\n+    array->obj_at_put(i, res);\n+    i++;\n+  }\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    if (ss.type() == T_INLINE_TYPE) {\n+      InlineKlass* vk = ss.as_inline_klass(holder);\n+      oop res = vk->allocate_instance(CHECK_NULL);\n+      array->obj_at_put(i, res);\n+      i++;\n+    }\n+  }\n+  return array();\n+}\n+\n+JRT_ENTRY(void, SharedRuntime::allocate_inline_types(JavaThread* thread, Method* callee_method, bool allocate_receiver))\n+  methodHandle callee(thread, callee_method);\n+  oop array = SharedRuntime::allocate_inline_types_impl(thread, callee, allocate_receiver, CHECK);\n+  thread->set_vm_result(array);\n+  thread->set_vm_result_2(callee()); \/\/ TODO: required to keep callee live?\n+JRT_END\n+\n+\/\/ TODO remove this once the AARCH64 dependency is gone\n+\/\/ Iterate over the array of heap allocated inline types and apply the GC post barrier to all reference fields.\n+\/\/ This is called from the C2I adapter after inline type arguments are heap allocated and initialized.\n+JRT_LEAF(void, SharedRuntime::apply_post_barriers(JavaThread* thread, objArrayOopDesc* array))\n+{\n+  assert(InlineTypePassFieldsAsArgs, \"no reason to call this\");\n+  assert(oopDesc::is_oop(array), \"should be oop\");\n+  for (int i = 0; i < array->length(); ++i) {\n+    instanceOop valueOop = (instanceOop)array->obj_at(i);\n+    InlineKlass* vk = InlineKlass::cast(valueOop->klass());\n+    if (vk->contains_oops()) {\n+      const address dst_oop_addr = ((address) (void*) valueOop);\n+      OopMapBlock* map = vk->start_of_nonstatic_oop_maps();\n+      OopMapBlock* const end = map + vk->nonstatic_oop_map_count();\n+      while (map != end) {\n+        address doop_address = dst_oop_addr + map->offset();\n+        barrier_set_cast<ModRefBarrierSet>(BarrierSet::barrier_set())->\n+          write_ref_array((HeapWord*) doop_address, map->count());\n+        map++;\n+      }\n+    }\n+  }\n+}\n+JRT_END\n+\n+\/\/ We're returning from an interpreted method: load each field into a\n+\/\/ register following the calling convention\n+JRT_LEAF(void, SharedRuntime::load_inline_type_fields_in_regs(JavaThread* thread, oopDesc* res))\n+{\n+  assert(res->klass()->is_inline_klass(), \"only inline types here\");\n+  ResourceMark rm;\n+  RegisterMap reg_map(thread);\n+  frame stubFrame = thread->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+  assert(callerFrame.is_interpreted_frame(), \"should be coming from interpreter\");\n+\n+  InlineKlass* vk = InlineKlass::cast(res->klass());\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  if (regs == NULL) {\n+    \/\/ The fields of the inline klass don't fit in registers, bail out\n+    return;\n+  }\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    address loc = reg_map.location(pair.first());\n+    switch(bt) {\n+    case T_BOOLEAN:\n+      *(jboolean*)loc = res->bool_field(off);\n+      break;\n+    case T_CHAR:\n+      *(jchar*)loc = res->char_field(off);\n+      break;\n+    case T_BYTE:\n+      *(jbyte*)loc = res->byte_field(off);\n+      break;\n+    case T_SHORT:\n+      *(jshort*)loc = res->short_field(off);\n+      break;\n+    case T_INT: {\n+      *(jint*)loc = res->int_field(off);\n+      break;\n+    }\n+    case T_LONG:\n+#ifdef _LP64\n+      *(intptr_t*)loc = res->long_field(off);\n+#else\n+      Unimplemented();\n+#endif\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY: {\n+      *(oop*)loc = res->obj_field(off);\n+      break;\n+    }\n+    case T_FLOAT:\n+      *(jfloat*)loc = res->float_field(off);\n+      break;\n+    case T_DOUBLE:\n+      *(jdouble*)loc = res->double_field(off);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+#ifdef ASSERT\n+  VMRegPair pair = regs->at(0);\n+  address loc = reg_map.location(pair.first());\n+  assert(*(oopDesc**)loc == res, \"overwritten object\");\n+#endif\n+\n+  thread->set_vm_result(res);\n+}\n+JRT_END\n+\n+\/\/ We've returned to an interpreted method, the interpreter needs a\n+\/\/ reference to an inline type instance. Allocate it and initialize it\n+\/\/ from field's values in registers.\n+JRT_BLOCK_ENTRY(void, SharedRuntime::store_inline_type_fields_to_buf(JavaThread* thread, intptr_t res))\n+{\n+  ResourceMark rm;\n+  RegisterMap reg_map(thread);\n+  frame stubFrame = thread->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+\n+#ifdef ASSERT\n+  InlineKlass* verif_vk = InlineKlass::returned_inline_klass(reg_map);\n+#endif\n+\n+  if (!is_set_nth_bit(res, 0)) {\n+    \/\/ We're not returning with inline type fields in registers (the\n+    \/\/ calling convention didn't allow it for this inline klass)\n+    assert(!Metaspace::contains((void*)res), \"should be oop or pointer in buffer area\");\n+    thread->set_vm_result((oopDesc*)res);\n+    assert(verif_vk == NULL, \"broken calling convention\");\n+    return;\n+  }\n+\n+  clear_nth_bit(res, 0);\n+  InlineKlass* vk = (InlineKlass*)res;\n+  assert(verif_vk == vk, \"broken calling convention\");\n+  assert(Metaspace::contains((void*)res), \"should be klass\");\n+\n+  \/\/ Allocate handles for every oop field so they are safe in case of\n+  \/\/ a safepoint when allocating\n+  GrowableArray<Handle> handles;\n+  vk->save_oop_fields(reg_map, handles);\n+\n+  \/\/ It's unsafe to safepoint until we are here\n+  JRT_BLOCK;\n+  {\n+    Thread* THREAD = thread;\n+    oop vt = vk->realloc_result(reg_map, handles, CHECK);\n+    thread->set_vm_result(vt);\n+  }\n+  JRT_BLOCK_END;\n+}\n+JRT_END\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":616,"deletions":99,"binary":false,"changes":715,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -1710,0 +1711,1 @@\n+  _return_buffered_value(nullptr),\n@@ -1766,1 +1768,0 @@\n-\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -457,0 +457,1 @@\n+ public:\n@@ -1029,0 +1030,1 @@\n+  friend class VTBuffer;\n@@ -1087,0 +1089,1 @@\n+  oop           _return_buffered_value; \/\/ buffered value being returned\n@@ -1539,0 +1542,3 @@\n+  oop return_buffered_value() const              { return _return_buffered_value; }\n+  void set_return_buffered_value(oop val)        { _return_buffered_value = val; }\n+\n@@ -1603,0 +1609,1 @@\n+  static ByteSize return_buffered_value_offset() { return byte_offset_of(JavaThread, _return_buffered_value); }\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -508,0 +508,4 @@\n+\n+void VM_PrintClassLayout::doit() {\n+  PrintClassLayout::print_class_layout(_out, _class_name);\n+}\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -114,0 +114,1 @@\n+  template(ClassPrintLayout)                      \\\n@@ -419,0 +420,10 @@\n+class VM_PrintClassLayout: public VM_Operation {\n+ private:\n+  outputStream* _out;\n+  char* _class_name;\n+ public:\n+  VM_PrintClassLayout(outputStream* st, char* class_name): _out(st), _class_name(class_name) {}\n+  VMOp_Type type() const { return VMOp_PrintClassHierarchy; }\n+  void doit();\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -221,1 +221,1 @@\n-  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ObjArrayKlass*)                        \\\n+  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ArrayKlass*)                        \\\n@@ -236,1 +236,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags,                                   u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags,                                   u4)                                    \\\n@@ -1580,0 +1580,1 @@\n+  declare_c2_type(CastI2NNode, TypeNode)                                  \\\n@@ -1583,0 +1584,1 @@\n+  declare_c2_type(CastN2INode, Node)                                      \\\n@@ -1623,0 +1625,1 @@\n+  declare_c2_type(MachVEPNode, MachIdealNode)                             \\\n@@ -2280,0 +2283,2 @@\n+  declare_constant(InstanceKlass::_misc_invalid_inline_super)             \\\n+  declare_constant(InstanceKlass::_misc_invalid_identity_super)           \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -423,1 +423,1 @@\n-                int ftype = DirectMethodHandle.ftypeKind(wrapper.primitiveType());\n+                int ftype = DirectMethodHandle.ftypeKind(wrapper.primitiveType(), true);\n@@ -425,1 +425,1 @@\n-                        .makePreparedFieldLambdaForm(b, \/*isVolatile*\/false, ftype);\n+                        .makePreparedFieldLambdaForm(b, false \/*isVolatile*\/, ftype);\n@@ -432,1 +432,1 @@\n-                        .makePreparedFieldLambdaForm(b, \/*isVolatile*\/true, ftype);\n+                        .makePreparedFieldLambdaForm(b, true \/*isVolatile*\/, ftype);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/GenerateJLIClassesHelper.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -399,0 +399,4 @@\n+# 0: symbol\n+compiler.err.cyclic.value.type.membership=\\\n+    cyclic inline type membership involving {0}\n+\n@@ -1775,0 +1779,3 @@\n+compiler.warn.get.class.compared.with.interface=\\\n+    return value of getClass() can never equal the class literal of an interface\n+\n@@ -2944,0 +2951,3 @@\n+compiler.misc.feature.inline.type=\\\n+    inline type\n+\n@@ -3742,0 +3752,74 @@\n+\n+# 0: name (of method)\n+compiler.err.inline.class.may.not.override=\\\n+    Inline classes may not override the method {0} from Object\n+\n+# 0: name (of method)\n+compiler.err.value.does.not.support=\\\n+    Inline types do not support {0}\n+\n+compiler.err.value.may.not.extend=\\\n+    Inline type may not extend another inline type or class\n+\n+compiler.err.value.instance.field.expected.here=\\\n+    withfield operator requires an instance field of an inline class here\n+\n+compiler.err.bad.value.based.anno=\\\n+    Unexpected @ValueBased annotation\n+\n+# 0: type\n+compiler.warn.suspicious.mix.of.null.with.value.based.class=\\\n+    Suspicious mix of null with value based class {0}\n+\n+# 0: type\n+compiler.warn.potential.null.pollution=\\\n+    Potential null pollution from nullable type {0}\n+\n+compiler.err.with.field.operator.disallowed=\\\n+    WithField operator is allowed only with -XDallowWithFieldOperator\n+\n+compiler.err.this.exposed.prematurely=\\\n+    Inine type instance should not be passed around before being fully initialized\n+\n+compiler.warn.this.exposed.prematurely=\\\n+    value based type instance should not be passed around before being fully initialized\n+\n+# 0: type\n+compiler.err.generic.parameterization.with.value.type=\\\n+    Inferred type {0} involves generic parameterization by an inline type\n+\n+# 0: type\n+compiler.err.inline.type.must.not.implement.identity.object=\\\n+    The inline type {0} attempts to implement the incompatible interface IdentityObject\n+\n+# 0: symbol, 1: type\n+compiler.err.concrete.supertype.for.inline.class=\\\n+    The concrete class {1} is not allowed to be a super class of the inline class {0} either directly or indirectly\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.method.cannot.be.synchronized=\\\n+    The method {0} in the super class {2} of the inline type {1} is synchronized. This is disallowed\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.constructor.cannot.take.arguments=\\\n+    The super class {2} of the inline type {1} defines a constructor {0} that takes arguments. This is disallowed\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.field.not.allowed=\\\n+    The super class {2} of the inline type {1} defines an instance field {0}. This is disallowed\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.no.arg.constructor.must.be.empty=\\\n+    The super class {2} of the inline type {1} defines a nonempty no-arg constructor {0}. This is disallowed\n+\n+# 0: symbol, 1: type\n+compiler.err.super.class.declares.init.block=\\\n+    The super class {1} of the inline class {0} declares one or more non-empty instance initializer blocks. This is disallowed.\n+\n+# 0: symbol, 1: type\n+compiler.err.super.class.cannot.be.inner=\\\n+    The super class {1} of the inline class {0} is an inner class. This is disallowed.\n+\n+compiler.err.projection.cant.be.instantiated=\\\n+    Illegal attempt to instantiate a projection type\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":84,"deletions":0,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -2561,1 +2561,1 @@\n-        return isKind(doctree, VALUE);\n+        return isKind(doctree, Kind.VALUE);\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/toolkit\/util\/Utils.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -65,0 +65,77 @@\n+# Valhalla\n+compiler\/arguments\/CheckCICompilerCount.java                        8205030 generic-all\n+compiler\/arguments\/CheckCompileThresholdScaling.java                8205030 generic-all\n+compiler\/codecache\/CheckSegmentedCodeCache.java                     8205030 generic-all\n+compiler\/codecache\/cli\/TestSegmentedCodeCacheOption.java            8205030 generic-all\n+compiler\/codecache\/cli\/codeheapsize\/TestCodeHeapSizeOptions.java    8205030 generic-all\n+compiler\/codecache\/cli\/printcodecache\/TestPrintCodeCacheOption.java 8205030 generic-all\n+compiler\/whitebox\/OSRFailureLevel4Test.java                         8205030 generic-all\n+\n+compiler\/aot\/cli\/DisabledAOTWithLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/SingleAOTOptionTest.java 8226295 generic-all\n+compiler\/aot\/cli\/MultipleAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileClassWithDebugTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileModuleTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/AtFileTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionWrongFileTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ClasspathOptionUnknownClassTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileDirectoryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionNotExistingTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileClassTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileJarTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/IgnoreErrorsTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileAbsoluteDirectoryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/NonExistingAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/SingleAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/IncorrectAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/RecompilationTest.java 8226295 generic-all\n+compiler\/aot\/SharedUsageTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/ClassSearchTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/SearchPathTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/module\/ModuleSourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/ClassSourceTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/directory\/DirectorySourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/jar\/JarSourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/NativeOrderOutputStreamTest.java 8226295 generic-all\n+compiler\/aot\/verification\/vmflags\/TrackedFlagTest.java 8226295 generic-all\n+compiler\/aot\/verification\/vmflags\/NotTrackedFlagTest.java 8226295 generic-all\n+compiler\/aot\/verification\/ClassAndLibraryNotMatchTest.java 8226295 generic-all\n+compiler\/aot\/DeoptimizationTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SelfChanged.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SelfChangedCDS.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SuperChanged.java 8226295 generic-all\n+\n@@ -90,0 +167,22 @@\n+# Valhalla TODO:\n+runtime\/CompressedOops\/CompressedClassPointers.java 8210258 generic-all\n+runtime\/RedefineTests\/RedefineLeak.java 8205032 generic-all\n+runtime\/SharedArchiveFile\/BootAppendTests.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/CdsDifferentCompactStrings.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/CdsDifferentObjectAlignment.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/NonBootLoaderClasses.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/PrintSharedArchiveAndExit.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedArchiveFile.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedStringsDedup.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedStringsRunAuto.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedSymbolTableBucketSize.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SpaceUtilizationCheck.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/TestInterpreterMethodEntries.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformInterfaceAndImplementor.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformSuperAndSubClasses.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformSuperSubTwoPckgs.java 8210258 generic-all\n+runtime\/appcds\/ClassLoaderTest.java 8210258 generic-all\n+runtime\/appcds\/HelloTest.java 8210258 generic-all\n+runtime\/appcds\/sharedStrings\/SharedStringsBasic.java 8210258 generic-all\n+\n+\n@@ -101,0 +200,26 @@\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":125,"deletions":0,"binary":false,"changes":125,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-  runtime\n+  runtime \\\n@@ -58,0 +58,7 @@\n+hotspot_valhalla = \\\n+  runtime\/valhalla \\\n+  compiler\/valhalla\n+\n+hotspot_valhalla_runtime = \\\n+  runtime\/valhalla\n+\n@@ -95,1 +102,1 @@\n-  compiler\/codegen\/aes \\\n+  compiler\/codegen\/aes \\\n@@ -144,0 +151,1 @@\n+  compiler\/valhalla\/ \\\n@@ -161,0 +169,7 @@\n+tier1_compiler_no_valhalla = \\\n+  :tier1_compiler_1 \\\n+  :tier1_compiler_2 \\\n+  :tier1_compiler_3 \\\n+  :tier1_compiler_not_xcomp \\\n+  -compiler\/valhalla\n+\n@@ -315,0 +330,4 @@\n+tier1_runtime_no_valhalla = \\\n+  :tier1_runtime \\\n+  -runtime\/valhalla\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":21,"deletions":2,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -330,0 +330,1 @@\n+        vmOptFinalFlag(map, \"TieredCompilation\");\n@@ -378,0 +379,3 @@\n+        if (WB.getBooleanVMFlag(\"EnableValhalla\").booleanValue()) {\n+            return \"false\";\n+        }\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -164,1 +164,0 @@\n-compiler.err.feature.not.supported.in.source                  # Generated for using diamond before source 7\n@@ -205,0 +204,17 @@\n+\n+# Value types\n+compiler.err.cyclic.value.type.membership\n+compiler.err.value.does.not.support\n+compiler.err.value.may.not.extend\n+compiler.warn.potential.null.pollution\n+compiler.err.this.exposed.prematurely\n+compiler.warn.this.exposed.prematurely\n+compiler.err.inline.type.must.not.implement.identity.object\n+compiler.err.concrete.supertype.for.inline.class\n+compiler.err.super.class.cannot.be.inner\n+compiler.err.super.class.declares.init.block\n+compiler.err.super.constructor.cannot.take.arguments\n+compiler.err.super.field.not.allowed\n+compiler.err.super.method.cannot.be.synchronized\n+compiler.err.super.no.arg.constructor.must.be.empty\n+compiler.err.generic.parameterization.with.value.type\n","filename":"test\/langtools\/tools\/javac\/diags\/examples.not-yet.txt","additions":17,"deletions":1,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -151,0 +151,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n","filename":"test\/lib\/sun\/hotspot\/WhiteBox.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"}]}