{"files":[{"patch":"@@ -7,0 +7,3 @@\n+      - lworld\n+      - type-restrictions\n+      - jep390\n@@ -12,1 +15,1 @@\n-        default: \"Linux x64, Linux x86, Windows x64, macOS x64\"\n+        default: \"Linux x64, Windows x64, macOS x64\"\n@@ -21,1 +24,0 @@\n-      platform_linux_x86: ${{ steps.check_platforms.outputs.platform_linux_x86 }}\n@@ -36,1 +38,0 @@\n-          echo \"::set-output name=platform_linux_x86::${{ contains(github.event.inputs.platforms, 'linux x86') || (github.event.inputs.platforms == '' && (secrets.JDK_SUBMIT_PLATFORMS == '' || contains(secrets.JDK_SUBMIT_PLATFORMS, 'linux x86'))) }}\"\n@@ -112,1 +113,0 @@\n-          - build hotspot zero\n@@ -121,3 +121,0 @@\n-          - flavor: build hotspot zero\n-            flags: --enable-debug --disable-precompiled-headers --with-jvm-variants=zero\n-            build-target: hotspot\n@@ -389,287 +386,0 @@\n-  linux_x86_build:\n-    name: Linux x86\n-    runs-on: \"ubuntu-latest\"\n-    needs: prerequisites\n-    if: needs.prerequisites.outputs.should_run != 'false' && needs.prerequisites.outputs.platform_linux_x86 != 'false'\n-\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        flavor:\n-          - build release\n-          - build debug\n-        include:\n-          - flavor: build debug\n-            flags: --enable-debug\n-            artifact: -debug\n-\n-    # Reduced 32-bit build uses the same boot JDK as 64-bit build\n-    env:\n-      JDK_VERSION: \"${{ fromJson(needs.prerequisites.outputs.dependencies).DEFAULT_VERSION_FEATURE }}\"\n-      BOOT_JDK_VERSION: \"${{ fromJson(needs.prerequisites.outputs.dependencies).BOOT_JDK_VERSION }}\"\n-      BOOT_JDK_FILENAME: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_FILENAME }}\"\n-      BOOT_JDK_URL: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_URL }}\"\n-      BOOT_JDK_SHA256: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_SHA256 }}\"\n-\n-    steps:\n-      - name: Checkout the source\n-        uses: actions\/checkout@v2\n-        with:\n-          path: jdk\n-\n-      - name: Restore boot JDK from cache\n-        id: bootjdk\n-        uses: actions\/cache@v2\n-        with:\n-          path: ~\/bootjdk\/${{ env.BOOT_JDK_VERSION }}\n-          key: bootjdk-${{ runner.os }}-${{ env.BOOT_JDK_VERSION }}-${{ env.BOOT_JDK_SHA256 }}-v1\n-\n-      - name: Download boot JDK\n-        run: |\n-          mkdir -p \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\"\n-          wget -O \"${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" \"${BOOT_JDK_URL}\"\n-          echo \"${BOOT_JDK_SHA256} ${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" | sha256sum -c >\/dev\/null -\n-          tar -xf \"${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" -C \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\"\n-          mv \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\/\"*\/* \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\/\"\n-        if: steps.bootjdk.outputs.cache-hit != 'true'\n-\n-      - name: Restore jtreg artifact\n-        id: jtreg_restore\n-        uses: actions\/download-artifact@v2\n-        with:\n-          name: transient_jtreg_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: ~\/jtreg\/\n-        continue-on-error: true\n-\n-      - name: Restore jtreg artifact (retry)\n-        uses: actions\/download-artifact@v2\n-        with:\n-          name: transient_jtreg_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: ~\/jtreg\/\n-        if: steps.jtreg_restore.outcome == 'failure'\n-\n-      - name: Checkout gtest sources\n-        uses: actions\/checkout@v2\n-        with:\n-          repository: \"google\/googletest\"\n-          ref: \"release-${{ fromJson(needs.prerequisites.outputs.dependencies).GTEST_VERSION }}\"\n-          path: gtest\n-\n-      # Roll in the multilib environment and its dependencies.\n-      # Some multilib libraries do not have proper inter-dependencies, so we have to\n-      # install their dependencies manually.\n-      - name: Install dependencies\n-        run: |\n-          sudo dpkg --add-architecture i386\n-          sudo apt-get update\n-          sudo apt-get install gcc-multilib g++-multilib libfreetype6-dev:i386 libxrandr-dev:i386 libxtst-dev:i386 libtiff-dev:i386 libcupsimage2-dev:i386 libcups2-dev:i386 libasound2-dev:i386\n-\n-      - name: Configure\n-        run: >\n-          bash configure\n-          --with-conf-name=linux-x86\n-          --with-target-bits=32\n-          ${{ matrix.flags }}\n-          --with-version-opt=${GITHUB_ACTOR}-${GITHUB_SHA}\n-          --with-version-build=0\n-          --with-boot-jdk=${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\n-          --with-jtreg=${HOME}\/jtreg\n-          --with-gtest=${GITHUB_WORKSPACE}\/gtest\n-          --with-default-make-target=\"product-bundles test-bundles\"\n-          --with-zlib=system\n-          --enable-jtreg-failure-handler\n-        working-directory: jdk\n-\n-      - name: Build\n-        run: make CONF_NAME=linux-x86 ${{ matrix.build-target }}\n-        working-directory: jdk\n-\n-      - name: Persist test bundles\n-        uses: actions\/upload-artifact@v2\n-        with:\n-          name: transient_jdk-linux-x86${{ matrix.artifact }}_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: |\n-            jdk\/build\/linux-x86\/bundles\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin${{ matrix.artifact }}.tar.gz\n-            jdk\/build\/linux-x86\/bundles\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin-tests${{ matrix.artifact }}.tar.gz\n-        if: matrix.build-target == false\n-\n-  linux_x86_test:\n-    name: Linux x86\n-    runs-on: \"ubuntu-latest\"\n-    needs:\n-      - prerequisites\n-      - linux_x86_build\n-\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        test:\n-          - jdk\/tier1 part 1\n-          - jdk\/tier1 part 2\n-          - jdk\/tier1 part 3\n-          - langtools\/tier1\n-          - hs\/tier1 common\n-          - hs\/tier1 compiler\n-          - hs\/tier1 gc\n-          - hs\/tier1 runtime\n-          - hs\/tier1 serviceability\n-        include:\n-          - test: jdk\/tier1 part 1\n-            suites: test\/jdk\/:tier1_part1\n-          - test: jdk\/tier1 part 2\n-            suites: test\/jdk\/:tier1_part2\n-          - test: jdk\/tier1 part 3\n-            suites: test\/jdk\/:tier1_part3\n-          - test: langtools\/tier1\n-            suites: test\/langtools\/:tier1\n-          - test: hs\/tier1 common\n-            suites: test\/hotspot\/jtreg\/:tier1_common\n-            artifact: -debug\n-          - test: hs\/tier1 compiler\n-            suites: test\/hotspot\/jtreg\/:tier1_compiler\n-            artifact: -debug\n-          - test: hs\/tier1 gc\n-            suites: test\/hotspot\/jtreg\/:tier1_gc\n-            artifact: -debug\n-          - test: hs\/tier1 runtime\n-            suites: test\/hotspot\/jtreg\/:tier1_runtime\n-            artifact: -debug\n-          - test: hs\/tier1 serviceability\n-            suites: test\/hotspot\/jtreg\/:tier1_serviceability\n-            artifact: -debug\n-\n-    # Reduced 32-bit build uses the same boot JDK as 64-bit build\n-    env:\n-      JDK_VERSION: \"${{ fromJson(needs.prerequisites.outputs.dependencies).DEFAULT_VERSION_FEATURE }}\"\n-      BOOT_JDK_VERSION: \"${{ fromJson(needs.prerequisites.outputs.dependencies).BOOT_JDK_VERSION }}\"\n-      BOOT_JDK_FILENAME: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_FILENAME }}\"\n-      BOOT_JDK_URL: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_URL }}\"\n-      BOOT_JDK_SHA256: \"${{ fromJson(needs.prerequisites.outputs.dependencies).LINUX_X64_BOOT_JDK_SHA256 }}\"\n-\n-    steps:\n-      - name: Checkout the source\n-        uses: actions\/checkout@v2\n-\n-      - name: Restore boot JDK from cache\n-        id: bootjdk\n-        uses: actions\/cache@v2\n-        with:\n-          path: ~\/bootjdk\/${{ env.BOOT_JDK_VERSION }}\n-          key: bootjdk-${{ runner.os }}-${{ env.BOOT_JDK_VERSION }}-${{ env.BOOT_JDK_SHA256 }}-v1\n-\n-      - name: Download boot JDK\n-        run: |\n-          mkdir -p \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\"\n-          wget -O \"${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" \"${BOOT_JDK_URL}\"\n-          echo \"${BOOT_JDK_SHA256} ${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" | sha256sum -c >\/dev\/null -\n-          tar -xf \"${HOME}\/bootjdk\/${BOOT_JDK_FILENAME}\" -C \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\"\n-          mv \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\/\"*\/* \"${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\/\"\n-        if: steps.bootjdk.outputs.cache-hit != 'true'\n-\n-      - name: Restore jtreg artifact\n-        id: jtreg_restore\n-        uses: actions\/download-artifact@v2\n-        with:\n-          name: transient_jtreg_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: ~\/jtreg\/\n-        continue-on-error: true\n-\n-      - name: Restore jtreg artifact (retry)\n-        uses: actions\/download-artifact@v2\n-        with:\n-          name: transient_jtreg_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: ~\/jtreg\/\n-        if: steps.jtreg_restore.outcome == 'failure'\n-\n-      - name: Restore build artifacts\n-        id: build_restore\n-        uses: actions\/download-artifact@v2\n-        with:\n-          name: transient_jdk-linux-x86${{ matrix.artifact }}_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: ~\/jdk-linux-x86${{ matrix.artifact }}\n-        continue-on-error: true\n-\n-      - name: Restore build artifacts (retry)\n-        uses: actions\/download-artifact@v2\n-        with:\n-          name: transient_jdk-linux-x86${{ matrix.artifact }}_${{ needs.prerequisites.outputs.bundle_id }}\n-          path: ~\/jdk-linux-x86${{ matrix.artifact }}\n-        if: steps.build_restore.outcome == 'failure'\n-\n-      - name: Unpack jdk\n-        run: |\n-          mkdir -p \"${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin${{ matrix.artifact }}\"\n-          tar -xf \"${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin${{ matrix.artifact }}.tar.gz\" -C \"${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin${{ matrix.artifact }}\"\n-\n-      - name: Unpack tests\n-        run: |\n-          mkdir -p \"${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin-tests${{ matrix.artifact }}\"\n-          tar -xf \"${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin-tests${{ matrix.artifact }}.tar.gz\" -C \"${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin-tests${{ matrix.artifact }}\"\n-\n-      - name: Find root of jdk image dir\n-        run: |\n-          imageroot=`find ${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin${{ matrix.artifact }} -name release -type f`\n-          echo \"imageroot=`dirname ${imageroot}`\" >> $GITHUB_ENV\n-\n-      - name: Run tests\n-        run: >\n-          JDK_IMAGE_DIR=${{ env.imageroot }}\n-          TEST_IMAGE_DIR=${HOME}\/jdk-linux-x86${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_linux-x86_bin-tests${{ matrix.artifact }}\n-          BOOT_JDK=${HOME}\/bootjdk\/${BOOT_JDK_VERSION}\n-          JT_HOME=${HOME}\/jtreg\n-          make test-prebuilt\n-          CONF_NAME=run-test-prebuilt\n-          LOG_CMDLINES=true\n-          JTREG_VERBOSE=fail,error,time\n-          TEST=\"${{ matrix.suites }}\"\n-          TEST_OPTS_JAVA_OPTIONS=\n-          JTREG_KEYWORDS=\"!headful\"\n-          JTREG=\"JAVA_OPTIONS=-XX:-CreateCoredumpOnCrash\"\n-\n-      - name: Check that all tests executed successfully\n-        if: always()\n-        run: >\n-          if ! grep --include=test-summary.txt -lqr build\/*\/test-results -e \"TEST SUCCESS\" ; then\n-            cat build\/*\/test-results\/*\/text\/newfailures.txt ;\n-            exit 1 ;\n-          fi\n-\n-      - name: Create suitable test log artifact name\n-        if: always()\n-        run: echo \"logsuffix=`echo ${{ matrix.test }} | sed -e 's!\/!_!'g -e 's! !_!'g`\" >> $GITHUB_ENV\n-\n-      - name: Package test results\n-        if: always()\n-        working-directory: build\/run-test-prebuilt\/test-results\/\n-        run: >\n-          zip -r9\n-          \"$HOME\/linux-x86${{ matrix.artifact }}_testresults_${{ env.logsuffix }}.zip\"\n-          .\n-        continue-on-error: true\n-\n-      - name: Package test support\n-        if: always()\n-        working-directory: build\/run-test-prebuilt\/test-support\/\n-        run: >\n-          zip -r9\n-          \"$HOME\/linux-x86${{ matrix.artifact }}_testsupport_${{ env.logsuffix }}.zip\"\n-          .\n-          -i *.jtr\n-          -i *\/hs_err*.log\n-          -i *\/replay*.log\n-        continue-on-error: true\n-\n-      - name: Persist test results\n-        if: always()\n-        uses: actions\/upload-artifact@v2\n-        with:\n-          path: ~\/linux-x86${{ matrix.artifact }}_testresults_${{ env.logsuffix }}.zip\n-        continue-on-error: true\n-\n-      - name: Persist test outputs\n-        if: always()\n-        uses: actions\/upload-artifact@v2\n-        with:\n-          path: ~\/linux-x86${{ matrix.artifact }}_testsupport_${{ env.logsuffix }}.zip\n-        continue-on-error: true\n-\n@@ -772,0 +482,1 @@\n+          --disable-precompiled-headers\n@@ -1282,1 +993,0 @@\n-      - linux_x86_test\n","filename":".github\/workflows\/submit.yml","additions":5,"deletions":295,"binary":false,"changes":300,"status":"modified"},{"patch":"@@ -1358,0 +1358,1 @@\n+        args = concat(args, \"--with-version-pre=\" + version_numbers.get(\"DEFAULT_PROMOTED_VERSION_PRE\"));\n","filename":"make\/conf\/jib-profiles.js","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -588,0 +588,1 @@\n+    case T_INLINE_TYPE :\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -1304,1 +1305,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1334,1 +1339,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1427,0 +1436,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -1476,0 +1489,33 @@\n+void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label& is_value) {\n+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  andr(temp_reg, temp_reg, JVM_ACC_INLINE);\n+  cbnz(temp_reg, is_value);\n+}\n+\n+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_inline_field_shift, is_inline);\n+}\n+\n+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbz(flags, ConstantPoolCacheEntry::is_inline_field_shift, not_inline);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label& is_flattened_array) {\n+  load_storage_props(temp_reg, oop);\n+  andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);\n+  cbnz(temp_reg, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  load_storage_props(temp_reg, oop);\n+  andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);\n+  cbnz(temp_reg, is_null_free_array);\n+}\n+\n@@ -3780,1 +3826,1 @@\n-void MacroAssembler::load_klass(Register dst, Register src) {\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n@@ -3783,1 +3829,0 @@\n-    decode_klass_not_null(dst);\n@@ -3789,0 +3834,10 @@\n+void MacroAssembler::load_klass(Register dst, Register src) {\n+  load_metadata(dst, src);\n+  if (UseCompressedClassPointers) {\n+    andr(dst, dst, oopDesc::compressed_klass_mask());\n+    decode_klass_not_null(dst);\n+  } else {\n+    ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);\n+  }\n+}\n+\n@@ -3820,0 +3875,9 @@\n+void MacroAssembler::load_storage_props(Register dst, Register src) {\n+  load_metadata(dst, src);\n+  if (UseCompressedClassPointers) {\n+    asrw(dst, dst, oopDesc::narrow_storage_props_shift);\n+  } else {\n+    asr(dst, dst, oopDesc::wide_storage_props_shift);\n+  }\n+}\n+\n@@ -4157,1 +4221,2 @@\n-                                     Register tmp1, Register thread_tmp) {\n+                                     Register tmp1, Register thread_tmp, Register tmp3) {\n+\n@@ -4162,1 +4227,1 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4164,1 +4229,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4188,2 +4253,2 @@\n-                                    Register thread_tmp, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);\n+                                    Register thread_tmp, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4194,1 +4259,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -5293,0 +5358,339 @@\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+\n+\/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->frame_size_in_bytes();\n+  assert(framesize % (2 * wordSize) == 0, \"must preserve 2 * wordSize alignment\");\n+\n+  \/\/ insert a nop at the start of the prolog so we can patch in a\n+  \/\/ branch if we need to invalidate the method later\n+  nop();\n+\n+  int bangsize = C->bang_size_in_bytes();\n+  if (C->need_stack_bang(bangsize) && UseStackBanging)\n+     generate_stack_overflow_check(bangsize);\n+\n+  build_frame(framesize);\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  cmp(r0, (u1) 1);\n+  br(Assembler::EQ, skip);\n+  int call_offset = -1;\n+\n+  Label slow_case;\n+\n+  \/\/ Try to allocate a new buffered inline type (from the heap)\n+  if (UseTLAB) {\n+\n+    if (vk != NULL) {\n+      \/\/ Called from C1, where the return type is statically known.\n+      mov(r1, (intptr_t)vk->get_InlineKlass());\n+      jint lh = vk->layout_helper();\n+      assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+      mov(r14, lh);\n+    } else {\n+       \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+       andr(r1, r0, -2);\n+       \/\/ get obj size\n+       ldrw(r14, Address(rscratch1 \/*klass*\/, Klass::layout_helper_offset()));\n+    }\n+\n+     ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+\n+     \/\/ check whether we have space in TLAB,\n+     \/\/ rscratch1 contains pointer to just allocated obj\n+      lea(r14, Address(r13, r14));\n+      ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));\n+\n+      cmp(r14, rscratch1);\n+      br(Assembler::GT, slow_case);\n+\n+      \/\/ OK we have room in TLAB,\n+      \/\/ Set new TLAB top\n+      str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+\n+      \/\/ Set new class always locked\n+      mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());\n+      str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));\n+\n+      store_klass_gap(r13, zr);  \/\/ zero klass gap for compressed oops\n+      if (vk == NULL) {\n+        \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n+         mov(r0, r1);\n+      }\n+\n+      store_klass(r13, r1);  \/\/ klass\n+\n+      if (vk != NULL) {\n+        \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+        mov(r0, r13);\n+        far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+      } else {\n+\n+        \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+        ldr(r1, Address(r0, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+        ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));\n+\n+        \/\/ Mov new class to r0 and call pack_handler\n+        mov(r0, r13);\n+        blr(r1);\n+      }\n+      b(skip);\n+  }\n+\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    ldr(rscratch1, RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    blr(rscratch1);\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        mov(to->as_Register(), from->as_Register());\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,\n+                                          int& to_index, RegState reg_state[]) {\n+  Register fromReg = from->is_reg() ? from->as_Register() : noreg;\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+\n+\n+  int vt = 1;\n+  bool done = true;\n+  bool mark_done = true;\n+  do {\n+    sig_index--;\n+    BasicType bt = sig->at(sig_index)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      vt--;\n+    } else if (bt == T_VOID &&\n+               sig->at(sig_index-1)._bt != T_LONG &&\n+               sig->at(sig_index-1)._bt != T_DOUBLE) {\n+      vt++;\n+    } else {\n+      assert(to_index >= 0, \"invalid to_index\");\n+      VMRegPair pair_to = regs_to[to_index--];\n+      VMReg to = pair_to.first();\n+\n+      if (bt == T_VOID) continue;\n+\n+      int idx = (int) to->value();\n+      if (reg_state[idx] == reg_readonly) {\n+         if (idx != from->value()) {\n+           mark_done = false;\n+         }\n+         done = false;\n+         continue;\n+      } else if (reg_state[idx] == reg_written) {\n+        continue;\n+      } else {\n+        assert(reg_state[idx] == reg_writable, \"must be writable\");\n+        reg_state[idx] = reg_written;\n+      }\n+\n+      if (fromReg == noreg) {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        ldr(rscratch2, Address(sp, st_off));\n+        fromReg = rscratch2;\n+      }\n+\n+      int off = sig->at(sig_index)._offset;\n+      assert(off > 0, \"offset in object should be positive\");\n+      bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+\n+      Address fromAddr = Address(fromReg, off);\n+      bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+\n+      if (!to->is_FloatRegister()) {\n+\n+        Register dst = to->is_stack() ? rscratch1 : to->as_Register();\n+\n+        if (is_oop) {\n+          load_heap_oop(dst, fromAddr);\n+        } else {\n+          load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+        }\n+        if (to->is_stack()) {\n+          int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+          str(dst, Address(sp, st_off));\n+        }\n+      } else {\n+        if (bt == T_DOUBLE) {\n+          ldrd(to->as_FloatRegister(), fromAddr);\n+        } else {\n+          assert(bt == T_FLOAT, \"must be float\");\n+          ldrs(to->as_FloatRegister(), fromAddr);\n+        }\n+     }\n+\n+    }\n+\n+  } while (vt != 0);\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"must be\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  Register val_array = r0;\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r10;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r1;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);\n+  VMRegPair from_pair;\n+  BasicType bt;\n+\n+  while (stream.next(from_pair, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    VMReg from_r1 = from_pair.first();\n+    VMReg from_r2 = from_pair.second();\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+\n+    if (!from_r1->is_FloatRegister()) {\n+      Register from_reg;\n+      if (from_r1->is_stack()) {\n+        from_reg = from_reg_tmp;\n+        int ld_off = from_r1->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        from_reg = from_r1->as_Register();\n+      }\n+\n+      if (is_oop) {\n+        DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;\n+        store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);\n+      } else {\n+        store_sized_value(dst, from_reg, size_in_bytes);\n+      }\n+    } else {\n+      if (from_r2->is_valid()) {\n+        strd(from_r1->as_FloatRegister(), dst);\n+      } else {\n+        strs(from_r1->as_FloatRegister(), dst);\n+      }\n+    }\n+\n+    reg_state[from_r1->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v0->as_VMReg() : r14->as_VMReg();\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":414,"deletions":10,"binary":false,"changes":424,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -322,0 +323,1 @@\n+    case T_INLINE_TYPE:\n@@ -355,0 +357,84 @@\n+\n+\/\/ const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_int = 6;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt, VMRegPair *regs, int total_args_passed) {\n+\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  \/\/ r1, r2 used to address klasses and states, exclude it from return convention to avoid colision\n+\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+     r0 \/* j_rarg7 *\/, j_rarg6, j_rarg5, j_rarg4, j_rarg3, j_rarg2\n+  };\n+\n+  static const FloatRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3, j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        \/\/ Should we have gurantee here?\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+      \/\/ Should T_METADATA be added to java_calling_convention as well ?\n+    case T_METADATA:\n+    case T_INLINE_TYPE:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -385,19 +471,40 @@\n-static void gen_c2i_adapter(MacroAssembler *masm,\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n-                            const VMRegPair *regs,\n-                            Label& skip_fixup) {\n-  \/\/ Before we get into the guts of the C2I adapter, see if we should be here\n-  \/\/ at all.  We've come from compiled code and are attempting to jump to the\n-  \/\/ interpreter, which means the caller made a static call to get here\n-  \/\/ (vcalls always get a compiled target if there is one).  Check for a\n-  \/\/ compiled target.  If there is one, we need to patch the caller's call.\n-  patch_callers_callsite(masm);\n-\n-  __ bind(skip_fixup);\n-\n-  int words_pushed = 0;\n-\n-  \/\/ Since all args are passed on the stack, total_args_passed *\n-  \/\/ Interpreter::stackElementSize is the space we need.\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+     for (int i = 0; i < sig_extended->length(); i++) {\n+       BasicType bt = sig_extended->at(i)._bt;\n+       if (bt == T_INLINE_TYPE) {\n+         \/\/ In sig_extended, an inline type argument starts with:\n+         \/\/ T_INLINE_TYPE, followed by the types of the fields of the\n+         \/\/ inline type and T_VOID to mark the end of the value\n+         \/\/ type. Inline types are flattened so, for instance, in the\n+         \/\/ case of an inline type with an int field and an inline type\n+         \/\/ field that itself has 2 fields, an int and a long:\n+         \/\/ T_INLINE_TYPE T_INT T_INLINE_TYPE T_INT T_LONG T_VOID (second\n+         \/\/ slot for the T_LONG) T_VOID (inner T_INLINE_TYPE) T_VOID\n+         \/\/ (outer T_INLINE_TYPE)\n+         total_args_passed++;\n+         int vt = 1;\n+         do {\n+           i++;\n+           BasicType bt = sig_extended->at(i)._bt;\n+           BasicType prev_bt = sig_extended->at(i-1)._bt;\n+           if (bt == T_INLINE_TYPE) {\n+             vt++;\n+           } else if (bt == T_VOID &&\n+                      prev_bt != T_LONG &&\n+                      prev_bt != T_DOUBLE) {\n+             vt--;\n+           }\n+         } while (vt != 0);\n+       } else {\n+         total_args_passed++;\n+       }\n+     }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n@@ -405,1 +512,2 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+  return total_args_passed;\n+}\n@@ -407,3 +515,1 @@\n-  __ mov(r13, sp);\n-  \/\/ stack is aligned, keep it that way\n-  extraspace = align_up(extraspace, 2*wordSize);\n+static void gen_c2i_adapter_helper(MacroAssembler* masm, BasicType bt, const VMRegPair& reg_pair, int extraspace, const Address& to) {\n@@ -412,13 +518,1 @@\n-  if (extraspace)\n-    __ sub(sp, sp, extraspace);\n-\n-  \/\/ Now write the args into the outgoing interpreter space\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n+    assert(bt != T_INLINE_TYPE || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n@@ -439,2 +533,5 @@\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n+    \/\/ int next_off = st_off - Interpreter::stackElementSize;\n+\n+    VMReg r_1 = reg_pair.first();\n+    VMReg r_2 = reg_pair.second();\n+\n@@ -443,1 +540,1 @@\n-      continue;\n+      return;\n@@ -445,0 +542,1 @@\n+\n@@ -447,3 +545,2 @@\n-      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size\n-                    + extraspace\n-                    + words_pushed * wordSize);\n+      \/\/ words_pushed is always 0 so we don't use it.\n+      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace \/* + word_pushed * wordSize *\/);\n@@ -453,1 +550,1 @@\n-        __ str(rscratch1, Address(sp, st_off));\n+        __ str(rscratch1, to);\n@@ -456,16 +553,1 @@\n-\n-\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ str(rscratch1, Address(sp, next_off));\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaaaull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ str(rscratch1, Address(sp, st_off));\n-        }\n+        __ str(rscratch1, to);\n@@ -476,19 +558,1 @@\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ str(r, Address(sp, st_off));\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ jlong\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaabull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-          __ str(r, Address(sp, next_off));\n-        } else {\n-          __ str(r, Address(sp, st_off));\n-        }\n-      }\n+      __ str(r, to);\n@@ -499,1 +563,1 @@\n-        __ strs(r_1->as_FloatRegister(), Address(sp, st_off));\n+        __ strs(r_1->as_FloatRegister(), to);\n@@ -501,6 +565,1 @@\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaacull);\n-        __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        __ strd(r_1->as_FloatRegister(), Address(sp, next_off));\n+        __ strd(r_1->as_FloatRegister(), to);\n@@ -508,0 +567,153 @@\n+   }\n+}\n+\n+static void gen_c2i_adapter(MacroAssembler *masm,\n+                            const GrowableArray<SigEntry>* sig_extended,\n+                            const VMRegPair *regs,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+\n+  \/\/ Before we get into the guts of the C2I adapter, see if we should be here\n+  \/\/ at all.  We've come from compiled code and are attempting to jump to the\n+  \/\/ interpreter, which means the caller made a static call to get here\n+  \/\/ (vcalls always get a compiled target if there is one).  Check for a\n+  \/\/ compiled target.  If there is one, we need to patch the caller's call.\n+  patch_callers_callsite(masm);\n+\n+  __ bind(skip_fixup);\n+\n+  bool has_inline_argument = false;\n+\n+  if (InlineTypePassFieldsAsArgs) {\n+      \/\/ Is there an inline type argument?\n+     for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+       has_inline_argument = (sig_extended->at(i)._bt == T_INLINE_TYPE);\n+     }\n+     if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);\n+\n+      frame_complete = __ offset();\n+      address the_pc = __ pc();\n+\n+      __ set_last_Java_frame(noreg, noreg, the_pc, rscratch1);\n+\n+      __ mov(c_rarg0, rthread);\n+      __ mov(c_rarg1, r1);\n+      __ mov(c_rarg2, (int64_t)alloc_inline_receiver);\n+\n+      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+      __ blr(rscratch1);\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ cbz(r0, no_exception);\n+\n+      __ str(zr, Address(rthread, JavaThread::vm_result_offset()));\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ b(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(r10, rthread);\n+      __ get_vm_result_2(r1, rthread); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n+  int words_pushed = 0;\n+\n+  \/\/ Since all args are passed on the stack, total_args_passed *\n+  \/\/ Interpreter::stackElementSize is the space we need.\n+\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n+  int extraspace = (total_args_passed * Interpreter::stackElementSize) + wordSize;\n+\n+  \/\/ stack is aligned, keep it that way\n+  extraspace = align_up(extraspace, 2 * wordSize);\n+\n+  __ mov(r13, sp);\n+\n+  if (extraspace)\n+    __ sub(sp, sp, extraspace);\n+\n+  \/\/ Now write the args into the outgoing interpreter space\n+\n+  int ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+  bool has_oop_field = false;\n+\n+  for (int next_arg_comp = 0; next_arg_comp < total_args_passed; next_arg_comp++) {\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    \/\/ offset to start parameters\n+    int st_off   = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;\n+\n+    if (!InlineTypePassFieldsAsArgs || bt != T_INLINE_TYPE) {\n+      if (bt == T_VOID) {\n+         assert(next_arg_comp > 0 && (sig_extended->at(next_arg_comp - 1)._bt == T_LONG || sig_extended->at(next_arg_comp - 1)._bt == T_DOUBLE), \"missing half\");\n+         next_arg_int ++;\n+         continue;\n+       }\n+\n+       int next_off = st_off - Interpreter::stackElementSize;\n+       int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+\n+       gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp], extraspace, Address(sp, offset));\n+       next_arg_int ++;\n+   } else {\n+       ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_INLINE_TYPE);\n+      __ load_heap_oop(rscratch1, Address(r10, index));\n+      next_vt_arg++;\n+      next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of value\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp - 1)._bt;\n+        if (bt == T_INLINE_TYPE) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n+        } else {\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          assert(off > 0, \"offset in object should be positive\");\n+\n+          bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+          has_oop_field = has_oop_field || is_oop;\n+\n+          gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp - ignored], extraspace, Address(r11, off));\n+        }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ str(rscratch1, Address(sp, st_off));\n+   }\n+\n+  }\n+\n+\/\/ If an inline type was allocated and initialized, apply post barrier to all oop fields\n+  if (has_inline_argument && has_oop_field) {\n+    __ push(r13); \/\/ save senderSP\n+    __ push(r1); \/\/ save callee\n+    \/\/ Allocate argument register save area\n+    if (frame::arg_reg_save_area_bytes != 0) {\n+      __ sub(sp, sp, frame::arg_reg_save_area_bytes);\n@@ -509,0 +721,7 @@\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::apply_post_barriers), rthread, r10);\n+    \/\/ De-allocate argument register save area\n+    if (frame::arg_reg_save_area_bytes != 0) {\n+      __ add(sp, sp, frame::arg_reg_save_area_bytes);\n+    }\n+    __ pop(r1); \/\/ restore callee\n+    __ pop(r13); \/\/ restore sender SP\n@@ -517,0 +736,1 @@\n+void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm, int comp_args_on_stack, const GrowableArray<SigEntry>* sig, const VMRegPair *regs) {\n@@ -518,5 +738,0 @@\n-void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,\n-                                    int total_args_passed,\n-                                    int comp_args_on_stack,\n-                                    const BasicType *sig_bt,\n-                                    const VMRegPair *regs) {\n@@ -582,1 +797,1 @@\n-  int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)>>LogBytesPerWord;\n+  int comp_words_on_stack = 0;\n@@ -584,2 +799,3 @@\n-    __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n-    __ andr(sp, rscratch1, -16);\n+     comp_words_on_stack = align_up(comp_args_on_stack * VMRegImpl::stack_slot_size, wordSize) >> LogBytesPerWord;\n+     __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n+     __ andr(sp, rscratch1, -16);\n@@ -604,0 +820,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -606,2 +824,5 @@\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+    BasicType bt = sig->at(i)._bt;\n+\n+    assert(bt != T_INLINE_TYPE, \"i2c adapter doesn't unpack inline typ args\");\n+    if (bt == T_VOID) {\n+      assert(i > 0 && (sig->at(i - 1)._bt == T_LONG || sig->at(i - 1)._bt == T_DOUBLE), \"missing half\");\n@@ -612,0 +833,1 @@\n+    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(), \"scrambled load targets?\");\n@@ -613,3 +835,1 @@\n-    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),\n-            \"scrambled load targets?\");\n-    int ld_off = (total_args_passed - i - 1)*Interpreter::stackElementSize;\n+    int ld_off = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n@@ -630,1 +850,1 @@\n-      int st_off = regs[i].first()->reg2stack()*VMRegImpl::stack_slot_size;\n+      int st_off = regs[i].first()->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -647,2 +867,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n@@ -651,11 +870,28 @@\n-        __ str(rscratch2, Address(sp, st_off));\n-      }\n-    } else if (r_1->is_Register()) {  \/\/ Register argument\n-      Register r = r_1->as_Register();\n-      if (r_2->is_valid()) {\n-        \/\/\n-        \/\/ We are using two VMRegs. This can be either T_OBJECT,\n-        \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n-        \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n-        \/\/ So we must adjust where to pick up the data to match the\n-        \/\/ interpreter.\n+         __ str(rscratch2, Address(sp, st_off));\n+       }\n+     } else if (r_1->is_Register()) {  \/\/ Register argument\n+       Register r = r_1->as_Register();\n+       if (r_2->is_valid()) {\n+         \/\/\n+         \/\/ We are using two VMRegs. This can be either T_OBJECT,\n+         \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n+         \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n+         \/\/ So we must adjust where to pick up the data to match the\n+         \/\/ interpreter.\n+\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n+\n+         \/\/ this can be a misaligned move\n+         __ ldr(r, Address(esp, offset));\n+       } else {\n+         \/\/ sign extend and use a full word?\n+         __ ldrw(r, Address(esp, ld_off));\n+       }\n+     } else {\n+       if (!r_2->is_valid()) {\n+         __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n+       } else {\n+         __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n+       }\n+     }\n+   }\n@@ -663,17 +899,0 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n-\n-        \/\/ this can be a misaligned move\n-        __ ldr(r, Address(esp, offset));\n-      } else {\n-        \/\/ sign extend and use a full word?\n-        __ ldrw(r, Address(esp, ld_off));\n-      }\n-    } else {\n-      if (!r_2->is_valid()) {\n-        __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n-      } else {\n-        __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n-      }\n-    }\n-  }\n@@ -692,1 +911,0 @@\n-\n@@ -696,13 +914,1 @@\n-\/\/ ---------------------------------------------------------------\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                                            int total_args_passed,\n-                                                            int comp_args_on_stack,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n-  address i2c_entry = __ pc();\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n-\n-  address c2i_unverified_entry = __ pc();\n-  Label skip_fixup;\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n@@ -743,0 +949,34 @@\n+}\n+\n+\n+\/\/ ---------------------------------------------------------------\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n+                                                            int comp_args_on_stack,\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n+\n+  address i2c_entry = __ pc();\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n+\n+  address c2i_unverified_entry = __ pc();\n+  Label skip_fixup;\n+\n+  gen_inline_cache_check(masm, skip_fixup);\n+\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    Label unused;\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+    skip_fixup = unused;\n+  }\n@@ -744,0 +984,1 @@\n+  \/\/ Scalarized c2i adapter\n@@ -748,0 +989,1 @@\n+\n@@ -750,4 +992,4 @@\n-\n-      __ ldrw(rscratch1, Address(rmethod, Method::access_flags_offset()));\n-      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n-      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n+        Register flags  = rscratch1;\n+      __ ldrw(flags, Address(rmethod, Method::access_flags_offset()));\n+      __ tst(flags, JVM_ACC_STATIC);\n+      __ br(Assembler::NE, L_skip_barrier); \/\/ non-static\n@@ -757,3 +999,6 @@\n-    __ load_method_holder(rscratch2, rmethod);\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    Register klass = rscratch1;\n+    __ load_method_holder(klass, rmethod);\n+    \/\/ We pass rthread to this function on x86\n+    __ clinit_barrier(klass, rscratch2, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n@@ -770,0 +1015,14 @@\n+  address c2i_unverified_inline_entry = c2i_unverified_entry;\n+\n+ \/\/ Non-scalarized c2i adapter\n+  address c2i_inline_entry = c2i_entry;\n+  if (regs != regs_cc) {\n+    Label inline_entry_skip_fixup;\n+    c2i_unverified_inline_entry = __ pc();\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    Label unused;\n+    gen_c2i_adapter(masm, sig, regs, inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+  }\n+\n@@ -771,1 +1030,8 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+\n+  \/\/ The c2i adapter might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+\n+  bool caller_must_gc_arguments = (regs != regs_cc);\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words + 10, oop_maps, caller_must_gc_arguments);\n+\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -814,0 +1080,1 @@\n+      case T_INLINE_TYPE:\n@@ -1633,0 +1900,1 @@\n+      case T_INLINE_TYPE:\n@@ -1820,0 +2088,1 @@\n+  case T_INLINE_TYPE:\n@@ -3063,0 +3332,105 @@\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler _masm(&buffer);\n+  MacroAssembler* masm = &_masm;\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ strs(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_DOUBLE) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(r0, val);\n+      \/\/ We don't need barriers because the destination is a newly allocated object.\n+      \/\/ Also, we cannot use store_heap_oop(to, val) because it uses r8 as tmp.\n+      if (UseCompressedOops) {\n+        __ encode_heap_oop(val);\n+        __ str(val, to);\n+      } else {\n+        __ str(val, to);\n+      }\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ store_sized_value(to, r_1->as_Register(), size_in_bytes);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(lr);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ ldrs(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ ldrd(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+       assert_different_registers(r0, r_1->as_Register());\n+       __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(lr);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, unpack_fields_off);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":525,"deletions":151,"binary":false,"changes":676,"status":"modified"},{"patch":"@@ -307,1 +307,1 @@\n-    \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+    \/\/ T_OBJECT, T_INLINE_TYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n@@ -311,1 +311,1 @@\n-    Label is_long, is_float, is_double, exit;\n+    Label is_long, is_float, is_double, is_value, exit;\n@@ -315,0 +315,2 @@\n+    __ cmp(j_rarg1, (u1)T_INLINE_TYPE);\n+    __ br(Assembler::EQ, is_value);\n@@ -369,0 +371,13 @@\n+    __ BIND(is_value);\n+    if (InlineTypeReturnedAsFields) {\n+      \/\/ Check for flattened return value\n+      __ cbz(r0, is_long);\n+      \/\/ Initialize pre-allocated buffer\n+      __ mov(r1, r0);\n+      __ andr(r1, r1, -2);\n+      __ ldr(r1, Address(r1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));\n+      __ ldr(r0, Address(j_rarg2, 0));\n+      __ blr(r1);\n+      __ b(exit);\n+    }\n@@ -1819,1 +1834,1 @@\n-    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, AS_RAW);  \/\/ store the oop\n+    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n@@ -6342,0 +6357,178 @@\n+  \/\/ Call here from the interpreter or compiled code to either load\n+  \/\/ multiple returned values from the inline type instance being\n+  \/\/ returned to registers or to store returned values to a newly\n+  \/\/ allocated inline type instance.\n+  address generate_return_value_stub(address destination, const char* name, bool has_res) {\n+\n+    \/\/ Information about frame layout at time of blocking runtime call.\n+    \/\/ Note that we only have to preserve callee-saved registers since\n+    \/\/ the compilers are responsible for supplying a continuation point\n+    \/\/ if they expect all registers to be preserved.\n+    \/\/ n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0\n+    enum layout {\n+      rfp_off = 0, rfp_off2,\n+\n+      j_rarg7_off, j_rarg7_2,\n+      j_rarg6_off, j_rarg6_2,\n+      j_rarg5_off, j_rarg5_2,\n+      j_rarg4_off, j_rarg4_2,\n+      j_rarg3_off, j_rarg3_2,\n+      j_rarg2_off, j_rarg2_2,\n+      j_rarg1_off, j_rarg1_2,\n+      j_rarg0_off, j_rarg0_2,\n+\n+      j_farg0_off, j_farg0_2,\n+      j_farg1_off, j_farg1_2,\n+      j_farg2_off, j_farg2_2,\n+      j_farg3_off, j_farg3_2,\n+      j_farg4_off, j_farg4_2,\n+      j_farg5_off, j_farg5_2,\n+      j_farg6_off, j_farg6_2,\n+      j_farg7_off, j_farg7_2,\n+\n+      return_off, return_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    int insts_size = 512;\n+    int locs_size  = 64;\n+\n+    CodeBuffer code(name, insts_size, locs_size);\n+    OopMapSet* oop_maps  = new OopMapSet();\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+\n+    address start = __ pc();\n+\n+    const Address f7_save       (rfp, j_farg7_off * wordSize);\n+    const Address f6_save       (rfp, j_farg6_off * wordSize);\n+    const Address f5_save       (rfp, j_farg5_off * wordSize);\n+    const Address f4_save       (rfp, j_farg4_off * wordSize);\n+    const Address f3_save       (rfp, j_farg3_off * wordSize);\n+    const Address f2_save       (rfp, j_farg2_off * wordSize);\n+    const Address f1_save       (rfp, j_farg1_off * wordSize);\n+    const Address f0_save       (rfp, j_farg0_off * wordSize);\n+\n+    const Address r0_save      (rfp, j_rarg0_off * wordSize);\n+    const Address r1_save      (rfp, j_rarg1_off * wordSize);\n+    const Address r2_save      (rfp, j_rarg2_off * wordSize);\n+    const Address r3_save      (rfp, j_rarg3_off * wordSize);\n+    const Address r4_save      (rfp, j_rarg4_off * wordSize);\n+    const Address r5_save      (rfp, j_rarg5_off * wordSize);\n+    const Address r6_save      (rfp, j_rarg6_off * wordSize);\n+    const Address r7_save      (rfp, j_rarg7_off * wordSize);\n+\n+    \/\/ Generate oop map\n+    OopMap* map = new OopMap(framesize, 0);\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(rfp_off), rfp->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg7_off), j_rarg7->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg6_off), j_rarg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+    \/\/ This is an inlined and slightly modified version of call_VM\n+    \/\/ which has the ability to fetch the return PC out of\n+    \/\/ thread-local storage and also sets up last_Java_sp slightly\n+    \/\/ differently than the real call_VM\n+\n+    __ enter(); \/\/ Save FP and LR before call\n+\n+    assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n+\n+    \/\/ lr and fp are already in place\n+    __ sub(sp, rfp, ((unsigned)framesize - 4) << LogBytesPerInt); \/\/ prolog\n+\n+    __ strd(j_farg7, f7_save);\n+    __ strd(j_farg6, f6_save);\n+    __ strd(j_farg5, f5_save);\n+    __ strd(j_farg4, f4_save);\n+    __ strd(j_farg3, f3_save);\n+    __ strd(j_farg2, f2_save);\n+    __ strd(j_farg1, f1_save);\n+    __ strd(j_farg0, f0_save);\n+\n+    __ str(j_rarg0, r0_save);\n+    __ str(j_rarg1, r1_save);\n+    __ str(j_rarg2, r2_save);\n+    __ str(j_rarg3, r3_save);\n+    __ str(j_rarg4, r4_save);\n+    __ str(j_rarg5, r5_save);\n+    __ str(j_rarg6, r6_save);\n+    __ str(j_rarg7, r7_save);\n+\n+    int frame_complete = __ pc() - start;\n+\n+    \/\/ Set up last_Java_sp and last_Java_fp\n+    address the_pc = __ pc();\n+    __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n+\n+    \/\/ Call runtime\n+    __ mov(c_rarg0, rthread);\n+    __ mov(c_rarg1, r0);\n+\n+    BLOCK_COMMENT(\"call runtime_entry\");\n+    __ mov(rscratch1, destination);\n+    __ blr(rscratch1);\n+\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    __ reset_last_Java_frame(false);\n+    __ maybe_isb();\n+\n+    __ ldrd(j_farg7, f7_save);\n+    __ ldrd(j_farg6, f6_save);\n+    __ ldrd(j_farg5, f5_save);\n+    __ ldrd(j_farg4, f4_save);\n+    __ ldrd(j_farg3, f3_save);\n+    __ ldrd(j_farg3, f2_save);\n+    __ ldrd(j_farg1, f1_save);\n+    __ ldrd(j_farg0, f0_save);\n+\n+    __ ldr(j_rarg0, r0_save);\n+    __ ldr(j_rarg1, r1_save);\n+    __ ldr(j_rarg2, r2_save);\n+    __ ldr(j_rarg3, r3_save);\n+    __ ldr(j_rarg4, r4_save);\n+    __ ldr(j_rarg5, r5_save);\n+    __ ldr(j_rarg6, r6_save);\n+    __ ldr(j_rarg7, r7_save);\n+\n+    __ leave();\n+\n+    \/\/ check for pending exceptions\n+    Label pending;\n+    __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ cmp(rscratch1, (u1)NULL_WORD);\n+    __ br(Assembler::NE, pending);\n+\n+    if (has_res) {\n+      __ get_vm_result(r0, rthread);\n+    }\n+    __ ret(lr);\n+\n+    __ bind(pending);\n+    __ ldr(r0, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+\n+    \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+    int frame_size_in_words = (framesize >> (LogBytesPerWord - LogBytesPerInt));\n+    RuntimeStub* stub =\n+      RuntimeStub::new_runtime_stub(name, &code, frame_complete, frame_size_in_words, oop_maps, false);\n+\n+    return stub->entry_point();\n+  }\n+\n@@ -6392,0 +6585,5 @@\n+    StubRoutines::_load_inline_type_fields_in_regs =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), \"store_inline_type_fields_to_buf\", true);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":201,"deletions":3,"binary":false,"changes":204,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -438,0 +439,5 @@\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(NULL, true);\n+  }\n+\n@@ -556,0 +562,1 @@\n+  case T_INLINE_TYPE: \/\/ fall through (value types are handled with oops)\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/objArrayKlass.hpp\"\n@@ -185,0 +186,73 @@\n+\/\/ Implementation of LoadFlattenedArrayStub\n+\n+LoadFlattenedArrayStub::LoadFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _result = result;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void LoadFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 1);\n+  ce->store_parameter(_index->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::load_flattened_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  if (_result->as_register() != rax) {\n+    __ movptr(_result->as_register(), rax);\n+  }\n+  __ jmp(_continuation);\n+}\n+\n+\n+\/\/ Implementation of StoreFlattenedArrayStub\n+\n+StoreFlattenedArrayStub::StoreFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr value, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _value = value;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+\n+void StoreFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 2);\n+  ce->store_parameter(_index->as_register(), 1);\n+  ce->store_parameter(_value->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::store_flattened_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ jmp(_continuation);\n+}\n+\n+\n+\/\/ Implementation of SubstitutabilityCheckStub\n+\n+SubstitutabilityCheckStub::SubstitutabilityCheckStub(LIR_Opr left, LIR_Opr right, CodeEmitInfo* info) {\n+  _left = left;\n+  _right = right;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void SubstitutabilityCheckStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_left->as_register(), 1);\n+  ce->store_parameter(_right->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::substitutability_check_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ jmp(_continuation);\n+}\n+\n+\n@@ -193,0 +267,1 @@\n+         stub_id == Runtime1::new_instance_no_inline_id       ||\n@@ -237,1 +312,2 @@\n-NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info) {\n+NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result,\n+                                       CodeEmitInfo* info, bool is_inline_type) {\n@@ -242,0 +318,1 @@\n+  _is_inline_type = is_inline_type;\n@@ -250,1 +327,5 @@\n-  __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::new_object_array_id)));\n+  if (_is_inline_type) {\n+    __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::new_flat_array_id)));\n+  } else {\n+    __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::new_object_array_id)));\n+  }\n@@ -260,1 +341,1 @@\n-MonitorEnterStub::MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)\n+MonitorEnterStub::MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info, CodeStub* throw_imse_stub, LIR_Opr scratch_reg)\n@@ -264,0 +345,5 @@\n+  _throw_imse_stub = throw_imse_stub;\n+  _scratch_reg = scratch_reg;\n+  if (_throw_imse_stub != NULL) {\n+    assert(_scratch_reg != LIR_OprFact::illegalOpr, \"must be\");\n+  }\n@@ -270,0 +356,9 @@\n+  if (_throw_imse_stub != NULL) {\n+    \/\/ When we come here, _obj_reg has already been checked to be non-null.\n+    const int is_value_mask = markWord::inline_type_pattern;\n+    Register mark = _scratch_reg->as_register();\n+    __ movptr(mark, Address(_obj_reg->as_register(), oopDesc::mark_offset_in_bytes()));\n+    __ andptr(mark, is_value_mask);\n+    __ cmpl(mark, is_value_mask);\n+    __ jcc(Assembler::equal, *_throw_imse_stub->entry());\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":98,"deletions":3,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -195,1 +197,1 @@\n-    if (const_opr->type() == T_OBJECT) {\n+    if (const_opr->type() == T_OBJECT || const_opr->type() == T_INLINE_TYPE) {\n@@ -482,1 +484,2 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  int initial_framesize = initial_frame_size_in_bytes();\n+  __ remove_frame(initial_framesize, needs_stack_repair(), initial_framesize - wordSize);\n@@ -527,0 +530,17 @@\n+  ciMethod* method = compilation()->method();\n+  ciType* return_type = method->return_type();\n+  if (InlineTypeReturnedAsFields && return_type->is_inlinetype()) {\n+    ciInlineKlass* vk = return_type->as_inline_klass();\n+    if (vk->can_be_returned_as_fields()) {\n+#ifndef _LP64\n+      Unimplemented();\n+#else\n+      address unpack_handler = vk->unpack_handler();\n+      assert(unpack_handler != NULL, \"must be\");\n+      __ call(RuntimeAddress(unpack_handler));\n+      \/\/ At this point, rax points to the value object (for interpreter or C1 caller).\n+      \/\/ The fields of the object are copied into registers (for C2 caller).\n+#endif\n+    }\n+  }\n+\n@@ -528,1 +548,2 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  int initial_framesize = initial_frame_size_in_bytes();\n+  __ remove_frame(initial_framesize, needs_stack_repair(), initial_framesize - wordSize);\n@@ -550,0 +571,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -610,0 +635,1 @@\n+    case T_INLINE_TYPE: \/\/ Fall through\n@@ -700,0 +726,1 @@\n+    case T_INLINE_TYPE: \/\/ Fall through\n@@ -739,0 +766,1 @@\n+    case T_INLINE_TYPE: \/\/ fall through\n@@ -827,1 +855,1 @@\n-    if (src->type() == T_OBJECT) {\n+    if (src->type() == T_OBJECT || src->type() == T_INLINE_TYPE) {\n@@ -1013,0 +1041,1 @@\n+    case T_INLINE_TYPE: \/\/ fall through\n@@ -1186,1 +1215,1 @@\n-  if (addr->base()->type() == T_OBJECT) {\n+  if (addr->base()->type() == T_OBJECT || addr->base()->type() == T_INLINE_TYPE) {\n@@ -1247,0 +1276,1 @@\n+    case T_INLINE_TYPE: \/\/ fall through\n@@ -1633,1 +1663,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->type() == T_INLINE_TYPE ||\n@@ -1732,14 +1762,16 @@\n-  __ cmpptr(obj, (int32_t)NULL_WORD);\n-  if (op->should_profile()) {\n-    Label not_null;\n-    __ jccb(Assembler::notEqual, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n-    int header_bits = BitData::null_seen_byte_constant();\n-    __ orb(data_addr, header_bits);\n-    __ jmp(*obj_is_null);\n-    __ bind(not_null);\n-  } else {\n-    __ jcc(Assembler::equal, *obj_is_null);\n+  if (op->need_null_check()) {\n+    __ cmpptr(obj, (int32_t)NULL_WORD);\n+    if (op->should_profile()) {\n+      Label not_null;\n+      __ jccb(Assembler::notEqual, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n+      int header_bits = BitData::null_seen_byte_constant();\n+      __ orb(data_addr, header_bits);\n+      __ jmp(*obj_is_null);\n+      __ bind(not_null);\n+    } else {\n+      __ jcc(Assembler::equal, *obj_is_null);\n+    }\n@@ -1957,0 +1989,105 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be flattened (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is flattened, take the slow path.\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  Register klass = op->tmp()->as_register();\n+  __ load_klass(klass, op->array()->as_register(), tmp_load_klass);\n+  __ movl(klass, Address(klass, Klass::layout_helper_offset()));\n+  __ testl(klass, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  __ jcc(Assembler::notZero, *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ The array is not flattened, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cmpptr(op->value()->as_register(), (int32_t)NULL_WORD);\n+    __ jcc(Assembler::notEqual, skip);\n+    __ testl(klass, Klass::_lh_null_free_bit_inplace);\n+    __ jcc(Assembler::notZero, *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  Register klass = op->tmp()->as_register();\n+  __ load_klass(klass, op->array()->as_register(), tmp_load_klass);\n+  __ movl(klass, Address(klass, Klass::layout_helper_offset()));\n+  __ testl(klass, Klass::_lh_null_free_bit_inplace);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmpptr(left, right);\n+  __ jcc(Assembler::equal, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  __ testptr(left, right);\n+  __ jcc(Assembler::zero, L_oops_not_equal);\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == NULL || right_klass == NULL) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ movptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ andptr(tmp1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(tmp1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ cmpptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ jcc(Assembler::notEqual, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != NULL && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ jmp(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedOops) {\n+      __ movl(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movl(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpl(left_klass_op, right_klass_op);\n+    } else {\n+      __ movptr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movptr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpptr(left_klass_op, right_klass_op);\n+    }\n+\n+    __ jcc(Assembler::equal, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  \/\/ We've returned from the stub. RAX contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cmpl(rax, 0);\n+  __ jcc(Assembler::equal, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n@@ -2017,0 +2154,15 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, NULL);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n@@ -2896,1 +3048,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2902,1 +3054,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -3098,0 +3250,19 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ testptr(obj, obj);\n+    __ jcc(Assembler::zero, *slow_path->entry());\n+  }\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  __ load_klass(tmp, obj, tmp_load_klass);\n+  __ movl(tmp, Address(tmp, Klass::layout_helper_offset()));\n+  if (is_dest) {\n+    \/\/ We also take slow path if it's a null_free destination array, just in case the source array\n+    \/\/ contains NULLs.\n+    __ testl(tmp, Klass::_lh_null_free_bit_inplace);\n+  } else {\n+    __ testl(tmp, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  }\n+  __ jcc(Assembler::notZero, *slow_path->entry());\n+}\n+\n+\n@@ -3119,0 +3290,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ jmp(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -3212,0 +3389,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -3796,0 +3981,20 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr());\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ testptr(obj, obj);\n+    __ jccb(Assembler::zero, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  __ orb(mdo_addr, flag);\n+\n+  __ bind(not_inline_type);\n+}\n+\n@@ -4056,0 +4261,3 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ cmpptr(frame_map()->address_for_orig_pc_addr(), (int32_t)NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":230,"deletions":22,"binary":false,"changes":252,"status":"modified"},{"patch":"@@ -548,1 +548,1 @@\n-              Address dst, Register val, Register tmp1, Register tmp2) {\n+              Address dst, Register val, Register tmp1, Register tmp2, Register tmp3) {\n@@ -586,1 +586,1 @@\n-      BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg);\n+      BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg, noreg);\n@@ -589,1 +589,1 @@\n-      BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg);\n+      BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg, noreg);\n@@ -593,1 +593,1 @@\n-    BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2);\n+    BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2, tmp3);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -80,1 +80,1 @@\n-                        Address dst, Register val, Register tmp1, Register tmp2);\n+                        Address dst, Register val, Register tmp1, Register tmp2, Register tmp3 = noreg);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -154,1 +155,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -199,1 +200,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n@@ -559,1 +560,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -567,1 +569,3 @@\n-  profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  if (profile) {\n+    profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  }\n@@ -573,1 +577,3 @@\n-  profile_typecheck_failed(rcx); \/\/ blows rcx\n+  if (profile) {\n+    profile_typecheck_failed(rcx); \/\/ blows rcx\n+  }\n@@ -1019,1 +1025,1 @@\n- \/\/ get method access flags\n+  \/\/ get method access flags\n@@ -1143,4 +1149,2 @@\n-  \/\/ remove activation\n-  \/\/ get sender sp\n-  movptr(rbx,\n-         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    movptr(rbx,\n+               Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n@@ -1168,0 +1172,34 @@\n+\n+  \/\/ remove activation\n+  \/\/ get sender sp\n+  movptr(rbx,\n+         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    Label skip;\n+    \/\/ Test if the return type is an inline type\n+    movptr(rdi, Address(rbp, frame::interpreter_frame_method_offset * wordSize));\n+    movptr(rdi, Address(rdi, Method::const_offset()));\n+    load_unsigned_byte(rdi, Address(rdi, ConstMethod::result_type_offset()));\n+    cmpl(rdi, T_INLINE_TYPE);\n+    jcc(Assembler::notEqual, skip);\n+\n+    \/\/ We are returning an inline type, load its fields into registers\n+#ifndef _LP64\n+    super_call_VM_leaf(StubRoutines::load_inline_type_fields_in_regs());\n+#else\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    load_klass(rdi, rax, tmp_load_klass);\n+    movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+\n+    testptr(rdi, rdi);\n+    jcc(Assembler::equal, skip);\n+\n+    call(rdi);\n+#endif\n+    \/\/ call above kills the value in rbx. Reload it.\n+    movptr(rbx, Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n@@ -1187,0 +1225,106 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  {\n+    SkipIfEqual skip_if(this, &DTraceAllocProbes, 0);\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::read_inlined_field(Register holder_klass,\n+                                                     Register field_index, Register field_offset,\n+                                                     Register obj) {\n+  Label alloc_failed, empty_value, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+  assert_different_registers(obj, holder_klass, field_index, field_offset, dst_temp);\n+\n+  \/\/ Grap the inline field klass\n+  push(holder_klass);\n+  const Register field_klass = holder_klass;\n+  get_inline_type_field_klass(holder_klass, field_index, field_klass);\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(field_klass, dst_temp, empty_value);\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  data_for_oop(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, field_klass);\n+  pop(obj);\n+  pop(holder_klass);\n+  jmp(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(field_klass, dst_temp, obj);\n+  pop(holder_klass);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  pop(holder_klass);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_inlined_field),\n+          obj, field_index, holder_klass);\n+\n+  bind(done);\n+}\n+\n+void InterpreterMacroAssembler::read_flattened_element(Register array, Register index,\n+                                                       Register t1, Register t2,\n+                                                       Register obj) {\n+  assert_different_registers(array, index, t1, t2);\n+  Label alloc_failed, empty_value, done;\n+  const Register array_klass = t2;\n+  const Register elem_klass = t1;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+\n+  \/\/ load in array->klass()->element_klass()\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(array_klass, array, tmp_load_klass);\n+  movptr(elem_klass, Address(array_klass, ArrayKlass::element_klass_offset()));\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(elem_klass, dst_temp, empty_value);\n+\n+  \/\/ calc source into \"array_klass\" and free up some regs\n+  const Register src = array_klass;\n+  push(index); \/\/ preserve index reg in case alloc_failed\n+  data_for_value_array_index(array, array_klass, index, src);\n+\n+  allocate_instance(elem_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+  \/\/ Have an oop instance buffer, copy into it\n+  store_ptr(0, obj); \/\/ preserve obj (overwrite index, no longer needed)\n+  data_for_oop(obj, dst_temp, elem_klass);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, elem_klass);\n+  pop(obj);\n+  jmp(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(elem_klass, dst_temp, obj);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(index);\n+  if (array == c_rarg2) {\n+    mov(elem_klass, array);\n+    array = elem_klass;\n+  }\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_load), array, index);\n+\n+  bind(done);\n+}\n+\n@@ -1238,0 +1382,5 @@\n+    if (EnableValhalla) {\n+      assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+    }\n@@ -1571,1 +1720,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1583,1 +1732,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()): in_bytes(BranchData::branch_data_size()));\n@@ -1958,0 +2107,78 @@\n+void InterpreterMacroAssembler::profile_array(Register mdp,\n+                                              Register array,\n+                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flattened_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element(Register mdp,\n+                                                Register element,\n+                                                Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadStoreData::array_load_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":239,"deletions":12,"binary":false,"changes":251,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -48,0 +49,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -51,0 +53,1 @@\n+#include \"vmreg_x86.inline.hpp\"\n@@ -52,0 +55,3 @@\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1636,0 +1642,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2664,0 +2674,140 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  testl(temp_reg, JVM_ACC_INLINE);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  testptr(object, object);\n+  jcc(Assembler::equal, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  movl(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  testl(temp_reg, InstanceKlass::misc_flags_is_empty_inline_type());\n+  jcc(Assembler::notZero, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::zero, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inlined_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inlined);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,\n+                                              Label&is_flattened_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flattened_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flattened_array_layout(temp_reg, is_flattened_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flattened_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flattened_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flattened_array_layout(temp_reg, is_non_flattened_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::nullfree_array_bit_in_place, true, is_null_free_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_null_free_array_layout(temp_reg, is_null_free_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::nullfree_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_null_free_array_layout(temp_reg, is_non_null_free_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_bit_inplace);\n+  jcc(Assembler::notZero, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_bit_inplace);\n+  jcc(Assembler::zero, is_non_null_free_array);\n+}\n+\n+\n@@ -3470,0 +3620,129 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax, according to barrier asm eden_allocate\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+    jcc(Assembler::equal, L);\n+    stop(\"klass not initialized\");\n+    bind(L);\n+  }\n+#endif\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+  const bool allow_shared_alloc =\n+    Universe::heap()->supports_inline_contig_alloc();\n+\n+  push(klass);\n+  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(klass);\n+#ifndef _LP64\n+  if (UseTLAB || allow_shared_alloc) {\n+    get_thread(thread);\n+  }\n+#endif \/\/ _LP64\n+\n+  if (UseTLAB) {\n+    tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    \/\/ Allocation in the shared Eden, if allowed.\n+    \/\/\n+    eden_allocate(thread, new_obj, layout_size, 0, t2, slow_case);\n+  }\n+\n+  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n+  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n+  if (UseTLAB || allow_shared_alloc) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      decrement(layout_size, sizeof(oopDesc));\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 1*oopSize), zero);\n+        NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 2*oopSize), zero));\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+#ifdef _LP64\n+    xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+#endif\n+    movptr(t2, klass);         \/\/ preserve klass\n+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    store_klass(new_obj, t2, tmp_store_klass);  \/\/ src klass reg is potentially compressed\n+\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -3547,0 +3826,50 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  movptr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(inline_klass, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  movptr(inline_klass, Address(inline_klass, index, Address::times_ptr));\n+}\n+\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  movptr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  movl(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  movptr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset, Address::times_1);\n+  load_heap_oop(obj, field);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n+\n@@ -3895,1 +4224,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -3991,1 +4324,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4494,0 +4831,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -4503,1 +4848,1 @@\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -4536,1 +4881,1 @@\n-                                     Register tmp1, Register tmp2) {\n+                                     Register tmp1, Register tmp2, Register tmp3) {\n@@ -4541,1 +4886,1 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n@@ -4543,1 +4888,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2);\n+    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n@@ -4547,0 +4892,40 @@\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE)));\n+}\n+\n@@ -4568,2 +4953,2 @@\n-                                    Register tmp2, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2);\n+                                    Register tmp2, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2, tmp3);\n@@ -4574,1 +4959,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -4888,0 +5273,1 @@\n+#ifdef COMPILER2\n@@ -4889,1 +5275,5 @@\n-void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n@@ -4942,0 +5332,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, C->output()->sp_inc_offset()), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -4970,5 +5366,1 @@\n-\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->nmethod_entry_barrier(this);\n-  }\n+#endif \/\/ COMPILER2\n@@ -4978,1 +5370,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, XMMRegister xtmp) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp) {\n@@ -4982,0 +5374,1 @@\n+  movdq(xtmp, val);\n@@ -4983,1 +5376,2 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -4985,1 +5379,1 @@\n-    pxor(xtmp, xtmp);\n+    punpcklqdq(xtmp, xtmp);\n@@ -5029,1 +5423,297 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp, bool is_large) {\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  Label slow_case;\n+\n+  \/\/ Try to allocate a new buffered inline type (from the heap)\n+  if (UseTLAB) {\n+    \/\/ FIXME -- for smaller code, the inline allocation (and the slow case) should be moved inside the pack handler.\n+    if (vk != NULL) {\n+      \/\/ Called from C1, where the return type is statically known.\n+      movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+      jint lh = vk->layout_helper();\n+      assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+      movl(r14, lh);\n+    } else {\n+      \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+      mov(rbx, rax);\n+      andptr(rbx, -2);\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+    }\n+\n+    movptr(r13, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));\n+    lea(r14, Address(r13, r14, Address::times_1));\n+    cmpptr(r14, Address(r15_thread, in_bytes(JavaThread::tlab_end_offset())));\n+    jcc(Assembler::above, slow_case);\n+    movptr(Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())), r14);\n+    movptr(Address(r13, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+\n+    xorl(rax, rax); \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(r13, rax);  \/\/ zero klass gap for compressed oops\n+\n+    if (vk == NULL) {\n+      \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n+      mov(rax, rbx);\n+    }\n+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    store_klass(r13, rbx, tmp_store_klass);  \/\/ klass\n+\n+    \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+    if (vk != NULL) {\n+      \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+      mov(rax, r13);\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      mov(rax, r13);\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must bevalid\");\n+  Register fromReg;\n+  if (from->is_reg()) {\n+    fromReg = from->as_Register();\n+  } else {\n+    int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+    movq(r10, Address(rsp, st_off));\n+    fromReg = r10;\n+  }\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  while (stream.next(toReg, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+     if (idx != from->value()) {\n+       mark_done = false;\n+     }\n+     done = false;\n+     continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    } else {\n+      assert(reg_state[idx] == reg_writable, \"must be writable\");\n+      reg_state[idx] = reg_written;\n+    }\n+\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? r13 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  Register val_array = rax;\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14; \/\/ Be careful with r14 because it's used for spilling\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  while (stream.next(fromReg, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+    reg_state[fromReg->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    movq(rbp, Address(rsp, initial_framesize));\n+    addq(rsp, Address(rsp, sp_inc_offset));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only) {\n@@ -5034,1 +5724,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"tmp register must be eax for rep stos\");\n@@ -5041,4 +5731,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n-\n@@ -5057,1 +5743,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -5066,1 +5752,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -5070,2 +5756,1 @@\n-    movptr(tmp, base);\n-    xmm_clear_mem(tmp, cnt, xtmp);\n+    xmm_clear_mem(base, cnt, val, xtmp);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":713,"deletions":28,"binary":false,"changes":741,"status":"modified"},{"patch":"@@ -31,0 +31,3 @@\n+#include \"runtime\/signature.hpp\"\n+\n+class ciInlineKlass;\n@@ -101,0 +104,31 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type);\n+  void test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type);\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type);\n+\n+  \/\/ Get the default value oop for the given InlineKlass\n+  void get_default_value_oop(Register inline_klass, Register temp_reg, Register obj);\n+  \/\/ The empty value oop, for the given InlineKlass (\"empty\" as in no instance fields)\n+  \/\/ get_default_value_oop with extra assertion for empty inline klass\n+  void get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj);\n+\n+  void test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline);\n+  void test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline);\n+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined);\n+\n+  \/\/ Check oops for special arrays, i.e. flattened and\/or null-free\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flattened_array_oop(Register oop, Register temp_reg, Label&is_flattened_array);\n+  void test_non_flattened_array_oop(Register oop, Register temp_reg, Label&is_non_flattened_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flatten or null-free arrays...\n+  void test_flattened_array_layout(Register lh, Label& is_flattened_array);\n+  void test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array);\n+  void test_null_free_array_layout(Register lh, Label& is_null_free_array);\n+  void test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array);\n+\n@@ -318,0 +352,1 @@\n+  void load_metadata(Register dst, Register src);\n@@ -324,1 +359,11 @@\n-                       Register tmp1, Register tmp2);\n+                       Register tmp1, Register tmp2, Register tmp3 = noreg);\n+\n+  void access_value_copy(DecoratorSet decorators, Register src, Register dst, Register inline_klass);\n+\n+  \/\/ inline type data payload offsets...\n+  void first_field_offset(Register inline_klass, Register offset);\n+  void data_for_oop(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -335,1 +380,1 @@\n-                      Register tmp2 = noreg, DecoratorSet decorators = 0);\n+                      Register tmp2 = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);\n@@ -511,0 +556,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -530,0 +584,3 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n@@ -699,1 +756,2 @@\n-  void andptr(Register src1, Register src2) { LP64_ONLY(andq(src1, src2)) NOT_LP64(andl(src1, src2)) ; }\n+  void andptr(Register dst, Register src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n+  void andptr(Register dst, Address src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n@@ -1676,1 +1734,15 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(Compile* C, int sp_inc = 0);\n+\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[]);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset);\n+  VMReg spill_reg_for(VMReg reg);\n@@ -1680,1 +1752,1 @@\n-  void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large);\n+  void clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only);\n@@ -1683,1 +1755,1 @@\n-  void xmm_clear_mem(Register base, Register cnt, XMMRegister xtmp);\n+  void xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":78,"deletions":6,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -339,5 +339,5 @@\n-    \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n-    __ movptr(c_rarg0, result);\n-    Label is_long, is_float, is_double, exit;\n-    __ movl(c_rarg1, result_type);\n-    __ cmpl(c_rarg1, T_OBJECT);\n+    \/\/ T_OBJECT, T_INLINE_TYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+    __ movptr(r13, result);\n+    Label is_long, is_float, is_double, is_value, exit;\n+    __ movl(rbx, result_type);\n+    __ cmpl(rbx, T_OBJECT);\n@@ -345,1 +345,3 @@\n-    __ cmpl(c_rarg1, T_LONG);\n+    __ cmpl(rbx, T_INLINE_TYPE);\n+    __ jcc(Assembler::equal, is_value);\n+    __ cmpl(rbx, T_LONG);\n@@ -347,1 +349,1 @@\n-    __ cmpl(c_rarg1, T_FLOAT);\n+    __ cmpl(rbx, T_FLOAT);\n@@ -349,1 +351,1 @@\n-    __ cmpl(c_rarg1, T_DOUBLE);\n+    __ cmpl(rbx, T_DOUBLE);\n@@ -353,1 +355,1 @@\n-    __ movl(Address(c_rarg0, 0), rax);\n+    __ movl(Address(r13, 0), rax);\n@@ -415,0 +417,13 @@\n+    __ BIND(is_value);\n+    if (InlineTypeReturnedAsFields) {\n+      \/\/ Check for flattened return value\n+      __ testptr(rax, 1);\n+      __ jcc(Assembler::zero, is_long);\n+      \/\/ Load pack handler address\n+      __ andptr(rax, -2);\n+      __ movptr(rax, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ movptr(rbx, Address(rax, InlineKlass::pack_handler_jobject_offset()));\n+      \/\/ Call pack handler to initialize the buffer\n+      __ call(rbx);\n+      __ jmp(exit);\n+    }\n@@ -416,1 +431,1 @@\n-    __ movq(Address(c_rarg0, 0), rax);\n+    __ movq(Address(r13, 0), rax);\n@@ -420,1 +435,1 @@\n-    __ movflt(Address(c_rarg0, 0), xmm0);\n+    __ movflt(Address(r13, 0), xmm0);\n@@ -424,1 +439,1 @@\n-    __ movdbl(Address(c_rarg0, 0), xmm0);\n+    __ movdbl(Address(r13, 0), xmm0);\n@@ -2843,1 +2858,1 @@\n-    __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, AS_RAW);  \/\/ store the oop\n+    __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n@@ -3138,0 +3153,8 @@\n+    \/\/ Check for flat inline type array -> return -1\n+    __ testl(rax_lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+    __ jcc(Assembler::notZero, L_failed);\n+\n+    \/\/ Check for null-free (non-flat) inline type array -> handle as object array\n+    __ testl(rax_lh, Klass::_lh_null_free_bit_inplace);\n+    __ jcc(Assembler::notZero, L_objArray);\n+\n@@ -3147,2 +3170,4 @@\n-      __ cmpl(rax_lh, (Klass::_lh_array_tag_type_value << Klass::_lh_array_tag_shift));\n-      __ jcc(Assembler::greaterEqual, L);\n+      __ movl(rklass_tmp, rax_lh);\n+      __ sarl(rklass_tmp, Klass::_lh_array_tag_shift);\n+      __ cmpl(rklass_tmp, Klass::_lh_array_tag_type_value);\n+      __ jcc(Assembler::equal, L);\n@@ -3256,0 +3281,1 @@\n+      \/\/ This check also fails for flat\/null-free arrays which are not supported.\n@@ -3259,0 +3285,13 @@\n+#ifdef ASSERT\n+      {\n+        BLOCK_COMMENT(\"assert not null-free array {\");\n+        Label L;\n+        __ movl(rklass_tmp, Address(rax, lh_offset));\n+        __ testl(rklass_tmp, Klass::_lh_null_free_bit_inplace);\n+        __ jcc(Assembler::zero, L);\n+        __ stop(\"unexpected null-free array\");\n+        __ bind(L);\n+        BLOCK_COMMENT(\"} assert not null-free array\");\n+      }\n+#endif\n+\n@@ -6695,0 +6734,140 @@\n+  \/\/ Call here from the interpreter or compiled code to either load\n+  \/\/ multiple returned values from the inline type instance being\n+  \/\/ returned to registers or to store returned values to a newly\n+  \/\/ allocated inline type instance.\n+  address generate_return_value_stub(address destination, const char* name, bool has_res) {\n+    \/\/ We need to save all registers the calling convention may use so\n+    \/\/ the runtime calls read or update those registers. This needs to\n+    \/\/ be in sync with SharedRuntime::java_return_convention().\n+    enum layout {\n+      pad_off = frame::arg_reg_save_area_bytes\/BytesPerInt, pad_off_2,\n+      rax_off, rax_off_2,\n+      j_rarg5_off, j_rarg5_2,\n+      j_rarg4_off, j_rarg4_2,\n+      j_rarg3_off, j_rarg3_2,\n+      j_rarg2_off, j_rarg2_2,\n+      j_rarg1_off, j_rarg1_2,\n+      j_rarg0_off, j_rarg0_2,\n+      j_farg0_off, j_farg0_2,\n+      j_farg1_off, j_farg1_2,\n+      j_farg2_off, j_farg2_2,\n+      j_farg3_off, j_farg3_2,\n+      j_farg4_off, j_farg4_2,\n+      j_farg5_off, j_farg5_2,\n+      j_farg6_off, j_farg6_2,\n+      j_farg7_off, j_farg7_2,\n+      rbp_off, rbp_off_2,\n+      return_off, return_off_2,\n+\n+      framesize\n+    };\n+\n+    CodeBuffer buffer(name, 1000, 512);\n+    MacroAssembler* masm = new MacroAssembler(&buffer);\n+\n+    int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+    assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+    int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+    int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+    OopMapSet *oop_maps = new OopMapSet();\n+    OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(rax_off), rax->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+    int start = __ offset();\n+\n+    __ subptr(rsp, frame_size_in_bytes - 8 \/* return address*\/);\n+\n+    __ movptr(Address(rsp, rbp_off * BytesPerInt), rbp);\n+    __ movdbl(Address(rsp, j_farg7_off * BytesPerInt), j_farg7);\n+    __ movdbl(Address(rsp, j_farg6_off * BytesPerInt), j_farg6);\n+    __ movdbl(Address(rsp, j_farg5_off * BytesPerInt), j_farg5);\n+    __ movdbl(Address(rsp, j_farg4_off * BytesPerInt), j_farg4);\n+    __ movdbl(Address(rsp, j_farg3_off * BytesPerInt), j_farg3);\n+    __ movdbl(Address(rsp, j_farg2_off * BytesPerInt), j_farg2);\n+    __ movdbl(Address(rsp, j_farg1_off * BytesPerInt), j_farg1);\n+    __ movdbl(Address(rsp, j_farg0_off * BytesPerInt), j_farg0);\n+\n+    __ movptr(Address(rsp, j_rarg0_off * BytesPerInt), j_rarg0);\n+    __ movptr(Address(rsp, j_rarg1_off * BytesPerInt), j_rarg1);\n+    __ movptr(Address(rsp, j_rarg2_off * BytesPerInt), j_rarg2);\n+    __ movptr(Address(rsp, j_rarg3_off * BytesPerInt), j_rarg3);\n+    __ movptr(Address(rsp, j_rarg4_off * BytesPerInt), j_rarg4);\n+    __ movptr(Address(rsp, j_rarg5_off * BytesPerInt), j_rarg5);\n+    __ movptr(Address(rsp, rax_off * BytesPerInt), rax);\n+\n+    int frame_complete = __ offset();\n+\n+    __ set_last_Java_frame(noreg, noreg, NULL);\n+\n+    __ mov(c_rarg0, r15_thread);\n+    __ mov(c_rarg1, rax);\n+\n+    __ call(RuntimeAddress(destination));\n+\n+    \/\/ Set an oopmap for the call site.\n+\n+    oop_maps->add_gc_map( __ offset() - start, map);\n+\n+    \/\/ clear last_Java_sp\n+    __ reset_last_Java_frame(false);\n+\n+    __ movptr(rbp, Address(rsp, rbp_off * BytesPerInt));\n+    __ movdbl(j_farg7, Address(rsp, j_farg7_off * BytesPerInt));\n+    __ movdbl(j_farg6, Address(rsp, j_farg6_off * BytesPerInt));\n+    __ movdbl(j_farg5, Address(rsp, j_farg5_off * BytesPerInt));\n+    __ movdbl(j_farg4, Address(rsp, j_farg4_off * BytesPerInt));\n+    __ movdbl(j_farg3, Address(rsp, j_farg3_off * BytesPerInt));\n+    __ movdbl(j_farg2, Address(rsp, j_farg2_off * BytesPerInt));\n+    __ movdbl(j_farg1, Address(rsp, j_farg1_off * BytesPerInt));\n+    __ movdbl(j_farg0, Address(rsp, j_farg0_off * BytesPerInt));\n+\n+    __ movptr(j_rarg0, Address(rsp, j_rarg0_off * BytesPerInt));\n+    __ movptr(j_rarg1, Address(rsp, j_rarg1_off * BytesPerInt));\n+    __ movptr(j_rarg2, Address(rsp, j_rarg2_off * BytesPerInt));\n+    __ movptr(j_rarg3, Address(rsp, j_rarg3_off * BytesPerInt));\n+    __ movptr(j_rarg4, Address(rsp, j_rarg4_off * BytesPerInt));\n+    __ movptr(j_rarg5, Address(rsp, j_rarg5_off * BytesPerInt));\n+    __ movptr(rax, Address(rsp, rax_off * BytesPerInt));\n+\n+    __ addptr(rsp, frame_size_in_bytes-8);\n+\n+    \/\/ check for pending exceptions\n+    Label pending;\n+    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+    __ jcc(Assembler::notEqual, pending);\n+\n+    if (has_res) {\n+      __ get_vm_result(rax, r15_thread);\n+    }\n+\n+    __ ret(0);\n+\n+    __ bind(pending);\n+\n+    __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+    \/\/ -------------\n+    \/\/ make sure all code is generated\n+    masm->flush();\n+\n+    RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_in_words, oop_maps, false);\n+    return stub->entry_point();\n+  }\n+\n@@ -6710,2 +6889,5 @@\n-    StubRoutines::_call_stub_entry =\n-      generate_call_stub(StubRoutines::_call_stub_return_address);\n+    \/\/ Generate these first because they are called from other stubs\n+    StubRoutines::_load_inline_type_fields_in_regs = generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf = generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), \"store_inline_type_fields_to_buf\", true);\n+\n+    StubRoutines::_call_stub_entry = generate_call_stub(StubRoutines::_call_stub_return_address);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":199,"deletions":17,"binary":false,"changes":216,"status":"modified"},{"patch":"@@ -610,4 +610,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->in_24_bit_fp_mode(), C->stub_function() != NULL);\n+  __ verified_entry(C);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -868,3 +868,0 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n@@ -886,1 +883,7 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL);\n+  __ verified_entry(C);\n+  __ bind(*_verified_entry);\n+\n+  if (C->stub_function() == NULL) {\n+    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->nmethod_entry_barrier(&_masm);\n+  }\n@@ -898,6 +901,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -951,23 +948,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    emit_opcode(cbuf, Assembler::REX_W);\n-    if (framesize < 0x80) {\n-      emit_opcode(cbuf, 0x83); \/\/ addq rsp, #framesize\n-      emit_rm(cbuf, 0x3, 0x00, RSP_enc);\n-      emit_d8(cbuf, framesize);\n-    } else {\n-      emit_opcode(cbuf, 0x81); \/\/ addq rsp, #framesize\n-      emit_rm(cbuf, 0x3, 0x00, RSP_enc);\n-      emit_d32(cbuf, framesize);\n-    }\n-  }\n-\n-  \/\/ popq rbp\n-  emit_opcode(cbuf, 0x58 | RBP_enc);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair(), C->output()->sp_inc_offset());\n@@ -991,6 +968,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1532,0 +1503,30 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+{\n+  MacroAssembler _masm(&cbuf);\n+  if (!_verified) {  \n+    uint insts_size = cbuf.insts_size();\n+    if (UseCompressedClassPointers) {\n+      __ load_klass(rscratch1, j_rarg0, rscratch2);\n+      __ cmpptr(rax, rscratch1);\n+    } else {\n+      __ cmpptr(rax, Address(j_rarg0, oopDesc::klass_offset_in_bytes()));\n+    }\n+    __ jump_cc(Assembler::notEqual, RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  } else {\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    __ jmp(*_verified_entry);\n+  }\n+}\n+\n@@ -1574,7 +1575,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n-\n@@ -3867,0 +3861,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -4209,1 +4219,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -6663,0 +6673,13 @@\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -10756,1 +10779,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -10759,3 +10782,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);\n@@ -10764,1 +10787,0 @@\n-    $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n@@ -10778,2 +10800,59 @@\n-       $$emit$$\"mov     rdi,rax\\n\\t\"\n-       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n@@ -10782,2 +10861,2 @@\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n@@ -10790,1 +10869,1 @@\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n@@ -10809,2 +10888,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n@@ -10815,1 +10894,1 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -10818,3 +10897,3 @@\n-  predicate(((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);\n@@ -10824,1 +10903,0 @@\n-       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n@@ -10828,2 +10906,49 @@\n-       $$emit$$\"mov     rdi,rax\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val, \n+                        Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n@@ -10832,2 +10957,2 @@\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n@@ -10840,1 +10965,1 @@\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n@@ -10854,1 +10979,0 @@\n-       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n@@ -10859,2 +10983,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register, \n+                 $tmp$$XMMRegister, true, true);\n@@ -12418,0 +12542,15 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == NULL);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -12420,0 +12559,1 @@\n+  predicate(n->as_Call()->entry_point() != NULL);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":212,"deletions":72,"binary":false,"changes":284,"status":"modified"},{"patch":"@@ -315,1 +315,1 @@\n-void ciReturnTypeEntry::translate_type_data_from(const ReturnTypeEntry* ret) {\n+void ciSingleTypeEntry::translate_type_data_from(const SingleTypeEntry* ret) {\n@@ -320,1 +320,1 @@\n-    set_type(ReturnTypeEntry::with_status((Klass*)NULL, k));\n+    set_type(SingleTypeEntry::with_status((Klass*)NULL, k));\n@@ -371,0 +371,4 @@\n+  case DataLayout::array_load_store_data_tag:\n+    return new ciArrayLoadStoreData(data_layout);\n+  case DataLayout::acmp_data_tag:\n+    return new ciACmpData(data_layout);\n@@ -847,1 +851,1 @@\n-void ciReturnTypeEntry::print_data_on(outputStream* st) const {\n+void ciSingleTypeEntry::print_data_on(outputStream* st) const {\n@@ -920,0 +924,21 @@\n+\n+void ciArrayLoadStoreData::print_data_on(outputStream* st, const char* extra) const {\n+  print_shared(st, \"ciArrayLoadStoreData\", extra);\n+  tab(st, true);\n+  st->print(\"array\");\n+  array()->print_data_on(st);\n+  tab(st, true);\n+  st->print(\"element\");\n+  element()->print_data_on(st);\n+}\n+\n+void ciACmpData::print_data_on(outputStream* st, const char* extra) const {\n+  BranchData::print_data_on(st, extra);\n+  st->cr();\n+  tab(st, true);\n+  st->print(\"left\");\n+  left()->print_data_on(st);\n+  tab(st, true);\n+  st->print(\"right\");\n+  right()->print_data_on(st);\n+}\n","filename":"src\/hotspot\/share\/ci\/ciMethodData.cpp","additions":28,"deletions":3,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -127,1 +127,1 @@\n-class ciReturnTypeEntry : public ReturnTypeEntry, ciTypeEntries {\n+class ciSingleTypeEntry : public SingleTypeEntry, ciTypeEntries {\n@@ -129,1 +129,1 @@\n-  void translate_type_data_from(const ReturnTypeEntry* ret);\n+  void translate_type_data_from(const SingleTypeEntry* ret);\n@@ -149,1 +149,1 @@\n-  ciReturnTypeEntry* ret() const { return (ciReturnTypeEntry*)CallTypeData::ret(); }\n+  ciSingleTypeEntry* ret() const { return (ciSingleTypeEntry*)CallTypeData::ret(); }\n@@ -261,1 +261,1 @@\n-  ciReturnTypeEntry* ret() const { return (ciReturnTypeEntry*)VirtualCallTypeData::ret(); }\n+  ciSingleTypeEntry* ret() const { return (ciSingleTypeEntry*)VirtualCallTypeData::ret(); }\n@@ -365,0 +365,34 @@\n+class ciArrayLoadStoreData : public ArrayLoadStoreData {\n+public:\n+  ciArrayLoadStoreData(DataLayout* layout) : ArrayLoadStoreData(layout) {}\n+\n+  ciSingleTypeEntry* array() const { return (ciSingleTypeEntry*)ArrayLoadStoreData::array(); }\n+  ciSingleTypeEntry* element() const { return (ciSingleTypeEntry*)ArrayLoadStoreData::element(); }\n+\n+  virtual void translate_from(const ProfileData* data) {\n+    array()->translate_type_data_from(data->as_ArrayLoadStoreData()->array());\n+    element()->translate_type_data_from(data->as_ArrayLoadStoreData()->element());\n+  }\n+\n+#ifndef PRODUCT\n+  void print_data_on(outputStream* st, const char* extra = NULL) const;\n+#endif\n+};\n+\n+class ciACmpData : public ACmpData {\n+public:\n+  ciACmpData(DataLayout* layout) : ACmpData(layout) {}\n+\n+  ciSingleTypeEntry* left() const { return (ciSingleTypeEntry*)ACmpData::left(); }\n+  ciSingleTypeEntry* right() const { return (ciSingleTypeEntry*)ACmpData::right(); }\n+\n+  virtual void translate_from(const ProfileData* data) {\n+    left()->translate_type_data_from(data->as_ACmpData()->left());\n+    right()->translate_type_data_from(data->as_ACmpData()->right());\n+  }\n+\n+#ifndef PRODUCT\n+  void print_data_on(outputStream* st, const char* extra = NULL) const;\n+#endif\n+};\n+\n","filename":"src\/hotspot\/share\/ci\/ciMethodData.hpp","additions":38,"deletions":4,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -744,0 +745,1 @@\n+\n@@ -790,26 +792,79 @@\n-  \/\/ staticfield <klass> <name> <signature> <value>\n-  \/\/\n-  \/\/ Initialize a class and fill in the value for a static field.\n-  \/\/ This is useful when the compile was dependent on the value of\n-  \/\/ static fields but it's impossible to properly rerun the static\n-  \/\/ initializer.\n-  void process_staticfield(TRAPS) {\n-    InstanceKlass* k = (InstanceKlass *)parse_klass(CHECK);\n-\n-    if (k == NULL || ReplaySuppressInitializers == 0 ||\n-        (ReplaySuppressInitializers == 2 && k->class_loader() == NULL)) {\n-      return;\n-    }\n-\n-    assert(k->is_initialized(), \"must be\");\n-\n-    const char* field_name = parse_escaped_string();\n-    const char* field_signature = parse_string();\n-    fieldDescriptor fd;\n-    Symbol* name = SymbolTable::new_symbol(field_name);\n-    Symbol* sig = SymbolTable::new_symbol(field_signature);\n-    if (!k->find_local_field(name, sig, &fd) ||\n-        !fd.is_static() ||\n-        fd.has_initial_value()) {\n-      report_error(field_name);\n-      return;\n+  class InlineTypeFieldInitializer : public FieldClosure {\n+    oop _vt;\n+    CompileReplay* _replay;\n+  public:\n+    InlineTypeFieldInitializer(oop vt, CompileReplay* replay)\n+  : _vt(vt), _replay(replay) {}\n+\n+    void do_field(fieldDescriptor* fd) {\n+      BasicType bt = fd->field_type();\n+      const char* string_value = bt != T_INLINE_TYPE ? _replay->parse_escaped_string() : NULL;\n+      switch (bt) {\n+      case T_BYTE: {\n+        int value = atoi(string_value);\n+        _vt->byte_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_BOOLEAN: {\n+        int value = atoi(string_value);\n+        _vt->bool_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_SHORT: {\n+        int value = atoi(string_value);\n+        _vt->short_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_CHAR: {\n+        int value = atoi(string_value);\n+        _vt->char_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_INT: {\n+        int value = atoi(string_value);\n+        _vt->int_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_LONG: {\n+        jlong value;\n+        if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n+          fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n+          break;\n+        }\n+        _vt->long_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_FLOAT: {\n+        float value = atof(string_value);\n+        _vt->float_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        double value = atof(string_value);\n+        _vt->double_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_ARRAY:\n+      case T_OBJECT: {\n+        Thread* THREAD = Thread::current();\n+        bool res = _replay->process_staticfield_reference(string_value, _vt, fd, THREAD);\n+        assert(res, \"should succeed for arrays & objects\");\n+        break;\n+      }\n+      case T_INLINE_TYPE: {\n+        InlineKlass* vk = InlineKlass::cast(fd->field_holder()->get_inline_type_field_klass(fd->index()));\n+        if (fd->is_inlined()) {\n+          int field_offset = fd->offset() - vk->first_field_offset();\n+          oop obj = (oop)(cast_from_oop<address>(_vt) + field_offset);\n+          InlineTypeFieldInitializer init_fields(obj, _replay);\n+          vk->do_nonstatic_fields(&init_fields);\n+        } else {\n+          oop value = vk->allocate_instance(Thread::current());\n+          _vt->obj_field_put(fd->offset(), value);\n+        }\n+        break;\n+      }\n+      default: {\n+        fatal(\"Unhandled type: %s\", type2name(bt));\n+      }\n+      }\n@@ -817,0 +872,1 @@\n+  };\n@@ -818,1 +874,1 @@\n-    oop java_mirror = k->java_mirror();\n+  bool process_staticfield_reference(const char* field_signature, oop java_mirror, fieldDescriptor* fd, TRAPS) {\n@@ -825,4 +881,2 @@\n-        ArrayKlass* kelem = (ArrayKlass *)parse_klass(CHECK);\n-        if (kelem == NULL) {\n-          return;\n-        }\n+        Klass* k = resolve_klass(field_signature, CHECK_(true));\n+        ArrayKlass* kelem = (ArrayKlass *)k;\n@@ -838,1 +892,1 @@\n-        value = kelem->multi_allocate(rank, dims, CHECK);\n+        value = kelem->multi_allocate(rank, dims, CHECK_(true));\n@@ -841,1 +895,1 @@\n-          value = oopFactory::new_byteArray(length, CHECK);\n+          value = oopFactory::new_byteArray(length, CHECK_(true));\n@@ -843,1 +897,1 @@\n-          value = oopFactory::new_boolArray(length, CHECK);\n+          value = oopFactory::new_boolArray(length, CHECK_(true));\n@@ -845,1 +899,1 @@\n-          value = oopFactory::new_charArray(length, CHECK);\n+          value = oopFactory::new_charArray(length, CHECK_(true));\n@@ -847,1 +901,1 @@\n-          value = oopFactory::new_shortArray(length, CHECK);\n+          value = oopFactory::new_shortArray(length, CHECK_(true));\n@@ -849,1 +903,1 @@\n-          value = oopFactory::new_floatArray(length, CHECK);\n+          value = oopFactory::new_floatArray(length, CHECK_(true));\n@@ -851,1 +905,1 @@\n-          value = oopFactory::new_doubleArray(length, CHECK);\n+          value = oopFactory::new_doubleArray(length, CHECK_(true));\n@@ -853,1 +907,1 @@\n-          value = oopFactory::new_intArray(length, CHECK);\n+          value = oopFactory::new_intArray(length, CHECK_(true));\n@@ -855,1 +909,1 @@\n-          value = oopFactory::new_longArray(length, CHECK);\n+          value = oopFactory::new_longArray(length, CHECK_(true));\n@@ -858,2 +912,6 @@\n-          Klass* kelem = resolve_klass(field_signature + 1, CHECK);\n-          value = oopFactory::new_objArray(kelem, length, CHECK);\n+          Klass* kelem = resolve_klass(field_signature + 1, CHECK_(true));\n+          value = oopFactory::new_objArray(kelem, length, CHECK_(true));\n+        } else if (field_signature[0] == JVM_SIGNATURE_ARRAY &&\n+                   field_signature[1] == JVM_SIGNATURE_INLINE_TYPE) {\n+          Klass* kelem = resolve_klass(field_signature + 1, CHECK_(true));\n+          value = oopFactory::new_flatArray(kelem, length, CHECK_(true));\n@@ -864,0 +922,86 @@\n+      java_mirror->obj_field_put(fd->offset(), value);\n+      return true;\n+    } else if (strcmp(field_signature, \"Ljava\/lang\/String;\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      Handle value = java_lang_String::create_from_str(string_value, CHECK_(true));\n+      java_mirror->obj_field_put(fd->offset(), value());\n+      return true;\n+    } else if (field_signature[0] == 'L') {\n+      const char* instance = parse_escaped_string();\n+      Klass* k = resolve_klass(instance, CHECK_(true));\n+      oop value = InstanceKlass::cast(k)->allocate_instance(CHECK_(true));\n+      java_mirror->obj_field_put(fd->offset(), value);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  \/\/ Initialize a class and fill in the value for a static field.\n+  \/\/ This is useful when the compile was dependent on the value of\n+  \/\/ static fields but it's impossible to properly rerun the static\n+  \/\/ initializer.\n+  void process_staticfield(TRAPS) {\n+    InstanceKlass* k = (InstanceKlass *)parse_klass(CHECK);\n+\n+    if (k == NULL || ReplaySuppressInitializers == 0 ||\n+        (ReplaySuppressInitializers == 2 && k->class_loader() == NULL)) {\n+      return;\n+    }\n+\n+    assert(k->is_initialized(), \"must be\");\n+\n+    const char* field_name = parse_escaped_string();\n+    const char* field_signature = parse_string();\n+    fieldDescriptor fd;\n+    Symbol* name = SymbolTable::new_symbol(field_name);\n+    Symbol* sig = SymbolTable::new_symbol(field_signature);\n+    if (!k->find_local_field(name, sig, &fd) ||\n+        !fd.is_static() ||\n+        fd.has_initial_value()) {\n+      report_error(field_name);\n+      return;\n+    }\n+\n+    oop java_mirror = k->java_mirror();\n+    if (strcmp(field_signature, \"I\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->int_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"B\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->byte_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"C\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->char_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"S\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->short_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"Z\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->bool_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"J\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      jlong value;\n+      if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n+        fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n+        return;\n+      }\n+      java_mirror->long_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"F\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      float value = atof(string_value);\n+      java_mirror->float_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"D\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      double value = atof(string_value);\n+      java_mirror->double_field_put(fd.offset(), value);\n+    } else if (field_signature[0] == JVM_SIGNATURE_INLINE_TYPE) {\n+      Klass* kelem = resolve_klass(field_signature, CHECK);\n+      InlineKlass* vk = InlineKlass::cast(kelem);\n+      oop value = vk->allocate_instance(CHECK);\n+      InlineTypeFieldInitializer init_fields(value, this);\n+      vk->do_nonstatic_fields(&init_fields);\n@@ -866,37 +1010,2 @@\n-      const char* string_value = parse_escaped_string();\n-      if (strcmp(field_signature, \"I\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->int_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"B\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->byte_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"C\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->char_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"S\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->short_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"Z\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->bool_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"J\") == 0) {\n-        jlong value;\n-        if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n-          fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n-          return;\n-        }\n-        java_mirror->long_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"F\") == 0) {\n-        float value = atof(string_value);\n-        java_mirror->float_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"D\") == 0) {\n-        double value = atof(string_value);\n-        java_mirror->double_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"Ljava\/lang\/String;\") == 0) {\n-        Handle value = java_lang_String::create_from_str(string_value, CHECK);\n-        java_mirror->obj_field_put(fd.offset(), value());\n-      } else if (field_signature[0] == JVM_SIGNATURE_CLASS) {\n-        Klass* k = resolve_klass(string_value, CHECK);\n-        oop value = InstanceKlass::cast(k)->allocate_instance(CHECK);\n-        java_mirror->obj_field_put(fd.offset(), value);\n-      } else {\n+      bool res = process_staticfield_reference(field_signature, java_mirror, &fd, CHECK);\n+      if (!res)  {\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":188,"deletions":79,"binary":false,"changes":267,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+\n@@ -53,0 +54,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -85,0 +87,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -138,0 +141,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        60\n+\n@@ -177,1 +182,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -510,1 +515,8 @@\n-        cp->unresolved_klass_at_put(index, class_index, num_klasses++);\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n+        if (name->is_Q_signature()) {\n+          cp->unresolved_qdescriptor_at_put(index, class_index, num_klasses++);\n+        } else {\n+          cp->unresolved_klass_at_put(index, class_index, num_klasses++);\n+        }\n@@ -766,2 +778,2 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+            if (name != vmSymbols::object_initializer_name()) {\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -774,1 +786,12 @@\n-              if (name == vmSymbols::object_initializer_name()) {\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else if (!signature->is_void_method_signature()\n+                         && ref_kind == JVM_REF_invokeStatic) {\n+                \/\/ also OK, could be a static factory call\n+              } else {\n@@ -934,3 +957,4 @@\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n+                                       bool is_inline_type,\n@@ -938,0 +962,7 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n+                                       bool* const is_declared_atomic,\n@@ -944,1 +975,1 @@\n-    _local_interfaces = Universe::the_empty_instance_klass_array();\n+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(0);\n@@ -947,3 +978,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);\n-\n-    int index;\n+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(itfs_len);\n+    int index = 0;\n@@ -967,1 +997,1 @@\n-        \/\/ Call resolve_super so classcircularity is checked\n+        \/\/ Call resolve_super so class circularity is checked\n@@ -985,1 +1015,14 @@\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+      InstanceKlass* ik = InstanceKlass::cast(interf);\n+      if (is_inline_type && ik->invalid_inline_super()) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(\n+          THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_IncompatibleClassChangeError(),\n+          \"Inline type %s attempts to implement interface java.lang.IdentityObject\",\n+          _class_name->as_klass_external_name());\n+        return;\n+      }\n+      if (ik->invalid_inline_super()) {\n+        set_invalid_inline_super();\n+      }\n+      if (ik->has_nonstatic_concrete_methods()) {\n@@ -988,1 +1031,7 @@\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      if (ik->is_declared_atomic()) {\n+        *is_declared_atomic = true;\n+      }\n+      if (ik->name() == vmSymbols::java_lang_IdentityObject()) {\n+        _implements_identityObject = true;\n+      }\n+      _temp_local_interfaces->append(ik);\n@@ -1006,1 +1055,1 @@\n-        const InstanceKlass* const k = _local_interfaces->at(index);\n+        const InstanceKlass* const k = _temp_local_interfaces->at(index);\n@@ -1491,0 +1540,1 @@\n+  STATIC_INLINE,        \/\/ inline type field\n@@ -1496,0 +1546,1 @@\n+  NONSTATIC_INLINE,\n@@ -1515,6 +1566,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  NONSTATIC_OOP,       \/\/ T_INLINE_TYPE = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20,\n@@ -1535,6 +1587,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  STATIC_OOP,          \/\/ T_INLINE_TYPE = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20\n@@ -1543,1 +1596,1 @@\n-static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type) {\n+static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type, bool is_inline_type) {\n@@ -1547,0 +1600,3 @@\n+  if (is_inline_type) {\n+    result = is_static ? STATIC_INLINE : NONSTATIC_INLINE;\n+  }\n@@ -1560,2 +1616,2 @@\n-  FieldAllocationType update(bool is_static, BasicType type) {\n-    FieldAllocationType atype = basic_type_to_atype(is_static, type);\n+  FieldAllocationType update(bool is_static, BasicType type, bool is_inline_type) {\n+    FieldAllocationType atype = basic_type_to_atype(is_static, type, is_inline_type);\n@@ -1575,0 +1631,1 @@\n+                                   bool is_inline_type,\n@@ -1597,1 +1654,5 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);\n@@ -1627,0 +1688,1 @@\n+  int instance_fields_count = 0;\n@@ -1631,0 +1693,4 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, is_interface, is_inline_type, CHECK);\n@@ -1632,2 +1698,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1649,0 +1713,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1708,1 +1773,1 @@\n-    const FieldAllocationType atype = fac->update(is_static, type);\n+    const FieldAllocationType atype = fac->update(is_static, type, type == T_INLINE_TYPE);\n@@ -1753,1 +1818,1 @@\n-      const FieldAllocationType atype = fac->update(false, type);\n+      const FieldAllocationType atype = fac->update(false, type, false);\n@@ -1759,0 +1824,29 @@\n+  if (is_inline_type) {\n+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);\n+    field->initialize(JVM_ACC_FIELD_INTERNAL | JVM_ACC_STATIC,\n+                      (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(default_value_name)),\n+                      (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(object_signature)),\n+                      0);\n+    const BasicType type = Signature::basic_type(vmSymbols::object_signature());\n+    const FieldAllocationType atype = fac->update(true, type, false);\n+    field->set_allocation_type(atype);\n+    index++;\n+  }\n+\n+  if (is_inline_type && instance_fields_count == 0) {\n+    _is_empty_inline_type = true;\n+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);\n+    field->initialize(JVM_ACC_FIELD_INTERNAL,\n+        (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(empty_marker_name)),\n+        (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(byte_signature)),\n+        0);\n+    const BasicType type = Signature::basic_type(vmSymbols::byte_signature());\n+    const FieldAllocationType atype = fac->update(false, type, false);\n+    field->set_allocation_type(atype);\n+    index++;\n+  }\n+\n+  if (instance_fields_count > 0) {\n+    _has_nonstatic_fields = true;\n+  }\n+\n@@ -2076,0 +2170,5 @@\n+  const char* class_note = \"\";\n+  if (is_inline_type() && name == vmSymbols::object_initializer_name()) {\n+    class_note = \" (an inline class)\";\n+  }\n+\n@@ -2079,2 +2178,2 @@\n-      \"%s \\\"%s\\\" in class %s has illegal signature \\\"%s\\\"\", type,\n-      name->as_C_string(), _class_name->as_C_string(), sig->as_C_string());\n+      \"%s \\\"%s\\\" in class %s%s has illegal signature \\\"%s\\\"\", type,\n+      name->as_C_string(), _class_name->as_C_string(), class_note, sig->as_C_string());\n@@ -2348,0 +2447,1 @@\n+                                      bool is_inline_type,\n@@ -2389,1 +2489,1 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n+    verify_legal_method_modifiers(flags, is_interface, is_inline_type, name, CHECK_NULL);\n@@ -2392,3 +2492,48 @@\n-  if (name == vmSymbols::object_initializer_name() && is_interface) {\n-    classfile_parse_error(\"Interface cannot have a method named <init>, class file %s\", THREAD);\n-    return NULL;\n+  if (name == vmSymbols::object_initializer_name()) {\n+    if (is_interface) {\n+      classfile_parse_error(\"Interface cannot have a method named <init>, class file %s\", THREAD);\n+      return NULL;\n+    } else if (!is_inline_type && signature->is_void_method_signature()) {\n+      \/\/ OK, a constructor\n+    } else if (is_inline_type && !signature->is_void_method_signature()) {\n+      \/\/ also OK, a static factory, as long as the return value is good\n+      bool ok = false;\n+      SignatureStream ss((Symbol*) signature, true);\n+      while (!ss.at_return_type())  ss.next();\n+      if (ss.is_reference()) {\n+        Symbol* ret = ss.as_symbol();\n+        const Symbol* required = class_name();\n+        if (is_hidden()) {\n+          \/\/ The original class name in hidden classes gets changed.  So using\n+          \/\/ the original name in the return type is no longer valid.\n+          \/\/ Note that expecting the return type for inline hidden class factory\n+          \/\/ methods to be java.lang.Object works around a JVM Spec issue for\n+          \/\/ hidden classes.\n+          required = vmSymbols::java_lang_Object();\n+        }\n+        ok = (ret == required);\n+      }\n+      if (!ok) {\n+        throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+      }\n+    } else {\n+      \/\/ not OK, so throw the same error as in verify_legal_method_signature.\n+      throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+    }\n+    \/\/ A declared <init> method must always be either a non-static\n+    \/\/ object constructor, with a void return, or else it must be a\n+    \/\/ static factory method, with a non-void return.  No other\n+    \/\/ definition of <init> is possible.\n+    \/\/\n+    \/\/ The verifier (in verify_invoke_instructions) will inspect the\n+    \/\/ signature of any attempt to invoke <init>, and ensures that it\n+    \/\/ returns non-void if and only if it is being invoked by\n+    \/\/ invokestatic, and void if and only if it is being invoked by\n+    \/\/ invokespecial.\n+    \/\/\n+    \/\/ When a symbolic reference to <init> is resolved for a\n+    \/\/ particular invocation mode (special or static), the mode is\n+    \/\/ matched to the JVM_ACC_STATIC modifier of the <init> method.\n+    \/\/ Thus, it is impossible to statically invoke a constructor, and\n+    \/\/ impossible to \"new + invokespecial\" a static factory, either\n+    \/\/ through bytecode or through reflection.\n@@ -2962,0 +3107,1 @@\n+                                    bool is_inline_type,\n@@ -2986,0 +3132,1 @@\n+                                    is_inline_type,\n@@ -3253,2 +3400,2 @@\n-    \/\/ Access flags\n-    jint flags;\n+\n+    jint recognized_modifiers = RECOGNIZED_INNER_CLASS_MODIFIERS;\n@@ -3257,3 +3404,1 @@\n-      flags = cfs->get_u2_fast() & (RECOGNIZED_INNER_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-    } else {\n-      flags = cfs->get_u2_fast() & RECOGNIZED_INNER_CLASS_MODIFIERS;\n+      recognized_modifiers |= JVM_ACC_MODULE;\n@@ -3261,0 +3406,8 @@\n+    \/\/ JVM_ACC_INLINE is defined for class file version 55 and later\n+    if (supports_inline_types()) {\n+      recognized_modifiers |= JVM_ACC_INLINE;\n+    }\n+\n+    \/\/ Access flags\n+    jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+\n@@ -3643,3 +3796,2 @@\n-  return _major_version == JVM_CLASSFILE_MAJOR_VERSION &&\n-         _minor_version == JAVA_PREVIEW_MINOR_VERSION &&\n-         Arguments::enable_preview();\n+  \/\/ temporarily disable the sealed type preview feature check\n+  return _major_version == JVM_CLASSFILE_MAJOR_VERSION;\n@@ -4115,1 +4267,2 @@\n-    check_property(_class_name == vmSymbols::java_lang_Object(),\n+    check_property(_class_name == vmSymbols::java_lang_Object()\n+                   || (_access_flags.get_flags() & JVM_ACC_INLINE),\n@@ -4258,0 +4411,19 @@\n+void ClassFileParser::throwInlineTypeLimitation(THREAD_AND_LOCATION_DECL,\n+                                                const char* msg,\n+                                                const Symbol* name,\n+                                                const Symbol* sig) const {\n+\n+  ResourceMark rm(THREAD);\n+  if (name == NULL || sig == NULL) {\n+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"class: %s - %s\", _class_name->as_C_string(), msg);\n+  }\n+  else {\n+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"\\\"%s\\\" sig: \\\"%s\\\" class: %s - %s\", name->as_C_string(), sig->as_C_string(),\n+        _class_name->as_C_string(), msg);\n+  }\n+}\n+\n@@ -4291,0 +4463,5 @@\n+      if (ik->is_inline_klass()) {\n+        Thread *THREAD = Thread::current();\n+        throwInlineTypeLimitation(THREAD_AND_LOCATION, \"Inline Types do not support Cloneable\");\n+        return;\n+      }\n@@ -4331,0 +4508,5 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 55 and later\n+  return _major_version >= JAVA_11_VERSION;\n+}\n+\n@@ -4374,3 +4556,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4397,0 +4580,5 @@\n+\n+    if (length == 1 && result->at(0) == SystemDictionary::IdentityObject_klass()) {\n+      return Universe::the_single_IdentityObject_klass_array();\n+    }\n+\n@@ -4610,0 +4798,1 @@\n+  const bool is_inline_type = (flags & JVM_ACC_INLINE) != 0;\n@@ -4611,0 +4800,1 @@\n+  assert(supports_inline_types() || !is_inline_type, \"JVM_ACC_INLINE should not be set\");\n@@ -4621,0 +4811,11 @@\n+  if (is_inline_type && !EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    Exceptions::fthrow(\n+      THREAD_AND_LOCATION,\n+      vmSymbols::java_lang_ClassFormatError(),\n+      \"Class modifier ACC_INLINE in class %s requires option -XX:+EnableValhalla\",\n+      _class_name->as_C_string()\n+    );\n+    return;\n+  }\n+\n@@ -4635,1 +4836,2 @@\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (is_inline_type && (is_interface || is_abstract || is_enum || !is_final))) {\n@@ -4637,0 +4839,2 @@\n+    const char* class_note = \"\";\n+    if (is_inline_type)  class_note = \" (an inline class)\";\n@@ -4640,2 +4844,2 @@\n-      \"Illegal class modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags\n+      \"Illegal class modifiers in class %s%s: 0x%X\",\n+      _class_name->as_C_string(), class_note, flags\n@@ -4711,0 +4915,1 @@\n+                                                   bool is_inline_type,\n@@ -4735,0 +4940,4 @@\n+    } else {\n+      if (is_inline_type && !is_static && !is_final) {\n+        is_illegal = true;\n+      }\n@@ -4751,0 +4960,1 @@\n+                                                    bool is_inline_type,\n@@ -4771,0 +4981,1 @@\n+  const char* class_note = \"\";\n@@ -4805,1 +5016,1 @@\n-        if (is_static || is_final || is_synchronized || is_native ||\n+        if (is_final || is_synchronized || is_native ||\n@@ -4809,0 +5020,9 @@\n+        if (!is_static && !is_inline_type) {\n+          \/\/ OK, an object constructor in a regular class\n+        } else if (is_static && is_inline_type) {\n+          \/\/ OK, a static init factory in an inline class\n+        } else {\n+          \/\/ but no other combinations are allowed\n+          is_illegal = true;\n+          class_note = (is_inline_type ? \" (an inline class)\" : \" (not an inline class)\");\n+        }\n@@ -4810,4 +5030,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || is_strict)))) {\n-            is_illegal = true;\n+        if (is_inline_type && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (an inline class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || is_strict)))) {\n+              is_illegal = true;\n+            }\n@@ -4825,2 +5050,2 @@\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+      \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+      name->as_C_string(), _class_name->as_C_string(), class_note, flags);\n@@ -4984,1 +5209,10 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_INLINE_TYPE:\n+      \/\/ Can't enable this check until JDK upgrades the bytecode generators\n+      \/\/ if (_major_version < CONSTANT_CLASS_DESCRIPTORS ) {\n+      \/\/   classfile_parse_error(\"Class name contains illegal Q-signature \"\n+      \/\/                                    \"in descriptor in class file %s\",\n+      \/\/                                    CHECK_0);\n+      \/\/ }\n+      \/\/ fall through\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -4995,1 +5229,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -5051,0 +5285,3 @@\n+    } else if (_major_version >= CONSTANT_CLASS_DESCRIPTORS && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; and Q...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -5148,0 +5385,3 @@\n+  if (!supports_inline_types() && (signature->is_Q_signature() || signature->is_Q_array_signature())) {\n+    throwIllegalSignature(\"Field\", name, signature, CHECK);\n+  }\n@@ -5200,1 +5440,1 @@\n-        \/\/ All internal methods must return void\n+        \/\/ All constructor methods must return void\n@@ -5204,0 +5444,16 @@\n+        \/\/ All static init methods must return the current class\n+        if ((length >= 3) && (p[length-1] == JVM_SIGNATURE_ENDCLASS)\n+            && name == vmSymbols::object_initializer_name()) {\n+          nextp = skip_over_field_signature(p, true, length, CHECK_0);\n+          if (nextp && ((int)length == (nextp - p))) {\n+            \/\/ The actual class will be checked against current class\n+            \/\/ when the method is defined (see parse_method).\n+            \/\/ A reference to a static init with a bad return type\n+            \/\/ will load and verify OK, but will fail to link.\n+            return args_size;\n+          }\n+        }\n+        \/\/ The distinction between static factory methods and\n+        \/\/ constructors depends on the JVM_ACC_STATIC modifier.\n+        \/\/ This distinction must be reflected in a void or non-void\n+        \/\/ return. For declared methods, the check is in parse_method.\n@@ -5361,0 +5617,6 @@\n+  if (ik->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    oop val = ik->allocate_instance(CHECK_NULL);\n+    vk->set_default_value(val);\n+  }\n+\n@@ -5364,0 +5626,34 @@\n+\/\/ Return true if the specified class is not a valid super class for an inline type.\n+\/\/ A valid super class for an inline type is abstract, has no instance fields,\n+\/\/ does not implement interface java.lang.IdentityObject (checked elsewhere), has\n+\/\/ an empty body-less no-arg constructor, and no synchronized instance methods.\n+\/\/ This function doesn't check if the class's super types are invalid.  Those checks\n+\/\/ are done elsewhere.  The final determination of whether or not a class is an\n+\/\/ invalid super type for an inline class is done in fill_instance_klass().\n+bool ClassFileParser::is_invalid_super_for_inline_type() {\n+  if (class_name() == vmSymbols::java_lang_IdentityObject()) {\n+    return true;\n+  }\n+  if (is_interface() || class_name() == vmSymbols::java_lang_Object()) {\n+    return false;\n+  }\n+  if (!access_flags().is_abstract() || _has_nonstatic_fields) {\n+    return true;\n+  } else {\n+    \/\/ Look at each method\n+    for (int x = 0; x < _methods->length(); x++) {\n+      const Method* const method = _methods->at(x);\n+      if (method->is_synchronized() && !method->is_static()) {\n+        return true;\n+\n+      } else if (method->name() == vmSymbols::object_initializer_name()) {\n+        if (method->signature() != vmSymbols::void_method_signature() ||\n+            !method->is_vanilla_constructor()) {\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -5396,0 +5692,12 @@\n+  if (_field_info->_is_naturally_atomic && ik->is_inline_klass()) {\n+    ik->set_is_naturally_atomic();\n+  }\n+\n+  if (this->_invalid_inline_super) {\n+    ik->set_invalid_inline_super();\n+  }\n+\n+  if (_has_injected_identityObject) {\n+    ik->set_has_injected_identityObject();\n+  }\n+\n@@ -5397,1 +5705,1 @@\n-  ik->set_static_oop_field_count(_fac->count[STATIC_OOP]);\n+  ik->set_static_oop_field_count(_fac->count[STATIC_OOP] + _fac->count[STATIC_INLINE]);\n@@ -5447,0 +5755,3 @@\n+  if (_is_declared_atomic) {\n+    ik->set_is_declared_atomic();\n+  }\n@@ -5558,0 +5869,37 @@\n+  bool all_fields_empty = true;\n+  int nfields = ik->java_fields_count();\n+  if (ik->is_inline_klass()) nfields++;\n+  for (int i = 0; i < nfields; i++) {\n+    if (((ik->field_access_flags(i) & JVM_ACC_STATIC) == 0)) {\n+      if (ik->field_is_inline_type(i)) {\n+        Symbol* klass_name = ik->field_signature(i)->fundamental_name(CHECK);\n+        \/\/ Inline classes for instance fields must have been pre-loaded\n+        \/\/ Inline classes for static fields might not have been loaded yet\n+        Klass* klass = SystemDictionary::find(klass_name,\n+            Handle(THREAD, ik->class_loader()),\n+            Handle(THREAD, ik->protection_domain()), CHECK);\n+        assert(klass != NULL, \"Just checking\");\n+        assert(klass->access_flags().is_inline_type(), \"Inline type expected\");\n+        ik->set_inline_type_field_klass(i, klass);\n+        klass_name->decrement_refcount();\n+        if (!InlineKlass::cast(klass)->is_empty_inline_type()) { all_fields_empty = false; }\n+      } else {\n+        all_fields_empty = false;\n+      }\n+    } else if (is_inline_type() && ((ik->field_access_flags(i) & JVM_ACC_FIELD_INTERNAL) != 0)) {\n+      InlineKlass::cast(ik)->set_default_value_offset(ik->field_offset(i));\n+    }\n+  }\n+\n+  if (_is_empty_inline_type || (is_inline_type() && all_fields_empty)) {\n+    ik->set_is_empty_inline_type();\n+  }\n+\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_alignment(_alignment);\n+    vk->set_first_field_offset(_first_field_offset);\n+    vk->set_exact_size_in_bytes(_exact_size_in_bytes);\n+    InlineKlass::cast(ik)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -5603,0 +5951,4 @@\n+  if (ik->name() == vmSymbols::java_lang_IdentityObject()) {\n+    Universe::initialize_the_single_IdentityObject_klass_array(ik, CHECK);\n+  }\n+\n@@ -5705,0 +6057,1 @@\n+  _temp_local_interfaces(NULL),\n@@ -5744,0 +6097,9 @@\n+  _has_inline_type_fields(false),\n+  _has_nonstatic_fields(false),\n+  _is_empty_inline_type(false),\n+  _is_naturally_atomic(false),\n+  _is_declared_atomic(false),\n+  _invalid_inline_super(false),\n+  _invalid_identity_super(false),\n+  _implements_identityObject(false),\n+  _has_injected_identityObject(false),\n@@ -5954,2 +6316,1 @@\n-  \/\/ Access flags\n-  jint flags;\n+  jint recognized_modifiers = JVM_RECOGNIZED_CLASS_MODIFIERS;\n@@ -5958,3 +6319,5 @@\n-    flags = stream->get_u2_fast() & (JVM_RECOGNIZED_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-  } else {\n-    flags = stream->get_u2_fast() & JVM_RECOGNIZED_CLASS_MODIFIERS;\n+    recognized_modifiers |= JVM_ACC_MODULE;\n+  }\n+  \/\/ JVM_ACC_INLINE is defined for class file version 55 and later\n+  if (supports_inline_types()) {\n+    recognized_modifiers |= JVM_ACC_INLINE;\n@@ -5963,0 +6326,3 @@\n+  \/\/ Access flags\n+  jint flags = stream->get_u2_fast() & recognized_modifiers;\n+\n@@ -6083,0 +6449,1 @@\n+                   is_inline_type(),\n@@ -6084,0 +6451,1 @@\n+                   &_is_declared_atomic,\n@@ -6086,1 +6454,1 @@\n-  assert(_local_interfaces != NULL, \"invariant\");\n+  assert(_temp_local_interfaces != NULL, \"invariant\");\n@@ -6091,1 +6459,2 @@\n-               _access_flags.is_interface(),\n+               is_interface(),\n+               is_inline_type(),\n@@ -6103,1 +6472,2 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                is_inline_type(),\n@@ -6185,3 +6555,3 @@\n-    check_property(_local_interfaces == Universe::the_empty_instance_klass_array(),\n-                   \"java.lang.Object cannot implement an interface in class file %s\",\n-                   CHECK);\n+    check_property(_temp_local_interfaces->length() == 0,\n+        \"java.lang.Object cannot implement an interface in class file %s\",\n+        CHECK);\n@@ -6192,1 +6562,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -6213,0 +6583,3 @@\n+    if (_super_klass->is_declared_atomic()) {\n+      _is_declared_atomic = true;\n+    }\n@@ -6218,0 +6591,54 @@\n+\n+    \/\/ For an inline class, only java\/lang\/Object or special abstract classes\n+    \/\/ are acceptable super classes.\n+    if (is_inline_type()) {\n+      const InstanceKlass* super_ik = _super_klass;\n+      if (super_ik->invalid_inline_super()) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(\n+          THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_IncompatibleClassChangeError(),\n+          \"inline class %s has an invalid super class %s\",\n+          _class_name->as_klass_external_name(),\n+          _super_klass->external_name());\n+        return;\n+      }\n+    }\n+  }\n+\n+  if (_class_name == vmSymbols::java_lang_NonTearable() && _loader_data->class_loader() == NULL) {\n+    \/\/ This is the original source of this condition.\n+    \/\/ It propagates by inheritance, as if testing \"instanceof NonTearable\".\n+    _is_declared_atomic = true;\n+  } else if (*ForceNonTearable != '\\0') {\n+    \/\/ Allow a command line switch to force the same atomicity property:\n+    const char* class_name_str = _class_name->as_C_string();\n+    if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+      _is_declared_atomic = true;\n+    }\n+  }\n+\n+  \/\/ Set ik->invalid_inline_super field to TRUE if already marked as invalid,\n+  \/\/ if super is marked invalid, or if is_invalid_super_for_inline_type()\n+  \/\/ returns true\n+  if (invalid_inline_super() ||\n+      (_super_klass != NULL && _super_klass->invalid_inline_super()) ||\n+      is_invalid_super_for_inline_type()) {\n+    set_invalid_inline_super();\n+  }\n+\n+  if (!is_inline_type() && invalid_inline_super() && (_super_klass == NULL || !_super_klass->invalid_inline_super())\n+      && !_implements_identityObject && class_name() != vmSymbols::java_lang_IdentityObject()) {\n+    _temp_local_interfaces->append(SystemDictionary::IdentityObject_klass());\n+    _has_injected_identityObject = true;\n+  }\n+  int itfs_len = _temp_local_interfaces->length();\n+  if (itfs_len == 0) {\n+    _local_interfaces = Universe::the_empty_instance_klass_array();\n+  } else if (itfs_len == 1 && _temp_local_interfaces->at(0) == SystemDictionary::IdentityObject_klass()) {\n+    _local_interfaces = Universe::the_single_IdentityObject_klass_array();\n+  } else {\n+    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);\n+    for (int i = 0; i < itfs_len; i++) {\n+      _local_interfaces->at_put(i, _temp_local_interfaces->at(i));\n+    }\n@@ -6219,0 +6646,2 @@\n+  _temp_local_interfaces = NULL;\n+  assert(_local_interfaces != NULL, \"invariant\");\n@@ -6248,1 +6677,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -6254,0 +6683,19 @@\n+\n+  for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {\n+    if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE  && !fs.access_flags().is_static()) {\n+      \/\/ Pre-load inline class\n+      Klass* klass = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+          Handle(THREAD, _loader_data->class_loader()),\n+          _protection_domain, true, CHECK);\n+      assert(klass != NULL, \"Sanity check\");\n+      if (!klass->access_flags().is_inline_type()) {\n+        assert(klass->is_instance_klass(), \"Sanity check\");\n+        ResourceMark rm(THREAD);\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"Class %s expects class %s to be an inline type, but it is not\",\n+                    _class_name->as_C_string(),\n+                    InstanceKlass::cast(klass)->external_name()));\n+      }\n+    }\n+  }\n+\n@@ -6256,2 +6704,9 @@\n-                        _parsed_annotations->is_contended(), _field_info);\n-  lb.build_layout();\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      loader_data(), _protection_domain, _field_info);\n+  lb.build_layout(CHECK);\n+  if (is_inline_type()) {\n+    _alignment = lb.get_alignment();\n+    _first_field_offset = lb.get_first_field_offset();\n+    _exact_size_in_bytes = lb.get_exact_size_in_byte();\n+  }\n+  _has_inline_type_fields = _field_info->_has_inline_fields;\n@@ -6259,1 +6714,1 @@\n-  \/\/ Compute reference typ\n+  \/\/ Compute reference type\n@@ -6261,1 +6716,0 @@\n-\n@@ -6293,0 +6747,1 @@\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":540,"deletions":85,"binary":false,"changes":625,"status":"modified"},{"patch":"@@ -395,1 +395,27 @@\n-    if (k->local_interfaces()->length() != _interfaces->length()) {\n+    int actual_num_interfaces = k->local_interfaces()->length();\n+    int specified_num_interfaces = _interfaces->length();\n+    int expected_num_interfaces, i;\n+\n+    bool identity_object_implemented = false;\n+    bool identity_object_specified = false;\n+    for (i = 0; i < actual_num_interfaces; i++) {\n+      if (k->local_interfaces()->at(i) == SystemDictionary::IdentityObject_klass()) {\n+        identity_object_implemented = true;\n+        break;\n+      }\n+    }\n+    for (i = 0; i < specified_num_interfaces; i++) {\n+      if (lookup_class_by_id(_interfaces->at(i)) == SystemDictionary::IdentityObject_klass()) {\n+        identity_object_specified = true;\n+        break;\n+      }\n+    }\n+\n+    expected_num_interfaces = actual_num_interfaces;\n+    if (identity_object_implemented  && !identity_object_specified) {\n+      \/\/ Backwards compatibility -- older classlists do not know about\n+      \/\/ java.lang.IdentityObject.\n+      expected_num_interfaces--;\n+    }\n+\n+    if (specified_num_interfaces != expected_num_interfaces) {\n@@ -399,1 +425,1 @@\n-            _interfaces->length(), k->local_interfaces()->length());\n+            specified_num_interfaces, expected_num_interfaces);\n@@ -649,0 +675,6 @@\n+  if (interface_name == vmSymbols::java_lang_IdentityObject()) {\n+    \/\/ Backwards compatibility -- older classlists do not know about\n+    \/\/ java.lang.IdentityObject.\n+    return SystemDictionary::IdentityObject_klass();\n+  }\n+\n","filename":"src\/hotspot\/share\/classfile\/classListParser.cpp","additions":34,"deletions":2,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+  f(jdk_internal_vm_jni_SubElementSelector) \\\n@@ -309,0 +310,1 @@\n+  static int component_mirror_offset()     { CHECK_INIT(_component_mirror_offset); }\n@@ -325,2 +327,0 @@\n-  static int component_mirror_offset() { return _component_mirror_offset; }\n-\n@@ -1132,1 +1132,1 @@\n-    MN_IS_CONSTRUCTOR        = 0x00020000, \/\/ constructor\n+    MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, \/\/ constructor\n@@ -1679,1 +1679,0 @@\n-\n@@ -1695,0 +1694,25 @@\n+class jdk_internal_vm_jni_SubElementSelector : AllStatic {\n+ private:\n+  static int _arrayElementType_offset;\n+  static int _subElementType_offset;\n+  static int _offset_offset;\n+  static int _isInlined_offset;\n+  static int _isInlineType_offset;\n+ public:\n+  static Symbol* symbol();\n+  static void compute_offsets();\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+\n+  static oop getArrayElementType(oop obj);\n+  static void setArrayElementType(oop obj, oop type);\n+  static oop getSubElementType(oop obj);\n+  static void setSubElementType(oop obj, oop type);\n+  static int getOffset(oop obj);\n+  static void setOffset(oop obj, int offset);\n+  static bool getIsInlined(oop obj);\n+  static void setIsInlined(oop obj, bool b);\n+  static bool getIsInlineType(oop obj);\n+  static void setIsInlineType(oop obj, bool b);\n+};\n+\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":28,"deletions":4,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -509,1 +509,2 @@\n-    if (klass->is_array_klass()) {\n+    \/\/ CMH: Valhalla flat arrays can split this work up, but for now, doesn't\n+    if (klass->is_array_klass() && !klass->is_flatArray_klass()) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -285,0 +285,1 @@\n+  oop obj_buffer_allocate(Klass* klass, int size, TRAPS); \/\/ doesn't clear memory\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -38,0 +38,5 @@\n+inline oop CollectedHeap::obj_buffer_allocate(Klass* klass, int size, TRAPS) {\n+  ObjBufferAllocator allocator(klass, size, THREAD);\n+  return allocator.allocate();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -834,1 +834,1 @@\n-    Node* offset = phase->igvn().MakeConX(in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n+    Node* offset = phase->MakeConX(in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n@@ -886,1 +886,1 @@\n-    phase->igvn().replace_node(ac, call);\n+    phase->replace_node(ac, call);\n@@ -912,1 +912,1 @@\n-void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* n) const {\n+void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseIterGVN* igvn, Node* n) const {\n@@ -914,1 +914,1 @@\n-    shenandoah_eliminate_wb_pre(n, &macro->igvn());\n+    shenandoah_eliminate_wb_pre(n, igvn);\n@@ -1047,1 +1047,1 @@\n-    uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain()->cnt();\n+    uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain_sig()->cnt();\n@@ -1141,1 +1141,1 @@\n-        uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain()->cnt();\n+        uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain_sig()->cnt();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -458,1 +458,1 @@\n-        const TypeTuple* args = n->as_Call()->_tf->domain();\n+        const TypeTuple* args = n->as_Call()->_tf->domain_sig();\n@@ -577,1 +577,1 @@\n-      uint stop = n->is_Call() ? n->as_Call()->tf()->domain()->cnt() : n->req();\n+      uint stop = n->is_Call() ? n->as_Call()->tf()->domain_sig()->cnt() : n->req();\n@@ -797,6 +797,5 @@\n-        CallProjections projs;\n-        c->as_Call()->extract_projections(&projs, true, false);\n-        if (projs.fallthrough_memproj != NULL) {\n-          if (projs.fallthrough_memproj->adr_type() == TypePtr::BOTTOM) {\n-            if (projs.catchall_memproj == NULL) {\n-              mem = projs.fallthrough_memproj;\n+        CallProjections* projs = c->as_Call()->extract_projections(true, false);\n+        if (projs->fallthrough_memproj != NULL) {\n+          if (projs->fallthrough_memproj->adr_type() == TypePtr::BOTTOM) {\n+            if (projs->catchall_memproj == NULL) {\n+              mem = projs->fallthrough_memproj;\n@@ -804,2 +803,2 @@\n-              if (phase->is_dominator(projs.fallthrough_catchproj, ctrl)) {\n-                mem = projs.fallthrough_memproj;\n+              if (phase->is_dominator(projs->fallthrough_catchproj, ctrl)) {\n+                mem = projs->fallthrough_memproj;\n@@ -807,2 +806,2 @@\n-                assert(phase->is_dominator(projs.catchall_catchproj, ctrl), \"one proj must dominate barrier\");\n-                mem = projs.catchall_memproj;\n+                assert(phase->is_dominator(projs->catchall_catchproj, ctrl), \"one proj must dominate barrier\");\n+                mem = projs->catchall_memproj;\n@@ -1066,1 +1065,1 @@\n-static Node* create_phis_on_call_return(Node* ctrl, Node* c, Node* n, Node* n_clone, const CallProjections& projs, PhaseIdealLoop* phase) {\n+static Node* create_phis_on_call_return(Node* ctrl, Node* c, Node* n, Node* n_clone, const CallProjections* projs, PhaseIdealLoop* phase) {\n@@ -1078,1 +1077,1 @@\n-    if (phase->is_dominator(projs.fallthrough_catchproj, in)) {\n+    if (phase->is_dominator(projs->fallthrough_catchproj, in)) {\n@@ -1080,1 +1079,1 @@\n-    } else if (phase->is_dominator(projs.catchall_catchproj, in)) {\n+    } else if (phase->is_dominator(projs->catchall_catchproj, in)) {\n@@ -1196,3 +1195,1 @@\n-      CallProjections projs;\n-      call->extract_projections(&projs, false, false);\n-\n+      CallProjections* projs = call->extract_projections(false, false);\n@@ -1203,2 +1200,2 @@\n-      phase->register_new_node(lrb_clone, projs.catchall_catchproj);\n-      phase->set_ctrl(lrb, projs.fallthrough_catchproj);\n+      phase->register_new_node(lrb_clone, projs->catchall_catchproj);\n+      phase->set_ctrl(lrb, projs->fallthrough_catchproj);\n@@ -1226,1 +1223,1 @@\n-          if (phase->is_dominator(call, c) && phase->is_dominator(c, projs.fallthrough_proj)) {\n+          if (phase->is_dominator(call, c) && phase->is_dominator(c, projs->fallthrough_proj)) {\n@@ -1234,1 +1231,1 @@\n-            phase->register_new_node(u_clone, projs.catchall_catchproj);\n+            phase->register_new_node(u_clone, projs->catchall_catchproj);\n@@ -1236,1 +1233,1 @@\n-            phase->set_ctrl(u, projs.fallthrough_catchproj);\n+            phase->set_ctrl(u, projs->fallthrough_catchproj);\n@@ -1242,1 +1239,1 @@\n-                  if (phase->is_dominator(projs.catchall_catchproj, u->in(0)->in(k))) {\n+                  if (phase->is_dominator(projs->catchall_catchproj, u->in(0)->in(k))) {\n@@ -1245,1 +1242,1 @@\n-                  } else if (!phase->is_dominator(projs.fallthrough_catchproj, u->in(0)->in(k))) {\n+                  } else if (!phase->is_dominator(projs->fallthrough_catchproj, u->in(0)->in(k))) {\n@@ -1252,1 +1249,1 @@\n-              if (phase->is_dominator(projs.catchall_catchproj, c)) {\n+              if (phase->is_dominator(projs->catchall_catchproj, c)) {\n@@ -1257,1 +1254,1 @@\n-              } else if (!phase->is_dominator(projs.fallthrough_catchproj, c)) {\n+              } else if (!phase->is_dominator(projs->fallthrough_catchproj, c)) {\n@@ -2419,5 +2416,4 @@\n-    CallProjections projs;\n-    call->extract_projections(&projs, true, false);\n-    if (projs.catchall_memproj != NULL) {\n-      if (projs.fallthrough_memproj == n) {\n-        c = projs.fallthrough_catchproj;\n+    CallProjections* projs = call->extract_projections(true, false);\n+    if (projs->catchall_memproj != NULL) {\n+      if (projs->fallthrough_memproj == n) {\n+        c = projs->fallthrough_catchproj;\n@@ -2425,2 +2421,2 @@\n-        assert(projs.catchall_memproj == n, \"\");\n-        c = projs.catchall_catchproj;\n+        assert(projs->catchall_memproj == n, \"\");\n+        c = projs->catchall_catchproj;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":30,"deletions":34,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -152,1 +152,1 @@\n-    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -311,1 +311,1 @@\n-bool ShenandoahBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+void ShenandoahBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -318,1 +318,1 @@\n-  return Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n+  Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1262,1 +1262,1 @@\n-              Deoptimization::reassign_fields(fst.current(), fst.register_map(), objects, realloc_failures, false);\n+              Deoptimization::reassign_fields(fst.current(), fst.register_map(), objects, realloc_failures, false, CHECK_NULL);\n@@ -1522,1 +1522,1 @@\n-  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false);\n+  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false, THREAD);\n@@ -1921,1 +1921,1 @@\n-    if (m->is_initializer() && !m->is_static()) {\n+    if (m->is_object_constructor()) {\n@@ -1951,1 +1951,1 @@\n-    if (!m->is_initializer() && !m->is_overpass()) {\n+    if (!(m->is_object_constructor() || m->is_class_initializer()) && !m->is_overpass()) {\n@@ -2570,2 +2570,1 @@\n-  if (m->is_initializer()) {\n-    if (m->is_static_initializer()) {\n+  if (m->is_class_initializer()) {\n@@ -2574,1 +2573,2 @@\n-    }\n+  }\n+  else if (m->is_object_constructor() || m->is_static_init_factory()) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -158,1 +158,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags,                                   u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags,                                   u4)                                    \\\n@@ -522,0 +522,2 @@\n+  declare_constant(DataLayout::array_load_store_data_tag)                 \\\n+  declare_constant(DataLayout::acmp_data_tag)                             \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -187,0 +187,1 @@\n+  LOG_TAG(valuetypes) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -195,1 +195,1 @@\n-    archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original));\n+    archived_oop->set_mark(markWord::prototype_for_klass(archived_oop->klass()).copy_set_hash(hash_original));\n","filename":"src\/hotspot\/share\/memory\/heapShared.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -57,0 +57,2 @@\n+  virtual void do_oop_no_buffering(oop* o) { do_oop(o); }\n+  virtual void do_oop_no_buffering(narrowOop* o) { do_oop(o); }\n@@ -117,0 +119,5 @@\n+class BufferedValueClosure : public Closure {\n+public:\n+  virtual void do_buffered_value(oop* p) = 0;\n+};\n+\n","filename":"src\/hotspot\/share\/memory\/iterator.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -56,0 +56,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/memory\/metaspaceShared.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -140,1 +140,1 @@\n-    st->print(\"flags(%d) \", flags);\n+    st->print(\"flags(%d) %p\/%d\", flags, data(), in_bytes(DataLayout::flags_offset()));\n@@ -210,1 +210,1 @@\n-  assert(TypeStackSlotEntries::per_arg_count() > ReturnTypeEntry::static_cell_count(), \"code to test for arguments\/results broken\");\n+  assert(TypeStackSlotEntries::per_arg_count() > SingleTypeEntry::static_cell_count(), \"code to test for arguments\/results broken\");\n@@ -220,1 +220,1 @@\n-    ret_cell = ReturnTypeEntry::static_cell_count();\n+    ret_cell = SingleTypeEntry::static_cell_count();\n@@ -323,1 +323,1 @@\n-void ReturnTypeEntry::clean_weak_klass_links(bool always_clean) {\n+void SingleTypeEntry::clean_weak_klass_links(bool always_clean) {\n@@ -361,1 +361,1 @@\n-void ReturnTypeEntry::print_data_on(outputStream* st) const {\n+void SingleTypeEntry::print_data_on(outputStream* st) const {\n@@ -526,0 +526,4 @@\n+  if (data()->flags()) {\n+    tty->cr();\n+    tab(st);\n+  }\n@@ -649,0 +653,21 @@\n+void ArrayLoadStoreData::print_data_on(outputStream* st, const char* extra) const {\n+  print_shared(st, \"ArrayLoadStore\", extra);\n+  st->cr();\n+  tab(st, true);\n+  st->print(\"array\");\n+  _array.print_data_on(st);\n+  tab(st, true);\n+  st->print(\"element\");\n+  _element.print_data_on(st);\n+}\n+\n+void ACmpData::print_data_on(outputStream* st, const char* extra) const {\n+  BranchData::print_data_on(st, extra);\n+  tab(st, true);\n+  st->print(\"left\");\n+  _left.print_data_on(st);\n+  tab(st, true);\n+  st->print(\"right\");\n+  _right.print_data_on(st);\n+}\n+\n@@ -669,1 +694,0 @@\n-  case Bytecodes::_aastore:\n@@ -675,0 +699,3 @@\n+  case Bytecodes::_aaload:\n+  case Bytecodes::_aastore:\n+    return ArrayLoadStoreData::static_cell_count();\n@@ -714,2 +741,0 @@\n-  case Bytecodes::_if_acmpeq:\n-  case Bytecodes::_if_acmpne:\n@@ -719,0 +744,3 @@\n+  case Bytecodes::_if_acmpne:\n+  case Bytecodes::_if_acmpeq:\n+    return ACmpData::static_cell_count();\n@@ -777,0 +805,1 @@\n+  case Bytecodes::_aaload:\n@@ -980,1 +1009,0 @@\n-  case Bytecodes::_aastore:\n@@ -989,0 +1017,5 @@\n+  case Bytecodes::_aaload:\n+  case Bytecodes::_aastore:\n+    cell_count = ArrayLoadStoreData::static_cell_count();\n+    tag = DataLayout::array_load_store_data_tag;\n+    break;\n@@ -1060,2 +1093,0 @@\n-  case Bytecodes::_if_acmpeq:\n-  case Bytecodes::_if_acmpne:\n@@ -1067,0 +1098,5 @@\n+  case Bytecodes::_if_acmpeq:\n+  case Bytecodes::_if_acmpne:\n+    cell_count = ACmpData::static_cell_count();\n+    tag = DataLayout::acmp_data_tag;\n+    break;\n@@ -1134,0 +1170,4 @@\n+  case DataLayout::array_load_store_data_tag:\n+    return ((new ArrayLoadStoreData(this))->cell_count());\n+  case DataLayout::acmp_data_tag:\n+    return ((new ACmpData(this))->cell_count());\n@@ -1168,0 +1208,4 @@\n+  case DataLayout::array_load_store_data_tag:\n+    return new ArrayLoadStoreData(this);\n+  case DataLayout::acmp_data_tag:\n+    return new ACmpData(this);\n","filename":"src\/hotspot\/share\/oops\/methodData.cpp","additions":55,"deletions":11,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -126,1 +126,3 @@\n-    speculative_trap_data_tag\n+    speculative_trap_data_tag,\n+    array_load_store_data_tag,\n+    acmp_data_tag\n@@ -262,0 +264,1 @@\n+class       ACmpData;\n@@ -267,0 +270,1 @@\n+class   ArrayLoadStoreData;\n@@ -274,1 +278,1 @@\n-  friend class ReturnTypeEntry;\n+  friend class SingleTypeEntry;\n@@ -395,0 +399,2 @@\n+  virtual bool is_ArrayLoadStoreData() const { return false; }\n+  virtual bool is_ACmpData()           const { return false; }\n@@ -453,0 +459,8 @@\n+  ArrayLoadStoreData* as_ArrayLoadStoreData() const {\n+    assert(is_ArrayLoadStoreData(), \"wrong type\");\n+    return is_ArrayLoadStoreData() ? (ArrayLoadStoreData*)this : NULL;\n+  }\n+  ACmpData* as_ACmpData() const {\n+    assert(is_ACmpData(), \"wrong type\");\n+    return is_ACmpData() ? (ACmpData*)this : NULL;\n+  }\n@@ -607,1 +621,2 @@\n-      layout->tag() == DataLayout::branch_data_tag, \"wrong type\");\n+      layout->tag() == DataLayout::branch_data_tag ||\n+      layout->tag() == DataLayout::acmp_data_tag, \"wrong type\");\n@@ -843,1 +858,1 @@\n-class ReturnTypeEntry : public TypeEntries {\n+class SingleTypeEntry : public TypeEntries {\n@@ -851,1 +866,1 @@\n-  ReturnTypeEntry(int base_off)\n+  SingleTypeEntry(int base_off)\n@@ -885,1 +900,1 @@\n-\/\/ (TypeStackSlotEntries), a return type (ReturnTypeEntry) and a\n+\/\/ (TypeStackSlotEntries), a return type (SingleTypeEntry) and a\n@@ -939,1 +954,1 @@\n-    return ReturnTypeEntry::size() + in_ByteSize(header_cell_count() * DataLayout::cell_size);\n+    return SingleTypeEntry::size() + in_ByteSize(header_cell_count() * DataLayout::cell_size);\n@@ -954,1 +969,1 @@\n-  ReturnTypeEntry _ret;\n+  SingleTypeEntry _ret;\n@@ -973,1 +988,1 @@\n-    _ret(cell_count() - ReturnTypeEntry::static_cell_count())\n+    _ret(cell_count() - SingleTypeEntry::static_cell_count())\n@@ -986,1 +1001,1 @@\n-  const ReturnTypeEntry* ret() const {\n+  const SingleTypeEntry* ret() const {\n@@ -1257,1 +1272,1 @@\n-  ReturnTypeEntry _ret;\n+  SingleTypeEntry _ret;\n@@ -1276,1 +1291,1 @@\n-    _ret(cell_count() - ReturnTypeEntry::static_cell_count())\n+    _ret(cell_count() - SingleTypeEntry::static_cell_count())\n@@ -1289,1 +1304,1 @@\n-  const ReturnTypeEntry* ret() const {\n+  const SingleTypeEntry* ret() const {\n@@ -1491,1 +1506,1 @@\n-    assert(layout->tag() == DataLayout::branch_data_tag, \"wrong type\");\n+    assert(layout->tag() == DataLayout::branch_data_tag || layout->tag() == DataLayout::acmp_data_tag, \"wrong type\");\n@@ -1848,0 +1863,146 @@\n+class ArrayLoadStoreData : public ProfileData {\n+private:\n+  enum {\n+    flat_array_flag = DataLayout::first_flag,\n+    null_free_array_flag = flat_array_flag + 1,\n+  };\n+\n+  SingleTypeEntry _array;\n+  SingleTypeEntry _element;\n+\n+public:\n+  ArrayLoadStoreData(DataLayout* layout) :\n+    ProfileData(layout),\n+    _array(0),\n+    _element(SingleTypeEntry::static_cell_count()) {\n+    assert(layout->tag() == DataLayout::array_load_store_data_tag, \"wrong type\");\n+    _array.set_profile_data(this);\n+    _element.set_profile_data(this);\n+  }\n+\n+  const SingleTypeEntry* array() const {\n+    return &_array;\n+  }\n+\n+  const SingleTypeEntry* element() const {\n+    return &_element;\n+  }\n+\n+  virtual bool is_ArrayLoadStoreData() const { return true; }\n+\n+  static int static_cell_count() {\n+    return SingleTypeEntry::static_cell_count() * 2;\n+  }\n+\n+  virtual int cell_count() const {\n+    return static_cell_count();\n+  }\n+\n+  void set_flat_array() { set_flag_at(flat_array_flag); }\n+  bool flat_array() const { return flag_at(flat_array_flag); }\n+\n+  void set_null_free_array() { set_flag_at(null_free_array_flag); }\n+  bool null_free_array() const { return flag_at(null_free_array_flag); }\n+\n+  \/\/ Code generation support\n+  static int flat_array_byte_constant() {\n+    return flag_number_to_constant(flat_array_flag);\n+  }\n+\n+  static int null_free_array_byte_constant() {\n+    return flag_number_to_constant(null_free_array_flag);\n+  }\n+\n+  static ByteSize array_offset() {\n+    return cell_offset(0);\n+  }\n+\n+  static ByteSize element_offset() {\n+    return cell_offset(SingleTypeEntry::static_cell_count());\n+  }\n+\n+  virtual void clean_weak_klass_links(bool always_clean) {\n+    _array.clean_weak_klass_links(always_clean);\n+    _element.clean_weak_klass_links(always_clean);\n+  }\n+\n+  static ByteSize array_load_store_data_size() {\n+    return cell_offset(static_cell_count());\n+  }\n+\n+  virtual void print_data_on(outputStream* st, const char* extra = NULL) const;\n+};\n+\n+class ACmpData : public BranchData {\n+private:\n+  enum {\n+    left_inline_type_flag = DataLayout::first_flag,\n+    right_inline_type_flag\n+  };\n+\n+  SingleTypeEntry _left;\n+  SingleTypeEntry _right;\n+\n+public:\n+  ACmpData(DataLayout* layout) :\n+    BranchData(layout),\n+    _left(BranchData::static_cell_count()),\n+    _right(BranchData::static_cell_count() + SingleTypeEntry::static_cell_count()) {\n+    assert(layout->tag() == DataLayout::acmp_data_tag, \"wrong type\");\n+    _left.set_profile_data(this);\n+    _right.set_profile_data(this);\n+  }\n+\n+  const SingleTypeEntry* left() const {\n+    return &_left;\n+  }\n+\n+  const SingleTypeEntry* right() const {\n+    return &_right;\n+  }\n+\n+  virtual bool is_ACmpData() const { return true; }\n+\n+  static int static_cell_count() {\n+    return BranchData::static_cell_count()+ SingleTypeEntry::static_cell_count() * 2;\n+  }\n+\n+  virtual int cell_count() const {\n+    return static_cell_count();\n+  }\n+\n+  void set_left_inline_type() { set_flag_at(left_inline_type_flag); }\n+  bool left_inline_type() const { return flag_at(left_inline_type_flag); }\n+\n+  void set_right_inline_type() { set_flag_at(right_inline_type_flag); }\n+  bool right_inline_type() const { return flag_at(right_inline_type_flag); }\n+\n+  \/\/ Code generation support\n+  static int left_inline_type_byte_constant() {\n+    return flag_number_to_constant(left_inline_type_flag);\n+  }\n+\n+  static int right_inline_type_byte_constant() {\n+    return flag_number_to_constant(right_inline_type_flag);\n+  }\n+\n+  static ByteSize left_offset() {\n+    return cell_offset(BranchData::static_cell_count());\n+  }\n+\n+  static ByteSize right_offset() {\n+    return cell_offset(BranchData::static_cell_count() + SingleTypeEntry::static_cell_count());\n+  }\n+\n+  virtual void clean_weak_klass_links(bool always_clean) {\n+    _left.clean_weak_klass_links(always_clean);\n+    _right.clean_weak_klass_links(always_clean);\n+  }\n+\n+  static ByteSize acmp_data_size() {\n+    return cell_offset(static_cell_count());\n+  }\n+\n+  virtual void print_data_on(outputStream* st, const char* extra = NULL) const;\n+};\n+\n","filename":"src\/hotspot\/share\/oops\/methodData.hpp","additions":175,"deletions":14,"binary":false,"changes":189,"status":"modified"},{"patch":"@@ -657,0 +657,6 @@\n+  if (tp->isa_aryptr()) {\n+    \/\/ In the case of a flattened inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return tp->is_aryptr()->add_field_offset_and_offset(txoffset);\n+  }\n@@ -677,0 +683,6 @@\n+  if (p1->isa_aryptr()) {\n+    \/\/ In the case of a flattened inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return p1->is_aryptr()->add_field_offset_and_offset(p2offset);\n+  }\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -45,0 +47,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -79,1 +82,1 @@\n-Node *StartNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *StartNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -103,11 +106,0 @@\n-\/\/------------------------------StartOSRNode----------------------------------\n-\/\/ The method start node for an on stack replacement adapter\n-\n-\/\/------------------------------osr_domain-----------------------------\n-const TypeTuple *StartOSRNode::osr_domain() {\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n-\n-  return TypeTuple::make(TypeFunc::Parms+1, fields);\n-}\n-\n@@ -483,0 +475,8 @@\n+      } else if (cik->is_flat_array_klass()) {\n+        ciKlass* cie = cik->as_flat_array_klass()->base_element_klass();\n+        cie->print_name_on(st);\n+        st->print(\"[%d]\", spobj->n_fields());\n+        int ndim = cik->as_array_klass()->dimension() - 1;\n+        while (ndim-- > 0) {\n+          st->print(\"[]\");\n+        }\n@@ -692,1 +692,1 @@\n-const Type *CallNode::bottom_type() const { return tf()->range(); }\n+const Type *CallNode::bottom_type() const { return tf()->range_cc(); }\n@@ -694,2 +694,4 @@\n-  if (phase->type(in(0)) == Type::TOP)  return Type::TOP;\n-  return tf()->range();\n+  if (!in(0) || phase->type(in(0)) == Type::TOP) {\n+    return Type::TOP;\n+  }\n+  return tf()->range_cc();\n@@ -699,1 +701,8 @@\n-void CallNode::calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const {\n+void CallNode::calling_convention(BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt) const {\n+  if (_entry_point == StubRoutines::store_inline_type_fields_to_buf()) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -708,2 +717,28 @@\n-Node *CallNode::match( const ProjNode *proj, const Matcher *match ) {\n-  switch (proj->_con) {\n+Node *CallNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n+  uint con = proj->_con;\n+  const TypeTuple *range_cc = tf()->range_cc();\n+  if (con >= TypeFunc::Parms) {\n+    if (is_CallRuntime()) {\n+      if (con == TypeFunc::Parms) {\n+        uint ideal_reg = range_cc->field_at(TypeFunc::Parms)->ideal_reg();\n+        OptoRegPair regs = match->c_return_value(ideal_reg,true);\n+        RegMask rm = RegMask(regs.first());\n+        if (OptoReg::is_valid(regs.second())) {\n+          rm.Insert(regs.second());\n+        }\n+        return new MachProjNode(this,con,rm,ideal_reg);\n+      } else {\n+        assert(con == TypeFunc::Parms+1, \"only one return value\");\n+        assert(range_cc->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n+        return new MachProjNode(this,con, RegMask::Empty, (uint)OptoReg::Bad);\n+      }\n+    } else {\n+      \/\/ The Call may return multiple values (inline type fields): we\n+      \/\/ create one projection per returned value.\n+      assert(con <= TypeFunc::Parms+1 || InlineTypeReturnedAsFields, \"only for multi value return\");\n+      uint ideal_reg = range_cc->field_at(con)->ideal_reg();\n+      return new MachProjNode(this, con, mask[con-TypeFunc::Parms], ideal_reg);\n+    }\n+  }\n+\n+  switch (con) {\n@@ -715,16 +750,0 @@\n-  case TypeFunc::Parms+1:       \/\/ For LONG & DOUBLE returns\n-    assert(tf()->range()->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n-    \/\/ 2nd half of doubles and longs\n-    return new MachProjNode(this,proj->_con, RegMask::Empty, (uint)OptoReg::Bad);\n-\n-  case TypeFunc::Parms: {       \/\/ Normal returns\n-    uint ideal_reg = tf()->range()->field_at(TypeFunc::Parms)->ideal_reg();\n-    OptoRegPair regs = is_CallRuntime()\n-      ? match->c_return_value(ideal_reg,true)  \/\/ Calls into C runtime\n-      : match->  return_value(ideal_reg,true); \/\/ Calls into compiled Java code\n-    RegMask rm = RegMask(regs.first());\n-    if( OptoReg::is_valid(regs.second()) )\n-      rm.Insert( regs.second() );\n-    return new MachProjNode(this,proj->_con,rm,ideal_reg);\n-  }\n-\n@@ -751,1 +770,1 @@\n-    const TypeTuple* args = _tf->domain();\n+    const TypeTuple* args = _tf->domain_sig();\n@@ -800,1 +819,1 @@\n-      const TypeTuple* d = tf()->domain();\n+      const TypeTuple* d = tf()->domain_cc();\n@@ -816,1 +835,1 @@\n-  const TypeTuple * d = tf()->domain();\n+  const TypeTuple * d = tf()->domain_cc();\n@@ -826,0 +845,11 @@\n+bool CallNode::has_debug_use(Node *n) {\n+  assert(jvms() != NULL, \"jvms should not be null\");\n+  for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+    Node *arg = in(i);\n+    if (arg == n) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -857,10 +887,15 @@\n-void CallNode::extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts) {\n-  projs->fallthrough_proj      = NULL;\n-  projs->fallthrough_catchproj = NULL;\n-  projs->fallthrough_ioproj    = NULL;\n-  projs->catchall_ioproj       = NULL;\n-  projs->catchall_catchproj    = NULL;\n-  projs->fallthrough_memproj   = NULL;\n-  projs->catchall_memproj      = NULL;\n-  projs->resproj               = NULL;\n-  projs->exobj                 = NULL;\n+CallProjections* CallNode::extract_projections(bool separate_io_proj, bool do_asserts) {\n+  uint max_res = TypeFunc::Parms-1;\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    ProjNode *pn = fast_out(i)->as_Proj();\n+    max_res = MAX2(max_res, pn->_con);\n+  }\n+\n+  assert(max_res < _tf->range_cc()->cnt(), \"result out of bounds\");\n+\n+  uint projs_size = sizeof(CallProjections);\n+  if (max_res > TypeFunc::Parms) {\n+    projs_size += (max_res-TypeFunc::Parms)*sizeof(Node*);\n+  }\n+  char* projs_storage = resource_allocate_bytes(projs_size);\n+  CallProjections* projs = new(projs_storage)CallProjections(max_res - TypeFunc::Parms + 1);\n@@ -912,1 +947,1 @@\n-      projs->resproj = pn;\n+      projs->resproj[0] = pn;\n@@ -915,1 +950,3 @@\n-      assert(false, \"unexpected projection from allocation node.\");\n+      assert(pn->_con <= max_res, \"unexpected projection from allocation node.\");\n+      projs->resproj[pn->_con-TypeFunc::Parms] = pn;\n+      break;\n@@ -922,1 +959,1 @@\n-  assert(projs->fallthrough_proj      != NULL, \"must be found\");\n+  assert(!do_asserts || projs->fallthrough_proj      != NULL, \"must be found\");\n@@ -932,0 +969,1 @@\n+  return projs;\n@@ -973,2 +1011,2 @@\n-  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain()->cnt() : (uint)TypeFunc::Parms+1;\n-  uint new_dbg_start = tf()->domain()->cnt();\n+  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain_sig()->cnt() : (uint)TypeFunc::Parms+1;\n+  uint new_dbg_start = tf()->domain_sig()->cnt();\n@@ -1015,0 +1053,4 @@\n+  Bytecodes::Code bc = jvms()->method()->java_code_at_bci(_bci);\n+  if (EnableValhalla && (bc == Bytecodes::_if_acmpeq || bc == Bytecodes::_if_acmpne)) {\n+    return true;\n+  }\n@@ -1069,0 +1111,151 @@\n+bool CallStaticJavaNode::remove_useless_allocation(PhaseGVN *phase, Node* ctl, Node* mem, Node* unc_arg) {\n+  \/\/ Split if can cause the flattened array branch of an array load to\n+  \/\/ end in an uncommon trap. In that case, the allocation of the\n+  \/\/ loaded value and its initialization is useless. Eliminate it. use\n+  \/\/ the jvm state of the allocation to create a new uncommon trap\n+  \/\/ call at the load.\n+  if (ctl == NULL || ctl->is_top() || mem == NULL || mem->is_top() || !mem->is_MergeMem()) {\n+    return false;\n+  }\n+  PhaseIterGVN* igvn = phase->is_IterGVN();\n+  if (ctl->is_Region()) {\n+    bool res = false;\n+    for (uint i = 1; i < ctl->req(); i++) {\n+      MergeMemNode* mm = mem->clone()->as_MergeMem();\n+      for (MergeMemStream mms(mm); mms.next_non_empty(); ) {\n+        Node* m = mms.memory();\n+        if (m->is_Phi() && m->in(0) == ctl) {\n+          mms.set_memory(m->in(i));\n+        }\n+      }\n+      if (remove_useless_allocation(phase, ctl->in(i), mm, unc_arg)) {\n+        res = true;\n+        if (!ctl->in(i)->is_Region()) {\n+          igvn->replace_input_of(ctl, i, phase->C->top());\n+        }\n+      }\n+      igvn->remove_dead_node(mm);\n+    }\n+    return res;\n+  }\n+  \/\/ verify the control flow is ok\n+  Node* c = ctl;\n+  Node* copy = NULL;\n+  Node* alloc = NULL;\n+  for (;;) {\n+    if (c == NULL || c->is_top()) {\n+      return false;\n+    }\n+    if (c->is_Proj() || c->is_Catch() || c->is_MemBar()) {\n+      c = c->in(0);\n+    } else if (c->Opcode() == Op_CallLeaf &&\n+               c->as_Call()->entry_point() == CAST_FROM_FN_PTR(address, OptoRuntime::load_unknown_inline)) {\n+      copy = c;\n+      c = c->in(0);\n+    } else if (c->is_Allocate()) {\n+      Node* new_obj = c->as_Allocate()->result_cast();\n+      if (copy == NULL || new_obj == NULL) {\n+        return false;\n+      }\n+      Node* copy_dest = copy->in(TypeFunc::Parms + 2);\n+      if (copy_dest != new_obj) {\n+        return false;\n+      }\n+      alloc = c;\n+      break;\n+    } else {\n+      return false;\n+    }\n+  }\n+\n+  JVMState* jvms = alloc->jvms();\n+  if (phase->C->too_many_traps(jvms->method(), jvms->bci(), Deoptimization::trap_request_reason(uncommon_trap_request()))) {\n+    return false;\n+  }\n+\n+  Node* alloc_mem = alloc->in(TypeFunc::Memory);\n+  if (alloc_mem == NULL || alloc_mem->is_top()) {\n+    return false;\n+  }\n+  if (!alloc_mem->is_MergeMem()) {\n+    alloc_mem = MergeMemNode::make(alloc_mem);\n+  }\n+\n+  \/\/ and that there's no unexpected side effect\n+  for (MergeMemStream mms2(mem->as_MergeMem(), alloc_mem->as_MergeMem()); mms2.next_non_empty2(); ) {\n+    Node* m1 = mms2.is_empty() ? mms2.base_memory() : mms2.memory();\n+    Node* m2 = mms2.memory2();\n+\n+    for (uint i = 0; i < 100; i++) {\n+      if (m1 == m2) {\n+        break;\n+      } else if (m1->is_Proj()) {\n+        m1 = m1->in(0);\n+      } else if (m1->is_MemBar()) {\n+        m1 = m1->in(TypeFunc::Memory);\n+      } else if (m1->Opcode() == Op_CallLeaf &&\n+                 m1->as_Call()->entry_point() == CAST_FROM_FN_PTR(address, OptoRuntime::load_unknown_inline)) {\n+        if (m1 != copy) {\n+          return false;\n+        }\n+        m1 = m1->in(TypeFunc::Memory);\n+      } else if (m1->is_Allocate()) {\n+        if (m1 != alloc) {\n+          return false;\n+        }\n+        break;\n+      } else if (m1->is_MergeMem()) {\n+        MergeMemNode* mm = m1->as_MergeMem();\n+        int idx = mms2.alias_idx();\n+        if (idx == Compile::AliasIdxBot) {\n+          m1 = mm->base_memory();\n+        } else {\n+          m1 = mm->memory_at(idx);\n+        }\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+  if (alloc_mem->outcnt() == 0) {\n+    igvn->remove_dead_node(alloc_mem);\n+  }\n+\n+  address call_addr = SharedRuntime::uncommon_trap_blob()->entry_point();\n+  CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\",\n+                                         jvms->bci(), NULL);\n+  unc->init_req(TypeFunc::Control, alloc->in(0));\n+  unc->init_req(TypeFunc::I_O, alloc->in(TypeFunc::I_O));\n+  unc->init_req(TypeFunc::Memory, alloc->in(TypeFunc::Memory));\n+  unc->init_req(TypeFunc::FramePtr,  alloc->in(TypeFunc::FramePtr));\n+  unc->init_req(TypeFunc::ReturnAdr, alloc->in(TypeFunc::ReturnAdr));\n+  unc->init_req(TypeFunc::Parms+0, unc_arg);\n+  unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+  unc->copy_call_debug_info(igvn, alloc->as_Allocate());\n+\n+  igvn->replace_input_of(alloc, 0, phase->C->top());\n+\n+  igvn->register_new_node_with_optimizer(unc);\n+\n+  Node* ctrl = phase->transform(new ProjNode(unc, TypeFunc::Control));\n+  Node* halt = phase->transform(new HaltNode(ctrl, alloc->in(TypeFunc::FramePtr), \"uncommon trap returned which should never happen\"));\n+  phase->C->root()->add_req(halt);\n+\n+  return true;\n+}\n+\n+\n+Node* CallStaticJavaNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  if (can_reshape && uncommon_trap_request() != 0) {\n+    if (remove_useless_allocation(phase, in(0), in(TypeFunc::Memory), in(TypeFunc::Parms))) {\n+      if (!in(0)->is_Region()) {\n+        PhaseIterGVN* igvn = phase->is_IterGVN();\n+        igvn->replace_input_of(this, 0, phase->C->top());\n+      }\n+      return this;\n+    }\n+  }\n+  return CallNode::Ideal(phase, can_reshape);\n+}\n+\n+\n@@ -1126,0 +1319,7 @@\n+  if (_entry_point == NULL) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -1142,0 +1342,6 @@\n+uint CallLeafNoFPNode::match_edge(uint idx) const {\n+  \/\/ Null entry point is a special case for which the target is in a\n+  \/\/ register. Need to match that edge.\n+  return entry_point() == NULL && idx == TypeFunc::Parms;\n+}\n+\n@@ -1407,1 +1613,3 @@\n-                           Node *size, Node *klass_node, Node *initial_test)\n+                           Node *size, Node *klass_node,\n+                           Node* initial_test,\n+                           InlineTypeBaseNode* inline_type_node)\n@@ -1415,0 +1623,1 @@\n+  _larval = false;\n@@ -1426,0 +1635,3 @@\n+  init_req( InlineTypeNode     , inline_type_node);\n+  \/\/ DefaultValue defaults to NULL\n+  \/\/ RawDefaultValue defaults to NULL\n@@ -1432,3 +1644,2 @@\n-         initializer->is_initializer() &&\n-         !initializer->is_static(),\n-             \"unexpected initializer method\");\n+         initializer->is_object_constructor_or_class_initializer(),\n+         \"unexpected initializer method\");\n@@ -1445,1 +1656,2 @@\n-Node *AllocateNode::make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem) {\n+\n+Node* AllocateNode::make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem) {\n@@ -1447,2 +1659,1 @@\n-  \/\/ For now only enable fast locking for non-array types\n-  if (UseBiasedLocking && Opcode() == Op_Allocate) {\n+  if (EnableValhalla) {\n@@ -1455,1 +1666,3 @@\n-  return mark_node;\n+  mark_node = phase->transform(mark_node);\n+  \/\/ Avoid returning a constant (old node) here because this method is used by LoadNode::Ideal\n+  return new OrXNode(mark_node, phase->MakeConX(_larval ? markWord::larval_bit_in_place : 0));\n@@ -1458,0 +1671,1 @@\n+\n@@ -1460,1 +1674,4 @@\n-  if (remove_dead_region(phase, can_reshape))  return this;\n+  Node* res = SafePointNode::Ideal(phase, can_reshape);\n+  if (res != NULL) {\n+    return res;\n+  }\n@@ -1885,1 +2102,3 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() &&\n+      !obj_type->isa_inlinetype() && !obj_type->is_inlinetypeptr()) {\n@@ -2053,1 +2272,3 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() &&\n+      !obj_type->isa_inlinetype() && !obj_type->is_inlinetypeptr()) {\n@@ -2135,1 +2356,2 @@\n-    dest_t = dest_t->add_offset(Type::OffsetBot)->is_oopptr();\n+    dest_t = dest_t->is_aryptr()->with_field_offset(Type::OffsetBot)->add_offset(Type::OffsetBot)->is_oopptr();\n+    t_oop = t_oop->is_aryptr()->with_field_offset(Type::OffsetBot);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":286,"deletions":64,"binary":false,"changes":350,"status":"modified"},{"patch":"@@ -82,1 +82,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n@@ -96,1 +96,0 @@\n-  static  const TypeTuple *osr_domain();\n@@ -548,1 +547,1 @@\n-class CallProjections : public StackObj {\n+class CallProjections {\n@@ -557,1 +556,19 @@\n-  Node* resproj;\n+  uint nb_resproj;\n+  Node* resproj[1]; \/\/ at least one projection\n+\n+  CallProjections(uint nbres) {\n+    fallthrough_proj      = NULL;\n+    fallthrough_catchproj = NULL;\n+    fallthrough_memproj   = NULL;\n+    fallthrough_ioproj    = NULL;\n+    catchall_catchproj    = NULL;\n+    catchall_memproj      = NULL;\n+    catchall_ioproj       = NULL;\n+    exobj                 = NULL;\n+    nb_resproj            = nbres;\n+    resproj[0]            = NULL;\n+    for (uint i = 1; i < nb_resproj; i++) {\n+      resproj[i]          = NULL;\n+    }\n+  }\n+\n@@ -580,1 +597,1 @@\n-    : SafePointNode(tf->domain()->cnt(), NULL, adr_type),\n+    : SafePointNode(tf->domain_cc()->cnt(), NULL, adr_type),\n@@ -607,1 +624,1 @@\n-  virtual Node       *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node       *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n@@ -626,0 +643,1 @@\n+  bool                has_debug_use(Node *n);\n@@ -632,2 +650,3 @@\n-    const TypeTuple *r = tf()->range();\n-    return (r->cnt() > TypeFunc::Parms &&\n+    const TypeTuple *r = tf()->range_sig();\n+    return (!tf()->returns_inline_type_as_fields() &&\n+            r->cnt() > TypeFunc::Parms &&\n@@ -640,1 +659,1 @@\n-  void extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts = true);\n+  CallProjections* extract_projections(bool separate_io_proj, bool do_asserts = true);\n@@ -711,0 +730,3 @@\n+\n+  bool remove_useless_allocation(PhaseGVN *phase, Node* ctl, Node* mem, Node* unc_arg);\n+\n@@ -719,0 +741,12 @@\n+    const TypeTuple *r = tf->range_sig();\n+    if (InlineTypeReturnedAsFields &&\n+        method != NULL &&\n+        method->is_method_handle_intrinsic() &&\n+        r->cnt() > TypeFunc::Parms &&\n+        r->field_at(TypeFunc::Parms)->isa_oopptr() &&\n+        r->field_at(TypeFunc::Parms)->is_oopptr()->can_be_inline_type()) {\n+      \/\/ Make sure this call is processed by PhaseMacroExpand::expand_mh_intrinsic_return\n+      init_flags(Flag_is_macro);\n+      C->add_macro_node(this);\n+    }\n+\n@@ -745,0 +779,2 @@\n+  virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);\n+\n@@ -820,0 +856,1 @@\n+  virtual uint match_edge(uint idx) const;\n@@ -843,0 +880,3 @@\n+    InlineTypeNode,                   \/\/ InlineTypeNode if this is an inline type allocation\n+    DefaultValue,                     \/\/ default value in case of non-flattened inline type array\n+    RawDefaultValue,                  \/\/ same as above but as raw machine word\n@@ -852,0 +892,3 @@\n+    fields[InlineTypeNode] = Type::BOTTOM;\n+    fields[DefaultValue] = TypeInstPtr::NOTNULL;\n+    fields[RawDefaultValue] = TypeX_X;\n@@ -869,0 +912,1 @@\n+  bool _larval;\n@@ -872,1 +916,2 @@\n-               Node *size, Node *klass_node, Node *initial_test);\n+               Node *size, Node *klass_node, Node *initial_test,\n+               InlineTypeBaseNode* inline_type_node = NULL);\n@@ -942,1 +987,1 @@\n-  Node* make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem);\n+  Node* make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem);\n@@ -953,4 +998,2 @@\n-                    Node* count_val\n-                    )\n-    : AllocateNode(C, atype, ctrl, mem, abio, size, klass_node,\n-                   initial_test)\n+                    Node* count_val, Node* default_value, Node* raw_default_value)\n+    : AllocateNode(C, atype, ctrl, mem, abio, size, klass_node, initial_test)\n@@ -960,0 +1003,2 @@\n+    init_req(AllocateNode::DefaultValue,  default_value);\n+    init_req(AllocateNode::RawDefaultValue, raw_default_value);\n@@ -1079,1 +1124,1 @@\n-    return TypeFunc::make(domain,range);\n+    return TypeFunc::make(domain, range);\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":61,"deletions":16,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -397,1 +398,1 @@\n-bool RegionNode::try_clean_mem_phi(PhaseGVN *phase) {\n+Node* PhiNode::try_clean_mem_phi(PhaseGVN *phase) {\n@@ -417,2 +418,1 @@\n-  PhiNode* phi = has_unique_phi();\n-  if (phi && phi->type() == Type::MEMORY && req() == 3 && phi->is_diamond_phi(true)) {\n+  if (type() == Type::MEMORY && is_diamond_phi(true)) {\n@@ -420,1 +420,2 @@\n-    assert(phi->req() == 3, \"same as region\");\n+    assert(req() == 3, \"same as region\");\n+    Node* r = in(0);\n@@ -422,2 +423,2 @@\n-      Node *mem = phi->in(i);\n-      if (mem && mem->is_MergeMem() && in(i)->outcnt() == 1) {\n+      Node *mem = in(i);\n+      if (mem && mem->is_MergeMem() && r->in(i)->outcnt() == 1) {\n@@ -427,1 +428,1 @@\n-        Node* other = phi->in(j);\n+        Node* other = in(j);\n@@ -431,2 +432,1 @@\n-          phase->is_IterGVN()->replace_node(phi, m);\n-          return true;\n+          return m;\n@@ -437,1 +437,1 @@\n-  return false;\n+  return NULL;\n@@ -452,2 +452,9 @@\n-    if (has_phis && try_clean_mem_phi(phase)) {\n-      has_phis = false;\n+    if (has_phis) {\n+      PhiNode* phi = has_unique_phi();\n+      if (phi != NULL) {\n+        Node* m = phi->try_clean_mem_phi(phase);\n+        if (m != NULL) {\n+          phase->is_IterGVN()->replace_node(phi, m);\n+          has_phis = false;\n+        }\n+      }\n@@ -923,1 +930,1 @@\n-  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at), \"flatten at\");\n+  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at) || (flatten_phi_adr_type(at) == TypeAryPtr::INLINES && Compile::current()->flattened_accesses_share_alias()), \"flatten at\");\n@@ -1133,9 +1140,4 @@\n-  if (ttip != NULL) {\n-    ciKlass* k = ttip->klass();\n-    if (k->is_loaded() && k->is_interface())\n-      is_intf = true;\n-  }\n-  if (ttkp != NULL) {\n-    ciKlass* k = ttkp->klass();\n-    if (k->is_loaded() && k->is_interface())\n-      is_intf = true;\n+  if (ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {\n+    is_intf = true;\n+  } else if (ttkp != NULL && ttkp->is_loaded() && ttkp->klass()->is_interface()) {\n+    is_intf = true;\n@@ -1198,1 +1200,1 @@\n-    if (!t->empty() && ttip && ttip->is_loaded() && ttip->klass()->is_interface()) {\n+    if (!t->empty() && ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {\n@@ -1200,1 +1202,1 @@\n-    } else if (!t->empty() && ttkp && ttkp->is_loaded() && ttkp->klass()->is_interface()) {\n+    } else if (!t->empty() && ttkp != NULL && ttkp->is_loaded() && ttkp->klass()->is_interface()) {\n@@ -1362,0 +1364,8 @@\n+  if (phase->is_IterGVN()) {\n+    Node* m = try_clean_mem_phi(phase);\n+    if (m != NULL) {\n+      return m;\n+    }\n+  }\n+\n+\n@@ -1897,0 +1907,18 @@\n+  \/\/ If all inputs are inline types of the same type, push the inline type node down\n+  \/\/ through the phi because inline type nodes should be merged through their input values.\n+  if (req() > 2 && in(1) != NULL && in(1)->is_InlineTypeBase() && (can_reshape || in(1)->is_InlineType())) {\n+    int opcode = in(1)->Opcode();\n+    uint i = 2;\n+    \/\/ Check if inputs are values of the same type\n+    for (; i < req() && in(i) && in(i)->is_InlineTypeBase() && in(i)->cmp(*in(1)); i++) {\n+      assert(in(i)->Opcode() == opcode, \"mixing pointers and values?\");\n+    }\n+    if (i == req()) {\n+      InlineTypeBaseNode* vt = in(1)->as_InlineTypeBase()->clone_with_phis(phase, in(0));\n+      for (uint i = 2; i < req(); ++i) {\n+        vt->merge_with(phase, in(i)->as_InlineTypeBase(), i, i == (req()-1));\n+      }\n+      return vt;\n+    }\n+  }\n+\n@@ -2186,0 +2214,2 @@\n+    \/\/ TODO revisit this with JDK-8247216\n+    bool mergemem_only = true;\n@@ -2198,0 +2228,2 @@\n+      } else {\n+        mergemem_only = false;\n@@ -2202,1 +2234,1 @@\n-    if (!saw_self && adr_type() == TypePtr::BOTTOM)  merge_width = 0;\n+    if (!mergemem_only && !saw_self && adr_type() == TypePtr::BOTTOM)  merge_width = 0;\n@@ -2273,0 +2305,5 @@\n+            if (igvn) {\n+              \/\/ TODO revisit this with JDK-8247216\n+              \/\/ Put 'n' on the worklist because it might be modified by MergeMemStream::iteration_setup\n+              igvn->_worklist.push(n);\n+            }\n@@ -2695,0 +2732,6 @@\n+\n+  \/\/ CheckCastPPNode::Ideal() for inline types reuses the exception\n+  \/\/ paths of a call to perform an allocation: we can see a Phi here.\n+  if (in(1)->is_Phi()) {\n+    return this;\n+  }\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":68,"deletions":25,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -1323,1 +1323,1 @@\n-Node *DivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModINode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1338,1 +1338,1 @@\n-Node *DivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModLNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n","filename":"src\/hotspot\/share\/opto\/divnode.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -145,0 +145,10 @@\n+    if ((n->Opcode() == Op_LoadX || n->Opcode() == Op_StoreX) &&\n+        !n->in(MemNode::Address)->is_AddP() &&\n+        _igvn->type(n->in(MemNode::Address))->isa_oopptr()) {\n+      \/\/ Load\/Store at mark work address is at offset 0 so has no AddP which confuses EA\n+      Node* addp = new AddPNode(n->in(MemNode::Address), n->in(MemNode::Address), _igvn->MakeConX(0));\n+      _igvn->register_new_node_with_optimizer(addp);\n+      _igvn->replace_input_of(n, MemNode::Address, addp);\n+      ideal_nodes.push(addp);\n+      _nodes.at_put_grow(addp->_idx, NULL, NULL);\n+    }\n@@ -390,1 +400,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_sig();\n@@ -463,0 +473,11 @@\n+      } else if (n->as_Call()->tf()->returns_inline_type_as_fields()) {\n+        bool returns_oop = false;\n+        for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax && !returns_oop; i++) {\n+          ProjNode* pn = n->fast_out(i)->as_Proj();\n+          if (pn->_con >= TypeFunc::Parms && pn->bottom_type()->isa_ptr()) {\n+            returns_oop = true;\n+          }\n+        }\n+        if (returns_oop) {\n+          add_call_node(n->as_Call());\n+        }\n@@ -494,0 +515,1 @@\n+    case Op_InlineTypePtr:\n@@ -566,2 +588,4 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_Call() &&\n+          (n->in(0)->as_Call()->returns_pointer() || n->bottom_type()->isa_ptr())) {\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+               n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -663,0 +687,1 @@\n+    case Op_InlineTypePtr:\n@@ -722,2 +747,4 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_Call() &&\n+          (n->in(0)->as_Call()->returns_pointer()|| n->bottom_type()->isa_ptr())) {\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+               n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -883,1 +910,1 @@\n-  assert(call->returns_pointer(), \"only for call which returns pointer\");\n+  assert(call->returns_pointer() || call->tf()->returns_inline_type_as_fields(), \"only for call which returns pointer\");\n@@ -970,1 +997,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -1017,1 +1044,1 @@\n-      const TypeTuple * d = call->tf()->domain();\n+      const TypeTuple * d = call->tf()->domain_sig();\n@@ -1047,1 +1074,4 @@\n-                               (aat->isa_aryptr() && aat->isa_aryptr()->klass()->is_obj_array_klass()));\n+                               (aat->isa_aryptr() && aat->isa_aryptr()->klass()->is_obj_array_klass()) ||\n+                               (aat->isa_aryptr() && aat->isa_aryptr()->elem() != NULL &&\n+                                aat->isa_aryptr()->is_flat() &&\n+                                aat->isa_aryptr()->elem()->inline_klass()->contains_oops()));\n@@ -1092,0 +1122,3 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"vectorizedMismatch\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"load_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_unknown_inline\") == 0 ||\n@@ -1153,1 +1186,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -1197,1 +1230,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_cc();\n@@ -1717,1 +1750,1 @@\n-                tty->print_cr(\"----------missed referernce to object-----------\");\n+                tty->print_cr(\"----------missed reference to object------------\");\n@@ -1719,1 +1752,1 @@\n-                tty->print_cr(\"----------object referernced by init store -----\");\n+                tty->print_cr(\"----------object referenced by init store-------\");\n@@ -1789,1 +1822,1 @@\n-         ptn->set_scalar_replaceable(false);\n+        ptn->set_scalar_replaceable(false);\n@@ -1952,1 +1985,3 @@\n-          if (not_global_escape(alock->obj_node())) {\n+          const Type* obj_type = igvn->type(alock->obj_node());\n+          if (not_global_escape(alock->obj_node()) &&\n+              !obj_type->isa_inlinetype() && !obj_type->is_inlinetypeptr()) {\n@@ -2154,0 +2189,1 @@\n+  int field_offset = adr_type->isa_aryptr() ? adr_type->isa_aryptr()->field_offset().get() : Type::OffsetBot;\n@@ -2155,1 +2191,1 @@\n-  if (offset == Type::OffsetBot) {\n+  if (offset == Type::OffsetBot && field_offset == Type::OffsetBot) {\n@@ -2167,1 +2203,1 @@\n-      ciField* field = _compile->alias_type(adr_type->isa_instptr())->field();\n+      ciField* field = _compile->alias_type(adr_type->is_ptr())->field();\n@@ -2187,1 +2223,7 @@\n-        bt = elemtype->array_element_basic_type();\n+        if (elemtype->isa_inlinetype() && field_offset != Type::OffsetBot) {\n+          ciInlineKlass* vk = elemtype->inline_klass();\n+          field_offset += vk->first_field_offset();\n+          bt = vk->get_field_by_offset(field_offset, false)->layout_type();\n+        } else {\n+          bt = elemtype->array_element_basic_type();\n+        }\n@@ -2205,1 +2247,1 @@\n-  assert(!_collecting, \"should not call when contructed graph\");\n+  assert(!_collecting, \"should not call when constructed graph\");\n@@ -2366,3 +2408,1 @@\n-  const TypePtr *t_ptr = adr_type->isa_ptr();\n-  assert(t_ptr != NULL, \"must be a pointer type\");\n-  return t_ptr->offset();\n+  return adr_type->is_ptr()->flattened_offset();\n@@ -2522,1 +2562,8 @@\n-    t = base_t->add_offset(offs)->is_oopptr();\n+    if (base_t->isa_aryptr() != NULL) {\n+      \/\/ In the case of a flattened inline type array, each field has its\n+      \/\/ own slice so we need to extract the field being accessed from\n+      \/\/ the address computation\n+      t = base_t->isa_aryptr()->add_field_offset_and_offset(offs)->is_oopptr();\n+    } else {\n+      t = base_t->add_offset(offs)->is_oopptr();\n+    }\n@@ -2524,1 +2571,1 @@\n-  int inst_id =  base_t->instance_id();\n+  int inst_id = base_t->instance_id();\n@@ -2538,1 +2585,1 @@\n-  \/\/ It could happened when CHA type is different from MDO type on a dead path\n+  \/\/ It could happen when CHA type is different from MDO type on a dead path\n@@ -2548,1 +2595,12 @@\n-  const TypeOopPtr *tinst = base_t->add_offset(t->offset())->is_oopptr();\n+  const TypePtr* tinst = base_t->add_offset(t->offset());\n+  if (tinst->isa_aryptr() && t->isa_aryptr()) {\n+    \/\/ In the case of a flattened inline type array, each field has its\n+    \/\/ own slice so we need to keep track of the field being accessed.\n+    tinst = tinst->is_aryptr()->with_field_offset(t->is_aryptr()->field_offset().get());\n+    \/\/ Keep array properties (not flat\/null-free)\n+    tinst = tinst->is_aryptr()->update_properties(t->is_aryptr());\n+    if (tinst == NULL) {\n+      return false; \/\/ Skip dead path with inconsistent properties\n+    }\n+  }\n+\n@@ -3221,0 +3279,7 @@\n+          if (tn_t->isa_aryptr()) {\n+            \/\/ Keep array properties (not flat\/null-free)\n+            tinst = tinst->is_aryptr()->update_properties(tn_t->is_aryptr());\n+            if (tinst == NULL) {\n+              continue; \/\/ Skip dead path with inconsistent properties\n+            }\n+          }\n@@ -3246,1 +3311,1 @@\n-      if(use->is_Mem() && use->in(MemNode::Address) == n) {\n+      if (use->is_Mem() && use->in(MemNode::Address) == n) {\n@@ -3282,0 +3347,8 @@\n+      } else if (use->Opcode() == Op_Return) {\n+        assert(_compile->tf()->returns_inline_type_as_fields(), \"must return an inline type\");\n+        \/\/ Get InlineKlass by removing the tag bit from the metadata pointer\n+        Node* klass = use->in(TypeFunc::Parms);\n+        intptr_t ptr = igvn->type(klass)->isa_rawptr()->get_con();\n+        clear_nth_bit(ptr, 0);\n+        assert(Metaspace::contains((void*)ptr), \"should be klass\");\n+        assert(((InlineKlass*)ptr)->contains_oops(), \"returned inline type must contain a reference field\");\n@@ -3293,1 +3366,1 @@\n-              op == Op_SubTypeCheck ||\n+              op == Op_SubTypeCheck || op == Op_InlineType || op == Op_InlineTypePtr ||\n@@ -3361,0 +3434,3 @@\n+    } else if (n->is_CallLeaf() && n->as_CallLeaf()->_name != NULL &&\n+               strcmp(n->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+      n = n->as_CallLeaf()->proj_out(TypeFunc::Memory);\n@@ -3401,1 +3477,1 @@\n-      } else if(use->is_Mem()) {\n+      } else if (use->is_Mem()) {\n@@ -3410,0 +3486,4 @@\n+      } else if (use->is_CallLeaf() && use->as_CallLeaf()->_name != NULL &&\n+                 strcmp(use->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+        \/\/ store_unknown_inline overwrites destination array\n+        memnode_worklist.append_if_missing(use);\n@@ -3431,1 +3511,1 @@\n-  \/\/            instance type to the the input corresponding to its alias index.\n+  \/\/            instance type to the input corresponding to its alias index.\n@@ -3503,2 +3583,2 @@\n-  \/\/ to recursively process Phi's encounted on the input memory\n-  \/\/ chains as is done in split_memory_phi() since they  will\n+  \/\/ to recursively process Phi's encountered on the input memory\n+  \/\/ chains as is done in split_memory_phi() since they will\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":112,"deletions":32,"binary":false,"changes":144,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -39,0 +41,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -240,1 +243,1 @@\n-               tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n+        tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n@@ -858,0 +861,1 @@\n+  case T_INLINE_TYPE:\n@@ -985,1 +989,1 @@\n-      BasicType ary_elem  = ary_t->klass()->as_array_klass()->element_type()->basic_type();\n+      BasicType ary_elem = ary_t->klass()->as_array_klass()->element_type()->basic_type();\n@@ -988,0 +992,4 @@\n+      if (ary_t->klass()->is_flat_array_klass()) {\n+        ciFlatArrayKlass* vak = ary_t->klass()->as_flat_array_klass();\n+        shift = vak->log2_element_size();\n+      }\n@@ -1115,0 +1123,6 @@\n+      assert(memory_type() != T_INLINE_TYPE, \"should not be used for inline types\");\n+      Node* default_value = ld_alloc->in(AllocateNode::DefaultValue);\n+      if (default_value != NULL) {\n+        return default_value;\n+      }\n+      assert(ld_alloc->in(AllocateNode::RawDefaultValue) == NULL, \"default value may not be null\");\n@@ -1182,0 +1196,27 @@\n+  \/\/ Loading from an InlineTypePtr? The InlineTypePtr has the values of\n+  \/\/ all fields as input. Look for the field with matching offset.\n+  Node* addr = in(Address);\n+  intptr_t offset;\n+  Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);\n+  if (base != NULL && base->is_InlineTypePtr() && offset > oopDesc::klass_offset_in_bytes()) {\n+    Node* value = base->as_InlineTypePtr()->field_value_by_offset((int)offset, true);\n+    if (value->is_InlineType()) {\n+      \/\/ Non-flattened inline type field\n+      InlineTypeNode* vt = value->as_InlineType();\n+      if (vt->is_allocated(phase)) {\n+        value = vt->get_oop();\n+      } else {\n+        \/\/ Not yet allocated, bail out\n+        value = NULL;\n+      }\n+    }\n+    if (value != NULL) {\n+      if (Opcode() == Op_LoadN) {\n+        \/\/ Encode oop value if we are loading a narrow oop\n+        assert(!phase->type(value)->isa_narrowoop(), \"should already be decoded\");\n+        value = phase->transform(new EncodePNode(value, bottom_type()));\n+      }\n+      return value;\n+    }\n+  }\n+\n@@ -1792,2 +1833,6 @@\n-  AllocateNode* alloc = is_new_object_mark_load(phase);\n-  if (alloc != NULL && alloc->Opcode() == Op_Allocate && UseBiasedLocking) {\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(address, phase);\n+  if (alloc != NULL && mem->is_Proj() &&\n+      mem->in(0) != NULL &&\n+      mem->in(0) == alloc->initialization() &&\n+      Opcode() == Op_LoadX &&\n+      alloc->initialization()->proj_out_or_null(0) != NULL) {\n@@ -1796,1 +1841,1 @@\n-    return alloc->make_ideal_mark(phase, address, control, mem);\n+    return alloc->make_ideal_mark(phase, control, mem);\n@@ -1888,0 +1933,1 @@\n+        && t->isa_inlinetype() == NULL\n@@ -1922,0 +1968,1 @@\n+            tp->is_oopptr()->klass() == ciEnv::current()->Class_klass() ||\n@@ -1927,1 +1974,3 @@\n-    \/\/ Optimize loads from constant fields.\n+    BasicType bt = memory_type();\n+\n+    \/\/ Optimize loads from constant fields.\n@@ -1931,1 +1980,10 @@\n-      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), memory_type());\n+      ciType* mirror_type = const_oop->as_instance()->java_mirror_type();\n+      if (mirror_type != NULL && mirror_type->is_inlinetype()) {\n+        ciInlineKlass* vk = mirror_type->as_inline_klass();\n+        if (off == vk->default_value_offset()) {\n+          \/\/ Loading a special hidden field that contains the oop of the default inline type\n+          const Type* const_oop = TypeInstPtr::make(vk->default_instance());\n+          return (bt == T_NARROWOOP) ? const_oop->make_narrowoop() : const_oop;\n+        }\n+      }\n+      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), bt);\n@@ -1939,0 +1997,1 @@\n+            tp->is_klassptr()->klass() == NULL ||\n@@ -1945,15 +2004,31 @@\n-  } else if (tp->base() == Type::RawPtr && adr->is_Load() && off == 0) {\n-    \/* With mirrors being an indirect in the Klass*\n-     * the VM is now using two loads. LoadKlass(LoadP(LoadP(Klass, mirror_offset), zero_offset))\n-     * The LoadP from the Klass has a RawPtr type (see LibraryCallKit::load_mirror_from_klass).\n-     *\n-     * So check the type and klass of the node before the LoadP.\n-     *\/\n-    Node* adr2 = adr->in(MemNode::Address);\n-    const TypeKlassPtr* tkls = phase->type(adr2)->isa_klassptr();\n-    if (tkls != NULL && !StressReflectiveCode) {\n-      ciKlass* klass = tkls->klass();\n-      if (klass->is_loaded() && tkls->klass_is_exact() && tkls->offset() == in_bytes(Klass::java_mirror_offset())) {\n-        assert(adr->Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n-        assert(Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n-        return TypeInstPtr::make(klass->java_mirror());\n+  } else if (tp->base() == Type::RawPtr && !StressReflectiveCode) {\n+    if (adr->is_Load() && off == 0) {\n+      \/* With mirrors being an indirect in the Klass*\n+       * the VM is now using two loads. LoadKlass(LoadP(LoadP(Klass, mirror_offset), zero_offset))\n+       * The LoadP from the Klass has a RawPtr type (see LibraryCallKit::load_mirror_from_klass).\n+       *\n+       * So check the type and klass of the node before the LoadP.\n+       *\/\n+      Node* adr2 = adr->in(MemNode::Address);\n+      const TypeKlassPtr* tkls = phase->type(adr2)->isa_klassptr();\n+      if (tkls != NULL) {\n+        ciKlass* klass = tkls->klass();\n+        if (klass != NULL && klass->is_loaded() && tkls->klass_is_exact() && tkls->offset() == in_bytes(Klass::java_mirror_offset())) {\n+          assert(adr->Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n+          assert(Opcode() == Op_LoadP, \"must load an oop from _java_mirror\");\n+          return TypeInstPtr::make(klass->java_mirror());\n+        }\n+      }\n+    } else {\n+      \/\/ Check for a load of the default value offset from the InlineKlassFixedBlock:\n+      \/\/ LoadI(LoadP(inline_klass, adr_inlineklass_fixed_block_offset), default_value_offset_offset)\n+      intptr_t offset = 0;\n+      Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+      if (base != NULL && base->is_Load() && offset == in_bytes(InlineKlass::default_value_offset_offset())) {\n+        const TypeKlassPtr* tkls = phase->type(base->in(MemNode::Address))->isa_klassptr();\n+        if (tkls != NULL && tkls->is_loaded() && tkls->klass_is_exact() && tkls->isa_inlinetype() &&\n+            tkls->offset() == in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())) {\n+          assert(base->Opcode() == Op_LoadP, \"must load an oop from klass\");\n+          assert(Opcode() == Op_LoadI, \"must load an int from fixed block\");\n+          return TypeInt::make(tkls->klass()->as_inline_klass()->default_value_offset());\n+        }\n@@ -1967,1 +2042,1 @@\n-    if (klass->is_loaded() && tkls->klass_is_exact()) {\n+    if (tkls->is_loaded() && tkls->klass_is_exact()) {\n@@ -1994,1 +2069,1 @@\n-    if (klass->is_loaded() ) {\n+    if (tkls->is_loaded()) {\n@@ -2057,4 +2132,5 @@\n-\n-  Node* alloc = is_new_object_mark_load(phase);\n-  if (alloc != NULL && !(alloc->Opcode() == Op_Allocate && UseBiasedLocking)) {\n-    return TypeX::make(markWord::prototype().value());\n+  if (!EnableValhalla) { \/\/ CMH: Fix JDK-8255045\n+    Node* alloc = is_new_object_mark_load(phase);\n+    if (alloc != NULL && !(alloc->Opcode() == Op_Allocate && UseBiasedLocking)) {\n+      return TypeX::make(markWord::prototype().value());\n+    }\n@@ -2199,1 +2275,2 @@\n-Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at, const TypeKlassPtr* tk) {\n+Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at,\n+                          const TypeKlassPtr* tk) {\n@@ -2286,1 +2363,1 @@\n-      return TypeKlassPtr::make(TypePtr::NotNull, ik, 0\/*offset*\/);\n+      return TypeKlassPtr::make(TypePtr::NotNull, ik, Type::Offset(0), tinst->flatten_array());\n@@ -2292,1 +2369,1 @@\n-  if( tary != NULL ) {\n+  if (tary != NULL) {\n@@ -2299,1 +2376,1 @@\n-      ciArrayKlass *ak = tary->klass()->as_array_klass();\n+      ciArrayKlass* ak = tary_klass->as_array_klass();\n@@ -2302,2 +2379,2 @@\n-      if( ak->is_obj_array_klass() ) {\n-        assert( ak->is_loaded(), \"\" );\n+      if (ak->is_obj_array_klass()) {\n+        assert(ak->is_loaded(), \"\");\n@@ -2305,2 +2382,2 @@\n-        if( base_k->is_loaded() && base_k->is_instance_klass() ) {\n-          ciInstanceKlass* ik = base_k->as_instance_klass();\n+        if (base_k->is_loaded() && base_k->is_instance_klass()) {\n+          ciInstanceKlass *ik = base_k->as_instance_klass();\n@@ -2317,3 +2394,2 @@\n-        return TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n-      } else {                  \/\/ Found a type-array?\n-        assert( ak->is_type_array_klass(), \"\" );\n+        return TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n+      } else if (ak->is_type_array_klass()) {\n@@ -2328,2 +2404,1 @@\n-    ciKlass* klass = tkls->klass();\n-    if( !klass->is_loaded() )\n+    if (!tkls->is_loaded()) {\n@@ -2331,0 +2406,2 @@\n+    }\n+    ciKlass* klass = tkls->klass();\n@@ -2340,1 +2417,5 @@\n-      return TypeKlassPtr::make(tkls->ptr(), elem, 0\/*offset*\/);\n+      return TypeKlassPtr::make(tkls->ptr(), elem, Type::Offset(0));\n+    } else if (klass->is_flat_array_klass() &&\n+               tkls->offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {\n+      ciKlass* elem = klass->as_flat_array_klass()->element_klass();\n+      return TypeKlassPtr::make(tkls->ptr(), elem, Type::Offset(0), \/* flatten_array= *\/ true);\n@@ -2548,0 +2629,1 @@\n+  case T_INLINE_TYPE:\n@@ -2609,1 +2691,1 @@\n-  {\n+  if (phase->C->get_adr_type(phase->C->get_alias_index(adr_type())) != TypeAryPtr::INLINES) {\n@@ -2629,0 +2711,1 @@\n+             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreN) ||\n@@ -2727,2 +2810,1 @@\n-  if (result == this &&\n-      ReduceFieldZeroing && phase->type(val)->is_zero_type()) {\n+  if (result == this && ReduceFieldZeroing) {\n@@ -2730,1 +2812,3 @@\n-    if (mem->is_Proj() && mem->in(0)->is_Allocate()) {\n+    if (mem->is_Proj() && mem->in(0)->is_Allocate() &&\n+        (phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::DefaultValue) == val)) {\n+      assert(!phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::DefaultValue) == NULL, \"storing null to inline type array is forbidden\");\n@@ -2743,1 +2827,9 @@\n-          result = mem;\n+          if (phase->type(val)->is_zero_type()) {\n+            result = mem;\n+          } else if (prev_mem->is_Proj() && prev_mem->in(0)->is_Initialize()) {\n+            InitializeNode* init = prev_mem->in(0)->as_Initialize();\n+            AllocateNode* alloc = init->allocation();\n+            if (alloc != NULL && alloc->in(AllocateNode::DefaultValue) == val) {\n+              result = mem;\n+            }\n+          }\n@@ -3052,1 +3144,1 @@\n-    return new ClearArrayNode(in(0), in(1), in(2), in(3), true);\n+    return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);\n@@ -3067,1 +3159,1 @@\n-  Node *zero = phase->makecon(TypeLong::ZERO);\n+  Node *val = in(4);\n@@ -3069,1 +3161,1 @@\n-  mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+  mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -3074,1 +3166,1 @@\n-    mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+    mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -3108,0 +3200,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -3118,1 +3212,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != NULL) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == NULL, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -3125,1 +3225,1 @@\n-  return clear_memory(ctl, mem, dest, phase->MakeConX(offset), end_offset, phase);\n+  return clear_memory(ctl, mem, dest, raw_val, phase->MakeConX(offset), end_offset, phase);\n@@ -3129,0 +3229,1 @@\n+                                   Node* raw_val,\n@@ -3151,1 +3252,4 @@\n-  mem = new ClearArrayNode(ctl, mem, zsize, adr, false);\n+  if (raw_val == NULL) {\n+    raw_val = phase->MakeConX(0);\n+  }\n+  mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);\n@@ -3156,0 +3260,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -3170,1 +3276,1 @@\n-    mem = clear_memory(ctl, mem, dest,\n+    mem = clear_memory(ctl, mem, dest, val, raw_val,\n@@ -3177,1 +3283,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != NULL) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == NULL, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -3316,1 +3428,1 @@\n-Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {\n+Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {\n@@ -3602,1 +3714,3 @@\n-  if (init == NULL || init->is_complete())  return false;\n+  if (init == NULL || init->is_complete()) {\n+    return false;\n+  }\n@@ -4361,0 +4475,2 @@\n+                                              allocation()->in(AllocateNode::DefaultValue),\n+                                              allocation()->in(AllocateNode::RawDefaultValue),\n@@ -4420,0 +4536,2 @@\n+                                            allocation()->in(AllocateNode::DefaultValue),\n+                                            allocation()->in(AllocateNode::RawDefaultValue),\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":176,"deletions":58,"binary":false,"changes":234,"status":"modified"},{"patch":"@@ -160,0 +160,12 @@\n+  \/\/ Code pattern on return from a call that returns an __Value.  Can\n+  \/\/ be optimized away if the return value turns out to be an oop.\n+  if (op == Op_AndX &&\n+      in(1) != NULL &&\n+      in(1)->Opcode() == Op_CastP2X &&\n+      in(1)->in(1) != NULL &&\n+      phase->type(in(1)->in(1))->isa_oopptr() &&\n+      t2->isa_intptr_t()->_lo >= 0 &&\n+      t2->isa_intptr_t()->_hi <= MinObjAlignmentInBytesMask) {\n+    return add_id();\n+  }\n+\n@@ -593,0 +605,7 @@\n+\n+    if (con == markWord::inline_type_pattern) {\n+      assert(EnableValhalla, \"should only be used for inline types\");\n+      if (in(1)->is_Load() && phase->type(in(1)->in(MemNode::Address))->is_inlinetypeptr()) {\n+        return in(2); \/\/ Obj is known to be an inline type\n+      }\n+    }\n","filename":"src\/hotspot\/share\/opto\/mulnode.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -470,1 +470,1 @@\n-  virtual void record_for_igvn(Node *n) { }\n+  virtual void record_for_igvn(Node *n) { _worklist.push(n); }\n@@ -522,0 +522,2 @@\n+  void replace_in_uses(Node* n, Node* m);\n+\n","filename":"src\/hotspot\/share\/opto\/phaseX.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -754,1 +754,8 @@\n-Node *CmpLNode::Ideal( PhaseGVN *phase, bool can_reshape ) {\n+\/\/------------------------------Ideal------------------------------------------\n+Node* CmpLNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* a = NULL;\n+  Node* b = NULL;\n+  if (is_double_null_check(phase, a, b) && (phase->type(a)->is_zero_type() || phase->type(b)->is_zero_type())) {\n+    \/\/ Degraded to a simple null check, use old acmp\n+    return new CmpPNode(a, b);\n+  }\n@@ -765,0 +772,25 @@\n+\/\/ Match double null check emitted by Compile::optimize_acmp()\n+bool CmpLNode::is_double_null_check(PhaseGVN* phase, Node*& a, Node*& b) const {\n+  if (in(1)->Opcode() == Op_OrL &&\n+      in(1)->in(1)->Opcode() == Op_CastP2X &&\n+      in(1)->in(2)->Opcode() == Op_CastP2X &&\n+      in(2)->bottom_type()->is_zero_type()) {\n+    assert(EnableValhalla, \"unexpected double null check\");\n+    a = in(1)->in(1)->in(1);\n+    b = in(1)->in(2)->in(1);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\/\/------------------------------Value------------------------------------------\n+const Type* CmpLNode::Value(PhaseGVN* phase) const {\n+  Node* a = NULL;\n+  Node* b = NULL;\n+  if (is_double_null_check(phase, a, b) && (!phase->type(a)->maybe_null() || !phase->type(b)->maybe_null())) {\n+    \/\/ One operand is never NULL, emit constant false\n+    return TypeInt::CC_GT;\n+  }\n+  return SubNode::Value(phase);\n+}\n+\n@@ -923,0 +955,8 @@\n+      if (!unrelated_classes) {\n+        \/\/ Handle inline type arrays\n+        if ((r0->flatten_array() && (!r1->can_be_inline_type() || (klass1->is_inlinetype() && !klass1->flatten_array()))) ||\n+            (r1->flatten_array() && (!r0->can_be_inline_type() || (klass0->is_inlinetype() && !klass0->flatten_array())))) {\n+          \/\/ One type is flattened in arrays but the other type is not. Must be unrelated.\n+          unrelated_classes = true;\n+        }\n+      }\n@@ -1008,1 +1048,1 @@\n-Node *CmpPNode::Ideal( PhaseGVN *phase, bool can_reshape ) {\n+Node* CmpPNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":42,"deletions":2,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -26,0 +26,3 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciField.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -50,0 +53,46 @@\n+const Type::Offset Type::Offset::top(Type::OffsetTop);\n+const Type::Offset Type::Offset::bottom(Type::OffsetBot);\n+\n+const Type::Offset Type::Offset::meet(const Type::Offset other) const {\n+  \/\/ Either is 'TOP' offset?  Return the other offset!\n+  int offset = other._offset;\n+  if (_offset == OffsetTop) return Offset(offset);\n+  if (offset == OffsetTop) return Offset(_offset);\n+  \/\/ If either is different, return 'BOTTOM' offset\n+  if (_offset != offset) return bottom;\n+  return Offset(_offset);\n+}\n+\n+const Type::Offset Type::Offset::dual() const {\n+  if (_offset == OffsetTop) return bottom;\/\/ Map 'TOP' into 'BOTTOM'\n+  if (_offset == OffsetBot) return top;\/\/ Map 'BOTTOM' into 'TOP'\n+  return Offset(_offset);               \/\/ Map everything else into self\n+}\n+\n+const Type::Offset Type::Offset::add(intptr_t offset) const {\n+  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n+  if (_offset == OffsetTop || offset == OffsetTop) return top;\n+  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n+  if (_offset == OffsetBot || offset == OffsetBot) return bottom;\n+  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n+  offset += (intptr_t)_offset;\n+  if (offset != (int)offset || offset == OffsetTop) return bottom;\n+\n+  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n+  \/\/ It is possible to construct a negative offset during PhaseCCP\n+\n+  return Offset((int)offset);        \/\/ Sum valid offsets\n+}\n+\n+void Type::Offset::dump2(outputStream *st) const {\n+  if (_offset == 0) {\n+    return;\n+  } else if (_offset == OffsetTop) {\n+    st->print(\"+top\");\n+  }\n+  else if (_offset == OffsetBot) {\n+    st->print(\"+bot\");\n+  } else if (_offset) {\n+    st->print(\"+%d\", _offset);\n+  }\n+}\n@@ -86,0 +135,1 @@\n+  { Bad,             T_INLINE_TYPE, \"inline:\",      false, Node::NotAMachineReg, relocInfo::none          },  \/\/ InlineType\n@@ -216,0 +266,9 @@\n+  case T_INLINE_TYPE: {\n+    ciInlineKlass* vk = type->as_inline_klass();\n+    if (vk->is_scalarizable()) {\n+      return TypeInlineType::make(vk);\n+    } else {\n+      return TypeOopPtr::make_from_klass(vk)->join_speculative(TypePtr::NOTNULL);\n+    }\n+  }\n+\n@@ -244,0 +303,1 @@\n+    case T_INLINE_TYPE:\n@@ -281,0 +341,1 @@\n+    case T_INLINE_TYPE: conbt = T_OBJECT; break;\n@@ -287,0 +348,1 @@\n+    case T_INLINE_TYPE: loadbt = T_OBJECT; break;\n@@ -522,3 +584,3 @@\n-  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, 0);\n-  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, OffsetBot);\n-  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, OffsetBot);\n+  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, Offset(0));\n+  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, Offset::bottom);\n+  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, Offset::bottom);\n@@ -541,1 +603,1 @@\n-                                           false, 0, oopDesc::mark_offset_in_bytes());\n+                                           false, 0, Offset(oopDesc::mark_offset_in_bytes()));\n@@ -543,2 +605,2 @@\n-                                           false, 0, oopDesc::klass_offset_in_bytes());\n-  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, OffsetBot, TypeOopPtr::InstanceBot);\n+                                           false, 0, Offset(oopDesc::klass_offset_in_bytes()));\n+  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, Offset::bottom, TypeOopPtr::InstanceBot);\n@@ -546,1 +608,3 @@\n-  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, NULL, OffsetBot);\n+  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, NULL, Offset::bottom);\n+\n+  TypeInlineType::BOTTOM = TypeInlineType::make(NULL);\n@@ -563,1 +627,1 @@\n-  TypeAryPtr::RANGE   = TypeAryPtr::make( TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), NULL \/* current->env()->Object_klass() *\/, false, arrayOopDesc::length_offset_in_bytes());\n+  TypeAryPtr::RANGE   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), NULL \/* current->env()->Object_klass() *\/, false, Offset(arrayOopDesc::length_offset_in_bytes()));\n@@ -565,1 +629,1 @@\n-  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), NULL \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), NULL \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -575,1 +639,1 @@\n-    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), NULL \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), NULL \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -577,7 +641,8 @@\n-  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Type::OffsetBot);\n-  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Type::OffsetBot);\n-  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Type::OffsetBot);\n-  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Type::OffsetBot);\n-  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Type::OffsetBot);\n-  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Type::OffsetBot);\n-  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Type::OffsetBot);\n+  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Offset::bottom);\n+  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Offset::bottom);\n+  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Offset::bottom);\n+  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Offset::bottom);\n+  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Offset::bottom);\n+  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Offset::bottom);\n+  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Offset::bottom);\n+  TypeAryPtr::INLINES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInlineType::BOTTOM,TypeInt::POS), NULL, false,  Offset::bottom);\n@@ -588,0 +653,1 @@\n+  TypeAryPtr::_array_body_type[T_INLINE_TYPE] = TypeAryPtr::OOPS;\n@@ -598,2 +664,2 @@\n-  TypeKlassPtr::OBJECT = TypeKlassPtr::make( TypePtr::NotNull, current->env()->Object_klass(), 0 );\n-  TypeKlassPtr::OBJECT_OR_NULL = TypeKlassPtr::make( TypePtr::BotPTR, current->env()->Object_klass(), 0 );\n+  TypeKlassPtr::OBJECT = TypeKlassPtr::make(TypePtr::NotNull, current->env()->Object_klass(), Offset(0));\n+  TypeKlassPtr::OBJECT_OR_NULL = TypeKlassPtr::make(TypePtr::BotPTR, current->env()->Object_klass(), Offset(0));\n@@ -638,0 +704,1 @@\n+  _const_basic_type[T_INLINE_TYPE] = TypeInstPtr::BOTTOM;\n@@ -654,0 +721,1 @@\n+  _zero_type[T_INLINE_TYPE] = TypePtr::NULL_PTR;\n@@ -835,1 +903,1 @@\n-  if( !interface_vs_oop(t) && (t2t != t->_dual || t2this != this->_dual) ) {\n+  if( !interface_vs_oop(t) && (t2t != t->_dual || t2this != this->_dual)) {\n@@ -939,0 +1007,3 @@\n+  case InlineType:\n+    return t->xmeet(this);\n+\n@@ -1007,0 +1078,1 @@\n+  Bad,          \/\/ InlineType - handled in v-call\n@@ -1910,0 +1982,12 @@\n+static void collect_inline_fields(ciInlineKlass* vk, const Type** field_array, uint& pos) {\n+  for (int j = 0; j < vk->nof_nonstatic_fields(); j++) {\n+    ciField* field = vk->nonstatic_field_at(j);\n+    BasicType bt = field->type()->basic_type();\n+    const Type* ft = Type::get_const_type(field->type());\n+    field_array[pos++] = ft;\n+    if (type2size[bt] == 2) {\n+      field_array[pos++] = Type::HALF;\n+    }\n+  }\n+}\n+\n@@ -1912,1 +1996,1 @@\n-const TypeTuple *TypeTuple::make_range(ciSignature* sig) {\n+const TypeTuple *TypeTuple::make_range(ciSignature* sig, bool ret_vt_fields) {\n@@ -1915,0 +1999,4 @@\n+  if (ret_vt_fields) {\n+    arg_cnt = return_type->as_inline_klass()->inline_arg_slots() + 1;\n+  }\n+\n@@ -1935,0 +2023,11 @@\n+  case T_INLINE_TYPE:\n+    if (ret_vt_fields) {\n+      uint pos = TypeFunc::Parms;\n+      field_array[pos] = TypePtr::BOTTOM;\n+      pos++;\n+      ExtendedSignature sig = ExtendedSignature(NULL, SigEntryFilter());\n+      collect_inline_fields(return_type->as_inline_klass(), field_array, pos);\n+    } else {\n+      field_array[TypeFunc::Parms] = get_const_type(return_type)->join_speculative(TypePtr::NOTNULL);\n+    }\n+    break;\n@@ -1944,2 +2043,11 @@\n-const TypeTuple *TypeTuple::make_domain(ciInstanceKlass* recv, ciSignature* sig) {\n-  uint arg_cnt = sig->size();\n+const TypeTuple *TypeTuple::make_domain(ciMethod* method, bool vt_fields_as_args) {\n+  ciSignature* sig = method->signature();\n+  ExtendedSignature sig_cc = ExtendedSignature(vt_fields_as_args ? method->get_sig_cc() : NULL, SigEntryFilter());\n+\n+  uint arg_cnt = sig->size() + (method->is_static() ? 0 : 1);\n+  if (vt_fields_as_args) {\n+    for (arg_cnt = 0; !sig_cc.at_end(); ++sig_cc) {\n+      arg_cnt += type2size[(*sig_cc)._bt];\n+    }\n+    sig_cc = ExtendedSignature(method->get_sig_cc(), SigEntryFilter());\n+  }\n@@ -1948,8 +2056,11 @@\n-  const Type **field_array;\n-  if (recv != NULL) {\n-    arg_cnt++;\n-    field_array = fields(arg_cnt);\n-    \/\/ Use get_const_type here because it respects UseUniqueSubclasses:\n-    field_array[pos++] = get_const_type(recv)->join_speculative(TypePtr::NOTNULL);\n-  } else {\n-    field_array = fields(arg_cnt);\n+  const Type** field_array = fields(arg_cnt);\n+  if (!method->is_static()) {\n+    ciInstanceKlass* recv = method->holder();\n+    if (vt_fields_as_args && recv->is_inlinetype() && recv->as_inline_klass()->can_be_passed_as_fields()) {\n+      collect_inline_fields(recv->as_inline_klass(), field_array, pos);\n+    } else {\n+      field_array[pos++] = get_const_type(recv)->join_speculative(TypePtr::NOTNULL);\n+      if (vt_fields_as_args) {\n+        ++sig_cc;\n+      }\n+    }\n@@ -1961,0 +2072,2 @@\n+    BasicType bt = type->basic_type();\n+    bool is_flattened = false;\n@@ -1962,1 +2075,1 @@\n-    switch (type->basic_type()) {\n+    switch (bt) {\n@@ -1983,0 +2096,9 @@\n+    case T_INLINE_TYPE: {\n+      if (vt_fields_as_args && type->as_inline_klass()->can_be_passed_as_fields()) {\n+        is_flattened = true;\n+        collect_inline_fields(type->as_inline_klass(), field_array, pos);\n+      } else {\n+        field_array[pos++] = get_const_type(type)->join_speculative(TypePtr::NOTNULL);\n+      }\n+      break;\n+    }\n@@ -1988,0 +2110,1 @@\n+  assert(pos == TypeFunc::Parms + arg_cnt, \"wrong number of arguments\");\n@@ -2122,1 +2245,2 @@\n-const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable) {\n+const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable,\n+                             bool not_flat, bool not_null_free) {\n@@ -2127,1 +2251,1 @@\n-  return (TypeAry*)(new TypeAry(elem,size,stable))->hashcons();\n+  return (TypeAry*)(new TypeAry(elem, size, stable, not_flat, not_null_free))->hashcons();\n@@ -2149,1 +2273,3 @@\n-                         _stable && a->_stable);\n+                         _stable && a->_stable,\n+                         _not_flat && a->_not_flat,\n+                         _not_null_free && a->_not_null_free);\n@@ -2162,1 +2288,1 @@\n-  return new TypeAry(_elem->dual(), size_dual, !_stable);\n+  return new TypeAry(_elem->dual(), size_dual, !_stable, !_not_flat, !_not_null_free);\n@@ -2171,1 +2297,4 @@\n-    _size == a->_size;\n+    _size == a->_size &&\n+    _not_flat == a->_not_flat &&\n+    _not_null_free == a->_not_null_free;\n+\n@@ -2184,1 +2313,1 @@\n-  return make(_elem->remove_speculative(), _size, _stable);\n+  return make(_elem->remove_speculative(), _size, _stable, _not_flat, _not_null_free);\n@@ -2191,1 +2320,1 @@\n-  return make(_elem->cleanup_speculative(), _size, _stable);\n+  return make(_elem->cleanup_speculative(), _size, _stable, _not_flat, _not_null_free);\n@@ -2225,0 +2354,4 @@\n+  if (Verbose) {\n+    if (_not_flat) st->print(\"not flat:\");\n+    if (_not_null_free) st->print(\"not null free:\");\n+  }\n@@ -2278,0 +2411,124 @@\n+\/\/==============================TypeInlineType=======================================\n+\n+const TypeInlineType* TypeInlineType::BOTTOM;\n+\n+\/\/------------------------------make-------------------------------------------\n+const TypeInlineType* TypeInlineType::make(ciInlineKlass* vk, bool larval) {\n+  return (TypeInlineType*)(new TypeInlineType(vk, larval))->hashcons();\n+}\n+\n+\/\/------------------------------meet-------------------------------------------\n+\/\/ Compute the MEET of two types.  It returns a new Type object.\n+const Type* TypeInlineType::xmeet(const Type* t) const {\n+  \/\/ Perform a fast test for common case; meeting the same types together.\n+  if(this == t) return this;  \/\/ Meeting same type-rep?\n+\n+  \/\/ Current \"this->_base\" is InlineType\n+  switch (t->base()) {          \/\/ switch on original type\n+\n+  case Int:\n+  case Long:\n+  case FloatTop:\n+  case FloatCon:\n+  case FloatBot:\n+  case DoubleTop:\n+  case DoubleCon:\n+  case DoubleBot:\n+  case NarrowKlass:\n+  case Bottom:\n+    return Type::BOTTOM;\n+\n+  case OopPtr:\n+  case MetadataPtr:\n+  case KlassPtr:\n+  case RawPtr:\n+    return TypePtr::BOTTOM;\n+\n+  case Top:\n+    return this;\n+\n+  case NarrowOop: {\n+    const Type* res = t->make_ptr()->xmeet(this);\n+    if (res->isa_ptr()) {\n+      return res->make_narrowoop();\n+    }\n+    return res;\n+  }\n+\n+  case AryPtr:\n+  case InstPtr: {\n+    return t->xmeet(this);\n+  }\n+\n+  case InlineType: {\n+    \/\/ All inline types inherit from Object\n+    const TypeInlineType* other = t->is_inlinetype();\n+    if (_vk == NULL) {\n+      return this;\n+    } else if (other->_vk == NULL) {\n+      return other;\n+    } else if (_vk == other->_vk) {\n+      if (_larval == other->_larval ||\n+          !_larval) {\n+        return this;\n+      } else {\n+        return t;\n+      }\n+    }\n+    return TypeInstPtr::NOTNULL;\n+  }\n+\n+  default:                      \/\/ All else is a mistake\n+    typerr(t);\n+\n+  }\n+  return this;\n+}\n+\n+\/\/------------------------------xdual------------------------------------------\n+const Type* TypeInlineType::xdual() const {\n+  return this;\n+}\n+\n+\/\/------------------------------eq---------------------------------------------\n+\/\/ Structural equality check for Type representations\n+bool TypeInlineType::eq(const Type* t) const {\n+  const TypeInlineType* vt = t->is_inlinetype();\n+  return (_vk == vt->inline_klass() && _larval == vt->larval());\n+}\n+\n+\/\/------------------------------hash-------------------------------------------\n+\/\/ Type-specific hashing function.\n+int TypeInlineType::hash(void) const {\n+  return (intptr_t)_vk;\n+}\n+\n+\/\/------------------------------singleton--------------------------------------\n+\/\/ TRUE if Type is a singleton type, FALSE otherwise. Singletons are simple constants.\n+bool TypeInlineType::singleton(void) const {\n+  return false;\n+}\n+\n+\/\/------------------------------empty------------------------------------------\n+\/\/ TRUE if Type is a type with no values, FALSE otherwise.\n+bool TypeInlineType::empty(void) const {\n+  return false;\n+}\n+\n+\/\/------------------------------dump2------------------------------------------\n+#ifndef PRODUCT\n+void TypeInlineType::dump2(Dict &d, uint depth, outputStream* st) const {\n+  if (_vk == NULL) {\n+    st->print(\"BOTTOM inlinetype\");\n+    return;\n+  }\n+  int count = _vk->nof_declared_nonstatic_fields();\n+  st->print(\"inlinetype[%d]:{\", count);\n+  st->print(\"%s\", count != 0 ? _vk->declared_nonstatic_field_at(0)->type()->name() : \"empty\");\n+  for (int i = 1; i < count; ++i) {\n+    st->print(\", %s\", _vk->declared_nonstatic_field_at(i)->type()->name());\n+  }\n+  st->print(\"}%s\", _larval?\" : larval\":\"\");\n+}\n+#endif\n+\n@@ -2423,1 +2680,1 @@\n-const TypePtr *TypePtr::make(TYPES t, enum PTR ptr, int offset, const TypePtr* speculative, int inline_depth) {\n+const TypePtr* TypePtr::make(TYPES t, enum PTR ptr, Offset offset, const TypePtr* speculative, int inline_depth) {\n@@ -2437,1 +2694,1 @@\n-  return _offset;\n+  return offset();\n@@ -2506,7 +2763,2 @@\n-int TypePtr::meet_offset( int offset ) const {\n-  \/\/ Either is 'TOP' offset?  Return the other offset!\n-  if( _offset == OffsetTop ) return offset;\n-  if( offset == OffsetTop ) return _offset;\n-  \/\/ If either is different, return 'BOTTOM' offset\n-  if( _offset != offset ) return OffsetBot;\n-  return _offset;\n+Type::Offset TypePtr::meet_offset(int offset) const {\n+  return _offset.meet(Offset(offset));\n@@ -2516,4 +2768,2 @@\n-int TypePtr::dual_offset( ) const {\n-  if( _offset == OffsetTop ) return OffsetBot;\/\/ Map 'TOP' into 'BOTTOM'\n-  if( _offset == OffsetBot ) return OffsetTop;\/\/ Map 'BOTTOM' into 'TOP'\n-  return _offset;               \/\/ Map everything else into self\n+Type::Offset TypePtr::dual_offset() const {\n+  return _offset.dual();\n@@ -2532,13 +2782,2 @@\n-int TypePtr::xadd_offset( intptr_t offset ) const {\n-  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n-  if( _offset == OffsetTop || offset == OffsetTop ) return OffsetTop;\n-  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n-  if( _offset == OffsetBot || offset == OffsetBot ) return OffsetBot;\n-  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n-  offset += (intptr_t)_offset;\n-  if (offset != (int)offset || offset == OffsetTop) return OffsetBot;\n-\n-  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n-  \/\/ It is possible to construct a negative offset during PhaseCCP\n-\n-  return (int)offset;        \/\/ Sum valid offsets\n+Type::Offset TypePtr::xadd_offset(intptr_t offset) const {\n+  return _offset.add(offset);\n@@ -2556,1 +2795,1 @@\n-  return _ptr == a->ptr() && _offset == a->offset() && eq_speculative(a) && _inline_depth == a->_inline_depth;\n+  return _ptr == a->ptr() && _offset == a->_offset && eq_speculative(a) && _inline_depth == a->_inline_depth;\n@@ -2562,1 +2801,1 @@\n-  return java_add(java_add((jint)_ptr, (jint)_offset), java_add((jint)hash_speculative(), (jint)_inline_depth));\n+  return java_add(java_add((jint)_ptr, (jint)offset()), java_add((jint)hash_speculative(), (jint)_inline_depth));\n@@ -2822,3 +3061,1 @@\n-  if( _offset == OffsetTop ) st->print(\"+top\");\n-  else if( _offset == OffsetBot ) st->print(\"+bot\");\n-  else if( _offset ) st->print(\"+%d\", _offset);\n+  _offset.dump2(st);\n@@ -2859,1 +3096,1 @@\n-  return (_offset != OffsetBot) && !below_centerline(_ptr);\n+  return (_offset != Offset::bottom) && !below_centerline(_ptr);\n@@ -2863,1 +3100,1 @@\n-  return (_offset == OffsetTop) || above_centerline(_ptr);\n+  return (_offset == Offset::top) || above_centerline(_ptr);\n@@ -3005,1 +3242,1 @@\n-TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, bool xk, ciObject* o, int offset,\n+TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, bool xk, ciObject* o, Offset offset, Offset field_offset,\n@@ -3015,2 +3252,2 @@\n-      (offset > 0) && xk && (k != 0) && k->is_instance_klass()) {\n-    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset);\n+      (offset.get() > 0) && xk && (k != 0) && k->is_instance_klass()) {\n+    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset.get());\n@@ -3019,2 +3256,2 @@\n-  if (_offset > 0 || _offset == Type::OffsetTop || _offset == Type::OffsetBot) {\n-    if (_offset == oopDesc::klass_offset_in_bytes()) {\n+  if (this->offset() > 0 || this->offset() == Type::OffsetTop || this->offset() == Type::OffsetBot) {\n+    if (this->offset() == oopDesc::klass_offset_in_bytes()) {\n@@ -3026,3 +3263,12 @@\n-    } else if (this->isa_aryptr()) {\n-      _is_ptr_to_narrowoop = (UseCompressedOops && klass()->is_obj_array_klass() &&\n-                             _offset != arrayOopDesc::length_offset_in_bytes());\n+    } else if (UseCompressedOops && this->isa_aryptr() && this->offset() != arrayOopDesc::length_offset_in_bytes()) {\n+      if (klass()->is_obj_array_klass()) {\n+        _is_ptr_to_narrowoop = true;\n+      } else if (klass()->is_flat_array_klass() && field_offset != Offset::top && field_offset != Offset::bottom) {\n+        \/\/ Check if the field of the inline type array element contains oops\n+        ciInlineKlass* vk = klass()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+        int foffset = field_offset.get() + vk->first_field_offset();\n+        ciField* field = vk->get_field_by_offset(foffset, false);\n+        assert(field != NULL, \"missing field\");\n+        BasicType bt = field->layout_type();\n+        _is_ptr_to_narrowoop = UseCompressedOops && is_reference_type(bt);\n+      }\n@@ -3030,2 +3276,0 @@\n-      ciInstanceKlass* ik = klass()->as_instance_klass();\n-      ciField* field = NULL;\n@@ -3034,1 +3278,1 @@\n-      } else if (_offset == OffsetBot || _offset == OffsetTop) {\n+      } else if (_offset == Offset::bottom || _offset == Offset::top) {\n@@ -3039,3 +3283,2 @@\n-\n-            (_offset == java_lang_Class::klass_offset() ||\n-             _offset == java_lang_Class::array_klass_offset())) {\n+            (this->offset() == java_lang_Class::klass_offset() ||\n+             this->offset() == java_lang_Class::array_klass_offset())) {\n@@ -3047,1 +3290,1 @@\n-                   _offset >= InstanceMirrorKlass::offset_of_static_fields()) {\n+                   this->offset() >= InstanceMirrorKlass::offset_of_static_fields()) {\n@@ -3052,8 +3295,14 @@\n-            field = k->get_field_by_offset(_offset, true);\n-          }\n-          if (field != NULL) {\n-            BasicType basic_elem_type = field->layout_type();\n-            _is_ptr_to_narrowoop = UseCompressedOops && is_reference_type(basic_elem_type);\n-          } else {\n-            \/\/ unsafe access\n-            _is_ptr_to_narrowoop = UseCompressedOops;\n+            if (k->is_inlinetype() && this->offset() == k->as_inline_klass()->default_value_offset()) {\n+              \/\/ Special hidden field that contains the oop of the default inline type\n+              \/\/ basic_elem_type = T_INLINE_TYPE;\n+             _is_ptr_to_narrowoop = UseCompressedOops;\n+            } else {\n+              field = k->get_field_by_offset(this->offset(), true);\n+              if (field != NULL) {\n+                BasicType basic_elem_type = field->layout_type();\n+                _is_ptr_to_narrowoop = UseCompressedOops && is_reference_type(basic_elem_type);\n+              } else {\n+                \/\/ unsafe access\n+                _is_ptr_to_narrowoop = UseCompressedOops;\n+              }\n+            }\n@@ -3063,1 +3312,2 @@\n-          field = ik->get_field_by_offset(_offset, false);\n+          ciInstanceKlass* ik = klass()->as_instance_klass();\n+          ciField* field = ik->get_field_by_offset(this->offset(), false);\n@@ -3083,2 +3333,2 @@\n-const TypeOopPtr *TypeOopPtr::make(PTR ptr, int offset, int instance_id,\n-                                     const TypePtr* speculative, int inline_depth) {\n+const TypeOopPtr *TypeOopPtr::make(PTR ptr, Offset offset, int instance_id,\n+                                   const TypePtr* speculative, int inline_depth) {\n@@ -3089,1 +3339,1 @@\n-  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, xk, o, offset, instance_id, speculative, inline_depth))->hashcons();\n+  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, xk, o, offset, Offset::bottom, instance_id, speculative, inline_depth))->hashcons();\n@@ -3114,1 +3364,0 @@\n-\n@@ -3124,1 +3373,1 @@\n-    return TypeKlassPtr::make(xk? Constant: NotNull, k, 0);\n+    return TypeKlassPtr::make(xk? Constant: NotNull, k, Offset(0));\n@@ -3162,1 +3411,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -3204,1 +3453,1 @@\n-  return new TypeOopPtr(_base, dual_ptr(), klass(), klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeOopPtr(_base, dual_ptr(), klass(), klass_is_exact(), const_oop(), dual_offset(), Offset::bottom, dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -3210,1 +3459,1 @@\n-  if (klass->is_instance_klass()) {\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n@@ -3236,1 +3485,1 @@\n-    return TypeInstPtr::make(TypePtr::BotPTR, klass, klass_is_exact, NULL, 0);\n+    return TypeInstPtr::make(TypePtr::BotPTR, klass, klass_is_exact, NULL, Offset(0));\n@@ -3238,2 +3487,14 @@\n-    \/\/ Element is an object array. Recursively call ourself.\n-    const TypeOopPtr *etype = TypeOopPtr::make_from_klass_common(klass->as_obj_array_klass()->element_klass(), false, try_for_exact);\n+    \/\/ Element is an object or inline type array. Recursively call ourself.\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ false, try_for_exact);\n+    if (etype->is_inlinetypeptr()) {\n+      etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n+    \/\/ Determine null-free\/flattened properties\n+    const TypeOopPtr* exact_etype = etype;\n+    if (etype->can_be_inline_type()) {\n+      \/\/ Use exact type if element can be an inline type\n+      exact_etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ true, \/* try_for_exact= *\/ true);\n+    }\n+    bool not_null_free = !exact_etype->can_be_inline_type();\n+    bool not_flat = !UseFlatArray || not_null_free || (exact_etype->is_inlinetypeptr() && !exact_etype->inline_klass()->flatten_array());\n+\n@@ -3241,1 +3502,1 @@\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, false, not_flat, not_null_free);\n@@ -3245,1 +3506,1 @@\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, xk, 0);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, xk, Offset(0));\n@@ -3250,1 +3511,2 @@\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS,\n+                                        \/* stable= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3253,1 +3515,6 @@\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, 0);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, Offset(0));\n+    return arr;\n+  } else if (klass->is_flat_array_klass()) {\n+    ciInlineKlass* vk = klass->as_array_klass()->element_klass()->as_inline_klass();\n+    const TypeAry* arr0 = TypeAry::make(TypeInlineType::make(vk), TypeInt::POS);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, Offset(0));\n@@ -3269,2 +3536,2 @@\n-  if (klass->is_instance_klass()) {\n-    \/\/ Element is an instance\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n+    \/\/ Element is an instance or inline type\n@@ -3274,1 +3541,1 @@\n-      return TypeInstPtr::make(TypePtr::NotNull, klass, true, NULL, 0);\n+      return TypeInstPtr::make(TypePtr::NotNull, klass, true, NULL, Offset(0));\n@@ -3278,3 +3545,8 @@\n-    const TypeOopPtr *etype =\n-      TypeOopPtr::make_from_klass_raw(klass->as_obj_array_klass()->element_klass());\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass());\n+    bool null_free = false;\n+    if (etype->is_inlinetypeptr()) {\n+      null_free = true;\n+      etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()),\n+                                        \/* stable= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ !null_free);\n@@ -3285,1 +3557,1 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3287,1 +3559,1 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3291,3 +3563,3 @@\n-    const Type* etype =\n-      (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const Type* etype = (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()),\n+                                        \/* stable= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3297,1 +3569,12 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n+    } else {\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n+    }\n+  } else if (klass->is_flat_array_klass()) {\n+    ciInlineKlass* vk = klass->as_array_klass()->element_klass()->as_inline_klass();\n+    const TypeAry* arr0 = TypeAry::make(TypeInlineType::make(vk), TypeInt::make(o->as_array()->length()));\n+    \/\/ We used to pass NotNull in here, asserting that the sub-arrays\n+    \/\/ are all not-null.  This is not true in generally, as code can\n+    \/\/ slam NULLs down in the subarrays.\n+    if (make_constant) {\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3299,1 +3582,1 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3310,1 +3593,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -3312,1 +3595,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -3405,6 +3688,1 @@\n-  switch( _offset ) {\n-  case OffsetTop: st->print(\"+top\"); break;\n-  case OffsetBot: st->print(\"+any\"); break;\n-  case         0: break;\n-  default:        st->print(\"+%d\",_offset); break;\n-  }\n+  _offset.dump2(st);\n@@ -3427,1 +3705,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -3519,7 +3797,10 @@\n-TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, bool xk, ciObject* o, int off,\n-                         int instance_id, const TypePtr* speculative, int inline_depth)\n-  : TypeOopPtr(InstPtr, ptr, k, xk, o, off, instance_id, speculative, inline_depth),\n-    _name(k->name()) {\n-   assert(k != NULL &&\n-          (k->is_loaded() || o == NULL),\n-          \"cannot have constants with non-loaded klass\");\n+TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, bool xk, ciObject* o, Offset off,\n+                         bool flatten_array, int instance_id, const TypePtr* speculative,\n+                         int inline_depth)\n+  : TypeOopPtr(InstPtr, ptr, k, xk, o, off, Offset::bottom, instance_id, speculative, inline_depth),\n+    _name(k->name()), _flatten_array(flatten_array) {\n+  assert(k != NULL &&\n+         (k->is_loaded() || o == NULL),\n+         \"cannot have constants with non-loaded klass\");\n+  assert(!klass()->flatten_array() || flatten_array, \"Should be flat in array\");\n+  assert(!flatten_array || can_be_inline_type(), \"Only inline types can be flat in array\");\n@@ -3533,1 +3814,2 @@\n-                                     int offset,\n+                                     Offset offset,\n+                                     bool flatten_array,\n@@ -3554,0 +3836,3 @@\n+  \/\/ Check if this type is known to be flat in arrays\n+  flatten_array = flatten_array || k->flatten_array();\n+\n@@ -3556,1 +3841,1 @@\n-    (TypeInstPtr*)(new TypeInstPtr(ptr, k, xk, o ,offset, instance_id, speculative, inline_depth))->hashcons();\n+    (TypeInstPtr*)(new TypeInstPtr(ptr, k, xk, o, offset, flatten_array, instance_id, speculative, inline_depth))->hashcons();\n@@ -3589,1 +3874,1 @@\n-  return make(ptr, klass(), klass_is_exact(), const_oop(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, klass(), klass_is_exact(), const_oop(), _offset, _flatten_array, _instance_id, _speculative, _inline_depth);\n@@ -3600,1 +3885,1 @@\n-  return make(ptr(), klass(), klass_is_exact, const_oop(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), klass(), klass_is_exact, const_oop(), _offset, _flatten_array, _instance_id, _speculative, _inline_depth);\n@@ -3606,1 +3891,1 @@\n-  return make(_ptr, klass(), _klass_is_exact, const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), _klass_is_exact, const_oop(), _offset, _flatten_array, instance_id, _speculative, _inline_depth);\n@@ -3613,1 +3898,1 @@\n-    int off = meet_offset(tinst->offset());\n+    Offset off = meet_offset(tinst->offset());\n@@ -3638,1 +3923,1 @@\n-      else if (loaded->ptr() == TypePtr::AnyNull) { return TypeInstPtr::make(ptr, unloaded->klass(), false, NULL, off, instance_id, speculative, depth); }\n+      else if (loaded->ptr() == TypePtr::AnyNull) { return TypeInstPtr::make(ptr, unloaded->klass(), false, NULL, off, false, instance_id, speculative, depth); }\n@@ -3691,1 +3976,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -3702,2 +3987,2 @@\n-      if (klass()->equals(ciEnv::current()->Object_klass()) && !klass_is_exact()) {\n-        return TypeAryPtr::make(ptr, tp->ary(), tp->klass(), tp->klass_is_exact(), offset, instance_id, speculative, depth);\n+      if (klass()->equals(ciEnv::current()->Object_klass()) && !klass_is_exact() && !flatten_array()) {\n+        return TypeAryPtr::make(ptr, tp->ary(), tp->klass(), tp->klass_is_exact(), offset, tp->field_offset(), instance_id, speculative, depth);\n@@ -3708,1 +3993,1 @@\n-        return TypeInstPtr::make( ptr, ciEnv::current()->Object_klass(), false, NULL, offset, instance_id, speculative, depth);\n+        return TypeInstPtr::make( ptr, ciEnv::current()->Object_klass(), false, NULL, offset, false, instance_id, speculative, depth);\n@@ -3720,1 +4005,1 @@\n-        if (klass()->equals(ciEnv::current()->Object_klass()) && !klass_is_exact()) {\n+        if (klass()->equals(ciEnv::current()->Object_klass()) && !klass_is_exact() && !flatten_array()) {\n@@ -3723,1 +4008,1 @@\n-                                  tp->ary(), tp->klass(), tp->klass_is_exact(), offset, instance_id, speculative, depth);\n+                                  tp->ary(), tp->klass(), tp->klass_is_exact(), offset, tp->field_offset(), instance_id, speculative, depth);\n@@ -3731,1 +4016,1 @@\n-      return make(ptr, ciEnv::current()->Object_klass(), false, NULL, offset, instance_id, speculative, depth);\n+      return make(ptr, ciEnv::current()->Object_klass(), false, NULL, offset, false, instance_id, speculative, depth);\n@@ -3739,1 +4024,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -3748,1 +4033,1 @@\n-                  (ptr == Constant ? const_oop() : NULL), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : NULL), offset, flatten_array(), instance_id, speculative, depth);\n@@ -3764,1 +4049,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -3776,1 +4061,1 @@\n-                  (ptr == Constant ? const_oop() : NULL), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : NULL), offset, flatten_array(), instance_id, speculative, depth);\n@@ -3804,1 +4089,1 @@\n-    int off = meet_offset( tinst->offset() );\n+    Offset off = meet_offset( tinst->offset() );\n@@ -3814,2 +4099,3 @@\n-    if (ptr != Constant && klass()->equals(tinst->klass()) && klass_is_exact() == tinst->klass_is_exact()) {\n-      return make(ptr, klass(), klass_is_exact(), NULL, off, instance_id, speculative, depth);\n+    if (ptr != Constant && klass()->equals(tinst->klass()) && klass_is_exact() == tinst->klass_is_exact() &&\n+        flatten_array() == tinst->flatten_array()) {\n+      return make(ptr, klass(), klass_is_exact(), NULL, off, flatten_array(), instance_id, speculative, depth);\n@@ -3823,0 +4109,2 @@\n+    bool tinst_flatten_array = tinst->flatten_array();\n+    bool this_flatten_array  = this->flatten_array();\n@@ -3845,0 +4133,3 @@\n+      tmp2 = tinst_flatten_array;\n+      tinst_flatten_array = this_flatten_array;\n+      this_flatten_array = tmp2;\n@@ -3856,0 +4147,1 @@\n+      bool flat_array;\n@@ -3864,0 +4156,1 @@\n+        flat_array = below_centerline(ptr) ? tinst_flatten_array    : this_flatten_array;\n@@ -3870,0 +4163,1 @@\n+        flat_array = above_centerline(ptr) ? tinst_flatten_array : false;\n@@ -3879,1 +4173,1 @@\n-      return make(ptr, k, xk, o, off, instance_id, speculative, depth);\n+      return make(ptr, k, xk, o, off, flat_array, instance_id, speculative, depth);\n@@ -3911,1 +4205,2 @@\n-    if( tinst_klass->equals(this_klass) ) {\n+    bool flat_array = false;\n+    if (tinst_klass->equals(this_klass)) {\n@@ -3914,1 +4209,2 @@\n-    } else if( !tinst_xk && this_klass->is_subtype_of( tinst_klass ) ) {\n+      flat_array = below_centerline(ptr) ? (this_flatten_array && tinst_flatten_array) : (this_flatten_array || tinst_flatten_array);\n+    } else if(!tinst_xk && this_klass->is_subtype_of(tinst_klass) && (!tinst_flatten_array || this_flatten_array)) {\n@@ -3917,1 +4213,2 @@\n-    } else if( !this_xk && tinst_klass->is_subtype_of( this_klass ) ) {\n+      flat_array = this_flatten_array;\n+    } else if(!this_xk && tinst_klass->is_subtype_of(this_klass) && (!this_flatten_array || tinst_flatten_array)) {\n@@ -3920,0 +4217,1 @@\n+      flat_array = tinst_flatten_array;\n@@ -3922,2 +4220,2 @@\n-    if( subtype ) {\n-      if( above_centerline(ptr) ) { \/\/ both are up?\n+    if (subtype) {\n+      if (above_centerline(ptr)) { \/\/ both are up?\n@@ -3926,1 +4224,2 @@\n-      } else if( above_centerline(this ->_ptr) && !above_centerline(tinst->_ptr) ) {\n+        this_flatten_array = tinst_flatten_array = flat_array;\n+      } else if (above_centerline(this ->_ptr) && !above_centerline(tinst->_ptr)) {\n@@ -3929,1 +4228,2 @@\n-      } else if( above_centerline(tinst->_ptr) && !above_centerline(this ->_ptr) ) {\n+        this_flatten_array = tinst_flatten_array;\n+      } else if (above_centerline(tinst->_ptr) && !above_centerline(this ->_ptr)) {\n@@ -3932,0 +4232,1 @@\n+        tinst_flatten_array = this_flatten_array;\n@@ -3934,0 +4235,1 @@\n+        this_flatten_array = flat_array;\n@@ -3956,1 +4258,1 @@\n-      return make(ptr, this_klass, this_xk, o, off, instance_id, speculative, depth);\n+      return make(ptr, this_klass, this_xk, o, off, this_flatten_array, instance_id, speculative, depth);\n@@ -3968,1 +4270,1 @@\n-    return make(ptr, k, false, NULL, off, instance_id, speculative, depth);\n+    return make(ptr, k, false, NULL, off, false, instance_id, speculative, depth);\n@@ -3971,0 +4273,21 @@\n+  case InlineType: {\n+    const TypeInlineType* tv = t->is_inlinetype();\n+    if (above_centerline(ptr())) {\n+      if (tv->inline_klass()->is_subtype_of(_klass)) {\n+        return t;\n+      } else {\n+        return TypeInstPtr::NOTNULL;\n+      }\n+    } else {\n+      PTR ptr = this->_ptr;\n+      if (ptr == Constant) {\n+        ptr = NotNull;\n+      }\n+      if (tv->inline_klass()->is_subtype_of(_klass)) {\n+        return TypeInstPtr::make(ptr, _klass);\n+      } else {\n+        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass());\n+      }\n+    }\n+  }\n+\n@@ -3983,1 +4306,0 @@\n-\n@@ -3992,1 +4314,1 @@\n-  return new TypeInstPtr(dual_ptr(), klass(), klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeInstPtr(dual_ptr(), klass(), klass_is_exact(), const_oop(), dual_offset(), flatten_array(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -4001,0 +4323,1 @@\n+    flatten_array() == p->flatten_array() &&\n@@ -4007,1 +4330,1 @@\n-  int hash = java_add((jint)klass()->hash(), (jint)TypeOopPtr::hash());\n+  int hash = java_add(java_add((jint)klass()->hash(), (jint)TypeOopPtr::hash()), (jint)flatten_array());\n@@ -4039,5 +4362,1 @@\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      st->print(\"+any\");\n-    else if( _offset == OffsetTop ) st->print(\"+unknown\");\n-    else st->print(\"+%d\", _offset);\n-  }\n+  _offset.dump2(st);\n@@ -4046,0 +4365,5 @@\n+\n+  if (flatten_array() && !klass()->is_inlinetype()) {\n+    st->print(\" (flatten array)\");\n+  }\n+\n@@ -4058,1 +4382,1 @@\n-  return make(_ptr, klass(), klass_is_exact(), const_oop(), xadd_offset(offset),\n+  return make(_ptr, klass(), klass_is_exact(), const_oop(), xadd_offset(offset), flatten_array(),\n@@ -4067,1 +4391,1 @@\n-  return make(_ptr, klass(), klass_is_exact(), const_oop(), _offset,\n+  return make(_ptr, klass(), klass_is_exact(), const_oop(), _offset, flatten_array(),\n@@ -4075,1 +4399,1 @@\n-  return make(_ptr, klass(), klass_is_exact(), const_oop(), _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, klass(), klass_is_exact(), const_oop(), _offset, flatten_array(), _instance_id, _speculative, depth);\n@@ -4080,1 +4404,5 @@\n-  return make(_ptr, klass(), klass_is_exact(), const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), klass_is_exact(), const_oop(), _offset, flatten_array(), instance_id, _speculative, _inline_depth);\n+}\n+\n+const TypeInstPtr *TypeInstPtr::cast_to_flatten_array() const {\n+  return make(_ptr, klass(), klass_is_exact(), const_oop(), _offset, true, _instance_id, _speculative, _inline_depth);\n@@ -4083,0 +4411,1 @@\n+\n@@ -4095,0 +4424,1 @@\n+const TypeAryPtr *TypeAryPtr::INLINES;\n@@ -4097,1 +4427,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4103,1 +4433,1 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, NULL, ary, k, xk, offset, instance_id, false, speculative, inline_depth))->hashcons();\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, NULL, ary, k, xk, offset, field_offset, instance_id, false, speculative, inline_depth))->hashcons();\n@@ -4107,1 +4437,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4115,1 +4445,1 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, field_offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n@@ -4121,1 +4451,1 @@\n-  return make(ptr, const_oop(), _ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, const_oop(), _ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4129,1 +4459,7 @@\n-  return make(ptr(), const_oop(), _ary, klass(), klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+\n+  const TypeAry* new_ary = _ary;\n+  if (klass() != NULL && klass()->is_obj_array_klass() && klass_is_exact) {\n+    \/\/ An object array can't be flat or null-free if the klass is exact\n+    new_ary = TypeAry::make(elem(), size(), is_stable(), \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n+  }\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4135,1 +4471,1 @@\n-  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4191,2 +4527,36 @@\n-  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable(), is_not_flat(), is_not_null_free());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+\/\/-------------------------------cast_to_not_flat------------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_flat(bool not_flat) const {\n+  if (not_flat == is_not_flat()) {\n+    return this;\n+  }\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), not_flat, is_not_null_free());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+\/\/-------------------------------cast_to_not_null_free-------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_null_free(bool not_null_free) const {\n+  if (not_null_free == is_not_null_free()) {\n+    return this;\n+  }\n+  \/\/ Not null free implies not flat\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), not_null_free ? true : is_not_flat(), not_null_free);\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+\/\/---------------------------------update_properties---------------------------\n+const TypeAryPtr* TypeAryPtr::update_properties(const TypeAryPtr* from) const {\n+  if ((from->is_flat()          && is_not_flat()) ||\n+      (from->is_not_flat()      && is_flat()) ||\n+      (from->is_null_free()     && is_not_null_free()) ||\n+      (from->is_not_null_free() && is_null_free())) {\n+    return NULL; \/\/ Inconsistent properties\n+  } else if (from->is_not_null_free()) {\n+    return cast_to_not_null_free(); \/\/ Implies not flat\n+  } else if (from->is_not_flat()) {\n+    return cast_to_not_flat();\n+  }\n+  return this;\n@@ -4208,1 +4578,1 @@\n-  const TypeAry* new_ary = TypeAry::make(elem, size(), stable);\n+  const TypeAry* new_ary = TypeAry::make(elem, size(), stable, is_not_flat(), is_not_null_free());\n@@ -4210,1 +4580,1 @@\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4230,2 +4600,2 @@\n-  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n+  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable(), is_not_flat(), is_not_null_free());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n@@ -4240,1 +4610,2 @@\n-    TypeOopPtr::eq(p);  \/\/ Check sub-parts\n+    TypeOopPtr::eq(p) &&\/\/ Check sub-parts\n+    _field_offset == p->_field_offset;\n@@ -4246,1 +4617,1 @@\n-  return (intptr_t)_ary + TypeOopPtr::hash();\n+  return (intptr_t)_ary + TypeOopPtr::hash() + _field_offset.get();\n@@ -4279,1 +4650,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4288,1 +4659,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4302,1 +4673,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4318,1 +4689,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4330,1 +4701,2 @@\n-    int off = meet_offset(tap->offset());\n+    Offset off = meet_offset(tap->offset());\n+    Offset field_off = meet_field_offset(tap->field_offset());\n@@ -4348,1 +4720,1 @@\n-        tary = TypeAry::make(Type::BOTTOM, tary->_size, tary->_stable);\n+        tary = TypeAry::make(Type::BOTTOM, tary->_size, tary->_stable, tary->_not_flat, tary->_not_null_free);\n@@ -4355,1 +4727,1 @@\n-          tap->_klass != NULL  && this->_klass != NULL   &&\n+          tap->_klass != NULL && this->_klass != NULL &&\n@@ -4358,1 +4730,1 @@\n-           \/\/ 'tap'  is exact and super or unrelated:\n+           \/\/ 'tap' is exact and super or unrelated:\n@@ -4363,1 +4735,13 @@\n-        tary = TypeAry::make(Type::BOTTOM, tary->_size, tary->_stable);\n+        tary = TypeAry::make(Type::BOTTOM, tary->_size, tary->_stable, tary->_not_flat, tary->_not_null_free);\n+      }\n+      return make(NotNull, NULL, tary, lazy_klass, false, off, field_off, InstanceBot, speculative, depth);\n+    } else if (klass() != NULL && tap->klass() != NULL && klass()->is_flat_array_klass() != tap->klass()->is_flat_array_klass()) {\n+      \/\/ Meeting flattened inline type array with non-flattened array. Adjust (field) offset accordingly.\n+      if (tary->_elem->isa_inlinetype()) {\n+        \/\/ Result is flattened\n+        off = Offset(is_flat() ? offset() : tap->offset());\n+        field_off = is_flat() ? field_offset() : tap->field_offset();\n+      } else if (tary->_elem->make_oopptr() != NULL && tary->_elem->make_oopptr()->isa_instptr() && below_centerline(ptr)) {\n+        \/\/ Result is non-flattened\n+        off = Offset(flattened_offset()).meet(Offset(tap->flattened_offset()));\n+        field_off = Offset::bottom;\n@@ -4365,1 +4749,0 @@\n-      return make(NotNull, NULL, tary, lazy_klass, false, off, InstanceBot, speculative, depth);\n@@ -4378,1 +4761,1 @@\n-      return make(ptr, const_oop(), tary, lazy_klass, xk, off, instance_id, speculative, depth);\n+      return make(ptr, const_oop(), tary, lazy_klass, xk, off, field_off, instance_id, speculative, depth);\n@@ -4397,1 +4780,1 @@\n-      return TypeAryPtr::make(ptr, o, tary, lazy_klass, xk, off, instance_id, speculative, depth);\n+      return TypeAryPtr::make(ptr, o, tary, lazy_klass, xk, off, field_off, instance_id, speculative, depth);\n@@ -4406,1 +4789,1 @@\n-      return TypeAryPtr::make(ptr, NULL, tary, lazy_klass, xk, off, instance_id, speculative, depth);\n+      return TypeAryPtr::make(ptr, NULL, tary, lazy_klass, xk, off, field_off, instance_id, speculative, depth);\n@@ -4414,1 +4797,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4425,2 +4808,2 @@\n-      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && !tp->klass_is_exact()) {\n-        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && !tp->klass_is_exact() && !tp->flatten_array()) {\n+        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4431,1 +4814,1 @@\n-        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), false, NULL,offset, instance_id, speculative, depth);\n+        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), false, NULL, offset, false, instance_id, speculative, depth);\n@@ -4443,1 +4826,1 @@\n-        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && !tp->klass_is_exact()) {\n+        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && !tp->klass_is_exact() && !tp->flatten_array()) {\n@@ -4446,1 +4829,1 @@\n-                      _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                      _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4454,1 +4837,1 @@\n-      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), false, NULL,offset, instance_id, speculative, depth);\n+      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), false, NULL, offset, false, instance_id, speculative, depth);\n@@ -4458,0 +4841,13 @@\n+\n+  case InlineType: {\n+    const TypeInlineType* tv = t->is_inlinetype();\n+    if (above_centerline(ptr())) {\n+      return TypeInstPtr::NOTNULL;\n+    } else {\n+      PTR ptr = this->_ptr;\n+      if (ptr == Constant) {\n+        ptr = NotNull;\n+      }\n+      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass());\n+    }\n+  }\n@@ -4465,1 +4861,10 @@\n-  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(),_klass, _klass_is_exact, dual_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(), _klass, _klass_is_exact, dual_offset(), dual_field_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+}\n+\n+Type::Offset TypeAryPtr::meet_field_offset(const Type::Offset offset) const {\n+  return _field_offset.meet(offset);\n+}\n+\n+\/\/------------------------------dual_offset------------------------------------\n+Type::Offset TypeAryPtr::dual_field_offset() const {\n+  return _field_offset.dual();\n@@ -4502,1 +4907,6 @@\n-  if( _offset != 0 ) {\n+  if (is_flat()) {\n+    st->print(\"(\");\n+    _field_offset.dump2(st);\n+    st->print(\")\");\n+  }\n+  if (offset() != 0) {\n@@ -4504,3 +4914,3 @@\n-    if( _offset == OffsetTop )       st->print(\"+undefined\");\n-    else if( _offset == OffsetBot )  st->print(\"+any\");\n-    else if( _offset < header_size ) st->print(\"+%d\", _offset);\n+    if( _offset == Offset::top )       st->print(\"+undefined\");\n+    else if( _offset == Offset::bottom )  st->print(\"+any\");\n+    else if( offset() < header_size ) st->print(\"+%d\", offset());\n@@ -4511,1 +4921,1 @@\n-      st->print(\"[%d]\", (_offset - array_base)\/elem_size);\n+      st->print(\"[%d]\", (offset() - array_base)\/elem_size);\n@@ -4532,1 +4942,1 @@\n-  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _instance_id, add_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _field_offset, _instance_id, add_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n@@ -4540,1 +4950,13 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, NULL, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, NULL, _inline_depth, _is_autobox_cache);\n+}\n+\n+const Type* TypeAryPtr::cleanup_speculative() const {\n+  if (speculative() == NULL) {\n+    return this;\n+  }\n+  \/\/ Keep speculative part if it contains information about flat-\/nullability\n+  const TypeAryPtr* spec_aryptr = speculative()->isa_aryptr();\n+  if (spec_aryptr != NULL && (spec_aryptr->is_not_flat() || spec_aryptr->is_not_null_free())) {\n+    return this;\n+  }\n+  return TypeOopPtr::cleanup_speculative();\n@@ -4547,1 +4969,52 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, depth, _is_autobox_cache);\n+}\n+\n+const TypeAryPtr* TypeAryPtr::with_field_offset(int offset) const {\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, Offset(offset), _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+const TypePtr* TypeAryPtr::add_field_offset_and_offset(intptr_t offset) const {\n+  int adj = 0;\n+  if (offset != Type::OffsetBot && offset != Type::OffsetTop) {\n+    const Type* elemtype = elem();\n+    if (elemtype->isa_inlinetype()) {\n+      if (_offset.get() != OffsetBot && _offset.get() != OffsetTop) {\n+        adj = _offset.get();\n+        offset += _offset.get();\n+      }\n+      uint header = arrayOopDesc::base_offset_in_bytes(T_OBJECT);\n+      if (_field_offset.get() != OffsetBot && _field_offset.get() != OffsetTop) {\n+        offset += _field_offset.get();\n+        if (_offset.get() == OffsetBot || _offset.get() == OffsetTop) {\n+          offset += header;\n+        }\n+      }\n+      if (offset >= (intptr_t)header || offset < 0) {\n+        \/\/ Try to get the field of the inline type array element we are pointing to\n+        ciKlass* arytype_klass = klass();\n+        ciFlatArrayKlass* vak = arytype_klass->as_flat_array_klass();\n+        ciInlineKlass* vk = vak->element_klass()->as_inline_klass();\n+        int shift = vak->log2_element_size();\n+        int mask = (1 << shift) - 1;\n+        intptr_t field_offset = ((offset - header) & mask);\n+        ciField* field = vk->get_field_by_offset(field_offset + vk->first_field_offset(), false);\n+        if (field == NULL) {\n+          \/\/ This may happen with nested AddP(base, AddP(base, base, offset), longcon(16))\n+          return add_offset(offset);\n+        } else {\n+          return with_field_offset(field_offset)->add_offset(offset - field_offset - adj);\n+        }\n+      }\n+    }\n+  }\n+  return add_offset(offset - adj);\n+}\n+\n+\/\/ Return offset incremented by field_offset for flattened inline type arrays\n+const int TypeAryPtr::flattened_offset() const {\n+  int offset = _offset.get();\n+  if (offset != Type::OffsetBot && offset != Type::OffsetTop &&\n+      _field_offset != Offset::bottom && _field_offset != Offset::top) {\n+    offset += _field_offset.get();\n+  }\n+  return offset;\n@@ -4552,1 +5025,1 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth);\n@@ -4557,0 +5030,1 @@\n+\n@@ -4645,1 +5119,0 @@\n-\n@@ -4651,0 +5124,3 @@\n+  case InlineType:\n+    return t->xmeet(this);\n+\n@@ -4729,1 +5205,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -4749,1 +5225,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -4751,1 +5227,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -4802,1 +5278,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4828,1 +5304,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4861,1 +5337,1 @@\n-  switch( _offset ) {\n+  switch (offset()) {\n@@ -4865,1 +5341,1 @@\n-  default:        st->print(\"+%d\",_offset); break;\n+  default:        st->print(\"+%d\",offset()); break;\n@@ -4875,1 +5351,1 @@\n-TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, int offset):\n+TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, Offset offset):\n@@ -4880,1 +5356,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -4883,1 +5359,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -4888,1 +5364,1 @@\n-const TypeMetadataPtr *TypeMetadataPtr::make(PTR ptr, ciMetadata* m, int offset) {\n+const TypeMetadataPtr* TypeMetadataPtr::make(PTR ptr, ciMetadata* m, Offset offset) {\n@@ -4902,2 +5378,4 @@\n-TypeKlassPtr::TypeKlassPtr( PTR ptr, ciKlass* klass, int offset )\n-  : TypePtr(KlassPtr, ptr, offset), _klass(klass), _klass_is_exact(ptr == Constant) {\n+TypeKlassPtr::TypeKlassPtr(PTR ptr, ciKlass* klass, Offset offset, bool flatten_array)\n+  : TypePtr(KlassPtr, ptr, offset), _klass(klass), _klass_is_exact(ptr == Constant), _flatten_array(flatten_array) {\n+  assert(!klass->flatten_array() || flatten_array, \"Should be flat in array\");\n+  assert(!flatten_array || can_be_inline_type(), \"Only inline types can be flat in array\");\n@@ -4908,7 +5386,5 @@\n-const TypeKlassPtr *TypeKlassPtr::make( PTR ptr, ciKlass* k, int offset ) {\n-  assert( k != NULL, \"Expect a non-NULL klass\");\n-  assert(k->is_instance_klass() || k->is_array_klass(), \"Incorrect type of klass oop\");\n-  TypeKlassPtr *r =\n-    (TypeKlassPtr*)(new TypeKlassPtr(ptr, k, offset))->hashcons();\n-\n-  return r;\n+const TypeKlassPtr* TypeKlassPtr::make(PTR ptr, ciKlass* k, Offset offset, bool flatten_array) {\n+  assert(k == NULL || k->is_instance_klass() || k->is_array_klass(), \"Incorrect type of klass oop\");\n+  \/\/ Check if this type is known to be flat in arrays\n+  flatten_array = flatten_array || k->flatten_array();\n+  return (TypeKlassPtr*)(new TypeKlassPtr(ptr, k, offset, flatten_array))->hashcons();\n@@ -4921,3 +5397,1 @@\n-  return\n-    klass()->equals(p->klass()) &&\n-    TypePtr::eq(p);\n+  return klass() == p->klass() && TypePtr::eq(p) && flatten_array() == p->flatten_array();\n@@ -4929,1 +5403,1 @@\n-  return java_add((jint)klass()->hash(), (jint)TypePtr::hash());\n+  return java_add(java_add(klass() != NULL ? klass()->hash() : (jint)0, (jint)TypePtr::hash()), (jint)flatten_array());\n@@ -4938,1 +5412,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -4950,1 +5424,1 @@\n-    if (!empty() && ktkp != NULL && ktkp->klass()->is_loaded() && ktkp->klass()->is_interface())\n+    if (!empty() && ktkp != NULL && ktkp->is_loaded() && ktkp->klass()->is_interface())\n@@ -4973,1 +5447,0 @@\n-  const TypeInstPtr *tinst;\n@@ -4981,3 +5454,8 @@\n-  if ((tinst = el->isa_instptr()) != NULL) {\n-    \/\/ Compute array klass from element klass\n-    k_ary = ciObjArrayKlass::make(tinst->klass());\n+  if (el->isa_instptr()) {\n+    \/\/ Compute object array klass from element klass\n+    k_ary = ciArrayKlass::make(el->is_oopptr()->klass());\n+  } else if (el->isa_inlinetype()) {\n+    \/\/ If element type is TypeInlineType::BOTTOM, inline_klass() will be null.\n+    if (el->inline_klass() != NULL) {\n+      k_ary = ciArrayKlass::make(el->inline_klass());\n+    }\n@@ -5048,1 +5526,1 @@\n-        _offset != 0 && _offset != arrayOopDesc::length_offset_in_bytes()) {\n+        offset() != 0 && offset() != arrayOopDesc::length_offset_in_bytes()) {\n@@ -5059,1 +5537,1 @@\n-  return make( _ptr, klass(), xadd_offset(offset) );\n+  return make(_ptr, klass(), xadd_offset(offset), flatten_array());\n@@ -5066,1 +5544,1 @@\n-  return make(ptr, _klass, _offset);\n+  return make(ptr, _klass, _offset, _flatten_array);\n@@ -5073,1 +5551,1 @@\n-  return make(klass_is_exact ? Constant : NotNull, _klass, _offset);\n+  return make(klass_is_exact ? Constant : NotNull, _klass, _offset, _flatten_array);\n@@ -5082,0 +5560,1 @@\n+  assert(k != NULL, \"klass should not be NULL\");\n@@ -5087,0 +5566,3 @@\n+  if (flatten_array() && !klass()->is_inlinetype()) {\n+    toop = toop->is_instptr()->cast_to_flatten_array();\n+  }\n@@ -5121,1 +5603,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5129,1 +5611,1 @@\n-      return make( ptr, klass(), offset );\n+      return make(ptr, klass(), offset, flatten_array());\n@@ -5162,1 +5644,1 @@\n-    int  off     = meet_offset(tkls->offset());\n+    Offset  off  = meet_offset(tkls->offset());\n@@ -5165,0 +5647,8 @@\n+    if (klass() == NULL || tkls->klass() == NULL) {\n+      ciKlass* k = NULL;\n+      if (ptr == Constant) {\n+        k = (klass() == NULL) ? tkls->klass() : klass();\n+      }\n+      return make(ptr, k, off);\n+    }\n+\n@@ -5169,2 +5659,2 @@\n-    if( ptr != Constant && tkls->klass()->equals(klass()) ) {\n-      return make( ptr, klass(), off );\n+    if (ptr != Constant && tkls->klass()->equals(klass()) && flatten_array() == tkls->flatten_array()) {\n+      return make(ptr, klass(), off, flatten_array());\n@@ -5178,0 +5668,3 @@\n+    bool tkls_flatten_array = tkls->flatten_array();\n+    bool this_flatten_array  = this->flatten_array();\n+    bool flatten_array = below_centerline(ptr) ? (this_flatten_array && tkls_flatten_array) : (this_flatten_array || tkls_flatten_array);\n@@ -5205,1 +5698,1 @@\n-      return make( ptr, this_klass, off );\n+      return make(ptr, this_klass, off, flatten_array);\n@@ -5214,1 +5707,1 @@\n-    return   make( ptr, k, off );\n+    return   make(ptr, k, off);\n@@ -5224,1 +5717,1 @@\n-  return new TypeKlassPtr( dual_ptr(), klass(), dual_offset() );\n+  return new TypeKlassPtr(dual_ptr(), klass(), dual_offset(), flatten_array());\n@@ -5230,1 +5723,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -5232,1 +5725,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -5255,2 +5748,2 @@\n-      const char *name = klass()->name()->as_utf8();\n-      if( name ) {\n+      if (klass() != NULL) {\n+        const char* name = klass()->name()->as_utf8();\n@@ -5259,1 +5752,1 @@\n-        ShouldNotReachHere();\n+        st->print(\"klass BOTTOM\");\n@@ -5273,5 +5766,1 @@\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      { st->print(\"+any\"); }\n-    else if( _offset == OffsetTop ) { st->print(\"+unknown\"); }\n-    else                            { st->print(\"+%d\", _offset); }\n-  }\n+  _offset.dump2(st);\n@@ -5289,2 +5778,14 @@\n-const TypeFunc *TypeFunc::make( const TypeTuple *domain, const TypeTuple *range ) {\n-  return (TypeFunc*)(new TypeFunc(domain,range))->hashcons();\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain_sig, const TypeTuple* domain_cc,\n+                               const TypeTuple *range_sig, const TypeTuple *range_cc) {\n+  return (TypeFunc*)(new TypeFunc(domain_sig, domain_cc, range_sig, range_cc))->hashcons();\n+}\n+\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain, const TypeTuple *range) {\n+  return make(domain, domain, range, range);\n+}\n+\n+\/\/------------------------------osr_domain-----------------------------\n+const TypeTuple* osr_domain() {\n+  const Type **fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n+  return TypeTuple::make(TypeFunc::Parms+1, fields);\n@@ -5294,1 +5795,1 @@\n-const TypeFunc *TypeFunc::make(ciMethod* method) {\n+const TypeFunc* TypeFunc::make(ciMethod* method, bool is_osr_compilation) {\n@@ -5296,7 +5797,20 @@\n-  const TypeFunc* tf = C->last_tf(method); \/\/ check cache\n-  if (tf != NULL)  return tf;  \/\/ The hit rate here is almost 50%.\n-  const TypeTuple *domain;\n-  if (method->is_static()) {\n-    domain = TypeTuple::make_domain(NULL, method->signature());\n-  } else {\n-    domain = TypeTuple::make_domain(method->holder(), method->signature());\n+  const TypeFunc* tf = NULL;\n+  if (!is_osr_compilation) {\n+    tf = C->last_tf(method); \/\/ check cache\n+    if (tf != NULL)  return tf;  \/\/ The hit rate here is almost 50%.\n+  }\n+  \/\/ Inline types are not passed\/returned by reference, instead each field of\n+  \/\/ the inline type is passed\/returned as an argument. We maintain two views of\n+  \/\/ the argument\/return list here: one based on the signature (with an inline\n+  \/\/ type argument\/return as a single slot), one based on the actual calling\n+  \/\/ convention (with an inline type argument\/return as a list of its fields).\n+  bool has_scalar_args = method->has_scalarized_args() && !is_osr_compilation;\n+  const TypeTuple* domain_sig = is_osr_compilation ? osr_domain() : TypeTuple::make_domain(method, false);\n+  const TypeTuple* domain_cc = has_scalar_args ? TypeTuple::make_domain(method, true) : domain_sig;\n+  ciSignature* sig = method->signature();\n+  bool has_scalar_ret = sig->return_type()->is_inlinetype() && sig->return_type()->as_inline_klass()->can_be_returned_as_fields();\n+  const TypeTuple* range_sig = TypeTuple::make_range(sig, false);\n+  const TypeTuple* range_cc = has_scalar_ret ? TypeTuple::make_range(sig, true) : range_sig;\n+  tf = TypeFunc::make(domain_sig, domain_cc, range_sig, range_cc);\n+  if (!is_osr_compilation) {\n+    C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -5304,3 +5818,0 @@\n-  const TypeTuple *range  = TypeTuple::make_range(method->signature());\n-  tf = TypeFunc::make(domain, range);\n-  C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -5341,2 +5852,4 @@\n-  return _domain == a->_domain &&\n-    _range == a->_range;\n+  return _domain_sig == a->_domain_sig &&\n+    _domain_cc == a->_domain_cc &&\n+    _range_sig == a->_range_sig &&\n+    _range_cc == a->_range_cc;\n@@ -5348,1 +5861,1 @@\n-  return (intptr_t)_domain + (intptr_t)_range;\n+  return (intptr_t)_domain_sig + (intptr_t)_domain_cc + (intptr_t)_range_sig + (intptr_t)_range_cc;\n@@ -5355,1 +5868,1 @@\n-  if( _range->cnt() <= Parms )\n+  if( _range_sig->cnt() <= Parms )\n@@ -5359,2 +5872,2 @@\n-    for (i = Parms; i < _range->cnt()-1; i++) {\n-      _range->field_at(i)->dump2(d,depth,st);\n+    for (i = Parms; i < _range_sig->cnt()-1; i++) {\n+      _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -5363,1 +5876,1 @@\n-    _range->field_at(i)->dump2(d,depth,st);\n+    _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -5372,3 +5885,3 @@\n-  if (Parms < _domain->cnt())\n-    _domain->field_at(Parms)->dump2(d,depth-1,st);\n-  for (uint i = Parms+1; i < _domain->cnt(); i++) {\n+  if (Parms < _domain_sig->cnt())\n+    _domain_sig->field_at(Parms)->dump2(d,depth-1,st);\n+  for (uint i = Parms+1; i < _domain_sig->cnt(); i++) {\n@@ -5376,1 +5889,1 @@\n-    _domain->field_at(i)->dump2(d,depth-1,st);\n+    _domain_sig->field_at(i)->dump2(d,depth-1,st);\n@@ -5396,1 +5909,1 @@\n-  if (range()->cnt() == TypeFunc::Parms) {\n+  if (range_sig()->cnt() == TypeFunc::Parms) {\n@@ -5399,1 +5912,1 @@\n-  return range()->field_at(TypeFunc::Parms)->basic_type();\n+  return range_sig()->field_at(TypeFunc::Parms)->basic_type();\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":820,"deletions":307,"binary":false,"changes":1127,"status":"modified"},{"patch":"@@ -57,0 +57,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -667,1 +668,22 @@\n-  return handle == NULL ? 0 : ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)) ;\n+  if (handle == NULL) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, Universe::inline_type_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(SystemDictionary::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return ObjectSynchronizer::FastHashCode(THREAD, obj);\n+  }\n@@ -723,0 +745,1 @@\n+       klass->is_inline_klass() ||\n@@ -1268,1 +1291,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1271,1 +1295,1 @@\n-    size = 2;\n+    size = 3;\n@@ -1281,1 +1305,2 @@\n-      Klass* k = InstanceKlass::cast(klass)->local_interfaces()->at(index);\n+      InstanceKlass* ik = InstanceKlass::cast(klass);\n+      Klass* k = ik->local_interfaces()->at(index);\n@@ -1285,1 +1310,1 @@\n-    \/\/ All arrays implement java.lang.Cloneable and java.io.Serializable\n+    \/\/ All arrays implement java.lang.Cloneable, java.io.Serializable and java.lang.IdentityObject\n@@ -1288,0 +1313,1 @@\n+    result->obj_at_put(2, SystemDictionary::IdentityObject_klass()->java_mirror());\n@@ -1902,0 +1928,2 @@\n+  bool is_ctor = (method->is_object_constructor() ||\n+                  method->is_static_init_factory());\n@@ -1903,1 +1931,1 @@\n-    return (method->is_initializer() && !method->is_static());\n+    return is_ctor;\n@@ -1905,1 +1933,3 @@\n-    return  (!method->is_initializer() && !method->is_overpass());\n+    return (!is_ctor &&\n+            !method->is_class_initializer() &&\n+            !method->is_overpass());\n@@ -1968,0 +1998,2 @@\n+        assert(method->is_object_constructor() ||\n+               method->is_static_init_factory(), \"must be\");\n@@ -2227,3 +2259,1 @@\n-  if (!m->is_initializer() || m->is_static()) {\n-    method = Reflection::new_method(m, true, CHECK_NULL);\n-  } else {\n+  if (m->is_object_constructor() || m->is_static_init_factory()) {\n@@ -2231,0 +2261,2 @@\n+  } else {\n+    method = Reflection::new_method(m, true, CHECK_NULL);\n@@ -2518,0 +2550,39 @@\n+\/\/ Arrays support \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+JVM_ENTRY(jboolean, JVM_ArrayIsAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  JVMWrapper(\"JVM_ArrayIsAccessAtomic\");\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == NULL) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  return ArrayKlass::cast(k)->element_access_is_atomic();\n+JVM_END\n+\n+JVM_ENTRY(jobject, JVM_ArrayEnsureAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  JVMWrapper(\"JVM_ArrayEnsureAccessAtomic\");\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == NULL) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vk = FlatArrayKlass::cast(k);\n+    if (!vk->element_access_is_atomic()) {\n+      \/**\n+       * Need to decide how to implement:\n+       *\n+       * 1) Change to objArrayOop layout, therefore oop->klass() differs so\n+       * then \"<atomic>[Qfoo;\" klass needs to subclass \"[Qfoo;\" to pass through\n+       * \"checkcast\" & \"instanceof\"\n+       *\n+       * 2) Use extra header in the flatArrayOop to flag atomicity required and\n+       * possibly per instance lock structure. Said info, could be placed in\n+       * \"trailer\" rather than disturb the current arrayOop\n+       *\/\n+      Unimplemented();\n+    }\n+  }\n+  return array;\n+JVM_END\n+\n@@ -2697,1 +2768,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3709,1 +3780,1 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3730,0 +3801,1 @@\n+  objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3731,1 +3803,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":84,"deletions":13,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -2521,1 +2521,2 @@\n-                                            src_st.access_flags().is_static());\n+                                            src_st.access_flags().is_static(),\n+                                            src_st.field_descriptor().is_inlined());\n@@ -2558,2 +2559,3 @@\n-    Array<InstanceKlass*>* interface_list = InstanceKlass::cast(k)->local_interfaces();\n-    const int result_length = (interface_list == NULL ? 0 : interface_list->length());\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    Array<InstanceKlass*>* interface_list = ik->local_interfaces();\n+    int result_length = (interface_list == NULL ? 0 : interface_list->length());\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -503,1 +503,2 @@\n-  if (ty_sign[0] == JVM_SIGNATURE_CLASS &&\n+  if ((ty_sign[0] == JVM_SIGNATURE_CLASS ||\n+       ty_sign[0] == JVM_SIGNATURE_INLINE_TYPE) &&\n@@ -571,0 +572,1 @@\n+  case T_INLINE_TYPE:\n@@ -696,1 +698,1 @@\n-      if (_type == T_OBJECT) {\n+      if (_type == T_OBJECT || _type == T_INLINE_TYPE) {\n@@ -714,1 +716,2 @@\n-      case T_OBJECT: {\n+      case T_OBJECT:\n+      case T_INLINE_TYPE: {\n@@ -735,1 +738,2 @@\n-        case T_OBJECT: {\n+        case T_OBJECT:\n+        case T_INLINE_TYPE: {\n","filename":"src\/hotspot\/share\/prims\/jvmtiImpl.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -56,0 +57,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -61,0 +63,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -1892,0 +1895,92 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return NULL;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(aoop->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return NULL;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayOop result_array =\n+      oopFactory::new_objArray(SystemDictionary::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  instanceOop ioop = ih();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ioop->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array);\n+WB_END\n+\n+class CollectOops : public BasicOopIterateClosure {\n+ public:\n+  GrowableArray<Handle>* array;\n+\n+  objArrayOop create_results(TRAPS) {\n+    objArrayOop result_array =\n+        oopFactory::new_objArray(SystemDictionary::Object_klass(), array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < array->length(); i++) {\n+      result_array->obj_at_put(i, array->at(i)());\n+    }\n+    return result_array;\n+  }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return (jobjectArray)JNIHandles::make_local(THREAD, create_results(THREAD));\n+  }\n+\n+  void add_oop(oop o) {\n+    \/\/ Value might be oop, but JLS can't see as Object, just iterate through it...\n+    if (o != NULL && o->is_inline_type()) {\n+      o->oop_iterate(this);\n+    } else {\n+      array->append(Handle(Thread::current(), o));\n+    }\n+  }\n+\n+  void do_oop(oop* o) { add_oop(*o); }\n+  void do_oop(narrowOop* v) { add_oop(CompressedOops::decode(*v)); }\n+};\n+\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+\n+  JNIHandles::resolve(thing)->oop_iterate(&collectOops);\n+\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  StackFrameStream sfs(thread, false \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, NULL, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\n@@ -2561,0 +2656,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":101,"deletions":0,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -2164,0 +2164,10 @@\n+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypePassFieldsAsArgs)) {\n+    FLAG_SET_CMDLINE(InlineTypePassFieldsAsArgs, false);\n+    warning(\"InlineTypePassFieldsAsArgs is not supported on this platform\");\n+  }\n+\n+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypeReturnedAsFields)) {\n+    FLAG_SET_CMDLINE(InlineTypeReturnedAsFields, false);\n+    warning(\"InlineTypeReturnedAsFields is not supported on this platform\");\n+  }\n+\n@@ -3086,0 +3096,18 @@\n+  if (EnableValhalla) {\n+    \/\/ create_property(\"valhalla.enableValhalla\", \"true\", InternalProperty)\n+    const char* prop_name = \"valhalla.enableValhalla\";\n+    const char* prop_value = \"true\";\n+    const size_t prop_len = strlen(prop_name) + strlen(prop_value) + 2;\n+    char* property = AllocateHeap(prop_len, mtArguments);\n+    int ret = jio_snprintf(property, prop_len, \"%s=%s\", prop_name, prop_value);\n+    if (ret < 0 || ret >= (int)prop_len) {\n+      FreeHeap(property);\n+      return JNI_ENOMEM;\n+    }\n+    bool added = add_property(property, UnwriteableProperty, InternalProperty);\n+    FreeHeap(property);\n+    if (!added) {\n+      return JNI_ENOMEM;\n+    }\n+  }\n+\n@@ -3200,0 +3228,5 @@\n+  if (UseBiasedLocking) {\n+    jio_fprintf(defaultStream::error_stream(), \"Valhalla does not support use with UseBiasedLocking\");\n+    return JNI_ERR;\n+  }\n+\n@@ -4194,0 +4227,5 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !is_dumping_archive())) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported\n+    InlineTypePassFieldsAsArgs = false;\n+    InlineTypeReturnedAsFields = false;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -44,0 +44,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n@@ -49,0 +51,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -201,2 +204,13 @@\n-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n-  Handle return_value;\n+  ScopeDesc* scope = chunk->at(0)->scope();\n+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n+  \/\/ In case of the return of multiple values, we must take care\n+  \/\/ of all oop return values.\n+  GrowableArray<Handle> return_oops;\n+  InlineKlass* vk = NULL;\n+  if (save_oop_result && scope->return_vt()) {\n+    vk = InlineKlass::returned_inline_klass(map);\n+    if (vk != NULL) {\n+      vk->save_oop_fields(map, return_oops);\n+      save_oop_result = false;\n+    }\n+  }\n@@ -208,1 +222,1 @@\n-    return_value = Handle(thread, result);\n+    return_oops.push(Handle(thread, result));\n@@ -215,1 +229,1 @@\n-  if (objects != NULL) {\n+  if (objects != NULL || vk != NULL) {\n@@ -220,1 +234,8 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+      if (vk != NULL) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, CHECK_AND_CLEAR_(true));\n+      }\n+      if (objects != NULL) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+        bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, CHECK_AND_CLEAR_(true));\n+      }\n@@ -224,1 +245,8 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+      if (vk != NULL) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);\n+      }\n+      if (objects != NULL) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+        bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, THREAD);\n+      }\n@@ -227,2 +255,0 @@\n-    bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();\n-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal);\n@@ -237,1 +263,1 @@\n-  if (save_oop_result) {\n+  if (save_oop_result || vk != NULL) {\n@@ -239,1 +265,2 @@\n-    deoptee.set_saved_oop_result(&map, return_value());\n+    assert(return_oops.length() == 1, \"no inline type\");\n+    deoptee.set_saved_oop_result(&map, return_oops.pop()());\n@@ -570,1 +597,1 @@\n-  \/\/ If the sender is deoptimized the we must retrieve the address of the handler\n+  \/\/ If the sender is deoptimized we must retrieve the address of the handler\n@@ -1069,0 +1096,4 @@\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* ak = FlatArrayKlass::cast(k);\n+      \/\/ Inline type array must be zeroed because not all memory is reassigned\n+      obj = ak->allocate(sv->field_size(), THREAD);\n@@ -1098,0 +1129,15 @@\n+\/\/ We're deoptimizing at the return of a call, inline type fields are\n+\/\/ in registers. When we go back to the interpreter, it will expect a\n+\/\/ reference to an inline type instance. Allocate and initialize it from\n+\/\/ the register values here.\n+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {\n+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);\n+  if (new_vt == NULL) {\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);\n+  }\n+  return_oops.clear();\n+  return_oops.push(Handle(THREAD, new_vt));\n+  return false;\n+}\n+\n@@ -1270,0 +1316,1 @@\n+  InstanceKlass* _klass;\n@@ -1274,0 +1321,1 @@\n+    _klass = NULL;\n@@ -1283,1 +1331,5 @@\n-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal) {\n+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal, int base_offset, TRAPS) {\n+  if (svIndex >= sv->field_size()) {\n+    \/\/ No fields left to re-assign.\n+    return svIndex;\n+  }\n@@ -1292,0 +1344,9 @@\n+        if (field._type == T_INLINE_TYPE) {\n+          field._type = T_OBJECT;\n+        }\n+        if (fs.is_inlined()) {\n+          \/\/ Resolve klass of flattened inline type field\n+          Klass* vk = klass->get_inline_type_field_klass(fs.index());\n+          field._klass = InlineKlass::cast(vk);\n+          field._type = T_INLINE_TYPE;\n+        }\n@@ -1302,1 +1363,1 @@\n-    int offset = fields->at(i)._offset;\n+    int offset = base_offset + fields->at(i)._offset;\n@@ -1305,1 +1366,2 @@\n-      case T_OBJECT: case T_ARRAY:\n+      case T_OBJECT:\n+      case T_ARRAY:\n@@ -1310,0 +1372,9 @@\n+      case T_INLINE_TYPE: {\n+        \/\/ Recursively re-assign flattened inline type fields\n+        InstanceKlass* vk = fields->at(i)._klass;\n+        assert(vk != NULL, \"must be resolved\");\n+        offset -= InlineKlass::cast(vk)->first_field_offset(); \/\/ Adjust offset to omit oop header\n+        svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, skip_internal, offset, CHECK_0);\n+        continue; \/\/ Continue because we don't need to increment svIndex\n+      }\n+\n@@ -1385,0 +1456,14 @@\n+\/\/ restore fields of an eliminated inline type array\n+void Deoptimization::reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool skip_internal, TRAPS) {\n+  InlineKlass* vk = vak->element_klass();\n+  assert(vk->flatten_array(), \"should only be used for flattened inline type arrays\");\n+  \/\/ Adjust offset to omit oop header\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE) - InlineKlass::cast(vk)->first_field_offset();\n+  \/\/ Initialize all elements of the flattened inline type array\n+  for (int i = 0; i < sv->field_size(); i++) {\n+    ScopeValue* val = sv->field_at(i);\n+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));\n+    reassign_fields_by_klass(vk, fr, reg_map, val->as_ObjectValue(), 0, (oop)obj, skip_internal, offset, CHECK);\n+  }\n+}\n+\n@@ -1386,1 +1471,1 @@\n-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal) {\n+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal, TRAPS) {\n@@ -1411,1 +1496,4 @@\n-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal);\n+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal, 0, CHECK);\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      reassign_flat_array_elements(fr, reg_map, sv, (flatArrayOop) obj(), vak, skip_internal, CHECK);\n@@ -1477,1 +1565,0 @@\n-\n@@ -1481,1 +1568,3 @@\n-    Handle obj = sv->value();\n+    print_object(k, sv->value(), realloc_failures);\n+  }\n+}\n@@ -1483,9 +1572,10 @@\n-    tty->print(\"     object <\" INTPTR_FORMAT \"> of type \", p2i(sv->value()()));\n-    k->print_value();\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n-    if (obj.is_null()) {\n-      tty->print(\" allocation failed\");\n-    } else {\n-      tty->print(\" allocated (%d bytes)\", obj->size() * HeapWordSize);\n-    }\n-    tty->cr();\n+void Deoptimization::print_object(Klass* k, Handle obj, bool realloc_failures) {\n+  tty->print(\"     object <\" INTPTR_FORMAT \"> of type \", p2i(obj()));\n+  k->print_value();\n+  assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+  if (obj.is_null()) {\n+    tty->print(\" allocation failed\");\n+  } else {\n+    tty->print(\" allocated (%d bytes)\", obj->size() * HeapWordSize);\n+  }\n+  tty->cr();\n@@ -1493,3 +1583,2 @@\n-    if (Verbose && !obj.is_null()) {\n-      k->oop_print_on(obj(), tty);\n-    }\n+  if (Verbose && !obj.is_null()) {\n+    k->oop_print_on(obj(), tty);\n@@ -1710,1 +1799,1 @@\n-  \/\/ Deoptimize only if the frame comes from compile code.\n+  \/\/ Deoptimize only if the frame comes from compiled code.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":120,"deletions":31,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -801,0 +801,18 @@\n+  notproduct(bool, PrintInlineLayout, false,                                \\\n+          \"Print field layout for each inline type\")                        \\\n+                                                                            \\\n+  notproduct(bool, PrintFlatArrayLayout, false,                             \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxSize, -1,                                \\\n+          \"Max size for flattening inline array elements, <0 no limit\")     \\\n+                                                                            \\\n+  product(intx, InlineFieldMaxFlatSize, 128,                                \\\n+          \"Max size for flattening inline type fields, <0 no limit\")        \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  product(bool, InlineArrayAtomicAccess, false,                             \\\n+          \"Atomic inline array accesses by-default, for all inline arrays\") \\\n+                                                                            \\\n@@ -816,1 +834,1 @@\n-  product(bool, UseBiasedLocking, false,                                    \\\n+  product(bool, UseBiasedLocking, false,                                     \\\n@@ -2497,0 +2515,20 @@\n+  product(bool, EnableValhalla, true,                                       \\\n+          \"Enable experimental Valhalla features\")                          \\\n+                                                                            \\\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressInlineTypeReturnedAsFields, false,                    \\\n+          \"Stress return of fields instead of an inline type reference\")    \\\n+                                                                            \\\n+  develop(bool, ScalarizeInlineTypes, true,                                 \\\n+          \"Scalarize inline types in compiled code\")                        \\\n+                                                                            \\\n+  product(ccstrlist, ForceNonTearable, \"\", DIAGNOSTIC,                      \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n@@ -2505,0 +2543,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":40,"deletions":1,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -220,1 +220,1 @@\n-  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ObjArrayKlass*)                        \\\n+  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ArrayKlass*)                        \\\n@@ -235,1 +235,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags,                                   u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags,                                   u4)                                    \\\n@@ -1624,0 +1624,1 @@\n+  declare_c2_type(MachVEPNode, MachIdealNode)                             \\\n@@ -2307,0 +2308,2 @@\n+  declare_constant(InstanceKlass::_misc_invalid_inline_super)             \\\n+  declare_constant(InstanceKlass::_misc_invalid_identity_super)           \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -99,0 +99,1 @@\n+  DCmdFactory::register_DCmdFactory(new DCmdFactoryImpl<PrintClassLayoutDCmd>(full_export, true, false));\n@@ -132,1 +133,0 @@\n-\n@@ -1041,1 +1041,24 @@\n-#endif\n+PrintClassLayoutDCmd::PrintClassLayoutDCmd(outputStream* output, bool heap) :\n+                                       DCmdWithParser(output, heap),\n+  _classname(\"classname\", \"Name of class whose layout should be printed. \",\n+             \"STRING\", true) {\n+  _dcmdparser.add_dcmd_argument(&_classname);\n+}\n+\n+void PrintClassLayoutDCmd::execute(DCmdSource source, TRAPS) {\n+  VM_PrintClassLayout printClassLayoutOp(output(), _classname.value());\n+  VMThread::execute(&printClassLayoutOp);\n+}\n+\n+int PrintClassLayoutDCmd::num_arguments() {\n+  ResourceMark rm;\n+  PrintClassLayoutDCmd* dcmd = new PrintClassLayoutDCmd(NULL, false);\n+  if (dcmd != NULL) {\n+    DCmdMark mark(dcmd);\n+    return dcmd->_dcmdparser.num_arguments();\n+  } else {\n+    return 0;\n+  }\n+}\n+\n+#endif \/\/ INCLUDE_SERVICES\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":25,"deletions":2,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -407,0 +407,26 @@\n+class PrintClassLayoutDCmd : public DCmdWithParser {\n+protected:\n+  DCmdArgument<char*> _classname; \/\/ lass name whose layout should be printed.\n+public:\n+  PrintClassLayoutDCmd(outputStream* output, bool heap);\n+  static const char* name() {\n+    return \"VM.class_print_layout\";\n+  }\n+  static const char* description() {\n+    return \"Print the layout of an instance of a class, including inlined fields. \"\n+           \"The name of each class is followed by the ClassLoaderData* of its ClassLoader, \"\n+           \"or \\\"null\\\" if loaded by the bootstrap class loader.\";\n+  }\n+  static const char* impact() {\n+      return \"Medium: Depends on number of loaded classes.\";\n+  }\n+  static const JavaPermission permission() {\n+    JavaPermission p = {\"java.lang.management.ManagementPermission\",\n+                        \"monitor\", NULL};\n+    return p;\n+  }\n+  static int num_arguments();\n+  virtual void execute(DCmdSource source, TRAPS);\n+};\n+\n+\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.hpp","additions":26,"deletions":0,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -573,0 +573,9 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Prototyping\n+\/\/ \"Code Missing Here\" macro, un-define when integrating back from prototyping stage and break\n+\/\/ compilation on purpose (i.e. \"forget me not\")\n+#define PROTOTYPE\n+#ifdef PROTOTYPE\n+#define CMH(m)\n+#endif\n+\n@@ -658,6 +667,7 @@\n-  T_VOID        = 14,\n-  T_ADDRESS     = 15,\n-  T_NARROWOOP   = 16,\n-  T_METADATA    = 17,\n-  T_NARROWKLASS = 18,\n-  T_CONFLICT    = 19, \/\/ for stack value type with conflicting contents\n+  T_INLINE_TYPE = 14,\n+  T_VOID        = 15,\n+  T_ADDRESS     = 16,\n+  T_NARROWOOP   = 17,\n+  T_METADATA    = 18,\n+  T_NARROWKLASS = 19,\n+  T_CONFLICT    = 20, \/\/ for stack value type with conflicting contents\n@@ -678,0 +688,1 @@\n+    F(JVM_SIGNATURE_INLINE_TYPE, T_INLINE_TYPE, N) \\\n@@ -703,1 +714,1 @@\n-  return (t == T_OBJECT || t == T_ARRAY);\n+  return (t == T_OBJECT || t == T_ARRAY || t == T_INLINE_TYPE);\n@@ -740,1 +751,2 @@\n-  T_VOID_size        = 0\n+  T_VOID_size        = 0,\n+  T_INLINE_TYPE_size = 1\n@@ -770,0 +782,1 @@\n+  T_INLINE_TYPE_aelem_bytes = 8,\n@@ -773,0 +786,1 @@\n+  T_INLINE_TYPE_aelem_bytes = 4,\n@@ -859,1 +873,1 @@\n-  vtos = 9,             \/\/ tos not cached\n+  vtos = 9,             \/\/ tos not cached,\n@@ -876,1 +890,2 @@\n-    case T_ARRAY  : \/\/ fall through\n+    case T_INLINE_TYPE: \/\/ fall through\n+    case T_ARRAY  :   \/\/ fall through\n@@ -1248,0 +1263,6 @@\n+\/\/ TEMP!!!!\n+\/\/ This should be removed after LW2 arrays are implemented (JDK-8220790).\n+\/\/ It's an alias to (EnableValhalla && (FlatArrayElementMaxSize != 0)),\n+\/\/ which is actually not 100% correct, but works for the current set of C1\/C2\n+\/\/ implementation and test cases.\n+#define UseFlatArray (EnableValhalla && (FlatArrayElementMaxSize != 0))\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":31,"deletions":10,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-    private static final int CLASSFILE_VERSION = 52;\n+    private static final int CLASSFILE_VERSION = V16;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/InnerClassLambdaMetafactory.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2385,0 +2385,6 @@\n+            \/\/ resolveOrFail could return a non-static <init> method if present\n+            \/\/ detect and throw NSME before producing a MethodHandle\n+            if (!method.isStatic() && name.equals(\"<init>\")) {\n+                throw new NoSuchMethodException(\"illegal method name: \" + name);\n+            }\n+\n@@ -2530,0 +2536,7 @@\n+         *\n+         * @apiNote\n+         * This method does not find a static {@code <init>} factory method as it is invoked\n+         * via {@code invokestatic} bytecode as opposed to {@code invokespecial} for an\n+         * object constructor.  To look up static {@code <init>} factory method, use\n+         * the {@link #findStatic(Class, String, MethodType) findStatic} method.\n+         *\n@@ -2545,0 +2558,3 @@\n+            if (type.returnType() != void.class) {\n+                throw new NoSuchMethodException(\"Constructors must have void return type: \" + refc.getName());\n+            }\n@@ -3220,1 +3236,1 @@\n-            assert(ctor.isConstructor());\n+            assert(ctor.isObjectConstructorOrStaticInitMethod());\n@@ -3223,1 +3239,9 @@\n-            return lookup.getDirectConstructorNoSecurityManager(ctor.getDeclaringClass(), ctor);\n+            if (ctor.isObjectConstructor()) {\n+                assert(ctor.getReturnType() == void.class);\n+                return lookup.getDirectConstructorNoSecurityManager(ctor.getDeclaringClass(), ctor);\n+            } else {\n+                \/\/ static init factory is a static method\n+                assert(ctor.isMethod() && ctor.getReturnType() == ctor.getDeclaringClass() && ctor.getReferenceKind() == REF_invokeStatic);\n+                assert(!MethodHandleNatives.isCallerSensitive(ctor));  \/\/ must not be caller-sensitive\n+                return lookup.getDirectMethodNoSecurityManager(ctor.getReferenceKind(), ctor.getDeclaringClass(), ctor, lookup);\n+            }\n@@ -3476,2 +3500,5 @@\n-            if (name.startsWith(\"<\") && refKind != REF_newInvokeSpecial)\n-                throw new NoSuchMethodException(\"illegal method name: \"+name);\n+            \/\/ \"<init>\" can only be invoked via invokespecial or it's a static init factory\n+            if (name.startsWith(\"<\") && refKind != REF_newInvokeSpecial &&\n+                    !(refKind == REF_invokeStatic && name.equals(\"<init>\"))) {\n+                    throw new NoSuchMethodException(\"illegal method name: \" + name);\n+            }\n@@ -3480,1 +3507,0 @@\n-\n@@ -3583,1 +3609,1 @@\n-            if (m.isConstructor())\n+            if (m.isObjectConstructor())\n@@ -3882,1 +3908,1 @@\n-            assert(ctor.isConstructor());\n+            assert(ctor.isObjectConstructor());\n@@ -4072,0 +4098,3 @@\n+        if (arrayClass.isInlineClass()) {\n+            throw new UnsupportedOperationException();\n+        }\n@@ -4833,1 +4862,7 @@\n-        return type.isPrimitive() ?  zero(Wrapper.forPrimitiveType(type), type) : zero(Wrapper.OBJECT, type);\n+        if (type.isPrimitive()) {\n+            return zero(Wrapper.forPrimitiveType(type), type);\n+        } else if (type.isInlineClass()) {\n+            throw new UnsupportedOperationException();\n+        } else {\n+            return zero(Wrapper.OBJECT, type);\n+        }\n@@ -4863,1 +4898,1 @@\n-        MethodType mtype = methodType(ptype, ptype);\n+        MethodType mtype = MethodType.methodType(ptype, ptype);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandles.java","additions":44,"deletions":9,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -33,0 +33,3 @@\n+ * <p>\n+ * The referent must not be an instance of an inline class; such a value\n+ * can never have another reference to it and cannot be held in a reference type.\n@@ -83,0 +86,2 @@\n+     * @throws IllegalArgumentException if the referent is an instance of an\n+     *         {@link Class#isInlineClass() inlineClass}\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/PhantomReference.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -425,0 +425,4 @@\n+        if (referent != null && referent.getClass().isInlineClass()) {\n+            throw new IllegalArgumentException(\"cannot reference an inline value of type: \" +\n+                    referent.getClass().getName());\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/Reference.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -132,1 +132,0 @@\n-\n@@ -353,1 +352,0 @@\n-\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2570,1 +2570,1 @@\n-        return isKind(doctree, VALUE);\n+        return isKind(doctree, Kind.VALUE);\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/toolkit\/util\/Utils.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -300,0 +300,1 @@\n+           (tag == JDWP_TAG(INLINE_OBJECT)) ||\n@@ -366,1 +367,0 @@\n-\n","filename":"src\/jdk.jdwp.agent\/share\/native\/libjdwp\/util.c","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -647,0 +647,3 @@\n+        if (f.getDeclaringClass().isInlineClass()) {\n+            throw new UnsupportedOperationException(\"can't get field offset on an inline class: \" + f);\n+        }\n@@ -679,0 +682,3 @@\n+        if (f.getDeclaringClass().isInlineClass()) {\n+            throw new UnsupportedOperationException(\"can't get static field offset on an inline class: \" + f);\n+        }\n@@ -704,0 +710,3 @@\n+        if (f.getDeclaringClass().isInlineClass()) {\n+            throw new UnsupportedOperationException(\"can't get base address on an inline class: \" + f);\n+        }\n","filename":"src\/jdk.unsupported\/share\/classes\/sun\/misc\/Unsafe.java","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-  runtime\n+  runtime \\\n@@ -58,0 +58,7 @@\n+hotspot_valhalla = \\\n+  runtime\/valhalla \\\n+  compiler\/valhalla\n+\n+hotspot_valhalla_runtime = \\\n+  runtime\/valhalla\n+\n@@ -96,1 +103,1 @@\n-  compiler\/codegen\/aes \\\n+  compiler\/codegen\/aes \\\n@@ -145,0 +152,1 @@\n+  compiler\/valhalla\/ \\\n@@ -162,0 +170,7 @@\n+tier1_compiler_no_valhalla = \\\n+  :tier1_compiler_1 \\\n+  :tier1_compiler_2 \\\n+  :tier1_compiler_3 \\\n+  :tier1_compiler_not_xcomp \\\n+  -compiler\/valhalla\n+\n@@ -316,0 +331,4 @@\n+tier1_runtime_no_valhalla = \\\n+  :tier1_runtime \\\n+  -runtime\/valhalla\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":21,"deletions":2,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -44,32 +44,0 @@\n- *                 -XX:+DoEscapeAnalysis -XX:+EliminateAllocations -XX:+EliminateLocks -XX:+EliminateNestedLocks -XX:+UseBiasedLocking\n- * @run driver EATests\n- *                 -XX:+UnlockDiagnosticVMOptions\n- *                 -Xms256m -Xmx256m\n- *                 -Xbootclasspath\/a:.\n- *                 -XX:CompileCommand=dontinline,*::dontinline_*\n- *                 -XX:+WhiteBoxAPI\n- *                 -Xbatch\n- *                 -XX:+DoEscapeAnalysis -XX:+EliminateAllocations -XX:-EliminateLocks -XX:+EliminateNestedLocks -XX:+UseBiasedLocking -XX:-UseOptoBiasInlining\n- * @run driver EATests\n- *                 -XX:+UnlockDiagnosticVMOptions\n- *                 -Xms256m -Xmx256m\n- *                 -Xbootclasspath\/a:.\n- *                 -XX:CompileCommand=dontinline,*::dontinline_*\n- *                 -XX:+WhiteBoxAPI\n- *                 -Xbatch\n- *                 -XX:+DoEscapeAnalysis -XX:-EliminateAllocations -XX:+EliminateLocks -XX:+EliminateNestedLocks -XX:+UseBiasedLocking\n- * @run driver EATests\n- *                 -XX:+UnlockDiagnosticVMOptions\n- *                 -Xms256m -Xmx256m\n- *                 -Xbootclasspath\/a:.\n- *                 -XX:CompileCommand=dontinline,*::dontinline_*\n- *                 -XX:+WhiteBoxAPI\n- *                 -Xbatch\n- *                 -XX:-DoEscapeAnalysis -XX:-EliminateAllocations -XX:+EliminateLocks -XX:+EliminateNestedLocks -XX:+UseBiasedLocking\n- * @run driver EATests\n- *                 -XX:+UnlockDiagnosticVMOptions\n- *                 -Xms256m -Xmx256m\n- *                 -Xbootclasspath\/a:.\n- *                 -XX:CompileCommand=dontinline,*::dontinline_*\n- *                 -XX:+WhiteBoxAPI\n- *                 -Xbatch\n","filename":"test\/jdk\/com\/sun\/jdi\/EATests.java","additions":0,"deletions":32,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -117,2 +117,2 @@\n-        assertTrue(intfs.length == 1);\n-        assertTrue(intfs[0] == HiddenTest.class);\n+        assertTrue(intfs.length == 1 || intfs.length == 2);\n+        assertTrue(intfs[0] == HiddenTest.class || (intfs.length == 2 && intfs[1] == HiddenTest.class));\n@@ -350,2 +350,2 @@\n-        assertTrue(intfs.length == 1);\n-        assertTrue(intfs[0] == HiddenTest.class);\n+        assertTrue(intfs.length == 1 || intfs.length == 2);\n+        assertTrue(intfs[0] == HiddenTest.class || (intfs.length == 2 && intfs[1] == HiddenTest.class));\n","filename":"test\/jdk\/java\/lang\/invoke\/defineHiddenClass\/BasicTest.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"}]}