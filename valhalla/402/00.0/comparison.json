{"files":[{"patch":"@@ -1368,2 +1368,2 @@\n-            jdk\/build\/macos-x64\/bundles\/jdk-${{ env.JDK_VERSION }}-internal+0_osx-x64_bin${{ matrix.artifact }}.tar.gz\n-            jdk\/build\/macos-x64\/bundles\/jdk-${{ env.JDK_VERSION }}-internal+0_osx-x64_bin-tests${{ matrix.artifact }}.tar.gz\n+            jdk\/build\/macos-x64\/bundles\/jdk-${{ env.JDK_VERSION }}-internal+0_macos-x64_bin${{ matrix.artifact }}.tar.gz\n+            jdk\/build\/macos-x64\/bundles\/jdk-${{ env.JDK_VERSION }}-internal+0_macos-x64_bin-tests${{ matrix.artifact }}.tar.gz\n@@ -1471,2 +1471,2 @@\n-            jdk\/build\/macos-aarch64\/bundles\/jdk-${{ env.JDK_VERSION }}-internal+0_osx-aarch64_bin${{ matrix.artifact }}.tar.gz\n-            jdk\/build\/macos-aarch64\/bundles\/jdk-${{ env.JDK_VERSION }}-internal+0_osx-aarch64_bin-tests${{ matrix.artifact }}.tar.gz\n+            jdk\/build\/macos-aarch64\/bundles\/jdk-${{ env.JDK_VERSION }}-internal+0_macos-aarch64_bin${{ matrix.artifact }}.tar.gz\n+            jdk\/build\/macos-aarch64\/bundles\/jdk-${{ env.JDK_VERSION }}-internal+0_macos-aarch64_bin-tests${{ matrix.artifact }}.tar.gz\n@@ -1579,2 +1579,2 @@\n-          mkdir -p \"${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_osx-x64_bin${{ matrix.artifact }}\"\n-          tar -xf \"${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_osx-x64_bin${{ matrix.artifact }}.tar.gz\" -C \"${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_osx-x64_bin${{ matrix.artifact }}\"\n+          mkdir -p \"${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_macos-x64_bin${{ matrix.artifact }}\"\n+          tar -xf \"${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_macos-x64_bin${{ matrix.artifact }}.tar.gz\" -C \"${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_macos-x64_bin${{ matrix.artifact }}\"\n@@ -1584,2 +1584,2 @@\n-          mkdir -p \"${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_osx-x64_bin-tests${{ matrix.artifact }}\"\n-          tar -xf \"${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_osx-x64_bin-tests${{ matrix.artifact }}.tar.gz\" -C \"${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_osx-x64_bin-tests${{ matrix.artifact }}\"\n+          mkdir -p \"${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_macos-x64_bin-tests${{ matrix.artifact }}\"\n+          tar -xf \"${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_macos-x64_bin-tests${{ matrix.artifact }}.tar.gz\" -C \"${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_macos-x64_bin-tests${{ matrix.artifact }}\"\n@@ -1595,1 +1595,1 @@\n-          imageroot=`find ${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_osx-x64_bin${{ matrix.artifact }} -name release -type f`\n+          imageroot=`find ${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_macos-x64_bin${{ matrix.artifact }} -name release -type f`\n@@ -1601,1 +1601,1 @@\n-          TEST_IMAGE_DIR=${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_osx-x64_bin-tests${{ matrix.artifact }}\n+          TEST_IMAGE_DIR=${HOME}\/jdk-macos-x64${{ matrix.artifact }}\/jdk-${{ env.JDK_VERSION }}-internal+0_macos-x64_bin-tests${{ matrix.artifact }}\n","filename":".github\/workflows\/submit.yml","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -208,1 +208,1 @@\n-        \"build\", \"{,**\/}webrev*\", \"{,**\/}.hg\", \"{,**\/}JTwork\", \"{,**\/}JTreport\",\n+        \"build\", \"{,**\/}webrev*\", \"{,**\/}.hg\", \"{,**\/}JTwork*\", \"{,**\/}JTreport*\",\n@@ -254,2 +254,0 @@\n-            \"--disable-jvm-feature-aot\",\n-            \"--disable-jvm-feature-graal\",\n@@ -399,2 +397,7 @@\n-    common.boot_jdk_version = \"16\";\n-    common.boot_jdk_build_number = \"36\";\n+    if (input.build_os == 'macosx' && input.build_cpu == 'aarch64') {\n+        common.boot_jdk_version = \"17\";\n+        common.boot_jdk_build_number = \"19\";\n+    } else {\n+        common.boot_jdk_version = \"16\";\n+        common.boot_jdk_build_number = \"36\";\n+    }\n@@ -672,4 +675,0 @@\n-    \/\/\n-    \/\/\n-    \/\/ Macosx bundles are named osx\n-    \/\/ tar.gz.\n@@ -685,1 +684,1 @@\n-            platform: \"osx-x64\",\n+            platform: \"macos-x64\",\n@@ -689,1 +688,1 @@\n-            platform: \"osx-aarch64\",\n+            platform: \"macos-aarch64\",\n@@ -1083,21 +1082,9 @@\n-    var boot_jdk;\n-    if (boot_jdk_platform == 'osx-aarch64') {\n-        boot_jdk = {\n-            organization: common.organization,\n-            ext: \"tar.gz\",\n-            module: \"jdk-macosx_aarch64\",\n-            revision: \"16+1.0-beta1\",\n-            configure_args: \"--with-boot-jdk=\" + common.boot_jdk_home,\n-            environment_path: common.boot_jdk_home + \"\/bin\"\n-        }\n-    } else {\n-        boot_jdk = {\n-            server: \"jpg\",\n-            product: \"jdk\",\n-            version: common.boot_jdk_version,\n-            build_number: common.boot_jdk_build_number,\n-            file: \"bundles\/\" + boot_jdk_platform + \"\/jdk-\" + common.boot_jdk_version + \"_\"\n-                + boot_jdk_platform + \"_bin\" + boot_jdk_ext,\n-            configure_args: \"--with-boot-jdk=\" + common.boot_jdk_home,\n-            environment_path: common.boot_jdk_home + \"\/bin\"\n-        }\n+    var boot_jdk = {\n+        server: \"jpg\",\n+        product: \"jdk\",\n+        version: common.boot_jdk_version,\n+        build_number: common.boot_jdk_build_number,\n+        file: \"bundles\/\" + boot_jdk_platform + \"\/jdk-\" + common.boot_jdk_version + \"_\"\n+            + boot_jdk_platform + \"_bin\" + boot_jdk_ext,\n+        configure_args: \"--with-boot-jdk=\" + common.boot_jdk_home,\n+        environment_path: common.boot_jdk_home + \"\/bin\"\n","filename":"make\/conf\/jib-profiles.js","additions":19,"deletions":32,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -1363,1 +1363,1 @@\n-    if (UseCompressedOops && (CompressedOops::ptrs_base() != NULL || UseAOT)) {\n+    if (UseCompressedOops && (CompressedOops::ptrs_base() != NULL)) {\n@@ -3127,26 +3127,0 @@\n-  \/\/ This encoding class is generated automatically from ad_encode.m4.\n-  \/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n-  enc_class aarch64_enc_strw_immn(immN src, memory1 mem) %{\n-    C2_MacroAssembler _masm(&cbuf);\n-    address con = (address)$src$$constant;\n-    \/\/ need to do this the hard way until we can manage relocs\n-    \/\/ for 32 bit constants\n-    __ movoop(rscratch2, (jobject)con);\n-    if (con) __ encode_heap_oop_not_null(rscratch2);\n-    loadStore(_masm, &MacroAssembler::strw, rscratch2, $mem->opcode(),\n-               as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp, 4);\n-  %}\n-\n-  \/\/ This encoding class is generated automatically from ad_encode.m4.\n-  \/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n-  enc_class aarch64_enc_strw_immnk(immN src, memory4 mem) %{\n-    C2_MacroAssembler _masm(&cbuf);\n-    address con = (address)$src$$constant;\n-    \/\/ need to do this the hard way until we can manage relocs\n-    \/\/ for 32 bit constants\n-    __ movoop(rscratch2, (jobject)con);\n-    __ encode_klass_not_null(rscratch2);\n-    loadStore(_masm, &MacroAssembler::strw, rscratch2, $mem->opcode(),\n-               as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp, 4);\n-  %}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":1,"deletions":27,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -2447,3 +2447,0 @@\n-  __ resolve(ACCESS_READ, src);\n-  __ resolve(ACCESS_WRITE, dst);\n-\n@@ -2807,1 +2804,0 @@\n-    __ resolve(ACCESS_READ | ACCESS_WRITE, obj);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -77,1 +77,0 @@\n-    _call_aot_stub_size = 0,\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1461,1 +1461,1 @@\n-  if (!CompilerConfig::is_c1_only_no_aot_or_jvmci()) {\n+  if (!CompilerConfig::is_c1_only_no_jvmci()) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -152,5 +152,0 @@\n-void BarrierSetAssembler::obj_equals(MacroAssembler* masm,\n-                                     Register obj1, Register obj2) {\n-  __ cmp(obj1, obj2);\n-}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -53,7 +53,0 @@\n-  virtual void obj_equals(MacroAssembler* masm,\n-                          Register obj1, Register obj2);\n-\n-  virtual void resolve(MacroAssembler* masm, DecoratorSet decorators, Register obj) {\n-    \/\/ Default implementation does not need to do anything.\n-  }\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -546,1 +546,1 @@\n-    ldr(rscratch2, Address(rthread, Thread::polling_word_offset()));\n+    ldr(rscratch2, Address(rthread, JavaThread::polling_word_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -300,1 +300,1 @@\n-    lea(rscratch1, Address(rthread, Thread::polling_word_offset()));\n+    lea(rscratch1, Address(rthread, JavaThread::polling_word_offset()));\n@@ -303,1 +303,1 @@\n-    ldr(rscratch1, Address(rthread, Thread::polling_word_offset()));\n+    ldr(rscratch1, Address(rthread, JavaThread::polling_word_offset()));\n@@ -3922,2 +3922,1 @@\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->obj_equals(this, obj1, obj2);\n+  cmp(obj1, obj2);\n@@ -4374,9 +4373,0 @@\n-void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {\n-  \/\/ Use stronger ACCESS_WRITE|ACCESS_READ by default.\n-  if ((decorators & (ACCESS_READ | ACCESS_WRITE)) == 0) {\n-    decorators |= ACCESS_READ | ACCESS_WRITE;\n-  }\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  return bs->resolve(this, decorators, obj);\n-}\n-\n@@ -4739,1 +4729,1 @@\n-  ldr(dest, Address(rthread, Thread::polling_page_offset()));\n+  ldr(dest, Address(rthread, JavaThread::polling_page_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":4,"deletions":14,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -884,4 +884,0 @@\n-  \/\/ Resolves obj for access. Result is placed in the same register.\n-  \/\/ All other registers are preserved.\n-  void resolve(DecoratorSet decorators, Register obj);\n-\n@@ -1146,1 +1142,1 @@\n-    return ReservedCodeCacheSize > branch_range || UseAOT;\n+    return ReservedCodeCacheSize > branch_range;\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -814,1 +814,1 @@\n-  if (EnableJVMCI || UseAOT) {\n+  if (EnableJVMCI) {\n@@ -2032,2 +2032,0 @@\n-    __ resolve(IS_NOT_NULL, obj_reg);\n-\n@@ -2190,2 +2188,0 @@\n-    __ resolve(IS_NOT_NULL, obj_reg);\n-\n@@ -2449,1 +2445,1 @@\n-  if (EnableJVMCI || UseAOT) {\n+  if (EnableJVMCI) {\n@@ -2524,1 +2520,1 @@\n-  if (EnableJVMCI || UseAOT) {\n+  if (EnableJVMCI) {\n@@ -2650,1 +2646,1 @@\n-  if (EnableJVMCI || UseAOT) {\n+  if (EnableJVMCI) {\n@@ -2813,1 +2809,1 @@\n-  if (EnableJVMCI || UseAOT) {\n+  if (EnableJVMCI) {\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":5,"deletions":9,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -511,1 +511,1 @@\n-  if ((EnableJVMCI || UseAOT) && state == vtos && step == 0) {\n+  if (EnableJVMCI && state == vtos && step == 0) {\n@@ -785,1 +785,0 @@\n-    __ resolve(IS_NOT_NULL, r0);\n@@ -1011,1 +1010,0 @@\n-      __ resolve(IS_NOT_NULL | ACCESS_READ, buf);\n@@ -1056,3 +1054,0 @@\n-    if (kind == Interpreter::java_util_zip_CRC32C_updateBytes) {\n-      __ resolve(IS_NOT_NULL | ACCESS_READ, buf);\n-    }\n@@ -1547,6 +1542,1 @@\n-  \/\/ Make room for locals\n-  __ sub(rscratch1, esp, r3, ext::uxtx, 3);\n-\n-  \/\/ Padding between locals and fixed part of activation frame to ensure\n-  \/\/ SP is always 16-byte aligned.\n-  __ andr(sp, rscratch1, -16);\n+  __ mov(rscratch1, esp);\n@@ -1557,0 +1547,4 @@\n+  \/\/ Initializing memory allocated for locals in the same direction as\n+  \/\/ the stack grows to ensure page initialization order according\n+  \/\/ to windows-aarch64 stack page growth requirement (see\n+  \/\/ https:\/\/docs.microsoft.com\/en-us\/cpp\/build\/arm64-windows-abi-conventions?view=msvc-160#stack)\n@@ -1562,1 +1556,1 @@\n-    __ str(zr, Address(__ post(rscratch1, wordSize)));\n+    __ str(zr, Address(__ pre(rscratch1, -wordSize)));\n@@ -1568,0 +1562,4 @@\n+  \/\/ Padding between locals and fixed part of activation frame to ensure\n+  \/\/ SP is always 16-byte aligned.\n+  __ andr(sp, rscratch1, -16);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":11,"deletions":13,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2557,1 +2557,1 @@\n-  if (!CompilerConfig::is_c1_or_interpreter_only_no_aot_or_jvmci()){\n+  if (!CompilerConfig::is_c1_or_interpreter_only_no_jvmci()){\n@@ -3298,1 +3298,1 @@\n-  if (!CompilerConfig::is_c1_or_interpreter_only_no_aot_or_jvmci()) {\n+  if (!CompilerConfig::is_c1_or_interpreter_only_no_jvmci()) {\n@@ -3386,1 +3386,1 @@\n-  if (!CompilerConfig::is_c1_or_interpreter_only_no_aot_or_jvmci()) {\n+  if (!CompilerConfig::is_c1_or_interpreter_only_no_jvmci()) {\n@@ -4049,2 +4049,0 @@\n-  __ resolve(IS_NOT_NULL, r0);\n-\n@@ -4159,2 +4157,0 @@\n-  __ resolve(IS_NOT_NULL, r0);\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2104,3 +2104,0 @@\n-  __ resolve(ACCESS_READ, src);\n-  __ resolve(ACCESS_WRITE, dst);\n-\n@@ -2446,1 +2443,0 @@\n-    __ resolve(ACCESS_READ | ACCESS_WRITE, obj);\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1358,1 +1358,1 @@\n-  __ ld(poll_addr, in_bytes(Thread::polling_page_offset()), R16_thread);\n+  __ ld(poll_addr, in_bytes(JavaThread::polling_page_offset()), R16_thread);\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -228,1 +228,1 @@\n-      ld(R0, in_bytes(Thread::polling_word_offset()), R16_thread);\n+      ld(R0, in_bytes(JavaThread::polling_word_offset()), R16_thread);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1212,1 +1212,1 @@\n-  __ z_lg(Z_R1_scratch, Address(Z_thread, Thread::polling_page_offset()));\n+  __ z_lg(Z_R1_scratch, Address(Z_thread, JavaThread::polling_page_offset()));\n@@ -1231,1 +1231,1 @@\n-  __ z_lg(poll_addr, Address(Z_thread, Thread::polling_page_offset()));\n+  __ z_lg(poll_addr, Address(Z_thread, JavaThread::polling_page_offset()));\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -124,1 +124,1 @@\n-      const Address poll_byte_addr(Z_thread, in_bytes(Thread::polling_word_offset()) + 7 \/* Big Endian *\/);\n+      const Address poll_byte_addr(Z_thread, in_bytes(JavaThread::polling_word_offset()) + 7 \/* Big Endian *\/);\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -583,1 +583,1 @@\n-  __ movptr(poll_addr, Address(r15_thread, Thread::polling_page_offset()));\n+  __ movptr(poll_addr, Address(r15_thread, JavaThread::polling_page_offset()));\n@@ -588,1 +588,1 @@\n-  __ movptr(poll_addr, Address(poll_addr, in_bytes(Thread::polling_page_offset())));\n+  __ movptr(poll_addr, Address(poll_addr, in_bytes(JavaThread::polling_page_offset())));\n@@ -3093,1 +3093,1 @@\n-  __ relocate(static_stub_Relocation::spec(call_pc, false \/* is_aot *\/));\n+  __ relocate(static_stub_Relocation::spec(call_pc));\n@@ -3100,10 +3100,0 @@\n-  if (UseAOT) {\n-    \/\/ Trampoline to aot code\n-    __ relocate(static_stub_Relocation::spec(call_pc, true \/* is_aot *\/));\n-#ifdef _LP64\n-    __ mov64(rax, CONST64(0));  \/\/ address is zapped till fixup time.\n-#else\n-    __ movl(rax, 0xdeadffff);  \/\/ address is zapped till fixup time.\n-#endif\n-    __ jmp(rax);\n-  }\n@@ -3304,3 +3294,0 @@\n-  __ resolve(ACCESS_READ, src);\n-  __ resolve(ACCESS_WRITE, dst);\n-\n@@ -3758,1 +3745,0 @@\n-    __ resolve(ACCESS_READ | ACCESS_WRITE, obj);\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":3,"deletions":17,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -51,1 +51,0 @@\n-    _call_aot_stub_size = NOT_LP64(7) LP64_ONLY(12),\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -337,1 +337,1 @@\n-  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_aot_or_jvmci()) {\n+  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_jvmci()) {\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -726,1 +726,1 @@\n-  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_aot_or_jvmci()) {\n+  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_jvmci()) {\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1053,0 +1053,29 @@\n+\/\/ Float\/Double signum\n+void C2_MacroAssembler::signum_fp(int opcode, XMMRegister dst,\n+                                  XMMRegister zero, XMMRegister one,\n+                                  Register scratch) {\n+  assert(opcode == Op_SignumF || opcode == Op_SignumD, \"sanity\");\n+\n+  Label DONE_LABEL;\n+\n+  if (opcode == Op_SignumF) {\n+    assert(UseSSE > 0, \"required\");\n+    ucomiss(dst, zero);\n+    jcc(Assembler::equal, DONE_LABEL);    \/\/ handle special case +0.0\/-0.0, if argument is +0.0\/-0.0, return argument\n+    jcc(Assembler::parity, DONE_LABEL);   \/\/ handle special case NaN, if argument NaN, return NaN\n+    movflt(dst, one);\n+    jcc(Assembler::above, DONE_LABEL);\n+    xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), scratch);\n+  } else if (opcode == Op_SignumD) {\n+    assert(UseSSE > 1, \"required\");\n+    ucomisd(dst, zero);\n+    jcc(Assembler::equal, DONE_LABEL);    \/\/ handle special case +0.0\/-0.0, if argument is +0.0\/-0.0, return argument\n+    jcc(Assembler::parity, DONE_LABEL);   \/\/ handle special case NaN, if argument NaN, return NaN\n+    movdbl(dst, one);\n+    jcc(Assembler::above, DONE_LABEL);\n+    xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), scratch);\n+  }\n+\n+  bind(DONE_LABEL);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -38,1 +38,0 @@\n-#include \"runtime\/os.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -215,22 +215,0 @@\n-\n-#ifndef _LP64\n-void BarrierSetAssembler::obj_equals(MacroAssembler* masm,\n-                                     Address obj1, jobject obj2) {\n-  __ cmpoop_raw(obj1, obj2);\n-}\n-\n-void BarrierSetAssembler::obj_equals(MacroAssembler* masm,\n-                                     Register obj1, jobject obj2) {\n-  __ cmpoop_raw(obj1, obj2);\n-}\n-#endif\n-void BarrierSetAssembler::obj_equals(MacroAssembler* masm,\n-                                     Register obj1, Address obj2) {\n-  __ cmpptr(obj1, obj2);\n-}\n-\n-void BarrierSetAssembler::obj_equals(MacroAssembler* masm,\n-                                     Register obj1, Register obj2) {\n-  __ cmpptr(obj1, obj2);\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.cpp","additions":0,"deletions":22,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -55,16 +55,0 @@\n-#ifndef _LP64\n-  virtual void obj_equals(MacroAssembler* masm,\n-                          Address obj1, jobject obj2);\n-  virtual void obj_equals(MacroAssembler* masm,\n-                          Register obj1, jobject obj2);\n-#endif\n-\n-  virtual void obj_equals(MacroAssembler* masm,\n-                          Register obj1, Register obj2);\n-  virtual void obj_equals(MacroAssembler* masm,\n-                          Register obj1, Address obj2);\n-\n-  virtual void resolve(MacroAssembler* masm, DecoratorSet decorators, Register obj) {\n-    \/\/ Default implementation does not need to do anything.\n-  }\n-\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.hpp","additions":0,"deletions":16,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -867,1 +867,1 @@\n-    testb(Address(r15_thread, Thread::polling_word_offset()), SafepointMechanism::poll_bit());\n+    testb(Address(r15_thread, JavaThread::polling_word_offset()), SafepointMechanism::poll_bit());\n@@ -886,1 +886,1 @@\n-    testb(Address(thread, Thread::polling_word_offset()), SafepointMechanism::poll_bit());\n+    testb(Address(thread, JavaThread::polling_word_offset()), SafepointMechanism::poll_bit());\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -130,10 +130,1 @@\n-void MacroAssembler::cmpoop_raw(Address src1, jobject obj) {\n-  cmp_literal32(src1, (int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::cmpoop_raw(Register src1, jobject obj) {\n-  cmp_literal32(src1, (int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->obj_equals(this, src1, obj);\n+  cmp_literal32(src1, (int32_t)obj, oop_Relocation::spec_for_immediate());\n@@ -144,2 +135,1 @@\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->obj_equals(this, src1, obj);\n+  cmp_literal32(src1, (int32_t)obj, oop_Relocation::spec_for_immediate());\n@@ -1807,2 +1797,1 @@\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->obj_equals(this, src1, src2);\n+  cmpptr(src1, src2);\n@@ -1812,2 +1801,1 @@\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->obj_equals(this, src1, src2);\n+  cmpptr(src1, src2);\n@@ -1819,2 +1807,1 @@\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->obj_equals(this, src1, rscratch1);\n+  cmpptr(src1, rscratch1);\n@@ -2961,1 +2948,1 @@\n-    cmpptr(in_nmethod ? rsp : rbp, Address(thread_reg, Thread::polling_word_offset()));\n+    cmpptr(in_nmethod ? rsp : rbp, Address(thread_reg, JavaThread::polling_word_offset()));\n@@ -2965,1 +2952,1 @@\n-  testb(Address(thread_reg, Thread::polling_word_offset()), SafepointMechanism::poll_bit());\n+  testb(Address(thread_reg, JavaThread::polling_word_offset()), SafepointMechanism::poll_bit());\n@@ -4949,9 +4936,0 @@\n-void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {\n-  \/\/ Use stronger ACCESS_WRITE|ACCESS_READ by default.\n-  if ((decorators & (ACCESS_READ | ACCESS_WRITE)) == 0) {\n-    decorators |= ACCESS_READ | ACCESS_WRITE;\n-  }\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  return bs->resolve(this, decorators, obj);\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":7,"deletions":29,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -375,4 +375,0 @@\n-  \/\/ Resolves obj access. Result is placed in the same register.\n-  \/\/ All other registers are preserved.\n-  void resolve(DecoratorSet decorators, Register obj);\n-\n@@ -774,1 +770,0 @@\n-  void cmpoop_raw(Address dst, jobject obj);\n@@ -780,1 +775,0 @@\n-  void cmpoop_raw(Register dst, jobject obj);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -562,1 +562,0 @@\n-      \/\/ Current C frame\n@@ -569,0 +568,6 @@\n+        \/\/\n+        \/\/ We have to start the search from cur_frame, because trace_calling_frame may be it.\n+        \/\/ It is guaranteed that trace_calling_frame is different from the top frame.\n+        \/\/ But os::current_frame() does NOT return the top frame: it returns the next frame under it (caller's frame).\n+        \/\/ (Due to inlining and tail call optimizations, caller's frame doesn't necessarily correspond to the immediate\n+        \/\/ caller in the source code.)\n@@ -570,1 +575,1 @@\n-        frame trace_calling_frame = os::get_sender_for_C_frame(&cur_frame);\n+        frame trace_calling_frame = cur_frame;\n@@ -572,0 +577,1 @@\n+          assert(trace_calling_frame.cb() == NULL, \"not a C frame\");\n@@ -574,0 +580,1 @@\n+        assert(trace_calling_frame.sp() < saved_regs, \"wrong frame\");\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1074,1 +1074,1 @@\n-  if (EnableJVMCI || UseAOT) {\n+  if (EnableJVMCI) {\n@@ -2520,1 +2520,0 @@\n-    __ resolve(IS_NOT_NULL, obj_reg);\n@@ -2680,1 +2679,0 @@\n-    __ resolve(IS_NOT_NULL, obj_reg);\n@@ -2914,1 +2912,1 @@\n-  if (EnableJVMCI || UseAOT) {\n+  if (EnableJVMCI) {\n@@ -2988,1 +2986,1 @@\n-  if (EnableJVMCI || UseAOT) {\n+  if (EnableJVMCI) {\n@@ -3103,1 +3101,1 @@\n-  if (EnableJVMCI || UseAOT) {\n+  if (EnableJVMCI) {\n@@ -3266,1 +3264,1 @@\n-  if (EnableJVMCI || UseAOT) {\n+  if (EnableJVMCI) {\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":5,"deletions":7,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -269,1 +269,1 @@\n-  if ((EnableJVMCI || UseAOT) && state == vtos && step == 0) {\n+  if (EnableJVMCI && state == vtos && step == 0) {\n@@ -600,1 +600,0 @@\n-    __ resolve(IS_NOT_NULL, rax);\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2730,1 +2730,1 @@\n-    __ testb(Address(r15_thread, Thread::polling_word_offset()), SafepointMechanism::poll_bit());\n+    __ testb(Address(r15_thread, JavaThread::polling_word_offset()), SafepointMechanism::poll_bit());\n@@ -2734,1 +2734,1 @@\n-    __ testb(Address(thread, Thread::polling_word_offset()), SafepointMechanism::poll_bit());\n+    __ testb(Address(thread, JavaThread::polling_word_offset()), SafepointMechanism::poll_bit());\n@@ -4614,2 +4614,0 @@\n-  __ resolve(IS_NOT_NULL, rax);\n-\n@@ -4722,2 +4720,0 @@\n-  __ resolve(IS_NOT_NULL, rax);\n-\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -48,1 +48,4 @@\n-const char* VM_Version::_features_names[] = { FEATURES_NAMES };\n+\n+#define DECLARE_CPU_FEATURE_NAME(id, name, bit) name,\n+const char* VM_Version::_features_names[] = { CPU_FEATURE_FLAGS(DECLARE_CPU_FEATURE_NAME)};\n+#undef DECLARE_CPU_FEATURE_FLAG\n@@ -785,1 +788,0 @@\n-  assert(log2i_exact((uint64_t)CPU_MAX_FEATURE) + 1 == sizeof(_features_names) \/ sizeof(char*), \"wrong size features_names\");\n@@ -1703,0 +1705,3 @@\n+  if (FLAG_IS_DEFAULT(UseSignumIntrinsic)) {\n+      FLAG_SET_DEFAULT(UseSignumIntrinsic, true);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1602,0 +1602,10 @@\n+    case Op_SignumF:\n+      if (UseSSE < 1) {\n+        return false;\n+      }\n+      break;\n+    case Op_SignumD:\n+      if (UseSSE < 2) {\n+        return false;\n+      }\n+      break;\n@@ -2122,0 +2132,4 @@\n+  if (is_vshift_con_pattern(n, m)) { \/\/ ShiftV src (ShiftCntV con)\n+    mstack.push(m, Visit);           \/\/ m = ShiftCntV\n+    return true;\n+  }\n@@ -5790,0 +5804,24 @@\n+\/\/ --------------------------------- Signum ---------------------------\n+\n+instruct signumF_reg(regF dst, regF zero, regF one, rRegP scratch, rFlagsReg cr) %{\n+  match(Set dst (SignumF dst (Binary zero one)));\n+  effect(TEMP scratch, KILL cr);\n+  format %{ \"signumF $dst, $dst\\t! using $scratch as TEMP\" %}\n+  ins_encode %{\n+    int opcode = this->ideal_Opcode();\n+    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister, $scratch$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct signumD_reg(regD dst, regD zero, regD one, rRegP scratch, rFlagsReg cr) %{\n+  match(Set dst (SignumD dst (Binary zero one)));\n+  effect(TEMP scratch, KILL cr);\n+  format %{ \"signumD $dst, $dst\\t! using $scratch as TEMP\" %}\n+  ins_encode %{\n+    int opcode = this->ideal_Opcode();\n+    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister, $scratch$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -7531,0 +7569,12 @@\n+instruct vmaskcast(vec dst) %{\n+  predicate((vector_length(n) == vector_length(n->in(1))) &&\n+            (vector_length_in_bytes(n) == vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorMaskCast dst));\n+  ins_cost(0);\n+  format %{ \"vector_mask_cast $dst\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(empty);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":50,"deletions":0,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -2231,3 +2231,0 @@\n-#if INCLUDE_AOT\n-      CompiledStaticCall::emit_to_aot_stub(cbuf, mark);\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -416,4 +416,0 @@\n-#if INCLUDE_AOT\n-  bool         _immutable_PIC;\n-#endif\n-\n@@ -436,3 +432,0 @@\n-#if INCLUDE_AOT\n-    _immutable_PIC   = false;\n-#endif\n@@ -685,7 +678,0 @@\n-#if INCLUDE_AOT\n-  \/\/ True if this is a code buffer used for immutable PIC, i.e. AOT\n-  \/\/ compilation.\n-  bool immutable_PIC() { return _immutable_PIC; }\n-  void set_immutable_PIC(bool pic) { _immutable_PIC = pic; }\n-#endif\n-\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.hpp","additions":1,"deletions":15,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -275,1 +275,1 @@\n-    return CompilerConfig::is_c1_only_no_aot_or_jvmci() && !is_profiling() &&\n+    return CompilerConfig::is_c1_only_no_jvmci() && !is_profiling() &&\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -494,1 +494,1 @@\n-  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_aot_or_jvmci()) {\n+  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_jvmci()) {\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -277,5 +277,1 @@\n-    if (UseAOT) {\n-      return _call_stub_size + _call_aot_stub_size;\n-    } else {\n-      return _call_stub_size;\n-    }\n+    return _call_stub_size;\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.hpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -468,1 +468,1 @@\n-  if ((!CompilerConfig::is_c1_only_no_aot_or_jvmci() && need_resolve) || !obj->is_loaded() || PatchALot) {\n+  if ((!CompilerConfig::is_c1_only_no_jvmci() && need_resolve) || !obj->is_loaded() || PatchALot) {\n@@ -669,1 +669,1 @@\n-  } else if (PrintNotLoaded && (!CompilerConfig::is_c1_only_no_aot_or_jvmci() && new_instance->is_unresolved())) {\n+  } else if (PrintNotLoaded && (!CompilerConfig::is_c1_only_no_jvmci() && new_instance->is_unresolved())) {\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,1194 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"cds\/archiveBuilder.hpp\"\n+#include \"cds\/archiveUtils.hpp\"\n+#include \"cds\/cppVtables.hpp\"\n+#include \"cds\/dumpAllocStats.hpp\"\n+#include \"cds\/metaspaceShared.hpp\"\n+#include \"classfile\/classLoaderDataShared.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionaryShared.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"interpreter\/abstractInterpreter.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/allStatic.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/objArrayKlass.hpp\"\n+#include \"oops\/oopHandle.inline.hpp\"\n+#include \"runtime\/arguments.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/hashtable.inline.hpp\"\n+\n+ArchiveBuilder* ArchiveBuilder::_current = NULL;\n+\n+ArchiveBuilder::OtherROAllocMark::~OtherROAllocMark() {\n+  char* newtop = ArchiveBuilder::current()->_ro_region.top();\n+  ArchiveBuilder::alloc_stats()->record_other_type(int(newtop - _oldtop), true);\n+}\n+\n+ArchiveBuilder::SourceObjList::SourceObjList() : _ptrmap(16 * K) {\n+  _total_bytes = 0;\n+  _objs = new (ResourceObj::C_HEAP, mtClassShared) GrowableArray<SourceObjInfo*>(128 * K, mtClassShared);\n+}\n+\n+ArchiveBuilder::SourceObjList::~SourceObjList() {\n+  delete _objs;\n+}\n+\n+void ArchiveBuilder::SourceObjList::append(MetaspaceClosure::Ref* enclosing_ref, SourceObjInfo* src_info) {\n+  \/\/ Save this source object for copying\n+  _objs->append(src_info);\n+\n+  \/\/ Prepare for marking the pointers in this source object\n+  assert(is_aligned(_total_bytes, sizeof(address)), \"must be\");\n+  src_info->set_ptrmap_start(_total_bytes \/ sizeof(address));\n+  _total_bytes = align_up(_total_bytes + (uintx)src_info->size_in_bytes(), sizeof(address));\n+  src_info->set_ptrmap_end(_total_bytes \/ sizeof(address));\n+\n+  BitMap::idx_t bitmap_size_needed = BitMap::idx_t(src_info->ptrmap_end());\n+  if (_ptrmap.size() <= bitmap_size_needed) {\n+    _ptrmap.resize((bitmap_size_needed + 1) * 2);\n+  }\n+}\n+\n+void ArchiveBuilder::SourceObjList::remember_embedded_pointer(SourceObjInfo* src_info, MetaspaceClosure::Ref* ref) {\n+  \/\/ src_obj contains a pointer. Remember the location of this pointer in _ptrmap,\n+  \/\/ so that we can copy\/relocate it later. E.g., if we have\n+  \/\/    class Foo { intx scala; Bar* ptr; }\n+  \/\/    Foo *f = 0x100;\n+  \/\/ To mark the f->ptr pointer on 64-bit platform, this function is called with\n+  \/\/    src_info()->obj() == 0x100\n+  \/\/    ref->addr() == 0x108\n+  address src_obj = src_info->obj();\n+  address* field_addr = ref->addr();\n+  assert(src_info->ptrmap_start() < _total_bytes, \"sanity\");\n+  assert(src_info->ptrmap_end() <= _total_bytes, \"sanity\");\n+  assert(*field_addr != NULL, \"should have checked\");\n+\n+  intx field_offset_in_bytes = ((address)field_addr) - src_obj;\n+  DEBUG_ONLY(int src_obj_size = src_info->size_in_bytes();)\n+  assert(field_offset_in_bytes >= 0, \"must be\");\n+  assert(field_offset_in_bytes + intx(sizeof(intptr_t)) <= intx(src_obj_size), \"must be\");\n+  assert(is_aligned(field_offset_in_bytes, sizeof(address)), \"must be\");\n+\n+  BitMap::idx_t idx = BitMap::idx_t(src_info->ptrmap_start() + (uintx)(field_offset_in_bytes \/ sizeof(address)));\n+  _ptrmap.set_bit(BitMap::idx_t(idx));\n+}\n+\n+class RelocateEmbeddedPointers : public BitMapClosure {\n+  ArchiveBuilder* _builder;\n+  address _dumped_obj;\n+  BitMap::idx_t _start_idx;\n+public:\n+  RelocateEmbeddedPointers(ArchiveBuilder* builder, address dumped_obj, BitMap::idx_t start_idx) :\n+    _builder(builder), _dumped_obj(dumped_obj), _start_idx(start_idx) {}\n+\n+  bool do_bit(BitMap::idx_t bit_offset) {\n+    uintx FLAG_MASK = 0x03; \/\/ See comments around MetaspaceClosure::FLAG_MASK\n+    size_t field_offset = size_t(bit_offset - _start_idx) * sizeof(address);\n+    address* ptr_loc = (address*)(_dumped_obj + field_offset);\n+\n+    uintx old_p_and_bits = (uintx)(*ptr_loc);\n+    uintx flag_bits = (old_p_and_bits & FLAG_MASK);\n+    address old_p = (address)(old_p_and_bits & (~FLAG_MASK));\n+    address new_p = _builder->get_dumped_addr(old_p);\n+    uintx new_p_and_bits = ((uintx)new_p) | flag_bits;\n+\n+    log_trace(cds)(\"Ref: [\" PTR_FORMAT \"] -> \" PTR_FORMAT \" => \" PTR_FORMAT,\n+                   p2i(ptr_loc), p2i(old_p), p2i(new_p));\n+\n+    ArchivePtrMarker::set_and_mark_pointer(ptr_loc, (address)(new_p_and_bits));\n+    return true; \/\/ keep iterating the bitmap\n+  }\n+};\n+\n+void ArchiveBuilder::SourceObjList::relocate(int i, ArchiveBuilder* builder) {\n+  SourceObjInfo* src_info = objs()->at(i);\n+  assert(src_info->should_copy(), \"must be\");\n+  BitMap::idx_t start = BitMap::idx_t(src_info->ptrmap_start()); \/\/ inclusive\n+  BitMap::idx_t end = BitMap::idx_t(src_info->ptrmap_end());     \/\/ exclusive\n+\n+  RelocateEmbeddedPointers relocator(builder, src_info->dumped_addr(), start);\n+  _ptrmap.iterate(&relocator, start, end);\n+}\n+\n+ArchiveBuilder::ArchiveBuilder() :\n+  _current_dump_space(NULL),\n+  _buffer_bottom(NULL),\n+  _last_verified_top(NULL),\n+  _num_dump_regions_used(0),\n+  _other_region_used_bytes(0),\n+  _requested_static_archive_bottom(NULL),\n+  _requested_static_archive_top(NULL),\n+  _requested_dynamic_archive_bottom(NULL),\n+  _requested_dynamic_archive_top(NULL),\n+  _mapped_static_archive_bottom(NULL),\n+  _mapped_static_archive_top(NULL),\n+  _buffer_to_requested_delta(0),\n+  _rw_region(\"rw\", MAX_SHARED_DELTA),\n+  _ro_region(\"ro\", MAX_SHARED_DELTA),\n+  _rw_src_objs(),\n+  _ro_src_objs(),\n+  _src_obj_table(INITIAL_TABLE_SIZE),\n+  _num_instance_klasses(0),\n+  _num_obj_array_klasses(0),\n+  _num_type_array_klasses(0),\n+  _total_closed_heap_region_size(0),\n+  _total_open_heap_region_size(0),\n+  _estimated_metaspaceobj_bytes(0),\n+  _estimated_hashtable_bytes(0)\n+{\n+  _klasses = new (ResourceObj::C_HEAP, mtClassShared) GrowableArray<Klass*>(4 * K, mtClassShared);\n+  _symbols = new (ResourceObj::C_HEAP, mtClassShared) GrowableArray<Symbol*>(256 * K, mtClassShared);\n+  _special_refs = new (ResourceObj::C_HEAP, mtClassShared) GrowableArray<SpecialRefInfo>(24 * K, mtClassShared);\n+\n+  assert(_current == NULL, \"must be\");\n+  _current = this;\n+}\n+\n+ArchiveBuilder::~ArchiveBuilder() {\n+  assert(_current == this, \"must be\");\n+  _current = NULL;\n+\n+  clean_up_src_obj_table();\n+\n+  for (int i = 0; i < _symbols->length(); i++) {\n+    _symbols->at(i)->decrement_refcount();\n+  }\n+\n+  delete _klasses;\n+  delete _symbols;\n+  delete _special_refs;\n+}\n+\n+bool ArchiveBuilder::is_dumping_full_module_graph() {\n+  return DumpSharedSpaces && MetaspaceShared::use_full_module_graph();\n+}\n+\n+class GatherKlassesAndSymbols : public UniqueMetaspaceClosure {\n+  ArchiveBuilder* _builder;\n+\n+public:\n+  GatherKlassesAndSymbols(ArchiveBuilder* builder) : _builder(builder) {}\n+\n+  virtual bool do_unique_ref(Ref* ref, bool read_only) {\n+    return _builder->gather_klass_and_symbol(ref, read_only);\n+  }\n+};\n+\n+bool ArchiveBuilder::gather_klass_and_symbol(MetaspaceClosure::Ref* ref, bool read_only) {\n+  if (ref->obj() == NULL) {\n+    return false;\n+  }\n+  if (get_follow_mode(ref) != make_a_copy) {\n+    return false;\n+  }\n+  if (ref->msotype() == MetaspaceObj::ClassType) {\n+    Klass* klass = (Klass*)ref->obj();\n+    assert(klass->is_klass(), \"must be\");\n+    if (!is_excluded(klass)) {\n+      _klasses->append(klass);\n+      if (klass->is_instance_klass()) {\n+        _num_instance_klasses ++;\n+      } else if (klass->is_objArray_klass()) {\n+        _num_obj_array_klasses ++;\n+      } else {\n+        assert(klass->is_typeArray_klass(), \"sanity\");\n+        _num_type_array_klasses ++;\n+      }\n+    }\n+    \/\/ See RunTimeSharedClassInfo::get_for()\n+    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, SharedSpaceObjectAlignment);\n+  } else if (ref->msotype() == MetaspaceObj::SymbolType) {\n+    \/\/ Make sure the symbol won't be GC'ed while we are dumping the archive.\n+    Symbol* sym = (Symbol*)ref->obj();\n+    sym->increment_refcount();\n+    _symbols->append(sym);\n+  }\n+\n+  int bytes = ref->size() * BytesPerWord;\n+  _estimated_metaspaceobj_bytes += align_up(bytes, SharedSpaceObjectAlignment);\n+\n+  return true; \/\/ recurse\n+}\n+\n+void ArchiveBuilder::gather_klasses_and_symbols() {\n+  ResourceMark rm;\n+  log_info(cds)(\"Gathering classes and symbols ... \");\n+  GatherKlassesAndSymbols doit(this);\n+  iterate_roots(&doit, \/*is_relocating_pointers=*\/false);\n+#if INCLUDE_CDS_JAVA_HEAP\n+  if (is_dumping_full_module_graph()) {\n+    ClassLoaderDataShared::iterate_symbols(&doit);\n+  }\n+#endif\n+  doit.finish();\n+\n+  log_info(cds)(\"Number of classes %d\", _num_instance_klasses + _num_obj_array_klasses + _num_type_array_klasses);\n+  log_info(cds)(\"    instance classes   = %5d\", _num_instance_klasses);\n+  log_info(cds)(\"    obj array classes  = %5d\", _num_obj_array_klasses);\n+  log_info(cds)(\"    type array classes = %5d\", _num_type_array_klasses);\n+  log_info(cds)(\"               symbols = %5d\", _symbols->length());\n+\n+  if (DumpSharedSpaces) {\n+    \/\/ To ensure deterministic contents in the static archive, we need to ensure that\n+    \/\/ we iterate the MetaspaceObjs in a deterministic order. It doesn't matter where\n+    \/\/ the MetaspaceObjs are located originally, as they are copied sequentially into\n+    \/\/ the archive during the iteration.\n+    \/\/\n+    \/\/ The only issue here is that the symbol table and the system directories may be\n+    \/\/ randomly ordered, so we copy the symbols and klasses into two arrays and sort\n+    \/\/ them deterministically.\n+    \/\/\n+    \/\/ During -Xshare:dump, the order of Symbol creation is strictly determined by\n+    \/\/ the SharedClassListFile (class loading is done in a single thread and the JIT\n+    \/\/ is disabled). Also, Symbols are allocated in monotonically increasing addresses\n+    \/\/ (see Symbol::operator new(size_t, int)). So if we iterate the Symbols by\n+    \/\/ ascending address order, we ensure that all Symbols are copied into deterministic\n+    \/\/ locations in the archive.\n+    \/\/\n+    \/\/ TODO: in the future, if we want to produce deterministic contents in the\n+    \/\/ dynamic archive, we might need to sort the symbols alphabetically (also see\n+    \/\/ DynamicArchiveBuilder::sort_methods()).\n+    sort_symbols_and_fix_hash();\n+    sort_klasses();\n+\n+    \/\/ TODO -- we need a proper estimate for the archived modules, etc,\n+    \/\/ but this should be enough for now\n+    _estimated_metaspaceobj_bytes += 200 * 1024 * 1024;\n+  }\n+}\n+\n+int ArchiveBuilder::compare_symbols_by_address(Symbol** a, Symbol** b) {\n+  if (a[0] < b[0]) {\n+    return -1;\n+  } else {\n+    assert(a[0] > b[0], \"Duplicated symbol %s unexpected\", (*a)->as_C_string());\n+    return 1;\n+  }\n+}\n+\n+void ArchiveBuilder::sort_symbols_and_fix_hash() {\n+  log_info(cds)(\"Sorting symbols and fixing identity hash ... \");\n+  os::init_random(0x12345678);\n+  _symbols->sort(compare_symbols_by_address);\n+  for (int i = 0; i < _symbols->length(); i++) {\n+    assert(_symbols->at(i)->is_permanent(), \"archived symbols must be permanent\");\n+    _symbols->at(i)->update_identity_hash();\n+  }\n+}\n+\n+int ArchiveBuilder::compare_klass_by_name(Klass** a, Klass** b) {\n+  return a[0]->name()->fast_compare(b[0]->name());\n+}\n+\n+void ArchiveBuilder::sort_klasses() {\n+  log_info(cds)(\"Sorting classes ... \");\n+  _klasses->sort(compare_klass_by_name);\n+}\n+\n+size_t ArchiveBuilder::estimate_archive_size() {\n+  \/\/ size of the symbol table and two dictionaries, plus the RunTimeSharedClassInfo's\n+  size_t symbol_table_est = SymbolTable::estimate_size_for_archive();\n+  size_t dictionary_est = SystemDictionaryShared::estimate_size_for_archive();\n+  _estimated_hashtable_bytes = symbol_table_est + dictionary_est;\n+\n+  size_t total = 0;\n+\n+  total += _estimated_metaspaceobj_bytes;\n+  total += _estimated_hashtable_bytes;\n+\n+  \/\/ allow fragmentation at the end of each dump region\n+  total += _total_dump_regions * MetaspaceShared::core_region_alignment();\n+\n+  log_info(cds)(\"_estimated_hashtable_bytes = \" SIZE_FORMAT \" + \" SIZE_FORMAT \" = \" SIZE_FORMAT,\n+                symbol_table_est, dictionary_est, _estimated_hashtable_bytes);\n+  log_info(cds)(\"_estimated_metaspaceobj_bytes = \" SIZE_FORMAT, _estimated_metaspaceobj_bytes);\n+  log_info(cds)(\"total estimate bytes = \" SIZE_FORMAT, total);\n+\n+  return align_up(total, MetaspaceShared::core_region_alignment());\n+}\n+\n+address ArchiveBuilder::reserve_buffer() {\n+  size_t buffer_size = estimate_archive_size();\n+  ReservedSpace rs(buffer_size, MetaspaceShared::core_region_alignment(), os::vm_page_size());\n+  if (!rs.is_reserved()) {\n+    log_error(cds)(\"Failed to reserve \" SIZE_FORMAT \" bytes of output buffer.\", buffer_size);\n+    vm_direct_exit(0);\n+  }\n+\n+  \/\/ buffer_bottom is the lowest address of the 2 core regions (rw, ro) when\n+  \/\/ we are copying the class metadata into the buffer.\n+  address buffer_bottom = (address)rs.base();\n+  log_info(cds)(\"Reserved output buffer space at \" PTR_FORMAT \" [\" SIZE_FORMAT \" bytes]\",\n+                p2i(buffer_bottom), buffer_size);\n+  _shared_rs = rs;\n+\n+  _buffer_bottom = buffer_bottom;\n+  _last_verified_top = buffer_bottom;\n+  _current_dump_space = &_rw_region;\n+  _num_dump_regions_used = 1;\n+  _other_region_used_bytes = 0;\n+  _current_dump_space->init(&_shared_rs, &_shared_vs);\n+\n+  ArchivePtrMarker::initialize(&_ptrmap, &_shared_vs);\n+\n+  \/\/ The bottom of the static archive should be mapped at this address by default.\n+  _requested_static_archive_bottom = (address)MetaspaceShared::requested_base_address();\n+\n+  \/\/ The bottom of the archive (that I am writing now) should be mapped at this address by default.\n+  address my_archive_requested_bottom;\n+\n+  if (DumpSharedSpaces) {\n+    my_archive_requested_bottom = _requested_static_archive_bottom;\n+  } else {\n+    _mapped_static_archive_bottom = (address)MetaspaceObj::shared_metaspace_base();\n+    _mapped_static_archive_top  = (address)MetaspaceObj::shared_metaspace_top();\n+    assert(_mapped_static_archive_top >= _mapped_static_archive_bottom, \"must be\");\n+    size_t static_archive_size = _mapped_static_archive_top - _mapped_static_archive_bottom;\n+\n+    \/\/ At run time, we will mmap the dynamic archive at my_archive_requested_bottom\n+    _requested_static_archive_top = _requested_static_archive_bottom + static_archive_size;\n+    my_archive_requested_bottom = align_up(_requested_static_archive_top, MetaspaceShared::core_region_alignment());\n+\n+    _requested_dynamic_archive_bottom = my_archive_requested_bottom;\n+  }\n+\n+  _buffer_to_requested_delta = my_archive_requested_bottom - _buffer_bottom;\n+\n+  address my_archive_requested_top = my_archive_requested_bottom + buffer_size;\n+  if (my_archive_requested_bottom <  _requested_static_archive_bottom ||\n+      my_archive_requested_top    <= _requested_static_archive_bottom) {\n+    \/\/ Size overflow.\n+    log_error(cds)(\"my_archive_requested_bottom = \" INTPTR_FORMAT, p2i(my_archive_requested_bottom));\n+    log_error(cds)(\"my_archive_requested_top    = \" INTPTR_FORMAT, p2i(my_archive_requested_top));\n+    log_error(cds)(\"SharedBaseAddress (\" INTPTR_FORMAT \") is too high. \"\n+                   \"Please rerun java -Xshare:dump with a lower value\", p2i(_requested_static_archive_bottom));\n+    vm_direct_exit(0);\n+  }\n+\n+  if (DumpSharedSpaces) {\n+    \/\/ We don't want any valid object to be at the very bottom of the archive.\n+    \/\/ See ArchivePtrMarker::mark_pointer().\n+    rw_region()->allocate(16);\n+  }\n+\n+  return buffer_bottom;\n+}\n+\n+void ArchiveBuilder::iterate_sorted_roots(MetaspaceClosure* it, bool is_relocating_pointers) {\n+  int i;\n+\n+  if (!is_relocating_pointers) {\n+    \/\/ Don't relocate _symbol, so we can safely call decrement_refcount on the\n+    \/\/ original symbols.\n+    int num_symbols = _symbols->length();\n+    for (i = 0; i < num_symbols; i++) {\n+      it->push(_symbols->adr_at(i));\n+    }\n+  }\n+\n+  int num_klasses = _klasses->length();\n+  for (i = 0; i < num_klasses; i++) {\n+    it->push(_klasses->adr_at(i));\n+  }\n+\n+  iterate_roots(it, is_relocating_pointers);\n+}\n+\n+class GatherSortedSourceObjs : public MetaspaceClosure {\n+  ArchiveBuilder* _builder;\n+\n+public:\n+  GatherSortedSourceObjs(ArchiveBuilder* builder) : _builder(builder) {}\n+\n+  virtual bool do_ref(Ref* ref, bool read_only) {\n+    return _builder->gather_one_source_obj(enclosing_ref(), ref, read_only);\n+  }\n+\n+  virtual void push_special(SpecialRef type, Ref* ref, intptr_t* p) {\n+    address src_obj = ref->obj();\n+    size_t field_offset = pointer_delta(p, src_obj,  sizeof(u1));\n+    _builder->add_special_ref(type, src_obj, field_offset, ref->size() * BytesPerWord);\n+  };\n+\n+  virtual void do_pending_ref(Ref* ref) {\n+    if (ref->obj() != NULL) {\n+      _builder->remember_embedded_pointer_in_copied_obj(enclosing_ref(), ref);\n+    }\n+  }\n+};\n+\n+bool ArchiveBuilder::gather_one_source_obj(MetaspaceClosure::Ref* enclosing_ref,\n+                                           MetaspaceClosure::Ref* ref, bool read_only) {\n+  address src_obj = ref->obj();\n+  if (src_obj == NULL) {\n+    return false;\n+  }\n+  ref->set_keep_after_pushing();\n+  remember_embedded_pointer_in_copied_obj(enclosing_ref, ref);\n+\n+  FollowMode follow_mode = get_follow_mode(ref);\n+  SourceObjInfo src_info(ref, read_only, follow_mode);\n+  bool created;\n+  SourceObjInfo* p = _src_obj_table.add_if_absent(src_obj, src_info, &created);\n+  if (created) {\n+    if (_src_obj_table.maybe_grow(MAX_TABLE_SIZE)) {\n+      log_info(cds, hashtables)(\"Expanded _src_obj_table table to %d\", _src_obj_table.table_size());\n+    }\n+  }\n+\n+  assert(p->read_only() == src_info.read_only(), \"must be\");\n+\n+  if (created && src_info.should_copy()) {\n+    ref->set_user_data((void*)p);\n+    if (read_only) {\n+      _ro_src_objs.append(enclosing_ref, p);\n+    } else {\n+      _rw_src_objs.append(enclosing_ref, p);\n+    }\n+    return true; \/\/ Need to recurse into this ref only if we are copying it\n+  } else {\n+    return false;\n+  }\n+}\n+\n+void ArchiveBuilder::remember_embedded_pointer_in_copied_obj(MetaspaceClosure::Ref* enclosing_ref,\n+                                                             MetaspaceClosure::Ref* ref) {\n+  assert(ref->obj() != NULL, \"should have checked\");\n+\n+  if (enclosing_ref != NULL) {\n+    SourceObjInfo* src_info = (SourceObjInfo*)enclosing_ref->user_data();\n+    if (src_info == NULL) {\n+      \/\/ source objects of point_to_it\/set_to_null types are not copied\n+      \/\/ so we don't need to remember their pointers.\n+    } else {\n+      if (src_info->read_only()) {\n+        _ro_src_objs.remember_embedded_pointer(src_info, ref);\n+      } else {\n+        _rw_src_objs.remember_embedded_pointer(src_info, ref);\n+      }\n+    }\n+  }\n+}\n+\n+void ArchiveBuilder::gather_source_objs() {\n+  ResourceMark rm;\n+  log_info(cds)(\"Gathering all archivable objects ... \");\n+  gather_klasses_and_symbols();\n+  GatherSortedSourceObjs doit(this);\n+  iterate_sorted_roots(&doit, \/*is_relocating_pointers=*\/false);\n+  doit.finish();\n+}\n+\n+bool ArchiveBuilder::is_excluded(Klass* klass) {\n+  if (klass->is_instance_klass()) {\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    return SystemDictionaryShared::is_excluded_class(ik);\n+  } else if (klass->is_objArray_klass()) {\n+    if (DynamicDumpSharedSpaces) {\n+      \/\/ Don't support archiving of array klasses for now (WHY???)\n+      return true;\n+    }\n+    Klass* bottom = ObjArrayKlass::cast(klass)->bottom_klass();\n+    if (bottom->is_instance_klass()) {\n+      return SystemDictionaryShared::is_excluded_class(InstanceKlass::cast(bottom));\n+    }\n+  }\n+\n+  return false;\n+}\n+\n+ArchiveBuilder::FollowMode ArchiveBuilder::get_follow_mode(MetaspaceClosure::Ref *ref) {\n+  address obj = ref->obj();\n+  if (MetaspaceShared::is_in_shared_metaspace(obj)) {\n+    \/\/ Don't dump existing shared metadata again.\n+    return point_to_it;\n+  } else if (ref->msotype() == MetaspaceObj::MethodDataType) {\n+    return set_to_null;\n+  } else {\n+    if (ref->msotype() == MetaspaceObj::ClassType) {\n+      Klass* klass = (Klass*)ref->obj();\n+      assert(klass->is_klass(), \"must be\");\n+      if (is_excluded(klass)) {\n+        ResourceMark rm;\n+        log_debug(cds, dynamic)(\"Skipping class (excluded): %s\", klass->external_name());\n+        return set_to_null;\n+      }\n+    }\n+\n+    return make_a_copy;\n+  }\n+}\n+\n+void ArchiveBuilder::start_dump_space(DumpRegion* next) {\n+  address bottom = _last_verified_top;\n+  address top = (address)(current_dump_space()->top());\n+  _other_region_used_bytes += size_t(top - bottom);\n+\n+  current_dump_space()->pack(next);\n+  _current_dump_space = next;\n+  _num_dump_regions_used ++;\n+\n+  _last_verified_top = (address)(current_dump_space()->top());\n+}\n+\n+void ArchiveBuilder::verify_estimate_size(size_t estimate, const char* which) {\n+  address bottom = _last_verified_top;\n+  address top = (address)(current_dump_space()->top());\n+  size_t used = size_t(top - bottom) + _other_region_used_bytes;\n+  int diff = int(estimate) - int(used);\n+\n+  log_info(cds)(\"%s estimate = \" SIZE_FORMAT \" used = \" SIZE_FORMAT \"; diff = %d bytes\", which, estimate, used, diff);\n+  assert(diff >= 0, \"Estimate is too small\");\n+\n+  _last_verified_top = top;\n+  _other_region_used_bytes = 0;\n+}\n+\n+void ArchiveBuilder::dump_rw_metadata() {\n+  ResourceMark rm;\n+  log_info(cds)(\"Allocating RW objects ... \");\n+  make_shallow_copies(&_rw_region, &_rw_src_objs);\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+  if (is_dumping_full_module_graph()) {\n+    \/\/ Archive the ModuleEntry's and PackageEntry's of the 3 built-in loaders\n+    char* start = rw_region()->top();\n+    ClassLoaderDataShared::allocate_archived_tables();\n+    alloc_stats()->record_modules(rw_region()->top() - start, \/*read_only*\/false);\n+  }\n+#endif\n+}\n+\n+void ArchiveBuilder::dump_ro_metadata() {\n+  ResourceMark rm;\n+  log_info(cds)(\"Allocating RO objects ... \");\n+\n+  start_dump_space(&_ro_region);\n+  make_shallow_copies(&_ro_region, &_ro_src_objs);\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+  if (is_dumping_full_module_graph()) {\n+    char* start = ro_region()->top();\n+    ClassLoaderDataShared::init_archived_tables();\n+    alloc_stats()->record_modules(ro_region()->top() - start, \/*read_only*\/true);\n+  }\n+#endif\n+}\n+\n+void ArchiveBuilder::make_shallow_copies(DumpRegion *dump_region,\n+                                         const ArchiveBuilder::SourceObjList* src_objs) {\n+  for (int i = 0; i < src_objs->objs()->length(); i++) {\n+    make_shallow_copy(dump_region, src_objs->objs()->at(i));\n+  }\n+  log_info(cds)(\"done (%d objects)\", src_objs->objs()->length());\n+}\n+\n+void ArchiveBuilder::make_shallow_copy(DumpRegion *dump_region, SourceObjInfo* src_info) {\n+  MetaspaceClosure::Ref* ref = src_info->ref();\n+  address src = ref->obj();\n+  int bytes = src_info->size_in_bytes();\n+  char* dest;\n+  char* oldtop;\n+  char* newtop;\n+\n+  oldtop = dump_region->top();\n+  if (ref->msotype() == MetaspaceObj::ClassType) {\n+    \/\/ Save a pointer immediate in front of an InstanceKlass, so\n+    \/\/ we can do a quick lookup from InstanceKlass* -> RunTimeSharedClassInfo*\n+    \/\/ without building another hashtable. See RunTimeSharedClassInfo::get_for()\n+    \/\/ in systemDictionaryShared.cpp.\n+    Klass* klass = (Klass*)src;\n+    if (klass->is_instance_klass()) {\n+      SystemDictionaryShared::validate_before_archiving(InstanceKlass::cast(klass));\n+      dump_region->allocate(sizeof(address));\n+    }\n+  }\n+  dest = dump_region->allocate(bytes);\n+  newtop = dump_region->top();\n+\n+  memcpy(dest, src, bytes);\n+\n+  intptr_t* archived_vtable = CppVtables::get_archived_vtable(ref->msotype(), (address)dest);\n+  if (archived_vtable != NULL) {\n+    *(address*)dest = (address)archived_vtable;\n+    ArchivePtrMarker::mark_pointer((address*)dest);\n+  }\n+\n+  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d\", p2i(src), p2i(dest), bytes);\n+  src_info->set_dumped_addr((address)dest);\n+\n+  _alloc_stats.record(ref->msotype(), int(newtop - oldtop), src_info->read_only());\n+}\n+\n+address ArchiveBuilder::get_dumped_addr(address src_obj) const {\n+  SourceObjInfo* p = _src_obj_table.lookup(src_obj);\n+  assert(p != NULL, \"must be\");\n+\n+  return p->dumped_addr();\n+}\n+\n+void ArchiveBuilder::relocate_embedded_pointers(ArchiveBuilder::SourceObjList* src_objs) {\n+  for (int i = 0; i < src_objs->objs()->length(); i++) {\n+    src_objs->relocate(i, this);\n+  }\n+}\n+\n+void ArchiveBuilder::update_special_refs() {\n+  for (int i = 0; i < _special_refs->length(); i++) {\n+    SpecialRefInfo s = _special_refs->at(i);\n+    size_t field_offset = s.field_offset();\n+    address src_obj = s.src_obj();\n+    address dst_obj = get_dumped_addr(src_obj);\n+    intptr_t* src_p = (intptr_t*)(src_obj + field_offset);\n+    intptr_t* dst_p = (intptr_t*)(dst_obj + field_offset);\n+\n+\n+    MetaspaceClosure::assert_valid(s.type());\n+    switch (s.type()) {\n+    case MetaspaceClosure::_method_entry_ref:\n+      assert(*src_p == *dst_p, \"must be a copy\");\n+      break;\n+    case MetaspaceClosure::_internal_pointer_ref:\n+      {\n+        \/\/ *src_p points to a location inside src_obj. Let's make *dst_p point to\n+        \/\/ the same location inside dst_obj.\n+        size_t off = pointer_delta(*((address*)src_p), src_obj, sizeof(u1));\n+        assert(off < s.src_obj_size_in_bytes(), \"must point to internal address\");\n+        *((address*)dst_p) = dst_obj + off;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+    ArchivePtrMarker::mark_pointer((address*)dst_p);\n+  }\n+}\n+\n+class RefRelocator: public MetaspaceClosure {\n+  ArchiveBuilder* _builder;\n+\n+public:\n+  RefRelocator(ArchiveBuilder* builder) : _builder(builder) {}\n+\n+  virtual bool do_ref(Ref* ref, bool read_only) {\n+    if (ref->not_null()) {\n+      ref->update(_builder->get_dumped_addr(ref->obj()));\n+      ArchivePtrMarker::mark_pointer(ref->addr());\n+    }\n+    return false; \/\/ Do not recurse.\n+  }\n+};\n+\n+void ArchiveBuilder::relocate_roots() {\n+  log_info(cds)(\"Relocating external roots ... \");\n+  ResourceMark rm;\n+  RefRelocator doit(this);\n+  iterate_sorted_roots(&doit, \/*is_relocating_pointers=*\/true);\n+  doit.finish();\n+  log_info(cds)(\"done\");\n+}\n+\n+void ArchiveBuilder::relocate_metaspaceobj_embedded_pointers() {\n+  log_info(cds)(\"Relocating embedded pointers in core regions ... \");\n+  relocate_embedded_pointers(&_rw_src_objs);\n+  relocate_embedded_pointers(&_ro_src_objs);\n+  update_special_refs();\n+}\n+\n+\/\/ We must relocate vmClasses::_klasses[] only after we have copied the\n+\/\/ java objects in during dump_java_heap_objects(): during the object copy, we operate on\n+\/\/ old objects which assert that their klass is the original klass.\n+void ArchiveBuilder::relocate_vm_classes() {\n+  log_info(cds)(\"Relocating vmClasses::_klasses[] ... \");\n+  ResourceMark rm;\n+  RefRelocator doit(this);\n+  vmClasses::metaspace_pointers_do(&doit);\n+}\n+\n+void ArchiveBuilder::make_klasses_shareable() {\n+  for (int i = 0; i < klasses()->length(); i++) {\n+    Klass* k = klasses()->at(i);\n+    k->remove_java_mirror();\n+    if (k->is_objArray_klass()) {\n+      \/\/ InstanceKlass and TypeArrayKlass will in turn call remove_unshareable_info\n+      \/\/ on their array classes.\n+    } else if (k->is_typeArray_klass()) {\n+      k->remove_unshareable_info();\n+    } else {\n+      assert(k->is_instance_klass(), \" must be\");\n+      InstanceKlass* ik = InstanceKlass::cast(k);\n+      if (DynamicDumpSharedSpaces) {\n+        \/\/ For static dump, class loader type are already set.\n+        ik->assign_class_loader_type();\n+      }\n+\n+      MetaspaceShared::rewrite_nofast_bytecodes_and_calculate_fingerprints(Thread::current(), ik);\n+      ik->remove_unshareable_info();\n+\n+      if (log_is_enabled(Debug, cds, class)) {\n+        ResourceMark rm;\n+        log_debug(cds, class)(\"klasses[%4d] = \" PTR_FORMAT \" %s\", i, p2i(to_requested(ik)), ik->external_name());\n+      }\n+    }\n+  }\n+}\n+\n+uintx ArchiveBuilder::buffer_to_offset(address p) const {\n+  address requested_p = to_requested(p);\n+  assert(requested_p >= _requested_static_archive_bottom, \"must be\");\n+  return requested_p - _requested_static_archive_bottom;\n+}\n+\n+uintx ArchiveBuilder::any_to_offset(address p) const {\n+  if (is_in_mapped_static_archive(p)) {\n+    assert(DynamicDumpSharedSpaces, \"must be\");\n+    return p - _mapped_static_archive_bottom;\n+  }\n+  return buffer_to_offset(p);\n+}\n+\n+\/\/ Update a Java object to point its Klass* to the new location after\n+\/\/ shared archive has been compacted.\n+void ArchiveBuilder::relocate_klass_ptr(oop o) {\n+  assert(DumpSharedSpaces, \"sanity\");\n+  Klass* k = get_relocated_klass(o->klass());\n+  Klass* requested_k = to_requested(k);\n+  narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n+  o->set_narrow_klass(nk);\n+}\n+\n+\/\/ RelocateBufferToRequested --- Relocate all the pointers in rw\/ro,\n+\/\/ so that the archive can be mapped to the \"requested\" location without runtime relocation.\n+\/\/\n+\/\/ - See ArchiveBuilder header for the definition of \"buffer\", \"mapped\" and \"requested\"\n+\/\/ - ArchivePtrMarker::ptrmap() marks all the pointers in the rw\/ro regions\n+\/\/ - Every pointer must have one of the following values:\n+\/\/   [a] NULL:\n+\/\/       No relocation is needed. Remove this pointer from ptrmap so we don't need to\n+\/\/       consider it at runtime.\n+\/\/   [b] Points into an object X which is inside the buffer:\n+\/\/       Adjust this pointer by _buffer_to_requested_delta, so it points to X\n+\/\/       when the archive is mapped at the requested location.\n+\/\/   [c] Points into an object Y which is inside mapped static archive:\n+\/\/       - This happens only during dynamic dump\n+\/\/       - Adjust this pointer by _mapped_to_requested_static_archive_delta,\n+\/\/         so it points to Y when the static archive is mapped at the requested location.\n+template <bool STATIC_DUMP>\n+class RelocateBufferToRequested : public BitMapClosure {\n+  ArchiveBuilder* _builder;\n+  address _buffer_bottom;\n+  intx _buffer_to_requested_delta;\n+  intx _mapped_to_requested_static_archive_delta;\n+  size_t _max_non_null_offset;\n+\n+ public:\n+  RelocateBufferToRequested(ArchiveBuilder* builder) {\n+    _builder = builder;\n+    _buffer_bottom = _builder->buffer_bottom();\n+    _buffer_to_requested_delta = builder->buffer_to_requested_delta();\n+    _mapped_to_requested_static_archive_delta = builder->requested_static_archive_bottom() - builder->mapped_static_archive_bottom();\n+    _max_non_null_offset = 0;\n+\n+    address bottom = _builder->buffer_bottom();\n+    address top = _builder->buffer_top();\n+    address new_bottom = bottom + _buffer_to_requested_delta;\n+    address new_top = top + _buffer_to_requested_delta;\n+    log_debug(cds)(\"Relocating archive from [\" INTPTR_FORMAT \" - \" INTPTR_FORMAT \"] to \"\n+                   \"[\" INTPTR_FORMAT \" - \" INTPTR_FORMAT \"]\",\n+                   p2i(bottom), p2i(top),\n+                   p2i(new_bottom), p2i(new_top));\n+  }\n+\n+  bool do_bit(size_t offset) {\n+    address* p = (address*)_buffer_bottom + offset;\n+    assert(_builder->is_in_buffer_space(p), \"pointer must live in buffer space\");\n+\n+    if (*p == NULL) {\n+      \/\/ todo -- clear bit, etc\n+      ArchivePtrMarker::ptrmap()->clear_bit(offset);\n+    } else {\n+      if (STATIC_DUMP) {\n+        assert(_builder->is_in_buffer_space(*p), \"old pointer must point inside buffer space\");\n+        *p += _buffer_to_requested_delta;\n+        assert(_builder->is_in_requested_static_archive(*p), \"new pointer must point inside requested archive\");\n+      } else {\n+        if (_builder->is_in_buffer_space(*p)) {\n+          *p += _buffer_to_requested_delta;\n+          \/\/ assert is in requested dynamic archive\n+        } else {\n+          assert(_builder->is_in_mapped_static_archive(*p), \"old pointer must point inside buffer space or mapped static archive\");\n+          *p += _mapped_to_requested_static_archive_delta;\n+          assert(_builder->is_in_requested_static_archive(*p), \"new pointer must point inside requested archive\");\n+        }\n+      }\n+      _max_non_null_offset = offset;\n+    }\n+\n+    return true; \/\/ keep iterating\n+  }\n+\n+  void doit() {\n+    ArchivePtrMarker::ptrmap()->iterate(this);\n+    ArchivePtrMarker::compact(_max_non_null_offset);\n+  }\n+};\n+\n+\n+void ArchiveBuilder::relocate_to_requested() {\n+  ro_region()->pack();\n+\n+  size_t my_archive_size = buffer_top() - buffer_bottom();\n+\n+  if (DumpSharedSpaces) {\n+    _requested_static_archive_top = _requested_static_archive_bottom + my_archive_size;\n+    RelocateBufferToRequested<true> patcher(this);\n+    patcher.doit();\n+  } else {\n+    assert(DynamicDumpSharedSpaces, \"must be\");\n+    _requested_dynamic_archive_top = _requested_dynamic_archive_bottom + my_archive_size;\n+    RelocateBufferToRequested<false> patcher(this);\n+    patcher.doit();\n+  }\n+}\n+\n+\/\/ Write detailed info to a mapfile to analyze contents of the archive.\n+\/\/ static dump:\n+\/\/   java -Xshare:dump -Xlog:cds+map=trace:file=cds.map:none:filesize=0\n+\/\/ dynamic dump:\n+\/\/   java -cp MyApp.jar -XX:ArchiveClassesAtExit=MyApp.jsa \\\n+\/\/        -Xlog:cds+map=trace:file=cds.map:none:filesize=0 MyApp\n+\/\/\n+\/\/ We need to do some address translation because the buffers used at dump time may be mapped to\n+\/\/ a different location at runtime. At dump time, the buffers may be at arbitrary locations\n+\/\/ picked by the OS. At runtime, we try to map at a fixed location (SharedBaseAddress). For\n+\/\/ consistency, we log everything using runtime addresses.\n+class ArchiveBuilder::CDSMapLogger : AllStatic {\n+  static intx buffer_to_runtime_delta() {\n+    \/\/ Translate the buffers used by the RW\/RO regions to their eventual (requested) locations\n+    \/\/ at runtime.\n+    return ArchiveBuilder::current()->buffer_to_requested_delta();\n+  }\n+\n+  \/\/ rw\/ro regions only\n+  static void write_dump_region(const char* name, DumpRegion* region) {\n+    address region_base = address(region->base());\n+    address region_top  = address(region->top());\n+    write_region(name, region_base, region_top, region_base + buffer_to_runtime_delta());\n+  }\n+\n+#define _LOG_PREFIX PTR_FORMAT \": @@ %-17s %d\"\n+\n+  static void write_klass(Klass* k, address runtime_dest, const char* type_name, int bytes, Thread* THREAD) {\n+    ResourceMark rm(THREAD);\n+    log_debug(cds, map)(_LOG_PREFIX \" %s\",\n+                        p2i(runtime_dest), type_name, bytes, k->external_name());\n+  }\n+  static void write_method(Method* m, address runtime_dest, const char* type_name, int bytes, Thread* THREAD) {\n+    ResourceMark rm(THREAD);\n+    log_debug(cds, map)(_LOG_PREFIX \" %s\",\n+                        p2i(runtime_dest), type_name, bytes,  m->external_name());\n+  }\n+\n+  \/\/ rw\/ro regions only\n+  static void write_objects(DumpRegion* region, const ArchiveBuilder::SourceObjList* src_objs) {\n+    address last_obj_base = address(region->base());\n+    address last_obj_end  = address(region->base());\n+    address region_end    = address(region->end());\n+    Thread* THREAD = Thread::current();\n+    for (int i = 0; i < src_objs->objs()->length(); i++) {\n+      SourceObjInfo* src_info = src_objs->at(i);\n+      address src = src_info->orig_obj();\n+      address dest = src_info->dumped_addr();\n+      write_data(last_obj_base, dest, last_obj_base + buffer_to_runtime_delta());\n+      address runtime_dest = dest + buffer_to_runtime_delta();\n+      int bytes = src_info->size_in_bytes();\n+\n+      MetaspaceObj::Type type = src_info->msotype();\n+      const char* type_name = MetaspaceObj::type_name(type);\n+\n+      switch (type) {\n+      case MetaspaceObj::ClassType:\n+        write_klass((Klass*)src, runtime_dest, type_name, bytes, THREAD);\n+        break;\n+      case MetaspaceObj::ConstantPoolType:\n+        write_klass(((ConstantPool*)src)->pool_holder(),\n+                    runtime_dest, type_name, bytes, THREAD);\n+        break;\n+      case MetaspaceObj::ConstantPoolCacheType:\n+        write_klass(((ConstantPoolCache*)src)->constant_pool()->pool_holder(),\n+                    runtime_dest, type_name, bytes, THREAD);\n+        break;\n+      case MetaspaceObj::MethodType:\n+        write_method((Method*)src, runtime_dest, type_name, bytes, THREAD);\n+        break;\n+      case MetaspaceObj::ConstMethodType:\n+        write_method(((ConstMethod*)src)->method(), runtime_dest, type_name, bytes, THREAD);\n+        break;\n+      case MetaspaceObj::SymbolType:\n+        {\n+          ResourceMark rm(THREAD);\n+          Symbol* s = (Symbol*)src;\n+          log_debug(cds, map)(_LOG_PREFIX \" %s\", p2i(runtime_dest), type_name, bytes,\n+                              s->as_quoted_ascii());\n+        }\n+        break;\n+      default:\n+        log_debug(cds, map)(_LOG_PREFIX, p2i(runtime_dest), type_name, bytes);\n+        break;\n+      }\n+\n+      last_obj_base = dest;\n+      last_obj_end  = dest + bytes;\n+    }\n+\n+    write_data(last_obj_base, last_obj_end, last_obj_base + buffer_to_runtime_delta());\n+    if (last_obj_end < region_end) {\n+      log_debug(cds, map)(PTR_FORMAT \": @@ Misc data \" SIZE_FORMAT \" bytes\",\n+                          p2i(last_obj_end + buffer_to_runtime_delta()),\n+                          size_t(region_end - last_obj_end));\n+      write_data(last_obj_end, region_end, last_obj_end + buffer_to_runtime_delta());\n+    }\n+  }\n+\n+#undef _LOG_PREFIX\n+\n+  \/\/ Write information about a region, whose address at dump time is [base .. top). At\n+  \/\/ runtime, this region will be mapped to runtime_base.  runtime_base is 0 if this\n+  \/\/ region will be mapped at os-selected addresses (such as the bitmap region), or will\n+  \/\/ be accessed with os::read (the header).\n+  static void write_region(const char* name, address base, address top, address runtime_base) {\n+    size_t size = top - base;\n+    base = runtime_base;\n+    top = runtime_base + size;\n+    log_info(cds, map)(\"[%-18s \" PTR_FORMAT \" - \" PTR_FORMAT \" \" SIZE_FORMAT_W(9) \" bytes]\",\n+                       name, p2i(base), p2i(top), size);\n+  }\n+\n+  \/\/ open and closed archive regions\n+  static void write_heap_region(const char* which, GrowableArray<MemRegion> *regions) {\n+    for (int i = 0; i < regions->length(); i++) {\n+      address start = address(regions->at(i).start());\n+      address end = address(regions->at(i).end());\n+      write_region(which, start, end, start);\n+      write_data(start, end, start);\n+    }\n+  }\n+\n+  \/\/ Dump all the data [base...top). Pretend that the base address\n+  \/\/ will be mapped to runtime_base at run-time.\n+  static void write_data(address base, address top, address runtime_base) {\n+    assert(top >= base, \"must be\");\n+\n+    LogStreamHandle(Trace, cds, map) lsh;\n+    if (lsh.is_enabled()) {\n+      os::print_hex_dump(&lsh, base, top, sizeof(address), 32, runtime_base);\n+    }\n+  }\n+\n+  static void write_header(FileMapInfo* mapinfo) {\n+    LogStreamHandle(Info, cds, map) lsh;\n+    if (lsh.is_enabled()) {\n+      mapinfo->print(&lsh);\n+    }\n+  }\n+\n+public:\n+  static void write(ArchiveBuilder* builder, FileMapInfo* mapinfo,\n+             GrowableArray<MemRegion> *closed_heap_regions,\n+             GrowableArray<MemRegion> *open_heap_regions,\n+             char* bitmap, size_t bitmap_size_in_bytes) {\n+    log_info(cds, map)(\"%s CDS archive map for %s\", DumpSharedSpaces ? \"Static\" : \"Dynamic\", mapinfo->full_path());\n+\n+    address header = address(mapinfo->header());\n+    address header_end = header + mapinfo->header()->header_size();\n+    write_region(\"header\", header, header_end, 0);\n+    write_header(mapinfo);\n+    write_data(header, header_end, 0);\n+\n+    DumpRegion* rw_region = &builder->_rw_region;\n+    DumpRegion* ro_region = &builder->_ro_region;\n+\n+    write_dump_region(\"rw region\", rw_region);\n+    write_objects(rw_region, &builder->_rw_src_objs);\n+\n+    write_dump_region(\"ro region\", ro_region);\n+    write_objects(ro_region, &builder->_ro_src_objs);\n+\n+    address bitmap_end = address(bitmap + bitmap_size_in_bytes);\n+    write_region(\"bitmap\", address(bitmap), bitmap_end, 0);\n+    write_data(header, header_end, 0);\n+\n+    if (closed_heap_regions != NULL) {\n+      write_heap_region(\"closed heap region\", closed_heap_regions);\n+    }\n+    if (open_heap_regions != NULL) {\n+      write_heap_region(\"open heap region\", open_heap_regions);\n+    }\n+\n+    log_info(cds, map)(\"[End of CDS archive map]\");\n+  }\n+};\n+\n+void ArchiveBuilder::print_stats() {\n+  _alloc_stats.print_stats(int(_ro_region.used()), int(_rw_region.used()));\n+}\n+\n+void ArchiveBuilder::clean_up_src_obj_table() {\n+  SrcObjTableCleaner cleaner;\n+  _src_obj_table.iterate(&cleaner);\n+}\n+\n+void ArchiveBuilder::write_archive(FileMapInfo* mapinfo,\n+                                   GrowableArray<MemRegion>* closed_heap_regions,\n+                                   GrowableArray<MemRegion>* open_heap_regions,\n+                                   GrowableArray<ArchiveHeapOopmapInfo>* closed_heap_oopmaps,\n+                                   GrowableArray<ArchiveHeapOopmapInfo>* open_heap_oopmaps) {\n+  \/\/ Make sure NUM_CDS_REGIONS (exported in cds.h) agrees with\n+  \/\/ MetaspaceShared::n_regions (internal to hotspot).\n+  assert(NUM_CDS_REGIONS == MetaspaceShared::n_regions, \"sanity\");\n+\n+  write_region(mapinfo, MetaspaceShared::rw, &_rw_region, \/*read_only=*\/false,\/*allow_exec=*\/false);\n+  write_region(mapinfo, MetaspaceShared::ro, &_ro_region, \/*read_only=*\/true, \/*allow_exec=*\/false);\n+\n+  size_t bitmap_size_in_bytes;\n+  char* bitmap = mapinfo->write_bitmap_region(ArchivePtrMarker::ptrmap(), closed_heap_oopmaps, open_heap_oopmaps,\n+                                              bitmap_size_in_bytes);\n+\n+  if (closed_heap_regions != NULL) {\n+    _total_closed_heap_region_size = mapinfo->write_archive_heap_regions(\n+                                        closed_heap_regions,\n+                                        closed_heap_oopmaps,\n+                                        MetaspaceShared::first_closed_archive_heap_region,\n+                                        MetaspaceShared::max_closed_archive_heap_region);\n+    _total_open_heap_region_size = mapinfo->write_archive_heap_regions(\n+                                        open_heap_regions,\n+                                        open_heap_oopmaps,\n+                                        MetaspaceShared::first_open_archive_heap_region,\n+                                        MetaspaceShared::max_open_archive_heap_region);\n+  }\n+\n+  print_region_stats(mapinfo, closed_heap_regions, open_heap_regions);\n+\n+  mapinfo->set_requested_base((char*)MetaspaceShared::requested_base_address());\n+  if (mapinfo->header()->magic() == CDS_DYNAMIC_ARCHIVE_MAGIC) {\n+    mapinfo->set_header_base_archive_name_size(strlen(Arguments::GetSharedArchivePath()) + 1);\n+    mapinfo->set_header_base_archive_is_default(FLAG_IS_DEFAULT(SharedArchiveFile));\n+  }\n+  mapinfo->set_header_crc(mapinfo->compute_header_crc());\n+  \/\/ After this point, we should not write any data into mapinfo->header() since this\n+  \/\/ would corrupt its checksum we have calculated before.\n+  mapinfo->write_header();\n+  mapinfo->close();\n+\n+  if (log_is_enabled(Info, cds)) {\n+    print_stats();\n+  }\n+\n+  if (log_is_enabled(Info, cds, map)) {\n+    CDSMapLogger::write(this, mapinfo, closed_heap_regions, open_heap_regions,\n+                        bitmap, bitmap_size_in_bytes);\n+  }\n+  FREE_C_HEAP_ARRAY(char, bitmap);\n+}\n+\n+void ArchiveBuilder::write_region(FileMapInfo* mapinfo, int region_idx, DumpRegion* dump_region, bool read_only,  bool allow_exec) {\n+  mapinfo->write_region(region_idx, dump_region->base(), dump_region->used(), read_only, allow_exec);\n+}\n+\n+void ArchiveBuilder::print_region_stats(FileMapInfo *mapinfo,\n+                                        GrowableArray<MemRegion>* closed_heap_regions,\n+                                        GrowableArray<MemRegion>* open_heap_regions) {\n+  \/\/ Print statistics of all the regions\n+  const size_t bitmap_used = mapinfo->space_at(MetaspaceShared::bm)->used();\n+  const size_t bitmap_reserved = mapinfo->space_at(MetaspaceShared::bm)->used_aligned();\n+  const size_t total_reserved = _ro_region.reserved()  + _rw_region.reserved() +\n+                                bitmap_reserved +\n+                                _total_closed_heap_region_size +\n+                                _total_open_heap_region_size;\n+  const size_t total_bytes = _ro_region.used()  + _rw_region.used() +\n+                             bitmap_used +\n+                             _total_closed_heap_region_size +\n+                             _total_open_heap_region_size;\n+  const double total_u_perc = percent_of(total_bytes, total_reserved);\n+\n+  _rw_region.print(total_reserved);\n+  _ro_region.print(total_reserved);\n+\n+  print_bitmap_region_stats(bitmap_used, total_reserved);\n+\n+  if (closed_heap_regions != NULL) {\n+    print_heap_region_stats(closed_heap_regions, \"ca\", total_reserved);\n+    print_heap_region_stats(open_heap_regions, \"oa\", total_reserved);\n+  }\n+\n+  log_debug(cds)(\"total    : \" SIZE_FORMAT_W(9) \" [100.0%% of total] out of \" SIZE_FORMAT_W(9) \" bytes [%5.1f%% used]\",\n+                 total_bytes, total_reserved, total_u_perc);\n+}\n+\n+void ArchiveBuilder::print_bitmap_region_stats(size_t size, size_t total_size) {\n+  log_debug(cds)(\"bm  space: \" SIZE_FORMAT_W(9) \" [ %4.1f%% of total] out of \" SIZE_FORMAT_W(9) \" bytes [100.0%% used]\",\n+                 size, size\/double(total_size)*100.0, size);\n+}\n+\n+void ArchiveBuilder::print_heap_region_stats(GrowableArray<MemRegion> *heap_mem,\n+                                             const char *name, size_t total_size) {\n+  int arr_len = heap_mem == NULL ? 0 : heap_mem->length();\n+  for (int i = 0; i < arr_len; i++) {\n+      char* start = (char*)heap_mem->at(i).start();\n+      size_t size = heap_mem->at(i).byte_size();\n+      char* top = start + size;\n+      log_debug(cds)(\"%s%d space: \" SIZE_FORMAT_W(9) \" [ %4.1f%% of total] out of \" SIZE_FORMAT_W(9) \" bytes [100.0%% used] at \" INTPTR_FORMAT,\n+                     name, i, size, size\/double(total_size)*100.0, size, p2i(start));\n+  }\n+}\n+\n+void ArchiveBuilder::report_out_of_space(const char* name, size_t needed_bytes) {\n+  \/\/ This is highly unlikely to happen on 64-bits because we have reserved a 4GB space.\n+  \/\/ On 32-bit we reserve only 256MB so you could run out of space with 100,000 classes\n+  \/\/ or so.\n+  _rw_region.print_out_of_space_msg(name, needed_bytes);\n+  _ro_region.print_out_of_space_msg(name, needed_bytes);\n+\n+  vm_exit_during_initialization(err_msg(\"Unable to allocate from '%s' region\", name),\n+                                \"Please reduce the number of shared classes.\");\n+}\n+\n+\n+#ifndef PRODUCT\n+void ArchiveBuilder::assert_is_vm_thread() {\n+  assert(Thread::current()->is_VM_thread(), \"ArchiveBuilder should be used only inside the VMThread\");\n+}\n+#endif\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":1194,"deletions":0,"binary":false,"changes":1194,"status":"added"},{"patch":"@@ -0,0 +1,440 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_CDS_ARCHIVEBUILDER_HPP\n+#define SHARE_CDS_ARCHIVEBUILDER_HPP\n+\n+#include \"cds\/archiveUtils.hpp\"\n+#include \"cds\/dumpAllocStats.hpp\"\n+#include \"memory\/metaspaceClosure.hpp\"\n+#include \"oops\/array.hpp\"\n+#include \"oops\/klass.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/bitMap.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/hashtable.hpp\"\n+#include \"utilities\/resourceHash.hpp\"\n+\n+struct ArchiveHeapOopmapInfo;\n+class CHeapBitMap;\n+class FileMapInfo;\n+class Klass;\n+class MemRegion;\n+class Symbol;\n+\n+\/\/ Metaspace::allocate() requires that all blocks must be aligned with KlassAlignmentInBytes.\n+\/\/ We enforce the same alignment rule in blocks allocated from the shared space.\n+const int SharedSpaceObjectAlignment = KlassAlignmentInBytes;\n+\n+\/\/ Overview of CDS archive creation (for both static and dynamic dump):\n+\/\/\n+\/\/ [1] Load all classes (static dump: from the classlist, dynamic dump: as part of app execution)\n+\/\/ [2] Allocate \"output buffer\"\n+\/\/ [3] Copy contents of the 2 \"core\" regions (rw\/ro) into the output buffer.\n+\/\/       - allocate the cpp vtables in rw (static dump only)\n+\/\/       - memcpy the MetaspaceObjs into rw\/ro:\n+\/\/         dump_rw_region();\n+\/\/         dump_ro_region();\n+\/\/       - fix all the pointers in the MetaspaceObjs to point to the copies\n+\/\/         relocate_metaspaceobj_embedded_pointers()\n+\/\/ [4] Copy symbol table, dictionary, etc, into the ro region\n+\/\/ [5] Relocate all the pointers in rw\/ro, so that the archive can be mapped to\n+\/\/     the \"requested\" location without runtime relocation. See relocate_to_requested()\n+class ArchiveBuilder : public StackObj {\n+protected:\n+  DumpRegion* _current_dump_space;\n+  address _buffer_bottom;                      \/\/ for writing the contents of rw\/ro regions\n+  address _last_verified_top;\n+  int _num_dump_regions_used;\n+  size_t _other_region_used_bytes;\n+\n+  \/\/ These are the addresses where we will request the static and dynamic archives to be\n+  \/\/ mapped at run time. If the request fails (due to ASLR), we will map the archives at\n+  \/\/ os-selected addresses.\n+  address _requested_static_archive_bottom;     \/\/ This is determined solely by the value of\n+                                                \/\/ SharedBaseAddress during -Xshare:dump.\n+  address _requested_static_archive_top;\n+  address _requested_dynamic_archive_bottom;    \/\/ Used only during dynamic dump. It's placed\n+                                                \/\/ immediately above _requested_static_archive_top.\n+  address _requested_dynamic_archive_top;\n+\n+  \/\/ (Used only during dynamic dump) where the static archive is actually mapped. This\n+  \/\/ may be different than _requested_static_archive_{bottom,top} due to ASLR\n+  address _mapped_static_archive_bottom;\n+  address _mapped_static_archive_top;\n+\n+  intx _buffer_to_requested_delta;\n+\n+  DumpRegion* current_dump_space() const {  return _current_dump_space;  }\n+\n+public:\n+  enum FollowMode {\n+    make_a_copy, point_to_it, set_to_null\n+  };\n+\n+private:\n+  class SpecialRefInfo {\n+    \/\/ We have a \"special pointer\" of the given _type at _field_offset of _src_obj.\n+    \/\/ See MetaspaceClosure::push_special().\n+    MetaspaceClosure::SpecialRef _type;\n+    address _src_obj;\n+    size_t _field_offset;\n+    DEBUG_ONLY(size_t _src_obj_size_in_bytes;)\n+\n+  public:\n+    SpecialRefInfo() {}\n+    SpecialRefInfo(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset, size_t src_obj_size_in_bytes)\n+      : _type(type), _src_obj(src_obj), _field_offset(field_offset) {\n+      DEBUG_ONLY(_src_obj_size_in_bytes = src_obj_size_in_bytes);\n+    }\n+\n+    MetaspaceClosure::SpecialRef type() const { return _type;         }\n+    address src_obj()                   const { return _src_obj;      }\n+    size_t field_offset()               const { return _field_offset; }\n+\n+    DEBUG_ONLY(size_t src_obj_size_in_bytes() const { return _src_obj_size_in_bytes; })\n+  };\n+\n+  class SourceObjInfo {\n+    MetaspaceClosure::Ref* _ref;\n+    uintx _ptrmap_start;     \/\/ The bit-offset of the start of this object (inclusive)\n+    uintx _ptrmap_end;       \/\/ The bit-offset of the end   of this object (exclusive)\n+    bool _read_only;\n+    FollowMode _follow_mode;\n+    int _size_in_bytes;\n+    MetaspaceObj::Type _msotype;\n+    address _dumped_addr;    \/\/ Address this->obj(), as used by the dumped archive.\n+    address _orig_obj;       \/\/ The value of the original object (_ref->obj()) when this\n+                             \/\/ SourceObjInfo was created. Note that _ref->obj() may change\n+                             \/\/ later if _ref is relocated.\n+\n+  public:\n+    SourceObjInfo(MetaspaceClosure::Ref* ref, bool read_only, FollowMode follow_mode) :\n+      _ref(ref), _ptrmap_start(0), _ptrmap_end(0), _read_only(read_only), _follow_mode(follow_mode),\n+      _size_in_bytes(ref->size() * BytesPerWord), _msotype(ref->msotype()),\n+      _orig_obj(ref->obj()) {\n+      if (follow_mode == point_to_it) {\n+        _dumped_addr = ref->obj();\n+      } else {\n+        _dumped_addr = NULL;\n+      }\n+    }\n+\n+    bool should_copy() const { return _follow_mode == make_a_copy; }\n+    MetaspaceClosure::Ref* ref() const { return  _ref; }\n+    void set_dumped_addr(address dumped_addr)  {\n+      assert(should_copy(), \"must be\");\n+      assert(_dumped_addr == NULL, \"cannot be copied twice\");\n+      assert(dumped_addr != NULL, \"must be a valid copy\");\n+      _dumped_addr = dumped_addr;\n+    }\n+    void set_ptrmap_start(uintx v) { _ptrmap_start = v;    }\n+    void set_ptrmap_end(uintx v)   { _ptrmap_end = v;      }\n+    uintx ptrmap_start()  const    { return _ptrmap_start; } \/\/ inclusive\n+    uintx ptrmap_end()    const    { return _ptrmap_end;   } \/\/ exclusive\n+    bool read_only()      const    { return _read_only;    }\n+    int size_in_bytes()   const    { return _size_in_bytes; }\n+    address orig_obj()    const    { return _orig_obj; }\n+    address dumped_addr() const    { return _dumped_addr; }\n+    MetaspaceObj::Type msotype() const { return _msotype; }\n+\n+    \/\/ convenience accessor\n+    address obj() const { return ref()->obj(); }\n+  };\n+\n+  class SourceObjList {\n+    uintx _total_bytes;\n+    GrowableArray<SourceObjInfo*>* _objs;     \/\/ Source objects to be archived\n+    CHeapBitMap _ptrmap;                      \/\/ Marks the addresses of the pointer fields\n+                                              \/\/ in the source objects\n+  public:\n+    SourceObjList();\n+    ~SourceObjList();\n+\n+    GrowableArray<SourceObjInfo*>* objs() const { return _objs; }\n+\n+    void append(MetaspaceClosure::Ref* enclosing_ref, SourceObjInfo* src_info);\n+    void remember_embedded_pointer(SourceObjInfo* pointing_obj, MetaspaceClosure::Ref* ref);\n+    void relocate(int i, ArchiveBuilder* builder);\n+\n+    \/\/ convenience accessor\n+    SourceObjInfo* at(int i) const { return objs()->at(i); }\n+  };\n+\n+  class SrcObjTableCleaner {\n+  public:\n+    bool do_entry(address key, const SourceObjInfo* value) {\n+      delete value->ref();\n+      return true;\n+    }\n+  };\n+\n+  class CDSMapLogger;\n+\n+  static const int INITIAL_TABLE_SIZE = 15889;\n+  static const int MAX_TABLE_SIZE     = 1000000;\n+\n+  ReservedSpace _shared_rs;\n+  VirtualSpace _shared_vs;\n+\n+  DumpRegion _rw_region;\n+  DumpRegion _ro_region;\n+  CHeapBitMap _ptrmap;    \/\/ bitmap used by ArchivePtrMarker\n+\n+  SourceObjList _rw_src_objs;                 \/\/ objs to put in rw region\n+  SourceObjList _ro_src_objs;                 \/\/ objs to put in ro region\n+  KVHashtable<address, SourceObjInfo, mtClassShared> _src_obj_table;\n+  GrowableArray<Klass*>* _klasses;\n+  GrowableArray<Symbol*>* _symbols;\n+  GrowableArray<SpecialRefInfo>* _special_refs;\n+\n+  \/\/ statistics\n+  int _num_instance_klasses;\n+  int _num_obj_array_klasses;\n+  int _num_type_array_klasses;\n+  DumpAllocStats _alloc_stats;\n+  size_t _total_closed_heap_region_size;\n+  size_t _total_open_heap_region_size;\n+\n+  void print_region_stats(FileMapInfo *map_info,\n+                          GrowableArray<MemRegion>* closed_heap_regions,\n+                          GrowableArray<MemRegion>* open_heap_regions);\n+  void print_bitmap_region_stats(size_t size, size_t total_size);\n+  void print_heap_region_stats(GrowableArray<MemRegion> *heap_mem,\n+                               const char *name, size_t total_size);\n+\n+  \/\/ For global access.\n+  static ArchiveBuilder* _current;\n+\n+public:\n+  \/\/ Use this when you allocate space outside of ArchiveBuilder::dump_{rw,ro}_region.\n+  \/\/ These are usually for misc tables that are allocated in the RO space.\n+  class OtherROAllocMark {\n+    char* _oldtop;\n+  public:\n+    OtherROAllocMark() {\n+      _oldtop = _current->_ro_region.top();\n+    }\n+    ~OtherROAllocMark();\n+  };\n+\n+private:\n+  bool is_dumping_full_module_graph();\n+  FollowMode get_follow_mode(MetaspaceClosure::Ref *ref);\n+\n+  void iterate_sorted_roots(MetaspaceClosure* it, bool is_relocating_pointers);\n+  void sort_symbols_and_fix_hash();\n+  void sort_klasses();\n+  static int compare_symbols_by_address(Symbol** a, Symbol** b);\n+  static int compare_klass_by_name(Klass** a, Klass** b);\n+\n+  void make_shallow_copies(DumpRegion *dump_region, const SourceObjList* src_objs);\n+  void make_shallow_copy(DumpRegion *dump_region, SourceObjInfo* src_info);\n+\n+  void update_special_refs();\n+  void relocate_embedded_pointers(SourceObjList* src_objs);\n+\n+  bool is_excluded(Klass* k);\n+  void clean_up_src_obj_table();\n+\n+protected:\n+  virtual void iterate_roots(MetaspaceClosure* it, bool is_relocating_pointers) = 0;\n+\n+  \/\/ Conservative estimate for number of bytes needed for:\n+  size_t _estimated_metaspaceobj_bytes;   \/\/ all archived MetaspaceObj's.\n+  size_t _estimated_hashtable_bytes;     \/\/ symbol table and dictionaries\n+\n+  static const int _total_dump_regions = 2;\n+\n+  size_t estimate_archive_size();\n+\n+  void start_dump_space(DumpRegion* next);\n+  void verify_estimate_size(size_t estimate, const char* which);\n+\n+public:\n+  address reserve_buffer();\n+\n+  address buffer_bottom()                    const { return _buffer_bottom;                       }\n+  address buffer_top()                       const { return (address)current_dump_space()->top(); }\n+  address requested_static_archive_bottom()  const { return  _requested_static_archive_bottom;    }\n+  address mapped_static_archive_bottom()     const { return  _mapped_static_archive_bottom;       }\n+  intx buffer_to_requested_delta()           const { return _buffer_to_requested_delta;           }\n+\n+  bool is_in_buffer_space(address p) const {\n+    return (buffer_bottom() <= p && p < buffer_top());\n+  }\n+\n+  template <typename T> bool is_in_requested_static_archive(T p) const {\n+    return _requested_static_archive_bottom <= (address)p && (address)p < _requested_static_archive_top;\n+  }\n+\n+  template <typename T> bool is_in_mapped_static_archive(T p) const {\n+    return _mapped_static_archive_bottom <= (address)p && (address)p < _mapped_static_archive_top;\n+  }\n+\n+  template <typename T> bool is_in_buffer_space(T obj) const {\n+    return is_in_buffer_space(address(obj));\n+  }\n+\n+  template <typename T> T to_requested(T obj) const {\n+    assert(is_in_buffer_space(obj), \"must be\");\n+    return (T)(address(obj) + _buffer_to_requested_delta);\n+  }\n+\n+  static intx get_buffer_to_requested_delta() {\n+    return current()->buffer_to_requested_delta();\n+  }\n+\n+public:\n+  static const uintx MAX_SHARED_DELTA = 0x7FFFFFFF;\n+\n+  \/\/ The address p points to an object inside the output buffer. When the archive is mapped\n+  \/\/ at the requested address, what's the offset of this object from _requested_static_archive_bottom?\n+  uintx buffer_to_offset(address p) const;\n+\n+  \/\/ Same as buffer_to_offset, except that the address p points to either (a) an object\n+  \/\/ inside the output buffer, or (b), an object in the currently mapped static archive.\n+  uintx any_to_offset(address p) const;\n+\n+  template <typename T>\n+  u4 buffer_to_offset_u4(T p) const {\n+    uintx offset = buffer_to_offset((address)p);\n+    guarantee(offset <= MAX_SHARED_DELTA, \"must be 32-bit offset\");\n+    return (u4)offset;\n+  }\n+\n+  template <typename T>\n+  u4 any_to_offset_u4(T p) const {\n+    uintx offset = any_to_offset((address)p);\n+    guarantee(offset <= MAX_SHARED_DELTA, \"must be 32-bit offset\");\n+    return (u4)offset;\n+  }\n+\n+  static void assert_is_vm_thread() PRODUCT_RETURN;\n+\n+public:\n+  ArchiveBuilder();\n+  ~ArchiveBuilder();\n+\n+  void gather_klasses_and_symbols();\n+  void gather_source_objs();\n+  bool gather_klass_and_symbol(MetaspaceClosure::Ref* ref, bool read_only);\n+  bool gather_one_source_obj(MetaspaceClosure::Ref* enclosing_ref, MetaspaceClosure::Ref* ref, bool read_only);\n+  void add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset, size_t src_obj_size_in_bytes) {\n+    _special_refs->append(SpecialRefInfo(type, src_obj, field_offset, src_obj_size_in_bytes));\n+  }\n+  void remember_embedded_pointer_in_copied_obj(MetaspaceClosure::Ref* enclosing_ref, MetaspaceClosure::Ref* ref);\n+\n+  DumpRegion* rw_region() { return &_rw_region; }\n+  DumpRegion* ro_region() { return &_ro_region; }\n+\n+  static char* rw_region_alloc(size_t num_bytes) {\n+    return current()->rw_region()->allocate(num_bytes);\n+  }\n+  static char* ro_region_alloc(size_t num_bytes) {\n+    return current()->ro_region()->allocate(num_bytes);\n+  }\n+\n+  template <typename T>\n+  static Array<T>* new_ro_array(int length) {\n+    size_t byte_size = Array<T>::byte_sizeof(length, sizeof(T));\n+    Array<T>* array = (Array<T>*)ro_region_alloc(byte_size);\n+    array->initialize(length);\n+    return array;\n+  }\n+\n+  template <typename T>\n+  static Array<T>* new_rw_array(int length) {\n+    size_t byte_size = Array<T>::byte_sizeof(length, sizeof(T));\n+    Array<T>* array = (Array<T>*)rw_region_alloc(byte_size);\n+    array->initialize(length);\n+    return array;\n+  }\n+\n+  template <typename T>\n+  static size_t ro_array_bytesize(int length) {\n+    size_t byte_size = Array<T>::byte_sizeof(length, sizeof(T));\n+    return align_up(byte_size, SharedSpaceObjectAlignment);\n+  }\n+\n+  void dump_rw_metadata();\n+  void dump_ro_metadata();\n+  void relocate_metaspaceobj_embedded_pointers();\n+  void relocate_roots();\n+  void relocate_vm_classes();\n+  void make_klasses_shareable();\n+  void relocate_to_requested();\n+  void write_archive(FileMapInfo* mapinfo,\n+                     GrowableArray<MemRegion>* closed_heap_regions,\n+                     GrowableArray<MemRegion>* open_heap_regions,\n+                     GrowableArray<ArchiveHeapOopmapInfo>* closed_heap_oopmaps,\n+                     GrowableArray<ArchiveHeapOopmapInfo>* open_heap_oopmaps);\n+  void write_region(FileMapInfo* mapinfo, int region_idx, DumpRegion* dump_region,\n+                    bool read_only,  bool allow_exec);\n+\n+  address get_dumped_addr(address src_obj) const;\n+\n+  \/\/ All klasses and symbols that will be copied into the archive\n+  GrowableArray<Klass*>*  klasses() const { return _klasses; }\n+  GrowableArray<Symbol*>* symbols() const { return _symbols; }\n+\n+  static bool is_active() {\n+    return (_current != NULL);\n+  }\n+\n+  static ArchiveBuilder* current() {\n+    assert_is_vm_thread();\n+    assert(_current != NULL, \"ArchiveBuilder must be active\");\n+    return _current;\n+  }\n+\n+  static DumpAllocStats* alloc_stats() {\n+    return &(current()->_alloc_stats);\n+  }\n+\n+  static CompactHashtableStats* symbol_stats() {\n+    return alloc_stats()->symbol_stats();\n+  }\n+\n+  static CompactHashtableStats* string_stats() {\n+    return alloc_stats()->string_stats();\n+  }\n+\n+  void relocate_klass_ptr(oop o);\n+\n+  static Klass* get_relocated_klass(Klass* orig_klass) {\n+    Klass* klass = (Klass*)current()->get_dumped_addr((address)orig_klass);\n+    assert(klass != NULL && klass->is_klass(), \"must be\");\n+    return klass;\n+  }\n+\n+  static Symbol* get_relocated_symbol(Symbol* orig_symbol) {\n+    return (Symbol*)current()->get_dumped_addr((address)orig_symbol);\n+  }\n+\n+  void print_stats();\n+  void report_out_of_space(const char* name, size_t needed_bytes);\n+};\n+\n+#endif \/\/ SHARE_CDS_ARCHIVEBUILDER_HPP\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":440,"deletions":0,"binary":false,"changes":440,"status":"added"},{"patch":"@@ -0,0 +1,769 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"jvm.h\"\n+#include \"jimage.hpp\"\n+#include \"cds\/archiveUtils.hpp\"\n+#include \"cds\/classListParser.hpp\"\n+#include \"cds\/lambdaFormInvokers.hpp\"\n+#include \"cds\/metaspaceShared.hpp\"\n+#include \"classfile\/classLoaderExt.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/systemDictionaryShared.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"interpreter\/bytecode.hpp\"\n+#include \"interpreter\/bytecodeStream.hpp\"\n+#include \"interpreter\/linkResolver.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logTag.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/constantPool.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/javaCalls.hpp\"\n+#include \"utilities\/defaultStream.hpp\"\n+#include \"utilities\/hashtable.inline.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+volatile Thread* ClassListParser::_parsing_thread = NULL;\n+ClassListParser* ClassListParser::_instance = NULL;\n+\n+ClassListParser::ClassListParser(const char* file) {\n+  _classlist_file = file;\n+  _file = NULL;\n+  \/\/ Use os::open() because neither fopen() nor os::fopen()\n+  \/\/ can handle long path name on Windows.\n+  int fd = os::open(file, O_RDONLY, S_IREAD);\n+  if (fd != -1) {\n+    \/\/ Obtain a File* from the file descriptor so that fgets()\n+    \/\/ can be used in parse_one_line()\n+    _file = os::open(fd, \"r\");\n+  }\n+  if (_file == NULL) {\n+    char errmsg[JVM_MAXPATHLEN];\n+    os::lasterror(errmsg, JVM_MAXPATHLEN);\n+    vm_exit_during_initialization(\"Loading classlist failed\", errmsg);\n+  }\n+  _line_no = 0;\n+  _interfaces = new (ResourceObj::C_HEAP, mtClass) GrowableArray<int>(10, mtClass);\n+  _indy_items = new (ResourceObj::C_HEAP, mtClass) GrowableArray<const char*>(9, mtClass);\n+\n+  \/\/ _instance should only be accessed by the thread that created _instance.\n+  assert(_instance == NULL, \"must be singleton\");\n+  _instance = this;\n+  Atomic::store(&_parsing_thread, Thread::current());\n+}\n+\n+bool ClassListParser::is_parsing_thread() {\n+  return Atomic::load(&_parsing_thread) == Thread::current();\n+}\n+\n+ClassListParser::~ClassListParser() {\n+  if (_file) {\n+    fclose(_file);\n+  }\n+  Atomic::store(&_parsing_thread, (Thread*)NULL);\n+  _instance = NULL;\n+}\n+\n+int ClassListParser::parse(TRAPS) {\n+  int class_count = 0;\n+\n+  while (parse_one_line()) {\n+    if (lambda_form_line()) {\n+      \/\/ The current line is \"@lambda-form-invoker ...\". It has been recorded in LambdaFormInvokers,\n+      \/\/ and will be processed later.\n+      continue;\n+    }\n+\n+    TempNewSymbol class_name_symbol = SymbolTable::new_symbol(_class_name);\n+    if (_indy_items->length() > 0) {\n+      \/\/ The current line is \"@lambda-proxy class_name\". Load the proxy class.\n+      resolve_indy(THREAD, class_name_symbol);\n+      class_count++;\n+      continue;\n+    }\n+\n+    Klass* klass = load_current_class(class_name_symbol, THREAD);\n+    if (HAS_PENDING_EXCEPTION) {\n+      if (PENDING_EXCEPTION->is_a(vmClasses::OutOfMemoryError_klass())) {\n+        \/\/ If we have run out of memory, don't try to load the rest of the classes in\n+        \/\/ the classlist. Throw an exception, which will terminate the dumping process.\n+        return 0; \/\/ THROW\n+      }\n+\n+      \/\/ We might have an invalid class name or an bad class. Warn about it\n+      \/\/ and keep going to the next line.\n+      CLEAR_PENDING_EXCEPTION;\n+      log_warning(cds)(\"Preload Warning: Cannot find %s\", _class_name);\n+      continue;\n+    }\n+\n+    assert(klass != NULL, \"sanity\");\n+    if (log_is_enabled(Trace, cds)) {\n+      ResourceMark rm(THREAD);\n+      log_trace(cds)(\"Shared spaces preloaded: %s\", klass->external_name());\n+    }\n+\n+    if (klass->is_instance_klass()) {\n+      InstanceKlass* ik = InstanceKlass::cast(klass);\n+\n+      \/\/ Link the class to cause the bytecodes to be rewritten and the\n+      \/\/ cpcache to be created. The linking is done as soon as classes\n+      \/\/ are loaded in order that the related data structures (klass and\n+      \/\/ cpCache) are located together.\n+      MetaspaceShared::try_link_class(THREAD, ik);\n+    }\n+\n+    class_count++;\n+  }\n+\n+  return class_count;\n+}\n+\n+bool ClassListParser::parse_one_line() {\n+  for (;;) {\n+    if (fgets(_line, sizeof(_line), _file) == NULL) {\n+      return false;\n+    }\n+    ++ _line_no;\n+    _line_len = (int)strlen(_line);\n+    if (_line_len > _max_allowed_line_len) {\n+      error(\"input line too long (must be no longer than %d chars)\", _max_allowed_line_len);\n+    }\n+    if (*_line == '#') { \/\/ comment\n+      continue;\n+    }\n+\n+    {\n+      int len = (int)strlen(_line);\n+      int i;\n+      \/\/ Replace \\t\\r\\n\\f with ' '\n+      for (i=0; i<len; i++) {\n+        if (_line[i] == '\\t' || _line[i] == '\\r' || _line[i] == '\\n' || _line[i] == '\\f') {\n+          _line[i] = ' ';\n+        }\n+      }\n+\n+      \/\/ Remove trailing newline\/space\n+      while (len > 0) {\n+        if (_line[len-1] == ' ') {\n+          _line[len-1] = '\\0';\n+          len --;\n+        } else {\n+          break;\n+        }\n+      }\n+      _line_len = len;\n+    }\n+\n+    \/\/ valid line\n+    break;\n+  }\n+\n+  _class_name = _line;\n+  _id = _unspecified;\n+  _super = _unspecified;\n+  _interfaces->clear();\n+  _source = NULL;\n+  _interfaces_specified = false;\n+  _indy_items->clear();\n+  _lambda_form_line = false;\n+\n+  if (_line[0] == '@') {\n+    return parse_at_tags();\n+  }\n+\n+  if ((_token = strchr(_line, ' ')) == NULL) {\n+    \/\/ No optional arguments are specified.\n+    return true;\n+  }\n+\n+  \/\/ Mark the end of the name, and go to the next input char\n+  *_token++ = '\\0';\n+\n+  while (*_token) {\n+    skip_whitespaces();\n+\n+    if (parse_uint_option(\"id:\", &_id)) {\n+      continue;\n+    } else if (parse_uint_option(\"super:\", &_super)) {\n+      check_already_loaded(\"Super class\", _super);\n+      continue;\n+    } else if (skip_token(\"interfaces:\")) {\n+      int i;\n+      while (try_parse_uint(&i)) {\n+        check_already_loaded(\"Interface\", i);\n+        _interfaces->append(i);\n+      }\n+    } else if (skip_token(\"source:\")) {\n+      skip_whitespaces();\n+      _source = _token;\n+      char* s = strchr(_token, ' ');\n+      if (s == NULL) {\n+        break; \/\/ end of input line\n+      } else {\n+        *s = '\\0'; \/\/ mark the end of _source\n+        _token = s+1;\n+      }\n+    } else {\n+      error(\"Unknown input\");\n+    }\n+  }\n+\n+  \/\/ if src is specified\n+  \/\/     id super interfaces must all be specified\n+  \/\/     loader may be specified\n+  \/\/ else\n+  \/\/     # the class is loaded from classpath\n+  \/\/     id may be specified\n+  \/\/     super, interfaces, loader must not be specified\n+  return true;\n+}\n+\n+void ClassListParser::split_tokens_by_whitespace(int offset) {\n+  int start = offset;\n+  int end;\n+  bool done = false;\n+  while (!done) {\n+    while (_line[start] == ' ' || _line[start] == '\\t') start++;\n+    end = start;\n+    while (_line[end] && _line[end] != ' ' && _line[end] != '\\t') end++;\n+    if (_line[end] == '\\0') {\n+      done = true;\n+    } else {\n+      _line[end] = '\\0';\n+    }\n+    _indy_items->append(_line + start);\n+    start = ++end;\n+  }\n+}\n+\n+int ClassListParser::split_at_tag_from_line() {\n+  _token = _line;\n+  char* ptr;\n+  if ((ptr = strchr(_line, ' ')) == NULL) {\n+    error(\"Too few items following the @ tag \\\"%s\\\" line #%d\", _line, _line_no);\n+    return 0;\n+  }\n+  *ptr++ = '\\0';\n+  while (*ptr == ' ' || *ptr == '\\t') ptr++;\n+  return (int)(ptr - _line);\n+}\n+\n+bool ClassListParser::parse_at_tags() {\n+  assert(_line[0] == '@', \"must be\");\n+  int offset;\n+  if ((offset = split_at_tag_from_line()) == 0) {\n+    return false;\n+  }\n+\n+  if (strcmp(_token, LAMBDA_PROXY_TAG) == 0) {\n+    split_tokens_by_whitespace(offset);\n+    if (_indy_items->length() < 2) {\n+      error(\"Line with @ tag has too few items \\\"%s\\\" line #%d\", _token, _line_no);\n+      return false;\n+    }\n+    \/\/ set the class name\n+    _class_name = _indy_items->at(0);\n+    return true;\n+  } else if (strcmp(_token, LAMBDA_FORM_TAG) == 0) {\n+    LambdaFormInvokers::append(os::strdup((const char*)(_line + offset), mtInternal));\n+    _lambda_form_line = true;\n+    return true;\n+  } else {\n+    error(\"Invalid @ tag at the beginning of line \\\"%s\\\" line #%d\", _token, _line_no);\n+    return false;\n+  }\n+}\n+\n+void ClassListParser::skip_whitespaces() {\n+  while (*_token == ' ' || *_token == '\\t') {\n+    _token ++;\n+  }\n+}\n+\n+void ClassListParser::skip_non_whitespaces() {\n+  while (*_token && *_token != ' ' && *_token != '\\t') {\n+    _token ++;\n+  }\n+}\n+\n+void ClassListParser::parse_int(int* value) {\n+  skip_whitespaces();\n+  if (sscanf(_token, \"%i\", value) == 1) {\n+    skip_non_whitespaces();\n+  } else {\n+    error(\"Error: expected integer\");\n+  }\n+}\n+\n+void ClassListParser::parse_uint(int* value) {\n+  parse_int(value);\n+  if (*value < 0) {\n+    error(\"Error: negative integers not allowed (%d)\", *value);\n+  }\n+}\n+\n+bool ClassListParser::try_parse_uint(int* value) {\n+  skip_whitespaces();\n+  if (sscanf(_token, \"%i\", value) == 1) {\n+    skip_non_whitespaces();\n+    return true;\n+  }\n+  return false;\n+}\n+\n+bool ClassListParser::skip_token(const char* option_name) {\n+  size_t len = strlen(option_name);\n+  if (strncmp(_token, option_name, len) == 0) {\n+    _token += len;\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+bool ClassListParser::parse_int_option(const char* option_name, int* value) {\n+  if (skip_token(option_name)) {\n+    if (*value != _unspecified) {\n+      error(\"%s specified twice\", option_name);\n+    } else {\n+      parse_int(value);\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ClassListParser::parse_uint_option(const char* option_name, int* value) {\n+  if (skip_token(option_name)) {\n+    if (*value != _unspecified) {\n+      error(\"%s specified twice\", option_name);\n+    } else {\n+      parse_uint(value);\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+void ClassListParser::print_specified_interfaces() {\n+  const int n = _interfaces->length();\n+  jio_fprintf(defaultStream::error_stream(), \"Currently specified interfaces[%d] = {\\n\", n);\n+  for (int i=0; i<n; i++) {\n+    InstanceKlass* k = lookup_class_by_id(_interfaces->at(i));\n+    jio_fprintf(defaultStream::error_stream(), \"  %4d = %s\\n\", _interfaces->at(i), k->name()->as_klass_external_name());\n+  }\n+  jio_fprintf(defaultStream::error_stream(), \"}\\n\");\n+}\n+\n+void ClassListParser::print_actual_interfaces(InstanceKlass* ik) {\n+  int n = ik->local_interfaces()->length();\n+  jio_fprintf(defaultStream::error_stream(), \"Actual interfaces[%d] = {\\n\", n);\n+  for (int i = 0; i < n; i++) {\n+    InstanceKlass* e = ik->local_interfaces()->at(i);\n+    jio_fprintf(defaultStream::error_stream(), \"  %s\\n\", e->name()->as_klass_external_name());\n+  }\n+  jio_fprintf(defaultStream::error_stream(), \"}\\n\");\n+}\n+\n+void ClassListParser::error(const char* msg, ...) {\n+  va_list ap;\n+  va_start(ap, msg);\n+  int error_index = _token - _line;\n+  if (error_index >= _line_len) {\n+    error_index = _line_len - 1;\n+  }\n+  if (error_index < 0) {\n+    error_index = 0;\n+  }\n+\n+  jio_fprintf(defaultStream::error_stream(),\n+              \"An error has occurred while processing class list file %s %d:%d.\\n\",\n+              _classlist_file, _line_no, (error_index + 1));\n+  jio_vfprintf(defaultStream::error_stream(), msg, ap);\n+\n+  if (_line_len <= 0) {\n+    jio_fprintf(defaultStream::error_stream(), \"\\n\");\n+  } else {\n+    jio_fprintf(defaultStream::error_stream(), \":\\n\");\n+    for (int i=0; i<_line_len; i++) {\n+      char c = _line[i];\n+      if (c == '\\0') {\n+        jio_fprintf(defaultStream::error_stream(), \"%s\", \" \");\n+      } else {\n+        jio_fprintf(defaultStream::error_stream(), \"%c\", c);\n+      }\n+    }\n+    jio_fprintf(defaultStream::error_stream(), \"\\n\");\n+    for (int i=0; i<error_index; i++) {\n+      jio_fprintf(defaultStream::error_stream(), \"%s\", \" \");\n+    }\n+    jio_fprintf(defaultStream::error_stream(), \"^\\n\");\n+  }\n+\n+  vm_exit_during_initialization(\"class list format error.\", NULL);\n+  va_end(ap);\n+}\n+\n+\/\/ This function is used for loading classes for customized class loaders\n+\/\/ during archive dumping.\n+InstanceKlass* ClassListParser::load_class_from_source(Symbol* class_name, TRAPS) {\n+#if !(defined(_LP64) && (defined(LINUX) || defined(__APPLE__)))\n+  \/\/ The only supported platforms are: (1) Linux\/64-bit and (2) Solaris\/64-bit and\n+  \/\/ (3) MacOSX\/64-bit\n+  \/\/ This #if condition should be in sync with the areCustomLoadersSupportedForCDS\n+  \/\/ method in test\/lib\/jdk\/test\/lib\/Platform.java.\n+  error(\"AppCDS custom class loaders not supported on this platform\");\n+#endif\n+\n+  if (!is_super_specified()) {\n+    error(\"If source location is specified, super class must be also specified\");\n+  }\n+  if (!is_id_specified()) {\n+    error(\"If source location is specified, id must be also specified\");\n+  }\n+  if (strncmp(_class_name, \"java\/\", 5) == 0) {\n+    log_info(cds)(\"Prohibited package for non-bootstrap classes: %s.class from %s\",\n+          _class_name, _source);\n+    THROW_NULL(vmSymbols::java_lang_ClassNotFoundException());\n+  }\n+\n+  InstanceKlass* k = ClassLoaderExt::load_class(class_name, _source, CHECK_NULL);\n+  if (k->local_interfaces()->length() != _interfaces->length()) {\n+    print_specified_interfaces();\n+    print_actual_interfaces(k);\n+    error(\"The number of interfaces (%d) specified in class list does not match the class file (%d)\",\n+          _interfaces->length(), k->local_interfaces()->length());\n+  }\n+\n+  if (k != NULL) {\n+    int actual_num_interfaces = k->local_interfaces()->length();\n+    int specified_num_interfaces = _interfaces->length();\n+    int expected_num_interfaces, i;\n+\n+    bool identity_object_implemented = false;\n+    bool identity_object_specified = false;\n+    bool primitive_object_implemented = false;\n+    bool primitive_object_specified = false;\n+    for (i = 0; i < actual_num_interfaces; i++) {\n+      if (k->local_interfaces()->at(i) == vmClasses::IdentityObject_klass()) {\n+        identity_object_implemented = true;\n+        break;\n+      }\n+      if (k->local_interfaces()->at(i) == vmClasses::PrimitiveObject_klass()) {\n+        primitive_object_implemented = true;\n+        break;\n+      }\n+    }\n+    for (i = 0; i < specified_num_interfaces; i++) {\n+      if (lookup_class_by_id(_interfaces->at(i)) == vmClasses::IdentityObject_klass()) {\n+        identity_object_specified = true;\n+        break;\n+      }\n+      if (lookup_class_by_id(_interfaces->at(i)) == vmClasses::PrimitiveObject_klass()) {\n+        primitive_object_specified = true;\n+        break;\n+      }\n+    }\n+\n+    expected_num_interfaces = actual_num_interfaces;\n+    if ( (identity_object_implemented  && !identity_object_specified) ||\n+         (primitive_object_implemented && !primitive_object_specified) ){\n+      \/\/ Backwards compatibility -- older classlists do not know about\n+      \/\/ java.lang.IdentityObject or java.lang.PrimitiveObject\n+      expected_num_interfaces--;\n+    }\n+    if (specified_num_interfaces != expected_num_interfaces) {\n+      print_specified_interfaces();\n+      print_actual_interfaces(k);\n+      error(\"The number of interfaces (%d) specified in class list does not match the class file (%d)\",\n+            specified_num_interfaces, expected_num_interfaces);\n+    }\n+\n+    bool added = SystemDictionaryShared::add_unregistered_class(THREAD, k);\n+    if (!added) {\n+      \/\/ We allow only a single unregistered class for each unique name.\n+      error(\"Duplicated class %s\", _class_name);\n+    }\n+\n+    \/\/ This tells JVM_FindLoadedClass to not find this class.\n+    k->set_shared_classpath_index(UNREGISTERED_INDEX);\n+    k->clear_shared_class_loader_type();\n+  }\n+\n+  \/\/ This tells JVM_FindLoadedClass to not find this class.\n+  k->set_shared_classpath_index(UNREGISTERED_INDEX);\n+  k->clear_shared_class_loader_type();\n+\n+  return k;\n+}\n+\n+void ClassListParser::populate_cds_indy_info(const constantPoolHandle &pool, int cp_index, CDSIndyInfo* cii, TRAPS) {\n+  \/\/ Caller needs to allocate ResourceMark.\n+  int type_index = pool->bootstrap_name_and_type_ref_index_at(cp_index);\n+  int name_index = pool->name_ref_index_at(type_index);\n+  cii->add_item(pool->symbol_at(name_index)->as_C_string());\n+  int sig_index = pool->signature_ref_index_at(type_index);\n+  cii->add_item(pool->symbol_at(sig_index)->as_C_string());\n+  int argc = pool->bootstrap_argument_count_at(cp_index);\n+  if (argc > 0) {\n+    for (int arg_i = 0; arg_i < argc; arg_i++) {\n+      int arg = pool->bootstrap_argument_index_at(cp_index, arg_i);\n+      jbyte tag = pool->tag_at(arg).value();\n+      if (tag == JVM_CONSTANT_MethodType) {\n+        cii->add_item(pool->method_type_signature_at(arg)->as_C_string());\n+      } else if (tag == JVM_CONSTANT_MethodHandle) {\n+        cii->add_ref_kind(pool->method_handle_ref_kind_at(arg));\n+        int callee_index = pool->method_handle_klass_index_at(arg);\n+        Klass* callee = pool->klass_at(callee_index, CHECK);\n+        cii->add_item(callee->name()->as_C_string());\n+        cii->add_item(pool->method_handle_name_ref_at(arg)->as_C_string());\n+        cii->add_item(pool->method_handle_signature_ref_at(arg)->as_C_string());\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n+  }\n+}\n+\n+bool ClassListParser::is_matching_cp_entry(constantPoolHandle &pool, int cp_index, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  CDSIndyInfo cii;\n+  populate_cds_indy_info(pool, cp_index, &cii, CHECK_0);\n+  GrowableArray<const char*>* items = cii.items();\n+  int indy_info_offset = 1;\n+  if (_indy_items->length() - indy_info_offset != items->length()) {\n+    return false;\n+  }\n+  for (int i = 0; i < items->length(); i++) {\n+    if (strcmp(_indy_items->at(i + indy_info_offset), items->at(i)) != 0) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+void ClassListParser::resolve_indy(Thread* current, Symbol* class_name_symbol) {\n+  ExceptionMark em(current);\n+  Thread* THREAD = current; \/\/ For exception macros.\n+  ClassListParser::resolve_indy_impl(class_name_symbol, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    ResourceMark rm(current);\n+    char* ex_msg = (char*)\"\";\n+    oop message = java_lang_Throwable::message(PENDING_EXCEPTION);\n+    if (message != NULL) {\n+      ex_msg = java_lang_String::as_utf8_string(message);\n+    }\n+    log_warning(cds)(\"resolve_indy for class %s has encountered exception: %s %s\",\n+                     class_name_symbol->as_C_string(),\n+                     PENDING_EXCEPTION->klass()->external_name(),\n+                     ex_msg);\n+    CLEAR_PENDING_EXCEPTION;\n+  }\n+}\n+\n+void ClassListParser::resolve_indy_impl(Symbol* class_name_symbol, TRAPS) {\n+  Handle class_loader(THREAD, SystemDictionary::java_system_loader());\n+  Handle protection_domain;\n+  Klass* klass = SystemDictionary::resolve_or_fail(class_name_symbol, class_loader, protection_domain, true, CHECK);\n+  if (klass->is_instance_klass()) {\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    MetaspaceShared::try_link_class(THREAD, ik);\n+    if (!ik->is_linked()) {\n+      \/\/ Verification of ik has failed\n+      return;\n+    }\n+\n+    ConstantPool* cp = ik->constants();\n+    ConstantPoolCache* cpcache = cp->cache();\n+    bool found = false;\n+    for (int cpcindex = 0; cpcindex < cpcache->length(); cpcindex ++) {\n+      int indy_index = ConstantPool::encode_invokedynamic_index(cpcindex);\n+      ConstantPoolCacheEntry* cpce = cpcache->entry_at(cpcindex);\n+      int pool_index = cpce->constant_pool_index();\n+      constantPoolHandle pool(THREAD, cp);\n+      if (pool->tag_at(pool_index).is_invoke_dynamic()) {\n+        BootstrapInfo bootstrap_specifier(pool, pool_index, indy_index);\n+        Handle bsm = bootstrap_specifier.resolve_bsm(CHECK);\n+        if (!SystemDictionaryShared::is_supported_invokedynamic(&bootstrap_specifier)) {\n+          log_debug(cds, lambda)(\"is_supported_invokedynamic check failed for cp_index %d\", pool_index);\n+          continue;\n+        }\n+        bool matched = is_matching_cp_entry(pool, pool_index, CHECK);\n+        if (matched) {\n+          found = true;\n+          CallInfo info;\n+          bool is_done = bootstrap_specifier.resolve_previously_linked_invokedynamic(info, CHECK);\n+          if (!is_done) {\n+            \/\/ resolve it\n+            Handle recv;\n+            LinkResolver::resolve_invoke(info, recv, pool, indy_index, Bytecodes::_invokedynamic, CHECK);\n+            break;\n+          }\n+          cpce->set_dynamic_call(pool, info);\n+        }\n+      }\n+    }\n+    if (!found) {\n+      ResourceMark rm(THREAD);\n+      log_warning(cds)(\"No invoke dynamic constant pool entry can be found for class %s. The classlist is probably out-of-date.\",\n+                     class_name_symbol->as_C_string());\n+    }\n+  }\n+}\n+\n+Klass* ClassListParser::load_current_class(Symbol* class_name_symbol, TRAPS) {\n+  Klass* klass;\n+  if (!is_loading_from_source()) {\n+    \/\/ Load classes for the boot\/platform\/app loaders only.\n+    if (is_super_specified()) {\n+      error(\"If source location is not specified, super class must not be specified\");\n+    }\n+    if (are_interfaces_specified()) {\n+      error(\"If source location is not specified, interface(s) must not be specified\");\n+    }\n+\n+    if (Signature::is_array(class_name_symbol)) {\n+      \/\/ array classes are not supported in class list.\n+      THROW_NULL(vmSymbols::java_lang_ClassNotFoundException());\n+    }\n+\n+    JavaValue result(T_OBJECT);\n+    \/\/ Call java_system_loader().loadClass() directly, which will\n+    \/\/ delegate to the correct loader (boot, platform or app) depending on\n+    \/\/ the package name.\n+\n+    Handle s = java_lang_String::create_from_symbol(class_name_symbol, CHECK_NULL);\n+    \/\/ ClassLoader.loadClass() wants external class name format, i.e., convert '\/' chars to '.'\n+    Handle ext_class_name = java_lang_String::externalize_classname(s, CHECK_NULL);\n+    Handle loader = Handle(THREAD, SystemDictionary::java_system_loader());\n+\n+    JavaCalls::call_virtual(&result,\n+                            loader, \/\/SystemDictionary::java_system_loader(),\n+                            vmClasses::ClassLoader_klass(),\n+                            vmSymbols::loadClass_name(),\n+                            vmSymbols::string_class_signature(),\n+                            ext_class_name,\n+                            CHECK_NULL);\n+\n+    assert(result.get_type() == T_OBJECT, \"just checking\");\n+    oop obj = result.get_oop();\n+    assert(obj != NULL, \"jdk.internal.loader.BuiltinClassLoader::loadClass never returns null\");\n+    klass = java_lang_Class::as_Klass(obj);\n+  } else {\n+    \/\/ If \"source:\" tag is specified, all super class and super interfaces must be specified in the\n+    \/\/ class list file.\n+    klass = load_class_from_source(class_name_symbol, CHECK_NULL);\n+  }\n+\n+  assert(klass != NULL, \"exception should have been thrown\");\n+  assert(klass->is_instance_klass(), \"array classes should have been filtered out\");\n+\n+  if (is_id_specified()) {\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    int id = this->id();\n+    SystemDictionaryShared::update_shared_entry(ik, id);\n+    InstanceKlass** old_ptr = table()->lookup(id);\n+    if (old_ptr != NULL) {\n+      error(\"Duplicated ID %d for class %s\", id, _class_name);\n+    }\n+    table()->add(id, ik);\n+  }\n+\n+  return klass;\n+}\n+\n+bool ClassListParser::is_loading_from_source() {\n+  return (_source != NULL);\n+}\n+\n+InstanceKlass* ClassListParser::lookup_class_by_id(int id) {\n+  InstanceKlass** klass_ptr = table()->lookup(id);\n+  if (klass_ptr == NULL) {\n+    error(\"Class ID %d has not been defined\", id);\n+  }\n+  assert(*klass_ptr != NULL, \"must be\");\n+  return *klass_ptr;\n+}\n+\n+\n+InstanceKlass* ClassListParser::lookup_super_for_current_class(Symbol* super_name) {\n+  if (!is_loading_from_source()) {\n+    return NULL;\n+  }\n+\n+  InstanceKlass* k = lookup_class_by_id(super());\n+  if (super_name != k->name()) {\n+    error(\"The specified super class %s (id %d) does not match actual super class %s\",\n+          k->name()->as_klass_external_name(), super(),\n+          super_name->as_klass_external_name());\n+  }\n+  return k;\n+}\n+\n+InstanceKlass* ClassListParser::lookup_interface_for_current_class(Symbol* interface_name) {\n+  if (!is_loading_from_source()) {\n+    return NULL;\n+  }\n+\n+  if (interface_name == vmSymbols::java_lang_IdentityObject()) {\n+    \/\/ Backwards compatibility -- older classlists do not know about\n+    \/\/ java.lang.IdentityObject.\n+    return vmClasses::IdentityObject_klass();\n+  }\n+  if (interface_name == vmSymbols::java_lang_PrimitiveObject()) {\n+    \/\/ Backwards compatibility -- older classlists do not know about\n+    \/\/ java.lang.PrimitiveObject.\n+    return vmClasses::PrimitiveObject_klass();\n+  }\n+\n+  const int n = _interfaces->length();\n+  if (n == 0) {\n+    error(\"Class %s implements the interface %s, but no interface has been specified in the input line\",\n+          _class_name, interface_name->as_klass_external_name());\n+    ShouldNotReachHere();\n+  }\n+\n+  int i;\n+  for (i=0; i<n; i++) {\n+    InstanceKlass* k = lookup_class_by_id(_interfaces->at(i));\n+    if (interface_name == k->name()) {\n+      return k;\n+    }\n+  }\n+\n+  \/\/ interface_name is not specified by the \"interfaces:\" keyword.\n+  print_specified_interfaces();\n+  error(\"The interface %s implemented by class %s does not match any of the specified interface IDs\",\n+        interface_name->as_klass_external_name(), _class_name);\n+  ShouldNotReachHere();\n+  return NULL;\n+}\n","filename":"src\/hotspot\/share\/cds\/classListParser.cpp","additions":769,"deletions":0,"binary":false,"changes":769,"status":"added"},{"patch":"@@ -0,0 +1,296 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"cds\/archiveUtils.hpp\"\n+#include \"cds\/archiveBuilder.hpp\"\n+#include \"cds\/cppVtables.hpp\"\n+#include \"cds\/metaspaceShared.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n+#include \"oops\/instanceClassLoaderKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceRefKlass.hpp\"\n+#include \"oops\/methodData.hpp\"\n+#include \"oops\/objArrayKlass.hpp\"\n+#include \"oops\/typeArrayKlass.hpp\"\n+#include \"runtime\/arguments.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ Objects of the Metadata types (such as Klass and ConstantPool) have C++ vtables.\n+\/\/ (In GCC this is the field <Type>::_vptr, i.e., first word in the object.)\n+\/\/\n+\/\/ Addresses of the vtables and the methods may be different across JVM runs,\n+\/\/ if libjvm.so is dynamically loaded at a different base address.\n+\/\/\n+\/\/ To ensure that the Metadata objects in the CDS archive always have the correct vtable:\n+\/\/\n+\/\/ + at dump time:  we redirect the _vptr to point to our own vtables inside\n+\/\/                  the CDS image\n+\/\/ + at run time:   we clone the actual contents of the vtables from libjvm.so\n+\/\/                  into our own tables.\n+\n+\/\/ Currently, the archive contains ONLY the following types of objects that have C++ vtables.\n+#define CPP_VTABLE_TYPES_DO(f) \\\n+  f(ConstantPool) \\\n+  f(InstanceKlass) \\\n+  f(InstanceClassLoaderKlass) \\\n+  f(InstanceMirrorKlass) \\\n+  f(InstanceRefKlass) \\\n+  f(Method) \\\n+  f(ObjArrayKlass) \\\n+  f(TypeArrayKlass) \\\n+  f(FlatArrayKlass) \\\n+  f(InlineKlass)\n+\n+class CppVtableInfo {\n+  intptr_t _vtable_size;\n+  intptr_t _cloned_vtable[1];\n+public:\n+  static int num_slots(int vtable_size) {\n+    return 1 + vtable_size; \/\/ Need to add the space occupied by _vtable_size;\n+  }\n+  int vtable_size()           { return int(uintx(_vtable_size)); }\n+  void set_vtable_size(int n) { _vtable_size = intptr_t(n); }\n+  intptr_t* cloned_vtable()   { return &_cloned_vtable[0]; }\n+  void zero()                 { memset(_cloned_vtable, 0, sizeof(intptr_t) * vtable_size()); }\n+  \/\/ Returns the address of the next CppVtableInfo that can be placed immediately after this CppVtableInfo\n+  static size_t byte_size(int vtable_size) {\n+    CppVtableInfo i;\n+    return pointer_delta(&i._cloned_vtable[vtable_size], &i, sizeof(u1));\n+  }\n+};\n+\n+static inline intptr_t* vtable_of(const Metadata* m) {\n+  return *((intptr_t**)m);\n+}\n+\n+template <class T> class CppVtableCloner {\n+  static int get_vtable_length(const char* name);\n+\n+public:\n+  \/\/ Allocate a clone of the vtable of T from the shared metaspace;\n+  \/\/ Initialize the contents of this clone.\n+  static CppVtableInfo* allocate_and_initialize(const char* name);\n+\n+  \/\/ Copy the contents of the vtable of T into info->_cloned_vtable;\n+  static void initialize(const char* name, CppVtableInfo* info);\n+\n+  static void init_orig_cpp_vtptr(int kind);\n+};\n+\n+template <class T>\n+CppVtableInfo* CppVtableCloner<T>::allocate_and_initialize(const char* name) {\n+  int n = get_vtable_length(name);\n+  CppVtableInfo* info =\n+      (CppVtableInfo*)ArchiveBuilder::current()->rw_region()->allocate(CppVtableInfo::byte_size(n));\n+  info->set_vtable_size(n);\n+  initialize(name, info);\n+  return info;\n+}\n+\n+template <class T>\n+void CppVtableCloner<T>::initialize(const char* name, CppVtableInfo* info) {\n+  T tmp; \/\/ Allocate temporary dummy metadata object to get to the original vtable.\n+  int n = info->vtable_size();\n+  intptr_t* srcvtable = vtable_of(&tmp);\n+  intptr_t* dstvtable = info->cloned_vtable();\n+\n+  \/\/ We already checked (and, if necessary, adjusted n) when the vtables were allocated, so we are\n+  \/\/ safe to do memcpy.\n+  log_debug(cds, vtables)(\"Copying %3d vtable entries for %s\", n, name);\n+  memcpy(dstvtable, srcvtable, sizeof(intptr_t) * n);\n+}\n+\n+\/\/ To determine the size of the vtable for each type, we use the following\n+\/\/ trick by declaring 2 subclasses:\n+\/\/\n+\/\/   class CppVtableTesterA: public InstanceKlass {virtual int   last_virtual_method() {return 1;}    };\n+\/\/   class CppVtableTesterB: public InstanceKlass {virtual void* last_virtual_method() {return NULL}; };\n+\/\/\n+\/\/ CppVtableTesterA and CppVtableTesterB's vtables have the following properties:\n+\/\/ - Their size (N+1) is exactly one more than the size of InstanceKlass's vtable (N)\n+\/\/ - The first N entries have are exactly the same as in InstanceKlass's vtable.\n+\/\/ - Their last entry is different.\n+\/\/\n+\/\/ So to determine the value of N, we just walk CppVtableTesterA and CppVtableTesterB's tables\n+\/\/ and find the first entry that's different.\n+\/\/\n+\/\/ This works on all C++ compilers supported by Oracle, but you may need to tweak it for more\n+\/\/ esoteric compilers.\n+\n+template <class T> class CppVtableTesterB: public T {\n+public:\n+  virtual int last_virtual_method() {return 1;}\n+};\n+\n+template <class T> class CppVtableTesterA : public T {\n+public:\n+  virtual void* last_virtual_method() {\n+    \/\/ Make this different than CppVtableTesterB::last_virtual_method so the C++\n+    \/\/ compiler\/linker won't alias the two functions.\n+    return NULL;\n+  }\n+};\n+\n+template <class T>\n+int CppVtableCloner<T>::get_vtable_length(const char* name) {\n+  CppVtableTesterA<T> a;\n+  CppVtableTesterB<T> b;\n+\n+  intptr_t* avtable = vtable_of(&a);\n+  intptr_t* bvtable = vtable_of(&b);\n+\n+  \/\/ Start at slot 1, because slot 0 may be RTTI (on Solaris\/Sparc)\n+  int vtable_len = 1;\n+  for (; ; vtable_len++) {\n+    if (avtable[vtable_len] != bvtable[vtable_len]) {\n+      break;\n+    }\n+  }\n+  log_debug(cds, vtables)(\"Found   %3d vtable entries for %s\", vtable_len, name);\n+\n+  return vtable_len;\n+}\n+\n+#define ALLOCATE_AND_INITIALIZE_VTABLE(c) \\\n+  _index[c##_Kind] = CppVtableCloner<c>::allocate_and_initialize(#c); \\\n+  ArchivePtrMarker::mark_pointer(&_index[c##_Kind]);\n+\n+#define INITIALIZE_VTABLE(c) \\\n+  CppVtableCloner<c>::initialize(#c, _index[c##_Kind]);\n+\n+#define INIT_ORIG_CPP_VTPTRS(c) \\\n+  CppVtableCloner<c>::init_orig_cpp_vtptr(c##_Kind);\n+\n+#define DECLARE_CLONED_VTABLE_KIND(c) c ## _Kind,\n+\n+enum ClonedVtableKind {\n+  \/\/ E.g., ConstantPool_Kind == 0, InstanceKlass_Kind == 1, etc.\n+  CPP_VTABLE_TYPES_DO(DECLARE_CLONED_VTABLE_KIND)\n+  _num_cloned_vtable_kinds\n+};\n+\n+\/\/ This is a map of all the original vtptrs. E.g., for\n+\/\/     ConstantPool *cp = new (...) ConstantPool(...) ; \/\/ a dynamically allocated constant pool\n+\/\/ the following holds true:\n+\/\/     _orig_cpp_vtptrs[ConstantPool_Kind] ==  ((intptr_t**)cp)[0]\n+static intptr_t* _orig_cpp_vtptrs[_num_cloned_vtable_kinds];\n+static bool _orig_cpp_vtptrs_inited = false;\n+\n+template <class T>\n+void CppVtableCloner<T>::init_orig_cpp_vtptr(int kind) {\n+  assert(kind < _num_cloned_vtable_kinds, \"sanity\");\n+  T tmp; \/\/ Allocate temporary dummy metadata object to get to the original vtable.\n+  intptr_t* srcvtable = vtable_of(&tmp);\n+  _orig_cpp_vtptrs[kind] = srcvtable;\n+}\n+\n+\/\/ This is the index of all the cloned vtables. E.g., for\n+\/\/     ConstantPool* cp = ....; \/\/ an archived constant pool\n+\/\/     InstanceKlass* ik = ....;\/\/ an archived class\n+\/\/ the following holds true:\n+\/\/     _index[ConstantPool_Kind]->cloned_vtable()  == ((intptr_t**)cp)[0]\n+\/\/     _index[InstanceKlass_Kind]->cloned_vtable() == ((intptr_t**)ik)[0]\n+CppVtableInfo** CppVtables::_index = NULL;\n+\n+char* CppVtables::dumptime_init(ArchiveBuilder* builder) {\n+  assert(DumpSharedSpaces, \"must\");\n+  size_t vtptrs_bytes = _num_cloned_vtable_kinds * sizeof(CppVtableInfo*);\n+  _index = (CppVtableInfo**)builder->rw_region()->allocate(vtptrs_bytes);\n+\n+  CPP_VTABLE_TYPES_DO(ALLOCATE_AND_INITIALIZE_VTABLE);\n+\n+  size_t cpp_tables_size = builder->rw_region()->top() - builder->rw_region()->base();\n+  builder->alloc_stats()->record_cpp_vtables((int)cpp_tables_size);\n+\n+  return (char*)_index;\n+}\n+\n+void CppVtables::serialize(SerializeClosure* soc) {\n+  soc->do_ptr((void**)&_index);\n+  if (soc->reading()) {\n+    CPP_VTABLE_TYPES_DO(INITIALIZE_VTABLE);\n+  }\n+}\n+\n+intptr_t* CppVtables::get_archived_vtable(MetaspaceObj::Type msotype, address obj) {\n+  if (!_orig_cpp_vtptrs_inited) {\n+    CPP_VTABLE_TYPES_DO(INIT_ORIG_CPP_VTPTRS);\n+    _orig_cpp_vtptrs_inited = true;\n+  }\n+\n+  Arguments::assert_is_dumping_archive();\n+  int kind = -1;\n+  switch (msotype) {\n+  case MetaspaceObj::SymbolType:\n+  case MetaspaceObj::TypeArrayU1Type:\n+  case MetaspaceObj::TypeArrayU2Type:\n+  case MetaspaceObj::TypeArrayU4Type:\n+  case MetaspaceObj::TypeArrayU8Type:\n+  case MetaspaceObj::TypeArrayOtherType:\n+  case MetaspaceObj::ConstMethodType:\n+  case MetaspaceObj::ConstantPoolCacheType:\n+  case MetaspaceObj::AnnotationsType:\n+  case MetaspaceObj::MethodCountersType:\n+  case MetaspaceObj::RecordComponentType:\n+    \/\/ These have no vtables.\n+    break;\n+  case MetaspaceObj::MethodDataType:\n+    \/\/ We don't archive MethodData <-- should have been removed in removed_unsharable_info\n+    ShouldNotReachHere();\n+    break;\n+  default:\n+    for (kind = 0; kind < _num_cloned_vtable_kinds; kind ++) {\n+      if (vtable_of((Metadata*)obj) == _orig_cpp_vtptrs[kind]) {\n+        break;\n+      }\n+    }\n+    if (kind >= _num_cloned_vtable_kinds) {\n+      fatal(\"Cannot find C++ vtable for \" INTPTR_FORMAT \" -- you probably added\"\n+            \" a new subtype of Klass or MetaData without updating CPP_VTABLE_TYPES_DO\",\n+            p2i(obj));\n+    }\n+  }\n+\n+  if (kind >= 0) {\n+    assert(kind < _num_cloned_vtable_kinds, \"must be\");\n+    return _index[kind]->cloned_vtable();\n+  } else {\n+    return NULL;\n+  }\n+}\n+\n+void CppVtables::zero_archived_vtables() {\n+  assert(DumpSharedSpaces, \"dump-time only\");\n+  for (int kind = 0; kind < _num_cloned_vtable_kinds; kind ++) {\n+    _index[kind]->zero();\n+  }\n+}\n+\n+bool CppVtables::is_valid_shared_method(const Method* m) {\n+  assert(MetaspaceShared::is_in_shared_metaspace(m), \"must be\");\n+  return vtable_of(m) == _index[Method_Kind]->cloned_vtable();\n+}\n","filename":"src\/hotspot\/share\/cds\/cppVtables.cpp","additions":296,"deletions":0,"binary":false,"changes":296,"status":"added"},{"patch":"@@ -0,0 +1,1456 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"cds\/archiveBuilder.hpp\"\n+#include \"cds\/archiveUtils.hpp\"\n+#include \"cds\/filemap.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/metaspaceShared.hpp\"\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"classfile\/classLoaderDataShared.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"classfile\/moduleEntry.hpp\"\n+#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/systemDictionaryShared.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"gc\/shared\/gcLocker.hpp\"\n+#include \"gc\/shared\/gcVMOperations.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logMessage.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/metadataFactory.hpp\"\n+#include \"memory\/metaspaceClosure.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/objArrayOop.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"prims\/jvmtiExport.hpp\"\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"runtime\/init.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/javaCalls.hpp\"\n+#include \"runtime\/safepointVerifiers.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#if INCLUDE_G1GC\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#endif\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+bool HeapShared::_closed_archive_heap_region_mapped = false;\n+bool HeapShared::_open_archive_heap_region_mapped = false;\n+bool HeapShared::_archive_heap_region_fixed = false;\n+address   HeapShared::_narrow_oop_base;\n+int       HeapShared::_narrow_oop_shift;\n+DumpedInternedStrings *HeapShared::_dumped_interned_strings = NULL;\n+\n+\/\/\n+\/\/ If you add new entries to the following tables, you should know what you're doing!\n+\/\/\n+\n+\/\/ Entry fields for shareable subgraphs archived in the closed archive heap\n+\/\/ region. Warning: Objects in the subgraphs should not have reference fields\n+\/\/ assigned at runtime.\n+static ArchivableStaticFieldInfo closed_archive_subgraph_entry_fields[] = {\n+  {\"java\/lang\/Integer$IntegerCache\",              \"archivedCache\"},\n+  {\"java\/lang\/Long$LongCache\",                    \"archivedCache\"},\n+  {\"java\/lang\/Byte$ByteCache\",                    \"archivedCache\"},\n+  {\"java\/lang\/Short$ShortCache\",                  \"archivedCache\"},\n+  {\"java\/lang\/Character$CharacterCache\",          \"archivedCache\"},\n+  {\"java\/util\/jar\/Attributes$Name\",               \"KNOWN_NAMES\"},\n+  {\"sun\/util\/locale\/BaseLocale\",                  \"constantBaseLocales\"},\n+};\n+\/\/ Entry fields for subgraphs archived in the open archive heap region.\n+static ArchivableStaticFieldInfo open_archive_subgraph_entry_fields[] = {\n+  {\"jdk\/internal\/module\/ArchivedModuleGraph\",     \"archivedModuleGraph\"},\n+  {\"java\/util\/ImmutableCollections\",              \"archivedObjects\"},\n+  {\"java\/lang\/ModuleLayer\",                       \"EMPTY_LAYER\"},\n+  {\"java\/lang\/module\/Configuration\",              \"EMPTY_CONFIGURATION\"},\n+  {\"jdk\/internal\/math\/FDBigInteger\",              \"archivedCaches\"},\n+};\n+\n+\/\/ Entry fields for subgraphs archived in the open archive heap region (full module graph).\n+static ArchivableStaticFieldInfo fmg_open_archive_subgraph_entry_fields[] = {\n+  {\"jdk\/internal\/loader\/ArchivedClassLoaders\",    \"archivedClassLoaders\"},\n+  {\"jdk\/internal\/module\/ArchivedBootLayer\",       \"archivedBootLayer\"},\n+  {\"java\/lang\/Module$ArchivedData\",               \"archivedData\"},\n+};\n+\n+const static int num_closed_archive_subgraph_entry_fields =\n+  sizeof(closed_archive_subgraph_entry_fields) \/ sizeof(ArchivableStaticFieldInfo);\n+const static int num_open_archive_subgraph_entry_fields =\n+  sizeof(open_archive_subgraph_entry_fields) \/ sizeof(ArchivableStaticFieldInfo);\n+const static int num_fmg_open_archive_subgraph_entry_fields =\n+  sizeof(fmg_open_archive_subgraph_entry_fields) \/ sizeof(ArchivableStaticFieldInfo);\n+\n+GrowableArrayCHeap<oop, mtClassShared>* HeapShared::_pending_roots = NULL;\n+narrowOop HeapShared::_roots_narrow;\n+OopHandle HeapShared::_roots;\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ Java heap object archiving support\n+\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+void HeapShared::fixup_mapped_heap_regions() {\n+  FileMapInfo *mapinfo = FileMapInfo::current_info();\n+  mapinfo->fixup_mapped_heap_regions();\n+  set_archive_heap_region_fixed();\n+  if (is_mapped()) {\n+    _roots = OopHandle(Universe::vm_global(), decode_from_archive(_roots_narrow));\n+    if (!MetaspaceShared::use_full_module_graph()) {\n+      \/\/ Need to remove all the archived java.lang.Module objects from HeapShared::roots().\n+      ClassLoaderDataShared::clear_archived_oops();\n+    }\n+  }\n+  SystemDictionaryShared::update_archived_mirror_native_pointers();\n+}\n+\n+unsigned HeapShared::oop_hash(oop const& p) {\n+  assert(!UseBiasedLocking || !p->mark().has_bias_pattern(),\n+         \"this object should never have been locked\");  \/\/ so identity_hash won't safepoin\n+  unsigned hash = (unsigned)p->identity_hash();\n+  return hash;\n+}\n+\n+static void reset_states(oop obj, TRAPS) {\n+  Handle h_obj(THREAD, obj);\n+  InstanceKlass* klass = InstanceKlass::cast(obj->klass());\n+  TempNewSymbol method_name = SymbolTable::new_symbol(\"resetArchivedStates\");\n+  Symbol* method_sig = vmSymbols::void_method_signature();\n+\n+  while (klass != NULL) {\n+    Method* method = klass->find_method(method_name, method_sig);\n+    if (method != NULL) {\n+      assert(method->is_private(), \"must be\");\n+      if (log_is_enabled(Debug, cds)) {\n+        ResourceMark rm(THREAD);\n+        log_debug(cds)(\"  calling %s\", method->name_and_sig_as_C_string());\n+      }\n+      JavaValue result(T_VOID);\n+      JavaCalls::call_special(&result, h_obj, klass,\n+                              method_name, method_sig, CHECK);\n+    }\n+    klass = klass->java_super();\n+  }\n+}\n+\n+void HeapShared::reset_archived_object_states(TRAPS) {\n+  assert(DumpSharedSpaces, \"dump-time only\");\n+  log_debug(cds)(\"Resetting platform loader\");\n+  reset_states(SystemDictionary::java_platform_loader(), CHECK);\n+  log_debug(cds)(\"Resetting system loader\");\n+  reset_states(SystemDictionary::java_system_loader(), CHECK);\n+}\n+\n+HeapShared::ArchivedObjectCache* HeapShared::_archived_object_cache = NULL;\n+oop HeapShared::find_archived_heap_object(oop obj) {\n+  assert(DumpSharedSpaces, \"dump-time only\");\n+  ArchivedObjectCache* cache = archived_object_cache();\n+  oop* p = cache->get(obj);\n+  if (p != NULL) {\n+    return *p;\n+  } else {\n+    return NULL;\n+  }\n+}\n+\n+int HeapShared::append_root(oop obj) {\n+  assert(DumpSharedSpaces, \"dump-time only\");\n+\n+  \/\/ No GC should happen since we aren't scanning _pending_roots.\n+  assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n+\n+  if (_pending_roots == NULL) {\n+    _pending_roots = new GrowableArrayCHeap<oop, mtClassShared>(500);\n+  }\n+\n+  return _pending_roots->append(obj);\n+}\n+\n+objArrayOop HeapShared::roots() {\n+  if (DumpSharedSpaces) {\n+    assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n+    if (!is_heap_object_archiving_allowed()) {\n+      return NULL;\n+    }\n+  } else {\n+    assert(UseSharedSpaces, \"must be\");\n+  }\n+\n+  objArrayOop roots = (objArrayOop)_roots.resolve();\n+  assert(roots != NULL, \"should have been initialized\");\n+  return roots;\n+}\n+\n+void HeapShared::set_roots(narrowOop roots) {\n+  assert(UseSharedSpaces, \"runtime only\");\n+  assert(open_archive_heap_region_mapped(), \"must be\");\n+  _roots_narrow = roots;\n+}\n+\n+\/\/ Returns an objArray that contains all the roots of the archived objects\n+oop HeapShared::get_root(int index, bool clear) {\n+  assert(index >= 0, \"sanity\");\n+  if (DumpSharedSpaces) {\n+    assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n+    assert(_pending_roots != NULL, \"sanity\");\n+    return _pending_roots->at(index);\n+  } else {\n+    assert(UseSharedSpaces, \"must be\");\n+    assert(!_roots.is_empty(), \"must have loaded shared heap\");\n+    oop result = roots()->obj_at(index);\n+    if (clear) {\n+      clear_root(index);\n+    }\n+    return result;\n+  }\n+}\n+\n+void HeapShared::clear_root(int index) {\n+  assert(index >= 0, \"sanity\");\n+  assert(UseSharedSpaces, \"must be\");\n+  if (open_archive_heap_region_mapped()) {\n+    if (log_is_enabled(Debug, cds, heap)) {\n+      oop old = roots()->obj_at(index);\n+      log_debug(cds, heap)(\"Clearing root %d: was \" PTR_FORMAT, index, p2i(old));\n+    }\n+    roots()->obj_at_put(index, NULL);\n+  }\n+}\n+\n+oop HeapShared::archive_heap_object(oop obj) {\n+  assert(DumpSharedSpaces, \"dump-time only\");\n+\n+  oop ao = find_archived_heap_object(obj);\n+  if (ao != NULL) {\n+    \/\/ already archived\n+    return ao;\n+  }\n+\n+  int len = obj->size();\n+  if (G1CollectedHeap::heap()->is_archive_alloc_too_large(len)) {\n+    log_debug(cds, heap)(\"Cannot archive, object (\" PTR_FORMAT \") is too large: \" SIZE_FORMAT,\n+                         p2i(obj), (size_t)obj->size());\n+    return NULL;\n+  }\n+\n+  oop archived_oop = cast_to_oop(G1CollectedHeap::heap()->archive_mem_allocate(len));\n+  if (archived_oop != NULL) {\n+    Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(obj), cast_from_oop<HeapWord*>(archived_oop), len);\n+    \/\/ Reinitialize markword to remove age\/marking\/locking\/etc.\n+    \/\/\n+    \/\/ We need to retain the identity_hash, because it may have been used by some hashtables\n+    \/\/ in the shared heap. This also has the side effect of pre-initializing the\n+    \/\/ identity_hash for all shared objects, so they are less likely to be written\n+    \/\/ into during run time, increasing the potential of memory sharing.\n+    int hash_original = obj->identity_hash();\n+    archived_oop->set_mark(markWord::prototype_for_klass(archived_oop->klass()).copy_set_hash(hash_original));\n+    assert(archived_oop->mark().is_unlocked(), \"sanity\");\n+\n+    DEBUG_ONLY(int hash_archived = archived_oop->identity_hash());\n+    assert(hash_original == hash_archived, \"Different hash codes: original %x, archived %x\", hash_original, hash_archived);\n+\n+    ArchivedObjectCache* cache = archived_object_cache();\n+    cache->put(obj, archived_oop);\n+    if (log_is_enabled(Debug, cds, heap)) {\n+      ResourceMark rm;\n+      log_debug(cds, heap)(\"Archived heap object \" PTR_FORMAT \" ==> \" PTR_FORMAT \" : %s\",\n+                           p2i(obj), p2i(archived_oop), obj->klass()->external_name());\n+    }\n+  } else {\n+    log_error(cds, heap)(\n+      \"Cannot allocate space for object \" PTR_FORMAT \" in archived heap region\",\n+      p2i(obj));\n+    vm_direct_exit(-1,\n+      err_msg(\"Out of memory. Please run with a larger Java heap, current MaxHeapSize = \"\n+              SIZE_FORMAT \"M\", MaxHeapSize\/M));\n+  }\n+  return archived_oop;\n+}\n+\n+void HeapShared::archive_klass_objects() {\n+  GrowableArray<Klass*>* klasses = ArchiveBuilder::current()->klasses();\n+  assert(klasses != NULL, \"sanity\");\n+  for (int i = 0; i < klasses->length(); i++) {\n+    Klass* k = ArchiveBuilder::get_relocated_klass(klasses->at(i));\n+\n+    \/\/ archive mirror object\n+    java_lang_Class::archive_mirror(k);\n+\n+    \/\/ archive the resolved_referenes array\n+    if (k->is_instance_klass()) {\n+      InstanceKlass* ik = InstanceKlass::cast(k);\n+      ik->constants()->archive_resolved_references();\n+    }\n+  }\n+}\n+\n+void HeapShared::run_full_gc_in_vm_thread() {\n+  if (is_heap_object_archiving_allowed()) {\n+    \/\/ Avoid fragmentation while archiving heap objects.\n+    \/\/ We do this inside a safepoint, so that no further allocation can happen after GC\n+    \/\/ has finished.\n+    if (GCLocker::is_active()) {\n+      \/\/ Just checking for safety ...\n+      \/\/ This should not happen during -Xshare:dump. If you see this, probably the Java core lib\n+      \/\/ has been modified such that JNI code is executed in some clean up threads after\n+      \/\/ we have finished class loading.\n+      log_warning(cds)(\"GC locker is held, unable to start extra compacting GC. This may produce suboptimal results.\");\n+    } else {\n+      log_info(cds)(\"Run GC ...\");\n+      Universe::heap()->collect_as_vm_thread(GCCause::_archive_time_gc);\n+      log_info(cds)(\"Run GC done\");\n+    }\n+  }\n+}\n+\n+void HeapShared::archive_java_heap_objects(GrowableArray<MemRegion>* closed,\n+                                           GrowableArray<MemRegion>* open) {\n+\n+  G1HeapVerifier::verify_ready_for_archiving();\n+\n+  {\n+    NoSafepointVerifier nsv;\n+\n+    \/\/ Cache for recording where the archived objects are copied to\n+    create_archived_object_cache();\n+\n+    log_info(cds)(\"Heap range = [\" PTR_FORMAT \" - \"  PTR_FORMAT \"]\",\n+                  p2i(CompressedOops::begin()), p2i(CompressedOops::end()));\n+    log_info(cds)(\"Dumping objects to closed archive heap region ...\");\n+    copy_closed_archive_heap_objects(closed);\n+\n+    log_info(cds)(\"Dumping objects to open archive heap region ...\");\n+    copy_open_archive_heap_objects(open);\n+\n+    destroy_archived_object_cache();\n+  }\n+\n+  G1HeapVerifier::verify_archive_regions();\n+}\n+\n+void HeapShared::copy_closed_archive_heap_objects(\n+                                    GrowableArray<MemRegion> * closed_archive) {\n+  assert(is_heap_object_archiving_allowed(), \"Cannot archive java heap objects\");\n+\n+  G1CollectedHeap::heap()->begin_archive_alloc_range();\n+\n+  \/\/ Archive interned string objects\n+  StringTable::write_to_archive(_dumped_interned_strings);\n+\n+  archive_object_subgraphs(closed_archive_subgraph_entry_fields,\n+                           num_closed_archive_subgraph_entry_fields,\n+                           true \/* is_closed_archive *\/,\n+                           false \/* is_full_module_graph *\/);\n+\n+  G1CollectedHeap::heap()->end_archive_alloc_range(closed_archive,\n+                                                   os::vm_allocation_granularity());\n+}\n+\n+void HeapShared::copy_open_archive_heap_objects(\n+                                    GrowableArray<MemRegion> * open_archive) {\n+  assert(is_heap_object_archiving_allowed(), \"Cannot archive java heap objects\");\n+\n+  G1CollectedHeap::heap()->begin_archive_alloc_range(true \/* open *\/);\n+\n+  java_lang_Class::archive_basic_type_mirrors();\n+\n+  archive_klass_objects();\n+\n+  archive_object_subgraphs(open_archive_subgraph_entry_fields,\n+                           num_open_archive_subgraph_entry_fields,\n+                           false \/* is_closed_archive *\/,\n+                           false \/* is_full_module_graph *\/);\n+  if (MetaspaceShared::use_full_module_graph()) {\n+    archive_object_subgraphs(fmg_open_archive_subgraph_entry_fields,\n+                             num_fmg_open_archive_subgraph_entry_fields,\n+                             false \/* is_closed_archive *\/,\n+                             true \/* is_full_module_graph *\/);\n+    ClassLoaderDataShared::init_archived_oops();\n+  }\n+\n+  copy_roots();\n+\n+  G1CollectedHeap::heap()->end_archive_alloc_range(open_archive,\n+                                                   os::vm_allocation_granularity());\n+}\n+\n+\/\/ Copy _pending_archive_roots into an objArray\n+void HeapShared::copy_roots() {\n+  int length = _pending_roots != NULL ? _pending_roots->length() : 0;\n+  int size = objArrayOopDesc::object_size(length);\n+  Klass* k = Universe::objectArrayKlassObj(); \/\/ already relocated to point to archived klass\n+  HeapWord* mem = G1CollectedHeap::heap()->archive_mem_allocate(size);\n+\n+  memset(mem, 0, size * BytesPerWord);\n+  {\n+    \/\/ This is copied from MemAllocator::finish\n+    if (UseBiasedLocking) {\n+      oopDesc::set_mark(mem, k->prototype_header());\n+    } else {\n+      oopDesc::set_mark(mem, markWord::prototype());\n+    }\n+    oopDesc::release_set_klass(mem, k);\n+  }\n+  {\n+    \/\/ This is copied from ObjArrayAllocator::initialize\n+    arrayOopDesc::set_length(mem, length);\n+  }\n+\n+  _roots = OopHandle(Universe::vm_global(), cast_to_oop(mem));\n+  for (int i = 0; i < length; i++) {\n+    roots()->obj_at_put(i, _pending_roots->at(i));\n+  }\n+  log_info(cds)(\"archived obj roots[%d] = %d words, klass = %p, obj = %p\", length, size, k, mem);\n+}\n+\n+void HeapShared::init_narrow_oop_decoding(address base, int shift) {\n+  _narrow_oop_base = base;\n+  _narrow_oop_shift = shift;\n+}\n+\n+\/\/\n+\/\/ Subgraph archiving support\n+\/\/\n+HeapShared::DumpTimeKlassSubGraphInfoTable* HeapShared::_dump_time_subgraph_info_table = NULL;\n+HeapShared::RunTimeKlassSubGraphInfoTable   HeapShared::_run_time_subgraph_info_table;\n+\n+\/\/ Get the subgraph_info for Klass k. A new subgraph_info is created if\n+\/\/ there is no existing one for k. The subgraph_info records the relocated\n+\/\/ Klass* of the original k.\n+KlassSubGraphInfo* HeapShared::init_subgraph_info(Klass* k, bool is_full_module_graph) {\n+  assert(DumpSharedSpaces, \"dump time only\");\n+  bool created;\n+  Klass* relocated_k = ArchiveBuilder::get_relocated_klass(k);\n+  KlassSubGraphInfo* info =\n+    _dump_time_subgraph_info_table->put_if_absent(relocated_k, KlassSubGraphInfo(relocated_k, is_full_module_graph),\n+                                                  &created);\n+  assert(created, \"must not initialize twice\");\n+  return info;\n+}\n+\n+KlassSubGraphInfo* HeapShared::get_subgraph_info(Klass* k) {\n+  assert(DumpSharedSpaces, \"dump time only\");\n+  Klass* relocated_k = ArchiveBuilder::get_relocated_klass(k);\n+  KlassSubGraphInfo* info = _dump_time_subgraph_info_table->get(relocated_k);\n+  assert(info != NULL, \"must have been initialized\");\n+  return info;\n+}\n+\n+\/\/ Add an entry field to the current KlassSubGraphInfo.\n+void KlassSubGraphInfo::add_subgraph_entry_field(\n+      int static_field_offset, oop v, bool is_closed_archive) {\n+  assert(DumpSharedSpaces, \"dump time only\");\n+  if (_subgraph_entry_fields == NULL) {\n+    _subgraph_entry_fields =\n+      new(ResourceObj::C_HEAP, mtClass) GrowableArray<int>(10, mtClass);\n+  }\n+  _subgraph_entry_fields->append(static_field_offset);\n+  _subgraph_entry_fields->append(HeapShared::append_root(v));\n+}\n+\n+\/\/ Add the Klass* for an object in the current KlassSubGraphInfo's subgraphs.\n+\/\/ Only objects of boot classes can be included in sub-graph.\n+void KlassSubGraphInfo::add_subgraph_object_klass(Klass* orig_k) {\n+  assert(DumpSharedSpaces, \"dump time only\");\n+  Klass* relocated_k = ArchiveBuilder::get_relocated_klass(orig_k);\n+\n+  if (_subgraph_object_klasses == NULL) {\n+    _subgraph_object_klasses =\n+      new(ResourceObj::C_HEAP, mtClass) GrowableArray<Klass*>(50, mtClass);\n+  }\n+\n+  assert(ArchiveBuilder::current()->is_in_buffer_space(relocated_k), \"must be a shared class\");\n+\n+  if (_k == relocated_k) {\n+    \/\/ Don't add the Klass containing the sub-graph to it's own klass\n+    \/\/ initialization list.\n+    return;\n+  }\n+\n+  if (relocated_k->is_instance_klass()) {\n+    assert(InstanceKlass::cast(relocated_k)->is_shared_boot_class(),\n+          \"must be boot class\");\n+    \/\/ vmClasses::xxx_klass() are not updated, need to check\n+    \/\/ the original Klass*\n+    if (orig_k == vmClasses::String_klass() ||\n+        orig_k == vmClasses::Object_klass()) {\n+      \/\/ Initialized early during VM initialization. No need to be added\n+      \/\/ to the sub-graph object class list.\n+      return;\n+    }\n+  } else if (relocated_k->is_objArray_klass()) {\n+    Klass* abk = ObjArrayKlass::cast(relocated_k)->bottom_klass();\n+    if (abk->is_instance_klass()) {\n+      assert(InstanceKlass::cast(abk)->is_shared_boot_class(),\n+            \"must be boot class\");\n+    }\n+    if (relocated_k == Universe::objectArrayKlassObj()) {\n+      \/\/ Initialized early during Universe::genesis. No need to be added\n+      \/\/ to the list.\n+      return;\n+    }\n+  } else {\n+    assert(relocated_k->is_typeArray_klass(), \"must be\");\n+    \/\/ Primitive type arrays are created early during Universe::genesis.\n+    return;\n+  }\n+\n+  if (log_is_enabled(Debug, cds, heap)) {\n+    if (!_subgraph_object_klasses->contains(relocated_k)) {\n+      ResourceMark rm;\n+      log_debug(cds, heap)(\"Adding klass %s\", orig_k->external_name());\n+    }\n+  }\n+\n+  _subgraph_object_klasses->append_if_missing(relocated_k);\n+  _has_non_early_klasses |= is_non_early_klass(orig_k);\n+}\n+\n+bool KlassSubGraphInfo::is_non_early_klass(Klass* k) {\n+  if (k->is_objArray_klass()) {\n+    k = ObjArrayKlass::cast(k)->bottom_klass();\n+  }\n+  if (k->is_instance_klass()) {\n+    if (!SystemDictionaryShared::is_early_klass(InstanceKlass::cast(k))) {\n+      ResourceMark rm;\n+      log_info(cds, heap)(\"non-early: %s\", k->external_name());\n+      return true;\n+    } else {\n+      return false;\n+    }\n+  } else {\n+    return false;\n+  }\n+}\n+\n+\/\/ Initialize an archived subgraph_info_record from the given KlassSubGraphInfo.\n+void ArchivedKlassSubGraphInfoRecord::init(KlassSubGraphInfo* info) {\n+  _k = info->klass();\n+  _entry_field_records = NULL;\n+  _subgraph_object_klasses = NULL;\n+  _is_full_module_graph = info->is_full_module_graph();\n+\n+  if (_is_full_module_graph) {\n+    \/\/ Consider all classes referenced by the full module graph as early -- we will be\n+    \/\/ allocating objects of these classes during JVMTI early phase, so they cannot\n+    \/\/ be processed by (non-early) JVMTI ClassFileLoadHook\n+    _has_non_early_klasses = false;\n+  } else {\n+    _has_non_early_klasses = info->has_non_early_klasses();\n+  }\n+\n+  if (_has_non_early_klasses) {\n+    ResourceMark rm;\n+    log_info(cds, heap)(\n+          \"Subgraph of klass %s has non-early klasses and cannot be used when JVMTI ClassFileLoadHook is enabled\",\n+          _k->external_name());\n+  }\n+\n+  \/\/ populate the entry fields\n+  GrowableArray<int>* entry_fields = info->subgraph_entry_fields();\n+  if (entry_fields != NULL) {\n+    int num_entry_fields = entry_fields->length();\n+    assert(num_entry_fields % 2 == 0, \"sanity\");\n+    _entry_field_records =\n+      ArchiveBuilder::new_ro_array<int>(num_entry_fields);\n+    for (int i = 0 ; i < num_entry_fields; i++) {\n+      _entry_field_records->at_put(i, entry_fields->at(i));\n+    }\n+  }\n+\n+  \/\/ the Klasses of the objects in the sub-graphs\n+  GrowableArray<Klass*>* subgraph_object_klasses = info->subgraph_object_klasses();\n+  if (subgraph_object_klasses != NULL) {\n+    int num_subgraphs_klasses = subgraph_object_klasses->length();\n+    _subgraph_object_klasses =\n+      ArchiveBuilder::new_ro_array<Klass*>(num_subgraphs_klasses);\n+    for (int i = 0; i < num_subgraphs_klasses; i++) {\n+      Klass* subgraph_k = subgraph_object_klasses->at(i);\n+      if (log_is_enabled(Info, cds, heap)) {\n+        ResourceMark rm;\n+        log_info(cds, heap)(\n+          \"Archived object klass %s (%2d) => %s\",\n+          _k->external_name(), i, subgraph_k->external_name());\n+      }\n+      _subgraph_object_klasses->at_put(i, subgraph_k);\n+      ArchivePtrMarker::mark_pointer(_subgraph_object_klasses->adr_at(i));\n+    }\n+  }\n+\n+  ArchivePtrMarker::mark_pointer(&_k);\n+  ArchivePtrMarker::mark_pointer(&_entry_field_records);\n+  ArchivePtrMarker::mark_pointer(&_subgraph_object_klasses);\n+}\n+\n+struct CopyKlassSubGraphInfoToArchive : StackObj {\n+  CompactHashtableWriter* _writer;\n+  CopyKlassSubGraphInfoToArchive(CompactHashtableWriter* writer) : _writer(writer) {}\n+\n+  bool do_entry(Klass* klass, KlassSubGraphInfo& info) {\n+    if (info.subgraph_object_klasses() != NULL || info.subgraph_entry_fields() != NULL) {\n+      ArchivedKlassSubGraphInfoRecord* record =\n+        (ArchivedKlassSubGraphInfoRecord*)ArchiveBuilder::ro_region_alloc(sizeof(ArchivedKlassSubGraphInfoRecord));\n+      record->init(&info);\n+\n+      unsigned int hash = SystemDictionaryShared::hash_for_shared_dictionary((address)klass);\n+      u4 delta = ArchiveBuilder::current()->any_to_offset_u4(record);\n+      _writer->add(hash, delta);\n+    }\n+    return true; \/\/ keep on iterating\n+  }\n+};\n+\n+\/\/ Build the records of archived subgraph infos, which include:\n+\/\/ - Entry points to all subgraphs from the containing class mirror. The entry\n+\/\/   points are static fields in the mirror. For each entry point, the field\n+\/\/   offset, value and is_closed_archive flag are recorded in the sub-graph\n+\/\/   info. The value is stored back to the corresponding field at runtime.\n+\/\/ - A list of klasses that need to be loaded\/initialized before archived\n+\/\/   java object sub-graph can be accessed at runtime.\n+void HeapShared::write_subgraph_info_table() {\n+  \/\/ Allocate the contents of the hashtable(s) inside the RO region of the CDS archive.\n+  DumpTimeKlassSubGraphInfoTable* d_table = _dump_time_subgraph_info_table;\n+  CompactHashtableStats stats;\n+\n+  _run_time_subgraph_info_table.reset();\n+\n+  CompactHashtableWriter writer(d_table->_count, &stats);\n+  CopyKlassSubGraphInfoToArchive copy(&writer);\n+  d_table->iterate(&copy);\n+\n+  writer.dump(&_run_time_subgraph_info_table, \"subgraphs\");\n+}\n+\n+void HeapShared::serialize_subgraph_info_table_header(SerializeClosure* soc) {\n+  _run_time_subgraph_info_table.serialize_header(soc);\n+}\n+\n+static void verify_the_heap(Klass* k, const char* which) {\n+  if (VerifyArchivedFields) {\n+    ResourceMark rm;\n+    log_info(cds, heap)(\"Verify heap %s initializing static field(s) in %s\",\n+                        which, k->external_name());\n+\n+    VM_Verify verify_op;\n+    VMThread::execute(&verify_op);\n+\n+    if (!FLAG_IS_DEFAULT(VerifyArchivedFields)) {\n+      \/\/ If VerifyArchivedFields has a non-default value (e.g., specified on the command-line), do\n+      \/\/ more expensive checks.\n+      if (is_init_completed()) {\n+        FlagSetting fs1(VerifyBeforeGC, true);\n+        FlagSetting fs2(VerifyDuringGC, true);\n+        FlagSetting fs3(VerifyAfterGC,  true);\n+        Universe::heap()->collect(GCCause::_java_lang_system_gc);\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ Before GC can execute, we must ensure that all oops reachable from HeapShared::roots()\n+\/\/ have a valid klass. I.e., oopDesc::klass() must have already been resolved.\n+\/\/\n+\/\/ Note: if a ArchivedKlassSubGraphInfoRecord contains non-early classes, and JVMTI\n+\/\/ ClassFileLoadHook is enabled, it's possible for this class to be dynamically replaced. In\n+\/\/ this case, we will not load the ArchivedKlassSubGraphInfoRecord and will clear its roots.\n+void HeapShared::resolve_classes(Thread* THREAD) {\n+  if (!is_mapped()) {\n+    return; \/\/ nothing to do\n+  }\n+  resolve_classes_for_subgraphs(closed_archive_subgraph_entry_fields,\n+                                num_closed_archive_subgraph_entry_fields,\n+                                THREAD);\n+  resolve_classes_for_subgraphs(open_archive_subgraph_entry_fields,\n+                                num_open_archive_subgraph_entry_fields,\n+                                THREAD);\n+  resolve_classes_for_subgraphs(fmg_open_archive_subgraph_entry_fields,\n+                                num_fmg_open_archive_subgraph_entry_fields,\n+                                THREAD);\n+}\n+\n+void HeapShared::resolve_classes_for_subgraphs(ArchivableStaticFieldInfo fields[],\n+                                               int num, Thread* THREAD) {\n+  for (int i = 0; i < num; i++) {\n+    ArchivableStaticFieldInfo* info = &fields[i];\n+    TempNewSymbol klass_name = SymbolTable::new_symbol(info->klass_name);\n+    InstanceKlass* k = SystemDictionaryShared::find_builtin_class(klass_name);\n+    assert(k != NULL && k->is_shared_boot_class(), \"sanity\");\n+    resolve_classes_for_subgraph_of(k, THREAD);\n+  }\n+}\n+\n+void HeapShared::resolve_classes_for_subgraph_of(Klass* k, Thread* THREAD) {\n+  ExceptionMark em(THREAD);\n+  const ArchivedKlassSubGraphInfoRecord* record =\n+   resolve_or_init_classes_for_subgraph_of(k, \/*do_init=*\/false, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+   CLEAR_PENDING_EXCEPTION;\n+  }\n+  if (record == NULL) {\n+   clear_archived_roots_of(k);\n+  }\n+}\n+\n+void HeapShared::initialize_from_archived_subgraph(Klass* k, Thread* THREAD) {\n+  if (!is_mapped()) {\n+    return; \/\/ nothing to do\n+  }\n+\n+  ExceptionMark em(THREAD);\n+  const ArchivedKlassSubGraphInfoRecord* record =\n+    resolve_or_init_classes_for_subgraph_of(k, \/*do_init=*\/true, THREAD);\n+\n+  if (HAS_PENDING_EXCEPTION) {\n+    CLEAR_PENDING_EXCEPTION;\n+    \/\/ None of the field value will be set if there was an exception when initializing the classes.\n+    \/\/ The java code will not see any of the archived objects in the\n+    \/\/ subgraphs referenced from k in this case.\n+    return;\n+  }\n+\n+  if (record != NULL) {\n+    init_archived_fields_for(k, record);\n+  }\n+}\n+\n+const ArchivedKlassSubGraphInfoRecord*\n+HeapShared::resolve_or_init_classes_for_subgraph_of(Klass* k, bool do_init, TRAPS) {\n+  assert(!DumpSharedSpaces, \"Should not be called with DumpSharedSpaces\");\n+\n+  if (!k->is_shared()) {\n+    return NULL;\n+  }\n+  unsigned int hash = SystemDictionaryShared::hash_for_shared_dictionary_quick(k);\n+  const ArchivedKlassSubGraphInfoRecord* record = _run_time_subgraph_info_table.lookup(k, hash, 0);\n+\n+  \/\/ Initialize from archived data. Currently this is done only\n+  \/\/ during VM initialization time. No lock is needed.\n+  if (record != NULL) {\n+    if (record->is_full_module_graph() && !MetaspaceShared::use_full_module_graph()) {\n+      if (log_is_enabled(Info, cds, heap)) {\n+        ResourceMark rm(THREAD);\n+        log_info(cds, heap)(\"subgraph %s cannot be used because full module graph is disabled\",\n+                            k->external_name());\n+      }\n+      return NULL;\n+    }\n+\n+    if (record->has_non_early_klasses() && JvmtiExport::should_post_class_file_load_hook()) {\n+      if (log_is_enabled(Info, cds, heap)) {\n+        ResourceMark rm(THREAD);\n+        log_info(cds, heap)(\"subgraph %s cannot be used because JVMTI ClassFileLoadHook is enabled\",\n+                            k->external_name());\n+      }\n+      return NULL;\n+    }\n+\n+    resolve_or_init(k, do_init, CHECK_NULL);\n+\n+    \/\/ Load\/link\/initialize the klasses of the objects in the subgraph.\n+    \/\/ NULL class loader is used.\n+    Array<Klass*>* klasses = record->subgraph_object_klasses();\n+    if (klasses != NULL) {\n+      for (int i = 0; i < klasses->length(); i++) {\n+        Klass* klass = klasses->at(i);\n+        if (!klass->is_shared()) {\n+          return NULL;\n+        }\n+        resolve_or_init(klass, do_init, CHECK_NULL);\n+      }\n+    }\n+  }\n+\n+  return record;\n+}\n+\n+void HeapShared::resolve_or_init(Klass* k, bool do_init, TRAPS) {\n+  if (!do_init) {\n+    if (k->class_loader_data() == NULL) {\n+      Klass* resolved_k = SystemDictionary::resolve_or_null(k->name(), CHECK);\n+      assert(resolved_k == k, \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+    }\n+  } else {\n+    assert(k->class_loader_data() != NULL, \"must have been resolved by HeapShared::resolve_classes\");\n+    if (k->is_instance_klass()) {\n+      InstanceKlass* ik = InstanceKlass::cast(k);\n+      ik->initialize(CHECK);\n+    } else if (k->is_objArray_klass()) {\n+      ObjArrayKlass* oak = ObjArrayKlass::cast(k);\n+      oak->initialize(CHECK);\n+    }\n+  }\n+}\n+\n+void HeapShared::init_archived_fields_for(Klass* k, const ArchivedKlassSubGraphInfoRecord* record) {\n+  verify_the_heap(k, \"before\");\n+\n+  \/\/ Load the subgraph entry fields from the record and store them back to\n+  \/\/ the corresponding fields within the mirror.\n+  oop m = k->java_mirror();\n+  Array<int>* entry_field_records = record->entry_field_records();\n+  if (entry_field_records != NULL) {\n+    int efr_len = entry_field_records->length();\n+    assert(efr_len % 2 == 0, \"sanity\");\n+    for (int i = 0; i < efr_len; i += 2) {\n+      int field_offset = entry_field_records->at(i);\n+      int root_index = entry_field_records->at(i+1);\n+      oop v = get_root(root_index, \/*clear=*\/true);\n+      m->obj_field_put(field_offset, v);\n+      log_debug(cds, heap)(\"  \" PTR_FORMAT \" init field @ %2d = \" PTR_FORMAT, p2i(k), field_offset, p2i(v));\n+    }\n+\n+    \/\/ Done. Java code can see the archived sub-graphs referenced from k's\n+    \/\/ mirror after this point.\n+    if (log_is_enabled(Info, cds, heap)) {\n+      ResourceMark rm;\n+      log_info(cds, heap)(\"initialize_from_archived_subgraph %s \" PTR_FORMAT \"%s\",\n+                          k->external_name(), p2i(k), JvmtiExport::is_early_phase() ? \" (early)\" : \"\");\n+    }\n+  }\n+\n+  verify_the_heap(k, \"after \");\n+}\n+\n+void HeapShared::clear_archived_roots_of(Klass* k) {\n+  unsigned int hash = SystemDictionaryShared::hash_for_shared_dictionary_quick(k);\n+  const ArchivedKlassSubGraphInfoRecord* record = _run_time_subgraph_info_table.lookup(k, hash, 0);\n+  if (record != NULL) {\n+    Array<int>* entry_field_records = record->entry_field_records();\n+    if (entry_field_records != NULL) {\n+      int efr_len = entry_field_records->length();\n+      assert(efr_len % 2 == 0, \"sanity\");\n+      for (int i = 0; i < efr_len; i += 2) {\n+        int root_index = entry_field_records->at(i+1);\n+        clear_root(root_index);\n+      }\n+    }\n+  }\n+}\n+\n+class WalkOopAndArchiveClosure: public BasicOopIterateClosure {\n+  int _level;\n+  bool _is_closed_archive;\n+  bool _record_klasses_only;\n+  KlassSubGraphInfo* _subgraph_info;\n+  oop _orig_referencing_obj;\n+  oop _archived_referencing_obj;\n+ public:\n+  WalkOopAndArchiveClosure(int level,\n+                           bool is_closed_archive,\n+                           bool record_klasses_only,\n+                           KlassSubGraphInfo* subgraph_info,\n+                           oop orig, oop archived) :\n+    _level(level), _is_closed_archive(is_closed_archive),\n+    _record_klasses_only(record_klasses_only),\n+    _subgraph_info(subgraph_info),\n+    _orig_referencing_obj(orig), _archived_referencing_obj(archived) {}\n+  void do_oop(narrowOop *p) { WalkOopAndArchiveClosure::do_oop_work(p); }\n+  void do_oop(      oop *p) { WalkOopAndArchiveClosure::do_oop_work(p); }\n+\n+ protected:\n+  template <class T> void do_oop_work(T *p) {\n+    oop obj = RawAccess<>::oop_load(p);\n+    if (!CompressedOops::is_null(obj)) {\n+      assert(!HeapShared::is_archived_object(obj),\n+             \"original objects must not point to archived objects\");\n+\n+      size_t field_delta = pointer_delta(p, _orig_referencing_obj, sizeof(char));\n+      T* new_p = (T*)(cast_from_oop<address>(_archived_referencing_obj) + field_delta);\n+\n+      if (!_record_klasses_only && log_is_enabled(Debug, cds, heap)) {\n+        ResourceMark rm;\n+        log_debug(cds, heap)(\"(%d) %s[\" SIZE_FORMAT \"] ==> \" PTR_FORMAT \" size %d %s\", _level,\n+                             _orig_referencing_obj->klass()->external_name(), field_delta,\n+                             p2i(obj), obj->size() * HeapWordSize, obj->klass()->external_name());\n+        LogTarget(Trace, cds, heap) log;\n+        LogStream out(log);\n+        obj->print_on(&out);\n+      }\n+\n+      oop archived = HeapShared::archive_reachable_objects_from(\n+          _level + 1, _subgraph_info, obj, _is_closed_archive);\n+      assert(archived != NULL, \"VM should have exited with unarchivable objects for _level > 1\");\n+      assert(HeapShared::is_archived_object(archived), \"must be\");\n+\n+      if (!_record_klasses_only) {\n+        \/\/ Update the reference in the archived copy of the referencing object.\n+        log_debug(cds, heap)(\"(%d) updating oop @[\" PTR_FORMAT \"] \" PTR_FORMAT \" ==> \" PTR_FORMAT,\n+                             _level, p2i(new_p), p2i(obj), p2i(archived));\n+        RawAccess<IS_NOT_NULL>::oop_store(new_p, archived);\n+      }\n+    }\n+  }\n+};\n+\n+void HeapShared::check_closed_archive_heap_region_object(InstanceKlass* k) {\n+  \/\/ Check fields in the object\n+  for (JavaFieldStream fs(k); !fs.done(); fs.next()) {\n+    if (!fs.access_flags().is_static()) {\n+      BasicType ft = fs.field_descriptor().field_type();\n+      if (!fs.access_flags().is_final() && is_reference_type(ft)) {\n+        ResourceMark rm;\n+        log_warning(cds, heap)(\n+          \"Please check reference field in %s instance in closed archive heap region: %s %s\",\n+          k->external_name(), (fs.name())->as_C_string(),\n+          (fs.signature())->as_C_string());\n+      }\n+    }\n+  }\n+}\n+\n+void HeapShared::check_module_oop(oop orig_module_obj) {\n+  assert(DumpSharedSpaces, \"must be\");\n+  assert(java_lang_Module::is_instance(orig_module_obj), \"must be\");\n+  ModuleEntry* orig_module_ent = java_lang_Module::module_entry_raw(orig_module_obj);\n+  if (orig_module_ent == NULL) {\n+    \/\/ These special Module objects are created in Java code. They are not\n+    \/\/ defined via Modules::define_module(), so they don't have a ModuleEntry:\n+    \/\/     java.lang.Module::ALL_UNNAMED_MODULE\n+    \/\/     java.lang.Module::EVERYONE_MODULE\n+    \/\/     jdk.internal.loader.ClassLoaders$BootClassLoader::unnamedModule\n+    assert(java_lang_Module::name(orig_module_obj) == NULL, \"must be unnamed\");\n+    log_info(cds, heap)(\"Module oop with No ModuleEntry* @[\" PTR_FORMAT \"]\", p2i(orig_module_obj));\n+  } else {\n+    ClassLoaderData* loader_data = orig_module_ent->loader_data();\n+    assert(loader_data->is_builtin_class_loader_data(), \"must be\");\n+  }\n+}\n+\n+\n+\/\/ (1) If orig_obj has not been archived yet, archive it.\n+\/\/ (2) If orig_obj has not been seen yet (since start_recording_subgraph() was called),\n+\/\/     trace all  objects that are reachable from it, and make sure these objects are archived.\n+\/\/ (3) Record the klasses of all orig_obj and all reachable objects.\n+oop HeapShared::archive_reachable_objects_from(int level,\n+                                               KlassSubGraphInfo* subgraph_info,\n+                                               oop orig_obj,\n+                                               bool is_closed_archive) {\n+  assert(orig_obj != NULL, \"must be\");\n+  assert(!is_archived_object(orig_obj), \"sanity\");\n+\n+  if (!JavaClasses::is_supported_for_archiving(orig_obj)) {\n+    \/\/ This object has injected fields that cannot be supported easily, so we disallow them for now.\n+    \/\/ If you get an error here, you probably made a change in the JDK library that has added\n+    \/\/ these objects that are referenced (directly or indirectly) by static fields.\n+    ResourceMark rm;\n+    log_error(cds, heap)(\"Cannot archive object of class %s\", orig_obj->klass()->external_name());\n+    vm_direct_exit(1);\n+  }\n+\n+  \/\/ java.lang.Class instances cannot be included in an archived object sub-graph. We only support\n+  \/\/ them as Klass::_archived_mirror because they need to be specially restored at run time.\n+  \/\/\n+  \/\/ If you get an error here, you probably made a change in the JDK library that has added a Class\n+  \/\/ object that is referenced (directly or indirectly) by static fields.\n+  if (java_lang_Class::is_instance(orig_obj)) {\n+    log_error(cds, heap)(\"(%d) Unknown java.lang.Class object is in the archived sub-graph\", level);\n+    vm_direct_exit(1);\n+  }\n+\n+  oop archived_obj = find_archived_heap_object(orig_obj);\n+  if (java_lang_String::is_instance(orig_obj) && archived_obj != NULL) {\n+    \/\/ To save time, don't walk strings that are already archived. They just contain\n+    \/\/ pointers to a type array, whose klass doesn't need to be recorded.\n+    return archived_obj;\n+  }\n+\n+  if (has_been_seen_during_subgraph_recording(orig_obj)) {\n+    \/\/ orig_obj has already been archived and traced. Nothing more to do.\n+    return archived_obj;\n+  } else {\n+    set_has_been_seen_during_subgraph_recording(orig_obj);\n+  }\n+\n+  bool record_klasses_only = (archived_obj != NULL);\n+  if (archived_obj == NULL) {\n+    ++_num_new_archived_objs;\n+    archived_obj = archive_heap_object(orig_obj);\n+    if (archived_obj == NULL) {\n+      \/\/ Skip archiving the sub-graph referenced from the current entry field.\n+      ResourceMark rm;\n+      log_error(cds, heap)(\n+        \"Cannot archive the sub-graph referenced from %s object (\"\n+        PTR_FORMAT \") size %d, skipped.\",\n+        orig_obj->klass()->external_name(), p2i(orig_obj), orig_obj->size() * HeapWordSize);\n+      if (level == 1) {\n+        \/\/ Don't archive a subgraph root that's too big. For archives static fields, that's OK\n+        \/\/ as the Java code will take care of initializing this field dynamically.\n+        return NULL;\n+      } else {\n+        \/\/ We don't know how to handle an object that has been archived, but some of its reachable\n+        \/\/ objects cannot be archived. Bail out for now. We might need to fix this in the future if\n+        \/\/ we have a real use case.\n+        vm_direct_exit(1);\n+      }\n+    }\n+\n+    if (java_lang_Module::is_instance(orig_obj)) {\n+      check_module_oop(orig_obj);\n+      java_lang_Module::set_module_entry(archived_obj, NULL);\n+      java_lang_Module::set_loader(archived_obj, NULL);\n+    } else if (java_lang_ClassLoader::is_instance(orig_obj)) {\n+      \/\/ class_data will be restored explicitly at run time.\n+      guarantee(orig_obj == SystemDictionary::java_platform_loader() ||\n+                orig_obj == SystemDictionary::java_system_loader() ||\n+                java_lang_ClassLoader::loader_data_raw(orig_obj) == NULL, \"must be\");\n+      java_lang_ClassLoader::release_set_loader_data(archived_obj, NULL);\n+    }\n+  }\n+\n+  assert(archived_obj != NULL, \"must be\");\n+  Klass *orig_k = orig_obj->klass();\n+  subgraph_info->add_subgraph_object_klass(orig_k);\n+\n+  WalkOopAndArchiveClosure walker(level, is_closed_archive, record_klasses_only,\n+                                  subgraph_info, orig_obj, archived_obj);\n+  orig_obj->oop_iterate(&walker);\n+  if (is_closed_archive && orig_k->is_instance_klass()) {\n+    check_closed_archive_heap_region_object(InstanceKlass::cast(orig_k));\n+  }\n+  return archived_obj;\n+}\n+\n+\/\/\n+\/\/ Start from the given static field in a java mirror and archive the\n+\/\/ complete sub-graph of java heap objects that are reached directly\n+\/\/ or indirectly from the starting object by following references.\n+\/\/ Sub-graph archiving restrictions (current):\n+\/\/\n+\/\/ - All classes of objects in the archived sub-graph (including the\n+\/\/   entry class) must be boot class only.\n+\/\/ - No java.lang.Class instance (java mirror) can be included inside\n+\/\/   an archived sub-graph. Mirror can only be the sub-graph entry object.\n+\/\/\n+\/\/ The Java heap object sub-graph archiving process (see\n+\/\/ WalkOopAndArchiveClosure):\n+\/\/\n+\/\/ 1) Java object sub-graph archiving starts from a given static field\n+\/\/ within a Class instance (java mirror). If the static field is a\n+\/\/ refererence field and points to a non-null java object, proceed to\n+\/\/ the next step.\n+\/\/\n+\/\/ 2) Archives the referenced java object. If an archived copy of the\n+\/\/ current object already exists, updates the pointer in the archived\n+\/\/ copy of the referencing object to point to the current archived object.\n+\/\/ Otherwise, proceed to the next step.\n+\/\/\n+\/\/ 3) Follows all references within the current java object and recursively\n+\/\/ archive the sub-graph of objects starting from each reference.\n+\/\/\n+\/\/ 4) Updates the pointer in the archived copy of referencing object to\n+\/\/ point to the current archived object.\n+\/\/\n+\/\/ 5) The Klass of the current java object is added to the list of Klasses\n+\/\/ for loading and initialzing before any object in the archived graph can\n+\/\/ be accessed at runtime.\n+\/\/\n+void HeapShared::archive_reachable_objects_from_static_field(InstanceKlass *k,\n+                                                             const char* klass_name,\n+                                                             int field_offset,\n+                                                             const char* field_name,\n+                                                             bool is_closed_archive) {\n+  assert(DumpSharedSpaces, \"dump time only\");\n+  assert(k->is_shared_boot_class(), \"must be boot class\");\n+\n+  oop m = k->java_mirror();\n+\n+  KlassSubGraphInfo* subgraph_info = get_subgraph_info(k);\n+  oop f = m->obj_field(field_offset);\n+\n+  log_debug(cds, heap)(\"Start archiving from: %s::%s (\" PTR_FORMAT \")\", klass_name, field_name, p2i(f));\n+\n+  if (!CompressedOops::is_null(f)) {\n+    if (log_is_enabled(Trace, cds, heap)) {\n+      LogTarget(Trace, cds, heap) log;\n+      LogStream out(log);\n+      f->print_on(&out);\n+    }\n+\n+    oop af = archive_reachable_objects_from(1, subgraph_info, f, is_closed_archive);\n+\n+    if (af == NULL) {\n+      log_error(cds, heap)(\"Archiving failed %s::%s (some reachable objects cannot be archived)\",\n+                           klass_name, field_name);\n+    } else {\n+      \/\/ Note: the field value is not preserved in the archived mirror.\n+      \/\/ Record the field as a new subGraph entry point. The recorded\n+      \/\/ information is restored from the archive at runtime.\n+      subgraph_info->add_subgraph_entry_field(field_offset, af, is_closed_archive);\n+      log_info(cds, heap)(\"Archived field %s::%s => \" PTR_FORMAT, klass_name, field_name, p2i(af));\n+    }\n+  } else {\n+    \/\/ The field contains null, we still need to record the entry point,\n+    \/\/ so it can be restored at runtime.\n+    subgraph_info->add_subgraph_entry_field(field_offset, NULL, false);\n+  }\n+}\n+\n+#ifndef PRODUCT\n+class VerifySharedOopClosure: public BasicOopIterateClosure {\n+ private:\n+  bool _is_archived;\n+\n+ public:\n+  VerifySharedOopClosure(bool is_archived) : _is_archived(is_archived) {}\n+\n+  void do_oop(narrowOop *p) { VerifySharedOopClosure::do_oop_work(p); }\n+  void do_oop(      oop *p) { VerifySharedOopClosure::do_oop_work(p); }\n+\n+ protected:\n+  template <class T> void do_oop_work(T *p) {\n+    oop obj = RawAccess<>::oop_load(p);\n+    if (!CompressedOops::is_null(obj)) {\n+      HeapShared::verify_reachable_objects_from(obj, _is_archived);\n+    }\n+  }\n+};\n+\n+void HeapShared::verify_subgraph_from_static_field(InstanceKlass* k, int field_offset) {\n+  assert(DumpSharedSpaces, \"dump time only\");\n+  assert(k->is_shared_boot_class(), \"must be boot class\");\n+\n+  oop m = k->java_mirror();\n+  oop f = m->obj_field(field_offset);\n+  if (!CompressedOops::is_null(f)) {\n+    verify_subgraph_from(f);\n+  }\n+}\n+\n+void HeapShared::verify_subgraph_from(oop orig_obj) {\n+  oop archived_obj = find_archived_heap_object(orig_obj);\n+  if (archived_obj == NULL) {\n+    \/\/ It's OK for the root of a subgraph to be not archived. See comments in\n+    \/\/ archive_reachable_objects_from().\n+    return;\n+  }\n+\n+  \/\/ Verify that all objects reachable from orig_obj are archived.\n+  init_seen_objects_table();\n+  verify_reachable_objects_from(orig_obj, false);\n+  delete_seen_objects_table();\n+\n+  \/\/ Note: we could also verify that all objects reachable from the archived\n+  \/\/ copy of orig_obj can only point to archived objects, with:\n+  \/\/      init_seen_objects_table();\n+  \/\/      verify_reachable_objects_from(archived_obj, true);\n+  \/\/      init_seen_objects_table();\n+  \/\/ but that's already done in G1HeapVerifier::verify_archive_regions so we\n+  \/\/ won't do it here.\n+}\n+\n+void HeapShared::verify_reachable_objects_from(oop obj, bool is_archived) {\n+  _num_total_verifications ++;\n+  if (!has_been_seen_during_subgraph_recording(obj)) {\n+    set_has_been_seen_during_subgraph_recording(obj);\n+\n+    if (is_archived) {\n+      assert(is_archived_object(obj), \"must be\");\n+      assert(find_archived_heap_object(obj) == NULL, \"must be\");\n+    } else {\n+      assert(!is_archived_object(obj), \"must be\");\n+      assert(find_archived_heap_object(obj) != NULL, \"must be\");\n+    }\n+\n+    VerifySharedOopClosure walker(is_archived);\n+    obj->oop_iterate(&walker);\n+  }\n+}\n+#endif\n+\n+HeapShared::SeenObjectsTable* HeapShared::_seen_objects_table = NULL;\n+int HeapShared::_num_new_walked_objs;\n+int HeapShared::_num_new_archived_objs;\n+int HeapShared::_num_old_recorded_klasses;\n+\n+int HeapShared::_num_total_subgraph_recordings = 0;\n+int HeapShared::_num_total_walked_objs = 0;\n+int HeapShared::_num_total_archived_objs = 0;\n+int HeapShared::_num_total_recorded_klasses = 0;\n+int HeapShared::_num_total_verifications = 0;\n+\n+bool HeapShared::has_been_seen_during_subgraph_recording(oop obj) {\n+  return _seen_objects_table->get(obj) != NULL;\n+}\n+\n+void HeapShared::set_has_been_seen_during_subgraph_recording(oop obj) {\n+  assert(!has_been_seen_during_subgraph_recording(obj), \"sanity\");\n+  _seen_objects_table->put(obj, true);\n+  ++ _num_new_walked_objs;\n+}\n+\n+void HeapShared::start_recording_subgraph(InstanceKlass *k, const char* class_name, bool is_full_module_graph) {\n+  log_info(cds, heap)(\"Start recording subgraph(s) for archived fields in %s\", class_name);\n+  init_subgraph_info(k, is_full_module_graph);\n+  init_seen_objects_table();\n+  _num_new_walked_objs = 0;\n+  _num_new_archived_objs = 0;\n+  _num_old_recorded_klasses = get_subgraph_info(k)->num_subgraph_object_klasses();\n+}\n+\n+void HeapShared::done_recording_subgraph(InstanceKlass *k, const char* class_name) {\n+  int num_new_recorded_klasses = get_subgraph_info(k)->num_subgraph_object_klasses() -\n+    _num_old_recorded_klasses;\n+  log_info(cds, heap)(\"Done recording subgraph(s) for archived fields in %s: \"\n+                      \"walked %d objs, archived %d new objs, recorded %d classes\",\n+                      class_name, _num_new_walked_objs, _num_new_archived_objs,\n+                      num_new_recorded_klasses);\n+\n+  delete_seen_objects_table();\n+\n+  _num_total_subgraph_recordings ++;\n+  _num_total_walked_objs      += _num_new_walked_objs;\n+  _num_total_archived_objs    += _num_new_archived_objs;\n+  _num_total_recorded_klasses +=  num_new_recorded_klasses;\n+}\n+\n+class ArchivableStaticFieldFinder: public FieldClosure {\n+  InstanceKlass* _ik;\n+  Symbol* _field_name;\n+  bool _found;\n+  int _offset;\n+public:\n+  ArchivableStaticFieldFinder(InstanceKlass* ik, Symbol* field_name) :\n+    _ik(ik), _field_name(field_name), _found(false), _offset(-1) {}\n+\n+  virtual void do_field(fieldDescriptor* fd) {\n+    if (fd->name() == _field_name) {\n+      assert(!_found, \"fields cannot be overloaded\");\n+      assert(is_reference_type(fd->field_type()), \"can archive only fields that are references\");\n+      _found = true;\n+      _offset = fd->offset();\n+    }\n+  }\n+  bool found()     { return _found;  }\n+  int offset()     { return _offset; }\n+};\n+\n+void HeapShared::init_subgraph_entry_fields(ArchivableStaticFieldInfo fields[],\n+                                            int num, TRAPS) {\n+  for (int i = 0; i < num; i++) {\n+    ArchivableStaticFieldInfo* info = &fields[i];\n+    TempNewSymbol klass_name =  SymbolTable::new_symbol(info->klass_name);\n+    TempNewSymbol field_name =  SymbolTable::new_symbol(info->field_name);\n+\n+    Klass* k = SystemDictionary::resolve_or_fail(klass_name, true, CHECK);\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    assert(InstanceKlass::cast(ik)->is_shared_boot_class(),\n+           \"Only support boot classes\");\n+    ik->initialize(CHECK);\n+\n+    ArchivableStaticFieldFinder finder(ik, field_name);\n+    ik->do_local_static_fields(&finder);\n+    assert(finder.found(), \"field must exist\");\n+\n+    info->klass = ik;\n+    info->offset = finder.offset();\n+  }\n+}\n+\n+void HeapShared::init_subgraph_entry_fields(TRAPS) {\n+  assert(is_heap_object_archiving_allowed(), \"Sanity check\");\n+  _dump_time_subgraph_info_table = new (ResourceObj::C_HEAP, mtClass)DumpTimeKlassSubGraphInfoTable();\n+  init_subgraph_entry_fields(closed_archive_subgraph_entry_fields,\n+                             num_closed_archive_subgraph_entry_fields,\n+                             CHECK);\n+  init_subgraph_entry_fields(open_archive_subgraph_entry_fields,\n+                             num_open_archive_subgraph_entry_fields,\n+                             CHECK);\n+  if (MetaspaceShared::use_full_module_graph()) {\n+    init_subgraph_entry_fields(fmg_open_archive_subgraph_entry_fields,\n+                               num_fmg_open_archive_subgraph_entry_fields,\n+                               CHECK);\n+  }\n+}\n+\n+void HeapShared::init_for_dumping(TRAPS) {\n+  if (is_heap_object_archiving_allowed()) {\n+    _dumped_interned_strings = new (ResourceObj::C_HEAP, mtClass)DumpedInternedStrings();\n+    init_subgraph_entry_fields(CHECK);\n+  }\n+}\n+\n+void HeapShared::archive_object_subgraphs(ArchivableStaticFieldInfo fields[],\n+                                          int num, bool is_closed_archive,\n+                                          bool is_full_module_graph) {\n+  _num_total_subgraph_recordings = 0;\n+  _num_total_walked_objs = 0;\n+  _num_total_archived_objs = 0;\n+  _num_total_recorded_klasses = 0;\n+  _num_total_verifications = 0;\n+\n+  \/\/ For each class X that has one or more archived fields:\n+  \/\/ [1] Dump the subgraph of each archived field\n+  \/\/ [2] Create a list of all the class of the objects that can be reached\n+  \/\/     by any of these static fields.\n+  \/\/     At runtime, these classes are initialized before X's archived fields\n+  \/\/     are restored by HeapShared::initialize_from_archived_subgraph().\n+  int i;\n+  for (i = 0; i < num; ) {\n+    ArchivableStaticFieldInfo* info = &fields[i];\n+    const char* klass_name = info->klass_name;\n+    start_recording_subgraph(info->klass, klass_name, is_full_module_graph);\n+\n+    \/\/ If you have specified consecutive fields of the same klass in\n+    \/\/ fields[], these will be archived in the same\n+    \/\/ {start_recording_subgraph ... done_recording_subgraph} pass to\n+    \/\/ save time.\n+    for (; i < num; i++) {\n+      ArchivableStaticFieldInfo* f = &fields[i];\n+      if (f->klass_name != klass_name) {\n+        break;\n+      }\n+\n+      archive_reachable_objects_from_static_field(f->klass, f->klass_name,\n+                                                  f->offset, f->field_name,\n+                                                  is_closed_archive);\n+    }\n+    done_recording_subgraph(info->klass, klass_name);\n+  }\n+\n+  log_info(cds, heap)(\"Archived subgraph records in %s archive heap region = %d\",\n+                      is_closed_archive ? \"closed\" : \"open\",\n+                      _num_total_subgraph_recordings);\n+  log_info(cds, heap)(\"  Walked %d objects\", _num_total_walked_objs);\n+  log_info(cds, heap)(\"  Archived %d objects\", _num_total_archived_objs);\n+  log_info(cds, heap)(\"  Recorded %d klasses\", _num_total_recorded_klasses);\n+\n+#ifndef PRODUCT\n+  for (int i = 0; i < num; i++) {\n+    ArchivableStaticFieldInfo* f = &fields[i];\n+    verify_subgraph_from_static_field(f->klass, f->offset);\n+  }\n+  log_info(cds, heap)(\"  Verified %d references\", _num_total_verifications);\n+#endif\n+}\n+\n+\/\/ Not all the strings in the global StringTable are dumped into the archive, because\n+\/\/ some of those strings may be only referenced by classes that are excluded from\n+\/\/ the archive. We need to explicitly mark the strings that are:\n+\/\/   [1] used by classes that WILL be archived;\n+\/\/   [2] included in the SharedArchiveConfigFile.\n+void HeapShared::add_to_dumped_interned_strings(oop string) {\n+  assert_at_safepoint(); \/\/ DumpedInternedStrings uses raw oops\n+  bool created;\n+  _dumped_interned_strings->put_if_absent(string, true, &created);\n+}\n+\n+\/\/ At dump-time, find the location of all the non-null oop pointers in an archived heap\n+\/\/ region. This way we can quickly relocate all the pointers without using\n+\/\/ BasicOopIterateClosure at runtime.\n+class FindEmbeddedNonNullPointers: public BasicOopIterateClosure {\n+  narrowOop* _start;\n+  BitMap *_oopmap;\n+  int _num_total_oops;\n+  int _num_null_oops;\n+ public:\n+  FindEmbeddedNonNullPointers(narrowOop* start, BitMap* oopmap)\n+    : _start(start), _oopmap(oopmap), _num_total_oops(0),  _num_null_oops(0) {}\n+\n+  virtual void do_oop(narrowOop* p) {\n+    _num_total_oops ++;\n+    narrowOop v = *p;\n+    if (!CompressedOops::is_null(v)) {\n+      size_t idx = p - _start;\n+      _oopmap->set_bit(idx);\n+    } else {\n+      _num_null_oops ++;\n+    }\n+  }\n+  virtual void do_oop(oop *p) {\n+    ShouldNotReachHere();\n+  }\n+  int num_total_oops() const { return _num_total_oops; }\n+  int num_null_oops()  const { return _num_null_oops; }\n+};\n+\n+ResourceBitMap HeapShared::calculate_oopmap(MemRegion region) {\n+  assert(UseCompressedOops, \"must be\");\n+  size_t num_bits = region.byte_size() \/ sizeof(narrowOop);\n+  ResourceBitMap oopmap(num_bits);\n+\n+  HeapWord* p   = region.start();\n+  HeapWord* end = region.end();\n+  FindEmbeddedNonNullPointers finder((narrowOop*)p, &oopmap);\n+  ArchiveBuilder* builder = DumpSharedSpaces ? ArchiveBuilder::current() : NULL;\n+\n+  int num_objs = 0;\n+  while (p < end) {\n+    oop o = cast_to_oop(p);\n+    o->oop_iterate(&finder);\n+    p += o->size();\n+    if (DumpSharedSpaces) {\n+      builder->relocate_klass_ptr(o);\n+    }\n+    ++ num_objs;\n+  }\n+\n+  log_info(cds, heap)(\"calculate_oopmap: objects = %6d, embedded oops = %7d, nulls = %7d\",\n+                      num_objs, finder.num_total_oops(), finder.num_null_oops());\n+  return oopmap;\n+}\n+\n+\/\/ Patch all the embedded oop pointers inside an archived heap region,\n+\/\/ to be consistent with the runtime oop encoding.\n+class PatchEmbeddedPointers: public BitMapClosure {\n+  narrowOop* _start;\n+\n+ public:\n+  PatchEmbeddedPointers(narrowOop* start) : _start(start) {}\n+\n+  bool do_bit(size_t offset) {\n+    narrowOop* p = _start + offset;\n+    narrowOop v = *p;\n+    assert(!CompressedOops::is_null(v), \"null oops should have been filtered out at dump time\");\n+    oop o = HeapShared::decode_from_archive(v);\n+    RawAccess<IS_NOT_NULL>::oop_store(p, o);\n+    return true;\n+  }\n+};\n+\n+void HeapShared::patch_archived_heap_embedded_pointers(MemRegion region, address oopmap,\n+                                                       size_t oopmap_size_in_bits) {\n+  BitMapView bm((BitMap::bm_word_t*)oopmap, oopmap_size_in_bits);\n+\n+#ifndef PRODUCT\n+  ResourceMark rm;\n+  ResourceBitMap checkBm = calculate_oopmap(region);\n+  assert(bm.is_same(checkBm), \"sanity\");\n+#endif\n+\n+  PatchEmbeddedPointers patcher((narrowOop*)region.start());\n+  bm.iterate(&patcher);\n+}\n+\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":1456,"deletions":0,"binary":false,"changes":1456,"status":"added"},{"patch":"@@ -0,0 +1,1543 @@\n+\/*\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"jvm_io.h\"\n+#include \"cds\/archiveBuilder.hpp\"\n+#include \"cds\/classListParser.hpp\"\n+#include \"cds\/cppVtables.hpp\"\n+#include \"cds\/dumpAllocStats.hpp\"\n+#include \"cds\/filemap.hpp\"\n+#include \"cds\/heapShared.hpp\"\n+#include \"cds\/lambdaFormInvokers.hpp\"\n+#include \"cds\/metaspaceShared.hpp\"\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/classLoaderDataShared.hpp\"\n+#include \"classfile\/classLoaderExt.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"classfile\/loaderConstraints.hpp\"\n+#include \"classfile\/placeholders.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/systemDictionaryShared.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"gc\/shared\/gcVMOperations.hpp\"\n+#include \"interpreter\/bytecodeStream.hpp\"\n+#include \"interpreter\/bytecodes.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logMessage.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/metaspace.hpp\"\n+#include \"memory\/metaspaceClosure.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/klass.inline.hpp\"\n+#include \"oops\/objArrayOop.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/oopHandle.hpp\"\n+#include \"prims\/jvmtiExport.hpp\"\n+#include \"runtime\/arguments.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/safepointVerifiers.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"runtime\/vmOperations.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/defaultStream.hpp\"\n+#if INCLUDE_G1GC\n+#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n+#endif\n+\n+ReservedSpace MetaspaceShared::_symbol_rs;\n+VirtualSpace MetaspaceShared::_symbol_vs;\n+bool MetaspaceShared::_has_error_classes;\n+bool MetaspaceShared::_archive_loading_failed = false;\n+bool MetaspaceShared::_remapped_readwrite = false;\n+void* MetaspaceShared::_shared_metaspace_static_top = NULL;\n+intx MetaspaceShared::_relocation_delta;\n+char* MetaspaceShared::_requested_base_address;\n+bool MetaspaceShared::_use_optimized_module_handling = true;\n+bool MetaspaceShared::_use_full_module_graph = true;\n+\n+\/\/ The CDS archive is divided into the following regions:\n+\/\/     rw  - read-write metadata\n+\/\/     ro  - read-only metadata and read-only tables\n+\/\/\n+\/\/     ca0 - closed archive heap space #0\n+\/\/     ca1 - closed archive heap space #1 (may be empty)\n+\/\/     oa0 - open archive heap space #0\n+\/\/     oa1 - open archive heap space #1 (may be empty)\n+\/\/\n+\/\/     bm  - bitmap for relocating the above 7 regions.\n+\/\/\n+\/\/ The rw and ro regions are linearly allocated, in the order of rw->ro.\n+\/\/ These regions are aligned with MetaspaceShared::core_region_alignment().\n+\/\/\n+\/\/ These 2 regions are populated in the following steps:\n+\/\/ [0] All classes are loaded in MetaspaceShared::preload_classes(). All metadata are\n+\/\/     temporarily allocated outside of the shared regions.\n+\/\/ [1] We enter a safepoint and allocate a buffer for the rw\/ro regions.\n+\/\/ [2] C++ vtables are copied into the rw region.\n+\/\/ [3] ArchiveBuilder copies RW metadata into the rw region.\n+\/\/ [4] ArchiveBuilder copies RO metadata into the ro region.\n+\/\/ [5] SymbolTable, StringTable, SystemDictionary, and a few other read-only data\n+\/\/     are copied into the ro region as read-only tables.\n+\/\/\n+\/\/ The ca0\/ca1 and oa0\/oa1 regions are populated inside HeapShared::archive_java_heap_objects.\n+\/\/ Their layout is independent of the rw\/ro regions.\n+\n+static DumpRegion _symbol_region(\"symbols\");\n+\n+char* MetaspaceShared::symbol_space_alloc(size_t num_bytes) {\n+  return _symbol_region.allocate(num_bytes);\n+}\n+\n+\/\/ os::vm_allocation_granularity() is usually 4K for most OSes. However, on Linux\/aarch64,\n+\/\/ it can be either 4K or 64K and on Macosx-arm it is 16K. To generate archives that are\n+\/\/ compatible for both settings, an alternative cds core region alignment can be enabled\n+\/\/ at building time:\n+\/\/   --enable-compactible-cds-alignment\n+\/\/ Upon successful configuration, the compactible alignment then can be defined as in:\n+\/\/   os_linux_aarch64.hpp\n+\/\/ which is the highest page size configured on the platform.\n+size_t MetaspaceShared::core_region_alignment() {\n+#if defined(CDS_CORE_REGION_ALIGNMENT)\n+  return CDS_CORE_REGION_ALIGNMENT;\n+#else\n+  return (size_t)os::vm_allocation_granularity();\n+#endif \/\/ CDS_CORE_REGION_ALIGNMENT\n+}\n+\n+static bool shared_base_valid(char* shared_base) {\n+#ifdef _LP64\n+  return CompressedKlassPointers::is_valid_base((address)shared_base);\n+#else\n+  return true;\n+#endif\n+}\n+\n+class DumpClassListCLDClosure : public CLDClosure {\n+  fileStream *_stream;\n+public:\n+  DumpClassListCLDClosure(fileStream* f) : CLDClosure() { _stream = f; }\n+  void do_cld(ClassLoaderData* cld) {\n+    for (Klass* klass = cld->klasses(); klass != NULL; klass = klass->next_link()) {\n+      if (klass->is_instance_klass()) {\n+        InstanceKlass* ik = InstanceKlass::cast(klass);\n+        if (ik->is_shareable()) {\n+          _stream->print_cr(\"%s\", ik->name()->as_C_string());\n+        }\n+      }\n+    }\n+  }\n+};\n+\n+void MetaspaceShared::dump_loaded_classes(const char* file_name, TRAPS) {\n+  fileStream stream(file_name, \"w\");\n+  if (stream.is_open()) {\n+    MutexLocker lock(ClassLoaderDataGraph_lock);\n+    DumpClassListCLDClosure collect_classes(&stream);\n+    ClassLoaderDataGraph::loaded_cld_do(&collect_classes);\n+  } else {\n+    THROW_MSG(vmSymbols::java_io_IOException(), \"Failed to open file\");\n+  }\n+}\n+\n+static bool shared_base_too_high(char* specified_base, char* aligned_base, size_t cds_max) {\n+  if (specified_base != NULL && aligned_base < specified_base) {\n+    \/\/ SharedBaseAddress is very high (e.g., 0xffffffffffffff00) so\n+    \/\/ align_up(SharedBaseAddress, MetaspaceShared::core_region_alignment()) has wrapped around.\n+    return true;\n+  }\n+  if (max_uintx - uintx(aligned_base) < uintx(cds_max)) {\n+    \/\/ The end of the archive will wrap around\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+static char* compute_shared_base(size_t cds_max) {\n+  char* specified_base = (char*)SharedBaseAddress;\n+  char* aligned_base = align_up(specified_base, MetaspaceShared::core_region_alignment());\n+\n+  const char* err = NULL;\n+  if (shared_base_too_high(specified_base, aligned_base, cds_max)) {\n+    err = \"too high\";\n+  } else if (!shared_base_valid(aligned_base)) {\n+    err = \"invalid for this platform\";\n+  } else {\n+    return aligned_base;\n+  }\n+\n+  log_warning(cds)(\"SharedBaseAddress (\" INTPTR_FORMAT \") is %s. Reverted to \" INTPTR_FORMAT,\n+                   p2i((void*)SharedBaseAddress), err,\n+                   p2i((void*)Arguments::default_SharedBaseAddress()));\n+\n+  specified_base = (char*)Arguments::default_SharedBaseAddress();\n+  aligned_base = align_up(specified_base, MetaspaceShared::core_region_alignment());\n+\n+  \/\/ Make sure the default value of SharedBaseAddress specified in globals.hpp is sane.\n+  assert(!shared_base_too_high(specified_base, aligned_base, cds_max), \"Sanity\");\n+  assert(shared_base_valid(aligned_base), \"Sanity\");\n+  return aligned_base;\n+}\n+\n+void MetaspaceShared::initialize_for_static_dump() {\n+  assert(DumpSharedSpaces, \"should be called for dump time only\");\n+  log_info(cds)(\"Core region alignment: \" SIZE_FORMAT, core_region_alignment());\n+  \/\/ The max allowed size for CDS archive. We use this to limit SharedBaseAddress\n+  \/\/ to avoid address space wrap around.\n+  size_t cds_max;\n+  const size_t reserve_alignment = core_region_alignment();\n+\n+#ifdef _LP64\n+  const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);\n+  cds_max = align_down(UnscaledClassSpaceMax, reserve_alignment);\n+#else\n+  \/\/ We don't support archives larger than 256MB on 32-bit due to limited\n+  \/\/  virtual address space.\n+  cds_max = align_down(256*M, reserve_alignment);\n+#endif\n+\n+  _requested_base_address = compute_shared_base(cds_max);\n+  SharedBaseAddress = (size_t)_requested_base_address;\n+\n+  size_t symbol_rs_size = LP64_ONLY(3 * G) NOT_LP64(128 * M);\n+  _symbol_rs = ReservedSpace(symbol_rs_size);\n+  if (!_symbol_rs.is_reserved()) {\n+    vm_exit_during_initialization(\"Unable to reserve memory for symbols\",\n+                                  err_msg(SIZE_FORMAT \" bytes.\", symbol_rs_size));\n+  }\n+  _symbol_region.init(&_symbol_rs, &_symbol_vs);\n+}\n+\n+\/\/ Called by universe_post_init()\n+void MetaspaceShared::post_initialize(TRAPS) {\n+  if (UseSharedSpaces) {\n+    int size = FileMapInfo::get_number_of_shared_paths();\n+    if (size > 0) {\n+      SystemDictionaryShared::allocate_shared_data_arrays(size, CHECK);\n+      if (!DynamicDumpSharedSpaces) {\n+        FileMapInfo* info;\n+        if (FileMapInfo::dynamic_info() == NULL) {\n+          info = FileMapInfo::current_info();\n+        } else {\n+          info = FileMapInfo::dynamic_info();\n+        }\n+        ClassLoaderExt::init_paths_start_index(info->app_class_paths_start_index());\n+        ClassLoaderExt::init_app_module_paths_start_index(info->app_module_paths_start_index());\n+      }\n+    }\n+  }\n+}\n+\n+static GrowableArrayCHeap<OopHandle, mtClassShared>* _extra_interned_strings = NULL;\n+static GrowableArrayCHeap<Symbol*, mtClassShared>* _extra_symbols = NULL;\n+\n+void MetaspaceShared::read_extra_data(Thread* current, const char* filename) {\n+  _extra_interned_strings = new GrowableArrayCHeap<OopHandle, mtClassShared>(10000);\n+  _extra_symbols = new GrowableArrayCHeap<Symbol*, mtClassShared>(1000);\n+\n+  HashtableTextDump reader(filename);\n+  reader.check_version(\"VERSION: 1.0\");\n+\n+  while (reader.remain() > 0) {\n+    int utf8_length;\n+    int prefix_type = reader.scan_prefix(&utf8_length);\n+    ResourceMark rm(current);\n+    if (utf8_length == 0x7fffffff) {\n+      \/\/ buf_len will overflown 32-bit value.\n+      vm_exit_during_initialization(err_msg(\"string length too large: %d\", utf8_length));\n+    }\n+    int buf_len = utf8_length+1;\n+    char* utf8_buffer = NEW_RESOURCE_ARRAY(char, buf_len);\n+    reader.get_utf8(utf8_buffer, utf8_length);\n+    utf8_buffer[utf8_length] = '\\0';\n+\n+    if (prefix_type == HashtableTextDump::SymbolPrefix) {\n+      _extra_symbols->append(SymbolTable::new_permanent_symbol(utf8_buffer));\n+    } else{\n+      assert(prefix_type == HashtableTextDump::StringPrefix, \"Sanity\");\n+      ExceptionMark em(current);\n+      Thread* THREAD = current; \/\/ For exception macros.\n+      oop str = StringTable::intern(utf8_buffer, THREAD);\n+\n+      if (HAS_PENDING_EXCEPTION) {\n+        log_warning(cds, heap)(\"[line %d] extra interned string allocation failed; size too large: %d\",\n+                               reader.last_line_no(), utf8_length);\n+        CLEAR_PENDING_EXCEPTION;\n+      } else {\n+#if INCLUDE_G1GC\n+        if (UseG1GC) {\n+          typeArrayOop body = java_lang_String::value(str);\n+          const HeapRegion* hr = G1CollectedHeap::heap()->heap_region_containing(body);\n+          if (hr->is_humongous()) {\n+            \/\/ Don't keep it alive, so it will be GC'ed before we dump the strings, in order\n+            \/\/ to maximize free heap space and minimize fragmentation.\n+            log_warning(cds, heap)(\"[line %d] extra interned string ignored; size too large: %d\",\n+                                reader.last_line_no(), utf8_length);\n+            continue;\n+          }\n+        }\n+#endif\n+        \/\/ Make sure this string is included in the dumped interned string table.\n+        assert(str != NULL, \"must succeed\");\n+        _extra_interned_strings->append(OopHandle(Universe::vm_global(), str));\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ Read\/write a data stream for restoring\/preserving metadata pointers and\n+\/\/ miscellaneous data from\/to the shared archive file.\n+\n+void MetaspaceShared::serialize(SerializeClosure* soc) {\n+  int tag = 0;\n+  soc->do_tag(--tag);\n+\n+  \/\/ Verify the sizes of various metadata in the system.\n+  soc->do_tag(sizeof(Method));\n+  soc->do_tag(sizeof(ConstMethod));\n+  soc->do_tag(arrayOopDesc::base_offset_in_bytes(T_BYTE));\n+  soc->do_tag(sizeof(ConstantPool));\n+  soc->do_tag(sizeof(ConstantPoolCache));\n+  soc->do_tag(objArrayOopDesc::base_offset_in_bytes());\n+  soc->do_tag(typeArrayOopDesc::base_offset_in_bytes(T_BYTE));\n+  soc->do_tag(sizeof(Symbol));\n+\n+  \/\/ Dump\/restore miscellaneous metadata.\n+  JavaClasses::serialize_offsets(soc);\n+  Universe::serialize(soc);\n+  soc->do_tag(--tag);\n+\n+  \/\/ Dump\/restore references to commonly used names and signatures.\n+  vmSymbols::serialize(soc);\n+  soc->do_tag(--tag);\n+\n+  \/\/ Dump\/restore the symbol\/string\/subgraph_info tables\n+  SymbolTable::serialize_shared_table_header(soc);\n+  StringTable::serialize_shared_table_header(soc);\n+  HeapShared::serialize_subgraph_info_table_header(soc);\n+  SystemDictionaryShared::serialize_dictionary_headers(soc);\n+\n+  InstanceMirrorKlass::serialize_offsets(soc);\n+\n+  \/\/ Dump\/restore well known classes (pointers)\n+  SystemDictionaryShared::serialize_vm_classes(soc);\n+  soc->do_tag(--tag);\n+\n+  CppVtables::serialize(soc);\n+  soc->do_tag(--tag);\n+\n+  CDS_JAVA_HEAP_ONLY(ClassLoaderDataShared::serialize(soc);)\n+\n+  LambdaFormInvokers::serialize(soc);\n+  soc->do_tag(666);\n+}\n+\n+static void rewrite_nofast_bytecode(const methodHandle& method) {\n+  BytecodeStream bcs(method);\n+  while (!bcs.is_last_bytecode()) {\n+    Bytecodes::Code opcode = bcs.next();\n+    switch (opcode) {\n+    case Bytecodes::_getfield:      *bcs.bcp() = Bytecodes::_nofast_getfield;      break;\n+    case Bytecodes::_putfield:      *bcs.bcp() = Bytecodes::_nofast_putfield;      break;\n+    case Bytecodes::_aload_0:       *bcs.bcp() = Bytecodes::_nofast_aload_0;       break;\n+    case Bytecodes::_iload: {\n+      if (!bcs.is_wide()) {\n+        *bcs.bcp() = Bytecodes::_nofast_iload;\n+      }\n+      break;\n+    }\n+    default: break;\n+    }\n+  }\n+}\n+\n+\/\/ [1] Rewrite all bytecodes as needed, so that the ConstMethod* will not be modified\n+\/\/     at run time by RewriteBytecodes\/RewriteFrequentPairs\n+\/\/ [2] Assign a fingerprint, so one doesn't need to be assigned at run-time.\n+void MetaspaceShared::rewrite_nofast_bytecodes_and_calculate_fingerprints(Thread* thread, InstanceKlass* ik) {\n+  for (int i = 0; i < ik->methods()->length(); i++) {\n+    methodHandle m(thread, ik->methods()->at(i));\n+    if (!is_old_class(ik)) {\n+      rewrite_nofast_bytecode(m);\n+    }\n+    Fingerprinter fp(m);\n+    \/\/ The side effect of this call sets method's fingerprint field.\n+    fp.fingerprint();\n+  }\n+}\n+\n+class VM_PopulateDumpSharedSpace : public VM_GC_Operation {\n+private:\n+  GrowableArray<MemRegion> *_closed_archive_heap_regions;\n+  GrowableArray<MemRegion> *_open_archive_heap_regions;\n+\n+  GrowableArray<ArchiveHeapOopmapInfo> *_closed_archive_heap_oopmaps;\n+  GrowableArray<ArchiveHeapOopmapInfo> *_open_archive_heap_oopmaps;\n+\n+  void dump_java_heap_objects(GrowableArray<Klass*>* klasses) NOT_CDS_JAVA_HEAP_RETURN;\n+  void dump_archive_heap_oopmaps() NOT_CDS_JAVA_HEAP_RETURN;\n+  void dump_archive_heap_oopmaps(GrowableArray<MemRegion>* regions,\n+                                 GrowableArray<ArchiveHeapOopmapInfo>* oopmaps);\n+  void dump_shared_symbol_table(GrowableArray<Symbol*>* symbols) {\n+    log_info(cds)(\"Dumping symbol table ...\");\n+    SymbolTable::write_to_archive(symbols);\n+  }\n+  char* dump_read_only_tables();\n+\n+public:\n+\n+  VM_PopulateDumpSharedSpace() :\n+    VM_GC_Operation(0 \/* total collections, ignored *\/, GCCause::_archive_time_gc),\n+    _closed_archive_heap_regions(NULL),\n+    _open_archive_heap_regions(NULL),\n+    _closed_archive_heap_oopmaps(NULL),\n+    _open_archive_heap_oopmaps(NULL) {}\n+\n+  bool skip_operation() const { return false; }\n+\n+  VMOp_Type type() const { return VMOp_PopulateDumpSharedSpace; }\n+  void doit();   \/\/ outline because gdb sucks\n+  bool allow_nested_vm_operations() const { return true; }\n+}; \/\/ class VM_PopulateDumpSharedSpace\n+\n+class StaticArchiveBuilder : public ArchiveBuilder {\n+public:\n+  StaticArchiveBuilder() : ArchiveBuilder() {}\n+\n+  virtual void iterate_roots(MetaspaceClosure* it, bool is_relocating_pointers) {\n+    FileMapInfo::metaspace_pointers_do(it, false);\n+    SystemDictionaryShared::dumptime_classes_do(it);\n+    Universe::metaspace_pointers_do(it);\n+    vmSymbols::metaspace_pointers_do(it);\n+\n+    \/\/ The above code should find all the symbols that are referenced by the\n+    \/\/ archived classes. We just need to add the extra symbols which\n+    \/\/ may not be used by any of the archived classes -- these are usually\n+    \/\/ symbols that we anticipate to be used at run time, so we can store\n+    \/\/ them in the RO region, to be shared across multiple processes.\n+    if (_extra_symbols != NULL) {\n+      for (int i = 0; i < _extra_symbols->length(); i++) {\n+        it->push(_extra_symbols->adr_at(i));\n+      }\n+    }\n+  }\n+};\n+\n+char* VM_PopulateDumpSharedSpace::dump_read_only_tables() {\n+  ArchiveBuilder::OtherROAllocMark mark;\n+\n+  SystemDictionaryShared::write_to_archive();\n+\n+  \/\/ Write lambform lines into archive\n+  LambdaFormInvokers::dump_static_archive_invokers();\n+  \/\/ Write the other data to the output array.\n+  DumpRegion* ro_region = ArchiveBuilder::current()->ro_region();\n+  char* start = ro_region->top();\n+  WriteClosure wc(ro_region);\n+  MetaspaceShared::serialize(&wc);\n+\n+  \/\/ Write the bitmaps for patching the archive heap regions\n+  dump_archive_heap_oopmaps();\n+\n+  return start;\n+}\n+\n+void VM_PopulateDumpSharedSpace::doit() {\n+  HeapShared::run_full_gc_in_vm_thread();\n+\n+  \/\/ We should no longer allocate anything from the metaspace, so that:\n+  \/\/\n+  \/\/ (1) Metaspace::allocate might trigger GC if we have run out of\n+  \/\/     committed metaspace, but we can't GC because we're running\n+  \/\/     in the VM thread.\n+  \/\/ (2) ArchiveBuilder needs to work with a stable set of MetaspaceObjs.\n+  Metaspace::freeze();\n+  DEBUG_ONLY(SystemDictionaryShared::NoClassLoadingMark nclm);\n+\n+  FileMapInfo::check_nonempty_dir_in_shared_path_table();\n+\n+  NOT_PRODUCT(SystemDictionary::verify();)\n+\n+  \/\/ At this point, many classes have been loaded.\n+  \/\/ Gather systemDictionary classes in a global array and do everything to\n+  \/\/ that so we don't have to walk the SystemDictionary again.\n+  SystemDictionaryShared::check_excluded_classes();\n+\n+  StaticArchiveBuilder builder;\n+  builder.gather_source_objs();\n+  builder.reserve_buffer();\n+\n+  char* cloned_vtables = CppVtables::dumptime_init(&builder);\n+\n+  builder.dump_rw_metadata();\n+  builder.dump_ro_metadata();\n+  builder.relocate_metaspaceobj_embedded_pointers();\n+\n+  \/\/ Dump supported java heap objects\n+  dump_java_heap_objects(builder.klasses());\n+\n+  builder.relocate_roots();\n+  dump_shared_symbol_table(builder.symbols());\n+\n+  builder.relocate_vm_classes();\n+\n+  log_info(cds)(\"Make classes shareable\");\n+  builder.make_klasses_shareable();\n+\n+  char* serialized_data = dump_read_only_tables();\n+\n+  SystemDictionaryShared::adjust_lambda_proxy_class_dictionary();\n+\n+  \/\/ The vtable clones contain addresses of the current process.\n+  \/\/ We don't want to write these addresses into the archive.\n+  CppVtables::zero_archived_vtables();\n+\n+  \/\/ relocate the data so that it can be mapped to MetaspaceShared::requested_base_address()\n+  \/\/ without runtime relocation.\n+  builder.relocate_to_requested();\n+\n+  \/\/ Write the archive file\n+  FileMapInfo* mapinfo = new FileMapInfo(true);\n+  mapinfo->populate_header(MetaspaceShared::core_region_alignment());\n+  mapinfo->set_serialized_data(serialized_data);\n+  mapinfo->set_cloned_vtables(cloned_vtables);\n+  mapinfo->open_for_write();\n+  builder.write_archive(mapinfo,\n+                        _closed_archive_heap_regions,\n+                        _open_archive_heap_regions,\n+                        _closed_archive_heap_oopmaps,\n+                        _open_archive_heap_oopmaps);\n+\n+  if (PrintSystemDictionaryAtExit) {\n+    SystemDictionary::print();\n+  }\n+\n+  if (AllowArchivingWithJavaAgent) {\n+    warning(\"This archive was created with AllowArchivingWithJavaAgent. It should be used \"\n+            \"for testing purposes only and should not be used in a production environment\");\n+  }\n+\n+  \/\/ There may be pending VM operations. We have changed some global states\n+  \/\/ (such as vmClasses::_klasses) that may cause these VM operations\n+  \/\/ to fail. For safety, forget these operations and exit the VM directly.\n+  vm_direct_exit(0);\n+}\n+\n+class CollectCLDClosure : public CLDClosure {\n+  GrowableArray<ClassLoaderData*> _loaded_cld;\n+public:\n+  CollectCLDClosure() {}\n+  ~CollectCLDClosure() {\n+    for (int i = 0; i < _loaded_cld.length(); i++) {\n+      ClassLoaderData* cld = _loaded_cld.at(i);\n+      cld->dec_keep_alive();\n+    }\n+  }\n+  void do_cld(ClassLoaderData* cld) {\n+    if (!cld->is_unloading()) {\n+      cld->inc_keep_alive();\n+      _loaded_cld.append(cld);\n+    }\n+  }\n+\n+  int nof_cld() const                { return _loaded_cld.length(); }\n+  ClassLoaderData* cld_at(int index) { return _loaded_cld.at(index); }\n+};\n+\n+\/\/ Check if a class or its super class\/interface is old.\n+bool MetaspaceShared::is_old_class(InstanceKlass* ik) {\n+  if (ik == NULL) {\n+    return false;\n+  }\n+  if (ik->major_version() < 50 \/*JAVA_6_VERSION*\/) {\n+    return true;\n+  }\n+  if (is_old_class(ik->java_super())) {\n+    return true;\n+  }\n+  Array<InstanceKlass*>* interfaces = ik->local_interfaces();\n+  int len = interfaces->length();\n+  for (int i = 0; i < len; i++) {\n+    if (is_old_class(interfaces->at(i))) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool MetaspaceShared::linking_required(InstanceKlass* ik) {\n+  \/\/ For static CDS dump, do not link old classes.\n+  \/\/ For dynamic CDS dump, only link classes loaded by the builtin class loaders.\n+  return DumpSharedSpaces ? !MetaspaceShared::is_old_class(ik) : !ik->is_shared_unregistered_class();\n+}\n+\n+bool MetaspaceShared::link_class_for_cds(InstanceKlass* ik, TRAPS) {\n+  \/\/ Link the class to cause the bytecodes to be rewritten and the\n+  \/\/ cpcache to be created. Class verification is done according\n+  \/\/ to -Xverify setting.\n+  bool res = MetaspaceShared::try_link_class(THREAD, ik);\n+\n+  if (DumpSharedSpaces) {\n+    \/\/ The following function is used to resolve all Strings in the statically\n+    \/\/ dumped classes to archive all the Strings. The archive heap is not supported\n+    \/\/ for the dynamic archive.\n+    ik->constants()->resolve_class_constants(CHECK_(false)); \/\/ may throw OOM when interning strings.\n+  }\n+  return res;\n+}\n+\n+void MetaspaceShared::link_and_cleanup_shared_classes(TRAPS) {\n+  \/\/ Collect all loaded ClassLoaderData.\n+  ResourceMark rm;\n+  CollectCLDClosure collect_cld;\n+  {\n+    \/\/ ClassLoaderDataGraph::loaded_cld_do requires ClassLoaderDataGraph_lock.\n+    \/\/ We cannot link the classes while holding this lock (or else we may run into deadlock).\n+    \/\/ Therefore, we need to first collect all the CLDs, and then link their classes after\n+    \/\/ releasing the lock.\n+    MutexLocker lock(ClassLoaderDataGraph_lock);\n+    ClassLoaderDataGraph::loaded_cld_do(&collect_cld);\n+  }\n+\n+  while (true) {\n+    bool has_linked = false;\n+    for (int i = 0; i < collect_cld.nof_cld(); i++) {\n+      ClassLoaderData* cld = collect_cld.cld_at(i);\n+      for (Klass* klass = cld->klasses(); klass != NULL; klass = klass->next_link()) {\n+        if (klass->is_instance_klass()) {\n+          InstanceKlass* ik = InstanceKlass::cast(klass);\n+          if (linking_required(ik)) {\n+            has_linked |= link_class_for_cds(ik, CHECK);\n+          }\n+        }\n+      }\n+    }\n+\n+    if (!has_linked) {\n+      break;\n+    }\n+    \/\/ Class linking includes verification which may load more classes.\n+    \/\/ Keep scanning until we have linked no more classes.\n+  }\n+}\n+\n+void MetaspaceShared::prepare_for_dumping() {\n+  Arguments::assert_is_dumping_archive();\n+  Arguments::check_unsupported_dumping_properties();\n+\n+  ClassLoader::initialize_shared_path(Thread::current());\n+}\n+\n+\/\/ Preload classes from a list, populate the shared spaces and dump to a\n+\/\/ file.\n+void MetaspaceShared::preload_and_dump() {\n+  EXCEPTION_MARK;\n+  ResourceMark rm(THREAD);\n+  preload_and_dump_impl(THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    if (PENDING_EXCEPTION->is_a(vmClasses::OutOfMemoryError_klass())) {\n+      vm_direct_exit(-1,  err_msg(\"Out of memory. Please run with a larger Java heap, current MaxHeapSize = \"\n+                                  SIZE_FORMAT \"M\", MaxHeapSize\/M));\n+    } else {\n+      log_error(cds)(\"%s: %s\", PENDING_EXCEPTION->klass()->external_name(),\n+                     java_lang_String::as_utf8_string(java_lang_Throwable::message(PENDING_EXCEPTION)));\n+      vm_direct_exit(-1, \"VM exits due to exception, use -Xlog:cds,exceptions=trace for detail\");\n+    }\n+  } else {\n+    \/\/ On success, the VM_PopulateDumpSharedSpace op should have\n+    \/\/ exited the VM.\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void MetaspaceShared::preload_classes(TRAPS) {\n+  char default_classlist[JVM_MAXPATHLEN];\n+  const char* classlist_path;\n+\n+  if (SharedClassListFile == NULL) {\n+    \/\/ Construct the path to the class list (in jre\/lib)\n+    \/\/ Walk up two directories from the location of the VM and\n+    \/\/ optionally tack on \"lib\" (depending on platform)\n+    os::jvm_path(default_classlist, sizeof(default_classlist));\n+    for (int i = 0; i < 3; i++) {\n+      char *end = strrchr(default_classlist, *os::file_separator());\n+      if (end != NULL) *end = '\\0';\n+    }\n+    int classlist_path_len = (int)strlen(default_classlist);\n+    if (classlist_path_len >= 3) {\n+      if (strcmp(default_classlist + classlist_path_len - 3, \"lib\") != 0) {\n+        if (classlist_path_len < JVM_MAXPATHLEN - 4) {\n+          jio_snprintf(default_classlist + classlist_path_len,\n+                       sizeof(default_classlist) - classlist_path_len,\n+                       \"%slib\", os::file_separator());\n+          classlist_path_len += 4;\n+        }\n+      }\n+    }\n+    if (classlist_path_len < JVM_MAXPATHLEN - 10) {\n+      jio_snprintf(default_classlist + classlist_path_len,\n+                   sizeof(default_classlist) - classlist_path_len,\n+                   \"%sclasslist\", os::file_separator());\n+    }\n+    classlist_path = default_classlist;\n+  } else {\n+    classlist_path = SharedClassListFile;\n+  }\n+\n+  log_info(cds)(\"Loading classes to share ...\");\n+  _has_error_classes = false;\n+  int class_count = parse_classlist(classlist_path, CHECK);\n+  if (ExtraSharedClassListFile) {\n+    class_count += parse_classlist(ExtraSharedClassListFile, CHECK);\n+  }\n+\n+  \/\/ Exercise the manifest processing code to ensure classes used by CDS at runtime\n+  \/\/ are always archived\n+  const char* dummy = \"Manifest-Version: 1.0\\n\";\n+  SystemDictionaryShared::create_jar_manifest(dummy, strlen(dummy), CHECK);\n+\n+  log_info(cds)(\"Loading classes to share: done.\");\n+  log_info(cds)(\"Shared spaces: preloaded %d classes\", class_count);\n+}\n+\n+void MetaspaceShared::preload_and_dump_impl(TRAPS) {\n+  preload_classes(CHECK);\n+\n+  if (SharedArchiveConfigFile) {\n+    log_info(cds)(\"Reading extra data from %s ...\", SharedArchiveConfigFile);\n+    read_extra_data(THREAD, SharedArchiveConfigFile);\n+    log_info(cds)(\"Reading extra data: done.\");\n+  }\n+\n+  if (LambdaFormInvokers::lambdaform_lines() != NULL) {\n+    log_info(cds)(\"Regenerate MethodHandle Holder classes...\");\n+    LambdaFormInvokers::regenerate_holder_classes(CHECK);\n+    log_info(cds)(\"Regenerate MethodHandle Holder classes done.\");\n+  }\n+\n+  HeapShared::init_for_dumping(CHECK);\n+\n+  \/\/ Rewrite and link classes\n+  log_info(cds)(\"Rewriting and linking classes ...\");\n+\n+  \/\/ Link any classes which got missed. This would happen if we have loaded classes that\n+  \/\/ were not explicitly specified in the classlist. E.g., if an interface implemented by class K\n+  \/\/ fails verification, all other interfaces that were not specified in the classlist but\n+  \/\/ are implemented by K are not verified.\n+  link_and_cleanup_shared_classes(CHECK);\n+  log_info(cds)(\"Rewriting and linking classes: done\");\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+    if (use_full_module_graph()) {\n+      HeapShared::reset_archived_object_states(CHECK);\n+    }\n+#endif\n+\n+  VM_PopulateDumpSharedSpace op;\n+  VMThread::execute(&op);\n+}\n+\n+\n+int MetaspaceShared::parse_classlist(const char* classlist_path, TRAPS) {\n+  ClassListParser parser(classlist_path);\n+  return parser.parse(THREAD); \/\/ returns the number of classes loaded.\n+}\n+\n+\/\/ Returns true if the class's status has changed.\n+bool MetaspaceShared::try_link_class(Thread* current, InstanceKlass* ik) {\n+  ExceptionMark em(current);\n+  Thread* THREAD = current; \/\/ For exception macros.\n+  Arguments::assert_is_dumping_archive();\n+  if (ik->is_loaded() && !ik->is_linked() && !MetaspaceShared::is_old_class(ik) &&\n+      !SystemDictionaryShared::has_class_failed_verification(ik)) {\n+    bool saved = BytecodeVerificationLocal;\n+    if (ik->is_shared_unregistered_class() && ik->class_loader() == NULL) {\n+      \/\/ The verification decision is based on BytecodeVerificationRemote\n+      \/\/ for non-system classes. Since we are using the NULL classloader\n+      \/\/ to load non-system classes for customized class loaders during dumping,\n+      \/\/ we need to temporarily change BytecodeVerificationLocal to be the same as\n+      \/\/ BytecodeVerificationRemote. Note this can cause the parent system\n+      \/\/ classes also being verified. The extra overhead is acceptable during\n+      \/\/ dumping.\n+      BytecodeVerificationLocal = BytecodeVerificationRemote;\n+    }\n+    ik->link_class(THREAD);\n+    if (HAS_PENDING_EXCEPTION) {\n+      ResourceMark rm(THREAD);\n+      log_warning(cds)(\"Preload Warning: Verification failed for %s\",\n+                    ik->external_name());\n+      CLEAR_PENDING_EXCEPTION;\n+      SystemDictionaryShared::set_class_has_failed_verification(ik);\n+      _has_error_classes = true;\n+    }\n+    BytecodeVerificationLocal = saved;\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+void VM_PopulateDumpSharedSpace::dump_java_heap_objects(GrowableArray<Klass*>* klasses) {\n+  if(!HeapShared::is_heap_object_archiving_allowed()) {\n+    log_info(cds)(\n+      \"Archived java heap is not supported as UseG1GC, \"\n+      \"UseCompressedOops and UseCompressedClassPointers are required.\"\n+      \"Current settings: UseG1GC=%s, UseCompressedOops=%s, UseCompressedClassPointers=%s.\",\n+      BOOL_TO_STR(UseG1GC), BOOL_TO_STR(UseCompressedOops),\n+      BOOL_TO_STR(UseCompressedClassPointers));\n+    return;\n+  }\n+  \/\/ Find all the interned strings that should be dumped.\n+  int i;\n+  for (i = 0; i < klasses->length(); i++) {\n+    Klass* k = klasses->at(i);\n+    if (k->is_instance_klass()) {\n+      InstanceKlass* ik = InstanceKlass::cast(k);\n+      if (ik->is_linked()) {\n+        ik->constants()->add_dumped_interned_strings();\n+      }\n+    }\n+  }\n+  if (_extra_interned_strings != NULL) {\n+    for (i = 0; i < _extra_interned_strings->length(); i ++) {\n+      OopHandle string = _extra_interned_strings->at(i);\n+      HeapShared::add_to_dumped_interned_strings(string.resolve());\n+    }\n+  }\n+\n+  \/\/ The closed and open archive heap space has maximum two regions.\n+  \/\/ See FileMapInfo::write_archive_heap_regions() for details.\n+  _closed_archive_heap_regions = new GrowableArray<MemRegion>(2);\n+  _open_archive_heap_regions = new GrowableArray<MemRegion>(2);\n+  HeapShared::archive_java_heap_objects(_closed_archive_heap_regions,\n+                                        _open_archive_heap_regions);\n+  ArchiveBuilder::OtherROAllocMark mark;\n+  HeapShared::write_subgraph_info_table();\n+}\n+\n+void VM_PopulateDumpSharedSpace::dump_archive_heap_oopmaps() {\n+  if (HeapShared::is_heap_object_archiving_allowed()) {\n+    _closed_archive_heap_oopmaps = new GrowableArray<ArchiveHeapOopmapInfo>(2);\n+    dump_archive_heap_oopmaps(_closed_archive_heap_regions, _closed_archive_heap_oopmaps);\n+\n+    _open_archive_heap_oopmaps = new GrowableArray<ArchiveHeapOopmapInfo>(2);\n+    dump_archive_heap_oopmaps(_open_archive_heap_regions, _open_archive_heap_oopmaps);\n+  }\n+}\n+\n+void VM_PopulateDumpSharedSpace::dump_archive_heap_oopmaps(GrowableArray<MemRegion>* regions,\n+                                                           GrowableArray<ArchiveHeapOopmapInfo>* oopmaps) {\n+  for (int i=0; i<regions->length(); i++) {\n+    ResourceBitMap oopmap = HeapShared::calculate_oopmap(regions->at(i));\n+    size_t size_in_bits = oopmap.size();\n+    size_t size_in_bytes = oopmap.size_in_bytes();\n+    uintptr_t* buffer = (uintptr_t*)NEW_C_HEAP_ARRAY(char, size_in_bytes, mtInternal);\n+    oopmap.write_to(buffer, size_in_bytes);\n+    log_info(cds, heap)(\"Oopmap = \" INTPTR_FORMAT \" (\" SIZE_FORMAT_W(6) \" bytes) for heap region \"\n+                        INTPTR_FORMAT \" (\" SIZE_FORMAT_W(8) \" bytes)\",\n+                        p2i(buffer), size_in_bytes,\n+                        p2i(regions->at(i).start()), regions->at(i).byte_size());\n+\n+    ArchiveHeapOopmapInfo info;\n+    info._oopmap = (address)buffer;\n+    info._oopmap_size_in_bits = size_in_bits;\n+    info._oopmap_size_in_bytes = size_in_bytes;\n+    oopmaps->append(info);\n+  }\n+}\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n+\n+void MetaspaceShared::set_shared_metaspace_range(void* base, void *static_top, void* top) {\n+  assert(base <= static_top && static_top <= top, \"must be\");\n+  _shared_metaspace_static_top = static_top;\n+  MetaspaceObj::set_shared_metaspace_range(base, top);\n+}\n+\n+\/\/ Return true if given address is in the misc data region\n+bool MetaspaceShared::is_in_shared_region(const void* p, int idx) {\n+  return UseSharedSpaces && FileMapInfo::current_info()->is_in_shared_region(p, idx);\n+}\n+\n+bool MetaspaceShared::is_shared_dynamic(void* p) {\n+  if ((p < MetaspaceObj::shared_metaspace_top()) &&\n+      (p >= _shared_metaspace_static_top)) {\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+void MetaspaceShared::initialize_runtime_shared_and_meta_spaces() {\n+  assert(UseSharedSpaces, \"Must be called when UseSharedSpaces is enabled\");\n+  MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;\n+\n+  FileMapInfo* static_mapinfo = open_static_archive();\n+  FileMapInfo* dynamic_mapinfo = NULL;\n+\n+  if (static_mapinfo != NULL) {\n+    log_info(cds)(\"Core region alignment: \" SIZE_FORMAT, static_mapinfo->core_region_alignment());\n+    dynamic_mapinfo = open_dynamic_archive();\n+\n+    \/\/ First try to map at the requested address\n+    result = map_archives(static_mapinfo, dynamic_mapinfo, true);\n+    if (result == MAP_ARCHIVE_MMAP_FAILURE) {\n+      \/\/ Mapping has failed (probably due to ASLR). Let's map at an address chosen\n+      \/\/ by the OS.\n+      log_info(cds)(\"Try to map archive(s) at an alternative address\");\n+      result = map_archives(static_mapinfo, dynamic_mapinfo, false);\n+    }\n+  }\n+\n+  if (result == MAP_ARCHIVE_SUCCESS) {\n+    bool dynamic_mapped = (dynamic_mapinfo != NULL && dynamic_mapinfo->is_mapped());\n+    char* cds_base = static_mapinfo->mapped_base();\n+    char* cds_end =  dynamic_mapped ? dynamic_mapinfo->mapped_end() : static_mapinfo->mapped_end();\n+    set_shared_metaspace_range(cds_base, static_mapinfo->mapped_end(), cds_end);\n+    _relocation_delta = static_mapinfo->relocation_delta();\n+    _requested_base_address = static_mapinfo->requested_base_address();\n+    if (dynamic_mapped) {\n+      FileMapInfo::set_shared_path_table(dynamic_mapinfo);\n+    } else {\n+      FileMapInfo::set_shared_path_table(static_mapinfo);\n+    }\n+  } else {\n+    set_shared_metaspace_range(NULL, NULL, NULL);\n+    UseSharedSpaces = false;\n+    FileMapInfo::fail_continue(\"Unable to map shared spaces\");\n+    if (PrintSharedArchiveAndExit) {\n+      vm_exit_during_initialization(\"Unable to use shared archive.\");\n+    }\n+  }\n+\n+  if (static_mapinfo != NULL && !static_mapinfo->is_mapped()) {\n+    delete static_mapinfo;\n+  }\n+  if (dynamic_mapinfo != NULL && !dynamic_mapinfo->is_mapped()) {\n+    delete dynamic_mapinfo;\n+  }\n+}\n+\n+FileMapInfo* MetaspaceShared::open_static_archive() {\n+  FileMapInfo* mapinfo = new FileMapInfo(true);\n+  if (!mapinfo->initialize()) {\n+    delete(mapinfo);\n+    return NULL;\n+  }\n+  return mapinfo;\n+}\n+\n+FileMapInfo* MetaspaceShared::open_dynamic_archive() {\n+  if (DynamicDumpSharedSpaces) {\n+    return NULL;\n+  }\n+  if (Arguments::GetSharedDynamicArchivePath() == NULL) {\n+    return NULL;\n+  }\n+\n+  FileMapInfo* mapinfo = new FileMapInfo(false);\n+  if (!mapinfo->initialize()) {\n+    delete(mapinfo);\n+    return NULL;\n+  }\n+  return mapinfo;\n+}\n+\n+\/\/ use_requested_addr:\n+\/\/  true  = map at FileMapHeader::_requested_base_address\n+\/\/  false = map at an alternative address picked by OS.\n+MapArchiveResult MetaspaceShared::map_archives(FileMapInfo* static_mapinfo, FileMapInfo* dynamic_mapinfo,\n+                                               bool use_requested_addr) {\n+  if (use_requested_addr && static_mapinfo->requested_base_address() == NULL) {\n+    log_info(cds)(\"Archive(s) were created with -XX:SharedBaseAddress=0. Always map at os-selected address.\");\n+    return MAP_ARCHIVE_MMAP_FAILURE;\n+  }\n+\n+  PRODUCT_ONLY(if (ArchiveRelocationMode == 1 && use_requested_addr) {\n+      \/\/ For product build only -- this is for benchmarking the cost of doing relocation.\n+      \/\/ For debug builds, the check is done below, after reserving the space, for better test coverage\n+      \/\/ (see comment below).\n+      log_info(cds)(\"ArchiveRelocationMode == 1: always map archive(s) at an alternative address\");\n+      return MAP_ARCHIVE_MMAP_FAILURE;\n+    });\n+\n+  if (ArchiveRelocationMode == 2 && !use_requested_addr) {\n+    log_info(cds)(\"ArchiveRelocationMode == 2: never map archive(s) at an alternative address\");\n+    return MAP_ARCHIVE_MMAP_FAILURE;\n+  };\n+\n+  if (dynamic_mapinfo != NULL) {\n+    \/\/ Ensure that the OS won't be able to allocate new memory spaces between the two\n+    \/\/ archives, or else it would mess up the simple comparision in MetaspaceObj::is_shared().\n+    assert(static_mapinfo->mapping_end_offset() == dynamic_mapinfo->mapping_base_offset(), \"no gap\");\n+  }\n+\n+  ReservedSpace total_space_rs, archive_space_rs, class_space_rs;\n+  MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;\n+  char* mapped_base_address = reserve_address_space_for_archives(static_mapinfo,\n+                                                                 dynamic_mapinfo,\n+                                                                 use_requested_addr,\n+                                                                 total_space_rs,\n+                                                                 archive_space_rs,\n+                                                                 class_space_rs);\n+  if (mapped_base_address == NULL) {\n+    result = MAP_ARCHIVE_MMAP_FAILURE;\n+    log_debug(cds)(\"Failed to reserve spaces (use_requested_addr=%u)\", (unsigned)use_requested_addr);\n+  } else {\n+\n+#ifdef ASSERT\n+    \/\/ Some sanity checks after reserving address spaces for archives\n+    \/\/  and class space.\n+    assert(archive_space_rs.is_reserved(), \"Sanity\");\n+    if (Metaspace::using_class_space()) {\n+      \/\/ Class space must closely follow the archive space. Both spaces\n+      \/\/  must be aligned correctly.\n+      assert(class_space_rs.is_reserved(),\n+             \"A class space should have been reserved\");\n+      assert(class_space_rs.base() >= archive_space_rs.end(),\n+             \"class space should follow the cds archive space\");\n+      assert(is_aligned(archive_space_rs.base(),\n+                        core_region_alignment()),\n+             \"Archive space misaligned\");\n+      assert(is_aligned(class_space_rs.base(),\n+                        Metaspace::reserve_alignment()),\n+             \"class space misaligned\");\n+    }\n+#endif \/\/ ASSERT\n+\n+    log_info(cds)(\"Reserved archive_space_rs [\" INTPTR_FORMAT \" - \" INTPTR_FORMAT \"] (\" SIZE_FORMAT \") bytes\",\n+                   p2i(archive_space_rs.base()), p2i(archive_space_rs.end()), archive_space_rs.size());\n+    log_info(cds)(\"Reserved class_space_rs   [\" INTPTR_FORMAT \" - \" INTPTR_FORMAT \"] (\" SIZE_FORMAT \") bytes\",\n+                   p2i(class_space_rs.base()), p2i(class_space_rs.end()), class_space_rs.size());\n+\n+    if (MetaspaceShared::use_windows_memory_mapping()) {\n+      \/\/ We have now reserved address space for the archives, and will map in\n+      \/\/  the archive files into this space.\n+      \/\/\n+      \/\/ Special handling for Windows: on Windows we cannot map a file view\n+      \/\/  into an existing memory mapping. So, we unmap the address range we\n+      \/\/  just reserved again, which will make it available for mapping the\n+      \/\/  archives.\n+      \/\/ Reserving this range has not been for naught however since it makes\n+      \/\/  us reasonably sure the address range is available.\n+      \/\/\n+      \/\/ But still it may fail, since between unmapping the range and mapping\n+      \/\/  in the archive someone else may grab the address space. Therefore\n+      \/\/  there is a fallback in FileMap::map_region() where we just read in\n+      \/\/  the archive files sequentially instead of mapping it in. We couple\n+      \/\/  this with use_requested_addr, since we're going to patch all the\n+      \/\/  pointers anyway so there's no benefit to mmap.\n+      if (use_requested_addr) {\n+        assert(!total_space_rs.is_reserved(), \"Should not be reserved for Windows\");\n+        log_info(cds)(\"Windows mmap workaround: releasing archive space.\");\n+        archive_space_rs.release();\n+      }\n+    }\n+    MapArchiveResult static_result = map_archive(static_mapinfo, mapped_base_address, archive_space_rs);\n+    MapArchiveResult dynamic_result = (static_result == MAP_ARCHIVE_SUCCESS) ?\n+                                     map_archive(dynamic_mapinfo, mapped_base_address, archive_space_rs) : MAP_ARCHIVE_OTHER_FAILURE;\n+\n+    DEBUG_ONLY(if (ArchiveRelocationMode == 1 && use_requested_addr) {\n+      \/\/ This is for simulating mmap failures at the requested address. In\n+      \/\/  debug builds, we do it here (after all archives have possibly been\n+      \/\/  mapped), so we can thoroughly test the code for failure handling\n+      \/\/  (releasing all allocated resource, etc).\n+      log_info(cds)(\"ArchiveRelocationMode == 1: always map archive(s) at an alternative address\");\n+      if (static_result == MAP_ARCHIVE_SUCCESS) {\n+        static_result = MAP_ARCHIVE_MMAP_FAILURE;\n+      }\n+      if (dynamic_result == MAP_ARCHIVE_SUCCESS) {\n+        dynamic_result = MAP_ARCHIVE_MMAP_FAILURE;\n+      }\n+    });\n+\n+    if (static_result == MAP_ARCHIVE_SUCCESS) {\n+      if (dynamic_result == MAP_ARCHIVE_SUCCESS) {\n+        result = MAP_ARCHIVE_SUCCESS;\n+      } else if (dynamic_result == MAP_ARCHIVE_OTHER_FAILURE) {\n+        assert(dynamic_mapinfo != NULL && !dynamic_mapinfo->is_mapped(), \"must have failed\");\n+        \/\/ No need to retry mapping the dynamic archive again, as it will never succeed\n+        \/\/ (bad file, etc) -- just keep the base archive.\n+        log_warning(cds, dynamic)(\"Unable to use shared archive. The top archive failed to load: %s\",\n+                                  dynamic_mapinfo->full_path());\n+        result = MAP_ARCHIVE_SUCCESS;\n+        \/\/ TODO, we can give the unused space for the dynamic archive to class_space_rs, but there's no\n+        \/\/ easy API to do that right now.\n+      } else {\n+        result = MAP_ARCHIVE_MMAP_FAILURE;\n+      }\n+    } else if (static_result == MAP_ARCHIVE_OTHER_FAILURE) {\n+      result = MAP_ARCHIVE_OTHER_FAILURE;\n+    } else {\n+      result = MAP_ARCHIVE_MMAP_FAILURE;\n+    }\n+  }\n+\n+  if (result == MAP_ARCHIVE_SUCCESS) {\n+    SharedBaseAddress = (size_t)mapped_base_address;\n+    LP64_ONLY({\n+        if (Metaspace::using_class_space()) {\n+          \/\/ Set up ccs in metaspace.\n+          Metaspace::initialize_class_space(class_space_rs);\n+\n+          \/\/ Set up compressed Klass pointer encoding: the encoding range must\n+          \/\/  cover both archive and class space.\n+          address cds_base = (address)static_mapinfo->mapped_base();\n+          address ccs_end = (address)class_space_rs.end();\n+          assert(ccs_end > cds_base, \"Sanity check\");\n+          CompressedKlassPointers::initialize(cds_base, ccs_end - cds_base);\n+\n+          \/\/ map_heap_regions() compares the current narrow oop and klass encodings\n+          \/\/ with the archived ones, so it must be done after all encodings are determined.\n+          static_mapinfo->map_heap_regions();\n+        }\n+      });\n+    log_info(cds)(\"optimized module handling: %s\", MetaspaceShared::use_optimized_module_handling() ? \"enabled\" : \"disabled\");\n+    log_info(cds)(\"full module graph: %s\", MetaspaceShared::use_full_module_graph() ? \"enabled\" : \"disabled\");\n+  } else {\n+    unmap_archive(static_mapinfo);\n+    unmap_archive(dynamic_mapinfo);\n+    release_reserved_spaces(total_space_rs, archive_space_rs, class_space_rs);\n+  }\n+\n+  return result;\n+}\n+\n+\n+\/\/ This will reserve two address spaces suitable to house Klass structures, one\n+\/\/  for the cds archives (static archive and optionally dynamic archive) and\n+\/\/  optionally one move for ccs.\n+\/\/\n+\/\/ Since both spaces must fall within the compressed class pointer encoding\n+\/\/  range, they are allocated close to each other.\n+\/\/\n+\/\/ Space for archives will be reserved first, followed by a potential gap,\n+\/\/  followed by the space for ccs:\n+\/\/\n+\/\/ +-- Base address             A        B                     End\n+\/\/ |                            |        |                      |\n+\/\/ v                            v        v                      v\n+\/\/ +-------------+--------------+        +----------------------+\n+\/\/ | static arc  | [dyn. arch]  | [gap]  | compr. class space   |\n+\/\/ +-------------+--------------+        +----------------------+\n+\/\/\n+\/\/ (The gap may result from different alignment requirements between metaspace\n+\/\/  and CDS)\n+\/\/\n+\/\/ If UseCompressedClassPointers is disabled, only one address space will be\n+\/\/  reserved:\n+\/\/\n+\/\/ +-- Base address             End\n+\/\/ |                            |\n+\/\/ v                            v\n+\/\/ +-------------+--------------+\n+\/\/ | static arc  | [dyn. arch]  |\n+\/\/ +-------------+--------------+\n+\/\/\n+\/\/ Base address: If use_archive_base_addr address is true, the Base address is\n+\/\/  determined by the address stored in the static archive. If\n+\/\/  use_archive_base_addr address is false, this base address is determined\n+\/\/  by the platform.\n+\/\/\n+\/\/ If UseCompressedClassPointers=1, the range encompassing both spaces will be\n+\/\/  suitable to en\/decode narrow Klass pointers: the base will be valid for\n+\/\/  encoding, the range [Base, End) not surpass KlassEncodingMetaspaceMax.\n+\/\/\n+\/\/ Return:\n+\/\/\n+\/\/ - On success:\n+\/\/    - total_space_rs will be reserved as whole for archive_space_rs and\n+\/\/      class_space_rs if UseCompressedClassPointers is true.\n+\/\/      On Windows, try reserve archive_space_rs and class_space_rs\n+\/\/      separately first if use_archive_base_addr is true.\n+\/\/    - archive_space_rs will be reserved and large enough to host static and\n+\/\/      if needed dynamic archive: [Base, A).\n+\/\/      archive_space_rs.base and size will be aligned to CDS reserve\n+\/\/      granularity.\n+\/\/    - class_space_rs: If UseCompressedClassPointers=1, class_space_rs will\n+\/\/      be reserved. Its start address will be aligned to metaspace reserve\n+\/\/      alignment, which may differ from CDS alignment. It will follow the cds\n+\/\/      archive space, close enough such that narrow class pointer encoding\n+\/\/      covers both spaces.\n+\/\/      If UseCompressedClassPointers=0, class_space_rs remains unreserved.\n+\/\/ - On error: NULL is returned and the spaces remain unreserved.\n+char* MetaspaceShared::reserve_address_space_for_archives(FileMapInfo* static_mapinfo,\n+                                                          FileMapInfo* dynamic_mapinfo,\n+                                                          bool use_archive_base_addr,\n+                                                          ReservedSpace& total_space_rs,\n+                                                          ReservedSpace& archive_space_rs,\n+                                                          ReservedSpace& class_space_rs) {\n+\n+  address const base_address = (address) (use_archive_base_addr ? static_mapinfo->requested_base_address() : NULL);\n+  const size_t archive_space_alignment = core_region_alignment();\n+\n+  \/\/ Size and requested location of the archive_space_rs (for both static and dynamic archives)\n+  assert(static_mapinfo->mapping_base_offset() == 0, \"Must be\");\n+  size_t archive_end_offset  = (dynamic_mapinfo == NULL) ? static_mapinfo->mapping_end_offset() : dynamic_mapinfo->mapping_end_offset();\n+  size_t archive_space_size = align_up(archive_end_offset, archive_space_alignment);\n+\n+  \/\/ If a base address is given, it must have valid alignment and be suitable as encoding base.\n+  if (base_address != NULL) {\n+    assert(is_aligned(base_address, archive_space_alignment),\n+           \"Archive base address invalid: \" PTR_FORMAT \".\", p2i(base_address));\n+    if (Metaspace::using_class_space()) {\n+      assert(CompressedKlassPointers::is_valid_base(base_address),\n+             \"Archive base address invalid: \" PTR_FORMAT \".\", p2i(base_address));\n+    }\n+  }\n+\n+  if (!Metaspace::using_class_space()) {\n+    \/\/ Get the simple case out of the way first:\n+    \/\/ no compressed class space, simple allocation.\n+    archive_space_rs = ReservedSpace(archive_space_size, archive_space_alignment,\n+                                     os::vm_page_size(), (char*)base_address);\n+    if (archive_space_rs.is_reserved()) {\n+      assert(base_address == NULL ||\n+             (address)archive_space_rs.base() == base_address, \"Sanity\");\n+      \/\/ Register archive space with NMT.\n+      MemTracker::record_virtual_memory_type(archive_space_rs.base(), mtClassShared);\n+      return archive_space_rs.base();\n+    }\n+    return NULL;\n+  }\n+\n+#ifdef _LP64\n+\n+  \/\/ Complex case: two spaces adjacent to each other, both to be addressable\n+  \/\/  with narrow class pointers.\n+  \/\/ We reserve the whole range spanning both spaces, then split that range up.\n+\n+  const size_t class_space_alignment = Metaspace::reserve_alignment();\n+\n+  \/\/ To simplify matters, lets assume that metaspace alignment will always be\n+  \/\/  equal or a multiple of archive alignment.\n+  assert(is_power_of_2(class_space_alignment) &&\n+                       is_power_of_2(archive_space_alignment) &&\n+                       class_space_alignment >= archive_space_alignment,\n+                       \"Sanity\");\n+\n+  const size_t class_space_size = CompressedClassSpaceSize;\n+  assert(CompressedClassSpaceSize > 0 &&\n+         is_aligned(CompressedClassSpaceSize, class_space_alignment),\n+         \"CompressedClassSpaceSize malformed: \"\n+         SIZE_FORMAT, CompressedClassSpaceSize);\n+\n+  const size_t ccs_begin_offset = align_up(base_address + archive_space_size,\n+                                           class_space_alignment) - base_address;\n+  const size_t gap_size = ccs_begin_offset - archive_space_size;\n+\n+  const size_t total_range_size =\n+      align_up(archive_space_size + gap_size + class_space_size, core_region_alignment());\n+\n+  assert(total_range_size > ccs_begin_offset, \"must be\");\n+  if (use_windows_memory_mapping() && use_archive_base_addr) {\n+    if (base_address != nullptr) {\n+      \/\/ On Windows, we cannot safely split a reserved memory space into two (see JDK-8255917).\n+      \/\/ Hence, we optimistically reserve archive space and class space side-by-side. We only\n+      \/\/ do this for use_archive_base_addr=true since for use_archive_base_addr=false case\n+      \/\/ caller will not split the combined space for mapping, instead read the archive data\n+      \/\/ via sequential file IO.\n+      address ccs_base = base_address + archive_space_size + gap_size;\n+      archive_space_rs = ReservedSpace(archive_space_size, archive_space_alignment,\n+                                       os::vm_page_size(), (char*)base_address);\n+      class_space_rs   = ReservedSpace(class_space_size, class_space_alignment,\n+                                       os::vm_page_size(), (char*)ccs_base);\n+    }\n+    if (!archive_space_rs.is_reserved() || !class_space_rs.is_reserved()) {\n+      release_reserved_spaces(total_space_rs, archive_space_rs, class_space_rs);\n+      return NULL;\n+    }\n+  } else {\n+    if (use_archive_base_addr && base_address != nullptr) {\n+      total_space_rs = ReservedSpace(total_range_size, archive_space_alignment,\n+                                     os::vm_page_size(), (char*) base_address);\n+    } else {\n+      \/\/ Reserve at any address, but leave it up to the platform to choose a good one.\n+      total_space_rs = Metaspace::reserve_address_space_for_compressed_classes(total_range_size);\n+    }\n+\n+    if (!total_space_rs.is_reserved()) {\n+      return NULL;\n+    }\n+\n+    \/\/ Paranoid checks:\n+    assert(base_address == NULL || (address)total_space_rs.base() == base_address,\n+           \"Sanity (\" PTR_FORMAT \" vs \" PTR_FORMAT \")\", p2i(base_address), p2i(total_space_rs.base()));\n+    assert(is_aligned(total_space_rs.base(), archive_space_alignment), \"Sanity\");\n+    assert(total_space_rs.size() == total_range_size, \"Sanity\");\n+    assert(CompressedKlassPointers::is_valid_base((address)total_space_rs.base()), \"Sanity\");\n+\n+    \/\/ Now split up the space into ccs and cds archive. For simplicity, just leave\n+    \/\/  the gap reserved at the end of the archive space. Do not do real splitting.\n+    archive_space_rs = total_space_rs.first_part(ccs_begin_offset,\n+                                                 (size_t)archive_space_alignment);\n+    class_space_rs = total_space_rs.last_part(ccs_begin_offset);\n+    MemTracker::record_virtual_memory_split_reserved(total_space_rs.base(), total_space_rs.size(),\n+                                                     ccs_begin_offset);\n+  }\n+  assert(is_aligned(archive_space_rs.base(), archive_space_alignment), \"Sanity\");\n+  assert(is_aligned(archive_space_rs.size(), archive_space_alignment), \"Sanity\");\n+  assert(is_aligned(class_space_rs.base(), class_space_alignment), \"Sanity\");\n+  assert(is_aligned(class_space_rs.size(), class_space_alignment), \"Sanity\");\n+\n+  \/\/ NMT: fix up the space tags\n+  MemTracker::record_virtual_memory_type(archive_space_rs.base(), mtClassShared);\n+  MemTracker::record_virtual_memory_type(class_space_rs.base(), mtClass);\n+\n+  return archive_space_rs.base();\n+\n+#else\n+  ShouldNotReachHere();\n+  return NULL;\n+#endif\n+\n+}\n+\n+void MetaspaceShared::release_reserved_spaces(ReservedSpace& total_space_rs,\n+                                              ReservedSpace& archive_space_rs,\n+                                              ReservedSpace& class_space_rs) {\n+  if (total_space_rs.is_reserved()) {\n+    log_debug(cds)(\"Released shared space (archive + class) \" INTPTR_FORMAT, p2i(total_space_rs.base()));\n+    total_space_rs.release();\n+  } else {\n+    if (archive_space_rs.is_reserved()) {\n+      log_debug(cds)(\"Released shared space (archive) \" INTPTR_FORMAT, p2i(archive_space_rs.base()));\n+      archive_space_rs.release();\n+    }\n+    if (class_space_rs.is_reserved()) {\n+      log_debug(cds)(\"Released shared space (classes) \" INTPTR_FORMAT, p2i(class_space_rs.base()));\n+      class_space_rs.release();\n+    }\n+  }\n+}\n+\n+static int archive_regions[]     = { MetaspaceShared::rw, MetaspaceShared::ro };\n+static int archive_regions_count = 2;\n+\n+MapArchiveResult MetaspaceShared::map_archive(FileMapInfo* mapinfo, char* mapped_base_address, ReservedSpace rs) {\n+  assert(UseSharedSpaces, \"must be runtime\");\n+  if (mapinfo == NULL) {\n+    return MAP_ARCHIVE_SUCCESS; \/\/ The dynamic archive has not been specified. No error has happened -- trivially succeeded.\n+  }\n+\n+  mapinfo->set_is_mapped(false);\n+  if (mapinfo->core_region_alignment() != (size_t)core_region_alignment()) {\n+    log_info(cds)(\"Unable to map CDS archive -- core_region_alignment() expected: \" SIZE_FORMAT\n+                  \" actual: \" SIZE_FORMAT, mapinfo->core_region_alignment(), core_region_alignment());\n+    return MAP_ARCHIVE_OTHER_FAILURE;\n+  }\n+\n+  MapArchiveResult result =\n+    mapinfo->map_regions(archive_regions, archive_regions_count, mapped_base_address, rs);\n+\n+  if (result != MAP_ARCHIVE_SUCCESS) {\n+    unmap_archive(mapinfo);\n+    return result;\n+  }\n+\n+  if (!mapinfo->validate_shared_path_table()) {\n+    unmap_archive(mapinfo);\n+    return MAP_ARCHIVE_OTHER_FAILURE;\n+  }\n+\n+  mapinfo->set_is_mapped(true);\n+  return MAP_ARCHIVE_SUCCESS;\n+}\n+\n+void MetaspaceShared::unmap_archive(FileMapInfo* mapinfo) {\n+  assert(UseSharedSpaces, \"must be runtime\");\n+  if (mapinfo != NULL) {\n+    mapinfo->unmap_regions(archive_regions, archive_regions_count);\n+    mapinfo->unmap_region(MetaspaceShared::bm);\n+    mapinfo->set_is_mapped(false);\n+  }\n+}\n+\n+\/\/ For -XX:PrintSharedArchiveAndExit\n+class CountSharedSymbols : public SymbolClosure {\n+ private:\n+   int _count;\n+ public:\n+   CountSharedSymbols() : _count(0) {}\n+  void do_symbol(Symbol** sym) {\n+    _count++;\n+  }\n+  int total() { return _count; }\n+\n+};\n+\n+\/\/ For -XX:PrintSharedArchiveAndExit\n+class CountSharedStrings : public OopClosure {\n+ private:\n+  int _count;\n+ public:\n+  CountSharedStrings() : _count(0) {}\n+  void do_oop(oop* p) {\n+    _count++;\n+  }\n+  void do_oop(narrowOop* p) {\n+    _count++;\n+  }\n+  int total() { return _count; }\n+};\n+\n+\/\/ Read the miscellaneous data from the shared file, and\n+\/\/ serialize it out to its various destinations.\n+\n+void MetaspaceShared::initialize_shared_spaces() {\n+  FileMapInfo *static_mapinfo = FileMapInfo::current_info();\n+\n+  \/\/ Verify various attributes of the archive, plus initialize the\n+  \/\/ shared string\/symbol tables\n+  char* buffer = static_mapinfo->serialized_data();\n+  intptr_t* array = (intptr_t*)buffer;\n+  ReadClosure rc(&array);\n+  serialize(&rc);\n+\n+  \/\/ Initialize the run-time symbol table.\n+  SymbolTable::create_table();\n+\n+  static_mapinfo->patch_archived_heap_embedded_pointers();\n+\n+  \/\/ Close the mapinfo file\n+  static_mapinfo->close();\n+\n+  static_mapinfo->unmap_region(MetaspaceShared::bm);\n+\n+  FileMapInfo *dynamic_mapinfo = FileMapInfo::dynamic_info();\n+  if (dynamic_mapinfo != NULL) {\n+    intptr_t* buffer = (intptr_t*)dynamic_mapinfo->serialized_data();\n+    ReadClosure rc(&buffer);\n+    SymbolTable::serialize_shared_table_header(&rc, false);\n+    SystemDictionaryShared::serialize_dictionary_headers(&rc, false);\n+    dynamic_mapinfo->close();\n+    dynamic_mapinfo->unmap_region(MetaspaceShared::bm);\n+  }\n+\n+  \/\/ Set up LambdaFormInvokers::_lambdaform_lines for dynamic dump\n+  if (DynamicDumpSharedSpaces) {\n+    \/\/ Read stored LF format lines stored in static archive\n+    LambdaFormInvokers::read_static_archive_invokers();\n+  }\n+\n+  if (PrintSharedArchiveAndExit) {\n+    \/\/ Print archive names\n+    if (dynamic_mapinfo != nullptr) {\n+      tty->print_cr(\"\\n\\nBase archive name: %s\", Arguments::GetSharedArchivePath());\n+      tty->print_cr(\"Base archive version %d\", static_mapinfo->version());\n+    } else {\n+      tty->print_cr(\"Static archive name: %s\", static_mapinfo->full_path());\n+      tty->print_cr(\"Static archive version %d\", static_mapinfo->version());\n+    }\n+\n+    SystemDictionaryShared::print_shared_archive(tty);\n+    if (dynamic_mapinfo != nullptr) {\n+      tty->print_cr(\"\\n\\nDynamic archive name: %s\", dynamic_mapinfo->full_path());\n+      tty->print_cr(\"Dynamic archive version %d\", dynamic_mapinfo->version());\n+      SystemDictionaryShared::print_shared_archive(tty, false\/*dynamic*\/);\n+    }\n+\n+    \/\/ collect shared symbols and strings\n+    CountSharedSymbols cl;\n+    SymbolTable::shared_symbols_do(&cl);\n+    tty->print_cr(\"Number of shared symbols: %d\", cl.total());\n+    CountSharedStrings cs;\n+    StringTable::shared_oops_do(&cs);\n+    tty->print_cr(\"Number of shared strings: %d\", cs.total());\n+    tty->print_cr(\"VM version: %s\\r\\n\", static_mapinfo->vm_version());\n+    if (FileMapInfo::current_info() == NULL || _archive_loading_failed) {\n+      tty->print_cr(\"archive is invalid\");\n+      vm_exit(1);\n+    } else {\n+      tty->print_cr(\"archive is valid\");\n+      vm_exit(0);\n+    }\n+  }\n+}\n+\n+\/\/ JVM\/TI RedefineClasses() support:\n+bool MetaspaceShared::remap_shared_readonly_as_readwrite() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"must be at safepoint\");\n+\n+  if (UseSharedSpaces) {\n+    \/\/ remap the shared readonly space to shared readwrite, private\n+    FileMapInfo* mapinfo = FileMapInfo::current_info();\n+    if (!mapinfo->remap_shared_readonly_as_readwrite()) {\n+      return false;\n+    }\n+    if (FileMapInfo::dynamic_info() != NULL) {\n+      mapinfo = FileMapInfo::dynamic_info();\n+      if (!mapinfo->remap_shared_readonly_as_readwrite()) {\n+        return false;\n+      }\n+    }\n+    _remapped_readwrite = true;\n+  }\n+  return true;\n+}\n+\n+bool MetaspaceShared::use_full_module_graph() {\n+#if INCLUDE_CDS_JAVA_HEAP\n+  if (ClassLoaderDataShared::is_full_module_graph_loaded()) {\n+    return true;\n+  }\n+#endif\n+  bool result = _use_optimized_module_handling && _use_full_module_graph &&\n+    (UseSharedSpaces || DumpSharedSpaces) && HeapShared::is_heap_object_archiving_allowed();\n+  if (result && UseSharedSpaces) {\n+    \/\/ Classes used by the archived full module graph are loaded in JVMTI early phase.\n+    assert(!(JvmtiExport::should_post_class_file_load_hook() && JvmtiExport::has_early_class_hook_env()),\n+           \"CDS should be disabled if early class hooks are enabled\");\n+  }\n+  return result;\n+}\n+\n+void MetaspaceShared::print_on(outputStream* st) {\n+  if (UseSharedSpaces) {\n+    st->print(\"CDS archive(s) mapped at: \");\n+    address base = (address)MetaspaceObj::shared_metaspace_base();\n+    address static_top = (address)_shared_metaspace_static_top;\n+    address top = (address)MetaspaceObj::shared_metaspace_top();\n+    st->print(\"[\" PTR_FORMAT \"-\" PTR_FORMAT \"-\" PTR_FORMAT \"), \", p2i(base), p2i(static_top), p2i(top));\n+    st->print(\"size \" SIZE_FORMAT \", \", top - base);\n+    st->print(\"SharedBaseAddress: \" PTR_FORMAT \", ArchiveRelocationMode: %d.\", SharedBaseAddress, (int)ArchiveRelocationMode);\n+  } else {\n+    st->print(\"CDS archive(s) not mapped\");\n+  }\n+  st->cr();\n+}\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":1543,"deletions":0,"binary":false,"changes":1543,"status":"added"},{"patch":"@@ -305,1 +305,3 @@\n-    gpi.compute_map(CATCH);\n+    if (!gpi.compute_map(THREAD)) {\n+      fatal(\"Unrecoverable verification or out-of-memory error\");\n+    }\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"aot\/aotLoader.hpp\"\n@@ -5618,19 +5617,0 @@\n-\n-  if (ik->should_store_fingerprint()) {\n-    ik->store_fingerprint(_stream->compute_fingerprint());\n-  }\n-\n-  ik->set_has_passed_fingerprint_check(false);\n-  if (UseAOT && ik->supers_have_passed_fingerprint_checks()) {\n-    uint64_t aot_fp = AOTLoader::get_saved_fingerprint(ik);\n-    uint64_t fp = ik->has_stored_fingerprint() ? ik->get_stored_fingerprint() : _stream->compute_fingerprint();\n-    if (aot_fp != 0 && aot_fp == fp) {\n-      \/\/ This class matches with a class saved in an AOT library\n-      ik->set_has_passed_fingerprint_check(true);\n-    } else {\n-      ResourceMark rm;\n-      log_info(class, fingerprint)(\"%s :  expected = \" PTR64_FORMAT \" actual = \" PTR64_FORMAT,\n-                                 ik->external_name(), aot_fp, _stream->compute_fingerprint());\n-    }\n-  }\n-\n@@ -6301,12 +6281,0 @@\n-  if (DumpSharedSpaces && _major_version < JAVA_6_VERSION) {\n-    ResourceMark rm;\n-    warning(\"Pre JDK 6 class not supported by CDS: %u.%u %s\",\n-            _major_version,  _minor_version, _class_name->as_C_string());\n-    Exceptions::fthrow(\n-      THREAD_AND_LOCATION,\n-      vmSymbols::java_lang_UnsupportedClassVersionError(),\n-      \"Unsupported major.minor version for dump time %u.%u\",\n-      _major_version,\n-      _minor_version);\n-  }\n-\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":0,"deletions":32,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cds\/filemap.hpp\"\n@@ -50,1 +51,0 @@\n-#include \"memory\/filemap.hpp\"\n@@ -68,1 +68,1 @@\n-#include \"runtime\/os.inline.hpp\"\n+#include \"runtime\/os.hpp\"\n@@ -108,1 +108,0 @@\n-static JImagePackageToModule_t         JImagePackageToModule  = NULL;\n@@ -111,1 +110,3 @@\n-static JImageResourceIterator_t        JImageResourceIterator = NULL;\n+\n+\/\/ JimageFile pointer, or null if exploded JDK build.\n+static JImageFile*                     JImage_file            = NULL;\n@@ -346,0 +347,11 @@\n+JImageFile* ClassPathImageEntry::jimage() const {\n+  return JImage_file;\n+}\n+\n+JImageFile* ClassPathImageEntry::jimage_non_null() const {\n+  assert(ClassLoader::has_jrt_entry(), \"must be\");\n+  assert(jimage() != NULL, \"should have been opened by ClassLoader::lookup_vm_options \"\n+                           \"and remained throughout normal JVM lifetime\");\n+  return jimage();\n+}\n+\n@@ -347,3 +359,3 @@\n-  if (_jimage != NULL) {\n-    (*JImageClose)(_jimage);\n-    _jimage = NULL;\n+  if (jimage() != NULL) {\n+    (*JImageClose)(jimage());\n+    JImage_file = NULL;\n@@ -354,2 +366,1 @@\n-  ClassPathEntry(),\n-  _jimage(jimage) {\n+  ClassPathEntry() {\n@@ -364,12 +375,0 @@\n-ClassPathImageEntry::~ClassPathImageEntry() {\n-  assert(_singleton == this, \"must be\");\n-  DEBUG_ONLY(_singleton = NULL);\n-\n-  FREE_C_HEAP_ARRAY(const char, _name);\n-\n-  if (_jimage != NULL) {\n-    (*JImageClose)(_jimage);\n-    _jimage = NULL;\n-  }\n-}\n-\n@@ -389,1 +388,1 @@\n-  JImageLocationRef location = (*JImageFindResource)(_jimage, \"\", get_jimage_version_string(), name, &size);\n+  JImageLocationRef location = (*JImageFindResource)(jimage_non_null(), \"\", get_jimage_version_string(), name, &size);\n@@ -397,1 +396,1 @@\n-        location = (*JImageFindResource)(_jimage, JAVA_BASE_NAME, get_jimage_version_string(), name, &size);\n+        location = (*JImageFindResource)(jimage_non_null(), JAVA_BASE_NAME, get_jimage_version_string(), name, &size);\n@@ -408,1 +407,1 @@\n-            location = (*JImageFindResource)(_jimage, module_name, get_jimage_version_string(), name, &size);\n+            location = (*JImageFindResource)(jimage_non_null(), module_name, get_jimage_version_string(), name, &size);\n@@ -419,1 +418,1 @@\n-    (*JImageGetResource)(_jimage, location, data, size);\n+    (*JImageGetResource)(jimage_non_null(), location, data, size);\n@@ -652,1 +651,4 @@\n-        ClassPathEntry* new_entry = create_class_path_entry(current, path, &st, false, false);\n+        if (JImage_file != NULL) {\n+          assert(Arguments::has_jimage(), \"sanity check\");\n+          const char* canonical_path = get_canonical_path(path, current);\n+          assert(canonical_path != NULL, \"canonical_path issue\");\n@@ -654,5 +656,2 @@\n-        \/\/ Check for a jimage\n-        if (Arguments::has_jimage()) {\n-          assert(_jrt_entry == NULL, \"should not setup bootstrap class search path twice\");\n-          _jrt_entry = new_entry;\n-          assert(new_entry != NULL && new_entry->is_modules_image(), \"No java runtime image present\");\n+          _jrt_entry = new ClassPathImageEntry(JImage_file, canonical_path);\n+          assert(_jrt_entry != NULL && _jrt_entry->is_modules_image(), \"No java runtime image present\");\n@@ -660,0 +659,3 @@\n+        } else {\n+          \/\/ It's an exploded build.\n+          ClassPathEntry* new_entry = create_class_path_entry(current, path, &st, false, false);\n@@ -727,1 +729,1 @@\n-    \/\/ Regular file, should be a zip or jimage file\n+    \/\/ Regular file, should be a zip file\n@@ -733,4 +735,4 @@\n-    jint error;\n-    JImageFile* jimage =(*JImageOpen)(canonical_path, &error);\n-    if (jimage != NULL) {\n-      new_entry = new ClassPathImageEntry(jimage, canonical_path);\n+    char* error_msg = NULL;\n+    jzfile* zip = open_zip_file(canonical_path, &error_msg, thread);\n+    if (zip != NULL && error_msg == NULL) {\n+      new_entry = new ClassPathZipEntry(zip, path, is_boot_append, from_class_path_attr);\n@@ -738,7 +740,1 @@\n-      char* error_msg = NULL;\n-      jzfile* zip = open_zip_file(canonical_path, &error_msg, thread);\n-      if (zip != NULL && error_msg == NULL) {\n-        new_entry = new ClassPathZipEntry(zip, path, is_boot_append, from_class_path_attr);\n-      } else {\n-        return NULL;\n-      }\n+      return NULL;\n@@ -971,1 +967,0 @@\n-  JImagePackageToModule = CAST_TO_FN_PTR(JImagePackageToModule_t, dll_lookup(handle, \"JIMAGE_PackageToModule\", path));\n@@ -974,1 +969,0 @@\n-  JImageResourceIterator = CAST_TO_FN_PTR(JImageResourceIterator_t, dll_lookup(handle, \"JIMAGE_ResourceIterator\", path));\n@@ -1432,2 +1426,2 @@\n-  JImageFile* jimage =(*JImageOpen)(modules_path, &error);\n-  if (jimage == NULL) {\n+  JImage_file =(*JImageOpen)(modules_path, &error);\n+  if (JImage_file == NULL) {\n@@ -1438,3 +1432,1 @@\n-  char *options = lookup_vm_resource(jimage, jimage_version, \"jdk\/internal\/vm\/options\");\n-\n-  (*JImageClose)(jimage);\n+  char *options = lookup_vm_resource(JImage_file, jimage_version, \"jdk\/internal\/vm\/options\");\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":42,"deletions":50,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -27,0 +27,3 @@\n+#include \"cds\/archiveBuilder.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/metaspaceShared.hpp\"\n@@ -44,3 +47,0 @@\n-#include \"memory\/archiveBuilder.hpp\"\n-#include \"memory\/heapShared.inline.hpp\"\n-#include \"memory\/metaspaceShared.hpp\"\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -385,2 +385,0 @@\n-  \/\/ Instance creation\n-  static oop create();\n@@ -523,1 +521,0 @@\n-  static void print_stack_usage(Handle stream);\n@@ -708,2 +705,0 @@\n-  static void set_parameter_annotations(oop method, oop value);\n-  static void set_annotation_default(oop method, oop value);\n@@ -890,2 +885,0 @@\n-  static inline oop queue(oop ref);\n-  static inline void set_queue(oop ref, oop value);\n@@ -926,2 +919,0 @@\n-class MethodHandleEntry;\n-\n@@ -1000,1 +991,0 @@\n-  static void       set_vmentry(oop lform, oop invoker);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"aot\/aotLoader.hpp\"\n+#include \"cds\/heapShared.hpp\"\n@@ -54,1 +54,0 @@\n-#include \"memory\/heapShared.hpp\"\n@@ -94,1 +93,0 @@\n-LoaderConstraintTable* SystemDictionary::_loader_constraints  = NULL;\n@@ -105,1 +103,0 @@\n-const int _loader_constraint_size = 107;                     \/\/ number of entries in constraint table\n@@ -111,1 +108,1 @@\n-PlaceholderTable* _placeholders   = NULL;\n+static PlaceholderTable* _placeholders   = NULL;\n@@ -114,0 +111,5 @@\n+\/\/ Constraints on class loaders\n+const int _loader_constraint_size = 107;                     \/\/ number of entries in constraint table\n+static LoaderConstraintTable*  _loader_constraints;\n+static LoaderConstraintTable* constraints() { return _loader_constraints; }\n+\n@@ -1309,15 +1311,0 @@\n-\n-  ik->set_has_passed_fingerprint_check(false);\n-  if (UseAOT && ik->supers_have_passed_fingerprint_checks()) {\n-    uint64_t aot_fp = AOTLoader::get_saved_fingerprint(ik);\n-    uint64_t cds_fp = ik->get_stored_fingerprint();\n-    if (aot_fp != 0 && aot_fp == cds_fp) {\n-      \/\/ This class matches with a class saved in an AOT library\n-      ik->set_has_passed_fingerprint_check(true);\n-    } else {\n-      if (log_is_enabled(Info, class, fingerprint)) {\n-        ResourceMark rm;\n-        log_info(class, fingerprint)(\"%s :  expected = \" PTR64_FORMAT \" actual = \" PTR64_FORMAT, ik->external_name(), aot_fp, cds_fp);\n-      }\n-    }\n-  }\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":7,"deletions":20,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -74,1 +74,0 @@\n-class LoaderConstraintTable;\n@@ -306,3 +305,0 @@\n-  \/\/ Constraints on class loaders\n-  static LoaderConstraintTable*  _loader_constraints;\n-\n@@ -328,2 +324,0 @@\n-  friend class VM_PopulateDumpSharedSpace;\n-  static LoaderConstraintTable* constraints() { return _loader_constraints; }\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -289,1 +289,3 @@\n-    !klass->is_shared() &&\n+    \/\/ However, bytecodes for shared old classes can be verified because\n+    \/\/ they have not been rewritten.\n+    !(klass->is_shared() && klass->is_rewritten()) &&\n","filename":"src\/hotspot\/share\/classfile\/verifier.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -302,6 +302,0 @@\n-  \/* used by CDS *\/                                                                               \\\n-  template(jdk_internal_misc_CDS,                     \"jdk\/internal\/misc\/CDS\")                    \\\n-  template(generateLambdaFormHolderClasses,           \"generateLambdaFormHolderClasses\")          \\\n-  template(generateLambdaFormHolderClasses_signature, \"([Ljava\/lang\/String;)[Ljava\/lang\/Object;\") \\\n-  template(dumpSharedArchive,                         \"dumpSharedArchive\")                        \\\n-  template(dumpSharedArchive_signature,               \"(ZLjava\/lang\/String;)V\")                   \\\n@@ -702,7 +696,16 @@\n-  \/* cds *\/                                                                                                       \\\n-  template(jdk_internal_loader_ClassLoaders,       \"jdk\/internal\/loader\/ClassLoaders\")                            \\\n-  template(java_util_concurrent_ConcurrentHashMap, \"java\/util\/concurrent\/ConcurrentHashMap\")                      \\\n-  template(java_util_ArrayList,                    \"java\/util\/ArrayList\")                                         \\\n-  template(toFileURL_name,                         \"toFileURL\")                                                   \\\n-  template(toFileURL_signature,                    \"(Ljava\/lang\/String;)Ljava\/net\/URL;\")                          \\\n-  template(url_void_signature,                     \"(Ljava\/net\/URL;)V\")                                           \\\n+  \/* CDS *\/                                                                                                       \\\n+  template(dumpSharedArchive,                               \"dumpSharedArchive\")                                  \\\n+  template(dumpSharedArchive_signature,                     \"(ZLjava\/lang\/String;)V\")                             \\\n+  template(generateLambdaFormHolderClasses,                 \"generateLambdaFormHolderClasses\")                    \\\n+  template(generateLambdaFormHolderClasses_signature,       \"([Ljava\/lang\/String;)[Ljava\/lang\/Object;\")           \\\n+  template(java_lang_invoke_Invokers_Holder,                \"java\/lang\/invoke\/Invokers$Holder\")                   \\\n+  template(java_lang_invoke_DirectMethodHandle_Holder,      \"java\/lang\/invoke\/DirectMethodHandle$Holder\")         \\\n+  template(java_lang_invoke_LambdaForm_Holder,              \"java\/lang\/invoke\/LambdaForm$Holder\")                 \\\n+  template(java_lang_invoke_DelegatingMethodHandle_Holder,  \"java\/lang\/invoke\/DelegatingMethodHandle$Holder\")     \\\n+  template(jdk_internal_loader_ClassLoaders,                \"jdk\/internal\/loader\/ClassLoaders\")                   \\\n+  template(jdk_internal_misc_CDS,                           \"jdk\/internal\/misc\/CDS\")                              \\\n+  template(java_util_concurrent_ConcurrentHashMap,          \"java\/util\/concurrent\/ConcurrentHashMap\")             \\\n+  template(java_util_ArrayList,                             \"java\/util\/ArrayList\")                                \\\n+  template(toFileURL_name,                                  \"toFileURL\")                                          \\\n+  template(toFileURL_signature,                             \"(Ljava\/lang\/String;)Ljava\/net\/URL;\")                 \\\n+  template(url_void_signature,                              \"(Ljava\/net\/URL;)V\")                                  \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":16,"deletions":13,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -47,2 +47,1 @@\n-    AOT                 = 4,    \/\/ AOT methods\n-    NumTypes            = 5     \/\/ Number of CodeBlobTypes\n+    NumTypes            = 4     \/\/ Number of CodeBlobTypes\n@@ -57,4 +56,0 @@\n-\/\/   AOTCompiledMethod   : AOT Compiled Java methods - Not in the CodeCache!\n-\/\/                         AOTCompiledMethod objects are allocated in the C-Heap, the code they\n-\/\/                         point to is allocated in the AOTCodeHeap which is in the C-Heap as\n-\/\/                         well (i.e. it's the memory where the shared library was loaded to)\n@@ -74,1 +69,1 @@\n-\/\/ Layout (all except AOTCompiledMethod) : continuous in the CodeCache\n+\/\/ Layout : continuous in the CodeCache\n@@ -80,5 +75,0 @@\n-\/\/\n-\/\/ Layout (AOTCompiledMethod) : in the C-Heap\n-\/\/   - header -\\\n-\/\/     ...     |\n-\/\/   - code  <-\/\n@@ -148,1 +138,0 @@\n-  virtual bool is_aot() const                         { return false; }\n@@ -250,1 +239,0 @@\n-    assert(!is_aot(), \"invalid on aot\");\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":2,"deletions":14,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -428,1 +428,1 @@\n-  if (info.to_interpreter() || info.to_aot()) {\n+  if (info.to_interpreter()) {\n@@ -442,1 +442,1 @@\n-         tty->print_cr (\"IC@\" INTPTR_FORMAT \": monomorphic to %s: %s\",\n+         tty->print_cr (\"IC@\" INTPTR_FORMAT \": monomorphic to interpreter: %s\",\n@@ -444,1 +444,0 @@\n-           (info.to_aot() ? \"aot\" : \"interpreter\"),\n@@ -545,3 +544,2 @@\n-  bool far_c2a = entry != NULL && caller_is_nmethod && method_code->is_far_code();\n-  if (entry != NULL && !far_c2a) {\n-    \/\/ Call to near compiled code (nmethod or aot).\n+  if (entry != NULL) {\n+    \/\/ Call to near compiled code.\n@@ -551,8 +549,3 @@\n-      if (far_c2a) {\n-        \/\/ Call to aot code from nmethod.\n-        info.set_aot_entry(entry, method());\n-      } else {\n-        \/\/ Use stub entry\n-        address entry = caller_is_c1 ? method()->get_c2i_inline_entry() : method()->get_c2i_entry();\n-        info.set_interpreter_entry(entry, method());\n-      }\n+      \/\/ Use stub entry\n+      address entry = caller_is_c1 ? method()->get_c2i_inline_entry() : method()->get_c2i_entry();\n+      info.set_interpreter_entry(entry, method());\n@@ -620,7 +613,0 @@\n-bool CompiledDirectStaticCall::is_call_to_far() const {\n-  \/\/ It is a call to aot method, if it calls to a stub. Hence, the destination\n-  \/\/ must be in the stub part of the nmethod that contains the call\n-  CodeBlob* desc = CodeCache::find_blob(instruction_address());\n-  return desc->as_compiled_method()->stub_contains(destination());\n-}\n-\n@@ -651,5 +637,0 @@\n-#if INCLUDE_AOT\n-  } else if (info._to_aot) {\n-    \/\/ Call to far code\n-    set_to_far(info.callee(), info.entry());\n-#endif\n@@ -668,6 +649,0 @@\n-    if (caller_is_nmethod && m_code->is_far_code()) {\n-      \/\/ Call to far aot code from nmethod.\n-      info._to_aot = true;\n-    } else {\n-      info._to_aot = false;\n-    }\n@@ -696,1 +671,1 @@\n-address CompiledDirectStaticCall::find_stub_for(address instruction, bool is_aot) {\n+address CompiledDirectStaticCall::find_stub_for(address instruction) {\n@@ -703,1 +678,1 @@\n-          return iter.static_call_reloc()->static_stub(is_aot);\n+          return iter.static_call_reloc()->static_stub();\n@@ -707,1 +682,1 @@\n-          return iter.opt_virtual_call_reloc()->static_stub(is_aot);\n+          return iter.opt_virtual_call_reloc()->static_stub();\n@@ -718,2 +693,2 @@\n-address CompiledDirectStaticCall::find_stub(bool is_aot) {\n-  return CompiledDirectStaticCall::find_stub_for(instruction_address(), is_aot);\n+address CompiledDirectStaticCall::find_stub() {\n+  return CompiledDirectStaticCall::find_stub_for(instruction_address());\n@@ -752,2 +727,0 @@\n-  } else if (is_call_to_far()) {\n-    tty->print(\"far\");\n","filename":"src\/hotspot\/share\/code\/compiledIC.cpp","additions":12,"deletions":39,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -87,1 +87,0 @@\n-  bool    _to_aot;             \/\/ Call it to aot code\n@@ -102,1 +101,0 @@\n-  bool          to_aot() const { return _to_aot; }\n@@ -108,1 +106,0 @@\n-    _to_aot = false;\n@@ -118,11 +115,0 @@\n-    _to_aot = false;\n-    _is_icholder = false;\n-    _is_optimized = true;\n-    _release_icholder = false;\n-  }\n-\n-  void set_aot_entry(address entry, Method* method) {\n-    _entry      = entry;\n-    _cached_value = (void*)method;\n-    _to_interpreter = false;\n-    _to_aot = true;\n@@ -138,1 +124,0 @@\n-    _to_aot = false;\n@@ -145,1 +130,1 @@\n-                    _is_optimized(false), _to_interpreter(false), _to_aot(false), _release_icholder(false) {\n+                    _is_optimized(false), _to_interpreter(false), _release_icholder(false) {\n@@ -344,1 +329,0 @@\n-  bool         _to_aot;         \/\/ call to aot method (otherwise compiled)\n@@ -361,3 +345,0 @@\n-  static void emit_to_aot_stub(CodeBuffer &cbuf, address mark = NULL);\n-  static int to_aot_stub_size();\n-  static int reloc_to_aot_stub();\n@@ -389,3 +370,0 @@\n-#if INCLUDE_AOT\n-  virtual void set_to_far(const methodHandle& callee, address entry) = 0;\n-#endif\n@@ -408,3 +386,0 @@\n-#if INCLUDE_AOT\n-  void set_to_far(const methodHandle& callee, address entry);\n-#endif\n@@ -440,1 +415,0 @@\n-  bool is_call_to_far() const;\n@@ -443,2 +417,2 @@\n-  static address find_stub_for(address instruction, bool is_aot);\n-  address find_stub(bool is_aot);\n+  static address find_stub_for(address instruction);\n+  address find_stub();\n","filename":"src\/hotspot\/share\/code\/compiledIC.hpp","additions":4,"deletions":30,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -75,1 +75,0 @@\n-    _is_far_code                = false;\n","filename":"src\/hotspot\/share\/code\/compiledMethod.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -155,3 +155,0 @@\n-  bool _is_far_code; \/\/ Code is far from CodeCache.\n-                     \/\/ Have to use far call instructions to call it from code in CodeCache.\n-\n@@ -350,2 +347,0 @@\n-  bool is_far_code() const { return _is_far_code; }\n-\n","filename":"src\/hotspot\/share\/code\/compiledMethod.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -442,1 +442,0 @@\n-  _is_far_code                = false; \/\/ nmethods are located in CodeCache\n@@ -2336,1 +2335,0 @@\n-  if (cm->is_aot()) return;  \/\/ FIXME: Revisit once _lock_count is added to aot_method\n@@ -2344,1 +2342,0 @@\n-  if (cm->is_aot()) return;  \/\/ FIXME: Revisit once _lock_count is added to aot_method\n@@ -2492,1 +2489,1 @@\n-        stub = iter.opt_virtual_call_reloc()->static_stub(false);\n+        stub = iter.opt_virtual_call_reloc()->static_stub();\n@@ -2496,1 +2493,1 @@\n-        stub = iter.static_call_reloc()->static_stub(false);\n+        stub = iter.static_call_reloc()->static_stub();\n@@ -3467,12 +3464,0 @@\n-#if INCLUDE_AOT\n-    if (UseAOT) {\n-      CodeBlob* callee = CodeCache::find_blob(dest);\n-      CompiledMethod* cm = callee->as_compiled_method_or_null();\n-      if (cm != NULL && cm->is_far_code()) {\n-        \/\/ Temporary fix, see JDK-8143106\n-        CompiledDirectStaticCall* csc = CompiledDirectStaticCall::at(instruction_address());\n-        csc->set_to_far(methodHandle(Thread::current(), cm->method()), dest);\n-        return;\n-      }\n-    }\n-#endif\n@@ -3484,5 +3469,0 @@\n-#if INCLUDE_AOT\n-    if (info.to_aot()) {\n-      csc->set_to_far(method, info.entry());\n-    } else\n-#endif\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":2,"deletions":22,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -656,0 +656,1 @@\n+#ifdef COMPILER1\n@@ -657,0 +658,1 @@\n+#endif \/\/ COMPILER1\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -621,3 +621,3 @@\n-    if (!r->evacuation_failed()) {\n-      r->set_evacuation_failed(true);\n-     _g1h->hr_printer()->evac_failure(r);\n+    if (r->set_evacuation_failed()) {\n+      _g1h->notify_region_failed_evacuation();\n+      _g1h->hr_printer()->evac_failure(r);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -26,1 +26,0 @@\n-#include \"aot\/aotLoader.hpp\"\n@@ -415,7 +414,0 @@\n-#ifdef  ASSERT\n-size_t add_obj_count;\n-size_t add_obj_size;\n-size_t mark_bitmap_count;\n-size_t mark_bitmap_size;\n-#endif  \/\/ #ifdef ASSERT\n-\n@@ -458,1 +450,1 @@\n-  ReservedSpace rs(_reserved_byte_size, rs_align, rs_align > 0);\n+  ReservedSpace rs(_reserved_byte_size, rs_align, page_sz);\n@@ -542,3 +534,0 @@\n-  DEBUG_ONLY(Atomic::inc(&add_obj_count);)\n-  DEBUG_ONLY(Atomic::add(&add_obj_size, len);)\n-\n@@ -996,3 +985,0 @@\n-  DEBUG_ONLY(add_obj_count = add_obj_size = 0;)\n-  DEBUG_ONLY(mark_bitmap_count = mark_bitmap_size = 0;)\n-\n@@ -1615,13 +1601,0 @@\n-#ifdef ASSERT\n-  log_develop_debug(gc, marking)(\n-      \"add_obj_count=\" SIZE_FORMAT \" \"\n-      \"add_obj_bytes=\" SIZE_FORMAT,\n-      add_obj_count,\n-      add_obj_size * HeapWordSize);\n-  log_develop_debug(gc, marking)(\n-      \"mark_bitmap_count=\" SIZE_FORMAT \" \"\n-      \"mark_bitmap_bytes=\" SIZE_FORMAT,\n-      mark_bitmap_count,\n-      mark_bitmap_size * HeapWordSize);\n-#endif \/\/ ASSERT\n-\n@@ -2015,1 +1988,0 @@\n-      AOTLoader::oops_do(&mark_and_push_closure);\n@@ -2222,1 +2194,0 @@\n-    PSAdjustSubTask_aot,\n@@ -2266,3 +2237,0 @@\n-    if (_sub_tasks.try_claim_task(PSAdjustSubTask_aot)) {\n-      AOT_ONLY(AOTLoader::oops_do(&adjust);)\n-    }\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":1,"deletions":33,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -712,1 +712,1 @@\n-Node* BarrierSetC2::obj_allocate(PhaseMacroExpand* macro, Node* ctrl, Node* mem, Node* toobig_false, Node* size_in_bytes,\n+Node* BarrierSetC2::obj_allocate(PhaseMacroExpand* macro, Node* mem, Node* toobig_false, Node* size_in_bytes,\n@@ -732,1 +732,1 @@\n-  Node *eden_end = macro->make_load(ctrl, mem, eden_end_adr, 0, TypeRawPtr::BOTTOM, T_ADDRESS);\n+  Node *eden_end = macro->make_load(toobig_false, mem, eden_end_adr, 0, TypeRawPtr::BOTTOM, T_ADDRESS);\n@@ -755,1 +755,1 @@\n-    ? new LoadPNode      (ctrl, contended_phi_rawmem, eden_top_adr, TypeRawPtr::BOTTOM, TypeRawPtr::BOTTOM, MemNode::unordered)\n+    ? new LoadPNode      (toobig_false, contended_phi_rawmem, eden_top_adr, TypeRawPtr::BOTTOM, TypeRawPtr::BOTTOM, MemNode::unordered)\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -247,1 +247,1 @@\n-  virtual Node* obj_allocate(PhaseMacroExpand* macro, Node* ctrl, Node* mem, Node* toobig_false, Node* size_in_bytes,\n+  virtual Node* obj_allocate(PhaseMacroExpand* macro, Node* mem, Node* toobig_false, Node* size_in_bytes,\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -177,1 +177,1 @@\n-  _thread->check_for_valid_safepoint_state();\n+  _thread->as_Java_thread()->check_for_valid_safepoint_state();\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -250,1 +250,1 @@\n-    assert(sizeof(size_t) == type2aelembytes(index_bt), \"Loading G1 SATBMarkQueue::_index with wrong size.\");\n+    assert(sizeof(size_t) == type2aelembytes(index_bt), \"Loading Shenandoah SATBMarkQueue::_index with wrong size.\");\n@@ -363,1 +363,1 @@\n-  \/\/ We could be accessing the referent field of a reference object. If so, when G1\n+  \/\/ We could be accessing the referent field of a reference object. If so, when Shenandoah\n@@ -981,1 +981,1 @@\n-    \/\/ Verify G1 pre-barriers\n+    \/\/ Verify Shenandoah pre-barriers\n@@ -1177,1 +1177,1 @@\n-      \/\/ Pointer stores in G1 barriers looks like unsafe access.\n+      \/\/ Pointer stores in Shenandoah barriers looks like unsafe access.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1531,1 +1531,1 @@\n-    assert(sizeof(size_t) == type2aelembytes(index_bt), \"Loading G1 SATBMarkQueue::_index with wrong size.\");\n+    assert(sizeof(size_t) == type2aelembytes(index_bt), \"Loading Shenandoah SATBMarkQueue::_index with wrong size.\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1638,1 +1638,1 @@\n-      DEBUG_ONLY(Thread::current()->check_possible_safepoint());\n+      DEBUG_ONLY(JavaThread::current()->check_possible_safepoint());\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/archiveUtils.hpp\"\n@@ -43,1 +44,0 @@\n-#include \"memory\/archiveUtils.hpp\"\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -93,1 +93,1 @@\n-  void compute_map(TRAPS);\n+  bool compute_map(Thread* current);\n@@ -105,1 +105,1 @@\n-void OopMapForCacheEntry::compute_map(TRAPS) {\n+bool OopMapForCacheEntry::compute_map(Thread* current) {\n@@ -112,1 +112,4 @@\n-    GenerateOopMap::compute_map(CATCH);\n+    if (!GenerateOopMap::compute_map(current)) {\n+      fatal(\"Unrecoverable verification or out-of-memory error\");\n+      return false;\n+    }\n@@ -115,0 +118,1 @@\n+  return true;\n@@ -336,2 +340,3 @@\n-    EXCEPTION_MARK;\n-    gen.compute_map(CATCH);\n+    if (!gen.compute_map(Thread::current())) {\n+      fatal(\"Unrecoverable verification or out-of-memory error\");\n+    }\n","filename":"src\/hotspot\/share\/interpreter\/oopMapCache.cpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/metaspaceShared.hpp\"\n@@ -570,2 +571,4 @@\n-  if (!DumpSharedSpaces) {\n-    assert(!klass->is_shared(), \"archive methods must not be rewritten at run time\");\n+#if INCLUDE_CDS\n+  if (klass->is_shared()) {\n+    assert(!klass->is_rewritten(), \"rewritten shared classes cannot be rewritten again\");\n+    assert(MetaspaceShared::is_old_class(klass), \"only shared old classes aren't rewritten\");\n@@ -573,0 +576,1 @@\n+#endif \/\/ INCLUDE_CDS\n","filename":"src\/hotspot\/share\/interpreter\/rewriter.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -128,1 +128,0 @@\n-    address earlyret_entry_itos = generate_earlyret_entry_for(itos);\n@@ -239,1 +238,0 @@\n-      address deopt_itos = generate_deopt_entry_for(itos, i);\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreterGenerator.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -175,64 +175,0 @@\n-#if INCLUDE_AOT\n-AOTOopRecorder::AOTOopRecorder(CodeInstaller* code_inst, Arena* arena, bool deduplicate) : OopRecorder(arena, deduplicate) {\n-  _code_inst = code_inst;\n-  _meta_refs = new GrowableArray<jobject>();\n-}\n-\n-int AOTOopRecorder::nr_meta_refs() const {\n-  return _meta_refs->length();\n-}\n-\n-jobject AOTOopRecorder::meta_element(int pos) const {\n-  return _meta_refs->at(pos);\n-}\n-\n-int AOTOopRecorder::find_index(Metadata* h) {\n-  JavaThread* THREAD = JavaThread::current();\n-  JVMCIEnv* JVMCIENV = _code_inst->jvmci_env();\n-  int oldCount = metadata_count();\n-  int index =  this->OopRecorder::find_index(h);\n-  int newCount = metadata_count();\n-\n-  if (oldCount == newCount) {\n-    \/\/ found a match\n-    return index;\n-  }\n-\n-  vmassert(index + 1 == newCount, \"must be last\");\n-\n-  JVMCIKlassHandle klass(THREAD);\n-  JVMCIObject result;\n-  guarantee(h != NULL,\n-            \"If DebugInformationRecorder::describe_scope passes NULL oldCount == newCount must hold.\");\n-  if (h->is_klass()) {\n-    klass = (Klass*) h;\n-    result = JVMCIENV->get_jvmci_type(klass, JVMCI_CATCH);\n-  } else if (h->is_method()) {\n-    Method* method = (Method*) h;\n-    methodHandle mh(THREAD, method);\n-    result = JVMCIENV->get_jvmci_method(mh, JVMCI_CATCH);\n-  }\n-  jobject ref = JVMCIENV->get_jobject(result);\n-  record_meta_ref(ref, index);\n-\n-  return index;\n-}\n-\n-int AOTOopRecorder::find_index(jobject h) {\n-  if (h == NULL) {\n-    return 0;\n-  }\n-  oop javaMirror = JNIHandles::resolve(h);\n-  Klass* klass = java_lang_Class::as_Klass(javaMirror);\n-  return find_index(klass);\n-}\n-\n-void AOTOopRecorder::record_meta_ref(jobject o, int index) {\n-  assert(index > 0, \"must be 1..n\");\n-  index -= 1; \/\/ reduce by one to convert to array index\n-\n-  assert(index == _meta_refs->length(), \"must be last\");\n-  _meta_refs->append(o);\n-}\n-#endif \/\/ INCLUDE_AOT\n-\n@@ -541,63 +477,0 @@\n-#if INCLUDE_AOT\n-RelocBuffer::~RelocBuffer() {\n-  FREE_C_HEAP_ARRAY(char, _buffer);\n-}\n-\n-address RelocBuffer::begin() const {\n-  if (_buffer != NULL) {\n-    return (address) _buffer;\n-  }\n-  return (address) _static_buffer;\n-}\n-\n-void RelocBuffer::set_size(size_t bytes) {\n-  assert(bytes <= _size, \"can't grow in size!\");\n-  _size = bytes;\n-}\n-\n-void RelocBuffer::ensure_size(size_t bytes) {\n-  assert(_buffer == NULL, \"can only be used once\");\n-  assert(_size == 0, \"can only be used once\");\n-  if (bytes >= RelocBuffer::stack_size) {\n-    _buffer = NEW_C_HEAP_ARRAY(char, bytes, mtJVMCI);\n-  }\n-  _size = bytes;\n-}\n-\n-JVMCI::CodeInstallResult CodeInstaller::gather_metadata(JVMCIObject target, JVMCIObject compiled_code, CodeMetadata& metadata, JVMCI_TRAPS) {\n-  assert(JVMCIENV->is_hotspot(), \"AOT code is executed only in HotSpot mode\");\n-  CodeBuffer buffer(\"JVMCI Compiler CodeBuffer for Metadata\");\n-  AOTOopRecorder* recorder = new AOTOopRecorder(this, &_arena, true);\n-  initialize_dependencies(compiled_code, recorder, JVMCI_CHECK_OK);\n-\n-  metadata.set_oop_recorder(recorder);\n-\n-  \/\/ Get instructions and constants CodeSections early because we need it.\n-  _instructions = buffer.insts();\n-  _constants = buffer.consts();\n-  buffer.set_immutable_PIC(_immutable_pic_compilation);\n-\n-  initialize_fields(target, compiled_code, JVMCI_CHECK_OK);\n-  JVMCI::CodeInstallResult result = initialize_buffer(buffer, false, JVMCI_CHECK_OK);\n-  if (result != JVMCI::ok) {\n-    return result;\n-  }\n-\n-  _debug_recorder->pcs_size(); \/\/ create the sentinel record\n-\n-  assert(_debug_recorder->pcs_length() >= 2, \"must be at least 2\");\n-\n-  metadata.set_pc_desc(_debug_recorder->pcs(), _debug_recorder->pcs_length());\n-  metadata.set_scopes(_debug_recorder->stream()->buffer(), _debug_recorder->data_size());\n-  metadata.set_exception_table(&_exception_handler_table);\n-  metadata.set_implicit_exception_table(&_implicit_exception_table);\n-\n-  RelocBuffer* reloc_buffer = metadata.get_reloc_buffer();\n-\n-  reloc_buffer->ensure_size(buffer.total_relocation_size());\n-  size_t size = (size_t) buffer.copy_relocations_to(reloc_buffer->begin(), (CodeBuffer::csize_t) reloc_buffer->size(), true);\n-  reloc_buffer->set_size(size);\n-  return JVMCI::ok;\n-}\n-#endif \/\/ INCLUDE_AOT\n-\n@@ -622,3 +495,0 @@\n-#if INCLUDE_AOT\n-  buffer.set_immutable_PIC(_immutable_pic_compilation);\n-#endif\n@@ -744,1 +614,1 @@\n-  \/\/ Estimate the number of static and aot call stubs that might be emitted.\n+  \/\/ Estimate the number of static call stubs that might be emitted.\n@@ -746,1 +616,0 @@\n-  int aot_call_stubs = 0;\n@@ -774,9 +643,0 @@\n-#if INCLUDE_AOT\n-      if (UseAOT && jvmci_env()->isa_site_Call(site)) {\n-        JVMCIObject target = jvmci_env()-> get_site_Call_target(site);\n-        if (!jvmci_env()->isa_HotSpotForeignCallTarget(target)) {\n-          \/\/ Add far aot trampolines.\n-          aot_call_stubs++;\n-        }\n-      }\n-#endif\n@@ -787,3 +647,0 @@\n-#if INCLUDE_AOT\n-  size += aot_call_stubs * CompiledStaticCall::to_aot_stub_size();\n-#endif\n@@ -1279,4 +1136,0 @@\n-    if (_immutable_pic_compilation) {\n-      \/\/ Use fake short distance during PIC compilation.\n-      foreign_call_destination = (jlong)(_instructions->start() + pc_offset);\n-    }\n@@ -1295,4 +1148,0 @@\n-#if INCLUDE_AOT\n-    \/\/ Trampoline to far aot code.\n-    CompiledStaticCall::emit_to_aot_stub(buffer, _instructions->start() + pc_offset);\n-#endif\n@@ -1322,4 +1171,1 @@\n-      if (!_immutable_pic_compilation) {\n-        \/\/ Do not patch during PIC compilation.\n-        pd_patch_OopConstant(pc_offset, constant, JVMCI_CHECK);\n-      }\n+      pd_patch_OopConstant(pc_offset, constant, JVMCI_CHECK);\n@@ -1327,4 +1173,1 @@\n-      if (!_immutable_pic_compilation) {\n-        \/\/ Do not patch during PIC compilation.\n-        pd_patch_OopConstant(pc_offset, constant, JVMCI_CHECK);\n-      }\n+      pd_patch_OopConstant(pc_offset, constant, JVMCI_CHECK);\n@@ -1332,9 +1175,1 @@\n-      if (!_immutable_pic_compilation) {\n-        pd_patch_MetaspaceConstant(pc_offset, constant, JVMCI_CHECK);\n-      }\n-#if INCLUDE_AOT\n-    } else if (jvmci_env()->isa_HotSpotSentinelConstant(constant)) {\n-      if (!_immutable_pic_compilation) {\n-        JVMCI_ERROR(\"sentinel constant not supported for normal compiles: %s\", jvmci_env()->klass_name(constant));\n-      }\n-#endif\n+      pd_patch_MetaspaceConstant(pc_offset, constant, JVMCI_CHECK);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":4,"deletions":169,"binary":false,"changes":173,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-  assert(!CompilerConfig::is_c1_or_interpreter_only_no_aot_or_jvmci(), \"JVMCI is launched, it's not c1\/interpreter only mode\");\n+  assert(!CompilerConfig::is_c1_or_interpreter_only_no_jvmci(), \"JVMCI is launched, it's not c1\/interpreter only mode\");\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompiler.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+#include \"oops\/instanceMirrorKlass.hpp\"\n@@ -252,12 +253,0 @@\n-C2V_VMENTRY_NULL(jobject, getObjectAtAddress, (JNIEnv* env, jobject c2vm, jlong oop_address))\n-  requireInHotSpot(\"getObjectAtAddress\", JVMCI_CHECK_NULL);\n-  if (oop_address == 0) {\n-    JVMCI_THROW_MSG_NULL(InternalError, \"Handle must be non-zero\");\n-  }\n-  oop obj = *((oopDesc**) oop_address);\n-  if (obj != NULL) {\n-    oopDesc::verify(obj);\n-  }\n-  return JNIHandles::make_local(THREAD, obj);\n-C2V_END\n-\n@@ -508,4 +497,5 @@\n-  ConstantPool* cp = method->constMethod()->constants();\n-  assert(cp != NULL, \"npe\");\n-  \/\/ don't inline method when constant pool contains a CONSTANT_Dynamic\n-  return !method->is_not_compilable(CompLevel_full_optimization) && !cp->has_dynamic_constant();\n+  \/\/ Skip redefined methods\n+  if (method->is_old()) {\n+    return false;\n+  }\n+  return !method->is_not_compilable(CompLevel_full_optimization);\n@@ -627,2 +617,35 @@\n-  oop result = cp->resolve_possibly_cached_constant_at(index, CHECK_NULL);\n-  return JVMCIENV->get_jobject(JVMCIENV->get_object_constant(result));\n+  oop obj = cp->resolve_possibly_cached_constant_at(index, CHECK_NULL);\n+  constantTag tag = cp->tag_at(index);\n+  if (tag.is_dynamic_constant() || tag.is_dynamic_constant_in_error()) {\n+    if (obj == Universe::the_null_sentinel()) {\n+      return JVMCIENV->get_jobject(JVMCIENV->get_JavaConstant_NULL_POINTER());\n+    }\n+    BasicType bt = Signature::basic_type(cp->uncached_signature_ref_at(index));\n+    if (!is_reference_type(bt)) {\n+      if (!is_java_primitive(bt)) {\n+        return JVMCIENV->get_jobject(JVMCIENV->get_JavaConstant_ILLEGAL());\n+      }\n+\n+      \/\/ Convert standard box (e.g. java.lang.Integer) to JVMCI box (e.g. jdk.vm.ci.meta.PrimitiveConstant)\n+      jvalue value;\n+      jlong raw_value;\n+      JVMCIObject kind;\n+      BasicType bt2 = java_lang_boxing_object::get_value(obj, &value);\n+      assert(bt2 == bt, \"\");\n+      switch (bt2) {\n+        case T_LONG:    kind = JVMCIENV->get_JavaKind_Long();    raw_value = value.j; break;\n+        case T_DOUBLE:  kind = JVMCIENV->get_JavaKind_Double();  raw_value = value.j; break;\n+        case T_FLOAT:   kind = JVMCIENV->get_JavaKind_Float();   raw_value = value.i; break;\n+        case T_INT:     kind = JVMCIENV->get_JavaKind_Int();     raw_value = value.i; break;\n+        case T_SHORT:   kind = JVMCIENV->get_JavaKind_Short();   raw_value = value.s; break;\n+        case T_BYTE:    kind = JVMCIENV->get_JavaKind_Byte();    raw_value = value.b; break;\n+        case T_CHAR:    kind = JVMCIENV->get_JavaKind_Char();    raw_value = value.c; break;\n+        case T_BOOLEAN: kind = JVMCIENV->get_JavaKind_Boolean(); raw_value = value.z; break;\n+        default:        return JVMCIENV->get_jobject(JVMCIENV->get_JavaConstant_ILLEGAL());\n+      }\n+\n+      JVMCIObject result = JVMCIENV->call_JavaConstant_forPrimitive(kind, raw_value, JVMCI_CHECK_NULL);\n+      return JVMCIENV->get_jobject(result);\n+    }\n+  }\n+  return JVMCIENV->get_jobject(JVMCIENV->get_object_constant(obj));\n@@ -856,2 +879,1 @@\n-  bool is_immutable_PIC = JVMCIENV->get_HotSpotCompiledCode_isImmutablePIC(compiled_code_handle) > 0;\n-  CodeInstaller installer(JVMCIENV, is_immutable_PIC);\n+  CodeInstaller installer(JVMCIENV);\n@@ -900,78 +922,0 @@\n-#if INCLUDE_AOT\n-  HandleMark hm(THREAD);\n-  assert(JVMCIENV->is_hotspot(), \"AOT code is executed only in HotSpot mode\");\n-\n-  JVMCIObject target_handle = JVMCIENV->wrap(target);\n-  JVMCIObject compiled_code_handle = JVMCIENV->wrap(compiled_code);\n-  JVMCIObject metadata_handle = JVMCIENV->wrap(metadata);\n-\n-  CodeMetadata code_metadata;\n-\n-  CodeInstaller installer(JVMCIENV, true \/* immutable PIC compilation *\/);\n-  JVMCI::CodeInstallResult result = installer.gather_metadata(target_handle, compiled_code_handle, code_metadata, JVMCI_CHECK_0);\n-  if (result != JVMCI::ok) {\n-    return result;\n-  }\n-\n-  if (code_metadata.get_nr_pc_desc() > 0) {\n-    int size = sizeof(PcDesc) * code_metadata.get_nr_pc_desc();\n-    JVMCIPrimitiveArray array = JVMCIENV->new_byteArray(size, JVMCI_CHECK_(JVMCI::cache_full));\n-    JVMCIENV->copy_bytes_from((jbyte*) code_metadata.get_pc_desc(), array, 0, size);\n-    HotSpotJVMCI::HotSpotMetaData::set_pcDescBytes(JVMCIENV, metadata_handle, array);\n-  }\n-\n-  if (code_metadata.get_scopes_size() > 0) {\n-    int size = code_metadata.get_scopes_size();\n-    JVMCIPrimitiveArray array = JVMCIENV->new_byteArray(size, JVMCI_CHECK_(JVMCI::cache_full));\n-    JVMCIENV->copy_bytes_from((jbyte*) code_metadata.get_scopes_desc(), array, 0, size);\n-    HotSpotJVMCI::HotSpotMetaData::set_scopesDescBytes(JVMCIENV, metadata_handle, array);\n-  }\n-\n-  RelocBuffer* reloc_buffer = code_metadata.get_reloc_buffer();\n-  int size = (int) reloc_buffer->size();\n-  JVMCIPrimitiveArray array = JVMCIENV->new_byteArray(size, JVMCI_CHECK_(JVMCI::cache_full));\n-  JVMCIENV->copy_bytes_from((jbyte*) reloc_buffer->begin(), array, 0, size);\n-  HotSpotJVMCI::HotSpotMetaData::set_relocBytes(JVMCIENV, metadata_handle, array);\n-\n-  const OopMapSet* oopMapSet = installer.oopMapSet();\n-  {\n-    ResourceMark mark;\n-    ImmutableOopMapBuilder builder(oopMapSet);\n-    int size = builder.heap_size();\n-    JVMCIPrimitiveArray array = JVMCIENV->new_byteArray(size, JVMCI_CHECK_(JVMCI::cache_full));\n-    builder.generate_into((address) HotSpotJVMCI::resolve(array)->byte_at_addr(0));\n-    HotSpotJVMCI::HotSpotMetaData::set_oopMaps(JVMCIENV, metadata_handle, array);\n-  }\n-\n-  AOTOopRecorder* recorder = code_metadata.get_oop_recorder();\n-\n-  int nr_meta_refs = recorder->nr_meta_refs();\n-  JVMCIObjectArray metadataArray = JVMCIENV->new_Object_array(nr_meta_refs, JVMCI_CHECK_(JVMCI::cache_full));\n-  for (int i = 0; i < nr_meta_refs; ++i) {\n-    jobject element = recorder->meta_element(i);\n-    if (element == NULL) {\n-      return JVMCI::cache_full;\n-    }\n-    JVMCIENV->put_object_at(metadataArray, i, JVMCIENV->wrap(element));\n-  }\n-  HotSpotJVMCI::HotSpotMetaData::set_metadata(JVMCIENV, metadata_handle, metadataArray);\n-\n-  ExceptionHandlerTable* handler = code_metadata.get_exception_table();\n-  int table_size = handler->size_in_bytes();\n-  JVMCIPrimitiveArray exceptionArray = JVMCIENV->new_byteArray(table_size, JVMCI_CHECK_(JVMCI::cache_full));\n-  if (table_size > 0) {\n-    handler->copy_bytes_to((address) HotSpotJVMCI::resolve(exceptionArray)->byte_at_addr(0));\n-  }\n-  HotSpotJVMCI::HotSpotMetaData::set_exceptionBytes(JVMCIENV, metadata_handle, exceptionArray);\n-\n-  ImplicitExceptionTable* implicit = code_metadata.get_implicit_exception_table();\n-  int implicit_table_size = implicit->size_in_bytes();\n-  JVMCIPrimitiveArray implicitExceptionArray = JVMCIENV->new_byteArray(implicit_table_size, JVMCI_CHECK_(JVMCI::cache_full));\n-  if (implicit_table_size > 0) {\n-    implicit->copy_bytes_to((address) HotSpotJVMCI::resolve(implicitExceptionArray)->byte_at_addr(0), implicit_table_size);\n-  }\n-  HotSpotJVMCI::HotSpotMetaData::set_implicitExceptionBytes(JVMCIENV, metadata_handle, implicitExceptionArray);\n-\n-  return result;\n-#else\n-#endif\n@@ -1153,5 +1097,0 @@\n-C2V_VMENTRY_NULL(jobject, readUncompressedOop, (JNIEnv* env, jobject, jlong addr))\n-  oop ret = RawAccess<>::oop_load((oop*)(address)addr);\n-  return JVMCIENV->get_jobject(JVMCIENV->get_object_constant(ret));\n- C2V_END\n-\n@@ -1630,9 +1569,0 @@\n-#if INCLUDE_AOT\n-  Klass *k = (Klass*) (address) metaspace_klass;\n-  if (k->is_instance_klass()) {\n-    return InstanceKlass::cast(k)->get_stored_fingerprint();\n-  } else {\n-    return 0;\n-  }\n-#else\n-#endif\n@@ -1911,2 +1841,2 @@\n-C2V_VMENTRY_NULL(jobject, readFieldValue, (JNIEnv* env, jobject, jobject object, jobject field, jboolean is_volatile))\n-  if (object == NULL || field == NULL) {\n+C2V_VMENTRY_NULL(jobject, readFieldValue, (JNIEnv* env, jobject, jobject object, jobject expected_type, long displacement, jboolean is_volatile, jobject kind_object))\n+  if (object == NULL || kind_object == NULL) {\n@@ -1915,18 +1845,7 @@\n-  JVMCIObject field_object = JVMCIENV->wrap(field);\n-  JVMCIObject java_type = JVMCIENV->get_HotSpotResolvedJavaFieldImpl_type(field_object);\n-  int modifiers = JVMCIENV->get_HotSpotResolvedJavaFieldImpl_modifiers(field_object);\n-  Klass* holder = JVMCIENV->asKlass(JVMCIENV->get_HotSpotResolvedJavaFieldImpl_holder(field_object));\n-  if (!holder->is_instance_klass()) {\n-    JVMCI_THROW_MSG_0(InternalError, err_msg(\"Holder %s must be instance klass\", holder->external_name()));\n-  }\n-  InstanceKlass* ik = InstanceKlass::cast(holder);\n-  BasicType constant_type;\n-  if (JVMCIENV->isa_HotSpotResolvedPrimitiveType(java_type)) {\n-    constant_type = JVMCIENV->kindToBasicType(JVMCIENV->get_HotSpotResolvedPrimitiveType_kind(java_type), JVMCI_CHECK_NULL);\n-  } else {\n-    constant_type = T_OBJECT;\n-  }\n-  int displacement = JVMCIENV->get_HotSpotResolvedJavaFieldImpl_offset(field_object);\n-  fieldDescriptor fd;\n-  if (!ik->find_local_field_from_offset(displacement, (modifiers & JVM_ACC_STATIC) != 0, &fd)) {\n-    JVMCI_THROW_MSG_0(InternalError, err_msg(\"Can't find field with displacement %d\", displacement));\n+\n+  JVMCIObject kind = JVMCIENV->wrap(kind_object);\n+  BasicType basic_type = JVMCIENV->kindToBasicType(kind, JVMCI_CHECK_NULL);\n+\n+  InstanceKlass* holder = NULL;\n+  if (expected_type != NULL) {\n+    holder = InstanceKlass::cast(JVMCIENV->asKlass(JVMCIENV->wrap(expected_type)));\n@@ -1934,1 +1853,2 @@\n-  JVMCIObject base = JVMCIENV->wrap(object);\n+\n+  bool is_static = false;\n@@ -1936,0 +1856,1 @@\n+  JVMCIObject base = JVMCIENV->wrap(object);\n@@ -1938,0 +1859,10 @@\n+    \/\/ asConstant will throw an NPE if a constant contains NULL\n+\n+    if (holder != NULL && !obj->is_a(holder)) {\n+      \/\/ Not a subtype of field holder\n+      return NULL;\n+    }\n+    is_static = false;\n+    if (holder == NULL && java_lang_Class::is_instance(obj()) && displacement >= InstanceMirrorKlass::offset_of_static_fields()) {\n+      is_static = true;\n+    }\n@@ -1939,0 +1870,1 @@\n+    is_static = true;\n@@ -1940,0 +1872,3 @@\n+    if (holder != NULL && holder != klass) {\n+      return NULL;\n+    }\n@@ -1942,2 +1877,2 @@\n-    JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n-                         err_msg(\"Unexpected type: %s\", JVMCIENV->klass_name(base)));\n+    \/\/ The Java code is expected to guard against this path\n+    ShouldNotReachHere();\n@@ -1946,5 +1881,41 @@\n-  if (displacement == java_lang_Class::component_mirror_offset() && java_lang_Class::is_instance(obj()) &&\n-      !java_lang_Class::as_Klass(obj())->is_array_klass()) {\n-    \/\/ Class.componentType for non-array classes can transiently contain an int[] that's\n-    \/\/ used for locking so always return null to mimic Class.getComponentType()\n-    return JVMCIENV->get_jobject(JVMCIENV->get_JavaConstant_NULL_POINTER());\n+  if (displacement < 0 || ((long) displacement + type2aelembytes(basic_type) > HeapWordSize * obj->size())) {\n+    \/\/ Reading outside of the object bounds\n+    JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"reading outside object bounds\");\n+  }\n+\n+  \/\/ Perform basic sanity checks on the read.  Primitive reads are permitted to read outside the\n+  \/\/ bounds of their fields but object reads must map exactly onto the underlying oop slot.\n+  if (basic_type == T_OBJECT) {\n+    if (obj->is_objArray()) {\n+      if (displacement < arrayOopDesc::base_offset_in_bytes(T_OBJECT)) {\n+        JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"reading from array header\");\n+      }\n+      if (displacement + heapOopSize > arrayOopDesc::base_offset_in_bytes(T_OBJECT) + arrayOop(obj())->length() * heapOopSize) {\n+        JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"reading after last array element\");\n+      }\n+      if (((displacement - arrayOopDesc::base_offset_in_bytes(T_OBJECT)) % heapOopSize) != 0) {\n+        JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"misaligned object read from array\");\n+      }\n+    } else if (obj->is_instance()) {\n+      InstanceKlass* klass = InstanceKlass::cast(is_static ? java_lang_Class::as_Klass(obj()) : obj->klass());\n+      fieldDescriptor fd;\n+      if (!klass->find_field_from_offset(displacement, is_static, &fd)) {\n+        JVMCI_THROW_MSG_NULL(IllegalArgumentException, err_msg(\"Can't find field at displacement %d in object of type %s\", (int) displacement, klass->external_name()));\n+      }\n+      if (fd.field_type() != T_OBJECT && fd.field_type() != T_ARRAY) {\n+        JVMCI_THROW_MSG_NULL(IllegalArgumentException, err_msg(\"Field at displacement %d in object of type %s is %s but expected %s\", (int) displacement,\n+                                                               klass->external_name(), type2name(fd.field_type()), type2name(basic_type)));\n+      }\n+    } else if (obj->is_typeArray()) {\n+      JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"Can't read objects from primitive array\");\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  } else {\n+    if (obj->is_objArray()) {\n+      JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"Reading primitive from object array\");\n+    } else if (obj->is_typeArray()) {\n+      if (displacement < arrayOopDesc::base_offset_in_bytes(ArrayKlass::cast(obj->klass())->element_type())) {\n+        JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"reading from array header\");\n+      }\n+    }\n@@ -1954,2 +1925,10 @@\n-  JVMCIObject kind;\n-  switch (constant_type) {\n+  switch (basic_type) {\n+    case T_BOOLEAN: value = is_volatile ? obj->bool_field_acquire(displacement)   : obj->bool_field(displacement);  break;\n+    case T_BYTE:    value = is_volatile ? obj->byte_field_acquire(displacement)   : obj->byte_field(displacement);  break;\n+    case T_SHORT:   value = is_volatile ? obj->short_field_acquire(displacement)  : obj->short_field(displacement); break;\n+    case T_CHAR:    value = is_volatile ? obj->char_field_acquire(displacement)   : obj->char_field(displacement);  break;\n+    case T_FLOAT:\n+    case T_INT:     value = is_volatile ? obj->int_field_acquire(displacement)    : obj->int_field(displacement);   break;\n+    case T_DOUBLE:\n+    case T_LONG:    value = is_volatile ? obj->long_field_acquire(displacement)   : obj->long_field(displacement);  break;\n+\n@@ -1957,3 +1936,4 @@\n-      oop object = is_volatile ? obj->obj_field_acquire(displacement) : obj->obj_field(displacement);\n-      JVMCIObject result = JVMCIENV->get_object_constant(object);\n-      if (result.is_null()) {\n+      if (displacement == java_lang_Class::component_mirror_offset() && java_lang_Class::is_instance(obj()) &&\n+          (java_lang_Class::as_Klass(obj()) == NULL || !java_lang_Class::as_Klass(obj())->is_array_klass())) {\n+        \/\/ Class.componentType for non-array classes can transiently contain an int[] that's\n+        \/\/ used for locking so always return null to mimic Class.getComponentType()\n@@ -1962,11 +1942,17 @@\n-      return JVMCIENV->get_jobject(result);\n-    }\n-    case T_FLOAT: {\n-      float f = is_volatile ? obj->float_field_acquire(displacement) : obj->float_field(displacement);\n-      JVMCIObject result = JVMCIENV->call_JavaConstant_forFloat(f, JVMCI_CHECK_NULL);\n-      return JVMCIENV->get_jobject(result);\n-    }\n-    case T_DOUBLE: {\n-      double f = is_volatile ? obj->double_field_acquire(displacement) : obj->double_field(displacement);\n-      JVMCIObject result = JVMCIENV->call_JavaConstant_forDouble(f, JVMCI_CHECK_NULL);\n-      return JVMCIENV->get_jobject(result);\n+\n+      oop value = is_volatile ? obj->obj_field_acquire(displacement) : obj->obj_field(displacement);\n+      if (value == NULL) {\n+        return JVMCIENV->get_jobject(JVMCIENV->get_JavaConstant_NULL_POINTER());\n+      } else {\n+        if (value != NULL && !oopDesc::is_oop(value)) {\n+          \/\/ Throw an exception to improve debuggability.  This check isn't totally reliable because\n+          \/\/ is_oop doesn't try to be completety safe but for most invalid values it provides a good\n+          \/\/ enough answer.  It possible to crash in the is_oop call but that just means the crash happens\n+          \/\/ closer to where things went wrong.\n+          JVMCI_THROW_MSG_NULL(InternalError, err_msg(\"Read bad oop \" INTPTR_FORMAT \" at offset \" JLONG_FORMAT \" in object \" INTPTR_FORMAT \" of type %s\",\n+                                                      p2i(value), displacement, p2i(obj()), obj->klass()->external_name()));\n+        }\n+\n+        JVMCIObject result = JVMCIENV->get_object_constant(value);\n+        return JVMCIENV->get_jobject(result);\n+      }\n@@ -1974,6 +1960,1 @@\n-    case T_BOOLEAN: value = is_volatile ? obj->bool_field_acquire(displacement) : obj->bool_field(displacement); break;\n-    case T_BYTE: value = is_volatile ? obj->byte_field_acquire(displacement) : obj->byte_field(displacement); break;\n-    case T_SHORT: value = is_volatile ? obj->short_field_acquire(displacement) : obj->short_field(displacement); break;\n-    case T_CHAR: value = is_volatile ? obj->char_field_acquire(displacement) : obj->char_field(displacement); break;\n-    case T_INT: value = is_volatile ? obj->int_field_acquire(displacement) : obj->int_field(displacement); break;\n-    case T_LONG: value = is_volatile ? obj->long_field_acquire(displacement) : obj->long_field(displacement); break;\n+\n@@ -1983,1 +1964,1 @@\n-  JVMCIObject result = JVMCIENV->call_PrimitiveConstant_forTypeChar(type2char(constant_type), value, JVMCI_CHECK_NULL);\n+  JVMCIObject result = JVMCIENV->call_JavaConstant_forPrimitive(kind, value, JVMCI_CHECK_NULL);\n@@ -2141,49 +2122,0 @@\n-C2V_VMENTRY_0(jbyte, getByte, (JNIEnv* env, jobject, jobject x, long displacement))\n-  if (x == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n-  Handle xobj = JVMCIENV->asConstant(JVMCIENV->wrap(x), JVMCI_CHECK_0);\n-  return xobj->byte_field(displacement);\n-}\n-\n-C2V_VMENTRY_0(jshort, getShort, (JNIEnv* env, jobject, jobject x, long displacement))\n-  if (x == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n-  Handle xobj = JVMCIENV->asConstant(JVMCIENV->wrap(x), JVMCI_CHECK_0);\n-  return xobj->short_field(displacement);\n-}\n-\n-C2V_VMENTRY_0(jint, getInt, (JNIEnv* env, jobject, jobject x, long displacement))\n-  if (x == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n-  Handle xobj = JVMCIENV->asConstant(JVMCIENV->wrap(x), JVMCI_CHECK_0);\n-  return xobj->int_field(displacement);\n-}\n-\n-C2V_VMENTRY_0(jlong, getLong, (JNIEnv* env, jobject, jobject x, long displacement))\n-  if (x == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n-  Handle xobj = JVMCIENV->asConstant(JVMCIENV->wrap(x), JVMCI_CHECK_0);\n-  return xobj->long_field(displacement);\n-}\n-\n-C2V_VMENTRY_NULL(jobject, getObject, (JNIEnv* env, jobject, jobject x, long displacement))\n-  if (x == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n-  Handle xobj = JVMCIENV->asConstant(JVMCIENV->wrap(x), JVMCI_CHECK_0);\n-  if (displacement == java_lang_Class::component_mirror_offset() && java_lang_Class::is_instance(xobj()) &&\n-      !java_lang_Class::as_Klass(xobj())->is_array_klass()) {\n-    \/\/ Class.componentType for non-array classes can transiently contain an int[] that's\n-    \/\/ used for locking so always return null to mimic Class.getComponentType()\n-    return JVMCIENV->get_jobject(JVMCIENV->get_JavaConstant_NULL_POINTER());\n-  }\n-\n-  oop res = xobj->obj_field(displacement);\n-  JVMCIObject result = JVMCIENV->get_object_constant(res);\n-  return JVMCIENV->get_jobject(result);\n-}\n-\n@@ -2703,1 +2635,1 @@\n-  {CC \"resolvePossiblyCachedConstantInPool\",          CC \"(\" HS_CONSTANT_POOL \"I)\" OBJECTCONSTANT,                                          FN_PTR(resolvePossiblyCachedConstantInPool)},\n+  {CC \"resolvePossiblyCachedConstantInPool\",          CC \"(\" HS_CONSTANT_POOL \"I)\" JAVACONSTANT,                                            FN_PTR(resolvePossiblyCachedConstantInPool)},\n@@ -2730,1 +2662,0 @@\n-  {CC \"readUncompressedOop\",                          CC \"(J)\" OBJECTCONSTANT,                                                              FN_PTR(readUncompressedOop)},\n@@ -2749,1 +2680,0 @@\n-  {CC \"getObjectAtAddress\",                           CC \"(J)\" OBJECT,                                                                      FN_PTR(getObjectAtAddress)},\n@@ -2760,2 +2690,2 @@\n-  {CC \"readFieldValue\",                               CC \"(\" HS_RESOLVED_KLASS HS_RESOLVED_FIELD \"Z)\" JAVACONSTANT,                         FN_PTR(readFieldValue)},\n-  {CC \"readFieldValue\",                               CC \"(\" OBJECTCONSTANT HS_RESOLVED_FIELD \"Z)\" JAVACONSTANT,                            FN_PTR(readFieldValue)},\n+  {CC \"readFieldValue\",                               CC \"(\" HS_RESOLVED_KLASS HS_RESOLVED_KLASS \"JZLjdk\/vm\/ci\/meta\/JavaKind;)\" JAVACONSTANT, FN_PTR(readFieldValue)},\n+  {CC \"readFieldValue\",                               CC \"(\" OBJECTCONSTANT HS_RESOLVED_KLASS \"JZLjdk\/vm\/ci\/meta\/JavaKind;)\" JAVACONSTANT,  FN_PTR(readFieldValue)},\n@@ -2773,5 +2703,0 @@\n-  {CC \"getByte\",                                      CC \"(\" OBJECTCONSTANT \"J)B\",                                                          FN_PTR(getByte)},\n-  {CC \"getShort\",                                     CC \"(\" OBJECTCONSTANT \"J)S\",                                                          FN_PTR(getShort)},\n-  {CC \"getInt\",                                       CC \"(\" OBJECTCONSTANT \"J)I\",                                                          FN_PTR(getInt)},\n-  {CC \"getLong\",                                      CC \"(\" OBJECTCONSTANT \"J)J\",                                                          FN_PTR(getLong)},\n-  {CC \"getObject\",                                    CC \"(\" OBJECTCONSTANT \"J)\" OBJECTCONSTANT,                                            FN_PTR(getObject)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":147,"deletions":222,"binary":false,"changes":369,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"jvmci\/vmStructs_compiler_runtime.hpp\"\n@@ -191,0 +190,1 @@\n+  nonstatic_field(JavaThread,                  _poll_data,                                    SafepointMechanism::ThreadData)        \\\n@@ -236,1 +236,0 @@\n-  AOT_ONLY(nonstatic_field(MethodCounters,     _method,                                       Method*))                              \\\n@@ -337,1 +336,0 @@\n-  nonstatic_field(Thread,                   _poll_data,                                       SafepointMechanism::ThreadData)        \\\n@@ -433,0 +431,1 @@\n+  declare_constant(JVM_CONSTANT_Dynamic)                                  \\\n@@ -754,12 +753,2 @@\n-#define VM_INT_CONSTANTS_CPU(declare_constant, declare_preprocessor_constant, declare_c1_constant, declare_c2_constant, declare_c2_preprocessor_constant) \\\n-  declare_constant(VM_Version::CPU_FP)                  \\\n-  declare_constant(VM_Version::CPU_ASIMD)               \\\n-  declare_constant(VM_Version::CPU_EVTSTRM)             \\\n-  declare_constant(VM_Version::CPU_AES)                 \\\n-  declare_constant(VM_Version::CPU_PMULL)               \\\n-  declare_constant(VM_Version::CPU_SHA1)                \\\n-  declare_constant(VM_Version::CPU_SHA2)                \\\n-  declare_constant(VM_Version::CPU_CRC32)               \\\n-  declare_constant(VM_Version::CPU_LSE)                 \\\n-  declare_constant(VM_Version::CPU_STXR_PREFETCH)       \\\n-  declare_constant(VM_Version::CPU_A53MAC)\n+#define DECLARE_INT_CPU_FEATURE_CONSTANT(id, name, bit) GENERATE_VM_INT_CONSTANT_ENTRY(VM_Version::CPU_##id)\n+#define VM_INT_CPU_FEATURE_CONSTANTS CPU_FEATURE_FLAGS(DECLARE_INT_CPU_FEATURE_CONSTANT)\n@@ -769,1 +758,0 @@\n-\n@@ -781,47 +769,2 @@\n-#define VM_LONG_CONSTANTS_CPU(declare_constant, declare_preprocessor_constant, declare_c1_constant, declare_c2_constant, declare_c2_preprocessor_constant) \\\n-  declare_constant(VM_Version::CPU_CX8)                             \\\n-  declare_constant(VM_Version::CPU_CMOV)                            \\\n-  declare_constant(VM_Version::CPU_FXSR)                            \\\n-  declare_constant(VM_Version::CPU_HT)                              \\\n-  declare_constant(VM_Version::CPU_MMX)                             \\\n-  declare_constant(VM_Version::CPU_3DNOW_PREFETCH)                  \\\n-  declare_constant(VM_Version::CPU_SSE)                             \\\n-  declare_constant(VM_Version::CPU_SSE2)                            \\\n-  declare_constant(VM_Version::CPU_SSE3)                            \\\n-  declare_constant(VM_Version::CPU_SSSE3)                           \\\n-  declare_constant(VM_Version::CPU_SSE4A)                           \\\n-  declare_constant(VM_Version::CPU_SSE4_1)                          \\\n-  declare_constant(VM_Version::CPU_SSE4_2)                          \\\n-  declare_constant(VM_Version::CPU_POPCNT)                          \\\n-  declare_constant(VM_Version::CPU_LZCNT)                           \\\n-  declare_constant(VM_Version::CPU_TSC)                             \\\n-  declare_constant(VM_Version::CPU_TSCINV)                          \\\n-  declare_constant(VM_Version::CPU_AVX)                             \\\n-  declare_constant(VM_Version::CPU_AVX2)                            \\\n-  declare_constant(VM_Version::CPU_AES)                             \\\n-  declare_constant(VM_Version::CPU_ERMS)                            \\\n-  declare_constant(VM_Version::CPU_CLMUL)                           \\\n-  declare_constant(VM_Version::CPU_BMI1)                            \\\n-  declare_constant(VM_Version::CPU_BMI2)                            \\\n-  declare_constant(VM_Version::CPU_RTM)                             \\\n-  declare_constant(VM_Version::CPU_ADX)                             \\\n-  declare_constant(VM_Version::CPU_AVX512F)                         \\\n-  declare_constant(VM_Version::CPU_AVX512DQ)                        \\\n-  declare_constant(VM_Version::CPU_AVX512PF)                        \\\n-  declare_constant(VM_Version::CPU_AVX512ER)                        \\\n-  declare_constant(VM_Version::CPU_AVX512CD)                        \\\n-  declare_constant(VM_Version::CPU_AVX512BW)                        \\\n-  declare_constant(VM_Version::CPU_AVX512VL)                        \\\n-  declare_constant(VM_Version::CPU_SHA)                             \\\n-  declare_constant(VM_Version::CPU_FMA)                             \\\n-  declare_constant(VM_Version::CPU_VZEROUPPER)                      \\\n-  declare_constant(VM_Version::CPU_AVX512_VPOPCNTDQ)                \\\n-  declare_constant(VM_Version::CPU_AVX512_VPCLMULQDQ)               \\\n-  declare_constant(VM_Version::CPU_AVX512_VAES)                     \\\n-  declare_constant(VM_Version::CPU_AVX512_VNNI)                     \\\n-  declare_constant(VM_Version::CPU_FLUSH)                           \\\n-  declare_constant(VM_Version::CPU_FLUSHOPT)                        \\\n-  declare_constant(VM_Version::CPU_CLWB)                            \\\n-  declare_constant(VM_Version::CPU_AVX512_VBMI2)                    \\\n-  declare_constant(VM_Version::CPU_AVX512_VBMI)                     \\\n-  declare_constant(VM_Version::CPU_HV)\n+#define DECLARE_LONG_CPU_FEATURE_CONSTANT(id, name, bit) GENERATE_VM_LONG_CONSTANT_ENTRY(VM_Version::CPU_##id)\n+#define VM_LONG_CPU_FEATURE_CONSTANTS CPU_FEATURE_FLAGS(DECLARE_LONG_CPU_FEATURE_CONSTANT)\n@@ -904,1 +847,3 @@\n-\n+#ifdef VM_INT_CPU_FEATURE_CONSTANTS\n+  VM_INT_CPU_FEATURE_CONSTANTS\n+#endif\n@@ -917,1 +862,3 @@\n-\n+#ifdef VM_LONG_CPU_FEATURE_CONSTANTS\n+  VM_LONG_CPU_FEATURE_CONSTANTS\n+#endif\n@@ -920,0 +867,1 @@\n+#undef DECLARE_CPU_FEATURE_FLAG\n@@ -925,3 +873,0 @@\n-  VM_ADDRESSES_COMPILER_RUNTIME(GENERATE_VM_ADDRESS_ENTRY,\n-               GENERATE_PREPROCESSOR_VM_ADDRESS_ENTRY,\n-               GENERATE_VM_FUNCTION_ENTRY)\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":13,"deletions":68,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -39,1 +39,0 @@\n-  LOG_TAG(aot) \\\n@@ -177,0 +176,1 @@\n+  LOG_TAG(suspend) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -26,1 +26,2 @@\n-#include \"aot\/aotLoader.hpp\"\n+#include \"cds\/heapShared.hpp\"\n+#include \"cds\/metaspaceShared.hpp\"\n@@ -47,1 +48,0 @@\n-#include \"memory\/heapShared.hpp\"\n@@ -51,1 +51,0 @@\n-#include \"memory\/metaspaceShared.hpp\"\n@@ -793,2 +792,0 @@\n-  AOTLoader::universe_init();\n-\n@@ -870,4 +867,8 @@\n-  bool use_large_pages = UseLargePages && is_aligned(alignment, os::large_page_size());\n-  assert(!UseLargePages\n-      || UseParallelGC\n-      || use_large_pages, \"Wrong alignment to use large pages\");\n+  size_t page_size = os::vm_page_size();\n+  if (UseLargePages && is_aligned(alignment, os::large_page_size())) {\n+    page_size = os::large_page_size();\n+  } else {\n+    \/\/ Parallel is the only collector that might opt out of using large pages\n+    \/\/ for the heap.\n+    assert(!UseLargePages || UseParallelGC , \"Wrong alignment to use large pages\");\n+  }\n@@ -876,1 +877,1 @@\n-  ReservedHeapSpace total_rs(total_reserved, alignment, use_large_pages, AllocateHeapAt);\n+  ReservedHeapSpace total_rs(total_reserved, alignment, page_size, AllocateHeapAt);\n@@ -902,1 +903,1 @@\n-  return ReservedHeapSpace(0, 0, false);\n+  return ReservedHeapSpace(0, 0, os::vm_page_size());\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":12,"deletions":11,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/heapShared.hpp\"\n@@ -39,1 +40,0 @@\n-#include \"memory\/heapShared.hpp\"\n@@ -359,0 +359,3 @@\n+  if (!_pool_holder->is_linked() && !_pool_holder->is_rewritten()) {\n+    return;\n+  }\n@@ -396,0 +399,3 @@\n+  if (!_pool_holder->is_linked() && _pool_holder->is_shared_old_klass()) {\n+    return;\n+  }\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/heapShared.hpp\"\n@@ -36,1 +37,0 @@\n-#include \"memory\/heapShared.hpp\"\n","filename":"src\/hotspot\/share\/oops\/cpCache.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -334,1 +334,1 @@\n-Klass* FlatArrayKlass::array_klass_impl(bool or_null, int n, TRAPS) {\n+Klass* FlatArrayKlass::array_klass(int n, TRAPS) {\n@@ -339,0 +339,1 @@\n+  \/\/ lock-free read needs acquire semantics\n@@ -340,1 +341,0 @@\n-    if (or_null)  return NULL;\n@@ -342,1 +342,1 @@\n-    ResourceMark rm;\n+    ResourceMark rm(THREAD);\n@@ -351,2 +351,1 @@\n-        Klass* k =\n-          ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, CHECK_NULL);\n+        Klass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, CHECK_NULL);\n@@ -355,1 +354,1 @@\n-        OrderAccess::storestore();\n+        \/\/ use 'release' to pair with lock-free load\n@@ -360,2 +359,0 @@\n-  } else {\n-    CHECK_UNHANDLED_OOPS_ONLY(Thread::current()->clear_unhandled_oops());\n@@ -365,3 +362,1 @@\n-  if (or_null) {\n-    return ak->array_klass_or_null(n);\n-  }\n+  THREAD->as_Java_thread()->check_possible_safepoint();\n@@ -371,2 +366,17 @@\n-Klass* FlatArrayKlass::array_klass_impl(bool or_null, TRAPS) {\n-  return array_klass_impl(or_null, dimension() +  1, THREAD);\n+Klass* FlatArrayKlass::array_klass_or_null(int n) {\n+\n+  assert(dimension() <= n, \"check order of chain\");\n+  int dim = dimension();\n+  if (dim == n) return this;\n+\n+  \/\/ lock-free read needs acquire semantics\n+  if (higher_dimension_acquire() == NULL) {\n+    return NULL;\n+  }\n+\n+  ObjArrayKlass *ak = ObjArrayKlass::cast(higher_dimension());\n+  return ak->array_klass_or_null(n);\n+}\n+\n+Klass* FlatArrayKlass::array_klass(TRAPS) {\n+  return array_klass(dimension() +  1, THREAD);\n@@ -375,0 +385,5 @@\n+Klass* FlatArrayKlass::array_klass_or_null() {\n+  return array_klass_or_null(dimension() +  1);\n+}\n+\n+\n","filename":"src\/hotspot\/share\/oops\/flatArrayKlass.cpp","additions":28,"deletions":13,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -46,7 +46,0 @@\n- protected:\n-  \/\/ Returns the ArrayKlass for n'th dimension.\n-  Klass* array_klass_impl(bool or_null, int n, TRAPS);\n-\n-  \/\/ Returns the array class with this class as element type.\n-  Klass* array_klass_impl(bool or_null, TRAPS);\n-\n@@ -57,0 +50,8 @@\n+  \/\/ Returns the ObjArrayKlass for n'th dimension.\n+  virtual Klass* array_klass(int n, TRAPS);\n+  virtual Klass* array_klass_or_null(int n);\n+\n+  \/\/ Returns the array class with this class as element type.\n+  virtual Klass* array_klass(TRAPS);\n+  virtual Klass* array_klass_or_null();\n+\n","filename":"src\/hotspot\/share\/oops\/flatArrayKlass.hpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2115,1 +2115,1 @@\n-void GenerateOopMap::compute_map(TRAPS) {\n+bool GenerateOopMap::compute_map(Thread* current) {\n@@ -2161,1 +2161,1 @@\n-    return;\n+    return true;\n@@ -2173,1 +2173,1 @@\n-    do_interpretation(THREAD);\n+    do_interpretation(current);\n@@ -2179,3 +2179,1 @@\n-  if (_got_error) {\n-    THROW_HANDLE(_exception);\n-  }\n+  return !_got_error;\n@@ -2185,6 +2183,3 @@\n-\/\/ These methods create an exception for the current thread which is thrown\n-\/\/ at the bottom of the call stack, when it returns to compute_map().  The\n-\/\/ _got_error flag controls execution.  NOT TODO: The VM exception propagation\n-\/\/ mechanism using TRAPS\/CHECKs could be used here instead but it would need\n-\/\/ to be added as a parameter to every function and checked for every call.\n-\/\/ The tons of extra code it would generate didn't seem worth the change.\n+\/\/ If we compute from a suitable JavaThread then we create an exception for the GenerateOopMap\n+\/\/ calling code to retrieve (via exception()) and throw if desired (in most cases errors are ignored).\n+\/\/ Otherwise it is considered a fatal error to hit malformed bytecode.\n@@ -2199,3 +2194,3 @@\n-  if (Thread::current()->can_call_java()) {\n-    _exception = Exceptions::new_exception(Thread::current(),\n-                  vmSymbols::java_lang_LinkageError(), msg_buffer2);\n+  Thread* current = Thread::current();\n+  if (current->can_call_java()) {\n+    _exception = Exceptions::new_exception(current, vmSymbols::java_lang_LinkageError(), msg_buffer2);\n@@ -2203,2 +2198,0 @@\n-    \/\/ We cannot instantiate an exception object from a compiler thread.\n-    \/\/ Exit the VM with a useful error message.\n@@ -2590,1 +2583,3 @@\n-  compute_map(CHECK_(methodHandle()));\n+  if (!compute_map(THREAD)) {\n+    THROW_HANDLE_(exception(), methodHandle());\n+  }\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.cpp","additions":13,"deletions":18,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -468,2 +468,5 @@\n-  \/\/ Compute the map.\n-  void compute_map(TRAPS);\n+  \/\/ Compute the map - returns true on success and false on error.\n+  bool compute_map(Thread* current);\n+  \/\/ Returns the exception related to any error, if the map was computed by a suitable JavaThread.\n+  Handle exception() { return _exception; }\n+\n@@ -568,1 +571,1 @@\n-  \/\/ Call compute_map(CHECK) to generate info.\n+  \/\/ Call compute_map() to generate info.\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.hpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -182,1 +182,1 @@\n-Klass* InlineKlass::array_klass_impl(bool or_null, int n, TRAPS) {\n+Klass* InlineKlass::array_klass(int n, TRAPS) {\n@@ -185,2 +185,0 @@\n-    if (or_null) return NULL;\n-\n@@ -188,1 +186,1 @@\n-    JavaThread *jt = (JavaThread *)THREAD;\n+    JavaThread *jt = THREAD->as_Java_thread();\n@@ -193,1 +191,0 @@\n-      ArrayKlass* k = NULL;\n@@ -196,0 +193,1 @@\n+        ArrayKlass* k;\n@@ -206,1 +204,1 @@\n-  \/\/ _this will always be set at this point\n+  \/\/ array_klasses() will always be set at this point\n@@ -208,1 +206,9 @@\n-  if (or_null) {\n+  return ak->array_klass(n, THREAD);\n+}\n+\n+Klass* InlineKlass::array_klass_or_null(int n) {\n+  \/\/ Need load-acquire for lock-free read\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == NULL) {\n+    return NULL;\n+  } else {\n@@ -211,1 +217,0 @@\n-  return ak->array_klass(n, THREAD);\n@@ -214,2 +219,2 @@\n-Klass* InlineKlass::array_klass_impl(bool or_null, TRAPS) {\n-  return array_klass_impl(or_null, 1, THREAD);\n+Klass* InlineKlass::array_klass(TRAPS) {\n+  return array_klass(1, THREAD);\n@@ -218,0 +223,5 @@\n+Klass* InlineKlass::array_klass_or_null() {\n+  return array_klass_or_null(1);\n+}\n+\n+\n","filename":"src\/hotspot\/share\/oops\/inlineKlass.cpp","additions":20,"deletions":10,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -131,2 +131,1 @@\n-\n- protected:\n+ public:\n@@ -134,1 +133,2 @@\n-  Klass* array_klass_impl(bool or_null, int n, TRAPS);\n+  virtual Klass* array_klass(int n, TRAPS);\n+  virtual Klass* array_klass_or_null(int n);\n@@ -137,1 +137,3 @@\n-  Klass* array_klass_impl(bool or_null, TRAPS);\n+  virtual Klass* array_klass(TRAPS);\n+  virtual Klass* array_klass_or_null();\n+\n@@ -139,1 +141,0 @@\n- public:\n","filename":"src\/hotspot\/share\/oops\/inlineKlass.hpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -40,5 +40,0 @@\n-  address adr_fing = adr_fingerprint();\n-  if (adr_fing != NULL) {\n-    return (InlineKlassFixedBlock*)(adr_fingerprint() + sizeof(u8));\n-  }\n-\n","filename":"src\/hotspot\/share\/oops\/inlineKlass.inline.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -27,1 +27,3 @@\n-#include \"aot\/aotLoader.hpp\"\n+#include \"cds\/archiveUtils.hpp\"\n+#include \"cds\/classListWriter.hpp\"\n+#include \"cds\/metaspaceShared.hpp\"\n@@ -30,1 +32,0 @@\n-#include \"classfile\/classListWriter.hpp\"\n@@ -53,1 +54,0 @@\n-#include \"memory\/archiveUtils.hpp\"\n@@ -57,1 +57,0 @@\n-#include \"memory\/metaspaceShared.hpp\"\n@@ -474,1 +473,0 @@\n-                                       should_store_fingerprint(is_hidden_or_anonymous),\n@@ -1285,3 +1283,0 @@\n-  \/\/ Look for aot compiled methods for this klass, including class initializer.\n-  AOTLoader::load_for_klass(this, THREAD);\n-\n@@ -1571,1 +1566,1 @@\n-Klass* InstanceKlass::array_klass_impl(bool or_null, int n, TRAPS) {\n+Klass* InstanceKlass::array_klass(int n, TRAPS) {\n@@ -1574,2 +1569,0 @@\n-    if (or_null) return NULL;\n-\n@@ -1590,4 +1583,12 @@\n-  \/\/ _this will always be set at this point\n-  ArrayKlass* oak = array_klasses();\n-  if (or_null) {\n-    return oak->array_klass_or_null(n);\n+  \/\/ array_klasses() will always be set at this point\n+  ArrayKlass* ak = array_klasses();\n+  return ak->array_klass(n, THREAD);\n+}\n+\n+Klass* InstanceKlass::array_klass_or_null(int n) {\n+  \/\/ Need load-acquire for lock-free read\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == NULL) {\n+    return NULL;\n+  } else {\n+    return ak->array_klass_or_null(n);\n@@ -1595,1 +1596,0 @@\n-  return oak->array_klass(n, THREAD);\n@@ -1598,2 +1598,6 @@\n-Klass* InstanceKlass::array_klass_impl(bool or_null, TRAPS) {\n-  return array_klass_impl(or_null, 1, THREAD);\n+Klass* InstanceKlass::array_klass(TRAPS) {\n+  return array_klass(1, THREAD);\n+}\n+\n+Klass* InstanceKlass::array_klass_or_null() {\n+  return array_klass_or_null(1);\n@@ -2519,71 +2523,0 @@\n-bool InstanceKlass::supers_have_passed_fingerprint_checks() {\n-  if (java_super() != NULL && !java_super()->has_passed_fingerprint_check()) {\n-    ResourceMark rm;\n-    log_trace(class, fingerprint)(\"%s : super %s not fingerprinted\", external_name(), java_super()->external_name());\n-    return false;\n-  }\n-\n-  Array<InstanceKlass*>* local_interfaces = this->local_interfaces();\n-  if (local_interfaces != NULL) {\n-    int length = local_interfaces->length();\n-    for (int i = 0; i < length; i++) {\n-      InstanceKlass* intf = local_interfaces->at(i);\n-      if (!intf->has_passed_fingerprint_check()) {\n-        ResourceMark rm;\n-        log_trace(class, fingerprint)(\"%s : interface %s not fingerprinted\", external_name(), intf->external_name());\n-        return false;\n-      }\n-    }\n-  }\n-\n-  return true;\n-}\n-\n-bool InstanceKlass::should_store_fingerprint(bool is_hidden_or_anonymous) {\n-#if INCLUDE_AOT\n-  \/\/ We store the fingerprint into the InstanceKlass only in the following 2 cases:\n-  if (CalculateClassFingerprint) {\n-    \/\/ (1) We are running AOT to generate a shared library.\n-    return true;\n-  }\n-  if (Arguments::is_dumping_archive()) {\n-    \/\/ (2) We are running -Xshare:dump or -XX:ArchiveClassesAtExit to create a shared archive\n-    return true;\n-  }\n-  if (UseAOT && is_hidden_or_anonymous) {\n-    \/\/ (3) We are using AOT code from a shared library and see a hidden or unsafe anonymous class\n-    return true;\n-  }\n-#endif\n-\n-  \/\/ In all other cases we might set the _misc_has_passed_fingerprint_check bit,\n-  \/\/ but do not store the 64-bit fingerprint to save space.\n-  return false;\n-}\n-\n-bool InstanceKlass::has_stored_fingerprint() const {\n-#if INCLUDE_AOT\n-  return should_store_fingerprint() || is_shared();\n-#else\n-  return false;\n-#endif\n-}\n-\n-uint64_t InstanceKlass::get_stored_fingerprint() const {\n-  address adr = adr_fingerprint();\n-  if (adr != NULL) {\n-    return (uint64_t)Bytes::get_native_u8(adr); \/\/ adr may not be 64-bit aligned\n-  }\n-  return 0;\n-}\n-\n-void InstanceKlass::store_fingerprint(uint64_t fingerprint) {\n-  address adr = adr_fingerprint();\n-  if (adr != NULL) {\n-    Bytes::put_native_u8(adr, (u8)fingerprint); \/\/ adr may not be 64-bit aligned\n-\n-    ResourceMark rm;\n-    log_trace(class, fingerprint)(\"stored as \" PTR64_FORMAT \" for class %s\", fingerprint, external_name());\n-  }\n-}\n-\n@@ -2600,1 +2533,5 @@\n-  it->push(&_constants);\n+  if (!is_rewritten()) {\n+    it->push(&_constants, MetaspaceClosure::_writable);\n+  } else {\n+    it->push(&_constants);\n+  }\n@@ -2643,0 +2580,6 @@\n+\n+  if (MetaspaceShared::is_old_class(this)) {\n+    \/\/ Set the old class bit.\n+    set_is_shared_old_klass();\n+  }\n+\n@@ -4234,24 +4177,0 @@\n-    \/\/ At least one method is live in this previous version.\n-    \/\/ Reset dead EMCP methods not to get breakpoints.\n-    \/\/ All methods are deallocated when all of the methods for this class are no\n-    \/\/ longer running.\n-    Array<Method*>* method_refs = pv_node->methods();\n-    if (method_refs != NULL) {\n-      log_trace(redefine, class, iklass, purge)(\"previous methods length=%d\", method_refs->length());\n-      for (int j = 0; j < method_refs->length(); j++) {\n-        Method* method = method_refs->at(j);\n-\n-        if (!method->on_stack()) {\n-          \/\/ no breakpoints for non-running methods\n-          if (method->is_running_emcp()) {\n-            method->set_running_emcp(false);\n-          }\n-        } else {\n-          assert (method->is_obsolete() || method->is_running_emcp(),\n-                  \"emcp method cannot run after emcp bit is cleared\");\n-          log_trace(redefine, class, iklass, purge)\n-            (\"purge: %s(%s): prev method @%d in version @%d is alive\",\n-             method->name()->as_C_string(), method->signature()->as_C_string(), j, version);\n-        }\n-      }\n-    }\n@@ -4357,23 +4276,0 @@\n-  if (emcp_method_count != 0) {\n-    \/\/ At least one method is still running, check for EMCP methods\n-    for (int i = 0; i < old_methods->length(); i++) {\n-      Method* old_method = old_methods->at(i);\n-      if (!old_method->is_obsolete() && old_method->on_stack()) {\n-        \/\/ if EMCP method (not obsolete) is on the stack, mark as EMCP so that\n-        \/\/ we can add breakpoints for it.\n-\n-        \/\/ We set the method->on_stack bit during safepoints for class redefinition\n-        \/\/ and use this bit to set the is_running_emcp bit.\n-        \/\/ After the safepoint, the on_stack bit is cleared and the running emcp\n-        \/\/ method may exit.   If so, we would set a breakpoint in a method that\n-        \/\/ is never reached, but this won't be noticeable to the programmer.\n-        old_method->set_running_emcp(true);\n-        log_trace(redefine, class, iklass, add)\n-          (\"EMCP method %s is on_stack \" INTPTR_FORMAT, old_method->name_and_sig_as_C_string(), p2i(old_method));\n-      } else if (!old_method->is_obsolete()) {\n-        log_trace(redefine, class, iklass, add)\n-          (\"EMCP method %s is NOT on_stack \" INTPTR_FORMAT, old_method->name_and_sig_as_C_string(), p2i(old_method));\n-      }\n-    }\n-  }\n-\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":33,"deletions":137,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-#include \"runtime\/os.hpp\"\n@@ -56,1 +55,0 @@\n-\/\/    [EMBEDDED fingerprint       ] only if should_store_fingerprint()==true\n@@ -280,2 +278,1 @@\n-    _misc_has_passed_fingerprint_check        = 1 << 8,  \/\/ when this class was loaded, the fingerprint computed from its\n-                                                         \/\/ code source was found to be matching the value recorded by AOT.\n+    _unused                                   = 1 << 8,  \/\/\n@@ -912,18 +909,0 @@\n-  bool has_passed_fingerprint_check() const {\n-    return (_misc_flags & _misc_has_passed_fingerprint_check) != 0;\n-  }\n-  void set_has_passed_fingerprint_check(bool b) {\n-    if (b) {\n-      _misc_flags |= _misc_has_passed_fingerprint_check;\n-    } else {\n-      _misc_flags &= ~_misc_has_passed_fingerprint_check;\n-    }\n-  }\n-  bool supers_have_passed_fingerprint_checks();\n-\n-  static bool should_store_fingerprint(bool is_hidden_or_anonymous);\n-  bool should_store_fingerprint() const { return should_store_fingerprint(is_hidden() || is_unsafe_anonymous()); }\n-  bool has_stored_fingerprint() const;\n-  uint64_t get_stored_fingerprint() const;\n-  void store_fingerprint(uint64_t fingerprint);\n-\n@@ -1191,1 +1170,1 @@\n-                  bool is_interface, bool is_unsafe_anonymous, bool has_stored_fingerprint,\n+                  bool is_interface, bool is_unsafe_anonymous,\n@@ -1199,1 +1178,0 @@\n-           (has_stored_fingerprint ? (int)sizeof(uint64_t*)\/wordSize : 0) +\n@@ -1208,1 +1186,0 @@\n-                                               has_stored_fingerprint(),\n@@ -1224,1 +1201,0 @@\n-  inline address adr_fingerprint() const;\n@@ -1362,0 +1338,9 @@\n+\n+  \/\/ Returns the array class for the n'th dimension\n+  virtual Klass* array_klass(int n, TRAPS);\n+  virtual Klass* array_klass_or_null(int n);\n+\n+  \/\/ Returns the array class with this class as element type\n+  virtual Klass* array_klass(TRAPS);\n+  virtual Klass* array_klass_or_null();\n+\n@@ -1372,8 +1357,0 @@\n-protected:\n-  \/\/ Returns the array class for the n'th dimension\n-  virtual Klass* array_klass_impl(bool or_null, int n, TRAPS);\n-\n-  \/\/ Returns the array class with this class as element type\n-  virtual Klass* array_klass_impl(bool or_null, TRAPS);\n-\n-private:\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":11,"deletions":34,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -98,18 +98,0 @@\n-inline address InstanceKlass::adr_fingerprint() const {\n-  if (has_stored_fingerprint()) {\n-    InstanceKlass** adr_host = adr_unsafe_anonymous_host();\n-    if (adr_host != NULL) {\n-      return (address)(adr_host + 1);\n-    }\n-\n-    InstanceKlass* volatile* adr_impl = adr_implementor();\n-    if (adr_impl != NULL) {\n-      return (address)(adr_impl + 1);\n-    }\n-\n-    return (address)end_of_nonstatic_oop_maps();\n-  } else {\n-    return NULL;\n-  }\n-}\n-\n@@ -118,5 +100,0 @@\n-    address adr_fing = adr_fingerprint();\n-    if (adr_fing != NULL) {\n-      return adr_fingerprint() + sizeof(u8);\n-    }\n-\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.inline.hpp","additions":0,"deletions":23,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/heapShared.hpp\"\n@@ -37,1 +38,0 @@\n-#include \"memory\/heapShared.hpp\"\n@@ -647,27 +647,0 @@\n-Klass* Klass::array_klass_or_null(int rank) {\n-  EXCEPTION_MARK;\n-  \/\/ No exception can be thrown by array_klass_impl when called with or_null == true.\n-  \/\/ (In anycase, the execption mark will fail if it do so)\n-  return array_klass_impl(true, rank, THREAD);\n-}\n-\n-\n-Klass* Klass::array_klass_or_null() {\n-  EXCEPTION_MARK;\n-  \/\/ No exception can be thrown by array_klass_impl when called with or_null == true.\n-  \/\/ (In anycase, the execption mark will fail if it do so)\n-  return array_klass_impl(true, THREAD);\n-}\n-\n-\n-Klass* Klass::array_klass_impl(bool or_null, int rank, TRAPS) {\n-  fatal(\"array_klass should be dispatched to InstanceKlass, ObjArrayKlass or TypeArrayKlass\");\n-  return NULL;\n-}\n-\n-\n-Klass* Klass::array_klass_impl(bool or_null, TRAPS) {\n-  fatal(\"array_klass should be dispatched to InstanceKlass, ObjArrayKlass or TypeArrayKlass\");\n-  return NULL;\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":1,"deletions":28,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -181,1 +181,2 @@\n-    _has_value_based_class_annotation = 4\n+    _has_value_based_class_annotation = 4,\n+    _is_shared_old_klass = 8\n@@ -337,0 +338,8 @@\n+  void set_is_shared_old_klass() {\n+    CDS_ONLY(_shared_class_flags |= _is_shared_old_klass;)\n+  }\n+  bool is_shared_old_klass() const {\n+    CDS_ONLY(return (_shared_class_flags & _is_shared_old_klass) != 0;)\n+    NOT_CDS(return false;)\n+  }\n+\n@@ -517,1 +526,1 @@\n-  Klass* array_klass(int rank, TRAPS)         {  return array_klass_impl(false, rank, THREAD); }\n+  virtual Klass* array_klass(int rank, TRAPS) = 0;\n@@ -520,1 +529,1 @@\n-  Klass* array_klass(TRAPS)                   {  return array_klass_impl(false, THREAD); }\n+  virtual Klass* array_klass(TRAPS) = 0;\n@@ -523,3 +532,2 @@\n-  \/\/ NB: these can block for a mutex, like other functions with TRAPS arg.\n-  Klass* array_klass_or_null(int rank);\n-  Klass* array_klass_or_null();\n+  virtual Klass* array_klass_or_null(int rank) = 0;\n+  virtual Klass* array_klass_or_null() = 0;\n@@ -534,2 +542,0 @@\n-  virtual Klass* array_klass_impl(bool or_null, int rank, TRAPS);\n-  virtual Klass* array_klass_impl(bool or_null, TRAPS);\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":14,"deletions":8,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/metaspaceShared.hpp\"\n@@ -34,1 +35,0 @@\n-#include \"memory\/metaspaceShared.hpp\"\n@@ -54,1 +54,1 @@\n-  return _klass->is_shared() && !MetaspaceShared::remapped_readwrite();\n+  return _klass->is_shared() && !MetaspaceShared::remapped_readwrite() && !_klass->is_shared_old_klass();\n@@ -1096,1 +1096,2 @@\n-     !MetaspaceShared::remapped_readwrite()) {\n+     !MetaspaceShared::remapped_readwrite() &&\n+     !MetaspaceShared::is_old_class(m->method_holder())) {\n","filename":"src\/hotspot\/share\/oops\/klassVtable.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"cds\/cppVtables.hpp\"\n+#include \"cds\/metaspaceShared.hpp\"\n@@ -44,1 +46,0 @@\n-#include \"memory\/cppVtables.hpp\"\n@@ -47,1 +48,0 @@\n-#include \"memory\/metaspaceShared.hpp\"\n@@ -357,1 +357,5 @@\n-  it->push(&_constMethod);\n+  if (!method_holder()->is_rewritten()) {\n+    it->push(&_constMethod, MetaspaceClosure::_writable);\n+  } else {\n+    it->push(&_constMethod);\n+  }\n@@ -360,2 +364,0 @@\n-\n-  Method* this_ptr = this;\n@@ -375,1 +377,1 @@\n-  if (is_shared() && !MetaspaceShared::remapped_readwrite()) {\n+  if (is_shared() && !MetaspaceShared::remapped_readwrite() && !method_holder()->is_shared_old_klass()) {\n@@ -386,1 +388,1 @@\n-  if (is_shared() && !MetaspaceShared::remapped_readwrite()) {\n+  if (is_shared() && !MetaspaceShared::remapped_readwrite() && !method_holder()->is_shared_old_klass()) {\n@@ -2296,2 +2298,0 @@\n-  assert(!value || !is_old() || is_obsolete() || is_running_emcp(),\n-         \"emcp methods cannot run after emcp bit is cleared\");\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -91,7 +91,6 @@\n-    _running_emcp          = 1 << 5,\n-    _intrinsic_candidate   = 1 << 6,\n-    _reserved_stack_access = 1 << 7,\n-    _scalarized_args       = 1 << 8,\n-    _c1_needs_stack_repair = 1 << 9,\n-    _c2_needs_stack_repair = 1 << 10,\n-    _scoped                = 1 << 11\n+    _intrinsic_candidate   = 1 << 5,\n+    _reserved_stack_access = 1 << 6,\n+    _scalarized_args       = 1 << 7,\n+    _c1_needs_stack_repair = 1 << 8,\n+    _c2_needs_stack_repair = 1 << 9,\n+    _scoped                = 1 << 10\n@@ -121,4 +120,0 @@\n-#if INCLUDE_AOT\n-  CompiledMethod* _aot_code;\n-#endif\n-\n@@ -413,12 +408,0 @@\n-#if INCLUDE_AOT\n-  void set_aot_code(CompiledMethod* aot_code) {\n-    _aot_code = aot_code;\n-  }\n-\n-  CompiledMethod* aot_code() const {\n-    return _aot_code;\n-  }\n-#else\n-  CompiledMethod* aot_code() const { return NULL; }\n-#endif \/\/ INCLUDE_AOT\n-\n@@ -695,2 +678,0 @@\n-  bool has_aot_code() const                      { return aot_code() != NULL; }\n-\n@@ -785,14 +766,0 @@\n-  bool is_running_emcp() const {\n-    \/\/ EMCP methods are old but not obsolete or deleted. Equivalent\n-    \/\/ Modulo Constant Pool means the method is equivalent except\n-    \/\/ the constant pool and instructions that access the constant\n-    \/\/ pool might be different.\n-    \/\/ If a breakpoint is set in a redefined method, its EMCP methods that are\n-    \/\/ still running must have a breakpoint also.\n-    return (_flags & _running_emcp) != 0;\n-  }\n-\n-  void set_running_emcp(bool x) {\n-    _flags = x ? (_flags | _running_emcp) : (_flags & ~_running_emcp);\n-  }\n-\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":6,"deletions":39,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -324,1 +324,1 @@\n-Klass* ObjArrayKlass::array_klass_impl(bool or_null, int n, TRAPS) {\n+Klass* ObjArrayKlass::array_klass(int n, TRAPS) {\n@@ -331,1 +331,0 @@\n-    if (or_null) return NULL;\n@@ -353,4 +352,1 @@\n-  if (or_null) {\n-    return ak->array_klass_or_null(n);\n-  }\n-  THREAD->check_possible_safepoint();\n+  THREAD->as_Java_thread()->check_possible_safepoint();\n@@ -360,2 +356,21 @@\n-Klass* ObjArrayKlass::array_klass_impl(bool or_null, TRAPS) {\n-  return array_klass_impl(or_null, dimension() +  1, THREAD);\n+Klass* ObjArrayKlass::array_klass_or_null(int n) {\n+\n+  assert(dimension() <= n, \"check order of chain\");\n+  int dim = dimension();\n+  if (dim == n) return this;\n+\n+  \/\/ lock-free read needs acquire semantics\n+  if (higher_dimension_acquire() == NULL) {\n+    return NULL;\n+  }\n+\n+  ObjArrayKlass *ak = ObjArrayKlass::cast(higher_dimension());\n+  return ak->array_klass_or_null(n);\n+}\n+\n+Klass* ObjArrayKlass::array_klass(TRAPS) {\n+  return array_klass(dimension() +  1, THREAD);\n+}\n+\n+Klass* ObjArrayKlass::array_klass_or_null() {\n+  return array_klass_or_null(dimension() +  1);\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":23,"deletions":8,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -89,1 +89,1 @@\n- protected:\n+ public:\n@@ -91,1 +91,2 @@\n-  virtual Klass* array_klass_impl(bool or_null, int n, TRAPS);\n+  virtual Klass* array_klass(int n, TRAPS);\n+  virtual Klass* array_klass_or_null(int n);\n@@ -94,3 +95,2 @@\n-  virtual Klass* array_klass_impl(bool or_null, TRAPS);\n-\n- public:\n+  virtual Klass* array_klass(TRAPS);\n+  virtual Klass* array_klass_or_null();\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -28,1 +29,0 @@\n-#include \"memory\/heapShared.inline.hpp\"\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,1 +37,0 @@\n-#include \"runtime\/os.hpp\"\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"cds\/metaspaceShared.hpp\"\n@@ -34,1 +35,0 @@\n-#include \"memory\/metaspaceShared.hpp\"\n","filename":"src\/hotspot\/share\/oops\/symbol.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -173,1 +173,1 @@\n-Klass* TypeArrayKlass::array_klass_impl(bool or_null, int n, TRAPS) {\n+Klass* TypeArrayKlass::array_klass(int n, TRAPS) {\n@@ -181,1 +181,0 @@\n-    if (or_null)  return NULL;\n@@ -202,4 +201,1 @@\n-  if (or_null) {\n-    return h_ak->array_klass_or_null(n);\n-  }\n-  THREAD->check_possible_safepoint();\n+  THREAD->as_Java_thread()->check_possible_safepoint();\n@@ -209,2 +205,22 @@\n-Klass* TypeArrayKlass::array_klass_impl(bool or_null, TRAPS) {\n-  return array_klass_impl(or_null, dimension() +  1, THREAD);\n+\/\/ return existing klass of array holding typeArrays\n+Klass* TypeArrayKlass::array_klass_or_null(int n) {\n+  int dim = dimension();\n+  assert(dim <= n, \"check order of chain\");\n+    if (dim == n)\n+      return this;\n+\n+  \/\/ lock-free read needs acquire semantics\n+  if (higher_dimension_acquire() == NULL) {\n+    return NULL;\n+  }\n+\n+  ObjArrayKlass* h_ak = ObjArrayKlass::cast(higher_dimension());\n+  return h_ak->array_klass_or_null(n);\n+}\n+\n+Klass* TypeArrayKlass::array_klass(TRAPS) {\n+  return array_klass(dimension() +  1, THREAD);\n+}\n+\n+Klass* TypeArrayKlass::array_klass_or_null() {\n+  return array_klass_or_null(dimension() +  1);\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":24,"deletions":8,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -115,2 +115,1 @@\n-                               int caller_bci, ciCallProfile& profile,\n-                               WarmCallInfo* wci_result) {\n+                               int caller_bci, ciCallProfile& profile) {\n@@ -119,5 +118,0 @@\n-    *wci_result = *(WarmCallInfo::always_hot());\n-    if (C->print_inlining() && Verbose) {\n-      CompileTask::print_inline_indent(inline_level());\n-      tty->print_cr(\"Inlined method is hot: \");\n-    }\n@@ -130,3 +124,3 @@\n-      set_msg(\"force inline by annotation\");\n-      _forced_inline = true;\n-      return true;\n+    set_msg(\"force inline by annotation\");\n+    _forced_inline = true;\n+    return true;\n@@ -149,1 +143,0 @@\n-    wci_result->set_profit(wci_result->profit() * 100);\n@@ -205,2 +198,1 @@\n-                                   JVMState* jvms,\n-                                   WarmCallInfo* wci_result) {\n+                                   JVMState* jvms) {\n@@ -364,1 +356,1 @@\n-                               WarmCallInfo* wci_result, bool& should_delay) {\n+                               bool& should_delay) {\n@@ -376,2 +368,1 @@\n-  if (!should_inline(callee_method, caller_method, caller_bci, profile,\n-                     wci_result)) {\n+  if (!should_inline(callee_method, caller_method, caller_bci, profile)) {\n@@ -380,1 +371,1 @@\n-  if (should_not_inline(callee_method, caller_method, jvms, wci_result)) {\n+  if (should_not_inline(callee_method, caller_method, jvms)) {\n@@ -563,1 +554,2 @@\n-WarmCallInfo* InlineTree::ok_to_inline(ciMethod* callee_method, JVMState* jvms, ciCallProfile& profile, WarmCallInfo* initial_wci, bool& should_delay) {\n+bool InlineTree::ok_to_inline(ciMethod* callee_method, JVMState* jvms, ciCallProfile& profile,\n+                              bool& should_delay) {\n@@ -583,1 +575,1 @@\n-    return NULL;\n+    return false;\n@@ -590,1 +582,1 @@\n-    return NULL;\n+    return false;\n@@ -594,19 +586,2 @@\n-  WarmCallInfo wci = *(initial_wci);\n-  bool success = try_to_inline(callee_method, caller_method, caller_bci,\n-                               jvms, profile, &wci, should_delay);\n-\n-#ifndef PRODUCT\n-  if (InlineWarmCalls && (PrintOpto || C->print_inlining())) {\n-    bool cold = wci.is_cold();\n-    bool hot  = !cold && wci.is_hot();\n-    bool old_cold = !success;\n-    if (old_cold != cold || (Verbose || WizardMode)) {\n-      if (msg() == NULL) {\n-        set_msg(\"OK\");\n-      }\n-      tty->print(\"   OldInlining= %4s : %s\\n           WCI=\",\n-                 old_cold ? \"cold\" : \"hot\", msg());\n-      wci.print();\n-    }\n-  }\n-#endif\n+  bool success = try_to_inline(callee_method, caller_method, caller_bci, jvms, profile,\n+                               should_delay); \/\/ out\n@@ -614,13 +589,0 @@\n-    wci = *(WarmCallInfo::always_hot());\n-  } else {\n-    wci = *(WarmCallInfo::always_cold());\n-  }\n-\n-  if (!InlineWarmCalls) {\n-    if (!wci.is_cold() && !wci.is_hot()) {\n-      \/\/ Do not inline the warm calls.\n-      wci = *(WarmCallInfo::always_cold());\n-    }\n-  }\n-\n-  if (!wci.is_cold()) {\n@@ -633,2 +595,5 @@\n-    if (InlineWarmCalls && !wci.is_hot()) {\n-      return new (C) WarmCallInfo(wci);  \/\/ copy to heap\n+    return true;\n+  } else {\n+    \/\/ Do not inline\n+    if (msg() == NULL) {\n+      set_msg(\"too cold to inline\");\n@@ -636,6 +601,2 @@\n-    return WarmCallInfo::always_hot();\n-  }\n-\n-  \/\/ Do not inline\n-  if (msg() == NULL) {\n-    set_msg(\"too cold to inline\");\n+    print_inlining(callee_method, caller_bci, caller_method, false \/* !success *\/ );\n+    return false;\n@@ -643,2 +604,0 @@\n-  print_inlining(callee_method, caller_bci, caller_method, false \/* !success *\/ );\n-  return NULL;\n","filename":"src\/hotspot\/share\/opto\/bytecodeInfo.cpp","additions":21,"deletions":62,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -422,40 +422,0 @@\n-  develop(intx, NodeCountInliningStep, 1000,                                \\\n-          \"Target size of warm calls inlined between optimization passes\")  \\\n-          range(0, max_jint)                                                \\\n-                                                                            \\\n-  develop(bool, InlineWarmCalls, false,                                     \\\n-          \"Use a heat-based priority queue to govern inlining\")             \\\n-                                                                            \\\n-  \/* Max values must not exceed WarmCallInfo::MAX_VALUE(). *\/               \\\n-  develop(intx, HotCallCountThreshold, 999999,                              \\\n-          \"large numbers of calls (per method invocation) force hotness\")   \\\n-          range(0, ((intx)MIN2((int64_t)max_intx,(int64_t)(+1.0e10))))      \\\n-                                                                            \\\n-  develop(intx, HotCallProfitThreshold, 999999,                             \\\n-          \"highly profitable inlining opportunities force hotness\")         \\\n-          range(0, ((intx)MIN2((int64_t)max_intx,(int64_t)(+1.0e10))))      \\\n-                                                                            \\\n-  develop(intx, HotCallTrivialWork, -1,                                     \\\n-          \"trivial execution time (no larger than this) forces hotness\")    \\\n-          range(-1, ((intx)MIN2((int64_t)max_intx,(int64_t)(+1.0e10))))     \\\n-                                                                            \\\n-  develop(intx, HotCallTrivialSize, -1,                                     \\\n-          \"trivial methods (no larger than this) force calls to be hot\")    \\\n-          range(-1, ((intx)MIN2((int64_t)max_intx,(int64_t)(+1.0e10))))     \\\n-                                                                            \\\n-  develop(intx, WarmCallMinCount, -1,                                       \\\n-          \"number of calls (per method invocation) to enable inlining\")     \\\n-          range(-1, ((intx)MIN2((int64_t)max_intx,(int64_t)(+1.0e10))))     \\\n-                                                                            \\\n-  develop(intx, WarmCallMinProfit, -1,                                      \\\n-          \"number of calls (per method invocation) to enable inlining\")     \\\n-          range(-1, ((intx)MIN2((int64_t)max_intx,(int64_t)(+1.0e10))))     \\\n-                                                                            \\\n-  develop(intx, WarmCallMaxWork, 999999,                                    \\\n-          \"execution time of the largest inlinable method\")                 \\\n-          range(0, ((intx)MIN2((int64_t)max_intx,(int64_t)(+1.0e10))))      \\\n-                                                                            \\\n-  develop(intx, WarmCallMaxSize, 999999,                                    \\\n-          \"size of the largest inlinable method\")                           \\\n-          range(0, ((intx)MIN2((int64_t)max_intx,(int64_t)(+1.0e10))))      \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":0,"deletions":40,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  assert(!CompilerConfig::is_c1_or_interpreter_only_no_aot_or_jvmci(), \"C2 compiler is launched, it's not c1\/interpreter only mode\");\n+  assert(!CompilerConfig::is_c1_or_interpreter_only_no_jvmci(), \"C2 compiler is launched, it's not c1\/interpreter only mode\");\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -950,75 +950,0 @@\n-\/\/---------------------------WarmCallGenerator--------------------------------\n-\/\/ Internal class which handles initial deferral of inlining decisions.\n-class WarmCallGenerator : public CallGenerator {\n-  WarmCallInfo*   _call_info;\n-  CallGenerator*  _if_cold;\n-  CallGenerator*  _if_hot;\n-  bool            _is_virtual;   \/\/ caches virtuality of if_cold\n-  bool            _is_inline;    \/\/ caches inline-ness of if_hot\n-\n-public:\n-  WarmCallGenerator(WarmCallInfo* ci,\n-                    CallGenerator* if_cold,\n-                    CallGenerator* if_hot)\n-    : CallGenerator(if_cold->method())\n-  {\n-    assert(method() == if_hot->method(), \"consistent choices\");\n-    _call_info  = ci;\n-    _if_cold    = if_cold;\n-    _if_hot     = if_hot;\n-    _is_virtual = if_cold->is_virtual();\n-    _is_inline  = if_hot->is_inline();\n-  }\n-\n-  virtual bool      is_inline() const           { return _is_inline; }\n-  virtual bool      is_virtual() const          { return _is_virtual; }\n-  virtual bool      is_deferred() const         { return true; }\n-\n-  virtual JVMState* generate(JVMState* jvms);\n-};\n-\n-\n-CallGenerator* CallGenerator::for_warm_call(WarmCallInfo* ci,\n-                                            CallGenerator* if_cold,\n-                                            CallGenerator* if_hot) {\n-  return new WarmCallGenerator(ci, if_cold, if_hot);\n-}\n-\n-JVMState* WarmCallGenerator::generate(JVMState* jvms) {\n-  Compile* C = Compile::current();\n-  C->print_inlining_update(this);\n-\n-  if (C->log() != NULL) {\n-    C->log()->elem(\"warm_call bci='%d'\", jvms->bci());\n-  }\n-  jvms = _if_cold->generate(jvms);\n-  if (jvms != NULL) {\n-    Node* m = jvms->map()->control();\n-    if (m->is_CatchProj()) m = m->in(0);  else m = C->top();\n-    if (m->is_Catch())     m = m->in(0);  else m = C->top();\n-    if (m->is_Proj())      m = m->in(0);  else m = C->top();\n-    if (m->is_CallJava()) {\n-      _call_info->set_call(m->as_Call());\n-      _call_info->set_hot_cg(_if_hot);\n-#ifndef PRODUCT\n-      if (PrintOpto || PrintOptoInlining) {\n-        tty->print_cr(\"Queueing for warm inlining at bci %d:\", jvms->bci());\n-        tty->print(\"WCI: \");\n-        _call_info->print();\n-      }\n-#endif\n-      _call_info->set_heat(_call_info->compute_heat());\n-      C->set_warm_calls(_call_info->insert_into(C->warm_calls()));\n-    }\n-  }\n-  return jvms;\n-}\n-\n-void WarmCallInfo::make_hot() {\n-  Unimplemented();\n-}\n-\n-void WarmCallInfo::make_cold() {\n-  \/\/ No action:  Just dequeue.\n-}\n-\n@@ -1689,155 +1614,0 @@\n-\n-#define NODES_OVERHEAD_PER_METHOD (30.0)\n-#define NODES_PER_BYTECODE (9.5)\n-\n-void WarmCallInfo::init(JVMState* call_site, ciMethod* call_method, ciCallProfile& profile, float prof_factor) {\n-  int call_count = profile.count();\n-  int code_size = call_method->code_size();\n-\n-  \/\/ Expected execution count is based on the historical count:\n-  _count = call_count < 0 ? 1 : call_site->method()->scale_count(call_count, prof_factor);\n-\n-  \/\/ Expected profit from inlining, in units of simple call-overheads.\n-  _profit = 1.0;\n-\n-  \/\/ Expected work performed by the call in units of call-overheads.\n-  \/\/ %%% need an empirical curve fit for \"work\" (time in call)\n-  float bytecodes_per_call = 3;\n-  _work = 1.0 + code_size \/ bytecodes_per_call;\n-\n-  \/\/ Expected size of compilation graph:\n-  \/\/ -XX:+PrintParseStatistics once reported:\n-  \/\/  Methods seen: 9184  Methods parsed: 9184  Nodes created: 1582391\n-  \/\/  Histogram of 144298 parsed bytecodes:\n-  \/\/ %%% Need an better predictor for graph size.\n-  _size = NODES_OVERHEAD_PER_METHOD + (NODES_PER_BYTECODE * code_size);\n-}\n-\n-\/\/ is_cold:  Return true if the node should never be inlined.\n-\/\/ This is true if any of the key metrics are extreme.\n-bool WarmCallInfo::is_cold() const {\n-  if (count()  <  WarmCallMinCount)        return true;\n-  if (profit() <  WarmCallMinProfit)       return true;\n-  if (work()   >  WarmCallMaxWork)         return true;\n-  if (size()   >  WarmCallMaxSize)         return true;\n-  return false;\n-}\n-\n-\/\/ is_hot:  Return true if the node should be inlined immediately.\n-\/\/ This is true if any of the key metrics are extreme.\n-bool WarmCallInfo::is_hot() const {\n-  assert(!is_cold(), \"eliminate is_cold cases before testing is_hot\");\n-  if (count()  >= HotCallCountThreshold)   return true;\n-  if (profit() >= HotCallProfitThreshold)  return true;\n-  if (work()   <= HotCallTrivialWork)      return true;\n-  if (size()   <= HotCallTrivialSize)      return true;\n-  return false;\n-}\n-\n-\/\/ compute_heat:\n-float WarmCallInfo::compute_heat() const {\n-  assert(!is_cold(), \"compute heat only on warm nodes\");\n-  assert(!is_hot(),  \"compute heat only on warm nodes\");\n-  int min_size = MAX2(0,   (int)HotCallTrivialSize);\n-  int max_size = MIN2(500, (int)WarmCallMaxSize);\n-  float method_size = (size() - min_size) \/ MAX2(1, max_size - min_size);\n-  float size_factor;\n-  if      (method_size < 0.05)  size_factor = 4;   \/\/ 2 sigmas better than avg.\n-  else if (method_size < 0.15)  size_factor = 2;   \/\/ 1 sigma better than avg.\n-  else if (method_size < 0.5)   size_factor = 1;   \/\/ better than avg.\n-  else                          size_factor = 0.5; \/\/ worse than avg.\n-  return (count() * profit() * size_factor);\n-}\n-\n-bool WarmCallInfo::warmer_than(WarmCallInfo* that) {\n-  assert(this != that, \"compare only different WCIs\");\n-  assert(this->heat() != 0 && that->heat() != 0, \"call compute_heat 1st\");\n-  if (this->heat() > that->heat())   return true;\n-  if (this->heat() < that->heat())   return false;\n-  assert(this->heat() == that->heat(), \"no NaN heat allowed\");\n-  \/\/ Equal heat.  Break the tie some other way.\n-  if (!this->call() || !that->call())  return (address)this > (address)that;\n-  return this->call()->_idx > that->call()->_idx;\n-}\n-\n-\/\/#define UNINIT_NEXT ((WarmCallInfo*)badAddress)\n-#define UNINIT_NEXT ((WarmCallInfo*)NULL)\n-\n-WarmCallInfo* WarmCallInfo::insert_into(WarmCallInfo* head) {\n-  assert(next() == UNINIT_NEXT, \"not yet on any list\");\n-  WarmCallInfo* prev_p = NULL;\n-  WarmCallInfo* next_p = head;\n-  while (next_p != NULL && next_p->warmer_than(this)) {\n-    prev_p = next_p;\n-    next_p = prev_p->next();\n-  }\n-  \/\/ Install this between prev_p and next_p.\n-  this->set_next(next_p);\n-  if (prev_p == NULL)\n-    head = this;\n-  else\n-    prev_p->set_next(this);\n-  return head;\n-}\n-\n-WarmCallInfo* WarmCallInfo::remove_from(WarmCallInfo* head) {\n-  WarmCallInfo* prev_p = NULL;\n-  WarmCallInfo* next_p = head;\n-  while (next_p != this) {\n-    assert(next_p != NULL, \"this must be in the list somewhere\");\n-    prev_p = next_p;\n-    next_p = prev_p->next();\n-  }\n-  next_p = this->next();\n-  debug_only(this->set_next(UNINIT_NEXT));\n-  \/\/ Remove this from between prev_p and next_p.\n-  if (prev_p == NULL)\n-    head = next_p;\n-  else\n-    prev_p->set_next(next_p);\n-  return head;\n-}\n-\n-WarmCallInfo WarmCallInfo::_always_hot(WarmCallInfo::MAX_VALUE(), WarmCallInfo::MAX_VALUE(),\n-                                       WarmCallInfo::MIN_VALUE(), WarmCallInfo::MIN_VALUE());\n-WarmCallInfo WarmCallInfo::_always_cold(WarmCallInfo::MIN_VALUE(), WarmCallInfo::MIN_VALUE(),\n-                                        WarmCallInfo::MAX_VALUE(), WarmCallInfo::MAX_VALUE());\n-\n-WarmCallInfo* WarmCallInfo::always_hot() {\n-  assert(_always_hot.is_hot(), \"must always be hot\");\n-  return &_always_hot;\n-}\n-\n-WarmCallInfo* WarmCallInfo::always_cold() {\n-  assert(_always_cold.is_cold(), \"must always be cold\");\n-  return &_always_cold;\n-}\n-\n-\n-#ifndef PRODUCT\n-\n-void WarmCallInfo::print() const {\n-  tty->print(\"%s : C=%6.1f P=%6.1f W=%6.1f S=%6.1f H=%6.1f -> %p\",\n-             is_cold() ? \"cold\" : is_hot() ? \"hot \" : \"warm\",\n-             count(), profit(), work(), size(), compute_heat(), next());\n-  tty->cr();\n-  if (call() != NULL)  call()->dump();\n-}\n-\n-void print_wci(WarmCallInfo* ci) {\n-  ci->print();\n-}\n-\n-void WarmCallInfo::print_all() const {\n-  for (const WarmCallInfo* p = this; p != NULL; p = p->next())\n-    p->print();\n-}\n-\n-int WarmCallInfo::count_all() const {\n-  int cnt = 0;\n-  for (const WarmCallInfo* p = this; p != NULL; p = p->next())\n-    cnt++;\n-  return cnt;\n-}\n-\n-#endif \/\/PRODUCT\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":0,"deletions":230,"binary":false,"changes":230,"status":"modified"},{"patch":"@@ -146,6 +146,0 @@\n-\n-  \/\/ How to make a call but defer the decision whether to inline or not.\n-  static CallGenerator* for_warm_call(WarmCallInfo* ci,\n-                                      CallGenerator* if_cold,\n-                                      CallGenerator* if_hot);\n-\n@@ -210,154 +204,0 @@\n-\n-\/\/---------------------------WarmCallInfo--------------------------------------\n-\/\/ A struct to collect information about a given call site.\n-\/\/ Helps sort call sites into \"hot\", \"medium\", and \"cold\".\n-\/\/ Participates in the queueing of \"medium\" call sites for possible inlining.\n-class WarmCallInfo : public ResourceObj {\n- private:\n-\n-  CallNode*     _call;   \/\/ The CallNode which may be inlined.\n-  CallGenerator* _hot_cg;\/\/ CG for expanding the call node\n-\n-  \/\/ These are the metrics we use to evaluate call sites:\n-\n-  float         _count;  \/\/ How often do we expect to reach this site?\n-  float         _profit; \/\/ How much time do we expect to save by inlining?\n-  float         _work;   \/\/ How long do we expect the average call to take?\n-  float         _size;   \/\/ How big do we expect the inlined code to be?\n-\n-  float         _heat;   \/\/ Combined score inducing total order on call sites.\n-  WarmCallInfo* _next;   \/\/ Next cooler call info in pending queue.\n-\n-  \/\/ Count is the number of times this call site is expected to be executed.\n-  \/\/ Large count is favorable for inlining, because the extra compilation\n-  \/\/ work will be amortized more completely.\n-\n-  \/\/ Profit is a rough measure of the amount of time we expect to save\n-  \/\/ per execution of this site if we inline it.  (1.0 == call overhead)\n-  \/\/ Large profit favors inlining.  Negative profit disables inlining.\n-\n-  \/\/ Work is a rough measure of the amount of time a typical out-of-line\n-  \/\/ call from this site is expected to take.  (1.0 == call, no-op, return)\n-  \/\/ Small work is somewhat favorable for inlining, since methods with\n-  \/\/ short \"hot\" traces are more likely to inline smoothly.\n-\n-  \/\/ Size is the number of graph nodes we expect this method to produce,\n-  \/\/ not counting the inlining of any further warm calls it may include.\n-  \/\/ Small size favors inlining, since small methods are more likely to\n-  \/\/ inline smoothly.  The size is estimated by examining the native code\n-  \/\/ if available.  The method bytecodes are also examined, assuming\n-  \/\/ empirically observed node counts for each kind of bytecode.\n-\n-  \/\/ Heat is the combined \"goodness\" of a site's inlining.  If we were\n-  \/\/ omniscient, it would be the difference of two sums of future execution\n-  \/\/ times of code emitted for this site (amortized across multiple sites if\n-  \/\/ sharing applies).  The two sums are for versions of this call site with\n-  \/\/ and without inlining.\n-\n-  \/\/ We approximate this mythical quantity by playing with averages,\n-  \/\/ rough estimates, and assumptions that history repeats itself.\n-  \/\/ The basic formula count * profit is heuristically adjusted\n-  \/\/ by looking at the expected compilation and execution times of\n-  \/\/ of the inlined call.\n-\n-  \/\/ Note:  Some of these metrics may not be present in the final product,\n-  \/\/ but exist in development builds to experiment with inline policy tuning.\n-\n-  \/\/ This heuristic framework does not model well the very significant\n-  \/\/ effects of multiple-level inlining.  It is possible to see no immediate\n-  \/\/ profit from inlining X->Y, but to get great profit from a subsequent\n-  \/\/ inlining X->Y->Z.\n-\n-  \/\/ This framework does not take well into account the problem of N**2 code\n-  \/\/ size in a clique of mutually inlinable methods.\n-\n-  WarmCallInfo*  next() const          { return _next; }\n-  void       set_next(WarmCallInfo* n) { _next = n; }\n-\n-  static WarmCallInfo _always_hot;\n-  static WarmCallInfo _always_cold;\n-\n-  \/\/ Constructor intitialization of always_hot and always_cold\n-  WarmCallInfo(float c, float p, float w, float s) {\n-    _call = NULL;\n-    _hot_cg = NULL;\n-    _next = NULL;\n-    _count = c;\n-    _profit = p;\n-    _work = w;\n-    _size = s;\n-    _heat = 0;\n-  }\n-\n- public:\n-  \/\/ Because WarmInfo objects live over the entire lifetime of the\n-  \/\/ Compile object, they are allocated into the comp_arena, which\n-  \/\/ does not get resource marked or reset during the compile process\n-  void *operator new( size_t x, Compile* C ) throw() { return C->comp_arena()->Amalloc(x); }\n-  void operator delete( void * ) { } \/\/ fast deallocation\n-\n-  static WarmCallInfo* always_hot();\n-  static WarmCallInfo* always_cold();\n-\n-  WarmCallInfo() {\n-    _call = NULL;\n-    _hot_cg = NULL;\n-    _next = NULL;\n-    _count = _profit = _work = _size = _heat = 0;\n-  }\n-\n-  CallNode* call() const { return _call; }\n-  float count()    const { return _count; }\n-  float size()     const { return _size; }\n-  float work()     const { return _work; }\n-  float profit()   const { return _profit; }\n-  float heat()     const { return _heat; }\n-\n-  void set_count(float x)     { _count = x; }\n-  void set_size(float x)      { _size = x; }\n-  void set_work(float x)      { _work = x; }\n-  void set_profit(float x)    { _profit = x; }\n-  void set_heat(float x)      { _heat = x; }\n-\n-  \/\/ Load initial heuristics from profiles, etc.\n-  \/\/ The heuristics can be tweaked further by the caller.\n-  void init(JVMState* call_site, ciMethod* call_method, ciCallProfile& profile, float prof_factor);\n-\n-  static float MAX_VALUE() { return +1.0e10; }\n-  static float MIN_VALUE() { return -1.0e10; }\n-\n-  float compute_heat() const;\n-\n-  void set_call(CallNode* call)      { _call = call; }\n-  void set_hot_cg(CallGenerator* cg) { _hot_cg = cg; }\n-\n-  \/\/ Do not queue very hot or very cold calls.\n-  \/\/ Make very cold ones out of line immediately.\n-  \/\/ Inline very hot ones immediately.\n-  \/\/ These queries apply various tunable limits\n-  \/\/ to the above metrics in a systematic way.\n-  \/\/ Test for coldness before testing for hotness.\n-  bool is_cold() const;\n-  bool is_hot() const;\n-\n-  \/\/ Force a warm call to be hot.  This worklists the call node for inlining.\n-  void make_hot();\n-\n-  \/\/ Force a warm call to be cold.  This worklists the call node for out-of-lining.\n-  void make_cold();\n-\n-  \/\/ A reproducible total ordering, in which heat is the major key.\n-  bool warmer_than(WarmCallInfo* that);\n-\n-  \/\/ List management.  These methods are called with the list head,\n-  \/\/ and return the new list head, inserting or removing the receiver.\n-  WarmCallInfo* insert_into(WarmCallInfo* head);\n-  WarmCallInfo* remove_from(WarmCallInfo* head);\n-\n-#ifndef PRODUCT\n-  void print() const;\n-  void print_all() const;\n-  int count_all() const;\n-#endif\n-};\n-\n","filename":"src\/hotspot\/share\/opto\/callGenerator.hpp","additions":0,"deletions":160,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -291,7 +291,0 @@\n-static inline bool not_a_node(const Node* n) {\n-  if (n == NULL)                   return true;\n-  if (((intptr_t)n & 1) != 0)      return true;  \/\/ uninitialized, etc.\n-  if (*(address*)n == badAddress)  return true;  \/\/ kill by Node::destruct\n-  return false;\n-}\n-\n@@ -441,1 +434,3 @@\n-  remove_useless_nodes(_macro_nodes,        useful); \/\/ remove useless macro and predicate opaq nodes\n+  remove_useless_nodes(_macro_nodes,        useful); \/\/ remove useless macro nodes\n+  remove_useless_nodes(_predicate_opaqs,    useful); \/\/ remove useless predicate opaque nodes\n+  remove_useless_nodes(_skeleton_predicate_opaqs, useful);\n@@ -561,0 +556,1 @@\n+                  _igv_idx(0),\n@@ -590,1 +586,0 @@\n-                  _warm_calls(NULL),\n@@ -761,8 +756,0 @@\n-  for (;;) {\n-    int successes = Inline_Warm();\n-    if (failing())  return;\n-    if (successes == 0)  break;\n-  }\n-\n-  \/\/ Drain the list.\n-  Finish_Warm();\n@@ -870,0 +857,1 @@\n+    _igv_idx(0),\n@@ -892,1 +880,0 @@\n-    _warm_calls(NULL),\n@@ -1815,51 +1802,0 @@\n-\n-\n-\/\/---------------------------pop_warm_call-------------------------------------\n-WarmCallInfo* Compile::pop_warm_call() {\n-  WarmCallInfo* wci = _warm_calls;\n-  if (wci != NULL)  _warm_calls = wci->remove_from(wci);\n-  return wci;\n-}\n-\n-\/\/----------------------------Inline_Warm--------------------------------------\n-int Compile::Inline_Warm() {\n-  \/\/ If there is room, try to inline some more warm call sites.\n-  \/\/ %%% Do a graph index compaction pass when we think we're out of space?\n-  if (!InlineWarmCalls)  return 0;\n-\n-  int calls_made_hot = 0;\n-  int room_to_grow   = NodeCountInliningCutoff - unique();\n-  int amount_to_grow = MIN2(room_to_grow, (int)NodeCountInliningStep);\n-  int amount_grown   = 0;\n-  WarmCallInfo* call;\n-  while (amount_to_grow > 0 && (call = pop_warm_call()) != NULL) {\n-    int est_size = (int)call->size();\n-    if (est_size > (room_to_grow - amount_grown)) {\n-      \/\/ This one won't fit anyway.  Get rid of it.\n-      call->make_cold();\n-      continue;\n-    }\n-    call->make_hot();\n-    calls_made_hot++;\n-    amount_grown   += est_size;\n-    amount_to_grow -= est_size;\n-  }\n-\n-  if (calls_made_hot > 0)  set_major_progress();\n-  return calls_made_hot;\n-}\n-\n-\n-\/\/----------------------------Finish_Warm--------------------------------------\n-void Compile::Finish_Warm() {\n-  if (!InlineWarmCalls)  return;\n-  if (failing())  return;\n-  if (warm_calls() == NULL)  return;\n-\n-  \/\/ Clean up loose ends, if we are out of space for inlining.\n-  WarmCallInfo* call;\n-  while ((call = pop_warm_call()) != NULL) {\n-    call->make_cold();\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":5,"deletions":69,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -93,1 +93,0 @@\n-class WarmCallInfo;\n@@ -298,0 +297,1 @@\n+  uint                  _igv_idx;               \/\/ Counter for IGV node identifiers\n@@ -386,1 +386,0 @@\n-  WarmCallInfo*         _warm_calls;            \/\/ Sorted work-list for heat-based inlining.\n@@ -619,0 +618,1 @@\n+  uint          next_igv_idx()                  { return _igv_idx++; }\n@@ -950,4 +950,0 @@\n-  WarmCallInfo*     warm_calls() const          { return _warm_calls; }\n-  void          set_warm_calls(WarmCallInfo* l) { _warm_calls = l; }\n-  WarmCallInfo* pop_warm_call();\n-\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -173,7 +173,1 @@\n-      WarmCallInfo scratch_ci;\n-      WarmCallInfo* ci = ilt->ok_to_inline(callee, jvms, profile, &scratch_ci, should_delay);\n-      assert(ci != &scratch_ci, \"do not let this pointer escape\");\n-      bool allow_inline   = (ci != NULL && !ci->is_cold());\n-      bool require_inline = (allow_inline && ci->is_hot());\n-\n-      if (allow_inline) {\n+      if (ilt->ok_to_inline(callee, jvms, profile, should_delay)) {\n@@ -182,2 +176,1 @@\n-\n-        if (require_inline && cg != NULL) {\n+        if (cg != NULL) {\n@@ -195,0 +188,2 @@\n+          } else {\n+            return cg;\n@@ -197,8 +192,0 @@\n-        if (cg == NULL || should_delay) {\n-          \/\/ Fall through.\n-        } else if (require_inline || !InlineWarmCalls) {\n-          return cg;\n-        } else {\n-          CallGenerator* cold_cg = call_generator(callee, vtable_index, call_does_dispatch, jvms, false, prof_factor);\n-          return CallGenerator::for_warm_call(ci, cold_cg, cg);\n-        }\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":4,"deletions":17,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -381,1 +381,15 @@\n-      \/\/ Hoist it up to the end of the test block.\n+      \/\/ Hoist it up to the end of the test block together with its inputs if they exist.\n+      for (uint i = 2; i < val->req(); i++) {\n+        \/\/ DecodeN has 2 regular inputs + optional MachTemp or load Base inputs.\n+        Node *temp = val->in(i);\n+        Block *tempb = get_block_for_node(temp);\n+        if (!tempb->dominates(block)) {\n+          assert(block->dominates(tempb), \"sanity check: temp node placement\");\n+          \/\/ We only expect nodes without further inputs, like MachTemp or load Base.\n+          assert(temp->req() == 0 || (temp->req() == 1 && temp->in(0) == (Node*)C->root()),\n+                 \"need for recursive hoisting not expected\");\n+          tempb->find_remove(temp);\n+          block->add_inst(temp);\n+          map_node_to_block(temp, block);\n+        }\n+      }\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1641,0 +1641,59 @@\n+\/\/------------------------------inline_math_pow-----------------------------\n+bool LibraryCallKit::inline_math_pow() {\n+  Node* exp = round_double_node(argument(2));\n+  const TypeD* d = _gvn.type(exp)->isa_double_constant();\n+  if (d != NULL) {\n+    if (d->getd() == 2.0) {\n+      \/\/ Special case: pow(x, 2.0) => x * x\n+      Node* base = round_double_node(argument(0));\n+      set_result(_gvn.transform(new MulDNode(base, base)));\n+      return true;\n+    } else if (d->getd() == 0.5 && Matcher::match_rule_supported(Op_SqrtD)) {\n+      \/\/ Special case: pow(x, 0.5) => sqrt(x)\n+      Node* base = round_double_node(argument(0));\n+      Node* zero = _gvn.zerocon(T_DOUBLE);\n+\n+      RegionNode* region = new RegionNode(3);\n+      Node* phi = new PhiNode(region, Type::DOUBLE);\n+\n+      Node* cmp  = _gvn.transform(new CmpDNode(base, zero));\n+      \/\/ According to the API specs, pow(-0.0, 0.5) = 0.0 and sqrt(-0.0) = -0.0.\n+      \/\/ So pow(-0.0, 0.5) shouldn't be replaced with sqrt(-0.0).\n+      \/\/ -0.0\/+0.0 are both excluded since floating-point comparison doesn't distinguish -0.0 from +0.0.\n+      Node* test = _gvn.transform(new BoolNode(cmp, BoolTest::le));\n+\n+      Node* if_pow = generate_slow_guard(test, NULL);\n+      Node* value_sqrt = _gvn.transform(new SqrtDNode(C, control(), base));\n+      phi->init_req(1, value_sqrt);\n+      region->init_req(1, control());\n+\n+      if (if_pow != NULL) {\n+        set_control(if_pow);\n+        address target = StubRoutines::dpow() != NULL ? StubRoutines::dpow() :\n+                                                        CAST_FROM_FN_PTR(address, SharedRuntime::dpow);\n+        const TypePtr* no_memory_effects = NULL;\n+        Node* trig = make_runtime_call(RC_LEAF, OptoRuntime::Math_DD_D_Type(), target, \"POW\",\n+                                       no_memory_effects, base, top(), exp, top());\n+        Node* value_pow = _gvn.transform(new ProjNode(trig, TypeFunc::Parms+0));\n+#ifdef ASSERT\n+        Node* value_top = _gvn.transform(new ProjNode(trig, TypeFunc::Parms+1));\n+        assert(value_top == top(), \"second value must be top\");\n+#endif\n+        phi->init_req(2, value_pow);\n+        region->init_req(2, _gvn.transform(new ProjNode(trig, TypeFunc::Control)));\n+      }\n+\n+      C->set_has_split_ifs(true); \/\/ Has chance for split-if optimization\n+      set_control(_gvn.transform(region));\n+      record_for_igvn(region);\n+      set_result(_gvn.transform(phi));\n+\n+      return true;\n+    }\n+  }\n+\n+  return StubRoutines::dpow() != NULL ?\n+    runtime_math(OptoRuntime::Math_DD_D_Type(), StubRoutines::dpow(),  \"dpow\") :\n+    runtime_math(OptoRuntime::Math_DD_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dpow),  \"POW\");\n+}\n+\n@@ -1681,13 +1740,0 @@\n-  case vmIntrinsics::_dpow: {\n-    Node* exp = round_double_node(argument(2));\n-    const TypeD* d = _gvn.type(exp)->isa_double_constant();\n-    if (d != NULL && d->getd() == 2.0) {\n-      \/\/ Special case: pow(x, 2.0) => x * x\n-      Node* base = round_double_node(argument(0));\n-      set_result(_gvn.transform(new MulDNode(base, base)));\n-      return true;\n-    }\n-    return StubRoutines::dpow() != NULL ?\n-      runtime_math(OptoRuntime::Math_DD_D_Type(), StubRoutines::dpow(),  \"dpow\") :\n-      runtime_math(OptoRuntime::Math_DD_D_Type(), FN_PTR(SharedRuntime::dpow),  \"POW\");\n-  }\n@@ -1696,0 +1742,1 @@\n+  case vmIntrinsics::_dpow:      return inline_math_pow();\n@@ -1698,2 +1745,2 @@\n-  case vmIntrinsics::_dsignum: return inline_double_math(id);\n-  case vmIntrinsics::_fsignum: return inline_math(id);\n+  case vmIntrinsics::_dsignum: return Matcher::match_rule_supported(Op_SignumD) ? inline_double_math(id) : false;\n+  case vmIntrinsics::_fsignum: return Matcher::match_rule_supported(Op_SignumF) ? inline_math(id) : false;\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":62,"deletions":15,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -235,0 +235,1 @@\n+  bool inline_math_pow();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -728,1 +728,3 @@\n-    if (phi == NULL)  break;\n+    if (phi == NULL || _igvn.type(phi) == Type::TOP) {\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1472,1 +1472,1 @@\n-      Node* fast_oop = bs->obj_allocate(this, ctrl, mem, toobig_false, size_in_bytes, i_o, needgc_ctrl,\n+      Node* fast_oop = bs->obj_allocate(this, mem, toobig_false, size_in_bytes, i_o, needgc_ctrl,\n@@ -2707,1 +2707,1 @@\n-    Node* fast_oop = bs->obj_allocate(this, allocation_ctl, mem, allocation_ctl, size_in_bytes, io, needgc_ctrl,\n+    Node* fast_oop = bs->obj_allocate(this, mem, allocation_ctl, size_in_bytes, io, needgc_ctrl,\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-#ifdef ASSERT\n+#ifndef PRODUCT\n@@ -77,1 +77,2 @@\n-#endif\n+  _reused(C->comp_arena()),\n+#endif \/\/ !PRODUCT\n@@ -1140,3 +1141,1 @@\n-#ifdef ASSERT\n-              _new2old_map.map(m->_idx, n);\n-#endif\n+              NOT_PRODUCT(record_new2old(m, n);)\n@@ -1147,3 +1146,1 @@\n-#ifdef ASSERT\n-              _new2old_map.map(m->_idx, n);\n-#endif\n+              NOT_PRODUCT(record_new2old(m, n);)\n@@ -1207,3 +1204,1 @@\n-#ifdef ASSERT\n-          _new2old_map.map(m->_idx, n);\n-#endif\n+          NOT_PRODUCT(record_new2old(m, n));\n@@ -1546,4 +1541,2 @@\n-#ifdef ASSERT\n-  _old2new_map.map(n->_idx, m);\n-  _new2old_map.map(m->_idx, (Node*)n);\n-#endif\n+  \/\/ New-to-old mapping is done in ReduceInst, to cover complex instructions.\n+  NOT_PRODUCT(_old2new_map.map(n->_idx, m);)\n@@ -1806,0 +1799,1 @@\n+  NOT_PRODUCT(record_new2old(mach, leaf);)\n@@ -1874,3 +1868,1 @@\n-#ifdef ASSERT\n-    _new2old_map.map(ex->_idx, s->_leaf);\n-#endif\n+    NOT_PRODUCT(record_new2old(ex, s->_leaf);)\n@@ -2080,1 +2072,1 @@\n-bool Matcher::is_vshift_con_pattern(Node *n, Node *m) {\n+bool Matcher::is_vshift_con_pattern(Node* n, Node* m) {\n@@ -2481,1 +2473,11 @@\n-#ifdef ASSERT\n+#ifndef PRODUCT\n+void Matcher::record_new2old(Node* newn, Node* old) {\n+  _new2old_map.map(newn->_idx, old);\n+  if (!_reused.test_set(old->_igv_idx)) {\n+    \/\/ Reuse the Ideal-level IGV identifier so that the node can be tracked\n+    \/\/ across matching. If there are multiple machine nodes expanded from the\n+    \/\/ same Ideal node, only one will reuse its IGV identifier.\n+    newn->_igv_idx = old->_igv_idx;\n+  }\n+}\n+\n@@ -2486,1 +2488,1 @@\n-#endif\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":23,"deletions":21,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -124,1 +124,1 @@\n-  bool is_vshift_con_pattern(Node *n, Node *m);\n+  bool is_vshift_con_pattern(Node* n, Node* m);\n@@ -138,2 +138,5 @@\n-  debug_only(Node_Array _old2new_map;)   \/\/ Map roots of ideal-trees to machine-roots\n-  debug_only(Node_Array _new2old_map;)   \/\/ Maps machine nodes back to ideal\n+#ifndef PRODUCT\n+  Node_Array _old2new_map;    \/\/ Map roots of ideal-trees to machine-roots\n+  Node_Array _new2old_map;    \/\/ Maps machine nodes back to ideal\n+  VectorSet _reused;          \/\/ Ideal IGV identifiers reused by machine nodes\n+#endif \/\/ !PRODUCT\n@@ -561,1 +564,4 @@\n-#ifdef ASSERT\n+#ifndef PRODUCT\n+  \/\/ Record mach-to-Ideal mapping, reusing the Ideal IGV identifier if possible.\n+  void record_new2old(Node* newn, Node* old);\n+\n@@ -567,1 +573,1 @@\n-#endif\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+#include \"opto\/vectornode.hpp\"\n@@ -1139,1 +1140,1 @@\n-      if (store_Opcode() != st->Opcode())\n+      if (store_Opcode() != st->Opcode()) {\n@@ -1141,0 +1142,9 @@\n+      }\n+      \/\/ LoadVector\/StoreVector needs additional check to ensure the types match.\n+      if (store_Opcode() == Op_StoreVector) {\n+        const TypeVect*  in_vt = st->as_StoreVector()->vect_type();\n+        const TypeVect* out_vt = as_LoadVector()->vect_type();\n+        if (in_vt != out_vt) {\n+          return NULL;\n+        }\n+      }\n@@ -1401,2 +1411,2 @@\n-Node* LoadNode::eliminate_autobox(PhaseGVN* phase) {\n-  assert(phase->C->eliminate_boxing(), \"sanity\");\n+Node* LoadNode::eliminate_autobox(PhaseIterGVN* igvn) {\n+  assert(igvn->C->eliminate_boxing(), \"sanity\");\n@@ -1404,1 +1414,1 @@\n-  Node* base = AddPNode::Ideal_base_and_offset(in(Address), phase, ignore);\n+  Node* base = AddPNode::Ideal_base_and_offset(in(Address), igvn, ignore);\n@@ -1438,1 +1448,1 @@\n-                            elements[1]->in(2) == phase->intcon(shift)))) {\n+                            elements[1]->in(2) == igvn->intcon(shift)))) {\n@@ -1465,1 +1475,1 @@\n-              result = phase->transform(new AddXNode(result, elements[i]));\n+              result = igvn->transform(new AddXNode(result, elements[i]));\n@@ -1468,1 +1478,1 @@\n-            result = phase->transform(new AddXNode(result, phase->MakeConX(-(int)offset)));\n+            result = igvn->transform(new AddXNode(result, igvn->MakeConX(-(int)offset)));\n@@ -1470,1 +1480,1 @@\n-            if (result->Opcode() == Op_LShiftX && result->in(2) == phase->intcon(shift)) {\n+            if (result->Opcode() == Op_LShiftX && result->in(2) == igvn->intcon(shift)) {\n@@ -1473,1 +1483,2 @@\n-              result = new RShiftXNode(result->in(1), phase->intcon(0));\n+              igvn->_worklist.push(result); \/\/ remove dead node later\n+              result = new RShiftXNode(result->in(1), igvn->intcon(0));\n@@ -1476,1 +1487,1 @@\n-                       result->in(1)->in(2) == phase->intcon(shift)) {\n+                       result->in(1)->in(2) == igvn->intcon(shift)) {\n@@ -1480,2 +1491,3 @@\n-              Node* add_con = new RShiftXNode(result->in(2), phase->intcon(shift));\n-              result = new AddXNode(result->in(1)->in(1), phase->transform(add_con));\n+              igvn->_worklist.push(result); \/\/ remove dead node later\n+              Node* add_con = new RShiftXNode(result->in(2), igvn->intcon(shift));\n+              result = new AddXNode(result->in(1)->in(1), igvn->transform(add_con));\n@@ -1483,1 +1495,1 @@\n-              result = new RShiftXNode(result, phase->intcon(shift));\n+              result = new RShiftXNode(result, igvn->intcon(shift));\n@@ -1487,1 +1499,1 @@\n-              result = new ConvL2INode(phase->transform(result));\n+              result = new ConvL2INode(igvn->transform(result));\n@@ -1491,1 +1503,1 @@\n-              result = new ConvI2LNode(phase->transform(result));\n+              result = new ConvI2LNode(igvn->transform(result));\n@@ -1498,1 +1510,1 @@\n-                result = new AndINode(phase->transform(result), phase->intcon(0xFF));\n+                result = new AndINode(igvn->transform(result), igvn->intcon(0xFF));\n@@ -1501,1 +1513,1 @@\n-                result = new AndINode(phase->transform(result), phase->intcon(0xFFFF));\n+                result = new AndINode(igvn->transform(result), igvn->intcon(0xFFFF));\n@@ -1794,1 +1806,2 @@\n-      if (igvn != NULL && igvn->_worklist.member(opt_mem)) {\n+      assert(igvn != NULL, \"must be PhaseIterGVN when can_reshape is true\");\n+      if (igvn->_worklist.member(opt_mem)) {\n@@ -1804,1 +1817,1 @@\n-        Node* result = eliminate_autobox(phase);\n+        Node* result = eliminate_autobox(igvn);\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":32,"deletions":19,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -255,1 +255,1 @@\n-  Node *eliminate_autobox(PhaseGVN *phase);\n+  Node *eliminate_autobox(PhaseIterGVN *igvn);\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -143,1 +143,1 @@\n-    if (NotANode(o)) {\n+    if (not_a_node(o)) {\n","filename":"src\/hotspot\/share\/opto\/multnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -306,0 +306,1 @@\n+  NOT_PRODUCT(_igv_idx = C->next_igv_idx());\n@@ -534,0 +535,1 @@\n+  NOT_PRODUCT(n->_igv_idx = C->next_igv_idx());\n@@ -566,0 +568,4 @@\n+\n+      C->print_inlining_assert_ready();\n+      C->print_inlining_move_to(cg);\n+      C->print_inlining_update(cloned_cg);\n@@ -1662,1 +1668,1 @@\n-  if (NotANode(n)) {\n+  if (not_a_node(n)) {\n@@ -1690,1 +1696,1 @@\n-  if (NotANode(orig)) orig = NULL;\n+  if (not_a_node(orig)) orig = NULL;\n@@ -1697,1 +1703,1 @@\n-  if (NotANode(fast)) fast = NULL;\n+  if (not_a_node(fast)) fast = NULL;\n@@ -1706,1 +1712,1 @@\n-    if (NotANode(orig)) orig = NULL;\n+    if (not_a_node(orig)) orig = NULL;\n@@ -1712,1 +1718,1 @@\n-      if (NotANode(fast)) fast = NULL;\n+      if (not_a_node(fast)) fast = NULL;\n@@ -1715,1 +1721,1 @@\n-        if (NotANode(fast)) fast = NULL;\n+        if (not_a_node(fast)) fast = NULL;\n@@ -1728,1 +1734,1 @@\n-  if (NotANode(orig))  orig = NULL;\n+  if (not_a_node(orig))  orig = NULL;\n@@ -1737,1 +1743,1 @@\n-    if (NotANode(orig))  orig = NULL;\n+    if (not_a_node(orig))  orig = NULL;\n@@ -1833,2 +1839,2 @@\n-    } else if (NotANode(d)) {\n-      st->print(\"NotANode \");  \/\/ uninitialized, sentinel, garbage, etc.\n+    } else if (not_a_node(d)) {\n+      st->print(\"not_a_node \");  \/\/ uninitialized, sentinel, garbage, etc.\n@@ -1850,1 +1856,1 @@\n-      if (NotANode(p)) { st->print(\"NotANode \"); continue; }\n+      if (not_a_node(p)) { st->print(\"not_a_node \"); continue; }\n@@ -1865,2 +1871,2 @@\n-    } else if (NotANode(u)) {\n-      st->print(\"NotANode \");\n+    } else if (not_a_node(u)) {\n+      st->print(\"not_a_node \");\n@@ -1903,1 +1909,1 @@\n-        if (NotANode(n))  continue;\n+        if (not_a_node(n))  continue;\n@@ -1924,1 +1930,1 @@\n-  if (NotANode(start)) return;\n+  if (not_a_node(start)) return;\n@@ -2092,1 +2098,1 @@\n-        if (NotANode(n)) {\n+        if (not_a_node(n)) {\n@@ -2154,1 +2160,1 @@\n-    if (NotANode(current)) {\n+    if (not_a_node(current)) {\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":24,"deletions":18,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -139,1 +139,0 @@\n-class NullCheckNode;\n@@ -332,0 +331,6 @@\n+  \/\/ IGV node identifier. Two nodes, possibly in different compilation phases,\n+  \/\/ have the same IGV identifier if (and only if) they are the very same node\n+  \/\/ (same memory address) or one is \"derived\" from the other (by e.g.\n+  \/\/ renumbering or matching). This identifier makes it possible to follow the\n+  \/\/ entire lifetime of a node in IGV even if its C2 identifier (_idx) changes.\n+  NOT_PRODUCT(node_idx_t _igv_idx;)\n@@ -1268,5 +1273,1 @@\n-\n-#ifndef PRODUCT\n-\n-\/\/ Used in debugging code to avoid walking across dead or uninitialized edges.\n-inline bool NotANode(const Node* n) {\n+inline bool not_a_node(const Node* n) {\n@@ -1279,3 +1280,0 @@\n-#endif\n-\n-\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":7,"deletions":9,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -619,4 +619,0 @@\n-#if INCLUDE_AOT\n-            stub_size  += CompiledStaticCall::to_aot_stub_size();\n-            reloc_size += CompiledStaticCall::reloc_to_aot_stub();\n-#endif\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -74,1 +74,0 @@\n-                            WarmCallInfo* wci_result,\n@@ -79,2 +78,1 @@\n-                            ciCallProfile& profile,\n-                            WarmCallInfo* wci_result);\n+                            ciCallProfile& profile);\n@@ -83,2 +81,1 @@\n-                                JVMState* jvms,\n-                                WarmCallInfo* wci_result);\n+                                JVMState* jvms);\n@@ -115,1 +112,1 @@\n-  WarmCallInfo* ok_to_inline(ciMethod *call_method, JVMState* caller_jvms, ciCallProfile& profile, WarmCallInfo* wci, bool& should_delay);\n+  bool ok_to_inline(ciMethod *call_method, JVMState* caller_jvms, ciCallProfile& profile, bool& should_delay);\n","filename":"src\/hotspot\/share\/opto\/parse.hpp","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2445,1 +2445,1 @@\n-  Node *polling_page_load_addr = _gvn.transform(basic_plus_adr(top(), thread, in_bytes(Thread::polling_page_offset())));\n+  Node *polling_page_load_addr = _gvn.transform(basic_plus_adr(top(), thread, in_bytes(JavaThread::polling_page_offset())));\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,5 @@\n+#include \"cds\/classListParser.hpp\"\n+#include \"cds\/classListWriter.hpp\"\n+#include \"cds\/dynamicArchive.hpp\"\n+#include \"cds\/heapShared.hpp\"\n+#include \"cds\/lambdaFormInvokers.hpp\"\n@@ -28,2 +33,0 @@\n-#include \"classfile\/classListParser.hpp\"\n-#include \"classfile\/classListWriter.hpp\"\n@@ -49,2 +52,0 @@\n-#include \"memory\/dynamicArchive.hpp\"\n-#include \"memory\/heapShared.hpp\"\n@@ -3052,16 +3053,2 @@\n-    \/\/ jthread refers to a live JavaThread.\n-    {\n-      MutexLocker ml(receiver->SR_lock(), Mutex::_no_safepoint_check_flag);\n-      if (receiver->is_external_suspend()) {\n-        \/\/ Don't allow nested external suspend requests. We can't return\n-        \/\/ an error from this interface so just ignore the problem.\n-        return;\n-      }\n-      if (receiver->is_exiting()) { \/\/ thread is in the process of exiting\n-        return;\n-      }\n-      receiver->set_external_suspend();\n-    }\n-\n-    \/\/ java_suspend() will catch threads in the process of exiting\n-    \/\/ and will ignore them.\n+    \/\/ jthread refers to a live JavaThread, but java_suspend() will\n+    \/\/ detect a thread that has started to exit and will ignore it.\n@@ -3069,9 +3056,0 @@\n-\n-    \/\/ It would be nice to have the following assertion in all the\n-    \/\/ time, but it is possible for a racing resume request to have\n-    \/\/ resumed this thread right after we suspended it. Temporarily\n-    \/\/ enable this assertion if you are chasing a different kind of\n-    \/\/ bug.\n-    \/\/\n-    \/\/ assert(java_lang_Thread::thread(receiver->threadObj()) == NULL ||\n-    \/\/   receiver->is_being_ext_suspended(), \"thread is not suspended\");\n@@ -3088,16 +3066,0 @@\n-\n-    \/\/ This is the original comment for this Threads_lock grab:\n-    \/\/   We need to *always* get the threads lock here, since this operation cannot be allowed during\n-    \/\/   a safepoint. The safepoint code relies on suspending a thread to examine its state. If other\n-    \/\/   threads randomly resumes threads, then a thread might not be suspended when the safepoint code\n-    \/\/   looks at it.\n-    \/\/\n-    \/\/ The above comment dates back to when we had both internal and\n-    \/\/ external suspend APIs that shared a common underlying mechanism.\n-    \/\/ External suspend is now entirely cooperative and doesn't share\n-    \/\/ anything with internal suspend. That said, there are some\n-    \/\/ assumptions in the VM that an external resume grabs the\n-    \/\/ Threads_lock. We can't drop the Threads_lock grab here until we\n-    \/\/ resolve the assumptions that exist elsewhere.\n-    \/\/\n-    MutexLocker ml(Threads_lock);\n@@ -3756,1 +3718,1 @@\n-  return ClassListWriter::is_enabled();\n+  return ClassListWriter::is_enabled() || DynamicDumpSharedSpaces;\n@@ -3764,1 +3726,1 @@\n-  assert(ClassListWriter::is_enabled(), \"Should be set and open\");\n+  assert(ClassListWriter::is_enabled() || DynamicDumpSharedSpaces,  \"Should be set and open or do dynamic dump\");\n@@ -3769,2 +3731,9 @@\n-    ClassListWriter w;\n-    w.stream()->print_cr(\"%s %s\", LAMBDA_FORM_TAG, c_line);\n+    if (DynamicDumpSharedSpaces) {\n+      \/\/ Note: LambdaFormInvokers::append_filtered and LambdaFormInvokers::append take same format which is not\n+      \/\/ same as below the print format. The line does not include LAMBDA_FORM_TAG.\n+      LambdaFormInvokers::append_filtered(os::strdup((const char*)c_line, mtInternal));\n+    }\n+    if (ClassListWriter::is_enabled()) {\n+      ClassListWriter w;\n+      w.stream()->print_cr(\"%s %s\", LAMBDA_FORM_TAG, c_line);\n+    }\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":18,"deletions":49,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -879,1 +879,1 @@\n-    if (java_thread->is_being_ext_suspended()) {\n+    if (java_thread->is_suspended()) {\n@@ -945,1 +945,1 @@\n-    return (JVMTI_ERROR_NONE);\n+    return JVMTI_ERROR_NONE;\n@@ -947,11 +947,2 @@\n-\n-  {\n-    MutexLocker ml(java_thread->SR_lock(), Mutex::_no_safepoint_check_flag);\n-    if (java_thread->is_external_suspend()) {\n-      \/\/ don't allow nested external suspend requests.\n-      return (JVMTI_ERROR_THREAD_SUSPENDED);\n-    }\n-    if (java_thread->is_exiting()) { \/\/ thread is in the process of exiting\n-      return (JVMTI_ERROR_THREAD_NOT_ALIVE);\n-    }\n-    java_thread->set_external_suspend();\n+  if (java_thread->is_suspended()) {\n+    return JVMTI_ERROR_THREAD_SUSPENDED;\n@@ -959,3 +950,6 @@\n-\n-    \/\/ the thread was in the process of exiting\n-    return (JVMTI_ERROR_THREAD_NOT_ALIVE);\n+    \/\/ Either the thread is already suspended or\n+    \/\/ it was in the process of exiting.\n+    if (java_thread->is_exiting()) {\n+      return JVMTI_ERROR_THREAD_NOT_ALIVE;\n+    }\n+    return JVMTI_ERROR_THREAD_SUSPENDED;\n@@ -973,0 +967,1 @@\n+  int self_index = -1;\n@@ -974,1 +969,2 @@\n-  ThreadsListHandle tlh;\n+  JavaThread* current = JavaThread::current();\n+  ThreadsListHandle tlh(current);\n@@ -987,13 +983,3 @@\n-\n-    {\n-      MutexLocker ml(java_thread->SR_lock(), Mutex::_no_safepoint_check_flag);\n-      if (java_thread->is_external_suspend()) {\n-        \/\/ don't allow nested external suspend requests.\n-        results[i] = JVMTI_ERROR_THREAD_SUSPENDED;\n-        continue;\n-      }\n-      if (java_thread->is_exiting()) { \/\/ thread is in the process of exiting\n-        results[i] = JVMTI_ERROR_THREAD_NOT_ALIVE;\n-        continue;\n-      }\n-      java_thread->set_external_suspend();\n+    if (java_thread->is_suspended()) {\n+      results[i] = JVMTI_ERROR_THREAD_SUSPENDED;\n+      continue;\n@@ -1001,7 +987,8 @@\n-    if (java_thread->thread_state() == _thread_in_native) {\n-      \/\/ We need to try and suspend native threads here. Threads in\n-      \/\/ other states will self-suspend on their next transition.\n-      if (!JvmtiSuspendControl::suspend(java_thread)) {\n-        \/\/ The thread was in the process of exiting. Force another\n-        \/\/ safepoint to make sure that this thread transitions.\n-        needSafepoint++;\n+    if (java_thread == current) {\n+      self_index = i;\n+      continue;\n+    }\n+    if (!JvmtiSuspendControl::suspend(java_thread)) {\n+      \/\/ Either the thread is already suspended or\n+      \/\/ it was in the process of exiting.\n+      if (java_thread->is_exiting()) {\n@@ -1011,2 +998,2 @@\n-    } else {\n-      needSafepoint++;\n+      results[i] = JVMTI_ERROR_THREAD_SUSPENDED;\n+      continue;\n@@ -1016,3 +1003,12 @@\n-  if (needSafepoint > 0) {\n-    VM_ThreadsSuspendJVMTI tsj;\n-    VMThread::execute(&tsj);\n+  if (self_index >= 0) {\n+    if (!JvmtiSuspendControl::suspend(current)) {\n+      \/\/ Either the thread is already suspended or\n+      \/\/ it was in the process of exiting.\n+      if (current->is_exiting()) {\n+        results[self_index] = JVMTI_ERROR_THREAD_NOT_ALIVE;\n+      } else {\n+        results[self_index] = JVMTI_ERROR_THREAD_SUSPENDED;\n+      }\n+    } else {\n+      results[self_index] = JVMTI_ERROR_NONE;  \/\/ indicate successful suspend\n+    }\n@@ -1033,2 +1029,1 @@\n-\n-  if (!java_thread->is_being_ext_suspended()) {\n+  if (!java_thread->is_suspended()) {\n@@ -1037,1 +1032,0 @@\n-\n@@ -1063,1 +1057,1 @@\n-    if (!java_thread->is_being_ext_suspended()) {\n+    if (!java_thread->is_suspended()) {\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":40,"deletions":46,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -1186,2 +1186,1 @@\n-    \/\/ same as is_being_ext_suspended() but without locking\n-    if (thr->is_ext_suspended() || thr->is_external_suspend()) {\n+    if (thr->is_suspended()) {\n@@ -1405,1 +1404,1 @@\n-    if (!java_thread->is_external_suspend()) {\n+    if (!java_thread->is_suspended()) {\n@@ -1538,1 +1537,1 @@\n-  if (!self && !java_thread->is_external_suspend()) {\n+  if (!self && !java_thread->is_suspended()) {\n@@ -1629,1 +1628,1 @@\n-  if (!self && !java_thread->is_external_suspend()) {\n+  if (!self && !java_thread->is_suspended()) {\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnvBase.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -246,2 +246,11 @@\n-      \/\/ Only set breakpoints in running EMCP methods.\n-      if (method->is_running_emcp() &&\n+      \/\/ Only set breakpoints in EMCP methods.\n+      \/\/ EMCP methods are old but not obsolete. Equivalent\n+      \/\/ Modulo Constant Pool means the method is equivalent except\n+      \/\/ the constant pool and instructions that access the constant\n+      \/\/ pool might be different.\n+      \/\/ If a breakpoint is set in a redefined method, its EMCP methods\n+      \/\/ must have a breakpoint also.\n+      \/\/ None of the methods are deleted until none are running.\n+      \/\/ This code could set a breakpoint in a method that\n+      \/\/ is never reached, but this won't be noticeable to the programmer.\n+      if (!method->is_obsolete() &&\n@@ -772,24 +781,1 @@\n-  \/\/ external suspend should have caught suspending a thread twice\n-\n-  \/\/ Immediate suspension required for JPDA back-end so JVMTI agent threads do\n-  \/\/ not deadlock due to later suspension on transitions while holding\n-  \/\/ raw monitors.  Passing true causes the immediate suspension.\n-  \/\/ java_suspend() will catch threads in the process of exiting\n-  \/\/ and will ignore them.\n-  java_thread->java_suspend();\n-\n-  \/\/ It would be nice to have the following assertion in all the time,\n-  \/\/ but it is possible for a racing resume request to have resumed\n-  \/\/ this thread right after we suspended it. Temporarily enable this\n-  \/\/ assertion if you are chasing a different kind of bug.\n-  \/\/\n-  \/\/ assert(java_lang_Thread::thread(java_thread->threadObj()) == NULL ||\n-  \/\/   java_thread->is_being_ext_suspended(), \"thread is not suspended\");\n-\n-  if (java_lang_Thread::thread(java_thread->threadObj()) == NULL) {\n-    \/\/ check again because we can get delayed in java_suspend():\n-    \/\/ the thread is in process of exiting.\n-    return false;\n-  }\n-\n-  return true;\n+  return java_thread->java_suspend();\n@@ -799,11 +785,1 @@\n-  \/\/ external suspend should have caught resuming a thread twice\n-  assert(java_thread->is_being_ext_suspended(), \"thread should be suspended\");\n-\n-  \/\/ resume thread\n-  {\n-    \/\/ must always grab Threads_lock, see JVM_SuspendThread\n-    MutexLocker ml(Threads_lock);\n-    java_thread->java_resume();\n-  }\n-\n-  return true;\n+  return java_thread->java_resume();\n@@ -812,1 +788,0 @@\n-\n@@ -824,1 +799,1 @@\n-    log_stream.print(\"%s(%c \", name, thread->is_being_ext_suspended() ? 'S' : '_');\n+    log_stream.print(\"%s(%c \", name, thread->is_suspended() ? 'S' : '_');\n","filename":"src\/hotspot\/share\/prims\/jvmtiImpl.cpp","additions":14,"deletions":39,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"aot\/aotLoader.hpp\"\n+#include \"cds\/metaspaceShared.hpp\"\n@@ -44,1 +44,0 @@\n-#include \"memory\/metaspaceShared.hpp\"\n@@ -4418,4 +4417,0 @@\n-  \/\/ Replace fingerprint data\n-  the_class->set_has_passed_fingerprint_check(scratch_class->has_passed_fingerprint_check());\n-  the_class->store_fingerprint(scratch_class->get_stored_fingerprint());\n-\n@@ -4424,6 +4419,0 @@\n-  if (!the_class->should_be_initialized()) {\n-    \/\/ Class was already initialized, so AOT has only seen the original version.\n-    \/\/ We need to let AOT look at it again.\n-    AOTLoader::load_for_klass(the_class, current);\n-  }\n-\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":1,"deletions":12,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -27,0 +27,4 @@\n+#include \"cds\/cdsoffsets.hpp\"\n+#include \"cds\/filemap.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/metaspaceShared.hpp\"\n@@ -46,2 +50,0 @@\n-#include \"memory\/filemap.hpp\"\n-#include \"memory\/heapShared.inline.hpp\"\n@@ -52,1 +54,0 @@\n-#include \"memory\/metaspaceShared.hpp\"\n@@ -98,3 +99,0 @@\n-#if INCLUDE_CDS\n-#include \"prims\/cdsoffsets.hpp\"\n-#endif \/\/ INCLUDE_CDS\n@@ -121,3 +119,0 @@\n-#if INCLUDE_AOT\n-#include \"aot\/aotLoader.hpp\"\n-#endif \/\/ INCLUDE_AOT\n@@ -262,1 +257,1 @@\n-  ReservedHeapSpace rhs(100 * granularity, granularity, false);\n+  ReservedHeapSpace rhs(100 * granularity, granularity, os::vm_page_size());\n@@ -289,1 +284,1 @@\n-  ReservedHeapSpace rhs(reserved_space_size * granularity, granularity, false);\n+  ReservedHeapSpace rhs(reserved_space_size * granularity, granularity, os::vm_page_size());\n@@ -1050,1 +1045,1 @@\n-  if (clinit == NULL) {\n+  if (clinit == NULL || clinit->method_holder()->is_not_initialized()) {\n@@ -1434,3 +1429,0 @@\n-  if (code->is_aot()) {\n-    return -1;\n-  }\n@@ -1494,1 +1486,1 @@\n-  int insts_size = comp_level == CompLevel_aot ? code->code_end() - code->code_begin() : code->insts_size();\n+  int insts_size = code->insts_size();\n@@ -2323,8 +2315,0 @@\n-WB_ENTRY(jint, WB_AotLibrariesCount(JNIEnv* env, jobject o))\n-  jint result = 0;\n-#if INCLUDE_AOT\n-  result = (jint) AOTLoader::heaps_count();\n-#endif\n-  return result;\n-WB_END\n-\n@@ -2683,1 +2667,0 @@\n-  {CC\"aotLibrariesCount\", CC\"()I\",                    (void*)&WB_AotLibrariesCount },\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":8,"deletions":25,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/filemap.hpp\"\n@@ -41,1 +42,0 @@\n-#include \"memory\/filemap.hpp\"\n@@ -50,1 +50,1 @@\n-#include \"runtime\/os.inline.hpp\"\n+#include \"runtime\/os.hpp\"\n@@ -524,0 +524,2 @@\n+  { \"SuspendRetryCount\",            JDK_Version::undefined(), JDK_Version::jdk(17), JDK_Version::jdk(18) },\n+  { \"SuspendRetryDelay\",            JDK_Version::undefined(), JDK_Version::jdk(17), JDK_Version::jdk(18) },\n@@ -3173,15 +3175,0 @@\n-#if !INCLUDE_AOT\n-  UNSUPPORTED_OPTION(UseAOT);\n-  UNSUPPORTED_OPTION(PrintAOT);\n-  UNSUPPORTED_OPTION(UseAOTStrictLoading);\n-  UNSUPPORTED_OPTION_NULL(AOTLibrary);\n-\n-  UNSUPPORTED_OPTION_INIT(Tier3AOTInvocationThreshold, 0);\n-  UNSUPPORTED_OPTION_INIT(Tier3AOTMinInvocationThreshold, 0);\n-  UNSUPPORTED_OPTION_INIT(Tier3AOTCompileThreshold, 0);\n-  UNSUPPORTED_OPTION_INIT(Tier3AOTBackEdgeThreshold, 0);\n-#ifndef PRODUCT\n-  UNSUPPORTED_OPTION(PrintAOTStatistics);\n-#endif\n-#endif\n-\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":4,"deletions":17,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -928,3 +928,1 @@\n-\n-\n-#if COMPILER2_OR_JVMCI || INCLUDE_AOT\n+#if COMPILER2_OR_JVMCI\n@@ -1058,2 +1056,0 @@\n-#endif \/\/ COMPILER2_OR_JVMCI || INCLUDE_AOT\n-#if COMPILER2_OR_JVMCI\n@@ -1077,1 +1073,0 @@\n-#if COMPILER2_OR_JVMCI || INCLUDE_AOT\n@@ -1086,1 +1081,1 @@\n-#endif \/\/ COMPILER2_OR_JVMCI || INCLUDE_AOT\n+\n@@ -1481,1 +1476,1 @@\n-#if COMPILER2_OR_JVMCI || INCLUDE_AOT\n+\n@@ -1486,1 +1481,0 @@\n-#endif \/\/ COMPILER2_OR_JVMCI || INCLUDE_AOT\n@@ -2063,1 +2057,1 @@\n-    if (nm->is_compiled_by_jvmci() && nm->is_nmethod()) { \/\/ Exclude AOTed methods\n+    if (nm->is_compiled_by_jvmci()) {\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":4,"deletions":10,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -594,1 +594,0 @@\n-\/\/    A: Java frame (aot compiled)\n@@ -636,3 +635,1 @@\n-        if (cm->is_aot()) {\n-          st->print(\"A %d \", cm->compile_id());\n-        } else if (cm->is_nmethod()) {\n+        if (cm->is_nmethod()) {\n@@ -1253,1 +1250,1 @@\n-                    FormatBuffer<1024>(\"#%d nmethod \" INTPTR_FORMAT \" for method %s%s%s\", frame_no,\n+                    FormatBuffer<1024>(\"#%d nmethod \" INTPTR_FORMAT \" for method J %s%s\", frame_no,\n@@ -1255,1 +1252,0 @@\n-                                       (cm->is_aot() ? \"A \": \"J \"),\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -429,10 +429,0 @@\n-  \/* 50 retries * (5 * current_retry_count) millis = ~6.375 seconds *\/      \\\n-  \/* typically, at most a few retries are needed                    *\/      \\\n-  product(intx, SuspendRetryCount, 50,                                      \\\n-          \"Maximum retry count for an external suspend request\")            \\\n-          range(0, max_intx)                                                \\\n-                                                                            \\\n-  product(intx, SuspendRetryDelay, 5,                                       \\\n-          \"Milliseconds to delay per retry (* current_retry_count)\")        \\\n-          range(0, max_intx)                                                \\\n-                                                                            \\\n@@ -1665,19 +1655,0 @@\n-  \/* AOT parameters *\/                                                      \\\n-  product(bool, UseAOT, false, EXPERIMENTAL,                                \\\n-          \"Use AOT compiled files\")                                         \\\n-                                                                            \\\n-  product(ccstrlist, AOTLibrary, NULL, EXPERIMENTAL,                        \\\n-          \"AOT library\")                                                    \\\n-                                                                            \\\n-  product(bool, PrintAOT, false, EXPERIMENTAL,                              \\\n-          \"Print used AOT klasses and methods\")                             \\\n-                                                                            \\\n-  notproduct(bool, PrintAOTStatistics, false,                               \\\n-          \"Print AOT statistics\")                                           \\\n-                                                                            \\\n-  product(bool, UseAOTStrictLoading, false, DIAGNOSTIC,                     \\\n-          \"Exit the VM if any of the AOT libraries has invalid config\")     \\\n-                                                                            \\\n-  product(bool, CalculateClassFingerprint, false,                           \\\n-          \"Calculate class fingerprint\")                                    \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":0,"deletions":29,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -69,1 +69,0 @@\n-void AOTLoader_init();\n@@ -122,1 +121,0 @@\n-  AOTLoader_init();               \/\/ depends on VM_Version_init to adjust vm options\n","filename":"src\/hotspot\/share\/runtime\/init.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"aot\/aotLoader.hpp\"\n@@ -1372,2 +1371,2 @@\n-            callee != NULL && (callee->is_compiled_by_jvmci() || callee->is_aot())) {\n-          return true; \/\/ skip patching for JVMCI or AOT code\n+            callee != NULL && callee->is_compiled_by_jvmci()) {\n+          return true; \/\/ skip patching for JVMCI\n@@ -3169,3 +3168,6 @@\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n-      VMRegPair*   regs = NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);\n-      int i=0;\n+      BasicType stack_sig_bt[16];\n+      VMRegPair stack_regs[16];\n+      BasicType* sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n+      VMRegPair* regs = (total_args_passed <= 16) ? stack_regs : NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);\n+\n+      int i = 0;\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -207,0 +207,6 @@\n+  if (_previous_name == vmSymbols::java_lang_Object()) {\n+    \/\/ no names were created\n+    assert(_names == NULL, \"_names unexpectedly created\");\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/signature.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"runtime\/os.inline.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"aot\/aotLoader.hpp\"\n+#include \"cds\/metaspaceShared.hpp\"\n@@ -58,1 +58,0 @@\n-#include \"memory\/metaspaceShared.hpp\"\n@@ -265,1 +264,0 @@\n-  NOT_PRODUCT(_no_safepoint_count = 0;)\n@@ -269,3 +267,0 @@\n-  _current_pending_monitor = NULL;\n-  _current_pending_monitor_is_from_java = true;\n-  _current_waiting_monitor = NULL;\n@@ -274,6 +269,0 @@\n-#ifdef ASSERT\n-  _visited_for_critical_count = false;\n-#endif\n-\n-  _SR_lock = new Monitor(Mutex::suspend_resume, \"SR_lock\", true,\n-                         Monitor::_safepoint_check_sometimes);\n@@ -288,4 +277,0 @@\n-  _OnTrap   = 0;\n-  _Stalled  = 0;\n-  _TypeTag  = 0x2BAD;\n-\n@@ -454,3 +439,3 @@\n-  \/\/ It's possible we can encounter a null _ParkEvent, etc., in stillborn threads.\n-  \/\/ We NULL out the fields for good hygiene.\n-  ParkEvent::Release(_ParkEvent); _ParkEvent   = NULL;\n+  ParkEvent::Release(_ParkEvent);\n+  \/\/ Set to NULL as a termination indicator for has_terminated().\n+  Atomic::store(&_ParkEvent, (ParkEvent*)NULL);\n@@ -461,5 +446,0 @@\n-  \/\/ SR_handler uses this as a termination indicator -\n-  \/\/ needs to happen before os::free_thread()\n-  delete _SR_lock;\n-  _SR_lock = NULL;\n-\n@@ -586,113 +566,0 @@\n-\n-\/\/ Check if an external suspend request has completed (or has been\n-\/\/ cancelled). Returns true if the thread is externally suspended and\n-\/\/ false otherwise.\n-bool JavaThread::is_ext_suspend_completed() {\n-  bool did_trans_retry = false;  \/\/ only do thread_in_native_trans retry once\n-  bool do_trans_retry;           \/\/ flag to force the retry\n-\n-  do {\n-    do_trans_retry = false;\n-\n-    if (is_exiting()) {\n-      \/\/ Thread is in the process of exiting. This is always checked\n-      \/\/ first to reduce the risk of dereferencing a freed JavaThread.\n-      return false;\n-    }\n-\n-    if (!is_external_suspend()) {\n-      \/\/ Suspend request is cancelled. This is always checked before\n-      \/\/ is_ext_suspended() to reduce the risk of a rogue resume\n-      \/\/ confusing the thread that made the suspend request.\n-      return false;\n-    }\n-\n-    if (is_ext_suspended()) {\n-      \/\/ thread is suspended\n-      return true;\n-    }\n-\n-    \/\/ Now that we no longer do hard suspends of threads running\n-    \/\/ native code, the target thread can be changing thread state\n-    \/\/ while we are in this routine:\n-    \/\/\n-    \/\/   _thread_in_native -> _thread_in_native_trans -> _thread_blocked\n-    \/\/\n-    \/\/ We save a copy of the thread state as observed at this moment\n-    \/\/ and make our decision about suspend completeness based on the\n-    \/\/ copy. This closes the race where the thread state is seen as\n-    \/\/ _thread_in_native_trans in the if-thread_blocked check, but is\n-    \/\/ seen as _thread_blocked in if-thread_in_native_trans check.\n-    JavaThreadState save_state = thread_state();\n-\n-    if (save_state == _thread_blocked && is_suspend_equivalent()) {\n-      \/\/ If the thread's state is _thread_blocked and this blocking\n-      \/\/ condition is known to be equivalent to a suspend, then we can\n-      \/\/ consider the thread to be externally suspended. This means that\n-      \/\/ the code that sets _thread_blocked has been modified to do\n-      \/\/ self-suspension if the blocking condition releases. We also\n-      \/\/ used to check for CONDVAR_WAIT here, but that is now covered by\n-      \/\/ the _thread_blocked with self-suspension check.\n-      \/\/\n-      \/\/ Return true since we wouldn't be here unless there was still an\n-      \/\/ external suspend request.\n-      return true;\n-    } else if (save_state == _thread_in_native && frame_anchor()->walkable()) {\n-      \/\/ Threads running native code will self-suspend on native==>VM\/Java\n-      \/\/ transitions. If its stack is walkable (should always be the case\n-      \/\/ unless this function is called before the actual java_suspend()\n-      \/\/ call), then the wait is done.\n-      return true;\n-    } else if (!did_trans_retry &&\n-               save_state == _thread_in_native_trans &&\n-               frame_anchor()->walkable()) {\n-      \/\/ The thread is transitioning from thread_in_native to another\n-      \/\/ thread state. check_safepoint_and_suspend_for_native_trans()\n-      \/\/ will force the thread to self-suspend. If it hasn't gotten\n-      \/\/ there yet we may have caught the thread in-between the native\n-      \/\/ code check above and the self-suspend.\n-      \/\/\n-      \/\/ Since we use the saved thread state in the if-statement above,\n-      \/\/ there is a chance that the thread has already transitioned to\n-      \/\/ _thread_blocked by the time we get here. In that case, we will\n-      \/\/ make a single unnecessary pass through the logic below. This\n-      \/\/ doesn't hurt anything since we still do the trans retry.\n-\n-      \/\/ Once the thread leaves thread_in_native_trans for another\n-      \/\/ thread state, we break out of this retry loop. We shouldn't\n-      \/\/ need this flag to prevent us from getting back here, but\n-      \/\/ sometimes paranoia is good.\n-      did_trans_retry = true;\n-\n-      \/\/ We wait for the thread to transition to a more usable state.\n-      for (int i = 1; i <= SuspendRetryCount; i++) {\n-        \/\/ We used to do an \"os::yield_all(i)\" call here with the intention\n-        \/\/ that yielding would increase on each retry. However, the parameter\n-        \/\/ is ignored on Linux which means the yield didn't scale up. Waiting\n-        \/\/ on the SR_lock below provides a much more predictable scale up for\n-        \/\/ the delay. It also provides a simple\/direct point to check for any\n-        \/\/ safepoint requests from the VMThread\n-\n-        \/\/ temporarily drops SR_lock while doing wait with safepoint check\n-        \/\/ (if we're a JavaThread - the WatcherThread can also call this)\n-        \/\/ and increase delay with each retry\n-        if (Thread::current()->is_Java_thread()) {\n-          SR_lock()->wait(i * SuspendRetryDelay);\n-        } else {\n-          SR_lock()->wait_without_safepoint_check(i * SuspendRetryDelay);\n-        }\n-\n-        \/\/ check the actual thread state instead of what we saved above\n-        if (thread_state() != _thread_in_native_trans) {\n-          \/\/ the thread has transitioned to another thread state so\n-          \/\/ try all the checks (except this one) one more time.\n-          do_trans_retry = true;\n-          break;\n-        }\n-      } \/\/ end retry loop\n-    }\n-  } while (do_trans_retry);\n-\n-  return false;\n-}\n-\n@@ -844,32 +711,0 @@\n-\n-\/\/ Checks safepoint allowed and clears unhandled oops at potential safepoints.\n-void Thread::check_possible_safepoint() {\n-  if (!is_Java_thread()) return;\n-\n-  if (_no_safepoint_count > 0) {\n-    print_owned_locks();\n-    assert(false, \"Possible safepoint reached by thread that does not allow it\");\n-  }\n-#ifdef CHECK_UNHANDLED_OOPS\n-  \/\/ Clear unhandled oops in JavaThreads so we get a crash right away.\n-  clear_unhandled_oops();\n-#endif \/\/ CHECK_UNHANDLED_OOPS\n-}\n-\n-void Thread::check_for_valid_safepoint_state() {\n-  if (!is_Java_thread()) return;\n-\n-  \/\/ Check NoSafepointVerifier, which is implied by locks taken that can be\n-  \/\/ shared with the VM thread.  This makes sure that no locks with allow_vm_block\n-  \/\/ are held.\n-  check_possible_safepoint();\n-\n-  if (this->as_Java_thread()->thread_state() != _thread_in_vm) {\n-    fatal(\"LEAF method calling lock?\");\n-  }\n-\n-  if (GCALotAtAllSafepoints) {\n-    \/\/ We could enter a safepoint here and thus have a gc\n-    InterfaceSupport::check_gc_alot();\n-  }\n-}\n@@ -950,7 +785,4 @@\n-static char java_version[64] = \"\";\n-static char java_runtime_name[128] = \"\";\n-static char java_runtime_version[128] = \"\";\n-static char java_runtime_vendor_version[128] = \"\";\n-static char java_runtime_vendor_vm_bug_url[128] = \"\";\n-\n-\/\/ Extract version and vendor specific information.\n+\/\/ Extract version and vendor specific information from\n+\/\/ java.lang.VersionProps fields.\n+\/\/ Returned char* is allocated in the thread's resource area\n+\/\/ so must be copied for permanency.\n@@ -958,3 +790,1 @@\n-                                         Symbol* field_name,\n-                                         char* buffer,\n-                                         int buffer_size) {\n+                                         Symbol* field_name) {\n@@ -970,3 +800,1 @@\n-    const char* name = java_lang_String::as_utf8_string(name_oop,\n-                                                        buffer,\n-                                                        buffer_size);\n+    const char* name = java_lang_String::as_utf8_string(name_oop);\n@@ -1168,0 +996,30 @@\n+#ifdef ASSERT\n+\/\/ Checks safepoint allowed and clears unhandled oops at potential safepoints.\n+void JavaThread::check_possible_safepoint() {\n+  if (_no_safepoint_count > 0) {\n+    print_owned_locks();\n+    assert(false, \"Possible safepoint reached by thread that does not allow it\");\n+  }\n+#ifdef CHECK_UNHANDLED_OOPS\n+  \/\/ Clear unhandled oops in JavaThreads so we get a crash right away.\n+  clear_unhandled_oops();\n+#endif \/\/ CHECK_UNHANDLED_OOPS\n+}\n+\n+void JavaThread::check_for_valid_safepoint_state() {\n+  \/\/ Check NoSafepointVerifier, which is implied by locks taken that can be\n+  \/\/ shared with the VM thread.  This makes sure that no locks with allow_vm_block\n+  \/\/ are held.\n+  check_possible_safepoint();\n+\n+  if (thread_state() != _thread_in_vm) {\n+    fatal(\"LEAF method calling lock?\");\n+  }\n+\n+  if (GCALotAtAllSafepoints) {\n+    \/\/ We could enter a safepoint here and thus have a gc\n+    InterfaceSupport::check_gc_alot();\n+  }\n+}\n+#endif \/\/ ASSERT\n+\n@@ -1186,0 +1044,5 @@\n+  _current_pending_monitor(NULL),\n+  _current_pending_monitor_is_from_java(true),\n+  _current_waiting_monitor(NULL),\n+  _Stalled(0),\n+\n@@ -1192,0 +1055,4 @@\n+#ifdef ASSERT\n+  _no_safepoint_count(0),\n+  _visited_for_critical_count(false),\n+#endif\n@@ -1194,1 +1061,0 @@\n-  _suspend_equivalent(false),\n@@ -1242,1 +1108,0 @@\n-\n@@ -1567,17 +1432,4 @@\n-    \/\/ We have notified the agents that we are exiting, before we go on,\n-    \/\/ we must check for a pending external suspend request and honor it\n-    \/\/ in order to not surprise the thread that made the suspend request.\n-    while (true) {\n-      {\n-        MutexLocker ml(SR_lock(), Mutex::_no_safepoint_check_flag);\n-        if (!is_external_suspend()) {\n-          set_terminated(_thread_exiting);\n-          ThreadService::current_thread_exiting(this, is_daemon(threadObj()));\n-          break;\n-        }\n-        \/\/ Implied else:\n-        \/\/ Things get a little tricky here. We have a pending external\n-        \/\/ suspend request, but we are holding the SR_lock so we\n-        \/\/ can't just self-suspend. So we temporarily drop the lock\n-        \/\/ and then self-suspend.\n-      }\n+    \/\/ The careful dance between thread suspension and exit is handled here.\n+    \/\/ Since we are in thread_in_vm state and suspension is done with handshakes,\n+    \/\/ we can just put in the exiting state and it will be correctly handled.\n+    set_terminated(_thread_exiting);\n@@ -1585,9 +1437,1 @@\n-      ThreadBlockInVM tbivm(this);\n-      java_suspend_self();\n-\n-      \/\/ We're done with this suspend request, but we have to loop around\n-      \/\/ and check again. Eventually we will get SR_lock without a pending\n-      \/\/ external suspend request and will be able to mark ourselves as\n-      \/\/ exiting.\n-    }\n-    \/\/ no more external suspends are allowed at this point\n+    ThreadService::current_thread_exiting(this, is_daemon(threadObj()));\n@@ -1867,6 +1711,0 @@\n-  \/\/ Check for pending external suspend.\n-  if (is_external_suspend_with_lock()) {\n-    frame_anchor()->make_walkable(this);\n-    java_suspend_self_with_safepoint_check();\n-  }\n-\n@@ -1934,0 +1772,1 @@\n+\n@@ -1936,9 +1775,3 @@\n-\/\/ Tell the VM to suspend a thread when ever it knows that it does not hold on\n-\/\/ to any VM_locks and it is at a transition\n-\/\/ Self-suspension will happen on the transition out of the vm.\n-\/\/ Catch \"this\" coming in from JNIEnv pointers when the thread has been freed\n-\/\/\n-\/\/ Guarantees on return:\n-\/\/   + Target thread will not execute any new bytecode (that's why we need to\n-\/\/     force a safepoint)\n-\/\/   + Target thread will not enter any new monitors\n+\/\/ Guarantees on return (for a valid target thread):\n+\/\/   - Target thread will not execute any new bytecode.\n+\/\/   - Target thread will not enter any new monitors.\n@@ -1946,1 +1779,1 @@\n-void JavaThread::java_suspend() {\n+bool JavaThread::java_suspend() {\n@@ -1948,32 +1781,3 @@\n-  if (!tlh.includes(this) || threadObj() == NULL || is_exiting()) {\n-    return;\n-  }\n-\n-  { MutexLocker ml(SR_lock(), Mutex::_no_safepoint_check_flag);\n-    if (!is_external_suspend()) {\n-      \/\/ a racing resume has cancelled us; bail out now\n-      return;\n-    }\n-\n-    \/\/ suspend is done\n-\n-    \/\/ Warning: is_ext_suspend_completed() may temporarily drop the\n-    \/\/ SR_lock to allow the thread to reach a stable thread state if\n-    \/\/ it is currently in a transient thread state.\n-    if (is_ext_suspend_completed()) {\n-      return;\n-    }\n-  }\n-\n-  if (Thread::current() == this) {\n-    \/\/ Safely self-suspend.\n-    \/\/ If we don't do this explicitly it will implicitly happen\n-    \/\/ before we transition back to Java, and on some other thread-state\n-    \/\/ transition paths, but not as we exit a JVM TI SuspendThread call.\n-    \/\/ As SuspendThread(current) must not return (until resumed) we must\n-    \/\/ self-suspend here.\n-    ThreadBlockInVM tbivm(this);\n-    java_suspend_self();\n-  } else {\n-    VM_ThreadSuspend vm_suspend;\n-    VMThread::execute(&vm_suspend);\n+  if (!tlh.includes(this)) {\n+    log_trace(thread, suspend)(\"JavaThread:\" INTPTR_FORMAT \" not on ThreadsList, no suspension\", p2i(this));\n+    return false;\n@@ -1981,0 +1785,1 @@\n+  return this->handshake_state()->suspend();\n@@ -1983,21 +1788,5 @@\n-\/\/ Part II of external suspension.\n-\/\/ A JavaThread self suspends when it detects a pending external suspend\n-\/\/ request. This is usually on transitions. It is also done in places\n-\/\/ where continuing to the next transition would surprise the caller,\n-\/\/ e.g., monitor entry.\n-\/\/\n-\/\/ Returns the number of times that the thread self-suspended.\n-\/\/\n-\/\/ Note: DO NOT call java_suspend_self() when you just want to block current\n-\/\/       thread. java_suspend_self() is the second stage of cooperative\n-\/\/       suspension for external suspend requests and should only be used\n-\/\/       to complete an external suspend request.\n-\/\/\n-int JavaThread::java_suspend_self() {\n-  assert(thread_state() == _thread_blocked, \"wrong state for java_suspend_self()\");\n-  int ret = 0;\n-\n-  \/\/ we are in the process of exiting so don't suspend\n-  if (is_exiting()) {\n-    clear_external_suspend();\n-    return ret;\n+bool JavaThread::java_resume() {\n+  ThreadsListHandle tlh;\n+  if (!tlh.includes(this)) {\n+    log_trace(thread, suspend)(\"JavaThread:\" INTPTR_FORMAT \" not on ThreadsList, nothing to resume\", p2i(this));\n+    return false;\n@@ -2005,73 +1794,1 @@\n-\n-  assert(_anchor.walkable() || !has_last_Java_frame(),\n-         \"must have walkable stack\");\n-\n-  MonitorLocker ml(SR_lock(), Mutex::_no_safepoint_check_flag);\n-\n-  assert(!this->is_ext_suspended(),\n-         \"a thread trying to self-suspend should not already be suspended\");\n-\n-  if (this->is_suspend_equivalent()) {\n-    \/\/ If we are self-suspending as a result of the lifting of a\n-    \/\/ suspend equivalent condition, then the suspend_equivalent\n-    \/\/ flag is not cleared until we set the ext_suspended flag.\n-    this->clear_suspend_equivalent();\n-  }\n-\n-  \/\/ A racing resume may have cancelled us before we grabbed SR_lock\n-  \/\/ above. Or another external suspend request could be waiting for us\n-  \/\/ by the time we return from SR_lock()->wait(). The thread\n-  \/\/ that requested the suspension may already be trying to walk our\n-  \/\/ stack and if we return now, we can change the stack out from under\n-  \/\/ it. This would be a \"bad thing (TM)\" and cause the stack walker\n-  \/\/ to crash. We stay self-suspended until there are no more pending\n-  \/\/ external suspend requests.\n-  while (is_external_suspend()) {\n-    ret++;\n-    this->set_ext_suspended();\n-\n-    \/\/ _ext_suspended flag is cleared by java_resume()\n-    while (is_ext_suspended()) {\n-      ml.wait();\n-    }\n-  }\n-  return ret;\n-}\n-\n-\/\/ Helper routine to set up the correct thread state before calling java_suspend_self.\n-\/\/ This is called when regular thread-state transition helpers can't be used because\n-\/\/ we can be in various states, in particular _thread_in_native_trans.\n-\/\/ We have to set the thread state directly to _thread_blocked so that it will\n-\/\/ be seen to be safepoint\/handshake safe whilst suspended. This is also\n-\/\/ necessary to allow a thread in is_ext_suspend_completed, that observed the\n-\/\/ _thread_in_native_trans state, to proceed.\n-\/\/ The problem with setting thread state directly is that a\n-\/\/ safepoint could happen just after java_suspend_self() returns after being resumed,\n-\/\/ and the VM thread will see the _thread_blocked state. So we must check for a safepoint\n-\/\/ after restoring the state to make sure we won't leave while a safepoint is in progress.\n-\/\/ However, not all initial-states are allowed when performing a safepoint check, as we\n-\/\/ should never be blocking at a safepoint whilst in those states(*). Of these 'bad' states\n-\/\/ only _thread_in_native is possible when executing this code (based on our two callers).\n-\/\/ A thread that is _thread_in_native is already safepoint-safe and so it doesn't matter\n-\/\/ whether the VMThread sees the _thread_blocked state, or the _thread_in_native state,\n-\/\/ and so we don't need the explicit safepoint check.\n-\/\/ (*) See switch statement in SafepointSynchronize::block() for thread states that are\n-\/\/ allowed when performing a safepoint check.\n-\n-void JavaThread::java_suspend_self_with_safepoint_check() {\n-  assert(this == Thread::current(), \"invariant\");\n-  JavaThreadState state = thread_state();\n-\n-  do {\n-    set_thread_state(_thread_blocked);\n-    java_suspend_self();\n-    \/\/ The current thread could have been suspended again. We have to check for\n-    \/\/ suspend after restoring the saved state. Without this the current thread\n-    \/\/ might return to _thread_in_Java and execute bytecodes for an arbitrary\n-    \/\/ long time.\n-    set_thread_state_fence(state);\n-\n-    if (state != _thread_in_native) {\n-      SafepointMechanism::process_if_requested(this);\n-    }\n-  } while (is_external_suspend());\n+  return this->handshake_state()->resume();\n@@ -2082,10 +1799,9 @@\n-\/\/ This method is very similar to JavaThread::java_suspend_self_with_safepoint_check()\n-\/\/ and has the same callers. It also performs a raw thread state transition to\n-\/\/ _thread_blocked and back again to the original state before returning. The current\n-\/\/ thread is required to change to _thread_blocked in order to be seen to be\n-\/\/ safepoint\/handshake safe whilst suspended and only after becoming handshake safe,\n-\/\/ the other thread can complete the handshake used to synchronize with this thread\n-\/\/ and then perform the reallocation and relocking. We cannot use the thread state\n-\/\/ transition helpers because we arrive here in various states and also because the\n-\/\/ helpers indirectly call this method.  After leaving _thread_blocked we have to\n-\/\/ check for safepoint\/handshake, except if _thread_in_native. The thread is safe\n+\/\/ Raw thread state transition to _thread_blocked and back again to the original\n+\/\/ state before returning are performed. The current thread is required to\n+\/\/ change to _thread_blocked in order to be seen to be safepoint\/handshake safe\n+\/\/ whilst suspended and only after becoming handshake safe, the other thread can\n+\/\/ complete the handshake used to synchronize with this thread and then perform\n+\/\/ the reallocation and relocking. We cannot use the thread state transition\n+\/\/ helpers because we arrive here in various states and also because the helpers\n+\/\/ indirectly call this method.  After leaving _thread_blocked we have to check\n+\/\/ for safepoint\/handshake, except if _thread_in_native. The thread is safe\n@@ -2103,4 +1819,0 @@\n-    \/\/ Check if _external_suspend was set in the previous loop iteration.\n-    if (is_external_suspend()) {\n-      java_suspend_self();\n-    }\n@@ -2134,1 +1846,1 @@\n-  } while (is_obj_deopt_suspend() || is_external_suspend());\n+  } while (is_obj_deopt_suspend());\n@@ -2148,8 +1860,6 @@\n-\/\/ Slow path when the native==>VM\/Java barriers detect a safepoint is in\n-\/\/ progress or when _suspend_flags is non-zero.\n-\/\/ Current thread needs to self-suspend if there is a suspend request and\/or\n-\/\/ block if a safepoint is in progress.\n-\/\/ Async exception ISN'T checked.\n-\/\/ Note only the ThreadInVMfromNative transition can call this function\n-\/\/ directly and when thread state is _thread_in_native_trans\n-void JavaThread::check_safepoint_and_suspend_for_native_trans(JavaThread *thread) {\n+\/\/ Slow path when the native==>Java barriers detect a safepoint\/handshake is\n+\/\/ pending, when _suspend_flags is non-zero or when we need to process a stack\n+\/\/ watermark. Also check for pending async exceptions (except unsafe access error).\n+\/\/ Note only the native==>Java barriers can call this function when thread state\n+\/\/ is _thread_in_native_trans.\n+void JavaThread::check_special_condition_for_native_trans(JavaThread *thread) {\n@@ -2157,1 +1867,1 @@\n-  assert(!thread->has_last_Java_frame() || thread->frame_anchor()->walkable(), \"Unwalkable stack in native->vm transition\");\n+  assert(!thread->has_last_Java_frame() || thread->frame_anchor()->walkable(), \"Unwalkable stack in native->Java transition\");\n@@ -2159,11 +1869,0 @@\n-  SafepointMechanism::process_if_requested_with_exit_check(thread, false \/* check asyncs *\/);\n-}\n-\n-\/\/ Slow path when the native==>VM\/Java barriers detect a safepoint is in\n-\/\/ progress or when _suspend_flags is non-zero.\n-\/\/ Current thread needs to self-suspend if there is a suspend request and\/or\n-\/\/ block if a safepoint is in progress.\n-\/\/ Also check for pending async exception (not including unsafe access error).\n-\/\/ Note only the native==>VM\/Java barriers can call this function and when\n-\/\/ thread state is _thread_in_native_trans.\n-void JavaThread::check_special_condition_for_native_trans(JavaThread *thread) {\n@@ -2173,1 +1872,1 @@\n-  check_safepoint_and_suspend_for_native_trans(thread);\n+  SafepointMechanism::process_if_requested_with_exit_check(thread, false \/* check asyncs *\/);\n@@ -2187,23 +1886,0 @@\n-\/\/ We need to guarantee the Threads_lock here, since resumes are not\n-\/\/ allowed during safepoint synchronization\n-\/\/ Can only resume from an external suspension\n-void JavaThread::java_resume() {\n-  assert_locked_or_safepoint(Threads_lock);\n-\n-  \/\/ Sanity check: thread is gone, has started exiting or the thread\n-  \/\/ was not externally suspended.\n-  ThreadsListHandle tlh;\n-  if (!tlh.includes(this) || is_exiting() || !is_external_suspend()) {\n-    return;\n-  }\n-\n-  MutexLocker ml(SR_lock(), Mutex::_no_safepoint_check_flag);\n-\n-  clear_external_suspend();\n-\n-  if (is_ext_suspended()) {\n-    clear_ext_suspended();\n-    SR_lock()->notify_all();\n-  }\n-}\n-\n@@ -2778,8 +2454,0 @@\n-\n-      this->set_suspend_equivalent();\n-      \/\/ cleared by handle_special_suspend_equivalent_condition() or\n-      \/\/ java_suspend_self() via check_and_wait_while_suspended()\n-\n-\n-      \/\/ were we externally suspended while we were waiting?\n-      this->check_and_wait_while_suspended();\n@@ -2993,1 +2661,3 @@\n-  \/\/ get the Java runtime name, version, and vendor info after java.lang.System is initialized\n+  \/\/ Get the Java runtime name, version, and vendor info after java.lang.System is initialized.\n+  \/\/ Some values are actually configure-time constants but some can be set via the jlink tool and\n+  \/\/ so must be read dynamically. We treat them all the same.\n@@ -2996,0 +2666,3 @@\n+  {\n+    ResourceMark rm(main_thread);\n+    JDK_Version::set_java_version(get_java_version_info(ik, vmSymbols::java_version_name()));\n@@ -2997,5 +2670,1 @@\n-  JDK_Version::set_java_version(get_java_version_info(ik, vmSymbols::java_version_name(),\n-                                                      java_version, sizeof(java_version)));\n-\n-  JDK_Version::set_runtime_name(get_java_version_info(ik, vmSymbols::java_runtime_name_name(),\n-                                                      java_runtime_name, sizeof(java_runtime_name)));\n+    JDK_Version::set_runtime_name(get_java_version_info(ik, vmSymbols::java_runtime_name_name()));\n@@ -3003,2 +2672,1 @@\n-  JDK_Version::set_runtime_version(get_java_version_info(ik, vmSymbols::java_runtime_version_name(),\n-                                                         java_runtime_version, sizeof(java_runtime_version)));\n+    JDK_Version::set_runtime_version(get_java_version_info(ik, vmSymbols::java_runtime_version_name()));\n@@ -3006,3 +2674,1 @@\n-  JDK_Version::set_runtime_vendor_version(get_java_version_info(ik, vmSymbols::java_runtime_vendor_version_name(),\n-                                                                java_runtime_vendor_version,\n-                                                                sizeof(java_runtime_vendor_version)));\n+    JDK_Version::set_runtime_vendor_version(get_java_version_info(ik, vmSymbols::java_runtime_vendor_version_name()));\n@@ -3010,3 +2676,2 @@\n-  JDK_Version::set_runtime_vendor_vm_bug_url(get_java_version_info(ik, vmSymbols::java_runtime_vendor_vm_bug_url_name(),\n-                                                                   java_runtime_vendor_vm_bug_url,\n-                                                                   sizeof(java_runtime_vendor_vm_bug_url)));\n+    JDK_Version::set_runtime_vendor_vm_bug_url(get_java_version_info(ik, vmSymbols::java_runtime_vendor_vm_bug_url_name()));\n+  }\n@@ -3023,3 +2688,0 @@\n-\n-  \/\/ Eager box cache initialization only if AOT is on and any library is loaded.\n-  AOTLoader::initialize_box_caches(CHECK);\n@@ -3232,2 +2894,2 @@\n-  \/\/ to initially define it have been changed. This is needed for both CDS and\n-  \/\/ AOT, since UseSharedSpaces and UseAOT may be changed after java.vm.info\n+  \/\/ to initially define it have been changed. This is needed for both CDS\n+  \/\/ since UseSharedSpaces may be changed after java.vm.info\n@@ -3710,1 +3372,2 @@\n-  { MonitorLocker nu(Threads_lock);\n+  {\n+    MonitorLocker nu(Threads_lock);\n@@ -3712,3 +3375,2 @@\n-      \/\/ This wait should make safepoint checks, wait without a timeout,\n-      \/\/ and wait as a suspend-equivalent condition.\n-      nu.wait(0, Mutex::_as_suspend_equivalent_flag);\n+      \/\/ This wait should make safepoint checks, wait without a timeout.\n+      nu.wait(0);\n@@ -3883,1 +3545,1 @@\n-    p->set_terminated_value();\n+    p->set_terminated(JavaThread::_thread_terminated);\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":105,"deletions":443,"binary":false,"changes":548,"status":"modified"},{"patch":"@@ -217,66 +217,0 @@\n- private:\n-\n-  \/\/ ***************************************************************\n-  \/\/ Suspend and resume support\n-  \/\/ ***************************************************************\n-  \/\/\n-  \/\/ VM suspend\/resume no longer exists - it was once used for various\n-  \/\/ things including safepoints but was deprecated and finally removed\n-  \/\/ in Java 7. Because VM suspension was considered \"internal\" Java-level\n-  \/\/ suspension was considered \"external\", and this legacy naming scheme\n-  \/\/ remains.\n-  \/\/\n-  \/\/ External suspend\/resume requests come from JVM_SuspendThread,\n-  \/\/ JVM_ResumeThread, JVMTI SuspendThread, and finally JVMTI\n-  \/\/ ResumeThread. External\n-  \/\/ suspend requests cause _external_suspend to be set and external\n-  \/\/ resume requests cause _external_suspend to be cleared.\n-  \/\/ External suspend requests do not nest on top of other external\n-  \/\/ suspend requests. The higher level APIs reject suspend requests\n-  \/\/ for already suspended threads.\n-  \/\/\n-  \/\/ The external_suspend\n-  \/\/ flag is checked by has_special_runtime_exit_condition() and java thread\n-  \/\/ will self-suspend when handle_special_runtime_exit_condition() is\n-  \/\/ called. Most uses of the _thread_blocked state in JavaThreads are\n-  \/\/ considered the same as being externally suspended; if the blocking\n-  \/\/ condition lifts, the JavaThread will self-suspend. Other places\n-  \/\/ where VM checks for external_suspend include:\n-  \/\/   + mutex granting (do not enter monitors when thread is suspended)\n-  \/\/   + state transitions from _thread_in_native\n-  \/\/\n-  \/\/ In general, java_suspend() does not wait for an external suspend\n-  \/\/ request to complete. When it returns, the only guarantee is that\n-  \/\/ the _external_suspend field is true.\n-  \/\/\n-  \/\/ wait_for_ext_suspend_completion() is used to wait for an external\n-  \/\/ suspend request to complete. External suspend requests are usually\n-  \/\/ followed by some other interface call that requires the thread to\n-  \/\/ be quiescent, e.g., GetCallTrace(). By moving the \"wait time\" into\n-  \/\/ the interface that requires quiescence, we give the JavaThread a\n-  \/\/ chance to self-suspend before we need it to be quiescent. This\n-  \/\/ improves overall suspend\/query performance.\n-  \/\/\n-  \/\/ _suspend_flags controls the behavior of java_ suspend\/resume.\n-  \/\/ It must be set under the protection of SR_lock. Read from the flag is\n-  \/\/ OK without SR_lock as long as the value is only used as a hint.\n-  \/\/ (e.g., check _external_suspend first without lock and then recheck\n-  \/\/ inside SR_lock and finish the suspension)\n-  \/\/\n-  \/\/ _suspend_flags is also overloaded for other \"special conditions\" so\n-  \/\/ that a single check indicates whether any special action is needed\n-  \/\/ eg. for async exceptions.\n-  \/\/ -------------------------------------------------------------------\n-  \/\/ Notes:\n-  \/\/ 1. The suspend\/resume logic no longer uses ThreadState in OSThread\n-  \/\/ but we still update its value to keep other part of the system (mainly\n-  \/\/ JVMTI) happy. ThreadState is legacy code (see notes in\n-  \/\/ osThread.hpp).\n-  \/\/\n-  \/\/ 2. It would be more natural if set_external_suspend() is private and\n-  \/\/ part of java_suspend(), but that probably would affect the suspend\/query\n-  \/\/ performance. Need more investigation on this.\n-\n-  \/\/ suspend\/resume lock: used for self-suspend\n-  Monitor* _SR_lock;\n- protected:\n@@ -288,3 +222,0 @@\n-    _external_suspend       = 0x20000000U, \/\/ thread is asked to self suspend\n-    _ext_suspended          = 0x40000000U, \/\/ thread has self-suspended\n-\n@@ -364,12 +295,0 @@\n- private:\n-\n-  \/\/ Debug support for checking if code allows safepoints or not.\n-  \/\/ Safepoints in the VM can happen because of allocation, invoking a VM operation, or blocking on\n-  \/\/ mutex, or blocking on an object synchronizer (Java locking).\n-  \/\/ If _no_safepoint_count is non-zero, then an assertion failure will happen in any of\n-  \/\/ the above cases.\n-  \/\/\n-  \/\/ The class NoSafepointVerifier is used to set this counter.\n-  \/\/\n-  NOT_PRODUCT(int _no_safepoint_count;)         \/\/ If 0, thread allow a safepoint to happen\n-\n@@ -381,5 +300,0 @@\n-  friend class NoSafepointVerifier;\n-  friend class PauseNoSafepointVerifier;\n-\n- protected:\n-  SafepointMechanism::ThreadData _poll_data;\n@@ -397,3 +311,0 @@\n-  ObjectMonitor* _current_pending_monitor;      \/\/ ObjectMonitor this thread\n-                                                \/\/ is waiting to lock\n-  bool _current_pending_monitor_is_from_java;   \/\/ locking is from Java code\n@@ -402,29 +313,0 @@\n-\n-\n-  \/\/ ObjectMonitor on which this thread called Object.wait()\n-  ObjectMonitor* _current_waiting_monitor;\n-\n-#ifdef ASSERT\n- private:\n-  volatile uint64_t _visited_for_critical_count;\n-\n- public:\n-  void set_visited_for_critical_count(uint64_t safepoint_id) {\n-    assert(_visited_for_critical_count == 0, \"Must be reset before set\");\n-    assert((safepoint_id & 0x1) == 1, \"Must be odd\");\n-    _visited_for_critical_count = safepoint_id;\n-  }\n-  void reset_visited_for_critical_count(uint64_t safepoint_id) {\n-    assert(_visited_for_critical_count == safepoint_id, \"Was not visited\");\n-    _visited_for_critical_count = 0;\n-  }\n-  bool was_visited_for_critical_count(uint64_t safepoint_id) const {\n-    return _visited_for_critical_count == safepoint_id;\n-  }\n-#endif\n-\n- public:\n-  enum {\n-    is_definitely_current_thread = true\n-  };\n-\n@@ -517,2 +399,0 @@\n-  Monitor* SR_lock() const                       { return _SR_lock; }\n-\n@@ -605,22 +485,0 @@\n-  \/\/ For tracking the heavyweight monitor the thread is pending on.\n-  ObjectMonitor* current_pending_monitor() {\n-    return _current_pending_monitor;\n-  }\n-  void set_current_pending_monitor(ObjectMonitor* monitor) {\n-    _current_pending_monitor = monitor;\n-  }\n-  void set_current_pending_monitor_is_from_java(bool from_java) {\n-    _current_pending_monitor_is_from_java = from_java;\n-  }\n-  bool current_pending_monitor_is_from_java() {\n-    return _current_pending_monitor_is_from_java;\n-  }\n-\n-  \/\/ For tracking the ObjectMonitor on which this thread called Object.wait()\n-  ObjectMonitor* current_waiting_monitor() {\n-    return _current_waiting_monitor;\n-  }\n-  void set_current_waiting_monitor(ObjectMonitor* monitor) {\n-    _current_waiting_monitor = monitor;\n-  }\n-\n@@ -778,5 +636,0 @@\n-  \/\/ These functions check conditions on a JavaThread before possibly going to a safepoint,\n-  \/\/ including NoSafepointVerifier.\n-  void check_for_valid_safepoint_state() NOT_DEBUG_RETURN;\n-  void check_possible_safepoint() NOT_DEBUG_RETURN;\n-\n@@ -799,3 +652,0 @@\n-  static ByteSize polling_word_offset()          { return byte_offset_of(Thread, _poll_data) + byte_offset_of(SafepointMechanism::ThreadData, _polling_word);}\n-  static ByteSize polling_page_offset()          { return byte_offset_of(Thread, _poll_data) + byte_offset_of(SafepointMechanism::ThreadData, _polling_page);}\n-\n@@ -812,3 +662,1 @@\n-  volatile intptr_t _Stalled;\n-  volatile int _TypeTag;\n-  ParkEvent * _ParkEvent;                     \/\/ for Object monitors, JVMTI raw monitors,\n+  ParkEvent * volatile _ParkEvent;            \/\/ for Object monitors, JVMTI raw monitors,\n@@ -816,2 +664,5 @@\n-  int NativeSyncRecursion;                    \/\/ diagnostic\n-  volatile int _OnTrap;                       \/\/ Resume-at IP delta\n+  \/\/ Termination indicator used by the signal handler.\n+  \/\/ _ParkEvent is just a convenient field we can NULL out after setting the JavaThread termination state\n+  \/\/ (which can't itself be read from the signal handler if a signal hits during the Thread destructor).\n+  bool has_terminated()                       { return Atomic::load(&_ParkEvent) == NULL; };\n+\n@@ -874,0 +725,1 @@\n+  friend class HandshakeState;\n@@ -892,8 +744,0 @@\n-#ifndef PRODUCT\n- public:\n-  enum {\n-    jump_ring_buffer_size = 16\n-  };\n- private:  \/\/ restore original namespace restriction\n-#endif\n-\n@@ -937,3 +781,30 @@\n-  MonitorChunk* _monitor_chunks;                 \/\/ Contains the off stack monitors\n-                                                 \/\/ allocated during deoptimization\n-                                                 \/\/ and by JNI_MonitorEnter\/Exit\n+  ObjectMonitor* _current_pending_monitor;              \/\/ ObjectMonitor this thread is waiting to lock\n+  bool           _current_pending_monitor_is_from_java; \/\/ locking is from Java code\n+  ObjectMonitor* _current_waiting_monitor;              \/\/ ObjectMonitor on which this thread called Object.wait()\n+ public:\n+  volatile intptr_t _Stalled;\n+\n+  \/\/ For tracking the heavyweight monitor the thread is pending on.\n+  ObjectMonitor* current_pending_monitor() {\n+    return _current_pending_monitor;\n+  }\n+  void set_current_pending_monitor(ObjectMonitor* monitor) {\n+    _current_pending_monitor = monitor;\n+  }\n+  void set_current_pending_monitor_is_from_java(bool from_java) {\n+    _current_pending_monitor_is_from_java = from_java;\n+  }\n+  bool current_pending_monitor_is_from_java() {\n+    return _current_pending_monitor_is_from_java;\n+  }\n+  ObjectMonitor* current_waiting_monitor() {\n+    return _current_waiting_monitor;\n+  }\n+  void set_current_waiting_monitor(ObjectMonitor* monitor) {\n+    _current_waiting_monitor = monitor;\n+  }\n+\n+ private:\n+  MonitorChunk* _monitor_chunks;              \/\/ Contains the off stack monitors\n+                                              \/\/ allocated during deoptimization\n+                                              \/\/ and by JNI_MonitorEnter\/Exit\n@@ -951,1 +822,1 @@\n- public:                                         \/\/ Expose _thread_state for SafeFetchInt()\n+ public:                                                        \/\/ Expose _thread_state for SafeFetchInt()\n@@ -954,3 +825,40 @@\n-  ThreadSafepointState* _safepoint_state;        \/\/ Holds information about a thread during a safepoint\n-  address               _saved_exception_pc;     \/\/ Saved pc of instruction where last implicit exception happened\n-  NOT_PRODUCT(bool      _requires_cross_modify_fence;) \/\/ State used by VerifyCrossModifyFence\n+  SafepointMechanism::ThreadData _poll_data;\n+  ThreadSafepointState*          _safepoint_state;              \/\/ Holds information about a thread during a safepoint\n+  address                        _saved_exception_pc;           \/\/ Saved pc of instruction where last implicit exception happened\n+  NOT_PRODUCT(bool               _requires_cross_modify_fence;) \/\/ State used by VerifyCrossModifyFence\n+#ifdef ASSERT\n+  \/\/ Debug support for checking if code allows safepoints or not.\n+  \/\/ Safepoints in the VM can happen because of allocation, invoking a VM operation, or blocking on\n+  \/\/ mutex, or blocking on an object synchronizer (Java locking).\n+  \/\/ If _no_safepoint_count is non-zero, then an assertion failure will happen in any of\n+  \/\/ the above cases. The class NoSafepointVerifier is used to set this counter.\n+  int _no_safepoint_count;                             \/\/ If 0, thread allow a safepoint to happen\n+\n+ public:\n+  void inc_no_safepoint_count() { _no_safepoint_count++; }\n+  void dec_no_safepoint_count() { _no_safepoint_count--; }\n+#endif \/\/ ASSERT\n+ public:\n+  \/\/ These functions check conditions before possibly going to a safepoint.\n+  \/\/ including NoSafepointVerifier.\n+  void check_for_valid_safepoint_state() NOT_DEBUG_RETURN;\n+  void check_possible_safepoint()        NOT_DEBUG_RETURN;\n+\n+#ifdef ASSERT\n+ private:\n+  volatile uint64_t _visited_for_critical_count;\n+\n+ public:\n+  void set_visited_for_critical_count(uint64_t safepoint_id) {\n+    assert(_visited_for_critical_count == 0, \"Must be reset before set\");\n+    assert((safepoint_id & 0x1) == 1, \"Must be odd\");\n+    _visited_for_critical_count = safepoint_id;\n+  }\n+  void reset_visited_for_critical_count(uint64_t safepoint_id) {\n+    assert(_visited_for_critical_count == safepoint_id, \"Was not visited\");\n+    _visited_for_critical_count = 0;\n+  }\n+  bool was_visited_for_critical_count(uint64_t safepoint_id) const {\n+    return _visited_for_critical_count == safepoint_id;\n+  }\n+#endif \/\/ ASSERT\n@@ -959,0 +867,1 @@\n+ public:\n@@ -967,0 +876,1 @@\n+ private:\n@@ -974,2 +884,1 @@\n-  \/\/ suspend\/resume support\n-  volatile bool         _suspend_equivalent;     \/\/ Suspend equivalent condition\n+\n@@ -1186,2 +1095,1 @@\n-  \/\/ special for Threads::remove() which is static:\n-  void set_terminated_value();\n+\n@@ -1213,3 +1121,3 @@\n- private:\n-  inline void set_ext_suspended();\n-  inline void clear_ext_suspended();\n+  bool java_suspend(); \/\/ higher-level suspension logic called by the public APIs\n+  bool java_resume();  \/\/ higher-level resume logic called by the public APIs\n+  bool is_suspended()     { return _handshake.is_suspended(); }\n@@ -1217,4 +1125,2 @@\n- public:\n-  void java_suspend(); \/\/ higher-level suspension logic called by the public APIs\n-  void java_resume();  \/\/ higher-level resume logic called by the public APIs\n-  int  java_suspend_self(); \/\/ low-level self-suspension mechanics\n+  \/\/ Check for async exception in addition to safepoint.\n+  static void check_special_condition_for_native_trans(JavaThread *thread);\n@@ -1226,79 +1132,0 @@\n- private:\n-  \/\/ mid-level wrapper around java_suspend_self to set up correct state and\n-  \/\/ check for a pending safepoint at the end\n-  void java_suspend_self_with_safepoint_check();\n-\n- public:\n-  void check_and_wait_while_suspended() {\n-    assert(JavaThread::current() == this, \"sanity check\");\n-\n-    bool do_self_suspend;\n-    do {\n-      \/\/ were we externally suspended while we were waiting?\n-      do_self_suspend = handle_special_suspend_equivalent_condition();\n-      if (do_self_suspend) {\n-        \/\/ don't surprise the thread that suspended us by returning\n-        java_suspend_self();\n-        set_suspend_equivalent();\n-      }\n-    } while (do_self_suspend);\n-  }\n-  static void check_safepoint_and_suspend_for_native_trans(JavaThread *thread);\n-  \/\/ Check for async exception in addition to safepoint and suspend request.\n-  static void check_special_condition_for_native_trans(JavaThread *thread);\n-\n-  bool is_ext_suspend_completed();\n-\n-  inline void set_external_suspend();\n-  inline void clear_external_suspend();\n-\n-  bool is_external_suspend() const {\n-    return (_suspend_flags & _external_suspend) != 0;\n-  }\n-  \/\/ Whenever a thread transitions from native to vm\/java it must suspend\n-  \/\/ if external|deopt suspend is present.\n-  bool is_suspend_after_native() const {\n-    return (_suspend_flags & (_external_suspend | _obj_deopt JFR_ONLY(| _trace_flag))) != 0;\n-  }\n-\n-  \/\/ external suspend request is completed\n-  bool is_ext_suspended() const {\n-    return (_suspend_flags & _ext_suspended) != 0;\n-  }\n-\n-  bool is_external_suspend_with_lock() const {\n-    MutexLocker ml(SR_lock(), Mutex::_no_safepoint_check_flag);\n-    return is_external_suspend();\n-  }\n-\n-  \/\/ Special method to handle a pending external suspend request\n-  \/\/ when a suspend equivalent condition lifts.\n-  bool handle_special_suspend_equivalent_condition() {\n-    assert(is_suspend_equivalent(),\n-           \"should only be called in a suspend equivalence condition\");\n-    MutexLocker ml(SR_lock(), Mutex::_no_safepoint_check_flag);\n-    bool ret = is_external_suspend();\n-    if (!ret) {\n-      \/\/ not about to self-suspend so clear suspend equivalence\n-      clear_suspend_equivalent();\n-    }\n-    \/\/ implied else:\n-    \/\/ We have a pending external suspend request so we leave the\n-    \/\/ suspend_equivalent flag set until java_suspend_self() sets\n-    \/\/ the ext_suspended flag and clears the suspend_equivalent\n-    \/\/ flag. This insures that wait_for_ext_suspend_completion()\n-    \/\/ will return consistent values.\n-    return ret;\n-  }\n-\n-  \/\/ utility methods to see if we are doing some kind of suspension\n-  bool is_being_ext_suspended() const            {\n-    MutexLocker ml(SR_lock(), Mutex::_no_safepoint_check_flag);\n-    return is_ext_suspended() || is_external_suspend();\n-  }\n-\n-  bool is_suspend_equivalent() const             { return _suspend_equivalent; }\n-\n-  void set_suspend_equivalent()                  { _suspend_equivalent = true; }\n-  void clear_suspend_equivalent()                { _suspend_equivalent = false; }\n-\n@@ -1324,10 +1151,1 @@\n-    \/\/ Because we don't use is_external_suspend_with_lock\n-    \/\/ it is possible that we won't see an asynchronous external suspend\n-    \/\/ request that has just gotten started, i.e., SR_lock grabbed but\n-    \/\/ _external_suspend field change either not made yet or not visible\n-    \/\/ yet. However, this is okay because the request is asynchronous and\n-    \/\/ we will see the new flag value the next time through. It's also\n-    \/\/ possible that the external suspend request is dropped after\n-    \/\/ we have checked is_external_suspend(), we will recheck its value\n-    \/\/ under SR_lock in java_suspend_self().\n-            is_external_suspend() || is_trace_suspend() || is_obj_deopt_suspend();\n+           (_suspend_flags & (_obj_deopt JFR_ONLY(| _trace_flag))) != 0;\n@@ -1446,0 +1264,2 @@\n+  static ByteSize polling_word_offset()          { return byte_offset_of(JavaThread, _poll_data) + byte_offset_of(SafepointMechanism::ThreadData, _polling_word);}\n+  static ByteSize polling_page_offset()          { return byte_offset_of(JavaThread, _poll_data) + byte_offset_of(SafepointMechanism::ThreadData, _polling_page);}\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":90,"deletions":270,"binary":false,"changes":360,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -231,1 +231,1 @@\n-    assert(method->is_native() || nm->is_aot(), \"Expect a native method or precompiled method\");\n+    assert(method->is_native(), \"Expect a native method\");\n","filename":"src\/hotspot\/share\/runtime\/vframe_hp.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/filemap.hpp\"\n@@ -31,1 +32,0 @@\n-#include \"classfile\/compactHashtable.hpp\"\n@@ -61,1 +61,0 @@\n-#include \"memory\/filemap.hpp\"\n@@ -303,1 +302,0 @@\n-  AOT_ONLY(nonstatic_field(MethodCounters,     _method,                                       Method*))                              \\\n@@ -739,3 +737,0 @@\n-  nonstatic_field(Thread,                      _current_pending_monitor,                      ObjectMonitor*)                        \\\n-  nonstatic_field(Thread,                      _current_pending_monitor_is_from_java,         bool)                                  \\\n-  nonstatic_field(Thread,                      _current_waiting_monitor,                      ObjectMonitor*)                        \\\n@@ -748,0 +743,3 @@\n+  nonstatic_field(JavaThread,                  _current_pending_monitor,                      ObjectMonitor*)                        \\\n+  nonstatic_field(JavaThread,                  _current_pending_monitor_is_from_java,         bool)                                  \\\n+  nonstatic_field(JavaThread,                  _current_waiting_monitor,                      ObjectMonitor*)                        \\\n@@ -2149,2 +2147,0 @@\n-  declare_constant(Thread::_external_suspend)                             \\\n-  declare_constant(Thread::_ext_suspended)                                \\\n@@ -2309,1 +2305,0 @@\n-  declare_constant(InstanceKlass::_misc_has_passed_fingerprint_check)     \\\n@@ -2502,1 +2497,0 @@\n-  declare_constant(CompLevel_aot)                                         \\\n@@ -2919,0 +2913,3 @@\n+#ifdef VM_INT_CPU_FEATURE_CONSTANTS\n+  VM_INT_CPU_FEATURE_CONSTANTS\n+#endif\n@@ -2952,0 +2949,3 @@\n+#ifdef VM_LONG_CPU_FEATURE_CONSTANTS\n+  VM_LONG_CPU_FEATURE_CONSTANTS\n+#endif\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-#include \"runtime\/os.inline.hpp\"\n+#include \"runtime\/os.hpp\"\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1808,1 +1808,1 @@\n-     * <p>The simple name of an {@linkplain isArray() array class} is the simple name of the\n+     * <p>The simple name of an {@linkplain #isArray() array class} is the simple name of the\n@@ -2538,0 +2538,13 @@\n+     * @apiNote\n+     * <p> The following method can be used to find the record canonical constructor:\n+     *\n+     * <pre>{@code\n+     * static <T extends Record> Constructor<T> getCanonicalConstructor(Class<T> cls)\n+     *     throws NoSuchMethodException {\n+     *   Class<?>[] paramTypes =\n+     *     Arrays.stream(cls.getRecordComponents())\n+     *           .map(RecordComponent::getType)\n+     *           .toArray(Class<?>[]::new);\n+     *   return cls.getDeclaredConstructor(paramTypes);\n+     * }}<\/pre>\n+     *\n@@ -3290,3 +3303,3 @@\n-        static <T> boolean casAnnotationType(Class<?> clazz,\n-                                             AnnotationType oldType,\n-                                             AnnotationType newType) {\n+        static boolean casAnnotationType(Class<?> clazz,\n+                                         AnnotationType oldType,\n+                                         AnnotationType newType) {\n@@ -3296,3 +3309,3 @@\n-        static <T> boolean casAnnotationData(Class<?> clazz,\n-                                             AnnotationData oldData,\n-                                             AnnotationData newData) {\n+        static boolean casAnnotationData(Class<?> clazz,\n+                                         AnnotationData oldData,\n+                                         AnnotationData newData) {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Class.java","additions":20,"deletions":7,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -62,1 +62,0 @@\n-import java.util.stream.Collectors;\n@@ -6688,2 +6687,1 @@\n-        final List<Class<?>> commonPrefix = iterationVariableTypes.stream().filter(t -> t != void.class).\n-                collect(Collectors.toList());\n+        final List<Class<?>> commonPrefix = iterationVariableTypes.stream().filter(t -> t != void.class).toList();\n@@ -6818,1 +6816,1 @@\n-        }).collect(Collectors.toList());\n+        }).toList();\n@@ -6822,1 +6820,1 @@\n-        return hs.stream().map(MethodHandle::asFixedArity).collect(Collectors.toList());\n+        return hs.stream().map(MethodHandle::asFixedArity).toList();\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandles.java","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -191,1 +191,1 @@\n-        if (!(task instanceof BasicJavacTask))\n+        if (!(task instanceof BasicJavacTask basicJavacTask))\n@@ -193,1 +193,1 @@\n-        return instance(((BasicJavacTask)task).getContext());\n+        return instance(basicJavacTask.getContext());\n@@ -198,1 +198,1 @@\n-        if (!(env instanceof JavacProcessingEnvironment))\n+        if (!(env instanceof JavacProcessingEnvironment javacProcessingEnvironment))\n@@ -200,1 +200,1 @@\n-        return instance(((JavacProcessingEnvironment)env).getContext());\n+        return instance(javacProcessingEnvironment.getContext());\n@@ -237,2 +237,2 @@\n-        if (t instanceof JavacTaskImpl)\n-            javacTaskImpl = (JavacTaskImpl) t;\n+        if (t instanceof JavacTaskImpl taskImpl)\n+            javacTaskImpl = taskImpl;\n@@ -267,2 +267,2 @@\n-                    if (tree instanceof DCEndPosTree) {\n-                        int endPos = ((DCEndPosTree) tree).getEndPos(dcComment);\n+                    if (tree instanceof DCEndPosTree<?> dcEndPosTree) {\n+                        int endPos = dcEndPosTree.getEndPos(dcComment);\n@@ -434,2 +434,2 @@\n-        if (tree instanceof DCReference)\n-            return attributeDocReference(path.getTreePath(), ((DCReference) tree));\n+        if (tree instanceof DCReference dcReference)\n+            return attributeDocReference(path.getTreePath(), dcReference);\n@@ -437,2 +437,2 @@\n-            if (path.getParentPath().getLeaf() instanceof DCParam) {\n-                return attributeParamIdentifier(path.getTreePath(), (DCParam) path.getParentPath().getLeaf());\n+            if (path.getParentPath().getLeaf() instanceof DCParam dcParam) {\n+                return attributeParamIdentifier(path.getTreePath(), dcParam);\n@@ -447,2 +447,2 @@\n-        if (tree instanceof DCReference) {\n-            JCTree qexpr = ((DCReference)tree).qualifierExpression;\n+        if (tree instanceof DCReference dcReference) {\n+            JCTree qexpr = dcReference.qualifierExpression;\n@@ -454,1 +454,1 @@\n-                    Type t = attr.attribType(((DCReference) tree).qualifierExpression, env);\n+                    Type t = attr.attribType(dcReference.qualifierExpression, env);\n@@ -552,2 +552,2 @@\n-                    while (e instanceof ArrayType)\n-                        e = ((ArrayType) e).elemtype;\n+                    while (e instanceof ArrayType arrayType)\n+                        e = arrayType.elemtype;\n@@ -811,4 +811,3 @@\n-        if (t instanceof JCTree.JCCompilationUnit && leaf instanceof JCTree) {\n-            JCCompilationUnit cu = (JCCompilationUnit) t;\n-            if (cu.docComments != null) {\n-                return cu.docComments.getCommentText((JCTree) leaf);\n+        if (t instanceof JCTree.JCCompilationUnit compilationUnit && leaf instanceof JCTree tree) {\n+            if (compilationUnit.docComments != null) {\n+                return compilationUnit.docComments.getCommentText(tree);\n@@ -824,4 +823,3 @@\n-        if (t instanceof JCTree.JCCompilationUnit && leaf instanceof JCTree) {\n-            JCCompilationUnit cu = (JCCompilationUnit) t;\n-            if (cu.docComments != null) {\n-                return cu.docComments.getCommentTree((JCTree) leaf);\n+        if (t instanceof JCTree.JCCompilationUnit compilationUnit && leaf instanceof JCTree tree) {\n+            if (compilationUnit.docComments != null) {\n+                return compilationUnit.docComments.getCommentTree(tree);\n@@ -856,5 +854,3 @@\n-        if (scope instanceof JavacScope && type instanceof ClassSymbol) {\n-            Env<AttrContext> env = ((JavacScope) scope).env;\n-            return resolve.isAccessible(env, (ClassSymbol)type, true);\n-        } else\n-            return false;\n+        return (scope instanceof JavacScope javacScope)\n+                && (type instanceof ClassSymbol classSymbol)\n+                && resolve.isAccessible(javacScope.env, classSymbol, true);\n@@ -865,7 +861,4 @@\n-        if (scope instanceof JavacScope\n-                && member instanceof Symbol\n-                && type instanceof com.sun.tools.javac.code.Type) {\n-            Env<AttrContext> env = ((JavacScope) scope).env;\n-            return resolve.isAccessible(env, (com.sun.tools.javac.code.Type)type, (Symbol)member, true);\n-        } else\n-            return false;\n+        return (scope instanceof JavacScope javacScope)\n+                && (member instanceof Symbol symbol)\n+                && (type instanceof com.sun.tools.javac.code.Type codeType)\n+                && resolve.isAccessible(javacScope.env, codeType, symbol, true);\n@@ -1074,2 +1067,1 @@\n-        if (fileObject instanceof JavaFileObject) {\n-            jfo = (JavaFileObject) fileObject;\n+        if (fileObject instanceof JavaFileObject javaFileObject) {\n@@ -1077,1 +1069,1 @@\n-            return jfo;\n+            return javaFileObject;\n@@ -1220,2 +1212,2 @@\n-        if (errorType instanceof com.sun.tools.javac.code.Type.ErrorType) {\n-            return ((com.sun.tools.javac.code.Type.ErrorType)errorType).getOriginalType();\n+        if (errorType instanceof com.sun.tools.javac.code.Type.ErrorType targetErrorType) {\n+            return targetErrorType.getOriginalType();\n@@ -1223,1 +1215,1 @@\n-        if (errorType instanceof com.sun.tools.javac.code.Type.ClassType &&\n+        if (errorType instanceof com.sun.tools.javac.code.Type.ClassType classType &&\n@@ -1225,4 +1217,3 @@\n-            ClassType ct = (ClassType) errorType;\n-            return extraType2OriginalMap.computeIfAbsent(ct, tt ->\n-                    new ClassType(ct.getEnclosingType(), ct.typarams_field,\n-                                  ct.tsym, ct.getMetadata(), ct.isReferenceProjection()) {\n+            return extraType2OriginalMap.computeIfAbsent(classType, tt ->\n+                    new ClassType(classType.getEnclosingType(), classType.typarams_field,\n+                                  classType.tsym, classType.getMetadata(), classType.isReferenceProjection()) {\n@@ -1230,1 +1221,1 @@\n-                        public Type baseType() { return ct; }\n+                        public Type baseType() { return classType; }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/api\/JavacTrees.java","additions":40,"deletions":49,"binary":false,"changes":89,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,0 +37,1 @@\n+import java.util.function.Predicate;\n@@ -1458,7 +1459,6 @@\n-            if (type instanceof ClassType) {\n-                ClassType t = (ClassType)type;\n-                if (t.interfaces_field == null) \/\/ FIXME: shouldn't be null\n-                    t.interfaces_field = List.nil();\n-                if (t.all_interfaces_field != null)\n-                    return Type.getModelTypes(t.all_interfaces_field);\n-                return t.interfaces_field;\n+            if (type instanceof ClassType classType) {\n+                if (classType.interfaces_field == null) \/\/ FIXME: shouldn't be null\n+                    classType.interfaces_field = List.nil();\n+                if (classType.all_interfaces_field != null)\n+                    return Type.getModelTypes(classType.all_interfaces_field);\n+                return classType.interfaces_field;\n@@ -1473,4 +1473,3 @@\n-            if (type instanceof ClassType) {\n-                ClassType t = (ClassType)type;\n-                if (t.supertype_field == null) \/\/ FIXME: shouldn't be null\n-                    t.supertype_field = Type.noType;\n+            if (type instanceof ClassType classType) {\n+                if (classType.supertype_field == null) \/\/ FIXME: shouldn't be null\n+                    classType.supertype_field = Type.noType;\n@@ -1478,1 +1477,1 @@\n-                return t.isInterface()\n+                return classType.isInterface()\n@@ -1480,1 +1479,1 @@\n-                    : t.supertype_field.getModelType();\n+                    : classType.supertype_field.getModelType();\n@@ -1623,9 +1622,8 @@\n-            if (type instanceof ClassType) {\n-                ClassType t = (ClassType)type;\n-                t.setEnclosingType(Type.noType);\n-                t.rank_field = -1;\n-                t.typarams_field = null;\n-                t.allparams_field = null;\n-                t.supertype_field = null;\n-                t.interfaces_field = null;\n-                t.all_interfaces_field = null;\n+            if (type instanceof ClassType classType) {\n+                classType.setEnclosingType(Type.noType);\n+                classType.rank_field = -1;\n+                classType.typarams_field = null;\n+                classType.allparams_field = null;\n+                classType.supertype_field = null;\n+                classType.interfaces_field = null;\n+                classType.all_interfaces_field = null;\n@@ -1792,1 +1790,1 @@\n-            } else if (data instanceof Callable<?>) {\n+            } else if (data instanceof Callable<?> callableData) {\n@@ -1795,1 +1793,0 @@\n-                Callable<?> eval = (Callable<?>)data;\n@@ -1798,1 +1795,1 @@\n-                    data = eval.call();\n+                    data = callableData.call();\n@@ -2201,1 +2198,1 @@\n-            public static final Filter<Symbol> implementation_filter = s ->\n+            public static final Predicate<Symbol> implementation_filter = s ->\n@@ -2204,1 +2201,1 @@\n-        public MethodSymbol implementation(TypeSymbol origin, Types types, boolean checkResult, Filter<Symbol> implFilter) {\n+        public MethodSymbol implementation(TypeSymbol origin, Types types, boolean checkResult, Predicate<Symbol> implFilter) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Symbol.java","additions":25,"deletions":28,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -691,0 +691,4 @@\n+        return lookupPackage(msym, flatName, false);\n+    }\n+\n+    private PackageSymbol lookupPackage(ModuleSymbol msym, Name flatName, boolean onlyExisting) {\n@@ -713,1 +717,1 @@\n-        if (pack != null && pack.exists())\n+        if ((pack != null && pack.exists()) || onlyExisting)\n@@ -785,1 +789,2 @@\n-        return lookupPackage(msym, fullname).exists();\n+        PackageSymbol pack = lookupPackage(msym, fullname, true);\n+        return pack != null && pack.exists();\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Symtab.java","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,1 @@\n+import java.util.function.Predicate;\n@@ -683,1 +684,1 @@\n-    public static List<Type> filter(List<Type> ts, Filter<Type> tf) {\n+    public static List<Type> filter(List<Type> ts, Predicate<Type> tf) {\n@@ -686,1 +687,1 @@\n-            if (tf.accepts(t)) {\n+            if (tf.test(t)) {\n@@ -1502,7 +1503,2 @@\n-            if (obj instanceof ArrayType) {\n-                ArrayType that = (ArrayType)obj;\n-                return this == that ||\n-                        elemtype.equals(that.elemtype);\n-            }\n-\n-            return false;\n+            return (obj instanceof ArrayType arrayType)\n+                    && (this == arrayType || elemtype.equals(arrayType.elemtype));\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Type.java","additions":6,"deletions":10,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2009, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2009, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -158,1 +158,1 @@\n-        if (!(atValue instanceof Attribute.Array)) {\n+        if (!(atValue instanceof Attribute.Array arrayVal)) {\n@@ -162,1 +162,1 @@\n-        List<Attribute> targets = ((Array)atValue).getValue();\n+        List<Attribute> targets = arrayVal.getValue();\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/TypeAnnotations.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,0 +38,1 @@\n+import java.util.function.Predicate;\n@@ -973,2 +974,4 @@\n-        private Filter<Symbol> bridgeFilter = new Filter<Symbol>() {\n-            public boolean accepts(Symbol t) {\n+        \/\/ Use anonymous class instead of lambda expression intentionally,\n+        \/\/ because the variable `names` has modifier: final.\n+        private Predicate<Symbol> bridgeFilter = new Predicate<Symbol>() {\n+            public boolean test(Symbol t) {\n@@ -981,0 +984,1 @@\n+\n@@ -1006,1 +1010,1 @@\n-    class DescriptorFilter implements Filter<Symbol> {\n+    class DescriptorFilter implements Predicate<Symbol> {\n@@ -1015,1 +1019,1 @@\n-       public boolean accepts(Symbol sym) {\n+       public boolean test(Symbol sym) {\n@@ -3088,1 +3092,1 @@\n-            final Filter<Symbol> implFilter;\n+            final Predicate<Symbol> implFilter;\n@@ -3093,1 +3097,1 @@\n-                    Filter<Symbol> scopeFilter,\n+                    Predicate<Symbol> scopeFilter,\n@@ -3102,1 +3106,1 @@\n-            boolean matches(Filter<Symbol> scopeFilter, boolean checkResult, int mark) {\n+            boolean matches(Predicate<Symbol> scopeFilter, boolean checkResult, int mark) {\n@@ -3109,1 +3113,1 @@\n-        MethodSymbol get(MethodSymbol ms, TypeSymbol origin, boolean checkResult, Filter<Symbol> implFilter) {\n+        MethodSymbol get(MethodSymbol ms, TypeSymbol origin, boolean checkResult, Predicate<Symbol> implFilter) {\n@@ -3129,1 +3133,1 @@\n-        private MethodSymbol implementationInternal(MethodSymbol ms, TypeSymbol origin, boolean checkResult, Filter<Symbol> implFilter) {\n+        private MethodSymbol implementationInternal(MethodSymbol ms, TypeSymbol origin, boolean checkResult, Predicate<Symbol> implFilter) {\n@@ -3154,1 +3158,1 @@\n-    public MethodSymbol implementation(MethodSymbol ms, TypeSymbol origin, boolean checkResult, Filter<Symbol> implFilter) {\n+    public MethodSymbol implementation(MethodSymbol ms, TypeSymbol origin, boolean checkResult, Predicate<Symbol> implFilter) {\n@@ -3175,2 +3179,2 @@\n-            Filter<Symbol> combine(Filter<Symbol> sf) {\n-                return s -> !s.owner.isInterface() && (sf == null || sf.accepts(s));\n+            Predicate<Symbol> combine(Predicate<Symbol> sf) {\n+                return s -> !s.owner.isInterface() && (sf == null || sf.test(s));\n@@ -3180,1 +3184,1 @@\n-            public Iterable<Symbol> getSymbols(Filter<Symbol> sf, LookupKind lookupKind) {\n+            public Iterable<Symbol> getSymbols(Predicate<Symbol> sf, LookupKind lookupKind) {\n@@ -3185,1 +3189,1 @@\n-            public Iterable<Symbol> getSymbolsByName(Name name, Filter<Symbol> sf, LookupKind lookupKind) {\n+            public Iterable<Symbol> getSymbolsByName(Name name, Predicate<Symbol> sf, LookupKind lookupKind) {\n@@ -3315,6 +3319,3 @@\n-                if (obj instanceof Entry) {\n-                    Entry e = (Entry)obj;\n-                    return e.msym == msym && isSameType(site, e.site);\n-                } else {\n-                    return false;\n-                }\n+                return (obj instanceof Entry entry)\n+                        && entry.msym == msym\n+                        && isSameType(site, entry.site);\n@@ -3345,1 +3346,1 @@\n-            Filter<Symbol> filter = new MethodFilter(ms, site);\n+            Predicate<Symbol> filter = new MethodFilter(ms, site);\n@@ -3378,1 +3379,1 @@\n-            private class MethodFilter implements Filter<Symbol> {\n+            private class MethodFilter implements Predicate<Symbol> {\n@@ -3388,1 +3389,2 @@\n-                public boolean accepts(Symbol s) {\n+                @Override\n+                public boolean test(Symbol s) {\n@@ -4003,5 +4005,3 @@\n-                if (!(obj instanceof TypePair))\n-                    return false;\n-                TypePair typePair = (TypePair)obj;\n-                return isSameType(t1, typePair.t1)\n-                    && isSameType(t2, typePair.t2);\n+                return (obj instanceof TypePair typePair)\n+                        && isSameType(t1, typePair.t1)\n+                        && isSameType(t2, typePair.t2);\n@@ -4476,0 +4476,2 @@\n+        if (t.hasTag(ERROR))\n+            return Type.noType;\n@@ -5037,2 +5039,2 @@\n-            return (obj instanceof UniqueType) &&\n-                types.isSameType(type, ((UniqueType)obj).type);\n+            return (obj instanceof UniqueType uniqueType) &&\n+                    types.isSameType(type, uniqueType.type);\n@@ -5196,2 +5198,2 @@\n-            if (value != null && value instanceof Attribute.Enum) {\n-                Name levelName = ((Attribute.Enum)value).value.name;\n+            if (value != null && value instanceof Attribute.Enum attributeEnum) {\n+                Name levelName = attributeEnum.value.name;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Types.java","additions":35,"deletions":33,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -453,2 +453,2 @@\n-            if (ae.getCause() instanceof BreakAttr) {\n-                return ((BreakAttr)(ae.getCause())).env;\n+            if (ae.getCause() instanceof BreakAttr breakAttr) {\n+                return breakAttr.env;\n@@ -1608,0 +1608,9 @@\n+\n+                    \/\/ Check the return type of the method iterator().\n+                    \/\/ This is the bare minimum we need to verify to make sure code generation doesn't crash.\n+                    Symbol iterSymbol = rs.resolveInternalMethod(tree.pos(),\n+                            loopEnv, exprType, names.iterator, List.nil(), List.nil());\n+                    if (types.asSuper(iterSymbol.type.getReturnType(), syms.iteratorType.tsym) == null) {\n+                        log.error(tree.pos(),\n+                                Errors.ForeachNotApplicableToType(exprType, Fragments.TypeReqArrayOrIterable));\n+                    }\n@@ -2040,1 +2049,1 @@\n-                return (!t.hasTag(TYPEVAR) && types.unboxedTypeOrType(t).isPrimitive());\n+                return (!t.hasTag(TYPEVAR) && !t.isErroneous() && types.unboxedTypeOrType(t).isPrimitive());\n@@ -5676,2 +5685,1 @@\n-                checkForDeclarationAnnotations(tree.recvparam.mods.annotations,\n-                        tree.recvparam.vartype.type.tsym);\n+                checkForDeclarationAnnotations(tree.recvparam.mods.annotations, tree.recvparam.sym);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import java.util.function.Predicate;\n@@ -318,1 +319,1 @@\n-        if (found instanceof Type && ((Type)found).hasTag(VOID)) {\n+        if (found instanceof Type type && type.hasTag(VOID)) {\n@@ -323,1 +324,1 @@\n-        return types.createErrorType(found instanceof Type ? (Type)found : syms.errType);\n+        return types.createErrorType(found instanceof Type type ? type : syms.errType);\n@@ -1496,2 +1497,1 @@\n-                    if (tree.init instanceof JCNewClass &&\n-                        ((JCNewClass) tree.init).def != null) {\n+                    if (tree.init instanceof JCNewClass newClass && newClass.def != null) {\n@@ -2310,1 +2310,1 @@\n-    private Filter<Symbol> equalsHasCodeFilter = s -> MethodSymbol.implementation_filter.accepts(s) &&\n+    private Predicate<Symbol> equalsHasCodeFilter = s -> MethodSymbol.implementation_filter.test(s) &&\n@@ -2389,2 +2389,2 @@\n-        return (cf.accepts(s1) &&\n-                cf.accepts(s2) &&\n+        return (cf.test(s1) &&\n+                cf.test(s2) &&\n@@ -2805,1 +2805,1 @@\n-     private class ClashFilter implements Filter<Symbol> {\n+     private class ClashFilter implements Predicate<Symbol> {\n@@ -2818,1 +2818,2 @@\n-         public boolean accepts(Symbol s) {\n+         @Override\n+         public boolean test(Symbol s) {\n@@ -2869,1 +2870,1 @@\n-     private class DefaultMethodClashFilter implements Filter<Symbol> {\n+     private class DefaultMethodClashFilter implements Predicate<Symbol> {\n@@ -2877,1 +2878,2 @@\n-         public boolean accepts(Symbol s) {\n+         @Override\n+         public boolean test(Symbol s) {\n@@ -3387,1 +3389,1 @@\n-                if (!(app instanceof Attribute.Enum)) {\n+                if (!(app instanceof Attribute.Enum attributeEnum)) {\n@@ -3390,2 +3392,1 @@\n-                Attribute.Enum e = (Attribute.Enum)app;\n-                containerTargets.add(e.value.name);\n+                containerTargets.add(attributeEnum.value.name);\n@@ -3402,1 +3403,1 @@\n-                if (!(app instanceof Attribute.Enum)) {\n+                if (!(app instanceof Attribute.Enum attributeEnum)) {\n@@ -3405,2 +3406,1 @@\n-                Attribute.Enum e = (Attribute.Enum)app;\n-                containedTargets.add(e.value.name);\n+                containedTargets.add(attributeEnum.value.name);\n@@ -3530,1 +3530,1 @@\n-                if (!(app instanceof Attribute.Enum)) {\n+                if (!(app instanceof Attribute.Enum attributeEnum)) {\n@@ -3533,2 +3533,1 @@\n-                Attribute.Enum e = (Attribute.Enum) app;\n-                targets[i] = e.value.name;\n+                targets[i] = attributeEnum.value.name;\n@@ -3562,1 +3561,1 @@\n-                if (!(app instanceof Attribute.Enum)) {\n+                if (!(app instanceof Attribute.Enum attributeEnum)) {\n@@ -3566,2 +3565,1 @@\n-                Attribute.Enum e = (Attribute.Enum) app;\n-                targets[i] = e.value.name;\n+                targets[i] = attributeEnum.value.name;\n@@ -3631,2 +3629,1 @@\n-        if (!(atValue instanceof Attribute.Array)) return null; \/\/ error recovery\n-        return (Attribute.Array) atValue;\n+        return (atValue instanceof Attribute.Array attributeArray) ? attributeArray : null;\n@@ -4024,1 +4021,1 @@\n-        Filter<Symbol> duplicates = candidate -> candidate != sym && !candidate.type.isErroneous();\n+        Predicate<Symbol> duplicates = candidate -> candidate != sym && !candidate.type.isErroneous();\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Check.java","additions":23,"deletions":26,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+import java.util.function.Predicate;\n@@ -705,1 +706,1 @@\n-                Filter<Symbol> enumConstantFilter =\n+                Predicate<Symbol> enumConstantFilter =\n@@ -753,5 +754,4 @@\n-                if (resource instanceof JCVariableDecl) {\n-                    JCVariableDecl vdecl = (JCVariableDecl) resource;\n-                    visitVarDef(vdecl);\n-                } else if (resource instanceof JCExpression) {\n-                    scan((JCExpression) resource);\n+                if (resource instanceof JCVariableDecl variableDecl) {\n+                    visitVarDef(variableDecl);\n+                } else if (resource instanceof JCExpression expression) {\n+                    scan(expression);\n@@ -947,2 +947,1 @@\n-                if (exit instanceof ThrownPendingExit) {\n-                    ThrownPendingExit thrownExit = (ThrownPendingExit) exit;\n+                if (exit instanceof ThrownPendingExit thrownExit) {\n@@ -1226,5 +1225,4 @@\n-                if (resource instanceof JCVariableDecl) {\n-                    JCVariableDecl vdecl = (JCVariableDecl) resource;\n-                    visitVarDef(vdecl);\n-                } else if (resource instanceof JCExpression) {\n-                    scan((JCExpression) resource);\n+                if (resource instanceof JCVariableDecl variableDecl) {\n+                    visitVarDef(variableDecl);\n+                } else if (resource instanceof JCExpression expression) {\n+                    scan(expression);\n@@ -2477,7 +2475,6 @@\n-                if (resource instanceof JCVariableDecl) {\n-                    JCVariableDecl vdecl = (JCVariableDecl) resource;\n-                    visitVarDef(vdecl);\n-                    unrefdResources.enter(vdecl.sym);\n-                    resourceVarDecls.append(vdecl);\n-                } else if (resource instanceof JCExpression) {\n-                    scanExpr((JCExpression) resource);\n+                if (resource instanceof JCVariableDecl variableDecl) {\n+                    visitVarDef(variableDecl);\n+                    unrefdResources.enter(variableDecl.sym);\n+                    resourceVarDecls.append(variableDecl);\n+                } else if (resource instanceof JCExpression expression) {\n+                    scanExpr(expression);\n@@ -2540,3 +2537,3 @@\n-                        if (exit instanceof AssignPendingExit) {\n-                            ((AssignPendingExit) exit).exit_inits.orSet(inits);\n-                            ((AssignPendingExit) exit).exit_uninits.andSet(uninits);\n+                        if (exit instanceof AssignPendingExit assignPendingExit) {\n+                            assignPendingExit.exit_inits.orSet(inits);\n+                            assignPendingExit.exit_uninits.andSet(uninits);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Flow.java","additions":20,"deletions":23,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2010, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2010, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -200,6 +200,3 @@\n-            if (!(o instanceof DedupedLambda)) {\n-                return false;\n-            }\n-            DedupedLambda that = (DedupedLambda) o;\n-            return types.isSameType(symbol.asType(), that.symbol.asType())\n-                    && new TreeDiffer(symbol.params(), that.symbol.params()).scan(tree, that.tree);\n+            return (o instanceof DedupedLambda dedupedLambda)\n+                    && types.isSameType(symbol.asType(), dedupedLambda.symbol.asType())\n+                    && new TreeDiffer(symbol.params(), dedupedLambda.symbol.params()).scan(tree, dedupedLambda.tree);\n@@ -1561,4 +1558,1 @@\n-            LambdaTranslationContext ltc = (context != null && context instanceof LambdaTranslationContext)?\n-                    (LambdaTranslationContext)context :\n-                    null;\n-            if (ltc != null) {\n+            if (context != null && context instanceof LambdaTranslationContext lambdaContext) {\n@@ -1566,1 +1560,1 @@\n-                    ltc.addSymbol(tree.sym, LOCAL_VAR);\n+                    lambdaContext.addSymbol(tree.sym, LOCAL_VAR);\n@@ -1770,4 +1764,1 @@\n-            LambdaTranslationContext lambdaContext =\n-                    context instanceof LambdaTranslationContext ?\n-                            (LambdaTranslationContext) context : null;\n-            return lambdaContext != null\n+            return (context instanceof LambdaTranslationContext lambdaContext)\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/LambdaToMethod.java","additions":7,"deletions":16,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1628,5 +1628,4 @@\n-        if (resource instanceof JCVariableDecl) {\n-            JCVariableDecl var = (JCVariableDecl) resource;\n-            resourceUse = make.Ident(var.sym).setType(resource.type);\n-            resourceNonNull = var.init != null && TreeInfo.skipParens(var.init).hasTag(NEWCLASS);\n-            stats.add(var);\n+        if (resource instanceof JCVariableDecl variableDecl) {\n+            resourceUse = make.Ident(variableDecl.sym).setType(resource.type);\n+            resourceNonNull = variableDecl.init != null && TreeInfo.skipParens(variableDecl.init).hasTag(NEWCLASS);\n+            stats.add(variableDecl);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Lower.java","additions":5,"deletions":6,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1624,1 +1624,1 @@\n-                    if (bestSoFar instanceof AccessError) {\n+                    if (bestSoFar instanceof AccessError accessError) {\n@@ -1628,2 +1628,2 @@\n-                        currentResolutionContext.addInapplicableCandidate(((AccessError) bestSoFar).sym,\n-                                ((AccessError) bestSoFar).getDiagnostic(JCDiagnostic.DiagnosticType.FRAGMENT, null, null, site, null, argtypes, typeargtypes));\n+                        currentResolutionContext.addInapplicableCandidate(accessError.sym,\n+                                accessError.getDiagnostic(JCDiagnostic.DiagnosticType.FRAGMENT, null, null, site, null, argtypes, typeargtypes));\n@@ -1656,1 +1656,1 @@\n-            } else if (bestSoFar.kind == HIDDEN && bestSoFar instanceof AccessError) {\n+            } else if (bestSoFar.kind == HIDDEN && bestSoFar instanceof AccessError accessError) {\n@@ -1659,2 +1659,2 @@\n-                currentResolutionContext.addInapplicableCandidate(((AccessError) bestSoFar).sym,\n-                        ((AccessError) bestSoFar).getDiagnostic(JCDiagnostic.DiagnosticType.FRAGMENT, null, null, site, null, argtypes, typeargtypes));\n+                currentResolutionContext.addInapplicableCandidate(accessError.sym,\n+                        accessError.getDiagnostic(JCDiagnostic.DiagnosticType.FRAGMENT, null, null, site, null, argtypes, typeargtypes));\n@@ -1823,1 +1823,1 @@\n-        class LookupFilter implements Filter<Symbol> {\n+        class LookupFilter implements Predicate<Symbol> {\n@@ -1831,1 +1831,2 @@\n-            public boolean accepts(Symbol s) {\n+            @Override\n+            public boolean test(Symbol s) {\n@@ -4109,1 +4110,1 @@\n-                if (location.kind == PCK && !site.tsym.exists()) {\n+                if (location.kind == PCK && !site.tsym.exists() && location.name != names.java) {\n@@ -4889,4 +4890,4 @@\n-                if (o instanceof Type) {\n-                    return ((Type)o).containsAny(ts);\n-                } else if (o instanceof JCDiagnostic) {\n-                    return containsAny((JCDiagnostic)o, ts);\n+                if (o instanceof Type type) {\n+                    return type.containsAny(ts);\n+                } else if (o instanceof JCDiagnostic diagnostic) {\n+                    return containsAny(diagnostic, ts);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Resolve.java","additions":14,"deletions":13,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -871,1 +871,1 @@\n-                    if (v instanceof Integer && !var.type.getTag().checkRange((Integer) v)) {\n+                    if (v instanceof Integer intVal && !var.type.getTag().checkRange(intVal)) {\n@@ -1477,3 +1477,2 @@\n-                        if (v.fst == names.value && v.snd instanceof Attribute.Constant) {\n-                            Attribute.Constant c = (Attribute.Constant)v.snd;\n-                            if (c.type == syms.intType && ((Integer)c.value) > profile.value) {\n+                        if (v.fst == names.value && v.snd instanceof Attribute.Constant constant) {\n+                            if (constant.type == syms.intType && ((Integer) constant.value) > profile.value) {\n@@ -1508,3 +1507,2 @@\n-                if (v.fst == attribute && v.snd instanceof Attribute.Constant) {\n-                    Attribute.Constant c = (Attribute.Constant)v.snd;\n-                    if (c.type == syms.booleanType && ((Integer)c.value) != 0) {\n+                if (v.fst == attribute && v.snd instanceof Attribute.Constant constant) {\n+                    if (constant.type == syms.booleanType && ((Integer) constant.value) != 0) {\n@@ -1563,3 +1561,0 @@\n-        \/\/ support preliminary jsr175-format class files\n-        if (poolReader.hasTag(i, CONSTANT_Class))\n-            return poolReader.getClass(i).type;\n@@ -2102,1 +2097,1 @@\n-            if (t instanceof ProxyType) {\n+            if (t instanceof ProxyType proxyType) {\n@@ -2107,1 +2102,1 @@\n-                    return ((ProxyType) t).resolve();\n+                    return proxyType.resolve();\n@@ -2176,3 +2171,2 @@\n-                        if (forRemoval instanceof Attribute.Constant) {\n-                            Attribute.Constant c = (Attribute.Constant) forRemoval;\n-                            if (c.type == syms.booleanType && ((Integer) c.value) != 0) {\n+                        if (forRemoval instanceof Attribute.Constant constant) {\n+                            if (constant.type == syms.booleanType && ((Integer) constant.value) != 0) {\n@@ -2901,6 +2895,2 @@\n-\n-            if (!(other instanceof SourceFileObject))\n-                return false;\n-\n-            SourceFileObject o = (SourceFileObject) other;\n-            return name.equals(o.name);\n+            return (other instanceof SourceFileObject sourceFileObject)\n+                    && name.equals(sourceFileObject.name);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassReader.java","additions":11,"deletions":21,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -551,2 +551,2 @@\n-            !(constValue instanceof String) ||\n-            ((String)constValue).length() < PoolWriter.MAX_STRING_LENGTH)\n+            !(constValue instanceof String str) ||\n+            str.length() < PoolWriter.MAX_STRING_LENGTH)\n@@ -845,2 +845,2 @@\n-            if (tree.sym.owner instanceof ClassSymbol) {\n-                poolWriter.putClass((ClassSymbol)tree.sym.owner);\n+            if (tree.sym.owner instanceof ClassSymbol classSymbol) {\n+                poolWriter.putClass(classSymbol);\n@@ -909,2 +909,2 @@\n-                sym instanceof DynamicVarSymbol &&\n-                ((DynamicVarSymbol)sym).isDynamic();\n+                sym instanceof DynamicVarSymbol dynamicVarSymbol &&\n+                dynamicVarSymbol.isDynamic();\n@@ -1722,3 +1722,2 @@\n-                if (alt instanceof JCAnnotatedType) {\n-                    JCAnnotatedType a = (JCAnnotatedType)alt;\n-                    res = res.prepend(new Pair<>(annotate.fromAnnotations(a.annotations), alt));\n+                if (alt instanceof JCAnnotatedType annotatedType) {\n+                    res = res.prepend(new Pair<>(annotate.fromAnnotations(annotatedType.annotations), alt));\n@@ -2088,1 +2087,1 @@\n-                l instanceof LocalItem &&\n+                l instanceof LocalItem localItem &&\n@@ -2094,1 +2093,1 @@\n-                ((LocalItem)l).incr(ival);\n+                localItem.incr(ival);\n@@ -2129,1 +2128,1 @@\n-                if (od instanceof LocalItem &&\n+                if (od instanceof LocalItem localItem &&\n@@ -2131,1 +2130,1 @@\n-                    ((LocalItem)od).incr(tree.hasTag(PREINC) ? 1 : -1);\n+                    localItem.incr(tree.hasTag(PREINC) ? 1 : -1);\n@@ -2147,1 +2146,1 @@\n-                if (od instanceof LocalItem &&\n+                if (od instanceof LocalItem localItem &&\n@@ -2150,1 +2149,1 @@\n-                    ((LocalItem)od).incr(tree.hasTag(POSTINC) ? 1 : -1);\n+                    localItem.incr(tree.hasTag(POSTINC) ? 1 : -1);\n@@ -2234,2 +2233,2 @@\n-                rhs.type.constValue() instanceof Number &&\n-                ((Number) rhs.type.constValue()).intValue() == 0) {\n+                    rhs.type.constValue() instanceof Number number &&\n+                    number.intValue() == 0) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/Gen.java","additions":16,"deletions":17,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -175,10 +175,10 @@\n-        if (o instanceof Integer) {\n-            return putConstant(LoadableConstant.Int((int)o));\n-        } else if (o instanceof Float) {\n-            return putConstant(LoadableConstant.Float((float)o));\n-        } else if (o instanceof Long) {\n-            return putConstant(LoadableConstant.Long((long)o));\n-        } else if (o instanceof Double) {\n-            return putConstant(LoadableConstant.Double((double)o));\n-        } else if (o instanceof String) {\n-            return putConstant(LoadableConstant.String((String)o));\n+        if (o instanceof Integer intVal) {\n+            return putConstant(LoadableConstant.Int(intVal));\n+        } else if (o instanceof Float floatVal) {\n+            return putConstant(LoadableConstant.Float(floatVal));\n+        } else if (o instanceof Long longVal) {\n+            return putConstant(LoadableConstant.Long(longVal));\n+        } else if (o instanceof Double doubleVal) {\n+            return putConstant(LoadableConstant.Double(doubleVal));\n+        } else if (o instanceof String strVal) {\n+            return putConstant(LoadableConstant.String(strVal));\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/PoolWriter.java","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -126,1 +126,1 @@\n-                .collect(Collectors.toList());\n+                .toList();\n@@ -332,1 +332,1 @@\n-        if (!(elem instanceof MethodSymbol))\n+        if (!(elem instanceof MethodSymbol methodSymbol))\n@@ -335,2 +335,1 @@\n-        MethodSymbol m = (MethodSymbol) elem;\n-        ClassSymbol origin = (ClassSymbol) m.owner;\n+        ClassSymbol origin = (ClassSymbol) methodSymbol.owner;\n@@ -342,2 +341,2 @@\n-                for (Symbol sym : c.members().getSymbolsByName(m.name)) {\n-                    if (sym.kind == MTH && m.overrides(sym, origin, types, true)) {\n+                for (Symbol sym : c.members().getSymbolsByName(methodSymbol.name)) {\n+                    if (sym.kind == MTH && methodSymbol.overrides(sym, origin, types, true)) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/model\/JavacTypes.java","additions":6,"deletions":7,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+import java.util.function.Predicate;\n@@ -286,1 +287,1 @@\n-    protected boolean peekToken(Filter<TokenKind> tk) {\n+    protected boolean peekToken(Predicate<TokenKind> tk) {\n@@ -290,2 +291,2 @@\n-    protected boolean peekToken(int lookahead, Filter<TokenKind> tk) {\n-        return tk.accepts(S.token(lookahead + 1).kind);\n+    protected boolean peekToken(int lookahead, Predicate<TokenKind> tk) {\n+        return tk.test(S.token(lookahead + 1).kind);\n@@ -294,1 +295,1 @@\n-    protected boolean peekToken(Filter<TokenKind> tk1, Filter<TokenKind> tk2) {\n+    protected boolean peekToken(Predicate<TokenKind> tk1, Predicate<TokenKind> tk2) {\n@@ -298,3 +299,3 @@\n-    protected boolean peekToken(int lookahead, Filter<TokenKind> tk1, Filter<TokenKind> tk2) {\n-        return tk1.accepts(S.token(lookahead + 1).kind) &&\n-                tk2.accepts(S.token(lookahead + 2).kind);\n+    protected boolean peekToken(int lookahead, Predicate<TokenKind> tk1, Predicate<TokenKind> tk2) {\n+        return tk1.test(S.token(lookahead + 1).kind) &&\n+                tk2.test(S.token(lookahead + 2).kind);\n@@ -303,1 +304,1 @@\n-    protected boolean peekToken(Filter<TokenKind> tk1, Filter<TokenKind> tk2, Filter<TokenKind> tk3) {\n+    protected boolean peekToken(Predicate<TokenKind> tk1, Predicate<TokenKind> tk2, Predicate<TokenKind> tk3) {\n@@ -307,4 +308,4 @@\n-    protected boolean peekToken(int lookahead, Filter<TokenKind> tk1, Filter<TokenKind> tk2, Filter<TokenKind> tk3) {\n-        return tk1.accepts(S.token(lookahead + 1).kind) &&\n-                tk2.accepts(S.token(lookahead + 2).kind) &&\n-                tk3.accepts(S.token(lookahead + 3).kind);\n+    protected boolean peekToken(int lookahead, Predicate<TokenKind> tk1, Predicate<TokenKind> tk2, Predicate<TokenKind> tk3) {\n+        return tk1.test(S.token(lookahead + 1).kind) &&\n+                tk2.test(S.token(lookahead + 2).kind) &&\n+                tk3.test(S.token(lookahead + 3).kind);\n@@ -313,5 +314,5 @@\n-    protected boolean peekToken(int lookahead, Filter<TokenKind> tk1, Filter<TokenKind> tk2, Filter<TokenKind> tk3, Filter<TokenKind> tk4) {\n-        return tk1.accepts(S.token(lookahead + 1).kind) &&\n-                tk2.accepts(S.token(lookahead + 2).kind) &&\n-                tk3.accepts(S.token(lookahead + 3).kind) &&\n-                tk4.accepts(S.token(lookahead + 4).kind);\n+    protected boolean peekToken(int lookahead, Predicate<TokenKind> tk1, Predicate<TokenKind> tk2, Predicate<TokenKind> tk3, Predicate<TokenKind> tk4) {\n+        return tk1.test(S.token(lookahead + 1).kind) &&\n+                tk2.test(S.token(lookahead + 2).kind) &&\n+                tk3.test(S.token(lookahead + 3).kind) &&\n+                tk4.test(S.token(lookahead + 4).kind);\n@@ -321,1 +322,1 @@\n-    protected boolean peekToken(Filter<TokenKind>... kinds) {\n+    protected boolean peekToken(Predicate<TokenKind>... kinds) {\n@@ -326,1 +327,1 @@\n-    protected boolean peekToken(int lookahead, Filter<TokenKind>... kinds) {\n+    protected boolean peekToken(int lookahead, Predicate<TokenKind>... kinds) {\n@@ -328,1 +329,1 @@\n-            if (!kinds[lookahead].accepts(S.token(lookahead + 1).kind)) {\n+            if (!kinds[lookahead].test(S.token(lookahead + 1).kind)) {\n@@ -1889,1 +1890,1 @@\n-    protected Filter<TokenKind> LAX_IDENTIFIER = t -> t == IDENTIFIER || t == UNDERSCORE || t == ASSERT || t == ENUM;\n+    protected Predicate<TokenKind> LAX_IDENTIFIER = t -> t == IDENTIFIER || t == UNDERSCORE || t == ASSERT || t == ENUM;\n@@ -2183,1 +2184,1 @@\n-        } else if (LAX_IDENTIFIER.accepts(token.kind)) {\n+        } else if (LAX_IDENTIFIER.test(token.kind)) {\n@@ -2266,1 +2267,1 @@\n-                if (LAX_IDENTIFIER.accepts(token.kind)) {\n+                if (LAX_IDENTIFIER.test(token.kind)) {\n@@ -2782,1 +2783,1 @@\n-            } else if ((lastmode & TYPE) != 0 && LAX_IDENTIFIER.accepts(token.kind)) {\n+            } else if ((lastmode & TYPE) != 0 && LAX_IDENTIFIER.test(token.kind)) {\n@@ -2935,1 +2936,1 @@\n-            Name label = LAX_IDENTIFIER.accepts(token.kind) ? ident() : null;\n+            Name label = LAX_IDENTIFIER.test(token.kind) ? ident() : null;\n@@ -2942,1 +2943,1 @@\n-            Name label = LAX_IDENTIFIER.accepts(token.kind) ? ident() : null;\n+            Name label = LAX_IDENTIFIER.test(token.kind) ? ident() : null;\n@@ -3134,1 +3135,1 @@\n-            if ((lastmode & TYPE) != 0 && LAX_IDENTIFIER.accepts(token.kind)) {\n+            if ((lastmode & TYPE) != 0 && LAX_IDENTIFIER.test(token.kind)) {\n@@ -3331,1 +3332,1 @@\n-        if (LAX_IDENTIFIER.accepts(token.kind)) {\n+        if (LAX_IDENTIFIER.test(token.kind)) {\n@@ -3568,1 +3569,1 @@\n-                LAX_IDENTIFIER.accepts(token.kind) ||\n+                LAX_IDENTIFIER.test(token.kind) ||\n@@ -3641,1 +3642,1 @@\n-        if ((lastmode & TYPE) != 0 && LAX_IDENTIFIER.accepts(token.kind)) {\n+        if ((lastmode & TYPE) != 0 && LAX_IDENTIFIER.test(token.kind)) {\n@@ -3722,2 +3723,2 @@\n-                if (def instanceof JCExpressionStatement)\n-                    def = ((JCExpressionStatement)def).expr;\n+                if (def instanceof JCExpressionStatement statement)\n+                    def = statement.expr;\n@@ -3903,1 +3904,1 @@\n-                if (LAX_IDENTIFIER.accepts(token.kind)) {\n+                if (LAX_IDENTIFIER.test(token.kind)) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":35,"deletions":34,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+import java.util.function.Predicate;\n@@ -93,1 +94,1 @@\n-    public enum TokenKind implements Formattable, Filter<TokenKind> {\n+    public enum TokenKind implements Formattable, Predicate<TokenKind> {\n@@ -270,1 +271,1 @@\n-        public boolean accepts(TokenKind that) {\n+        public boolean test(TokenKind that) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/Tokens.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -2247,1 +2247,1 @@\n-            return pattern instanceof JCPattern ? (JCPattern) pattern : null;\n+            return pattern instanceof JCPattern jcPattern ? jcPattern : null;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/JCTree.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -704,3 +704,3 @@\n-                    if (vartype instanceof JCAnnotatedType) {\n-                        tas = ((JCAnnotatedType)vartype).annotations;\n-                        vartype = ((JCAnnotatedType)vartype).underlyingType;\n+                    if (vartype instanceof JCAnnotatedType annotatedType) {\n+                        tas = annotatedType.annotations;\n+                        vartype = annotatedType.underlyingType;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/Pretty.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -134,2 +134,2 @@\n-                || (node instanceof JCExpressionStatement\n-                    && ((JCExpressionStatement)node).expr instanceof JCErroneous),\n+                || (node instanceof JCExpressionStatement expressionStatement\n+                    && expressionStatement.expr instanceof JCErroneous),\n@@ -916,2 +916,2 @@\n-        } else if (value instanceof Character) {\n-            int v = (int) (((Character) value).toString().charAt(0));\n+        } else if (value instanceof Character charVal) {\n+            int v = charVal.toString().charAt(0);\n@@ -929,2 +929,2 @@\n-        } else if (value instanceof Boolean) {\n-            int v = ((Boolean) value) ? 1 : 0;\n+        } else if (value instanceof Boolean boolVal) {\n+            int v = boolVal ? 1 : 0;\n@@ -951,2 +951,2 @@\n-            if (e instanceof UnresolvedClass) {\n-                result = ClassLiteral(((UnresolvedClass) e).classType).setType(syms.classType);\n+            if (e instanceof UnresolvedClass unresolvedClass) {\n+                result = ClassLiteral(unresolvedClass.classType).setType(syms.classType);\n@@ -958,2 +958,2 @@\n-            if (compound instanceof Attribute.TypeCompound) {\n-                result = visitTypeCompoundInternal((Attribute.TypeCompound) compound);\n+            if (compound instanceof Attribute.TypeCompound typeCompound) {\n+                result = visitTypeCompoundInternal(typeCompound);\n@@ -1026,1 +1026,1 @@\n-                Type(mtype.getReturnType()),\n+                m.name != names.init ? Type(mtype.getReturnType()) : null,\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeMaker.java","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -126,0 +126,1 @@\n+    public final Name java;\n@@ -312,0 +313,1 @@\n+        java = fromString(\"java\");\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/util\/Names.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2009, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2009, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -183,2 +183,2 @@\n-        if (arg instanceof Type) {\n-            preprocessType((Type)arg);\n+        if (arg instanceof Type type) {\n+            preprocessType(type);\n@@ -186,2 +186,2 @@\n-        else if (arg instanceof Symbol) {\n-            preprocessSymbol((Symbol)arg);\n+        else if (arg instanceof Symbol symbol) {\n+            preprocessSymbol(symbol);\n@@ -189,2 +189,2 @@\n-        else if (arg instanceof JCDiagnostic) {\n-            preprocessDiagnostic((JCDiagnostic)arg);\n+        else if (arg instanceof JCDiagnostic diagnostic) {\n+            preprocessDiagnostic(diagnostic);\n@@ -192,2 +192,2 @@\n-        else if (arg instanceof Iterable<?> && !(arg instanceof Path)) {\n-            for (Object o : (Iterable<?>)arg) {\n+        else if (arg instanceof Iterable<?> iterable && !(arg instanceof Path)) {\n+            for (Object o : iterable) {\n@@ -562,2 +562,2 @@\n-                while ((bound instanceof ErrorType))\n-                    bound = ((ErrorType)bound).getOriginalType();\n+                while ((bound instanceof ErrorType errorType))\n+                    bound = errorType.getOriginalType();\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/util\/RichDiagnosticFormatter.java","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -1040,4 +1040,0 @@\n-    JavaConstant readFieldValue(HotSpotResolvedJavaField field, boolean isVolatile) {\n-        return runtime().reflection.readFieldValue(this, field, isVolatile);\n-    }\n-\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotResolvedObjectTypeImpl.java","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -258,0 +258,2 @@\n+    final int jvmConstantDynamic = getConstant(\"JVM_CONSTANT_Dynamic\", Integer.class);\n+    final int jvmConstantDynamicInError = getConstant(\"JVM_CONSTANT_DynamicInError\", Integer.class);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -405,0 +405,4 @@\n+    public boolean isFinal(Element e) {\n+        return e.getModifiers().contains(Modifier.FINAL);\n+    }\n+\n@@ -2100,1 +2104,1 @@\n-                .collect(Collectors.toList());\n+                .toList();\n@@ -2107,1 +2111,1 @@\n-                .collect(Collectors.toList());\n+                .toList();\n@@ -2114,1 +2118,1 @@\n-                .collect(Collectors.toList());\n+                .toList();\n@@ -2604,1 +2608,1 @@\n-                .collect(Collectors.toList());\n+                .toList();\n@@ -2612,1 +2616,1 @@\n-                .collect(Collectors.toList());\n+                .toList();\n@@ -3030,1 +3034,1 @@\n-                usedInDeclaration.addAll(types2Classes(te.getRecordComponents().stream().map(c -> c.asType()).collect(Collectors.toList()))); \/\/TODO: annotations on record components???\n+                usedInDeclaration.addAll(types2Classes(te.getRecordComponents().stream().map(c -> c.asType()).toList())); \/\/TODO: annotations on record components???\n@@ -3040,1 +3044,1 @@\n-                usedInDeclaration.addAll(types2Classes(ee.getParameters().stream().map(p -> p.asType()).collect(Collectors.toList())));\n+                usedInDeclaration.addAll(types2Classes(ee.getParameters().stream().map(p -> p.asType()).toList()));\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/toolkit\/util\/Utils.java","additions":11,"deletions":7,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -140,0 +140,2 @@\n+compiler\/codecache\/jmx\/PoolsIndependenceTest.java 8264632 macosx-x64\n+\n@@ -174,0 +176,1 @@\n+serviceability\/dcmd\/gc\/RunFinalizationTest.java 8227120 linux-x64\n@@ -226,2 +229,0 @@\n-vmTestbase\/nsk\/jdb\/eval\/eval001\/eval001.java 8221503 generic-all\n-\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -36,2 +36,1 @@\n-  -:tier1_compiler_not_xcomp \\\n-  -compiler\/graalunit\n+  -:tier1_compiler_not_xcomp\n@@ -41,2 +40,1 @@\n-  -:tier1_compiler_aot_jvmci \\\n-  -compiler\/graalunit\n+  -compiler\/jvmci\n@@ -98,2 +96,1 @@\n-  -:hotspot_slow_compiler \\\n-  -compiler\/graalunit\n+  -:hotspot_slow_compiler\n@@ -162,1 +159,0 @@\n-  compiler\/aot \\\n@@ -165,4 +161,0 @@\n-tier1_compiler_aot_jvmci = \\\n-  compiler\/aot \\\n-  compiler\/jvmci\n-\n@@ -176,3 +168,0 @@\n-tier1_compiler_graal = \\\n-  compiler\/graalunit\/HotspotTest.java\n-\n@@ -193,1 +182,0 @@\n-  applications\/ctw\/modules\/jdk_internal_vm_compiler.java \\\n@@ -346,1 +334,2 @@\n- -runtime\/cds\/appcds\/jcmd\/JCmdTest.java \\\n+ -runtime\/cds\/appcds\/jcmd\/JCmdTestStaticDump.java \\\n+ -runtime\/cds\/appcds\/jcmd\/JCmdTestDynamicDump.java \\\n@@ -380,0 +369,17 @@\n+hotspot_cds_verify_shared_spaces = \\\n+  runtime\/cds\/appcds\/ArchiveRelocationTest.java \\\n+  runtime\/cds\/appcds\/BootClassPathMismatch.java \\\n+  runtime\/cds\/appcds\/HelloTest.java \\\n+  runtime\/cds\/appcds\/VerifierTest_0.java \\\n+  runtime\/cds\/appcds\/dynamicArchive\/BasicLambdaTest.java \\\n+  runtime\/cds\/appcds\/dynamicArchive\/DynamicArchiveRelocationTest.java \\\n+  runtime\/cds\/appcds\/dynamicArchive\/HelloDynamic.java \\\n+  runtime\/cds\/appcds\/dynamicArchive\/LinkClassTest.java \\\n+  runtime\/cds\/appcds\/dynamicArchive\/MismatchedBaseArchive.java \\\n+  runtime\/cds\/appcds\/customLoader\/HelloCustom.java \\\n+  runtime\/cds\/appcds\/customLoader\/LoaderSegregationTest.java \\\n+  runtime\/cds\/appcds\/javaldr\/ArrayTest.java \\\n+  runtime\/cds\/appcds\/jigsaw\/modulepath\/ExportModule.java \\\n+  runtime\/cds\/appcds\/jvmti\/dumpingWithAgent\/DumpingWithJavaAgent.java \\\n+  runtime\/cds\/appcds\/sharedStrings\/SharedStringsBasic.java\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":22,"deletions":16,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -77,4 +77,0 @@\n-    :jdk_beans \\\n-    :jdk_imageio \\\n-    :jdk_sound \\\n-    :jdk_client_sanity \\\n","filename":"test\/jdk\/TEST.groups","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -289,5 +289,1 @@\n-            Set<String> mods = Set.of(\"javafx.deploy\", \"jdk.deploy\", \"jdk.plugin\", \"jdk.javaws\",\n-                    \/\/ All JVMCI packages other than jdk.vm.ci.services are dynamically\n-                    \/\/ exported to jdk.internal.vm.compiler and jdk.aot\n-                    \"jdk.internal.vm.compiler\", \"jdk.aot\"\n-            );\n+            Set<String> mods = Set.of(\"javafx.deploy\", \"jdk.deploy\", \"jdk.plugin\", \"jdk.javaws\");\n","filename":"test\/jdk\/java\/lang\/Class\/getDeclaredField\/FieldSetAccessibleTest.java","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -110,2 +110,0 @@\n-        map.put(\"vm.aot\", this::vmAOT);\n-        map.put(\"vm.aot.enabled\", this::vmAotEnabled);\n@@ -363,43 +361,0 @@\n-    \/**\n-     * @return true if VM supports AOT and false otherwise\n-     *\/\n-    protected String vmAOT() {\n-        if (WB.getBooleanVMFlag(\"EnableValhalla\").booleanValue()) {\n-            return \"false\";\n-        }\n-        \/\/ builds with aot have jaotc in <JDK>\/bin\n-        Path bin = Paths.get(System.getProperty(\"java.home\"))\n-                        .resolve(\"bin\");\n-        Path jaotc;\n-        if (Platform.isWindows()) {\n-            jaotc = bin.resolve(\"jaotc.exe\");\n-        } else {\n-            jaotc = bin.resolve(\"jaotc\");\n-        }\n-\n-        if (!Files.exists(jaotc)) {\n-            \/\/ No jaotc => no AOT\n-            return \"false\";\n-        }\n-\n-        switch (GC.selected()) {\n-            case Serial:\n-            case Parallel:\n-            case G1:\n-                \/\/ These GCs are supported with AOT\n-                return \"true\";\n-            default:\n-                break;\n-        }\n-\n-        \/\/ Every other GC is not supported\n-        return \"false\";\n-    }\n-\n-    \/*\n-     * @return true if there is at least one loaded AOT'ed library.\n-     *\/\n-    protected String vmAotEnabled() {\n-        return \"\" + (WB.aotLibrariesCount() > 0);\n-    }\n-\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":0,"deletions":45,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -267,1 +267,1 @@\n-    return isMethodCompilable(method, -2 \/*any*\/);\n+    return isMethodCompilable(method, -1 \/*any*\/);\n@@ -315,1 +315,1 @@\n-    makeMethodNotCompilable(method, -2 \/*any*\/);\n+    makeMethodNotCompilable(method, -1 \/*any*\/);\n@@ -339,1 +339,1 @@\n-    return getCompileQueueSize(-2 \/*any*\/);\n+    return getCompileQueueSize(-1 \/*any*\/);\n@@ -642,3 +642,0 @@\n-  \/\/ Number of loaded AOT libraries\n-  public native int aotLibrariesCount();\n-\n","filename":"test\/lib\/sun\/hotspot\/WhiteBox.java","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"}]}