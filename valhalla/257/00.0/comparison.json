{"files":[{"patch":"@@ -1756,1 +1756,3 @@\n-  \/\/   far_call(addr)\n+  \/\/   bl(addr)\n+  \/\/ or with far branches\n+  \/\/   bl(trampoline_stub)\n@@ -1765,1 +1767,1 @@\n-    return MacroAssembler::far_branch_size();\n+    return 1 * NativeInstruction::instruction_size;\n@@ -18960,0 +18962,210 @@\n+instruct vsraa8B_imm(vecD dst, vecD src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 8);\n+  match(Set dst (AddVB dst (RShiftVB src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ssra    $dst, $src, $shift\\t# vector (8B)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 8) sh = 7;\n+    __ ssra(as_FloatRegister($dst$$reg), __ T8B,\n+           as_FloatRegister($src$$reg), sh);\n+  %}\n+  ins_pipe(vshift64_imm);\n+%}\n+\n+instruct vsraa16B_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 16);\n+  match(Set dst (AddVB dst (RShiftVB src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ssra    $dst, $src, $shift\\t# vector (16B)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 8) sh = 7;\n+    __ ssra(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src$$reg), sh);\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n+instruct vsraa4S_imm(vecD dst, vecD src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 4);\n+  match(Set dst (AddVS dst (RShiftVS src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ssra    $dst, $src, $shift\\t# vector (4H)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 16) sh = 15;\n+    __ ssra(as_FloatRegister($dst$$reg), __ T4H,\n+           as_FloatRegister($src$$reg), sh);\n+  %}\n+  ins_pipe(vshift64_imm);\n+%}\n+\n+instruct vsraa8S_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 8);\n+  match(Set dst (AddVS dst (RShiftVS src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ssra    $dst, $src, $shift\\t# vector (8H)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 16) sh = 15;\n+    __ ssra(as_FloatRegister($dst$$reg), __ T8H,\n+           as_FloatRegister($src$$reg), sh);\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n+instruct vsraa2I_imm(vecD dst, vecD src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 2);\n+  match(Set dst (AddVI dst (RShiftVI src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ssra    $dst, $src, $shift\\t# vector (2S)\" %}\n+  ins_encode %{\n+    __ ssra(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src$$reg),\n+            (int)$shift$$constant);\n+  %}\n+  ins_pipe(vshift64_imm);\n+%}\n+\n+instruct vsraa4I_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 4);\n+  match(Set dst (AddVI dst (RShiftVI src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ssra    $dst, $src, $shift\\t# vector (4S)\" %}\n+  ins_encode %{\n+    __ ssra(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src$$reg),\n+            (int)$shift$$constant);\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n+instruct vsraa2L_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 2);\n+  match(Set dst (AddVL dst (RShiftVL src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ssra    $dst, $src, $shift\\t# vector (2D)\" %}\n+  ins_encode %{\n+    __ ssra(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src$$reg),\n+            (int)$shift$$constant);\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n+instruct vsrla8B_imm(vecD dst, vecD src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 8);\n+  match(Set dst (AddVB dst (URShiftVB src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"usra    $dst, $src, $shift\\t# vector (8B)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 8) {\n+      __ eor(as_FloatRegister($src$$reg), __ T8B,\n+             as_FloatRegister($src$$reg),\n+             as_FloatRegister($src$$reg));\n+    } else {\n+      __ usra(as_FloatRegister($dst$$reg), __ T8B,\n+             as_FloatRegister($src$$reg), sh);\n+    }\n+  %}\n+  ins_pipe(vshift64_imm);\n+%}\n+\n+instruct vsrla16B_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 16);\n+  match(Set dst (AddVB dst (URShiftVB src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"usra    $dst, $src, $shift\\t# vector (16B)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 8) {\n+      __ eor(as_FloatRegister($src$$reg), __ T16B,\n+             as_FloatRegister($src$$reg),\n+             as_FloatRegister($src$$reg));\n+    } else {\n+      __ usra(as_FloatRegister($dst$$reg), __ T16B,\n+             as_FloatRegister($src$$reg), sh);\n+    }\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n+instruct vsrla4S_imm(vecD dst, vecD src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 4);\n+  match(Set dst (AddVS dst (URShiftVS src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"usra    $dst, $src, $shift\\t# vector (4H)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 16) {\n+      __ eor(as_FloatRegister($src$$reg), __ T8B,\n+             as_FloatRegister($src$$reg),\n+             as_FloatRegister($src$$reg));\n+    } else {\n+      __ ushr(as_FloatRegister($dst$$reg), __ T4H,\n+             as_FloatRegister($src$$reg), sh);\n+    }\n+  %}\n+  ins_pipe(vshift64_imm);\n+%}\n+\n+instruct vsrla8S_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 8);\n+  match(Set dst (AddVS dst (URShiftVS src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"usra    $dst, $src, $shift\\t# vector (8H)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 16) {\n+      __ eor(as_FloatRegister($src$$reg), __ T16B,\n+             as_FloatRegister($src$$reg),\n+             as_FloatRegister($src$$reg));\n+    } else {\n+      __ usra(as_FloatRegister($dst$$reg), __ T8H,\n+             as_FloatRegister($src$$reg), sh);\n+    }\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n+instruct vsrla2I_imm(vecD dst, vecD src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 2);\n+  match(Set dst (AddVI dst (URShiftVI src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"usra    $dst, $src, $shift\\t# vector (2S)\" %}\n+  ins_encode %{\n+    __ usra(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src$$reg),\n+            (int)$shift$$constant);\n+  %}\n+  ins_pipe(vshift64_imm);\n+%}\n+\n+instruct vsrla4I_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 4);\n+  match(Set dst (AddVI dst (URShiftVI src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"usra    $dst, $src, $shift\\t# vector (4S)\" %}\n+  ins_encode %{\n+    __ usra(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src$$reg),\n+            (int)$shift$$constant);\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n+instruct vsrla2L_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 2);\n+  match(Set dst (AddVL dst (URShiftVL src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"usra    $dst, $src, $shift\\t# vector (2D)\" %}\n+  ins_encode %{\n+    __ usra(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src$$reg),\n+            (int)$shift$$constant);\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":214,"deletions":2,"binary":false,"changes":216,"status":"modified"},{"patch":"@@ -938,17 +938,0 @@\n-\n-RegisterOrConstant MacroAssembler::delayed_value_impl(intptr_t* delayed_value_addr,\n-                                                      Register tmp,\n-                                                      int offset) {\n-  intptr_t value = *delayed_value_addr;\n-  if (value != 0)\n-    return RegisterOrConstant(value + offset);\n-\n-  \/\/ load indirectly to solve generation ordering problem\n-  ldr(tmp, ExternalAddress((address) delayed_value_addr));\n-\n-  if (offset != 0)\n-    add(tmp, tmp, offset);\n-\n-  return RegisterOrConstant(tmp);\n-}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":0,"deletions":17,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -1034,4 +1034,0 @@\n-  virtual RegisterOrConstant delayed_value_impl(intptr_t* delayed_value_addr,\n-                                                Register tmp,\n-                                                int offset);\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -5421,0 +5421,144 @@\n+  void generate_base64_encode_simdround(Register src, Register dst,\n+        FloatRegister codec, u8 size) {\n+\n+    FloatRegister in0  = v4,  in1  = v5,  in2  = v6;\n+    FloatRegister out0 = v16, out1 = v17, out2 = v18, out3 = v19;\n+    FloatRegister ind0 = v20, ind1 = v21, ind2 = v22, ind3 = v23;\n+\n+    Assembler::SIMD_Arrangement arrangement = size == 16 ? __ T16B : __ T8B;\n+\n+    __ ld3(in0, in1, in2, arrangement, __ post(src, 3 * size));\n+\n+    __ ushr(ind0, arrangement, in0,  2);\n+\n+    __ ushr(ind1, arrangement, in1,  2);\n+    __ shl(in0,   arrangement, in0,  6);\n+    __ orr(ind1,  arrangement, ind1, in0);\n+    __ ushr(ind1, arrangement, ind1, 2);\n+\n+    __ ushr(ind2, arrangement, in2,  4);\n+    __ shl(in1,   arrangement, in1,  4);\n+    __ orr(ind2,  arrangement, in1,  ind2);\n+    __ ushr(ind2, arrangement, ind2, 2);\n+\n+    __ shl(ind3,  arrangement, in2,  2);\n+    __ ushr(ind3, arrangement, ind3, 2);\n+\n+    __ tbl(out0,  arrangement, codec,  4, ind0);\n+    __ tbl(out1,  arrangement, codec,  4, ind1);\n+    __ tbl(out2,  arrangement, codec,  4, ind2);\n+    __ tbl(out3,  arrangement, codec,  4, ind3);\n+\n+    __ st4(out0,  out1, out2, out3, arrangement, __ post(dst, 4 * size));\n+  }\n+\n+   \/**\n+   *  Arguments:\n+   *\n+   *  Input:\n+   *  c_rarg0   - src_start\n+   *  c_rarg1   - src_offset\n+   *  c_rarg2   - src_length\n+   *  c_rarg3   - dest_start\n+   *  c_rarg4   - dest_offset\n+   *  c_rarg5   - isURL\n+   *\n+   *\/\n+  address generate_base64_encodeBlock() {\n+\n+    static const char toBase64[64] = {\n+      'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n+      'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n+      'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n+      'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n+      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '\/'\n+    };\n+\n+    static const char toBase64URL[64] = {\n+      'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n+      'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n+      'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n+      'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n+      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-', '_'\n+    };\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"encodeBlock\");\n+    address start = __ pc();\n+\n+    Register src   = c_rarg0;  \/\/ source array\n+    Register soff  = c_rarg1;  \/\/ source start offset\n+    Register send  = c_rarg2;  \/\/ source end offset\n+    Register dst   = c_rarg3;  \/\/ dest array\n+    Register doff  = c_rarg4;  \/\/ position for writing to dest array\n+    Register isURL = c_rarg5;  \/\/ Base64 or URL chracter set\n+\n+    \/\/ c_rarg6 and c_rarg7 are free to use as temps\n+    Register codec  = c_rarg6;\n+    Register length = c_rarg7;\n+\n+    Label ProcessData, Process48B, Process24B, Process3B, SIMDExit, Exit;\n+\n+    __ add(src, src, soff);\n+    __ add(dst, dst, doff);\n+    __ sub(length, send, soff);\n+\n+    \/\/ load the codec base address\n+    __ lea(codec, ExternalAddress((address) toBase64));\n+    __ cbz(isURL, ProcessData);\n+    __ lea(codec, ExternalAddress((address) toBase64URL));\n+\n+    __ BIND(ProcessData);\n+\n+    \/\/ too short to formup a SIMD loop, roll back\n+    __ cmp(length, (u1)24);\n+    __ br(Assembler::LT, Process3B);\n+\n+    __ ld1(v0, v1, v2, v3, __ T16B, Address(codec));\n+\n+    __ BIND(Process48B);\n+    __ cmp(length, (u1)48);\n+    __ br(Assembler::LT, Process24B);\n+    generate_base64_encode_simdround(src, dst, v0, 16);\n+    __ sub(length, length, 48);\n+    __ b(Process48B);\n+\n+    __ BIND(Process24B);\n+    __ cmp(length, (u1)24);\n+    __ br(Assembler::LT, SIMDExit);\n+    generate_base64_encode_simdround(src, dst, v0, 8);\n+    __ sub(length, length, 24);\n+\n+    __ BIND(SIMDExit);\n+    __ cbz(length, Exit);\n+\n+    __ BIND(Process3B);\n+    \/\/  3 src bytes, 24 bits\n+    __ ldrb(r10, __ post(src, 1));\n+    __ ldrb(r11, __ post(src, 1));\n+    __ ldrb(r12, __ post(src, 1));\n+    __ orrw(r11, r11, r10, Assembler::LSL, 8);\n+    __ orrw(r12, r12, r11, Assembler::LSL, 8);\n+    \/\/ codec index\n+    __ ubfmw(r15, r12, 18, 23);\n+    __ ubfmw(r14, r12, 12, 17);\n+    __ ubfmw(r13, r12, 6,  11);\n+    __ andw(r12,  r12, 63);\n+    \/\/ get the code based on the codec\n+    __ ldrb(r15, Address(codec, r15, Address::uxtw(0)));\n+    __ ldrb(r14, Address(codec, r14, Address::uxtw(0)));\n+    __ ldrb(r13, Address(codec, r13, Address::uxtw(0)));\n+    __ ldrb(r12, Address(codec, r12, Address::uxtw(0)));\n+    __ strb(r15, __ post(dst, 1));\n+    __ strb(r14, __ post(dst, 1));\n+    __ strb(r13, __ post(dst, 1));\n+    __ strb(r12, __ post(dst, 1));\n+    __ sub(length, length, 3);\n+    __ cbnz(length, Process3B);\n+\n+    __ BIND(Exit);\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -6682,0 +6826,4 @@\n+    if (UseBASE64Intrinsics) {\n+        StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":148,"deletions":0,"binary":false,"changes":148,"status":"modified"},{"patch":"@@ -36,2 +36,0 @@\n-define_pd_global(bool, UseTLAB,                        true );\n-define_pd_global(bool, ResizeTLAB,                     true );\n","filename":"src\/hotspot\/cpu\/x86\/c1_globals_x86.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -886,0 +886,1 @@\n+  assert(tmp == xnoreg || elem_bt == T_LONG, \"unused\");\n@@ -897,0 +898,1 @@\n+      assert_different_registers(dst, src, tmp);\n@@ -911,0 +913,1 @@\n+      assert_different_registers(dst, src, tmp);\n@@ -935,0 +938,1 @@\n+        assert_different_registers(dst, src1, src2);\n@@ -951,0 +955,1 @@\n+        assert_different_registers(dst, src1, src2);\n@@ -968,0 +973,1 @@\n+  assert_different_registers(a, b, tmp, atmp, btmp);\n@@ -1008,0 +1014,1 @@\n+  assert_different_registers(dst, a, b, atmp, btmp);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -4265,38 +4265,0 @@\n-RegisterOrConstant MacroAssembler::delayed_value_impl(intptr_t* delayed_value_addr,\n-                                                      Register tmp,\n-                                                      int offset) {\n-  intptr_t value = *delayed_value_addr;\n-  if (value != 0)\n-    return RegisterOrConstant(value + offset);\n-\n-  \/\/ load indirectly to solve generation ordering problem\n-  movptr(tmp, ExternalAddress((address) delayed_value_addr));\n-\n-#ifdef ASSERT\n-  { Label L;\n-    testptr(tmp, tmp);\n-    if (WizardMode) {\n-      const char* buf = NULL;\n-      {\n-        ResourceMark rm;\n-        stringStream ss;\n-        ss.print(\"DelayedValue=\" INTPTR_FORMAT, delayed_value_addr[1]);\n-        buf = code_string(ss.as_string());\n-      }\n-      jcc(Assembler::notZero, L);\n-      STOP(buf);\n-    } else {\n-      jccb(Assembler::notZero, L);\n-      hlt();\n-    }\n-    bind(L);\n-  }\n-#endif\n-\n-  if (offset != 0)\n-    addptr(tmp, offset);\n-\n-  return RegisterOrConstant(tmp);\n-}\n-\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":0,"deletions":38,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -711,4 +711,0 @@\n-  virtual RegisterOrConstant delayed_value_impl(intptr_t* delayed_value_addr,\n-                                                Register tmp,\n-                                                int offset);\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1616,1 +1616,1 @@\n-  FieldAllocationType update(bool is_static, BasicType type, bool is_inline_type) {\n+  void update(bool is_static, BasicType type, bool is_inline_type) {\n@@ -1623,1 +1623,0 @@\n-    return atype;\n@@ -1772,3 +1771,2 @@\n-    \/\/ Remember how many oops we encountered and compute allocation type\n-    const FieldAllocationType atype = fac->update(is_static, type, type == T_INLINE_TYPE);\n-    field->set_allocation_type(atype);\n+    \/\/ Update FieldAllocationCount for this kind of field\n+    fac->update(is_static, type, type == T_INLINE_TYPE);\n@@ -1817,3 +1815,2 @@\n-      \/\/ Remember how many oops we encountered and compute allocation type\n-      const FieldAllocationType atype = fac->update(false, type, false);\n-      field->set_allocation_type(atype);\n+      \/\/ Update FieldAllocationCount for this kind of field\n+      fac->update(false, type, false);\n@@ -1831,2 +1828,1 @@\n-    const FieldAllocationType atype = fac->update(true, type, false);\n-    field->set_allocation_type(atype);\n+    fac->update(true, type, false);\n@@ -1844,2 +1840,1 @@\n-    const FieldAllocationType atype = fac->update(false, type, false);\n-    field->set_allocation_type(atype);\n+    fac->update(false, type, false);\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":7,"deletions":12,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -490,1 +490,0 @@\n-\n@@ -511,2 +510,2 @@\n-           tty->print_cr(\"is_supported_invokedynamic check failed for cp_index %d\", pool_index);\n-           continue;\n+          log_debug(cds, lambda)(\"is_supported_invokedynamic check failed for cp_index %d\", pool_index);\n+          continue;\n","filename":"src\/hotspot\/share\/classfile\/classListParser.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -493,4 +493,2 @@\n-        \/\/ In this case, we have to install the mark word first,\n-        \/\/ otherwise obj looks to be forwarded (the old mark word,\n-        \/\/ which contains the forward pointer, was copied)\n-        obj->set_mark(old_mark);\n+        \/\/ In this case, we have to install the old mark word containing the\n+        \/\/ displacement tag, and update the age in the displaced mark word.\n@@ -499,0 +497,1 @@\n+        obj->set_mark(old_mark);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -514,0 +514,3 @@\n+  \/\/ Is the given object inside a CDS archive area?\n+  virtual bool is_archived_object(oop object) const;\n+\n@@ -518,1 +521,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -647,1 +647,2 @@\n-    load_store = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, load_store, ShenandoahBarrierSet::AccessKind::NORMAL));\n+    ShenandoahBarrierSet::AccessKind kind = ShenandoahBarrierSet::access_kind(access.decorators(), access.type());\n+    load_store = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, load_store, kind));\n@@ -715,1 +716,2 @@\n-    result = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, result, ShenandoahBarrierSet::AccessKind::NORMAL));\n+    ShenandoahBarrierSet::AccessKind kind = ShenandoahBarrierSet::access_kind(access.decorators(), access.type());\n+    result = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, result, kind));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2377,1 +2377,1 @@\n-    n->destruct();\n+    n->destruct(&_phase->igvn());\n@@ -2944,1 +2944,6 @@\n-  return t->is_oopptr();\n+\n+  if (kind() == ShenandoahBarrierSet::AccessKind::NORMAL) {\n+    return t;\n+  }\n+\n+  return t->meet(TypePtr::NULL_PTR);\n@@ -2956,2 +2961,5 @@\n-  const Type* type = t2->is_oopptr();\n-  return type;\n+  if (kind() == ShenandoahBarrierSet::AccessKind::NORMAL) {\n+    return t2;\n+  }\n+\n+  return t2->meet(TypePtr::NULL_PTR);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":12,"deletions":4,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -109,8 +109,9 @@\n-    Thread* thr = Thread::current();\n-    if (thr->is_Java_thread()) {\n-      return NULL;\n-    } else {\n-      \/\/ This path is sometimes (rarely) taken by GC threads.\n-      \/\/ See e.g.: https:\/\/bugs.openjdk.java.net\/browse\/JDK-8237874\n-      return obj;\n-    }\n+    return NULL;\n+  }\n+\n+  \/\/ Prevent resurrection of unreachable objects that are visited during\n+  \/\/ concurrent class-unloading.\n+  if (HasDecorator<decorators, AS_NO_KEEPALIVE>::value && obj != NULL &&\n+      _heap->is_evacuation_in_progress() &&\n+      !_heap->marking_context()->is_marked(obj)) {\n+    return obj;\n@@ -258,1 +259,1 @@\n-  res = ShenandoahBarrierSet::barrier_set()->load_reference_barrier(res);\n+  res = ShenandoahBarrierSet::barrier_set()->load_reference_barrier<decorators, T>(res, NULL);\n@@ -284,1 +285,1 @@\n-  previous = ShenandoahBarrierSet::barrier_set()->load_reference_barrier(previous);\n+  previous = ShenandoahBarrierSet::barrier_set()->load_reference_barrier<decorators, T>(previous, NULL);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":11,"deletions":10,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -1526,1 +1526,4 @@\n-JRT_ENTRY(void, InterpreterRuntime::post_method_exit(JavaThread *thread))\n+\/\/ This is a JRT_BLOCK_ENTRY because we have to stash away the return oop\n+\/\/ before transitioning to VM, and restore it after transitioning back\n+\/\/ to Java. The return oop at the top-of-stack, is not walked by the GC.\n+JRT_BLOCK_ENTRY(void, InterpreterRuntime::post_method_exit(JavaThread *thread))\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2298,0 +2298,1 @@\n+      guarantee(pure_name != NULL, \"Illegal native method name encountered\");\n@@ -2308,0 +2309,1 @@\n+        guarantee(long_name != NULL, \"Illegal native method name encountered\");\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -98,0 +98,1 @@\n+  LOG_TAG(lambda) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"utilities\/copy.hpp\"\n","filename":"src\/hotspot\/share\/memory\/heapShared.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n","filename":"src\/hotspot\/share\/memory\/metaspaceShared.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -50,7 +50,3 @@\n-#define FIELDINFO_TAG_BLANK            0\n-#define FIELDINFO_TAG_OFFSET           1\n-#define FIELDINFO_TAG_TYPE_PLAIN       2\n-#define FIELDINFO_TAG_TYPE_CONTENDED   3\n-#define FIELDINFO_TAG_TYPE_MASK        3\n-#define FIELDINFO_TAG_MASK             7\n-#define FIELDINFO_TAG_INLINED          4\n+#define FIELDINFO_TAG_OFFSET           1 << 0\n+#define FIELDINFO_TAG_CONTENDED        1 << 1\n+#define FIELDINFO_TAG_INLINED          1 << 2\n@@ -61,1 +57,3 @@\n-  \/\/    ..........................................00  - blank\n+  \/\/    ..........................................CO\n+  \/\/    ..........................................00  - non-contended field\n+  \/\/    [--contention_group--]...................I10  - contended field with contention group\n@@ -63,4 +61,5 @@\n-  \/\/    ......................[-------type------]I10  - plain field with type\n-  \/\/    [--contention_group--][-------type------]I11  - contended field with type and contention group\n-  \/\/\n-  \/\/ Bit I indicates if the field has been inlined  (I=1) or nor (I=0)\n+\n+  \/\/ Bit O indicates if the packed field contains an offset (O=1) or not (O=0)\n+  \/\/ Bit C indicates if the field is contended (C=1) or not (C=0)\n+  \/\/       (if it is contended, the high packed field contains the contention group)\n+  \/\/ Bit I indicates if the field has been inlined  (I=1) or not (I=0)\n@@ -111,15 +110,2 @@\n-    u2 lo = _shorts[low_packed_offset];\n-    switch(lo & FIELDINFO_TAG_TYPE_MASK) {\n-      case FIELDINFO_TAG_OFFSET:\n-        return build_int_from_shorts(_shorts[low_packed_offset], _shorts[high_packed_offset]) >> FIELDINFO_TAG_SIZE;\n-#ifndef PRODUCT\n-      case FIELDINFO_TAG_TYPE_PLAIN:\n-        fatal(\"Asking offset for the plain type field\");\n-      case FIELDINFO_TAG_TYPE_CONTENDED:\n-        fatal(\"Asking offset for the contended type field\");\n-      case FIELDINFO_TAG_BLANK:\n-        fatal(\"Asking offset for the blank field\");\n-#endif\n-    }\n-    ShouldNotReachHere();\n-    return 0;\n+    assert((_shorts[low_packed_offset] & FIELDINFO_TAG_OFFSET) != 0, \"Offset must have been set\");\n+    return build_int_from_shorts(_shorts[low_packed_offset], _shorts[high_packed_offset]) >> FIELDINFO_TAG_SIZE;\n@@ -129,15 +115,1 @@\n-    u2 lo = _shorts[low_packed_offset];\n-    switch(lo & FIELDINFO_TAG_TYPE_MASK) {\n-      case FIELDINFO_TAG_TYPE_PLAIN:\n-        return false;\n-      case FIELDINFO_TAG_TYPE_CONTENDED:\n-        return true;\n-#ifndef PRODUCT\n-      case FIELDINFO_TAG_OFFSET:\n-        fatal(\"Asking contended flag for the field with offset\");\n-      case FIELDINFO_TAG_BLANK:\n-        fatal(\"Asking contended flag for the blank field\");\n-#endif\n-    }\n-    ShouldNotReachHere();\n-    return false;\n+    return (_shorts[low_packed_offset] & FIELDINFO_TAG_CONTENDED) != 0;\n@@ -147,15 +119,3 @@\n-    u2 lo = _shorts[low_packed_offset];\n-    switch(lo & FIELDINFO_TAG_TYPE_MASK) {\n-      case FIELDINFO_TAG_TYPE_PLAIN:\n-        return 0;\n-      case FIELDINFO_TAG_TYPE_CONTENDED:\n-        return _shorts[high_packed_offset];\n-#ifndef PRODUCT\n-      case FIELDINFO_TAG_OFFSET:\n-        fatal(\"Asking the contended group for the field with offset\");\n-      case FIELDINFO_TAG_BLANK:\n-        fatal(\"Asking the contended group for the blank field\");\n-#endif\n-    }\n-    ShouldNotReachHere();\n-    return 0;\n+    assert((_shorts[low_packed_offset] & FIELDINFO_TAG_OFFSET) == 0, \"Offset must not have been set\");\n+    assert((_shorts[low_packed_offset] & FIELDINFO_TAG_CONTENDED) != 0, \"Field must be contended\");\n+    return _shorts[high_packed_offset];\n@@ -164,18 +124,1 @@\n-  u2 allocation_type() const {\n-    u2 lo = _shorts[low_packed_offset];\n-    switch(lo & FIELDINFO_TAG_TYPE_MASK) {\n-      case FIELDINFO_TAG_TYPE_PLAIN:\n-      case FIELDINFO_TAG_TYPE_CONTENDED:\n-        return (lo >> FIELDINFO_TAG_SIZE);\n-#ifndef PRODUCT\n-      case FIELDINFO_TAG_OFFSET:\n-        fatal(\"Asking the field type for field with offset\");\n-      case FIELDINFO_TAG_BLANK:\n-        fatal(\"Asking the field type for the blank field\");\n-#endif\n-    }\n-    ShouldNotReachHere();\n-    return 0;\n-  }\n-\n-    return (_shorts[low_packed_offset] & FIELDINFO_TAG_TYPE_MASK) == FIELDINFO_TAG_OFFSET;\n+    return (_shorts[low_packed_offset] & FIELDINFO_TAG_OFFSET)!= 0;\n@@ -211,20 +154,0 @@\n-  void set_allocation_type(int type) {\n-    bool b = is_inlined();\n-    u2 lo = _shorts[low_packed_offset];\n-    switch(lo & FIELDINFO_TAG_TYPE_MASK) {\n-      case FIELDINFO_TAG_BLANK:\n-        _shorts[low_packed_offset] |= ((type << FIELDINFO_TAG_SIZE)) & 0xFFFF;\n-        _shorts[low_packed_offset] &= ~FIELDINFO_TAG_TYPE_MASK;\n-        _shorts[low_packed_offset] |= FIELDINFO_TAG_TYPE_PLAIN;\n-        assert(is_inlined() || !b, \"Just checking\");\n-        return;\n-#ifndef PRODUCT\n-      case FIELDINFO_TAG_TYPE_PLAIN:\n-      case FIELDINFO_TAG_TYPE_CONTENDED:\n-      case FIELDINFO_TAG_OFFSET:\n-        fatal(\"Setting the field type with overwriting\");\n-#endif\n-    }\n-    ShouldNotReachHere();\n-  }\n-\n@@ -244,16 +167,4 @@\n-    u2 lo = _shorts[low_packed_offset];\n-    switch(lo & FIELDINFO_TAG_TYPE_MASK) {\n-      case FIELDINFO_TAG_TYPE_PLAIN:\n-        _shorts[low_packed_offset] |= FIELDINFO_TAG_TYPE_CONTENDED;\n-        _shorts[high_packed_offset] = val;\n-        return;\n-#ifndef PRODUCT\n-      case FIELDINFO_TAG_TYPE_CONTENDED:\n-        fatal(\"Overwriting contended group\");\n-      case FIELDINFO_TAG_BLANK:\n-        fatal(\"Setting contended group for the blank field\");\n-      case FIELDINFO_TAG_OFFSET:\n-        fatal(\"Setting contended group for field with offset\");\n-#endif\n-    }\n-    ShouldNotReachHere();\n+    assert((_shorts[low_packed_offset] & FIELDINFO_TAG_OFFSET) == 0, \"Offset must not have been set\");\n+    assert((_shorts[low_packed_offset] & FIELDINFO_TAG_CONTENDED) == 0, \"Overwritting contended group\");\n+    _shorts[low_packed_offset] |= FIELDINFO_TAG_CONTENDED;\n+    _shorts[high_packed_offset] = val;\n","filename":"src\/hotspot\/share\/oops\/fieldInfo.hpp","additions":22,"deletions":111,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -138,4 +138,0 @@\n-  int allocation_type() const {\n-    return field()->allocation_type();\n-  }\n-\n","filename":"src\/hotspot\/share\/oops\/fieldStreams.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3793,1 +3793,7 @@\n-      java_lang_invoke_MemberName::clazz(obj)->print_value_on(st);\n+      oop clazz = java_lang_invoke_MemberName::clazz(obj);\n+      oop name  = java_lang_invoke_MemberName::name(obj);\n+      if (clazz != NULL) {\n+        clazz->print_value_on(st);\n+      } else {\n+        st->print(\"NULL\");\n+      }\n@@ -3795,1 +3801,5 @@\n-      java_lang_invoke_MemberName::name(obj)->print_value_on(st);\n+      if (name != NULL) {\n+        name->print_value_on(st);\n+      } else {\n+        st->print(\"NULL\");\n+      }\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -396,10 +396,2 @@\n-  markWord displaced_mark_helper() const {\n-    assert(has_displaced_mark_helper(), \"check\");\n-    uintptr_t ptr = (value() & ~monitor_value);\n-    return *(markWord*)ptr;\n-  }\n-  void set_displaced_mark_helper(markWord m) const {\n-    assert(has_displaced_mark_helper(), \"check\");\n-    uintptr_t ptr = (value() & ~monitor_value);\n-    ((markWord*)ptr)->_value = m._value;\n-  }\n+  markWord displaced_mark_helper() const;\n+  void set_displaced_mark_helper(markWord m) const;\n@@ -498,1 +490,1 @@\n-  void print_on(outputStream* st) const;\n+  void print_on(outputStream* st, bool print_monitor_info = true) const;\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -192,1 +192,1 @@\n-  MergeMemNode* mem = MergeMemNode::make(in_mem);\n+  MergeMemNode* mem = phase->transform(MergeMemNode::make(in_mem))->as_MergeMem();\n@@ -450,1 +450,1 @@\n-    } else if(can_reshape) {\n+    } else if (can_reshape) {\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -363,3 +363,0 @@\n-  product(bool, UseRDPCForConstantTableBase, false,                         \\\n-          \"Use Sparc RDPC instruction for the constant table base.\")        \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2342,6 +2342,1 @@\n-        if (igvn) { \/\/ Unhook.\n-          igvn->hash_delete(hook);\n-          for (uint i = 1; i < hook->req(); i++) {\n-            hook->set_req(i, NULL);\n-          }\n-        }\n+        hook->destruct(igvn);\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1139,2 +1139,1 @@\n-  if (_modified_nodes != NULL && !_inlining_incrementally &&\n-      n->outcnt() != 0 && !n->is_Con()) {\n+  if (_modified_nodes != NULL && !_inlining_incrementally && !n->is_Con()) {\n@@ -2175,1 +2174,1 @@\n-                hook->destruct();\n+                hook->destruct(&igvn);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -329,8 +329,1 @@\n-  PhaseIterGVN* igvn = phase->is_IterGVN();\n-  if (igvn != NULL) {\n-    igvn->remove_dead_node(hook);\n-  } else {\n-    for (int i = 0; i < 4; i++) {\n-      hook->set_req(i, NULL);\n-    }\n-  }\n+  hook->destruct(phase);\n@@ -919,5 +912,1 @@\n-      if (can_reshape) {\n-        phase->is_IterGVN()->remove_dead_node(hook);\n-      } else {\n-        hook->set_req(0, NULL);   \/\/ Just yank bogus edge during Parse phase\n-      }\n+      hook->destruct(phase);\n@@ -975,5 +964,1 @@\n-  if (can_reshape) {\n-    phase->is_IterGVN()->remove_dead_node(hook);\n-  } else {\n-    hook->set_req(0, NULL);       \/\/ Just yank bogus edge during Parse phase\n-  }\n+  hook->destruct(phase);\n@@ -1094,5 +1079,1 @@\n-      if (can_reshape) {\n-        phase->is_IterGVN()->remove_dead_node(hook);\n-      } else {\n-        hook->set_req(0, NULL);   \/\/ Just yank bogus edge during Parse phase\n-      }\n+      hook->destruct(phase);\n@@ -1150,5 +1131,1 @@\n-  if (can_reshape) {\n-    phase->is_IterGVN()->remove_dead_node(hook);\n-  } else {\n-    hook->set_req(0, NULL);       \/\/ Just yank bogus edge during Parse phase\n-  }\n+  hook->destruct(phase);\n","filename":"src\/hotspot\/share\/opto\/divnode.cpp","additions":5,"deletions":28,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -1002,2 +1002,1 @@\n-    hook->del_req(0); \/\/ Just yank bogus edge\n-    hook->destruct();\n+    hook->destruct(igvn);\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -458,1 +458,1 @@\n-    tmp->destruct();\n+    tmp->destruct(NULL);\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1403,2 +1403,0 @@\n-  Node *hook = new Node(6);\n-\n@@ -1694,3 +1692,0 @@\n-  \/\/ Free up intermediate goo\n-  _igvn.remove_dead_node(hook);\n-\n@@ -2660,1 +2655,1 @@\n-        p->destruct();          \/\/ Recover useless new node\n+        p->destruct(&igvn);     \/\/ Recover useless new node\n","filename":"src\/hotspot\/share\/opto\/loopnode.cpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -298,1 +298,1 @@\n-    \/\/ In i486.ad, indOffset32X uses base==RegI and disp==RegP,\n+    \/\/ In x86_32.ad, indOffset32X uses base==RegI and disp==RegP,\n","filename":"src\/hotspot\/share\/opto\/machnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -155,1 +155,1 @@\n-  \/\/ Only returns non-null value for i486.ad's indOffset32X\n+  \/\/ Only returns non-null value for x86_32.ad's indOffset32X\n@@ -438,1 +438,0 @@\n-  virtual bool pinned() const { return UseRDPCForConstantTableBase; }\n","filename":"src\/hotspot\/share\/opto\/machnode.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -322,1 +322,1 @@\n-    MergeMemNode* mergemen = MergeMemNode::make(mem);\n+    MergeMemNode* mergemen = _igvn.transform(MergeMemNode::make(mem))->as_MergeMem();\n@@ -371,1 +371,1 @@\n-      MergeMemNode* mergemen = MergeMemNode::make(mem);\n+      MergeMemNode* mergemen = _igvn.transform(MergeMemNode::make(mem))->as_MergeMem();\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -587,2 +587,5 @@\n-void Node::destruct() {\n-  Compile* compile = Compile::current();\n+void Node::destruct(PhaseValues* phase) {\n+  Compile* compile = (phase != NULL) ? phase->C : Compile::current();\n+  if (phase != NULL && phase->is_IterGVN()) {\n+    phase->is_IterGVN()->_worklist.remove(this);\n+  }\n@@ -1408,1 +1411,0 @@\n-      igvn->C->remove_modified_node(dead);\n@@ -1447,0 +1449,1 @@\n+      igvn->C->remove_modified_node(dead);\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -244,1 +244,1 @@\n-  void destruct();\n+  void destruct(PhaseValues* phase);\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3140,1 +3140,1 @@\n-      \/\/ For i486.ad, FILD doesn't restrict precision to 24 or 53 bits.\n+      \/\/ For x86_32.ad, FILD doesn't restrict precision to 24 or 53 bits.\n@@ -3155,1 +3155,1 @@\n-    \/\/ For i486.ad, rounding is always necessary (see _l2f above).\n+    \/\/ For x86_32.ad, rounding is always necessary (see _l2f above).\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -752,1 +752,1 @@\n-    x->destruct();              \/\/ Hit, destroy duplicate constant\n+    x->destruct(this);          \/\/ Hit, destroy duplicate constant\n@@ -1072,1 +1072,1 @@\n-    if (n->outcnt() != 0 && !n->is_Con() && !_worklist.member(n)) {\n+    if (!n->is_Con() && !_worklist.member(n)) {\n@@ -1074,1 +1074,1 @@\n-      assert(false, \"modified node is not on IGVN._worklist\");\n+      fatal(\"modified node is not on IGVN._worklist\");\n@@ -1086,1 +1086,1 @@\n-    if (n->outcnt() != 0 && !n->is_Con()) { \/\/ skip dead and Con nodes\n+    if (!n->is_Con()) { \/\/ skip Con nodes\n@@ -1088,1 +1088,1 @@\n-      assert(false, \"modified node was not processed by IGVN.transform_old()\");\n+      fatal(\"modified node was not processed by IGVN.transform_old()\");\n@@ -1478,2 +1478,1 @@\n-  _worklist.remove(temp);   \/\/ this can be necessary\n-  temp->destruct();         \/\/ reuse the _idx of this little guy\n+  temp->destruct(this);     \/\/ reuse the _idx of this little guy\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":6,"deletions":7,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -341,1 +341,1 @@\n-        phi_post->destruct();\n+        phi_post->destruct(&_igvn);\n","filename":"src\/hotspot\/share\/opto\/split_if.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1459,1 +1459,2 @@\n-    if (VerifyMergedCPBytecodes) {\n+#ifdef ASSERT\n+    {\n@@ -1479,0 +1480,1 @@\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -390,0 +390,5 @@\n+#endif\n+#if INCLUDE_SHENANDOAHGC\n+  if (UseShenandoahGC) {\n+    return Universe::heap()->is_in(p);\n+  }\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -558,0 +558,2 @@\n+  { \"UseRDPCForConstantTableBase\",   JDK_Version::undefined(), JDK_Version::jdk(16), JDK_Version::jdk(17) },\n+  { \"VerifyMergedCPBytecodes\",       JDK_Version::undefined(), JDK_Version::jdk(16), JDK_Version::jdk(17) },\n@@ -3320,0 +3322,19 @@\n+#if !INCLUDE_AOT\n+  UNSUPPORTED_OPTION(UseAOT);\n+  UNSUPPORTED_OPTION(PrintAOT);\n+  UNSUPPORTED_OPTION(UseAOTStrictLoading);\n+  UNSUPPORTED_OPTION_NULL(AOTLibrary);\n+\n+  UNSUPPORTED_OPTION_INIT(Tier3AOTInvocationThreshold, 0);\n+  UNSUPPORTED_OPTION_INIT(Tier3AOTMinInvocationThreshold, 0);\n+  UNSUPPORTED_OPTION_INIT(Tier3AOTCompileThreshold, 0);\n+  UNSUPPORTED_OPTION_INIT(Tier3AOTBackEdgeThreshold, 0);\n+  UNSUPPORTED_OPTION_INIT(Tier0AOTInvocationThreshold, 0);\n+  UNSUPPORTED_OPTION_INIT(Tier0AOTMinInvocationThreshold, 0);\n+  UNSUPPORTED_OPTION_INIT(Tier0AOTCompileThreshold, 0);\n+  UNSUPPORTED_OPTION_INIT(Tier0AOTBackEdgeThreshold, 0);\n+#ifndef PRODUCT\n+  UNSUPPORTED_OPTION(PrintAOTStatistics);\n+#endif\n+#endif\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -535,1 +535,1 @@\n-    current->lock()->print_on(st);\n+    current->lock()->print_on(st, current->obj());\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -535,3 +535,0 @@\n-  product_pd(bool, UseOSErrorReporting,                                     \\\n-          \"Let VM fatal error propagate to the OS (ie. WER on Windows)\")    \\\n-                                                                            \\\n@@ -734,0 +731,14 @@\n+  \/* notice: the max range value here is max_jint, not max_intx  *\/         \\\n+  \/* because of overflow issue                                   *\/         \\\n+  product(intx, AvgMonitorsPerThreadEstimate, 1024, DIAGNOSTIC,             \\\n+          \"Used to estimate a variable ceiling based on number of threads \" \\\n+          \"for use with MonitorUsedDeflationThreshold (0 is off).\")         \\\n+          range(0, max_jint)                                                \\\n+                                                                            \\\n+  \/* notice: the max range value here is max_jint, not max_intx  *\/         \\\n+  \/* because of overflow issue                                   *\/         \\\n+  product(intx, MonitorDeflationMax, 1000000, DIAGNOSTIC,                   \\\n+          \"The maximum number of monitors to deflate, unlink and delete \"   \\\n+          \"at one time (minimum is 1024).\")                      \\\n+          range(1024, max_jint)                                             \\\n+                                                                            \\\n@@ -761,1 +772,1 @@\n-          \"Do not complain if the application installs signal handlers \"    \\\n+          \"Application will install primary signal handlers for the JVM \"   \\\n@@ -896,4 +907,0 @@\n-  \/* change to false by default sometime after Mustang *\/                   \\\n-  product(bool, VerifyMergedCPBytecodes, true,                              \\\n-          \"Verify bytecodes after RedefineClasses constant pool merging\")   \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":15,"deletions":8,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -548,5 +548,0 @@\n-    if (_subtasks.try_claim_task(SafepointSynchronize::SAFEPOINT_CLEANUP_DEFLATE_MONITORS)) {\n-      Tracer t(\"deflating idle monitors\");\n-      ObjectSynchronizer::do_safepoint_work();\n-    }\n-\n@@ -614,0 +609,6 @@\n+\n+  if (log_is_enabled(Debug, monitorinflation)) {\n+    \/\/ The VMThread calls do_final_audit_and_print_stats() which calls\n+    \/\/ audit_and_print_stats() at the Info level at VM exit time.\n+    ObjectSynchronizer::audit_and_print_stats(false \/* on_exit *\/);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -114,2 +114,0 @@\n-address StubRoutines::_zero_aligned_words = CAST_FROM_FN_PTR(address, Copy::zero_to_words);\n-\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -210,3 +210,0 @@\n-  \/\/ zero heap space aligned to jlong (8 bytes)\n-  static address _zero_aligned_words;\n-\n@@ -448,2 +445,0 @@\n-  static address zero_aligned_words()  { return _zero_aligned_words; }\n-\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"jfr\/jfrEvents.hpp\"\n@@ -29,1 +30,0 @@\n-#include \"jfr\/jfrEvents.hpp\"\n@@ -60,0 +60,112 @@\n+class MonitorList {\n+  ObjectMonitor* volatile _head;\n+  volatile size_t _count;\n+  volatile size_t _max;\n+\n+public:\n+  void add(ObjectMonitor* monitor);\n+  size_t unlink_deflated(Thread* self, LogStream* ls, elapsedTimer* timer_p,\n+                         GrowableArray<ObjectMonitor*>* unlinked_list);\n+  size_t count() const;\n+  size_t max() const;\n+\n+  class Iterator;\n+  Iterator iterator() const;\n+};\n+\n+class MonitorList::Iterator {\n+  ObjectMonitor* _current;\n+\n+public:\n+  Iterator(ObjectMonitor* head) : _current(head) {}\n+  bool has_next() const { return _current != NULL; }\n+  ObjectMonitor* next();\n+};\n+\n+void MonitorList::add(ObjectMonitor* m) {\n+  ObjectMonitor* head;\n+  do {\n+    head = Atomic::load(&_head);\n+    m->set_next_om(head);\n+  } while (Atomic::cmpxchg(&_head, head, m) != head);\n+\n+  size_t count = Atomic::add(&_count, 1u);\n+  if (count > max()) {\n+    Atomic::inc(&_max);\n+  }\n+}\n+\n+size_t MonitorList::count() const {\n+  return Atomic::load(&_count);\n+}\n+\n+size_t MonitorList::max() const {\n+  return Atomic::load(&_max);\n+}\n+\n+\/\/ Walk the in-use list and unlink (at most MonitorDeflationMax) deflated\n+\/\/ ObjectMonitors. Returns the number of unlinked ObjectMonitors.\n+size_t MonitorList::unlink_deflated(Thread* self, LogStream* ls,\n+                                    elapsedTimer* timer_p,\n+                                    GrowableArray<ObjectMonitor*>* unlinked_list) {\n+  size_t unlinked_count = 0;\n+  ObjectMonitor* prev = NULL;\n+  ObjectMonitor* head = Atomic::load_acquire(&_head);\n+  ObjectMonitor* m = head;\n+  do {\n+    if (m->is_being_async_deflated()) {\n+      \/\/ Find next live ObjectMonitor.\n+      ObjectMonitor* next = m;\n+      do {\n+        ObjectMonitor* next_next = next->next_om();\n+        unlinked_count++;\n+        unlinked_list->append(next);\n+        next = next_next;\n+        if (unlinked_count >= (size_t)MonitorDeflationMax) {\n+          \/\/ Reached the max so bail out on the gathering loop.\n+          break;\n+        }\n+      } while (next != NULL && next->is_being_async_deflated());\n+      if (prev == NULL) {\n+        ObjectMonitor* prev_head = Atomic::cmpxchg(&_head, head, next);\n+        if (prev_head != head) {\n+          \/\/ Find new prev ObjectMonitor that just got inserted.\n+          for (ObjectMonitor* n = prev_head; n != m; n = n->next_om()) {\n+            prev = n;\n+          }\n+          prev->set_next_om(next);\n+        }\n+      } else {\n+        prev->set_next_om(next);\n+      }\n+      if (unlinked_count >= (size_t)MonitorDeflationMax) {\n+        \/\/ Reached the max so bail out on the searching loop.\n+        break;\n+      }\n+      m = next;\n+    } else {\n+      prev = m;\n+      m = m->next_om();\n+    }\n+\n+    if (self->is_Java_thread()) {\n+      \/\/ A JavaThread must check for a safepoint\/handshake and honor it.\n+      ObjectSynchronizer::chk_for_block_req(self->as_Java_thread(), \"unlinking\",\n+                                            \"unlinked_count\", unlinked_count,\n+                                            ls, timer_p);\n+    }\n+  } while (m != NULL);\n+  Atomic::sub(&_count, unlinked_count);\n+  return unlinked_count;\n+}\n+\n+MonitorList::Iterator MonitorList::iterator() const {\n+  return Iterator(Atomic::load_acquire(&_head));\n+}\n+\n+ObjectMonitor* MonitorList::Iterator::next() {\n+  ObjectMonitor* current = _current;\n+  _current = current->next_om();\n+  return current;\n+}\n+\n@@ -62,2 +174,2 @@\n-\/\/ variants of the enter-exit fast-path operations.  See i486.ad fast_lock(),\n-\/\/ for instance.  If you make changes here, make sure to modify the\n+\/\/ variants of the enter-exit fast-path operations.  See c2_MacroAssembler_x86.cpp\n+\/\/ fast_lock(...) for instance.  If you make changes here, make sure to modify the\n@@ -77,1 +189,1 @@\n-  Symbol* klassname = ((oop)(obj))->klass()->name();                       \\\n+  Symbol* klassname = obj->klass()->name();                                \\\n@@ -121,2 +233,16 @@\n-\/\/ global list of blocks of monitors\n-PaddedObjectMonitor* ObjectSynchronizer::g_block_list = NULL;\n+static MonitorList _in_use_list;\n+\/\/ The ratio of the current _in_use_list count to the ceiling is used\n+\/\/ to determine if we are above MonitorUsedDeflationThreshold and need\n+\/\/ to do an async monitor deflation cycle. The ceiling is increased by\n+\/\/ AvgMonitorsPerThreadEstimate when a thread is added to the system\n+\/\/ and is decreased by AvgMonitorsPerThreadEstimate when a thread is\n+\/\/ removed from the system.\n+\/\/ Note: If the _in_use_list max exceeds the ceiling, then\n+\/\/ monitors_used_above_threshold() will use the in_use_list max instead\n+\/\/ of the thread count derived ceiling because we have used more\n+\/\/ ObjectMonitors than the estimated average.\n+\/\/\n+\/\/ Start the ceiling with the estimate for one thread.\n+\/\/ This is a 'jint' because the range of AvgMonitorsPerThreadEstimate\n+\/\/ is 0..max_jint:\n+static jint _in_use_list_ceiling = AvgMonitorsPerThreadEstimate;\n@@ -127,36 +253,0 @@\n-struct ObjectMonitorListGlobals {\n-  char         _pad_prefix[OM_CACHE_LINE_SIZE];\n-  \/\/ These are highly shared list related variables.\n-  \/\/ To avoid false-sharing they need to be the sole occupants of a cache line.\n-\n-  \/\/ Global ObjectMonitor free list. Newly allocated and deflated\n-  \/\/ ObjectMonitors are prepended here.\n-  ObjectMonitor* _free_list;\n-  DEFINE_PAD_MINUS_SIZE(1, OM_CACHE_LINE_SIZE, sizeof(ObjectMonitor*));\n-\n-  \/\/ Global ObjectMonitor in-use list. When a JavaThread is exiting,\n-  \/\/ ObjectMonitors on its per-thread in-use list are prepended here.\n-  ObjectMonitor* _in_use_list;\n-  DEFINE_PAD_MINUS_SIZE(2, OM_CACHE_LINE_SIZE, sizeof(ObjectMonitor*));\n-\n-  \/\/ Global ObjectMonitor wait list. Deflated ObjectMonitors wait on\n-  \/\/ this list until after a handshake or a safepoint for platforms\n-  \/\/ that don't support handshakes. After the handshake or safepoint,\n-  \/\/ the deflated ObjectMonitors are prepended to free_list.\n-  ObjectMonitor* _wait_list;\n-  DEFINE_PAD_MINUS_SIZE(3, OM_CACHE_LINE_SIZE, sizeof(ObjectMonitor*));\n-\n-  int _free_count;    \/\/ # on free_list\n-  DEFINE_PAD_MINUS_SIZE(4, OM_CACHE_LINE_SIZE, sizeof(int));\n-\n-  int _in_use_count;  \/\/ # on in_use_list\n-  DEFINE_PAD_MINUS_SIZE(5, OM_CACHE_LINE_SIZE, sizeof(int));\n-\n-  int _population;    \/\/ # Extant -- in circulation\n-  DEFINE_PAD_MINUS_SIZE(6, OM_CACHE_LINE_SIZE, sizeof(int));\n-\n-  int _wait_count;    \/\/ # on wait_list\n-  DEFINE_PAD_MINUS_SIZE(7, OM_CACHE_LINE_SIZE, sizeof(int));\n-};\n-static ObjectMonitorListGlobals om_list_globals;\n-\n@@ -175,263 +265,0 @@\n-\/\/ =====================> Spin-lock functions\n-\n-\/\/ ObjectMonitors are not lockable outside of this file. We use spin-locks\n-\/\/ implemented using a bit in the _next_om field instead of the heavier\n-\/\/ weight locking mechanisms for faster list management.\n-\n-#define OM_LOCK_BIT 0x1\n-\n-\/\/ Return true if the ObjectMonitor is locked.\n-\/\/ Otherwise returns false.\n-static bool is_locked(ObjectMonitor* om) {\n-  return ((intptr_t)om->next_om_acquire() & OM_LOCK_BIT) == OM_LOCK_BIT;\n-}\n-\n-\/\/ Mark an ObjectMonitor* with OM_LOCK_BIT and return it.\n-static ObjectMonitor* mark_om_ptr(ObjectMonitor* om) {\n-  return (ObjectMonitor*)((intptr_t)om | OM_LOCK_BIT);\n-}\n-\n-\/\/ Return the unmarked next field in an ObjectMonitor. Note: the next\n-\/\/ field may or may not have been marked with OM_LOCK_BIT originally.\n-static ObjectMonitor* unmarked_next(ObjectMonitor* om) {\n-  return (ObjectMonitor*)((intptr_t)om->next_om() & ~OM_LOCK_BIT);\n-}\n-\n-\/\/ Try to lock an ObjectMonitor. Returns true if locking was successful.\n-\/\/ Otherwise returns false.\n-static bool try_om_lock(ObjectMonitor* om) {\n-  \/\/ Get current next field without any OM_LOCK_BIT value.\n-  ObjectMonitor* next = unmarked_next(om);\n-  if (om->try_set_next_om(next, mark_om_ptr(next)) != next) {\n-    return false;  \/\/ Cannot lock the ObjectMonitor.\n-  }\n-  return true;\n-}\n-\n-\/\/ Lock an ObjectMonitor.\n-static void om_lock(ObjectMonitor* om) {\n-  while (true) {\n-    if (try_om_lock(om)) {\n-      return;\n-    }\n-  }\n-}\n-\n-\/\/ Unlock an ObjectMonitor.\n-static void om_unlock(ObjectMonitor* om) {\n-  ObjectMonitor* next = om->next_om();\n-  guarantee(((intptr_t)next & OM_LOCK_BIT) == OM_LOCK_BIT, \"next=\" INTPTR_FORMAT\n-            \" must have OM_LOCK_BIT=%x set.\", p2i(next), OM_LOCK_BIT);\n-\n-  next = (ObjectMonitor*)((intptr_t)next & ~OM_LOCK_BIT);  \/\/ Clear OM_LOCK_BIT.\n-  om->release_set_next_om(next);\n-}\n-\n-\/\/ Get the list head after locking it. Returns the list head or NULL\n-\/\/ if the list is empty.\n-static ObjectMonitor* get_list_head_locked(ObjectMonitor** list_p) {\n-  while (true) {\n-    \/\/ Acquire semantics not needed on this list load since we're\n-    \/\/ checking for NULL here or following up with a cmpxchg() via\n-    \/\/ try_om_lock() below and we retry on cmpxchg() failure.\n-    ObjectMonitor* mid = Atomic::load(list_p);\n-    if (mid == NULL) {\n-      return NULL;  \/\/ The list is empty.\n-    }\n-    if (try_om_lock(mid)) {\n-      \/\/ Acquire semantics not needed on this list load since memory is\n-      \/\/ already consistent due to the cmpxchg() via try_om_lock() above.\n-      if (Atomic::load(list_p) != mid) {\n-        \/\/ The list head changed before we could lock it so we have to retry.\n-        om_unlock(mid);\n-        continue;\n-      }\n-      return mid;\n-    }\n-  }\n-}\n-\n-#undef OM_LOCK_BIT\n-\n-\n-\/\/ =====================> List Management functions\n-\n-\/\/ Prepend a list of ObjectMonitors to the specified *list_p. 'tail' is\n-\/\/ the last ObjectMonitor in the list and there are 'count' on the list.\n-\/\/ Also updates the specified *count_p.\n-static void prepend_list_to_common(ObjectMonitor* list, ObjectMonitor* tail,\n-                                   int count, ObjectMonitor** list_p,\n-                                   int* count_p) {\n-  while (true) {\n-    \/\/ Acquire semantics not needed on this list load since we're\n-    \/\/ following up with a cmpxchg() via try_om_lock() below and we\n-    \/\/ retry on cmpxchg() failure.\n-    ObjectMonitor* cur = Atomic::load(list_p);\n-    \/\/ Prepend list to *list_p.\n-    if (!try_om_lock(tail)) {\n-      \/\/ Failed to lock tail due to a list walker so try it all again.\n-      continue;\n-    }\n-    \/\/ Release semantics not needed on this \"unlock\" since memory is\n-    \/\/ already consistent due to the cmpxchg() via try_om_lock() above.\n-    tail->set_next_om(cur);  \/\/ tail now points to cur (and unlocks tail)\n-    if (cur == NULL) {\n-      \/\/ No potential race with takers or other prependers since\n-      \/\/ *list_p is empty.\n-      if (Atomic::cmpxchg(list_p, cur, list) == cur) {\n-        \/\/ Successfully switched *list_p to the list value.\n-        Atomic::add(count_p, count);\n-        break;\n-      }\n-      \/\/ Implied else: try it all again\n-    } else {\n-      if (!try_om_lock(cur)) {\n-        continue;  \/\/ failed to lock cur so try it all again\n-      }\n-      \/\/ We locked cur so try to switch *list_p to the list value.\n-      if (Atomic::cmpxchg(list_p, cur, list) != cur) {\n-        \/\/ The list head has changed so unlock cur and try again:\n-        om_unlock(cur);\n-        continue;\n-      }\n-      Atomic::add(count_p, count);\n-      om_unlock(cur);\n-      break;\n-    }\n-  }\n-}\n-\n-\/\/ Prepend a newly allocated block of ObjectMonitors to g_block_list and\n-\/\/ om_list_globals._free_list. Also updates om_list_globals._population\n-\/\/ and om_list_globals._free_count.\n-void ObjectSynchronizer::prepend_block_to_lists(PaddedObjectMonitor* new_blk) {\n-  \/\/ First we handle g_block_list:\n-  while (true) {\n-    PaddedObjectMonitor* cur = Atomic::load(&g_block_list);\n-    \/\/ Prepend new_blk to g_block_list. The first ObjectMonitor in\n-    \/\/ a block is reserved for use as linkage to the next block.\n-    new_blk[0].set_next_om(cur);\n-    if (Atomic::cmpxchg(&g_block_list, cur, new_blk) == cur) {\n-      \/\/ Successfully switched g_block_list to the new_blk value.\n-      Atomic::add(&om_list_globals._population, _BLOCKSIZE - 1);\n-      break;\n-    }\n-    \/\/ Implied else: try it all again\n-  }\n-\n-  \/\/ Second we handle om_list_globals._free_list:\n-  prepend_list_to_common(new_blk + 1, &new_blk[_BLOCKSIZE - 1], _BLOCKSIZE - 1,\n-                         &om_list_globals._free_list, &om_list_globals._free_count);\n-}\n-\n-\/\/ Prepend a list of ObjectMonitors to om_list_globals._free_list.\n-\/\/ 'tail' is the last ObjectMonitor in the list and there are 'count'\n-\/\/ on the list. Also updates om_list_globals._free_count.\n-static void prepend_list_to_global_free_list(ObjectMonitor* list,\n-                                             ObjectMonitor* tail, int count) {\n-  prepend_list_to_common(list, tail, count, &om_list_globals._free_list,\n-                         &om_list_globals._free_count);\n-}\n-\n-\/\/ Prepend a list of ObjectMonitors to om_list_globals._wait_list.\n-\/\/ 'tail' is the last ObjectMonitor in the list and there are 'count'\n-\/\/ on the list. Also updates om_list_globals._wait_count.\n-static void prepend_list_to_global_wait_list(ObjectMonitor* list,\n-                                             ObjectMonitor* tail, int count) {\n-  prepend_list_to_common(list, tail, count, &om_list_globals._wait_list,\n-                         &om_list_globals._wait_count);\n-}\n-\n-\/\/ Prepend a list of ObjectMonitors to om_list_globals._in_use_list.\n-\/\/ 'tail' is the last ObjectMonitor in the list and there are 'count'\n-\/\/ on the list. Also updates om_list_globals._in_use_list.\n-static void prepend_list_to_global_in_use_list(ObjectMonitor* list,\n-                                               ObjectMonitor* tail, int count) {\n-  prepend_list_to_common(list, tail, count, &om_list_globals._in_use_list,\n-                         &om_list_globals._in_use_count);\n-}\n-\n-\/\/ Prepend an ObjectMonitor to the specified list. Also updates\n-\/\/ the specified counter.\n-static void prepend_to_common(ObjectMonitor* m, ObjectMonitor** list_p,\n-                              int* count_p) {\n-  while (true) {\n-    om_lock(m);  \/\/ Lock m so we can safely update its next field.\n-    ObjectMonitor* cur = NULL;\n-    \/\/ Lock the list head to guard against races with a list walker\n-    \/\/ or async deflater thread (which only races in om_in_use_list):\n-    if ((cur = get_list_head_locked(list_p)) != NULL) {\n-      \/\/ List head is now locked so we can safely switch it. Release\n-      \/\/ semantics not needed on this \"unlock\" since memory is already\n-      \/\/ consistent due to the cmpxchg() via get_list_head_locked() above.\n-      m->set_next_om(cur);  \/\/ m now points to cur (and unlocks m)\n-      OrderAccess::storestore();  \/\/ Make sure set_next_om() is seen first.\n-      Atomic::store(list_p, m);  \/\/ Switch list head to unlocked m.\n-      om_unlock(cur);\n-      break;\n-    }\n-    \/\/ The list is empty so try to set the list head.\n-    assert(cur == NULL, \"cur must be NULL: cur=\" INTPTR_FORMAT, p2i(cur));\n-    \/\/ Release semantics not needed on this \"unlock\" since memory\n-    \/\/ is already consistent.\n-    m->set_next_om(cur);  \/\/ m now points to NULL (and unlocks m)\n-    if (Atomic::cmpxchg(list_p, cur, m) == cur) {\n-      \/\/ List head is now unlocked m.\n-      break;\n-    }\n-    \/\/ Implied else: try it all again\n-  }\n-  Atomic::inc(count_p);\n-}\n-\n-\/\/ Prepend an ObjectMonitor to a per-thread om_free_list.\n-\/\/ Also updates the per-thread om_free_count.\n-static void prepend_to_om_free_list(Thread* self, ObjectMonitor* m) {\n-  prepend_to_common(m, &self->om_free_list, &self->om_free_count);\n-}\n-\n-\/\/ Prepend an ObjectMonitor to a per-thread om_in_use_list.\n-\/\/ Also updates the per-thread om_in_use_count.\n-static void prepend_to_om_in_use_list(Thread* self, ObjectMonitor* m) {\n-  prepend_to_common(m, &self->om_in_use_list, &self->om_in_use_count);\n-}\n-\n-\/\/ Take an ObjectMonitor from the start of the specified list. Also\n-\/\/ decrements the specified counter. Returns NULL if none are available.\n-static ObjectMonitor* take_from_start_of_common(ObjectMonitor** list_p,\n-                                                int* count_p) {\n-  ObjectMonitor* take = NULL;\n-  \/\/ Lock the list head to guard against races with a list walker\n-  \/\/ or async deflater thread (which only races in om_list_globals._free_list):\n-  if ((take = get_list_head_locked(list_p)) == NULL) {\n-    return NULL;  \/\/ None are available.\n-  }\n-  ObjectMonitor* next = unmarked_next(take);\n-  \/\/ Switch locked list head to next (which unlocks the list head, but\n-  \/\/ leaves take locked). Release semantics not needed on this \"unlock\"\n-  \/\/ since memory is already consistent due to the cmpxchg() via\n-  \/\/ get_list_head_locked() above.\n-  Atomic::store(list_p, next);\n-  Atomic::dec(count_p);\n-  \/\/ Unlock take, but leave the next value for any lagging list\n-  \/\/ walkers. It will get cleaned up when take is prepended to\n-  \/\/ the in-use list:\n-  om_unlock(take);\n-  return take;\n-}\n-\n-\/\/ Take an ObjectMonitor from the start of the om_list_globals._free_list.\n-\/\/ Also updates om_list_globals._free_count. Returns NULL if none are\n-\/\/ available.\n-static ObjectMonitor* take_from_start_of_global_free_list() {\n-  return take_from_start_of_common(&om_list_globals._free_list,\n-                                   &om_list_globals._free_count);\n-}\n-\n-\/\/ Take an ObjectMonitor from the start of a per-thread free-list.\n-\/\/ Also updates om_free_count. Returns NULL if none are available.\n-static ObjectMonitor* take_from_start_of_om_free_list(Thread* self) {\n-  return take_from_start_of_common(&self->om_free_list, &self->om_free_count);\n-}\n-\n-\n@@ -1338,16 +1165,13 @@\n-  PaddedObjectMonitor* block = Atomic::load(&g_block_list);\n-  while (block != NULL) {\n-    assert(block->is_chainmarker(), \"must be a block header\");\n-    for (int i = _BLOCKSIZE - 1; i > 0; i--) {\n-      ObjectMonitor* mid = (ObjectMonitor *)(block + i);\n-      if (mid->object_peek() != NULL) {\n-        \/\/ Only process with closure if the object is set.\n-\n-        \/\/ monitors_iterate() is only called at a safepoint or when the\n-        \/\/ target thread is suspended or when the target thread is\n-        \/\/ operating on itself. The current closures in use today are\n-        \/\/ only interested in an owned ObjectMonitor and ownership\n-        \/\/ cannot be dropped under the calling contexts so the\n-        \/\/ ObjectMonitor cannot be async deflated.\n-        closure->do_monitor(mid);\n-      }\n+  MonitorList::Iterator iter = _in_use_list.iterator();\n+  while (iter.has_next()) {\n+    ObjectMonitor* mid = iter.next();\n+    if (!mid->is_being_async_deflated() && mid->object_peek() != NULL) {\n+      \/\/ Only process with closure if the object is set.\n+\n+      \/\/ monitors_iterate() is only called at a safepoint or when the\n+      \/\/ target thread is suspended or when the target thread is\n+      \/\/ operating on itself. The current closures in use today are\n+      \/\/ only interested in an owned ObjectMonitor and ownership\n+      \/\/ cannot be dropped under the calling contexts so the\n+      \/\/ ObjectMonitor cannot be async deflated.\n+      closure->do_monitor(mid);\n@@ -1355,3 +1179,0 @@\n-    \/\/ unmarked_next() is not needed with g_block_list (no locking\n-    \/\/ used with block linkage _next_om fields).\n-    block = (PaddedObjectMonitor*)block->next_om();\n@@ -1361,3 +1182,8 @@\n-static bool monitors_used_above_threshold() {\n-  int population = Atomic::load(&om_list_globals._population);\n-  if (population == 0) {\n+static bool monitors_used_above_threshold(MonitorList* list) {\n+  \/\/ Start with ceiling based on a per-thread estimate:\n+  size_t ceiling = ObjectSynchronizer::in_use_list_ceiling();\n+  if (ceiling < list->max()) {\n+    \/\/ The max used by the system has exceeded the ceiling so use that:\n+    ceiling = list->max();\n+  }\n+  if (ceiling == 0) {\n@@ -1367,4 +1193,3 @@\n-    int monitors_used = population - Atomic::load(&om_list_globals._free_count) -\n-                        Atomic::load(&om_list_globals._wait_count);\n-    int monitor_usage = (monitors_used * 100LL) \/ population;\n-    return monitor_usage > MonitorUsedDeflationThreshold;\n+    size_t monitors_used = list->count();\n+    size_t monitor_usage = (monitors_used * 100LL) \/ ceiling;\n+    return int(monitor_usage) > MonitorUsedDeflationThreshold;\n@@ -1375,0 +1200,19 @@\n+size_t ObjectSynchronizer::in_use_list_ceiling() {\n+  \/\/ _in_use_list_ceiling is a jint so this cast could lose precision,\n+  \/\/ but in reality the ceiling should never get that high.\n+  return (size_t)_in_use_list_ceiling;\n+}\n+\n+void ObjectSynchronizer::dec_in_use_list_ceiling() {\n+  Atomic::add(&_in_use_list_ceiling, (jint)-AvgMonitorsPerThreadEstimate);\n+#ifdef ASSERT\n+  size_t l_in_use_list_ceiling = in_use_list_ceiling();\n+#endif\n+  assert(l_in_use_list_ceiling > 0, \"in_use_list_ceiling=\" SIZE_FORMAT\n+         \": must be > 0\", l_in_use_list_ceiling);\n+}\n+\n+void ObjectSynchronizer::inc_in_use_list_ceiling() {\n+  Atomic::add(&_in_use_list_ceiling, (jint)AvgMonitorsPerThreadEstimate);\n+}\n+\n@@ -1382,1 +1226,1 @@\n-      monitors_used_above_threshold()) {\n+      monitors_used_above_threshold(&_in_use_list)) {\n@@ -1386,1 +1230,1 @@\n-    \/\/ in order to not swamp the ServiceThread.\n+    \/\/ in order to not swamp the MonitorDeflationThread.\n@@ -1399,1 +1243,1 @@\n-    MonitorLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);\n+    MonitorLocker ml(MonitorDeflation_lock, Mutex::_no_safepoint_check_flag);\n@@ -1428,404 +1272,0 @@\n-\n-\/\/ -----------------------------------------------------------------------------\n-\/\/ ObjectMonitor Lifecycle\n-\/\/ -----------------------\n-\/\/ Inflation unlinks monitors from om_list_globals._free_list or a per-thread\n-\/\/ free list and associates them with objects. Async deflation disassociates\n-\/\/ idle monitors from objects. Such scavenged monitors are returned to the\n-\/\/ om_list_globals._free_list.\n-\/\/\n-\/\/ ObjectMonitors reside in type-stable memory (TSM) and are immortal.\n-\/\/\n-\/\/ Lifecycle:\n-\/\/ --   unassigned and on the om_list_globals._free_list\n-\/\/ --   unassigned and on a per-thread free list\n-\/\/ --   assigned to an object.  The object is inflated and the mark refers\n-\/\/      to the ObjectMonitor.\n-\n-ObjectMonitor* ObjectSynchronizer::om_alloc(Thread* self) {\n-  \/\/ A large MAXPRIVATE value reduces both list lock contention\n-  \/\/ and list coherency traffic, but also tends to increase the\n-  \/\/ number of ObjectMonitors in circulation as well as the\n-  \/\/ scavenge costs.  As usual, we lean toward time in space-time\n-  \/\/ tradeoffs.\n-  const int MAXPRIVATE = 1024;\n-  NoSafepointVerifier nsv;\n-\n-  for (;;) {\n-    ObjectMonitor* m;\n-\n-    \/\/ 1: try to allocate from the thread's local om_free_list.\n-    \/\/ Threads will attempt to allocate first from their local list, then\n-    \/\/ from the global list, and only after those attempts fail will the\n-    \/\/ thread attempt to instantiate new monitors. Thread-local free lists\n-    \/\/ improve allocation latency, as well as reducing coherency traffic\n-    \/\/ on the shared global list.\n-    m = take_from_start_of_om_free_list(self);\n-    if (m != NULL) {\n-      guarantee(m->object_peek() == NULL, \"invariant\");\n-      m->set_allocation_state(ObjectMonitor::New);\n-      prepend_to_om_in_use_list(self, m);\n-      return m;\n-    }\n-\n-    \/\/ 2: try to allocate from the global om_list_globals._free_list\n-    \/\/ If we're using thread-local free lists then try\n-    \/\/ to reprovision the caller's free list.\n-    \/\/ Acquire semantics not needed on this list load since memory\n-    \/\/ is already consistent due to the cmpxchg() via\n-    \/\/ take_from_start_of_om_free_list() above.\n-    if (Atomic::load(&om_list_globals._free_list) != NULL) {\n-      \/\/ Reprovision the thread's om_free_list.\n-      \/\/ Use bulk transfers to reduce the allocation rate and heat\n-      \/\/ on various locks.\n-      for (int i = self->om_free_provision; --i >= 0;) {\n-        ObjectMonitor* take = take_from_start_of_global_free_list();\n-        if (take == NULL) {\n-          break;  \/\/ No more are available.\n-        }\n-        guarantee(take->object_peek() == NULL, \"invariant\");\n-        \/\/ We allowed 3 field values to linger during async deflation.\n-        \/\/ Clear or restore them as appropriate.\n-        take->set_header(markWord::zero());\n-        \/\/ DEFLATER_MARKER is the only non-NULL value we should see here.\n-        take->try_set_owner_from(DEFLATER_MARKER, NULL);\n-        if (take->contentions() < 0) {\n-          \/\/ Add back max_jint to restore the contentions field to its\n-          \/\/ proper value.\n-          take->add_to_contentions(max_jint);\n-\n-#ifdef ASSERT\n-          jint l_contentions = take->contentions();\n-          assert(l_contentions >= 0, \"must not be negative: l_contentions=%d, contentions=%d\",\n-                 l_contentions, take->contentions());\n-#endif\n-        }\n-        take->Recycle();\n-        \/\/ Since we're taking from the global free-list, take must be Free.\n-        \/\/ om_release() also sets the allocation state to Free because it\n-        \/\/ is called from other code paths.\n-        assert(take->is_free(), \"invariant\");\n-        om_release(self, take, false);\n-      }\n-      self->om_free_provision += 1 + (self->om_free_provision \/ 2);\n-      if (self->om_free_provision > MAXPRIVATE) self->om_free_provision = MAXPRIVATE;\n-      continue;\n-    }\n-\n-    \/\/ 3: allocate a block of new ObjectMonitors\n-    \/\/ Both the local and global free lists are empty -- resort to malloc().\n-    \/\/ In the current implementation ObjectMonitors are TSM - immortal.\n-    \/\/ Ideally, we'd write \"new ObjectMonitor[_BLOCKSIZE], but we want\n-    \/\/ each ObjectMonitor to start at the beginning of a cache line,\n-    \/\/ so we use align_up().\n-    \/\/ A better solution would be to use C++ placement-new.\n-    \/\/ BEWARE: As it stands currently, we don't run the ctors!\n-    assert(_BLOCKSIZE > 1, \"invariant\");\n-    size_t neededsize = sizeof(PaddedObjectMonitor) * _BLOCKSIZE;\n-    PaddedObjectMonitor* temp;\n-    size_t aligned_size = neededsize + (OM_CACHE_LINE_SIZE - 1);\n-    void* real_malloc_addr = NEW_C_HEAP_ARRAY(char, aligned_size, mtInternal);\n-    temp = (PaddedObjectMonitor*)align_up(real_malloc_addr, OM_CACHE_LINE_SIZE);\n-    (void)memset((void *) temp, 0, neededsize);\n-\n-    \/\/ Format the block.\n-    \/\/ initialize the linked list, each monitor points to its next\n-    \/\/ forming the single linked free list, the very first monitor\n-    \/\/ will points to next block, which forms the block list.\n-    \/\/ The trick of using the 1st element in the block as g_block_list\n-    \/\/ linkage should be reconsidered.  A better implementation would\n-    \/\/ look like: class Block { Block * next; int N; ObjectMonitor Body [N] ; }\n-\n-    for (int i = 1; i < _BLOCKSIZE; i++) {\n-      temp[i].set_next_om((ObjectMonitor*)&temp[i + 1]);\n-      assert(temp[i].is_free(), \"invariant\");\n-    }\n-\n-    \/\/ terminate the last monitor as the end of list\n-    temp[_BLOCKSIZE - 1].set_next_om((ObjectMonitor*)NULL);\n-\n-    \/\/ Element [0] is reserved for global list linkage\n-    temp[0].set_allocation_state(ObjectMonitor::ChainMarker);\n-\n-    \/\/ Consider carving out this thread's current request from the\n-    \/\/ block in hand.  This avoids some lock traffic and redundant\n-    \/\/ list activity.\n-\n-    prepend_block_to_lists(temp);\n-  }\n-}\n-\n-\/\/ Place \"m\" on the caller's private per-thread om_free_list.\n-\/\/ In practice there's no need to clamp or limit the number of\n-\/\/ monitors on a thread's om_free_list as the only non-allocation time\n-\/\/ we'll call om_release() is to return a monitor to the free list after\n-\/\/ a CAS attempt failed. This doesn't allow unbounded #s of monitors to\n-\/\/ accumulate on a thread's free list.\n-\/\/\n-\/\/ Key constraint: all ObjectMonitors on a thread's free list and the global\n-\/\/ free list must have their object field set to null. This prevents the\n-\/\/ scavenger -- deflate_monitor_list() -- from reclaiming them\n-\/\/ while we are trying to release them.\n-\n-void ObjectSynchronizer::om_release(Thread* self, ObjectMonitor* m,\n-                                    bool from_per_thread_alloc) {\n-  guarantee(m->header().value() == 0, \"invariant\");\n-  guarantee(m->object_peek() == NULL, \"invariant\");\n-  NoSafepointVerifier nsv;\n-\n-  if ((m->is_busy() | m->_recursions) != 0) {\n-    stringStream ss;\n-    fatal(\"freeing in-use monitor: %s, recursions=\" INTX_FORMAT,\n-          m->is_busy_to_string(&ss), m->_recursions);\n-  }\n-  m->set_allocation_state(ObjectMonitor::Free);\n-  \/\/ _next_om is used for both per-thread in-use and free lists so\n-  \/\/ we have to remove 'm' from the in-use list first (as needed).\n-  if (from_per_thread_alloc) {\n-    \/\/ Need to remove 'm' from om_in_use_list.\n-    ObjectMonitor* mid = NULL;\n-    ObjectMonitor* next = NULL;\n-\n-    \/\/ This list walk can race with another list walker or with async\n-    \/\/ deflation so we have to worry about an ObjectMonitor being\n-    \/\/ removed from this list while we are walking it.\n-\n-    \/\/ Lock the list head to avoid racing with another list walker\n-    \/\/ or with async deflation.\n-    if ((mid = get_list_head_locked(&self->om_in_use_list)) == NULL) {\n-      fatal(\"thread=\" INTPTR_FORMAT \" in-use list must not be empty.\", p2i(self));\n-    }\n-    next = unmarked_next(mid);\n-    if (m == mid) {\n-      \/\/ First special case:\n-      \/\/ 'm' matches mid, is the list head and is locked. Switch the list\n-      \/\/ head to next which unlocks the list head, but leaves the extracted\n-      \/\/ mid locked. Release semantics not needed on this \"unlock\" since\n-      \/\/ memory is already consistent due to the get_list_head_locked()\n-      \/\/ above.\n-      Atomic::store(&self->om_in_use_list, next);\n-    } else if (m == next) {\n-      \/\/ Second special case:\n-      \/\/ 'm' matches next after the list head and we already have the list\n-      \/\/ head locked so set mid to what we are extracting:\n-      mid = next;\n-      \/\/ Lock mid to prevent races with a list walker or an async\n-      \/\/ deflater thread that's ahead of us. The locked list head\n-      \/\/ prevents races from behind us.\n-      om_lock(mid);\n-      \/\/ Update next to what follows mid (if anything):\n-      next = unmarked_next(mid);\n-      \/\/ Switch next after the list head to new next which unlocks the\n-      \/\/ list head, but leaves the extracted mid locked. Release semantics\n-      \/\/ not needed on this \"unlock\" since memory is already consistent\n-      \/\/ due to the get_list_head_locked() above.\n-      self->om_in_use_list->set_next_om(next);\n-    } else {\n-      \/\/ We have to search the list to find 'm'.\n-      guarantee(next != NULL, \"thread=\" INTPTR_FORMAT \": om_in_use_list=\" INTPTR_FORMAT\n-                \" is too short.\", p2i(self), p2i(self->om_in_use_list));\n-      \/\/ Our starting anchor is next after the list head which is the\n-      \/\/ last ObjectMonitor we checked:\n-      ObjectMonitor* anchor = next;\n-      \/\/ Lock anchor to prevent races with a list walker or an async\n-      \/\/ deflater thread that's ahead of us. The locked list head\n-      \/\/ prevents races from behind us.\n-      om_lock(anchor);\n-      om_unlock(mid);  \/\/ Unlock the list head now that anchor is locked.\n-      while ((mid = unmarked_next(anchor)) != NULL) {\n-        if (m == mid) {\n-          \/\/ We found 'm' on the per-thread in-use list so extract it.\n-          \/\/ Update next to what follows mid (if anything):\n-          next = unmarked_next(mid);\n-          \/\/ Switch next after the anchor to new next which unlocks the\n-          \/\/ anchor, but leaves the extracted mid locked. Release semantics\n-          \/\/ not needed on this \"unlock\" since memory is already consistent\n-          \/\/ due to the om_unlock() above before entering the loop or the\n-          \/\/ om_unlock() below before looping again.\n-          anchor->set_next_om(next);\n-          break;\n-        } else {\n-          \/\/ Lock the next anchor to prevent races with a list walker\n-          \/\/ or an async deflater thread that's ahead of us. The locked\n-          \/\/ current anchor prevents races from behind us.\n-          om_lock(mid);\n-          \/\/ Unlock current anchor now that next anchor is locked:\n-          om_unlock(anchor);\n-          anchor = mid;  \/\/ Advance to new anchor and try again.\n-        }\n-      }\n-    }\n-\n-    if (mid == NULL) {\n-      \/\/ Reached end of the list and didn't find 'm' so:\n-      fatal(\"thread=\" INTPTR_FORMAT \" must find m=\" INTPTR_FORMAT \"on om_in_use_list=\"\n-            INTPTR_FORMAT, p2i(self), p2i(m), p2i(self->om_in_use_list));\n-    }\n-\n-    \/\/ At this point mid is disconnected from the in-use list so\n-    \/\/ its lock no longer has any effects on the in-use list.\n-    Atomic::dec(&self->om_in_use_count);\n-    \/\/ Unlock mid, but leave the next value for any lagging list\n-    \/\/ walkers. It will get cleaned up when mid is prepended to\n-    \/\/ the thread's free list:\n-    om_unlock(mid);\n-  }\n-\n-  prepend_to_om_free_list(self, m);\n-  guarantee(m->is_free(), \"invariant\");\n-}\n-\n-\/\/ Return ObjectMonitors on a moribund thread's free and in-use\n-\/\/ lists to the appropriate global lists. The ObjectMonitors on the\n-\/\/ per-thread in-use list may still be in use by other threads.\n-\/\/\n-\/\/ We currently call om_flush() from Threads::remove() before the\n-\/\/ thread has been excised from the thread list and is no longer a\n-\/\/ mutator.\n-\/\/\n-\/\/ deflate_global_idle_monitors() and deflate_per_thread_idle_monitors()\n-\/\/ (in another thread) can run at the same time as om_flush() so we have\n-\/\/ to follow a careful protocol to prevent list corruption.\n-\n-void ObjectSynchronizer::om_flush(Thread* self) {\n-  \/\/ Process the per-thread in-use list first to be consistent.\n-  int in_use_count = 0;\n-  ObjectMonitor* in_use_list = NULL;\n-  ObjectMonitor* in_use_tail = NULL;\n-  NoSafepointVerifier nsv;\n-\n-  \/\/ This function can race with a list walker or with an async\n-  \/\/ deflater thread so we lock the list head to prevent confusion.\n-  \/\/ An async deflater thread checks to see if the target thread\n-  \/\/ is exiting, but if it has made it past that check before we\n-  \/\/ started exiting, then it is racing to get to the in-use list.\n-  if ((in_use_list = get_list_head_locked(&self->om_in_use_list)) != NULL) {\n-    \/\/ At this point, we have locked the in-use list head so a racing\n-    \/\/ thread cannot come in after us. However, a racing thread could\n-    \/\/ be ahead of us; we'll detect that and delay to let it finish.\n-    \/\/\n-    \/\/ The thread is going away, however the ObjectMonitors on the\n-    \/\/ om_in_use_list may still be in-use by other threads. Link\n-    \/\/ them to in_use_tail, which will be linked into the global\n-    \/\/ in-use list (om_list_globals._in_use_list) below.\n-    \/\/\n-    \/\/ Account for the in-use list head before the loop since it is\n-    \/\/ already locked (by this thread):\n-    in_use_tail = in_use_list;\n-    in_use_count++;\n-    for (ObjectMonitor* cur_om = unmarked_next(in_use_list); cur_om != NULL;) {\n-      if (is_locked(cur_om)) {\n-        \/\/ cur_om is locked so there must be a racing walker or async\n-        \/\/ deflater thread ahead of us so we'll give it a chance to finish.\n-        while (is_locked(cur_om)) {\n-          os::naked_short_sleep(1);\n-        }\n-        \/\/ Refetch the possibly changed next field and try again.\n-        cur_om = unmarked_next(in_use_tail);\n-        continue;\n-      }\n-      if (cur_om->object_peek() == NULL) {\n-        \/\/ Two reasons for object() to be NULL here:\n-        \/\/ 1) cur_om was deflated and the object ref was cleared while it\n-        \/\/ was locked. We happened to see it just after it was unlocked\n-        \/\/ (and added to the free list).\n-        \/\/ 2) The object has been GC'ed so the association with object is\n-        \/\/ already broken, but we don't want to do the deflation work now.\n-\n-        \/\/ Refetch the possibly changed next field:\n-        ObjectMonitor* in_use_next = unmarked_next(in_use_tail);\n-        if (cur_om != in_use_next) {\n-          \/\/ The NULL is because of async deflation so try again:\n-          cur_om = in_use_next;\n-          continue;\n-        }\n-        \/\/ Implied else: The NULL is because of GC, but we leave the\n-        \/\/ node on the in-use list to be deflated after it has been\n-        \/\/ moved to the global in-use list.\n-      }\n-      in_use_tail = cur_om;\n-      in_use_count++;\n-      cur_om = unmarked_next(cur_om);\n-    }\n-    guarantee(in_use_tail != NULL, \"invariant\");\n-#ifdef ASSERT\n-    int l_om_in_use_count = Atomic::load(&self->om_in_use_count);\n-    assert(l_om_in_use_count == in_use_count, \"in-use counts don't match: \"\n-           \"l_om_in_use_count=%d, in_use_count=%d\", l_om_in_use_count, in_use_count);\n-#endif\n-    Atomic::store(&self->om_in_use_count, 0);\n-    OrderAccess::storestore();  \/\/ Make sure counter update is seen first.\n-    \/\/ Clear the in-use list head (which also unlocks it):\n-    Atomic::store(&self->om_in_use_list, (ObjectMonitor*)NULL);\n-    om_unlock(in_use_list);\n-  }\n-\n-  int free_count = 0;\n-  ObjectMonitor* free_list = NULL;\n-  ObjectMonitor* free_tail = NULL;\n-  \/\/ This function can race with a list walker thread so we lock the\n-  \/\/ list head to prevent confusion.\n-  if ((free_list = get_list_head_locked(&self->om_free_list)) != NULL) {\n-    \/\/ At this point, we have locked the free list head so a racing\n-    \/\/ thread cannot come in after us. However, a racing thread could\n-    \/\/ be ahead of us; we'll detect that and delay to let it finish.\n-    \/\/\n-    \/\/ The thread is going away. Set 'free_tail' to the last per-thread free\n-    \/\/ monitor which will be linked to om_list_globals._free_list below.\n-    \/\/\n-    \/\/ Account for the free list head before the loop since it is\n-    \/\/ already locked (by this thread):\n-    free_tail = free_list;\n-    free_count++;\n-    for (ObjectMonitor* s = unmarked_next(free_list); s != NULL; s = unmarked_next(s)) {\n-      if (is_locked(s)) {\n-        \/\/ s is locked so there must be a racing walker thread ahead\n-        \/\/ of us so we'll give it a chance to finish.\n-        while (is_locked(s)) {\n-          os::naked_short_sleep(1);\n-        }\n-      }\n-      free_tail = s;\n-      free_count++;\n-      guarantee(s->object_peek() == NULL, \"invariant\");\n-      if (s->is_busy()) {\n-        stringStream ss;\n-        fatal(\"must be !is_busy: %s\", s->is_busy_to_string(&ss));\n-      }\n-    }\n-    guarantee(free_tail != NULL, \"invariant\");\n-#ifdef ASSERT\n-    int l_om_free_count = Atomic::load(&self->om_free_count);\n-    assert(l_om_free_count == free_count, \"free counts don't match: \"\n-           \"l_om_free_count=%d, free_count=%d\", l_om_free_count, free_count);\n-#endif\n-    Atomic::store(&self->om_free_count, 0);\n-    OrderAccess::storestore();  \/\/ Make sure counter update is seen first.\n-    Atomic::store(&self->om_free_list, (ObjectMonitor*)NULL);\n-    om_unlock(free_list);\n-  }\n-\n-  if (free_tail != NULL) {\n-    prepend_list_to_global_free_list(free_list, free_tail, free_count);\n-  }\n-\n-  if (in_use_tail != NULL) {\n-    prepend_list_to_global_in_use_list(in_use_list, in_use_tail, in_use_count);\n-  }\n-\n-  LogStreamHandle(Debug, monitorinflation) lsh_debug;\n-  LogStreamHandle(Info, monitorinflation) lsh_info;\n-  LogStream* ls = NULL;\n-  if (log_is_enabled(Debug, monitorinflation)) {\n-    ls = &lsh_debug;\n-  } else if ((free_count != 0 || in_use_count != 0) &&\n-             log_is_enabled(Info, monitorinflation)) {\n-    ls = &lsh_info;\n-  }\n-  if (ls != NULL) {\n-    ls->print_cr(\"om_flush: jt=\" INTPTR_FORMAT \", free_count=%d\"\n-                 \", in_use_count=%d\" \", om_free_provision=%d\",\n-                 p2i(self), free_count, in_use_count, self->om_free_provision);\n-  }\n-}\n-\n@@ -1848,1 +1288,0 @@\n-    assert(ObjectSynchronizer::verify_objmon_isinpool(monitor), \"monitor=\" INTPTR_FORMAT \" is invalid\", p2i(monitor));\n@@ -1880,1 +1319,0 @@\n-      assert(ObjectSynchronizer::verify_objmon_isinpool(inf), \"monitor is invalid\");\n@@ -1904,9 +1342,0 @@\n-    \/\/\n-    \/\/ We now use per-thread private ObjectMonitor free lists.\n-    \/\/ These list are reprovisioned from the global free list outside the\n-    \/\/ critical INFLATING...ST interval.  A thread can transfer\n-    \/\/ multiple ObjectMonitors en-mass from the global free list to its local free list.\n-    \/\/ This reduces coherency traffic and lock contention on the global free list.\n-    \/\/ Using such local free lists, it doesn't matter if the om_alloc() call appears\n-    \/\/ before or after the CAS(INFLATING) operation.\n-    \/\/ See the comments in om_alloc().\n@@ -1917,1 +1346,1 @@\n-      ObjectMonitor* m = om_alloc(self);\n+      ObjectMonitor* m = new ObjectMonitor(object);\n@@ -1921,3 +1350,0 @@\n-      m->Recycle();\n-      m->_Responsible  = NULL;\n-      m->_SpinDuration = ObjectMonitor::Knob_SpinLimit;   \/\/ Consider: maintain by type\/class\n@@ -1927,2 +1353,1 @@\n-        \/\/ om_release() will reset the allocation state from New to Free.\n-        om_release(self, m, true);\n+        delete m;\n@@ -1975,2 +1400,1 @@\n-      m->set_owner_from(NULL, DEFLATER_MARKER, mark.locker());\n-      m->set_object(object);\n+      m->set_owner_from(NULL, mark.locker());\n@@ -1987,3 +1411,1 @@\n-      assert(m->is_new(), \"freshly allocated monitor must be new\");\n-      \/\/ Release semantics needed to keep allocation_state from floating up.\n-      m->release_set_allocation_state(ObjectMonitor::Old);\n+      _in_use_list.add(m);\n@@ -2018,1 +1440,1 @@\n-    ObjectMonitor* m = om_alloc(self);\n+    ObjectMonitor* m = new ObjectMonitor(object);\n@@ -2020,6 +1442,0 @@\n-    m->Recycle();\n-    \/\/ DEFLATER_MARKER is the only non-NULL value we should see here.\n-    m->try_set_owner_from(DEFLATER_MARKER, NULL);\n-    m->set_object(object);\n-    m->_Responsible  = NULL;\n-    m->_SpinDuration = ObjectMonitor::Knob_SpinLimit;       \/\/ consider: keep metastats by type\/class\n@@ -2029,5 +1445,1 @@\n-      m->set_header(markWord::zero());\n-      m->set_object(NULL);\n-      m->Recycle();\n-      \/\/ om_release() will reset the allocation state from New to Free.\n-      om_release(self, m, true);\n+      delete m;\n@@ -2043,4 +1455,1 @@\n-    assert(m->is_new(), \"freshly allocated monitor must be new\");\n-    \/\/ Release semantics are not needed to keep allocation_state from\n-    \/\/ floating up since cas_set_mark() takes care of it.\n-    m->set_allocation_state(ObjectMonitor::Old);\n+    _in_use_list.add(m);\n@@ -2064,43 +1473,5 @@\n-\n-\/\/ An async deflation request is registered with the ServiceThread\n-\/\/ and it is notified.\n-void ObjectSynchronizer::do_safepoint_work() {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"must be at safepoint\");\n-\n-  log_debug(monitorinflation)(\"requesting async deflation of idle monitors.\");\n-  \/\/ Request deflation of idle monitors by the ServiceThread:\n-  set_is_async_deflation_requested(true);\n-  MonitorLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);\n-  ml.notify_all();\n-\n-  if (log_is_enabled(Debug, monitorinflation)) {\n-    \/\/ The VMThread calls do_final_audit_and_print_stats() which calls\n-    \/\/ audit_and_print_stats() at the Info level at VM exit time.\n-    ObjectSynchronizer::audit_and_print_stats(false \/* on_exit *\/);\n-  }\n-}\n-\n-\/\/ Deflate the specified ObjectMonitor if not in-use. Returns true if it\n-\/\/ was deflated and false otherwise.\n-\/\/\n-\/\/ The async deflation protocol sets owner to DEFLATER_MARKER and\n-\/\/ makes contentions negative as signals to contending threads that\n-\/\/ an async deflation is in progress. There are a number of checks\n-\/\/ as part of the protocol to make sure that the calling thread has\n-\/\/ not lost the race to a contending thread.\n-\/\/\n-\/\/ The ObjectMonitor has been successfully async deflated when:\n-\/\/   (contentions < 0)\n-\/\/ Contending threads that see that condition know to retry their operation.\n-\/\/\n-bool ObjectSynchronizer::deflate_monitor(ObjectMonitor* mid,\n-                                         ObjectMonitor** free_head_p,\n-                                         ObjectMonitor** free_tail_p) {\n-  \/\/ A newly allocated ObjectMonitor should not be seen here so we\n-  \/\/ avoid an endless inflate\/deflate cycle.\n-  assert(mid->is_old(), \"must be old: allocation_state=%d\",\n-         (int) mid->allocation_state());\n-\n-  if (mid->is_busy()) {\n-    \/\/ Easy checks are first - the ObjectMonitor is busy so no deflation.\n-    return false;\n+void ObjectSynchronizer::chk_for_block_req(JavaThread* self, const char* op_name,\n+                                           const char* cnt_name, size_t cnt,\n+                                           LogStream* ls, elapsedTimer* timer_p) {\n+  if (!SafepointMechanism::should_process(self)) {\n+    return;\n@@ -2109,41 +1480,7 @@\n-  const oop obj = mid->object_peek();\n-\n-  if (obj == NULL) {\n-    \/\/ If the object died, we can recycle the monitor without racing with\n-    \/\/ Java threads. The GC already broke the association with the object.\n-    mid->set_owner_from(NULL, DEFLATER_MARKER);\n-    mid->_contentions = -max_jint;\n-  } else {\n-    \/\/ Set a NULL owner to DEFLATER_MARKER to force any contending thread\n-    \/\/ through the slow path. This is just the first part of the async\n-    \/\/ deflation dance.\n-    if (mid->try_set_owner_from(NULL, DEFLATER_MARKER) != NULL) {\n-      \/\/ The owner field is no longer NULL so we lost the race since the\n-      \/\/ ObjectMonitor is now busy.\n-      return false;\n-    }\n-\n-    if (mid->contentions() > 0 || mid->_waiters != 0) {\n-      \/\/ Another thread has raced to enter the ObjectMonitor after\n-      \/\/ mid->is_busy() above or has already entered and waited on\n-      \/\/ it which makes it busy so no deflation. Restore owner to\n-      \/\/ NULL if it is still DEFLATER_MARKER.\n-      if (mid->try_set_owner_from(DEFLATER_MARKER, NULL) != DEFLATER_MARKER) {\n-        \/\/ Deferred decrement for the JT EnterI() that cancelled the async deflation.\n-        mid->add_to_contentions(-1);\n-      }\n-      return false;\n-    }\n-\n-    \/\/ Make a zero contentions field negative to force any contending threads\n-    \/\/ to retry. This is the second part of the async deflation dance.\n-    if (Atomic::cmpxchg(&mid->_contentions, (jint)0, -max_jint) != 0) {\n-      \/\/ Contentions was no longer 0 so we lost the race since the\n-      \/\/ ObjectMonitor is now busy. Restore owner to NULL if it is\n-      \/\/ still DEFLATER_MARKER:\n-      if (mid->try_set_owner_from(DEFLATER_MARKER, NULL) != DEFLATER_MARKER) {\n-        \/\/ Deferred decrement for the JT EnterI() that cancelled the async deflation.\n-        mid->add_to_contentions(-1);\n-      }\n-      return false;\n-    }\n+  \/\/ A safepoint\/handshake has started.\n+  if (ls != NULL) {\n+    timer_p->stop();\n+    ls->print_cr(\"pausing %s: %s=\" SIZE_FORMAT \", in_use_list stats: ceiling=\"\n+                 SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                 op_name, cnt_name, cnt, in_use_list_ceiling(),\n+                 _in_use_list.count(), _in_use_list.max());\n@@ -2152,22 +1489,3 @@\n-  \/\/ Sanity checks for the races:\n-  guarantee(mid->owner_is_DEFLATER_MARKER(), \"must be deflater marker\");\n-  guarantee(mid->contentions() < 0, \"must be negative: contentions=%d\",\n-            mid->contentions());\n-  guarantee(mid->_waiters == 0, \"must be 0: waiters=%d\", mid->_waiters);\n-  guarantee(mid->_cxq == NULL, \"must be no contending threads: cxq=\"\n-            INTPTR_FORMAT, p2i(mid->_cxq));\n-  guarantee(mid->_EntryList == NULL,\n-            \"must be no entering threads: EntryList=\" INTPTR_FORMAT,\n-            p2i(mid->_EntryList));\n-\n-  if (obj != NULL) {\n-    if (log_is_enabled(Trace, monitorinflation)) {\n-      ResourceMark rm;\n-      log_trace(monitorinflation)(\"deflate_monitor: object=\" INTPTR_FORMAT\n-                                  \", mark=\" INTPTR_FORMAT \", type='%s'\",\n-                                  p2i(obj), obj->mark().value(),\n-                                  obj->klass()->external_name());\n-    }\n-\n-    \/\/ Install the old mark word if nobody else has already done it.\n-    mid->install_displaced_markword_in_object(obj);\n+  {\n+    \/\/ Honor block request.\n+    ThreadBlockInVM tbivm(self);\n@@ -2175,23 +1493,5 @@\n-  mid->clear_common();\n-  assert(mid->object_peek() == NULL, \"must be NULL: object=\" INTPTR_FORMAT,\n-         p2i(mid->object_peek()));\n-  assert(mid->is_free(), \"must be free: allocation_state=%d\",\n-         (int)mid->allocation_state());\n-\n-  \/\/ Move the deflated ObjectMonitor to the working free list\n-  \/\/ defined by free_head_p and free_tail_p.\n-  if (*free_head_p == NULL) {\n-    \/\/ First one on the list.\n-    *free_head_p = mid;\n-  }\n-  if (*free_tail_p != NULL) {\n-    \/\/ We append to the list so the caller can use mid->_next_om\n-    \/\/ to fix the linkages in its context.\n-    ObjectMonitor* prevtail = *free_tail_p;\n-    \/\/ prevtail should have been cleaned up by the caller:\n-#ifdef ASSERT\n-    ObjectMonitor* l_next_om = unmarked_next(prevtail);\n-    assert(l_next_om == NULL, \"must be NULL: _next_om=\" INTPTR_FORMAT, p2i(l_next_om));\n-#endif\n-    om_lock(prevtail);\n-    prevtail->set_next_om(mid);  \/\/ prevtail now points to mid (and is unlocked)\n+  if (ls != NULL) {\n+    ls->print_cr(\"resuming %s: in_use_list stats: ceiling=\" SIZE_FORMAT\n+                 \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT, op_name,\n+                 in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n+    timer_p->start();\n@@ -2200,10 +1500,0 @@\n-  *free_tail_p = mid;\n-\n-  \/\/ At this point, mid->_next_om still refers to its current\n-  \/\/ value and another ObjectMonitor's _next_om field still\n-  \/\/ refers to this ObjectMonitor. Those linkages have to be\n-  \/\/ cleaned up by the caller who has the complete context.\n-\n-  \/\/ We leave owner == DEFLATER_MARKER and contentions < 0\n-  \/\/ to force any racing threads to retry.\n-  return true;  \/\/ Success, ObjectMonitor has been deflated.\n@@ -2212,18 +1502,6 @@\n-\/\/ Walk a given ObjectMonitor list and deflate idle ObjectMonitors.\n-\/\/ Returns the number of deflated ObjectMonitors. The given\n-\/\/ list could be a per-thread in-use list or the global in-use list.\n-\/\/ If self is a JavaThread and a safepoint has started, then we save state\n-\/\/ via saved_mid_in_use_p and return to the caller to honor the safepoint.\n-\/\/\n-int ObjectSynchronizer::deflate_monitor_list(Thread* self,\n-                                             ObjectMonitor** list_p,\n-                                             int* count_p,\n-                                             ObjectMonitor** free_head_p,\n-                                             ObjectMonitor** free_tail_p,\n-                                             ObjectMonitor** saved_mid_in_use_p) {\n-  ObjectMonitor* cur_mid_in_use = NULL;\n-  ObjectMonitor* mid = NULL;\n-  ObjectMonitor* next = NULL;\n-  ObjectMonitor* next_next = NULL;\n-  int deflated_count = 0;\n-  NoSafepointVerifier nsv;\n+\/\/ Walk the in-use list and deflate (at most MonitorDeflationMax) idle\n+\/\/ ObjectMonitors. Returns the number of deflated ObjectMonitors.\n+size_t ObjectSynchronizer::deflate_monitor_list(Thread *self, LogStream* ls,\n+                                                elapsedTimer* timer_p) {\n+  MonitorList::Iterator iter = _in_use_list.iterator();\n+  size_t deflated_count = 0;\n@@ -2231,39 +1509,3 @@\n-  \/\/ We use the more complicated lock-cur_mid_in_use-and-mid-as-we-go\n-  \/\/ protocol because om_release() can do list deletions in parallel;\n-  \/\/ this also prevents races with a list walker thread. We also\n-  \/\/ lock-next-next-as-we-go to prevent an om_flush() that is behind\n-  \/\/ this thread from passing us.\n-  if (*saved_mid_in_use_p == NULL) {\n-    \/\/ No saved state so start at the beginning.\n-    \/\/ Lock the list head so we can possibly deflate it:\n-    if ((mid = get_list_head_locked(list_p)) == NULL) {\n-      return 0;  \/\/ The list is empty so nothing to deflate.\n-    }\n-    next = unmarked_next(mid);\n-  } else {\n-    \/\/ We're restarting after a safepoint so restore the necessary state\n-    \/\/ before we resume.\n-    cur_mid_in_use = *saved_mid_in_use_p;\n-    \/\/ Lock cur_mid_in_use so we can possibly update its\n-    \/\/ next field to extract a deflated ObjectMonitor.\n-    om_lock(cur_mid_in_use);\n-    mid = unmarked_next(cur_mid_in_use);\n-    if (mid == NULL) {\n-      om_unlock(cur_mid_in_use);\n-      *saved_mid_in_use_p = NULL;\n-      return 0;  \/\/ The remainder is empty so nothing more to deflate.\n-    }\n-    \/\/ Lock mid so we can possibly deflate it:\n-    om_lock(mid);\n-    next = unmarked_next(mid);\n-  }\n-\n-  while (true) {\n-    \/\/ The current mid is locked at this point. If we have a\n-    \/\/ cur_mid_in_use, then it is also locked at this point.\n-\n-    if (next != NULL) {\n-      \/\/ We lock next so that an om_flush() thread that is behind us\n-      \/\/ cannot pass us when we unlock the current mid.\n-      om_lock(next);\n-      next_next = unmarked_next(next);\n+  while (iter.has_next()) {\n+    if (deflated_count >= (size_t)MonitorDeflationMax) {\n+      break;\n@@ -2271,23 +1513,2 @@\n-\n-    \/\/ Only try to deflate if mid is old (is not newly allocated and\n-    \/\/ is not newly freed).\n-    if (mid->is_old() && deflate_monitor(mid, free_head_p, free_tail_p)) {\n-      \/\/ Deflation succeeded and already updated free_head_p and\n-      \/\/ free_tail_p as needed. Finish the move to the local free list\n-      \/\/ by unlinking mid from the global or per-thread in-use list.\n-      if (cur_mid_in_use == NULL) {\n-        \/\/ mid is the list head and it is locked. Switch the list head\n-        \/\/ to next which is also locked (if not NULL) and also leave\n-        \/\/ mid locked. Release semantics needed since not all code paths\n-        \/\/ in deflate_monitor() ensure memory consistency.\n-        Atomic::release_store(list_p, next);\n-      } else {\n-        ObjectMonitor* locked_next = mark_om_ptr(next);\n-        \/\/ mid and cur_mid_in_use are locked. Switch cur_mid_in_use's\n-        \/\/ next field to locked_next and also leave mid locked.\n-        \/\/ Release semantics needed since not all code paths in\n-        \/\/ deflate_monitor() ensure memory consistency.\n-        cur_mid_in_use->release_set_next_om(locked_next);\n-      }\n-      \/\/ At this point mid is disconnected from the in-use list so\n-      \/\/ its lock longer has any effects on in-use list.\n+    ObjectMonitor* mid = iter.next();\n+    if (mid->deflate_monitor()) {\n@@ -2295,51 +1516,0 @@\n-      Atomic::dec(count_p);\n-      \/\/ mid is current tail in the free_head_p list so NULL terminate\n-      \/\/ it (which also unlocks it). No release semantics needed since\n-      \/\/ Atomic::dec() already provides it.\n-      mid->set_next_om(NULL);\n-\n-      \/\/ All the list management is done so move on to the next one:\n-      mid = next;  \/\/ mid keeps non-NULL next's locked state\n-      next = next_next;\n-    } else {\n-      \/\/ mid is considered in-use if mid is not old or deflation did not\n-      \/\/ succeed. A mid->is_new() node can be seen here when it is freshly\n-      \/\/ returned by om_alloc() (and skips the deflation code path).\n-      \/\/ A mid->is_old() node can be seen here when deflation failed.\n-      \/\/ A mid->is_free() node can be seen here when a fresh node from\n-      \/\/ om_alloc() is released by om_release() due to losing the race\n-      \/\/ in inflate().\n-\n-      \/\/ All the list management is done so move on to the next one:\n-      if (cur_mid_in_use != NULL) {\n-        om_unlock(cur_mid_in_use);\n-      }\n-      \/\/ The next cur_mid_in_use keeps mid's lock state so\n-      \/\/ that it is stable for a possible next field change. It\n-      \/\/ cannot be modified by om_release() while it is locked.\n-      cur_mid_in_use = mid;\n-      mid = next;  \/\/ mid keeps non-NULL next's locked state\n-      next = next_next;\n-\n-      if (self->is_Java_thread() &&\n-          SafepointMechanism::should_process(self->as_Java_thread()) &&\n-          \/\/ Acquire semantics are not needed on this list load since\n-          \/\/ it is not dependent on the following load which does have\n-          \/\/ acquire semantics.\n-          cur_mid_in_use != Atomic::load(list_p) && cur_mid_in_use->is_old()) {\n-        \/\/ If a safepoint has started and cur_mid_in_use is not the list\n-        \/\/ head and is old, then it is safe to use as saved state. Return\n-        \/\/ to the caller before blocking.\n-        *saved_mid_in_use_p = cur_mid_in_use;\n-        om_unlock(cur_mid_in_use);\n-        if (mid != NULL) {\n-          om_unlock(mid);\n-        }\n-        return deflated_count;\n-      }\n-    }\n-    if (mid == NULL) {\n-      if (cur_mid_in_use != NULL) {\n-        om_unlock(cur_mid_in_use);\n-      }\n-      break;  \/\/ Reached end of the list so nothing more to deflate.\n@@ -2348,2 +1518,5 @@\n-    \/\/ The current mid's next field is locked at this point. If we have\n-    \/\/ a cur_mid_in_use, then it is also locked at this point.\n+    if (self->is_Java_thread()) {\n+      \/\/ A JavaThread must check for a safepoint\/handshake and honor it.\n+      chk_for_block_req(self->as_Java_thread(), \"deflation\", \"deflated_count\",\n+                        deflated_count, ls, timer_p);\n+    }\n@@ -2351,3 +1524,1 @@\n-  \/\/ We finished the list without a safepoint starting so there's\n-  \/\/ no need to save state.\n-  *saved_mid_in_use_p = NULL;\n+\n@@ -2367,3 +1538,4 @@\n-\/\/ This function is called by the ServiceThread to deflate monitors.\n-\/\/ It is also called by do_final_audit_and_print_stats() by the VMThread.\n-void ObjectSynchronizer::deflate_idle_monitors() {\n+\/\/ This function is called by the MonitorDeflationThread to deflate\n+\/\/ ObjectMonitors. It is also called via do_final_audit_and_print_stats()\n+\/\/ by the VMThread.\n+size_t ObjectSynchronizer::deflate_idle_monitors() {\n@@ -2371,26 +1543,0 @@\n-  \/\/ Deflate any global idle monitors.\n-  deflate_global_idle_monitors(self);\n-\n-  int count = 0;\n-  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {\n-    if (Atomic::load(&jt->om_in_use_count) > 0 && !jt->is_exiting()) {\n-      \/\/ This JavaThread is using ObjectMonitors so deflate any that\n-      \/\/ are idle unless this JavaThread is exiting; do not race with\n-      \/\/ ObjectSynchronizer::om_flush().\n-      deflate_per_thread_idle_monitors(self, jt);\n-      count++;\n-    }\n-  }\n-  if (count > 0) {\n-    log_debug(monitorinflation)(\"did async deflation of idle monitors for %d thread(s).\", count);\n-  }\n-\n-  log_info(monitorinflation)(\"async global_population=%d, global_in_use_count=%d, \"\n-                             \"global_free_count=%d, global_wait_count=%d\",\n-                             Atomic::load(&om_list_globals._population),\n-                             Atomic::load(&om_list_globals._in_use_count),\n-                             Atomic::load(&om_list_globals._free_count),\n-                             Atomic::load(&om_list_globals._wait_count));\n-\n-  GVars.stw_random = os::random();\n-\n@@ -2403,37 +1549,7 @@\n-  if (Atomic::load(&om_list_globals._wait_count) > 0) {\n-    \/\/ There are deflated ObjectMonitors waiting for a handshake\n-    \/\/ (or a safepoint) for safety.\n-\n-    ObjectMonitor* list = Atomic::load(&om_list_globals._wait_list);\n-    assert(list != NULL, \"om_list_globals._wait_list must not be NULL\");\n-    int count = Atomic::load(&om_list_globals._wait_count);\n-    Atomic::store(&om_list_globals._wait_count, 0);\n-    OrderAccess::storestore();  \/\/ Make sure counter update is seen first.\n-    Atomic::store(&om_list_globals._wait_list, (ObjectMonitor*)NULL);\n-\n-    \/\/ Find the tail for prepend_list_to_common(). No need to mark\n-    \/\/ ObjectMonitors for this list walk since only the deflater\n-    \/\/ thread manages the wait list.\n-#ifdef ASSERT\n-    int l_count = 0;\n-#endif\n-    ObjectMonitor* tail = NULL;\n-    for (ObjectMonitor* n = list; n != NULL; n = unmarked_next(n)) {\n-      tail = n;\n-#ifdef ASSERT\n-      l_count++;\n-#endif\n-    }\n-    assert(count == l_count, \"count=%d != l_count=%d\", count, l_count);\n-\n-    if (self->is_Java_thread()) {\n-      \/\/ A JavaThread needs to handshake in order to safely free the\n-      \/\/ monitors that were deflated in this cycle.\n-      HandshakeForDeflation hfd_hc;\n-      Handshake::execute(&hfd_hc);\n-   }\n-\n-    prepend_list_to_common(list, tail, count, &om_list_globals._free_list,\n-                           &om_list_globals._free_count);\n-\n-    log_info(monitorinflation)(\"moved %d idle monitors from global waiting list to global free list\", count);\n+  LogStreamHandle(Debug, monitorinflation) lsh_debug;\n+  LogStreamHandle(Info, monitorinflation) lsh_info;\n+  LogStream* ls = NULL;\n+  if (log_is_enabled(Debug, monitorinflation)) {\n+    ls = &lsh_debug;\n+  } else if (log_is_enabled(Info, monitorinflation)) {\n+    ls = &lsh_info;\n@@ -2441,25 +1557,3 @@\n-}\n-\n-\/\/ Deflate global idle ObjectMonitors.\n-\/\/\n-void ObjectSynchronizer::deflate_global_idle_monitors(Thread* self) {\n-  deflate_common_idle_monitors(self, true \/* is_global *\/, NULL \/* target *\/);\n-}\n-\n-\/\/ Deflate the specified JavaThread's idle ObjectMonitors.\n-\/\/\n-void ObjectSynchronizer::deflate_per_thread_idle_monitors(Thread* self,\n-                                                          JavaThread* target) {\n-  deflate_common_idle_monitors(self, false \/* !is_global *\/, target);\n-}\n-\/\/ Deflate global or per-thread idle ObjectMonitors.\n-\/\/\n-void ObjectSynchronizer::deflate_common_idle_monitors(Thread* self,\n-                                                      bool is_global,\n-                                                      JavaThread* target) {\n-  int deflated_count = 0;\n-  ObjectMonitor* free_head_p = NULL;  \/\/ Local SLL of scavenged ObjectMonitors\n-  ObjectMonitor* free_tail_p = NULL;\n-  ObjectMonitor* saved_mid_in_use_p = NULL;\n-\n-  if (log_is_enabled(Info, monitorinflation)) {\n+  if (ls != NULL) {\n+    ls->print_cr(\"begin deflating: in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                 in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n@@ -2471,5 +1565,7 @@\n-  if (is_global) {\n-    OM_PERFDATA_OP(MonExtant, set_value(Atomic::load(&om_list_globals._in_use_count)));\n-  } else {\n-    OM_PERFDATA_OP(MonExtant, inc(Atomic::load(&target->om_in_use_count)));\n-  }\n+  \/\/ Deflate some idle ObjectMonitors.\n+  size_t deflated_count = deflate_monitor_list(self, ls, &timer);\n+  if (deflated_count > 0 || is_final_audit()) {\n+    \/\/ There are ObjectMonitors that have been deflated or this is the\n+    \/\/ final audit and all the remaining ObjectMonitors have been\n+    \/\/ deflated, BUT the MonitorDeflationThread blocked for the final\n+    \/\/ safepoint during unlinking.\n@@ -2477,33 +1573,14 @@\n-  do {\n-    int local_deflated_count;\n-    if (is_global) {\n-      local_deflated_count =\n-          deflate_monitor_list(self, &om_list_globals._in_use_list,\n-                               &om_list_globals._in_use_count,\n-                               &free_head_p, &free_tail_p,\n-                               &saved_mid_in_use_p);\n-    } else {\n-      local_deflated_count =\n-          deflate_monitor_list(self, &target->om_in_use_list,\n-                               &target->om_in_use_count, &free_head_p,\n-                               &free_tail_p, &saved_mid_in_use_p);\n-    }\n-    deflated_count += local_deflated_count;\n-\n-    if (free_head_p != NULL) {\n-      \/\/ Move the deflated ObjectMonitors to the global free list.\n-      guarantee(free_tail_p != NULL && local_deflated_count > 0, \"free_tail_p=\" INTPTR_FORMAT \", local_deflated_count=%d\", p2i(free_tail_p), local_deflated_count);\n-      \/\/ Note: The target thread can be doing an om_alloc() that\n-      \/\/ is trying to prepend an ObjectMonitor on its in-use list\n-      \/\/ at the same time that we have deflated the current in-use\n-      \/\/ list head and put it on the local free list. prepend_to_common()\n-      \/\/ will detect the race and retry which avoids list corruption,\n-      \/\/ but the next field in free_tail_p can flicker to marked\n-      \/\/ and then unmarked while prepend_to_common() is sorting it\n-      \/\/ all out.\n-#ifdef ASSERT\n-      ObjectMonitor* l_next_om = unmarked_next(free_tail_p);\n-      assert(l_next_om == NULL, \"must be NULL: _next_om=\" INTPTR_FORMAT, p2i(l_next_om));\n-#endif\n-\n-      prepend_list_to_global_wait_list(free_head_p, free_tail_p, local_deflated_count);\n+    \/\/ Unlink deflated ObjectMonitors from the in-use list.\n+    ResourceMark rm;\n+    GrowableArray<ObjectMonitor*> delete_list((int)deflated_count);\n+    size_t unlinked_count = _in_use_list.unlink_deflated(self, ls, &timer,\n+                                                         &delete_list);\n+    if (self->is_Java_thread()) {\n+      if (ls != NULL) {\n+        timer.stop();\n+        ls->print_cr(\"before handshaking: unlinked_count=\" SIZE_FORMAT\n+                     \", in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\"\n+                     SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                     unlinked_count, in_use_list_ceiling(),\n+                     _in_use_list.count(), _in_use_list.max());\n+      }\n@@ -2511,2 +1588,4 @@\n-      OM_PERFDATA_OP(Deflations, inc(local_deflated_count));\n-    }\n+      \/\/ A JavaThread needs to handshake in order to safely free the\n+      \/\/ ObjectMonitors that were deflated in this cycle.\n+      HandshakeForDeflation hfd_hc;\n+      Handshake::execute(&hfd_hc);\n@@ -2514,18 +1593,4 @@\n-    if (saved_mid_in_use_p != NULL) {\n-      \/\/ deflate_monitor_list() detected a safepoint starting.\n-      timer.stop();\n-      {\n-        if (is_global) {\n-          log_debug(monitorinflation)(\"pausing deflation of global idle monitors for a safepoint.\");\n-        } else {\n-          log_debug(monitorinflation)(\"jt=\" INTPTR_FORMAT \": pausing deflation of per-thread idle monitors for a safepoint.\", p2i(target));\n-        }\n-        assert(self->is_Java_thread() &&\n-               SafepointMechanism::should_process(self->as_Java_thread()),\n-               \"sanity check\");\n-        ThreadBlockInVM blocker(self->as_Java_thread());\n-      }\n-      \/\/ Prepare for another loop after the safepoint.\n-      free_head_p = NULL;\n-      free_tail_p = NULL;\n-      if (log_is_enabled(Info, monitorinflation)) {\n+      if (ls != NULL) {\n+        ls->print_cr(\"after handshaking: in_use_list stats: ceiling=\"\n+                     SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                     in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n@@ -2535,9 +1600,13 @@\n-  } while (saved_mid_in_use_p != NULL);\n-  timer.stop();\n-  LogStreamHandle(Debug, monitorinflation) lsh_debug;\n-  LogStreamHandle(Info, monitorinflation) lsh_info;\n-  LogStream* ls = NULL;\n-  if (log_is_enabled(Debug, monitorinflation)) {\n-    ls = &lsh_debug;\n-  } else if (deflated_count != 0 && log_is_enabled(Info, monitorinflation)) {\n-    ls = &lsh_info;\n+    \/\/ After the handshake, safely free the ObjectMonitors that were\n+    \/\/ deflated in this cycle.\n+    size_t deleted_count = 0;\n+    for (ObjectMonitor* monitor: delete_list) {\n+      delete monitor;\n+      deleted_count++;\n+\n+      if (self->is_Java_thread()) {\n+        \/\/ A JavaThread must check for a safepoint\/handshake and honor it.\n+        chk_for_block_req(self->as_Java_thread(), \"deletion\", \"deleted_count\",\n+                          deleted_count, ls, &timer);\n+      }\n+    }\n@@ -2546,0 +1615,1 @@\n+\n@@ -2547,4 +1617,4 @@\n-    if (is_global) {\n-      ls->print_cr(\"async-deflating global idle monitors, %3.7f secs, %d monitors\", timer.seconds(), deflated_count);\n-    } else {\n-      ls->print_cr(\"jt=\" INTPTR_FORMAT \": async-deflating per-thread idle monitors, %3.7f secs, %d monitors\", p2i(target), timer.seconds(), deflated_count);\n+    timer.stop();\n+    if (deflated_count != 0 || log_is_enabled(Debug, monitorinflation)) {\n+      ls->print_cr(\"deflated \" SIZE_FORMAT \" monitors in %3.7f secs\",\n+                   deflated_count, timer.seconds());\n@@ -2552,0 +1622,2 @@\n+    ls->print_cr(\"end deflating: in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                 in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n@@ -2553,0 +1625,7 @@\n+\n+  OM_PERFDATA_OP(MonExtant, set_value(_in_use_list.count()));\n+  OM_PERFDATA_OP(Deflations, inc(deflated_count));\n+\n+  GVars.stw_random = os::random();\n+\n+  return deflated_count;\n@@ -2630,0 +1709,23 @@\n+\/\/ Do the final audit and print of ObjectMonitor stats; must be done\n+\/\/ by the VMThread at VM exit time.\n+void ObjectSynchronizer::do_final_audit_and_print_stats() {\n+  assert(Thread::current()->is_VM_thread(), \"sanity check\");\n+\n+  if (is_final_audit()) {  \/\/ Only do the audit once.\n+    return;\n+  }\n+  set_is_final_audit();\n+\n+  if (log_is_enabled(Info, monitorinflation)) {\n+    \/\/ Do a deflation in order to reduce the in-use monitor population\n+    \/\/ that is reported by ObjectSynchronizer::log_in_use_monitor_details()\n+    \/\/ which is called by ObjectSynchronizer::audit_and_print_stats().\n+    while (ObjectSynchronizer::deflate_idle_monitors() != 0) {\n+      ; \/\/ empty\n+    }\n+    \/\/ The other audit_and_print_stats() call is done at the Debug\n+    \/\/ level at a safepoint in ObjectSynchronizer::do_safepoint_work().\n+    ObjectSynchronizer::audit_and_print_stats(true \/* on_exit *\/);\n+  }\n+}\n+\n@@ -2656,2 +1758,0 @@\n-  \/\/ Log counts for the global and per-thread monitor lists:\n-  int chk_om_population = log_monitor_list_counts(ls);\n@@ -2660,35 +1760,2 @@\n-  ls->print_cr(\"Checking global lists:\");\n-\n-  \/\/ Check om_list_globals._population:\n-  if (Atomic::load(&om_list_globals._population) == chk_om_population) {\n-    ls->print_cr(\"global_population=%d equals chk_om_population=%d\",\n-                 Atomic::load(&om_list_globals._population), chk_om_population);\n-  } else {\n-    \/\/ With fine grained locks on the monitor lists, it is possible for\n-    \/\/ log_monitor_list_counts() to return a value that doesn't match\n-    \/\/ om_list_globals._population. So far a higher value has been\n-    \/\/ seen in testing so something is being double counted by\n-    \/\/ log_monitor_list_counts().\n-    ls->print_cr(\"WARNING: global_population=%d is not equal to \"\n-                 \"chk_om_population=%d\",\n-                 Atomic::load(&om_list_globals._population), chk_om_population);\n-  }\n-\n-  \/\/ Check om_list_globals._in_use_list and om_list_globals._in_use_count:\n-  chk_global_in_use_list_and_count(ls, &error_cnt);\n-\n-  \/\/ Check om_list_globals._free_list and om_list_globals._free_count:\n-  chk_global_free_list_and_count(ls, &error_cnt);\n-\n-  \/\/ Check om_list_globals._wait_list and om_list_globals._wait_count:\n-  chk_global_wait_list_and_count(ls, &error_cnt);\n-\n-  ls->print_cr(\"Checking per-thread lists:\");\n-\n-  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {\n-    \/\/ Check om_in_use_list and om_in_use_count:\n-    chk_per_thread_in_use_list_and_count(jt, ls, &error_cnt);\n-\n-    \/\/ Check om_free_list and om_free_count:\n-    chk_per_thread_free_list_and_count(jt, ls, &error_cnt);\n-  }\n+  ls->print_cr(\"Checking in_use_list:\");\n+  chk_in_use_list(ls, &error_cnt);\n@@ -2697,1 +1764,1 @@\n-    ls->print_cr(\"No errors found in monitor list checks.\");\n+    ls->print_cr(\"No errors found in in_use_list checks.\");\n@@ -2699,1 +1766,1 @@\n-    log_error(monitorinflation)(\"found monitor list errors: error_cnt=%d\", error_cnt);\n+    log_error(monitorinflation)(\"found in_use_list errors: error_cnt=%d\", error_cnt);\n@@ -2715,56 +1782,6 @@\n-\/\/ Check a free monitor entry; log any errors.\n-void ObjectSynchronizer::chk_free_entry(JavaThread* jt, ObjectMonitor* n,\n-                                        outputStream * out, int *error_cnt_p) {\n-  stringStream ss;\n-  if (n->is_busy()) {\n-    if (jt != NULL) {\n-      out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \", monitor=\" INTPTR_FORMAT\n-                    \": free per-thread monitor must not be busy: %s\", p2i(jt),\n-                    p2i(n), n->is_busy_to_string(&ss));\n-    } else {\n-      out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": free global monitor \"\n-                    \"must not be busy: %s\", p2i(n), n->is_busy_to_string(&ss));\n-    }\n-    *error_cnt_p = *error_cnt_p + 1;\n-  }\n-  if (n->header().value() != 0) {\n-    if (jt != NULL) {\n-      out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \", monitor=\" INTPTR_FORMAT\n-                    \": free per-thread monitor must have NULL _header \"\n-                    \"field: _header=\" INTPTR_FORMAT, p2i(jt), p2i(n),\n-                    n->header().value());\n-      *error_cnt_p = *error_cnt_p + 1;\n-    }\n-  }\n-  if (n->object_peek() != NULL) {\n-    if (jt != NULL) {\n-      out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \", monitor=\" INTPTR_FORMAT\n-                    \": free per-thread monitor must have NULL _object \"\n-                    \"field: _object=\" INTPTR_FORMAT, p2i(jt), p2i(n),\n-                    p2i(n->object_peek()));\n-    } else {\n-      out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": free global monitor \"\n-                    \"must have NULL _object field: _object=\" INTPTR_FORMAT,\n-                    p2i(n), p2i(n->object_peek()));\n-    }\n-    *error_cnt_p = *error_cnt_p + 1;\n-  }\n-}\n-\n-\/\/ Lock the next ObjectMonitor for traversal and unlock the current\n-\/\/ ObjectMonitor. Returns the next ObjectMonitor if there is one.\n-\/\/ Otherwise returns NULL (after unlocking the current ObjectMonitor).\n-\/\/ This function is used by the various list walker functions to\n-\/\/ safely walk a list without allowing an ObjectMonitor to be moved\n-\/\/ to another list in the middle of a walk.\n-static ObjectMonitor* lock_next_for_traversal(ObjectMonitor* cur) {\n-  assert(is_locked(cur), \"cur=\" INTPTR_FORMAT \" must be locked\", p2i(cur));\n-  ObjectMonitor* next = unmarked_next(cur);\n-  if (next == NULL) {  \/\/ Reached the end of the list.\n-    om_unlock(cur);\n-    return NULL;\n-  }\n-  om_lock(next);   \/\/ Lock next before unlocking current to keep\n-  om_unlock(cur);  \/\/ from being by-passed by another thread.\n-  return next;\n-}\n+\/\/ Check the in_use_list; log the results of the checks.\n+void ObjectSynchronizer::chk_in_use_list(outputStream* out, int *error_cnt_p) {\n+  size_t l_in_use_count = _in_use_list.count();\n+  size_t l_in_use_max = _in_use_list.max();\n+  out->print_cr(\"count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT, l_in_use_count,\n+                l_in_use_max);\n@@ -2772,16 +1789,6 @@\n-\/\/ Check the global free list and count; log the results of the checks.\n-void ObjectSynchronizer::chk_global_free_list_and_count(outputStream * out,\n-                                                        int *error_cnt_p) {\n-  int chk_om_free_count = 0;\n-  ObjectMonitor* cur = NULL;\n-  if ((cur = get_list_head_locked(&om_list_globals._free_list)) != NULL) {\n-    \/\/ Marked the global free list head so process the list.\n-    while (true) {\n-      chk_free_entry(NULL \/* jt *\/, cur, out, error_cnt_p);\n-      chk_om_free_count++;\n-\n-      cur = lock_next_for_traversal(cur);\n-      if (cur == NULL) {\n-        break;\n-      }\n-    }\n+  size_t ck_in_use_count = 0;\n+  MonitorList::Iterator iter = _in_use_list.iterator();\n+  while (iter.has_next()) {\n+    ObjectMonitor* mid = iter.next();\n+    chk_in_use_entry(mid, out, error_cnt_p);\n+    ck_in_use_count++;\n@@ -2789,36 +1796,3 @@\n-  int l_free_count = Atomic::load(&om_list_globals._free_count);\n-  if (l_free_count == chk_om_free_count) {\n-    out->print_cr(\"global_free_count=%d equals chk_om_free_count=%d\",\n-                  l_free_count, chk_om_free_count);\n-  } else {\n-    \/\/ With fine grained locks on om_list_globals._free_list, it\n-    \/\/ is possible for an ObjectMonitor to be prepended to\n-    \/\/ om_list_globals._free_list after we started calculating\n-    \/\/ chk_om_free_count so om_list_globals._free_count may not\n-    \/\/ match anymore.\n-    out->print_cr(\"WARNING: global_free_count=%d is not equal to \"\n-                  \"chk_om_free_count=%d\", l_free_count, chk_om_free_count);\n-  }\n-}\n-\/\/ Check the global wait list and count; log the results of the checks.\n-void ObjectSynchronizer::chk_global_wait_list_and_count(outputStream * out,\n-                                                        int *error_cnt_p) {\n-  int chk_om_wait_count = 0;\n-  ObjectMonitor* cur = NULL;\n-  if ((cur = get_list_head_locked(&om_list_globals._wait_list)) != NULL) {\n-    \/\/ Marked the global wait list head so process the list.\n-    while (true) {\n-      \/\/ Rules for om_list_globals._wait_list are the same as for\n-      \/\/ om_list_globals._free_list:\n-      chk_free_entry(NULL \/* jt *\/, cur, out, error_cnt_p);\n-      chk_om_wait_count++;\n-\n-      cur = lock_next_for_traversal(cur);\n-      if (cur == NULL) {\n-        break;\n-      }\n-    }\n-  }\n-  if (Atomic::load(&om_list_globals._wait_count) == chk_om_wait_count) {\n-    out->print_cr(\"global_wait_count=%d equals chk_om_wait_count=%d\",\n-                  Atomic::load(&om_list_globals._wait_count), chk_om_wait_count);\n+  if (l_in_use_count == ck_in_use_count) {\n+    out->print_cr(\"in_use_count=\" SIZE_FORMAT \" equals ck_in_use_count=\"\n+                  SIZE_FORMAT, l_in_use_count, ck_in_use_count);\n@@ -2827,4 +1801,3 @@\n-    out->print_cr(\"ERROR: global_wait_count=%d is not equal to \"\n-                  \"chk_om_wait_count=%d\",\n-                  Atomic::load(&om_list_globals._wait_count), chk_om_wait_count);\n-    *error_cnt_p = *error_cnt_p + 1;\n+    out->print_cr(\"WARNING: in_use_count=\" SIZE_FORMAT \" is not equal to \"\n+                  \"ck_in_use_count=\" SIZE_FORMAT, l_in_use_count,\n+                  ck_in_use_count);\n@@ -2832,22 +1805,4 @@\n-}\n-\/\/ Check the global in-use list and count; log the results of the checks.\n-void ObjectSynchronizer::chk_global_in_use_list_and_count(outputStream * out,\n-                                                          int *error_cnt_p) {\n-  int chk_om_in_use_count = 0;\n-  ObjectMonitor* cur = NULL;\n-  if ((cur = get_list_head_locked(&om_list_globals._in_use_list)) != NULL) {\n-    \/\/ Marked the global in-use list head so process the list.\n-    while (true) {\n-      chk_in_use_entry(NULL \/* jt *\/, cur, out, error_cnt_p);\n-      chk_om_in_use_count++;\n-\n-      cur = lock_next_for_traversal(cur);\n-      if (cur == NULL) {\n-        break;\n-      }\n-    }\n-  }\n-  int l_in_use_count = Atomic::load(&om_list_globals._in_use_count);\n-  if (l_in_use_count == chk_om_in_use_count) {\n-    out->print_cr(\"global_in_use_count=%d equals chk_om_in_use_count=%d\",\n-                  l_in_use_count, chk_om_in_use_count);\n+  size_t ck_in_use_max = _in_use_list.max();\n+  if (l_in_use_max == ck_in_use_max) {\n+    out->print_cr(\"in_use_max=\" SIZE_FORMAT \" equals ck_in_use_max=\"\n+                  SIZE_FORMAT, l_in_use_max, ck_in_use_max);\n@@ -2856,5 +1811,2 @@\n-    \/\/ With fine grained locks on the monitor lists, it is possible for\n-    \/\/ an exiting JavaThread to put its in-use ObjectMonitors on the\n-    \/\/ global in-use list after chk_om_in_use_count is calculated above.\n-    out->print_cr(\"WARNING: global_in_use_count=%d is not equal to chk_om_in_use_count=%d\",\n-                  l_in_use_count, chk_om_in_use_count);\n+    out->print_cr(\"WARNING: in_use_max=\" SIZE_FORMAT \" is not equal to \"\n+                  \"ck_in_use_max=\" SIZE_FORMAT, l_in_use_max, ck_in_use_max);\n@@ -2865,2 +1817,8 @@\n-void ObjectSynchronizer::chk_in_use_entry(JavaThread* jt, ObjectMonitor* n,\n-                                          outputStream * out, int *error_cnt_p) {\n+void ObjectSynchronizer::chk_in_use_entry(ObjectMonitor* n, outputStream* out,\n+                                          int* error_cnt_p) {\n+  if (n->owner_is_DEFLATER_MARKER()) {\n+    \/\/ This should not happen, but if it does, it is not fatal.\n+    out->print_cr(\"WARNING: monitor=\" INTPTR_FORMAT \": in-use monitor is \"\n+                  \"deflated.\", p2i(n));\n+    return;\n+  }\n@@ -2868,8 +1826,2 @@\n-    if (jt != NULL) {\n-      out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \", monitor=\" INTPTR_FORMAT\n-                    \": in-use per-thread monitor must have non-NULL _header \"\n-                    \"field.\", p2i(jt), p2i(n));\n-    } else {\n-      out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use global monitor \"\n-                    \"must have non-NULL _header field.\", p2i(n));\n-    }\n+    out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use monitor must \"\n+                  \"have non-NULL _header field.\", p2i(n));\n@@ -2882,11 +1834,4 @@\n-      if (jt != NULL) {\n-        out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \", monitor=\" INTPTR_FORMAT\n-                      \": in-use per-thread monitor's object does not think \"\n-                      \"it has a monitor: obj=\" INTPTR_FORMAT \", mark=\"\n-                      INTPTR_FORMAT,  p2i(jt), p2i(n), p2i(obj), mark.value());\n-      } else {\n-        out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use global \"\n-                      \"monitor's object does not think it has a monitor: obj=\"\n-                      INTPTR_FORMAT \", mark=\" INTPTR_FORMAT, p2i(n),\n-                      p2i(obj), mark.value());\n-      }\n+      out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use monitor's \"\n+                    \"object does not think it has a monitor: obj=\"\n+                    INTPTR_FORMAT \", mark=\" INTPTR_FORMAT, p2i(n),\n+                    p2i(obj), mark.value());\n@@ -2897,12 +1842,4 @@\n-      if (jt != NULL) {\n-        out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \", monitor=\" INTPTR_FORMAT\n-                      \": in-use per-thread monitor's object does not refer \"\n-                      \"to the same monitor: obj=\" INTPTR_FORMAT \", mark=\"\n-                      INTPTR_FORMAT \", obj_mon=\" INTPTR_FORMAT, p2i(jt),\n-                      p2i(n), p2i(obj), mark.value(), p2i(obj_mon));\n-      } else {\n-        out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use global \"\n-                      \"monitor's object does not refer to the same monitor: obj=\"\n-                      INTPTR_FORMAT \", mark=\" INTPTR_FORMAT \", obj_mon=\"\n-                      INTPTR_FORMAT, p2i(n), p2i(obj), mark.value(), p2i(obj_mon));\n-      }\n+      out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use monitor's \"\n+                    \"object does not refer to the same monitor: obj=\"\n+                    INTPTR_FORMAT \", mark=\" INTPTR_FORMAT \", obj_mon=\"\n+                    INTPTR_FORMAT, p2i(n), p2i(obj), mark.value(), p2i(obj_mon));\n@@ -2914,83 +1851,1 @@\n-\/\/ Check the thread's free list and count; log the results of the checks.\n-void ObjectSynchronizer::chk_per_thread_free_list_and_count(JavaThread *jt,\n-                                                            outputStream * out,\n-                                                            int *error_cnt_p) {\n-  int chk_om_free_count = 0;\n-  ObjectMonitor* cur = NULL;\n-  if ((cur = get_list_head_locked(&jt->om_free_list)) != NULL) {\n-    \/\/ Marked the per-thread free list head so process the list.\n-    while (true) {\n-      chk_free_entry(jt, cur, out, error_cnt_p);\n-      chk_om_free_count++;\n-\n-      cur = lock_next_for_traversal(cur);\n-      if (cur == NULL) {\n-        break;\n-      }\n-    }\n-  }\n-  int l_om_free_count = Atomic::load(&jt->om_free_count);\n-  if (l_om_free_count == chk_om_free_count) {\n-    out->print_cr(\"jt=\" INTPTR_FORMAT \": om_free_count=%d equals \"\n-                  \"chk_om_free_count=%d\", p2i(jt), l_om_free_count, chk_om_free_count);\n-  } else {\n-    out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \": om_free_count=%d is not \"\n-                  \"equal to chk_om_free_count=%d\", p2i(jt), l_om_free_count,\n-                  chk_om_free_count);\n-    *error_cnt_p = *error_cnt_p + 1;\n-  }\n-}\n-\n-\/\/ Check the thread's in-use list and count; log the results of the checks.\n-void ObjectSynchronizer::chk_per_thread_in_use_list_and_count(JavaThread *jt,\n-                                                              outputStream * out,\n-                                                              int *error_cnt_p) {\n-  int chk_om_in_use_count = 0;\n-  ObjectMonitor* cur = NULL;\n-  if ((cur = get_list_head_locked(&jt->om_in_use_list)) != NULL) {\n-    \/\/ Marked the per-thread in-use list head so process the list.\n-    while (true) {\n-      chk_in_use_entry(jt, cur, out, error_cnt_p);\n-      chk_om_in_use_count++;\n-\n-      cur = lock_next_for_traversal(cur);\n-      if (cur == NULL) {\n-        break;\n-      }\n-    }\n-  }\n-  int l_om_in_use_count = Atomic::load(&jt->om_in_use_count);\n-  if (l_om_in_use_count == chk_om_in_use_count) {\n-    out->print_cr(\"jt=\" INTPTR_FORMAT \": om_in_use_count=%d equals \"\n-                  \"chk_om_in_use_count=%d\", p2i(jt), l_om_in_use_count,\n-                  chk_om_in_use_count);\n-  } else {\n-    out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \": om_in_use_count=%d is not \"\n-                  \"equal to chk_om_in_use_count=%d\", p2i(jt), l_om_in_use_count,\n-                  chk_om_in_use_count);\n-    *error_cnt_p = *error_cnt_p + 1;\n-  }\n-}\n-\n-\/\/ Do the final audit and print of ObjectMonitor stats; must be done\n-\/\/ by the VMThread (at VM exit time).\n-void ObjectSynchronizer::do_final_audit_and_print_stats() {\n-  assert(Thread::current()->is_VM_thread(), \"sanity check\");\n-\n-  if (is_final_audit()) {  \/\/ Only do the audit once.\n-    return;\n-  }\n-  set_is_final_audit();\n-\n-  if (log_is_enabled(Info, monitorinflation)) {\n-    \/\/ Do a deflation in order to reduce the in-use monitor population\n-    \/\/ that is reported by ObjectSynchronizer::log_in_use_monitor_details()\n-    \/\/ which is called by ObjectSynchronizer::audit_and_print_stats().\n-    ObjectSynchronizer::deflate_idle_monitors();\n-    \/\/ The other audit_and_print_stats() call is done at the Debug\n-    \/\/ level at a safepoint in ObjectSynchronizer::do_safepoint_work().\n-    ObjectSynchronizer::audit_and_print_stats(true \/* on_exit *\/);\n-  }\n-}\n-\n-\/\/ Log details about ObjectMonitors on the in-use lists. The 'BHL'\n+\/\/ Log details about ObjectMonitors on the in_use_list. The 'BHL'\n@@ -2999,1 +1854,1 @@\n-void ObjectSynchronizer::log_in_use_monitor_details(outputStream * out) {\n+void ObjectSynchronizer::log_in_use_monitor_details(outputStream* out) {\n@@ -3001,2 +1856,2 @@\n-  if (Atomic::load(&om_list_globals._in_use_count) > 0) {\n-    out->print_cr(\"In-use global monitor info:\");\n+  if (_in_use_list.count() > 0) {\n+    out->print_cr(\"In-use monitor info:\");\n@@ -3007,51 +1862,12 @@\n-    ObjectMonitor* cur = NULL;\n-    if ((cur = get_list_head_locked(&om_list_globals._in_use_list)) != NULL) {\n-      \/\/ Marked the global in-use list head so process the list.\n-      while (true) {\n-        const oop obj = cur->object_peek();\n-        const markWord mark = cur->header();\n-        ResourceMark rm;\n-        out->print(INTPTR_FORMAT \"  %d%d%d  \" INTPTR_FORMAT \"  %s\", p2i(cur),\n-                   cur->is_busy() != 0, mark.hash() != 0, cur->owner() != NULL,\n-                   p2i(obj), obj == NULL ? \"\" : obj->klass()->external_name());\n-        if (cur->is_busy() != 0) {\n-          out->print(\" (%s)\", cur->is_busy_to_string(&ss));\n-          ss.reset();\n-        }\n-        out->cr();\n-\n-        cur = lock_next_for_traversal(cur);\n-        if (cur == NULL) {\n-          break;\n-        }\n-      }\n-    }\n-  }\n-\n-  out->print_cr(\"In-use per-thread monitor info:\");\n-  out->print_cr(\"(B -> is_busy, H -> has hash code, L -> lock status)\");\n-  out->print_cr(\"%18s  %18s  %s  %18s  %18s\",\n-                \"jt\", \"monitor\", \"BHL\", \"object\", \"object type\");\n-  out->print_cr(\"==================  ==================  ===  ==================  ==================\");\n-  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {\n-    ObjectMonitor* cur = NULL;\n-    if ((cur = get_list_head_locked(&jt->om_in_use_list)) != NULL) {\n-      \/\/ Marked the global in-use list head so process the list.\n-      while (true) {\n-        const oop obj = cur->object_peek();\n-        const markWord mark = cur->header();\n-        ResourceMark rm;\n-        out->print(INTPTR_FORMAT \"  \" INTPTR_FORMAT \"  %d%d%d  \" INTPTR_FORMAT\n-                   \"  %s\", p2i(jt), p2i(cur), cur->is_busy() != 0,\n-                   mark.hash() != 0, cur->owner() != NULL, p2i(obj),\n-                   obj == NULL ? \"\" : obj->klass()->external_name());\n-        if (cur->is_busy() != 0) {\n-          out->print(\" (%s)\", cur->is_busy_to_string(&ss));\n-          ss.reset();\n-        }\n-        out->cr();\n-\n-        cur = lock_next_for_traversal(cur);\n-        if (cur == NULL) {\n-          break;\n-        }\n+    MonitorList::Iterator iter = _in_use_list.iterator();\n+    while (iter.has_next()) {\n+      ObjectMonitor* mid = iter.next();\n+      const oop obj = mid->object_peek();\n+      const markWord mark = mid->header();\n+      ResourceMark rm;\n+      out->print(INTPTR_FORMAT \"  %d%d%d  \" INTPTR_FORMAT \"  %s\", p2i(mid),\n+                 mid->is_busy() != 0, mark.hash() != 0, mid->owner() != NULL,\n+                 p2i(obj), obj == NULL ? \"\" : obj->klass()->external_name());\n+      if (mid->is_busy() != 0) {\n+        out->print(\" (%s)\", mid->is_busy_to_string(&ss));\n+        ss.reset();\n@@ -3059,0 +1875,1 @@\n+      out->cr();\n@@ -3064,55 +1881,0 @@\n-\n-\/\/ Log counts for the global and per-thread monitor lists and return\n-\/\/ the population count.\n-int ObjectSynchronizer::log_monitor_list_counts(outputStream * out) {\n-  int pop_count = 0;\n-  out->print_cr(\"%18s  %10s  %10s  %10s  %10s\",\n-                \"Global Lists:\", \"InUse\", \"Free\", \"Wait\", \"Total\");\n-  out->print_cr(\"==================  ==========  ==========  ==========  ==========\");\n-  int l_in_use_count = Atomic::load(&om_list_globals._in_use_count);\n-  int l_free_count = Atomic::load(&om_list_globals._free_count);\n-  int l_wait_count = Atomic::load(&om_list_globals._wait_count);\n-  out->print_cr(\"%18s  %10d  %10d  %10d  %10d\", \"\", l_in_use_count,\n-                l_free_count, l_wait_count,\n-                Atomic::load(&om_list_globals._population));\n-  pop_count += l_in_use_count + l_free_count + l_wait_count;\n-\n-  out->print_cr(\"%18s  %10s  %10s  %10s\",\n-                \"Per-Thread Lists:\", \"InUse\", \"Free\", \"Provision\");\n-  out->print_cr(\"==================  ==========  ==========  ==========\");\n-\n-  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {\n-    int l_om_in_use_count = Atomic::load(&jt->om_in_use_count);\n-    int l_om_free_count = Atomic::load(&jt->om_free_count);\n-    out->print_cr(INTPTR_FORMAT \"  %10d  %10d  %10d\", p2i(jt),\n-                  l_om_in_use_count, l_om_free_count, jt->om_free_provision);\n-    pop_count += l_om_in_use_count + l_om_free_count;\n-  }\n-  return pop_count;\n-}\n-\n-#ifndef PRODUCT\n-\n-\/\/ Check if monitor belongs to the monitor cache\n-\/\/ The list is grow-only so it's *relatively* safe to traverse\n-\/\/ the list of extant blocks without taking a lock.\n-\n-int ObjectSynchronizer::verify_objmon_isinpool(ObjectMonitor *monitor) {\n-  PaddedObjectMonitor* block = Atomic::load(&g_block_list);\n-  while (block != NULL) {\n-    assert(block->is_chainmarker(), \"must be a block header\");\n-    if (monitor > &block[0] && monitor < &block[_BLOCKSIZE]) {\n-      address mon = (address)monitor;\n-      address blk = (address)block;\n-      size_t diff = mon - blk;\n-      assert((diff % sizeof(PaddedObjectMonitor)) == 0, \"must be aligned\");\n-      return 1;\n-    }\n-    \/\/ unmarked_next() is not needed with g_block_list (no locking\n-    \/\/ used with block linkage _next_om fields).\n-    block = (PaddedObjectMonitor*)block->next_om();\n-  }\n-  return 0;\n-}\n-\n-#endif\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":378,"deletions":1616,"binary":false,"changes":1994,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+class LogStream;\n@@ -37,8 +38,0 @@\n-#ifndef OM_CACHE_LINE_SIZE\n-\/\/ Use DEFAULT_CACHE_LINE_SIZE if not already specified for\n-\/\/ the current build platform.\n-#define OM_CACHE_LINE_SIZE DEFAULT_CACHE_LINE_SIZE\n-#endif\n-\n-typedef PaddedEnd<ObjectMonitor, OM_CACHE_LINE_SIZE> PaddedObjectMonitor;\n-\n@@ -47,0 +40,1 @@\n+\n@@ -102,6 +96,0 @@\n-  \/\/ thread-specific and global ObjectMonitor free list accessors\n-  static ObjectMonitor* om_alloc(Thread* self);\n-  static void om_release(Thread* self, ObjectMonitor* m,\n-                         bool FromPerThreadAlloc);\n-  static void om_flush(Thread* self);\n-\n@@ -130,17 +118,12 @@\n-  \/\/ Basically we deflate all monitors that are not busy.\n-  \/\/ An adaptive profile-based deflation policy could be used if needed\n-  static void deflate_idle_monitors();\n-  static void deflate_global_idle_monitors(Thread* self);\n-  static void deflate_per_thread_idle_monitors(Thread* self,\n-                                               JavaThread* target);\n-  static void deflate_common_idle_monitors(Thread* self, bool is_global,\n-                                           JavaThread* target);\n-\n-  \/\/ For a given in-use monitor list: global or per-thread, deflate idle\n-  \/\/ monitors.\n-  static int deflate_monitor_list(Thread* self, ObjectMonitor** list_p,\n-                                  int* count_p, ObjectMonitor** free_head_p,\n-                                  ObjectMonitor** free_tail_p,\n-                                  ObjectMonitor** saved_mid_in_use_p);\n-  static bool deflate_monitor(ObjectMonitor* mid, ObjectMonitor** free_head_p,\n-                              ObjectMonitor** free_tail_p);\n+  \/\/ Basically we try to deflate all monitors that are not busy.\n+  static size_t deflate_idle_monitors();\n+\n+  \/\/ Deflate idle monitors:\n+  static void chk_for_block_req(JavaThread* self, const char* op_name,\n+                                const char* cnt_name, size_t cnt, LogStream* ls,\n+                                elapsedTimer* timer_p);\n+  static size_t deflate_monitor_list(Thread* self, LogStream* ls,\n+                                     elapsedTimer* timer_p);\n+  static size_t in_use_list_ceiling();\n+  static void dec_in_use_list_ceiling();\n+  static void inc_in_use_list_ceiling();\n@@ -158,16 +141,3 @@\n-  static void chk_free_entry(JavaThread* jt, ObjectMonitor* n,\n-                             outputStream * out, int *error_cnt_p);\n-  static void chk_global_free_list_and_count(outputStream * out,\n-                                             int *error_cnt_p);\n-  static void chk_global_wait_list_and_count(outputStream * out,\n-                                             int *error_cnt_p);\n-  static void chk_global_in_use_list_and_count(outputStream * out,\n-                                               int *error_cnt_p);\n-  static void chk_in_use_entry(JavaThread* jt, ObjectMonitor* n,\n-                               outputStream * out, int *error_cnt_p);\n-  static void chk_per_thread_in_use_list_and_count(JavaThread *jt,\n-                                                   outputStream * out,\n-                                                   int *error_cnt_p);\n-  static void chk_per_thread_free_list_and_count(JavaThread *jt,\n-                                                 outputStream * out,\n-                                                 int *error_cnt_p);\n+  static void chk_in_use_list(outputStream* out, int* error_cnt_p);\n+  static void chk_in_use_entry(ObjectMonitor* n, outputStream* out,\n+                               int* error_cnt_p);\n@@ -175,5 +145,1 @@\n-  static void log_in_use_monitor_details(outputStream * out);\n-  static int  log_monitor_list_counts(outputStream * out);\n-  static int  verify_objmon_isinpool(ObjectMonitor *addr) PRODUCT_RETURN0;\n-\n-  static void do_safepoint_work();\n+  static void log_in_use_monitor_details(outputStream* out);\n@@ -184,3 +150,0 @@\n-  enum { _BLOCKSIZE = 128 };\n-  \/\/ global list of blocks of monitors\n-  static PaddedObjectMonitor* g_block_list;\n@@ -191,3 +154,0 @@\n-  \/\/ Function to prepend new blocks to the appropriate lists:\n-  static void prepend_block_to_lists(PaddedObjectMonitor* new_blk);\n-\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":18,"deletions":58,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+#include \"runtime\/monitorDeflationThread.hpp\"\n@@ -266,5 +267,0 @@\n-  om_free_list = NULL;\n-  om_free_count = 0;\n-  om_free_provision = 32;\n-  om_in_use_list = NULL;\n-  om_in_use_count = 0;\n@@ -3695,0 +3691,3 @@\n+  \/\/ Start the monitor deflation thread:\n+  MonitorDeflationThread::initialize();\n+\n@@ -4237,0 +4236,3 @@\n+  \/\/ Increase the ObjectMonitor ceiling for the new thread.\n+  ObjectSynchronizer::inc_in_use_list_ceiling();\n+\n@@ -4245,4 +4247,0 @@\n-\n-  \/\/ Reclaim the ObjectMonitors from the om_in_use_list and om_free_list of the moribund thread.\n-  ObjectSynchronizer::om_flush(p);\n-\n@@ -4253,5 +4251,4 @@\n-    \/\/ We must flush any deferred card marks and other various GC barrier\n-    \/\/ related buffers (e.g. G1 SATB buffer and G1 dirty card queue buffer)\n-    \/\/ before removing a thread from the list of active threads.\n-    \/\/ This must be done after ObjectSynchronizer::om_flush(), as GC barriers\n-    \/\/ are used in om_flush().\n+    \/\/ BarrierSet state must be destroyed after the last thread transition\n+    \/\/ before the thread terminates. Thread transitions result in calls to\n+    \/\/ StackWatermarkSet::on_safepoint(), which performs GC processing,\n+    \/\/ requiring the GC state to be alive.\n@@ -4287,0 +4284,3 @@\n+  \/\/ Reduce the ObjectMonitor ceiling for the exiting thread.\n+  ObjectSynchronizer::dec_in_use_list_ceiling();\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":14,"deletions":14,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -419,8 +419,0 @@\n-  \/\/ Per-thread ObjectMonitor lists:\n- public:\n-  ObjectMonitor* om_free_list;                  \/\/ SLL of free ObjectMonitors\n-  int om_free_count;                            \/\/ # on om_free_list\n-  int om_free_provision;                        \/\/ # to try to allocate next\n-  ObjectMonitor* om_in_use_list;                \/\/ SLL of in-use ObjectMonitors\n-  int om_in_use_count;                          \/\/ # on om_in_use_list\n-\n@@ -489,0 +481,1 @@\n+  virtual bool is_monitor_deflation_thread() const   { return false; }\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -89,0 +89,1 @@\n+#include \"runtime\/monitorDeflationThread.hpp\"\n@@ -890,1 +891,0 @@\n-  static_field(ObjectSynchronizer,             g_block_list,                                  PaddedObjectMonitor*)                  \\\n@@ -1341,0 +1341,1 @@\n+        declare_type(MonitorDeflationThread, JavaThread)                  \\\n@@ -1465,1 +1466,0 @@\n-  declare_toplevel_type(PaddedObjectMonitor)                              \\\n@@ -1997,1 +1997,0 @@\n-  declare_toplevel_type(PaddedObjectMonitor*)                             \\\n@@ -2261,1 +2260,0 @@\n-  declare_preprocessor_constant(\"FIELDINFO_TAG_MASK\", FIELDINFO_TAG_MASK) \\\n@@ -2529,6 +2527,0 @@\n-  \/* ObjectSynchronizer *\/                                                \\\n-  \/**********************\/                                                \\\n-                                                                          \\\n-  declare_constant(ObjectSynchronizer::_BLOCKSIZE)                        \\\n-                                                                          \\\n-  \/**********************\/                                                \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":2,"deletions":10,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -189,1 +189,0 @@\n- * @author  unascribed\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Class.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -39,1 +39,0 @@\n- * @author  unascribed\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Object.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+import java.lang.reflect.Parameter;\n@@ -36,0 +37,1 @@\n+import java.util.LinkedHashMap;\n@@ -46,0 +48,2 @@\n+import static java.util.stream.Collectors.joining;\n+import static java.util.stream.Collectors.toList;\n@@ -352,1 +356,2 @@\n-            return maybeAdapt((VarHandle)fac.invoke(be, size, offset, alignmentMask, strides));\n+            boolean exact = false;\n+            return maybeAdapt((VarHandle)fac.invoke(be, size, offset, alignmentMask, exact, strides));\n@@ -362,1 +367,1 @@\n-        MethodType mtype = target.accessModeType(VarHandle.AccessMode.GET).dropParameterTypes(0, 1);\n+        MethodType mtype = target.accessModeType(VarHandle.AccessMode.GET);\n@@ -692,11 +697,16 @@\n-\/\/                \"@ForceInline\\n\" +\n-\/\/                \"@LambdaForm.Compiled\\n\" +\n-\/\/                \"final static <METHOD> throws Throwable {\\n\" +\n-\/\/                \"    if (handle.isDirect() && handle.vform.methodType_table[ad.type] == ad.symbolicMethodType) {\\n\" +\n-\/\/                \"        <RESULT_ERASED>MethodHandle.linkToStatic(<LINK_TO_STATIC_ARGS>);<RETURN_ERASED>\\n\" +\n-\/\/                \"    }\\n\" +\n-\/\/                \"    else {\\n\" +\n-\/\/                \"        MethodHandle mh = handle.getMethodHandle(ad.mode);\\n\" +\n-\/\/                \"        <RETURN>mh.asType(ad.symbolicMethodTypeInvoker).invokeBasic(<LINK_TO_INVOKER_ARGS>);\\n\" +\n-\/\/                \"    }\\n\" +\n-\/\/                \"}\";\n+\/\/                \"\"\"\n+\/\/                @ForceInline\n+\/\/                @LambdaForm.Compiled\n+\/\/                @Hidden\n+\/\/                final static <METHOD> throws Throwable {\n+\/\/                    if (handle.hasInvokeExactBehavior() && handle.accessModeType(ad.type) != ad.symbolicMethodTypeExact) {\n+\/\/                        throw new WrongMethodTypeException(\"expected \" + handle.accessModeType(ad.type) + \" but found \"\n+\/\/                                + ad.symbolicMethodTypeExact);\n+\/\/                    }\n+\/\/                    if (handle.isDirect() && handle.vform.methodType_table[ad.type] == ad.symbolicMethodTypeErased) {\n+\/\/                        <RESULT_ERASED>MethodHandle.linkToStatic(<LINK_TO_STATIC_ARGS>);<RETURN_ERASED>\n+\/\/                    } else {\n+\/\/                        MethodHandle mh = handle.getMethodHandle(ad.mode);\n+\/\/                        <RETURN>mh.asType(ad.symbolicMethodTypeInvoker).invokeBasic(<LINK_TO_INVOKER_ARGS>);\n+\/\/                    }\n+\/\/                }\"\"\";\n@@ -705,14 +715,18 @@\n-\/\/                \"@ForceInline\\n\" +\n-\/\/                \"@LambdaForm.Compiled\\n\" +\n-\/\/                \"final static <METHOD> throws Throwable {\\n\" +\n-\/\/                \"    if (handle.isDirect() && handle.vform.methodType_table[ad.type] == ad.symbolicMethodType) {\\n\" +\n-\/\/                \"        MethodHandle.linkToStatic(<LINK_TO_STATIC_ARGS>);\\n\" +\n-\/\/                \"    }\\n\" +\n-\/\/                \"    else if (handle.isDirect() && handle.vform.getMethodType_V(ad.type) == ad.symbolicMethodType) {\\n\" +\n-\/\/                \"        MethodHandle.linkToStatic(<LINK_TO_STATIC_ARGS>);\\n\" +\n-\/\/                \"    }\\n\" +\n-\/\/                \"    else {\\n\" +\n-\/\/                \"        MethodHandle mh = handle.getMethodHandle(ad.mode);\\n\" +\n-\/\/                \"        mh.asType(ad.symbolicMethodTypeInvoker).invokeBasic(<LINK_TO_INVOKER_ARGS>);\\n\" +\n-\/\/                \"    }\\n\" +\n-\/\/                \"}\";\n+\/\/                \"\"\"\n+\/\/                @ForceInline\n+\/\/                @LambdaForm.Compiled\n+\/\/                @Hidden\n+\/\/                final static <METHOD> throws Throwable {\n+\/\/                    if (handle.hasInvokeExactBehavior() && handle.accessModeType(ad.type) != ad.symbolicMethodTypeExact) {\n+\/\/                        throw new WrongMethodTypeException(\"expected \" + handle.accessModeType(ad.type) + \" but found \"\n+\/\/                                + ad.symbolicMethodTypeExact);\n+\/\/                    }\n+\/\/                    if (handle.isDirect() && handle.vform.methodType_table[ad.type] == ad.symbolicMethodTypeErased) {\n+\/\/                        MethodHandle.linkToStatic(<LINK_TO_STATIC_ARGS>);\n+\/\/                    } else if (handle.isDirect() && handle.vform.getMethodType_V(ad.type) == ad.symbolicMethodTypeErased) {\n+\/\/                        MethodHandle.linkToStatic(<LINK_TO_STATIC_ARGS>);\n+\/\/                    } else {\n+\/\/                        MethodHandle mh = handle.getMethodHandle(ad.mode);\n+\/\/                        mh.asType(ad.symbolicMethodTypeInvoker).invokeBasic(<LINK_TO_INVOKER_ARGS>);\n+\/\/                    }\n+\/\/                }\"\"\";\n@@ -754,0 +768,1 @@\n+\/\/            System.out.println(\"import jdk.internal.vm.annotation.Hidden;\");\n@@ -806,5 +821,2 @@\n-\/\/                    map(mt -> generateMethod(mt)).\n-\/\/                    forEach(s -> {\n-\/\/                        System.out.println(s);\n-\/\/                        System.out.println();\n-\/\/                    });\n+\/\/                    map(GuardMethodGenerator::generateMethod).\n+\/\/                    forEach(System.out::println);\n@@ -866,0 +878,1 @@\n+\/\/            LINK_TO_INVOKER_ARGS.set(0, LINK_TO_INVOKER_ARGS.get(0) + \".asDirect()\");\n@@ -881,1 +894,1 @@\n-\/\/                                   : \" return ad.returnType.cast(r);\";\n+\/\/                                   : \"\\n        return ad.returnType.cast(r);\";\n@@ -898,1 +911,1 @@\n-\/\/                    ;\n+\/\/                    .indent(4);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/VarHandles.java","additions":47,"deletions":34,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-            this(receiverType, fieldOffset{#if[Object]?, fieldType}, FieldInstanceReadOnly.FORM);\n+            this(receiverType, fieldOffset{#if[Object]?, fieldType}, FieldInstanceReadOnly.FORM, false);\n@@ -52,2 +52,2 @@\n-                                        VarForm form) {\n-            super(form);\n+                                        VarForm form, boolean exact) {\n+            super(form, exact);\n@@ -62,2 +62,16 @@\n-        final MethodType accessModeTypeUncached(AccessMode accessMode) {\n-            return accessMode.at.accessModeType(receiverType, {#if[Object]?fieldType:$type$.class});\n+        public FieldInstanceReadOnly withInvokeExactBehavior() {\n+            return hasInvokeExactBehavior()\n+                ? this\n+                : new FieldInstanceReadOnly(receiverType, fieldOffset{#if[Object]?, fieldType}, vform, true);\n+        }\n+\n+        @Override\n+        public FieldInstanceReadOnly withInvokeBehavior() {\n+            return !hasInvokeExactBehavior()\n+                ? this\n+                : new FieldInstanceReadOnly(receiverType, fieldOffset{#if[Object]?, fieldType}, vform, false);\n+        }\n+\n+        @Override\n+        final MethodType accessModeTypeUncached(AccessType at) {\n+            return at.accessModeType(receiverType, {#if[Object]?fieldType:$type$.class});\n@@ -111,1 +125,20 @@\n-            super(receiverType, fieldOffset{#if[Object]?, fieldType}, FieldInstanceReadWrite.FORM);\n+            this(receiverType, fieldOffset{#if[Object]?, fieldType}, false);\n+        }\n+\n+        private FieldInstanceReadWrite(Class<?> receiverType, long fieldOffset{#if[Object]?, Class<?> fieldType},\n+                                       boolean exact) {\n+            super(receiverType, fieldOffset{#if[Object]?, fieldType}, FieldInstanceReadWrite.FORM, exact);\n+        }\n+\n+        @Override\n+        public FieldInstanceReadWrite withInvokeExactBehavior() {\n+            return hasInvokeExactBehavior()\n+                ? this\n+                : new FieldInstanceReadWrite(receiverType, fieldOffset{#if[Object]?, fieldType}, true);\n+        }\n+\n+        @Override\n+        public FieldInstanceReadWrite withInvokeBehavior() {\n+            return !hasInvokeExactBehavior()\n+                ? this\n+                : new FieldInstanceReadWrite(receiverType, fieldOffset{#if[Object]?, fieldType}, false);\n@@ -367,1 +400,1 @@\n-            this(base, fieldOffset{#if[Object]?, fieldType}, FieldStaticReadOnly.FORM);\n+            this(base, fieldOffset{#if[Object]?, fieldType}, FieldStaticReadOnly.FORM, false);\n@@ -371,2 +404,2 @@\n-                                      VarForm form) {\n-            super(form);\n+                                      VarForm form, boolean exact) {\n+            super(form, exact);\n@@ -380,0 +413,14 @@\n+        @Override\n+        public FieldStaticReadOnly withInvokeExactBehavior() {\n+            return hasInvokeExactBehavior()\n+                ? this\n+                : new FieldStaticReadOnly(base, fieldOffset{#if[Object]?, fieldType}, vform, true);\n+        }\n+\n+        @Override\n+        public FieldStaticReadOnly withInvokeBehavior() {\n+            return !hasInvokeExactBehavior()\n+                ? this\n+                : new FieldStaticReadOnly(base, fieldOffset{#if[Object]?, fieldType}, vform, false);\n+        }\n+\n@@ -396,2 +443,2 @@\n-        final MethodType accessModeTypeUncached(AccessMode accessMode) {\n-            return accessMode.at.accessModeType(null, {#if[Object]?fieldType:$type$.class});\n+        final MethodType accessModeTypeUncached(AccessType at) {\n+            return at.accessModeType(null, {#if[Object]?fieldType:$type$.class});\n@@ -432,1 +479,20 @@\n-            super(base, fieldOffset{#if[Object]?, fieldType}, FieldStaticReadWrite.FORM);\n+            this(base, fieldOffset{#if[Object]?, fieldType}, false);\n+        }\n+\n+        private FieldStaticReadWrite(Object base, long fieldOffset{#if[Object]?, Class<?> fieldType},\n+                                     boolean exact) {\n+            super(base, fieldOffset{#if[Object]?, fieldType}, FieldStaticReadWrite.FORM, exact);\n+        }\n+\n+        @Override\n+        public FieldStaticReadWrite withInvokeExactBehavior() {\n+            return hasInvokeExactBehavior()\n+                ? this\n+                : new FieldStaticReadWrite(base, fieldOffset{#if[Object]?, fieldType}, true);\n+        }\n+\n+        @Override\n+        public FieldStaticReadWrite withInvokeBehavior() {\n+            return !hasInvokeExactBehavior()\n+                ? this\n+                : new FieldStaticReadWrite(base, fieldOffset{#if[Object]?, fieldType}, false);\n@@ -696,1 +762,5 @@\n-            super(Array.FORM);\n+            this(abase, ashift{#if[Object]?, arrayType}, false);\n+        }\n+\n+        private Array(int abase, int ashift{#if[Object]?, Class<?> arrayType}, boolean exact) {\n+            super(Array.FORM, exact);\n@@ -705,0 +775,14 @@\n+        @Override\n+        public Array withInvokeExactBehavior() {\n+            return hasInvokeExactBehavior()\n+                ? this\n+                : new Array(abase, ashift{#if[Object]?, arrayType}, true);\n+        }\n+\n+        @Override\n+        public Array withInvokeBehavior() {\n+            return !hasInvokeExactBehavior()\n+                ? this\n+                : new Array(abase, ashift{#if[Object]?, arrayType}, false);\n+        }\n+\n@@ -715,2 +799,2 @@\n-        final MethodType accessModeTypeUncached(AccessMode accessMode) {\n-            return accessMode.at.accessModeType({#if[Object]?arrayType:$type$[].class}, {#if[Object]?arrayType.getComponentType():$type$.class}, int.class);\n+        final MethodType accessModeTypeUncached(AccessType at) {\n+            return at.accessModeType({#if[Object]?arrayType:$type$[].class}, {#if[Object]?arrayType.getComponentType():$type$.class}, int.class);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/X-VarHandle.java.template","additions":99,"deletions":15,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -345,0 +345,14 @@\n+    \/**\n+     * Load referent with strong semantics. Treating the referent\n+     * as strong referent is ok when the Reference is inactive,\n+     * because then the referent is switched to strong semantics\n+     * anyway.\n+     *\n+     * This is only used from Finalizer to bypass the intrinsic,\n+     * which might return a null referent, even though it is not\n+     * null, and would subsequently not finalize the referent\/finalizee.\n+     *\/\n+    T getInactive() {\n+        return this.referent;\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/Reference.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -223,8 +223,0 @@\n-         * {@preview Associated with pattern matching for instanceof, a preview feature of\n-         *           the Java language.\n-         *\n-         *           This enum constant is associated with <i>pattern matching for instanceof<\/i>, a preview\n-         *           feature of the Java language. Preview features\n-         *           may be removed in a future release, or upgraded to permanent\n-         *           features of the Java language.}\n-         *\n@@ -233,1 +225,1 @@\n-         * @since 14\n+         * @since 16\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/tree\/Tree.java","additions":1,"deletions":9,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -261,8 +261,0 @@\n-     * {@preview Associated with pattern matching for instanceof, a preview feature of\n-     *           the Java language.\n-     *\n-     *           This method is associated with <i>pattern matching for instanceof<\/i>, a preview\n-     *           feature of the Java language. Preview features\n-     *           may be removed in a future release, or upgraded to permanent\n-     *           features of the Java language.}\n-     *\n@@ -273,1 +265,1 @@\n-     * @since 14\n+     * @since 16\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/tree\/TreeVisitor.java","additions":1,"deletions":9,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -708,1 +708,1 @@\n-        return scan(node.getType(), p);\n+        return scan(node.getVariable(), p);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/util\/TreeScanner.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1906,0 +1906,2 @@\n+        private final boolean isVarargs;\n+\n@@ -1913,0 +1915,6 @@\n+            \/* it is better to store the original information for this one, instead of relying\n+             * on the info in the type of the symbol. This is because on the presence of APs\n+             * the symbol will be blown out and we won't be able to know if the original\n+             * record component was declared varargs or not.\n+             *\/\n+            this.isVarargs = type.hasTag(TypeTag.ARRAY) && ((ArrayType)type).isVarargs();\n@@ -1918,1 +1926,1 @@\n-            return type.hasTag(TypeTag.ARRAY) && ((ArrayType)type).isVarargs();\n+            return isVarargs;\n@@ -1968,1 +1976,1 @@\n-            super(Flags.FINAL | Flags.HASINIT | Flags.MATCH_BINDING, name, type, owner);\n+            super(Flags.HASINIT | Flags.MATCH_BINDING, name, type, owner);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Symbol.java","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -321,2 +321,0 @@\n-            } else if ((v.flags() & MATCH_BINDING) != 0) {\n-                log.error(pos, Errors.PatternBindingMayNotBeAssigned(v));\n@@ -4065,0 +4063,4 @@\n+            if (types.isSubtype(exprtype, clazztype) &&\n+                !exprtype.isErroneous() && !clazztype.isErroneous()) {\n+                log.error(tree.pos(), Errors.InstanceofPatternNoSubtype(clazztype, exprtype));\n+            }\n@@ -4066,1 +4068,1 @@\n-            typeTree = pattern.vartype;\n+            typeTree = pattern.var.vartype;\n@@ -4068,1 +4070,1 @@\n-                clazztype = chk.checkClassOrArrayType(pattern.vartype.pos(), clazztype);\n+                clazztype = chk.checkClassOrArrayType(pattern.var.vartype.pos(), clazztype);\n@@ -4094,1 +4096,3 @@\n-                log.error(typeTree.pos(), Errors.IllegalGenericTypeForInstof);\n+                log.error(DiagnosticFlag.SOURCE_LEVEL, tree.pos(),\n+                          Feature.REIFIABLE_TYPES_INSTANCEOF.error(this.sourceName));\n+                allowReifiableTypesInInstanceof = true;\n@@ -4107,6 +4111,8 @@\n-        tree.type = attribTree(tree.vartype, env, varInfo);\n-        VarSymbol v = tree.symbol = new BindingSymbol(tree.name, tree.vartype.type, env.info.scope.owner);\n-        if (chk.checkUnique(tree.pos(), v, env.info.scope)) {\n-            chk.checkTransparentVar(tree.pos(), v, env.info.scope);\n-        }\n-        annotate.queueScanTreeAndTypeAnnotate(tree.vartype, env, v, tree.pos());\n+        tree.type = tree.var.type = attribTree(tree.var.vartype, env, varInfo);\n+        BindingSymbol v = new BindingSymbol(tree.var.name, tree.var.vartype.type, env.info.scope.owner);\n+        v.pos = tree.pos;\n+        tree.var.sym = v;\n+        if (chk.checkUnique(tree.var.pos(), v, env.info.scope)) {\n+            chk.checkTransparentVar(tree.var.pos(), v, env.info.scope);\n+        }\n+        annotate.queueScanTreeAndTypeAnnotate(tree.var.vartype, env, v, tree.var.pos());\n@@ -4115,1 +4121,1 @@\n-        matchBindings = new MatchBindings(List.of(tree.symbol), List.nil());\n+        matchBindings = new MatchBindings(List.of(v), List.nil());\n@@ -5902,3 +5908,3 @@\n-            if (that.symbol == null) {\n-                that.symbol = new BindingSymbol(that.name, that.type, syms.noSymbol);\n-                that.symbol.adr = 0;\n+            if (that.var.sym == null) {\n+                that.var.sym = new BindingSymbol(that.var.name, that.var.type, syms.noSymbol);\n+                that.var.sym.adr = 0;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":21,"deletions":15,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -1913,1 +1913,2 @@\n-                !inits.isMember(sym.adr)) {\n+                !inits.isMember(sym.adr) &&\n+                (sym.flags_field & CLASH) == 0) {\n@@ -2828,0 +2829,6 @@\n+        @Override\n+        public void visitBindingPattern(JCBindingPattern tree) {\n+            super.visitBindingPattern(tree);\n+            initParam(tree.var);\n+        }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Flow.java","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -585,3 +585,1 @@\n-        if (tree.vartype != null) {\n-            tree.vartype = translate(tree.vartype, null);\n-        }\n+        tree.var = translate(tree.var, null);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/TransTypes.java","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2629,0 +2629,2 @@\n+        boolean previewClassFile =\n+                minorVersion == ClassFile.PREVIEW_MINOR_VERSION;\n@@ -2632,1 +2634,1 @@\n-            if (majorVersion == (maxMajor + 1))\n+            if (majorVersion == (maxMajor + 1) && !previewClassFile)\n@@ -2644,1 +2646,1 @@\n-        if (minorVersion == ClassFile.PREVIEW_MINOR_VERSION) {\n+        if (previewClassFile) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassReader.java","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -966,1 +966,3 @@\n-                JCTree pattern = parseType();\n+                int typePos = token.pos;\n+                JCExpression type = parseType();\n+                JCTree pattern;\n@@ -969,1 +971,7 @@\n-                    pattern = toP(F.at(token.pos).BindingPattern(ident(), pattern));\n+                    JCModifiers mods = F.at(Position.NOPOS).Modifiers(0);\n+                    JCVariableDecl var = toP(F.at(token.pos).VarDef(mods, ident(), type, null));\n+                    TreeInfo.getStartPos(var);\n+                    pattern = toP(F.at(typePos).BindingPattern(var));\n+                    TreeInfo.getStartPos(pattern);\n+                } else {\n+                    pattern = type;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -552,4 +552,0 @@\n-# 0: symbol\n-compiler.err.pattern.binding.may.not.be.assigned=\\\n-    pattern binding {0} may not be assigned\n-\n@@ -627,3 +623,0 @@\n-compiler.err.illegal.generic.type.for.instof=\\\n-    illegal generic type for instanceof\n-\n@@ -1425,0 +1418,4 @@\n+# 0: type, 1: type\n+compiler.err.instanceof.pattern.no.subtype=\\\n+    pattern type {0} is a subtype of expression type {1}\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":4,"deletions":7,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2203,1 +2203,1 @@\n-        public JCTree getType() { return pattern instanceof JCPattern ? pattern.hasTag(BINDINGPATTERN) ? ((JCBindingPattern) pattern).vartype : null : pattern; }\n+        public JCTree getType() { return pattern instanceof JCPattern ? pattern.hasTag(BINDINGPATTERN) ? ((JCBindingPattern) pattern).var.vartype : null : pattern; }\n@@ -2227,3 +2227,0 @@\n-        public JCExpression constExpression() {\n-            return null;\n-        }\n@@ -2234,9 +2231,1 @@\n-        public Name name;\n-        public BindingSymbol symbol;\n-        public JCTree vartype;\n-\n-        protected JCBindingPattern(Name name, BindingSymbol symbol, JCTree vartype) {\n-            this.name = name;\n-            this.symbol = symbol;\n-            this.vartype = vartype;\n-        }\n+        public JCVariableDecl var;\n@@ -2244,3 +2233,2 @@\n-        @DefinedBy(Api.COMPILER_TREE)\n-        public Name getBinding() {\n-            return name;\n+        protected JCBindingPattern(JCVariableDecl var) {\n+            this.var = var;\n@@ -2250,2 +2238,2 @@\n-        public Tree getType() {\n-            return vartype;\n+        public VariableTree getVariable() {\n+            return var;\n@@ -3250,1 +3238,1 @@\n-        JCBindingPattern BindingPattern(Name name, JCTree vartype);\n+        JCBindingPattern BindingPattern(JCVariableDecl var);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/JCTree.java","additions":7,"deletions":19,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -910,3 +910,1 @@\n-            printExpr(patt.vartype);\n-            print(\" \");\n-            print(patt.name);\n+            printExpr(patt.var);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/Pretty.java","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -498,2 +498,2 @@\n-        JCTree vartype = copy(t.vartype, p);\n-        return M.at(t.pos).BindingPattern(t.name, vartype);\n+        JCVariableDecl var = copy(t.var, p);\n+        return M.at(t.pos).BindingPattern(var);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeCopier.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+import com.sun.tools.javac.code.Symbol.VarSymbol;\n@@ -541,1 +542,1 @@\n-                return getStartPos(node.vartype);\n+                return getStartPos(node.var);\n@@ -934,2 +935,0 @@\n-        case BINDINGPATTERN:\n-            return ((JCBindingPattern) node).symbol;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeInfo.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -485,2 +485,2 @@\n-    public JCBindingPattern BindingPattern(Name name, JCTree vartype) {\n-        JCBindingPattern tree = new JCBindingPattern(name, null, vartype);\n+    public JCBindingPattern BindingPattern(JCVariableDecl var) {\n+        JCBindingPattern tree = new JCBindingPattern(var);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeMaker.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -312,2 +312,1 @@\n-        if (tree.vartype != null)\n-            scan(tree.vartype);\n+        scan(tree.var);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeScanner.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -368,1 +368,1 @@\n-        tree.vartype = translate(tree.vartype);\n+        tree.var = translate(tree.var);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeTranslator.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1011,10 +1011,3 @@\n-    while (JNI_TRUE) {\n-        error = JVMTI_FUNC_PTR(gdata->jvmti,RawMonitorEnter)\n-                        (gdata->jvmti, monitor);\n-        error = ignore_vm_death(error);\n-        if (error == JVMTI_ERROR_INTERRUPT) {\n-            handleInterrupt();\n-        } else {\n-            break;\n-        }\n-    }\n+    error = JVMTI_FUNC_PTR(gdata->jvmti,RawMonitorEnter)\n+            (gdata->jvmti, monitor);\n+    error = ignore_vm_death(error);\n","filename":"src\/jdk.jdwp.agent\/share\/native\/libjdwp\/util.c","additions":3,"deletions":10,"binary":false,"changes":13,"status":"modified"}]}