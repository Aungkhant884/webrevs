{"files":[{"patch":"@@ -734,0 +734,1 @@\n+    case new_flat_array_id:\n@@ -741,1 +742,2 @@\n-        } else {\n+        }\n+        else if (id == new_object_array_id) {\n@@ -744,0 +746,3 @@\n+        else {\n+          __ set_info(\"new_flat_array\", dont_gc_arguments);\n+        }\n@@ -752,3 +757,8 @@\n-          int tag = ((id == new_type_array_id)\n-                     ? Klass::_lh_array_tag_type_value\n-                     : Klass::_lh_array_tag_obj_value);\n+\n+          int tag = 0;\n+          switch (id) {\n+           case new_type_array_id: tag = Klass::_lh_array_tag_type_value; break;\n+           case new_object_array_id: tag = Klass::_lh_array_tag_obj_value; break;\n+           case new_flat_array_id: tag = Klass::_lh_array_tag_vt_value; break;\n+           default:  ShouldNotReachHere();\n+          }\n@@ -815,0 +825,1 @@\n+          \/\/ Runtime1::new_object_array handles both object and flat arrays\n@@ -850,0 +861,78 @@\n+    case buffer_inline_args_id:\n+    case buffer_inline_args_no_receiver_id:\n+    {\n+        const char* name = (id == buffer_inline_args_id) ?\n+          \"buffer_inline_args\" : \"buffer_inline_args_no_receiver\";\n+        StubFrame f(sasm, name, dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 2);\n+        Register method = r1;\n+        address entry = (id == buffer_inline_args_id) ?\n+          CAST_FROM_FN_PTR(address, buffer_inline_args) :\n+          CAST_FROM_FN_PTR(address, buffer_inline_args_no_receiver);\n+        int call_offset = __ call_RT(r0, noreg, entry, method);\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+        __ verify_oop(r0);  \/\/ r0: an array of buffered value objects\n+     }\n+     break;\n+\n+    case load_flattened_array_id:\n+      {\n+        StubFrame f(sasm, \"load_flattened_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 3);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, r0); \/\/ r0,: array\n+        f.load_argument(0, r1); \/\/ r1,: index\n+        int call_offset = __ call_RT(r0, noreg, CAST_FROM_FN_PTR(address, load_flattened_array), r0, r1);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+\n+        \/\/ r0: loaded element at array[index]\n+        __ verify_oop(r0);\n+      }\n+      break;\n+\n+    case store_flattened_array_id:\n+      {\n+        StubFrame f(sasm, \"store_flattened_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 4);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(2, r0); \/\/ r0: array\n+        f.load_argument(1, r1); \/\/ r1: index\n+        f.load_argument(0, r2); \/\/ r2: value\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, store_flattened_array), r0, r1, r2);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+      }\n+      break;\n+\n+      case substitutability_check_id:\n+      {\n+        StubFrame f(sasm, \"substitutability_check\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 3);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, r0); \/\/ r0,: left\n+        f.load_argument(0, r1); \/\/ r1,: right\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, substitutability_check), r0, r1);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+\n+        \/\/ r0,: are the two operands substitutable\n+      }\n+      break;\n+\n+\n+\n@@ -889,1 +978,1 @@\n-      { StubFrame f(sasm, \"throw_incompatible_class_cast_exception\", dont_gc_arguments);\n+      { StubFrame f(sasm, \"throw_incompatible_class_change_exception\", dont_gc_arguments);\n@@ -894,0 +983,6 @@\n+    case throw_illegal_monitor_state_exception_id:\n+      { StubFrame f(sasm, \"throw_illegal_monitor_state_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_illegal_monitor_state_exception), false);\n+      }\n+      break;\n+\n@@ -1097,0 +1192,2 @@\n+      \/\/ FIXME: For unhandled trap_id this code fails with assert during vm intialization\n+      \/\/ rather than insert a call to unimplemented_entry\n@@ -1105,0 +1202,2 @@\n+\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":104,"deletions":5,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -589,0 +589,1 @@\n+    case T_INLINE_TYPE :\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -1290,1 +1291,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1320,1 +1325,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1412,0 +1421,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -1461,0 +1474,33 @@\n+void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label& is_value) {\n+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  andr(temp_reg, temp_reg, JVM_ACC_INLINE);\n+  cbnz(temp_reg, is_value);\n+}\n+\n+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_inline_field_shift, is_inline);\n+}\n+\n+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbz(flags, ConstantPoolCacheEntry::is_inline_field_shift, not_inline);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label& is_flattened_array) {\n+  load_storage_props(temp_reg, oop);\n+  andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);\n+  cbnz(temp_reg, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  load_storage_props(temp_reg, oop);\n+  andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);\n+  cbnz(temp_reg, is_null_free_array);\n+}\n+\n@@ -3774,1 +3820,1 @@\n-void MacroAssembler::load_klass(Register dst, Register src) {\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n@@ -3777,1 +3823,0 @@\n-    decode_klass_not_null(dst);\n@@ -3783,0 +3828,10 @@\n+void MacroAssembler::load_klass(Register dst, Register src) {\n+  load_metadata(dst, src);\n+  if (UseCompressedClassPointers) {\n+    andr(dst, dst, oopDesc::compressed_klass_mask());\n+    decode_klass_not_null(dst);\n+  } else {\n+    ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);\n+  }\n+}\n+\n@@ -3814,0 +3869,9 @@\n+void MacroAssembler::load_storage_props(Register dst, Register src) {\n+  load_metadata(dst, src);\n+  if (UseCompressedClassPointers) {\n+    asrw(dst, dst, oopDesc::narrow_storage_props_shift);\n+  } else {\n+    asr(dst, dst, oopDesc::wide_storage_props_shift);\n+  }\n+}\n+\n@@ -4151,1 +4215,2 @@\n-                                     Register tmp1, Register thread_tmp) {\n+                                     Register tmp1, Register thread_tmp, Register tmp3) {\n+\n@@ -4156,1 +4221,1 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4158,1 +4223,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4182,2 +4247,2 @@\n-                                    Register thread_tmp, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);\n+                                    Register thread_tmp, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4188,1 +4253,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -5288,0 +5353,339 @@\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+\n+\/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->frame_size_in_bytes();\n+  assert(framesize % (2 * wordSize) == 0, \"must preserve 2 * wordSize alignment\");\n+\n+  \/\/ insert a nop at the start of the prolog so we can patch in a\n+  \/\/ branch if we need to invalidate the method later\n+  nop();\n+\n+  int bangsize = C->bang_size_in_bytes();\n+  if (C->need_stack_bang(bangsize) && UseStackBanging)\n+     generate_stack_overflow_check(bangsize);\n+\n+  build_frame(framesize);\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  cmp(r0, (u1) 1);\n+  br(Assembler::EQ, skip);\n+  int call_offset = -1;\n+\n+  Label slow_case;\n+\n+  \/\/ Try to allocate a new buffered inline type (from the heap)\n+  if (UseTLAB) {\n+\n+    if (vk != NULL) {\n+      \/\/ Called from C1, where the return type is statically known.\n+      mov(r1, (intptr_t)vk->get_InlineKlass());\n+      jint lh = vk->layout_helper();\n+      assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+      mov(r14, lh);\n+    } else {\n+       \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+       andr(r1, r0, -2);\n+       \/\/ get obj size\n+       ldrw(r14, Address(rscratch1 \/*klass*\/, Klass::layout_helper_offset()));\n+    }\n+\n+     ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+\n+     \/\/ check whether we have space in TLAB,\n+     \/\/ rscratch1 contains pointer to just allocated obj\n+      lea(r14, Address(r13, r14));\n+      ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));\n+\n+      cmp(r14, rscratch1);\n+      br(Assembler::GT, slow_case);\n+\n+      \/\/ OK we have room in TLAB,\n+      \/\/ Set new TLAB top\n+      str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+\n+      \/\/ Set new class always locked\n+      mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());\n+      str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));\n+\n+      store_klass_gap(r13, zr);  \/\/ zero klass gap for compressed oops\n+      if (vk == NULL) {\n+        \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n+         mov(r0, r1);\n+      }\n+\n+      store_klass(r13, r1);  \/\/ klass\n+\n+      if (vk != NULL) {\n+        \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+        mov(r0, r13);\n+        far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+      } else {\n+\n+        \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+        ldr(r1, Address(r0, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+        ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));\n+\n+        \/\/ Mov new class to r0 and call pack_handler\n+        mov(r0, r13);\n+        blr(r1);\n+      }\n+      b(skip);\n+  }\n+\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    ldr(rscratch1, RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    blr(rscratch1);\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        mov(to->as_Register(), from->as_Register());\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,\n+                                          int& to_index, RegState reg_state[]) {\n+  Register fromReg = from->is_reg() ? from->as_Register() : noreg;\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+\n+\n+  int vt = 1;\n+  bool done = true;\n+  bool mark_done = true;\n+  do {\n+    sig_index--;\n+    BasicType bt = sig->at(sig_index)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      vt--;\n+    } else if (bt == T_VOID &&\n+               sig->at(sig_index-1)._bt != T_LONG &&\n+               sig->at(sig_index-1)._bt != T_DOUBLE) {\n+      vt++;\n+    } else {\n+      assert(to_index >= 0, \"invalid to_index\");\n+      VMRegPair pair_to = regs_to[to_index--];\n+      VMReg to = pair_to.first();\n+\n+      if (bt == T_VOID) continue;\n+\n+      int idx = (int) to->value();\n+      if (reg_state[idx] == reg_readonly) {\n+         if (idx != from->value()) {\n+           mark_done = false;\n+         }\n+         done = false;\n+         continue;\n+      } else if (reg_state[idx] == reg_written) {\n+        continue;\n+      } else {\n+        assert(reg_state[idx] == reg_writable, \"must be writable\");\n+        reg_state[idx] = reg_written;\n+      }\n+\n+      if (fromReg == noreg) {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        ldr(rscratch2, Address(sp, st_off));\n+        fromReg = rscratch2;\n+      }\n+\n+      int off = sig->at(sig_index)._offset;\n+      assert(off > 0, \"offset in object should be positive\");\n+      bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+\n+      Address fromAddr = Address(fromReg, off);\n+      bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+\n+      if (!to->is_FloatRegister()) {\n+\n+        Register dst = to->is_stack() ? rscratch1 : to->as_Register();\n+\n+        if (is_oop) {\n+          load_heap_oop(dst, fromAddr);\n+        } else {\n+          load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+        }\n+        if (to->is_stack()) {\n+          int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+          str(dst, Address(sp, st_off));\n+        }\n+      } else {\n+        if (bt == T_DOUBLE) {\n+          ldrd(to->as_FloatRegister(), fromAddr);\n+        } else {\n+          assert(bt == T_FLOAT, \"must be float\");\n+          ldrs(to->as_FloatRegister(), fromAddr);\n+        }\n+     }\n+\n+    }\n+\n+  } while (vt != 0);\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"must be\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  Register val_array = r0;\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r10;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r1;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);\n+  VMRegPair from_pair;\n+  BasicType bt;\n+\n+  while (stream.next(from_pair, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    VMReg from_r1 = from_pair.first();\n+    VMReg from_r2 = from_pair.second();\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+\n+    if (!from_r1->is_FloatRegister()) {\n+      Register from_reg;\n+      if (from_r1->is_stack()) {\n+        from_reg = from_reg_tmp;\n+        int ld_off = from_r1->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        from_reg = from_r1->as_Register();\n+      }\n+\n+      if (is_oop) {\n+        DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;\n+        store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);\n+      } else {\n+        store_sized_value(dst, from_reg, size_in_bytes);\n+      }\n+    } else {\n+      if (from_r2->is_valid()) {\n+        strd(from_r1->as_FloatRegister(), dst);\n+      } else {\n+        strs(from_r1->as_FloatRegister(), dst);\n+      }\n+    }\n+\n+    reg_state[from_r1->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v0->as_VMReg() : r14->as_VMReg();\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":414,"deletions":10,"binary":false,"changes":424,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -330,0 +331,1 @@\n+    case T_INLINE_TYPE:\n@@ -363,0 +365,84 @@\n+\n+\/\/ const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_int = 6;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt, VMRegPair *regs, int total_args_passed) {\n+\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  \/\/ r1, r2 used to address klasses and states, exclude it from return convention to avoid colision\n+\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+     r0 \/* j_rarg7 *\/, j_rarg6, j_rarg5, j_rarg4, j_rarg3, j_rarg2\n+  };\n+\n+  static const FloatRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3, j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        \/\/ Should we have gurantee here?\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+      \/\/ Should T_METADATA be added to java_calling_convention as well ?\n+    case T_METADATA:\n+    case T_INLINE_TYPE:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -396,19 +482,40 @@\n-static void gen_c2i_adapter(MacroAssembler *masm,\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n-                            const VMRegPair *regs,\n-                            Label& skip_fixup) {\n-  \/\/ Before we get into the guts of the C2I adapter, see if we should be here\n-  \/\/ at all.  We've come from compiled code and are attempting to jump to the\n-  \/\/ interpreter, which means the caller made a static call to get here\n-  \/\/ (vcalls always get a compiled target if there is one).  Check for a\n-  \/\/ compiled target.  If there is one, we need to patch the caller's call.\n-  patch_callers_callsite(masm);\n-\n-  __ bind(skip_fixup);\n-\n-  int words_pushed = 0;\n-\n-  \/\/ Since all args are passed on the stack, total_args_passed *\n-  \/\/ Interpreter::stackElementSize is the space we need.\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+     for (int i = 0; i < sig_extended->length(); i++) {\n+       BasicType bt = sig_extended->at(i)._bt;\n+       if (bt == T_INLINE_TYPE) {\n+         \/\/ In sig_extended, an inline type argument starts with:\n+         \/\/ T_INLINE_TYPE, followed by the types of the fields of the\n+         \/\/ inline type and T_VOID to mark the end of the value\n+         \/\/ type. Inline types are flattened so, for instance, in the\n+         \/\/ case of an inline type with an int field and an inline type\n+         \/\/ field that itself has 2 fields, an int and a long:\n+         \/\/ T_INLINE_TYPE T_INT T_INLINE_TYPE T_INT T_LONG T_VOID (second\n+         \/\/ slot for the T_LONG) T_VOID (inner T_INLINE_TYPE) T_VOID\n+         \/\/ (outer T_INLINE_TYPE)\n+         total_args_passed++;\n+         int vt = 1;\n+         do {\n+           i++;\n+           BasicType bt = sig_extended->at(i)._bt;\n+           BasicType prev_bt = sig_extended->at(i-1)._bt;\n+           if (bt == T_INLINE_TYPE) {\n+             vt++;\n+           } else if (bt == T_VOID &&\n+                      prev_bt != T_LONG &&\n+                      prev_bt != T_DOUBLE) {\n+             vt--;\n+           }\n+         } while (vt != 0);\n+       } else {\n+         total_args_passed++;\n+       }\n+     }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n@@ -416,1 +523,2 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+  return total_args_passed;\n+}\n@@ -418,3 +526,1 @@\n-  __ mov(r13, sp);\n-  \/\/ stack is aligned, keep it that way\n-  extraspace = align_up(extraspace, 2*wordSize);\n+static void gen_c2i_adapter_helper(MacroAssembler* masm, BasicType bt, const VMRegPair& reg_pair, int extraspace, const Address& to) {\n@@ -423,13 +529,1 @@\n-  if (extraspace)\n-    __ sub(sp, sp, extraspace);\n-\n-  \/\/ Now write the args into the outgoing interpreter space\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n+    assert(bt != T_INLINE_TYPE || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n@@ -450,2 +544,5 @@\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n+    \/\/ int next_off = st_off - Interpreter::stackElementSize;\n+\n+    VMReg r_1 = reg_pair.first();\n+    VMReg r_2 = reg_pair.second();\n+\n@@ -454,1 +551,1 @@\n-      continue;\n+      return;\n@@ -456,0 +553,1 @@\n+\n@@ -458,3 +556,2 @@\n-      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size\n-                    + extraspace\n-                    + words_pushed * wordSize);\n+      \/\/ words_pushed is always 0 so we don't use it.\n+      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace \/* + word_pushed * wordSize *\/);\n@@ -464,1 +561,1 @@\n-        __ str(rscratch1, Address(sp, st_off));\n+        __ str(rscratch1, to);\n@@ -467,16 +564,1 @@\n-\n-\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ str(rscratch1, Address(sp, next_off));\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaaaull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ str(rscratch1, Address(sp, st_off));\n-        }\n+        __ str(rscratch1, to);\n@@ -487,19 +569,1 @@\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ str(r, Address(sp, st_off));\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ jlong\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaabull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-          __ str(r, Address(sp, next_off));\n-        } else {\n-          __ str(r, Address(sp, st_off));\n-        }\n-      }\n+      __ str(r, to);\n@@ -510,1 +574,1 @@\n-        __ strs(r_1->as_FloatRegister(), Address(sp, st_off));\n+        __ strs(r_1->as_FloatRegister(), to);\n@@ -512,6 +576,1 @@\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaacull);\n-        __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        __ strd(r_1->as_FloatRegister(), Address(sp, next_off));\n+        __ strd(r_1->as_FloatRegister(), to);\n@@ -519,0 +578,153 @@\n+   }\n+}\n+\n+static void gen_c2i_adapter(MacroAssembler *masm,\n+                            const GrowableArray<SigEntry>* sig_extended,\n+                            const VMRegPair *regs,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+\n+  \/\/ Before we get into the guts of the C2I adapter, see if we should be here\n+  \/\/ at all.  We've come from compiled code and are attempting to jump to the\n+  \/\/ interpreter, which means the caller made a static call to get here\n+  \/\/ (vcalls always get a compiled target if there is one).  Check for a\n+  \/\/ compiled target.  If there is one, we need to patch the caller's call.\n+  patch_callers_callsite(masm);\n+\n+  __ bind(skip_fixup);\n+\n+  bool has_inline_argument = false;\n+\n+  if (InlineTypePassFieldsAsArgs) {\n+      \/\/ Is there an inline type argument?\n+     for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+       has_inline_argument = (sig_extended->at(i)._bt == T_INLINE_TYPE);\n+     }\n+     if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);\n+\n+      frame_complete = __ offset();\n+      address the_pc = __ pc();\n+\n+      __ set_last_Java_frame(noreg, noreg, the_pc, rscratch1);\n+\n+      __ mov(c_rarg0, rthread);\n+      __ mov(c_rarg1, r1);\n+      __ mov(c_rarg2, (int64_t)alloc_inline_receiver);\n+\n+      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+      __ blr(rscratch1);\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ cbz(r0, no_exception);\n+\n+      __ str(zr, Address(rthread, JavaThread::vm_result_offset()));\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ b(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(r10, rthread);\n+      __ get_vm_result_2(r1, rthread); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n+  int words_pushed = 0;\n+\n+  \/\/ Since all args are passed on the stack, total_args_passed *\n+  \/\/ Interpreter::stackElementSize is the space we need.\n+\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n+  int extraspace = (total_args_passed * Interpreter::stackElementSize) + wordSize;\n+\n+  \/\/ stack is aligned, keep it that way\n+  extraspace = align_up(extraspace, 2 * wordSize);\n+\n+  __ mov(r13, sp);\n+\n+  if (extraspace)\n+    __ sub(sp, sp, extraspace);\n+\n+  \/\/ Now write the args into the outgoing interpreter space\n+\n+  int ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+  bool has_oop_field = false;\n+\n+  for (int next_arg_comp = 0; next_arg_comp < total_args_passed; next_arg_comp++) {\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    \/\/ offset to start parameters\n+    int st_off   = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;\n+\n+    if (!InlineTypePassFieldsAsArgs || bt != T_INLINE_TYPE) {\n+      if (bt == T_VOID) {\n+         assert(next_arg_comp > 0 && (sig_extended->at(next_arg_comp - 1)._bt == T_LONG || sig_extended->at(next_arg_comp - 1)._bt == T_DOUBLE), \"missing half\");\n+         next_arg_int ++;\n+         continue;\n+       }\n+\n+       int next_off = st_off - Interpreter::stackElementSize;\n+       int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+\n+       gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp], extraspace, Address(sp, offset));\n+       next_arg_int ++;\n+   } else {\n+       ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_INLINE_TYPE);\n+      __ load_heap_oop(rscratch1, Address(r10, index));\n+      next_vt_arg++;\n+      next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of value\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp - 1)._bt;\n+        if (bt == T_INLINE_TYPE) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n+        } else {\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          assert(off > 0, \"offset in object should be positive\");\n+\n+          bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+          has_oop_field = has_oop_field || is_oop;\n+\n+          gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp - ignored], extraspace, Address(r11, off));\n+        }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ str(rscratch1, Address(sp, st_off));\n+   }\n+\n+  }\n+\n+\/\/ If an inline type was allocated and initialized, apply post barrier to all oop fields\n+  if (has_inline_argument && has_oop_field) {\n+    __ push(r13); \/\/ save senderSP\n+    __ push(r1); \/\/ save callee\n+    \/\/ Allocate argument register save area\n+    if (frame::arg_reg_save_area_bytes != 0) {\n+      __ sub(sp, sp, frame::arg_reg_save_area_bytes);\n@@ -520,0 +732,7 @@\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::apply_post_barriers), rthread, r10);\n+    \/\/ De-allocate argument register save area\n+    if (frame::arg_reg_save_area_bytes != 0) {\n+      __ add(sp, sp, frame::arg_reg_save_area_bytes);\n+    }\n+    __ pop(r1); \/\/ restore callee\n+    __ pop(r13); \/\/ restore sender SP\n@@ -528,0 +747,1 @@\n+void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm, int comp_args_on_stack, const GrowableArray<SigEntry>* sig, const VMRegPair *regs) {\n@@ -529,5 +749,0 @@\n-void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,\n-                                    int total_args_passed,\n-                                    int comp_args_on_stack,\n-                                    const BasicType *sig_bt,\n-                                    const VMRegPair *regs) {\n@@ -593,1 +808,1 @@\n-  int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)>>LogBytesPerWord;\n+  int comp_words_on_stack = 0;\n@@ -595,2 +810,3 @@\n-    __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n-    __ andr(sp, rscratch1, -16);\n+     comp_words_on_stack = align_up(comp_args_on_stack * VMRegImpl::stack_slot_size, wordSize) >> LogBytesPerWord;\n+     __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n+     __ andr(sp, rscratch1, -16);\n@@ -615,0 +831,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -617,2 +835,5 @@\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+    BasicType bt = sig->at(i)._bt;\n+\n+    assert(bt != T_INLINE_TYPE, \"i2c adapter doesn't unpack inline typ args\");\n+    if (bt == T_VOID) {\n+      assert(i > 0 && (sig->at(i - 1)._bt == T_LONG || sig->at(i - 1)._bt == T_DOUBLE), \"missing half\");\n@@ -623,0 +844,1 @@\n+    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(), \"scrambled load targets?\");\n@@ -624,3 +846,1 @@\n-    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),\n-            \"scrambled load targets?\");\n-    int ld_off = (total_args_passed - i - 1)*Interpreter::stackElementSize;\n+    int ld_off = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n@@ -641,1 +861,1 @@\n-      int st_off = regs[i].first()->reg2stack()*VMRegImpl::stack_slot_size;\n+      int st_off = regs[i].first()->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -658,2 +878,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n@@ -662,11 +881,28 @@\n-        __ str(rscratch2, Address(sp, st_off));\n-      }\n-    } else if (r_1->is_Register()) {  \/\/ Register argument\n-      Register r = r_1->as_Register();\n-      if (r_2->is_valid()) {\n-        \/\/\n-        \/\/ We are using two VMRegs. This can be either T_OBJECT,\n-        \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n-        \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n-        \/\/ So we must adjust where to pick up the data to match the\n-        \/\/ interpreter.\n+         __ str(rscratch2, Address(sp, st_off));\n+       }\n+     } else if (r_1->is_Register()) {  \/\/ Register argument\n+       Register r = r_1->as_Register();\n+       if (r_2->is_valid()) {\n+         \/\/\n+         \/\/ We are using two VMRegs. This can be either T_OBJECT,\n+         \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n+         \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n+         \/\/ So we must adjust where to pick up the data to match the\n+         \/\/ interpreter.\n+\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n+\n+         \/\/ this can be a misaligned move\n+         __ ldr(r, Address(esp, offset));\n+       } else {\n+         \/\/ sign extend and use a full word?\n+         __ ldrw(r, Address(esp, ld_off));\n+       }\n+     } else {\n+       if (!r_2->is_valid()) {\n+         __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n+       } else {\n+         __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n+       }\n+     }\n+   }\n@@ -674,17 +910,0 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n-\n-        \/\/ this can be a misaligned move\n-        __ ldr(r, Address(esp, offset));\n-      } else {\n-        \/\/ sign extend and use a full word?\n-        __ ldrw(r, Address(esp, ld_off));\n-      }\n-    } else {\n-      if (!r_2->is_valid()) {\n-        __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n-      } else {\n-        __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n-      }\n-    }\n-  }\n@@ -703,1 +922,0 @@\n-\n@@ -707,13 +925,1 @@\n-\/\/ ---------------------------------------------------------------\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                                            int total_args_passed,\n-                                                            int comp_args_on_stack,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n-  address i2c_entry = __ pc();\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n-\n-  address c2i_unverified_entry = __ pc();\n-  Label skip_fixup;\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n@@ -754,0 +960,34 @@\n+}\n+\n+\n+\/\/ ---------------------------------------------------------------\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n+                                                            int comp_args_on_stack,\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n+\n+  address i2c_entry = __ pc();\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n+\n+  address c2i_unverified_entry = __ pc();\n+  Label skip_fixup;\n+\n+  gen_inline_cache_check(masm, skip_fixup);\n+\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    Label unused;\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+    skip_fixup = unused;\n+  }\n@@ -755,0 +995,1 @@\n+  \/\/ Scalarized c2i adapter\n@@ -759,0 +1000,1 @@\n+\n@@ -761,4 +1003,4 @@\n-\n-      __ ldrw(rscratch1, Address(rmethod, Method::access_flags_offset()));\n-      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n-      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n+        Register flags  = rscratch1;\n+      __ ldrw(flags, Address(rmethod, Method::access_flags_offset()));\n+      __ tst(flags, JVM_ACC_STATIC);\n+      __ br(Assembler::NE, L_skip_barrier); \/\/ non-static\n@@ -768,3 +1010,6 @@\n-    __ load_method_holder(rscratch2, rmethod);\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    Register klass = rscratch1;\n+    __ load_method_holder(klass, rmethod);\n+    \/\/ We pass rthread to this function on x86\n+    __ clinit_barrier(klass, rscratch2, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n@@ -781,0 +1026,14 @@\n+  address c2i_unverified_inline_entry = c2i_unverified_entry;\n+\n+ \/\/ Non-scalarized c2i adapter\n+  address c2i_inline_entry = c2i_entry;\n+  if (regs != regs_cc) {\n+    Label inline_entry_skip_fixup;\n+    c2i_unverified_inline_entry = __ pc();\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    Label unused;\n+    gen_c2i_adapter(masm, sig, regs, inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+  }\n+\n@@ -782,1 +1041,8 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+\n+  \/\/ The c2i adapter might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+\n+  bool caller_must_gc_arguments = (regs != regs_cc);\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words + 10, oop_maps, caller_must_gc_arguments);\n+\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -825,0 +1091,1 @@\n+      case T_INLINE_TYPE:\n@@ -1639,0 +1906,1 @@\n+      case T_INLINE_TYPE:\n@@ -1826,0 +2094,1 @@\n+  case T_INLINE_TYPE:\n@@ -3076,0 +3345,105 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler _masm(&buffer);\n+  MacroAssembler* masm = &_masm;\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ strs(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_DOUBLE) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(r0, val);\n+      \/\/ We don't need barriers because the destination is a newly allocated object.\n+      \/\/ Also, we cannot use store_heap_oop(to, val) because it uses r8 as tmp.\n+      if (UseCompressedOops) {\n+        __ encode_heap_oop(val);\n+        __ str(val, to);\n+      } else {\n+        __ str(val, to);\n+      }\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ store_sized_value(to, r_1->as_Register(), size_in_bytes);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(lr);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ ldrs(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ ldrd(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+       assert_different_registers(r0, r_1->as_Register());\n+       __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(lr);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, unpack_fields_off);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":525,"deletions":151,"binary":false,"changes":676,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-define_pd_global(bool, TieredCompilation,              false);\n+define_pd_global(bool, TieredCompilation,              true);\n","filename":"src\/hotspot\/cpu\/x86\/c1_globals_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -140,1 +140,0 @@\n-      sender_unextended_sp = sender_sp;\n@@ -144,2 +143,2 @@\n-      saved_fp = (intptr_t*) *(sender_sp - frame::sender_sp_offset);\n-    }\n+      intptr_t** saved_fp_addr = (intptr_t**) (sender_sp - frame::sender_sp_offset);\n+      saved_fp = *saved_fp_addr;\n@@ -147,0 +146,4 @@\n+      \/\/ Repair the sender sp if this is a method with scalarized inline type args\n+      sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+      sender_unextended_sp = sender_sp;\n+    }\n@@ -445,3 +448,3 @@\n-  intptr_t* unextended_sp = sender_sp;\n-  \/\/ On Intel the return_address is always the word on the stack\n-  address sender_pc = (address) *(sender_sp-1);\n+#ifdef ASSERT\n+  address sender_pc_copy = (address) *(sender_sp-1);\n+#endif\n@@ -454,0 +457,16 @@\n+  \/\/ Repair the sender sp if the frame has been extended\n+  sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+\n+  \/\/ On Intel the return_address is always the word on the stack\n+  address sender_pc = (address) *(sender_sp-1);\n+\n+#ifdef ASSERT\n+  if (sender_pc != sender_pc_copy) {\n+    \/\/ When extending the stack in the callee method entry to make room for unpacking of value\n+    \/\/ type args, we keep a copy of the sender pc at the expected location in the callee frame.\n+    \/\/ If the sender pc is patched due to deoptimization, the copy is not consistent anymore.\n+    nmethod* nm = CodeCache::find_blob(sender_pc)->as_nmethod();\n+    assert(sender_pc == nm->deopt_mh_handler_begin() || sender_pc == nm->deopt_handler_begin(), \"unexpected sender pc\");\n+  }\n+#endif\n+\n@@ -458,1 +477,20 @@\n-    map->set_include_argument_oops(_cb->caller_must_gc_arguments(map->thread()));\n+    bool caller_args = _cb->caller_must_gc_arguments(map->thread());\n+#ifdef COMPILER1\n+    if (!caller_args) {\n+      nmethod* nm = _cb->as_nmethod_or_null();\n+      if (nm != NULL && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+          pc() < nm->verified_inline_entry_point()) {\n+        \/\/ The VEP and VIEP(RO) of C1-compiled methods call buffer_inline_args_xxx\n+        \/\/ before doing any argument shuffling, so we need to scan the oops\n+        \/\/ as the caller passes them.\n+        caller_args = true;\n+#ifdef ASSERT\n+        NativeCall* call = nativeCall_before(pc());\n+        address dest = call->destination();\n+        assert(dest == Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id) ||\n+               dest == Runtime1::entry_for(Runtime1::buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+      }\n+    }\n+#endif\n+    map->set_include_argument_oops(caller_args);\n@@ -470,1 +508,1 @@\n-  return frame(sender_sp, unextended_sp, *saved_fp_addr, sender_pc);\n+  return frame(sender_sp, sender_sp, *saved_fp_addr, sender_pc);\n@@ -477,1 +515,1 @@\n-  \/\/ Default is we done have to follow them. The sender_for_xxx will\n+  \/\/ Default is we don't have to follow them. The sender_for_xxx will\n@@ -582,0 +620,1 @@\n+    case T_INLINE_TYPE:\n@@ -683,0 +722,15 @@\n+\/\/ Check for a method with scalarized inline type arguments that needs\n+\/\/ a stack repair and return the repaired sender stack pointer.\n+intptr_t* frame::repair_sender_sp(intptr_t* sender_sp, intptr_t** saved_fp_addr) const {\n+  CompiledMethod* cm = _cb->as_compiled_method_or_null();\n+  if (cm != NULL && cm->needs_stack_repair()) {\n+    \/\/ The stack increment resides just below the saved rbp on the stack\n+    \/\/ and does not account for the return address.\n+    intptr_t* real_frame_size_addr = (intptr_t*) (saved_fp_addr - 1);\n+    int real_frame_size = ((*real_frame_size_addr) + wordSize) \/ wordSize;\n+    assert(real_frame_size >= _cb->frame_size() && real_frame_size <= 1000000, \"invalid frame size\");\n+    sender_sp = unextended_sp() + real_frame_size;\n+  }\n+  return sender_sp;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":63,"deletions":9,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -51,0 +52,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -54,0 +56,1 @@\n+#include \"vmreg_x86.inline.hpp\"\n@@ -55,0 +58,3 @@\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1629,0 +1635,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2655,0 +2665,140 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  testl(temp_reg, JVM_ACC_INLINE);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  testptr(object, object);\n+  jcc(Assembler::equal, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  movl(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  testl(temp_reg, InstanceKlass::misc_flags_is_empty_inline_type());\n+  jcc(Assembler::notZero, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::zero, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inlined_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inlined);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,\n+                                              Label&is_flattened_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flattened_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flattened_array_layout(temp_reg, is_flattened_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flattened_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flattened_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flattened_array_layout(temp_reg, is_non_flattened_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::nullfree_array_bit_in_place, true, is_null_free_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_null_free_array_layout(temp_reg, is_null_free_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::nullfree_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_null_free_array_layout(temp_reg, is_non_null_free_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_bit_inplace);\n+  jcc(Assembler::notZero, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_bit_inplace);\n+  jcc(Assembler::zero, is_non_null_free_array);\n+}\n+\n+\n@@ -3471,0 +3621,129 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax, according to barrier asm eden_allocate\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+    jcc(Assembler::equal, L);\n+    stop(\"klass not initialized\");\n+    bind(L);\n+  }\n+#endif\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+  const bool allow_shared_alloc =\n+    Universe::heap()->supports_inline_contig_alloc();\n+\n+  push(klass);\n+  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(klass);\n+#ifndef _LP64\n+  if (UseTLAB || allow_shared_alloc) {\n+    get_thread(thread);\n+  }\n+#endif \/\/ _LP64\n+\n+  if (UseTLAB) {\n+    tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    \/\/ Allocation in the shared Eden, if allowed.\n+    \/\/\n+    eden_allocate(thread, new_obj, layout_size, 0, t2, slow_case);\n+  }\n+\n+  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n+  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n+  if (UseTLAB || allow_shared_alloc) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      decrement(layout_size, sizeof(oopDesc));\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 1*oopSize), zero);\n+        NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 2*oopSize), zero));\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+#ifdef _LP64\n+    xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+#endif\n+    movptr(t2, klass);         \/\/ preserve klass\n+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    store_klass(new_obj, t2, tmp_store_klass);  \/\/ src klass reg is potentially compressed\n+\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -3548,0 +3827,50 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  movptr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(inline_klass, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  movptr(inline_klass, Address(inline_klass, index, Address::times_ptr));\n+}\n+\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  movptr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  movl(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  movptr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset, Address::times_1);\n+  load_heap_oop(obj, field);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n+\n@@ -3896,1 +4225,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -3955,1 +4288,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4457,0 +4794,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -4466,1 +4811,1 @@\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -4499,1 +4844,1 @@\n-                                     Register tmp1, Register tmp2) {\n+                                     Register tmp1, Register tmp2, Register tmp3) {\n@@ -4504,1 +4849,23 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+  } else {\n+    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+  }\n+}\n+\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n@@ -4506,1 +4873,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2);\n+    lea(data, Address(oop, offset));\n@@ -4510,0 +4877,18 @@\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE)));\n+}\n+\n@@ -4531,2 +4916,2 @@\n-                                    Register tmp2, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2);\n+                                    Register tmp2, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2, tmp3);\n@@ -4537,1 +4922,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -4851,0 +5236,1 @@\n+#ifdef COMPILER2\n@@ -4852,1 +5238,5 @@\n-void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n@@ -4905,0 +5295,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, C->output()->sp_inc_offset()), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -4933,5 +5329,1 @@\n-\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->nmethod_entry_barrier(this);\n-  }\n+#endif \/\/ COMPILER2\n@@ -4943,1 +5335,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp) {\n@@ -4949,1 +5341,1 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_512bit);\n+    evpbroadcastq(xtmp, val, AVX_512bit);\n@@ -4951,1 +5343,3 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -4953,1 +5347,2 @@\n-    pxor(xtmp, xtmp);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n@@ -4976,1 +5371,1 @@\n-    fill64_masked_avx(3, base, 0, xtmp, k2, cnt, rtmp, true);\n+    fill64_masked_avx(3, base, 0, xtmp, k2, cnt, val, true);\n@@ -4995,1 +5390,1 @@\n-    fill32_masked_avx(3, base, 0, xtmp, k2, cnt, rtmp);\n+    fill32_masked_avx(3, base, 0, xtmp, k2, cnt, val);\n@@ -5008,0 +5403,296 @@\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  Label slow_case;\n+\n+  \/\/ Try to allocate a new buffered inline type (from the heap)\n+  if (UseTLAB) {\n+    \/\/ FIXME -- for smaller code, the inline allocation (and the slow case) should be moved inside the pack handler.\n+    if (vk != NULL) {\n+      \/\/ Called from C1, where the return type is statically known.\n+      movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+      jint lh = vk->layout_helper();\n+      assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+      movl(r14, lh);\n+    } else {\n+      \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+      mov(rbx, rax);\n+      andptr(rbx, -2);\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+    }\n+\n+    movptr(r13, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));\n+    lea(r14, Address(r13, r14, Address::times_1));\n+    cmpptr(r14, Address(r15_thread, in_bytes(JavaThread::tlab_end_offset())));\n+    jcc(Assembler::above, slow_case);\n+    movptr(Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())), r14);\n+    movptr(Address(r13, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+\n+    xorl(rax, rax); \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(r13, rax);  \/\/ zero klass gap for compressed oops\n+\n+    if (vk == NULL) {\n+      \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n+      mov(rax, rbx);\n+    }\n+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    store_klass(r13, rbx, tmp_store_klass);  \/\/ klass\n+\n+    \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+    if (vk != NULL) {\n+      \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+      mov(rax, r13);\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      mov(rax, r13);\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must bevalid\");\n+  Register fromReg;\n+  if (from->is_reg()) {\n+    fromReg = from->as_Register();\n+  } else {\n+    int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+    movq(r10, Address(rsp, st_off));\n+    fromReg = r10;\n+  }\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  while (stream.next(toReg, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+     if (idx != from->value()) {\n+       mark_done = false;\n+     }\n+     done = false;\n+     continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    } else {\n+      assert(reg_state[idx] == reg_writable, \"must be writable\");\n+      reg_state[idx] = reg_written;\n+    }\n+\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? r13 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  Register val_array = rax;\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14; \/\/ Be careful with r14 because it's used for spilling\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  while (stream.next(fromReg, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+    reg_state[fromReg->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    movq(rbp, Address(rsp, initial_framesize));\n+    addq(rsp, Address(rsp, sp_inc_offset));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n@@ -5079,1 +5770,1 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp, bool is_large) {\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only) {\n@@ -5084,1 +5775,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"val register must be eax for rep stos\");\n@@ -5090,3 +5781,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n@@ -5106,1 +5794,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -5115,1 +5803,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -5119,1 +5807,1 @@\n-    xmm_clear_mem(base, cnt, tmp, xtmp);\n+    xmm_clear_mem(base, cnt, val, xtmp);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":717,"deletions":29,"binary":false,"changes":746,"status":"modified"},{"patch":"@@ -473,0 +473,1 @@\n+    case T_INLINE_TYPE:\n@@ -523,0 +524,9 @@\n+const uint SharedRuntime::java_return_convention_max_int = 1;\n+const uint SharedRuntime::java_return_convention_max_float = 1;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  Unimplemented();\n+  return 0;\n+}\n+\n@@ -584,3 +594,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>& sig_extended,\n@@ -588,1 +596,5 @@\n-                            Label& skip_fixup) {\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet*& oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words) {\n@@ -610,1 +622,1 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+  int extraspace = sig_extended.length() * Interpreter::stackElementSize;\n@@ -621,3 +633,3 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+  for (int i = 0; i < sig_extended.length(); i++) {\n+    if (sig_extended.at(i)._bt == T_VOID) {\n+      assert(i > 0 && (sig_extended.at(i-1)._bt == T_LONG || sig_extended.at(i-1)._bt == T_DOUBLE), \"missing half\");\n@@ -628,1 +640,1 @@\n-    int st_off = ((total_args_passed - 1) - i) * Interpreter::stackElementSize;\n+    int st_off = ((sig_extended.length() - 1) - i) * Interpreter::stackElementSize;\n@@ -678,1 +690,1 @@\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n+        if (sig_extended.at(i)._bt == T_LONG || sig_extended.at(i)._bt == T_DOUBLE) {\n@@ -695,1 +707,1 @@\n-        assert(sig_bt[i] == T_DOUBLE || sig_bt[i] == T_LONG, \"wrong type\");\n+        assert(sig_extended.at(i)._bt == T_DOUBLE || sig_extended.at(i)._bt == T_LONG, \"wrong type\");\n@@ -728,2 +740,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>& sig_extended,\n@@ -732,0 +743,1 @@\n+\n@@ -820,2 +832,2 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n+  for (int i = 0; i < sig_extended.length(); i++) {\n+    if (sig_extended.at(i)._bt == T_VOID) {\n@@ -824,1 +836,1 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      assert(i > 0 && (sig_extended.at(i-1)._bt == T_LONG || sig_extended.at(i-1)._bt == T_DOUBLE), \"missing half\");\n@@ -833,1 +845,1 @@\n-    int ld_off = (total_args_passed - i) * Interpreter::stackElementSize;\n+    int ld_off = (sig_extended.length() - i) * Interpreter::stackElementSize;\n@@ -874,1 +886,1 @@\n-        const int offset = (NOT_LP64(true ||) sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (NOT_LP64(true ||) sig_extended.at(i)._bt==T_LONG||sig_extended.at(i)._bt==T_DOUBLE)?\n@@ -892,1 +904,1 @@\n-        const int offset = (NOT_LP64(true ||) sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (NOT_LP64(true ||) sig_extended.at(i)._bt==T_LONG||sig_extended.at(i)._bt==T_DOUBLE)?\n@@ -940,2 +952,1 @@\n-                                                            int total_args_passed,\n-                                                            const BasicType *sig_bt,\n+                                                            const GrowableArray<SigEntry>& sig_extended,\n@@ -944,1 +955,2 @@\n-                                                            AdapterFingerPrint* fingerprint) {\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n@@ -947,1 +959,1 @@\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig_extended, regs);\n@@ -987,1 +999,4 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  OopMapSet* oop_maps = NULL;\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+  gen_c2i_adapter(masm, sig_extended, regs, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words);\n@@ -990,0 +1005,1 @@\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps);\n@@ -1013,0 +1029,1 @@\n+    case T_INLINE_TYPE:\n@@ -1704,0 +1721,1 @@\n+      case T_INLINE_TYPE:\n@@ -1885,0 +1903,1 @@\n+  case T_INLINE_TYPE:           \/\/ Really a handle\n@@ -2983,0 +3002,5 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  Unimplemented();\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":48,"deletions":24,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -513,0 +514,1 @@\n+    case T_INLINE_TYPE:\n@@ -546,0 +548,82 @@\n+\/\/ Same as java_calling_convention() but for multiple return\n+\/\/ values. There's no way to store them on the stack so if we don't\n+\/\/ have enough registers, multiple values can't be returned.\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    rax, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+  static const XMMRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3,\n+    j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_INLINE_TYPE:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+    case T_METADATA:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -588,0 +672,106 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+    for (int i = 0; i < sig_extended->length(); i++) {\n+      BasicType bt = sig_extended->at(i)._bt;\n+      if (bt == T_INLINE_TYPE) {\n+        \/\/ In sig_extended, an inline type argument starts with:\n+        \/\/ T_INLINE_TYPE, followed by the types of the fields of the\n+        \/\/ inline type and T_VOID to mark the end of the value\n+        \/\/ type. Inline types are flattened so, for instance, in the\n+        \/\/ case of an inline type with an int field and an inline type\n+        \/\/ field that itself has 2 fields, an int and a long:\n+        \/\/ T_INLINE_TYPE T_INT T_INLINE_TYPE T_INT T_LONG T_VOID (second\n+        \/\/ slot for the T_LONG) T_VOID (inner T_INLINE_TYPE) T_VOID\n+        \/\/ (outer T_INLINE_TYPE)\n+        total_args_passed++;\n+        int vt = 1;\n+        do {\n+          i++;\n+          BasicType bt = sig_extended->at(i)._bt;\n+          BasicType prev_bt = sig_extended->at(i-1)._bt;\n+          if (bt == T_INLINE_TYPE) {\n+            vt++;\n+          } else if (bt == T_VOID &&\n+                     prev_bt != T_LONG &&\n+                     prev_bt != T_DOUBLE) {\n+            vt--;\n+          }\n+        } while (vt != 0);\n+      } else {\n+        total_args_passed++;\n+      }\n+    }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  assert(bt != T_INLINE_TYPE || !InlineTypePassFieldsAsArgs, \"no inline type here\");\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"must be invalid\");\n+    return;\n+  }\n+\n+  if (!r_1->is_XMMRegister()) {\n+    Register val = rax;\n+    if (r_1->is_stack()) {\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, rscratch1);\n+    if (is_oop) {\n+      __ push(r13);\n+      __ push(rbx);\n+      __ store_heap_oop(to, val, rscratch1, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      __ pop(rbx);\n+      __ pop(r13);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    }\n+  }\n+}\n@@ -590,3 +780,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -594,1 +782,6 @@\n-                            Label& skip_fixup) {\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n@@ -604,0 +797,42 @@\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_INLINE_TYPE);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types.\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ false);\n+\n+      frame_complete = __ offset();\n+\n+      __ set_last_Java_frame(noreg, noreg, NULL);\n+\n+      __ mov(c_rarg0, r15_thread);\n+      __ mov(c_rarg1, rbx);\n+      __ mov64(c_rarg2, (int64_t)alloc_inline_receiver);\n+      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+      __ jcc(Assembler::equal, no_exception);\n+\n+      __ movptr(Address(r15_thread, JavaThread::vm_result_offset()), (int)NULL_WORD);\n+      __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+      __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(rscratch2, r15_thread); \/\/ Use rscratch2 (r11) as temporary because rscratch1 (r10) is trashed by movptr()\n+      __ get_vm_result_2(rbx, r15_thread); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n@@ -608,1 +843,1 @@\n-\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n@@ -626,46 +861,24 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n-\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n-    }\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rax\n-      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ movl(rax, Address(rsp, ld_off));\n-        __ movptr(Address(rsp, st_off), rax);\n-\n-      } else {\n-\n-        __ movq(rax, Address(rsp, ld_off));\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ movq(Address(rsp, next_off), rax);\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_INLINE_TYPE,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_INLINE_TYPE\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_INLINE_TYPE) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(rsp, offset), extraspace, false);\n+      next_arg_int++;\n@@ -674,7 +887,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n-          __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ movq(Address(rsp, st_off), rax);\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n+        __ movptr(Address(rsp, st_off), rax);\n@@ -682,16 +892,25 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ movl(Address(rsp, st_off), r);\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ long\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaab));\n-          __ movptr(Address(rsp, st_off), rax);\n-          __ movq(Address(rsp, next_off), r);\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_INLINE_TYPE);\n+      __ load_heap_oop(r14, Address(rscratch2, index));\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp-1)._bt;\n+        if (bt == T_INLINE_TYPE) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID &&\n+                   prev_bt != T_LONG &&\n+                   prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -700,1 +919,6 @@\n-          __ movptr(Address(rsp, st_off), r);\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(r14, off), extraspace, is_oop);\n@@ -702,14 +926,3 @@\n-      }\n-    } else {\n-      assert(r_1->is_XMMRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ movflt(Address(rsp, st_off), r_1->as_XMMRegister());\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov64(rax, CONST64(0xdeadffffdeadaaac));\n-        __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        __ movdbl(Address(rsp, next_off), r_1->as_XMMRegister());\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ movptr(Address(rsp, st_off), r14);\n@@ -738,2 +951,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>* sig,\n@@ -832,1 +1044,1 @@\n-  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_offset())));\n+  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_inline_offset())));\n@@ -846,0 +1058,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -849,1 +1063,3 @@\n-    if (sig_bt[i] == T_VOID) {\n+    BasicType bt = sig->at(i)._bt;\n+    assert(bt != T_INLINE_TYPE, \"i2c adapter doesn't unpack inline type args\");\n+    if (bt == T_VOID) {\n@@ -852,1 +1068,2 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      BasicType prev_bt = (i > 0) ? sig->at(i-1)._bt : T_ILLEGAL;\n+      assert(i > 0 && (prev_bt == T_LONG || prev_bt == T_DOUBLE), \"missing half\");\n@@ -894,1 +1111,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -909,1 +1126,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -940,1 +1157,1 @@\n-  \/\/ only needed becaus eof c2 resolve stubs return Method* as a result in\n+  \/\/ only needed because of c2 resolve stubs return Method* as a result in\n@@ -946,0 +1163,22 @@\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Label ok;\n+\n+  Register holder = rax;\n+  Register receiver = j_rarg0;\n+  Register temp = rbx;\n+\n+  __ load_klass(temp, receiver, rscratch1);\n+  __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n+  __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n+  __ jcc(Assembler::equal, ok);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+\n+  __ bind(ok);\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);\n+  __ jcc(Assembler::equal, skip_fixup);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n+\n@@ -948,4 +1187,8 @@\n-                                                            int total_args_passed,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n@@ -954,2 +1197,1 @@\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -968,11 +1210,1 @@\n-  Label ok;\n-\n-  Register holder = rax;\n-  Register receiver = j_rarg0;\n-  Register temp = rbx;\n-  {\n-    __ load_klass(temp, receiver, rscratch1);\n-    __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ jcc(Assembler::equal, ok);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -981,7 +1213,9 @@\n-    __ bind(ok);\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);\n-    __ jcc(Assembler::equal, skip_fixup);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+    skip_fixup.reset();\n@@ -990,0 +1224,1 @@\n+  \/\/ Scalarized c2i adapter\n@@ -1018,1 +1253,14 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, true);\n+\n+  address c2i_unverified_inline_entry = c2i_unverified_entry;\n+\n+  \/\/ Non-scalarized c2i adapter\n+  address c2i_inline_entry = c2i_entry;\n+  if (regs != regs_cc) {\n+    Label inline_entry_skip_fixup;\n+    c2i_unverified_inline_entry = __ pc();\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);\n+  }\n@@ -1021,1 +1269,7 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+\n+  \/\/ The c2i adapters might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  bool caller_must_gc_arguments = (regs != regs_cc);\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n+\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -1079,0 +1333,1 @@\n+      case T_INLINE_TYPE:\n@@ -2112,0 +2367,1 @@\n+      case T_INLINE_TYPE:\n@@ -2247,0 +2503,6 @@\n+    if (EnableValhalla) {\n+      assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      __ andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+    }\n+\n@@ -2306,0 +2568,1 @@\n+  case T_INLINE_TYPE:           \/\/ Really a handle\n@@ -4065,0 +4328,111 @@\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  __ movptr(rax, Address(r13, 0));\n+  __ resolve_jobject(rax \/* value *\/,\n+                     r15_thread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ movptr(Address(r13, 0), rax);\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r14, r13, rbx, rscratch1);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r14, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(0);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(rax, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(rax, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  if (StressInlineTypeReturnedAsFields) {\n+    __ load_klass(rax, rax, rscratch1);\n+    __ orptr(rax, 1);\n+  }\n+\n+  __ ret(0);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":497,"deletions":123,"binary":false,"changes":620,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -462,1 +463,3 @@\n-      (sym->char_at(1) == JVM_SIGNATURE_ARRAY || sym->char_at(1) == JVM_SIGNATURE_CLASS)) {\n+      (sym->char_at(1) == JVM_SIGNATURE_ARRAY ||\n+       sym->char_at(1) == JVM_SIGNATURE_CLASS ||\n+       sym->char_at(1) == JVM_SIGNATURE_INLINE_TYPE )) {\n@@ -475,1 +478,1 @@\n-      return ciObjArrayKlass::make_impl(elem_klass);\n+      return ciArrayKlass::make(elem_klass);\n@@ -501,0 +504,15 @@\n+  int i = 0;\n+  while (sym->char_at(i) == JVM_SIGNATURE_ARRAY) {\n+    i++;\n+  }\n+  if (i > 0 && sym->char_at(i) == JVM_SIGNATURE_INLINE_TYPE) {\n+    \/\/ An unloaded array class of inline types is an ObjArrayKlass, an\n+    \/\/ unloaded inline type class is an InstanceKlass. For consistency,\n+    \/\/ make the signature of the unloaded array of inline type use L\n+    \/\/ rather than Q.\n+    char *new_name = CURRENT_THREAD_ENV->name_buffer(sym->utf8_length()+1);\n+    strncpy(new_name, (char*)sym->base(), sym->utf8_length());\n+    new_name[i] = JVM_SIGNATURE_CLASS;\n+    new_name[sym->utf8_length()] = '\\0';\n+    return get_unloaded_klass(accessing_klass, ciSymbol::make(new_name));\n+  }\n@@ -531,1 +549,1 @@\n-    klass =  ConstantPool::klass_at_if_loaded(cpool, index);\n+    klass = ConstantPool::klass_at_if_loaded(cpool, index);\n@@ -583,0 +601,8 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciEnv::is_inline_klass\n+\/\/\n+\/\/ Check if the klass is an inline klass.\n+bool ciEnv::is_inline_klass(const constantPoolHandle& cpool, int index) {\n+  GUARDED_VM_ENTRY(return cpool->klass_name_at(index)->is_Q_signature();)\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":29,"deletions":3,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -138,0 +138,2 @@\n+  bool       is_inline_klass(const constantPoolHandle& cpool,\n+                             int klass_index);\n@@ -202,0 +204,4 @@\n+  ciFlatArrayKlass* get_flat_array_klass(Klass* o) {\n+    if (o == NULL) return NULL;\n+    return get_metadata(o)->as_flat_array_klass();\n+  }\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -39,0 +41,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -121,2 +124,3 @@\n-                                 jobject loader, jobject protection_domain)\n-  : ciKlass(name, T_OBJECT)\n+                                 jobject loader, jobject protection_domain,\n+                                 BasicType bt)\n+  : ciKlass(name, bt)\n@@ -128,1 +132,1 @@\n-  _nonstatic_fields = NULL;\n+  _nonstatic_fields = NULL;            \/\/ initialized lazily by compute_nonstatic_fields\n@@ -338,1 +342,1 @@\n-    _flags.print_klass_flags();\n+    _flags.print_klass_flags(st);\n@@ -342,1 +346,1 @@\n-      _super->print_name();\n+      _super->print_name_on(st);\n@@ -438,0 +442,23 @@\n+ciField* ciInstanceKlass::get_non_flattened_field_by_offset(int field_offset) {\n+  if (super() != NULL && super()->has_nonstatic_fields()) {\n+    ciField* f = super()->get_non_flattened_field_by_offset(field_offset);\n+    if (f != NULL) {\n+      return f;\n+    }\n+  }\n+\n+  VM_ENTRY_MARK;\n+  InstanceKlass* k = get_instanceKlass();\n+  Arena* arena = CURRENT_ENV->arena();\n+  for (JavaFieldStream fs(k); !fs.done(); fs.next()) {\n+    if (fs.access_flags().is_static())  continue;\n+    fieldDescriptor& fd = fs.field_descriptor();\n+    if (fd.offset() == field_offset) {\n+      ciField* f = new (arena) ciField(&fd);\n+      return f;\n+    }\n+  }\n+\n+  return NULL;\n+}\n+\n@@ -499,6 +526,1 @@\n-  int flen = fields->length();\n-\n-  \/\/ Now sort them by offset, ascending.\n-  \/\/ (In principle, they could mix with superclass fields.)\n-  fields->sort(sort_field_by_offset);\n-  return flen;\n+  return fields->length();\n@@ -508,3 +530,1 @@\n-GrowableArray<ciField*>*\n-ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>*\n-                                               super_fields) {\n+GrowableArray<ciField*>* ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields, bool flatten) {\n@@ -528,0 +548,1 @@\n+\n@@ -536,2 +557,22 @@\n-    ciField* field = new (arena) ciField(&fd);\n-    fields->append(field);\n+    if (fd.is_inlined() && flatten) {\n+      \/\/ Inline type fields are embedded\n+      int field_offset = fd.offset();\n+      \/\/ Get InlineKlass and adjust number of fields\n+      Klass* k = get_instanceKlass()->get_inline_type_field_klass(fd.index());\n+      ciInlineKlass* vk = CURRENT_ENV->get_klass(k)->as_inline_klass();\n+      flen += vk->nof_nonstatic_fields() - 1;\n+      \/\/ Iterate over fields of the flattened inline type and copy them to 'this'\n+      for (int i = 0; i < vk->nof_nonstatic_fields(); ++i) {\n+        ciField* flattened_field = vk->nonstatic_field_at(i);\n+        \/\/ Adjust offset to account for missing oop header\n+        int offset = field_offset + (flattened_field->offset() - vk->first_field_offset());\n+        \/\/ A flattened field can be treated as final if the non-flattened\n+        \/\/ field is declared final or the holder klass is an inline type itself.\n+        bool is_final = fd.is_final() || is_inlinetype();\n+        ciField* field = new (arena) ciField(flattened_field, this, offset, is_final);\n+        fields->append(field);\n+      }\n+    } else {\n+      ciField* field = new (arena) ciField(&fd);\n+      fields->append(field);\n+    }\n@@ -540,0 +581,3 @@\n+  \/\/ Now sort them by offset, ascending.\n+  \/\/ (In principle, they could mix with superclass fields.)\n+  fields->sort(sort_field_by_offset);\n@@ -637,0 +681,16 @@\n+bool ciInstanceKlass::can_be_inline_klass(bool is_exact) {\n+  if (!EnableValhalla) {\n+    return false;\n+  }\n+  if (!is_loaded() || is_inlinetype()) {\n+    \/\/ Not loaded or known to be an inline klass\n+    return true;\n+  }\n+  if (!is_exact) {\n+    \/\/ Not exact, check if this is a valid super for an inline klass\n+    VM_ENTRY_MARK;\n+    return !get_instanceKlass()->invalid_inline_super();\n+  }\n+  return false;\n+}\n+\n@@ -655,1 +715,2 @@\n-class StaticFinalFieldPrinter : public FieldClosure {\n+class StaticFieldPrinter : public FieldClosure {\n+protected:\n@@ -657,0 +718,8 @@\n+public:\n+  StaticFieldPrinter(outputStream* out) :\n+    _out(out) {\n+  }\n+  void do_field_helper(fieldDescriptor* fd, oop obj, bool flattened);\n+};\n+\n+class StaticFinalFieldPrinter : public StaticFieldPrinter {\n@@ -660,2 +729,1 @@\n-    _out(out),\n-    _holder(holder) {\n+    StaticFieldPrinter(out), _holder(holder) {\n@@ -666,46 +734,58 @@\n-      oop mirror = fd->field_holder()->java_mirror();\n-      _out->print(\"staticfield %s %s %s \", _holder, fd->name()->as_quoted_ascii(), fd->signature()->as_quoted_ascii());\n-      switch (fd->field_type()) {\n-        case T_BYTE:    _out->print_cr(\"%d\", mirror->byte_field(fd->offset()));   break;\n-        case T_BOOLEAN: _out->print_cr(\"%d\", mirror->bool_field(fd->offset()));   break;\n-        case T_SHORT:   _out->print_cr(\"%d\", mirror->short_field(fd->offset()));  break;\n-        case T_CHAR:    _out->print_cr(\"%d\", mirror->char_field(fd->offset()));   break;\n-        case T_INT:     _out->print_cr(\"%d\", mirror->int_field(fd->offset()));    break;\n-        case T_LONG:    _out->print_cr(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n-        case T_FLOAT: {\n-          float f = mirror->float_field(fd->offset());\n-          _out->print_cr(\"%d\", *(int*)&f);\n-          break;\n-        }\n-        case T_DOUBLE: {\n-          double d = mirror->double_field(fd->offset());\n-          _out->print_cr(INT64_FORMAT, *(int64_t*)&d);\n-          break;\n-        }\n-        case T_ARRAY:  \/\/ fall-through\n-        case T_OBJECT: {\n-          oop value =  mirror->obj_field_acquire(fd->offset());\n-          if (value == NULL) {\n-            _out->print_cr(\"null\");\n-          } else if (value->is_instance()) {\n-            assert(fd->field_type() == T_OBJECT, \"\");\n-            if (value->is_a(vmClasses::String_klass())) {\n-              const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n-              _out->print(\"\\\"%s\\\"\", (ascii_value != NULL) ? ascii_value : \"\");\n-            } else {\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print_cr(\"%s\", klass_name);\n-            }\n-          } else if (value->is_array()) {\n-            typeArrayOop ta = (typeArrayOop)value;\n-            _out->print(\"%d\", ta->length());\n-            if (value->is_objArray()) {\n-              objArrayOop oa = (objArrayOop)value;\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print(\" %s\", klass_name);\n-            }\n-            _out->cr();\n-          } else {\n-            ShouldNotReachHere();\n-          }\n-          break;\n+      InstanceKlass* holder = fd->field_holder();\n+      oop mirror = holder->java_mirror();\n+      _out->print(\"staticfield %s %s \", _holder, fd->name()->as_quoted_ascii());\n+      BasicType bt = fd->field_type();\n+      if (bt != T_OBJECT && bt != T_ARRAY) {\n+        _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+      }\n+      do_field_helper(fd, mirror, false);\n+      _out->cr();\n+    }\n+  }\n+};\n+\n+class InlineTypeFieldPrinter : public StaticFieldPrinter {\n+  oop _obj;\n+public:\n+  InlineTypeFieldPrinter(outputStream* out, oop obj) :\n+    StaticFieldPrinter(out), _obj(obj) {\n+  }\n+  void do_field(fieldDescriptor* fd) {\n+    do_field_helper(fd, _obj, true);\n+    _out->print(\" \");\n+  }\n+};\n+\n+void StaticFieldPrinter::do_field_helper(fieldDescriptor* fd, oop mirror, bool flattened) {\n+  BasicType bt = fd->field_type();\n+  switch (bt) {\n+    case T_BYTE:    _out->print(\"%d\", mirror->byte_field(fd->offset()));   break;\n+    case T_BOOLEAN: _out->print(\"%d\", mirror->bool_field(fd->offset()));   break;\n+    case T_SHORT:   _out->print(\"%d\", mirror->short_field(fd->offset()));  break;\n+    case T_CHAR:    _out->print(\"%d\", mirror->char_field(fd->offset()));   break;\n+    case T_INT:     _out->print(\"%d\", mirror->int_field(fd->offset()));    break;\n+    case T_LONG:    _out->print(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n+    case T_FLOAT: {\n+      float f = mirror->float_field(fd->offset());\n+      _out->print(\"%d\", *(int*)&f);\n+      break;\n+    }\n+    case T_DOUBLE: {\n+      double d = mirror->double_field(fd->offset());\n+      _out->print(INT64_FORMAT, *(int64_t*)&d);\n+      break;\n+    }\n+    case T_ARRAY:  \/\/ fall-through\n+    case T_OBJECT: {\n+      _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+      oop value =  mirror->obj_field_acquire(fd->offset());\n+      if (value == NULL) {\n+        _out->print_cr(\"null\");\n+      } else if (value->is_instance()) {\n+        assert(fd->field_type() == T_OBJECT, \"\");\n+        if (value->is_a(vmClasses::String_klass())) {\n+          const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n+          _out->print(\"\\\"%s\\\"\", (ascii_value != NULL) ? ascii_value : \"\");\n+         } else {\n+          const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+          _out->print(\"%s\", klass_name);\n@@ -713,2 +793,7 @@\n-        default:\n-          ShouldNotReachHere();\n+      } else if (value->is_array()) {\n+        typeArrayOop ta = (typeArrayOop)value;\n+        _out->print(\"%d\", ta->length());\n+        if (value->is_objArray() || value->is_flatArray()) {\n+          objArrayOop oa = (objArrayOop)value;\n+          const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+          _out->print(\" %s\", klass_name);\n@@ -716,0 +801,4 @@\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+      break;\n@@ -717,0 +806,25 @@\n+    case T_INLINE_TYPE: {\n+      ResetNoHandleMark rnhm;\n+      Thread* THREAD = Thread::current();\n+      SignatureStream ss(fd->signature(), false);\n+      Symbol* name = ss.as_symbol();\n+      assert(!HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+      InstanceKlass* holder = fd->field_holder();\n+      InstanceKlass* k = SystemDictionary::find_instance_klass(name,\n+                                                               Handle(THREAD, holder->class_loader()),\n+                                                               Handle(THREAD, holder->protection_domain()));\n+      assert(k != NULL && !HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+      InlineKlass* vk = InlineKlass::cast(k);\n+      oop obj;\n+      if (flattened) {\n+        int field_offset = fd->offset() - vk->first_field_offset();\n+        obj = (oop) (cast_from_oop<address>(mirror) + field_offset);\n+      } else {\n+        obj =  mirror->obj_field_acquire(fd->offset());\n+      }\n+      InlineTypeFieldPrinter print_field(_out, obj);\n+      vk->do_nonstatic_fields(&print_field);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n@@ -718,1 +832,1 @@\n-};\n+}\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.cpp","additions":182,"deletions":68,"binary":false,"changes":250,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+\n@@ -54,0 +55,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -87,0 +89,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -142,0 +145,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        61\n+\n@@ -181,1 +186,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -514,1 +519,8 @@\n-        cp->unresolved_klass_at_put(index, class_index, num_klasses++);\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n+        if (name->is_Q_signature()) {\n+          cp->unresolved_qdescriptor_at_put(index, class_index, num_klasses++);\n+        } else {\n+          cp->unresolved_klass_at_put(index, class_index, num_klasses++);\n+        }\n@@ -770,2 +782,2 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+            if (name != vmSymbols::object_initializer_name()) {\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -778,1 +790,12 @@\n-              if (name == vmSymbols::object_initializer_name()) {\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else if (!signature->is_void_method_signature()\n+                         && ref_kind == JVM_REF_invokeStatic) {\n+                \/\/ also OK, could be a static factory call\n+              } else {\n@@ -938,3 +961,3 @@\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n@@ -942,0 +965,7 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n+                                       bool* const is_declared_atomic,\n@@ -948,1 +978,1 @@\n-    _local_interfaces = Universe::the_empty_instance_klass_array();\n+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(0);\n@@ -951,3 +981,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);\n-\n-    int index;\n+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(itfs_len);\n+    int index = 0;\n@@ -989,1 +1018,14 @@\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+      InstanceKlass* ik = InstanceKlass::cast(interf);\n+      if (is_inline_type() && ik->invalid_inline_super()) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(\n+          THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_IncompatibleClassChangeError(),\n+          \"Inline type %s attempts to implement interface java.lang.IdentityObject\",\n+          _class_name->as_klass_external_name());\n+        return;\n+      }\n+      if (ik->invalid_inline_super()) {\n+        set_invalid_inline_super();\n+      }\n+      if (ik->has_nonstatic_concrete_methods()) {\n@@ -992,1 +1034,12 @@\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      if (ik->is_declared_atomic()) {\n+        *is_declared_atomic = true;\n+      }\n+      if (ik->name() == vmSymbols::java_lang_IdentityObject()) {\n+        _implements_identityObject = true;\n+      }\n+      if (ik->name() == vmSymbols::java_lang_PrimitiveObject()) {\n+        \/\/ further checks for \"is_invalid_super_for_inline_type()\" needed later\n+        \/\/ needs field parsing, delay unitl post_process_parse_stream()\n+        _implements_primitiveObject = true;\n+      }\n+      _temp_local_interfaces->append(ik);\n@@ -1010,1 +1063,1 @@\n-        const InstanceKlass* const k = _local_interfaces->at(index);\n+        const InstanceKlass* const k = _temp_local_interfaces->at(index);\n@@ -1497,0 +1550,1 @@\n+  STATIC_INLINE,        \/\/ inline type field\n@@ -1502,0 +1556,1 @@\n+  NONSTATIC_INLINE,\n@@ -1521,6 +1576,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  NONSTATIC_OOP,       \/\/ T_INLINE_TYPE = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20,\n@@ -1541,6 +1597,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  STATIC_OOP,          \/\/ T_INLINE_TYPE = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20\n@@ -1549,1 +1606,1 @@\n-static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type) {\n+static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type, bool is_inline_type) {\n@@ -1553,0 +1610,3 @@\n+  if (is_inline_type) {\n+    result = is_static ? STATIC_INLINE : NONSTATIC_INLINE;\n+  }\n@@ -1566,2 +1626,2 @@\n-  void update(bool is_static, BasicType type) {\n-    FieldAllocationType atype = basic_type_to_atype(is_static, type);\n+  void update(bool is_static, BasicType type, bool is_inline_type) {\n+    FieldAllocationType atype = basic_type_to_atype(is_static, type, is_inline_type);\n@@ -1580,0 +1640,1 @@\n+                                   bool is_inline_type,\n@@ -1602,1 +1663,5 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);\n@@ -1632,0 +1697,1 @@\n+  int instance_fields_count = 0;\n@@ -1636,0 +1702,4 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, is_interface, is_inline_type, CHECK);\n@@ -1637,2 +1707,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1654,0 +1722,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1713,1 +1782,1 @@\n-    fac->update(is_static, type);\n+    fac->update(is_static, type, type == T_INLINE_TYPE);\n@@ -1757,1 +1826,1 @@\n-      fac->update(false, type);\n+      fac->update(false, type, false);\n@@ -1762,0 +1831,27 @@\n+  if (is_inline_type) {\n+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);\n+    field->initialize(JVM_ACC_FIELD_INTERNAL | JVM_ACC_STATIC,\n+                      (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(default_value_name)),\n+                      (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(object_signature)),\n+                      0);\n+    const BasicType type = Signature::basic_type(vmSymbols::object_signature());\n+    fac->update(true, type, false);\n+    index++;\n+  }\n+\n+  if (is_inline_type && instance_fields_count == 0) {\n+    _is_empty_inline_type = true;\n+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);\n+    field->initialize(JVM_ACC_FIELD_INTERNAL,\n+        (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(empty_marker_name)),\n+        (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(byte_signature)),\n+        0);\n+    const BasicType type = Signature::basic_type(vmSymbols::byte_signature());\n+    fac->update(false, type, false);\n+    index++;\n+  }\n+\n+  if (instance_fields_count > 0) {\n+    _has_nonstatic_fields = true;\n+  }\n+\n@@ -2079,0 +2175,5 @@\n+  const char* class_note = \"\";\n+  if (is_inline_type() && name == vmSymbols::object_initializer_name()) {\n+    class_note = \" (an inline class)\";\n+  }\n+\n@@ -2082,2 +2183,2 @@\n-      \"%s \\\"%s\\\" in class %s has illegal signature \\\"%s\\\"\", type,\n-      name->as_C_string(), _class_name->as_C_string(), sig->as_C_string());\n+      \"%s \\\"%s\\\" in class %s%s has illegal signature \\\"%s\\\"\", type,\n+      name->as_C_string(), _class_name->as_C_string(), class_note, sig->as_C_string());\n@@ -2372,0 +2473,1 @@\n+                                      bool is_inline_type,\n@@ -2413,1 +2515,1 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n+    verify_legal_method_modifiers(flags, is_interface, is_inline_type, name, CHECK_NULL);\n@@ -2416,3 +2518,48 @@\n-  if (name == vmSymbols::object_initializer_name() && is_interface) {\n-    classfile_parse_error(\"Interface cannot have a method named <init>, class file %s\", THREAD);\n-    return NULL;\n+  if (name == vmSymbols::object_initializer_name()) {\n+    if (is_interface) {\n+      classfile_parse_error(\"Interface cannot have a method named <init>, class file %s\", THREAD);\n+      return NULL;\n+    } else if (!is_inline_type && signature->is_void_method_signature()) {\n+      \/\/ OK, a constructor\n+    } else if (is_inline_type && !signature->is_void_method_signature()) {\n+      \/\/ also OK, a static factory, as long as the return value is good\n+      bool ok = false;\n+      SignatureStream ss((Symbol*) signature, true);\n+      while (!ss.at_return_type())  ss.next();\n+      if (ss.is_reference()) {\n+        Symbol* ret = ss.as_symbol();\n+        const Symbol* required = class_name();\n+        if (is_hidden()) {\n+          \/\/ The original class name in hidden classes gets changed.  So using\n+          \/\/ the original name in the return type is no longer valid.\n+          \/\/ Note that expecting the return type for inline hidden class factory\n+          \/\/ methods to be java.lang.Object works around a JVM Spec issue for\n+          \/\/ hidden classes.\n+          required = vmSymbols::java_lang_Object();\n+        }\n+        ok = (ret == required);\n+      }\n+      if (!ok) {\n+        throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+      }\n+    } else {\n+      \/\/ not OK, so throw the same error as in verify_legal_method_signature.\n+      throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+    }\n+    \/\/ A declared <init> method must always be either a non-static\n+    \/\/ object constructor, with a void return, or else it must be a\n+    \/\/ static factory method, with a non-void return.  No other\n+    \/\/ definition of <init> is possible.\n+    \/\/\n+    \/\/ The verifier (in verify_invoke_instructions) will inspect the\n+    \/\/ signature of any attempt to invoke <init>, and ensures that it\n+    \/\/ returns non-void if and only if it is being invoked by\n+    \/\/ invokestatic, and void if and only if it is being invoked by\n+    \/\/ invokespecial.\n+    \/\/\n+    \/\/ When a symbolic reference to <init> is resolved for a\n+    \/\/ particular invocation mode (special or static), the mode is\n+    \/\/ matched to the JVM_ACC_STATIC modifier of the <init> method.\n+    \/\/ Thus, it is impossible to statically invoke a constructor, and\n+    \/\/ impossible to \"new + invokespecial\" a static factory, either\n+    \/\/ through bytecode or through reflection.\n@@ -2986,0 +3133,1 @@\n+                                    bool is_inline_type,\n@@ -3010,0 +3158,1 @@\n+                                    is_inline_type,\n@@ -3277,2 +3426,2 @@\n-    \/\/ Access flags\n-    jint flags;\n+\n+    jint recognized_modifiers = RECOGNIZED_INNER_CLASS_MODIFIERS;\n@@ -3281,3 +3430,1 @@\n-      flags = cfs->get_u2_fast() & (RECOGNIZED_INNER_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-    } else {\n-      flags = cfs->get_u2_fast() & RECOGNIZED_INNER_CLASS_MODIFIERS;\n+      recognized_modifiers |= JVM_ACC_MODULE;\n@@ -3285,0 +3432,8 @@\n+    \/\/ JVM_ACC_INLINE is defined for class file version 55 and later\n+    if (supports_inline_types()) {\n+      recognized_modifiers |= JVM_ACC_INLINE;\n+    }\n+\n+    \/\/ Access flags\n+    jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+\n@@ -3665,3 +3820,2 @@\n-  return _major_version == JVM_CLASSFILE_MAJOR_VERSION &&\n-         _minor_version == JAVA_PREVIEW_MINOR_VERSION &&\n-         Arguments::enable_preview();\n+  \/\/ temporarily disable the sealed type preview feature check\n+  return _major_version == JVM_CLASSFILE_MAJOR_VERSION;\n@@ -4137,1 +4291,2 @@\n-    check_property(_class_name == vmSymbols::java_lang_Object(),\n+    check_property(_class_name == vmSymbols::java_lang_Object()\n+                   || (_access_flags.get_flags() & JVM_ACC_INLINE),\n@@ -4280,0 +4435,19 @@\n+void ClassFileParser::throwInlineTypeLimitation(THREAD_AND_LOCATION_DECL,\n+                                                const char* msg,\n+                                                const Symbol* name,\n+                                                const Symbol* sig) const {\n+\n+  ResourceMark rm(THREAD);\n+  if (name == NULL || sig == NULL) {\n+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"class: %s - %s\", _class_name->as_C_string(), msg);\n+  }\n+  else {\n+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"\\\"%s\\\" sig: \\\"%s\\\" class: %s - %s\", name->as_C_string(), sig->as_C_string(),\n+        _class_name->as_C_string(), msg);\n+  }\n+}\n+\n@@ -4313,0 +4487,5 @@\n+      if (ik->is_inline_klass()) {\n+        Thread *THREAD = Thread::current();\n+        throwInlineTypeLimitation(THREAD_AND_LOCATION, \"Inline Types do not support Cloneable\");\n+        return;\n+      }\n@@ -4353,0 +4532,5 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 55 and later\n+  return _major_version >= JAVA_11_VERSION;\n+}\n+\n@@ -4396,3 +4580,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4419,0 +4604,8 @@\n+\n+    if (length == 1 && result->at(0) == vmClasses::IdentityObject_klass()) {\n+      return Universe::the_single_IdentityObject_klass_array();\n+    }\n+    if (length == 1 && result->at(0) == vmClasses::PrimitiveObject_klass()) {\n+      return Universe::the_single_PrimitiveObject_klass_array();\n+    }\n+\n@@ -4632,0 +4825,1 @@\n+  const bool is_inline_type = (flags & JVM_ACC_INLINE) != 0;\n@@ -4633,0 +4827,1 @@\n+  assert(supports_inline_types() || !is_inline_type, \"JVM_ACC_INLINE should not be set\");\n@@ -4643,0 +4838,11 @@\n+  if (is_inline_type && !EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    Exceptions::fthrow(\n+      THREAD_AND_LOCATION,\n+      vmSymbols::java_lang_ClassFormatError(),\n+      \"Class modifier ACC_INLINE in class %s requires option -XX:+EnableValhalla\",\n+      _class_name->as_C_string()\n+    );\n+    return;\n+  }\n+\n@@ -4657,1 +4863,2 @@\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (is_inline_type && (is_interface || is_abstract || is_enum || !is_final))) {\n@@ -4659,0 +4866,2 @@\n+    const char* class_note = \"\";\n+    if (is_inline_type)  class_note = \" (an inline class)\";\n@@ -4662,2 +4871,2 @@\n-      \"Illegal class modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags\n+      \"Illegal class modifiers in class %s%s: 0x%X\",\n+      _class_name->as_C_string(), class_note, flags\n@@ -4733,0 +4942,1 @@\n+                                                   bool is_inline_type,\n@@ -4757,0 +4967,4 @@\n+    } else {\n+      if (is_inline_type && !is_static && !is_final) {\n+        is_illegal = true;\n+      }\n@@ -4773,0 +4987,1 @@\n+                                                    bool is_inline_type,\n@@ -4793,0 +5008,1 @@\n+  const char* class_note = \"\";\n@@ -4827,1 +5043,1 @@\n-        if (is_static || is_final || is_synchronized || is_native ||\n+        if (is_final || is_synchronized || is_native ||\n@@ -4831,0 +5047,9 @@\n+        if (!is_static && !is_inline_type) {\n+          \/\/ OK, an object constructor in a regular class\n+        } else if (is_static && is_inline_type) {\n+          \/\/ OK, a static init factory in an inline class\n+        } else {\n+          \/\/ but no other combinations are allowed\n+          is_illegal = true;\n+          class_note = (is_inline_type ? \" (an inline class)\" : \" (not an inline class)\");\n+        }\n@@ -4832,4 +5057,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || is_strict)))) {\n-            is_illegal = true;\n+        if (is_inline_type && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (an inline class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || is_strict)))) {\n+              is_illegal = true;\n+            }\n@@ -4847,2 +5077,2 @@\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+      \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+      name->as_C_string(), _class_name->as_C_string(), class_note, flags);\n@@ -5006,1 +5236,10 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_INLINE_TYPE:\n+      \/\/ Can't enable this check until JDK upgrades the bytecode generators\n+      \/\/ if (_major_version < CONSTANT_CLASS_DESCRIPTORS ) {\n+      \/\/   classfile_parse_error(\"Class name contains illegal Q-signature \"\n+      \/\/                                    \"in descriptor in class file %s\",\n+      \/\/                                    CHECK_0);\n+      \/\/ }\n+      \/\/ fall through\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -5017,1 +5256,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -5073,0 +5312,3 @@\n+    } else if (_major_version >= CONSTANT_CLASS_DESCRIPTORS && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; and Q...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -5170,0 +5412,3 @@\n+  if (!supports_inline_types() && (signature->is_Q_signature() || signature->is_Q_array_signature())) {\n+    throwIllegalSignature(\"Field\", name, signature, CHECK);\n+  }\n@@ -5222,1 +5467,1 @@\n-        \/\/ All internal methods must return void\n+        \/\/ All constructor methods must return void\n@@ -5226,0 +5471,16 @@\n+        \/\/ All static init methods must return the current class\n+        if ((length >= 3) && (p[length-1] == JVM_SIGNATURE_ENDCLASS)\n+            && name == vmSymbols::object_initializer_name()) {\n+          nextp = skip_over_field_signature(p, true, length, CHECK_0);\n+          if (nextp && ((int)length == (nextp - p))) {\n+            \/\/ The actual class will be checked against current class\n+            \/\/ when the method is defined (see parse_method).\n+            \/\/ A reference to a static init with a bad return type\n+            \/\/ will load and verify OK, but will fail to link.\n+            return args_size;\n+          }\n+        }\n+        \/\/ The distinction between static factory methods and\n+        \/\/ constructors depends on the JVM_ACC_STATIC modifier.\n+        \/\/ This distinction must be reflected in a void or non-void\n+        \/\/ return. For declared methods, the check is in parse_method.\n@@ -5382,0 +5643,6 @@\n+  if (ik->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    oop val = ik->allocate_instance(CHECK_NULL);\n+    vk->set_default_value(val);\n+  }\n+\n@@ -5385,0 +5652,34 @@\n+\/\/ Return true if the specified class is not a valid super class for an inline type.\n+\/\/ A valid super class for an inline type is abstract, has no instance fields,\n+\/\/ does not implement interface java.lang.IdentityObject (checked elsewhere), has\n+\/\/ an empty body-less no-arg constructor, and no synchronized instance methods.\n+\/\/ This function doesn't check if the class's super types are invalid.  Those checks\n+\/\/ are done elsewhere.  The final determination of whether or not a class is an\n+\/\/ invalid super type for an inline class is done in fill_instance_klass().\n+bool ClassFileParser::is_invalid_super_for_inline_type() {\n+  if (class_name() == vmSymbols::java_lang_IdentityObject()) {\n+    return true;\n+  }\n+  if (is_interface() || class_name() == vmSymbols::java_lang_Object()) {\n+    return false;\n+  }\n+  if (!access_flags().is_abstract() || _has_nonstatic_fields) {\n+    return true;\n+  } else {\n+    \/\/ Look at each method\n+    for (int x = 0; x < _methods->length(); x++) {\n+      const Method* const method = _methods->at(x);\n+      if (method->is_synchronized() && !method->is_static()) {\n+        return true;\n+\n+      } else if (method->name() == vmSymbols::object_initializer_name()) {\n+        if (method->signature() != vmSymbols::void_method_signature() ||\n+            !method->is_vanilla_constructor()) {\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -5417,0 +5718,15 @@\n+  if (_field_info->_is_naturally_atomic && ik->is_inline_klass()) {\n+    ik->set_is_naturally_atomic();\n+  }\n+\n+  if (this->_invalid_inline_super) {\n+    ik->set_invalid_inline_super();\n+  }\n+\n+  if (_has_injected_identityObject) {\n+    ik->set_has_injected_identityObject();\n+  }\n+  if (_has_injected_primitiveObject) {\n+    ik->set_has_injected_primitiveObject();\n+  }\n+\n@@ -5418,1 +5734,1 @@\n-  ik->set_static_oop_field_count(_fac->count[STATIC_OOP]);\n+  ik->set_static_oop_field_count(_fac->count[STATIC_OOP] + _fac->count[STATIC_INLINE]);\n@@ -5468,0 +5784,3 @@\n+  if (_is_declared_atomic) {\n+    ik->set_is_declared_atomic();\n+  }\n@@ -5579,0 +5898,37 @@\n+  bool all_fields_empty = true;\n+  int nfields = ik->java_fields_count();\n+  if (ik->is_inline_klass()) nfields++;\n+  for (int i = 0; i < nfields; i++) {\n+    if (((ik->field_access_flags(i) & JVM_ACC_STATIC) == 0)) {\n+      if (ik->field_is_inline_type(i)) {\n+        Symbol* klass_name = ik->field_signature(i)->fundamental_name(CHECK);\n+        \/\/ Inline classes for instance fields must have been pre-loaded\n+        \/\/ Inline classes for static fields might not have been loaded yet\n+        InstanceKlass* klass = SystemDictionary::find_instance_klass(klass_name,\n+            Handle(THREAD, ik->class_loader()),\n+            Handle(THREAD, ik->protection_domain()));\n+        assert(klass != NULL, \"Just checking\");\n+        assert(klass->access_flags().is_inline_type(), \"Inline type expected\");\n+        ik->set_inline_type_field_klass(i, klass);\n+        klass_name->decrement_refcount();\n+        if (!InlineKlass::cast(klass)->is_empty_inline_type()) { all_fields_empty = false; }\n+      } else {\n+        all_fields_empty = false;\n+      }\n+    } else if (is_inline_type() && ((ik->field_access_flags(i) & JVM_ACC_FIELD_INTERNAL) != 0)) {\n+      InlineKlass::cast(ik)->set_default_value_offset(ik->field_offset(i));\n+    }\n+  }\n+\n+  if (_is_empty_inline_type || (is_inline_type() && all_fields_empty)) {\n+    ik->set_is_empty_inline_type();\n+  }\n+\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_alignment(_alignment);\n+    vk->set_first_field_offset(_first_field_offset);\n+    vk->set_exact_size_in_bytes(_exact_size_in_bytes);\n+    InlineKlass::cast(ik)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -5624,0 +5980,10 @@\n+  \/\/ the common single interface arrays need setup here to provide the\n+  \/\/ correct answer to \"compute_transitive_interfaces()\", during\n+  \/\/ \"SystemDictionary::initialize()\"\n+  if (ik->name() == vmSymbols::java_lang_IdentityObject()) {\n+    Universe::initialize_the_single_IdentityObject_klass_array(ik, CHECK);\n+  }\n+  if (ik->name() == vmSymbols::java_lang_PrimitiveObject()) {\n+    Universe::initialize_the_single_PrimitiveObject_klass_array(ik, CHECK);\n+  }\n+\n@@ -5726,0 +6092,1 @@\n+  _temp_local_interfaces(NULL),\n@@ -5765,0 +6132,11 @@\n+  _has_inline_type_fields(false),\n+  _has_nonstatic_fields(false),\n+  _is_empty_inline_type(false),\n+  _is_naturally_atomic(false),\n+  _is_declared_atomic(false),\n+  _invalid_inline_super(false),\n+  _invalid_identity_super(false),\n+  _implements_identityObject(false),\n+  _has_injected_identityObject(false),\n+  _implements_primitiveObject(false),\n+  _has_injected_primitiveObject(false),\n@@ -5975,2 +6353,1 @@\n-  \/\/ Access flags\n-  jint flags;\n+  jint recognized_modifiers = JVM_RECOGNIZED_CLASS_MODIFIERS;\n@@ -5979,3 +6356,5 @@\n-    flags = stream->get_u2_fast() & (JVM_RECOGNIZED_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-  } else {\n-    flags = stream->get_u2_fast() & JVM_RECOGNIZED_CLASS_MODIFIERS;\n+    recognized_modifiers |= JVM_ACC_MODULE;\n+  }\n+  \/\/ JVM_ACC_INLINE is defined for class file version 55 and later\n+  if (supports_inline_types()) {\n+    recognized_modifiers |= JVM_ACC_INLINE;\n@@ -5984,0 +6363,3 @@\n+  \/\/ Access flags\n+  jint flags = stream->get_u2_fast() & recognized_modifiers;\n+\n@@ -6105,0 +6487,1 @@\n+                   &_is_declared_atomic,\n@@ -6107,1 +6490,1 @@\n-  assert(_local_interfaces != NULL, \"invariant\");\n+  assert(_temp_local_interfaces != NULL, \"invariant\");\n@@ -6112,1 +6495,2 @@\n-               _access_flags.is_interface(),\n+               is_interface(),\n+               is_inline_type(),\n@@ -6124,1 +6508,2 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                is_inline_type(),\n@@ -6206,3 +6591,3 @@\n-    check_property(_local_interfaces == Universe::the_empty_instance_klass_array(),\n-                   \"java.lang.Object cannot implement an interface in class file %s\",\n-                   CHECK);\n+    check_property(_temp_local_interfaces->length() == 0,\n+        \"java.lang.Object cannot implement an interface in class file %s\",\n+        CHECK);\n@@ -6213,1 +6598,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -6234,0 +6619,3 @@\n+    if (_super_klass->is_declared_atomic()) {\n+      _is_declared_atomic = true;\n+    }\n@@ -6239,0 +6627,22 @@\n+\n+    \/\/ For an inline class, only java\/lang\/Object or special abstract classes\n+    \/\/ are acceptable super classes.\n+    if (is_inline_type()) {\n+      const InstanceKlass* super_ik = _super_klass;\n+      if (super_ik->invalid_inline_super()) {\n+        classfile_icce_error(\"inline class %s has an invalid super class %s\", _super_klass, THREAD);\n+        return;\n+      }\n+    }\n+  }\n+\n+  if (_class_name == vmSymbols::java_lang_NonTearable() && _loader_data->class_loader() == NULL) {\n+    \/\/ This is the original source of this condition.\n+    \/\/ It propagates by inheritance, as if testing \"instanceof NonTearable\".\n+    _is_declared_atomic = true;\n+  } else if (*ForceNonTearable != '\\0') {\n+    \/\/ Allow a command line switch to force the same atomicity property:\n+    const char* class_name_str = _class_name->as_C_string();\n+    if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+      _is_declared_atomic = true;\n+    }\n@@ -6241,0 +6651,42 @@\n+  \/\/ Set ik->invalid_inline_super field to TRUE if already marked as invalid,\n+  \/\/ if super is marked invalid, or if is_invalid_super_for_inline_type()\n+  \/\/ returns true\n+  if (invalid_inline_super() ||\n+      (_super_klass != NULL && _super_klass->invalid_inline_super()) ||\n+      is_invalid_super_for_inline_type()) {\n+    set_invalid_inline_super();\n+  }\n+\n+  if (!is_inline_type() && invalid_inline_super() && (_super_klass == NULL || !_super_klass->invalid_inline_super())\n+      && !_implements_identityObject && class_name() != vmSymbols::java_lang_IdentityObject()) {\n+    _temp_local_interfaces->append(vmClasses::IdentityObject_klass());\n+    _has_injected_identityObject = true;\n+  }\n+  \/\/ Check if declared as PrimitiveObject...else add if needed\n+  if (_implements_primitiveObject) {\n+    if (!is_inline_type() && invalid_inline_super()) {\n+      classfile_icce_error(\"class %s can not implement %s, neither valid inline classes or valid supertype\",\n+                            vmClasses::PrimitiveObject_klass(), THREAD);\n+      return;\n+    }\n+  } else if (is_inline_type()) {\n+    _temp_local_interfaces->append(vmClasses::PrimitiveObject_klass());\n+    _has_injected_primitiveObject = true;\n+  }\n+\n+  int itfs_len = _temp_local_interfaces->length();\n+  if (itfs_len == 0) {\n+    _local_interfaces = Universe::the_empty_instance_klass_array();\n+  } else if (itfs_len == 1 && _temp_local_interfaces->at(0) == vmClasses::IdentityObject_klass()) {\n+    _local_interfaces = Universe::the_single_IdentityObject_klass_array();\n+  } else if (itfs_len == 1 && _temp_local_interfaces->at(0) == vmClasses::PrimitiveObject_klass()) {\n+    _local_interfaces = Universe::the_single_PrimitiveObject_klass_array();\n+  } else {\n+    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);\n+    for (int i = 0; i < itfs_len; i++) {\n+      _local_interfaces->at_put(i, _temp_local_interfaces->at(i));\n+    }\n+  }\n+  _temp_local_interfaces = NULL;\n+  assert(_local_interfaces != NULL, \"invariant\");\n+\n@@ -6269,1 +6721,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -6275,0 +6727,19 @@\n+\n+  for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {\n+    if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE  && !fs.access_flags().is_static()) {\n+      \/\/ Pre-load inline class\n+      Klass* klass = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+          Handle(THREAD, _loader_data->class_loader()),\n+          _protection_domain, true, CHECK);\n+      assert(klass != NULL, \"Sanity check\");\n+      if (!klass->access_flags().is_inline_type()) {\n+        assert(klass->is_instance_klass(), \"Sanity check\");\n+        ResourceMark rm(THREAD);\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"Class %s expects class %s to be an inline type, but it is not\",\n+                    _class_name->as_C_string(),\n+                    InstanceKlass::cast(klass)->external_name()));\n+      }\n+    }\n+  }\n+\n@@ -6277,2 +6748,9 @@\n-                        _parsed_annotations->is_contended(), _field_info);\n-  lb.build_layout();\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      loader_data(), _protection_domain, _field_info);\n+  lb.build_layout(CHECK);\n+  if (is_inline_type()) {\n+    _alignment = lb.get_alignment();\n+    _first_field_offset = lb.get_first_field_offset();\n+    _exact_size_in_bytes = lb.get_exact_size_in_byte();\n+  }\n+  _has_inline_type_fields = _field_info->_has_inline_fields;\n@@ -6280,1 +6758,1 @@\n-  \/\/ Compute reference typ\n+  \/\/ Compute reference type\n@@ -6282,1 +6760,0 @@\n-\n@@ -6314,0 +6791,1 @@\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":562,"deletions":84,"binary":false,"changes":646,"status":"modified"},{"patch":"@@ -187,0 +187,1 @@\n+  void inline_classes_do(void f(InlineKlass*));\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -69,0 +70,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -77,0 +79,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -288,1 +291,1 @@\n-    \/\/ Ignore wrapping L and ;.\n+    \/\/ Ignore wrapping L and ;. (and Q and ; for value types);\n@@ -316,0 +319,4 @@\n+      if ((class_name->is_Q_array_signature() && !k->is_inline_klass()) ||\n+          (!class_name->is_Q_array_signature() && k->is_inline_klass())) {\n+            THROW_MSG_NULL(vmSymbols::java_lang_IncompatibleClassChangeError(), \"L\/Q mismatch on bottom type\");\n+          }\n@@ -325,1 +332,0 @@\n-\n@@ -458,0 +464,44 @@\n+Klass* SystemDictionary::resolve_inline_type_field_or_fail(AllFieldStream* fs,\n+                                                           Handle class_loader,\n+                                                           Handle protection_domain,\n+                                                           bool throw_error,\n+                                                           TRAPS) {\n+  Symbol* class_name = fs->signature()->fundamental_name(THREAD);\n+  class_loader = Handle(THREAD, java_lang_ClassLoader::non_reflection_class_loader(class_loader()));\n+  ClassLoaderData* loader_data = class_loader_data(class_loader);\n+  unsigned int p_hash = placeholders()->compute_hash(class_name);\n+  bool throw_circularity_error = false;\n+  PlaceholderEntry* oldprobe;\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    oldprobe = placeholders()->get_entry(p_hash, class_name, loader_data);\n+    if (oldprobe != NULL &&\n+      oldprobe->check_seen_thread(THREAD, PlaceholderTable::INLINE_TYPE_FIELD)) {\n+      throw_circularity_error = true;\n+\n+    } else {\n+      placeholders()->find_and_add(p_hash, class_name, loader_data,\n+                                   PlaceholderTable::INLINE_TYPE_FIELD, NULL, THREAD);\n+    }\n+  }\n+\n+  Klass* klass = NULL;\n+  if (!throw_circularity_error) {\n+    klass = SystemDictionary::resolve_or_fail(class_name, class_loader,\n+                                               protection_domain, true, THREAD);\n+  } else {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG_NULL(vmSymbols::java_lang_ClassCircularityError(), class_name->as_C_string());\n+  }\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    placeholders()->find_and_remove(p_hash, class_name, loader_data,\n+                                    PlaceholderTable::INLINE_TYPE_FIELD, THREAD);\n+  }\n+\n+  class_name->decrement_refcount();\n+  return klass;\n+}\n+\n@@ -938,1 +988,1 @@\n-    if (t != T_OBJECT) {\n+    if (t != T_OBJECT && t != T_INLINE_TYPE) {\n@@ -1315,0 +1365,18 @@\n+\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik->fields(), ik->constants()); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        if (!fs.access_flags().is_static()) {\n+          \/\/ Pre-load inline class\n+          Klass* real_k = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+            class_loader, protection_domain, true, CHECK_NULL);\n+          Klass* k = ik->get_inline_type_field_klass_or_null(fs.index());\n+          if (real_k != k) {\n+            \/\/ oops, the app has substituted a different version of k!\n+            return NULL;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1350,0 +1418,7 @@\n+\n+  if (ik->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    oop val = ik->allocate_instance(CHECK_NULL);\n+    vk->set_default_value(val);\n+  }\n+\n@@ -1910,1 +1985,1 @@\n-    if (t != T_OBJECT) {\n+    if (t != T_OBJECT && t != T_INLINE_TYPE) {\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":79,"deletions":4,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -76,0 +76,1 @@\n+class AllFieldStream;\n@@ -125,0 +126,6 @@\n+  static Klass* resolve_inline_type_field_or_fail(AllFieldStream* fs,\n+                                                  Handle class_loader,\n+                                                  Handle protection_domain,\n+                                                  bool throw_error,\n+                                                  TRAPS);\n+\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -646,0 +646,6 @@\n+\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\"); \/\/ for the next 3 fields\n+    _inline_entry_point       = _entry_point;\n+    _verified_inline_entry_point = _verified_entry_point;\n+    _verified_inline_ro_entry_point = _verified_entry_point;\n+\n@@ -819,0 +825,3 @@\n+    _inline_entry_point       = code_begin()         + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point = code_begin()      + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin()   + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n@@ -938,0 +947,3 @@\n+static nmethod* _nmethod_to_print = NULL;\n+static const CompiledEntrySignature* _nmethod_to_print_ces = NULL;\n+\n@@ -939,0 +951,6 @@\n+  ResourceMark rm;\n+  CompiledEntrySignature ces(method());\n+  ces.compute_calling_conventions();\n+  \/\/ ces.compute_calling_conventions() needs to grab the ProtectionDomainSet_lock, so we\n+  \/\/ can't do that (inside nmethod::print_entry_parameters) while holding the ttyLocker.\n+  \/\/ Hence we have do compute it here and pass via a global. Yuck.\n@@ -940,0 +958,3 @@\n+  assert(_nmethod_to_print == NULL && _nmethod_to_print_ces == NULL, \"no nesting\");\n+  _nmethod_to_print = this;\n+  _nmethod_to_print_ces = &ces;\n@@ -1019,0 +1040,3 @@\n+\n+  _nmethod_to_print = NULL;\n+  _nmethod_to_print_ces = NULL;\n@@ -3098,0 +3122,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -3099,0 +3124,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -3108,0 +3135,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -3110,33 +3147,12 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != NULL) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n-    }\n-  }\n-\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != NULL) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n-    }\n-    if (m != NULL && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != NULL) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n@@ -3144,54 +3160,65 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      intptr_t out_preserve = SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n-        }\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n-        }\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+    }\n+  }\n+\n+  if (_nmethod_to_print != this) {\n+    return;\n+  }\n+  Method* m = method();\n+  if (m == NULL || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN4(entry_point(), verified_entry_point(), verified_inline_entry_point(), verified_inline_ro_entry_point());\n+  low = MIN2(low, inline_entry_point());\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  const CompiledEntrySignature* ces = _nmethod_to_print_ces;\n+  const GrowableArray<SigEntry>* sig_cc;\n+  const VMRegPair* regs;\n+  if (block_begin == verified_entry_point()) {\n+    sig_cc = &ces->sig_cc();\n+    regs = ces->regs_cc();\n+  } else if (block_begin == verified_inline_entry_point()) {\n+    sig_cc = &ces->sig();\n+    regs = ces->regs();\n+  } else if (block_begin == verified_inline_ro_entry_point()) {\n+    sig_cc = &ces->sig_cc_ro();\n+    regs = ces->regs_cc_ro();\n+  } else {\n+    return;\n+  }\n+\n+  bool has_this = !m->is_static();\n+  if (ces->has_inline_recv() && block_begin == verified_entry_point()) {\n+    \/\/ <this> argument is scalarized for verified_entry_point()\n+    has_this = false;\n+  }\n+  const char* spname = \"sp\"; \/\/ make arch-specific?\n+  int stack_slot_offset = this->frame_size() * wordSize;\n+  int tab1 = 14, tab2 = 24;\n+  int sig_index = 0;\n+  int arg_index = has_this ? -1 : 0;\n+  bool did_old_sp = false;\n+  for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n+    bool at_this = (arg_index == -1);\n+    bool at_old_sp = false;\n+    BasicType t = (*sig)._bt;\n+    if (at_this) {\n+      stream->print(\"  # this: \");\n+    } else {\n+      stream->print(\"  # parm%d: \", arg_index);\n+    }\n+    stream->move_to(tab1);\n+    VMReg fst = regs[sig_index].first();\n+    VMReg snd = regs[sig_index].second();\n+    if (fst->is_reg()) {\n+      stream->print(\"%s\", fst->name());\n+      if (snd->is_valid())  {\n+        stream->print(\":%s\", snd->name());\n@@ -3199,6 +3226,18 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n-        stream->print(\"  (%s of caller)\", spname);\n-        stream->cr();\n+    } else if (fst->is_stack()) {\n+      int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+      if (offset == stack_slot_offset)  at_old_sp = true;\n+      stream->print(\"[%s+0x%x]\", spname, offset);\n+    } else {\n+      stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+    }\n+    stream->print(\" \");\n+    stream->move_to(tab2);\n+    stream->print(\"= \");\n+    if (at_this) {\n+      m->method_holder()->print_value_on(stream);\n+    } else {\n+      bool did_name = false;\n+      if (is_reference_type(t)) {\n+        Symbol* name = (*sig)._symbol;\n+        name->print_value_on(stream);\n+        did_name = true;\n@@ -3206,0 +3245,2 @@\n+      if (!did_name)\n+        stream->print(\"%s\", type2name(t));\n@@ -3207,0 +3248,14 @@\n+    if (at_old_sp) {\n+      stream->print(\"  (%s of caller)\", spname);\n+      did_old_sp = true;\n+    }\n+    stream->cr();\n+    sig_index += type2size[t];\n+    arg_index += 1;\n+  }\n+  if (!did_old_sp) {\n+    stream->print(\"  # \");\n+    stream->move_to(tab1);\n+    stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+    stream->print(\"  (%s of caller)\", spname);\n+    stream->cr();\n@@ -3330,1 +3385,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_vt=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_vt());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":149,"deletions":94,"binary":false,"changes":243,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"compiler\/compilerDefinitions.hpp\"\n@@ -196,0 +197,3 @@\n+  address _inline_entry_point;               \/\/ inline type entry point (unpack all inline type args) with class check\n+  address _verified_inline_entry_point;      \/\/ inline type entry point (unpack all inline type args) without class check\n+  address _verified_inline_ro_entry_point;   \/\/ inline type entry point (unpack receiver only) without class check\n@@ -456,2 +460,5 @@\n-  address entry_point() const                     { return _entry_point;             } \/\/ normal entry point\n-  address verified_entry_point() const            { return _verified_entry_point;    } \/\/ if klass is correct\n+  address entry_point() const                     { return _entry_point;             }        \/\/ normal entry point\n+  address verified_entry_point() const            { return _verified_entry_point;    }        \/\/ normal entry point without class check\n+  address inline_entry_point() const              { return _inline_entry_point; }             \/\/ inline type entry point (unpack all inline type args)\n+  address verified_inline_entry_point() const     { return _verified_inline_entry_point; }    \/\/ inline type entry point (unpack all inline type args) without class check\n+  address verified_inline_ro_entry_point() const  { return _verified_inline_ro_entry_point; } \/\/ inline type entry point (only unpack receiver) without class check\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1279,1 +1279,1 @@\n-          if (vfst.method()->is_static_initializer() ||\n+        if (vfst.method()->is_class_initializer() ||\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -253,0 +253,1 @@\n+  oop obj_buffer_allocate(Klass* klass, int size, TRAPS); \/\/ doesn't clear memory\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1267,1 +1267,1 @@\n-              Deoptimization::reassign_fields(fst.current(), fst.register_map(), objects, realloc_failures, false);\n+              Deoptimization::reassign_fields(fst.current(), fst.register_map(), objects, realloc_failures, false, CHECK_NULL);\n@@ -1527,1 +1527,1 @@\n-  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false);\n+  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false, THREAD);\n@@ -1857,1 +1857,1 @@\n-    if (m->is_initializer() && !m->is_static()) {\n+    if (m->is_object_constructor()) {\n@@ -1887,1 +1887,1 @@\n-    if (!m->is_initializer() && !m->is_overpass()) {\n+    if (!(m->is_object_constructor() || m->is_class_initializer()) && !m->is_overpass()) {\n@@ -2508,2 +2508,1 @@\n-  if (m->is_initializer()) {\n-    if (m->is_static_initializer()) {\n+  if (m->is_class_initializer()) {\n@@ -2512,1 +2511,2 @@\n-    }\n+  }\n+  else if (m->is_object_constructor() || m->is_static_init_factory()) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -123,0 +123,2 @@\n+LatestMethodCache* Universe::_is_substitutable_cache  = NULL;\n+LatestMethodCache* Universe::_inline_type_hash_code_cache = NULL;\n@@ -131,0 +133,2 @@\n+Array<InstanceKlass*>* Universe::_the_single_IdentityObject_klass_array = NULL;\n+Array<InstanceKlass*>* Universe::_the_single_PrimitiveObject_klass_array = NULL;\n@@ -221,0 +225,2 @@\n+  it->push(&_the_single_IdentityObject_klass_array);\n+  it->push(&_the_single_PrimitiveObject_klass_array);\n@@ -227,0 +233,2 @@\n+  _is_substitutable_cache->metaspace_pointers_do(it);\n+  _inline_type_hash_code_cache->metaspace_pointers_do(it);\n@@ -269,0 +277,2 @@\n+  f->do_ptr((void**)&_the_single_IdentityObject_klass_array);\n+  f->do_ptr((void**)&_the_single_PrimitiveObject_klass_array);\n@@ -274,0 +284,2 @@\n+  _is_substitutable_cache->serialize(f);\n+  _inline_type_hash_code_cache->serialize(f);\n@@ -323,1 +335,1 @@\n-        _the_array_interfaces_array     = MetadataFactory::new_array<Klass*>(null_cld, 2, NULL, CHECK);\n+        _the_array_interfaces_array     = MetadataFactory::new_array<Klass*>(null_cld, 3, NULL, CHECK);\n@@ -350,0 +362,7 @@\n+      assert(_the_array_interfaces_array->at(2) ==\n+                   vmClasses::IdentityObject_klass(), \"u3\");\n+\n+      assert(_the_single_IdentityObject_klass_array->at(0) ==\n+          vmClasses::IdentityObject_klass(), \"u3\");\n+      assert(_the_single_PrimitiveObject_klass_array->at(0) ==\n+          vmClasses::PrimitiveObject_klass(), \"u3\");\n@@ -356,0 +375,1 @@\n+      _the_array_interfaces_array->at_put(2, vmClasses::IdentityObject_klass());\n@@ -460,0 +480,17 @@\n+void Universe::initialize_the_single_IdentityObject_klass_array(InstanceKlass* ik, TRAPS) {\n+    assert(_the_single_IdentityObject_klass_array == NULL, \"Must not be initialized twice\");\n+    assert(ik->name() == vmSymbols::java_lang_IdentityObject(), \"Must be\");\n+    Array<InstanceKlass*>* array = MetadataFactory::new_array<InstanceKlass*>(ik->class_loader_data(), 1, NULL, CHECK);\n+    array->at_put(0, ik);\n+    _the_single_IdentityObject_klass_array = array;\n+}\n+\n+void Universe::initialize_the_single_PrimitiveObject_klass_array(InstanceKlass* ik, TRAPS) {\n+    assert(_the_single_PrimitiveObject_klass_array == NULL, \"Must not be initialized twice\");\n+    assert(ik->name() == vmSymbols::java_lang_PrimitiveObject(), \"Must be\");\n+    Array<InstanceKlass*>* array = MetadataFactory::new_array<InstanceKlass*>(ik->class_loader_data(), 1, NULL, CHECK);\n+    array->at_put(0, ik);\n+    _the_single_PrimitiveObject_klass_array = array;\n+}\n+\n+\n@@ -748,1 +785,0 @@\n-\n@@ -771,0 +807,2 @@\n+  Universe::_is_substitutable_cache = new LatestMethodCache();\n+  Universe::_inline_type_hash_code_cache = new LatestMethodCache();\n@@ -929,0 +967,11 @@\n+\n+  \/\/ Set up substitutability testing\n+  ResourceMark rm;\n+  initialize_known_method(_is_substitutable_cache,\n+                          vmClasses::ValueBootstrapMethods_klass(),\n+                          vmSymbols::isSubstitutable_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true, CHECK);\n+  initialize_known_method(_inline_type_hash_code_cache,\n+                          vmClasses::ValueBootstrapMethods_klass(),\n+                          vmSymbols::inlineObjectHashCode_name()->as_C_string(),\n+                          vmSymbols::object_int_signature(), true, CHECK);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":51,"deletions":2,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -120,0 +120,2 @@\n+  static LatestMethodCache* _is_substitutable_cache;   \/\/ ValueBootstrapMethods.isSubstitutable() method\n+  static LatestMethodCache* _inline_type_hash_code_cache;  \/\/ ValueBootstrapMethods.inlineObjectHashCode() method\n@@ -125,0 +127,2 @@\n+  static Array<InstanceKlass*>* _the_single_IdentityObject_klass_array;  \/\/ Common single interface array for IdentityObjects\n+  static Array<InstanceKlass*>* _the_single_PrimitiveObject_klass_array; \/\/ Common single interface array for PrimitiveObjects\n@@ -147,0 +151,1 @@\n+\n@@ -263,0 +268,3 @@\n+  static Method*      is_substitutable_method()       { return _is_substitutable_cache->get_method(); }\n+  static Method*      inline_type_hash_code_method()  { return _inline_type_hash_code_cache->get_method(); }\n+\n@@ -287,0 +295,12 @@\n+  static Array<InstanceKlass*>*  the_single_IdentityObject_klass_array() {\n+    assert(_the_single_IdentityObject_klass_array != NULL, \"Must be initialized before use\");\n+    assert(_the_single_IdentityObject_klass_array->length() == 1, \"Sanity check\");\n+    return _the_single_IdentityObject_klass_array;\n+  }\n+  static void initialize_the_single_IdentityObject_klass_array(InstanceKlass* ik, TRAPS);\n+  static Array<InstanceKlass*>*  the_single_PrimitiveObject_klass_array() {\n+    assert(_the_single_PrimitiveObject_klass_array != NULL, \"Must be initialized before use\");\n+    assert(_the_single_PrimitiveObject_klass_array->length() == 1, \"Sanity check\");\n+    return _the_single_PrimitiveObject_klass_array;\n+  }\n+  static void initialize_the_single_PrimitiveObject_klass_array(InstanceKlass* ik, TRAPS);\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -232,1 +233,1 @@\n-      \/\/ All of these should have been reverted back to ClassIndex before calling\n+      \/\/ All of these should have been reverted back to Unresolved before calling\n@@ -256,0 +257,1 @@\n+  jbyte qdesc_bit = (name->is_Q_signature()) ? (jbyte) JVM_CONSTANT_QDescBit : 0;\n@@ -257,1 +259,1 @@\n-    release_tag_at_put(class_index, JVM_CONSTANT_Class);\n+    release_tag_at_put(class_index, JVM_CONSTANT_Class | qdesc_bit);\n@@ -259,1 +261,1 @@\n-    release_tag_at_put(class_index, JVM_CONSTANT_UnresolvedClass);\n+    release_tag_at_put(class_index, JVM_CONSTANT_UnresolvedClass | qdesc_bit);\n@@ -273,0 +275,1 @@\n+  assert(!k->name()->is_Q_signature(), \"Q-type without JVM_CONSTANT_QDescBit\");\n@@ -409,0 +412,1 @@\n+    jbyte qdesc_bit = tag_at(index).is_Qdescriptor_klass() ? (jbyte) JVM_CONSTANT_QDescBit : 0;\n@@ -410,1 +414,1 @@\n-      tag_at_put(index, JVM_CONSTANT_UnresolvedClass);\n+      tag_at_put(index, JVM_CONSTANT_UnresolvedClass | qdesc_bit);\n@@ -435,1 +439,1 @@\n-        tag_at_put(index, JVM_CONSTANT_UnresolvedClass);\n+        tag_at_put(index, JVM_CONSTANT_UnresolvedClass | qdesc_bit);\n@@ -485,0 +489,6 @@\n+void check_is_inline_type(Klass* k, TRAPS) {\n+  if (!k->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+}\n+\n@@ -518,0 +528,5 @@\n+  bool inline_type_signature = false;\n+  if (name->is_Q_signature()) {\n+    name = name->fundamental_name(THREAD);\n+    inline_type_signature = true;\n+  }\n@@ -527,0 +542,3 @@\n+  if (inline_type_signature) {\n+    name->decrement_refcount();\n+  }\n@@ -535,0 +553,16 @@\n+  if (!HAS_PENDING_EXCEPTION && inline_type_signature) {\n+    check_is_inline_type(k, THREAD);\n+  }\n+\n+  if (!HAS_PENDING_EXCEPTION) {\n+    Klass* bottom_klass = NULL;\n+    if (k->is_objArray_klass()) {\n+      bottom_klass = ObjArrayKlass::cast(k)->bottom_klass();\n+      assert(bottom_klass != NULL, \"Should be set\");\n+      assert(bottom_klass->is_instance_klass() || bottom_klass->is_typeArray_klass(), \"Sanity check\");\n+    } else if (k->is_flatArray_klass()) {\n+      bottom_klass = FlatArrayKlass::cast(k)->element_klass();\n+      assert(bottom_klass != NULL, \"Should be set\");\n+    }\n+  }\n+\n@@ -539,1 +573,5 @@\n-      save_and_throw_exception(this_cp, which, constantTag(JVM_CONSTANT_UnresolvedClass), CHECK_NULL);\n+      jbyte tag = JVM_CONSTANT_UnresolvedClass;\n+      if (this_cp->tag_at(which).is_Qdescriptor_klass()) {\n+        tag |= JVM_CONSTANT_QDescBit;\n+      }\n+      save_and_throw_exception(this_cp, which, constantTag(tag), CHECK_NULL);\n@@ -560,1 +598,5 @@\n-  this_cp->release_tag_at_put(which, JVM_CONSTANT_Class);\n+  jbyte tag = JVM_CONSTANT_Class;\n+  if (this_cp->tag_at(which).is_Qdescriptor_klass()) {\n+    tag |= JVM_CONSTANT_QDescBit;\n+  }\n+  this_cp->release_tag_at_put(which, tag);\n@@ -1884,0 +1926,6 @@\n+      case (JVM_CONSTANT_Class | JVM_CONSTANT_QDescBit): {\n+        idx1 = Bytes::get_Java_u2(bytes);\n+        printf(\"qclass        #%03d\", idx1);\n+        ent_size = 2;\n+        break;\n+      }\n@@ -1926,0 +1974,4 @@\n+      case (JVM_CONSTANT_UnresolvedClass | JVM_CONSTANT_QDescBit): {\n+        printf(\"UnresolvedQClass: %s\", WARN_MSG);\n+        break;\n+      }\n@@ -2097,0 +2149,1 @@\n+        assert(!tag_at(idx).is_Qdescriptor_klass(), \"Failed to encode QDesc\");\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":60,"deletions":7,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -0,0 +1,138 @@\n+\/*\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#ifndef SHARE_VM_OOPS_INLINEKLASS_INLINE_HPP\n+#define SHARE_VM_OOPS_INLINEKLASS_INLINE_HPP\n+\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n+#include \"oops\/instanceKlass.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+inline InlineKlassFixedBlock* InlineKlass::inlineklass_static_block() const {\n+  address adr_jf = adr_inline_type_field_klasses();\n+  if (adr_jf != NULL) {\n+    return (InlineKlassFixedBlock*)(adr_jf + this->java_fields_count() * sizeof(Klass*));\n+  }\n+\n+  address adr_fing = adr_fingerprint();\n+  if (adr_fing != NULL) {\n+    return (InlineKlassFixedBlock*)(adr_fingerprint() + sizeof(u8));\n+  }\n+\n+  InstanceKlass** adr_host = adr_unsafe_anonymous_host();\n+  if (adr_host != NULL) {\n+    return (InlineKlassFixedBlock*)(adr_host + 1);\n+  }\n+\n+  InstanceKlass* volatile* adr_impl = adr_implementor();\n+  if (adr_impl != NULL) {\n+    return (InlineKlassFixedBlock*)(adr_impl + 1);\n+  }\n+\n+  return (InlineKlassFixedBlock*)end_of_nonstatic_oop_maps();\n+}\n+\n+inline address InlineKlass::adr_return_regs() const {\n+  InlineKlassFixedBlock* vkst = inlineklass_static_block();\n+  return ((address)_adr_inlineklass_fixed_block) + in_bytes(byte_offset_of(InlineKlassFixedBlock, _return_regs));\n+}\n+\n+inline Array<VMRegPair>* InlineKlass::return_regs() const {\n+  return *((Array<VMRegPair>**)adr_return_regs());\n+}\n+\n+\n+inline InlineKlass* InlineKlass::cast(Klass* k) {\n+  assert(k->is_inline_klass(), \"cast to InlineKlass\");\n+  return (InlineKlass*) k;\n+}\n+\n+inline address InlineKlass::data_for_oop(oop o) const {\n+  return ((address) (void*) o) + first_field_offset();\n+}\n+\n+inline oop InlineKlass::oop_for_data(address data) const {\n+  oop o = (oop) (data - first_field_offset());\n+  assert(oopDesc::is_oop(o, false), \"Not an oop\");\n+  return o;\n+}\n+\n+inline void InlineKlass::inline_copy_payload_to_new_oop(void* src, oop dst) {\n+  HeapAccess<IS_DEST_UNINITIALIZED>::value_copy(src, data_for_oop(dst), this);\n+}\n+\n+inline void InlineKlass::inline_copy_oop_to_new_oop(oop src, oop dst) {\n+  HeapAccess<IS_DEST_UNINITIALIZED>::value_copy(data_for_oop(src), data_for_oop(dst), this);\n+}\n+\n+inline void InlineKlass::inline_copy_oop_to_new_payload(oop src, void* dst) {\n+  HeapAccess<IS_DEST_UNINITIALIZED>::value_copy(data_for_oop(src), dst, this);\n+}\n+\n+inline void InlineKlass::inline_copy_oop_to_payload(oop src, void* dst) {\n+  HeapAccess<>::value_copy(data_for_oop(src), dst, this);\n+}\n+\n+\n+template <typename T, class OopClosureType>\n+void InlineKlass::oop_iterate_specialized(const address oop_addr, OopClosureType* closure) {\n+  OopMapBlock* map = start_of_nonstatic_oop_maps();\n+  OopMapBlock* const end_map = map + nonstatic_oop_map_count();\n+\n+  for (; map < end_map; map++) {\n+    T* p = (T*) (oop_addr + map->offset());\n+    T* const end = p + map->count();\n+    for (; p < end; ++p) {\n+      Devirtualizer::do_oop(closure, p);\n+    }\n+  }\n+}\n+\n+template <typename T, class OopClosureType>\n+inline void InlineKlass::oop_iterate_specialized_bounded(const address oop_addr, OopClosureType* closure, void* lo, void* hi) {\n+  OopMapBlock* map = start_of_nonstatic_oop_maps();\n+  OopMapBlock* const end_map = map + nonstatic_oop_map_count();\n+\n+  T* const l   = (T*) lo;\n+  T* const h   = (T*) hi;\n+\n+  for (; map < end_map; map++) {\n+    T* p = (T*) (oop_addr + map->offset());\n+    T* end = p + map->count();\n+    if (p < l) {\n+      p = l;\n+    }\n+    if (end > h) {\n+      end = h;\n+    }\n+    for (; p < end; ++p) {\n+      Devirtualizer::do_oop(closure, p);\n+    }\n+  }\n+}\n+\n+\n+#endif \/\/ SHARE_VM_OOPS_INLINEKLASS_INLINE_HPP\n","filename":"src\/hotspot\/share\/oops\/inlineKlass.inline.hpp","additions":138,"deletions":0,"binary":false,"changes":138,"status":"added"},{"patch":"@@ -72,0 +72,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -164,0 +165,2 @@\n+bool InstanceKlass::field_is_inline_type(int index) const { return Signature::basic_type(field(index)->signature(constants())) == T_INLINE_TYPE; }\n+\n@@ -471,1 +474,3 @@\n-                                       should_store_fingerprint(is_hidden_or_anonymous));\n+                                       should_store_fingerprint(is_hidden_or_anonymous),\n+                                       parser.has_inline_fields() ? parser.java_fields_count() : 0,\n+                                       parser.is_inline_type());\n@@ -485,2 +490,1 @@\n-    }\n-    else if (is_class_loader(class_name, parser)) {\n+    } else if (is_class_loader(class_name, parser)) {\n@@ -489,0 +493,3 @@\n+    } else if (parser.is_inline_type()) {\n+      \/\/ inline type\n+      ik = new (loader_data, size, THREAD) InlineKlass(parser);\n@@ -504,0 +511,7 @@\n+#ifdef ASSERT\n+  assert(ik->size() == size, \"\");\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -507,0 +521,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = NULL;\n+  address end = NULL;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -541,1 +578,3 @@\n-  _init_thread(NULL)\n+  _init_thread(NULL),\n+  _inline_type_field_klasses(NULL),\n+  _adr_inlineklass_fixed_block(NULL)\n@@ -550,0 +589,4 @@\n+    if (parser.has_inline_fields()) {\n+      set_has_inline_type_fields();\n+    }\n+    _java_fields_count = parser.java_fields_count();\n@@ -551,3 +594,3 @@\n-  assert(NULL == _methods, \"underlying memory not zeroed?\");\n-  assert(is_instance_klass(), \"is layout incorrect?\");\n-  assert(size_helper() == parser.layout_size(), \"incorrect size_helper?\");\n+    assert(NULL == _methods, \"underlying memory not zeroed?\");\n+    assert(is_instance_klass(), \"is layout incorrect?\");\n+    assert(size_helper() == parser.layout_size(), \"incorrect size_helper?\");\n@@ -560,0 +603,3 @@\n+  if (has_inline_type_fields()) {\n+    _inline_type_field_klasses = (const Klass**) adr_inline_type_field_klasses();\n+  }\n@@ -589,1 +635,3 @@\n-    if (ti != sti && ti != NULL && !ti->is_shared()) {\n+    if (ti != sti && ti != NULL && !ti->is_shared() &&\n+        ti != Universe::the_single_IdentityObject_klass_array() &&\n+        ti != Universe::the_single_PrimitiveObject_klass_array()) {\n@@ -596,1 +644,3 @@\n-      local_interfaces != NULL && !local_interfaces->is_shared()) {\n+      local_interfaces != NULL && !local_interfaces->is_shared() &&\n+      local_interfaces != Universe::the_single_IdentityObject_klass_array() &&\n+      local_interfaces != Universe::the_single_PrimitiveObject_klass_array()) {\n@@ -928,0 +978,56 @@\n+\n+  \/\/ If a class declares a method that uses an inline class as an argument\n+  \/\/ type or return inline type, this inline class must be loaded during the\n+  \/\/ linking of this class because size and properties of the inline class\n+  \/\/ must be known in order to be able to perform inline type optimizations.\n+  \/\/ The implementation below is an approximation of this rule, the code\n+  \/\/ iterates over all methods of the current class (including overridden\n+  \/\/ methods), not only the methods declared by this class. This\n+  \/\/ approximation makes the code simpler, and doesn't change the semantic\n+  \/\/ because classes declaring methods overridden by the current class are\n+  \/\/ linked (and have performed their own pre-loading) before the linking\n+  \/\/ of the current class.\n+\n+\n+  \/\/ Note:\n+  \/\/ Inline class types are loaded during\n+  \/\/ the loading phase (see ClassFileParser::post_process_parsed_stream()).\n+  \/\/ Inline class types used as element types for array creation\n+  \/\/ are not pre-loaded. Their loading is triggered by either anewarray\n+  \/\/ or multianewarray bytecodes.\n+\n+  \/\/ Could it be possible to do the following processing only if the\n+  \/\/ class uses inline types?\n+  {\n+    ResourceMark rm(THREAD);\n+    for (int i = 0; i < methods()->length(); i++) {\n+      Method* m = methods()->at(i);\n+      for (SignatureStream ss(m->signature()); !ss.is_done(); ss.next()) {\n+        if (ss.is_reference()) {\n+          if (ss.is_array()) {\n+            ss.skip_array_prefix();\n+          }\n+          if (ss.type() == T_INLINE_TYPE) {\n+            Symbol* symb = ss.as_symbol();\n+\n+            oop loader = class_loader();\n+            oop protection_domain = this->protection_domain();\n+            Klass* klass = SystemDictionary::resolve_or_fail(symb,\n+                                                             Handle(THREAD, loader), Handle(THREAD, protection_domain), true,\n+                                                             CHECK_false);\n+            if (klass == NULL) {\n+              THROW_(vmSymbols::java_lang_LinkageError(), false);\n+            }\n+            if (!klass->is_inline_klass()) {\n+              Exceptions::fthrow(\n+                THREAD_AND_LOCATION,\n+                vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                \"class %s is not an inline type\",\n+                klass->external_name());\n+            }\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -999,0 +1105,1 @@\n+\n@@ -1149,0 +1256,29 @@\n+  \/\/ Step 8\n+  \/\/ Initialize classes of inline fields\n+  {\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        Klass* klass = get_inline_type_field_klass_or_null(fs.index());\n+        if (fs.access_flags().is_static() && klass == NULL) {\n+          klass = SystemDictionary::resolve_or_fail(field_signature(fs.index())->fundamental_name(THREAD),\n+              Handle(THREAD, class_loader()),\n+              Handle(THREAD, protection_domain()),\n+              true, CHECK);\n+          if (klass == NULL) {\n+            THROW(vmSymbols::java_lang_NoClassDefFoundError());\n+          }\n+          if (!klass->is_inline_klass()) {\n+            THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+          }\n+          set_inline_type_field_klass(fs.index(), klass);\n+        }\n+        InstanceKlass::cast(klass)->initialize(CHECK);\n+        if (fs.access_flags().is_static()) {\n+          if (java_mirror()->obj_field(fs.offset()) == NULL) {\n+            java_mirror()->obj_field_put(fs.offset(), InlineKlass::cast(klass)->default_value());\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1153,1 +1289,1 @@\n-  \/\/ Step 8\n+  \/\/ Step 9\n@@ -1175,1 +1311,1 @@\n-  \/\/ Step 9\n+  \/\/ Step 10\n@@ -1183,1 +1319,1 @@\n-    \/\/ Step 10 and 11\n+    \/\/ Step 11 and 12\n@@ -1455,1 +1591,1 @@\n-  ObjArrayKlass* oak = array_klasses();\n+  ArrayKlass* oak = array_klasses();\n@@ -1471,1 +1607,1 @@\n-  if (clinit != NULL && clinit->has_valid_initializer_flags()) {\n+  if (clinit != NULL && clinit->is_class_initializer()) {\n@@ -1509,1 +1645,1 @@\n-    MutexLocker x(OopMapCacheAlloc_lock);\n+    MutexLocker x(OopMapCacheAlloc_lock,  Mutex::_no_safepoint_check_flag);\n@@ -1521,5 +1657,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n-\n@@ -1596,0 +1727,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->first_field_offset() && offset < (vk->first_field_offset() + vk->get_exact_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -1983,0 +2123,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited, not even as a static factory\n+    }\n@@ -2491,0 +2634,6 @@\n+\n+  if (has_inline_type_fields()) {\n+    for (int i = 0; i < java_fields_count(); i++) {\n+      it->push(&((Klass**)adr_inline_type_field_klasses())[i]);\n+    }\n+  }\n@@ -2526,0 +2675,8 @@\n+  if (has_inline_type_fields()) {\n+    for (AllFieldStream fs(fields(), constants()); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        reset_inline_type_field_klass(fs.index());\n+      }\n+    }\n+  }\n+\n@@ -2586,0 +2743,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2617,1 +2778,1 @@\n-  if (UseBiasedLocking && BiasedLocking::enabled()) {\n+  if (UseBiasedLocking && BiasedLocking::enabled() && !is_inline_klass()) {\n@@ -2788,1 +2949,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -2790,1 +2951,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = is_inline_klass() ? JVM_SIGNATURE_INLINE_TYPE : JVM_SIGNATURE_CLASS;\n@@ -3357,1 +3518,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3361,0 +3525,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3364,0 +3533,6 @@\n+    } else if (self != NULL && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3370,1 +3545,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(NULL, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == NULL) { st->print_cr(\"NULL\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == NULL) { st->print_cr(\"NULL\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3380,0 +3576,1 @@\n+  st->print(BULLET\"misc flags:        0x%x\", _misc_flags);                        st->cr();\n@@ -3406,15 +3603,3 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);                  st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);      st->cr();\n-  if (Verbose && default_methods() != NULL) {\n-    Array<Method*>* method_array = default_methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n+  st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3422,1 +3607,1 @@\n-    st->print(BULLET\"default vtable indices:   \"); default_vtable_indices()->print_value_on(st);       st->cr();\n+    st->print(BULLET\"default vtable indices:   \"); print_array_on(st, default_vtable_indices());\n@@ -3424,2 +3609,2 @@\n-  st->print(BULLET\"local interfaces:  \"); local_interfaces()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"trans. interfaces: \"); transitive_interfaces()->print_value_on(st); st->cr();\n+  st->print(BULLET\"local interfaces:  \"); print_array_on(st, local_interfaces());\n+  st->print(BULLET\"trans. interfaces: \"); print_array_on(st, transitive_interfaces());\n@@ -3482,1 +3667,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(NULL, start_of_itable(), itable_length(), st);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":229,"deletions":44,"binary":false,"changes":273,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -56,0 +57,2 @@\n+\/\/    [EMBEDDED inline_type_field_klasses] only if has_inline_fields() == true\n+\/\/    [EMBEDDED InlineKlassFixedBlock] only if is an InlineKlass instance\n@@ -72,0 +75,1 @@\n+class BufferedInlineTypeBlob;\n@@ -134,0 +138,16 @@\n+class SigEntry;\n+\n+class InlineKlassFixedBlock {\n+  Array<SigEntry>** _extended_sig;\n+  Array<VMRegPair>** _return_regs;\n+  address* _pack_handler;\n+  address* _pack_handler_jobject;\n+  address* _unpack_handler;\n+  int* _default_value_offset;\n+  int _alignment;\n+  int _first_field_offset;\n+  int _exact_size_in_bytes;\n+\n+  friend class InlineKlass;\n+};\n+\n@@ -139,0 +159,1 @@\n+  friend class TemplateTable;\n@@ -156,1 +177,1 @@\n-    fully_initialized,                  \/\/ initialized (successfull final state)\n+    fully_initialized,                  \/\/ initialized (successful final state)\n@@ -172,1 +193,1 @@\n-  ObjArrayKlass* volatile _array_klasses;\n+  ArrayKlass* volatile _array_klasses;\n@@ -239,1 +260,1 @@\n-  \/\/ This can be used to quickly discriminate among the four kinds of\n+  \/\/ This can be used to quickly discriminate among the five kinds of\n@@ -245,0 +266,1 @@\n+  static const unsigned _kind_inline_type  = 4; \/\/ InlineKlass\n@@ -266,1 +288,9 @@\n-    _misc_has_contended_annotations           = 1 << 15  \/\/ has @Contended annotation\n+    _misc_has_contended_annotations           = 1 << 15,  \/\/ has @Contended annotation\n+    _misc_has_inline_type_fields              = 1 << 16, \/\/ has inline fields and related embedded section is not empty\n+    _misc_is_empty_inline_type                = 1 << 17, \/\/ empty inline type (*)\n+    _misc_is_naturally_atomic                 = 1 << 18, \/\/ loaded\/stored in one instruction\n+    _misc_is_declared_atomic                  = 1 << 19, \/\/ implements jl.NonTearable\n+    _misc_invalid_inline_super                = 1 << 20, \/\/ invalid super type for an inline type\n+    _misc_invalid_identity_super              = 1 << 21, \/\/ invalid super type for an identity type\n+    _misc_has_injected_identityObject         = 1 << 22, \/\/ IdentityObject has been injected by the JVM\n+    _misc_has_injected_primitiveObject        = 1 << 23  \/\/ PrimitiveObject has been injected by the JVM\n@@ -268,0 +298,6 @@\n+\n+  \/\/ (*) An inline type is considered empty if it contains no non-static fields or\n+  \/\/ if it contains only empty inline fields. Note that JITs have a slightly different\n+  \/\/ definition: empty inline fields must be flattened otherwise the container won't\n+  \/\/ be considered empty\n+\n@@ -271,1 +307,1 @@\n-  u2              _misc_flags;           \/\/ There is more space in access_flags for more flags.\n+  u4              _misc_flags;           \/\/ There is more space in access_flags for more flags.\n@@ -323,0 +359,3 @@\n+  const Klass**   _inline_type_field_klasses; \/\/ For \"inline class\" fields, NULL if none present\n+\n+  const InlineKlassFixedBlock* _adr_inlineklass_fixed_block;\n@@ -383,0 +422,73 @@\n+  bool has_inline_type_fields() const          {\n+    return (_misc_flags & _misc_has_inline_type_fields) != 0;\n+  }\n+  void set_has_inline_type_fields()  {\n+    _misc_flags |= _misc_has_inline_type_fields;\n+  }\n+\n+  bool is_empty_inline_type() const {\n+    return (_misc_flags & _misc_is_empty_inline_type) != 0;\n+  }\n+  void set_is_empty_inline_type() {\n+    _misc_flags |= _misc_is_empty_inline_type;\n+  }\n+\n+  \/\/ Note:  The naturally_atomic property only applies to\n+  \/\/ inline classes; it is never true on identity classes.\n+  \/\/ The bit is placed on instanceKlass for convenience.\n+\n+  \/\/ Query if h\/w provides atomic load\/store for instances.\n+  bool is_naturally_atomic() const {\n+    return (_misc_flags & _misc_is_naturally_atomic) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_is_naturally_atomic() {\n+    _misc_flags |= _misc_is_naturally_atomic;\n+  }\n+\n+  \/\/ Query if this class implements jl.NonTearable or was\n+  \/\/ mentioned in the JVM option ForceNonTearable.\n+  \/\/ This bit can occur anywhere, but is only significant\n+  \/\/ for inline classes *and* their super types.\n+  \/\/ It inherits from supers along with NonTearable.\n+  bool is_declared_atomic() const {\n+    return (_misc_flags & _misc_is_declared_atomic) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_is_declared_atomic() {\n+    _misc_flags |= _misc_is_declared_atomic;\n+  }\n+\n+  \/\/ Query if class is an invalid super class for an inline type.\n+  bool invalid_inline_super() const {\n+    return (_misc_flags & _misc_invalid_inline_super) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_invalid_inline_super() {\n+    _misc_flags |= _misc_invalid_inline_super;\n+  }\n+  \/\/ Query if class is an invalid super class for an identity type.\n+  bool invalid_identity_super() const {\n+    return (_misc_flags & _misc_invalid_identity_super) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_invalid_identity_super() {\n+    _misc_flags |= _misc_invalid_identity_super;\n+  }\n+\n+  bool has_injected_identityObject() const {\n+    return (_misc_flags & _misc_has_injected_identityObject);\n+  }\n+\n+  void set_has_injected_identityObject() {\n+    _misc_flags |= _misc_has_injected_identityObject;\n+  }\n+\n+  bool has_injected_primitiveObject() const {\n+    return (_misc_flags & _misc_has_injected_primitiveObject);\n+  }\n+\n+  void set_has_injected_primitiveObject() {\n+    _misc_flags |= _misc_has_injected_primitiveObject;\n+  }\n+\n@@ -398,4 +510,4 @@\n-  ObjArrayKlass* array_klasses() const     { return _array_klasses; }\n-  inline ObjArrayKlass* array_klasses_acquire() const; \/\/ load with acquire semantics\n-  void set_array_klasses(ObjArrayKlass* k) { _array_klasses = k; }\n-  inline void release_set_array_klasses(ObjArrayKlass* k); \/\/ store with release semantics\n+  ArrayKlass* array_klasses() const     { return _array_klasses; }\n+  inline ArrayKlass* array_klasses_acquire() const; \/\/ load with acquire semantics\n+  void set_array_klasses(ArrayKlass* k) { _array_klasses = k; }\n+  inline void release_set_array_klasses(ArrayKlass* k); \/\/ store with release semantics\n@@ -445,0 +557,2 @@\n+  bool    field_is_inlined(int index) const { return field(index)->is_inlined(); }\n+  bool    field_is_inline_type(int index) const;\n@@ -572,0 +686,4 @@\n+  static ByteSize kind_offset() { return in_ByteSize(offset_of(InstanceKlass, _kind)); }\n+  static ByteSize misc_flags_offset() { return in_ByteSize(offset_of(InstanceKlass, _misc_flags)); }\n+  static u4 misc_flags_is_empty_inline_type() { return _misc_is_empty_inline_type; }\n+\n@@ -754,0 +872,1 @@\n+\n@@ -755,1 +874,1 @@\n-    return ((_misc_flags & _misc_is_being_redefined) != 0);\n+    return (_misc_flags & _misc_is_being_redefined);\n@@ -840,0 +959,1 @@\n+  bool is_inline_type_klass()           const { return is_kind(_kind_inline_type); }\n@@ -1009,0 +1129,3 @@\n+  static ByteSize inline_type_field_klasses_offset() { return in_ByteSize(offset_of(InstanceKlass, _inline_type_field_klasses)); }\n+  static ByteSize adr_inlineklass_fixed_block_offset() { return in_ByteSize(offset_of(InstanceKlass, _adr_inlineklass_fixed_block)); }\n+\n@@ -1043,2 +1166,2 @@\n-  void array_klasses_do(void f(Klass* k));\n-  void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);\n+  virtual void array_klasses_do(void f(Klass* k));\n+  virtual void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);\n@@ -1065,1 +1188,2 @@\n-                  bool is_interface, bool is_unsafe_anonymous, bool has_stored_fingerprint) {\n+                  bool is_interface, bool is_unsafe_anonymous, bool has_stored_fingerprint,\n+                  int java_fields, bool is_inline_type) {\n@@ -1072,1 +1196,3 @@\n-           (has_stored_fingerprint ? (int)sizeof(uint64_t*)\/wordSize : 0));\n+           (has_stored_fingerprint ? (int)sizeof(uint64_t*)\/wordSize : 0) +\n+           (java_fields * (int)sizeof(Klass*)\/wordSize) +\n+           (is_inline_type ? (int)sizeof(InlineKlassFixedBlock) : 0));\n@@ -1079,1 +1205,3 @@\n-                                               has_stored_fingerprint());\n+                                               has_stored_fingerprint(),\n+                                               has_inline_type_fields() ? java_fields_count() : 0,\n+                                               is_inline_klass());\n@@ -1086,0 +1214,1 @@\n+  bool bounds_check(address addr, bool edge_ok = false, intptr_t size_in_bytes = -1) const PRODUCT_RETURN0;\n@@ -1094,0 +1223,6 @@\n+  inline address adr_inline_type_field_klasses() const;\n+  inline Klass* get_inline_type_field_klass(int idx) const;\n+  inline Klass* get_inline_type_field_klass_or_null(int idx) const;\n+  inline void set_inline_type_field_klass(int idx, Klass* k);\n+  inline void reset_inline_type_field_klass(int idx);\n+\n@@ -1095,1 +1230,1 @@\n-  int size_helper() const {\n+  virtual int size_helper() const {\n@@ -1232,1 +1367,1 @@\n-\n+protected:\n@@ -1234,1 +1369,1 @@\n-  Klass* array_klass_impl(bool or_null, int n, TRAPS);\n+  virtual Klass* array_klass_impl(bool or_null, int n, TRAPS);\n@@ -1237,1 +1372,3 @@\n-  Klass* array_klass_impl(bool or_null, TRAPS);\n+  virtual Klass* array_klass_impl(bool or_null, TRAPS);\n+\n+private:\n@@ -1267,1 +1404,1 @@\n-  void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n+  virtual void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":157,"deletions":20,"binary":false,"changes":177,"status":"modified"},{"patch":"@@ -116,1 +116,56 @@\n-inline ObjArrayKlass* InstanceKlass::array_klasses_acquire() const {\n+inline address InstanceKlass::adr_inline_type_field_klasses() const {\n+  if (has_inline_type_fields()) {\n+    address adr_fing = adr_fingerprint();\n+    if (adr_fing != NULL) {\n+      return adr_fingerprint() + sizeof(u8);\n+    }\n+\n+    InstanceKlass** adr_host = adr_unsafe_anonymous_host();\n+    if (adr_host != NULL) {\n+      return (address)(adr_host + 1);\n+    }\n+\n+    InstanceKlass* volatile* adr_impl = adr_implementor();\n+    if (adr_impl != NULL) {\n+      return (address)(adr_impl + 1);\n+    }\n+\n+    return (address)end_of_nonstatic_oop_maps();\n+  } else {\n+    return NULL;\n+  }\n+}\n+\n+inline Klass* InstanceKlass::get_inline_type_field_klass(int idx) const {\n+  assert(has_inline_type_fields(), \"Sanity checking\");\n+  assert(idx < java_fields_count(), \"IOOB\");\n+  Klass* k = ((Klass**)adr_inline_type_field_klasses())[idx];\n+  assert(k != NULL, \"Should always be set before being read\");\n+  assert(k->is_inline_klass(), \"Must be an inline type\");\n+  return k;\n+}\n+\n+inline Klass* InstanceKlass::get_inline_type_field_klass_or_null(int idx) const {\n+  assert(has_inline_type_fields(), \"Sanity checking\");\n+  assert(idx < java_fields_count(), \"IOOB\");\n+  Klass* k = ((Klass**)adr_inline_type_field_klasses())[idx];\n+  assert(k == NULL || k->is_inline_klass(), \"Must be an inline type\");\n+  return k;\n+}\n+\n+inline void InstanceKlass::set_inline_type_field_klass(int idx, Klass* k) {\n+  assert(has_inline_type_fields(), \"Sanity checking\");\n+  assert(idx < java_fields_count(), \"IOOB\");\n+  assert(k != NULL, \"Should not be set to NULL\");\n+  assert(((Klass**)adr_inline_type_field_klasses())[idx] == NULL, \"Should not be set twice\");\n+  ((Klass**)adr_inline_type_field_klasses())[idx] = k;\n+}\n+\n+inline void InstanceKlass::reset_inline_type_field_klass(int idx) {\n+  assert(has_inline_type_fields(), \"Sanity checking\");\n+  assert(idx < java_fields_count(), \"IOOB\");\n+  ((Klass**)adr_inline_type_field_klasses())[idx] = NULL;\n+}\n+\n+\n+inline ArrayKlass* InstanceKlass::array_klasses_acquire() const {\n@@ -120,1 +175,1 @@\n-inline void InstanceKlass::release_set_array_klasses(ObjArrayKlass* k) {\n+inline void InstanceKlass::release_set_array_klasses(ArrayKlass* k) {\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.inline.hpp","additions":57,"deletions":2,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -121,1 +122,0 @@\n-\n@@ -158,0 +158,5 @@\n+address Method::get_c2i_inline_entry() {\n+  assert(adapter() != NULL, \"must have\");\n+  return adapter()->get_c2i_inline_entry();\n+}\n+\n@@ -163,0 +168,5 @@\n+address Method::get_c2i_unverified_inline_entry() {\n+  assert(adapter() != NULL, \"must have\");\n+  return adapter()->get_c2i_unverified_inline_entry();\n+}\n+\n@@ -355,0 +365,2 @@\n+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_inline_ro_entry);\n+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_inline_entry);\n@@ -599,0 +611,18 @@\n+\/\/ InlineKlass the method is declared to return. This must not\n+\/\/ safepoint as it is called with references live on the stack at\n+\/\/ locations the GC is unaware of.\n+InlineKlass* Method::returned_inline_type(Thread* thread) const {\n+  SignatureStream ss(signature());\n+  while (!ss.at_return_type()) {\n+    ss.next();\n+  }\n+  Handle class_loader(thread, method_holder()->class_loader());\n+  Handle protection_domain(thread, method_holder()->protection_domain());\n+  Klass* k = NULL;\n+  {\n+    NoSafepointVerifier nsv;\n+    k = ss.as_klass(class_loader, protection_domain, SignatureStream::ReturnNull, thread);\n+  }\n+  assert(k != NULL && !thread->has_pending_exception(), \"can't resolve klass\");\n+  return InlineKlass::cast(k);\n+}\n@@ -609,1 +639,1 @@\n-  \/\/   aload_0\n+  \/\/   aload_0, _fast_aload_0, or _nofast_aload_0\n@@ -633,1 +663,2 @@\n-  if (cb[0] != Bytecodes::_aload_0 || cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {\n+  if ((cb[0] != Bytecodes::_aload_0 && cb[0] != Bytecodes::_fast_aload_0 && cb[0] != Bytecodes::_nofast_aload_0) ||\n+       cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {\n@@ -813,7 +844,2 @@\n-bool Method::is_initializer() const {\n-  return is_object_initializer() || is_static_initializer();\n-}\n-\n-bool Method::has_valid_initializer_flags() const {\n-  return (is_static() ||\n-          method_holder()->major_version() < 51);\n+bool Method::is_object_constructor_or_class_initializer() const {\n+  return (is_object_constructor() || is_class_initializer());\n@@ -822,1 +848,1 @@\n-bool Method::is_static_initializer() const {\n+bool Method::is_class_initializer() const {\n@@ -826,2 +852,8 @@\n-  return name() == vmSymbols::class_initializer_name() &&\n-         has_valid_initializer_flags();\n+  return (name() == vmSymbols::class_initializer_name() &&\n+          (is_static() ||\n+           method_holder()->major_version() < 51));\n+}\n+\n+\/\/ A method named <init>, if non-static, is a classic object constructor.\n+bool Method::is_object_constructor() const {\n+   return name() == vmSymbols::object_initializer_name() && !is_static();\n@@ -830,2 +862,3 @@\n-bool Method::is_object_initializer() const {\n-   return name() == vmSymbols::object_initializer_name();\n+\/\/ A static method named <init> is a factory for an inline class.\n+bool Method::is_static_init_factory() const {\n+   return name() == vmSymbols::object_initializer_name() && is_static();\n@@ -889,1 +922,1 @@\n-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {\n+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {\n@@ -905,1 +938,3 @@\n-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;\n+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {\n+      return false;\n+    }\n@@ -1074,0 +1109,2 @@\n+    _from_compiled_inline_entry = NULL;\n+    _from_compiled_inline_ro_entry = NULL;\n@@ -1076,0 +1113,2 @@\n+    _from_compiled_inline_entry = adapter()->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = adapter()->get_c2i_inline_ro_entry();\n@@ -1116,0 +1155,4 @@\n+  assert(*((int*)_from_compiled_inline_ro_entry) == 0,\n+         \"must be NULL during dump time, to be initialized at run time\");\n+  assert(*((int*)_from_compiled_inline_entry) == 0,\n+         \"must be NULL during dump time, to be initialized at run time\");\n@@ -1267,0 +1310,2 @@\n+    assert(mh->_from_compiled_inline_entry != NULL, \"must be\");\n+    assert(mh->_from_compiled_inline_ro_entry != NULL, \"must be\");\n@@ -1270,0 +1315,2 @@\n+    mh->_from_compiled_inline_entry = adapter->get_c2i_inline_entry();\n+    mh->_from_compiled_inline_ro_entry = adapter->get_c2i_inline_ro_entry();\n@@ -1277,0 +1324,12 @@\n+#if 0\n+  \/*\n+   * CDS:TODO --\n+   * \"Q\" classes in the method signature must be resolved during link_method.\n+   * However, at this point we are still inside method_holder()->restore_unshareable_info.\n+   * If we try to resolve method_holder(), or multually dependent classes, it will\n+   * cause deadlock and other ill effects.\n+   *\n+   * For now, lets do method linking inside InstanceKlass::link_class(). Optimization\n+   * may be possible if we know that resolution will never happen.\n+   *\/\n+\n@@ -1283,0 +1342,1 @@\n+#endif\n@@ -1285,1 +1345,1 @@\n-address Method::from_compiled_entry_no_trampoline() const {\n+address Method::from_compiled_entry_no_trampoline(bool caller_is_c1) const {\n@@ -1287,2 +1347,7 @@\n-  if (code) {\n-    return code->verified_entry_point();\n+  if (caller_is_c1) {\n+    \/\/ C1 - inline type arguments are passed as objects\n+    if (code) {\n+      return code->verified_inline_entry_point();\n+    } else {\n+      return adapter()->get_c2i_inline_entry();\n+    }\n@@ -1290,1 +1355,6 @@\n-    return adapter()->get_c2i_entry();\n+    \/\/ C2 - inline type arguments may be passed as fields\n+    if (code) {\n+      return code->verified_entry_point();\n+    } else {\n+      return adapter()->get_c2i_entry();\n+    }\n@@ -1307,0 +1377,12 @@\n+address Method::verified_inline_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_entry != NULL, \"must be set\");\n+  return _from_compiled_inline_entry;\n+}\n+\n+address Method::verified_inline_ro_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_ro_entry != NULL, \"must be set\");\n+  return _from_compiled_inline_ro_entry;\n+}\n+\n@@ -1338,0 +1420,2 @@\n+  mh->_from_compiled_inline_entry = code->verified_inline_entry_point();\n+  mh->_from_compiled_inline_ro_entry = code->verified_inline_ro_entry_point();\n@@ -2364,0 +2448,4 @@\n+#ifdef ASSERT\n+  if (valid_itable_index())\n+    st->print_cr(\" - itable index:      %d\",   itable_index());\n+#endif\n@@ -2441,0 +2529,1 @@\n+  if (WizardMode) access_flags().print_on(st);\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":110,"deletions":21,"binary":false,"changes":131,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -117,3 +119,7 @@\n-  Node* src = in(ArrayCopyNode::Src);\n-  const Type* src_type = phase->type(src);\n-\n+    Node* src = in(ArrayCopyNode::Src);\n+    const Type* src_type = phase->type(src);\n+\n+    if (src_type == Type::TOP) {\n+      return -1;\n+    }\n+\n@@ -141,0 +147,1 @@\n+             (UseFlatArray && ary_src->elem()->make_oopptr() != NULL && ary_src->elem()->make_oopptr()->can_be_inline_type()) ||\n@@ -193,0 +200,1 @@\n+  phase->record_for_igvn(mem);\n@@ -270,2 +278,6 @@\n-    if (is_reference_type(src_elem))   src_elem  = T_OBJECT;\n-    if (is_reference_type(dest_elem))  dest_elem = T_OBJECT;\n+    if (src_elem == T_ARRAY || (src_elem == T_INLINE_TYPE && ary_src->klass()->is_obj_array_klass())) {\n+      src_elem  = T_OBJECT;\n+    }\n+    if (dest_elem == T_ARRAY || (dest_elem == T_INLINE_TYPE && ary_dest->klass()->is_obj_array_klass())) {\n+      dest_elem = T_OBJECT;\n+    }\n@@ -279,3 +291,4 @@\n-    if (bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), dest_elem, false, BarrierSetC2::Optimization)) {\n-      \/\/ It's an object array copy but we can't emit the card marking\n-      \/\/ that is needed\n+    if (bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), dest_elem, false, BarrierSetC2::Optimization) ||\n+        (src_elem == T_INLINE_TYPE && ary_src->elem()->inline_klass()->contains_oops() &&\n+         bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), T_OBJECT, false, BarrierSetC2::Optimization))) {\n+      \/\/ It's an object array copy but we can't emit the card marking that is needed\n@@ -288,0 +301,4 @@\n+    if (dest_elem == T_INLINE_TYPE) {\n+      ciFlatArrayKlass* vak = ary_src->klass()->as_flat_array_klass();\n+      shift = vak->log2_element_size();\n+    }\n@@ -313,2 +330,4 @@\n-    adr_src  = phase->transform(new AddPNode(base_src, base_src, src_offset));\n-    adr_dest = phase->transform(new AddPNode(base_dest, base_dest, dest_offset));\n+    if (ary_src->elem()->make_oopptr() != NULL &&\n+        ary_src->elem()->make_oopptr()->can_be_inline_type()) {\n+      return false;\n+    }\n@@ -317,1 +336,1 @@\n-    if (is_reference_type(elem)) {\n+    if (elem == T_ARRAY || (elem == T_INLINE_TYPE && ary_src->klass()->is_obj_array_klass())) {\n@@ -322,1 +341,4 @@\n-    if (bs->array_copy_requires_gc_barriers(true, elem, true, BarrierSetC2::Optimization)) {\n+    if (bs->array_copy_requires_gc_barriers(true, elem, true, BarrierSetC2::Optimization) ||\n+        (elem == T_INLINE_TYPE && ary_src->elem()->inline_klass()->contains_oops() &&\n+         bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Optimization))) {\n+      \/\/ It's an object array copy but we can't emit the card marking that is needed\n@@ -326,0 +348,3 @@\n+    adr_src  = phase->transform(new AddPNode(base_src, base_src, src_offset));\n+    adr_dest = phase->transform(new AddPNode(base_dest, base_dest, dest_offset));\n+\n@@ -343,1 +368,1 @@\n-const TypePtr* ArrayCopyNode::get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n) {\n+const TypeAryPtr* ArrayCopyNode::get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n) {\n@@ -348,1 +373,1 @@\n-  return atp->add_offset(Type::OffsetBot);\n+  return atp->add_offset(Type::OffsetBot)->is_aryptr();\n@@ -351,2 +376,2 @@\n-void ArrayCopyNode::array_copy_test_overlap(PhaseGVN *phase, bool can_reshape, bool disjoint_bases, int count, Node*& forward_ctl, Node*& backward_ctl) {\n-  Node* ctl = in(TypeFunc::Control);\n+void ArrayCopyNode::array_copy_test_overlap(GraphKit& kit, bool disjoint_bases, int count, Node*& backward_ctl) {\n+  Node* ctl = kit.control();\n@@ -354,0 +379,1 @@\n+    PhaseGVN& gvn = kit.gvn();\n@@ -357,2 +383,2 @@\n-    Node* cmp = phase->transform(new CmpINode(src_offset, dest_offset));\n-    Node *bol = phase->transform(new BoolNode(cmp, BoolTest::lt));\n+    Node* cmp = gvn.transform(new CmpINode(src_offset, dest_offset));\n+    Node *bol = gvn.transform(new BoolNode(cmp, BoolTest::lt));\n@@ -361,1 +387,6 @@\n-    phase->transform(iff);\n+    gvn.transform(iff);\n+\n+    kit.set_control(gvn.transform(new IfFalseNode(iff)));\n+    backward_ctl = gvn.transform(new IfTrueNode(iff));\n+  }\n+}\n@@ -363,2 +394,33 @@\n-    forward_ctl = phase->transform(new IfFalseNode(iff));\n-    backward_ctl = phase->transform(new IfTrueNode(iff));\n+void ArrayCopyNode::copy(GraphKit& kit,\n+                         const TypeAryPtr* atp_src,\n+                         const TypeAryPtr* atp_dest,\n+                         int i,\n+                         Node* base_src,\n+                         Node* base_dest,\n+                         Node* adr_src,\n+                         Node* adr_dest,\n+                         BasicType copy_type,\n+                         const Type* value_type) {\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  Node* ctl = kit.control();\n+  if (copy_type == T_INLINE_TYPE) {\n+    ciFlatArrayKlass* vak = atp_src->klass()->as_flat_array_klass();\n+    ciInlineKlass* vk = vak->element_klass()->as_inline_klass();\n+    for (int j = 0; j < vk->nof_nonstatic_fields(); j++) {\n+      ciField* field = vk->nonstatic_field_at(j);\n+      int off_in_vt = field->offset() - vk->first_field_offset();\n+      Node* off  = kit.MakeConX(off_in_vt + i * vak->element_byte_size());\n+      ciType* ft = field->type();\n+      BasicType bt = type2field[ft->basic_type()];\n+      assert(!field->is_flattened(), \"flattened field encountered\");\n+      if (bt == T_INLINE_TYPE) {\n+        bt = T_OBJECT;\n+      }\n+      const Type* rt = Type::get_const_type(ft);\n+      const TypePtr* adr_type = atp_src->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+      assert(!bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), bt, false, BarrierSetC2::Optimization), \"GC barriers required\");\n+      Node* next_src = kit.gvn().transform(new AddPNode(base_src, adr_src, off));\n+      Node* next_dest = kit.gvn().transform(new AddPNode(base_dest, adr_dest, off));\n+      Node* v = load(bs, &kit.gvn(), ctl, kit.merged_memory(), next_src, adr_type, rt, bt);\n+      store(bs, &kit.gvn(), ctl, kit.merged_memory(), next_dest, adr_type, v, rt, bt);\n+    }\n@@ -366,1 +428,5 @@\n-    forward_ctl = ctl;\n+    Node* off = kit.MakeConX(type2aelembytes(copy_type) * i);\n+    Node* next_src = kit.gvn().transform(new AddPNode(base_src, adr_src, off));\n+    Node* next_dest = kit.gvn().transform(new AddPNode(base_dest, adr_dest, off));\n+    Node* v = load(bs, &kit.gvn(), ctl, kit.merged_memory(), next_src, atp_src, value_type, copy_type);\n+    store(bs, &kit.gvn(), ctl, kit.merged_memory(), next_dest, atp_dest, v, value_type, copy_type);\n@@ -368,0 +434,1 @@\n+  kit.set_control(ctl);\n@@ -370,16 +437,13 @@\n-Node* ArrayCopyNode::array_copy_forward(PhaseGVN *phase,\n-                                        bool can_reshape,\n-                                        Node*& forward_ctl,\n-                                        Node* mem,\n-                                        const TypePtr* atp_src,\n-                                        const TypePtr* atp_dest,\n-                                        Node* adr_src,\n-                                        Node* base_src,\n-                                        Node* adr_dest,\n-                                        Node* base_dest,\n-                                        BasicType copy_type,\n-                                        const Type* value_type,\n-                                        int count) {\n-  if (!forward_ctl->is_top()) {\n-    \/\/ copy forward\n-    MergeMemNode* mm = MergeMemNode::make(mem);\n+void ArrayCopyNode::array_copy_forward(GraphKit& kit,\n+                                       bool can_reshape,\n+                                       const TypeAryPtr* atp_src,\n+                                       const TypeAryPtr* atp_dest,\n+                                       Node* adr_src,\n+                                       Node* base_src,\n+                                       Node* adr_dest,\n+                                       Node* base_dest,\n+                                       BasicType copy_type,\n+                                       const Type* value_type,\n+                                       int count) {\n+  if (!kit.stopped()) {\n+    \/\/ copy forward\n@@ -388,9 +452,2 @@\n-      BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-      Node* v = load(bs, phase, forward_ctl, mm, adr_src, atp_src, value_type, copy_type);\n-      store(bs, phase, forward_ctl, mm, adr_dest, atp_dest, v, value_type, copy_type);\n-      for (int i = 1; i < count; i++) {\n-        Node* off  = phase->MakeConX(type2aelembytes(copy_type) * i);\n-        Node* next_src = phase->transform(new AddPNode(base_src,adr_src,off));\n-        Node* next_dest = phase->transform(new AddPNode(base_dest,adr_dest,off));\n-        v = load(bs, phase, forward_ctl, mm, next_src, atp_src, value_type, copy_type);\n-        store(bs, phase, forward_ctl, mm, next_dest, atp_dest, v, value_type, copy_type);\n+      for (int i = 0; i < count; i++) {\n+        copy(kit, atp_src, atp_dest, i, base_src, base_dest, adr_src, adr_dest, copy_type, value_type);\n@@ -399,3 +456,4 @@\n-      PhaseIterGVN* igvn = phase->is_IterGVN();\n-      igvn->_worklist.push(adr_src);\n-      igvn->_worklist.push(adr_dest);\n+      PhaseGVN& gvn = kit.gvn();\n+      assert(gvn.is_IterGVN(), \"\");\n+      gvn.record_for_igvn(adr_src);\n+      gvn.record_for_igvn(adr_dest);\n@@ -403,2 +461,0 @@\n-    return mm;\n-  return phase->C->top();\n@@ -408,14 +464,12 @@\n-Node* ArrayCopyNode::array_copy_backward(PhaseGVN *phase,\n-                                         bool can_reshape,\n-                                         Node*& backward_ctl,\n-                                         Node* mem,\n-                                         const TypePtr* atp_src,\n-                                         const TypePtr* atp_dest,\n-                                         Node* adr_src,\n-                                         Node* base_src,\n-                                         Node* adr_dest,\n-                                         Node* base_dest,\n-                                         BasicType copy_type,\n-                                         const Type* value_type,\n-                                         int count) {\n-  if (!backward_ctl->is_top()) {\n+void ArrayCopyNode::array_copy_backward(GraphKit& kit,\n+                                        bool can_reshape,\n+                                        const TypeAryPtr* atp_src,\n+                                        const TypeAryPtr* atp_dest,\n+                                        Node* adr_src,\n+                                        Node* base_src,\n+                                        Node* adr_dest,\n+                                        Node* base_dest,\n+                                        BasicType copy_type,\n+                                        const Type* value_type,\n+                                        int count) {\n+  if (!kit.stopped()) {\n@@ -423,4 +477,1 @@\n-    MergeMemNode* mm = MergeMemNode::make(mem);\n-\n-    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-    assert(copy_type != T_OBJECT || !bs->array_copy_requires_gc_barriers(false, T_OBJECT, false, BarrierSetC2::Optimization), \"only tightly coupled allocations for object arrays\");\n+    PhaseGVN& gvn = kit.gvn();\n@@ -429,6 +480,2 @@\n-      for (int i = count-1; i >= 1; i--) {\n-        Node* off  = phase->MakeConX(type2aelembytes(copy_type) * i);\n-        Node* next_src = phase->transform(new AddPNode(base_src,adr_src,off));\n-        Node* next_dest = phase->transform(new AddPNode(base_dest,adr_dest,off));\n-        Node* v = load(bs, phase, backward_ctl, mm, next_src, atp_src, value_type, copy_type);\n-        store(bs, phase, backward_ctl, mm, next_dest, atp_dest, v, value_type, copy_type);\n+      for (int i = count-1; i >= 0; i--) {\n+        copy(kit, atp_src, atp_dest, i, base_src, base_dest, adr_src, adr_dest, copy_type, value_type);\n@@ -436,6 +483,5 @@\n-      Node* v = load(bs, phase, backward_ctl, mm, adr_src, atp_src, value_type, copy_type);\n-      store(bs, phase, backward_ctl, mm, adr_dest, atp_dest, v, value_type, copy_type);\n-    } else if (can_reshape) {\n-      PhaseIterGVN* igvn = phase->is_IterGVN();\n-      igvn->_worklist.push(adr_src);\n-      igvn->_worklist.push(adr_dest);\n+    } else if(can_reshape) {\n+      PhaseGVN& gvn = kit.gvn();\n+      assert(gvn.is_IterGVN(), \"\");\n+      gvn.record_for_igvn(adr_src);\n+      gvn.record_for_igvn(adr_dest);\n@@ -443,2 +489,0 @@\n-    return phase->transform(mm);\n-  return phase->C->top();\n@@ -470,2 +514,1 @@\n-      CallProjections callprojs;\n-      extract_projections(&callprojs, true, false);\n+      CallProjections* callprojs = extract_projections(true, false);\n@@ -473,2 +516,2 @@\n-      if (callprojs.fallthrough_ioproj != NULL) {\n-        igvn->replace_node(callprojs.fallthrough_ioproj, in(TypeFunc::I_O));\n+      if (callprojs->fallthrough_ioproj != NULL) {\n+        igvn->replace_node(callprojs->fallthrough_ioproj, in(TypeFunc::I_O));\n@@ -476,2 +519,2 @@\n-      if (callprojs.fallthrough_memproj != NULL) {\n-        igvn->replace_node(callprojs.fallthrough_memproj, mem);\n+      if (callprojs->fallthrough_memproj != NULL) {\n+        igvn->replace_node(callprojs->fallthrough_memproj, mem);\n@@ -479,2 +522,2 @@\n-      if (callprojs.fallthrough_catchproj != NULL) {\n-        igvn->replace_node(callprojs.fallthrough_catchproj, ctl);\n+      if (callprojs->fallthrough_catchproj != NULL) {\n+        igvn->replace_node(callprojs->fallthrough_catchproj, ctl);\n@@ -495,0 +538,9 @@\n+#ifdef ASSERT\n+      Node* src = in(ArrayCopyNode::Src);\n+      const Type* src_type = phase->type(src);\n+      const TypeAryPtr* ary_src = src_type->isa_aryptr();\n+      BasicType elem = ary_src != NULL ? ary_src->klass()->as_array_klass()->element_type()->basic_type() : T_CONFLICT;\n+      BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+      assert(!is_clonebasic() || bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Optimization) ||\n+             (ary_src != NULL && elem == T_INLINE_TYPE && ary_src->klass()->is_obj_array_klass()), \"added control for clone?\");\n+#endif\n@@ -505,1 +557,5 @@\n-  if (remove_dead_region(phase, can_reshape))  return this;\n+  \/\/ Perform any generic optimizations first\n+  Node* result = SafePointNode::Ideal(phase, can_reshape);\n+  if (result != NULL) {\n+    return result;\n+  }\n@@ -547,0 +603,11 @@\n+  Node* src = in(ArrayCopyNode::Src);\n+  Node* dest = in(ArrayCopyNode::Dest);\n+  const Type* src_type = phase->type(src);\n+  const Type* dest_type = phase->type(dest);\n+\n+  if (src_type->isa_aryptr() && dest_type->isa_instptr()) {\n+    \/\/ clone used for load of unknown inline type can't be optimized at\n+    \/\/ this point\n+    return NULL;\n+  }\n+\n@@ -566,5 +633,21 @@\n-  Node* src = in(ArrayCopyNode::Src);\n-  Node* dest = in(ArrayCopyNode::Dest);\n-  const TypePtr* atp_src = get_address_type(phase, _src_type, src);\n-  const TypePtr* atp_dest = get_address_type(phase, _dest_type, dest);\n-  Node* in_mem = in(TypeFunc::Memory);\n+  JVMState* new_jvms = NULL;\n+  SafePointNode* new_map = NULL;\n+  if (!is_clonebasic()) {\n+    new_jvms = jvms()->clone_shallow(phase->C);\n+    new_map = new SafePointNode(req(), new_jvms);\n+    for (uint i = TypeFunc::FramePtr; i < req(); i++) {\n+      new_map->init_req(i, in(i));\n+    }\n+    new_jvms->set_map(new_map);\n+  } else {\n+    new_jvms = new (phase->C) JVMState(0);\n+    new_map = new SafePointNode(TypeFunc::Parms, new_jvms);\n+    new_jvms->set_map(new_map);\n+  }\n+  new_map->set_control(in(TypeFunc::Control));\n+  new_map->set_memory(MergeMemNode::make(in(TypeFunc::Memory)));\n+  new_map->set_i_o(in(TypeFunc::I_O));\n+  phase->record_for_igvn(new_map);\n+\n+  const TypeAryPtr* atp_src = get_address_type(phase, _src_type, src);\n+  const TypeAryPtr* atp_dest = get_address_type(phase, _dest_type, dest);\n@@ -577,0 +660,4 @@\n+  GraphKit kit(new_jvms, phase);\n+\n+  SafePointNode* backward_map = NULL;\n+  SafePointNode* forward_map = NULL;\n@@ -578,36 +665,36 @@\n-  Node* forward_ctl = phase->C->top();\n-  array_copy_test_overlap(phase, can_reshape, disjoint_bases, count, forward_ctl, backward_ctl);\n-\n-  Node* forward_mem = array_copy_forward(phase, can_reshape, forward_ctl,\n-                                         in_mem,\n-                                         atp_src, atp_dest,\n-                                         adr_src, base_src, adr_dest, base_dest,\n-                                         copy_type, value_type, count);\n-\n-  Node* backward_mem = array_copy_backward(phase, can_reshape, backward_ctl,\n-                                           in_mem,\n-                                           atp_src, atp_dest,\n-                                           adr_src, base_src, adr_dest, base_dest,\n-                                           copy_type, value_type, count);\n-\n-  Node* ctl = NULL;\n-  if (!forward_ctl->is_top() && !backward_ctl->is_top()) {\n-    ctl = new RegionNode(3);\n-    ctl->init_req(1, forward_ctl);\n-    ctl->init_req(2, backward_ctl);\n-    ctl = phase->transform(ctl);\n-    MergeMemNode* forward_mm = forward_mem->as_MergeMem();\n-    MergeMemNode* backward_mm = backward_mem->as_MergeMem();\n-    for (MergeMemStream mms(forward_mm, backward_mm); mms.next_non_empty2(); ) {\n-      if (mms.memory() != mms.memory2()) {\n-        Node* phi = new PhiNode(ctl, Type::MEMORY, phase->C->get_adr_type(mms.alias_idx()));\n-        phi->init_req(1, mms.memory());\n-        phi->init_req(2, mms.memory2());\n-        phi = phase->transform(phi);\n-        mms.set_memory(phi);\n-      }\n-    }\n-    mem = forward_mem;\n-  } else if (!forward_ctl->is_top()) {\n-    ctl = forward_ctl;\n-    mem = forward_mem;\n+\n+  array_copy_test_overlap(kit, disjoint_bases, count, backward_ctl);\n+\n+  {\n+    PreserveJVMState pjvms(&kit);\n+\n+    array_copy_forward(kit, can_reshape,\n+                       atp_src, atp_dest,\n+                       adr_src, base_src, adr_dest, base_dest,\n+                       copy_type, value_type, count);\n+\n+    forward_map = kit.stop();\n+  }\n+\n+  kit.set_control(backward_ctl);\n+  array_copy_backward(kit, can_reshape,\n+                      atp_src, atp_dest,\n+                      adr_src, base_src, adr_dest, base_dest,\n+                      copy_type, value_type, count);\n+\n+  backward_map = kit.stop();\n+\n+  if (!forward_map->control()->is_top() && !backward_map->control()->is_top()) {\n+    assert(forward_map->i_o() == backward_map->i_o(), \"need a phi on IO?\");\n+    Node* ctl = new RegionNode(3);\n+    Node* mem = new PhiNode(ctl, Type::MEMORY, TypePtr::BOTTOM);\n+    kit.set_map(forward_map);\n+    ctl->init_req(1, kit.control());\n+    mem->init_req(1, kit.reset_memory());\n+    kit.set_map(backward_map);\n+    ctl->init_req(2, kit.control());\n+    mem->init_req(2, kit.reset_memory());\n+    kit.set_control(phase->transform(ctl));\n+    kit.set_all_memory(phase->transform(mem));\n+  } else if (!forward_map->control()->is_top()) {\n+    kit.set_map(forward_map);\n@@ -615,3 +702,2 @@\n-    assert(!backward_ctl->is_top(), \"no copy?\");\n-    ctl = backward_ctl;\n-    mem = backward_mem;\n+    assert(!backward_map->control()->is_top(), \"no copy?\");\n+    kit.set_map(backward_map);\n@@ -625,1 +711,5 @@\n-  if (!finish_transform(phase, can_reshape, ctl, mem)) {\n+  mem = kit.map()->memory();\n+  if (!finish_transform(phase, can_reshape, kit.control(), mem)) {\n+    if (!can_reshape) {\n+      phase->record_for_igvn(this);\n+    }\n@@ -718,1 +808,2 @@\n-  BasicType ary_elem = ary_t->klass()->as_array_klass()->element_type()->basic_type();\n+  ciArrayKlass* klass = ary_t->klass()->as_array_klass();\n+  BasicType ary_elem = klass->element_type()->basic_type();\n@@ -721,0 +812,3 @@\n+  if (klass->is_flat_array_klass()) {\n+    elemsize = klass->as_flat_array_klass()->element_byte_size();\n+  }\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.cpp","additions":233,"deletions":139,"binary":false,"changes":372,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -400,0 +401,3 @@\n+  if (dead->is_InlineTypeBase()) {\n+    remove_inline_type(dead);\n+  }\n@@ -441,0 +445,1 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n@@ -575,0 +580,1 @@\n+                  _inline_type_nodes (comp_arena(), 8, 0, NULL),\n@@ -679,4 +685,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -689,1 +693,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -824,0 +828,4 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n@@ -981,0 +989,4 @@\n+  _has_flattened_accesses = false;\n+  _flattened_accesses_share_alias = true;\n+  _scalarize_in_safepoints = false;\n+\n@@ -1283,1 +1295,2 @@\n-    assert(InlineUnsafeOps, \"indeterminate pointers come only from unsafe ops\");\n+    bool default_value_load = EnableValhalla && tj->is_instptr()->klass() == ciEnv::current()->Class_klass();\n+    assert(InlineUnsafeOps || default_value_load, \"indeterminate pointers come only from unsafe ops\");\n@@ -1296,0 +1309,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1300,1 +1322,1 @@\n-      tj = ta = TypeAryPtr::make(ptr, ta->ary(), ta->klass(), true, offset, ta->instance_id());\n+      tj = ta = TypeAryPtr::make(ptr, ta->ary(), ta->klass(), true, Type::Offset(offset), ta->field_offset(), ta->instance_id());\n@@ -1306,0 +1328,2 @@\n+    \/\/ For flattened inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1309,1 +1333,1 @@\n-        tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),ta->ary(),ta->klass(),false,offset);\n+        tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1323,1 +1347,1 @@\n-        tj = ta = TypeAryPtr::make(ptr,ta->ary(),ta->klass(),false,offset);\n+        tj = ta = TypeAryPtr::make(ptr,ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1329,1 +1353,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,ta->klass(),false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1334,1 +1358,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());\n@@ -1338,1 +1362,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInlineType::BOTTOM && _flattened_accesses_share_alias) {\n+      const TypeAry *tary = TypeAry::make(TypeInlineType::BOTTOM, ta->size());\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1345,1 +1374,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1351,1 +1380,1 @@\n-      tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta->ary(),ta->klass(),false,offset);\n+      tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n@@ -1365,1 +1394,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,Type::Offset(offset));\n@@ -1373,1 +1402,1 @@\n-      tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,offset);\n+      tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,Type::Offset(offset));\n@@ -1376,1 +1405,1 @@\n-      tj = to = TypeInstPtr::make(to->ptr(),to->klass(),to->klass_is_exact(),to->const_oop(),to->offset(), to->instance_id());\n+      tj = to = TypeInstPtr::make(to->ptr(),to->klass(),to->klass_is_exact(),to->const_oop(),Type::Offset(to->offset()), to->klass()->flatten_array(), to->instance_id());\n@@ -1383,1 +1412,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, Type::Offset(offset));\n@@ -1397,1 +1426,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, offset, to->instance_id());\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, Type::Offset(offset), canonical_holder->flatten_array(), to->instance_id());\n@@ -1399,1 +1428,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, offset);\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, Type::Offset(offset));\n@@ -1416,1 +1445,1 @@\n-                                   offset);\n+                                   Type::Offset(offset));\n@@ -1420,1 +1449,1 @@\n-    if( klass->is_obj_array_klass() ) {\n+    if (klass != NULL && klass->is_obj_array_klass()) {\n@@ -1424,1 +1453,1 @@\n-      tj = tk = TypeKlassPtr::make( TypePtr::NotNull, k, offset );\n+      tj = tk = TypeKlassPtr::make(TypePtr::NotNull, k, Type::Offset(offset));\n@@ -1440,1 +1469,1 @@\n-      tj = tk = TypeKlassPtr::make( TypePtr::NotNull, tk->klass(), offset );\n+      tj = tk = TypeKlassPtr::make(TypePtr::NotNull, tk->klass(), Type::Offset(offset));\n@@ -1579,1 +1608,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1583,3 +1612,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = NULL;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1635,0 +1667,1 @@\n+    ciField* field = NULL;\n@@ -1641,0 +1674,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1642,1 +1676,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (elemtype->isa_inlinetype() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->first_field_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1654,0 +1695,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1664,1 +1707,0 @@\n-      ciField* field;\n@@ -1671,0 +1713,4 @@\n+      } else if (tinst->klass()->is_inlinetype()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1672,1 +1718,1 @@\n-        ciInstanceKlass *k = tinst->klass()->as_instance_klass();\n+        ciInstanceKlass* k = tinst->klass()->as_instance_klass();\n@@ -1675,7 +1721,14 @@\n-      assert(field == NULL ||\n-             original_field == NULL ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset() == original_field->offset() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != NULL)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == NULL ||\n+           original_field == NULL ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset() == original_field->offset() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != NULL) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_aryptr()->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1686,3 +1739,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1690,6 +1744,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == NULL) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == NULL) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1863,0 +1918,377 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineTypeBase(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineTypeBase(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    assert(!n->is_InlineType(), \"chain of inline type nodes\");\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineTypePtr()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  if (_inline_type_nodes.length() == 0) {\n+    return;\n+  }\n+  \/\/ Make sure that the return value does not keep an otherwise unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = NULL;\n+    for (uint i = 1; i < root()->req(); i++){\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == NULL, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != NULL) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeBaseNode* vt = _inline_type_nodes.pop()->as_InlineTypeBase();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+      } else if (vt->is_InlineTypePtr()) {\n+        igvn.replace_node(vt, vt->get_oop());\n+      } else {\n+#ifdef ASSERT\n+        for (DUIterator_Fast imax, i = vt->fast_outs(imax); i < imax; i++) {\n+          assert(vt->fast_out(i)->is_InlineTypeBase(), \"Unexpected inline type user\");\n+        }\n+#endif\n+        igvn.replace_node(vt, igvn.C->top());\n+      }\n+    }\n+  } else {\n+    \/\/ Give inline types a chance to be scalarized in safepoints\n+    \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info\n+    set_scalarize_in_safepoints(true);\n+    for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+      igvn._worklist.push(_inline_type_nodes.at(i));\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flattened_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flattened_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flattened array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flattened array) correct. We're done with parsing so we\n+  \/\/ now know all flattened array accesses in this compile\n+  \/\/ unit. Let's move flattened array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flattened memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flattened array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = NULL;\n+      if (n->Opcode() == Op_StoreCM) {\n+        adr_type = get_adr_type(get_alias_index(n->in(MemNode::OopStore)->adr_type()));\n+      } else {\n+        adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      }\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != NULL) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flattened_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flattened array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != NULL &&\n+          ace->_adr_type->isa_aryptr() &&\n+          ace->_adr_type->is_aryptr()->is_flat()) {\n+        ace->_adr_type = NULL;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the NULL adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = NULL;\n+      if (m->Opcode() == Op_StoreCM) {\n+        adr_type = m->in(MemNode::OopStore)->adr_type();\n+        if (adr_type != TypeAryPtr::INLINES) {\n+          \/\/ store was optimized out and we lost track of the adr_type\n+          Node* clone = new StoreCMNode(m->in(MemNode::Control), m->in(MemNode::Memory), m->in(MemNode::Address),\n+                                        m->adr_type(), m->in(MemNode::ValueIn), m->in(MemNode::OopStore),\n+                                        get_alias_index(adr_type));\n+          igvn.register_new_node_with_optimizer(clone);\n+          igvn.replace_node(m, clone);\n+        }\n+      } else {\n+        adr_type = m->adr_type();\n+#ifdef ASSERT\n+        m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      }\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flattened array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = NULL;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = NULL;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == NULL) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const Type* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flattened arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flattened array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const Type* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != NULL) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const Type* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flattened_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != NULL) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n+\n@@ -2208,0 +2640,5 @@\n+  \/\/ Process inline type nodes now that all inlining is over\n+  process_inline_types(igvn);\n+\n+  adjust_flattened_array_access_aliases(igvn);\n+\n@@ -2311,0 +2748,3 @@\n+  \/\/ Process inline type nodes again after loop opts\n+  process_inline_types(igvn);\n+\n@@ -2321,0 +2761,4 @@\n+  \/\/ Process inline type nodes again and remove them. From here\n+  \/\/ on we don't need to keep track of field values anymore.\n+  process_inline_types(igvn, \/* remove= *\/ true);\n+\n@@ -2899,0 +3343,1 @@\n+\n@@ -3052,1 +3497,16 @@\n-      n->add_prec(prec);\n+      if (prec->is_MergeMem()) {\n+        MergeMemNode* mm = prec->as_MergeMem();\n+        Node* base = mm->base_memory();\n+        for (int i = AliasIdxRaw + 1; i < num_alias_types(); i++) {\n+          const Type* adr_type = get_adr_type(i);\n+          if (adr_type->isa_aryptr() && adr_type->is_aryptr()->is_flat()) {\n+            Node* m = mm->memory_at(i);\n+            n->add_prec(m);\n+          }\n+        }\n+        if (mm->outcnt() == 0) {\n+          mm->disconnect_inputs(this);\n+        }\n+      } else {\n+        n->add_prec(prec);\n+      }\n@@ -3623,0 +4083,8 @@\n+#ifdef ASSERT\n+  case Op_InlineTypePtr:\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n+#endif\n@@ -3970,2 +4438,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -3979,1 +4447,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4100,1 +4568,1 @@\n-  if (StressReflectiveCode) {\n+  if (StressReflectiveCode || superk == NULL || subk == NULL) {\n@@ -4109,1 +4577,2 @@\n-  if (superelem->is_array_klass())\n+  if (superelem->is_array_klass()) {\n+    ciArrayKlass* ak = superelem->as_array_klass();\n@@ -4111,0 +4580,1 @@\n+  }\n@@ -4552,0 +5022,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == NULL || tb == NULL ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are NULL.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(NULL, a));\n+    b = phase->transform(new CastP2XNode(NULL, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":544,"deletions":53,"binary":false,"changes":597,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+class CallNode;\n@@ -90,0 +91,1 @@\n+class InlineTypeBaseNode;\n@@ -306,0 +308,3 @@\n+  bool                  _has_flattened_accesses; \/\/ Any known flattened array accesses?\n+  bool                  _flattened_accesses_share_alias; \/\/ Initially all flattened array share a single slice\n+  bool                  _scalarize_in_safepoints; \/\/ Scalarize inline types in safepoint debug info\n@@ -321,0 +326,1 @@\n+  GrowableArray<Node*>  _inline_type_nodes;     \/\/ List of InlineType nodes\n@@ -597,0 +603,9 @@\n+  void          set_flattened_accesses()         { _has_flattened_accesses = true; }\n+  bool          flattened_accesses_share_alias() const { return _flattened_accesses_share_alias; }\n+  void          set_flattened_accesses_share_alias(bool z) { _flattened_accesses_share_alias = z; }\n+  bool          scalarize_in_safepoints() const { return _scalarize_in_safepoints; }\n+  void          set_scalarize_in_safepoints(bool z) { _scalarize_in_safepoints = z; }\n+\n+  \/\/ Support for scalarized inline type calling convention\n+  bool              has_scalarized_args() const  { return _method != NULL && _method->has_scalarized_args(); }\n+  bool              needs_stack_repair()  const  { return _method != NULL && _method->get_Method()->c2_needs_stack_repair(); }\n@@ -711,0 +726,7 @@\n+  \/\/ Keep track of inline type nodes for later processing\n+  void add_inline_type(Node* n);\n+  void remove_inline_type(Node* n);\n+  void process_inline_types(PhaseIterGVN &igvn, bool remove = false);\n+\n+  void adjust_flattened_array_access_aliases(PhaseIterGVN& igvn);\n+\n@@ -850,1 +872,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = NULL) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = NULL, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }\n@@ -854,1 +876,1 @@\n-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }\n+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, NULL, uncached)->index(); }\n@@ -1090,1 +1112,1 @@\n-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);\n+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);\n@@ -1162,1 +1184,3 @@\n-  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  Node* optimize_acmp(PhaseGVN* phase, Node* a, Node* b);\n+\n+  \/\/ Auxiliary method for randomized fuzzing\/stressing\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":28,"deletions":4,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -145,0 +146,10 @@\n+    if ((n->Opcode() == Op_LoadX || n->Opcode() == Op_StoreX) &&\n+        !n->in(MemNode::Address)->is_AddP() &&\n+        _igvn->type(n->in(MemNode::Address))->isa_oopptr()) {\n+      \/\/ Load\/Store at mark work address is at offset 0 so has no AddP which confuses EA\n+      Node* addp = new AddPNode(n->in(MemNode::Address), n->in(MemNode::Address), _igvn->MakeConX(0));\n+      _igvn->register_new_node_with_optimizer(addp);\n+      _igvn->replace_input_of(n, MemNode::Address, addp);\n+      ideal_nodes.push(addp);\n+      _nodes.at_put_grow(addp->_idx, NULL, NULL);\n+    }\n@@ -390,1 +401,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_sig();\n@@ -463,0 +474,11 @@\n+      } else if (n->as_Call()->tf()->returns_inline_type_as_fields()) {\n+        bool returns_oop = false;\n+        for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax && !returns_oop; i++) {\n+          ProjNode* pn = n->fast_out(i)->as_Proj();\n+          if (pn->_con >= TypeFunc::Parms && pn->bottom_type()->isa_ptr()) {\n+            returns_oop = true;\n+          }\n+        }\n+        if (returns_oop) {\n+          add_call_node(n->as_Call());\n+        }\n@@ -494,0 +516,1 @@\n+    case Op_InlineTypePtr:\n@@ -566,2 +589,4 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_Call() &&\n+          (n->in(0)->as_Call()->returns_pointer() || n->bottom_type()->isa_ptr())) {\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+               n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -663,0 +688,1 @@\n+    case Op_InlineTypePtr:\n@@ -722,2 +748,4 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_Call() &&\n+          (n->in(0)->as_Call()->returns_pointer()|| n->bottom_type()->isa_ptr())) {\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+               n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -883,1 +911,1 @@\n-  assert(call->returns_pointer(), \"only for call which returns pointer\");\n+  assert(call->returns_pointer() || call->tf()->returns_inline_type_as_fields(), \"only for call which returns pointer\");\n@@ -976,1 +1004,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -1023,1 +1051,1 @@\n-      const TypeTuple * d = call->tf()->domain();\n+      const TypeTuple * d = call->tf()->domain_sig();\n@@ -1053,1 +1081,4 @@\n-                               (aat->isa_aryptr() && aat->isa_aryptr()->klass()->is_obj_array_klass()));\n+                               (aat->isa_aryptr() && aat->isa_aryptr()->klass()->is_obj_array_klass()) ||\n+                               (aat->isa_aryptr() && aat->isa_aryptr()->elem() != NULL &&\n+                                aat->isa_aryptr()->is_flat() &&\n+                                aat->isa_aryptr()->elem()->inline_klass()->contains_oops()));\n@@ -1099,0 +1130,3 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"vectorizedMismatch\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"load_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_unknown_inline\") == 0 ||\n@@ -1160,1 +1194,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -1204,1 +1238,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_cc();\n@@ -1615,3 +1649,12 @@\n-    if (alloc->is_Allocate() && !pta->arraycopy_dst())\n-      return 0;\n-    assert(pta->arraycopy_dst() || alloc->as_CallStaticJava(), \"sanity\");\n+    if (alloc->is_Allocate() && !pta->arraycopy_dst()) {\n+      if (alloc->as_Allocate()->in(AllocateNode::DefaultValue) != NULL) {\n+        \/\/ Non-flattened inline type arrays are initialized with\n+        \/\/ the default value instead of null. Handle them here.\n+        init_val = ptnode_adr(alloc->as_Allocate()->in(AllocateNode::DefaultValue)->_idx);\n+        assert(init_val != NULL, \"default value should be registered\");\n+      } else {\n+        return 0;\n+      }\n+    }\n+    \/\/ Non-escaped allocation returned from Java or runtime call has unknown values in fields.\n+    assert(pta->arraycopy_dst() || alloc->is_CallStaticJava() || init_val != phantom_obj, \"sanity\");\n@@ -1619,1 +1662,1 @@\n-    if (!pta->arraycopy_dst() && alloc->as_CallStaticJava()->method() == NULL) {\n+    if (alloc->is_CallStaticJava() && alloc->as_CallStaticJava()->method() == NULL) {\n@@ -1624,2 +1667,0 @@\n-    \/\/ Non-escaped allocation returned from Java or runtime call have\n-    \/\/ unknown values in fields.\n@@ -1629,1 +1670,1 @@\n-        if (add_edge(field, phantom_obj)) {\n+        if (add_edge(field, init_val)) {\n@@ -1640,1 +1681,1 @@\n-  if (!alloc->is_Allocate())\n+  if (!alloc->is_Allocate() || alloc->as_Allocate()->in(AllocateNode::DefaultValue) != NULL) {\n@@ -1642,0 +1683,1 @@\n+  }\n@@ -1724,1 +1766,1 @@\n-                tty->print_cr(\"----------missed referernce to object-----------\");\n+                tty->print_cr(\"----------missed reference to object------------\");\n@@ -1726,1 +1768,1 @@\n-                tty->print_cr(\"----------object referernced by init store -----\");\n+                tty->print_cr(\"----------object referenced by init store-------\");\n@@ -1796,1 +1838,1 @@\n-         ptn->set_scalar_replaceable(false);\n+        ptn->set_scalar_replaceable(false);\n@@ -1959,1 +2001,3 @@\n-          if (not_global_escape(alock->obj_node())) {\n+          const Type* obj_type = igvn->type(alock->obj_node());\n+          if (not_global_escape(alock->obj_node()) &&\n+              !obj_type->isa_inlinetype() && !obj_type->is_inlinetypeptr()) {\n@@ -2161,0 +2205,1 @@\n+  int field_offset = adr_type->isa_aryptr() ? adr_type->isa_aryptr()->field_offset().get() : Type::OffsetBot;\n@@ -2162,1 +2207,1 @@\n-  if (offset == Type::OffsetBot) {\n+  if (offset == Type::OffsetBot && field_offset == Type::OffsetBot) {\n@@ -2174,1 +2219,1 @@\n-      ciField* field = _compile->alias_type(adr_type->isa_instptr())->field();\n+      ciField* field = _compile->alias_type(adr_type->is_ptr())->field();\n@@ -2194,1 +2239,7 @@\n-        bt = elemtype->array_element_basic_type();\n+        if (elemtype->isa_inlinetype() && field_offset != Type::OffsetBot) {\n+          ciInlineKlass* vk = elemtype->inline_klass();\n+          field_offset += vk->first_field_offset();\n+          bt = vk->get_field_by_offset(field_offset, false)->layout_type();\n+        } else {\n+          bt = elemtype->array_element_basic_type();\n+        }\n@@ -2212,1 +2263,1 @@\n-  assert(!_collecting, \"should not call when contructed graph\");\n+  assert(!_collecting, \"should not call when constructed graph\");\n@@ -2373,3 +2424,1 @@\n-  const TypePtr *t_ptr = adr_type->isa_ptr();\n-  assert(t_ptr != NULL, \"must be a pointer type\");\n-  return t_ptr->offset();\n+  return adr_type->is_ptr()->flattened_offset();\n@@ -2529,1 +2578,8 @@\n-    t = base_t->add_offset(offs)->is_oopptr();\n+    if (base_t->isa_aryptr() != NULL) {\n+      \/\/ In the case of a flattened inline type array, each field has its\n+      \/\/ own slice so we need to extract the field being accessed from\n+      \/\/ the address computation\n+      t = base_t->isa_aryptr()->add_field_offset_and_offset(offs)->is_oopptr();\n+    } else {\n+      t = base_t->add_offset(offs)->is_oopptr();\n+    }\n@@ -2531,1 +2587,1 @@\n-  int inst_id =  base_t->instance_id();\n+  int inst_id = base_t->instance_id();\n@@ -2545,1 +2601,1 @@\n-  \/\/ It could happened when CHA type is different from MDO type on a dead path\n+  \/\/ It could happen when CHA type is different from MDO type on a dead path\n@@ -2555,1 +2611,12 @@\n-  const TypeOopPtr *tinst = base_t->add_offset(t->offset())->is_oopptr();\n+  const TypePtr* tinst = base_t->add_offset(t->offset());\n+  if (tinst->isa_aryptr() && t->isa_aryptr()) {\n+    \/\/ In the case of a flattened inline type array, each field has its\n+    \/\/ own slice so we need to keep track of the field being accessed.\n+    tinst = tinst->is_aryptr()->with_field_offset(t->is_aryptr()->field_offset().get());\n+    \/\/ Keep array properties (not flat\/null-free)\n+    tinst = tinst->is_aryptr()->update_properties(t->is_aryptr());\n+    if (tinst == NULL) {\n+      return false; \/\/ Skip dead path with inconsistent properties\n+    }\n+  }\n+\n@@ -3228,0 +3295,7 @@\n+          if (tn_t->isa_aryptr()) {\n+            \/\/ Keep array properties (not flat\/null-free)\n+            tinst = tinst->is_aryptr()->update_properties(tn_t->is_aryptr());\n+            if (tinst == NULL) {\n+              continue; \/\/ Skip dead path with inconsistent properties\n+            }\n+          }\n@@ -3253,1 +3327,1 @@\n-      if(use->is_Mem() && use->in(MemNode::Address) == n) {\n+      if (use->is_Mem() && use->in(MemNode::Address) == n) {\n@@ -3289,0 +3363,8 @@\n+      } else if (use->Opcode() == Op_Return) {\n+        assert(_compile->tf()->returns_inline_type_as_fields(), \"must return an inline type\");\n+        \/\/ Get InlineKlass by removing the tag bit from the metadata pointer\n+        Node* klass = use->in(TypeFunc::Parms);\n+        intptr_t ptr = igvn->type(klass)->isa_rawptr()->get_con();\n+        clear_nth_bit(ptr, 0);\n+        assert(Metaspace::contains((void*)ptr), \"should be klass\");\n+        assert(((InlineKlass*)ptr)->contains_oops(), \"returned inline type must contain a reference field\");\n@@ -3300,1 +3382,1 @@\n-              op == Op_SubTypeCheck ||\n+              op == Op_SubTypeCheck || op == Op_InlineType || op == Op_InlineTypePtr || op == Op_FlatArrayCheck ||\n@@ -3368,0 +3450,3 @@\n+    } else if (n->is_CallLeaf() && n->as_CallLeaf()->_name != NULL &&\n+               strcmp(n->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+      n = n->as_CallLeaf()->proj_out(TypeFunc::Memory);\n@@ -3408,1 +3493,1 @@\n-      } else if(use->is_Mem()) {\n+      } else if (use->is_Mem()) {\n@@ -3417,0 +3502,4 @@\n+      } else if (use->is_CallLeaf() && use->as_CallLeaf()->_name != NULL &&\n+                 strcmp(use->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+        \/\/ store_unknown_inline overwrites destination array\n+        memnode_worklist.append_if_missing(use);\n@@ -3426,1 +3515,1 @@\n-              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {\n+              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar || op == Op_FlatArrayCheck)) {\n@@ -3438,1 +3527,1 @@\n-  \/\/            instance type to the the input corresponding to its alias index.\n+  \/\/            instance type to the input corresponding to its alias index.\n@@ -3510,2 +3599,2 @@\n-  \/\/ to recursively process Phi's encounted on the input memory\n-  \/\/ chains as is done in split_memory_phi() since they  will\n+  \/\/ to recursively process Phi's encountered on the input memory\n+  \/\/ chains as is done in split_memory_phi() since they will\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":130,"deletions":41,"binary":false,"changes":171,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -41,0 +43,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -44,0 +47,1 @@\n+#include \"opto\/narrowptrnode.hpp\"\n@@ -57,1 +61,1 @@\n-GraphKit::GraphKit(JVMState* jvms)\n+GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)\n@@ -60,1 +64,1 @@\n-    _gvn(*C->initial_gvn()),\n+    _gvn((gvn != NULL) ? *gvn : *C->initial_gvn()),\n@@ -63,0 +67,1 @@\n+  assert(gvn == NULL || !gvn->is_IterGVN() || gvn->is_IterGVN()->delay_transform(), \"delay transform should be enabled\");\n@@ -66,0 +71,7 @@\n+#ifdef ASSERT\n+  if (_gvn.is_IterGVN() != NULL) {\n+    assert(_gvn.is_IterGVN()->delay_transform(), \"Transformation must be delayed if IterGVN is used\");\n+    \/\/ Save the initial size of _for_igvn worklist for verification (see ~GraphKit)\n+    _worklist_size = _gvn.C->for_igvn()->size();\n+  }\n+#endif\n@@ -833,1 +845,1 @@\n-           (is_anewarray && code == Bytecodes::_multianewarray);\n+           (is_anewarray && (code == Bytecodes::_multianewarray));\n@@ -1093,0 +1105,9 @@\n+  case Bytecodes::_withfield: {\n+    bool ignored_will_link;\n+    ciField* field = method()->get_field_at_bci(bci(), ignored_will_link);\n+    int      size  = field->type()->size();\n+    inputs = size+1;\n+    depth = rsize - inputs;\n+    break;\n+  }\n+\n@@ -1175,1 +1196,1 @@\n-  return _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+  return _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeKlassPtr::OBJECT));\n@@ -1218,0 +1239,1 @@\n+    case T_INLINE_TYPE : \/\/ fall through\n@@ -1389,0 +1411,16 @@\n+Node* GraphKit::null2default(Node* value, ciInlineKlass* vk) {\n+  assert(!vk->is_scalarizable(), \"Should only be used for non scalarizable inline klasses\");\n+  Node* null_ctl = top();\n+  value = null_check_oop(value, &null_ctl);\n+  if (!null_ctl->is_top()) {\n+    \/\/ Return default value if oop is null\n+    Node* region = new RegionNode(3);\n+    region->init_req(1, control());\n+    region->init_req(2, null_ctl);\n+    value = PhiNode::make(region, value, TypeInstPtr::make(TypePtr::BotPTR, vk));\n+    value->set_req(2, InlineTypeNode::default_oop(gvn(), vk));\n+    set_control(gvn().transform(region));\n+    value = gvn().transform(value);\n+  }\n+  return value;\n+}\n@@ -1393,0 +1431,3 @@\n+  if (obj->is_InlineType()) {\n+    return obj;\n+  }\n@@ -1402,0 +1443,5 @@\n+  if (t->is_inlinetypeptr() && t->inline_klass()->is_scalarizable()) {\n+    \/\/ Scalarize inline type now that we know it's non-null\n+    cast = InlineTypeNode::make_from_oop(this, cast, t->inline_klass())->as_ptr(&gvn());\n+  }\n+\n@@ -1526,1 +1572,2 @@\n-  if (((bt == T_OBJECT) && C->do_escape_analysis()) || C->eliminate_boxing()) {\n+\n+  if (((bt == T_OBJECT || bt == T_INLINE_TYPE) && C->do_escape_analysis()) || C->eliminate_boxing()) {\n@@ -1577,1 +1624,2 @@\n-                                DecoratorSet decorators) {\n+                                DecoratorSet decorators,\n+                                bool safe_for_replace) {\n@@ -1590,0 +1638,7 @@\n+  if (val->is_InlineType()) {\n+    \/\/ Store to non-flattened field. Buffer the inline type and make sure\n+    \/\/ the store is re-executed if the allocation triggers deoptimization.\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    val = val->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n@@ -1606,1 +1661,2 @@\n-                               DecoratorSet decorators) {\n+                               DecoratorSet decorators,\n+                               Node* ctl) {\n@@ -1612,1 +1668,1 @@\n-  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr, ctl);\n@@ -1710,2 +1766,2 @@\n-void GraphKit::access_clone(Node* src, Node* dst, Node* size, bool is_array) {\n-  return _barrier_set->clone(this, src, dst, size, is_array);\n+void GraphKit::access_clone(Node* src_base, Node* dst_base, Node* countx, bool is_array) {\n+  return _barrier_set->clone(this, src_base, dst_base, countx, is_array);\n@@ -1718,0 +1774,5 @@\n+  ciKlass* arytype_klass = _gvn.type(ary)->is_aryptr()->klass();\n+  if (arytype_klass != NULL && arytype_klass->is_flat_array_klass()) {\n+    ciFlatArrayKlass* vak = arytype_klass->as_flat_array_klass();\n+    shift = vak->log2_element_size();\n+  }\n@@ -1738,0 +1799,1 @@\n+  assert(elembt != T_INLINE_TYPE, \"inline types are not supported by this method\");\n@@ -1748,6 +1810,32 @@\n-void GraphKit::set_arguments_for_java_call(CallJavaNode* call) {\n-  \/\/ Add the call arguments:\n-  uint nargs = call->method()->arg_size();\n-  for (uint i = 0; i < nargs; i++) {\n-    Node* arg = argument(i);\n-    call->init_req(i + TypeFunc::Parms, arg);\n+void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {\n+  PreserveReexecuteState preexecs(this);\n+  if (EnableValhalla) {\n+    \/\/ Make sure the call is \"re-executed\", if buffering of inline type arguments triggers deoptimization.\n+    \/\/ At this point, the call hasn't been executed yet, so we will only ever execute the call once.\n+    jvms()->set_should_reexecute(true);\n+    int arg_size = method()->get_declared_signature_at_bci(bci())->arg_size_for_bc(java_bc());\n+    inc_sp(arg_size);\n+  }\n+  \/\/ Add the call arguments\n+  const TypeTuple* domain = call->tf()->domain_sig();\n+  uint nargs = domain->cnt();\n+  for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i < nargs; i++) {\n+    Node* arg = argument(i-TypeFunc::Parms);\n+    const Type* t = domain->field_at(i);\n+    if (call->method()->has_scalarized_args() && t->is_inlinetypeptr() && !t->maybe_null() && t->inline_klass()->can_be_passed_as_fields()) {\n+      \/\/ We don't pass inline type arguments by reference but instead pass each field of the inline type\n+      InlineTypeNode* vt = arg->as_InlineType();\n+      vt->pass_fields(this, call, idx);\n+      \/\/ If an inline type argument is passed as fields, attach the Method* to the call site\n+      \/\/ to be able to access the extended signature later via attached_method_before_pc().\n+      \/\/ For example, see CompiledMethod::preserve_callee_argument_oops().\n+      call->set_override_symbolic_info(true);\n+      continue;\n+    } else if (arg->is_InlineType()) {\n+      \/\/ Pass inline type argument via oop to callee\n+      arg = arg->as_InlineType()->buffer(this);\n+      if (!is_late_inline) {\n+        arg = arg->as_InlineTypePtr()->get_oop();\n+      }\n+    }\n+    call->init_req(idx++, arg);\n@@ -1791,7 +1879,0 @@\n-  \/\/ Capture the return value, if any.\n-  Node* ret;\n-  if (call->method() == NULL ||\n-      call->method()->return_type()->basic_type() == T_VOID)\n-        ret = top();\n-  else  ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n-\n@@ -1810,0 +1891,15 @@\n+\n+  \/\/ Capture the return value, if any.\n+  Node* ret;\n+  if (call->method() == NULL || call->method()->return_type()->basic_type() == T_VOID) {\n+    ret = top();\n+  } else if (call->tf()->returns_inline_type_as_fields()) {\n+    \/\/ Return of multiple values (inline type fields): we create a\n+    \/\/ InlineType node, each field is a projection from the call.\n+    ciInlineKlass* vk = call->method()->return_type()->as_inline_klass();\n+    uint base_input = TypeFunc::Parms + 1;\n+    ret = InlineTypeNode::make_from_multi(this, call, vk, base_input, false);\n+  } else {\n+    ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+  }\n+\n@@ -1900,2 +1996,1 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true);\n+  CallProjections* callprojs = call->extract_projections(true);\n@@ -1910,2 +2005,2 @@\n-  if (callprojs.fallthrough_catchproj != NULL) {\n-    C->gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);\n+  if (callprojs->fallthrough_catchproj != NULL) {\n+    C->gvn_replace_by(callprojs->fallthrough_catchproj, final_ctl);\n@@ -1913,1 +2008,1 @@\n-  if (callprojs.fallthrough_memproj != NULL) {\n+  if (callprojs->fallthrough_memproj != NULL) {\n@@ -1918,1 +2013,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);\n+    C->gvn_replace_by(callprojs->fallthrough_memproj,   final_mem);\n@@ -1921,2 +2016,2 @@\n-  if (callprojs.fallthrough_ioproj != NULL) {\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);\n+  if (callprojs->fallthrough_ioproj != NULL) {\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj,    final_io);\n@@ -1926,2 +2021,3 @@\n-  if (callprojs.resproj != NULL && result != NULL) {\n-    C->gvn_replace_by(callprojs.resproj, result);\n+  if (callprojs->resproj[0] != NULL && result != NULL) {\n+    assert(callprojs->nb_resproj == 1, \"unexpected number of results\");\n+    C->gvn_replace_by(callprojs->resproj[0], result);\n@@ -1932,2 +2028,2 @@\n-    if (callprojs.catchall_catchproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, C->top());\n+    if (callprojs->catchall_catchproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, C->top());\n@@ -1935,2 +2031,2 @@\n-    if (callprojs.catchall_memproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_memproj,   C->top());\n+    if (callprojs->catchall_memproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_memproj,   C->top());\n@@ -1938,2 +2034,2 @@\n-    if (callprojs.catchall_ioproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    C->top());\n+    if (callprojs->catchall_ioproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    C->top());\n@@ -1942,2 +2038,2 @@\n-    if (callprojs.exobj != NULL) {\n-      C->gvn_replace_by(callprojs.exobj, C->top());\n+    if (callprojs->exobj != NULL) {\n+      C->gvn_replace_by(callprojs->exobj, C->top());\n@@ -1954,2 +2050,2 @@\n-    if (callprojs.catchall_catchproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, ekit.control());\n+    if (callprojs->catchall_catchproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, ekit.control());\n@@ -1958,1 +2054,1 @@\n-    if (callprojs.catchall_memproj != NULL) {\n+    if (callprojs->catchall_memproj != NULL) {\n@@ -1960,1 +2056,1 @@\n-      C->gvn_replace_by(callprojs.catchall_memproj,   ex_mem);\n+      C->gvn_replace_by(callprojs->catchall_memproj,   ex_mem);\n@@ -1963,2 +2059,2 @@\n-    if (callprojs.catchall_ioproj != NULL) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());\n+    if (callprojs->catchall_ioproj != NULL) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    ekit.i_o());\n@@ -1968,2 +2064,2 @@\n-    if (callprojs.exobj != NULL) {\n-      C->gvn_replace_by(callprojs.exobj, ex_oop);\n+    if (callprojs->exobj != NULL) {\n+      C->gvn_replace_by(callprojs->exobj, ex_oop);\n@@ -1983,1 +2079,1 @@\n-  if (callprojs.fallthrough_catchproj != NULL && !final_ctl->is_top() && do_replaced_nodes) {\n+  if (callprojs->fallthrough_catchproj != NULL && !final_ctl->is_top() && do_replaced_nodes) {\n@@ -2181,1 +2277,1 @@\n-    const TypePtr* ptr = (ptr_kind == ProfileMaybeNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n+    const TypePtr* ptr = (ptr_kind != ProfileNeverNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n@@ -2204,1 +2300,1 @@\n-    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::OffsetBot, TypeOopPtr::InstanceBot, speculative);\n+    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, speculative);\n@@ -2238,2 +2334,9 @@\n-      if (!data->as_BitData()->null_seen()) {\n-        ptr_kind = ProfileNeverNull;\n+      if (java_bc() == Bytecodes::_aastore) {\n+        ciKlass* array_type = NULL;\n+        ciKlass* element_type = NULL;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+        exact_kls = element_type;\n+        ptr_kind = element_ptr;\n@@ -2241,7 +2344,11 @@\n-        assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n-        ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n-        uint i = 0;\n-        for (; i < call->row_limit(); i++) {\n-          ciKlass* receiver = call->receiver(i);\n-          if (receiver != NULL) {\n-            break;\n+        if (!data->as_BitData()->null_seen()) {\n+          ptr_kind = ProfileNeverNull;\n+        } else {\n+          assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n+          ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n+          uint i = 0;\n+          for (; i < call->row_limit(); i++) {\n+            ciKlass* receiver = call->receiver(i);\n+            if (receiver != NULL) {\n+              break;\n+            }\n@@ -2249,0 +2356,1 @@\n+          ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2250,1 +2358,0 @@\n-        ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2269,1 +2376,1 @@\n-  int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+  int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2272,1 +2379,1 @@\n-    const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+    const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2343,1 +2450,1 @@\n-    int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+    int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2345,1 +2452,1 @@\n-      const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+      const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2599,1 +2706,1 @@\n-      const Type* type = call_type->domain()->field_at(TypeFunc::Parms + vm_unfiltered_arg_pos);\n+      const Type* type = call_type->domain_sig()->field_at(TypeFunc::Parms + vm_unfiltered_arg_pos);\n@@ -2610,1 +2717,1 @@\n-  uint n_returns = call_type->range()->cnt() - TypeFunc::Parms;\n+  uint n_returns = call_type->range_sig()->cnt() - TypeFunc::Parms;\n@@ -2618,1 +2725,1 @@\n-      const Type* type = call_type->range()->field_at(TypeFunc::Parms + vm_ret_pos);\n+      const Type* type = call_type->range_sig()->field_at(TypeFunc::Parms + vm_ret_pos);\n@@ -2946,0 +3053,4 @@\n+  const Type* sub_t = _gvn.type(obj_or_subklass);\n+  if (sub_t->isa_inlinetype()) {\n+    obj_or_subklass = makecon(TypeKlassPtr::make(sub_t->inline_klass()));\n+  }\n@@ -2950,1 +3061,1 @@\n-    if (!_gvn.type(obj_or_subklass)->isa_klassptr()) {\n+    if (!sub_t->isa_klassptr() && !sub_t->isa_inlinetype()) {\n@@ -2953,1 +3064,0 @@\n-\n@@ -2959,1 +3069,0 @@\n-  const TypePtr* adr_type = TypeKlassPtr::make(TypePtr::NotNull, C->env()->Object_klass(), Type::OffsetBot);\n@@ -2969,2 +3078,13 @@\n-                                    float prob,\n-                                    Node* *casted_receiver) {\n+                                    float prob, Node* *casted_receiver) {\n+  Node* fail = top();\n+  const Type* rec_t = _gvn.type(receiver);\n+  if (rec_t->isa_inlinetype()) {\n+    if (klass->equals(rec_t->inline_klass())) {\n+      (*casted_receiver) = receiver; \/\/ Always passes\n+    } else {\n+      (*casted_receiver) = top();    \/\/ Always fails\n+      fail = control();\n+      set_control(top());\n+    }\n+    return fail;\n+  }\n@@ -2973,7 +3093,1 @@\n-  Node* want_klass = makecon(tklass);\n-  Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass) );\n-  Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );\n-  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n-  set_control( _gvn.transform( new IfTrueNode (iff) ));\n-  Node* fail = _gvn.transform( new IfFalseNode(iff) );\n-\n+  fail = type_check(recv_klass, tklass, prob);\n@@ -2986,1 +3100,7 @@\n-  (*casted_receiver) = _gvn.transform(cast);\n+  Node* res = _gvn.transform(cast);\n+  if (recv_xtype->is_inlinetypeptr() && recv_xtype->inline_klass()->is_scalarizable()) {\n+    assert(!gvn().type(res)->maybe_null(), \"receiver should never be null\");\n+    res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass())->as_ptr(&gvn());\n+  }\n+\n+  (*casted_receiver) = res;\n@@ -2992,0 +3112,11 @@\n+Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,\n+                           float prob) {\n+  Node* want_klass = makecon(tklass);\n+  Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass));\n+  Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );\n+  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n+  set_control(  _gvn.transform( new IfTrueNode (iff)));\n+  Node* fail = _gvn.transform( new IfFalseNode(iff));\n+  return fail;\n+}\n+\n@@ -3031,0 +3162,3 @@\n+    if (java_bc() == Bytecodes::_aastore) {\n+      return ((ciArrayLoadStoreData*)data->as_ArrayLoadStoreData())->element()->ptr_kind() == ProfileNeverNull;\n+    }\n@@ -3110,1 +3244,14 @@\n-  ciKlass* exact_kls = spec_klass == NULL ? profile_has_unique_klass() : spec_klass;\n+  ciKlass* exact_kls = spec_klass;\n+  if (exact_kls == NULL) {\n+    if (java_bc() == Bytecodes::_aastore) {\n+      ciKlass* array_type = NULL;\n+      ciKlass* element_type = NULL;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      exact_kls = element_type;\n+    } else {\n+      exact_kls = profile_has_unique_klass();\n+    }\n+  }\n@@ -3215,0 +3362,1 @@\n+  bool is_value = obj->is_InlineType();\n@@ -3218,1 +3366,1 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = is_value ? obj : null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n@@ -3236,7 +3384,9 @@\n-  bool known_statically = false;\n-  if (_gvn.type(superklass)->singleton()) {\n-    ciKlass* superk = _gvn.type(superklass)->is_klassptr()->klass();\n-    ciKlass* subk = _gvn.type(obj)->is_oopptr()->klass();\n-    if (subk != NULL && subk->is_loaded()) {\n-      int static_res = C->static_subtype_check(superk, subk);\n-      known_statically = (static_res == Compile::SSC_always_true || static_res == Compile::SSC_always_false);\n+  if (!is_value) {\n+    bool known_statically = false;\n+    if (_gvn.type(superklass)->singleton()) {\n+      ciKlass* superk = _gvn.type(superklass)->is_klassptr()->klass();\n+      ciKlass* subk = _gvn.type(obj)->is_oopptr()->klass();\n+      if (subk != NULL && subk->is_loaded()) {\n+        int static_res = C->static_subtype_check(superk, subk);\n+        known_statically = (static_res == Compile::SSC_always_true || static_res == Compile::SSC_always_false);\n+      }\n@@ -3244,14 +3394,17 @@\n-  }\n-  if (!known_statically) {\n-    const TypeOopPtr* obj_type = _gvn.type(obj)->is_oopptr();\n-    \/\/ We may not have profiling here or it may not help us. If we\n-    \/\/ have a speculative type use it to perform an exact cast.\n-    ciKlass* spec_obj_type = obj_type->speculative_type();\n-    if (spec_obj_type != NULL || (ProfileDynamicTypes && data != NULL)) {\n-      Node* cast_obj = maybe_cast_profiled_receiver(not_null_obj, NULL, spec_obj_type, safe_for_replace);\n-      if (stopped()) {            \/\/ Profile disagrees with this path.\n-        set_control(null_ctl);    \/\/ Null is the only remaining possibility.\n-        return intcon(0);\n-      }\n-      if (cast_obj != NULL) {\n-        not_null_obj = cast_obj;\n+    if (!known_statically) {\n+      const TypeOopPtr* obj_type = _gvn.type(obj)->is_oopptr();\n+      \/\/ We may not have profiling here or it may not help us. If we\n+      \/\/ have a speculative type use it to perform an exact cast.\n+      ciKlass* spec_obj_type = obj_type->speculative_type();\n+      if (spec_obj_type != NULL || (ProfileDynamicTypes && data != NULL)) {\n+        Node* cast_obj = maybe_cast_profiled_receiver(not_null_obj, NULL, spec_obj_type, safe_for_replace);\n+        if (stopped()) {            \/\/ Profile disagrees with this path.\n+          set_control(null_ctl);    \/\/ Null is the only remaining possibility.\n+          return intcon(0);\n+        }\n+        if (cast_obj != NULL &&\n+            \/\/ A value that's sometimes null is not something we can optimize well\n+            !(cast_obj->is_InlineType() && null_ctl != top())) {\n+          not_null_obj = cast_obj;\n+          is_value = not_null_obj->is_InlineType();\n+        }\n@@ -3281,1 +3434,1 @@\n-  if (safe_for_replace) {\n+  if (safe_for_replace && !is_value) {\n@@ -3296,2 +3449,1 @@\n-Node* GraphKit::gen_checkcast(Node *obj, Node* superklass,\n-                              Node* *failure_control) {\n+Node* GraphKit::gen_checkcast(Node *obj, Node* superklass, Node* *failure_control) {\n@@ -3299,2 +3451,6 @@\n-  const TypeKlassPtr *tk = _gvn.type(superklass)->is_klassptr();\n-  const Type *toop = TypeOopPtr::make_from_klass(tk->klass());\n+  const TypeKlassPtr* tk = _gvn.type(superklass)->is_klassptr();\n+  const TypeOopPtr* toop = TypeOopPtr::make_from_klass(tk->klass());\n+\n+  \/\/ Check if inline types are involved\n+  bool from_inline = obj->is_InlineType();\n+  bool to_inline = tk->klass()->is_inlinetype();\n@@ -3309,3 +3465,11 @@\n-    const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n-    if (objtp != NULL && objtp->klass() != NULL) {\n-      switch (C->static_subtype_check(tk->klass(), objtp->klass())) {\n+    ciKlass* klass = NULL;\n+    if (from_inline) {\n+      klass = _gvn.type(obj)->inline_klass();\n+    } else {\n+      const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n+      if (objtp != NULL) {\n+        klass = objtp->klass();\n+      }\n+    }\n+    if (klass != NULL) {\n+      switch (C->static_subtype_check(tk->klass(), klass)) {\n@@ -3316,1 +3480,10 @@\n-        return record_profiled_receiver_for_speculation(obj);\n+        if (!from_inline) {\n+          obj = record_profiled_receiver_for_speculation(obj);\n+          if (to_inline) {\n+            obj = null_check(obj);\n+            if (toop->inline_klass()->is_scalarizable()) {\n+              obj = InlineTypeNode::make_from_oop(this, obj, toop->inline_klass());\n+            }\n+          }\n+        }\n+        return obj;\n@@ -3318,4 +3491,6 @@\n-        \/\/ It needs a null check because a null will *pass* the cast check.\n-        \/\/ A non-null value will always produce an exception.\n-        if (!objtp->maybe_null()) {\n-          builtin_throw(Deoptimization::Reason_class_check, makecon(TypeKlassPtr::make(objtp->klass())));\n+        if (from_inline || to_inline) {\n+          if (!from_inline) {\n+            null_check(obj);\n+          }\n+          \/\/ Inline type is never null. Always throw an exception.\n+          builtin_throw(Deoptimization::Reason_class_check, makecon(TypeKlassPtr::make(klass)));\n@@ -3323,2 +3498,10 @@\n-        } else if (!too_many_traps_or_recompiles(Deoptimization::Reason_null_assert)) {\n-          return null_assert(obj);\n+        } else {\n+          \/\/ It needs a null check because a null will *pass* the cast check.\n+          const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n+          if (!objtp->maybe_null()) {\n+            builtin_throw(Deoptimization::Reason_class_check, makecon(TypeKlassPtr::make(objtp->klass())));\n+            return top();\n+          } else if (!too_many_traps_or_recompiles(Deoptimization::Reason_null_assert)) {\n+            return null_assert(obj);\n+          }\n+          break; \/\/ Fall through to full check\n@@ -3326,1 +3509,0 @@\n-        break; \/\/ Fall through to full check\n@@ -3337,1 +3519,3 @@\n-    data = method()->method_data()->bci_to_data(bci());\n+    if (method()->method_data()->is_mature()) {\n+      data = method()->method_data()->bci_to_data(bci());\n+    }\n@@ -3345,0 +3529,3 @@\n+  _gvn.set_type(region, Type::CONTROL);\n+  _gvn.set_type(phi, toop);\n+\n@@ -3354,1 +3541,8 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = NULL;\n+  if (from_inline) {\n+    not_null_obj = obj;\n+  } else if (to_inline) {\n+    not_null_obj = null_check(obj);\n+  } else {\n+    not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  }\n@@ -3372,1 +3566,1 @@\n-  if (tk->klass_is_exact()) {\n+  if (!from_inline && tk->klass_is_exact()) {\n@@ -3383,0 +3577,7 @@\n+      if (cast_obj != NULL && cast_obj->is_InlineType()) {\n+        if (null_ctl != top()) {\n+          cast_obj = NULL; \/\/ A value that's sometimes null is not something we can optimize well\n+        } else {\n+          return cast_obj;\n+        }\n+      }\n@@ -3394,1 +3595,1 @@\n-    Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass );\n+    Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass);\n@@ -3397,1 +3598,1 @@\n-    cast_obj = _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));\n+    cast_obj = from_inline ? not_null_obj : _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));\n@@ -3403,1 +3604,7 @@\n-        builtin_throw(Deoptimization::Reason_class_check, load_object_klass(not_null_obj));\n+        Node* obj_klass = NULL;\n+        if (from_inline) {\n+          obj_klass = makecon(TypeKlassPtr::make(_gvn.type(not_null_obj)->inline_klass()));\n+        } else {\n+          obj_klass = load_object_klass(not_null_obj);\n+        }\n+        builtin_throw(Deoptimization::Reason_class_check, obj_klass);\n@@ -3430,1 +3637,114 @@\n-  return record_profiled_receiver_for_speculation(res);\n+  bool not_inline = !toop->can_be_inline_type();\n+  bool not_flattened = !UseFlatArray || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->flatten_array());\n+  if (EnableValhalla && not_flattened) {\n+    \/\/ Check if obj has been loaded from an array\n+    obj = obj->isa_DecodeN() ? obj->in(1) : obj;\n+    Node* array = NULL;\n+    if (obj->isa_Load()) {\n+      Node* address = obj->in(MemNode::Address);\n+      if (address->isa_AddP()) {\n+        array = address->as_AddP()->in(AddPNode::Base);\n+      }\n+    } else if (obj->is_Phi()) {\n+      Node* region = obj->in(0);\n+      \/\/ TODO make this more robust (see JDK-8231346)\n+      if (region->req() == 3 && region->in(2) != NULL && region->in(2)->in(0) != NULL) {\n+        IfNode* iff = region->in(2)->in(0)->isa_If();\n+        if (iff != NULL) {\n+          iff->is_flat_array_check(&_gvn, &array);\n+        }\n+      }\n+    }\n+    if (array != NULL) {\n+      const TypeAryPtr* ary_t = _gvn.type(array)->isa_aryptr();\n+      if (ary_t != NULL) {\n+        if (!ary_t->is_not_null_free() && not_inline) {\n+          \/\/ Casting array element to a non-inline-type, mark array as not null-free.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_null_free()));\n+          replace_in_map(array, cast);\n+        } else if (!ary_t->is_not_flat()) {\n+          \/\/ Casting array element to a non-flattened type, mark array as not flat.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_flat()));\n+          replace_in_map(array, cast);\n+        }\n+      }\n+    }\n+  }\n+\n+  if (!stopped() && !from_inline) {\n+    res = record_profiled_receiver_for_speculation(res);\n+    if (to_inline && toop->inline_klass()->is_scalarizable()) {\n+      assert(!gvn().type(res)->maybe_null(), \"Inline types are null-free\");\n+      res = InlineTypeNode::make_from_oop(this, res, toop->inline_klass());\n+    }\n+  }\n+  return res;\n+}\n+\n+Node* GraphKit::inline_type_test(Node* obj, bool is_inline) {\n+  Node* mark_adr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+  Node* mark = make_load(NULL, mark_adr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  Node* mask = MakeConX(markWord::inline_type_pattern);\n+  Node* masked = _gvn.transform(new AndXNode(mark, mask));\n+  Node* cmp = _gvn.transform(new CmpXNode(masked, mask));\n+  return _gvn.transform(new BoolNode(cmp, is_inline ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::array_lh_test(Node* klass, jint mask, jint val, bool eq) {\n+  Node* lh_adr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+  \/\/ Make sure to use immutable memory here to enable hoisting the check out of loops\n+  Node* lh_val = _gvn.transform(LoadNode::make(_gvn, NULL, immutable_memory(), lh_adr, lh_adr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+  Node* masked = _gvn.transform(new AndINode(lh_val, intcon(mask)));\n+  Node* cmp = _gvn.transform(new CmpINode(masked, intcon(val)));\n+  return _gvn.transform(new BoolNode(cmp, eq ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::flat_array_test(Node* ary, bool flat) {\n+  \/\/ We can't use immutable memory here because the mark word is mutable.\n+  \/\/ PhaseIdealLoop::move_flat_array_check_out_of_loop will make sure the\n+  \/\/ check is moved out of loops (mainly to enable loop unswitching).\n+  Node* mem = UseArrayMarkWordCheck ? memory(Compile::AliasIdxRaw) : immutable_memory();\n+  Node* cmp = _gvn.transform(new FlatArrayCheckNode(C, mem, ary));\n+  record_for_igvn(cmp); \/\/ Give it a chance to be optimized out by IGVN\n+  return _gvn.transform(new BoolNode(cmp, flat ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::null_free_array_test(Node* klass, bool null_free) {\n+  return array_lh_test(klass, Klass::_lh_null_free_bit_inplace, 0, !null_free);\n+}\n+\n+\/\/ Deoptimize if 'ary' is a null-free inline type array and 'val' is null\n+Node* GraphKit::inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {\n+  const Type* val_t = _gvn.type(val);\n+  if (val->is_InlineType() || !TypePtr::NULL_PTR->higher_equal(val_t)) {\n+    return ary; \/\/ Never null\n+  }\n+  RegionNode* region = new RegionNode(3);\n+  Node* null_ctl = top();\n+  null_check_oop(val, &null_ctl);\n+  if (null_ctl != top()) {\n+    PreserveJVMState pjvms(this);\n+    set_control(null_ctl);\n+    {\n+      \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(load_object_klass(ary), \/* null_free = *\/ false), PROB_MAX);\n+      inc_sp(nargs);\n+      uncommon_trap(Deoptimization::Reason_null_check,\n+                    Deoptimization::Action_none);\n+    }\n+    region->init_req(1, control());\n+  }\n+  region->init_req(2, control());\n+  set_control(_gvn.transform(region));\n+  record_for_igvn(region);\n+  const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+  if (val_t == TypePtr::NULL_PTR && !ary_t->is_not_null_free()) {\n+    \/\/ Since we were just successfully storing null, the array can't be null free.\n+    ary_t = ary_t->cast_to_not_null_free();\n+    Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+    if (safe_for_replace) {\n+      replace_in_map(ary, cast);\n+    }\n+    ary = cast;\n+  }\n+  return ary;\n@@ -3498,0 +3818,1 @@\n+\n@@ -3570,0 +3891,1 @@\n+  assert(!obj->is_InlineTypeBase(), \"should not unlock on inline type\");\n@@ -3610,2 +3932,8 @@\n-    bool    xklass = inst_klass->klass_is_exact();\n-    if (xklass || klass->is_array_klass()) {\n+    assert(klass != NULL, \"klass should not be NULL\");\n+    bool xklass = inst_klass->klass_is_exact();\n+    bool can_be_flattened = false;\n+    if (UseFlatArray && klass->is_obj_array_klass()) {\n+      ciKlass* elem = klass->as_obj_array_klass()->element_klass();\n+      can_be_flattened = elem->can_be_inline_klass() && (!elem->is_inlinetype() || elem->flatten_array());\n+    }\n+    if (xklass || (klass->is_array_klass() && !can_be_flattened)) {\n@@ -3673,0 +4001,1 @@\n+    _gvn.set_type(minit_in, Type::MEMORY);\n@@ -3680,3 +4009,26 @@\n-      const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n-      int            elemidx  = C->get_alias_index(telemref);\n-      hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      const TypeAryPtr* arytype = oop_type->is_aryptr();\n+      if (arytype->klass()->is_flat_array_klass()) {\n+        \/\/ Initially all flattened array accesses share a single slice\n+        \/\/ but that changes after parsing. Prepare the memory graph so\n+        \/\/ it can optimize flattened array accesses properly once they\n+        \/\/ don't share a single slice.\n+        assert(C->flattened_accesses_share_alias(), \"should be set at parse time\");\n+        C->set_flattened_accesses_share_alias(false);\n+        ciFlatArrayKlass* vak = arytype->klass()->as_flat_array_klass();\n+        ciInlineKlass* vk = vak->element_klass()->as_inline_klass();\n+        for (int i = 0, len = vk->nof_nonstatic_fields(); i < len; i++) {\n+          ciField* field = vk->nonstatic_field_at(i);\n+          if (field->offset() >= TrackedInitializationLimit * HeapWordSize)\n+            continue;  \/\/ do not bother to track really large numbers of fields\n+          int off_in_vt = field->offset() - vk->first_field_offset();\n+          const TypePtr* adr_type = arytype->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+          int fieldidx = C->get_alias_index(adr_type, true);\n+          hook_memory_on_init(*this, fieldidx, minit_in, minit_out);\n+        }\n+        C->set_flattened_accesses_share_alias(true);\n+        hook_memory_on_init(*this, C->get_alias_index(TypeAryPtr::INLINES), minit_in, minit_out);\n+      } else {\n+        const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n+        int            elemidx  = C->get_alias_index(telemref);\n+        hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      }\n@@ -3684,0 +4036,1 @@\n+      set_memory(minit_out, C->get_alias_index(oop_type)); \/\/ mark word\n@@ -3734,1 +4087,2 @@\n-                             bool deoptimize_on_exception) {\n+                             bool deoptimize_on_exception,\n+                             InlineTypeBaseNode* inline_type_node) {\n@@ -3741,1 +4095,1 @@\n-  int   layout_is_con = (layout_val == NULL);\n+  bool  layout_is_con = (layout_val == NULL);\n@@ -3792,1 +4146,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3799,1 +4153,1 @@\n-                                         initial_slow_test);\n+                                         initial_slow_test, inline_type_node);\n@@ -3805,1 +4159,1 @@\n-\/\/ helper for both newarray and anewarray\n+\/\/ helper for newarray and anewarray\n@@ -3815,1 +4169,1 @@\n-  int   layout_is_con = (layout_val == NULL);\n+  bool  layout_is_con = (layout_val == NULL);\n@@ -3845,1 +4199,1 @@\n-    fast_size_limit <<= (LogBytesPerLong - log2_esize);\n+    fast_size_limit <<= MAX2(LogBytesPerLong - log2_esize, 0);\n@@ -3863,1 +4217,1 @@\n-    BasicType etype  = Klass::layout_helper_element_type(layout_con);\n+    bool is_flat_array = Klass::layout_helper_is_flatArray(layout_con);\n@@ -3866,1 +4220,1 @@\n-    assert((hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n+    assert(is_flat_array || (hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n@@ -3950,1 +4304,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3959,0 +4313,63 @@\n+  const TypeKlassPtr* ary_klass = _gvn.type(klass_node)->isa_klassptr();\n+  const TypeOopPtr* ary_type = ary_klass->as_instance_type();\n+  const TypeAryPtr* ary_ptr = ary_type->isa_aryptr();\n+\n+  \/\/ Inline type array variants:\n+  \/\/ - null-ok:              MyValue.ref[] (ciObjArrayKlass \"[LMyValue$ref\")\n+  \/\/ - null-free:            MyValue.val[] (ciObjArrayKlass \"[QMyValue$val\")\n+  \/\/ - null-free, flattened: MyValue.val[] (ciFlatArrayKlass \"[QMyValue$val\")\n+  \/\/ Check if array is a null-free, non-flattened inline type array\n+  \/\/ that needs to be initialized with the default inline type.\n+  Node* default_value = NULL;\n+  Node* raw_default_value = NULL;\n+  if (ary_ptr != NULL && ary_ptr->klass_is_exact()) {\n+    \/\/ Array type is known\n+    ciKlass* elem_klass = ary_ptr->klass()->as_array_klass()->element_klass();\n+    if (elem_klass != NULL && elem_klass->is_inlinetype()) {\n+      ciInlineKlass* vk = elem_klass->as_inline_klass();\n+      if (!vk->flatten_array()) {\n+        default_value = InlineTypeNode::default_oop(gvn(), vk);\n+      }\n+    }\n+  } else if (ary_klass->klass()->can_be_inline_array_klass()) {\n+    \/\/ Array type is not known, add runtime checks\n+    assert(!ary_klass->klass_is_exact(), \"unexpected exact type\");\n+    Node* r = new RegionNode(3);\n+    default_value = new PhiNode(r, TypeInstPtr::BOTTOM);\n+\n+    Node* bol = array_lh_test(klass_node, Klass::_lh_array_tag_vt_value_bit_inplace | Klass::_lh_null_free_bit_inplace, Klass::_lh_null_free_bit_inplace);\n+    IfNode* iff = create_and_map_if(control(), bol, PROB_FAIR, COUNT_UNKNOWN);\n+\n+    \/\/ Null-free, non-flattened inline type array, initialize with the default value\n+    set_control(_gvn.transform(new IfTrueNode(iff)));\n+    Node* p = basic_plus_adr(klass_node, in_bytes(ArrayKlass::element_klass_offset()));\n+    Node* eklass = _gvn.transform(LoadKlassNode::make(_gvn, control(), immutable_memory(), p, TypeInstPtr::KLASS));\n+    Node* adr_fixed_block_addr = basic_plus_adr(eklass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    Node* adr_fixed_block = make_load(control(), adr_fixed_block_addr, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+    Node* default_value_offset_addr = basic_plus_adr(adr_fixed_block, in_bytes(InlineKlass::default_value_offset_offset()));\n+    Node* default_value_offset = make_load(control(), default_value_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+    Node* elem_mirror = load_mirror_from_klass(eklass);\n+    Node* default_value_addr = basic_plus_adr(elem_mirror, ConvI2X(default_value_offset));\n+    Node* val = access_load_at(elem_mirror, default_value_addr, _gvn.type(default_value_addr)->is_ptr(), TypeInstPtr::BOTTOM, T_OBJECT, IN_HEAP);\n+    r->init_req(1, control());\n+    default_value->init_req(1, val);\n+\n+    \/\/ Otherwise initialize with all zero\n+    r->init_req(2, _gvn.transform(new IfFalseNode(iff)));\n+    default_value->init_req(2, null());\n+\n+    set_control(_gvn.transform(r));\n+    default_value = _gvn.transform(default_value);\n+  }\n+  if (default_value != NULL) {\n+    if (UseCompressedOops) {\n+      \/\/ With compressed oops, the 64-bit init value is built from two 32-bit compressed oops\n+      default_value = _gvn.transform(new EncodePNode(default_value, default_value->bottom_type()->make_narrowoop()));\n+      Node* lower = _gvn.transform(new CastP2XNode(control(), default_value));\n+      Node* upper = _gvn.transform(new LShiftLNode(lower, intcon(32)));\n+      raw_default_value = _gvn.transform(new OrLNode(lower, upper));\n+    } else {\n+      raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));\n+    }\n+  }\n+\n@@ -3960,6 +4377,6 @@\n-  AllocateArrayNode* alloc\n-    = new AllocateArrayNode(C, AllocateArrayNode::alloc_type(TypeInt::INT),\n-                            control(), mem, i_o(),\n-                            size, klass_node,\n-                            initial_slow_test,\n-                            length);\n+  AllocateArrayNode* alloc = new AllocateArrayNode(C, AllocateArrayNode::alloc_type(TypeInt::INT),\n+                                                   control(), mem, i_o(),\n+                                                   size, klass_node,\n+                                                   initial_slow_test,\n+                                                   length, default_value,\n+                                                   raw_default_value);\n@@ -3972,1 +4389,0 @@\n-  const TypeOopPtr* ary_type = _gvn.type(klass_node)->is_klassptr()->as_instance_type();\n@@ -4130,1 +4546,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4133,2 +4549,2 @@\n-                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS),\n-                                                  ciTypeArrayKlass::make(T_BYTE), true, 0);\n+                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, true, true),\n+                                                  ciTypeArrayKlass::make(T_BYTE), true, Type::Offset(0));\n@@ -4147,1 +4563,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4159,1 +4575,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4169,1 +4585,1 @@\n-                                                     false, NULL, 0);\n+                                                     false, NULL, Type::Offset(0));\n@@ -4280,1 +4696,9 @@\n-    return makecon(con_type);\n+    Node* con = makecon(con_type);\n+    \/\/ Check type of constant which might be more precise\n+    if (con_type->is_inlinetypeptr() && con_type->inline_klass()->is_scalarizable()) {\n+      assert(!con_type->is_zero_type(), \"Inline types are null-free\");\n+      con = InlineTypeNode::make_from_oop(this, con, con_type->inline_klass());\n+    } else if (con_type->is_zero_type() && field->type()->is_inlinetype()) {\n+      con = InlineTypeNode::default_oop(gvn(), field->type()->as_inline_klass());\n+    }\n+    return con;\n@@ -4284,0 +4708,9 @@\n+\n+\/\/---------------------------load_mirror_from_klass----------------------------\n+\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n+Node* GraphKit::load_mirror_from_klass(Node* klass) {\n+  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n+  Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+  \/\/ mirror = ((OopHandle)mirror)->resolve();\n+  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n+}\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":590,"deletions":157,"binary":false,"changes":747,"status":"modified"},{"patch":"@@ -1194,0 +1194,17 @@\n+\/\/ Returns true if this IfNode belongs to a flat array check\n+\/\/ and returns the corresponding array in the 'array' parameter.\n+bool IfNode::is_flat_array_check(PhaseTransform* phase, Node** array) {\n+  Node* bol = in(1);\n+  if (!bol->is_Bool()) {\n+    return false;\n+  }\n+  Node* cmp = bol->in(1);\n+  if (cmp->isa_FlatArrayCheck()) {\n+    if (array != NULL) {\n+      *array = cmp->in(FlatArrayCheckNode::Array);\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -26,0 +26,3 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciField.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -51,0 +54,46 @@\n+const Type::Offset Type::Offset::top(Type::OffsetTop);\n+const Type::Offset Type::Offset::bottom(Type::OffsetBot);\n+\n+const Type::Offset Type::Offset::meet(const Type::Offset other) const {\n+  \/\/ Either is 'TOP' offset?  Return the other offset!\n+  int offset = other._offset;\n+  if (_offset == OffsetTop) return Offset(offset);\n+  if (offset == OffsetTop) return Offset(_offset);\n+  \/\/ If either is different, return 'BOTTOM' offset\n+  if (_offset != offset) return bottom;\n+  return Offset(_offset);\n+}\n+\n+const Type::Offset Type::Offset::dual() const {\n+  if (_offset == OffsetTop) return bottom;\/\/ Map 'TOP' into 'BOTTOM'\n+  if (_offset == OffsetBot) return top;\/\/ Map 'BOTTOM' into 'TOP'\n+  return Offset(_offset);               \/\/ Map everything else into self\n+}\n+\n+const Type::Offset Type::Offset::add(intptr_t offset) const {\n+  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n+  if (_offset == OffsetTop || offset == OffsetTop) return top;\n+  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n+  if (_offset == OffsetBot || offset == OffsetBot) return bottom;\n+  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n+  offset += (intptr_t)_offset;\n+  if (offset != (int)offset || offset == OffsetTop) return bottom;\n+\n+  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n+  \/\/ It is possible to construct a negative offset during PhaseCCP\n+\n+  return Offset((int)offset);        \/\/ Sum valid offsets\n+}\n+\n+void Type::Offset::dump2(outputStream *st) const {\n+  if (_offset == 0) {\n+    return;\n+  } else if (_offset == OffsetTop) {\n+    st->print(\"+top\");\n+  }\n+  else if (_offset == OffsetBot) {\n+    st->print(\"+bot\");\n+  } else if (_offset) {\n+    st->print(\"+%d\", _offset);\n+  }\n+}\n@@ -87,0 +136,1 @@\n+  { Bad,             T_INLINE_TYPE, \"inline:\",      false, Node::NotAMachineReg, relocInfo::none          },  \/\/ InlineType\n@@ -217,0 +267,9 @@\n+  case T_INLINE_TYPE: {\n+    ciInlineKlass* vk = type->as_inline_klass();\n+    if (vk->is_scalarizable()) {\n+      return TypeInlineType::make(vk);\n+    } else {\n+      return TypeOopPtr::make_from_klass(vk)->join_speculative(TypePtr::NOTNULL);\n+    }\n+  }\n+\n@@ -245,0 +304,1 @@\n+    case T_INLINE_TYPE:\n@@ -282,0 +342,1 @@\n+    case T_INLINE_TYPE: conbt = T_OBJECT; break;\n@@ -288,0 +349,1 @@\n+    case T_INLINE_TYPE: loadbt = T_OBJECT; break;\n@@ -523,3 +585,3 @@\n-  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, 0);\n-  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, OffsetBot);\n-  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, OffsetBot);\n+  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, Offset(0));\n+  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, Offset::bottom);\n+  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, Offset::bottom);\n@@ -542,1 +604,1 @@\n-                                           false, 0, oopDesc::mark_offset_in_bytes());\n+                                           false, 0, Offset(oopDesc::mark_offset_in_bytes()));\n@@ -544,2 +606,2 @@\n-                                           false, 0, oopDesc::klass_offset_in_bytes());\n-  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, OffsetBot, TypeOopPtr::InstanceBot);\n+                                           false, 0, Offset(oopDesc::klass_offset_in_bytes()));\n+  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, Offset::bottom, TypeOopPtr::InstanceBot);\n@@ -547,1 +609,3 @@\n-  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, NULL, OffsetBot);\n+  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, NULL, Offset::bottom);\n+\n+  TypeInlineType::BOTTOM = TypeInlineType::make(NULL);\n@@ -564,1 +628,1 @@\n-  TypeAryPtr::RANGE   = TypeAryPtr::make( TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), NULL \/* current->env()->Object_klass() *\/, false, arrayOopDesc::length_offset_in_bytes());\n+  TypeAryPtr::RANGE   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), NULL \/* current->env()->Object_klass() *\/, false, Offset(arrayOopDesc::length_offset_in_bytes()));\n@@ -566,1 +630,1 @@\n-  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), NULL \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), NULL \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -576,1 +640,1 @@\n-    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), NULL \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), NULL \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -578,7 +642,8 @@\n-  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Type::OffsetBot);\n-  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Type::OffsetBot);\n-  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Type::OffsetBot);\n-  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Type::OffsetBot);\n-  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Type::OffsetBot);\n-  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Type::OffsetBot);\n-  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Type::OffsetBot);\n+  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Offset::bottom);\n+  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Offset::bottom);\n+  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Offset::bottom);\n+  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Offset::bottom);\n+  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Offset::bottom);\n+  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Offset::bottom);\n+  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Offset::bottom);\n+  TypeAryPtr::INLINES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInlineType::BOTTOM,TypeInt::POS), NULL, false,  Offset::bottom);\n@@ -589,0 +654,1 @@\n+  TypeAryPtr::_array_body_type[T_INLINE_TYPE] = TypeAryPtr::OOPS;\n@@ -599,2 +665,2 @@\n-  TypeKlassPtr::OBJECT = TypeKlassPtr::make( TypePtr::NotNull, current->env()->Object_klass(), 0 );\n-  TypeKlassPtr::OBJECT_OR_NULL = TypeKlassPtr::make( TypePtr::BotPTR, current->env()->Object_klass(), 0 );\n+  TypeKlassPtr::OBJECT = TypeKlassPtr::make(TypePtr::NotNull, current->env()->Object_klass(), Offset(0));\n+  TypeKlassPtr::OBJECT_OR_NULL = TypeKlassPtr::make(TypePtr::BotPTR, current->env()->Object_klass(), Offset(0));\n@@ -639,0 +705,1 @@\n+  _const_basic_type[T_INLINE_TYPE] = TypeInstPtr::BOTTOM;\n@@ -655,0 +722,1 @@\n+  _zero_type[T_INLINE_TYPE] = TypePtr::NULL_PTR;\n@@ -941,0 +1009,3 @@\n+  case InlineType:\n+    return t->xmeet(this);\n+\n@@ -1009,0 +1080,1 @@\n+  Bad,          \/\/ InlineType - handled in v-call\n@@ -2003,0 +2075,12 @@\n+static void collect_inline_fields(ciInlineKlass* vk, const Type** field_array, uint& pos) {\n+  for (int j = 0; j < vk->nof_nonstatic_fields(); j++) {\n+    ciField* field = vk->nonstatic_field_at(j);\n+    BasicType bt = field->type()->basic_type();\n+    const Type* ft = Type::get_const_type(field->type());\n+    field_array[pos++] = ft;\n+    if (type2size[bt] == 2) {\n+      field_array[pos++] = Type::HALF;\n+    }\n+  }\n+}\n+\n@@ -2005,1 +2089,1 @@\n-const TypeTuple *TypeTuple::make_range(ciSignature* sig) {\n+const TypeTuple *TypeTuple::make_range(ciSignature* sig, bool ret_vt_fields) {\n@@ -2008,0 +2092,4 @@\n+  if (ret_vt_fields) {\n+    arg_cnt = return_type->as_inline_klass()->inline_arg_slots() + 1;\n+  }\n+\n@@ -2028,0 +2116,10 @@\n+  case T_INLINE_TYPE:\n+    if (ret_vt_fields) {\n+      uint pos = TypeFunc::Parms;\n+      field_array[pos] = TypePtr::BOTTOM;\n+      pos++;\n+      collect_inline_fields(return_type->as_inline_klass(), field_array, pos);\n+    } else {\n+      field_array[TypeFunc::Parms] = get_const_type(return_type)->join_speculative(TypePtr::NOTNULL);\n+    }\n+    break;\n@@ -2037,2 +2135,9 @@\n-const TypeTuple *TypeTuple::make_domain(ciInstanceKlass* recv, ciSignature* sig) {\n-  uint arg_cnt = sig->size();\n+const TypeTuple *TypeTuple::make_domain(ciMethod* method, bool vt_fields_as_args) {\n+  ciSignature* sig = method->signature();\n+  uint arg_cnt = sig->size() + (method->is_static() ? 0 : 1);\n+  if (vt_fields_as_args) {\n+    arg_cnt = 0;\n+    for (ExtendedSignature sig_cc = ExtendedSignature(method->get_sig_cc(), SigEntryFilter()); !sig_cc.at_end(); ++sig_cc) {\n+      arg_cnt += type2size[(*sig_cc)._bt];\n+    }\n+  }\n@@ -2041,8 +2146,8 @@\n-  const Type **field_array;\n-  if (recv != NULL) {\n-    arg_cnt++;\n-    field_array = fields(arg_cnt);\n-    \/\/ Use get_const_type here because it respects UseUniqueSubclasses:\n-    field_array[pos++] = get_const_type(recv)->join_speculative(TypePtr::NOTNULL);\n-  } else {\n-    field_array = fields(arg_cnt);\n+  const Type** field_array = fields(arg_cnt);\n+  if (!method->is_static()) {\n+    ciInstanceKlass* recv = method->holder();\n+    if (vt_fields_as_args && recv->is_inlinetype() && recv->as_inline_klass()->can_be_passed_as_fields()) {\n+      collect_inline_fields(recv->as_inline_klass(), field_array, pos);\n+    } else {\n+      field_array[pos++] = get_const_type(recv)->join_speculative(TypePtr::NOTNULL);\n+    }\n@@ -2054,0 +2159,1 @@\n+    BasicType bt = type->basic_type();\n@@ -2055,1 +2161,1 @@\n-    switch (type->basic_type()) {\n+    switch (bt) {\n@@ -2076,0 +2182,8 @@\n+    case T_INLINE_TYPE: {\n+      if (vt_fields_as_args && type->as_inline_klass()->can_be_passed_as_fields()) {\n+        collect_inline_fields(type->as_inline_klass(), field_array, pos);\n+      } else {\n+        field_array[pos++] = get_const_type(type)->join_speculative(TypePtr::NOTNULL);\n+      }\n+      break;\n+    }\n@@ -2081,0 +2195,1 @@\n+  assert(pos == TypeFunc::Parms + arg_cnt, \"wrong number of arguments\");\n@@ -2215,1 +2330,2 @@\n-const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable) {\n+const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable,\n+                             bool not_flat, bool not_null_free) {\n@@ -2220,1 +2336,1 @@\n-  return (TypeAry*)(new TypeAry(elem,size,stable))->hashcons();\n+  return (TypeAry*)(new TypeAry(elem, size, stable, not_flat, not_null_free))->hashcons();\n@@ -2242,1 +2358,3 @@\n-                         _stable && a->_stable);\n+                         _stable && a->_stable,\n+                         _not_flat && a->_not_flat,\n+                         _not_null_free && a->_not_null_free);\n@@ -2255,1 +2373,1 @@\n-  return new TypeAry(_elem->dual(), size_dual, !_stable);\n+  return new TypeAry(_elem->dual(), size_dual, !_stable, !_not_flat, !_not_null_free);\n@@ -2264,1 +2382,4 @@\n-    _size == a->_size;\n+    _size == a->_size &&\n+    _not_flat == a->_not_flat &&\n+    _not_null_free == a->_not_null_free;\n+\n@@ -2277,1 +2398,1 @@\n-  return make(_elem->remove_speculative(), _size, _stable);\n+  return make(_elem->remove_speculative(), _size, _stable, _not_flat, _not_null_free);\n@@ -2284,1 +2405,1 @@\n-  return make(_elem->cleanup_speculative(), _size, _stable);\n+  return make(_elem->cleanup_speculative(), _size, _stable, _not_flat, _not_null_free);\n@@ -2318,0 +2439,4 @@\n+  if (Verbose) {\n+    if (_not_flat) st->print(\"not flat:\");\n+    if (_not_null_free) st->print(\"not null free:\");\n+  }\n@@ -2371,0 +2496,124 @@\n+\/\/==============================TypeInlineType=======================================\n+\n+const TypeInlineType* TypeInlineType::BOTTOM;\n+\n+\/\/------------------------------make-------------------------------------------\n+const TypeInlineType* TypeInlineType::make(ciInlineKlass* vk, bool larval) {\n+  return (TypeInlineType*)(new TypeInlineType(vk, larval))->hashcons();\n+}\n+\n+\/\/------------------------------meet-------------------------------------------\n+\/\/ Compute the MEET of two types.  It returns a new Type object.\n+const Type* TypeInlineType::xmeet(const Type* t) const {\n+  \/\/ Perform a fast test for common case; meeting the same types together.\n+  if(this == t) return this;  \/\/ Meeting same type-rep?\n+\n+  \/\/ Current \"this->_base\" is InlineType\n+  switch (t->base()) {          \/\/ switch on original type\n+\n+  case Int:\n+  case Long:\n+  case FloatTop:\n+  case FloatCon:\n+  case FloatBot:\n+  case DoubleTop:\n+  case DoubleCon:\n+  case DoubleBot:\n+  case NarrowKlass:\n+  case Bottom:\n+    return Type::BOTTOM;\n+\n+  case OopPtr:\n+  case MetadataPtr:\n+  case KlassPtr:\n+  case RawPtr:\n+    return TypePtr::BOTTOM;\n+\n+  case Top:\n+    return this;\n+\n+  case NarrowOop: {\n+    const Type* res = t->make_ptr()->xmeet(this);\n+    if (res->isa_ptr()) {\n+      return res->make_narrowoop();\n+    }\n+    return res;\n+  }\n+\n+  case AryPtr:\n+  case InstPtr: {\n+    return t->xmeet(this);\n+  }\n+\n+  case InlineType: {\n+    \/\/ All inline types inherit from Object\n+    const TypeInlineType* other = t->is_inlinetype();\n+    if (_vk == NULL) {\n+      return this;\n+    } else if (other->_vk == NULL) {\n+      return other;\n+    } else if (_vk == other->_vk) {\n+      if (_larval == other->_larval ||\n+          !_larval) {\n+        return this;\n+      } else {\n+        return t;\n+      }\n+    }\n+    return TypeInstPtr::NOTNULL;\n+  }\n+\n+  default:                      \/\/ All else is a mistake\n+    typerr(t);\n+\n+  }\n+  return this;\n+}\n+\n+\/\/------------------------------xdual------------------------------------------\n+const Type* TypeInlineType::xdual() const {\n+  return this;\n+}\n+\n+\/\/------------------------------eq---------------------------------------------\n+\/\/ Structural equality check for Type representations\n+bool TypeInlineType::eq(const Type* t) const {\n+  const TypeInlineType* vt = t->is_inlinetype();\n+  return (_vk == vt->inline_klass() && _larval == vt->larval());\n+}\n+\n+\/\/------------------------------hash-------------------------------------------\n+\/\/ Type-specific hashing function.\n+int TypeInlineType::hash(void) const {\n+  return (intptr_t)_vk;\n+}\n+\n+\/\/------------------------------singleton--------------------------------------\n+\/\/ TRUE if Type is a singleton type, FALSE otherwise. Singletons are simple constants.\n+bool TypeInlineType::singleton(void) const {\n+  return false;\n+}\n+\n+\/\/------------------------------empty------------------------------------------\n+\/\/ TRUE if Type is a type with no values, FALSE otherwise.\n+bool TypeInlineType::empty(void) const {\n+  return false;\n+}\n+\n+\/\/------------------------------dump2------------------------------------------\n+#ifndef PRODUCT\n+void TypeInlineType::dump2(Dict &d, uint depth, outputStream* st) const {\n+  if (_vk == NULL) {\n+    st->print(\"BOTTOM inlinetype\");\n+    return;\n+  }\n+  int count = _vk->nof_declared_nonstatic_fields();\n+  st->print(\"inlinetype[%d]:{\", count);\n+  st->print(\"%s\", count != 0 ? _vk->declared_nonstatic_field_at(0)->type()->name() : \"empty\");\n+  for (int i = 1; i < count; ++i) {\n+    st->print(\", %s\", _vk->declared_nonstatic_field_at(i)->type()->name());\n+  }\n+  st->print(\"}%s\", _larval?\" : larval\":\"\");\n+}\n+#endif\n+\n@@ -2516,1 +2765,1 @@\n-const TypePtr *TypePtr::make(TYPES t, enum PTR ptr, int offset, const TypePtr* speculative, int inline_depth) {\n+const TypePtr* TypePtr::make(TYPES t, enum PTR ptr, Offset offset, const TypePtr* speculative, int inline_depth) {\n@@ -2530,1 +2779,1 @@\n-  return _offset;\n+  return offset();\n@@ -2599,7 +2848,2 @@\n-int TypePtr::meet_offset( int offset ) const {\n-  \/\/ Either is 'TOP' offset?  Return the other offset!\n-  if( _offset == OffsetTop ) return offset;\n-  if( offset == OffsetTop ) return _offset;\n-  \/\/ If either is different, return 'BOTTOM' offset\n-  if( _offset != offset ) return OffsetBot;\n-  return _offset;\n+Type::Offset TypePtr::meet_offset(int offset) const {\n+  return _offset.meet(Offset(offset));\n@@ -2609,4 +2853,2 @@\n-int TypePtr::dual_offset( ) const {\n-  if( _offset == OffsetTop ) return OffsetBot;\/\/ Map 'TOP' into 'BOTTOM'\n-  if( _offset == OffsetBot ) return OffsetTop;\/\/ Map 'BOTTOM' into 'TOP'\n-  return _offset;               \/\/ Map everything else into self\n+Type::Offset TypePtr::dual_offset() const {\n+  return _offset.dual();\n@@ -2625,13 +2867,2 @@\n-int TypePtr::xadd_offset( intptr_t offset ) const {\n-  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n-  if( _offset == OffsetTop || offset == OffsetTop ) return OffsetTop;\n-  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n-  if( _offset == OffsetBot || offset == OffsetBot ) return OffsetBot;\n-  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n-  offset += (intptr_t)_offset;\n-  if (offset != (int)offset || offset == OffsetTop) return OffsetBot;\n-\n-  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n-  \/\/ It is possible to construct a negative offset during PhaseCCP\n-\n-  return (int)offset;        \/\/ Sum valid offsets\n+Type::Offset TypePtr::xadd_offset(intptr_t offset) const {\n+  return _offset.add(offset);\n@@ -2649,1 +2880,1 @@\n-  return _ptr == a->ptr() && _offset == a->offset() && eq_speculative(a) && _inline_depth == a->_inline_depth;\n+  return _ptr == a->ptr() && _offset == a->_offset && eq_speculative(a) && _inline_depth == a->_inline_depth;\n@@ -2655,1 +2886,1 @@\n-  return java_add(java_add((jint)_ptr, (jint)_offset), java_add((jint)hash_speculative(), (jint)_inline_depth));\n+  return java_add(java_add((jint)_ptr, (jint)offset()), java_add((jint)hash_speculative(), (jint)_inline_depth));\n@@ -2915,3 +3146,1 @@\n-  if( _offset == OffsetTop ) st->print(\"+top\");\n-  else if( _offset == OffsetBot ) st->print(\"+bot\");\n-  else if( _offset ) st->print(\"+%d\", _offset);\n+  _offset.dump2(st);\n@@ -2952,1 +3181,1 @@\n-  return (_offset != OffsetBot) && !below_centerline(_ptr);\n+  return (_offset != Offset::bottom) && !below_centerline(_ptr);\n@@ -2956,1 +3185,1 @@\n-  return (_offset == OffsetTop) || above_centerline(_ptr);\n+  return (_offset == Offset::top) || above_centerline(_ptr);\n@@ -3098,1 +3327,1 @@\n-TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, bool xk, ciObject* o, int offset,\n+TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, bool xk, ciObject* o, Offset offset, Offset field_offset,\n@@ -3108,2 +3337,2 @@\n-      (offset > 0) && xk && (k != 0) && k->is_instance_klass()) {\n-    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset);\n+      (offset.get() > 0) && xk && (k != 0) && k->is_instance_klass()) {\n+    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset.get());\n@@ -3112,2 +3341,2 @@\n-  if (_offset > 0 || _offset == Type::OffsetTop || _offset == Type::OffsetBot) {\n-    if (_offset == oopDesc::klass_offset_in_bytes()) {\n+  if (this->offset() > 0 || this->offset() == Type::OffsetTop || this->offset() == Type::OffsetBot) {\n+    if (this->offset() == oopDesc::klass_offset_in_bytes()) {\n@@ -3119,3 +3348,12 @@\n-    } else if (this->isa_aryptr()) {\n-      _is_ptr_to_narrowoop = (UseCompressedOops && klass()->is_obj_array_klass() &&\n-                             _offset != arrayOopDesc::length_offset_in_bytes());\n+    } else if (UseCompressedOops && this->isa_aryptr() && this->offset() != arrayOopDesc::length_offset_in_bytes()) {\n+      if (klass()->is_obj_array_klass()) {\n+        _is_ptr_to_narrowoop = true;\n+      } else if (klass()->is_flat_array_klass() && field_offset != Offset::top && field_offset != Offset::bottom) {\n+        \/\/ Check if the field of the inline type array element contains oops\n+        ciInlineKlass* vk = klass()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+        int foffset = field_offset.get() + vk->first_field_offset();\n+        ciField* field = vk->get_field_by_offset(foffset, false);\n+        assert(field != NULL, \"missing field\");\n+        BasicType bt = field->layout_type();\n+        _is_ptr_to_narrowoop = UseCompressedOops && is_reference_type(bt);\n+      }\n@@ -3123,2 +3361,0 @@\n-      ciInstanceKlass* ik = klass()->as_instance_klass();\n-      ciField* field = NULL;\n@@ -3127,1 +3363,1 @@\n-      } else if (_offset == OffsetBot || _offset == OffsetTop) {\n+      } else if (_offset == Offset::bottom || _offset == Offset::top) {\n@@ -3132,3 +3368,2 @@\n-\n-            (_offset == java_lang_Class::klass_offset() ||\n-             _offset == java_lang_Class::array_klass_offset())) {\n+            (this->offset() == java_lang_Class::klass_offset() ||\n+             this->offset() == java_lang_Class::array_klass_offset())) {\n@@ -3140,1 +3375,1 @@\n-                   _offset >= InstanceMirrorKlass::offset_of_static_fields()) {\n+                   this->offset() >= InstanceMirrorKlass::offset_of_static_fields()) {\n@@ -3145,8 +3380,14 @@\n-            field = k->get_field_by_offset(_offset, true);\n-          }\n-          if (field != NULL) {\n-            BasicType basic_elem_type = field->layout_type();\n-            _is_ptr_to_narrowoop = UseCompressedOops && is_reference_type(basic_elem_type);\n-          } else {\n-            \/\/ unsafe access\n-            _is_ptr_to_narrowoop = UseCompressedOops;\n+            if (k->is_inlinetype() && this->offset() == k->as_inline_klass()->default_value_offset()) {\n+              \/\/ Special hidden field that contains the oop of the default inline type\n+              \/\/ basic_elem_type = T_INLINE_TYPE;\n+             _is_ptr_to_narrowoop = UseCompressedOops;\n+            } else {\n+              field = k->get_field_by_offset(this->offset(), true);\n+              if (field != NULL) {\n+                BasicType basic_elem_type = field->layout_type();\n+                _is_ptr_to_narrowoop = UseCompressedOops && is_reference_type(basic_elem_type);\n+              } else {\n+                \/\/ unsafe access\n+                _is_ptr_to_narrowoop = UseCompressedOops;\n+              }\n+            }\n@@ -3156,1 +3397,2 @@\n-          field = ik->get_field_by_offset(_offset, false);\n+          ciInstanceKlass* ik = klass()->as_instance_klass();\n+          ciField* field = ik->get_field_by_offset(this->offset(), false);\n@@ -3176,2 +3418,2 @@\n-const TypeOopPtr *TypeOopPtr::make(PTR ptr, int offset, int instance_id,\n-                                     const TypePtr* speculative, int inline_depth) {\n+const TypeOopPtr *TypeOopPtr::make(PTR ptr, Offset offset, int instance_id,\n+                                   const TypePtr* speculative, int inline_depth) {\n@@ -3182,1 +3424,1 @@\n-  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, xk, o, offset, instance_id, speculative, inline_depth))->hashcons();\n+  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, xk, o, offset, Offset::bottom, instance_id, speculative, inline_depth))->hashcons();\n@@ -3207,1 +3449,0 @@\n-\n@@ -3217,1 +3458,1 @@\n-    return TypeKlassPtr::make(xk? Constant: NotNull, k, 0);\n+    return TypeKlassPtr::make(xk? Constant: NotNull, k, Offset(0));\n@@ -3255,1 +3496,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -3297,1 +3538,1 @@\n-  return new TypeOopPtr(_base, dual_ptr(), klass(), klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeOopPtr(_base, dual_ptr(), klass(), klass_is_exact(), const_oop(), dual_offset(), Offset::bottom, dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -3303,1 +3544,1 @@\n-  if (klass->is_instance_klass()) {\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n@@ -3329,1 +3570,1 @@\n-    return TypeInstPtr::make(TypePtr::BotPTR, klass, klass_is_exact, NULL, 0);\n+    return TypeInstPtr::make(TypePtr::BotPTR, klass, klass_is_exact, NULL, Offset(0));\n@@ -3331,2 +3572,14 @@\n-    \/\/ Element is an object array. Recursively call ourself.\n-    const TypeOopPtr *etype = TypeOopPtr::make_from_klass_common(klass->as_obj_array_klass()->element_klass(), false, try_for_exact);\n+    \/\/ Element is an object or inline type array. Recursively call ourself.\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ false, try_for_exact);\n+    if (etype->is_inlinetypeptr()) {\n+      etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n+    \/\/ Determine null-free\/flattened properties\n+    const TypeOopPtr* exact_etype = etype;\n+    if (etype->can_be_inline_type()) {\n+      \/\/ Use exact type if element can be an inline type\n+      exact_etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ true, \/* try_for_exact= *\/ true);\n+    }\n+    bool not_null_free = !exact_etype->can_be_inline_type();\n+    bool not_flat = !UseFlatArray || not_null_free || (exact_etype->is_inlinetypeptr() && !exact_etype->inline_klass()->flatten_array());\n+\n@@ -3334,1 +3587,1 @@\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, false, not_flat, not_null_free);\n@@ -3338,1 +3591,1 @@\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, xk, 0);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, xk, Offset(0));\n@@ -3343,1 +3596,2 @@\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS,\n+                                        \/* stable= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3346,1 +3600,6 @@\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, 0);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, Offset(0));\n+    return arr;\n+  } else if (klass->is_flat_array_klass()) {\n+    ciInlineKlass* vk = klass->as_array_klass()->element_klass()->as_inline_klass();\n+    const TypeAry* arr0 = TypeAry::make(TypeInlineType::make(vk), TypeInt::POS);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, Offset(0));\n@@ -3362,2 +3621,2 @@\n-  if (klass->is_instance_klass()) {\n-    \/\/ Element is an instance\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n+    \/\/ Element is an instance or inline type\n@@ -3367,1 +3626,1 @@\n-      return TypeInstPtr::make(TypePtr::NotNull, klass, true, NULL, 0);\n+      return TypeInstPtr::make(TypePtr::NotNull, klass, true, NULL, Offset(0));\n@@ -3371,3 +3630,8 @@\n-    const TypeOopPtr *etype =\n-      TypeOopPtr::make_from_klass_raw(klass->as_obj_array_klass()->element_klass());\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass());\n+    bool null_free = false;\n+    if (etype->is_inlinetypeptr()) {\n+      null_free = true;\n+      etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()),\n+                                        \/* stable= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ !null_free);\n@@ -3378,1 +3642,1 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3380,1 +3644,1 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3384,3 +3648,3 @@\n-    const Type* etype =\n-      (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const Type* etype = (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()),\n+                                        \/* stable= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3390,1 +3654,12 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n+    } else {\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n+    }\n+  } else if (klass->is_flat_array_klass()) {\n+    ciInlineKlass* vk = klass->as_array_klass()->element_klass()->as_inline_klass();\n+    const TypeAry* arr0 = TypeAry::make(TypeInlineType::make(vk), TypeInt::make(o->as_array()->length()));\n+    \/\/ We used to pass NotNull in here, asserting that the sub-arrays\n+    \/\/ are all not-null.  This is not true in generally, as code can\n+    \/\/ slam NULLs down in the subarrays.\n+    if (make_constant) {\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3392,1 +3667,1 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3403,1 +3678,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -3405,1 +3680,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -3498,6 +3773,1 @@\n-  switch( _offset ) {\n-  case OffsetTop: st->print(\"+top\"); break;\n-  case OffsetBot: st->print(\"+any\"); break;\n-  case         0: break;\n-  default:        st->print(\"+%d\",_offset); break;\n-  }\n+  _offset.dump2(st);\n@@ -3520,1 +3790,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -3612,7 +3882,10 @@\n-TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, bool xk, ciObject* o, int off,\n-                         int instance_id, const TypePtr* speculative, int inline_depth)\n-  : TypeOopPtr(InstPtr, ptr, k, xk, o, off, instance_id, speculative, inline_depth),\n-    _name(k->name()) {\n-   assert(k != NULL &&\n-          (k->is_loaded() || o == NULL),\n-          \"cannot have constants with non-loaded klass\");\n+TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, bool xk, ciObject* o, Offset off,\n+                         bool flatten_array, int instance_id, const TypePtr* speculative,\n+                         int inline_depth)\n+  : TypeOopPtr(InstPtr, ptr, k, xk, o, off, Offset::bottom, instance_id, speculative, inline_depth),\n+    _name(k->name()), _flatten_array(flatten_array) {\n+  assert(k != NULL &&\n+         (k->is_loaded() || o == NULL),\n+         \"cannot have constants with non-loaded klass\");\n+  assert(!klass()->flatten_array() || flatten_array, \"Should be flat in array\");\n+  assert(!flatten_array || can_be_inline_type(), \"Only inline types can be flat in array\");\n@@ -3626,1 +3899,2 @@\n-                                     int offset,\n+                                     Offset offset,\n+                                     bool flatten_array,\n@@ -3647,0 +3921,3 @@\n+  \/\/ Check if this type is known to be flat in arrays\n+  flatten_array = flatten_array || k->flatten_array();\n+\n@@ -3649,1 +3926,1 @@\n-    (TypeInstPtr*)(new TypeInstPtr(ptr, k, xk, o ,offset, instance_id, speculative, inline_depth))->hashcons();\n+    (TypeInstPtr*)(new TypeInstPtr(ptr, k, xk, o, offset, flatten_array, instance_id, speculative, inline_depth))->hashcons();\n@@ -3682,1 +3959,1 @@\n-  return make(ptr, klass(), klass_is_exact(), const_oop(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, klass(), klass_is_exact(), const_oop(), _offset, _flatten_array, _instance_id, _speculative, _inline_depth);\n@@ -3693,1 +3970,1 @@\n-  return make(ptr(), klass(), klass_is_exact, const_oop(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), klass(), klass_is_exact, const_oop(), _offset, _flatten_array, _instance_id, _speculative, _inline_depth);\n@@ -3699,1 +3976,1 @@\n-  return make(_ptr, klass(), _klass_is_exact, const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), _klass_is_exact, const_oop(), _offset, _flatten_array, instance_id, _speculative, _inline_depth);\n@@ -3706,1 +3983,1 @@\n-    int off = meet_offset(tinst->offset());\n+    Offset off = meet_offset(tinst->offset());\n@@ -3731,1 +4008,1 @@\n-      else if (loaded->ptr() == TypePtr::AnyNull) { return TypeInstPtr::make(ptr, unloaded->klass(), false, NULL, off, instance_id, speculative, depth); }\n+      else if (loaded->ptr() == TypePtr::AnyNull) { return TypeInstPtr::make(ptr, unloaded->klass(), false, NULL, off, false, instance_id, speculative, depth); }\n@@ -3784,1 +4061,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -3795,2 +4072,2 @@\n-      if (klass()->equals(ciEnv::current()->Object_klass()) && !klass_is_exact()) {\n-        return TypeAryPtr::make(ptr, tp->ary(), tp->klass(), tp->klass_is_exact(), offset, instance_id, speculative, depth);\n+      if (klass()->equals(ciEnv::current()->Object_klass()) && !klass_is_exact() && !flatten_array()) {\n+        return TypeAryPtr::make(ptr, tp->ary(), tp->klass(), tp->klass_is_exact(), offset, tp->field_offset(), instance_id, speculative, depth);\n@@ -3801,1 +4078,1 @@\n-        return TypeInstPtr::make( ptr, ciEnv::current()->Object_klass(), false, NULL, offset, instance_id, speculative, depth);\n+        return TypeInstPtr::make( ptr, ciEnv::current()->Object_klass(), false, NULL, offset, false, instance_id, speculative, depth);\n@@ -3813,1 +4090,1 @@\n-        if (klass()->equals(ciEnv::current()->Object_klass()) && !klass_is_exact()) {\n+        if (klass()->equals(ciEnv::current()->Object_klass()) && !klass_is_exact() && !flatten_array()) {\n@@ -3816,1 +4093,1 @@\n-                                  tp->ary(), tp->klass(), tp->klass_is_exact(), offset, instance_id, speculative, depth);\n+                                  tp->ary(), tp->klass(), tp->klass_is_exact(), offset, tp->field_offset(), instance_id, speculative, depth);\n@@ -3824,1 +4101,1 @@\n-      return make(ptr, ciEnv::current()->Object_klass(), false, NULL, offset, instance_id, speculative, depth);\n+      return make(ptr, ciEnv::current()->Object_klass(), false, NULL, offset, false, instance_id, speculative, depth);\n@@ -3832,1 +4109,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -3841,1 +4118,1 @@\n-                  (ptr == Constant ? const_oop() : NULL), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : NULL), offset, flatten_array(), instance_id, speculative, depth);\n@@ -3857,1 +4134,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -3869,1 +4146,1 @@\n-                  (ptr == Constant ? const_oop() : NULL), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : NULL), offset, flatten_array(), instance_id, speculative, depth);\n@@ -3897,1 +4174,1 @@\n-    int off = meet_offset( tinst->offset() );\n+    Offset off = meet_offset( tinst->offset() );\n@@ -3907,2 +4184,3 @@\n-    if (ptr != Constant && klass()->equals(tinst->klass()) && klass_is_exact() == tinst->klass_is_exact()) {\n-      return make(ptr, klass(), klass_is_exact(), NULL, off, instance_id, speculative, depth);\n+    if (ptr != Constant && klass()->equals(tinst->klass()) && klass_is_exact() == tinst->klass_is_exact() &&\n+        flatten_array() == tinst->flatten_array()) {\n+      return make(ptr, klass(), klass_is_exact(), NULL, off, flatten_array(), instance_id, speculative, depth);\n@@ -3916,0 +4194,2 @@\n+    bool tinst_flatten_array = tinst->flatten_array();\n+    bool this_flatten_array  = this->flatten_array();\n@@ -3938,0 +4218,3 @@\n+      tmp2 = tinst_flatten_array;\n+      tinst_flatten_array = this_flatten_array;\n+      this_flatten_array = tmp2;\n@@ -3949,0 +4232,1 @@\n+      bool flat_array;\n@@ -3957,0 +4241,1 @@\n+        flat_array = below_centerline(ptr) ? tinst_flatten_array    : this_flatten_array;\n@@ -3963,0 +4248,1 @@\n+        flat_array = above_centerline(ptr) ? tinst_flatten_array : false;\n@@ -3972,1 +4258,1 @@\n-      return make(ptr, k, xk, o, off, instance_id, speculative, depth);\n+      return make(ptr, k, xk, o, off, flat_array, instance_id, speculative, depth);\n@@ -4004,1 +4290,2 @@\n-    if( tinst_klass->equals(this_klass) ) {\n+    bool flat_array = false;\n+    if (tinst_klass->equals(this_klass)) {\n@@ -4007,1 +4294,2 @@\n-    } else if( !tinst_xk && this_klass->is_subtype_of( tinst_klass ) ) {\n+      flat_array = below_centerline(ptr) ? (this_flatten_array && tinst_flatten_array) : (this_flatten_array || tinst_flatten_array);\n+    } else if(!tinst_xk && this_klass->is_subtype_of(tinst_klass) && (!tinst_flatten_array || this_flatten_array)) {\n@@ -4010,1 +4298,2 @@\n-    } else if( !this_xk && tinst_klass->is_subtype_of( this_klass ) ) {\n+      flat_array = this_flatten_array;\n+    } else if(!this_xk && tinst_klass->is_subtype_of(this_klass) && (!this_flatten_array || tinst_flatten_array)) {\n@@ -4013,0 +4302,1 @@\n+      flat_array = tinst_flatten_array;\n@@ -4015,2 +4305,2 @@\n-    if( subtype ) {\n-      if( above_centerline(ptr) ) { \/\/ both are up?\n+    if (subtype) {\n+      if (above_centerline(ptr)) { \/\/ both are up?\n@@ -4019,1 +4309,2 @@\n-      } else if( above_centerline(this ->_ptr) && !above_centerline(tinst->_ptr) ) {\n+        this_flatten_array = tinst_flatten_array = flat_array;\n+      } else if (above_centerline(this ->_ptr) && !above_centerline(tinst->_ptr)) {\n@@ -4022,1 +4313,2 @@\n-      } else if( above_centerline(tinst->_ptr) && !above_centerline(this ->_ptr) ) {\n+        this_flatten_array = tinst_flatten_array;\n+      } else if (above_centerline(tinst->_ptr) && !above_centerline(this ->_ptr)) {\n@@ -4025,0 +4317,1 @@\n+        tinst_flatten_array = this_flatten_array;\n@@ -4027,0 +4320,1 @@\n+        this_flatten_array = flat_array;\n@@ -4049,1 +4343,1 @@\n-      return make(ptr, this_klass, this_xk, o, off, instance_id, speculative, depth);\n+      return make(ptr, this_klass, this_xk, o, off, this_flatten_array, instance_id, speculative, depth);\n@@ -4061,1 +4355,1 @@\n-    return make(ptr, k, false, NULL, off, instance_id, speculative, depth);\n+    return make(ptr, k, false, NULL, off, false, instance_id, speculative, depth);\n@@ -4064,0 +4358,21 @@\n+  case InlineType: {\n+    const TypeInlineType* tv = t->is_inlinetype();\n+    if (above_centerline(ptr())) {\n+      if (tv->inline_klass()->is_subtype_of(_klass)) {\n+        return t;\n+      } else {\n+        return TypeInstPtr::NOTNULL;\n+      }\n+    } else {\n+      PTR ptr = this->_ptr;\n+      if (ptr == Constant) {\n+        ptr = NotNull;\n+      }\n+      if (tv->inline_klass()->is_subtype_of(_klass)) {\n+        return TypeInstPtr::make(ptr, _klass);\n+      } else {\n+        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass());\n+      }\n+    }\n+  }\n+\n@@ -4076,1 +4391,0 @@\n-\n@@ -4085,1 +4399,1 @@\n-  return new TypeInstPtr(dual_ptr(), klass(), klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeInstPtr(dual_ptr(), klass(), klass_is_exact(), const_oop(), dual_offset(), flatten_array(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -4094,0 +4408,1 @@\n+    flatten_array() == p->flatten_array() &&\n@@ -4100,1 +4415,1 @@\n-  int hash = java_add((jint)klass()->hash(), (jint)TypeOopPtr::hash());\n+  int hash = java_add(java_add((jint)klass()->hash(), (jint)TypeOopPtr::hash()), (jint)flatten_array());\n@@ -4140,5 +4455,1 @@\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      st->print(\"+any\");\n-    else if( _offset == OffsetTop ) st->print(\"+unknown\");\n-    else st->print(\"+%d\", _offset);\n-  }\n+  _offset.dump2(st);\n@@ -4147,0 +4458,5 @@\n+\n+  if (flatten_array() && !klass()->is_inlinetype()) {\n+    st->print(\" (flatten array)\");\n+  }\n+\n@@ -4159,1 +4475,1 @@\n-  return make(_ptr, klass(), klass_is_exact(), const_oop(), xadd_offset(offset),\n+  return make(_ptr, klass(), klass_is_exact(), const_oop(), xadd_offset(offset), flatten_array(),\n@@ -4168,1 +4484,1 @@\n-  return make(_ptr, klass(), klass_is_exact(), const_oop(), _offset,\n+  return make(_ptr, klass(), klass_is_exact(), const_oop(), _offset, flatten_array(),\n@@ -4176,1 +4492,1 @@\n-  return make(_ptr, klass(), klass_is_exact(), const_oop(), _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, klass(), klass_is_exact(), const_oop(), _offset, flatten_array(), _instance_id, _speculative, depth);\n@@ -4181,1 +4497,5 @@\n-  return make(_ptr, klass(), klass_is_exact(), const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), klass_is_exact(), const_oop(), _offset, flatten_array(), instance_id, _speculative, _inline_depth);\n+}\n+\n+const TypeInstPtr *TypeInstPtr::cast_to_flatten_array() const {\n+  return make(_ptr, klass(), klass_is_exact(), const_oop(), _offset, true, _instance_id, _speculative, _inline_depth);\n@@ -4184,0 +4504,1 @@\n+\n@@ -4196,0 +4517,1 @@\n+const TypeAryPtr *TypeAryPtr::INLINES;\n@@ -4198,1 +4520,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4204,1 +4526,1 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, NULL, ary, k, xk, offset, instance_id, false, speculative, inline_depth))->hashcons();\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, NULL, ary, k, xk, offset, field_offset, instance_id, false, speculative, inline_depth))->hashcons();\n@@ -4208,1 +4530,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4216,1 +4538,1 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, field_offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n@@ -4222,1 +4544,1 @@\n-  return make(ptr, const_oop(), _ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, const_oop(), _ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4230,1 +4552,7 @@\n-  return make(ptr(), const_oop(), _ary, klass(), klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+\n+  const TypeAry* new_ary = _ary;\n+  if (klass() != NULL && klass()->is_obj_array_klass() && klass_is_exact) {\n+    \/\/ An object array can't be flat or null-free if the klass is exact\n+    new_ary = TypeAry::make(elem(), size(), is_stable(), \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n+  }\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4236,1 +4564,1 @@\n-  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4292,2 +4620,36 @@\n-  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable(), is_not_flat(), is_not_null_free());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+\/\/-------------------------------cast_to_not_flat------------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_flat(bool not_flat) const {\n+  if (not_flat == is_not_flat()) {\n+    return this;\n+  }\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), not_flat, is_not_null_free());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+\/\/-------------------------------cast_to_not_null_free-------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_null_free(bool not_null_free) const {\n+  if (not_null_free == is_not_null_free()) {\n+    return this;\n+  }\n+  \/\/ Not null free implies not flat\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), not_null_free ? true : is_not_flat(), not_null_free);\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+\/\/---------------------------------update_properties---------------------------\n+const TypeAryPtr* TypeAryPtr::update_properties(const TypeAryPtr* from) const {\n+  if ((from->is_flat()          && is_not_flat()) ||\n+      (from->is_not_flat()      && is_flat()) ||\n+      (from->is_null_free()     && is_not_null_free()) ||\n+      (from->is_not_null_free() && is_null_free())) {\n+    return NULL; \/\/ Inconsistent properties\n+  } else if (from->is_not_null_free()) {\n+    return cast_to_not_null_free(); \/\/ Implies not flat\n+  } else if (from->is_not_flat()) {\n+    return cast_to_not_flat();\n+  }\n+  return this;\n@@ -4309,1 +4671,1 @@\n-  const TypeAry* new_ary = TypeAry::make(elem, size(), stable);\n+  const TypeAry* new_ary = TypeAry::make(elem, size(), stable, is_not_flat(), is_not_null_free());\n@@ -4311,1 +4673,1 @@\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4331,2 +4693,2 @@\n-  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n+  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable(), is_not_flat(), is_not_null_free());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n@@ -4341,1 +4703,2 @@\n-    TypeOopPtr::eq(p);  \/\/ Check sub-parts\n+    TypeOopPtr::eq(p) &&\/\/ Check sub-parts\n+    _field_offset == p->_field_offset;\n@@ -4347,1 +4710,1 @@\n-  return (intptr_t)_ary + TypeOopPtr::hash();\n+  return (intptr_t)_ary + TypeOopPtr::hash() + _field_offset.get();\n@@ -4380,1 +4743,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4389,1 +4752,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4403,1 +4766,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4419,1 +4782,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4431,1 +4794,2 @@\n-    int off = meet_offset(tap->offset());\n+    Offset off = meet_offset(tap->offset());\n+    Offset field_off = meet_field_offset(tap->field_offset());\n@@ -4449,1 +4813,1 @@\n-        tary = TypeAry::make(Type::BOTTOM, tary->_size, tary->_stable);\n+        tary = TypeAry::make(Type::BOTTOM, tary->_size, tary->_stable, tary->_not_flat, tary->_not_null_free);\n@@ -4456,1 +4820,1 @@\n-          tap->_klass != NULL  && this->_klass != NULL   &&\n+          tap->_klass != NULL && this->_klass != NULL &&\n@@ -4459,1 +4823,1 @@\n-           \/\/ 'tap'  is exact and super or unrelated:\n+           \/\/ 'tap' is exact and super or unrelated:\n@@ -4463,2 +4827,15 @@\n-      if (above_centerline(ptr) || (tary->_elem->make_ptr() && above_centerline(tary->_elem->make_ptr()->_ptr))) {\n-        tary = TypeAry::make(Type::BOTTOM, tary->_size, tary->_stable);\n+      if (above_centerline(ptr) || (tary->_elem->make_ptr() && above_centerline(tary->_elem->make_ptr()->_ptr)) ||\n+          tary->_elem->isa_inlinetype()) {\n+        tary = TypeAry::make(Type::BOTTOM, tary->_size, tary->_stable, tary->_not_flat, tary->_not_null_free);\n+      }\n+      return make(NotNull, NULL, tary, lazy_klass, false, off, field_off, InstanceBot, speculative, depth);\n+    } else if (klass() != NULL && tap->klass() != NULL && klass()->is_flat_array_klass() != tap->klass()->is_flat_array_klass()) {\n+      \/\/ Meeting flattened inline type array with non-flattened array. Adjust (field) offset accordingly.\n+      if (tary->_elem->isa_inlinetype()) {\n+        \/\/ Result is flattened\n+        off = Offset(is_flat() ? offset() : tap->offset());\n+        field_off = is_flat() ? field_offset() : tap->field_offset();\n+      } else if (tary->_elem->make_oopptr() != NULL && tary->_elem->make_oopptr()->isa_instptr() && below_centerline(ptr)) {\n+        \/\/ Result is non-flattened\n+        off = Offset(flattened_offset()).meet(Offset(tap->flattened_offset()));\n+        field_off = Offset::bottom;\n@@ -4466,1 +4843,0 @@\n-      return make(NotNull, NULL, tary, lazy_klass, false, off, InstanceBot, speculative, depth);\n@@ -4479,1 +4855,1 @@\n-      return make(ptr, const_oop(), tary, lazy_klass, xk, off, instance_id, speculative, depth);\n+      return make(ptr, const_oop(), tary, lazy_klass, xk, off, field_off, instance_id, speculative, depth);\n@@ -4498,1 +4874,1 @@\n-      return TypeAryPtr::make(ptr, o, tary, lazy_klass, xk, off, instance_id, speculative, depth);\n+      return TypeAryPtr::make(ptr, o, tary, lazy_klass, xk, off, field_off, instance_id, speculative, depth);\n@@ -4507,1 +4883,1 @@\n-      return TypeAryPtr::make(ptr, NULL, tary, lazy_klass, xk, off, instance_id, speculative, depth);\n+      return TypeAryPtr::make(ptr, NULL, tary, lazy_klass, xk, off, field_off, instance_id, speculative, depth);\n@@ -4515,1 +4891,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4526,2 +4902,2 @@\n-      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && !tp->klass_is_exact()) {\n-        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && !tp->klass_is_exact() && !tp->flatten_array()) {\n+        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4532,1 +4908,1 @@\n-        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), false, NULL,offset, instance_id, speculative, depth);\n+        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), false, NULL, offset, false, instance_id, speculative, depth);\n@@ -4544,1 +4920,1 @@\n-        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && !tp->klass_is_exact()) {\n+        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && !tp->klass_is_exact() && !tp->flatten_array()) {\n@@ -4547,1 +4923,1 @@\n-                      _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                      _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4555,1 +4931,1 @@\n-      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), false, NULL,offset, instance_id, speculative, depth);\n+      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), false, NULL, offset, false, instance_id, speculative, depth);\n@@ -4559,0 +4935,13 @@\n+\n+  case InlineType: {\n+    const TypeInlineType* tv = t->is_inlinetype();\n+    if (above_centerline(ptr())) {\n+      return TypeInstPtr::NOTNULL;\n+    } else {\n+      PTR ptr = this->_ptr;\n+      if (ptr == Constant) {\n+        ptr = NotNull;\n+      }\n+      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass());\n+    }\n+  }\n@@ -4566,1 +4955,10 @@\n-  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(),_klass, _klass_is_exact, dual_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(), _klass, _klass_is_exact, dual_offset(), dual_field_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+}\n+\n+Type::Offset TypeAryPtr::meet_field_offset(const Type::Offset offset) const {\n+  return _field_offset.meet(offset);\n+}\n+\n+\/\/------------------------------dual_offset------------------------------------\n+Type::Offset TypeAryPtr::dual_field_offset() const {\n+  return _field_offset.dual();\n@@ -4603,1 +5001,6 @@\n-  if( _offset != 0 ) {\n+  if (is_flat()) {\n+    st->print(\"(\");\n+    _field_offset.dump2(st);\n+    st->print(\")\");\n+  }\n+  if (offset() != 0) {\n@@ -4605,3 +5008,3 @@\n-    if( _offset == OffsetTop )       st->print(\"+undefined\");\n-    else if( _offset == OffsetBot )  st->print(\"+any\");\n-    else if( _offset < header_size ) st->print(\"+%d\", _offset);\n+    if( _offset == Offset::top )       st->print(\"+undefined\");\n+    else if( _offset == Offset::bottom )  st->print(\"+any\");\n+    else if( offset() < header_size ) st->print(\"+%d\", offset());\n@@ -4612,1 +5015,1 @@\n-      st->print(\"[%d]\", (_offset - array_base)\/elem_size);\n+      st->print(\"[%d]\", (offset() - array_base)\/elem_size);\n@@ -4633,1 +5036,1 @@\n-  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _instance_id, add_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _field_offset, _instance_id, add_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n@@ -4641,1 +5044,13 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, NULL, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, NULL, _inline_depth, _is_autobox_cache);\n+}\n+\n+const Type* TypeAryPtr::cleanup_speculative() const {\n+  if (speculative() == NULL) {\n+    return this;\n+  }\n+  \/\/ Keep speculative part if it contains information about flat-\/nullability\n+  const TypeAryPtr* spec_aryptr = speculative()->isa_aryptr();\n+  if (spec_aryptr != NULL && (spec_aryptr->is_not_flat() || spec_aryptr->is_not_null_free())) {\n+    return this;\n+  }\n+  return TypeOopPtr::cleanup_speculative();\n@@ -4648,1 +5063,52 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, depth, _is_autobox_cache);\n+}\n+\n+const TypeAryPtr* TypeAryPtr::with_field_offset(int offset) const {\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, Offset(offset), _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+const TypePtr* TypeAryPtr::add_field_offset_and_offset(intptr_t offset) const {\n+  int adj = 0;\n+  if (offset != Type::OffsetBot && offset != Type::OffsetTop) {\n+    const Type* elemtype = elem();\n+    if (elemtype->isa_inlinetype()) {\n+      if (_offset.get() != OffsetBot && _offset.get() != OffsetTop) {\n+        adj = _offset.get();\n+        offset += _offset.get();\n+      }\n+      uint header = arrayOopDesc::base_offset_in_bytes(T_OBJECT);\n+      if (_field_offset.get() != OffsetBot && _field_offset.get() != OffsetTop) {\n+        offset += _field_offset.get();\n+        if (_offset.get() == OffsetBot || _offset.get() == OffsetTop) {\n+          offset += header;\n+        }\n+      }\n+      if (offset >= (intptr_t)header || offset < 0) {\n+        \/\/ Try to get the field of the inline type array element we are pointing to\n+        ciKlass* arytype_klass = klass();\n+        ciFlatArrayKlass* vak = arytype_klass->as_flat_array_klass();\n+        ciInlineKlass* vk = vak->element_klass()->as_inline_klass();\n+        int shift = vak->log2_element_size();\n+        int mask = (1 << shift) - 1;\n+        intptr_t field_offset = ((offset - header) & mask);\n+        ciField* field = vk->get_field_by_offset(field_offset + vk->first_field_offset(), false);\n+        if (field == NULL) {\n+          \/\/ This may happen with nested AddP(base, AddP(base, base, offset), longcon(16))\n+          return add_offset(offset);\n+        } else {\n+          return with_field_offset(field_offset)->add_offset(offset - field_offset - adj);\n+        }\n+      }\n+    }\n+  }\n+  return add_offset(offset - adj);\n+}\n+\n+\/\/ Return offset incremented by field_offset for flattened inline type arrays\n+const int TypeAryPtr::flattened_offset() const {\n+  int offset = _offset.get();\n+  if (offset != Type::OffsetBot && offset != Type::OffsetTop &&\n+      _field_offset != Offset::bottom && _field_offset != Offset::top) {\n+    offset += _field_offset.get();\n+  }\n+  return offset;\n@@ -4653,1 +5119,1 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth);\n@@ -4658,0 +5124,1 @@\n+\n@@ -4746,1 +5213,0 @@\n-\n@@ -4752,0 +5218,3 @@\n+  case InlineType:\n+    return t->xmeet(this);\n+\n@@ -4830,1 +5299,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -4850,1 +5319,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -4852,1 +5321,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -4903,1 +5372,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4929,1 +5398,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4962,1 +5431,1 @@\n-  switch( _offset ) {\n+  switch (offset()) {\n@@ -4966,1 +5435,1 @@\n-  default:        st->print(\"+%d\",_offset); break;\n+  default:        st->print(\"+%d\",offset()); break;\n@@ -4976,1 +5445,1 @@\n-TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, int offset):\n+TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, Offset offset):\n@@ -4981,1 +5450,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -4984,1 +5453,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -4989,1 +5458,1 @@\n-const TypeMetadataPtr *TypeMetadataPtr::make(PTR ptr, ciMetadata* m, int offset) {\n+const TypeMetadataPtr* TypeMetadataPtr::make(PTR ptr, ciMetadata* m, Offset offset) {\n@@ -5003,2 +5472,5 @@\n-TypeKlassPtr::TypeKlassPtr( PTR ptr, ciKlass* klass, int offset )\n-  : TypePtr(KlassPtr, ptr, offset), _klass(klass), _klass_is_exact(ptr == Constant) {\n+TypeKlassPtr::TypeKlassPtr(PTR ptr, ciKlass* klass, Offset offset, bool flatten_array, bool not_flat, bool not_null_free)\n+  : TypePtr(KlassPtr, ptr, offset), _klass(klass), _klass_is_exact(ptr == Constant),\n+    _flatten_array(flatten_array), _not_flat(not_flat), _not_null_free(not_null_free) {\n+  assert(!klass->flatten_array() || flatten_array, \"Should be flat in array\");\n+  assert(!flatten_array || can_be_inline_type(), \"Only inline types can be flat in array\");\n@@ -5009,7 +5481,5 @@\n-const TypeKlassPtr *TypeKlassPtr::make( PTR ptr, ciKlass* k, int offset ) {\n-  assert( k != NULL, \"Expect a non-NULL klass\");\n-  assert(k->is_instance_klass() || k->is_array_klass(), \"Incorrect type of klass oop\");\n-  TypeKlassPtr *r =\n-    (TypeKlassPtr*)(new TypeKlassPtr(ptr, k, offset))->hashcons();\n-\n-  return r;\n+const TypeKlassPtr* TypeKlassPtr::make(PTR ptr, ciKlass* k, Offset offset, bool flatten_array, bool not_flat, bool not_null_free) {\n+  assert(k == NULL || k->is_instance_klass() || k->is_array_klass(), \"Incorrect type of klass oop\");\n+  \/\/ Check if this type is known to be flat in arrays\n+  flatten_array = flatten_array || k->flatten_array();\n+  return (TypeKlassPtr*)(new TypeKlassPtr(ptr, k, offset, flatten_array, not_flat, not_null_free))->hashcons();\n@@ -5022,3 +5492,2 @@\n-  return\n-    klass()->equals(p->klass()) &&\n-    TypePtr::eq(p);\n+  return klass() == p->klass() && TypePtr::eq(p) && flatten_array() == p->flatten_array() &&\n+      is_not_flat() == p->is_not_flat() && is_not_null_free() == p->is_not_null_free();\n@@ -5030,1 +5499,2 @@\n-  return java_add((jint)klass()->hash(), (jint)TypePtr::hash());\n+  return java_add(java_add(java_add(java_add(klass() != NULL ? klass()->hash() : (jint)0, (jint)TypePtr::hash()),\n+      (jint)flatten_array()), (jint)is_not_flat()), (jint)is_not_null_free());\n@@ -5039,1 +5509,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -5051,1 +5521,1 @@\n-    if (!empty() && ktkp != NULL && ktkp->klass()->is_loaded() && ktkp->klass()->is_interface())\n+    if (!empty() && ktkp != NULL && ktkp->is_loaded() && ktkp->klass()->is_interface())\n@@ -5074,1 +5544,0 @@\n-  const TypeInstPtr *tinst;\n@@ -5082,3 +5551,8 @@\n-  if ((tinst = el->isa_instptr()) != NULL) {\n-    \/\/ Compute array klass from element klass\n-    k_ary = ciObjArrayKlass::make(tinst->klass());\n+  if (el->isa_instptr()) {\n+    \/\/ Compute object array klass from element klass\n+    k_ary = ciArrayKlass::make(el->is_oopptr()->klass());\n+  } else if (el->isa_inlinetype()) {\n+    \/\/ If element type is TypeInlineType::BOTTOM, inline_klass() will be null.\n+    if (el->inline_klass() != NULL) {\n+      k_ary = ciArrayKlass::make(el->inline_klass());\n+    }\n@@ -5149,1 +5623,1 @@\n-        _offset != 0 && _offset != arrayOopDesc::length_offset_in_bytes()) {\n+        offset() != 0 && offset() != arrayOopDesc::length_offset_in_bytes()) {\n@@ -5160,1 +5634,1 @@\n-  return make( _ptr, klass(), xadd_offset(offset) );\n+  return make(_ptr, klass(), xadd_offset(offset), flatten_array(), is_not_flat(), is_not_null_free());\n@@ -5167,1 +5641,1 @@\n-  return make(ptr, _klass, _offset);\n+  return make(ptr, _klass, _offset, _flatten_array, _not_flat, _not_null_free);\n@@ -5174,1 +5648,1 @@\n-  return make(klass_is_exact ? Constant : NotNull, _klass, _offset);\n+  return make(klass_is_exact ? Constant : NotNull, _klass, _offset, _flatten_array, _not_flat, _not_null_free);\n@@ -5183,0 +5657,1 @@\n+  assert(k != NULL, \"klass should not be NULL\");\n@@ -5184,1 +5659,0 @@\n-  \/\/return TypeInstPtr::make(TypePtr::NotNull, k, xk, NULL, 0);\n@@ -5188,0 +5662,7 @@\n+  if (flatten_array() && !klass()->is_inlinetype()) {\n+    toop = toop->is_instptr()->cast_to_flatten_array();\n+  } else if (is_not_null_free()) {\n+    toop = toop->is_aryptr()->cast_to_not_null_free();\n+  } else if (is_not_flat()) {\n+    toop = toop->is_aryptr()->cast_to_not_flat();\n+  }\n@@ -5222,1 +5703,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5230,1 +5711,1 @@\n-      return make( ptr, klass(), offset );\n+      return make(ptr, klass(), offset, flatten_array(), is_not_flat(), is_not_null_free());\n@@ -5263,1 +5744,1 @@\n-    int  off     = meet_offset(tkls->offset());\n+    Offset  off  = meet_offset(tkls->offset());\n@@ -5266,0 +5747,8 @@\n+    if (klass() == NULL || tkls->klass() == NULL) {\n+      ciKlass* k = NULL;\n+      if (ptr == Constant) {\n+        k = (klass() == NULL) ? tkls->klass() : klass();\n+      }\n+      return make(ptr, k, off);\n+    }\n+\n@@ -5270,2 +5759,2 @@\n-    if( ptr != Constant && tkls->klass()->equals(klass()) ) {\n-      return make( ptr, klass(), off );\n+    if (ptr != Constant && tkls->klass()->equals(klass()) && flatten_array() == tkls->flatten_array() && is_not_flat() == tkls->is_not_flat() && is_not_null_free() && tkls->is_not_null_free()) {\n+      return make(ptr, klass(), off, flatten_array(), is_not_flat(), is_not_null_free());\n@@ -5279,0 +5768,3 @@\n+    bool flatten_array = below_centerline(ptr) ? (this->flatten_array() && tkls->flatten_array()) : (this->flatten_array() || tkls->flatten_array());\n+    bool is_not_flat = this->is_not_flat() && tkls->is_not_flat();\n+    bool is_not_null_free = this->is_not_null_free() && tkls->is_not_null_free();\n@@ -5306,1 +5798,1 @@\n-      return make( ptr, this_klass, off );\n+      return make(ptr, this_klass, off, flatten_array, is_not_flat, is_not_null_free);\n@@ -5315,1 +5807,1 @@\n-    return   make( ptr, k, off );\n+    return make(ptr, k, off, false, is_not_flat, is_not_null_free);\n@@ -5325,1 +5817,1 @@\n-  return new TypeKlassPtr( dual_ptr(), klass(), dual_offset() );\n+  return new TypeKlassPtr(dual_ptr(), klass(), dual_offset(), flatten_array(), !is_not_flat(), !is_not_null_free());\n@@ -5331,1 +5823,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -5333,1 +5825,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -5356,2 +5848,2 @@\n-      const char *name = klass()->name()->as_utf8();\n-      if( name ) {\n+      if (klass() != NULL) {\n+        const char* name = klass()->name()->as_utf8();\n@@ -5360,1 +5852,1 @@\n-        ShouldNotReachHere();\n+        st->print(\"klass BOTTOM\");\n@@ -5373,5 +5865,4 @@\n-\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      { st->print(\"+any\"); }\n-    else if( _offset == OffsetTop ) { st->print(\"+unknown\"); }\n-    else                            { st->print(\"+%d\", _offset); }\n+  if (Verbose) {\n+    if (_flatten_array) st->print(\":flatten array\");\n+    if (_not_flat) st->print(\":not flat\");\n+    if (_not_null_free) st->print(\":not null free\");\n@@ -5380,0 +5871,2 @@\n+  _offset.dump2(st);\n+\n@@ -5390,2 +5883,14 @@\n-const TypeFunc *TypeFunc::make( const TypeTuple *domain, const TypeTuple *range ) {\n-  return (TypeFunc*)(new TypeFunc(domain,range))->hashcons();\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain_sig, const TypeTuple* domain_cc,\n+                               const TypeTuple *range_sig, const TypeTuple *range_cc) {\n+  return (TypeFunc*)(new TypeFunc(domain_sig, domain_cc, range_sig, range_cc))->hashcons();\n+}\n+\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain, const TypeTuple *range) {\n+  return make(domain, domain, range, range);\n+}\n+\n+\/\/------------------------------osr_domain-----------------------------\n+const TypeTuple* osr_domain() {\n+  const Type **fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n+  return TypeTuple::make(TypeFunc::Parms+1, fields);\n@@ -5395,1 +5900,1 @@\n-const TypeFunc *TypeFunc::make(ciMethod* method) {\n+const TypeFunc* TypeFunc::make(ciMethod* method, bool is_osr_compilation) {\n@@ -5397,7 +5902,20 @@\n-  const TypeFunc* tf = C->last_tf(method); \/\/ check cache\n-  if (tf != NULL)  return tf;  \/\/ The hit rate here is almost 50%.\n-  const TypeTuple *domain;\n-  if (method->is_static()) {\n-    domain = TypeTuple::make_domain(NULL, method->signature());\n-  } else {\n-    domain = TypeTuple::make_domain(method->holder(), method->signature());\n+  const TypeFunc* tf = NULL;\n+  if (!is_osr_compilation) {\n+    tf = C->last_tf(method); \/\/ check cache\n+    if (tf != NULL)  return tf;  \/\/ The hit rate here is almost 50%.\n+  }\n+  \/\/ Inline types are not passed\/returned by reference, instead each field of\n+  \/\/ the inline type is passed\/returned as an argument. We maintain two views of\n+  \/\/ the argument\/return list here: one based on the signature (with an inline\n+  \/\/ type argument\/return as a single slot), one based on the actual calling\n+  \/\/ convention (with an inline type argument\/return as a list of its fields).\n+  bool has_scalar_args = method->has_scalarized_args() && !is_osr_compilation;\n+  const TypeTuple* domain_sig = is_osr_compilation ? osr_domain() : TypeTuple::make_domain(method, false);\n+  const TypeTuple* domain_cc = has_scalar_args ? TypeTuple::make_domain(method, true) : domain_sig;\n+  ciSignature* sig = method->signature();\n+  bool has_scalar_ret = sig->return_type()->is_inlinetype() && sig->return_type()->as_inline_klass()->can_be_returned_as_fields();\n+  const TypeTuple* range_sig = TypeTuple::make_range(sig, false);\n+  const TypeTuple* range_cc = has_scalar_ret ? TypeTuple::make_range(sig, true) : range_sig;\n+  tf = TypeFunc::make(domain_sig, domain_cc, range_sig, range_cc);\n+  if (!is_osr_compilation) {\n+    C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -5405,3 +5923,0 @@\n-  const TypeTuple *range  = TypeTuple::make_range(method->signature());\n-  tf = TypeFunc::make(domain, range);\n-  C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -5442,2 +5957,4 @@\n-  return _domain == a->_domain &&\n-    _range == a->_range;\n+  return _domain_sig == a->_domain_sig &&\n+    _domain_cc == a->_domain_cc &&\n+    _range_sig == a->_range_sig &&\n+    _range_cc == a->_range_cc;\n@@ -5449,1 +5966,1 @@\n-  return (intptr_t)_domain + (intptr_t)_range;\n+  return (intptr_t)_domain_sig + (intptr_t)_domain_cc + (intptr_t)_range_sig + (intptr_t)_range_cc;\n@@ -5456,1 +5973,1 @@\n-  if( _range->cnt() <= Parms )\n+  if( _range_sig->cnt() <= Parms )\n@@ -5460,2 +5977,2 @@\n-    for (i = Parms; i < _range->cnt()-1; i++) {\n-      _range->field_at(i)->dump2(d,depth,st);\n+    for (i = Parms; i < _range_sig->cnt()-1; i++) {\n+      _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -5464,1 +5981,1 @@\n-    _range->field_at(i)->dump2(d,depth,st);\n+    _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -5473,3 +5990,3 @@\n-  if (Parms < _domain->cnt())\n-    _domain->field_at(Parms)->dump2(d,depth-1,st);\n-  for (uint i = Parms+1; i < _domain->cnt(); i++) {\n+  if (Parms < _domain_sig->cnt())\n+    _domain_sig->field_at(Parms)->dump2(d,depth-1,st);\n+  for (uint i = Parms+1; i < _domain_sig->cnt(); i++) {\n@@ -5477,1 +5994,1 @@\n-    _domain->field_at(i)->dump2(d,depth-1,st);\n+    _domain_sig->field_at(i)->dump2(d,depth-1,st);\n@@ -5497,1 +6014,1 @@\n-  if (range()->cnt() == TypeFunc::Parms) {\n+  if (range_sig()->cnt() == TypeFunc::Parms) {\n@@ -5500,1 +6017,1 @@\n-  return range()->field_at(TypeFunc::Parms)->basic_type();\n+  return range_sig()->field_at(TypeFunc::Parms)->basic_type();\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":825,"deletions":308,"binary":false,"changes":1133,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -595,1 +596,22 @@\n-  return handle == NULL ? 0 : ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)) ;\n+  if (handle == NULL) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, Universe::inline_type_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return ObjectSynchronizer::FastHashCode(THREAD, obj);\n+  }\n@@ -647,0 +669,1 @@\n+       klass->is_inline_klass() ||\n@@ -1144,1 +1167,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1147,1 +1171,1 @@\n-    size = 2;\n+    size = 3;\n@@ -1157,1 +1181,2 @@\n-      Klass* k = InstanceKlass::cast(klass)->local_interfaces()->at(index);\n+      InstanceKlass* ik = InstanceKlass::cast(klass);\n+      Klass* k = ik->local_interfaces()->at(index);\n@@ -1161,1 +1186,1 @@\n-    \/\/ All arrays implement java.lang.Cloneable and java.io.Serializable\n+    \/\/ All arrays implement java.lang.Cloneable, java.io.Serializable and java.lang.IdentityObject\n@@ -1164,0 +1189,1 @@\n+    result->obj_at_put(2, vmClasses::IdentityObject_klass()->java_mirror());\n@@ -1759,0 +1785,2 @@\n+  bool is_ctor = (method->is_object_constructor() ||\n+                  method->is_static_init_factory());\n@@ -1760,1 +1788,1 @@\n-    return (method->is_initializer() && !method->is_static());\n+    return is_ctor;\n@@ -1762,1 +1790,3 @@\n-    return  (!method->is_initializer() && !method->is_overpass());\n+    return (!is_ctor &&\n+            !method->is_class_initializer() &&\n+            !method->is_overpass());\n@@ -1825,0 +1855,2 @@\n+        assert(method->is_object_constructor() ||\n+               method->is_static_init_factory(), \"must be\");\n@@ -2107,3 +2139,1 @@\n-  if (!m->is_initializer() || m->is_static()) {\n-    method = Reflection::new_method(m, true, CHECK_NULL);\n-  } else {\n+  if (m->is_object_constructor() || m->is_static_init_factory()) {\n@@ -2111,0 +2141,2 @@\n+  } else {\n+    method = Reflection::new_method(m, true, CHECK_NULL);\n@@ -2381,0 +2413,37 @@\n+\/\/ Arrays support \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+JVM_ENTRY(jboolean, JVM_ArrayIsAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == NULL) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  return ArrayKlass::cast(k)->element_access_is_atomic();\n+JVM_END\n+\n+JVM_ENTRY(jobject, JVM_ArrayEnsureAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == NULL) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vk = FlatArrayKlass::cast(k);\n+    if (!vk->element_access_is_atomic()) {\n+      \/**\n+       * Need to decide how to implement:\n+       *\n+       * 1) Change to objArrayOop layout, therefore oop->klass() differs so\n+       * then \"<atomic>[Qfoo;\" klass needs to subclass \"[Qfoo;\" to pass through\n+       * \"checkcast\" & \"instanceof\"\n+       *\n+       * 2) Use extra header in the flatArrayOop to flag atomicity required and\n+       * possibly per instance lock structure. Said info, could be placed in\n+       * \"trailer\" rather than disturb the current arrayOop\n+       *\/\n+      Unimplemented();\n+    }\n+  }\n+  return array;\n+JVM_END\n+\n@@ -2543,1 +2612,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3515,1 +3584,1 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3535,0 +3604,1 @@\n+  objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3536,1 +3606,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":82,"deletions":13,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -58,0 +59,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -64,0 +66,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -1807,0 +1810,87 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return NULL;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return NULL;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+class CollectOops : public BasicOopIterateClosure {\n+ public:\n+  GrowableArray<Handle>* array;\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < array->length(); i++) {\n+      result_array->obj_at_put(i, array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    \/\/ Value might be oop, but JLS can't see as Object, just iterate through it...\n+    if (oh != NULL && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      array->append(oh);\n+    }\n+  }\n+\n+  void do_oop(oop* o) { add_oop(HeapAccess<>::oop_load(o)); }\n+  void do_oop(narrowOop* v) { add_oop(HeapAccess<>::oop_load(v)); }\n+};\n+\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  StackFrameStream sfs(thread, false \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, NULL, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\n@@ -2512,0 +2602,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":96,"deletions":0,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -46,0 +46,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n@@ -51,0 +53,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -202,2 +205,13 @@\n-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n-  Handle return_value;\n+  ScopeDesc* scope = chunk->at(0)->scope();\n+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n+  \/\/ In case of the return of multiple values, we must take care\n+  \/\/ of all oop return values.\n+  GrowableArray<Handle> return_oops;\n+  InlineKlass* vk = NULL;\n+  if (save_oop_result && scope->return_vt()) {\n+    vk = InlineKlass::returned_inline_klass(map);\n+    if (vk != NULL) {\n+      vk->save_oop_fields(map, return_oops);\n+      save_oop_result = false;\n+    }\n+  }\n@@ -209,1 +223,1 @@\n-    return_value = Handle(thread, result);\n+    return_oops.push(Handle(thread, result));\n@@ -216,1 +230,1 @@\n-  if (objects != NULL) {\n+  if (objects != NULL || vk != NULL) {\n@@ -221,1 +235,8 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+      if (vk != NULL) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, CHECK_AND_CLEAR_(true));\n+      }\n+      if (objects != NULL) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+        bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, CHECK_AND_CLEAR_(true));\n+      }\n@@ -225,1 +246,8 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+      if (vk != NULL) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);\n+      }\n+      if (objects != NULL) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+        bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, THREAD);\n+      }\n@@ -228,2 +256,0 @@\n-    bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();\n-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal);\n@@ -238,1 +264,1 @@\n-  if (save_oop_result) {\n+  if (save_oop_result || vk != NULL) {\n@@ -240,1 +266,2 @@\n-    deoptee.set_saved_oop_result(&map, return_value());\n+    assert(return_oops.length() == 1, \"no inline type\");\n+    deoptee.set_saved_oop_result(&map, return_oops.pop()());\n@@ -571,1 +598,1 @@\n-  \/\/ If the sender is deoptimized the we must retrieve the address of the handler\n+  \/\/ If the sender is deoptimized we must retrieve the address of the handler\n@@ -1072,0 +1099,4 @@\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* ak = FlatArrayKlass::cast(k);\n+      \/\/ Inline type array must be zeroed because not all memory is reassigned\n+      obj = ak->allocate(sv->field_size(), THREAD);\n@@ -1101,0 +1132,15 @@\n+\/\/ We're deoptimizing at the return of a call, inline type fields are\n+\/\/ in registers. When we go back to the interpreter, it will expect a\n+\/\/ reference to an inline type instance. Allocate and initialize it from\n+\/\/ the register values here.\n+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {\n+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);\n+  if (new_vt == NULL) {\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);\n+  }\n+  return_oops.clear();\n+  return_oops.push(Handle(THREAD, new_vt));\n+  return false;\n+}\n+\n@@ -1273,0 +1319,1 @@\n+  InstanceKlass* _klass;\n@@ -1277,0 +1324,1 @@\n+    _klass = NULL;\n@@ -1286,1 +1334,1 @@\n-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal) {\n+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal, int base_offset, TRAPS) {\n@@ -1295,0 +1343,8 @@\n+        if (field._type == T_INLINE_TYPE) {\n+          if (fs.is_inlined()) {\n+            \/\/ Resolve klass of flattened inline type field\n+            field._klass = InlineKlass::cast(klass->get_inline_type_field_klass(fs.index()));\n+          } else {\n+            field._type = T_OBJECT;\n+          }\n+        }\n@@ -1302,0 +1358,11 @@\n+    BasicType type = fields->at(i)._type;\n+    int offset = base_offset + fields->at(i)._offset;\n+    \/\/ Check for flattened inline type field before accessing the ScopeValue because it might not have any fields\n+    if (type == T_INLINE_TYPE) {\n+      \/\/ Recursively re-assign flattened inline type fields\n+      InstanceKlass* vk = fields->at(i)._klass;\n+      assert(vk != NULL, \"must be resolved\");\n+      offset -= InlineKlass::cast(vk)->first_field_offset(); \/\/ Adjust offset to omit oop header\n+      svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, skip_internal, offset, CHECK_0);\n+      continue; \/\/ Continue because we don't need to increment svIndex\n+    }\n@@ -1305,3 +1372,2 @@\n-    int offset = fields->at(i)._offset;\n-    BasicType type = fields->at(i)._type;\n-      case T_OBJECT: case T_ARRAY:\n+      case T_OBJECT:\n+      case T_ARRAY:\n@@ -1388,0 +1454,14 @@\n+\/\/ restore fields of an eliminated inline type array\n+void Deoptimization::reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool skip_internal, TRAPS) {\n+  InlineKlass* vk = vak->element_klass();\n+  assert(vk->flatten_array(), \"should only be used for flattened inline type arrays\");\n+  \/\/ Adjust offset to omit oop header\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE) - InlineKlass::cast(vk)->first_field_offset();\n+  \/\/ Initialize all elements of the flattened inline type array\n+  for (int i = 0; i < sv->field_size(); i++) {\n+    ScopeValue* val = sv->field_at(i);\n+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));\n+    reassign_fields_by_klass(vk, fr, reg_map, val->as_ObjectValue(), 0, (oop)obj, skip_internal, offset, CHECK);\n+  }\n+}\n+\n@@ -1389,1 +1469,1 @@\n-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal) {\n+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal, TRAPS) {\n@@ -1414,1 +1494,4 @@\n-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal);\n+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal, 0, CHECK);\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      reassign_flat_array_elements(fr, reg_map, sv, (flatArrayOop) obj(), vak, skip_internal, CHECK);\n@@ -1480,1 +1563,0 @@\n-\n@@ -1484,1 +1566,3 @@\n-    Handle obj = sv->value();\n+    print_object(k, sv->value(), realloc_failures);\n+  }\n+}\n@@ -1486,9 +1570,10 @@\n-    tty->print(\"     object <\" INTPTR_FORMAT \"> of type \", p2i(sv->value()()));\n-    k->print_value();\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n-    if (obj.is_null()) {\n-      tty->print(\" allocation failed\");\n-    } else {\n-      tty->print(\" allocated (%d bytes)\", obj->size() * HeapWordSize);\n-    }\n-    tty->cr();\n+void Deoptimization::print_object(Klass* k, Handle obj, bool realloc_failures) {\n+  tty->print(\"     object <\" INTPTR_FORMAT \"> of type \", p2i(obj()));\n+  k->print_value();\n+  assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+  if (obj.is_null()) {\n+    tty->print(\" allocation failed\");\n+  } else {\n+    tty->print(\" allocated (%d bytes)\", obj->size() * HeapWordSize);\n+  }\n+  tty->cr();\n@@ -1496,3 +1581,2 @@\n-    if (Verbose && !obj.is_null()) {\n-      k->oop_print_on(obj(), tty);\n-    }\n+  if (Verbose && !obj.is_null()) {\n+    k->oop_print_on(obj(), tty);\n@@ -1710,1 +1794,1 @@\n-  \/\/ Deoptimize only if the frame comes from compile code.\n+  \/\/ Deoptimize only if the frame comes from compiled code.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":116,"deletions":32,"binary":false,"changes":148,"status":"modified"},{"patch":"@@ -802,0 +802,18 @@\n+  notproduct(bool, PrintInlineLayout, false,                                \\\n+          \"Print field layout for each inline type\")                        \\\n+                                                                            \\\n+  notproduct(bool, PrintFlatArrayLayout, false,                             \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxSize, -1,                                \\\n+          \"Max size for flattening inline array elements, <0 no limit\")     \\\n+                                                                            \\\n+  product(intx, InlineFieldMaxFlatSize, 128,                                \\\n+          \"Max size for flattening inline type fields, <0 no limit\")        \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  product(bool, InlineArrayAtomicAccess, false,                             \\\n+          \"Atomic inline array accesses by-default, for all inline arrays\") \\\n+                                                                            \\\n@@ -817,2 +835,2 @@\n-  product(bool, UseBiasedLocking, false,                                    \\\n-          \"(Deprecated) Enable biased locking in JVM\")                      \\\n+  product(bool, UseBiasedLocking, false,                                     \\\n+          \"(Deprecated) Enable biased locking in JVM (completely disabled by Valhalla)\") \\\n@@ -2079,0 +2097,23 @@\n+  product(bool, EnableValhalla, true,                                       \\\n+          \"Enable experimental Valhalla features\")                          \\\n+                                                                            \\\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressInlineTypeReturnedAsFields, false,                    \\\n+          \"Stress return of fields instead of an inline type reference\")    \\\n+                                                                            \\\n+  develop(bool, ScalarizeInlineTypes, true,                                 \\\n+          \"Scalarize inline types in compiled code\")                        \\\n+                                                                            \\\n+  product(bool, UseArrayMarkWordCheck, NOT_LP64(false) LP64_ONLY(true),     \\\n+          \"Use bits in the mark word to check for flat\/null-free arrays\")   \\\n+                                                                            \\\n+  product(ccstrlist, ForceNonTearable, \"\", DIAGNOSTIC,                      \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":43,"deletions":2,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"asm\/codeBuffer.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -41,0 +43,1 @@\n+class SigEntry;\n@@ -57,1 +60,2 @@\n-                                         bool is_optimized, TRAPS);\n+                                         bool is_optimized,\n+                                         bool* caller_is_c1, TRAPS);\n@@ -67,1 +71,0 @@\n-  static address             _resolve_static_call_entry;\n@@ -88,1 +91,0 @@\n-\n@@ -321,1 +323,2 @@\n-                                     bool is_optimized, TRAPS);\n+                                     bool is_optimized,\n+                                     bool* caller_is_c1, TRAPS);\n@@ -329,1 +332,1 @@\n-                                             bool& needs_ic_stub_refill, TRAPS);\n+                                             bool& needs_ic_stub_refill, bool& is_optimized, bool caller_is_c1, TRAPS);\n@@ -335,1 +338,1 @@\n-  static methodHandle reresolve_call_site(JavaThread *thread, TRAPS);\n+  static methodHandle reresolve_call_site(JavaThread *thread, bool& is_static_call, bool& is_optimized, bool& caller_is_c1, TRAPS);\n@@ -339,1 +342,1 @@\n-  static methodHandle handle_ic_miss_helper(JavaThread* thread, TRAPS);\n+  static methodHandle handle_ic_miss_helper(JavaThread* thread, bool& is_optimized, bool& caller_is_c1, TRAPS);\n@@ -348,0 +351,13 @@\n+  static address entry_for_handle_wrong_method(methodHandle callee_method, bool is_static_call, bool is_optimized, bool caller_is_c1) {\n+    assert(callee_method->verified_code_entry() != NULL, \"Jump to zero!\");\n+    assert(callee_method->verified_inline_code_entry() != NULL, \"Jump to zero!\");\n+    assert(callee_method->verified_inline_ro_code_entry() != NULL, \"Jump to zero!\");\n+    if (caller_is_c1) {\n+      return callee_method->verified_inline_code_entry();\n+    } else if (is_static_call || is_optimized) {\n+      return callee_method->verified_code_entry();\n+    } else {\n+      return callee_method->verified_inline_ro_code_entry();\n+    }\n+  }\n+\n@@ -375,0 +391,8 @@\n+  static int java_calling_convention(const GrowableArray<SigEntry>* sig, VMRegPair* regs) {\n+    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig->length());\n+    int total_args_passed = SigEntry::fill_sig_bt(sig, sig_bt);\n+    return java_calling_convention(sig_bt, regs, total_args_passed);\n+  }\n+  static int java_return_convention(const BasicType* sig_bt, VMRegPair* regs, int total_args_passed);\n+  static const uint java_return_convention_max_int;\n+  static const uint java_return_convention_max_float;\n@@ -422,6 +446,10 @@\n-  static AdapterHandlerEntry* generate_i2c2i_adapters(MacroAssembler *_masm,\n-                                                      int total_args_passed,\n-                                                      int max_arg,\n-                                                      const BasicType *sig_bt,\n-                                                      const VMRegPair *regs,\n-                                                      AdapterFingerPrint* fingerprint);\n+  static AdapterHandlerEntry* generate_i2c2i_adapters(MacroAssembler *masm,\n+                                                      int comp_args_on_stack,\n+                                                      const GrowableArray<SigEntry>* sig,\n+                                                      const VMRegPair* regs,\n+                                                      const GrowableArray<SigEntry>* sig_cc,\n+                                                      const VMRegPair* regs_cc,\n+                                                      const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                      const VMRegPair* regs_cc_ro,\n+                                                      AdapterFingerPrint* fingerprint,\n+                                                      AdapterBlob*& new_adapter);\n@@ -430,2 +458,1 @@\n-                              int total_args_passed,\n-                              const BasicType *sig_bt,\n+                              const GrowableArray<SigEntry>* sig,\n@@ -507,0 +534,3 @@\n+  static void load_inline_type_fields_in_regs(JavaThread *thread, oopDesc* res);\n+  static void store_inline_type_fields_to_buf(JavaThread *thread, intptr_t res);\n+\n@@ -517,0 +547,3 @@\n+  static void allocate_inline_types(JavaThread* thread, Method* callee, bool allocate_receiver);\n+  static oop allocate_inline_types_impl(JavaThread* thread, methodHandle callee, bool allocate_receiver, TRAPS);\n+  static void apply_post_barriers(JavaThread* thread, objArrayOopDesc* array);\n@@ -520,0 +553,1 @@\n+  static BufferedInlineTypeBlob* generate_buffered_inline_type_adapter(const InlineKlass* vk);\n@@ -639,0 +673,2 @@\n+  address _c2i_inline_entry;\n+  address _c2i_inline_ro_entry;\n@@ -640,0 +676,1 @@\n+  address _c2i_unverified_inline_entry;\n@@ -642,0 +679,3 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  const GrowableArray<SigEntry>* _sig_cc;\n+\n@@ -649,1 +689,2 @@\n-  void init(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry, address c2i_unverified_entry, address c2i_no_clinit_check_entry) {\n+  void init(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry, address c2i_inline_entry,\n+            address c2i_inline_ro_entry, address c2i_unverified_entry, address c2i_unverified_inline_entry, address c2i_no_clinit_check_entry) {\n@@ -653,0 +694,2 @@\n+    _c2i_inline_entry = c2i_inline_entry;\n+    _c2i_inline_ro_entry = c2i_inline_ro_entry;\n@@ -654,0 +697,1 @@\n+    _c2i_unverified_inline_entry = c2i_unverified_inline_entry;\n@@ -655,0 +699,1 @@\n+    _sig_cc = NULL;\n@@ -667,4 +712,7 @@\n-  address get_i2c_entry()                  const { return _i2c_entry; }\n-  address get_c2i_entry()                  const { return _c2i_entry; }\n-  address get_c2i_unverified_entry()       const { return _c2i_unverified_entry; }\n-  address get_c2i_no_clinit_check_entry()  const { return _c2i_no_clinit_check_entry; }\n+  address get_i2c_entry()                   const { return _i2c_entry; }\n+  address get_c2i_entry()                   const { return _c2i_entry; }\n+  address get_c2i_inline_entry()            const { return _c2i_inline_entry; }\n+  address get_c2i_inline_ro_entry()         const { return _c2i_inline_ro_entry; }\n+  address get_c2i_unverified_entry()        const { return _c2i_unverified_entry; }\n+  address get_c2i_unverified_inline_entry() const { return _c2i_unverified_inline_entry; }\n+  address get_c2i_no_clinit_check_entry()   const { return _c2i_no_clinit_check_entry; }\n@@ -675,0 +723,4 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  void set_sig_cc(const GrowableArray<SigEntry>* sig)  { _sig_cc = sig; }\n+  const GrowableArray<SigEntry>* get_sig_cc()    const { return _sig_cc; }\n+\n@@ -703,4 +755,2 @@\n-                                        address i2c_entry,\n-                                        address c2i_entry,\n-                                        address c2i_unverified_entry,\n-                                        address c2i_no_clinit_check_entry = NULL);\n+                                        address i2c_entry, address c2i_entry, address c2i_inline_entry, address c2i_inline_ro_entry,\n+                                        address c2i_unverified_entry, address c2i_unverified_inline_entry, address c2i_no_clinit_check_entry = NULL);\n@@ -719,0 +769,60 @@\n+\/\/ Utility class for computing the calling convention of the 3 types\n+\/\/ of compiled method entries:\n+\/\/     Method::_from_compiled_entry               - sig_cc\n+\/\/     Method::_from_compiled_inline_ro_entry     - sig_cc_ro\n+\/\/     Method::_from_compiled_inline_entry        - sig\n+class CompiledEntrySignature : public StackObj {\n+  Method* _method;\n+  int  _num_inline_args;\n+  bool _has_inline_recv;\n+  GrowableArray<SigEntry> *_sig;\n+  GrowableArray<SigEntry> *_sig_cc;\n+  GrowableArray<SigEntry> *_sig_cc_ro;\n+  VMRegPair* _regs;\n+  VMRegPair* _regs_cc;\n+  VMRegPair* _regs_cc_ro;\n+\n+  int _args_on_stack;\n+  int _args_on_stack_cc;\n+  int _args_on_stack_cc_ro;\n+\n+  bool _c1_needs_stack_repair;\n+  bool _c2_needs_stack_repair;\n+  bool _has_scalarized_args;\n+\n+public:\n+  Method* method()                     const { return _method; }\n+\n+  \/\/ Used by Method::_from_compiled_inline_entry\n+  GrowableArray<SigEntry>& sig()       const { return *_sig; }\n+\n+  \/\/ Used by Method::_from_compiled_entry\n+  GrowableArray<SigEntry>& sig_cc()    const { return *_sig_cc; }\n+\n+  \/\/ Used by Method::_from_compiled_inline_ro_entry\n+  GrowableArray<SigEntry>& sig_cc_ro() const { return *_sig_cc_ro; }\n+\n+  VMRegPair* regs()                    const { return _regs; }\n+  VMRegPair* regs_cc()                 const { return _regs_cc; }\n+  VMRegPair* regs_cc_ro()              const { return _regs_cc_ro; }\n+\n+  int args_on_stack()                  const { return _args_on_stack; }\n+  int args_on_stack_cc()               const { return _args_on_stack_cc; }\n+  int args_on_stack_cc_ro()            const { return _args_on_stack_cc_ro; }\n+\n+  int  num_inline_args()               const { return _num_inline_args; }\n+  bool has_inline_arg()                const { return _num_inline_args > 0; }\n+  bool has_inline_recv()               const { return _has_inline_recv; }\n+\n+  bool has_scalarized_args()           const { return _has_scalarized_args; }\n+  bool c1_needs_stack_repair()         const { return _c1_needs_stack_repair; }\n+  bool c2_needs_stack_repair()         const { return _c2_needs_stack_repair; }\n+  CodeOffsets::Entries c1_inline_ro_entry_type() const;\n+\n+  CompiledEntrySignature(Method* method);\n+  void compute_calling_conventions();\n+\n+private:\n+  int compute_scalarized_cc(GrowableArray<SigEntry>*& sig_cc, VMRegPair*& regs_cc, bool scalar_receiver);\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":134,"deletions":24,"binary":false,"changes":158,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -49,1 +50,1 @@\n-\/\/ FieldType  = \"B\" | \"C\" | \"D\" | \"F\" | \"I\" | \"J\" | \"S\" | \"Z\" | \"L\" ClassName \";\" | \"[\" FieldType.\n+\/\/ FieldType  = \"B\" | \"C\" | \"D\" | \"F\" | \"I\" | \"J\" | \"S\" | \"Z\" | \"L\" ClassName \";\" | \"Q\" ValueClassName \";\" | \"[\" FieldType.\n@@ -222,0 +223,1 @@\n+  case T_INLINE_TYPE:\n@@ -301,0 +303,1 @@\n+  case JVM_SIGNATURE_INLINE_TYPE:\n@@ -382,0 +385,9 @@\n+InlineKlass* SignatureStream::as_inline_klass(InstanceKlass* holder) {\n+  Thread* THREAD = Thread::current();\n+  Handle class_loader(THREAD, holder->class_loader());\n+  Handle protection_domain(THREAD, holder->protection_domain());\n+  Klass* k = as_klass(class_loader, protection_domain, SignatureStream::ReturnNull, THREAD);\n+  assert(k != NULL && !HAS_PENDING_EXCEPTION, \"unresolved inline klass\");\n+  return InlineKlass::cast(k);\n+}\n+\n@@ -477,1 +489,0 @@\n-\n@@ -496,1 +507,1 @@\n-bool SignatureVerifier::is_valid_method_signature(Symbol* sig) {\n+bool SignatureVerifier::is_valid_method_signature(const Symbol* sig) {\n@@ -519,1 +530,1 @@\n-bool SignatureVerifier::is_valid_type_signature(Symbol* sig) {\n+bool SignatureVerifier::is_valid_type_signature(const Symbol* sig) {\n@@ -548,0 +559,1 @@\n+    case JVM_SIGNATURE_INLINE_TYPE: \/\/ fall through\n@@ -566,0 +578,53 @@\n+\n+\/\/ Adds an argument to the signature\n+void SigEntry::add_entry(GrowableArray<SigEntry>* sig, BasicType bt, Symbol* symbol, int offset) {\n+  sig->append(SigEntry(bt, offset, symbol));\n+  if (bt == T_LONG || bt == T_DOUBLE) {\n+    sig->append(SigEntry(T_VOID, offset, symbol)); \/\/ Longs and doubles take two stack slots\n+  }\n+}\n+\n+\/\/ Returns true if the argument at index 'i' is not an inline type delimiter\n+bool SigEntry::skip_value_delimiters(const GrowableArray<SigEntry>* sig, int i) {\n+  return (sig->at(i)._bt != T_INLINE_TYPE &&\n+          (sig->at(i)._bt != T_VOID || sig->at(i-1)._bt == T_LONG || sig->at(i-1)._bt == T_DOUBLE));\n+}\n+\n+\/\/ Fill basic type array from signature array\n+int SigEntry::fill_sig_bt(const GrowableArray<SigEntry>* sig, BasicType* sig_bt) {\n+  int count = 0;\n+  for (int i = 0; i < sig->length(); i++) {\n+    if (skip_value_delimiters(sig, i)) {\n+      sig_bt[count++] = sig->at(i)._bt;\n+    }\n+  }\n+  return count;\n+}\n+\n+\/\/ Create a temporary symbol from the signature array\n+TempNewSymbol SigEntry::create_symbol(const GrowableArray<SigEntry>* sig) {\n+  ResourceMark rm;\n+  int length = sig->length();\n+  char* sig_str = NEW_RESOURCE_ARRAY(char, 2*length + 3);\n+  int idx = 0;\n+  sig_str[idx++] = '(';\n+  for (int i = 0; i < length; i++) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_INLINE_TYPE || bt == T_VOID) {\n+      \/\/ Ignore\n+    } else {\n+      if (bt == T_ARRAY) {\n+        bt = T_OBJECT; \/\/ We don't know the element type, treat as Object\n+      }\n+      sig_str[idx++] = type2char(bt);\n+      if (bt == T_OBJECT) {\n+        sig_str[idx++] = ';';\n+      }\n+    }\n+  }\n+  sig_str[idx++] = ')';\n+  \/\/ Add a dummy return type. It won't be used but SignatureStream needs it.\n+  sig_str[idx++] = 'V';\n+  sig_str[idx++] = '\\0';\n+  return SymbolTable::new_symbol(sig_str);\n+}\n","filename":"src\/hotspot\/share\/runtime\/signature.cpp","additions":69,"deletions":4,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -1254,0 +1255,1 @@\n+  _return_buffered_value(nullptr),\n@@ -1310,1 +1312,0 @@\n-\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -429,0 +429,1 @@\n+ public:\n@@ -860,0 +861,1 @@\n+  friend class VTBuffer;\n@@ -917,0 +919,1 @@\n+  oop           _return_buffered_value; \/\/ buffered value being returned\n@@ -1358,0 +1361,3 @@\n+  oop return_buffered_value() const              { return _return_buffered_value; }\n+  void set_return_buffered_value(oop val)        { _return_buffered_value = val; }\n+\n@@ -1422,0 +1428,1 @@\n+  static ByteSize return_buffered_value_offset() { return byte_offset_of(JavaThread, _return_buffered_value); }\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -257,0 +257,2 @@\n+#define THREAD_AND_LOCATION_DECL                 TRAPS, const char* file, int line\n+#define THREAD_AND_LOCATION_ARGS                 THREAD, file, line\n","filename":"src\/hotspot\/share\/utilities\/exceptions.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2285,0 +2285,9 @@\n+            \/* Per our interim inline class translation scheme, the reference projection classes\n+               are completely empty, so we want the methods in the value class to be invoked instead.\n+               As the lambda meta factory isn't clued into this, it will try to invoke the method in\n+               C$ref.class and fail with a NoSuchMethodError. we need to workaround it ourselves.\n+            *\/\n+            boolean receiverIsReferenceProjection() {\n+                return tree.sym.kind == MTH && tree.sym.owner.isReferenceProjection();\n+            }\n+\n@@ -2337,0 +2346,1 @@\n+                        receiverIsReferenceProjection() ||\n@@ -2339,1 +2349,1 @@\n-                          (tree.sym.owner.isDirectlyOrIndirectlyLocal() || tree.sym.owner.isInner()));\n+                          (tree.sym.owner.isDirectlyOrIndirectlyLocal() || tree.sym.owner.isInner() || tree.sym.owner.isPrimitiveClass()));\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/LambdaToMethod.java","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+import com.sun.tools.javac.code.Source.Feature;\n@@ -873,0 +874,3 @@\n+        else if (enclOp.hasTag(WITHFIELD) &&\n+                tree == TreeInfo.skipParens(((JCWithField) enclOp).field))\n+            return AccessCode.WITHFIELD.code;\n@@ -983,1 +987,1 @@\n-            } else if (acode == AccessCode.ASSIGN.code)\n+            } else if (acode == AccessCode.ASSIGN.code || acode == AccessCode.WITHFIELD.code)\n@@ -987,1 +991,1 @@\n-            restype = vsym.erasure(types);\n+            restype = acode == AccessCode.WITHFIELD.code ? vsym.owner.erasure(types) : vsym.erasure(types);\n@@ -1356,0 +1360,3 @@\n+            case WITHFIELD:\n+                expr = make.WithField(ref, args.head);\n+                break;\n@@ -1361,1 +1368,1 @@\n-            stat = make.Return(expr.setType(sym.type));\n+            stat = make.Return(expr.setType(aCode == AccessCode.WITHFIELD ? sym.owner.type : sym.type));\n@@ -1455,0 +1462,1 @@\n+            final Type type = v.erasure(types);\n@@ -1456,1 +1464,1 @@\n-                flags, proxyName, v.erasure(types), owner);\n+                flags, proxyName, type, owner);\n@@ -1519,1 +1527,3 @@\n-        VarSymbol outerThis = makeOuterThisVarSymbol(owner, FINAL | SYNTHETIC);\n+        Type target = types.erasure(owner.enclClass().type.getEnclosingType());\n+        long flags = FINAL | SYNTHETIC;\n+        VarSymbol outerThis = makeOuterThisVarSymbol(owner, flags);\n@@ -1703,1 +1713,1 @@\n-        if (types.asSuper(resource.type, syms.autoCloseableType.tsym) == null) {\n+        if (types.asSuper(resource.type, syms.autoCloseableType.tsym, true) == null) {\n@@ -2084,1 +2094,2 @@\n-        return (tree == null) ? null : boxIfNeeded(translate(tree), type);\n+        return (tree == null) ? null :\n+                applyPrimitiveConversionsAsNeeded(boxIfNeeded(translate(tree), type), type);\n@@ -3081,0 +3092,17 @@\n+    \/** Apply primitive value\/reference conversions as needed *\/\n+    @SuppressWarnings(\"unchecked\")\n+    <T extends JCExpression> T applyPrimitiveConversionsAsNeeded(T tree, Type type) {\n+        boolean haveValue = tree.type.isPrimitiveClass();\n+        if (haveValue == type.isPrimitiveClass())\n+            return tree;\n+        if (haveValue) {\n+            \/\/ widening coversion is a NOP for the VM due to subtyping relationship at class file\n+            return tree;\n+        } else {\n+            \/\/ For narrowing conversion, insert a cast which should trigger a null check\n+            return (T) make.TypeCast(type, tree);\n+        }\n+    }\n+\n+\n+\n@@ -3480,1 +3508,1 @@\n-                                              syms.iterableType.tsym);\n+                                              syms.iterableType.tsym, true);\n@@ -3569,0 +3597,17 @@\n+    public void visitWithField(JCWithField tree) {\n+        Type fieldType = tree.field.type;\n+        tree.field = translate(tree.field, tree);\n+        tree.value = translate(tree.value, fieldType); \/\/ important to use pre-translation type.\n+\n+        \/\/ If translated field is an Apply, we are\n+        \/\/ seeing an access method invocation. In this case, append\n+        \/\/ right hand side as last argument of the access method.\n+        if (tree.field.hasTag(APPLY)) {\n+            JCMethodInvocation app = (JCMethodInvocation) tree.field;\n+            app.args = List.of(tree.value).prependList(app.args);\n+            result = app;\n+        } else {\n+            result = tree;\n+        }\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Lower.java","additions":53,"deletions":8,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+import com.sun.tools.javac.code.Flags.Flag;\n@@ -51,0 +52,1 @@\n+import static com.sun.tools.javac.code.Flags.asFlagSet;\n@@ -59,0 +61,1 @@\n+import static com.sun.tools.javac.parser.Tokens.TokenKind.SYNCHRONIZED;\n@@ -187,0 +190,1 @@\n+        this.allowWithFieldOperator = fac.options.isSet(\"allowWithFieldOperator\");\n@@ -206,0 +210,4 @@\n+    \/** Switch: should we allow withField operator at source level ?\n+    *\/\n+    boolean allowWithFieldOperator;\n+\n@@ -305,0 +313,7 @@\n+    protected boolean peekToken(int lookahead, Filter<TokenKind> tk1, Filter<TokenKind> tk2, Filter<TokenKind> tk3, Filter<TokenKind> tk4) {\n+        return tk1.accepts(S.token(lookahead + 1).kind) &&\n+                tk2.accepts(S.token(lookahead + 2).kind) &&\n+                tk3.accepts(S.token(lookahead + 3).kind) &&\n+                tk4.accepts(S.token(lookahead + 4).kind);\n+    }\n+\n@@ -474,0 +489,16 @@\n+    \/** If next input token matches one of the two given tokens, skip it, otherwise report\n+     *  an error.\n+     *\n+     * @return The actual token kind.\n+     *\/\n+    public TokenKind accept2(TokenKind tk1, TokenKind tk2) {\n+        TokenKind returnValue = token.kind;\n+        if (token.kind == tk1 || token.kind == tk2) {\n+            nextToken();\n+        } else {\n+            setErrorEndPos(token.pos);\n+            reportSyntaxError(S.prevToken().endPos, Errors.Expected2(tk1, tk2));\n+        }\n+        return returnValue;\n+    }\n+\n@@ -1143,0 +1174,15 @@\n+        case WITHFIELD:\n+            if (!allowWithFieldOperator) {\n+                log.error(pos, Errors.WithFieldOperatorDisallowed);\n+            }\n+            if (typeArgs == null && (mode & EXPR) != 0) {\n+                nextToken();\n+                accept(LPAREN);\n+                mode = EXPR;\n+                t = term();\n+                accept(COMMA);\n+                mode = EXPR;\n+                JCExpression v = term();\n+                accept(RPAREN);\n+                return F.at(pos).WithField(t, v);\n+            } else return illegal();\n@@ -1311,0 +1357,6 @@\n+                            case DEFAULT:\n+                                if (typeArgs != null) return illegal();\n+                                selectExprMode();\n+                                t = to(F.at(pos).Select(t, names._default));\n+                                nextToken();\n+                                break loop;\n@@ -1368,3 +1420,4 @@\n-                        if ((mode & TYPE) == 0 && isUnboundMemberRef()) {\n-                            \/\/this is an unbound method reference whose qualifier\n-                            \/\/is a generic type i.e. A<S>::m\n+                        if ((mode & TYPE) == 0 && isParameterizedTypePrefix()) {\n+                            \/\/this is either an unbound method reference whose qualifier\n+                            \/\/is a generic type i.e. A<S>::m or a default value creation of\n+                            \/\/the form ValueType<S>.default\n@@ -1383,0 +1436,6 @@\n+                                if (token.kind == DEFAULT) {\n+                                    t =  toP(F.at(token.pos).Select(t, names._default));\n+                                    nextToken();\n+                                    selectExprMode();\n+                                    return term3Rest(t, typeArgs);\n+                                }\n@@ -1547,1 +1606,1 @@\n-                } else if (token.kind == NEW && (mode & EXPR) != 0) {\n+                } else if ((token.kind == NEW) && (mode & EXPR) != 0) {\n@@ -1601,1 +1660,2 @@\n-     * method reference or a binary expression. To disambiguate, look for a\n+     * method reference or a default value creation that uses a parameterized type\n+     * or a binary expression. To disambiguate, look for a\n@@ -1605,1 +1665,1 @@\n-    boolean isUnboundMemberRef() {\n+    boolean isParameterizedTypePrefix() {\n@@ -1727,2 +1787,2 @@\n-                    if (peekToken(lookahead, LAX_IDENTIFIER)) {\n-                        \/\/ Identifier, Identifier\/'_'\/'assert'\/'enum' -> explicit lambda\n+                    if (peekToken(lookahead, LAX_IDENTIFIER) || (peekToken(lookahead, QUES, LAX_IDENTIFIER) && (peekToken(lookahead + 2, RPAREN) || peekToken(lookahead + 2, COMMA)))) {\n+                        \/\/ Identifier[?], Identifier\/'_'\/'assert'\/'enum' -> explicit lambda\n@@ -1804,0 +1864,2 @@\n+                                peekToken(lookahead, QUES, LAX_IDENTIFIER, COMMA) ||\n+                                peekToken(lookahead, QUES, LAX_IDENTIFIER, RPAREN, ARROW) ||\n@@ -2200,1 +2262,1 @@\n-            accept(CLASS);\n+            TokenKind selector = accept2(CLASS, DEFAULT);\n@@ -2218,1 +2280,1 @@\n-                t = toP(F.at(pos).Select(t, names._class));\n+                t = toP(F.at(pos).Select(t, selector == CLASS ? names._class : names._default));\n@@ -2263,2 +2325,5 @@\n-        List<JCAnnotation> newAnnotations = typeAnnotationsOpt();\n-\n+        final JCModifiers mods = modifiersOpt();\n+        List<JCAnnotation> newAnnotations = mods.annotations;\n+        if (!newAnnotations.isEmpty()) {\n+            checkSourceLevel(newAnnotations.head.pos, Feature.TYPE_ANNOTATIONS);\n+        }\n@@ -2268,0 +2333,4 @@\n+            if (mods.flags != 0) {\n+                long badModifiers = (mods.flags & Flags.PRIMITIVE_CLASS) != 0 ? mods.flags & ~Flags.FINAL : mods.flags;\n+                log.error(token.pos, Errors.ModNotAllowedHere(asFlagSet(badModifiers)));\n+            }\n@@ -2336,0 +2405,3 @@\n+            long badModifiers = mods.flags & ~(Flags.PRIMITIVE_CLASS | Flags.FINAL);\n+            if (badModifiers != 0)\n+                log.error(token.pos, Errors.ModNotAllowedHere(asFlagSet(badModifiers)));\n@@ -2340,1 +2412,6 @@\n-            return classCreatorRest(newpos, null, typeArgs, t);\n+            JCNewClass newClass = classCreatorRest(newpos, null, typeArgs, t, mods.flags);\n+            if ((newClass.def == null) && (mods.flags != 0)) {\n+                badModifiers = (mods.flags & Flags.PRIMITIVE_CLASS) != 0 ? mods.flags & ~Flags.FINAL : mods.flags;\n+                log.error(newClass.pos, Errors.ModNotAllowedHere(asFlagSet(badModifiers)));\n+            }\n+            return newClass;\n@@ -2365,1 +2442,1 @@\n-        return classCreatorRest(newpos, encl, typeArgs, t);\n+        return classCreatorRest(newpos, encl, typeArgs, t, 0);\n@@ -2447,1 +2524,2 @@\n-                                  JCExpression t)\n+                                  JCExpression t,\n+                                  long flags)\n@@ -2454,1 +2532,1 @@\n-            JCModifiers mods = F.at(Position.NOPOS).Modifiers(0);\n+            JCModifiers mods = F.at(Position.NOPOS).Modifiers(flags);\n@@ -2457,1 +2535,2 @@\n-        return toP(F.at(newpos).NewClass(encl, typeArgs, t, args, body));\n+        JCNewClass newClass = toP(F.at(newpos).NewClass(encl, typeArgs, t, args, body));\n+        return newClass;\n@@ -2588,0 +2667,1 @@\n+        token = recastToken(token);\n@@ -2598,0 +2678,1 @@\n+        case PRIMITIVE:\n@@ -3050,1 +3131,4 @@\n-                return variableDeclarators(modifiersOpt(), t, stats, true).toList();\n+                pos = token.pos;\n+                JCModifiers mods = F.at(Position.NOPOS).Modifiers(0);\n+                F.at(pos);\n+                return variableDeclarators(mods, t, stats, true).toList();\n@@ -3120,0 +3204,1 @@\n+            token = recastToken(token);\n@@ -3129,0 +3214,1 @@\n+            case PRIMITIVE   : flag = Flags.PRIMITIVE_CLASS; break;\n@@ -3160,2 +3246,7 @@\n-                    annotations.append(ann);\n-                    flag = 0;\n+                    final Name name = TreeInfo.name(ann.annotationType);\n+                    if (name == names.__primitive__ || name == names.java_lang___primitive__) {\n+                        flag = Flags.PRIMITIVE_CLASS;\n+                    } else {\n+                        annotations.append(ann);\n+                        flag = 0;\n+                    }\n@@ -3177,0 +3268,5 @@\n+        \/\/ Force value classes to be automatically final.\n+        if ((flags & (Flags.PRIMITIVE_CLASS | Flags.ABSTRACT | Flags.INTERFACE | Flags.ENUM)) == Flags.PRIMITIVE_CLASS) {\n+            flags |= Flags.FINAL;\n+        }\n+\n@@ -3366,0 +3462,36 @@\n+    \/\/ Does the given token signal a primitive modifier ? If yes, suitably reclassify token.\n+    Token recastToken(Token token) {\n+        if (token.kind != IDENTIFIER || token.name() != names.primitive) {\n+            return token;\n+        }\n+        if (peekToken(t->t == PRIVATE ||\n+                         t == PROTECTED ||\n+                         t == PUBLIC ||\n+                         t == STATIC ||\n+                         t == TRANSIENT ||\n+                         t == FINAL ||\n+                         t == ABSTRACT ||\n+                         t == NATIVE ||\n+                         t == VOLATILE ||\n+                         t == SYNCHRONIZED ||\n+                         t == STRICTFP ||\n+                         t == MONKEYS_AT ||\n+                         t == DEFAULT ||\n+                         t == BYTE ||\n+                         t == SHORT ||\n+                         t == CHAR ||\n+                         t == INT ||\n+                         t == LONG ||\n+                         t == FLOAT ||\n+                         t == DOUBLE ||\n+                         t == BOOLEAN ||\n+                         t == CLASS ||\n+                         t == INTERFACE ||\n+                         t == ENUM ||\n+                         t == IDENTIFIER)) { \/\/ new value Comparable() {}\n+            checkSourceLevel(Feature.PRIMITIVE_CLASSES);\n+            return new Token(PRIMITIVE, token.pos, token.endPos, token.comments);\n+        }\n+        return token;\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":152,"deletions":20,"binary":false,"changes":172,"status":"modified"},{"patch":"@@ -136,0 +136,4 @@\n+        \/** Withfields, of type WithField.\n+         *\/\n+        WITHFIELD,\n+\n@@ -713,1 +717,0 @@\n-\n@@ -873,0 +876,3 @@\n+        \/** nascent value that evolves into the return value for a value factory *\/\n+        public VarSymbol factoryProduct;\n+\n@@ -1136,0 +1142,30 @@\n+    \/**\n+     * A withfield expression\n+     *\/\n+    public static class JCWithField extends JCExpression implements WithFieldTree {\n+        public JCExpression field;\n+        public JCExpression value;\n+        protected JCWithField(JCExpression field, JCExpression value) {\n+            this.field = field;\n+            this.value = value;\n+        }\n+        @Override\n+        public void accept(Visitor v) { v.visitWithField(this); }\n+\n+        @DefinedBy(Api.COMPILER_TREE)\n+        public Kind getKind() { return Kind.WITH_FIELD; }\n+        @DefinedBy(Api.COMPILER_TREE)\n+        public JCExpression getField() { return field; }\n+        @DefinedBy(Api.COMPILER_TREE)\n+        public JCExpression getValue() { return value; }\n+        @Override @DefinedBy(Api.COMPILER_TREE)\n+        public <R,D> R accept(TreeVisitor<R,D> v, D d) {\n+            return v.visitWithField(this, d);\n+        }\n+\n+        @Override\n+        public Tag getTag() {\n+            return WITHFIELD;\n+        }\n+    }\n+\n@@ -3248,0 +3284,1 @@\n+        public void visitWithField(JCWithField that)         { visitTree(that); }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/JCTree.java","additions":38,"deletions":1,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -113,0 +113,1 @@\n+     *  Optionally, check only for no-arg ctor invocation\n@@ -114,1 +115,1 @@\n-    public static Name getConstructorInvocationName(List<? extends JCTree> trees, Names names) {\n+    public static Name getConstructorInvocationName(List<? extends JCTree> trees, Names names, boolean argsAllowed) {\n@@ -120,4 +121,6 @@\n-                    Name methName = TreeInfo.name(apply.meth);\n-                    if (methName == names._this ||\n-                        methName == names._super) {\n-                        return methName;\n+                    if (argsAllowed || apply.args.size() == 0) {\n+                        Name methName = TreeInfo.name(apply.meth);\n+                        if (methName == names._this ||\n+                                methName == names._super) {\n+                            return methName;\n+                        }\n@@ -629,0 +632,2 @@\n+            case WITHFIELD:\n+                return getEndPos(((JCWithField) tree).value, endPosTable);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeInfo.java","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2571,1 +2571,1 @@\n-        return isKind(doctree, VALUE);\n+        return isKind(doctree, Kind.VALUE);\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/toolkit\/util\/Utils.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -532,0 +532,5 @@\n+                String returnType = getJavaReturnType(d);\n+                if (!returnType.equals(\"void\")) { \/\/ value static factories\n+                    print(returnType);\n+                    print(\" \");\n+                }\n","filename":"src\/jdk.jdeps\/share\/classes\/com\/sun\/tools\/javap\/ClassWriter.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -65,0 +65,77 @@\n+# Valhalla\n+compiler\/arguments\/CheckCICompilerCount.java                        8205030 generic-all\n+compiler\/arguments\/CheckCompileThresholdScaling.java                8205030 generic-all\n+compiler\/codecache\/CheckSegmentedCodeCache.java                     8205030 generic-all\n+compiler\/codecache\/cli\/TestSegmentedCodeCacheOption.java            8205030 generic-all\n+compiler\/codecache\/cli\/codeheapsize\/TestCodeHeapSizeOptions.java    8205030 generic-all\n+compiler\/codecache\/cli\/printcodecache\/TestPrintCodeCacheOption.java 8205030 generic-all\n+compiler\/whitebox\/OSRFailureLevel4Test.java                         8205030 generic-all\n+\n+compiler\/aot\/cli\/DisabledAOTWithLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/SingleAOTOptionTest.java 8226295 generic-all\n+compiler\/aot\/cli\/MultipleAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileClassWithDebugTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileModuleTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/AtFileTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionWrongFileTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ClasspathOptionUnknownClassTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileDirectoryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/ListOptionNotExistingTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileClassTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileJarTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/IgnoreErrorsTest.java 8226295 generic-all\n+compiler\/aot\/cli\/jaotc\/CompileAbsoluteDirectoryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/NonExistingAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/SingleAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/cli\/IncorrectAOTLibraryTest.java 8226295 generic-all\n+compiler\/aot\/RecompilationTest.java 8226295 generic-all\n+compiler\/aot\/SharedUsageTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/ClassSearchTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/SearchPathTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/module\/ModuleSourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/ClassSourceTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/directory\/DirectorySourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/collect\/jar\/JarSourceProviderTest.java 8226295 generic-all\n+compiler\/aot\/jdk.tools.jaotc.test\/src\/jdk\/tools\/jaotc\/test\/NativeOrderOutputStreamTest.java 8226295 generic-all\n+compiler\/aot\/verification\/vmflags\/TrackedFlagTest.java 8226295 generic-all\n+compiler\/aot\/verification\/vmflags\/NotTrackedFlagTest.java 8226295 generic-all\n+compiler\/aot\/verification\/ClassAndLibraryNotMatchTest.java 8226295 generic-all\n+compiler\/aot\/DeoptimizationTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeInterface2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeDynamic2CompiledTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeStatic2NativeTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromAot\/AotInvokeSpecial2InterpretedTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromNative\/NativeInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromCompiled\/CompiledInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeDynamic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeStatic2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeVirtual2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeSpecial2AotTest.java 8226295 generic-all\n+compiler\/aot\/calls\/fromInterpreted\/InterpretedInvokeInterface2AotTest.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SelfChanged.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SelfChangedCDS.java 8226295 generic-all\n+compiler\/aot\/fingerprint\/SuperChanged.java 8226295 generic-all\n+\n@@ -88,0 +165,22 @@\n+# Valhalla TODO:\n+runtime\/CompressedOops\/CompressedClassPointers.java 8210258 generic-all\n+runtime\/RedefineTests\/RedefineLeak.java 8205032 generic-all\n+runtime\/SharedArchiveFile\/BootAppendTests.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/CdsDifferentCompactStrings.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/CdsDifferentObjectAlignment.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/NonBootLoaderClasses.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/PrintSharedArchiveAndExit.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedArchiveFile.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedStringsDedup.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedStringsRunAuto.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SharedSymbolTableBucketSize.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/SpaceUtilizationCheck.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/TestInterpreterMethodEntries.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformInterfaceAndImplementor.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformSuperAndSubClasses.java 8210258 generic-all\n+runtime\/SharedArchiveFile\/serviceability\/transformRelatedClasses\/TransformSuperSubTwoPckgs.java 8210258 generic-all\n+runtime\/appcds\/ClassLoaderTest.java 8210258 generic-all\n+runtime\/appcds\/HelloTest.java 8210258 generic-all\n+runtime\/appcds\/sharedStrings\/SharedStringsBasic.java 8210258 generic-all\n+\n+\n@@ -102,0 +201,28 @@\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":127,"deletions":0,"binary":false,"changes":127,"status":"modified"}]}