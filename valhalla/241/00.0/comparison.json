{"files":[{"patch":"@@ -462,1 +462,1 @@\n-    TARGET_DIR := $(IMAGES_OUTPUTDIR)\/javase-docs\/api, \\\n+    TARGET_DIR := $(DOCS_JAVASE_IMAGE_DIR)\/api, \\\n@@ -480,1 +480,1 @@\n-    TARGET_DIR := $(IMAGES_OUTPUTDIR)\/reference-docs\/api, \\\n+    TARGET_DIR := $(DOCS_REFERENCE_IMAGE_DIR)\/api, \\\n","filename":"make\/Docs.gmk","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -409,1 +409,1 @@\n-                \"--enable-full-docs\", \"--with-zlib=system\",\n+                \"--with-zlib=system\",\n@@ -412,1 +412,0 @@\n-            default_make_targets: [\"docs-bundles\"],\n@@ -683,2 +682,14 @@\n-    profilesArtifacts = {\n-        \"linux-x64\": {\n+    buildJdkDep = input.build_os + \"-\" + input.build_cpu + \".jdk\";\n+    docsProfiles = {\n+        \"docs\": {\n+            target_os: input.build_os,\n+            target_cpu: input.build_cpu,\n+            dependencies: [\n+                \"boot_jdk\", \"devkit\", \"graphviz\", \"pandoc\", buildJdkDep,\n+            ],\n+            configure_args: [\n+                \"--enable-full-docs\",\n+                \"--with-build-jdk=\" + input.get(buildJdkDep, \"home_path\")\n+                    + (input.build_os == \"macosx\" ? \"\/Contents\/Home\" : \"\")\n+            ],\n+            default_make_targets: [\"all-docs-bundles\"],\n@@ -687,1 +698,1 @@\n-                    local: \"bundles\/\\\\(jdk.*doc-api-spec.tar.gz\\\\)\",\n+                    local: \"bundles\/\\\\(jdk-\" + data.version + \".*doc-api-spec.tar.gz\\\\)\",\n@@ -693,0 +704,14 @@\n+                javase_doc_api_spec: {\n+                    local: \"bundles\/\\\\(javase-\" + data.version + \".*doc-api-spec.tar.gz\\\\)\",\n+                    remote: [\n+                        \"bundles\/common\/javase-\" + data.version + \"_doc-api-spec.tar.gz\",\n+                        \"bundles\/common\/\\\\1\"\n+                    ],\n+                },\n+                reference_doc_api_spec: {\n+                    local: \"bundles\/\\\\(jdk-reference-\" + data.version + \".*doc-api-spec.tar.gz\\\\)\",\n+                    remote: [\n+                        \"bundles\/common\/jdk-reference-\" + data.version + \"_doc-api-spec.tar.gz\",\n+                        \"bundles\/common\/\\\\1\"\n+                    ],\n+                },\n@@ -696,1 +721,1 @@\n-    profiles = concatObjects(profiles, profilesArtifacts);\n+    profiles = concatObjects(profiles, docsProfiles);\n@@ -963,1 +988,1 @@\n-        macosx_x64: \"Xcode11.3.1-MacOSX10.15+1.0\",\n+        macosx_x64: \"Xcode11.3.1-MacOSX10.15+1.1\",\n","filename":"make\/conf\/jib-profiles.js","additions":32,"deletions":7,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -617,3 +617,1 @@\n-\/\/ 2) reg_class compiler_method_reg        ( \/* as def'd in frame section *\/ )\n-\/\/ 2) reg_class interpreter_method_reg     ( \/* as def'd in frame section *\/ )\n-\/\/ 3) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n+\/\/ 2) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n@@ -2632,5 +2630,0 @@\n-\/\/ No-op on amd64\n-void Matcher::pd_implicit_null_fixup(MachNode *node, uint idx) {\n-  Unimplemented();\n-}\n-\n@@ -3795,0 +3788,4 @@\n+      if (call == NULL) {\n+        ciEnv::current()->record_failure(\"CodeCache is full\");\n+        return;\n+      }\n@@ -3800,1 +3797,4 @@\n-\n+      if (call == NULL) {\n+        ciEnv::current()->record_failure(\"CodeCache is full\");\n+        return;\n+      }\n@@ -3808,4 +3808,2 @@\n-    if (call == NULL) {\n-      ciEnv::current()->record_failure(\"CodeCache is full\");\n-      return;\n-    } else if (UseSVE > 0 && Compile::current()->max_vector_size() >= 16) {\n+\n+    if (UseSVE > 0 && Compile::current()->max_vector_size() >= 16) {\n@@ -4113,3 +4111,0 @@\n-  \/\/ Method Register when calling interpreter.\n-  interpreter_method_reg(R12);\n-\n@@ -5703,10 +5698,0 @@\n-operand interpreter_method_RegP(iRegP reg)\n-%{\n-  constraint(ALLOC_IN_RC(method_reg)); \/\/ interpreter_method_reg\n-  match(reg);\n-  match(iRegPNoSp);\n-  op_cost(0);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n@@ -14783,1 +14768,5 @@\n-    __ zero_words($base$$Register, $cnt$$Register);\n+    address tpc = __ zero_words($base$$Register, $cnt$$Register);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n@@ -16061,2 +16050,2 @@\n-  ins_encode( aarch64_enc_java_static_call(meth),\n-              aarch64_enc_call_epilog );\n+  ins_encode(aarch64_enc_java_static_call(meth),\n+             aarch64_enc_call_epilog);\n@@ -16080,2 +16069,2 @@\n-  ins_encode( aarch64_enc_java_dynamic_call(meth),\n-               aarch64_enc_call_epilog );\n+  ins_encode(aarch64_enc_java_dynamic_call(meth),\n+             aarch64_enc_call_epilog);\n@@ -16547,4 +16536,8 @@\n-    __ arrays_equals($ary1$$Register, $ary2$$Register,\n-                     $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n-                     $result$$Register, $tmp$$Register, 1);\n-    %}\n+    address tpc = __ arrays_equals($ary1$$Register, $ary2$$Register,\n+                                   $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n+                                   $result$$Register, $tmp$$Register, 1);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+  %}\n@@ -16564,3 +16557,7 @@\n-    __ arrays_equals($ary1$$Register, $ary2$$Register,\n-                     $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n-                     $result$$Register, $tmp$$Register, 2);\n+    address tpc = __ arrays_equals($ary1$$Register, $ary2$$Register,\n+                                   $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n+                                   $result$$Register, $tmp$$Register, 2);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n@@ -16577,1 +16574,5 @@\n-    __ has_negatives($ary1$$Register, $len$$Register, $result$$Register);\n+    address tpc = __ has_negatives($ary1$$Register, $len$$Register, $result$$Register);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n@@ -16610,2 +16611,7 @@\n-    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n-                          $tmp1$$FloatRegister, $tmp2$$FloatRegister, $tmp3$$FloatRegister, $tmp4$$Register);\n+    address tpc = __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n+                                        $tmp1$$FloatRegister, $tmp2$$FloatRegister,\n+                                        $tmp3$$FloatRegister, $tmp4$$Register);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":47,"deletions":41,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -709,1 +709,1 @@\n-address MacroAssembler::trampoline_call(Address entry, CodeBuffer *cbuf) {\n+address MacroAssembler::trampoline_call(Address entry, CodeBuffer* cbuf) {\n@@ -730,0 +730,1 @@\n+        postcond(pc() == badAddress);\n@@ -743,0 +744,1 @@\n+  postcond(pc() != badAddress);\n@@ -4558,1 +4560,1 @@\n-void MacroAssembler::has_negatives(Register ary1, Register len, Register result) {\n+address MacroAssembler::has_negatives(Register ary1, Register len, Register result) {\n@@ -4595,1 +4597,1 @@\n-    RuntimeAddress has_neg =  RuntimeAddress(StubRoutines::aarch64::has_negatives());\n+    RuntimeAddress has_neg = RuntimeAddress(StubRoutines::aarch64::has_negatives());\n@@ -4597,1 +4599,6 @@\n-    trampoline_call(has_neg);\n+    address tpc1 = trampoline_call(has_neg);\n+    if (tpc1 == NULL) {\n+      DEBUG_ONLY(reset_labels(STUB_LONG, SET_RESULT, DONE));\n+      postcond(pc() == badAddress);\n+      return NULL;\n+    }\n@@ -4601,2 +4608,1 @@\n-    RuntimeAddress has_neg_long =  RuntimeAddress(\n-            StubRoutines::aarch64::has_negatives_long());\n+    RuntimeAddress has_neg_long = RuntimeAddress(StubRoutines::aarch64::has_negatives_long());\n@@ -4604,1 +4610,6 @@\n-    trampoline_call(has_neg_long);\n+    address tpc2 = trampoline_call(has_neg_long);\n+    if (tpc2 == NULL) {\n+      DEBUG_ONLY(reset_labels(SET_RESULT, DONE));\n+      postcond(pc() == badAddress);\n+      return NULL;\n+    }\n@@ -4611,0 +4622,2 @@\n+  postcond(pc() != badAddress);\n+  return pc();\n@@ -4613,3 +4626,3 @@\n-void MacroAssembler::arrays_equals(Register a1, Register a2, Register tmp3,\n-                                   Register tmp4, Register tmp5, Register result,\n-                                   Register cnt1, int elem_size) {\n+address MacroAssembler::arrays_equals(Register a1, Register a2, Register tmp3,\n+                                      Register tmp4, Register tmp5, Register result,\n+                                      Register cnt1, int elem_size) {\n@@ -4719,1 +4732,1 @@\n-    Label NEXT_DWORD, SHORT, TAIL, TAIL2, STUB, EARLY_OUT,\n+    Label NEXT_DWORD, SHORT, TAIL, TAIL2, STUB,\n@@ -4778,1 +4791,6 @@\n-    trampoline_call(stub);\n+    address tpc = trampoline_call(stub);\n+    if (tpc == NULL) {\n+      DEBUG_ONLY(reset_labels(SHORT, LAST_CHECK, CSET_EQ, SAME, DONE));\n+      postcond(pc() == badAddress);\n+      return NULL;\n+    }\n@@ -4781,1 +4799,0 @@\n-    bind(EARLY_OUT);\n@@ -4808,0 +4825,2 @@\n+  postcond(pc() != badAddress);\n+  return pc();\n@@ -4915,1 +4934,1 @@\n-void MacroAssembler::zero_words(Register ptr, Register cnt)\n+address MacroAssembler::zero_words(Register ptr, Register cnt)\n@@ -4925,1 +4944,1 @@\n-    RuntimeAddress zero_blocks =  RuntimeAddress(StubRoutines::aarch64::zero_blocks());\n+    RuntimeAddress zero_blocks = RuntimeAddress(StubRoutines::aarch64::zero_blocks());\n@@ -4928,1 +4947,6 @@\n-      trampoline_call(zero_blocks);\n+      address tpc = trampoline_call(zero_blocks);\n+      if (tpc == NULL) {\n+        DEBUG_ONLY(reset_labels(around));\n+        postcond(pc() == badAddress);\n+        return NULL;\n+      }\n@@ -4949,0 +4973,2 @@\n+  postcond(pc() != badAddress);\n+  return pc();\n@@ -4961,1 +4987,1 @@\n-    for (; i < (int)cnt; i += 2)\n+    for (; i < (int)cnt; i += 2) {\n@@ -4963,0 +4989,1 @@\n+    }\n@@ -4966,1 +4993,1 @@\n-    for (; i < remainder; i += 2)\n+    for (; i < remainder; i += 2) {\n@@ -4968,1 +4995,1 @@\n-\n+    }\n@@ -4978,1 +5005,1 @@\n-    for (i = 1; i < unroll; i++)\n+    for (i = 1; i < unroll; i++) {\n@@ -4980,0 +5007,1 @@\n+    }\n@@ -5195,3 +5223,3 @@\n-void MacroAssembler::byte_array_inflate(Register src, Register dst, Register len,\n-                                        FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3,\n-                                        Register tmp4) {\n+address MacroAssembler::byte_array_inflate(Register src, Register dst, Register len,\n+                                           FloatRegister vtmp1, FloatRegister vtmp2,\n+                                           FloatRegister vtmp3, Register tmp4) {\n@@ -5234,1 +5262,1 @@\n-      RuntimeAddress stub =  RuntimeAddress(StubRoutines::aarch64::large_byte_array_inflate());\n+      RuntimeAddress stub = RuntimeAddress(StubRoutines::aarch64::large_byte_array_inflate());\n@@ -5236,1 +5264,6 @@\n-      trampoline_call(stub);\n+      address tpc = trampoline_call(stub);\n+      if (tpc == NULL) {\n+        DEBUG_ONLY(reset_labels(big, done));\n+        postcond(pc() == badAddress);\n+        return NULL;\n+      }\n@@ -5290,0 +5323,2 @@\n+  postcond(pc() != badAddress);\n+  return pc();\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":60,"deletions":25,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -1083,0 +1083,14 @@\n+#ifdef ASSERT\n+  \/\/ Template short-hand support to clean-up after a failed call to trampoline\n+  \/\/ call generation (see trampoline_call() below),  when a set of Labels must\n+  \/\/ be reset (before returning).\n+  template<typename Label, typename... More>\n+  void reset_labels(Label &lbl, More&... more) {\n+    lbl.reset(); reset_labels(more...);\n+  }\n+  template<typename Label>\n+  void reset_labels(Label &lbl) {\n+    lbl.reset();\n+  }\n+#endif\n+\n@@ -1086,1 +1100,1 @@\n-  address trampoline_call(Address entry, CodeBuffer *cbuf = NULL);\n+  address trampoline_call(Address entry, CodeBuffer* cbuf = NULL);\n@@ -1272,1 +1286,1 @@\n-  void has_negatives(Register ary1, Register len, Register result);\n+  address has_negatives(Register ary1, Register len, Register result);\n@@ -1274,2 +1288,2 @@\n-  void arrays_equals(Register a1, Register a2, Register result, Register cnt1,\n-                     Register tmp1, Register tmp2, Register tmp3, int elem_size);\n+  address arrays_equals(Register a1, Register a2, Register result, Register cnt1,\n+                        Register tmp1, Register tmp2, Register tmp3, int elem_size);\n@@ -1284,1 +1298,1 @@\n-  void zero_words(Register ptr, Register cnt);\n+  address zero_words(Register ptr, Register cnt);\n@@ -1289,3 +1303,3 @@\n-  void byte_array_inflate(Register src, Register dst, Register len,\n-                          FloatRegister vtmp1, FloatRegister vtmp2,\n-                          FloatRegister vtmp3, Register tmp4);\n+  address byte_array_inflate(Register src, Register dst, Register len,\n+                             FloatRegister vtmp1, FloatRegister vtmp2,\n+                             FloatRegister vtmp3, Register tmp4);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":22,"deletions":8,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -1351,14 +1351,0 @@\n-\n-\/\/ Check GCLocker::needs_gc and enter the runtime if it's true.  This\n-\/\/ keeps a new JNI critical region from starting until a GC has been\n-\/\/ forced.  Save down any oops in registers and describe them in an\n-\/\/ OopMap.\n-static void check_needs_gc_for_critical_native(MacroAssembler* masm,\n-                                               int stack_slots,\n-                                               int total_c_args,\n-                                               int total_in_args,\n-                                               int arg_save_area,\n-                                               OopMapSet* oop_maps,\n-                                               VMRegPair* in_regs,\n-                                               BasicType* in_sig_bt) { Unimplemented(); }\n-\n@@ -1530,4 +1516,3 @@\n-\/\/ passing them to the callee and perform checks before and after the\n-\/\/ native call to ensure that they GCLocker\n-\/\/ lock_critical\/unlock_critical semantics are followed.  Some other\n-\/\/ parts of JNI setup are skipped like the tear down of the JNI handle\n+\/\/ passing them to the callee. Critical native functions leave the state _in_Java,\n+\/\/ since they block out GC.\n+\/\/ Some other parts of JNI setup are skipped like the tear down of the JNI handle\n@@ -1537,12 +1522,0 @@\n-\/\/ They are roughly structured like this:\n-\/\/    if (GCLocker::needs_gc())\n-\/\/      SharedRuntime::block_for_jni_critical();\n-\/\/    tranistion to thread_in_native\n-\/\/    unpack arrray arguments and call native entry point\n-\/\/    check for safepoint in progress\n-\/\/    check if any thread suspend flags are set\n-\/\/      call into JVM and possible unlock the JNI critical\n-\/\/      if a GC was suppressed while in the critical native.\n-\/\/    transition back to thread_in_Java\n-\/\/    return to caller\n-\/\/\n@@ -1816,5 +1789,0 @@\n-  if (is_critical_native) {\n-    check_needs_gc_for_critical_native(masm, stack_slots, total_c_args, total_in_args,\n-                                       oop_handle_offset, oop_maps, in_regs, in_sig_bt);\n-  }\n-\n@@ -2094,5 +2062,5 @@\n-  }\n-  \/\/ Now set thread in native\n-  __ mov(rscratch1, _thread_in_native);\n-  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n-  __ stlrw(rscratch1, rscratch2);\n+    \/\/ Now set thread in native\n+    __ mov(rscratch1, _thread_in_native);\n+    __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n+    __ stlrw(rscratch1, rscratch2);\n+  }\n@@ -2128,0 +2096,15 @@\n+  Label safepoint_in_progress, safepoint_in_progress_done;\n+  Label after_transition;\n+\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    __ safepoint_poll(needs_safepoint, false \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/);\n+    __ ldrw(rscratch1, Address(rthread, JavaThread::suspend_flags_offset()));\n+    __ cbnzw(rscratch1, needs_safepoint);\n+    __ b(after_transition);\n+    __ bind(needs_safepoint);\n+  }\n+\n@@ -2148,1 +2131,0 @@\n-  Label safepoint_in_progress, safepoint_in_progress_done;\n@@ -2166,1 +2148,0 @@\n-  Label after_transition;\n@@ -2371,5 +2352,1 @@\n-    if (!is_critical_native) {\n-      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n-    } else {\n-      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition)));\n-    }\n+    __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n@@ -2381,6 +2358,0 @@\n-    if (is_critical_native) {\n-      \/\/ The call above performed the transition to thread_in_Java so\n-      \/\/ skip the transition logic above.\n-      __ b(after_transition);\n-    }\n-\n@@ -2435,5 +2406,0 @@\n-  if (is_critical_native) {\n-    nm->set_lazy_critical_native(true);\n-  }\n-\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":24,"deletions":58,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -3327,0 +3327,219 @@\n+  \/\/ Arguments:\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - byte[]  source+offset\n+  \/\/   c_rarg1   - byte[]   SHA.state\n+  \/\/   c_rarg2   - int     digest_length\n+  \/\/   c_rarg3   - int     offset\n+  \/\/   c_rarg4   - int     limit\n+  \/\/\n+  address generate_sha3_implCompress(bool multi_block, const char *name) {\n+    static const uint64_t round_consts[24] = {\n+      0x0000000000000001L, 0x0000000000008082L, 0x800000000000808AL,\n+      0x8000000080008000L, 0x000000000000808BL, 0x0000000080000001L,\n+      0x8000000080008081L, 0x8000000000008009L, 0x000000000000008AL,\n+      0x0000000000000088L, 0x0000000080008009L, 0x000000008000000AL,\n+      0x000000008000808BL, 0x800000000000008BL, 0x8000000000008089L,\n+      0x8000000000008003L, 0x8000000000008002L, 0x8000000000000080L,\n+      0x000000000000800AL, 0x800000008000000AL, 0x8000000080008081L,\n+      0x8000000000008080L, 0x0000000080000001L, 0x8000000080008008L\n+    };\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+    address start = __ pc();\n+\n+    Register buf           = c_rarg0;\n+    Register state         = c_rarg1;\n+    Register digest_length = c_rarg2;\n+    Register ofs           = c_rarg3;\n+    Register limit         = c_rarg4;\n+\n+    Label sha3_loop, rounds24_loop;\n+    Label sha3_512, sha3_384_or_224, sha3_256;\n+\n+    __ stpd(v8, v9, __ pre(sp, -64));\n+    __ stpd(v10, v11, Address(sp, 16));\n+    __ stpd(v12, v13, Address(sp, 32));\n+    __ stpd(v14, v15, Address(sp, 48));\n+\n+    \/\/ load state\n+    __ add(rscratch1, state, 32);\n+    __ ld1(v0, v1, v2,  v3,  __ T1D, state);\n+    __ ld1(v4, v5, v6,  v7,  __ T1D, __ post(rscratch1, 32));\n+    __ ld1(v8, v9, v10, v11, __ T1D, __ post(rscratch1, 32));\n+    __ ld1(v12, v13, v14, v15, __ T1D, __ post(rscratch1, 32));\n+    __ ld1(v16, v17, v18, v19, __ T1D, __ post(rscratch1, 32));\n+    __ ld1(v20, v21, v22, v23, __ T1D, __ post(rscratch1, 32));\n+    __ ld1(v24, __ T1D, rscratch1);\n+\n+    __ BIND(sha3_loop);\n+\n+    \/\/ 24 keccak rounds\n+    __ movw(rscratch2, 24);\n+\n+    \/\/ load round_constants base\n+    __ lea(rscratch1, ExternalAddress((address) round_consts));\n+\n+    \/\/ load input\n+    __ ld1(v25, v26, v27, v28, __ T8B, __ post(buf, 32));\n+    __ ld1(v29, v30, v31, __ T8B, __ post(buf, 24));\n+    __ eor(v0, __ T8B, v0, v25);\n+    __ eor(v1, __ T8B, v1, v26);\n+    __ eor(v2, __ T8B, v2, v27);\n+    __ eor(v3, __ T8B, v3, v28);\n+    __ eor(v4, __ T8B, v4, v29);\n+    __ eor(v5, __ T8B, v5, v30);\n+    __ eor(v6, __ T8B, v6, v31);\n+\n+    \/\/ digest_length == 64, SHA3-512\n+    __ tbnz(digest_length, 6, sha3_512);\n+\n+    __ ld1(v25, v26, v27, v28, __ T8B, __ post(buf, 32));\n+    __ ld1(v29, v30, __ T8B, __ post(buf, 16));\n+    __ eor(v7, __ T8B, v7, v25);\n+    __ eor(v8, __ T8B, v8, v26);\n+    __ eor(v9, __ T8B, v9, v27);\n+    __ eor(v10, __ T8B, v10, v28);\n+    __ eor(v11, __ T8B, v11, v29);\n+    __ eor(v12, __ T8B, v12, v30);\n+\n+    \/\/ digest_length == 28, SHA3-224;  digest_length == 48, SHA3-384\n+    __ tbnz(digest_length, 4, sha3_384_or_224);\n+\n+    \/\/ SHA3-256\n+    __ ld1(v25, v26, v27, v28, __ T8B, __ post(buf, 32));\n+    __ eor(v13, __ T8B, v13, v25);\n+    __ eor(v14, __ T8B, v14, v26);\n+    __ eor(v15, __ T8B, v15, v27);\n+    __ eor(v16, __ T8B, v16, v28);\n+    __ b(rounds24_loop);\n+\n+    __ BIND(sha3_384_or_224);\n+    __ tbz(digest_length, 2, rounds24_loop); \/\/ bit 2 cleared? SHA-384\n+\n+    \/\/ SHA3-224\n+    __ ld1(v25, v26, v27, v28, __ T8B, __ post(buf, 32));\n+    __ ld1(v29, __ T8B, __ post(buf, 8));\n+    __ eor(v13, __ T8B, v13, v25);\n+    __ eor(v14, __ T8B, v14, v26);\n+    __ eor(v15, __ T8B, v15, v27);\n+    __ eor(v16, __ T8B, v16, v28);\n+    __ eor(v17, __ T8B, v17, v29);\n+    __ b(rounds24_loop);\n+\n+    __ BIND(sha3_512);\n+    __ ld1(v25, v26, __ T8B, __ post(buf, 16));\n+    __ eor(v7, __ T8B, v7, v25);\n+    __ eor(v8, __ T8B, v8, v26);\n+\n+    __ BIND(rounds24_loop);\n+    __ subw(rscratch2, rscratch2, 1);\n+\n+    __ eor3(v29, __ T16B, v4, v9, v14);\n+    __ eor3(v26, __ T16B, v1, v6, v11);\n+    __ eor3(v28, __ T16B, v3, v8, v13);\n+    __ eor3(v25, __ T16B, v0, v5, v10);\n+    __ eor3(v27, __ T16B, v2, v7, v12);\n+    __ eor3(v29, __ T16B, v29, v19, v24);\n+    __ eor3(v26, __ T16B, v26, v16, v21);\n+    __ eor3(v28, __ T16B, v28, v18, v23);\n+    __ eor3(v25, __ T16B, v25, v15, v20);\n+    __ eor3(v27, __ T16B, v27, v17, v22);\n+\n+    __ rax1(v30, __ T2D, v29, v26);\n+    __ rax1(v26, __ T2D, v26, v28);\n+    __ rax1(v28, __ T2D, v28, v25);\n+    __ rax1(v25, __ T2D, v25, v27);\n+    __ rax1(v27, __ T2D, v27, v29);\n+\n+    __ eor(v0, __ T16B, v0, v30);\n+    __ xar(v29, __ T2D, v1,  v25, (64 - 1));\n+    __ xar(v1,  __ T2D, v6,  v25, (64 - 44));\n+    __ xar(v6,  __ T2D, v9,  v28, (64 - 20));\n+    __ xar(v9,  __ T2D, v22, v26, (64 - 61));\n+    __ xar(v22, __ T2D, v14, v28, (64 - 39));\n+    __ xar(v14, __ T2D, v20, v30, (64 - 18));\n+    __ xar(v31, __ T2D, v2,  v26, (64 - 62));\n+    __ xar(v2,  __ T2D, v12, v26, (64 - 43));\n+    __ xar(v12, __ T2D, v13, v27, (64 - 25));\n+    __ xar(v13, __ T2D, v19, v28, (64 - 8));\n+    __ xar(v19, __ T2D, v23, v27, (64 - 56));\n+    __ xar(v23, __ T2D, v15, v30, (64 - 41));\n+    __ xar(v15, __ T2D, v4,  v28, (64 - 27));\n+    __ xar(v28, __ T2D, v24, v28, (64 - 14));\n+    __ xar(v24, __ T2D, v21, v25, (64 - 2));\n+    __ xar(v8,  __ T2D, v8,  v27, (64 - 55));\n+    __ xar(v4,  __ T2D, v16, v25, (64 - 45));\n+    __ xar(v16, __ T2D, v5,  v30, (64 - 36));\n+    __ xar(v5,  __ T2D, v3,  v27, (64 - 28));\n+    __ xar(v27, __ T2D, v18, v27, (64 - 21));\n+    __ xar(v3,  __ T2D, v17, v26, (64 - 15));\n+    __ xar(v25, __ T2D, v11, v25, (64 - 10));\n+    __ xar(v26, __ T2D, v7,  v26, (64 - 6));\n+    __ xar(v30, __ T2D, v10, v30, (64 - 3));\n+\n+    __ bcax(v20, __ T16B, v31, v22, v8);\n+    __ bcax(v21, __ T16B, v8,  v23, v22);\n+    __ bcax(v22, __ T16B, v22, v24, v23);\n+    __ bcax(v23, __ T16B, v23, v31, v24);\n+    __ bcax(v24, __ T16B, v24, v8,  v31);\n+\n+    __ ld1r(v31, __ T2D, __ post(rscratch1, 8));\n+\n+    __ bcax(v17, __ T16B, v25, v19, v3);\n+    __ bcax(v18, __ T16B, v3,  v15, v19);\n+    __ bcax(v19, __ T16B, v19, v16, v15);\n+    __ bcax(v15, __ T16B, v15, v25, v16);\n+    __ bcax(v16, __ T16B, v16, v3,  v25);\n+\n+    __ bcax(v10, __ T16B, v29, v12, v26);\n+    __ bcax(v11, __ T16B, v26, v13, v12);\n+    __ bcax(v12, __ T16B, v12, v14, v13);\n+    __ bcax(v13, __ T16B, v13, v29, v14);\n+    __ bcax(v14, __ T16B, v14, v26, v29);\n+\n+    __ bcax(v7, __ T16B, v30, v9,  v4);\n+    __ bcax(v8, __ T16B, v4,  v5,  v9);\n+    __ bcax(v9, __ T16B, v9,  v6,  v5);\n+    __ bcax(v5, __ T16B, v5,  v30, v6);\n+    __ bcax(v6, __ T16B, v6,  v4,  v30);\n+\n+    __ bcax(v3, __ T16B, v27, v0,  v28);\n+    __ bcax(v4, __ T16B, v28, v1,  v0);\n+    __ bcax(v0, __ T16B, v0,  v2,  v1);\n+    __ bcax(v1, __ T16B, v1,  v27, v2);\n+    __ bcax(v2, __ T16B, v2,  v28, v27);\n+\n+    __ eor(v0, __ T16B, v0, v31);\n+\n+    __ cbnzw(rscratch2, rounds24_loop);\n+\n+    if (multi_block) {\n+      \/\/ block_size =  200 - 2 * digest_length, ofs += block_size\n+      __ add(ofs, ofs, 200);\n+      __ sub(ofs, ofs, digest_length, Assembler::LSL, 1);\n+\n+      __ cmp(ofs, limit);\n+      __ br(Assembler::LE, sha3_loop);\n+      __ mov(c_rarg0, ofs); \/\/ return ofs\n+    }\n+\n+    __ st1(v0, v1, v2,  v3,  __ T1D, __ post(state, 32));\n+    __ st1(v4, v5, v6,  v7,  __ T1D, __ post(state, 32));\n+    __ st1(v8, v9, v10, v11, __ T1D, __ post(state, 32));\n+    __ st1(v12, v13, v14, v15, __ T1D, __ post(state, 32));\n+    __ st1(v16, v17, v18, v19, __ T1D, __ post(state, 32));\n+    __ st1(v20, v21, v22, v23, __ T1D, __ post(state, 32));\n+    __ st1(v24, __ T1D, state);\n+\n+    __ ldpd(v14, v15, Address(sp, 48));\n+    __ ldpd(v12, v13, Address(sp, 32));\n+    __ ldpd(v10, v11, Address(sp, 16));\n+    __ ldpd(v8, v9, __ post(sp, 64));\n+\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -6249,0 +6468,4 @@\n+    if (UseSHA3Intrinsics) {\n+      StubRoutines::_sha3_implCompress     = generate_sha3_implCompress(false,   \"sha3_implCompress\");\n+      StubRoutines::_sha3_implCompressMB   = generate_sha3_implCompress(true,    \"sha3_implCompressMB\");\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":223,"deletions":0,"binary":false,"changes":223,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-  int call_offset;\n+  int call_offset = -1;\n@@ -136,0 +136,2 @@\n+\n+  assert(call_offset >= 0, \"Should be set\");\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2725,1 +2725,1 @@\n-  addl(result, ch);\n+  addptr(result, ch);\n@@ -2819,1 +2819,1 @@\n-  addl(result, ch);\n+  addptr(result, ch);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4423,0 +4423,3 @@\n+      default:\n+        rc = NULL; \/\/ silence compiler warnings\n+        fatal(\"Unknown rounding control: %d\", rounding_control());\n@@ -4431,0 +4434,3 @@\n+      default:\n+        pc = NULL; \/\/ silence compiler warnings\n+        fatal(\"Unknown precision control: %d\", precision_control());\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1234,260 +1234,0 @@\n-\n-static void save_or_restore_arguments(MacroAssembler* masm,\n-                                      const int stack_slots,\n-                                      const int total_in_args,\n-                                      const int arg_save_area,\n-                                      OopMap* map,\n-                                      VMRegPair* in_regs,\n-                                      BasicType* in_sig_bt) {\n-  \/\/ if map is non-NULL then the code should store the values,\n-  \/\/ otherwise it should load them.\n-  int handle_index = 0;\n-  \/\/ Save down double word first\n-  for ( int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_XMMRegister() && in_sig_bt[i] == T_DOUBLE) {\n-      int slot = handle_index * VMRegImpl::slots_per_word + arg_save_area;\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      handle_index += 2;\n-      assert(handle_index <= stack_slots, \"overflow\");\n-      if (map != NULL) {\n-        __ movdbl(Address(rsp, offset), in_regs[i].first()->as_XMMRegister());\n-      } else {\n-        __ movdbl(in_regs[i].first()->as_XMMRegister(), Address(rsp, offset));\n-      }\n-    }\n-    if (in_regs[i].first()->is_Register() && in_sig_bt[i] == T_LONG) {\n-      int slot = handle_index * VMRegImpl::slots_per_word + arg_save_area;\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      handle_index += 2;\n-      assert(handle_index <= stack_slots, \"overflow\");\n-      if (map != NULL) {\n-        __ movl(Address(rsp, offset), in_regs[i].first()->as_Register());\n-        if (in_regs[i].second()->is_Register()) {\n-          __ movl(Address(rsp, offset + 4), in_regs[i].second()->as_Register());\n-        }\n-      } else {\n-        __ movl(in_regs[i].first()->as_Register(), Address(rsp, offset));\n-        if (in_regs[i].second()->is_Register()) {\n-          __ movl(in_regs[i].second()->as_Register(), Address(rsp, offset + 4));\n-        }\n-      }\n-    }\n-  }\n-  \/\/ Save or restore single word registers\n-  for ( int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_Register()) {\n-      int slot = handle_index++ * VMRegImpl::slots_per_word + arg_save_area;\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      assert(handle_index <= stack_slots, \"overflow\");\n-      if (in_sig_bt[i] == T_ARRAY && map != NULL) {\n-        map->set_oop(VMRegImpl::stack2reg(slot));;\n-      }\n-\n-      \/\/ Value is in an input register pass we must flush it to the stack\n-      const Register reg = in_regs[i].first()->as_Register();\n-      switch (in_sig_bt[i]) {\n-        case T_ARRAY:\n-          if (map != NULL) {\n-            __ movptr(Address(rsp, offset), reg);\n-          } else {\n-            __ movptr(reg, Address(rsp, offset));\n-          }\n-          break;\n-        case T_BOOLEAN:\n-        case T_CHAR:\n-        case T_BYTE:\n-        case T_SHORT:\n-        case T_INT:\n-          if (map != NULL) {\n-            __ movl(Address(rsp, offset), reg);\n-          } else {\n-            __ movl(reg, Address(rsp, offset));\n-          }\n-          break;\n-        case T_OBJECT:\n-        case T_INLINE_TYPE:\n-        default: ShouldNotReachHere();\n-      }\n-    } else if (in_regs[i].first()->is_XMMRegister()) {\n-      if (in_sig_bt[i] == T_FLOAT) {\n-        int slot = handle_index++ * VMRegImpl::slots_per_word + arg_save_area;\n-        int offset = slot * VMRegImpl::stack_slot_size;\n-        assert(handle_index <= stack_slots, \"overflow\");\n-        if (map != NULL) {\n-          __ movflt(Address(rsp, offset), in_regs[i].first()->as_XMMRegister());\n-        } else {\n-          __ movflt(in_regs[i].first()->as_XMMRegister(), Address(rsp, offset));\n-        }\n-      }\n-    } else if (in_regs[i].first()->is_stack()) {\n-      if (in_sig_bt[i] == T_ARRAY && map != NULL) {\n-        int offset_in_older_frame = in_regs[i].first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-        map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ Registers need to be saved for runtime call\n-static Register caller_saved_registers[] = {\n-  rcx, rdx, rsi, rdi\n-};\n-\n-\/\/ Save caller saved registers except r1 and r2\n-static void save_registers_except(MacroAssembler* masm, Register r1, Register r2) {\n-  int reg_len = (int)(sizeof(caller_saved_registers) \/ sizeof(Register));\n-  for (int index = 0; index < reg_len; index ++) {\n-    Register this_reg = caller_saved_registers[index];\n-    if (this_reg != r1 && this_reg != r2) {\n-      __ push(this_reg);\n-    }\n-  }\n-}\n-\n-\/\/ Restore caller saved registers except r1 and r2\n-static void restore_registers_except(MacroAssembler* masm, Register r1, Register r2) {\n-  int reg_len = (int)(sizeof(caller_saved_registers) \/ sizeof(Register));\n-  for (int index = reg_len - 1; index >= 0; index --) {\n-    Register this_reg = caller_saved_registers[index];\n-    if (this_reg != r1 && this_reg != r2) {\n-      __ pop(this_reg);\n-    }\n-  }\n-}\n-\n-\/\/ Pin object, return pinned object or null in rax\n-static void gen_pin_object(MacroAssembler* masm,\n-                           Register thread, VMRegPair reg) {\n-  __ block_comment(\"gen_pin_object {\");\n-\n-  Label is_null;\n-  Register tmp_reg = rax;\n-  VMRegPair tmp(tmp_reg->as_VMReg());\n-  if (reg.first()->is_stack()) {\n-    \/\/ Load the arg up from the stack\n-    simple_move32(masm, reg, tmp);\n-    reg = tmp;\n-  } else {\n-    __ movl(tmp_reg, reg.first()->as_Register());\n-  }\n-  __ testptr(reg.first()->as_Register(), reg.first()->as_Register());\n-  __ jccb(Assembler::equal, is_null);\n-\n-  \/\/ Save registers that may be used by runtime call\n-  Register arg = reg.first()->is_Register() ? reg.first()->as_Register() : noreg;\n-  save_registers_except(masm, arg, thread);\n-\n-  __ call_VM_leaf(\n-    CAST_FROM_FN_PTR(address, SharedRuntime::pin_object),\n-    thread, reg.first()->as_Register());\n-\n-  \/\/ Restore saved registers\n-  restore_registers_except(masm, arg, thread);\n-\n-  __ bind(is_null);\n-  __ block_comment(\"} gen_pin_object\");\n-}\n-\n-\/\/ Unpin object\n-static void gen_unpin_object(MacroAssembler* masm,\n-                             Register thread, VMRegPair reg) {\n-  __ block_comment(\"gen_unpin_object {\");\n-  Label is_null;\n-\n-  \/\/ temp register\n-  __ push(rax);\n-  Register tmp_reg = rax;\n-  VMRegPair tmp(tmp_reg->as_VMReg());\n-\n-  simple_move32(masm, reg, tmp);\n-\n-  __ testptr(rax, rax);\n-  __ jccb(Assembler::equal, is_null);\n-\n-  \/\/ Save registers that may be used by runtime call\n-  Register arg = reg.first()->is_Register() ? reg.first()->as_Register() : noreg;\n-  save_registers_except(masm, arg, thread);\n-\n-  __ call_VM_leaf(\n-    CAST_FROM_FN_PTR(address, SharedRuntime::unpin_object),\n-    thread, rax);\n-\n-  \/\/ Restore saved registers\n-  restore_registers_except(masm, arg, thread);\n-  __ bind(is_null);\n-  __ pop(rax);\n-  __ block_comment(\"} gen_unpin_object\");\n-}\n-\n-\/\/ Check GCLocker::needs_gc and enter the runtime if it's true.  This\n-\/\/ keeps a new JNI critical region from starting until a GC has been\n-\/\/ forced.  Save down any oops in registers and describe them in an\n-\/\/ OopMap.\n-static void check_needs_gc_for_critical_native(MacroAssembler* masm,\n-                                               Register thread,\n-                                               int stack_slots,\n-                                               int total_c_args,\n-                                               int total_in_args,\n-                                               int arg_save_area,\n-                                               OopMapSet* oop_maps,\n-                                               VMRegPair* in_regs,\n-                                               BasicType* in_sig_bt) {\n-  __ block_comment(\"check GCLocker::needs_gc\");\n-  Label cont;\n-  __ cmp8(ExternalAddress((address)GCLocker::needs_gc_address()), false);\n-  __ jcc(Assembler::equal, cont);\n-\n-  \/\/ Save down any incoming oops and call into the runtime to halt for a GC\n-\n-  OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, map, in_regs, in_sig_bt);\n-\n-  address the_pc = __ pc();\n-  oop_maps->add_gc_map( __ offset(), map);\n-  __ set_last_Java_frame(thread, rsp, noreg, the_pc);\n-\n-  __ block_comment(\"block_for_jni_critical\");\n-  __ push(thread);\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::block_for_jni_critical)));\n-  __ increment(rsp, wordSize);\n-\n-  __ get_thread(thread);\n-  __ reset_last_Java_frame(thread, false);\n-\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, NULL, in_regs, in_sig_bt);\n-\n-  __ bind(cont);\n-#ifdef ASSERT\n-  if (StressCriticalJNINatives) {\n-    \/\/ Stress register saving\n-    OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, map, in_regs, in_sig_bt);\n-    \/\/ Destroy argument registers\n-    for (int i = 0; i < total_in_args - 1; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        const Register reg = in_regs[i].first()->as_Register();\n-        __ xorptr(reg, reg);\n-      } else if (in_regs[i].first()->is_XMMRegister()) {\n-        __ xorpd(in_regs[i].first()->as_XMMRegister(), in_regs[i].first()->as_XMMRegister());\n-      } else if (in_regs[i].first()->is_FloatRegister()) {\n-        ShouldNotReachHere();\n-      } else if (in_regs[i].first()->is_stack()) {\n-        \/\/ Nothing to do\n-      } else {\n-        ShouldNotReachHere();\n-      }\n-      if (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_DOUBLE) {\n-        i++;\n-      }\n-    }\n-\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, NULL, in_regs, in_sig_bt);\n-  }\n-#endif\n-}\n-\n@@ -1618,4 +1358,3 @@\n-\/\/ passing them to the callee and perform checks before and after the\n-\/\/ native call to ensure that they GCLocker\n-\/\/ lock_critical\/unlock_critical semantics are followed.  Some other\n-\/\/ parts of JNI setup are skipped like the tear down of the JNI handle\n+\/\/ passing them to the callee. Critical native functions leave the state _in_Java,\n+\/\/ since they cannot stop for GC.\n+\/\/ Some other parts of JNI setup are skipped like the tear down of the JNI handle\n@@ -1625,11 +1364,0 @@\n-\/\/ They are roughly structured like this:\n-\/\/    if (GCLocker::needs_gc())\n-\/\/      SharedRuntime::block_for_jni_critical();\n-\/\/    tranistion to thread_in_native\n-\/\/    unpack arrray arguments and call native entry point\n-\/\/    check for safepoint in progress\n-\/\/    check if any thread suspend flags are set\n-\/\/      call into JVM and possible unlock the JNI critical\n-\/\/      if a GC was suppressed while in the critical native.\n-\/\/    transition back to thread_in_Java\n-\/\/    return to caller\n@@ -1947,5 +1675,0 @@\n-  if (is_critical_native && !Universe::heap()->supports_object_pinning()) {\n-    check_needs_gc_for_critical_native(masm, thread, stack_slots, total_c_args, total_in_args,\n-                                       oop_handle_offset, oop_maps, in_regs, in_sig_bt);\n-  }\n-\n@@ -1985,5 +1708,0 @@\n-  \/\/ Inbound arguments that need to be pinned for critical natives\n-  GrowableArray<int> pinned_args(total_in_args);\n-  \/\/ Current stack slot for storing register based array argument\n-  int pinned_slot = oop_handle_offset;\n-\n@@ -2002,20 +1720,0 @@\n-          if (Universe::heap()->supports_object_pinning()) {\n-            \/\/ gen_pin_object handles save and restore\n-            \/\/ of any clobbered registers\n-            gen_pin_object(masm, thread, in_arg);\n-            pinned_args.append(i);\n-\n-            \/\/ rax has pinned array\n-            VMRegPair result_reg(rax->as_VMReg());\n-            if (!in_arg.first()->is_stack()) {\n-              assert(pinned_slot <= stack_slots, \"overflow\");\n-              simple_move32(masm, result_reg, VMRegImpl::stack2reg(pinned_slot));\n-              pinned_slot += VMRegImpl::slots_per_word;\n-            } else {\n-              \/\/ Write back pinned value, it will be used to unpin this argument\n-              __ movptr(Address(rbp, reg2offset_in(in_arg.first())), result_reg.first()->as_Register());\n-            }\n-            \/\/ We have the array in register, use it\n-            in_arg = result_reg;\n-          }\n-\n@@ -2177,1 +1875,0 @@\n-\n@@ -2182,3 +1879,3 @@\n-  }\n-  \/\/ Now set thread in native\n-  __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_native);\n+    \/\/ Now set thread in native\n+    __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_native);\n+  }\n@@ -2217,18 +1914,11 @@\n-  \/\/ unpin pinned arguments\n-  pinned_slot = oop_handle_offset;\n-  if (pinned_args.length() > 0) {\n-    \/\/ save return value that may be overwritten otherwise.\n-    save_native_result(masm, ret_type, stack_slots);\n-    for (int index = 0; index < pinned_args.length(); index ++) {\n-      int i = pinned_args.at(index);\n-      assert(pinned_slot <= stack_slots, \"overflow\");\n-      if (!in_regs[i].first()->is_stack()) {\n-        int offset = pinned_slot * VMRegImpl::stack_slot_size;\n-        __ movl(in_regs[i].first()->as_Register(), Address(rsp, offset));\n-        pinned_slot += VMRegImpl::slots_per_word;\n-      }\n-      \/\/ gen_pin_object handles save and restore\n-      \/\/ of any other clobbered registers\n-      gen_unpin_object(masm, thread, in_regs[i]);\n-    }\n-    restore_native_result(masm, ret_type, stack_slots);\n+  Label after_transition;\n+\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    __ safepoint_poll(needs_safepoint, thread, false \/* at_return *\/, false \/* in_nmethod *\/);\n+    __ cmpl(Address(thread, JavaThread::suspend_flags_offset()), 0);\n+    __ jcc(Assembler::equal, after_transition);\n+    __ bind(needs_safepoint);\n@@ -2256,2 +1946,0 @@\n-  Label after_transition;\n-\n@@ -2277,2 +1965,1 @@\n-    if (!is_critical_native) {\n-      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address,\n+    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address,\n@@ -2280,4 +1967,0 @@\n-    } else {\n-      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address,\n-                                              JavaThread::check_special_condition_for_native_trans_and_transition)));\n-    }\n@@ -2287,7 +1970,0 @@\n-\n-    if (is_critical_native) {\n-      \/\/ The call above performed the transition to thread_in_Java so\n-      \/\/ skip the transition logic below.\n-      __ jmpb(after_transition);\n-    }\n-\n@@ -2534,4 +2210,0 @@\n-  if (is_critical_native) {\n-    nm->set_lazy_critical_native(true);\n-  }\n-\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":18,"deletions":346,"binary":false,"changes":364,"status":"modified"},{"patch":"@@ -1636,217 +1636,0 @@\n-\n-static void save_or_restore_arguments(MacroAssembler* masm,\n-                                      const int stack_slots,\n-                                      const int total_in_args,\n-                                      const int arg_save_area,\n-                                      OopMap* map,\n-                                      VMRegPair* in_regs,\n-                                      BasicType* in_sig_bt) {\n-  \/\/ if map is non-NULL then the code should store the values,\n-  \/\/ otherwise it should load them.\n-  int slot = arg_save_area;\n-  \/\/ Save down double word first\n-  for ( int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_XMMRegister() && in_sig_bt[i] == T_DOUBLE) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      slot += VMRegImpl::slots_per_word;\n-      assert(slot <= stack_slots, \"overflow\");\n-      if (map != NULL) {\n-        __ movdbl(Address(rsp, offset), in_regs[i].first()->as_XMMRegister());\n-      } else {\n-        __ movdbl(in_regs[i].first()->as_XMMRegister(), Address(rsp, offset));\n-      }\n-    }\n-    if (in_regs[i].first()->is_Register() &&\n-        (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_ARRAY)) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      if (map != NULL) {\n-        __ movq(Address(rsp, offset), in_regs[i].first()->as_Register());\n-        if (in_sig_bt[i] == T_ARRAY) {\n-          map->set_oop(VMRegImpl::stack2reg(slot));\n-        }\n-      } else {\n-        __ movq(in_regs[i].first()->as_Register(), Address(rsp, offset));\n-      }\n-      slot += VMRegImpl::slots_per_word;\n-    }\n-  }\n-  \/\/ Save or restore single word registers\n-  for ( int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_Register()) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      slot++;\n-      assert(slot <= stack_slots, \"overflow\");\n-\n-      \/\/ Value is in an input register pass we must flush it to the stack\n-      const Register reg = in_regs[i].first()->as_Register();\n-      switch (in_sig_bt[i]) {\n-        case T_BOOLEAN:\n-        case T_CHAR:\n-        case T_BYTE:\n-        case T_SHORT:\n-        case T_INT:\n-          if (map != NULL) {\n-            __ movl(Address(rsp, offset), reg);\n-          } else {\n-            __ movl(reg, Address(rsp, offset));\n-          }\n-          break;\n-        case T_ARRAY:\n-        case T_LONG:\n-          \/\/ handled above\n-          break;\n-        case T_OBJECT:\n-        case T_INLINE_TYPE:\n-        default: ShouldNotReachHere();\n-      }\n-    } else if (in_regs[i].first()->is_XMMRegister()) {\n-      if (in_sig_bt[i] == T_FLOAT) {\n-        int offset = slot * VMRegImpl::stack_slot_size;\n-        slot++;\n-        assert(slot <= stack_slots, \"overflow\");\n-        if (map != NULL) {\n-          __ movflt(Address(rsp, offset), in_regs[i].first()->as_XMMRegister());\n-        } else {\n-          __ movflt(in_regs[i].first()->as_XMMRegister(), Address(rsp, offset));\n-        }\n-      }\n-    } else if (in_regs[i].first()->is_stack()) {\n-      if (in_sig_bt[i] == T_ARRAY && map != NULL) {\n-        int offset_in_older_frame = in_regs[i].first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-        map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ Pin object, return pinned object or null in rax\n-static void gen_pin_object(MacroAssembler* masm,\n-                           VMRegPair reg) {\n-  __ block_comment(\"gen_pin_object {\");\n-\n-  \/\/ rax always contains oop, either incoming or\n-  \/\/ pinned.\n-  Register tmp_reg = rax;\n-\n-  Label is_null;\n-  VMRegPair tmp;\n-  VMRegPair in_reg = reg;\n-\n-  tmp.set_ptr(tmp_reg->as_VMReg());\n-  if (reg.first()->is_stack()) {\n-    \/\/ Load the arg up from the stack\n-    move_ptr(masm, reg, tmp);\n-    reg = tmp;\n-  } else {\n-    __ movptr(rax, reg.first()->as_Register());\n-  }\n-  __ testptr(reg.first()->as_Register(), reg.first()->as_Register());\n-  __ jccb(Assembler::equal, is_null);\n-\n-  if (reg.first()->as_Register() != c_rarg1) {\n-    __ movptr(c_rarg1, reg.first()->as_Register());\n-  }\n-\n-  __ call_VM_leaf(\n-    CAST_FROM_FN_PTR(address, SharedRuntime::pin_object),\n-    r15_thread, c_rarg1);\n-\n-  __ bind(is_null);\n-  __ block_comment(\"} gen_pin_object\");\n-}\n-\n-\/\/ Unpin object\n-static void gen_unpin_object(MacroAssembler* masm,\n-                             VMRegPair reg) {\n-  __ block_comment(\"gen_unpin_object {\");\n-  Label is_null;\n-\n-  if (reg.first()->is_stack()) {\n-    __ movptr(c_rarg1, Address(rbp, reg2offset_in(reg.first())));\n-  } else if (reg.first()->as_Register() != c_rarg1) {\n-    __ movptr(c_rarg1, reg.first()->as_Register());\n-  }\n-\n-  __ testptr(c_rarg1, c_rarg1);\n-  __ jccb(Assembler::equal, is_null);\n-\n-  __ call_VM_leaf(\n-    CAST_FROM_FN_PTR(address, SharedRuntime::unpin_object),\n-    r15_thread, c_rarg1);\n-\n-  __ bind(is_null);\n-  __ block_comment(\"} gen_unpin_object\");\n-}\n-\n-\/\/ Check GCLocker::needs_gc and enter the runtime if it's true.  This\n-\/\/ keeps a new JNI critical region from starting until a GC has been\n-\/\/ forced.  Save down any oops in registers and describe them in an\n-\/\/ OopMap.\n-static void check_needs_gc_for_critical_native(MacroAssembler* masm,\n-                                               int stack_slots,\n-                                               int total_c_args,\n-                                               int total_in_args,\n-                                               int arg_save_area,\n-                                               OopMapSet* oop_maps,\n-                                               VMRegPair* in_regs,\n-                                               BasicType* in_sig_bt) {\n-  __ block_comment(\"check GCLocker::needs_gc\");\n-  Label cont;\n-  __ cmp8(ExternalAddress((address)GCLocker::needs_gc_address()), false);\n-  __ jcc(Assembler::equal, cont);\n-\n-  \/\/ Save down any incoming oops and call into the runtime to halt for a GC\n-\n-  OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, map, in_regs, in_sig_bt);\n-\n-  address the_pc = __ pc();\n-  oop_maps->add_gc_map( __ offset(), map);\n-  __ set_last_Java_frame(rsp, noreg, the_pc);\n-\n-  __ block_comment(\"block_for_jni_critical\");\n-  __ movptr(c_rarg0, r15_thread);\n-  __ mov(r12, rsp); \/\/ remember sp\n-  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n-  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::block_for_jni_critical)));\n-  __ mov(rsp, r12); \/\/ restore sp\n-  __ reinit_heapbase();\n-\n-  __ reset_last_Java_frame(false);\n-\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, NULL, in_regs, in_sig_bt);\n-  __ bind(cont);\n-#ifdef ASSERT\n-  if (StressCriticalJNINatives) {\n-    \/\/ Stress register saving\n-    OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, map, in_regs, in_sig_bt);\n-    \/\/ Destroy argument registers\n-    for (int i = 0; i < total_in_args - 1; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        const Register reg = in_regs[i].first()->as_Register();\n-        __ xorptr(reg, reg);\n-      } else if (in_regs[i].first()->is_XMMRegister()) {\n-        __ xorpd(in_regs[i].first()->as_XMMRegister(), in_regs[i].first()->as_XMMRegister());\n-      } else if (in_regs[i].first()->is_FloatRegister()) {\n-        ShouldNotReachHere();\n-      } else if (in_regs[i].first()->is_stack()) {\n-        \/\/ Nothing to do\n-      } else {\n-        ShouldNotReachHere();\n-      }\n-      if (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_DOUBLE) {\n-        i++;\n-      }\n-    }\n-\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, NULL, in_regs, in_sig_bt);\n-  }\n-#endif\n-}\n-\n@@ -2157,4 +1940,3 @@\n-\/\/ passing them to the callee and perform checks before and after the\n-\/\/ native call to ensure that they GCLocker\n-\/\/ lock_critical\/unlock_critical semantics are followed.  Some other\n-\/\/ parts of JNI setup are skipped like the tear down of the JNI handle\n+\/\/ passing them to the callee. Critical native functions leave the state _in_Java,\n+\/\/ since they cannot stop for GC.\n+\/\/ Some other parts of JNI setup are skipped like the tear down of the JNI handle\n@@ -2164,12 +1946,0 @@\n-\/\/ They are roughly structured like this:\n-\/\/    if (GCLocker::needs_gc())\n-\/\/      SharedRuntime::block_for_jni_critical();\n-\/\/    tranistion to thread_in_native\n-\/\/    unpack arrray arguments and call native entry point\n-\/\/    check for safepoint in progress\n-\/\/    check if any thread suspend flags are set\n-\/\/      call into JVM and possible unlock the JNI critical\n-\/\/      if a GC was suppressed while in the critical native.\n-\/\/    transition back to thread_in_Java\n-\/\/    return to caller\n-\/\/\n@@ -2476,5 +2246,0 @@\n-  if (is_critical_native && !Universe::heap()->supports_object_pinning()) {\n-    check_needs_gc_for_critical_native(masm, stack_slots, total_c_args, total_in_args,\n-                                       oop_handle_offset, oop_maps, in_regs, in_sig_bt);\n-  }\n-\n@@ -2533,4 +2298,0 @@\n-  \/\/ Inbound arguments that need to be pinned for critical natives\n-  GrowableArray<int> pinned_args(total_in_args);\n-  \/\/ Current stack slot for storing register based array argument\n-  int pinned_slot = oop_handle_offset;\n@@ -2585,17 +2346,0 @@\n-          \/\/ pin before unpack\n-          if (Universe::heap()->supports_object_pinning()) {\n-            save_args(masm, total_c_args, 0, out_regs);\n-            gen_pin_object(masm, in_regs[i]);\n-            pinned_args.append(i);\n-            restore_args(masm, total_c_args, 0, out_regs);\n-\n-            \/\/ rax has pinned array\n-            VMRegPair result_reg;\n-            result_reg.set_ptr(rax->as_VMReg());\n-            move_ptr(masm, result_reg, in_regs[i]);\n-            if (!in_regs[i].first()->is_stack()) {\n-              assert(pinned_slot <= stack_slots, \"overflow\");\n-              move_ptr(masm, result_reg, VMRegImpl::stack2reg(pinned_slot));\n-              pinned_slot += VMRegImpl::slots_per_word;\n-            }\n-          }\n@@ -2787,1 +2531,0 @@\n-\n@@ -2790,1 +2533,0 @@\n-\n@@ -2794,3 +2536,3 @@\n-  }\n-  \/\/ Now set thread in native\n-  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n+    \/\/ Now set thread in native\n+    __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n+  }\n@@ -2824,16 +2566,11 @@\n-  \/\/ unpin pinned arguments\n-  pinned_slot = oop_handle_offset;\n-  if (pinned_args.length() > 0) {\n-    \/\/ save return value that may be overwritten otherwise.\n-    save_native_result(masm, ret_type, stack_slots);\n-    for (int index = 0; index < pinned_args.length(); index ++) {\n-      int i = pinned_args.at(index);\n-      assert(pinned_slot <= stack_slots, \"overflow\");\n-      if (!in_regs[i].first()->is_stack()) {\n-        int offset = pinned_slot * VMRegImpl::stack_slot_size;\n-        __ movq(in_regs[i].first()->as_Register(), Address(rsp, offset));\n-        pinned_slot += VMRegImpl::slots_per_word;\n-      }\n-      gen_unpin_object(masm, in_regs[i]);\n-    }\n-    restore_native_result(masm, ret_type, stack_slots);\n+  Label after_transition;\n+\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    __ safepoint_poll(needs_safepoint, r15_thread, false \/* at_return *\/, false \/* in_nmethod *\/);\n+    __ cmpl(Address(r15_thread, JavaThread::suspend_flags_offset()), 0);\n+    __ jcc(Assembler::equal, after_transition);\n+    __ bind(needs_safepoint);\n@@ -2856,2 +2593,0 @@\n-  Label after_transition;\n-\n@@ -2881,5 +2616,1 @@\n-    if (!is_critical_native) {\n-      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n-    } else {\n-      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition)));\n-    }\n+    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n@@ -2890,7 +2621,0 @@\n-\n-    if (is_critical_native) {\n-      \/\/ The call above performed the transition to thread_in_Java so\n-      \/\/ skip the transition logic below.\n-      __ jmpb(after_transition);\n-    }\n-\n@@ -3120,5 +2844,0 @@\n-  if (is_critical_native) {\n-    nm->set_lazy_critical_native(true);\n-  }\n-\n-\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":18,"deletions":299,"binary":false,"changes":317,"status":"modified"},{"patch":"@@ -984,0 +984,5 @@\n+  if (UseSHA3Intrinsics) {\n+    warning(\"Intrinsics for SHA3-224, SHA3-256, SHA3-384 and SHA3-512 crypto hash functions not available on this CPU.\");\n+    FLAG_SET_DEFAULT(UseSHA3Intrinsics, false);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -134,3 +134,1 @@\n-\/\/ 2) reg_class compiler_method_reg        ( \/* as def'd in frame section *\/ )\n-\/\/ 2) reg_class interpreter_method_reg     ( \/* as def'd in frame section *\/ )\n-\/\/ 3) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n+\/\/ 2) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n@@ -153,1 +151,0 @@\n-\/\/ This register class can be used for implicit null checks on win95.\n@@ -1446,51 +1443,0 @@\n-\n-void Matcher::pd_implicit_null_fixup(MachNode *node, uint idx) {\n-  \/\/ Get the memory operand from the node\n-  uint numopnds = node->num_opnds();        \/\/ Virtual call for number of operands\n-  uint skipped  = node->oper_input_base();  \/\/ Sum of leaves skipped so far\n-  assert( idx >= skipped, \"idx too low in pd_implicit_null_fixup\" );\n-  uint opcnt     = 1;                 \/\/ First operand\n-  uint num_edges = node->_opnds[1]->num_edges(); \/\/ leaves for first operand\n-  while( idx >= skipped+num_edges ) {\n-    skipped += num_edges;\n-    opcnt++;                          \/\/ Bump operand count\n-    assert( opcnt < numopnds, \"Accessing non-existent operand\" );\n-    num_edges = node->_opnds[opcnt]->num_edges(); \/\/ leaves for next operand\n-  }\n-\n-  MachOper *memory = node->_opnds[opcnt];\n-  MachOper *new_memory = NULL;\n-  switch (memory->opcode()) {\n-  case DIRECT:\n-  case INDOFFSET32X:\n-    \/\/ No transformation necessary.\n-    return;\n-  case INDIRECT:\n-    new_memory = new indirect_win95_safeOper( );\n-    break;\n-  case INDOFFSET8:\n-    new_memory = new indOffset8_win95_safeOper(memory->disp(NULL, NULL, 0));\n-    break;\n-  case INDOFFSET32:\n-    new_memory = new indOffset32_win95_safeOper(memory->disp(NULL, NULL, 0));\n-    break;\n-  case INDINDEXOFFSET:\n-    new_memory = new indIndexOffset_win95_safeOper(memory->disp(NULL, NULL, 0));\n-    break;\n-  case INDINDEXSCALE:\n-    new_memory = new indIndexScale_win95_safeOper(memory->scale());\n-    break;\n-  case INDINDEXSCALEOFFSET:\n-    new_memory = new indIndexScaleOffset_win95_safeOper(memory->scale(), memory->disp(NULL, NULL, 0));\n-    break;\n-  case LOAD_LONG_INDIRECT:\n-  case LOAD_LONG_INDOFFSET32:\n-    \/\/ Does not use EBP as address register, use { EDX, EBX, EDI, ESI}\n-    return;\n-  default:\n-    assert(false, \"unexpected memory operand in pd_implicit_null_fixup()\");\n-    return;\n-  }\n-  node->_opnds[opcnt] = new_memory;\n-}\n-\n@@ -3190,1 +3136,0 @@\n-  interpreter_method_reg(EBX);          \/\/ Method Register when calling interpreter\n@@ -4404,92 +4349,0 @@\n-\/\/----------Memory Operands - Win95 Implicit Null Variants----------------\n-\/\/ Indirect Memory Operand\n-operand indirect_win95_safe(eRegP_no_EBP reg)\n-%{\n-  constraint(ALLOC_IN_RC(int_reg));\n-  match(reg);\n-\n-  op_cost(100);\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp(0x0);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Short Offset Operand\n-operand indOffset8_win95_safe(eRegP_no_EBP reg, immI8 off)\n-%{\n-  match(AddP reg off);\n-\n-  op_cost(100);\n-  format %{ \"[$reg + $off]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Long Offset Operand\n-operand indOffset32_win95_safe(eRegP_no_EBP reg, immI off)\n-%{\n-  match(AddP reg off);\n-\n-  op_cost(100);\n-  format %{ \"[$reg + $off]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n-operand indIndexOffset_win95_safe(eRegP_no_EBP reg, rRegI ireg, immI off)\n-%{\n-  match(AddP (AddP reg ireg) off);\n-\n-  op_cost(100);\n-  format %{\"[$reg + $off + $ireg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($ireg);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Times Scale Plus Index Register\n-operand indIndexScale_win95_safe(eRegP_no_EBP reg, rRegI ireg, immI2 scale)\n-%{\n-  match(AddP reg (LShiftI ireg scale));\n-\n-  op_cost(100);\n-  format %{\"[$reg + $ireg << $scale]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($ireg);\n-    scale($scale);\n-    disp(0x0);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Times Scale Plus Index Register Plus Offset Operand\n-operand indIndexScaleOffset_win95_safe(eRegP_no_EBP reg, immI off, rRegI ireg, immI2 scale)\n-%{\n-  match(AddP (AddP reg (LShiftI ireg scale)) off);\n-\n-  op_cost(100);\n-  format %{\"[$reg + $off + $ireg << $scale]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($ireg);\n-    scale($scale);\n-    disp($off);\n-  %}\n-%}\n-\n@@ -13244,0 +13097,22 @@\n+instruct cmovLL_reg_LEGT_U(cmpOpU_commute cmp, flagsReg_ulong_LEGT flags, eRegL dst, eRegL src) %{\n+  match(Set dst (CMoveL (Binary cmp flags) (Binary dst src)));\n+  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n+  ins_cost(400);\n+  format %{ \"CMOV$cmp $dst.lo,$src.lo\\n\\t\"\n+            \"CMOV$cmp $dst.hi,$src.hi\" %}\n+  opcode(0x0F,0x40);\n+  ins_encode( enc_cmov(cmp), RegReg_Lo2( dst, src ), enc_cmov(cmp), RegReg_Hi2( dst, src ) );\n+  ins_pipe( pipe_cmov_reg_long );\n+%}\n+\n+instruct cmovLL_mem_LEGT_U(cmpOpU_commute cmp, flagsReg_ulong_LEGT flags, eRegL dst, load_long_memory src) %{\n+  match(Set dst (CMoveL (Binary cmp flags) (Binary dst (LoadL src))));\n+  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n+  ins_cost(500);\n+  format %{ \"CMOV$cmp $dst.lo,$src.lo\\n\\t\"\n+            \"CMOV$cmp $dst.hi,$src.hi+4\" %}\n+  opcode(0x0F,0x40);\n+  ins_encode( enc_cmov(cmp), RegMem(dst, src), enc_cmov(cmp), RegMem_Hi(dst, src) );\n+  ins_pipe( pipe_cmov_reg_long );\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":23,"deletions":148,"binary":false,"changes":171,"status":"modified"},{"patch":"@@ -164,3 +164,1 @@\n-\/\/ 2) reg_class compiler_method_reg        ( \/* as def'd in frame section *\/ )\n-\/\/ 2) reg_class interpreter_method_reg     ( \/* as def'd in frame section *\/ )\n-\/\/ 3) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n+\/\/ 2) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n@@ -1658,3 +1656,0 @@\n-\/\/ No-op on amd64\n-void Matcher::pd_implicit_null_fixup(MachNode *node, uint idx) {}\n-\n@@ -2739,2 +2734,0 @@\n-  interpreter_method_reg(RBX);          \/\/ Method Register when\n-                                        \/\/ calling interpreter\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -102,1 +102,0 @@\n-  bool        _frozen;          \/\/ no more expansion of this section\n@@ -119,1 +118,0 @@\n-    _frozen        = false;\n@@ -171,1 +169,0 @@\n-  csize_t     locs_remaining()const { return (csize_t)(_locs_limit - _locs_end); }\n@@ -176,1 +173,0 @@\n-  bool        is_frozen() const     { return _frozen; }\n@@ -194,2 +190,0 @@\n-  void    set_mark_off(int offset)  { assert(contains2(offset+_start),\"not in codeBuffer\");\n-                                      _mark = offset + _start; }\n@@ -269,4 +263,0 @@\n-  \/\/ Mark a section frozen.  Assign its remaining space to\n-  \/\/ the following section.  It will never expand after this point.\n-  inline void freeze();         \/\/  { _outer->freeze_section(this); }\n-\n@@ -294,1 +284,0 @@\n-#endif\n@@ -300,1 +289,0 @@\n-#ifndef PRODUCT\n@@ -305,1 +293,0 @@\n-#endif\n@@ -308,0 +295,1 @@\n+#endif\n@@ -320,0 +308,1 @@\n+#ifndef PRODUCT\n@@ -328,1 +317,1 @@\n-  const char* add_string(const char * string) PRODUCT_RETURN_(return NULL;);\n+  const char* add_string(const char * string);\n@@ -330,5 +319,3 @@\n-  void add_comment(intptr_t offset, const char * comment) PRODUCT_RETURN;\n-  bool has_block_comment(intptr_t offset) const;\n-  void print_block_comment(outputStream* stream, intptr_t offset) const PRODUCT_RETURN;\n-  \/\/ MOVE strings from other to this; invalidate other.\n-  void assign(CodeStrings& other)  PRODUCT_RETURN;\n+  void add_comment(intptr_t offset, const char * comment);\n+  void print_block_comment(outputStream* stream, intptr_t offset) const;\n+  int  count() const;\n@@ -336,1 +323,1 @@\n-  void copy(CodeStrings& other)  PRODUCT_RETURN;\n+  void copy(CodeStrings& other);\n@@ -338,1 +325,1 @@\n-  void free() PRODUCT_RETURN;\n+  void free();\n@@ -342,2 +329,0 @@\n-#ifdef ASSERT\n-#endif\n@@ -348,2 +333,1 @@\n-#ifndef PRODUCT\n-#endif\n+#endif \/\/ !PRODUCT\n@@ -420,2 +404,1 @@\n-  CodeStrings  _code_strings;\n-  bool         _collect_comments;      \/\/ Indicate if we need to collect block comments at all.\n+\n@@ -431,1 +414,4 @@\n-  address      _decode_begin;   \/\/ start address for decode\n+#ifndef PRODUCT\n+  CodeStrings  _code_strings;\n+  bool         _collect_comments; \/\/ Indicate if we need to collect block comments at all.\n+  address      _decode_begin;     \/\/ start address for decode\n@@ -433,0 +419,1 @@\n+#endif\n@@ -441,2 +428,0 @@\n-    _decode_begin    = NULL;\n-    _code_strings    = CodeStrings();\n@@ -449,0 +434,3 @@\n+#ifndef PRODUCT\n+    _decode_begin    = NULL;\n+    _code_strings    = CodeStrings();\n@@ -457,0 +445,1 @@\n+#endif\n@@ -474,2 +463,0 @@\n-  void freeze_section(CodeSection* cs);\n-\n@@ -567,1 +554,5 @@\n-  address    locator_address(int locator) const;\n+  address    locator_address(int locator) const {\n+    if (locator < 0)  return NULL;\n+    address start = code_section(locator_sect(locator))->start();\n+    return start + locator_pos(locator);\n+  }\n@@ -584,1 +575,0 @@\n-  address       insts_limit() const      { return _insts.limit();      }\n@@ -587,1 +577,0 @@\n-  void    clear_insts_mark()             {        _insts.clear_mark(); }\n@@ -645,2 +634,1 @@\n-  OopRecorder* oop_recorder() const   { return _oop_recorder; }\n-  CodeStrings& strings()              { return _code_strings; }\n+  OopRecorder* oop_recorder() const { return _oop_recorder; }\n@@ -652,0 +640,3 @@\n+#ifndef PRODUCT\n+  CodeStrings& strings() { return _code_strings; }\n+\n@@ -657,16 +648,0 @@\n-\n-  \/\/ Directly disassemble code buffer.\n-  \/\/ Print the comment associated with offset on stream, if there is one.\n-  virtual void print_block_comment(outputStream* stream, address block_begin) {\n-#ifndef PRODUCT\n-    intptr_t offset = (intptr_t)(block_begin - _total_start);  \/\/ I assume total_start is not correct for all code sections.\n-    _code_strings.print_block_comment(stream, offset);\n-#endif\n-  }\n-  bool has_block_comment(address block_begin) {\n-#ifndef PRODUCT\n-    intptr_t offset = (intptr_t)(block_begin - _total_start);  \/\/ I assume total_start is not correct for all code sections.\n-    return _code_strings.has_block_comment(offset);\n-#else\n-    return false;\n-  }\n@@ -698,3 +673,0 @@\n-  \/\/ Transform an address from the code in this code buffer to a specified code buffer\n-  address transform_address(const CodeBuffer &cb, address addr) const;\n-\n@@ -729,5 +701,0 @@\n-\n-inline void CodeSection::freeze() {\n-  _outer->freeze_section(this);\n-}\n-\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.hpp","additions":28,"deletions":61,"binary":false,"changes":89,"status":"modified"},{"patch":"@@ -185,20 +185,0 @@\n-void IRScopeDebugInfo::record_debug_info(DebugInformationRecorder* recorder, int pc_offset, bool topmost, bool is_method_handle_invoke, bool maybe_return_as_fields) {\n-  if (caller() != NULL) {\n-    \/\/ Order is significant:  Must record caller first.\n-    caller()->record_debug_info(recorder, pc_offset, false\/*topmost*\/);\n-  }\n-  DebugToken* locvals = recorder->create_scope_values(locals());\n-  DebugToken* expvals = recorder->create_scope_values(expressions());\n-  DebugToken* monvals = recorder->create_monitor_values(monitors());\n-  \/\/ reexecute allowed only for the topmost frame\n-  bool reexecute = topmost ? should_reexecute() : false;\n-  bool return_oop = false; \/\/ This flag will be ignored since it used only for C2 with escape analysis.\n-  bool rethrow_exception = false;\n-  bool return_vt = false;\n-  if (maybe_return_as_fields) {\n-    return_oop = true;\n-    return_vt = true;\n-  }\n-  recorder->describe_scope(pc_offset, methodHandle(), scope()->method(), bci(), reexecute, rethrow_exception, is_method_handle_invoke, return_oop, return_vt, locvals, expvals, monvals);\n-}\n-\n","filename":"src\/hotspot\/share\/c1\/c1_IR.cpp","additions":0,"deletions":20,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -238,1 +238,23 @@\n-  void record_debug_info(DebugInformationRecorder* recorder, int pc_offset, bool topmost, bool is_method_handle_invoke = false, bool maybe_return_as_fields = false);\n+  void record_debug_info(DebugInformationRecorder* recorder, int pc_offset, bool topmost, bool is_method_handle_invoke = false, bool maybe_return_as_fields = false) {\n+    if (caller() != NULL) {\n+      \/\/ Order is significant:  Must record caller first.\n+      caller()->record_debug_info(recorder, pc_offset, false\/*topmost*\/);\n+    }\n+    DebugToken* locvals = recorder->create_scope_values(locals());\n+    DebugToken* expvals = recorder->create_scope_values(expressions());\n+    DebugToken* monvals = recorder->create_monitor_values(monitors());\n+    \/\/ reexecute allowed only for the topmost frame\n+    bool reexecute = topmost ? should_reexecute() : false;\n+    bool return_oop = false; \/\/ This flag will be ignored since it used only for C2 with escape analysis.\n+    bool return_vt = false;\n+    if (maybe_return_as_fields) {\n+      return_oop = true;\n+      return_vt = true;\n+    }\n+    bool rethrow_exception = false;\n+    bool has_ea_local_in_scope = false;\n+    bool arg_escape = false;\n+    recorder->describe_scope(pc_offset, methodHandle(), scope()->method(), bci(),\n+                             reexecute, rethrow_exception, is_method_handle_invoke, return_oop, return_vt,\n+                             has_ea_local_in_scope, arg_escape, locvals, expvals, monvals);\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_IR.hpp","additions":24,"deletions":2,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -246,0 +246,1 @@\n+  _jvmti_can_walk_any_space             = JvmtiExport::can_walk_any_space();\n@@ -275,0 +276,4 @@\n+  if (!_jvmti_can_walk_any_space &&\n+      JvmtiExport::can_walk_any_space()) {\n+    return true;\n+  }\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -77,0 +77,1 @@\n+  bool  _jvmti_can_walk_any_space;\n@@ -358,0 +359,1 @@\n+  bool  jvmti_can_walk_any_space()             const { return _jvmti_can_walk_any_space; }\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3271,0 +3271,74 @@\n+\/\/ Find index of the InnerClasses entry for the specified inner_class_info_index.\n+\/\/ Return -1 if none is found.\n+static int inner_classes_find_index(const Array<u2>* inner_classes, int inner, const ConstantPool* cp, int length) {\n+  Symbol* cp_klass_name =  cp->klass_name_at(inner);\n+  for (int idx = 0; idx < length; idx += InstanceKlass::inner_class_next_offset) {\n+    int idx_inner = inner_classes->at(idx + InstanceKlass::inner_class_inner_class_info_offset);\n+    if (cp->klass_name_at(idx_inner) == cp_klass_name) {\n+      return idx;\n+    }\n+  }\n+  return -1;\n+}\n+\n+\/\/ Return the outer_class_info_index for the InnerClasses entry containing the\n+\/\/ specified inner_class_info_index.  Return -1 if no InnerClasses entry is found.\n+static int inner_classes_jump_to_outer(const Array<u2>* inner_classes, int inner, const ConstantPool* cp, int length) {\n+  if (inner == 0) return -1;\n+  int idx = inner_classes_find_index(inner_classes, inner, cp, length);\n+  if (idx == -1) return -1;\n+  int result = inner_classes->at(idx + InstanceKlass::inner_class_outer_class_info_offset);\n+  return result;\n+}\n+\n+\/\/ Return true if circularity is found, false if no circularity is found.\n+\/\/ Use Floyd's cycle finding algorithm.\n+static bool inner_classes_check_loop_through_outer(const Array<u2>* inner_classes, int idx, const ConstantPool* cp, int length) {\n+  int slow = inner_classes->at(idx + InstanceKlass::inner_class_inner_class_info_offset);\n+  int fast = inner_classes->at(idx + InstanceKlass::inner_class_outer_class_info_offset);\n+  while (fast != -1 && fast != 0) {\n+    if (slow != 0 && (cp->klass_name_at(slow) == cp->klass_name_at(fast))) {\n+      return true;  \/\/ found a circularity\n+    }\n+    fast = inner_classes_jump_to_outer(inner_classes, fast, cp, length);\n+    if (fast == -1) return false;\n+    fast = inner_classes_jump_to_outer(inner_classes, fast, cp, length);\n+    if (fast == -1) return false;\n+    slow = inner_classes_jump_to_outer(inner_classes, slow, cp, length);\n+    assert(slow != -1, \"sanity check\");\n+  }\n+  return false;\n+}\n+\n+\/\/ Loop through each InnerClasses entry checking for circularities and duplications\n+\/\/ with other entries.  If duplicate entries are found then throw CFE.  Otherwise,\n+\/\/ return true if a circularity or entries with duplicate inner_class_info_indexes\n+\/\/ are found.\n+bool ClassFileParser::check_inner_classes_circularity(const ConstantPool* cp, int length, TRAPS) {\n+  \/\/ Loop through each InnerClasses entry.\n+  for (int idx = 0; idx < length; idx += InstanceKlass::inner_class_next_offset) {\n+    \/\/ Return true if there are circular entries.\n+    if (inner_classes_check_loop_through_outer(_inner_classes, idx, cp, length)) {\n+      return true;\n+    }\n+    \/\/ Check if there are duplicate entries or entries with the same inner_class_info_index.\n+    for (int y = idx + InstanceKlass::inner_class_next_offset; y < length;\n+         y += InstanceKlass::inner_class_next_offset) {\n+\n+      \/\/ To maintain compatibility, throw an exception if duplicate inner classes\n+      \/\/ entries are found.\n+      guarantee_property((_inner_classes->at(idx) != _inner_classes->at(y) ||\n+                          _inner_classes->at(idx+1) != _inner_classes->at(y+1) ||\n+                          _inner_classes->at(idx+2) != _inner_classes->at(y+2) ||\n+                          _inner_classes->at(idx+3) != _inner_classes->at(y+3)),\n+                         \"Duplicate entry in InnerClasses attribute in class file %s\",\n+                         CHECK_(true));\n+      \/\/ Return true if there are two entries with the same inner_class_info_index.\n+      if (_inner_classes->at(y) == _inner_classes->at(idx)) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -3273,0 +3347,1 @@\n+                                                            const ConstantPool* cp,\n@@ -3296,1 +3371,1 @@\n-  Array<u2>* const inner_classes = MetadataFactory::new_array<u2>(_loader_data, size, CHECK_0);\n+  Array<u2>* inner_classes = MetadataFactory::new_array<u2>(_loader_data, size, CHECK_0);\n@@ -3353,0 +3428,2 @@\n+  \/\/ Also, check for circular entries.\n+  bool has_circularity = false;\n@@ -3354,8 +3431,10 @@\n-    for(int i = 0; i < length * 4; i += 4) {\n-      for(int j = i + 4; j < length * 4; j += 4) {\n-        guarantee_property((inner_classes->at(i)   != inner_classes->at(j) ||\n-                            inner_classes->at(i+1) != inner_classes->at(j+1) ||\n-                            inner_classes->at(i+2) != inner_classes->at(j+2) ||\n-                            inner_classes->at(i+3) != inner_classes->at(j+3)),\n-                            \"Duplicate entry in InnerClasses in class file %s\",\n-                            CHECK_0);\n+    has_circularity = check_inner_classes_circularity(cp, length * 4, CHECK_0);\n+    if (has_circularity) {\n+      \/\/ If circularity check failed then ignore InnerClasses attribute.\n+      MetadataFactory::free_array<u2>(_loader_data, _inner_classes);\n+      index = 0;\n+      if (parsed_enclosingmethod_attribute) {\n+        inner_classes = MetadataFactory::new_array<u2>(_loader_data, 2, CHECK_0);\n+        _inner_classes = inner_classes;\n+      } else {\n+        _inner_classes = Universe::the_empty_short_array();\n@@ -3365,1 +3444,0 @@\n-\n@@ -3371,1 +3449,1 @@\n-  assert(index == size, \"wrong size\");\n+  assert(index == size || has_circularity, \"wrong size\");\n@@ -4063,0 +4141,1 @@\n+                            cp,\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":90,"deletions":11,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -342,0 +342,3 @@\n+  \/\/ Check for circularity in InnerClasses attribute.\n+  bool check_inner_classes_circularity(const ConstantPool* cp, int length, TRAPS);\n+\n@@ -343,0 +346,1 @@\n+                                               const ConstantPool* cp,\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -35,1 +35,3 @@\n-#include \"classfile\/vmSymbols.hpp\"\n+#include \"interpreter\/bytecode.hpp\"\n+#include \"interpreter\/bytecodeStream.hpp\"\n+#include \"interpreter\/linkResolver.hpp\"\n@@ -40,0 +42,1 @@\n+#include \"oops\/constantPool.hpp\"\n@@ -69,0 +72,1 @@\n+  _indy_items = new (ResourceObj::C_HEAP, mtClass) GrowableArray<const char*>(9, mtClass);\n@@ -131,0 +135,5 @@\n+  _indy_items->clear();\n+\n+  if (_line[0] == '@') {\n+    return parse_at_tags();\n+  }\n@@ -143,1 +152,1 @@\n-    if (parse_int_option(\"id:\", &_id)) {\n+    if (parse_uint_option(\"id:\", &_id)) {\n@@ -145,1 +154,1 @@\n-    } else if (parse_int_option(\"super:\", &_super)) {\n+    } else if (parse_uint_option(\"super:\", &_super)) {\n@@ -150,1 +159,1 @@\n-      while (try_parse_int(&i)) {\n+      while (try_parse_uint(&i)) {\n@@ -179,0 +188,35 @@\n+void ClassListParser::split_tokens_by_whitespace() {\n+  int start = 0;\n+  int end;\n+  bool done = false;\n+  while (!done) {\n+    while (_line[start] == ' ' || _line[start] == '\\t') start++;\n+    end = start;\n+    while (_line[end] && _line[end] != ' ' && _line[end] != '\\t') end++;\n+    if (_line[end] == '\\0') {\n+      done = true;\n+    } else {\n+      _line[end] = '\\0';\n+    }\n+    _indy_items->append(_line + start);\n+    start = ++end;\n+  }\n+}\n+\n+bool ClassListParser::parse_at_tags() {\n+  assert(_line[0] == '@', \"must be\");\n+  split_tokens_by_whitespace();\n+  if (strcmp(_indy_items->at(0), LAMBDA_PROXY_TAG) == 0) {\n+    if (_indy_items->length() < 3) {\n+      error(\"Line with @ tag has too few items \\\"%s\\\" line #%d\", _line, _line_no);\n+      return false;\n+    }\n+    \/\/ set the class name\n+    _class_name = _indy_items->at(1);\n+    return true;\n+  } else {\n+    error(\"Invalid @ tag at the beginning of line \\\"%s\\\" line #%d\", _line, _line_no);\n+    return false;\n+  }\n+}\n+\n@@ -195,3 +239,0 @@\n-    if (*value < 0) {\n-      error(\"Error: negative integers not allowed (%d)\", *value);\n-    }\n@@ -203,1 +244,8 @@\n-bool ClassListParser::try_parse_int(int* value) {\n+void ClassListParser::parse_uint(int* value) {\n+  parse_int(value);\n+  if (*value < 0) {\n+    error(\"Error: negative integers not allowed (%d)\", *value);\n+  }\n+}\n+\n+bool ClassListParser::try_parse_uint(int* value) {\n@@ -234,0 +282,12 @@\n+bool ClassListParser::parse_uint_option(const char* option_name, int* value) {\n+  if (skip_token(option_name)) {\n+    if (*value != _unspecified) {\n+      error(\"%s specified twice\", option_name);\n+    } else {\n+      parse_uint(value);\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -366,0 +426,108 @@\n+void ClassListParser::populate_cds_indy_info(const constantPoolHandle &pool, int cp_index, CDSIndyInfo* cii, TRAPS) {\n+  \/\/ Caller needs to allocate ResourceMark.\n+  int type_index = pool->bootstrap_name_and_type_ref_index_at(cp_index);\n+  int name_index = pool->name_ref_index_at(type_index);\n+  cii->add_item(pool->symbol_at(name_index)->as_C_string());\n+  int sig_index = pool->signature_ref_index_at(type_index);\n+  cii->add_item(pool->symbol_at(sig_index)->as_C_string());\n+  int argc = pool->bootstrap_argument_count_at(cp_index);\n+  if (argc > 0) {\n+    for (int arg_i = 0; arg_i < argc; arg_i++) {\n+      int arg = pool->bootstrap_argument_index_at(cp_index, arg_i);\n+      jbyte tag = pool->tag_at(arg).value();\n+      if (tag == JVM_CONSTANT_MethodType) {\n+        cii->add_item(pool->method_type_signature_at(arg)->as_C_string());\n+      } else if (tag == JVM_CONSTANT_MethodHandle) {\n+        cii->add_ref_kind(pool->method_handle_ref_kind_at(arg));\n+        int callee_index = pool->method_handle_klass_index_at(arg);\n+        Klass* callee = pool->klass_at(callee_index, THREAD);\n+        if (callee != NULL) {\n+          cii->add_item(callee->name()->as_C_string());\n+        }\n+        cii->add_item(pool->method_handle_name_ref_at(arg)->as_C_string());\n+        cii->add_item(pool->method_handle_signature_ref_at(arg)->as_C_string());\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n+  }\n+}\n+\n+bool ClassListParser::is_matching_cp_entry(constantPoolHandle &pool, int cp_index, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  CDSIndyInfo cii;\n+  populate_cds_indy_info(pool, cp_index, &cii, THREAD);\n+  GrowableArray<const char*>* items = cii.items();\n+  int indy_info_offset = 2;\n+  if (_indy_items->length() - indy_info_offset != items->length()) {\n+    return false;\n+  }\n+  for (int i = 0; i < items->length(); i++) {\n+    if (strcmp(_indy_items->at(i + indy_info_offset), items->at(i)) != 0) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+void ClassListParser::resolve_indy(Symbol* class_name_symbol, TRAPS) {\n+\n+  Handle class_loader(THREAD, SystemDictionary::java_system_loader());\n+  Handle protection_domain;\n+  Klass* klass = SystemDictionary::resolve_or_fail(class_name_symbol, class_loader, protection_domain, true, THREAD); \/\/ FIXME should really be just a lookup\n+  if (klass != NULL && klass->is_instance_klass()) {\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    MetaspaceShared::try_link_class(ik, THREAD);\n+    assert(!HAS_PENDING_EXCEPTION, \"unexpected exception\");\n+\n+    ConstantPool* cp = ik->constants();\n+    ConstantPoolCache* cpcache = cp->cache();\n+    bool found = false;\n+    for (int cpcindex = 0; cpcindex < cpcache->length(); cpcindex ++) {\n+      int indy_index = ConstantPool::encode_invokedynamic_index(cpcindex);\n+      ConstantPoolCacheEntry* cpce = cpcache->entry_at(cpcindex);\n+      int pool_index = cpce->constant_pool_index();\n+      constantPoolHandle pool(THREAD, cp);\n+      if (pool->tag_at(pool_index).is_invoke_dynamic()) {\n+        BootstrapInfo bootstrap_specifier(pool, pool_index, indy_index);\n+        Handle bsm = bootstrap_specifier.resolve_bsm(THREAD);\n+        if (!SystemDictionaryShared::is_supported_invokedynamic(&bootstrap_specifier)) {\n+           tty->print_cr(\"is_supported_invokedynamic check failed for cp_index %d\", pool_index);\n+           continue;\n+        }\n+        if (is_matching_cp_entry(pool, pool_index, THREAD)) {\n+          found = true;\n+          CallInfo info;\n+          bool is_done = bootstrap_specifier.resolve_previously_linked_invokedynamic(info, THREAD);\n+          if (!is_done) {\n+            \/\/ resolve it\n+            Handle recv;\n+            LinkResolver::resolve_invoke(info, recv, pool, indy_index, Bytecodes::_invokedynamic, THREAD);\n+            break;\n+          }\n+          cpce->set_dynamic_call(pool, info);\n+          if (HAS_PENDING_EXCEPTION) {\n+            ResourceMark rm(THREAD);\n+            tty->print(\"resolve_indy for class %s has\", class_name_symbol->as_C_string());\n+            oop message = java_lang_Throwable::message(PENDING_EXCEPTION);\n+            if (message != NULL) {\n+              char* ex_msg = java_lang_String::as_utf8_string(message);\n+              tty->print_cr(\" exception pending '%s %s'\",\n+                         PENDING_EXCEPTION->klass()->external_name(), ex_msg);\n+            } else {\n+              tty->print_cr(\" exception pending %s \",\n+                         PENDING_EXCEPTION->klass()->external_name());\n+            }\n+            exit(1);\n+          }\n+        }\n+      }\n+    }\n+    if (!found) {\n+      ResourceMark rm(THREAD);\n+      log_warning(cds)(\"No invoke dynamic constant pool entry can be found for class %s. The classlist is probably out-of-date.\",\n+                     class_name_symbol->as_C_string());\n+    }\n+  }\n+}\n+\n@@ -369,0 +537,5 @@\n+  if (_indy_items->length() > 0) {\n+    resolve_indy(class_name_symbol, CHECK_NULL);\n+    return NULL;\n+  }\n+\n","filename":"src\/hotspot\/share\/classfile\/classListParser.cpp","additions":181,"deletions":8,"binary":false,"changes":189,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"memory\/classLoaderMetaspace.hpp\"\n@@ -62,0 +63,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -973,0 +975,1 @@\n+#ifdef ASSERT\n@@ -976,0 +979,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+class ClassLoaderMetaspace;\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1407,2 +1407,4 @@\n-  assert(shared_nest_host->is_same_class_package(ik),\n-         \"lambda proxy class and its nest host must be in the same package\");\n+  if (loaded_ik != NULL) {\n+    assert(shared_nest_host->is_same_class_package(ik),\n+           \"lambda proxy class and its nest host must be in the same package\");\n+  }\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -62,47 +62,0 @@\n-inline bool match_F_RNY(jshort flags) {\n-  const int req = JVM_ACC_NATIVE | JVM_ACC_SYNCHRONIZED;\n-  const int neg = JVM_ACC_STATIC;\n-  return (flags & (req | neg)) == req;\n-}\n-\n-static vmIntrinsics::ID wrapper_intrinsic(BasicType type, bool unboxing) {\n-#define TYPE2(type, unboxing) ((int)(type)*2 + ((unboxing) ? 1 : 0))\n-  switch (TYPE2(type, unboxing)) {\n-#define BASIC_TYPE_CASE(type, box, unbox) \\\n-    case TYPE2(type, false):  return vmIntrinsics::box; \\\n-    case TYPE2(type, true):   return vmIntrinsics::unbox\n-    BASIC_TYPE_CASE(T_BOOLEAN, _Boolean_valueOf,   _booleanValue);\n-    BASIC_TYPE_CASE(T_BYTE,    _Byte_valueOf,      _byteValue);\n-    BASIC_TYPE_CASE(T_CHAR,    _Character_valueOf, _charValue);\n-    BASIC_TYPE_CASE(T_SHORT,   _Short_valueOf,     _shortValue);\n-    BASIC_TYPE_CASE(T_INT,     _Integer_valueOf,   _intValue);\n-    BASIC_TYPE_CASE(T_LONG,    _Long_valueOf,      _longValue);\n-    BASIC_TYPE_CASE(T_FLOAT,   _Float_valueOf,     _floatValue);\n-    BASIC_TYPE_CASE(T_DOUBLE,  _Double_valueOf,    _doubleValue);\n-#undef BASIC_TYPE_CASE\n-  }\n-#undef TYPE2\n-  return vmIntrinsics::_none;\n-}\n-\n-vmIntrinsics::ID vmIntrinsics::for_boxing(BasicType type) {\n-  return wrapper_intrinsic(type, false);\n-}\n-vmIntrinsics::ID vmIntrinsics::for_unboxing(BasicType type) {\n-  return wrapper_intrinsic(type, true);\n-}\n-\n-vmIntrinsics::ID vmIntrinsics::for_raw_conversion(BasicType src, BasicType dest) {\n-#define SRC_DEST(s,d) (((int)(s) << 4) + (int)(d))\n-  switch (SRC_DEST(src, dest)) {\n-  case SRC_DEST(T_INT, T_FLOAT):   return vmIntrinsics::_intBitsToFloat;\n-  case SRC_DEST(T_FLOAT, T_INT):   return vmIntrinsics::_floatToRawIntBits;\n-\n-  case SRC_DEST(T_LONG, T_DOUBLE): return vmIntrinsics::_longBitsToDouble;\n-  case SRC_DEST(T_DOUBLE, T_LONG): return vmIntrinsics::_doubleToRawLongBits;\n-  }\n-#undef SRC_DEST\n-\n-  return vmIntrinsics::_none;\n-}\n-\n@@ -229,1 +182,1 @@\n-    return 4;\n+    return 5;\n@@ -490,0 +443,3 @@\n+  case vmIntrinsics::_sha3_implCompress:\n+    if (!UseSHA3Intrinsics) return true;\n+    break;\n@@ -491,1 +447,1 @@\n-    if (!(UseMD5Intrinsics || UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics)) return true;\n+    if (!(UseMD5Intrinsics || UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics || UseSHA3Intrinsics)) return true;\n@@ -738,1 +694,0 @@\n-  case F_RNY:fname = \"native synchronized \"; break;\n@@ -756,0 +711,1 @@\n+#ifdef ASSERT\n@@ -797,63 +753,1 @@\n-\n-\n-#ifndef PRODUCT\n-\/\/ verify_method performs an extra check on a matched intrinsic method\n-\n-static bool match_method(Method* m, Symbol* n, Symbol* s) {\n-  return (m->name() == n &&\n-          m->signature() == s);\n-}\n-\n-static vmIntrinsics::ID match_method_with_klass(Method* m, Symbol* mk) {\n-#define VM_INTRINSIC_MATCH(id, klassname, namepart, sigpart, flags) \\\n-  { Symbol* k = vmSymbols::klassname(); \\\n-    if (mk == k) { \\\n-      Symbol* n = vmSymbols::namepart(); \\\n-      Symbol* s = vmSymbols::sigpart(); \\\n-      if (match_method(m, n, s)) \\\n-        return vmIntrinsics::id; \\\n-    } }\n-  VM_INTRINSICS_DO(VM_INTRINSIC_MATCH,\n-                   VM_SYMBOL_IGNORE, VM_SYMBOL_IGNORE, VM_SYMBOL_IGNORE, VM_ALIAS_IGNORE);\n-  return vmIntrinsics::_none;\n-#undef VM_INTRINSIC_MATCH\n-}\n-\n-void vmIntrinsics::verify_method(ID actual_id, Method* m) {\n-  Symbol* mk = m->method_holder()->name();\n-  ID declared_id = match_method_with_klass(m, mk);\n-\n-  if (declared_id == actual_id)  return; \/\/ success\n-\n-  if (declared_id == _none && actual_id != _none && mk == vmSymbols::java_lang_StrictMath()) {\n-    \/\/ Here are a few special cases in StrictMath not declared in vmSymbols.hpp.\n-    switch (actual_id) {\n-    case _min:\n-    case _max:\n-    case _dsqrt:\n-      declared_id = match_method_with_klass(m, vmSymbols::java_lang_Math());\n-      if (declared_id == actual_id)  return; \/\/ acceptable alias\n-      break;\n-    default:\n-        break;\n-    }\n-  }\n-\n-  const char* declared_name = name_at(declared_id);\n-  const char* actual_name   = name_at(actual_id);\n-  m = NULL;\n-  ttyLocker ttyl;\n-  if (xtty != NULL) {\n-    xtty->begin_elem(\"intrinsic_misdeclared actual='%s' declared='%s'\",\n-                     actual_name, declared_name);\n-    xtty->method(m);\n-    xtty->end_elem(\"%s\", \"\");\n-  }\n-  if (PrintMiscellaneous && (WizardMode || Verbose)) {\n-    tty->print_cr(\"*** misidentified method; %s(%d) should be %s(%d):\",\n-                  declared_name, declared_id, actual_name, actual_id);\n-    m->print_short_name(tty);\n-    tty->cr();\n-  }\n-}\n-#endif \/\/PRODUCT\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":7,"deletions":113,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -435,0 +435,4 @@\n+  \/* support for sun.security.provider.SHA3 *\/                                                                          \\\n+  do_class(sun_security_provider_sha3,                             \"sun\/security\/provider\/SHA3\")                        \\\n+  do_intrinsic(_sha3_implCompress, sun_security_provider_sha3, implCompress_name, implCompress_signature, F_R)          \\\n+                                                                                                                        \\\n@@ -1049,1 +1053,0 @@\n-    F_RNY,                      \/\/ !static  native  synchronized\n@@ -1054,1 +1057,1 @@\n-    log2_FLAG_LIMIT = 4         \/\/ checked by an assert at start-up\n+    log2_FLAG_LIMIT = 3         \/\/ checked by an assert at start-up\n@@ -1092,2 +1095,1 @@\n-  static void verify_method(ID actual_id, Method* m) PRODUCT_RETURN;\n-\n+#ifdef ASSERT\n@@ -1099,0 +1101,1 @@\n+#endif\n@@ -1102,7 +1105,0 @@\n-  \/\/ Wrapper object methods:\n-  static ID for_boxing(BasicType type);\n-  static ID for_unboxing(BasicType type);\n-\n-  \/\/ Raw conversion:\n-  static ID for_raw_conversion(BasicType src, BasicType dest);\n-\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":7,"deletions":11,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -90,1 +90,1 @@\n-  _strings(CodeStrings()),\n+  NOT_PRODUCT(COMMA _strings(CodeStrings()))\n@@ -118,1 +118,1 @@\n-  _strings(CodeStrings()),\n+  NOT_PRODUCT(COMMA _strings(CodeStrings()))\n@@ -161,1 +161,1 @@\n-  _strings.free();\n+  NOT_PRODUCT(_strings.free();)\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -113,1 +113,1 @@\n-  CodeStrings         _strings;\n+\n@@ -117,0 +117,2 @@\n+  NOT_PRODUCT(CodeStrings _strings;)\n+\n@@ -235,4 +237,0 @@\n-  bool has_block_comment(address block_begin) const {\n-    intptr_t offset = (intptr_t)(block_begin - code_begin());\n-    return _strings.has_block_comment(offset);\n-  }\n@@ -241,0 +239,1 @@\n+  #ifndef PRODUCT\n@@ -243,0 +242,1 @@\n+  #endif\n@@ -245,1 +245,1 @@\n-  \/\/ Transfer ownership of comments to this CodeBlob\n+#ifndef PRODUCT\n@@ -248,9 +248,1 @@\n-    _strings.assign(strings);\n-  }\n-\n-  static ByteSize name_field_offset() {\n-    return byte_offset_of(CodeBlob, _name);\n-  }\n-\n-  static ByteSize oop_maps_field_offset() {\n-    return byte_offset_of(CodeBlob, _oop_maps);\n+    _strings.copy(strings);\n@@ -258,0 +250,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":8,"deletions":15,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -81,1 +81,0 @@\n-  _lazy_critical_native       = 0;\n@@ -296,3 +295,1 @@\n-  return new ScopeDesc(this, pd->scope_decode_offset(),\n-                       pd->obj_decode_offset(), pd->should_reexecute(), pd->rethrow_exception(),\n-                       pd->return_oop(), pd->return_vt());\n+  return new ScopeDesc(this, pd);\n@@ -304,3 +301,1 @@\n-  return new ScopeDesc(this, pd->scope_decode_offset(),\n-                       pd->obj_decode_offset(), pd->should_reexecute(), pd->rethrow_exception(),\n-                       pd->return_oop(), pd->return_vt());\n+  return new ScopeDesc(this, pd);\n","filename":"src\/hotspot\/share\/code\/compiledMethod.cpp","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -160,1 +160,0 @@\n-  unsigned int _lazy_critical_native:1;      \/\/ Lazy JNI critical native\n@@ -199,3 +198,0 @@\n-  bool  is_lazy_critical_native() const           { return _lazy_critical_native; }\n-  void  set_lazy_critical_native(bool z)          { _lazy_critical_native = z; }\n-\n","filename":"src\/hotspot\/share\/code\/compiledMethod.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -292,0 +292,2 @@\n+                                              bool        has_ea_local_in_scope,\n+                                              bool        arg_escape,\n@@ -309,0 +311,2 @@\n+  last_pd->set_has_ea_local_in_scope(has_ea_local_in_scope);\n+  last_pd->set_arg_escape(arg_escape);\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -109,0 +109,2 @@\n+                      bool        has_ea_local_in_scope = false,\n+                      bool        arg_escape = false,\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2446,3 +2446,1 @@\n-  for (ScopeDesc* sd = new ScopeDesc(this, pd->scope_decode_offset(),\n-                                     pd->obj_decode_offset(), pd->should_reexecute(), pd->rethrow_exception(),\n-                                     pd->return_oop(), pd->return_vt());\n+  for (ScopeDesc* sd = new ScopeDesc(this, pd);\n@@ -3083,3 +3081,1 @@\n-    return new ScopeDesc(this, p->scope_decode_offset(),\n-                         p->obj_decode_offset(), p->should_reexecute(), p->rethrow_exception(),\n-                         p->return_oop(), p->return_vt());\n+    return new ScopeDesc(this, p);\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -705,3 +705,1 @@\n-  bool has_block_comment(address block_begin) {\n-    return CodeBlob::has_block_comment(block_begin);\n-  }\n+\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -46,1 +46,3 @@\n-    PCDESC_return_vt               = 1 << 4\n+    PCDESC_has_ea_local_in_scope   = 1 << 4,\n+    PCDESC_arg_escape              = 1 << 5,\n+    PCDESC_return_vt               = 1 << 6\n@@ -95,0 +97,9 @@\n+  \/\/ Indicates if there are objects in scope that, based on escape analysis, are local to the\n+  \/\/ compiled method or local to the current thread, i.e. NoEscape or ArgEscape\n+  bool     has_ea_local_in_scope()         const { return (_flags & PCDESC_has_ea_local_in_scope) != 0; }\n+  void set_has_ea_local_in_scope(bool z)         { set_flag(PCDESC_has_ea_local_in_scope, z); }\n+\n+  \/\/ Indicates if this pc descriptor is at a call site where objects that do not escape the\n+  \/\/ current thread are passed as arguments.\n+  bool     arg_escape()                    const { return (_flags & PCDESC_arg_escape) != 0; }\n+  void set_arg_escape(bool z)                    { set_flag(PCDESC_arg_escape, z); }\n","filename":"src\/hotspot\/share\/code\/pcDesc.hpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -34,1 +34,2 @@\n-ScopeDesc::ScopeDesc(const CompiledMethod* code, int decode_offset, int obj_decode_offset, bool reexecute, bool rethrow_exception, bool return_oop, bool return_vt) {\n+ScopeDesc::ScopeDesc(const CompiledMethod* code, PcDesc* pd, bool ignore_objects) {\n+  int obj_decode_offset = ignore_objects ? DebugInformationRecorder::serialized_null : pd->obj_decode_offset();\n@@ -36,1 +37,1 @@\n-  _decode_offset = decode_offset;\n+  _decode_offset = pd->scope_decode_offset();\n@@ -38,15 +39,6 @@\n-  _reexecute     = reexecute;\n-  _rethrow_exception = rethrow_exception;\n-  _return_oop    = return_oop;\n-  _return_vt     = return_vt;\n-  decode_body();\n-}\n-\n-ScopeDesc::ScopeDesc(const CompiledMethod* code, int decode_offset, bool reexecute, bool rethrow_exception, bool return_oop, bool return_vt) {\n-  _code          = code;\n-  _decode_offset = decode_offset;\n-  _objects       = decode_object_values(DebugInformationRecorder::serialized_null);\n-  _reexecute     = reexecute;\n-  _rethrow_exception = rethrow_exception;\n-  _return_oop    = return_oop;\n-  _return_vt     = return_vt;\n+  _reexecute     = pd->should_reexecute();\n+  _rethrow_exception = pd->rethrow_exception();\n+  _return_oop    = pd->return_oop();\n+  _return_vt     = pd->return_vt();\n+  _has_ea_local_in_scope = ignore_objects ? false : pd->has_ea_local_in_scope();\n+  _arg_escape    = ignore_objects ? false : pd->arg_escape();\n@@ -65,0 +57,2 @@\n+  _has_ea_local_in_scope = parent->has_ea_local_in_scope();\n+  _arg_escape    = false;\n","filename":"src\/hotspot\/share\/code\/scopeDesc.cpp","additions":11,"deletions":17,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -63,6 +63,1 @@\n-  ScopeDesc(const CompiledMethod* code, int decode_offset, int obj_decode_offset, bool reexecute, bool rethrow_exception, bool return_oop, bool return_vt);\n-\n-  \/\/ Calls above, giving default value of \"serialized_null\" to the\n-  \/\/ \"obj_decode_offset\" argument.  (We don't use a default argument to\n-  \/\/ avoid a .hpp-.hpp dependency.)\n-  ScopeDesc(const CompiledMethod* code, int decode_offset, bool reexecute, bool rethrow_exception, bool return_oop, bool return_vt);\n+  ScopeDesc(const CompiledMethod* code, PcDesc* pd, bool ignore_objects = false);\n@@ -80,0 +75,4 @@\n+  \/\/ Returns true if one or more NoEscape or ArgEscape objects exist in\n+  \/\/ any of the scopes at compiled pc.\n+  bool has_ea_local_in_scope() const { return _has_ea_local_in_scope; }\n+  bool arg_escape()       const { return _arg_escape; }\n@@ -110,1 +109,3 @@\n-\n+  bool          _has_ea_local_in_scope;       \/\/ One or more NoEscape or ArgEscape objects exist in\n+                                              \/\/ any of the scopes at compiled pc.\n+  bool          _arg_escape;                  \/\/ Compiled Java call in youngest scope passes ArgEscape\n","filename":"src\/hotspot\/share\/code\/scopeDesc.hpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"runtime\/escapeBarrier.hpp\"\n@@ -801,0 +802,4 @@\n+#if defined(ASSERT) && COMPILER2_OR_JVMCI\n+\/\/ Stress testing. Dedicated threads revert optimizations based on escape analysis concurrently to\n+\/\/ the running java application.  Configured with vm options DeoptimizeObjectsALot*.\n+class DeoptimizeObjectsALotThread : public JavaThread {\n@@ -802,1 +807,64 @@\n-JavaThread* CompileBroker::make_thread(jobject thread_handle, CompileQueue* queue, AbstractCompiler* comp, Thread* THREAD) {\n+  static void deopt_objs_alot_thread_entry(JavaThread* thread, TRAPS);\n+  void deoptimize_objects_alot_loop_single();\n+  void deoptimize_objects_alot_loop_all();\n+\n+public:\n+  DeoptimizeObjectsALotThread() : JavaThread(&deopt_objs_alot_thread_entry) { }\n+\n+  bool is_hidden_from_external_view() const      { return true; }\n+};\n+\n+\/\/ Entry for DeoptimizeObjectsALotThread. The threads are started in\n+\/\/ CompileBroker::init_compiler_sweeper_threads() iff DeoptimizeObjectsALot is enabled\n+void DeoptimizeObjectsALotThread::deopt_objs_alot_thread_entry(JavaThread* thread, TRAPS) {\n+    DeoptimizeObjectsALotThread* dt = ((DeoptimizeObjectsALotThread*) thread);\n+    bool enter_single_loop;\n+    {\n+      MonitorLocker ml(dt, EscapeBarrier_lock, Mutex::_no_safepoint_check_flag);\n+      static int single_thread_count = 0;\n+      enter_single_loop = single_thread_count++ < DeoptimizeObjectsALotThreadCountSingle;\n+    }\n+    if (enter_single_loop) {\n+      dt->deoptimize_objects_alot_loop_single();\n+    } else {\n+      dt->deoptimize_objects_alot_loop_all();\n+    }\n+  }\n+\n+\/\/ Execute EscapeBarriers in an endless loop to revert optimizations based on escape analysis. Each\n+\/\/ barrier targets a single thread which is selected round robin.\n+void DeoptimizeObjectsALotThread::deoptimize_objects_alot_loop_single() {\n+  HandleMark hm(this);\n+  while (true) {\n+    for (JavaThreadIteratorWithHandle jtiwh; JavaThread *deoptee_thread = jtiwh.next(); ) {\n+      { \/\/ Begin new scope for escape barrier\n+        HandleMarkCleaner hmc(this);\n+        ResourceMark rm(this);\n+        EscapeBarrier eb(true, this, deoptee_thread);\n+        eb.deoptimize_objects(100);\n+      }\n+      \/\/ Now sleep after the escape barriers destructor resumed deoptee_thread.\n+      sleep(DeoptimizeObjectsALotInterval);\n+    }\n+  }\n+}\n+\n+\/\/ Execute EscapeBarriers in an endless loop to revert optimizations based on escape analysis. Each\n+\/\/ barrier targets all java threads in the vm at once.\n+void DeoptimizeObjectsALotThread::deoptimize_objects_alot_loop_all() {\n+  HandleMark hm(this);\n+  while (true) {\n+    { \/\/ Begin new scope for escape barrier\n+      HandleMarkCleaner hmc(this);\n+      ResourceMark rm(this);\n+      EscapeBarrier eb(true, this);\n+      eb.deoptimize_objects_all_threads();\n+    }\n+    \/\/ Now sleep after the escape barriers destructor resumed the java threads.\n+    sleep(DeoptimizeObjectsALotInterval);\n+  }\n+}\n+#endif \/\/ defined(ASSERT) && COMPILER2_OR_JVMCI\n+\n+\n+JavaThread* CompileBroker::make_thread(ThreadType type, jobject thread_handle, CompileQueue* queue, AbstractCompiler* comp, Thread* THREAD) {\n@@ -806,7 +874,18 @@\n-    if (comp != NULL) {\n-      if (!InjectCompilerCreationFailure || comp->num_compiler_threads() == 0) {\n-        CompilerCounters* counters = new CompilerCounters();\n-        new_thread = new CompilerThread(queue, counters);\n-      }\n-    } else {\n-      new_thread = new CodeCacheSweeperThread();\n+    switch (type) {\n+      case compiler_t:\n+        assert(comp != NULL, \"Compiler instance missing.\");\n+        if (!InjectCompilerCreationFailure || comp->num_compiler_threads() == 0) {\n+          CompilerCounters* counters = new CompilerCounters();\n+          new_thread = new CompilerThread(queue, counters);\n+        }\n+        break;\n+      case sweeper_t:\n+        new_thread = new CodeCacheSweeperThread();\n+        break;\n+#if defined(ASSERT) && COMPILER2_OR_JVMCI\n+      case deoptimizer_t:\n+        new_thread = new DeoptimizeObjectsALotThread();\n+        break;\n+#endif \/\/ ASSERT\n+      default:\n+        ShouldNotReachHere();\n@@ -814,0 +893,1 @@\n+\n@@ -855,1 +935,1 @@\n-      if (comp != NULL) {\n+      if (type == compiler_t) {\n@@ -865,1 +945,1 @@\n-    if (UseDynamicNumberOfCompilerThreads && comp != NULL && comp->num_compiler_threads() > 0) {\n+    if (UseDynamicNumberOfCompilerThreads && type == compiler_t && comp->num_compiler_threads() > 0) {\n@@ -920,1 +1000,1 @@\n-      JavaThread *ct = make_thread(thread_handle, _c2_compile_queue, _compilers[1], THREAD);\n+      JavaThread *ct = make_thread(compiler_t, thread_handle, _c2_compile_queue, _compilers[1], THREAD);\n@@ -940,1 +1020,1 @@\n-      JavaThread *ct = make_thread(thread_handle, _c1_compile_queue, _compilers[0], THREAD);\n+      JavaThread *ct = make_thread(compiler_t, thread_handle, _c1_compile_queue, _compilers[0], THREAD);\n@@ -959,1 +1039,12 @@\n-    make_thread(thread_handle, NULL, NULL, THREAD);\n+    make_thread(sweeper_t, thread_handle, NULL, NULL, THREAD);\n+  }\n+\n+#if defined(ASSERT) && COMPILER2_OR_JVMCI\n+  if (DeoptimizeObjectsALot) {\n+    \/\/ Initialize and start the object deoptimizer threads\n+    const int total_count = DeoptimizeObjectsALotThreadCountSingle + DeoptimizeObjectsALotThreadCountAll;\n+    for (int count = 0; count < total_count; count++) {\n+      Handle thread_oop = create_thread_oop(\"Deoptimize objects a lot single mode\", CHECK);\n+      jobject thread_handle = JNIHandles::make_local(THREAD, thread_oop());\n+      make_thread(deoptimizer_t, thread_handle, NULL, NULL, THREAD);\n+    }\n@@ -961,0 +1052,1 @@\n+#endif \/\/ defined(ASSERT) && COMPILER2_OR_JVMCI\n@@ -1014,1 +1106,1 @@\n-      JavaThread *ct = make_thread(compiler2_object(i), _c2_compile_queue, _compilers[1], THREAD);\n+      JavaThread *ct = make_thread(compiler_t, compiler2_object(i), _c2_compile_queue, _compilers[1], THREAD);\n@@ -1034,1 +1126,1 @@\n-      JavaThread *ct = make_thread(compiler1_object(i), _c1_compile_queue, _compilers[0], THREAD);\n+      JavaThread *ct = make_thread(compiler_t, compiler1_object(i), _c1_compile_queue, _compilers[0], THREAD);\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":107,"deletions":15,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -1062,1 +1062,1 @@\n-  MetaspaceUtils::verify_metrics();\n+  DEBUG_ONLY(MetaspaceUtils::verify();)\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"classfile\/javaClasses.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -62,5 +62,0 @@\n-      if (C->range_check_cast_count() > 0) {\n-        \/\/ No more loop optimizations. Remove all range check dependent CastIINodes.\n-        C->remove_range_check_casts(igvn);\n-        igvn.optimize();\n-      }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"memory\/archiveUtils.hpp\"\n@@ -1785,0 +1786,3 @@\n+\n+  \/\/ Log dynamic info to CDS classlist.\n+  ArchiveUtils::log_to_classlist(&bootstrap_specifier, THREAD);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -107,0 +107,13 @@\n+EntryPoint::EntryPoint(address aentry, address ientry, address lentry, address fentry, address dentry, address ventry) {\n+  assert(number_of_states == 10, \"check the code below\");\n+  _entry[btos] = ientry;\n+  _entry[ztos] = ientry;\n+  _entry[ctos] = ientry;\n+  _entry[stos] = ientry;\n+  _entry[atos] = aentry;\n+  _entry[itos] = ientry;\n+  _entry[ltos] = lentry;\n+  _entry[ftos] = fentry;\n+  _entry[dtos] = dentry;\n+  _entry[vtos] = ventry;\n+}\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreter.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -72,4 +72,0 @@\n-                 generate_trace_code(btos),\n-                 generate_trace_code(ztos),\n-                 generate_trace_code(ctos),\n-                 generate_trace_code(stos),\n@@ -87,1 +83,0 @@\n-    const int index_size = sizeof(u2);\n@@ -90,1 +85,0 @@\n-      address return_itos = generate_return_entry_for(itos, i, index_size);\n@@ -93,10 +87,6 @@\n-                   return_itos,\n-                   return_itos,\n-                   return_itos,\n-                   return_itos,\n-                   generate_return_entry_for(atos, i, index_size),\n-                   return_itos,\n-                   generate_return_entry_for(ltos, i, index_size),\n-                   generate_return_entry_for(ftos, i, index_size),\n-                   generate_return_entry_for(dtos, i, index_size),\n-                   generate_return_entry_for(vtos, i, index_size)\n+                   generate_return_entry_for(atos, i, sizeof(u2)),\n+                   generate_return_entry_for(itos, i, sizeof(u2)),\n+                   generate_return_entry_for(ltos, i, sizeof(u2)),\n+                   generate_return_entry_for(ftos, i, sizeof(u2)),\n+                   generate_return_entry_for(dtos, i, sizeof(u2)),\n+                   generate_return_entry_for(vtos, i, sizeof(u2))\n@@ -108,3 +98,3 @@\n-    \/\/ These states are in order specified in TosState, except btos\/ztos\/ctos\/stos are\n-    \/\/ really the same as itos since there is no top of stack optimization for these types\n-    const TosState states[] = {itos, itos, itos, itos, itos, ltos, ftos, dtos, atos, vtos, ilgl};\n+    \/\/ These states are in order specified in TosState, except btos\/ztos\/ctos\/stos which\n+    \/\/ are the same as itos since there is no top of stack optimization for these types\n+    const TosState states[] = {ilgl, ilgl, ilgl, ilgl, itos, ltos, ftos, dtos, atos, vtos, ilgl};\n@@ -115,1 +105,4 @@\n-    for (int i = 0; i < Interpreter::number_of_return_addrs; i++) {\n+    assert(invoke_length >= 0 && invoke_length < Interpreter::number_of_return_entries, \"invariant\");\n+    assert(invokeinterface_length >= 0 && invokeinterface_length < Interpreter::number_of_return_entries, \"invariant\");\n+\n+    for (int i = itos; i < Interpreter::number_of_return_addrs; i++) {\n@@ -118,3 +111,13 @@\n-      Interpreter::_invoke_return_entry[i] = generate_return_entry_for(state, invoke_length, sizeof(u2));\n-      Interpreter::_invokeinterface_return_entry[i] = generate_return_entry_for(state, invokeinterface_length, sizeof(u2));\n-      Interpreter::_invokedynamic_return_entry[i] = generate_return_entry_for(state, invokedynamic_length, sizeof(u4));\n+\n+      \/\/ Reuse generated entry points\n+      Interpreter::_invoke_return_entry[i]          = Interpreter::_return_entry[invoke_length].entry(state);\n+      Interpreter::_invokeinterface_return_entry[i] = Interpreter::_return_entry[invokeinterface_length].entry(state);\n+\n+      Interpreter::_invokedynamic_return_entry[i]   = generate_return_entry_for(state, invokedynamic_length, sizeof(u4));\n+    }\n+\n+    \/\/ set itos entry points for btos\/ztos\/ctos\/stos\n+    for (int i = 0; i < itos; i++) {\n+      Interpreter::_invoke_return_entry[i]          = Interpreter::_invoke_return_entry[itos];\n+      Interpreter::_invokeinterface_return_entry[i] = Interpreter::_invokeinterface_return_entry[itos];\n+      Interpreter::_invokedynamic_return_entry[i]   = Interpreter::_invokedynamic_return_entry[itos];\n@@ -125,0 +128,1 @@\n+    address earlyret_entry_itos = generate_earlyret_entry_for(itos);\n@@ -127,4 +131,0 @@\n-                 generate_earlyret_entry_for(btos),\n-                 generate_earlyret_entry_for(ztos),\n-                 generate_earlyret_entry_for(ctos),\n-                 generate_earlyret_entry_for(stos),\n@@ -157,4 +157,0 @@\n-                 generate_safept_entry_for(btos, CAST_FROM_FN_PTR(address, InterpreterRuntime::at_safepoint)),\n-                 generate_safept_entry_for(ztos, CAST_FROM_FN_PTR(address, InterpreterRuntime::at_safepoint)),\n-                 generate_safept_entry_for(ctos, CAST_FROM_FN_PTR(address, InterpreterRuntime::at_safepoint)),\n-                 generate_safept_entry_for(stos, CAST_FROM_FN_PTR(address, InterpreterRuntime::at_safepoint)),\n@@ -246,5 +242,1 @@\n-                   deopt_itos, \/* btos *\/\n-                   deopt_itos, \/* ztos *\/\n-                   deopt_itos, \/* ctos *\/\n-                   deopt_itos, \/* stos *\/\n-                   deopt_itos, \/* itos *\/\n+                   generate_deopt_entry_for(itos, i),\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreterGenerator.cpp","additions":28,"deletions":36,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -1191,0 +1191,3 @@\n+  \/\/ has_ea_local_in_scope and arg_escape should be added to JVMCI\n+  const bool has_ea_local_in_scope = false;\n+  const bool arg_escape            = false;\n@@ -1192,0 +1195,1 @@\n+                                  has_ea_local_in_scope, arg_escape,\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2352,1 +2352,1 @@\n-C2V_VMENTRY_PREFIX(jboolean, attachCurrentThread, (JNIEnv* env, jobject c2vm, jboolean as_daemon))\n+C2V_VMENTRY_PREFIX(jboolean, attachCurrentThread, (JNIEnv* env, jobject c2vm, jbyteArray name, jboolean as_daemon))\n@@ -2355,0 +2355,2 @@\n+    guarantee(name != NULL, \"libjvmci caller must pass non-null name\");\n+\n@@ -2357,2 +2359,12 @@\n-    jint res = as_daemon ? main_vm.AttachCurrentThreadAsDaemon((void**) &hotspotEnv, NULL) :\n-                           main_vm.AttachCurrentThread((void**) &hotspotEnv, NULL);\n+\n+    int name_len = env->GetArrayLength(name);\n+    char name_buf[64]; \/\/ Cannot use Resource heap as it requires a current thread\n+    int to_copy = MIN2(name_len, (int) sizeof(name_buf) - 1);\n+    env->GetByteArrayRegion(name, 0, to_copy, (jbyte*) name_buf);\n+    name_buf[to_copy] = '\\0';\n+    JavaVMAttachArgs attach_args;\n+    attach_args.version = JNI_VERSION_1_2;\n+    attach_args.name = name_buf;\n+    attach_args.group = NULL;\n+    jint res = as_daemon ? main_vm.AttachCurrentThreadAsDaemon((void**) &hotspotEnv, &attach_args) :\n+                           main_vm.AttachCurrentThread((void**) &hotspotEnv, &attach_args);\n@@ -2806,1 +2818,1 @@\n-  {CC \"attachCurrentThread\",                          CC \"(Z)Z\",                                                                            FN_PTR(attachCurrentThread)},\n+  {CC \"attachCurrentThread\",                          CC \"([BZ)Z\",                                                                          FN_PTR(attachCurrentThread)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+class CollectedHeap;\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -394,0 +394,1 @@\n+  declare_constant(JVMCINMethodData::SPECULATION_LENGTH_BITS)             \\\n@@ -410,0 +411,1 @@\n+  declare_preprocessor_constant(\"JVM_ACC_FIELD_INITIALIZED_FINAL_UPDATE\", JVM_ACC_FIELD_INITIALIZED_FINAL_UPDATE) \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+  NOT_PRODUCT(LOG_TAG(codestrings)) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -145,0 +145,1 @@\n+  f(mtMetaspace,      \"Metaspace\")                                                   \\\n","filename":"src\/hotspot\/share\/memory\/allocation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1153,3 +1153,0 @@\n-  virtual bool should_verify_oops(void) {\n-    return false;\n-  }\n","filename":"src\/hotspot\/share\/memory\/heapShared.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -107,8 +107,0 @@\n-\n-#ifdef ASSERT\n-  \/\/ Default verification of each visited oop field.\n-  template <typename T> void verify(T* p);\n-\n-  \/\/ Can be used by subclasses to turn off the default verification of oop fields.\n-  virtual bool should_verify_oops() { return true; }\n-#endif\n@@ -367,1 +359,0 @@\n-  template <typename OopClosureType, typename T> static void do_oop_no_verify(OopClosureType* closure, T* p);\n","filename":"src\/hotspot\/share\/memory\/iterator.hpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"memory\/universe.hpp\"\n@@ -56,16 +55,0 @@\n-#ifdef ASSERT\n-\/\/ This verification is applied to all visited oops.\n-\/\/ The closures can turn is off by overriding should_verify_oops().\n-template <typename T>\n-void OopIterateClosure::verify(T* p) {\n-  if (should_verify_oops()) {\n-    T heap_oop = RawAccess<>::oop_load(p);\n-    if (!CompressedOops::is_null(heap_oop)) {\n-      oop o = CompressedOops::decode_not_null(heap_oop);\n-      assert(Universe::heap()->is_in(o),\n-             \"should be in closed *p \" PTR_FORMAT \" \" PTR_FORMAT, p2i(p), p2i(o));\n-    }\n-  }\n-}\n-#endif\n-\n@@ -127,5 +110,0 @@\n-template <typename OopClosureType, typename T>\n-inline void Devirtualizer::do_oop_no_verify(OopClosureType* closure, T* p) {\n-  call_do_oop<T>(&OopClosureType::do_oop, &OopClosure::do_oop, closure, p);\n-}\n-\n@@ -134,3 +112,1 @@\n-  debug_only(closure->verify(p));\n-\n-  do_oop_no_verify(closure, p);\n+  call_do_oop<T>(&OopClosureType::do_oop, &OopClosure::do_oop, closure, p);\n","filename":"src\/hotspot\/share\/memory\/iterator.inline.hpp","additions":1,"deletions":25,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -112,1 +112,1 @@\n-  class Ref : public CHeapObj<mtInternal> {\n+  class Ref : public CHeapObj<mtMetaspace> {\n","filename":"src\/hotspot\/share\/memory\/metaspaceClosure.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -781,0 +781,2 @@\n+  SystemDictionaryShared::adjust_lambda_proxy_class_dictionary();\n+\n","filename":"src\/hotspot\/share\/memory\/metaspaceShared.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1163,1 +1163,1 @@\n-    MetaspaceUtils::verify_free_chunks();\n+    DEBUG_ONLY(MetaspaceUtils::verify();)\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"classfile\/classListWriter.hpp\"\n@@ -3153,0 +3154,13 @@\n+\n+static bool is_prohibited_package_slow(Symbol* class_name) {\n+  \/\/ Caller has ResourceMark\n+  int length;\n+  jchar* unicode = class_name->as_unicode(length);\n+  return (length >= 5 &&\n+          unicode[0] == 'j' &&\n+          unicode[1] == 'a' &&\n+          unicode[2] == 'v' &&\n+          unicode[3] == 'a' &&\n+          unicode[4] == '\/');\n+}\n+\n@@ -3159,1 +3173,1 @@\n-      class_name != NULL) {\n+      class_name != NULL && class_name->utf8_length() >= 5) {\n@@ -3161,2 +3175,9 @@\n-    char* name = class_name->as_C_string();\n-    if (strncmp(name, JAVAPKG, JAVAPKG_LEN) == 0 && name[JAVAPKG_LEN] == '\/') {\n+    bool prohibited;\n+    const u1* base = class_name->base();\n+    if ((base[0] | base[1] | base[2] | base[3] | base[4]) & 0x80) {\n+      prohibited = is_prohibited_package_slow(class_name);\n+    } else {\n+      char* name = class_name->as_C_string();\n+      prohibited = (strncmp(name, JAVAPKG, JAVAPKG_LEN) == 0 && name[JAVAPKG_LEN] == '\/');\n+    }\n+    if (prohibited) {\n@@ -3165,1 +3186,1 @@\n-      name = pkg_name->as_C_string();\n+      char* name = pkg_name->as_C_string();\n@@ -4381,1 +4402,1 @@\n-  if (DumpLoadedClassList && classlist_file->is_open()) {\n+  if (ClassListWriter::is_enabled()) {\n@@ -4394,0 +4415,5 @@\n+      if (is_hidden()) {\n+        \/\/ Don't include archived lambda proxy class in the classlist.\n+        assert(!is_non_strong_hidden(), \"unexpected non-strong hidden class\");\n+        return;\n+      }\n@@ -4421,2 +4447,3 @@\n-      classlist_file->print_cr(\"%s\", name()->as_C_string());\n-      classlist_file->flush();\n+      ClassListWriter w;\n+      w.stream()->print_cr(\"%s\", name()->as_C_string());\n+      w.stream()->flush();\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":34,"deletions":7,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -621,0 +621,1 @@\n+    ResourceMark rm(THREAD);\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1019,0 +1019,8 @@\n+\/\/ Check if addition of an integer with type 't' and a constant 'c' can overflow\n+static bool can_overflow(const TypeInt* t, jint c) {\n+  jint t_lo = t->_lo;\n+  jint t_hi = t->_hi;\n+  return ((c < 0 && (java_add(t_lo, c) > t_lo)) ||\n+          (c > 0 && (java_add(t_hi, c) < t_hi)));\n+}\n+\n@@ -1041,1 +1049,1 @@\n-  int x_off = 0;\n+  jint x_off = 0;\n@@ -1052,1 +1060,1 @@\n-  int y_off = 0;\n+  jint y_off = 0;\n@@ -1066,0 +1074,1 @@\n+  const TypeInt* tx = phase->type(x)->isa_int();\n@@ -1082,5 +1091,7 @@\n-    \/\/ See if covers: MIN2(x+c0,MIN2(y+c1,z))\n-    if( !phase->eqv(x,y) ) return NULL;\n-    \/\/ If (y == x) transform MIN2(x+c0, MIN2(x+c1,z)) into\n-    \/\/ MIN2(x+c0 or x+c1 which less, z).\n-    return new MinINode(phase->transform(new AddINode(x,phase->intcon(MIN2(x_off,y_off)))),r->in(2));\n+    \/\/ Transform MIN2(x + c0, MIN2(x + c1, z)) into MIN2(x + MIN2(c0, c1), z)\n+    \/\/ if x == y and the additions can't overflow.\n+    if (phase->eqv(x,y) && tx != NULL &&\n+        !can_overflow(tx, x_off) &&\n+        !can_overflow(tx, y_off)) {\n+      return new MinINode(phase->transform(new AddINode(x, phase->intcon(MIN2(x_off, y_off)))), r->in(2));\n+    }\n@@ -1088,4 +1099,7 @@\n-    \/\/ See if covers: MIN2(x+c0,y+c1)\n-    if( !phase->eqv(x,y) ) return NULL;\n-    \/\/ If (y == x) transform MIN2(x+c0,x+c1) into x+c0 or x+c1 which less.\n-    return new AddINode(x,phase->intcon(MIN2(x_off,y_off)));\n+    \/\/ Transform MIN2(x + c0, y + c1) into x + MIN2(c0, c1)\n+    \/\/ if x == y and the additions can't overflow.\n+    if (phase->eqv(x,y) && tx != NULL &&\n+        !can_overflow(tx, x_off) &&\n+        !can_overflow(tx, y_off)) {\n+      return new AddINode(x,phase->intcon(MIN2(x_off,y_off)));\n+    }\n@@ -1093,1 +1107,1 @@\n-\n+  return NULL;\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":26,"deletions":12,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -796,1 +796,7 @@\n-          \"Do not use subtype check macro node\")\n+          \"Do not use subtype check macro node\")                            \\\n+                                                                            \\\n+  develop(uintx, StressLongCountedLoop, 0,                                  \\\n+          \"if > 0, convert int counted loops to long counted loops\"         \\\n+          \"to stress handling of long counted loops: run inner loop\"        \\\n+          \"for at most jint_max \/ StressLongCountedLoop\")                   \\\n+          range(0, max_juint)                                               \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -96,2 +96,1 @@\n-  bool do_escape_analysis = DoEscapeAnalysis && !env->should_retain_local_variables()\n-                                             && !env->jvmti_can_get_owned_monitor_info();\n+  bool do_escape_analysis = DoEscapeAnalysis;\n@@ -499,1 +498,1 @@\n-  \/\/ case vmIntrinsics::_indexOfL_char: \/\/ Disable it until found issues are fixed\n+  case vmIntrinsics::_indexOfL_char:\n@@ -634,0 +633,1 @@\n+  case vmIntrinsics::_sha3_implCompress:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1009,1 +1009,1 @@\n-void CallJavaNode::copy_call_debug_info(PhaseIterGVN* phase, CallNode *oldcall) {\n+void CallJavaNode::copy_call_debug_info(PhaseIterGVN* phase, SafePointNode *sfpt) {\n@@ -1011,1 +1011,1 @@\n-  uint old_dbg_start = oldcall->tf()->domain_sig()->cnt();\n+  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain_sig()->cnt() : (uint)TypeFunc::Parms+1;\n@@ -1020,2 +1020,2 @@\n-  for (uint i = old_dbg_start; i < oldcall->req(); i++) {\n-    Node* old_in = oldcall->in(i);\n+  for (uint i = old_dbg_start; i < sfpt->req(); i++) {\n+    Node* old_in = sfpt->in(i);\n@@ -1025,3 +1025,3 @@\n-      uint old_unique = C->unique();\n-      Node* new_in = old_sosn->clone(sosn_map);\n-      if (old_unique != C->unique()) { \/\/ New node?\n+      bool new_node;\n+      Node* new_in = old_sosn->clone(sosn_map, new_node);\n+      if (new_node) { \/\/ New node?\n@@ -1037,1 +1037,1 @@\n-  set_jvms(oldcall->jvms() != NULL ? oldcall->jvms()->clone_deep(C) : NULL);\n+  set_jvms(sfpt->jvms() != NULL ? sfpt->jvms()->clone_deep(C) : NULL);\n@@ -1408,1 +1408,3 @@\n-  if( in(0)->is_Proj() ) {\n+  \/\/ Transforming long counted loops requires a safepoint node. Do not\n+  \/\/ eliminate a safepoint until loop opts are over.\n+  if (in(0)->is_Proj() && !phase->C->major_progress()) {\n@@ -1581,1 +1583,1 @@\n-SafePointScalarObjectNode::clone(Dict* sosn_map) const {\n+SafePointScalarObjectNode::clone(Dict* sosn_map, bool& new_node) const {\n@@ -1584,0 +1586,1 @@\n+    new_node = false;\n@@ -1586,0 +1589,1 @@\n+  new_node = true;\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":14,"deletions":10,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -331,1 +331,2 @@\n-      _adr_type(adr_type)\n+      _adr_type(adr_type),\n+      _has_ea_local_in_scope(false)\n@@ -339,0 +340,1 @@\n+  bool            _has_ea_local_in_scope; \/\/ NoEscape or ArgEscape objects in JVM States\n@@ -458,0 +460,6 @@\n+  void set_has_ea_local_in_scope(bool b) {\n+    _has_ea_local_in_scope = b;\n+  }\n+  bool has_ea_local_in_scope() const {\n+    return _has_ea_local_in_scope;\n+  }\n@@ -529,1 +537,1 @@\n-  SafePointScalarObjectNode* clone(Dict* sosn_map) const;\n+  SafePointScalarObjectNode* clone(Dict* sosn_map, bool& new_node) const;\n@@ -657,1 +665,1 @@\n-  virtual void copy_call_debug_info(PhaseIterGVN* phase, CallNode *oldcall) {}\n+  virtual void copy_call_debug_info(PhaseIterGVN* phase, SafePointNode *sfpt) {}\n@@ -680,0 +688,1 @@\n+  bool    _arg_escape;             \/\/ ArgEscape in parameter list\n@@ -687,1 +696,2 @@\n-      _method(method), _bci(bci)\n+      _method(method),\n+      _arg_escape(false), _bci(bci)\n@@ -701,2 +711,3 @@\n-\n-  void copy_call_debug_info(PhaseIterGVN* phase, CallNode *oldcall);\n+  void  set_arg_escape(bool f)             { _arg_escape = f; }\n+  bool  arg_escape() const                 { return _arg_escape; }\n+  void copy_call_debug_info(PhaseIterGVN* phase, SafePointNode *sfpt);\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":17,"deletions":6,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -237,9 +237,10 @@\n-  if (can_reshape && !_range_check_dependency && !phase->C->major_progress()) {\n-    const TypeInt* this_type = this->type()->is_int();\n-    const TypeInt* in_type = phase->type(in(1))->isa_int();\n-    if (in_type != NULL && this_type != NULL &&\n-        (in_type->_lo != this_type->_lo ||\n-         in_type->_hi != this_type->_hi)) {\n-      jint lo1 = this_type->_lo;\n-      jint hi1 = this_type->_hi;\n-      int w1  = this_type->_widen;\n+  if (can_reshape && !_range_check_dependency) {\n+    if (phase->C->post_loop_opts_phase()) {\n+      const TypeInt* this_type = this->type()->is_int();\n+      const TypeInt* in_type = phase->type(in(1))->isa_int();\n+      if (in_type != NULL && this_type != NULL &&\n+          (in_type->_lo != this_type->_lo ||\n+           in_type->_hi != this_type->_hi)) {\n+        jint lo1 = this_type->_lo;\n+        jint hi1 = this_type->_hi;\n+        int w1  = this_type->_widen;\n@@ -247,15 +248,16 @@\n-      if (lo1 >= 0) {\n-        \/\/ Keep a range assertion of >=0.\n-        lo1 = 0;        hi1 = max_jint;\n-      } else if (hi1 < 0) {\n-        \/\/ Keep a range assertion of <0.\n-        lo1 = min_jint; hi1 = -1;\n-      } else {\n-        lo1 = min_jint; hi1 = max_jint;\n-      }\n-      const TypeInt* wtype = TypeInt::make(MAX2(in_type->_lo, lo1),\n-                                           MIN2(in_type->_hi, hi1),\n-                                           MAX2((int)in_type->_widen, w1));\n-      if (wtype != type()) {\n-        set_type(wtype);\n-        return this;\n+        if (lo1 >= 0) {\n+          \/\/ Keep a range assertion of >=0.\n+          lo1 = 0;        hi1 = max_jint;\n+        } else if (hi1 < 0) {\n+          \/\/ Keep a range assertion of <0.\n+          lo1 = min_jint; hi1 = -1;\n+        } else {\n+          lo1 = min_jint; hi1 = max_jint;\n+        }\n+        const TypeInt* wtype = TypeInt::make(MAX2(in_type->_lo, lo1),\n+                                             MIN2(in_type->_hi, hi1),\n+                                             MAX2((int)in_type->_widen, w1));\n+        if (wtype != type()) {\n+          set_type(wtype);\n+          return this;\n+        }\n@@ -263,0 +265,2 @@\n+    } else {\n+      phase->C->record_for_post_loop_opts_igvn(this);\n@@ -268,0 +272,15 @@\n+Node* CastIINode::Identity(PhaseGVN* phase) {\n+  Node* progress = ConstraintCastNode::Identity(phase);\n+  if (progress != this) {\n+    return progress;\n+  }\n+  if (_range_check_dependency) {\n+    if (phase->C->post_loop_opts_phase()) {\n+      return this->in(1);\n+    } else {\n+      phase->C->record_for_post_loop_opts_igvn(this);\n+    }\n+  }\n+  return this;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/castnode.cpp","additions":43,"deletions":24,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+  virtual Node* Identity(PhaseGVN* phase);\n","filename":"src\/hotspot\/share\/opto\/castnode.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -123,3 +123,3 @@\n-  for (int i = 1; i < _intrinsics->length(); i++) {\n-    CallGenerator* cg1 = _intrinsics->at(i-1);\n-    CallGenerator* cg2 = _intrinsics->at(i);\n+  for (int i = 1; i < _intrinsics.length(); i++) {\n+    CallGenerator* cg1 = _intrinsics.at(i-1);\n+    CallGenerator* cg2 = _intrinsics.at(i);\n@@ -133,1 +133,1 @@\n-  return _intrinsics->find_sorted<IntrinsicDescPair*, IntrinsicDescPair::compare>(&pair, found);\n+  return _intrinsics.find_sorted<IntrinsicDescPair*, IntrinsicDescPair::compare>(&pair, found);\n@@ -137,4 +137,0 @@\n-  if (_intrinsics == NULL) {\n-    _intrinsics = new (comp_arena())GrowableArray<CallGenerator*>(comp_arena(), 60, 0, NULL);\n-  }\n-  int len = _intrinsics->length();\n@@ -144,1 +140,1 @@\n-  _intrinsics->insert_before(index, cg);\n+  _intrinsics.insert_before(index, cg);\n@@ -150,1 +146,1 @@\n-  if (_intrinsics != NULL) {\n+  if (_intrinsics.length() > 0) {\n@@ -154,1 +150,1 @@\n-      return _intrinsics->at(index);\n+      return _intrinsics.at(index);\n@@ -172,3 +168,1 @@\n-\/\/ Compile:: register_library_intrinsics and make_vm_intrinsic are defined\n-\/\/ in library_call.cpp.\n-\n+\/\/ Compile::make_vm_intrinsic is defined in library_call.cpp.\n@@ -356,0 +350,9 @@\n+void Compile::remove_useless_nodes(GrowableArray<Node*>& node_list, Unique_Node_List& useful) {\n+  for (int i = node_list.length() - 1; i >= 0; i--) {\n+    Node* n = node_list.at(i);\n+    if (!useful.member(n)) {\n+      node_list.remove_if_existing(n);\n+    }\n+  }\n+}\n+\n@@ -370,1 +373,1 @@\n-      if (! useful.member(child)) {\n+      if (!useful.member(child)) {\n@@ -383,35 +386,6 @@\n-  \/\/ Remove useless macro and predicate opaq nodes\n-  for (int i = C->macro_count()-1; i >= 0; i--) {\n-    Node* n = C->macro_node(i);\n-    if (!useful.member(n)) {\n-      remove_macro_node(n);\n-    }\n-  }\n-  \/\/ Remove useless CastII nodes with range check dependency\n-  for (int i = range_check_cast_count() - 1; i >= 0; i--) {\n-    Node* cast = range_check_cast_node(i);\n-    if (!useful.member(cast)) {\n-      remove_range_check_cast(cast);\n-    }\n-  }\n-  \/\/ Remove useless expensive nodes\n-  for (int i = C->expensive_count()-1; i >= 0; i--) {\n-    Node* n = C->expensive_node(i);\n-    if (!useful.member(n)) {\n-      remove_expensive_node(n);\n-    }\n-  }\n-  \/\/ Remove useless Opaque4 nodes\n-  for (int i = opaque4_count() - 1; i >= 0; i--) {\n-    Node* opaq = opaque4_node(i);\n-    if (!useful.member(opaq)) {\n-      remove_opaque4_node(opaq);\n-    }\n-  }\n-  \/\/ Remove useless inline type nodes\n-  for (int i = _inline_type_nodes->length() - 1; i >= 0; i--) {\n-    Node* vt = _inline_type_nodes->at(i);\n-    if (!useful.member(vt)) {\n-      _inline_type_nodes->remove(vt);\n-    }\n-  }\n+\n+  remove_useless_nodes(_macro_nodes,        useful); \/\/ remove useless macro and predicate opaq nodes\n+  remove_useless_nodes(_expensive_nodes,    useful); \/\/ remove useless expensive nodes\n+  remove_useless_nodes(_for_post_loop_igvn, useful); \/\/ remove useless node recorded for post loop opts IGVN pass\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n+\n@@ -527,0 +501,1 @@\n+                  _post_loop_opts_phase(false),\n@@ -544,0 +519,6 @@\n+                  _intrinsics        (comp_arena(), 0, 0, NULL),\n+                  _macro_nodes       (comp_arena(), 8, 0, NULL),\n+                  _predicate_opaqs   (comp_arena(), 8, 0, NULL),\n+                  _expensive_nodes   (comp_arena(), 8, 0, NULL),\n+                  _for_post_loop_igvn(comp_arena(), 8, 0, NULL),\n+                  _inline_type_nodes (comp_arena(), 8, 0, NULL),\n@@ -830,0 +811,1 @@\n+    _post_loop_opts_phase(false),\n@@ -1031,8 +1013,0 @@\n-  _intrinsics = NULL;\n-  _macro_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);\n-  _predicate_opaqs = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);\n-  _expensive_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);\n-  _range_check_casts = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);\n-  _opaque4_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);\n-  _inline_type_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);\n-  register_library_intrinsics();\n@@ -1853,12 +1827,5 @@\n-void Compile::add_range_check_cast(Node* n) {\n-  assert(n->isa_CastII()->has_range_check(), \"CastII should have range check dependency\");\n-  assert(!_range_check_casts->contains(n), \"duplicate entry in range check casts\");\n-  _range_check_casts->append(n);\n-}\n-\n-\/\/ Remove all range check dependent CastIINodes.\n-void Compile::remove_range_check_casts(PhaseIterGVN &igvn) {\n-  for (int i = range_check_cast_count(); i > 0; i--) {\n-    Node* cast = range_check_cast_node(i-1);\n-    assert(cast->isa_CastII()->has_range_check(), \"CastII should have range check dependency\");\n-    igvn.replace_node(cast, cast->in(1));\n+void Compile::record_for_post_loop_opts_igvn(Node* n) {\n+  if (!n->for_post_loop_opts_igvn()) {\n+    assert(!_for_post_loop_igvn.contains(n), \"duplicate\");\n+    n->add_flag(Node::NodeFlags::Flag_for_post_loop_opts_igvn);\n+    _for_post_loop_igvn.append(n);\n@@ -1866,1 +1833,0 @@\n-  assert(range_check_cast_count() == 0, \"should be empty\");\n@@ -1869,4 +1835,3 @@\n-void Compile::add_opaque4_node(Node* n) {\n-  assert(n->Opcode() == Op_Opaque4, \"Opaque4 only\");\n-  assert(!_opaque4_nodes->contains(n), \"duplicate entry in Opaque4 list\");\n-  _opaque4_nodes->append(n);\n+void Compile::remove_from_post_loop_opts_igvn(Node* n) {\n+  n->remove_flag(Node::NodeFlags::Flag_for_post_loop_opts_igvn);\n+  _for_post_loop_igvn.remove(n);\n@@ -1875,16 +1840,8 @@\n-\/\/ Remove all Opaque4 nodes.\n-void Compile::remove_opaque4_nodes(PhaseIterGVN &igvn) {\n-  for (int i = opaque4_count(); i > 0; i--) {\n-    Node* opaq = opaque4_node(i-1);\n-    assert(opaq->Opcode() == Op_Opaque4, \"Opaque4 only\");\n-    \/\/ With Opaque4 nodes, the expectation is that the test of input 1\n-    \/\/ is always equal to the constant value of input 2. So we can\n-    \/\/ remove the Opaque4 and replace it by input 2. In debug builds,\n-    \/\/ leave the non constant test in instead to sanity check that it\n-    \/\/ never fails (if it does, that subgraph was constructed so, at\n-    \/\/ runtime, a Halt node is executed).\n-#ifdef ASSERT\n-    igvn.replace_node(opaq, opaq->in(1));\n-#else\n-    igvn.replace_node(opaq, opaq->in(2));\n-#endif\n+void Compile::process_for_post_loop_opts_igvn(PhaseIterGVN& igvn) {\n+  if (_for_post_loop_igvn.length() == 0) {\n+    return; \/\/ no work to do\n+  }\n+  while (_for_post_loop_igvn.length() > 0) {\n+    Node* n = _for_post_loop_igvn.pop();\n+    n->remove_flag(Node::NodeFlags::Flag_for_post_loop_opts_igvn);\n+    igvn._worklist.push(n);\n@@ -1892,1 +1849,2 @@\n-  assert(opaque4_count() == 0, \"should be empty\");\n+  igvn.optimize();\n+  assert(_for_post_loop_igvn.length() == 0, \"no more delayed nodes allowed\");\n@@ -1897,3 +1855,1 @@\n-  if (_inline_type_nodes != NULL) {\n-    _inline_type_nodes->push(n);\n-  }\n+  _inline_type_nodes.push(n);\n@@ -1904,2 +1860,2 @@\n-  if (_inline_type_nodes != NULL && _inline_type_nodes->contains(n)) {\n-    _inline_type_nodes->remove(n);\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n@@ -1938,2 +1894,2 @@\n-  for (int i = _inline_type_nodes->length()-1; i >= 0; i--) {\n-    InlineTypeBaseNode* vt = _inline_type_nodes->at(i)->as_InlineTypeBase();\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    InlineTypeBaseNode* vt = _inline_type_nodes.at(i)->as_InlineTypeBase();\n@@ -1944,2 +1900,2 @@\n-    while (_inline_type_nodes->length() > 0) {\n-      InlineTypeBaseNode* vt = _inline_type_nodes->pop()->as_InlineTypeBase();\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeBaseNode* vt = _inline_type_nodes.pop()->as_InlineTypeBase();\n@@ -2400,1 +2356,1 @@\n-  if(_loop_opts_cnt > 0) {\n+  if (_loop_opts_cnt > 0) {\n@@ -2402,1 +2358,1 @@\n-    while(major_progress() && (_loop_opts_cnt > 0)) {\n+    while (major_progress() && (_loop_opts_cnt > 0)) {\n@@ -2532,0 +2488,1 @@\n+    Unique_Node_List* save_for_igvn = for_igvn();\n@@ -2535,0 +2492,1 @@\n+    set_for_igvn(save_for_igvn);\n@@ -2537,1 +2495,1 @@\n-  if (_inline_type_nodes->length() > 0) {\n+  if (_inline_type_nodes.length() > 0) {\n@@ -2650,6 +2608,0 @@\n-  if (range_check_cast_count() > 0) {\n-    \/\/ No more loop optimizations. Remove all range check dependent CastIINodes.\n-    C->remove_range_check_casts(igvn);\n-    igvn.optimize();\n-  }\n-\n@@ -2660,1 +2612,1 @@\n-  if (_inline_type_nodes->length() > 0) {\n+  if (_inline_type_nodes.length() > 0) {\n@@ -2685,4 +2637,3 @@\n-  if (opaque4_count() > 0) {\n-    C->remove_opaque4_nodes(igvn);\n-    igvn.optimize();\n-  }\n+  C->set_post_loop_opts_phase(); \/\/ no more loop opts allowed\n+\n+  process_for_post_loop_opts_igvn(igvn);\n@@ -2696,0 +2647,1 @@\n+  assert(igvn._worklist.size() == 0, \"not empty\");\n@@ -3852,0 +3804,1 @@\n+    assert(!n->as_Loop()->is_transformed_long_loop() || _loop_opts_cnt == 0, \"should have been turned into a counted loop\");\n@@ -4114,1 +4067,1 @@\n-    _expensive_nodes->at(i)->set_req(0, NULL);\n+    _expensive_nodes.at(i)->set_req(0, NULL);\n@@ -4537,2 +4490,0 @@\n-    \/\/ Save CastII node to remove it after loop optimizations.\n-    phase->C->add_range_check_cast(value);\n@@ -4768,1 +4719,1 @@\n-    _expensive_nodes->sort(cmp_expensive_nodes);\n+    _expensive_nodes.sort(cmp_expensive_nodes);\n@@ -4773,2 +4724,2 @@\n-  for (int i = 1; i < _expensive_nodes->length(); i++) {\n-    if (cmp_expensive_nodes(_expensive_nodes->adr_at(i), _expensive_nodes->adr_at(i-1)) < 0) {\n+  for (int i = 1; i < _expensive_nodes.length(); i++) {\n+    if (cmp_expensive_nodes(_expensive_nodes.adr_at(i), _expensive_nodes.adr_at(i-1)) < 0) {\n@@ -4782,1 +4733,1 @@\n-  if (_expensive_nodes->length() == 0) {\n+  if (_expensive_nodes.length() == 0) {\n@@ -4790,2 +4741,2 @@\n-  for (int i = 0; i < _expensive_nodes->length(); i++) {\n-    Node* n = _expensive_nodes->at(i);\n+  for (int i = 0; i < _expensive_nodes.length(); i++) {\n+    Node* n = _expensive_nodes.at(i);\n@@ -4794,1 +4745,1 @@\n-      _expensive_nodes->at_put(j, n);\n+      _expensive_nodes.at_put(j, n);\n@@ -4798,1 +4749,1 @@\n-  _expensive_nodes->trunc_to(j);\n+  _expensive_nodes.trunc_to(j);\n@@ -4805,2 +4756,2 @@\n-  for (int i = 0; i < _expensive_nodes->length()-1; i++) {\n-    if (cmp_expensive_nodes(_expensive_nodes->adr_at(i), _expensive_nodes->adr_at(i+1)) == 0) {\n+  for (int i = 0; i < _expensive_nodes.length()-1; i++) {\n+    if (cmp_expensive_nodes(_expensive_nodes.adr_at(i), _expensive_nodes.adr_at(i+1)) == 0) {\n@@ -4815,1 +4766,1 @@\n-  if (_expensive_nodes->length() == 0) {\n+  if (_expensive_nodes.length() == 0) {\n@@ -4829,1 +4780,1 @@\n-  for (; i < _expensive_nodes->length()-1; i++) {\n+  for (; i < _expensive_nodes.length()-1; i++) {\n@@ -4831,1 +4782,1 @@\n-    if (_expensive_nodes->at(i)->Opcode() == _expensive_nodes->at(i+1)->Opcode()) {\n+    if (_expensive_nodes.at(i)->Opcode() == _expensive_nodes.at(i+1)->Opcode()) {\n@@ -4833,1 +4784,1 @@\n-      _expensive_nodes->at_put(j++, _expensive_nodes->at(i));\n+      _expensive_nodes.at_put(j++, _expensive_nodes.at(i));\n@@ -4837,1 +4788,1 @@\n-      _expensive_nodes->at_put(j++, _expensive_nodes->at(i));\n+      _expensive_nodes.at_put(j++, _expensive_nodes.at(i));\n@@ -4840,1 +4791,1 @@\n-      Node* n = _expensive_nodes->at(i);\n+      Node* n = _expensive_nodes.at(i);\n@@ -4847,3 +4798,3 @@\n-    _expensive_nodes->at_put(j++, _expensive_nodes->at(i));\n-  } else if (_expensive_nodes->length() >= 1) {\n-    Node* n = _expensive_nodes->at(i);\n+    _expensive_nodes.at_put(j++, _expensive_nodes.at(i));\n+  } else if (_expensive_nodes.length() >= 1) {\n+    Node* n = _expensive_nodes.at(i);\n@@ -4854,1 +4805,1 @@\n-  _expensive_nodes->trunc_to(j);\n+  _expensive_nodes.trunc_to(j);\n@@ -4861,1 +4812,1 @@\n-  assert(!_expensive_nodes->contains(n), \"duplicate entry in expensive list\");\n+  assert(!_expensive_nodes.contains(n), \"duplicate entry in expensive list\");\n@@ -4865,1 +4816,1 @@\n-    _expensive_nodes->append(n);\n+    _expensive_nodes.append(n);\n@@ -5064,2 +5015,2 @@\n-        _macro_nodes->at_put(allocates, n);\n-        _macro_nodes->at_put(i, tmp);\n+        _macro_nodes.at_put(allocates, n);\n+        _macro_nodes.at_put(i, tmp);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":93,"deletions":142,"binary":false,"changes":235,"status":"modified"},{"patch":"@@ -267,0 +267,2 @@\n+  bool                  _post_loop_opts_phase;  \/\/ Loop opts are finished.\n+\n@@ -316,7 +318,6 @@\n-  GrowableArray<CallGenerator*>* _intrinsics;   \/\/ List of intrinsics.\n-  GrowableArray<Node*>* _macro_nodes;           \/\/ List of nodes which need to be expanded before matching.\n-  GrowableArray<Node*>* _predicate_opaqs;       \/\/ List of Opaque1 nodes for the loop predicates.\n-  GrowableArray<Node*>* _expensive_nodes;       \/\/ List of nodes that are expensive to compute and that we'd better not let the GVN freely common\n-  GrowableArray<Node*>* _range_check_casts;     \/\/ List of CastII nodes with a range check dependency\n-  GrowableArray<Node*>* _opaque4_nodes;         \/\/ List of Opaque4 nodes that have a default value\n-  GrowableArray<Node*>* _inline_type_nodes;     \/\/ List of InlineType nodes\n+  GrowableArray<CallGenerator*> _intrinsics;    \/\/ List of intrinsics.\n+  GrowableArray<Node*>  _macro_nodes;           \/\/ List of nodes which need to be expanded before matching.\n+  GrowableArray<Node*>  _predicate_opaqs;       \/\/ List of Opaque1 nodes for the loop predicates.\n+  GrowableArray<Node*>  _expensive_nodes;       \/\/ List of nodes that are expensive to compute and that we'd better not let the GVN freely common\n+  GrowableArray<Node*>  _for_post_loop_igvn;    \/\/ List of nodes for IGVN after loop opts are over\n+  GrowableArray<Node*>  _inline_type_nodes;     \/\/ List of InlineType nodes\n@@ -384,2 +385,1 @@\n-  GrowableArray<CallGenerator*> _late_inlines;        \/\/ List of CallGenerators to be revisited after\n-                                                      \/\/ main parsing has finished.\n+  GrowableArray<CallGenerator*> _late_inlines;        \/\/ List of CallGenerators to be revisited after main parsing has finished.\n@@ -387,1 +387,0 @@\n-\n@@ -674,6 +673,8 @@\n-  int           macro_count()             const { return _macro_nodes->length(); }\n-  int           predicate_count()         const { return _predicate_opaqs->length();}\n-  int           expensive_count()         const { return _expensive_nodes->length(); }\n-  Node*         macro_node(int idx)       const { return _macro_nodes->at(idx); }\n-  Node*         predicate_opaque1_node(int idx) const { return _predicate_opaqs->at(idx);}\n-  Node*         expensive_node(int idx)   const { return _expensive_nodes->at(idx); }\n+  int           macro_count()             const { return _macro_nodes.length(); }\n+  int           predicate_count()         const { return _predicate_opaqs.length();}\n+  int           expensive_count()         const { return _expensive_nodes.length(); }\n+\n+  Node*         macro_node(int idx)       const { return _macro_nodes.at(idx); }\n+  Node*         predicate_opaque1_node(int idx) const { return _predicate_opaqs.at(idx);}\n+  Node*         expensive_node(int idx)   const { return _expensive_nodes.at(idx); }\n+\n@@ -684,2 +685,2 @@\n-    assert(!_macro_nodes->contains(n), \"duplicate entry in expand list\");\n-    _macro_nodes->append(n);\n+    assert(!_macro_nodes.contains(n), \"duplicate entry in expand list\");\n+    _macro_nodes.append(n);\n@@ -690,1 +691,1 @@\n-    _macro_nodes->remove_if_existing(n);\n+    _macro_nodes.remove_if_existing(n);\n@@ -693,1 +694,1 @@\n-      _predicate_opaqs->remove_if_existing(n);\n+      _predicate_opaqs.remove_if_existing(n);\n@@ -698,1 +699,1 @@\n-    _expensive_nodes->remove_if_existing(n);\n+    _expensive_nodes.remove_if_existing(n);\n@@ -701,3 +702,3 @@\n-    assert(!_predicate_opaqs->contains(n), \"duplicate entry in predicate opaque1\");\n-    assert(_macro_nodes->contains(n), \"should have already been in macro list\");\n-    _predicate_opaqs->append(n);\n+    assert(!_predicate_opaqs.contains(n), \"duplicate entry in predicate opaque1\");\n+    assert(_macro_nodes.contains(n), \"should have already been in macro list\");\n+    _predicate_opaqs.append(n);\n@@ -706,17 +707,6 @@\n-  \/\/ Range check dependent CastII nodes that can be removed after loop optimizations\n-  void add_range_check_cast(Node* n);\n-  void remove_range_check_cast(Node* n) {\n-    _range_check_casts->remove_if_existing(n);\n-  }\n-  Node* range_check_cast_node(int idx) const { return _range_check_casts->at(idx);  }\n-  int   range_check_cast_count()       const { return _range_check_casts->length(); }\n-  \/\/ Remove all range check dependent CastIINodes.\n-  void  remove_range_check_casts(PhaseIterGVN &igvn);\n-\n-  void add_opaque4_node(Node* n);\n-  void remove_opaque4_node(Node* n) {\n-    _opaque4_nodes->remove_if_existing(n);\n-  }\n-  Node* opaque4_node(int idx) const { return _opaque4_nodes->at(idx);  }\n-  int   opaque4_count()       const { return _opaque4_nodes->length(); }\n-  void  remove_opaque4_nodes(PhaseIterGVN &igvn);\n+  bool     post_loop_opts_phase() { return _post_loop_opts_phase; }\n+  void set_post_loop_opts_phase() { _post_loop_opts_phase = true; }\n+\n+  void record_for_post_loop_opts_igvn(Node* n);\n+  void remove_from_post_loop_opts_igvn(Node* n);\n+  void process_for_post_loop_opts_igvn(PhaseIterGVN& igvn);\n@@ -736,2 +726,2 @@\n-  bool is_predicate_opaq(Node * n) {\n-    return _predicate_opaqs->contains(n);\n+  bool is_predicate_opaq(Node* n) {\n+    return _predicate_opaqs.contains(n);\n@@ -974,0 +964,1 @@\n+  void remove_useless_nodes       (GrowableArray<Node*>&        node_list, Unique_Node_List &useful);\n@@ -1106,1 +1097,0 @@\n-  void           register_library_intrinsics();                            \/\/ initializer\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":34,"deletions":44,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -126,0 +126,1 @@\n+  GrowableArray<SafePointNode*>  sfn_worklist;\n@@ -201,0 +202,3 @@\n+    if (n-> is_SafePoint()) {\n+      sfn_worklist.append(n->as_SafePoint());\n+    }\n@@ -330,0 +334,19 @@\n+\n+  \/\/ Annotate at safepoints if they have <= ArgEscape objects in their scope and at\n+  \/\/ java calls if they pass ArgEscape objects as parameters.\n+  if (has_non_escaping_obj &&\n+      (C->env()->should_retain_local_variables() ||\n+       C->env()->jvmti_can_get_owned_monitor_info() ||\n+       C->env()->jvmti_can_walk_any_space() ||\n+       DeoptimizeObjectsALot)) {\n+    int sfn_length = sfn_worklist.length();\n+    for (int next = 0; next < sfn_length; next++) {\n+      SafePointNode* sfn = sfn_worklist.at(next);\n+      sfn->set_has_ea_local_in_scope(has_ea_local_in_scope(sfn));\n+      if (sfn->is_CallJava()) {\n+        CallJavaNode* call = sfn->as_CallJava();\n+        call->set_arg_escape(has_arg_escape(call));\n+      }\n+    }\n+  }\n+\n@@ -333,0 +356,61 @@\n+\/\/ Returns true if there is an object in the scope of sfn that does not escape globally.\n+bool ConnectionGraph::has_ea_local_in_scope(SafePointNode* sfn) {\n+  Compile* C = _compile;\n+  for (JVMState* jvms = sfn->jvms(); jvms != NULL; jvms = jvms->caller()) {\n+    if (C->env()->should_retain_local_variables() || C->env()->jvmti_can_walk_any_space() ||\n+        DeoptimizeObjectsALot) {\n+      \/\/ Jvmti agents can access locals. Must provide info about local objects at runtime.\n+      int num_locs = jvms->loc_size();\n+      for (int idx = 0; idx < num_locs; idx++) {\n+        Node* l = sfn->local(jvms, idx);\n+        if (not_global_escape(l)) {\n+          return true;\n+        }\n+      }\n+    }\n+    if (C->env()->jvmti_can_get_owned_monitor_info() ||\n+        C->env()->jvmti_can_walk_any_space() || DeoptimizeObjectsALot) {\n+      \/\/ Jvmti agents can read monitors. Must provide info about locked objects at runtime.\n+      int num_mon = jvms->nof_monitors();\n+      for (int idx = 0; idx < num_mon; idx++) {\n+        Node* m = sfn->monitor_obj(jvms, idx);\n+        if (m != NULL && not_global_escape(m)) {\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+\/\/ Returns true if at least one of the arguments to the call is an object\n+\/\/ that does not escape globally.\n+bool ConnectionGraph::has_arg_escape(CallJavaNode* call) {\n+  if (call->method() != NULL) {\n+    uint max_idx = TypeFunc::Parms + call->method()->arg_size();\n+    for (uint idx = TypeFunc::Parms; idx < max_idx; idx++) {\n+      Node* p = call->in(idx);\n+      if (not_global_escape(p)) {\n+        return true;\n+      }\n+    }\n+  } else {\n+    const char* name = call->as_CallStaticJava()->_name;\n+    assert(name != NULL, \"no name\");\n+    \/\/ no arg escapes through uncommon traps\n+    if (strcmp(name, \"uncommon_trap\") != 0) {\n+      \/\/ process_call_arguments() assumes that all arguments escape globally\n+      const TypeTuple* d = call->tf()->domain_sig();\n+      for (uint i = TypeFunc::Parms; i < d->cnt(); i++) {\n+        const Type* at = d->field_at(i);\n+        if (at->isa_oopptr() != NULL) {\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+\n+\n@@ -1034,0 +1118,2 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"sha3_implCompress\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"sha3_implCompressMB\") == 0 ||\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":86,"deletions":0,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -99,6 +99,0 @@\n-\/\/----------------------register_library_intrinsics-----------------------\n-\/\/ Initialize this file's data structures, for each Compile instance.\n-void Compile::register_library_intrinsics() {\n-  \/\/ Nothing to do here.\n-}\n-\n@@ -556,0 +550,1 @@\n+  case vmIntrinsics::_sha3_implCompress:\n@@ -6542,0 +6537,3 @@\n+\/\/ Calculate SHA3 (i.e., SHA3-224 or SHA3-256 or SHA3-384 or SHA3-512) for single-block byte[] array.\n+\/\/ void com.sun.security.provider.SHA3.implCompress(byte[] buf, int ofs)\n+\/\/\n@@ -6564,0 +6562,1 @@\n+  Node* digest_length = NULL;\n@@ -6570,1 +6569,1 @@\n-    state = get_state_from_digest_object(digestBase_obj);\n+    state = get_state_from_digest_object(digestBase_obj, \"[I\");\n@@ -6576,1 +6575,1 @@\n-    state = get_state_from_digest_object(digestBase_obj);\n+    state = get_state_from_digest_object(digestBase_obj, \"[I\");\n@@ -6582,1 +6581,1 @@\n-    state = get_state_from_digest_object(digestBase_obj);\n+    state = get_state_from_digest_object(digestBase_obj, \"[I\");\n@@ -6588,1 +6587,1 @@\n-    state = get_long_state_from_digest_object(digestBase_obj);\n+    state = get_state_from_digest_object(digestBase_obj, \"[J\");\n@@ -6592,0 +6591,8 @@\n+  case vmIntrinsics::_sha3_implCompress:\n+    assert(UseSHA3Intrinsics, \"need SHA3 instruction support\");\n+    state = get_state_from_digest_object(digestBase_obj, \"[B\");\n+    stubAddr = StubRoutines::sha3_implCompress();\n+    stubName = \"sha3_implCompress\";\n+    digest_length = get_digest_length_from_digest_object(digestBase_obj);\n+    if (digest_length == NULL) return false;\n+    break;\n@@ -6602,3 +6609,10 @@\n-  Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::digestBase_implCompress_Type(),\n-                                 stubAddr, stubName, TypePtr::BOTTOM,\n-                                 src_start, state);\n+  Node* call;\n+  if (digest_length == NULL) {\n+    call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::digestBase_implCompress_Type(false),\n+                             stubAddr, stubName, TypePtr::BOTTOM,\n+                             src_start, state);\n+  } else {\n+    call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::digestBase_implCompress_Type(true),\n+                             stubAddr, stubName, TypePtr::BOTTOM,\n+                             src_start, state, digest_length);\n+  }\n@@ -6611,1 +6625,1 @@\n-\/\/ Calculate MD5\/SHA\/SHA2\/SHA5 for multi-block byte[] array.\n+\/\/ Calculate MD5\/SHA\/SHA2\/SHA5\/SHA3 for multi-block byte[] array.\n@@ -6615,3 +6629,3 @@\n-  assert(UseMD5Intrinsics || UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,\n-         \"need MD5\/SHA1\/SHA256\/SHA512 instruction support\");\n-  assert((uint)predicate < 4, \"sanity\");\n+  assert(UseMD5Intrinsics || UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics || UseSHA3Intrinsics,\n+         \"need MD5\/SHA1\/SHA256\/SHA512\/SHA3 instruction support\");\n+  assert((uint)predicate < 5, \"sanity\");\n@@ -6643,1 +6657,1 @@\n-  bool        long_state = false;\n+  const char* state_type = \"[I\";\n@@ -6672,1 +6686,9 @@\n-      long_state = true;\n+      state_type = \"[J\";\n+    }\n+    break;\n+  case 4:\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_sha3_implCompress)) {\n+      klass_digestBase_name = \"sun\/security\/provider\/SHA3\";\n+      stub_name = \"sha3_implCompressMB\";\n+      stub_addr = StubRoutines::sha3_implCompressMB();\n+      state_type = \"[B\";\n@@ -6690,1 +6712,1 @@\n-    return inline_digestBase_implCompressMB(digestBase_obj, instklass_digestBase, long_state, stub_addr, stub_name, src_start, ofs, limit);\n+    return inline_digestBase_implCompressMB(digestBase_obj, instklass_digestBase, state_type, stub_addr, stub_name, src_start, ofs, limit);\n@@ -6697,1 +6719,1 @@\n-                                                      bool long_state, address stubAddr, const char *stubName,\n+                                                      const char* state_type, address stubAddr, const char *stubName,\n@@ -6704,6 +6726,1 @@\n-  Node* state;\n-  if (long_state) {\n-    state = get_long_state_from_digest_object(digest_obj);\n-  } else {\n-    state = get_state_from_digest_object(digest_obj);\n-  }\n+  Node* state = get_state_from_digest_object(digest_obj, state_type);\n@@ -6712,0 +6729,6 @@\n+  Node* digest_length = NULL;\n+  if (strcmp(\"sha3_implCompressMB\", stubName) == 0) {\n+    digest_length = get_digest_length_from_digest_object(digest_obj);\n+    if (digest_length == NULL) return false;\n+  }\n+\n@@ -6713,4 +6736,13 @@\n-  Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,\n-                                 OptoRuntime::digestBase_implCompressMB_Type(),\n-                                 stubAddr, stubName, TypePtr::BOTTOM,\n-                                 src_start, state, ofs, limit);\n+  Node* call;\n+  if (digest_length == NULL) {\n+    call = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                             OptoRuntime::digestBase_implCompressMB_Type(false),\n+                             stubAddr, stubName, TypePtr::BOTTOM,\n+                             src_start, state, ofs, limit);\n+  } else {\n+     call = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                             OptoRuntime::digestBase_implCompressMB_Type(true),\n+                             stubAddr, stubName, TypePtr::BOTTOM,\n+                             src_start, state, digest_length, ofs, limit);\n+  }\n+\n@@ -6725,3 +6757,3 @@\n-Node * LibraryCallKit::get_state_from_digest_object(Node *digest_object) {\n-  Node* digest_state = load_field_from_object(digest_object, \"state\", \"[I\", \/*is_exact*\/ false);\n-  assert (digest_state != NULL, \"wrong version of sun.security.provider.MD5\/SHA\/SHA2\");\n+Node * LibraryCallKit::get_state_from_digest_object(Node *digest_object, const char *state_type) {\n+  Node* digest_state = load_field_from_object(digest_object, \"state\", state_type, \/*is_exact*\/ false);\n+  assert (digest_state != NULL, \"wrong version of sun.security.provider.MD5\/SHA\/SHA2\/SHA5\/SHA3\");\n@@ -6735,9 +6767,5 @@\n-\/\/------------------------------get_long_state_from_digest_object-----------------------\n-Node * LibraryCallKit::get_long_state_from_digest_object(Node *digest_object) {\n-  Node* digest_state = load_field_from_object(digest_object, \"state\", \"[J\", \/*is_exact*\/ false);\n-  assert (digest_state != NULL, \"wrong version of sun.security.provider.SHA5\");\n-  if (digest_state == NULL) return (Node *) NULL;\n-\n-  \/\/ now have the array, need to get the start address of the state array\n-  Node* state = array_element_address(digest_state, intcon(0), T_LONG);\n-  return state;\n+\/\/------------------------------get_digest_length_from_sha3_object----------------------------------\n+Node * LibraryCallKit::get_digest_length_from_digest_object(Node *digest_object) {\n+  Node* digest_length = load_field_from_object(digest_object, \"digestLength\", \"I\", \/*is_exact*\/ false);\n+  assert (digest_length != NULL, \"sanity\");\n+  return digest_length;\n@@ -6749,1 +6777,1 @@\n-\/\/    if (digestBaseObj instanceof MD5\/SHA\/SHA2\/SHA5) do_intrinsic, else do_javapath\n+\/\/    if (digestBaseObj instanceof MD5\/SHA\/SHA2\/SHA5\/SHA3) do_intrinsic, else do_javapath\n@@ -6752,3 +6780,3 @@\n-  assert(UseMD5Intrinsics || UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,\n-         \"need MD5\/SHA1\/SHA256\/SHA512 instruction support\");\n-  assert((uint)predicate < 4, \"sanity\");\n+  assert(UseMD5Intrinsics || UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics || UseSHA3Intrinsics,\n+         \"need MD5\/SHA1\/SHA256\/SHA512\/SHA3 instruction support\");\n+  assert((uint)predicate < 5, \"sanity\");\n@@ -6790,0 +6818,6 @@\n+  case 4:\n+    if (UseSHA3Intrinsics) {\n+      \/\/ we want to do an instanceof comparison against the SHA3 class\n+      klass_name = \"sun\/security\/provider\/SHA3\";\n+    }\n+    break;\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":81,"deletions":47,"binary":false,"changes":128,"status":"modified"},{"patch":"@@ -315,1 +315,1 @@\n-                                        bool long_state, address stubAddr, const char *stubName,\n+                                        const char* state_type, address stubAddr, const char *stubName,\n@@ -317,2 +317,2 @@\n-  Node* get_state_from_digest_object(Node *digestBase_object);\n-  Node* get_long_state_from_digest_object(Node *digestBase_object);\n+  Node* get_state_from_digest_object(Node *digestBase_object, const char* state_type);\n+  Node* get_digest_length_from_digest_object(Node *digestBase_object);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"opto\/opaquenode.hpp\"\n@@ -44,0 +45,1 @@\n+#include \"opto\/runtime.hpp\"\n@@ -45,0 +47,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -504,0 +507,723 @@\n+static int check_stride_overflow(jlong stride_con, const TypeLong* limit_t) {\n+  if (stride_con > 0) {\n+    if (limit_t->_lo > (max_jlong - stride_con)) {\n+      return -1;\n+    }\n+    if (limit_t->_hi > (max_jlong - stride_con)) {\n+      return 1;\n+    }\n+  } else {\n+    if (limit_t->_hi < (min_jlong - stride_con)) {\n+      return -1;\n+    }\n+    if (limit_t->_lo < (min_jlong - stride_con)) {\n+      return 1;\n+    }\n+  }\n+  return 0;\n+}\n+\n+static bool condition_stride_ok(BoolTest::mask bt, jlong stride_con) {\n+  \/\/ If the condition is inverted and we will be rolling\n+  \/\/ through MININT to MAXINT, then bail out.\n+  if (bt == BoolTest::eq || \/\/ Bail out, but this loop trips at most twice!\n+      \/\/ Odd stride\n+      (bt == BoolTest::ne && stride_con != 1 && stride_con != -1) ||\n+      \/\/ Count down loop rolls through MAXINT\n+      ((bt == BoolTest::le || bt == BoolTest::lt) && stride_con < 0) ||\n+      \/\/ Count up loop rolls through MININT\n+      ((bt == BoolTest::ge || bt == BoolTest::gt) && stride_con > 0)) {\n+    return false; \/\/ Bail out\n+  }\n+  return true;\n+}\n+\n+void PhaseIdealLoop::long_loop_replace_long_iv(Node* iv_to_replace, Node* inner_iv, Node* outer_phi, Node* inner_head) {\n+  Node* iv_as_long = new ConvI2LNode(inner_iv, TypeLong::INT);\n+  register_new_node(iv_as_long, inner_head);\n+  Node* iv_replacement = new AddLNode(outer_phi, iv_as_long);\n+  register_new_node(iv_replacement, inner_head);\n+  for (DUIterator_Last imin, i = iv_to_replace->last_outs(imin); i >= imin;) {\n+    Node* u = iv_to_replace->last_out(i);\n+#ifdef ASSERT\n+    if (!is_dominator(inner_head, ctrl_or_self(u))) {\n+      assert(u->is_Phi(), \"should be a Phi\");\n+      for (uint j = 1; j < u->req(); j++) {\n+        if (u->in(j) == iv_to_replace) {\n+          assert(is_dominator(inner_head, u->in(0)->in(j)), \"iv use above loop?\");\n+        }\n+      }\n+    }\n+#endif\n+    _igvn.rehash_node_delayed(u);\n+    int nb = u->replace_edge(iv_to_replace, iv_replacement);\n+    i -= nb;\n+  }\n+}\n+\n+void PhaseIdealLoop::add_empty_predicate(Deoptimization::DeoptReason reason, Node* inner_head, IdealLoopTree* loop, SafePointNode* sfpt) {\n+  if (!C->too_many_traps(reason)) {\n+    Node *cont = _igvn.intcon(1);\n+    Node* opq = new Opaque1Node(C, cont);\n+    _igvn.register_new_node_with_optimizer(opq);\n+    Node *bol = new Conv2BNode(opq);\n+    _igvn.register_new_node_with_optimizer(bol);\n+    set_subtree_ctrl(bol);\n+    IfNode* iff = new IfNode(inner_head->in(LoopNode::EntryControl), bol, PROB_MAX, COUNT_UNKNOWN);\n+    register_control(iff, loop, inner_head->in(LoopNode::EntryControl));\n+    Node* iffalse = new IfFalseNode(iff);\n+    register_control(iffalse, _ltree_root, iff);\n+    Node* iftrue = new IfTrueNode(iff);\n+    register_control(iftrue, loop, iff);\n+    C->add_predicate_opaq(opq);\n+\n+    int trap_request = Deoptimization::make_trap_request(reason, Deoptimization::Action_maybe_recompile);\n+    address call_addr = SharedRuntime::uncommon_trap_blob()->entry_point();\n+    const TypePtr* no_memory_effects = NULL;\n+    JVMState* jvms = sfpt->jvms();\n+    CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\",\n+                                           jvms->bci(), no_memory_effects);\n+\n+    Node* mem = NULL;\n+    Node* i_o = NULL;\n+    if (sfpt->is_Call()) {\n+      mem = sfpt->proj_out(TypeFunc::Memory);\n+      i_o = sfpt->proj_out(TypeFunc::I_O);\n+    } else {\n+      mem = sfpt->memory();\n+      i_o = sfpt->i_o();\n+    }\n+\n+    Node *frame = new ParmNode(C->start(), TypeFunc::FramePtr);\n+    register_new_node(frame, C->start());\n+    Node *ret = new ParmNode(C->start(), TypeFunc::ReturnAdr);\n+    register_new_node(ret, C->start());\n+\n+    unc->init_req(TypeFunc::Control, iffalse);\n+    unc->init_req(TypeFunc::I_O, i_o);\n+    unc->init_req(TypeFunc::Memory, mem); \/\/ may gc ptrs\n+    unc->init_req(TypeFunc::FramePtr, frame);\n+    unc->init_req(TypeFunc::ReturnAdr, ret);\n+    unc->init_req(TypeFunc::Parms+0, _igvn.intcon(trap_request));\n+    unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+    unc->copy_call_debug_info(&_igvn, sfpt);\n+\n+    for (uint i = TypeFunc::Parms; i < unc->req(); i++) {\n+      set_subtree_ctrl(unc->in(i));\n+    }\n+    register_control(unc, _ltree_root, iffalse);\n+\n+    Node* ctrl = new ProjNode(unc, TypeFunc::Control);\n+    register_control(ctrl, _ltree_root, unc);\n+    Node* halt = new HaltNode(ctrl, frame, \"uncommon trap returned which should never happen\" PRODUCT_ONLY(COMMA \/*reachable*\/false));\n+    register_control(halt, _ltree_root, ctrl);\n+    C->root()->add_req(halt);\n+\n+    _igvn.replace_input_of(inner_head, LoopNode::EntryControl, iftrue);\n+    set_idom(inner_head, iftrue, dom_depth(inner_head));\n+  }\n+}\n+\n+\/\/ Find a safepoint node that dominates the back edge. We need a\n+\/\/ SafePointNode so we can use its jvm state to create empty\n+\/\/ predicates.\n+SafePointNode* PhaseIdealLoop::find_safepoint(Node* back_control, Node* x, IdealLoopTree* loop) {\n+  IfNode* exit_test = back_control->in(0)->as_If();\n+  SafePointNode* safepoint = NULL;\n+  if (exit_test->in(0)->is_SafePoint() && exit_test->in(0)->outcnt() == 1) {\n+    safepoint = exit_test->in(0)->as_SafePoint();\n+  } else {\n+    Node* c = back_control;\n+    while (c != x && c->Opcode() != Op_SafePoint) {\n+      c = idom(c);\n+    }\n+\n+    if (c->Opcode() == Op_SafePoint) {\n+      safepoint = c->as_SafePoint();\n+    }\n+\n+    if (safepoint == NULL) {\n+      return NULL;\n+    }\n+\n+    Node* mem = safepoint->in(TypeFunc::Memory);\n+\n+    \/\/ We can only use that safepoint if there's not side effect\n+    \/\/ between the backedge and the safepoint.\n+\n+#ifdef ASSERT\n+    \/\/ mm is used for book keeping\n+    MergeMemNode* mm = NULL;\n+    if (mem->is_MergeMem()) {\n+      mm = mem->clone()->as_MergeMem();\n+      for (MergeMemStream mms(mem->as_MergeMem()); mms.next_non_empty(); ) {\n+        if (mms.alias_idx() != Compile::AliasIdxBot && loop != get_loop(ctrl_or_self(mms.memory()))) {\n+          mm->set_memory_at(mms.alias_idx(), mem->as_MergeMem()->base_memory());\n+        }\n+      }\n+    }\n+#endif\n+    for (DUIterator_Fast imax, i = x->fast_outs(imax); i < imax; i++) {\n+      Node* u = x->fast_out(i);\n+      if (u->is_Phi() && u->bottom_type() == Type::MEMORY) {\n+        Node* m = u->in(LoopNode::LoopBackControl);\n+        if (u->adr_type() == TypePtr::BOTTOM) {\n+          if (m->is_MergeMem() && mem->is_MergeMem()) {\n+            if (m != mem DEBUG_ONLY(|| true)) {\n+              for (MergeMemStream mms(m->as_MergeMem(), mem->as_MergeMem()); mms.next_non_empty2(); ) {\n+                if (!mms.is_empty()) {\n+                  if (mms.memory() != mms.memory2()) {\n+                    return NULL;\n+                  }\n+#ifdef ASSERT\n+                  if (mms.alias_idx() != Compile::AliasIdxBot) {\n+                    mm->set_memory_at(mms.alias_idx(), mem->as_MergeMem()->base_memory());\n+                  }\n+#endif\n+                }\n+              }\n+            }\n+          } else if (mem->is_MergeMem()) {\n+            if (m != mem->as_MergeMem()->base_memory()) {\n+              return NULL;\n+            }\n+          } else {\n+            return NULL;\n+          }\n+        } else {\n+          if (mem->is_MergeMem()) {\n+            if (m != mem->as_MergeMem()->memory_at(C->get_alias_index(u->adr_type()))) {\n+              return NULL;\n+            }\n+#ifdef ASSERT\n+            mm->set_memory_at(C->get_alias_index(u->adr_type()), mem->as_MergeMem()->base_memory());\n+#endif\n+          } else {\n+            if (m != mem) {\n+              return NULL;\n+            }\n+          }\n+        }\n+      }\n+    }\n+#ifdef ASSERT\n+    if (mm != NULL) {\n+      assert (_igvn.transform(mm) == mem->as_MergeMem()->base_memory(), \"all memory state should have been processed\");\n+      _igvn.remove_dead_node(mm);\n+    }\n+#endif\n+  }\n+  return safepoint;\n+}\n+\n+\/\/ If the loop has the shape of a counted loop but with a long\n+\/\/ induction variable, transform the loop in a loop nest: an inner\n+\/\/ loop that iterates for at most max int iterations with an integer\n+\/\/ induction variable and an outer loop that iterates over the full\n+\/\/ range of long values from the initial loop in (at most) max int\n+\/\/ steps. That is:\n+\/\/\n+\/\/ x: for (long phi = init; phi < limit; phi += stride) {\n+\/\/   \/\/ phi := Phi(L, init, incr)\n+\/\/   \/\/ incr := AddL(phi, longcon(stride))\n+\/\/   \/\/ phi_incr := phi (test happens before increment)\n+\/\/   long incr = phi + stride;\n+\/\/   ... use phi and incr ...\n+\/\/ }\n+\/\/\n+\/\/ OR:\n+\/\/\n+\/\/ x: for (long phi = init; (phi += stride) < limit; ) {\n+\/\/   \/\/ phi := Phi(L, AddL(init, stride), incr)\n+\/\/   \/\/ incr := AddL(phi, longcon(stride))\n+\/\/   \/\/ phi_incr := NULL (test happens after increment)\n+\/\/   long incr = phi + stride;\n+\/\/   ... use phi and (phi + stride) ...\n+\/\/ }\n+\/\/\n+\/\/ ==transform=>\n+\/\/\n+\/\/ const ulong inner_iters_limit = INT_MAX - stride - 1;  \/\/near 0x7FFFFFF0\n+\/\/ assert(stride <= inner_iters_limit);  \/\/ else abort transform\n+\/\/ assert((extralong)limit + stride <= LONG_MAX);  \/\/ else deopt\n+\/\/ outer_head: for (long outer_phi = init;;) {\n+\/\/   \/\/ outer_phi := Phi(outer_head, init, AddL(outer_phi, I2L(inner_phi)))\n+\/\/   ulong inner_iters_max = (ulong) MAX(0, ((extralong)limit + stride - outer_phi));\n+\/\/   long inner_iters_actual = MIN(inner_iters_limit, inner_iters_max);\n+\/\/   assert(inner_iters_actual == (int)inner_iters_actual);\n+\/\/   int inner_phi, inner_incr;\n+\/\/   x: for (inner_phi = 0;; inner_phi = inner_incr) {\n+\/\/     \/\/ inner_phi := Phi(x, intcon(0), inner_incr)\n+\/\/     \/\/ inner_incr := AddI(inner_phi, intcon(stride))\n+\/\/     inner_incr = inner_phi + stride;\n+\/\/     if (inner_incr < inner_iters_actual) {\n+\/\/       ... use phi=>(outer_phi+inner_phi) and incr=>(outer_phi+inner_incr) ...\n+\/\/       continue;\n+\/\/     }\n+\/\/     else break;\n+\/\/   }\n+\/\/   if ((outer_phi+inner_phi) < limit)  \/\/OR (outer_phi+inner_incr) < limit\n+\/\/     continue;\n+\/\/   else break;\n+\/\/ }\n+bool PhaseIdealLoop::is_long_counted_loop(Node* x, IdealLoopTree* loop, Node_List &old_new) {\n+  \/\/ Only for inner loops\n+  if (loop->_child != NULL) {\n+    return false;\n+  }\n+\n+  \/\/ Checks whether the loop has the shape of a counted loop\n+  Node* back_control = loop_exit_control(x, loop);\n+  if (back_control == NULL) {\n+    return false;\n+  }\n+\n+  BoolTest::mask bt = BoolTest::illegal;\n+  float cl_prob = 0;\n+  Node* incr = NULL;\n+  Node* limit = NULL;\n+\n+  Node* cmp = loop_exit_test(back_control, loop, incr, limit, bt, cl_prob);\n+  if (cmp == NULL || cmp->Opcode() != Op_CmpL) {\n+    return false; \/\/ Avoid pointer & float & 32-bit compares\n+  }\n+\n+  Node* phi_incr = NULL;\n+  incr = loop_iv_incr(incr, x, loop, phi_incr);\n+  if (incr == NULL || incr->Opcode() != Op_AddL) {\n+    return false;\n+  }\n+\n+  Node* xphi = NULL;\n+  Node* stride = loop_iv_stride(incr, loop, xphi);\n+\n+  if (stride == NULL) {\n+    return false;\n+  }\n+\n+#ifndef PRODUCT\n+  Atomic::inc(&_long_loop_candidates);\n+#endif\n+\n+  jlong stride_con = stride->get_long();\n+  assert(stride_con != 0, \"missed some peephole opt\");\n+  \/\/ We can't iterate for more than max int at a time.\n+  if (stride_con != (jint)stride_con) {\n+    return false;\n+  }\n+  \/\/ The number of iterations for the integer count loop: guarantee no\n+  \/\/ overflow: max_jint - stride_con max. -1 so there's no need for a\n+  \/\/ loop limit check if the exit test is <= or >=.\n+  int iters_limit = max_jint - ABS(stride_con) - 1;\n+#ifdef ASSERT\n+  if (StressLongCountedLoop > 0) {\n+    iters_limit = iters_limit \/ StressLongCountedLoop;\n+  }\n+#endif\n+  \/\/ At least 2 iterations so counted loop construction doesn't fail\n+  if (iters_limit\/ABS(stride_con) < 2) {\n+    return false;\n+  }\n+\n+  PhiNode* phi = loop_iv_phi(xphi, phi_incr, x, loop);\n+\n+  if (phi == NULL || phi->in(LoopNode::LoopBackControl) != incr) {\n+    return false;\n+  }\n+\n+  \/\/ Safepoint on backedge not supported\n+  if (x->in(LoopNode::LoopBackControl)->Opcode() == Op_SafePoint) {\n+    return false;\n+  }\n+\n+  \/\/ data nodes on back branch not supported\n+  if (back_control->outcnt() > 1) {\n+    return false;\n+  }\n+\n+  if (!condition_stride_ok(bt, stride_con)) {\n+    return false;\n+  }\n+\n+  \/\/ We'll need to use the loop limit before the inner loop is entered\n+  if (!is_dominator(get_ctrl(limit), x)) {\n+    return false;\n+  }\n+\n+  IfNode* exit_test = back_control->in(0)->as_If();\n+\n+  \/\/ We need a safepoint to insert empty predicates for the inner loop.\n+  SafePointNode* safepoint = find_safepoint(back_control, x, loop);\n+  if (safepoint == NULL) {\n+    \/\/ If exit condition is ne, then a loop limit check is likely needed\n+    if (bt == BoolTest::ne) {\n+      return false;\n+    }\n+  } else if (C->too_many_traps(safepoint->jvms()->method(),\n+                        safepoint->jvms()->bci(),\n+                        Deoptimization::Reason_loop_limit_check)) {\n+    \/\/ We must have transformed the loop already and a loop limit\n+    \/\/ check must have failed.\n+    return false;\n+  }\n+\n+  Node* exit_branch = exit_test->proj_out(back_control->Opcode() == Op_IfFalse);\n+  Node* entry_control = x->in(LoopNode::EntryControl);\n+\n+  \/\/ if the loop exit test is on the IV before it is incremented: i <\n+  \/\/ limit, we transform the exit test so it is performed on the exit\n+  \/\/ test after it is incremented: i + stride < limit + stride.  We\n+  \/\/ need limit + stride to not overflow. See adjusted_limit below.\n+  bool limit_check_required = false;\n+  if (phi_incr != NULL) {\n+    const TypeLong* limit_t = _igvn.type(limit)->is_long();\n+    int sov = check_stride_overflow(stride_con, limit_t);\n+    if (sov != 0) {\n+      if (sov < 0) {\n+        return false;  \/\/ Bailout: integer overflow is certain.\n+      }\n+      \/\/ Check that inserting a predicate is indeed possible\n+      if (find_predicate_insertion_point(x->in(LoopNode::EntryControl), Deoptimization::Reason_loop_limit_check) == NULL) {\n+        return false;\n+      }\n+      limit_check_required = true;\n+    }\n+  }\n+\n+  \/\/ Clone the control flow of the loop to build an outer loop\n+  Node* outer_back_branch = back_control->clone();\n+  Node* outer_exit_test = exit_test->clone();\n+  Node* inner_exit_branch = exit_branch->clone();\n+\n+  Node* outer_head = new LoopNode(entry_control, outer_back_branch);\n+  IdealLoopTree* outer_ilt = insert_outer_loop(loop, outer_head->as_Loop(), outer_back_branch);\n+\n+  const bool body_populated = true;\n+  register_control(outer_head, outer_ilt, entry_control, body_populated);\n+\n+  _igvn.register_new_node_with_optimizer(inner_exit_branch);\n+  set_loop(inner_exit_branch, outer_ilt);\n+  set_idom(inner_exit_branch, exit_test, dom_depth(exit_branch));\n+\n+  outer_exit_test->set_req(0, inner_exit_branch);\n+  register_control(outer_exit_test, outer_ilt, inner_exit_branch, body_populated);\n+\n+  _igvn.replace_input_of(exit_branch, 0, outer_exit_test);\n+  set_idom(exit_branch, outer_exit_test, dom_depth(exit_branch));\n+\n+  outer_back_branch->set_req(0, outer_exit_test);\n+  register_control(outer_back_branch, outer_ilt, outer_exit_test, body_populated);\n+\n+  _igvn.replace_input_of(x, LoopNode::EntryControl, outer_head);\n+  set_idom(x, outer_head, dom_depth(x));\n+\n+  \/\/ add an iv phi to the outer loop and use it to compute the inner\n+  \/\/ loop iteration limit\n+  Node* outer_phi = phi->clone();\n+  outer_phi->set_req(0, outer_head);\n+  register_new_node(outer_phi, outer_head);\n+\n+  Node* adjusted_limit = limit;\n+  if (phi_incr != NULL) {\n+    \/\/ If compare points directly to the phi we need to adjust the\n+    \/\/ compare so that it points to the incr.\n+    Node* long_stride = _igvn.longcon(stride_con);\n+    set_ctrl(long_stride, C->root());\n+    adjusted_limit = new AddLNode(limit, long_stride);\n+    _igvn.register_new_node_with_optimizer(adjusted_limit);\n+  }\n+  Node* inner_iters_max = NULL;\n+  if (stride_con > 0) {\n+    inner_iters_max = MaxNode::max_diff_with_zero(adjusted_limit, outer_phi, TypeLong::LONG, _igvn);\n+  } else {\n+    inner_iters_max = MaxNode::max_diff_with_zero(outer_phi, adjusted_limit, TypeLong::LONG, _igvn);\n+  }\n+\n+  Node* inner_iters_limit = _igvn.longcon(iters_limit);\n+  \/\/ inner_iters_max may not fit in a signed integer (iterating from\n+  \/\/ Long.MIN_VALUE to Long.MAX_VALUE for instance). Use an unsigned\n+  \/\/ min.\n+  Node* inner_iters_actual = MaxNode::unsigned_min(inner_iters_max, inner_iters_limit, TypeLong::make(0, iters_limit, Type::WidenMin), _igvn);\n+\n+  Node* inner_iters_actual_int = new ConvL2INode(inner_iters_actual);\n+  _igvn.register_new_node_with_optimizer(inner_iters_actual_int);\n+\n+  Node* zero = _igvn.intcon(0);\n+  set_ctrl(zero, C->root());\n+  if (stride_con < 0) {\n+    inner_iters_actual_int = new SubINode(zero, inner_iters_actual_int);\n+    _igvn.register_new_node_with_optimizer(inner_iters_actual_int);\n+  }\n+\n+  \/\/ Clone the iv data nodes as an integer iv\n+  Node* int_stride = _igvn.intcon((int)stride_con);\n+  set_ctrl(int_stride, C->root());\n+  Node* inner_phi = new PhiNode(x->in(0), TypeInt::INT);\n+  Node* inner_incr = new AddINode(inner_phi, int_stride);\n+  Node* inner_cmp = NULL;\n+  if (cmp->in(1) == incr || cmp->in(1) == phi) {\n+    inner_cmp = new CmpINode(inner_incr, inner_iters_actual_int);\n+  }  else {\n+    assert(cmp->in(2) == incr || cmp->in(2) == phi, \"bad iv shape\");\n+    inner_cmp = new CmpINode(inner_iters_actual_int, inner_incr);\n+  }\n+  Node* inner_bol = new BoolNode(inner_cmp, exit_test->in(1)->as_Bool()->_test._test);\n+  inner_phi->set_req(LoopNode::EntryControl, zero);\n+  inner_phi->set_req(LoopNode::LoopBackControl, inner_incr);\n+  register_new_node(inner_phi, x);\n+  register_new_node(inner_incr, x);\n+  register_new_node(inner_cmp, x);\n+  register_new_node(inner_bol, x);\n+\n+  _igvn.replace_input_of(exit_test, 1, inner_bol);\n+\n+  \/\/ Add a predicate to guarantee limit adjustment doesn't overflow\n+  if (limit_check_required) {\n+    assert(phi_incr != NULL, \"only when exit test must be transformed\");\n+    ProjNode *limit_check_proj = find_predicate_insertion_point(outer_head->in(LoopNode::EntryControl), Deoptimization::Reason_loop_limit_check);\n+    assert(limit_check_proj != NULL, \"was tested before\");\n+    IfNode* check_iff = limit_check_proj->in(0)->as_If();\n+    Node* cmp_limit;\n+    Node* bol;\n+\n+    if (stride_con > 0) {\n+      cmp_limit = new CmpLNode(limit, _igvn.longcon(max_jlong - stride_con));\n+      bol = new BoolNode(cmp_limit, BoolTest::le);\n+    } else {\n+      cmp_limit = new CmpLNode(limit, _igvn.longcon(min_jlong - stride_con));\n+      bol = new BoolNode(cmp_limit, BoolTest::ge);\n+    }\n+\n+    insert_loop_limit_check(limit_check_proj, cmp_limit, bol);\n+    Node* new_predicate = limit_check_proj->in(0)->in(0);\n+    Node* above_predicate = new_predicate->in(0)->in(0);\n+    Node* entry = outer_head->in(LoopNode::EntryControl);\n+    _igvn.replace_input_of(limit_check_proj->in(0), 0, above_predicate);\n+    _igvn.replace_input_of(new_predicate->in(0), 0, entry);\n+    _igvn.replace_input_of(outer_head, LoopNode::EntryControl, new_predicate);\n+    set_idom(new_predicate->in(0), entry, dom_depth(entry));\n+    set_idom(new_predicate, new_predicate->in(0), dom_depth(entry));\n+    Node* region = new_predicate->in(0)->as_If()->proj_out(new_predicate->Opcode() == Op_IfFalse)->unique_ctrl_out();\n+    assert(region->is_Region(), \"should be region merging predicates\");\n+    set_idom(region, entry, dom_depth(entry));\n+    set_idom(limit_check_proj->in(0), above_predicate, dom_depth(above_predicate));\n+  }\n+\n+  LoopNode* inner_head = x->as_Loop();\n+\n+  \/\/ Clone inner loop phis to outer loop\n+  for (uint i = 0; i < inner_head->outcnt(); i++) {\n+    Node* u = inner_head->raw_out(i);\n+    if (u->is_Phi() && u != inner_phi && u != phi) {\n+      assert(u->in(0) == inner_head, \"inconsistent\");\n+      Node* clone = u->clone();\n+      clone->set_req(0, outer_head);\n+      register_new_node(clone, outer_head);\n+      _igvn.replace_input_of(u, LoopNode::EntryControl, clone);\n+    }\n+  }\n+\n+  \/\/ Replace inner loop long iv phi as inner loop int iv phi + outer\n+  \/\/ loop iv phi\n+  long_loop_replace_long_iv(phi, inner_phi, outer_phi, inner_head);\n+\n+  \/\/ Replace inner loop long iv incr with inner loop int incr + outer\n+  \/\/ loop iv phi\n+  long_loop_replace_long_iv(incr, inner_incr, outer_phi, inner_head);\n+\n+  set_subtree_ctrl(inner_iters_actual_int);\n+\n+  \/\/ Summary of steps from inital loop to loop nest:\n+  \/\/\n+  \/\/ == old IR nodes =>\n+  \/\/\n+  \/\/ entry_control: {...}\n+  \/\/ x:\n+  \/\/ for (long phi = init;;) {\n+  \/\/   \/\/ phi := Phi(x, init, incr)\n+  \/\/   \/\/ incr := AddL(phi, longcon(stride))\n+  \/\/   exit_test:\n+  \/\/   if (phi < limit)\n+  \/\/     back_control: fallthrough;\n+  \/\/   else\n+  \/\/     exit_branch: break;\n+  \/\/   \/\/ test happens before increment => phi == phi_incr != NULL\n+  \/\/   long incr = phi + stride;\n+  \/\/   ... use phi and incr ...\n+  \/\/   phi = incr;\n+  \/\/ }\n+  \/\/\n+  \/\/ == new IR nodes (just before final peel) =>\n+  \/\/\n+  \/\/ entry_control: {...}\n+  \/\/ long adjusted_limit = limit + stride;  \/\/because phi_incr != NULL\n+  \/\/ assert(!limit_check_required || (extralong)limit + stride == adjusted_limit);  \/\/ else deopt\n+  \/\/ ulong inner_iters_limit = max_jint - ABS(stride) - 1;  \/\/near 0x7FFFFFF0\n+  \/\/ outer_head:\n+  \/\/ for (long outer_phi = init;;) {\n+  \/\/   \/\/ outer_phi := phi->clone(), in(0):=outer_head, => Phi(outer_head, init, incr)\n+  \/\/   \/\/ REPLACE phi  => AddL(outer_phi, I2L(inner_phi))\n+  \/\/   \/\/ REPLACE incr => AddL(outer_phi, I2L(inner_incr))\n+  \/\/   \/\/ SO THAT outer_phi := Phi(outer_head, init, AddL(outer_phi, I2L(inner_incr)))\n+  \/\/   ulong inner_iters_max = (ulong) MAX(0, ((extralong)adjusted_limit - outer_phi) * SGN(stride));\n+  \/\/   int inner_iters_actual_int = (int) MIN(inner_iters_limit, inner_iters_max) * SGN(stride);\n+  \/\/   inner_head: x: \/\/in(1) := outer_head\n+  \/\/   int inner_phi;\n+  \/\/   for (inner_phi = 0;;) {\n+  \/\/     \/\/ inner_phi := Phi(x, intcon(0), inner_phi + stride)\n+  \/\/     int inner_incr = inner_phi + stride;\n+  \/\/     bool inner_bol = (inner_incr < inner_iters_actual_int);\n+  \/\/     exit_test: \/\/exit_test->in(1) := inner_bol;\n+  \/\/     if (inner_bol) \/\/ WAS (phi < limit)\n+  \/\/       back_control: fallthrough;\n+  \/\/     else\n+  \/\/       inner_exit_branch: break;  \/\/exit_branch->clone()\n+  \/\/     ... use phi=>(outer_phi+inner_phi) and incr=>(outer_phi+inner_incr) ...\n+  \/\/     inner_phi = inner_phi + stride;  \/\/ inner_incr\n+  \/\/   }\n+  \/\/   outer_exit_test:  \/\/exit_test->clone(), in(0):=inner_exit_branch\n+  \/\/   if ((outer_phi+inner_phi) < limit)  \/\/ WAS (phi < limit)\n+  \/\/     outer_back_branch: fallthrough;  \/\/back_control->clone(), in(0):=outer_exit_test\n+  \/\/   else\n+  \/\/     exit_branch: break;  \/\/in(0) := outer_exit_test\n+  \/\/ }\n+\n+  \/\/ Peel one iteration of the loop and use the safepoint at the end\n+  \/\/ of the peeled iteration to insert empty predicates. If no well\n+  \/\/ positioned safepoint peel to guarantee a safepoint in the outer\n+  \/\/ loop.\n+  if (safepoint != NULL || !loop->_has_call) {\n+    old_new.clear();\n+    do_peeling(loop, old_new);\n+  }\n+\n+  if (safepoint != NULL) {\n+    SafePointNode* cloned_sfpt = old_new[safepoint->_idx]->as_SafePoint();\n+\n+    if (UseLoopPredicate) {\n+      add_empty_predicate(Deoptimization::Reason_predicate, inner_head, outer_ilt, cloned_sfpt);\n+    }\n+    if (UseProfiledLoopPredicate) {\n+      add_empty_predicate(Deoptimization::Reason_profile_predicate, inner_head, outer_ilt, cloned_sfpt);\n+    }\n+    add_empty_predicate(Deoptimization::Reason_loop_limit_check, inner_head, outer_ilt, cloned_sfpt);\n+  }\n+\n+#ifndef PRODUCT\n+  Atomic::inc(&_long_loop_nests);\n+#endif\n+\n+  inner_head->mark_transformed_long_loop();\n+\n+  return true;\n+}\n+\n+#ifdef ASSERT\n+\/\/ convert an int counted loop to a long counted to stress handling of\n+\/\/ long counted loops\n+bool PhaseIdealLoop::convert_to_long_loop(Node* cmp, Node* phi, IdealLoopTree* loop) {\n+  Unique_Node_List iv_nodes;\n+  Node_List old_new;\n+  iv_nodes.push(cmp);\n+  bool failed = false;\n+\n+  for (uint i = 0; i < iv_nodes.size() && !failed; i++) {\n+    Node* n = iv_nodes.at(i);\n+    switch(n->Opcode()) {\n+      case Op_Phi: {\n+        Node* clone = new PhiNode(n->in(0), TypeLong::LONG);\n+        old_new.map(n->_idx, clone);\n+        break;\n+      }\n+      case Op_CmpI: {\n+        Node* clone = new CmpLNode(NULL, NULL);\n+        old_new.map(n->_idx, clone);\n+        break;\n+      }\n+      case Op_AddI: {\n+        Node* clone = new AddLNode(NULL, NULL);\n+        old_new.map(n->_idx, clone);\n+        break;\n+      }\n+      case Op_CastII: {\n+        failed = true;\n+        break;\n+      }\n+      default:\n+        DEBUG_ONLY(n->dump());\n+        fatal(\"unexpected\");\n+    }\n+\n+    for (uint i = 1; i < n->req(); i++) {\n+      Node* in = n->in(i);\n+      if (in == NULL) {\n+        continue;\n+      }\n+      if (loop->is_member(get_loop(get_ctrl(in)))) {\n+        iv_nodes.push(in);\n+      }\n+    }\n+  }\n+\n+  if (failed) {\n+    for (uint i = 0; i < iv_nodes.size(); i++) {\n+      Node* n = iv_nodes.at(i);\n+      Node* clone = old_new[n->_idx];\n+      if (clone != NULL) {\n+        _igvn.remove_dead_node(clone);\n+      }\n+    }\n+    return false;\n+  }\n+\n+  for (uint i = 0; i < iv_nodes.size(); i++) {\n+    Node* n = iv_nodes.at(i);\n+    Node* clone = old_new[n->_idx];\n+    for (uint i = 1; i < n->req(); i++) {\n+      Node* in = n->in(i);\n+      if (in == NULL) {\n+        continue;\n+      }\n+      Node* in_clone = old_new[in->_idx];\n+      if (in_clone == NULL) {\n+        assert(_igvn.type(in)->isa_int(), \"\");\n+        in_clone = new ConvI2LNode(in);\n+        _igvn.register_new_node_with_optimizer(in_clone);\n+        set_subtree_ctrl(in_clone);\n+      }\n+      if (in_clone->in(0) == NULL) {\n+        in_clone->set_req(0, C->top());\n+        clone->set_req(i, in_clone);\n+        in_clone->set_req(0, NULL);\n+      } else {\n+        clone->set_req(i, in_clone);\n+      }\n+    }\n+    _igvn.register_new_node_with_optimizer(clone);\n+  }\n+  set_ctrl(old_new[phi->_idx], phi->in(0));\n+\n+  for (uint i = 0; i < iv_nodes.size(); i++) {\n+    Node* n = iv_nodes.at(i);\n+    Node* clone = old_new[n->_idx];\n+    set_subtree_ctrl(clone);\n+    Node* m = n->Opcode() == Op_CmpI ? clone : NULL;\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* u = n->fast_out(i);\n+      if (iv_nodes.member(u)) {\n+        continue;\n+      }\n+      if (m == NULL) {\n+        m = new ConvL2INode(clone);\n+        _igvn.register_new_node_with_optimizer(m);\n+        set_subtree_ctrl(m);\n+      }\n+      _igvn.rehash_node_delayed(u);\n+      int nb = u->replace_edge(n, m);\n+      --i, imax -= nb;\n+    }\n+  }\n+  return true;\n+}\n+#endif\n+\n@@ -517,1 +1243,0 @@\n-\n@@ -643,10 +1368,2 @@\n-  \/\/ If the condition is inverted and we will be rolling\n-  \/\/ through MININT to MAXINT, then bail out.\n-  if (bt == BoolTest::eq || \/\/ Bail out, but this loop trips at most twice!\n-      \/\/ Odd stride\n-      (bt == BoolTest::ne && stride_con != 1 && stride_con != -1) ||\n-      \/\/ Count down loop rolls through MAXINT\n-      ((bt == BoolTest::le || bt == BoolTest::lt) && stride_con < 0) ||\n-      \/\/ Count up loop rolls through MININT\n-      ((bt == BoolTest::ge || bt == BoolTest::gt) && stride_con > 0)) {\n-    return false; \/\/ Bail out\n+  if (!condition_stride_ok(bt, stride_con)) {\n+    return false;\n@@ -725,0 +1442,1 @@\n+    assert(!x->as_Loop()->is_transformed_long_loop(), \"long loop was transformed\");\n@@ -812,0 +1530,6 @@\n+#ifdef ASSERT\n+  if (!x->as_Loop()->is_transformed_long_loop() && StressLongCountedLoop > 0 && trunc1 == NULL && convert_to_long_loop(cmp, phi, loop)) {\n+    return false;\n+  }\n+#endif\n+\n@@ -998,0 +1722,6 @@\n+#ifndef PRODUCT\n+  if (x->as_Loop()->is_transformed_long_loop()) {\n+    Atomic::inc(&_long_loop_counted_loops);\n+  }\n+#endif\n+\n@@ -1129,1 +1859,2 @@\n-      assert(outer->outcnt() >= phis + 2 && outer->outcnt() <= phis + 2 + stores + 1, \"only phis\");\n+      \/\/ TODO disabled until JDK-8255120 is fixed\n+      \/\/ assert(outer->outcnt() >= phis + 2 && outer->outcnt() <= phis + 2 + stores + 1, \"only phis\");\n@@ -2577,1 +3308,3 @@\n-  } else if (_parent != NULL && !_irreducible) {\n+  } else {\n+    assert(!_head->is_Loop() || !_head->as_Loop()->is_transformed_long_loop(), \"transformation to counted loop should not fail\");\n+    if (_parent != NULL && !_irreducible) {\n@@ -2582,0 +3315,1 @@\n+  }\n@@ -2756,1 +3490,1 @@\n-static void log_loop_tree(IdealLoopTree* root, IdealLoopTree* loop, CompileLog* log) {\n+static void log_loop_tree_helper(IdealLoopTree* root, IdealLoopTree* loop, CompileLog* log) {\n@@ -2761,1 +3495,1 @@\n-      if( loop->_child ) log_loop_tree(root, loop->_child, log);\n+      log_loop_tree_helper(root, loop->_child, log);\n@@ -2765,1 +3499,1 @@\n-  } else {\n+  } else if (loop != NULL) {\n@@ -2771,1 +3505,1 @@\n-      if (head->as_Loop()->is_inner_loop()) log->print(\"inner_loop='1' \");\n+      if (head->as_Loop()->is_inner_loop())        log->print(\"inner_loop='1' \");\n@@ -2773,2 +3507,1 @@\n-    }\n-    if (head->is_CountedLoop()) {\n+    } else if (head->is_CountedLoop()) {\n@@ -2778,1 +3511,1 @@\n-      if (cl->is_post_loop()) log->print(\"post_loop='%d' \",  cl->main_idx());\n+      if (cl->is_post_loop()) log->print(\"post_loop='%d' \", cl->main_idx());\n@@ -2781,1 +3514,1 @@\n-    if( loop->_child ) log_loop_tree(root, loop->_child, log);\n+    log_loop_tree_helper(root, loop->_child, log);\n@@ -2783,1 +3516,7 @@\n-    if( loop->_next  ) log_loop_tree(root, loop->_next, log);\n+    log_loop_tree_helper(root, loop->_next, log);\n+  }\n+}\n+\n+void PhaseIdealLoop::log_loop_tree() {\n+  if (C->log() != NULL) {\n+    log_loop_tree_helper(_ltree_root, _ltree_root, C->log());\n@@ -2970,0 +3709,2 @@\n+  assert(!C->post_loop_opts_phase(), \"no loop opts allowed\");\n+\n@@ -3046,1 +3787,0 @@\n-    _igvn.optimize();           \/\/ Cleanup NeverBranches\n@@ -3156,1 +3896,0 @@\n-    _igvn.optimize();\n@@ -3179,8 +3918,0 @@\n-    \/\/ restore major progress flag\n-\n-    \/\/ Cleanup any modified bits\n-    _igvn.optimize();\n-\n-    if (C->log() != NULL) {\n-      log_loop_tree(_ltree_root, _ltree_root, C->log());\n-    }\n@@ -3209,6 +3940,0 @@\n-\n-    _igvn.optimize();\n-\n-    if (C->log() != NULL) {\n-      log_loop_tree(_ltree_root, _ltree_root, C->log());\n-    }\n@@ -3219,4 +3944,0 @@\n-    _igvn.optimize();\n-    if (C->log() != NULL) {\n-      log_loop_tree(_ltree_root, _ltree_root, C->log());\n-    }\n@@ -3226,1 +3947,6 @@\n-  if (ReassociateInvariants) {\n+  for (LoopTreeIterator iter(_ltree_root); !iter.done(); iter.next()) {\n+    IdealLoopTree* lpt = iter.current();\n+    is_long_counted_loop(lpt->_head, lpt, worklist);\n+  }\n+\n+  if (ReassociateInvariants && !C->major_progress()) {\n@@ -3254,1 +3980,1 @@\n-  if( SplitIfBlocks && do_split_ifs ) {\n+  if (!C->major_progress() && SplitIfBlocks && do_split_ifs) {\n@@ -3356,3 +4082,0 @@\n-  \/\/ Cleanup any modified bits\n-  _igvn.optimize();\n-\n@@ -3362,4 +4085,0 @@\n-\n-  if (C->log() != NULL) {\n-    log_loop_tree(_ltree_root, _ltree_root, C->log());\n-  }\n@@ -3372,0 +4091,3 @@\n+volatile int PhaseIdealLoop::_long_loop_candidates=0; \/\/ Number of long loops seen\n+volatile int PhaseIdealLoop::_long_loop_nests=0; \/\/ Number of long loops successfully transformed to a nest\n+volatile int PhaseIdealLoop::_long_loop_counted_loops=0; \/\/ Number of long loops successfully transformed to a counted loop\n@@ -3373,1 +4095,1 @@\n-  tty->print_cr(\"PhaseIdealLoop=%d, sum _unique=%d\", _loop_invokes, _loop_work);\n+  tty->print_cr(\"PhaseIdealLoop=%d, sum _unique=%d, long loops=%d\/%d\/%d\", _loop_invokes, _loop_work, _long_loop_counted_loops, _long_loop_nests, _long_loop_candidates);\n","filename":"src\/hotspot\/share\/opto\/loopnode.cpp","additions":774,"deletions":52,"binary":false,"changes":826,"status":"modified"},{"patch":"@@ -80,1 +80,2 @@\n-         FlattenedArrays=262144};\n+         TransformedLongLoop=262144,\n+         FlattenedArrays=524288};\n@@ -105,0 +106,1 @@\n+  bool is_transformed_long_loop() const { return _loop_flags & TransformedLongLoop; }\n@@ -120,0 +122,1 @@\n+  void mark_transformed_long_loop() { _loop_flags |= TransformedLongLoop; }\n@@ -620,7 +623,0 @@\n-  \/\/ Return TRUE or FALSE if the loop should be cache-line aligned.\n-  \/\/ Gather the expression that does the alignment.  Note that only\n-  \/\/ one array base can be aligned in a loop (unless the VM guarantees\n-  \/\/ mutual alignment).  Note that if we vectorize short memory ops\n-  \/\/ into longer memory ops, we may want to increase alignment.\n-  bool policy_align( PhaseIdealLoop *phase ) const;\n-\n@@ -815,0 +811,2 @@\n+  void log_loop_tree();\n+\n@@ -1040,0 +1038,8 @@\n+\n+    Compile* C = Compile::current();\n+    if (!C->failing()) {\n+      \/\/ Cleanup any modified bits\n+      igvn.optimize();\n+\n+      v.log_loop_tree();\n+    }\n@@ -1055,0 +1061,7 @@\n+  void long_loop_replace_long_iv(Node* iv_to_replace, Node* inner_iv, Node* outer_phi, Node* inner_head);\n+  bool is_long_counted_loop(Node* x, IdealLoopTree* loop, Node_List &old_new);\n+#ifdef ASSERT\n+  bool convert_to_long_loop(Node* cmp, Node* phi, IdealLoopTree* loop);\n+#endif\n+  void add_empty_predicate(Deoptimization::DeoptReason reason, Node* inner_head, IdealLoopTree* loop, SafePointNode* sfpt);\n+  SafePointNode* find_safepoint(Node* back_control, Node* x, IdealLoopTree* loop);\n@@ -1252,1 +1265,1 @@\n-  void add_constraint( int stride_con, int scale_con, Node *offset, Node *low_limit, Node *upper_limit, Node *pre_ctrl, Node **pre_limit, Node **main_limit );\n+  void add_constraint(jlong stride_con, jlong scale_con, Node* offset, Node* low_limit, Node* upper_limit, Node* pre_ctrl, Node** pre_limit, Node** main_limit);\n@@ -1254,1 +1267,1 @@\n-  Node* adjust_limit(int stride_con, Node * scale, Node *offset, Node *rc_limit, Node *loop_limit, Node *pre_ctrl, bool round_up);\n+  Node* adjust_limit(bool reduce, Node* scale, Node* offset, Node* rc_limit, Node* old_limit, Node* pre_ctrl, bool round);\n@@ -1478,0 +1491,3 @@\n+  static volatile int _long_loop_candidates;\n+  static volatile int _long_loop_nests;\n+  static volatile int _long_loop_counted_loops;\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":26,"deletions":10,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -1048,1 +1048,3 @@\n-  if( n_blk->is_CountedLoop() && n->Opcode() == Op_AddI ) return n;\n+  if ((n_blk->is_CountedLoop() || (n_blk->is_Loop() && n_blk->as_Loop()->is_transformed_long_loop())) && n->Opcode() == Op_AddI) {\n+    return n;\n+  }\n@@ -2673,1 +2675,7 @@\n-\n+  if (!lp_exit->is_IfFalse()) {\n+    \/\/ The loop exit condition is (i <u limit) ==> (i >= 0 && i < limit).\n+    \/\/ We therefore can't add a single exit condition.\n+    return NULL;\n+  }\n+  \/\/ The loop exit condition is !(i <u limit) ==> (i < 0 || i >= limit).\n+  \/\/ Split out the exit condition (i < 0) for stride < 0 or (i >= limit) for stride > 0.\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -859,0 +859,1 @@\n+  bool            _has_ea_local_in_scope; \/\/ NoEscape or ArgEscape objects in JVM States\n@@ -862,1 +863,1 @@\n-  MachSafePointNode() : MachReturnNode(), _oop_map(NULL), _jvms(NULL), _jvmadj(0) {\n+  MachSafePointNode() : MachReturnNode(), _oop_map(NULL), _jvms(NULL), _jvmadj(0), _has_ea_local_in_scope(false) {\n@@ -962,0 +963,1 @@\n+  bool      _arg_escape;             \/\/ ArgEscape in parameter list\n","filename":"src\/hotspot\/share\/opto\/machnode.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1154,2 +1154,1 @@\n-  \/\/ Don't do scalar replacement if the frame can be popped by JVMTI:\n-  \/\/ if reallocation fails during deoptimization we'll pop all\n+  \/\/ If reallocation fails during deoptimization we'll pop all\n@@ -1158,1 +1157,3 @@\n-  if (!EliminateAllocations || JvmtiExport::can_pop_frame()) {\n+  \/\/ We avoid this issue by eager reallocation when the popframe request\n+  \/\/ is received.\n+  if (!EliminateAllocations) {\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1293,0 +1293,1 @@\n+      mcall_java->_arg_escape = call_java->arg_escape();\n@@ -1317,0 +1318,1 @@\n+  msfpt->_has_ea_local_in_scope = sfpt->has_ea_local_in_scope();\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -423,7 +423,0 @@\n-  static OptoReg::Name  interpreter_method_reg();\n-  static int            interpreter_method_reg_encode();\n-\n-  static OptoReg::Name  compiler_method_reg();\n-  static const RegMask &compiler_method_reg_mask();\n-  static int            compiler_method_reg_encode();\n-\n@@ -540,4 +533,0 @@\n-  \/\/ Perform a platform dependent implicit null fixup.  This is needed\n-  \/\/ on windows95 to take care of some unusual register constraints.\n-  void pd_implicit_null_fixup(MachNode *load, uint idx);\n-\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1312,0 +1312,53 @@\n+bool LoadNode::has_reinterpret_variant(const Type* rt) {\n+  BasicType bt = rt->basic_type();\n+  switch (Opcode()) {\n+    case Op_LoadI: return (bt == T_FLOAT);\n+    case Op_LoadL: return (bt == T_DOUBLE);\n+    case Op_LoadF: return (bt == T_INT);\n+    case Op_LoadD: return (bt == T_LONG);\n+\n+    default: return false;\n+  }\n+}\n+\n+Node* LoadNode::convert_to_reinterpret_load(PhaseGVN& gvn, const Type* rt) {\n+  BasicType bt = rt->basic_type();\n+  assert(has_reinterpret_variant(rt), \"no reinterpret variant: %s %s\", Name(), type2name(bt));\n+  bool is_mismatched = is_mismatched_access();\n+  const TypeRawPtr* raw_type = gvn.type(in(MemNode::Memory))->isa_rawptr();\n+  if (raw_type == NULL) {\n+    is_mismatched = true; \/\/ conservatively match all non-raw accesses as mismatched\n+  }\n+  return LoadNode::make(gvn, in(MemNode::Control), in(MemNode::Memory), in(MemNode::Address),\n+                        raw_adr_type(), rt, bt, _mo, _control_dependency,\n+                        is_unaligned_access(), is_mismatched);\n+}\n+\n+bool StoreNode::has_reinterpret_variant(const Type* vt) {\n+  BasicType bt = vt->basic_type();\n+  switch (Opcode()) {\n+    case Op_StoreI: return (bt == T_FLOAT);\n+    case Op_StoreL: return (bt == T_DOUBLE);\n+    case Op_StoreF: return (bt == T_INT);\n+    case Op_StoreD: return (bt == T_LONG);\n+\n+    default: return false;\n+  }\n+}\n+\n+Node* StoreNode::convert_to_reinterpret_store(PhaseGVN& gvn, Node* val, const Type* vt) {\n+  BasicType bt = vt->basic_type();\n+  assert(has_reinterpret_variant(vt), \"no reinterpret variant: %s %s\", Name(), type2name(bt));\n+  StoreNode* st = StoreNode::make(gvn, in(MemNode::Control), in(MemNode::Memory), in(MemNode::Address), raw_adr_type(), val, bt, _mo);\n+\n+  bool is_mismatched = is_mismatched_access();\n+  const TypeRawPtr* raw_type = gvn.type(in(MemNode::Memory))->isa_rawptr();\n+  if (raw_type == NULL) {\n+    is_mismatched = true; \/\/ conservatively match all non-raw accesses as mismatched\n+  }\n+  if (is_mismatched) {\n+    st->set_mismatched_access();\n+  }\n+  return st;\n+}\n+\n@@ -2633,0 +2686,1 @@\n+  Node* value   = in(MemNode::ValueIn);\n@@ -2697,0 +2751,13 @@\n+  \/\/ Fold reinterpret cast into memory operation:\n+  \/\/    StoreX mem (MoveY2X v) => StoreY mem v\n+  if (value->is_Move()) {\n+    const Type* vt = value->in(1)->bottom_type();\n+    if (has_reinterpret_variant(vt)) {\n+      if (phase->C->post_loop_opts_phase()) {\n+        return convert_to_reinterpret_store(*phase, value->in(1), vt);\n+      } else {\n+        phase->C->record_for_post_loop_opts_igvn(this); \/\/ attempt the transformation once loop opts are over\n+      }\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":67,"deletions":0,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -289,0 +289,3 @@\n+  bool  has_reinterpret_variant(const Type* rt);\n+  Node* convert_to_reinterpret_load(PhaseGVN& gvn, const Type* rt);\n+\n@@ -640,0 +643,3 @@\n+  bool  has_reinterpret_variant(const Type* vt);\n+  Node* convert_to_reinterpret_store(PhaseGVN& gvn, Node* val, const Type* vt);\n+\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -518,1 +518,1 @@\n-  if (is_macro())\n+  if (is_macro()) {\n@@ -520,1 +520,2 @@\n-  if (is_expensive())\n+  }\n+  if (is_expensive()) {\n@@ -522,8 +523,4 @@\n-  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-  bs->register_potential_barrier_node(n);\n-  \/\/ If the cloned node is a range check dependent CastII, add it to the list.\n-  CastIINode* cast = n->isa_CastII();\n-  if (cast != NULL && cast->has_range_check()) {\n-    C->add_range_check_cast(cast);\n-  if (n->Opcode() == Op_Opaque4) {\n-    C->add_opaque4_node(n);\n+  if (for_post_loop_opts_igvn()) {\n+    \/\/ Don't add cloned node to Compile::_for_post_loop_opts_igvn list automatically.\n+    \/\/ If it is applicable, it will happen anyway when the cloned node is registered with IGVN.\n+    n->remove_flag(Node::NodeFlags::Flag_for_post_loop_opts_igvn);\n@@ -532,0 +529,2 @@\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  bs->register_potential_barrier_node(n);\n@@ -641,6 +640,2 @@\n-  CastIINode* cast = isa_CastII();\n-  if (cast != NULL && cast->has_range_check()) {\n-    compile->remove_range_check_cast(cast);\n-  }\n-  if (Opcode() == Op_Opaque4) {\n-    compile->remove_opaque4_node(this);\n+  if (for_post_loop_opts_igvn()) {\n+    compile->remove_from_post_loop_opts_igvn(this);\n@@ -1064,1 +1059,1 @@\n-  assert(max_flags() <= max_jushort, \"too many NodeProperty flags\");\n+  assert(max_flags() <= max_juint, \"too many NodeProperty flags\");\n@@ -1421,6 +1416,2 @@\n-      CastIINode* cast = dead->isa_CastII();\n-      if (cast != NULL && cast->has_range_check()) {\n-        igvn->C->remove_range_check_cast(cast);\n-      }\n-      if (dead->Opcode() == Op_Opaque4) {\n-        igvn->C->remove_opaque4_node(dead);\n+      if (dead->for_post_loop_opts_igvn()) {\n+        igvn->C->remove_from_post_loop_opts_igvn(dead);\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":14,"deletions":23,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -117,0 +117,1 @@\n+class MoveNode;\n@@ -734,2 +735,3 @@\n-    DEFINE_CLASS_ID(Halt, Node, 15)\n-    DEFINE_CLASS_ID(Opaque1, Node, 16)\n+    DEFINE_CLASS_ID(Halt,     Node, 15)\n+    DEFINE_CLASS_ID(Opaque1,  Node, 16)\n+    DEFINE_CLASS_ID(Move,     Node, 17)\n@@ -737,1 +739,1 @@\n-    _max_classes  = ClassMask_Opaque1\n+    _max_classes  = ClassMask_Move\n@@ -743,16 +745,17 @@\n-    Flag_is_Copy                     = 0x01, \/\/ should be first bit to avoid shift\n-    Flag_rematerialize               = Flag_is_Copy << 1,\n-    Flag_needs_anti_dependence_check = Flag_rematerialize << 1,\n-    Flag_is_macro                    = Flag_needs_anti_dependence_check << 1,\n-    Flag_is_Con                      = Flag_is_macro << 1,\n-    Flag_is_cisc_alternate           = Flag_is_Con << 1,\n-    Flag_is_dead_loop_safe           = Flag_is_cisc_alternate << 1,\n-    Flag_may_be_short_branch         = Flag_is_dead_loop_safe << 1,\n-    Flag_avoid_back_to_back_before   = Flag_may_be_short_branch << 1,\n-    Flag_avoid_back_to_back_after    = Flag_avoid_back_to_back_before << 1,\n-    Flag_has_call                    = Flag_avoid_back_to_back_after << 1,\n-    Flag_is_reduction                = Flag_has_call << 1,\n-    Flag_is_scheduled                = Flag_is_reduction << 1,\n-    Flag_has_vector_mask_set         = Flag_is_scheduled << 1,\n-    Flag_is_expensive                = Flag_has_vector_mask_set << 1,\n-    _last_flag                       = Flag_is_expensive\n+    Flag_is_Copy                     = 1 << 0, \/\/ should be first bit to avoid shift\n+    Flag_rematerialize               = 1 << 1,\n+    Flag_needs_anti_dependence_check = 1 << 2,\n+    Flag_is_macro                    = 1 << 3,\n+    Flag_is_Con                      = 1 << 4,\n+    Flag_is_cisc_alternate           = 1 << 5,\n+    Flag_is_dead_loop_safe           = 1 << 6,\n+    Flag_may_be_short_branch         = 1 << 7,\n+    Flag_avoid_back_to_back_before   = 1 << 8,\n+    Flag_avoid_back_to_back_after    = 1 << 9,\n+    Flag_has_call                    = 1 << 10,\n+    Flag_is_reduction                = 1 << 11,\n+    Flag_is_scheduled                = 1 << 12,\n+    Flag_has_vector_mask_set         = 1 << 13,\n+    Flag_is_expensive                = 1 << 14,\n+    Flag_for_post_loop_opts_igvn     = 1 << 15,\n+    _last_flag                       = Flag_for_post_loop_opts_igvn\n@@ -765,1 +768,1 @@\n-  jushort _flags;\n+  juint _flags;\n@@ -786,1 +789,1 @@\n-  const jushort flags() const { return _flags; }\n+  const juint flags() const { return _flags; }\n@@ -788,1 +791,1 @@\n-  void add_flag(jushort fl) { init_flags(fl); }\n+  void add_flag(juint fl) { init_flags(fl); }\n@@ -790,1 +793,1 @@\n-  void remove_flag(jushort fl) { clear_flag(fl); }\n+  void remove_flag(juint fl) { clear_flag(fl); }\n@@ -804,1 +807,1 @@\n-    assert(is_##type(), \"invalid node class\");               \\\n+    assert(is_##type(), \"invalid node class: %s\", Name()); \\\n@@ -884,0 +887,1 @@\n+  DEFINE_CLASS_QUERY(Move)\n@@ -968,0 +972,2 @@\n+  bool for_post_loop_opts_igvn() const { return (_flags & Flag_for_post_loop_opts_igvn) != 0; }\n+\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":30,"deletions":24,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -939,3 +939,3 @@\n-    } else if ( t->base() == Type::VectorS || t->base() == Type::VectorD ||\n-                t->base() == Type::VectorX || t->base() == Type::VectorY ||\n-                t->base() == Type::VectorZ) {\n+    } else if (t->base() == Type::VectorA || t->base() == Type::VectorS ||\n+               t->base() == Type::VectorD || t->base() == Type::VectorX ||\n+               t->base() == Type::VectorY || t->base() == Type::VectorZ) {\n@@ -1051,0 +1051,2 @@\n+  bool has_ea_local_in_scope = sfn->_has_ea_local_in_scope;\n+  bool arg_escape = false;\n@@ -1065,0 +1067,1 @@\n+      arg_escape = mcall->as_MachCallJava()->_arg_escape;\n@@ -1188,1 +1191,4 @@\n-    C->debug_info()->describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms->bci(), jvms->should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, return_vt, locvals, expvals, monvals);\n+    C->debug_info()->describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms->bci(),\n+                                    jvms->should_reexecute(), rethrow_exception, is_method_handle_invoke,\n+                                    return_oop, return_vt, has_ea_local_in_scope, arg_escape,\n+                                    locvals, expvals, monvals);\n@@ -1421,1 +1427,1 @@\n-  uint *inct_starts = NEW_RESOURCE_ARRAY(uint, nblocks+1);\n+  uint* inct_starts = NEW_RESOURCE_ARRAY(uint, nblocks+1);\n@@ -1424,1 +1430,1 @@\n-  uint *call_returns = NEW_RESOURCE_ARRAY(uint, nblocks+1);\n+  uint* call_returns = NEW_RESOURCE_ARRAY(uint, nblocks+1);\n@@ -1442,1 +1448,1 @@\n-  int *node_offsets      = NULL;\n+  int* node_offsets      = NULL;\n@@ -1462,2 +1468,2 @@\n-  Label *blk_labels = NEW_RESOURCE_ARRAY(Label, nblocks+1);\n-  for (uint i=0; i <= nblocks; i++) {\n+  Label* blk_labels = NEW_RESOURCE_ARRAY(Label, nblocks+1);\n+  for (uint i = 0; i <= nblocks; i++) {\n@@ -1467,3 +1473,1 @@\n-  \/\/ ------------------\n-  Node *delay_slot = NULL;\n-\n+  Node* delay_slot = NULL;\n@@ -1729,0 +1733,1 @@\n+      assert(!C->failing(), \"Should not reach here if failing.\");\n@@ -1731,1 +1736,1 @@\n-      DEBUG_ONLY( uint instr_offset = cb->insts_size(); )\n+      DEBUG_ONLY(uint instr_offset = cb->insts_size());\n@@ -1733,1 +1738,1 @@\n-      current_offset  = cb->insts_size();\n+      current_offset = cb->insts_size();\n@@ -1920,2 +1925,1 @@\n-                   C->is_osr_compilation()    ? \" compile_kind='osr'\" :\n-                   \"\");\n+                   C->is_osr_compilation() ? \" compile_kind='osr'\" : \"\");\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":20,"deletions":16,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2427,16 +2427,0 @@\n-  \/\/ See if we can avoid this safepoint.  No need for a SafePoint immediately\n-  \/\/ after a Call (except Leaf Call) or another SafePoint.\n-  Node *proj = control();\n-  if( proj->is_Proj() ) {\n-    Node *n0 = proj->in(0);\n-    if( n0->is_Catch() ) {\n-      n0 = n0->in(0)->in(0);\n-      assert( n0->is_Call(), \"expect a call here\" );\n-    }\n-    if( n0->is_Call() ) {\n-      if( n0->as_Call()->guaranteed_safepoint() )\n-        return;\n-    } else if( n0->is_SafePoint() && n0->req() >= parms ) {\n-      return;\n-    }\n-  }\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":0,"deletions":16,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -442,0 +442,1 @@\n+\/\/ 'worklist' is cleared upon returning.\n@@ -513,0 +514,3 @@\n+\n+  \/\/ Clear the original worklist\n+  worklist->clear();\n@@ -1408,6 +1412,2 @@\n-      CastIINode* cast = dead->isa_CastII();\n-      if (cast != NULL && cast->has_range_check()) {\n-        C->remove_range_check_cast(cast);\n-      }\n-      if (dead->Opcode() == Op_Opaque4) {\n-        C->remove_opaque4_node(dead);\n+      if (dead->for_post_loop_opts_igvn()) {\n+        C->remove_from_post_loop_opts_igvn(dead);\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -970,1 +970,1 @@\n-const TypeFunc* OptoRuntime::digestBase_implCompress_Type() {\n+const TypeFunc* OptoRuntime::digestBase_implCompress_Type(bool is_sha3) {\n@@ -972,1 +972,1 @@\n-  int num_args = 2;\n+  int num_args = is_sha3 ? 3 : 2;\n@@ -978,0 +978,1 @@\n+  if (is_sha3) fields[argp++] = TypeInt::INT; \/\/ digest_length\n@@ -991,1 +992,1 @@\n-const TypeFunc* OptoRuntime::digestBase_implCompressMB_Type() {\n+const TypeFunc* OptoRuntime::digestBase_implCompressMB_Type(bool is_sha3) {\n@@ -993,1 +994,1 @@\n-  int num_args = 4;\n+  int num_args = is_sha3 ? 5 : 4;\n@@ -999,0 +1000,1 @@\n+  if (is_sha3) fields[argp++] = TypeInt::INT; \/\/ digest_length\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -281,2 +281,2 @@\n-  static const TypeFunc* digestBase_implCompress_Type();\n-  static const TypeFunc* digestBase_implCompressMB_Type();\n+  static const TypeFunc* digestBase_implCompress_Type(bool is_sha3);\n+  static const TypeFunc* digestBase_implCompressMB_Type(bool is_sha3);\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"classfile\/classListWriter.hpp\"\n@@ -3816,1 +3817,1 @@\n-  if (!DynamicDumpSharedSpaces) {\n+  if (!Arguments::is_dumping_archive()) {\n@@ -3851,1 +3852,1 @@\n-                                                 method_type, m, instantiated_method_type);\n+                                                 method_type, m, instantiated_method_type, THREAD);\n@@ -3865,3 +3866,0 @@\n-  if (!DynamicArchive::is_mapped()) {\n-    return NULL;\n-  }\n@@ -3908,3 +3906,3 @@\n-JVM_ENTRY(jboolean, JVM_IsDynamicDumpingEnabled(JNIEnv* env))\n-    JVMWrapper(\"JVM_IsDynamicDumpingEnable\");\n-    return DynamicDumpSharedSpaces;\n+JVM_ENTRY(jboolean, JVM_IsCDSDumpingEnabled(JNIEnv* env))\n+    JVMWrapper(\"JVM_IsCDSDumpingEnabled\");\n+    return Arguments::is_dumping_archive();\n@@ -3944,1 +3942,1 @@\n-  return DumpLoadedClassList != NULL && classlist_file != NULL && classlist_file->is_open();\n+  return ClassListWriter::is_enabled();\n@@ -3953,1 +3951,1 @@\n-  assert(DumpLoadedClassList != NULL && classlist_file->is_open(), \"Should be set and open\");\n+  assert(ClassListWriter::is_enabled(), \"Should be set and open\");\n@@ -3958,1 +3956,2 @@\n-    classlist_file->print_cr(\"%s %s\", LambdaFormInvokers::lambda_form_invoker_tag(), c_line);\n+    ClassListWriter w;\n+    w.stream()->print_cr(\"%s %s\", LambdaFormInvokers::lambda_form_invoker_tag(), c_line);\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":10,"deletions":11,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -1209,0 +1209,5 @@\n+  EscapeBarrier eb(true, calling_thread, java_thread);\n+  if (!eb.deoptimize_objects(MaxJavaStackTraceDepth)) {\n+    return JVMTI_ERROR_OUT_OF_MEMORY;\n+  }\n+\n@@ -1254,0 +1259,5 @@\n+  EscapeBarrier eb(true, calling_thread, java_thread);\n+  if (!eb.deoptimize_objects(MaxJavaStackTraceDepth)) {\n+    return JVMTI_ERROR_OUT_OF_MEMORY;\n+  }\n+\n@@ -1637,4 +1647,0 @@\n-  JavaThread* current_thread  = JavaThread::current();\n-  HandleMark hm(current_thread);\n-  uint32_t debug_bits = 0;\n-\n@@ -1647,57 +1653,9 @@\n-  \/\/ Check if java_thread is fully suspended\n-  if (!java_thread->is_thread_fully_suspended(true \/* wait for suspend completion *\/, &debug_bits)) {\n-    return JVMTI_ERROR_THREAD_NOT_SUSPENDED;\n-  }\n-  \/\/ Check to see if a PopFrame was already in progress\n-  if (java_thread->popframe_condition() != JavaThread::popframe_inactive) {\n-    \/\/ Probably possible for JVMTI clients to trigger this, but the\n-    \/\/ JPDA backend shouldn't allow this to happen\n-    return JVMTI_ERROR_INTERNAL;\n-  }\n-\n-  {\n-    \/\/ Was workaround bug\n-    \/\/    4812902: popFrame hangs if the method is waiting at a synchronize\n-    \/\/ Catch this condition and return an error to avoid hanging.\n-    \/\/ Now JVMTI spec allows an implementation to bail out with an opaque frame error.\n-    OSThread* osThread = java_thread->osthread();\n-    if (osThread->get_state() == MONITOR_WAIT) {\n-      return JVMTI_ERROR_OPAQUE_FRAME;\n-    }\n-  }\n-\n-  {\n-    ResourceMark rm(current_thread);\n-    \/\/ Check if there are more than one Java frame in this thread, that the top two frames\n-    \/\/ are Java (not native) frames, and that there is no intervening VM frame\n-    int frame_count = 0;\n-    bool is_interpreted[2];\n-    intptr_t *frame_sp[2];\n-    \/\/ The 2-nd arg of constructor is needed to stop iterating at java entry frame.\n-    for (vframeStream vfs(java_thread, true, false \/* process_frames *\/); !vfs.at_end(); vfs.next()) {\n-      methodHandle mh(current_thread, vfs.method());\n-      if (mh->is_native()) return(JVMTI_ERROR_OPAQUE_FRAME);\n-      is_interpreted[frame_count] = vfs.is_interpreted_frame();\n-      frame_sp[frame_count] = vfs.frame_id();\n-      if (++frame_count > 1) break;\n-    }\n-    if (frame_count < 2)  {\n-      \/\/ We haven't found two adjacent non-native Java frames on the top.\n-      \/\/ There can be two situations here:\n-      \/\/  1. There are no more java frames\n-      \/\/  2. Two top java frames are separated by non-java native frames\n-      if(vframeForNoProcess(java_thread, 1) == NULL) {\n-        return JVMTI_ERROR_NO_MORE_FRAMES;\n-      } else {\n-        \/\/ Intervening non-java native or VM frames separate java frames.\n-        \/\/ Current implementation does not support this. See bug #5031735.\n-        \/\/ In theory it is possible to pop frames in such cases.\n-        return JVMTI_ERROR_OPAQUE_FRAME;\n-      }\n-    }\n-\n-    \/\/ If any of the top 2 frames is a compiled one, need to deoptimize it\n-    for (int i = 0; i < 2; i++) {\n-      if (!is_interpreted[i]) {\n-        Deoptimization::deoptimize_frame(java_thread, frame_sp[i]);\n-      }\n+  \/\/ Eagerly reallocate scalar replaced objects.\n+  JavaThread* current_thread = JavaThread::current();\n+  EscapeBarrier eb(true, current_thread, java_thread);\n+  if (eb.barrier_active()) {\n+    if (java_thread->frames_to_pop_failed_realloc() > 0) {\n+      \/\/ VM is in the process of popping the top frame because it has scalar replaced objects which\n+      \/\/ could not be reallocated on the heap.\n+      \/\/ Return JVMTI_ERROR_OUT_OF_MEMORY to avoid interfering with the VM.\n+      return JVMTI_ERROR_OUT_OF_MEMORY;\n@@ -1705,24 +1663,3 @@\n-\n-    \/\/ Update the thread state to reflect that the top frame is popped\n-    \/\/ so that cur_stack_depth is maintained properly and all frameIDs\n-    \/\/ are invalidated.\n-    \/\/ The current frame will be popped later when the suspended thread\n-    \/\/ is resumed and right before returning from VM to Java.\n-    \/\/ (see call_VM_base() in assembler_<cpu>.cpp).\n-\n-    \/\/ It's fine to update the thread state here because no JVMTI events\n-    \/\/ shall be posted for this PopFrame.\n-\n-    \/\/ It is only safe to perform the direct operation on the current\n-    \/\/ thread. All other usage needs to use a handshake for safety.\n-    {\n-      MutexLocker mu(JvmtiThreadState_lock);\n-      if (java_thread == JavaThread::current()) {\n-        state->update_for_pop_top_frame();\n-      } else {\n-        UpdateForPopTopFrameClosure op(state);\n-        Handshake::execute(&op, java_thread);\n-        if (op.result() != JVMTI_ERROR_NONE) {\n-          return op.result();\n-        }\n-      }\n+    if (!eb.deoptimize_objects(1)) {\n+      \/\/ Reallocation of scalar replaced objects failed -> return with error\n+      return JVMTI_ERROR_OUT_OF_MEMORY;\n@@ -1730,5 +1667,0 @@\n-\n-    java_thread->set_popframe_condition(JavaThread::popframe_pending_bit);\n-    \/\/ Set pending step flag for this popframe and it is cleared when next\n-    \/\/ step event is posted.\n-    state->set_pending_step_for_popframe();\n@@ -1737,1 +1669,8 @@\n-  return JVMTI_ERROR_NONE;\n+  MutexLocker mu(JvmtiThreadState_lock);\n+  UpdateForPopTopFrameClosure op(state);\n+  if (java_thread == current_thread) {\n+    op.doit(java_thread, true \/* self *\/);\n+  } else {\n+    Handshake::execute(&op, java_thread);\n+  }\n+  return op.result();\n@@ -1771,4 +1710,0 @@\n-  jvmtiError err = JVMTI_ERROR_NONE;\n-  ResourceMark rm;\n-  uint32_t debug_bits = 0;\n-\n@@ -1780,21 +1715,1 @@\n-  if (!java_thread->is_thread_fully_suspended(true, &debug_bits)) {\n-    return JVMTI_ERROR_THREAD_NOT_SUSPENDED;\n-  }\n-\n-  if (TraceJVMTICalls) {\n-    JvmtiSuspendControl::print();\n-  }\n-\n-  vframe *vf = vframeForNoProcess(java_thread, depth);\n-  if (vf == NULL) {\n-    return JVMTI_ERROR_NO_MORE_FRAMES;\n-  }\n-\n-  if (!vf->is_java_frame() || ((javaVFrame*) vf)->method()->is_native()) {\n-    return JVMTI_ERROR_OPAQUE_FRAME;\n-  }\n-\n-  assert(vf->frame_pointer() != NULL, \"frame pointer mustn't be NULL\");\n-\n-  \/\/ It is only safe to perform the direct operation on the current\n-  \/\/ thread. All other usage needs to use a vm-safepoint-op for safety.\n+  SetFramePopClosure op(this, state, depth);\n@@ -1803,2 +1718,1 @@\n-    int frame_number = state->count_frames() - depth;\n-    state->env_thread_state(this)->set_frame_pop(frame_number);\n+    op.doit(java_thread, true \/* self *\/);\n@@ -1806,3 +1720,1 @@\n-    SetFramePopClosure op(this, state, depth);\n-    err = op.result();\n-  return err;\n+  return op.result();\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":33,"deletions":121,"binary":false,"changes":154,"status":"modified"},{"patch":"@@ -429,0 +429,1 @@\n+  , _eb(false, NULL, NULL)\n@@ -443,0 +444,1 @@\n+  , _eb(type == T_OBJECT, JavaThread::current(), thread)\n@@ -456,0 +458,1 @@\n+  , _eb(true, calling_thread, thread)\n@@ -630,0 +633,56 @@\n+\/\/ Revert optimizations based on escape analysis if this is an access to a local object\n+bool VM_GetOrSetLocal::deoptimize_objects(javaVFrame* jvf) {\n+#if COMPILER2_OR_JVMCI\n+  assert(_type == T_OBJECT, \"EscapeBarrier should not be active if _type != T_OBJECT\");\n+  if (_depth < _thread->frames_to_pop_failed_realloc()) {\n+    \/\/ cannot access frame with failed reallocations\n+    _result = JVMTI_ERROR_OUT_OF_MEMORY;\n+    return false;\n+  }\n+  if (can_be_deoptimized(jvf)) {\n+    compiledVFrame* cf = compiledVFrame::cast(jvf);\n+    if (cf->has_ea_local_in_scope() && !_eb.deoptimize_objects(cf->fr().id())) {\n+      \/\/ reallocation of scalar replaced objects failed because heap is exhausted\n+      _result = JVMTI_ERROR_OUT_OF_MEMORY;\n+      return false;\n+    }\n+  }\n+\n+  \/\/ With this access the object could escape the thread changing its escape state from ArgEscape,\n+  \/\/ to GlobalEscape so we must deoptimize callers which could have optimized on the escape state.\n+  vframe* vf = jvf;\n+  do {\n+    \/\/ move to next physical frame\n+    while(!vf->is_top()) {\n+      vf = vf->sender();\n+    }\n+    vf = vf->sender();\n+\n+    if (vf != NULL && vf->is_compiled_frame()) {\n+      compiledVFrame* cvf = compiledVFrame::cast(vf);\n+      \/\/ Deoptimize objects if arg escape is being passed down the stack.\n+      \/\/ Note that deoptimizing the frame is not enough because objects need to be relocked\n+      if (cvf->arg_escape() && !_eb.deoptimize_objects(cvf->fr().id())) {\n+        \/\/ reallocation of scalar replaced objects failed because heap is exhausted\n+        _result = JVMTI_ERROR_OUT_OF_MEMORY;\n+        return false;\n+      }\n+    }\n+  } while(vf != NULL && !vf->is_entry_frame());\n+#endif \/\/ COMPILER2_OR_JVMCI\n+  return true;\n+}\n+\n+bool VM_GetOrSetLocal::doit_prologue() {\n+  if (_eb.barrier_active()) {\n+    _jvf = get_java_vframe();\n+    NULL_CHECK(_jvf, false);\n+\n+    if (!deoptimize_objects(_jvf)) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n@@ -631,1 +690,1 @@\n-  _jvf = get_java_vframe();\n+  _jvf = _jvf == NULL ? get_java_vframe() : _jvf;\n","filename":"src\/hotspot\/share\/prims\/jvmtiImpl.cpp","additions":60,"deletions":1,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -50,0 +50,2 @@\n+#include \"memory\/metaspace\/testHelpers.hpp\"\n+#include \"memory\/iterator.hpp\"\n@@ -83,0 +85,1 @@\n+#include \"runtime\/vframe.hpp\"\n@@ -90,0 +93,1 @@\n+#include \"utilities\/ostream.hpp\"\n@@ -885,0 +889,13 @@\n+WB_ENTRY(jboolean, WB_IsFrameDeoptimized(JNIEnv* env, jobject o, jint depth))\n+  bool result = false;\n+  if (thread->has_last_Java_frame()) {\n+    RegisterMap reg_map(thread);\n+    javaVFrame *jvf = thread->last_java_vframe(&reg_map);\n+    for (jint d = 0; d < depth && jvf != NULL; d++) {\n+      jvf = jvf->java_sender();\n+    }\n+    result = jvf != NULL && jvf->fr().is_deoptimized_frame();\n+  }\n+  return result;\n+WB_END\n+\n@@ -1699,0 +1716,59 @@\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ MetaspaceTestContext and MetaspaceTestArena\n+WB_ENTRY(jlong, WB_CreateMetaspaceTestContext(JNIEnv* env, jobject wb, jlong commit_limit, jlong reserve_limit))\n+  metaspace::MetaspaceTestContext* context =\n+      new metaspace::MetaspaceTestContext(\"whitebox-metaspace-context\", (size_t) commit_limit, (size_t) reserve_limit);\n+  return (jlong)p2i(context);\n+WB_END\n+\n+WB_ENTRY(void, WB_DestroyMetaspaceTestContext(JNIEnv* env, jobject wb, jlong context))\n+  delete (metaspace::MetaspaceTestContext*) context;\n+WB_END\n+\n+WB_ENTRY(void, WB_PurgeMetaspaceTestContext(JNIEnv* env, jobject wb, jlong context))\n+  metaspace::MetaspaceTestContext* context0 = (metaspace::MetaspaceTestContext*) context;\n+  context0->purge_area();\n+WB_END\n+\n+WB_ENTRY(void, WB_PrintMetaspaceTestContext(JNIEnv* env, jobject wb, jlong context))\n+  metaspace::MetaspaceTestContext* context0 = (metaspace::MetaspaceTestContext*) context;\n+  context0->print_on(tty);\n+WB_END\n+\n+WB_ENTRY(jlong, WB_GetTotalCommittedWordsInMetaspaceTestContext(JNIEnv* env, jobject wb, jlong context))\n+  metaspace::MetaspaceTestContext* context0 = (metaspace::MetaspaceTestContext*) context;\n+  return context0->committed_words();\n+WB_END\n+\n+WB_ENTRY(jlong, WB_GetTotalUsedWordsInMetaspaceTestContext(JNIEnv* env, jobject wb, jlong context))\n+  metaspace::MetaspaceTestContext* context0 = (metaspace::MetaspaceTestContext*) context;\n+  return context0->used_words();\n+WB_END\n+\n+WB_ENTRY(jlong, WB_CreateArenaInTestContext(JNIEnv* env, jobject wb, jlong context, jboolean is_micro))\n+  const Metaspace::MetaspaceType type = is_micro ? Metaspace::ReflectionMetaspaceType : Metaspace::StandardMetaspaceType;\n+  metaspace::MetaspaceTestContext* context0 = (metaspace::MetaspaceTestContext*) context;\n+  return (jlong)p2i(context0->create_arena(type));\n+WB_END\n+\n+WB_ENTRY(void, WB_DestroyMetaspaceTestArena(JNIEnv* env, jobject wb, jlong arena))\n+  delete (metaspace::MetaspaceTestArena*) arena;\n+WB_END\n+\n+WB_ENTRY(jlong, WB_AllocateFromMetaspaceTestArena(JNIEnv* env, jobject wb, jlong arena, jlong word_size))\n+  metaspace::MetaspaceTestArena* arena0 = (metaspace::MetaspaceTestArena*) arena;\n+  MetaWord* p = arena0->allocate((size_t) word_size);\n+  return (jlong)p2i(p);\n+WB_END\n+\n+WB_ENTRY(void, WB_DeallocateToMetaspaceTestArena(JNIEnv* env, jobject wb, jlong arena, jlong p, jlong word_size))\n+  metaspace::MetaspaceTestArena* arena0 = (metaspace::MetaspaceTestArena*) arena;\n+  arena0->deallocate((MetaWord*)p, (size_t) word_size);\n+WB_END\n+\n+WB_ENTRY(jlong, WB_GetMaxMetaspaceAllocationSize(JNIEnv* env, jobject wb))\n+  return (jlong) Metaspace::max_allocation_word_size() * BytesPerWord;\n+WB_END\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n@@ -1715,9 +1791,0 @@\n-WB_ENTRY(void, WB_FreeMetaspace(JNIEnv* env, jobject wb, jobject class_loader, jlong addr, jlong size))\n-  oop class_loader_oop = JNIHandles::resolve(class_loader);\n-  ClassLoaderData* cld = class_loader_oop != NULL\n-      ? java_lang_ClassLoader::loader_data_acquire(class_loader_oop)\n-      : ClassLoaderData::the_null_class_loader_data();\n-\n-  MetadataFactory::free_array(cld, (Array<u1>*)(uintptr_t)addr);\n-WB_END\n-\n@@ -2471,0 +2538,1 @@\n+  {CC\"isFrameDeoptimized\", CC\"(I)Z\",                  (void*)&WB_IsFrameDeoptimized},\n@@ -2551,2 +2619,0 @@\n-  {CC\"freeMetaspace\",\n-     CC\"(Ljava\/lang\/ClassLoader;JJ)V\",                (void*)&WB_FreeMetaspace },\n@@ -2654,0 +2720,13 @@\n+\n+  {CC\"createMetaspaceTestContext\", CC\"(JJ)J\",         (void*)&WB_CreateMetaspaceTestContext},\n+  {CC\"destroyMetaspaceTestContext\", CC\"(J)V\",         (void*)&WB_DestroyMetaspaceTestContext},\n+  {CC\"purgeMetaspaceTestContext\", CC\"(J)V\",           (void*)&WB_PurgeMetaspaceTestContext},\n+  {CC\"printMetaspaceTestContext\", CC\"(J)V\",           (void*)&WB_PrintMetaspaceTestContext},\n+  {CC\"getTotalCommittedWordsInMetaspaceTestContext\", CC\"(J)J\",(void*)&WB_GetTotalCommittedWordsInMetaspaceTestContext},\n+  {CC\"getTotalUsedWordsInMetaspaceTestContext\", CC\"(J)J\", (void*)&WB_GetTotalUsedWordsInMetaspaceTestContext},\n+  {CC\"createArenaInTestContext\", CC\"(JZ)J\",           (void*)&WB_CreateArenaInTestContext},\n+  {CC\"destroyMetaspaceTestArena\", CC\"(J)V\",           (void*)&WB_DestroyMetaspaceTestArena},\n+  {CC\"allocateFromMetaspaceTestArena\", CC\"(JJ)J\",     (void*)&WB_AllocateFromMetaspaceTestArena},\n+  {CC\"deallocateToMetaspaceTestArena\", CC\"(JJJ)V\",    (void*)&WB_DeallocateToMetaspaceTestArena},\n+  {CC\"maxMetaspaceAllocationSize\", CC\"()J\",           (void*)&WB_GetMaxMetaspaceAllocationSize},\n+\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":90,"deletions":11,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -534,0 +534,1 @@\n+  { \"CriticalJNINatives\",                  JDK_Version::jdk(16), JDK_Version::jdk(17), JDK_Version::jdk(18) },\n@@ -3936,0 +3937,16 @@\n+static void apply_debugger_ergo() {\n+  if (UseDebuggerErgo) {\n+    \/\/ Turn on sub-flags\n+    FLAG_SET_ERGO_IF_DEFAULT(UseDebuggerErgo1, true);\n+    FLAG_SET_ERGO_IF_DEFAULT(UseDebuggerErgo2, true);\n+  }\n+\n+  if (UseDebuggerErgo2) {\n+    \/\/ Debugging with limited number of CPUs\n+    FLAG_SET_ERGO_IF_DEFAULT(UseNUMA, false);\n+    FLAG_SET_ERGO_IF_DEFAULT(ConcGCThreads, 1);\n+    FLAG_SET_ERGO_IF_DEFAULT(ParallelGCThreads, 1);\n+    FLAG_SET_ERGO_IF_DEFAULT(CICompilerCount, 2);\n+  }\n+}\n+\n@@ -4132,0 +4149,2 @@\n+  apply_debugger_ergo();\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"prims\/jvmtiDeferredUpdates.hpp\"\n@@ -60,0 +61,1 @@\n+#include \"runtime\/escapeBarrier.hpp\"\n@@ -66,0 +68,1 @@\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -181,1 +184,2 @@\n-                                  frame& deoptee, RegisterMap& map, GrowableArray<compiledVFrame*>* chunk) {\n+                                  frame& deoptee, RegisterMap& map, GrowableArray<compiledVFrame*>* chunk,\n+                                  bool& deoptimized_objects) {\n@@ -185,0 +189,4 @@\n+  JavaThread* deoptee_thread = chunk->at(0)->thread();\n+  assert(exec_mode == Deoptimization::Unpack_none || (deoptee_thread == thread),\n+         \"a frame can only be deoptimized by the owner thread\");\n+\n@@ -222,2 +230,20 @@\n-    bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();\n-    JRT_BLOCK\n+    if (exec_mode == Deoptimization::Unpack_none) {\n+      assert(thread->thread_state() == _thread_in_vm, \"assumption\");\n+      Thread* THREAD = thread;\n+      \/\/ Clear pending OOM if reallocation fails and return true indicating allocation failure\n+      if (vk != NULL) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, CHECK_AND_CLEAR_(true));\n+      }\n+      if (objects != NULL) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+        bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, CHECK_AND_CLEAR_(true));\n+      }\n+      \/\/ Make sure the deoptee frame gets processed after a potential safepoint during\n+      \/\/ object reallocation. This is necessary because (a) deoptee_thread can be\n+      \/\/ different from the current thread and (b) the deoptee frame does not need to be\n+      \/\/ the top frame.\n+      StackWatermarkSet::finish_processing(deoptee_thread, NULL \/* context *\/, StackWatermarkKind::gc);\n+      deoptimized_objects = true;\n+    } else {\n+      JRT_BLOCK\n@@ -229,0 +255,1 @@\n+        bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();\n@@ -231,1 +258,2 @@\n-    JRT_END\n+      JRT_END\n+    }\n@@ -235,7 +263,2 @@\n-      tty->print_cr(\"REALLOC OBJECTS in thread \" INTPTR_FORMAT, p2i(thread));\n-      if (objects != NULL) {\n-        Deoptimization::print_objects(objects, realloc_failures);\n-      } else {\n-        Handle obj = realloc_failures ? Handle() : return_oops.first();\n-        Deoptimization::print_object(vk, obj, realloc_failures);\n-      }\n+      tty->print_cr(\"REALLOC OBJECTS in thread \" INTPTR_FORMAT, p2i(deoptee_thread));\n+      Deoptimization::print_objects(objects, realloc_failures);\n@@ -253,1 +276,4 @@\n-static void eliminate_locks(JavaThread* thread, GrowableArray<compiledVFrame*>* chunk, bool realloc_failures) {\n+static void eliminate_locks(JavaThread* thread, GrowableArray<compiledVFrame*>* chunk, bool realloc_failures,\n+                            frame& deoptee, int exec_mode, bool& deoptimized_objects) {\n+  JavaThread* deoptee_thread = chunk->at(0)->thread();\n+  assert(!EscapeBarrier::objs_are_deoptimized(deoptee_thread, deoptee.id()), \"must relock just once\");\n@@ -264,1 +290,3 @@\n-      Deoptimization::relock_objects(monitors, thread, realloc_failures);\n+      bool relocked = Deoptimization::relock_objects(thread, monitors, deoptee_thread, deoptee,\n+                                                     exec_mode, realloc_failures);\n+      deoptimized_objects = deoptimized_objects || relocked;\n@@ -275,0 +303,7 @@\n+            if (exec_mode == Deoptimization::Unpack_none) {\n+              ObjectMonitor* monitor = deoptee_thread->current_waiting_monitor();\n+              if (monitor != NULL && (oop)monitor->object() == mi->owner()) {\n+                tty->print_cr(\"     object <\" INTPTR_FORMAT \"> DEFERRED relocking after wait\", p2i(mi->owner()));\n+                continue;\n+              }\n+            }\n@@ -288,0 +323,30 @@\n+\n+\/\/ Deoptimize objects, that is reallocate and relock them, just before they escape through JVMTI.\n+\/\/ The given vframes cover one physical frame.\n+bool Deoptimization::deoptimize_objects_internal(JavaThread* thread, GrowableArray<compiledVFrame*>* chunk,\n+                                                 bool& realloc_failures) {\n+  frame deoptee = chunk->at(0)->fr();\n+  JavaThread* deoptee_thread = chunk->at(0)->thread();\n+  CompiledMethod* cm = deoptee.cb()->as_compiled_method_or_null();\n+  RegisterMap map(chunk->at(0)->register_map());\n+  bool deoptimized_objects = false;\n+\n+  bool const jvmci_enabled = JVMCI_ONLY(UseJVMCICompiler) NOT_JVMCI(false);\n+\n+  \/\/ Reallocate the non-escaping objects and restore their fields.\n+  if (jvmci_enabled COMPILER2_PRESENT(|| (DoEscapeAnalysis && EliminateAllocations))) {\n+    realloc_failures = eliminate_allocations(thread, Unpack_none, cm, deoptee, map, chunk, deoptimized_objects);\n+  }\n+\n+  \/\/ Revoke biases of objects with eliminated locks in the given frame.\n+  Deoptimization::revoke_for_object_deoptimization(deoptee_thread, deoptee, &map, thread);\n+\n+  \/\/ MonitorInfo structures used in eliminate_locks are not GC safe.\n+  NoSafepointVerifier no_safepoint;\n+\n+  \/\/ Now relock objects if synchronization on them was eliminated.\n+  if (jvmci_enabled COMPILER2_PRESENT(|| ((DoEscapeAnalysis || EliminateNestedLocks) && EliminateLocks))) {\n+    eliminate_locks(thread, chunk, realloc_failures, deoptee, Unpack_none, deoptimized_objects);\n+  }\n+  return deoptimized_objects;\n+}\n@@ -346,1 +411,2 @@\n-    realloc_failures = eliminate_allocations(thread, exec_mode, cm, deoptee, map, chunk);\n+    bool unused;\n+    realloc_failures = eliminate_allocations(thread, exec_mode, cm, deoptee, map, chunk, unused);\n@@ -361,2 +427,4 @@\n-  if (jvmci_enabled COMPILER2_PRESENT( || ((DoEscapeAnalysis || EliminateNestedLocks) && EliminateLocks) )) {\n-    eliminate_locks(thread, chunk, realloc_failures);\n+  if ((jvmci_enabled COMPILER2_PRESENT( || ((DoEscapeAnalysis || EliminateNestedLocks) && EliminateLocks) ))\n+      && !EscapeBarrier::objs_are_deoptimized(thread, deoptee.id())) {\n+    bool unused;\n+    eliminate_locks(thread, chunk, realloc_failures, deoptee, exec_mode, unused);\n@@ -393,22 +461,1 @@\n-  if (thread->deferred_locals() != NULL) {\n-    GrowableArray<jvmtiDeferredLocalVariableSet*>* list = thread->deferred_locals();\n-    int i = 0;\n-    do {\n-      \/\/ Because of inlining we could have multiple vframes for a single frame\n-      \/\/ and several of the vframes could have deferred writes. Find them all.\n-      if (list->at(i)->id() == array->original().id()) {\n-        jvmtiDeferredLocalVariableSet* dlv = list->at(i);\n-        list->remove_at(i);\n-        \/\/ individual jvmtiDeferredLocalVariableSet are CHeapObj's\n-        delete dlv;\n-      } else {\n-        i++;\n-      }\n-    } while ( i < list->length() );\n-    if (list->length() == 0) {\n-      thread->set_deferred_locals(NULL);\n-      \/\/ free the list and elements back to C heap.\n-      delete list;\n-    }\n-\n-  }\n+  JvmtiDeferredUpdates::delete_updates_for_frame(thread, array->original().id());\n@@ -1469,1 +1516,3 @@\n-void Deoptimization::relock_objects(GrowableArray<MonitorInfo*>* monitors, JavaThread* thread, bool realloc_failures) {\n+bool Deoptimization::relock_objects(JavaThread* thread, GrowableArray<MonitorInfo*>* monitors,\n+                                    JavaThread* deoptee_thread, frame& fr, int exec_mode, bool realloc_failures) {\n+  bool relocked_objects = false;\n@@ -1474,0 +1523,1 @@\n+      relocked_objects = true;\n@@ -1482,1 +1532,1 @@\n-                 mark.biased_locker() == thread, \"should be locked to current thread\");\n+                 mark.biased_locker() == deoptee_thread, \"should be locked to current thread\");\n@@ -1486,0 +1536,19 @@\n+        } else if (exec_mode == Unpack_none) {\n+          if (mark.has_locker() && fr.sp() > (intptr_t*)mark.locker()) {\n+            \/\/ With exec_mode == Unpack_none obj may be thread local and locked in\n+            \/\/ a callee frame. In this case the bias was revoked before in revoke_for_object_deoptimization().\n+            \/\/ Make the lock in the callee a recursive lock and restore the displaced header.\n+            markWord dmw = mark.displaced_mark_helper();\n+            mark.locker()->set_displaced_header(markWord::encode((BasicLock*) NULL));\n+            obj->set_mark(dmw);\n+          }\n+          if (mark.has_monitor()) {\n+            \/\/ defer relocking if the deoptee thread is currently waiting for obj\n+            ObjectMonitor* waiting_monitor = deoptee_thread->current_waiting_monitor();\n+            if (waiting_monitor != NULL && (oop)waiting_monitor->object() == obj()) {\n+              assert(fr.is_deoptimized_frame(), \"frame must be scheduled for deoptimization\");\n+              mon_info->lock()->set_displaced_header(markWord::unused_mark());\n+              JvmtiDeferredUpdates::inc_relock_count_after_wait(deoptee_thread);\n+              continue;\n+            }\n+          }\n@@ -1488,1 +1557,1 @@\n-        ObjectSynchronizer::enter(obj, lock, thread);\n+        ObjectSynchronizer::enter(obj, lock, deoptee_thread);\n@@ -1493,0 +1562,1 @@\n+  return relocked_objects;\n@@ -1611,1 +1681,2 @@\n-static void collect_monitors(compiledVFrame* cvf, GrowableArray<Handle>* objects_to_revoke) {\n+static void collect_monitors(compiledVFrame* cvf, GrowableArray<Handle>* objects_to_revoke,\n+                             bool only_eliminated) {\n@@ -1616,1 +1687,3 @@\n-    if (!mon_info->eliminated() && mon_info->owner() != NULL) {\n+    if (mon_info->eliminated() == only_eliminated &&\n+        !mon_info->owner_is_scalar_replaced() &&\n+        mon_info->owner() != NULL) {\n@@ -1622,1 +1695,2 @@\n-static void get_monitors_from_stack(GrowableArray<Handle>* objects_to_revoke, JavaThread* thread, frame fr, RegisterMap* map) {\n+static void get_monitors_from_stack(GrowableArray<Handle>* objects_to_revoke, JavaThread* thread,\n+                                    frame fr, RegisterMap* map, bool only_eliminated) {\n@@ -1642,1 +1716,1 @@\n-    collect_monitors(cvf, objects_to_revoke);\n+    collect_monitors(cvf, objects_to_revoke, only_eliminated);\n@@ -1645,1 +1719,1 @@\n-  collect_monitors(cvf, objects_to_revoke);\n+  collect_monitors(cvf, objects_to_revoke, only_eliminated);\n@@ -1656,1 +1730,1 @@\n-  get_monitors_from_stack(objects_to_revoke, thread, fr, map);\n+  get_monitors_from_stack(objects_to_revoke, thread, fr, map, false);\n@@ -1666,0 +1740,35 @@\n+\/\/ Revoke the bias of objects with eliminated locking to prepare subsequent relocking.\n+void Deoptimization::revoke_for_object_deoptimization(JavaThread* deoptee_thread, frame fr,\n+                                                      RegisterMap* map, JavaThread* thread) {\n+  if (!UseBiasedLocking) {\n+    return;\n+  }\n+  GrowableArray<Handle>* objects_to_revoke = new GrowableArray<Handle>();\n+  if (deoptee_thread != thread) {\n+    \/\/ Process stack of deoptee thread as we will access oops during object deoptimization.\n+    StackWatermarkSet::start_processing(deoptee_thread, StackWatermarkKind::gc);\n+  }\n+  \/\/ Collect monitors but only those with eliminated locking.\n+  get_monitors_from_stack(objects_to_revoke, deoptee_thread, fr, map, true);\n+\n+  int len = objects_to_revoke->length();\n+  for (int i = 0; i < len; i++) {\n+    oop obj = (objects_to_revoke->at(i))();\n+    markWord mark = obj->mark();\n+    if (!mark.has_bias_pattern() ||\n+        mark.is_biased_anonymously() || \/\/ eliminated locking does not bias an object if it wasn't before\n+        !obj->klass()->prototype_header().has_bias_pattern() || \/\/ bulk revoke ignores eliminated monitors\n+        (obj->klass()->prototype_header().bias_epoch() != mark.bias_epoch())) { \/\/ bulk rebias ignores eliminated monitors\n+      \/\/ We reach here regularly if there's just eliminated locking on obj.\n+      \/\/ We must not call BiasedLocking::revoke_own_lock() in this case, as we\n+      \/\/ would hit assertions because it is a prerequisite that there has to be\n+      \/\/ non-eliminated locking on obj by deoptee_thread.\n+      \/\/ Luckily we don't have to revoke here because obj has to be a\n+      \/\/ non-escaping obj and can be relocked without revoking the bias. See\n+      \/\/ Deoptimization::relock_objects().\n+      continue;\n+    }\n+    BiasedLocking::revoke(objects_to_revoke->at(i), thread);\n+    assert(!objects_to_revoke->at(i)->mark().has_bias_pattern(), \"biases should be revoked by now\");\n+  }\n+}\n@@ -1732,2 +1841,4 @@\n-  assert(thread == Thread::current() || SafepointSynchronize::is_at_safepoint(),\n-         \"can only deoptimize other thread at a safepoint\");\n+  assert(thread == Thread::current() ||\n+         thread->is_handshake_safe_for(Thread::current()) ||\n+         SafepointSynchronize::is_at_safepoint(),\n+         \"can only deoptimize other thread at a safepoint\/handshake\");\n@@ -1745,1 +1856,2 @@\n-  if (thread == Thread::current()) {\n+  Thread* current = Thread::current();\n+  if (thread == current || thread->is_handshake_safe_for(current)) {\n@@ -2712,0 +2824,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":163,"deletions":50,"binary":false,"changes":213,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,0 +44,1 @@\n+  friend class EscapeBarrier;\n@@ -137,1 +138,2 @@\n-    Unpack_LIMIT                = 4\n+    Unpack_none                 = 4, \/\/ not deoptimizing the frame, just reallocating\/relocking for JVMTI\n+    Unpack_LIMIT                = 5\n@@ -155,0 +157,3 @@\n+  static void revoke_for_object_deoptimization(JavaThread* deoptee_thread, frame fr,\n+                                               RegisterMap* map, JavaThread* thread);\n+\n@@ -169,0 +174,5 @@\n+  \/\/ Deoptimize objects, that is reallocate and relock them, just before they\n+  \/\/ escape through JVMTI.  The given vframes cover one physical frame.\n+  static bool deoptimize_objects_internal(JavaThread* thread, GrowableArray<compiledVFrame*>* chunk,\n+                                          bool& realloc_failures);\n+\n@@ -178,1 +188,2 @@\n-  static void relock_objects(GrowableArray<MonitorInfo*>* monitors, JavaThread* thread, bool realloc_failures);\n+  static bool relock_objects(JavaThread* thread, GrowableArray<MonitorInfo*>* monitors,\n+                             JavaThread* deoptee_thread, frame& fr, int exec_mode, bool realloc_failures);\n@@ -473,0 +484,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.hpp","additions":15,"deletions":3,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1185,0 +1185,2 @@\n+\/\/ callers need a ResourceMark because of name_and_sig_as_C_string() usage,\n+\/\/ RA allocated string is returned to the caller\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -324,5 +324,2 @@\n-  product(bool, CriticalJNINatives, true,                                   \\\n-          \"Check for critical JNI entry points\")                            \\\n-                                                                            \\\n-  notproduct(bool, StressCriticalJNINatives, false,                         \\\n-          \"Exercise register saving code in critical natives\")              \\\n+  product(bool, CriticalJNINatives, false,                                  \\\n+          \"(Deprecated) Check for critical JNI entry points\")               \\\n@@ -351,0 +348,4 @@\n+  product(bool, UseSHA3Intrinsics, false, DIAGNOSTIC,                       \\\n+          \"Use intrinsics for SHA3 crypto hash function. \"                  \\\n+          \"Requires that UseSHA is enabled.\")                               \\\n+                                                                            \\\n@@ -394,0 +395,23 @@\n+  develop(bool, DeoptimizeObjectsALot, false,                               \\\n+          \"For testing purposes concurrent threads revert optimizations \"   \\\n+          \"based on escape analysis at intervals given with \"               \\\n+          \"DeoptimizeObjectsALotInterval=n. The thread count is given \"     \\\n+          \"with DeoptimizeObjectsALotThreadCountSingle and \"                \\\n+          \"DeoptimizeObjectsALotThreadCountAll.\")                           \\\n+                                                                            \\\n+  develop(uint64_t, DeoptimizeObjectsALotInterval, 5,                       \\\n+          \"Interval for DeoptimizeObjectsALot.\")                            \\\n+          range(0, max_jlong)                                               \\\n+                                                                            \\\n+  develop(int, DeoptimizeObjectsALotThreadCountSingle, 1,                   \\\n+          \"The number of threads that revert optimizations based on \"       \\\n+          \"escape analysis for a single thread if DeoptimizeObjectsALot \"   \\\n+          \"is enabled. The target thread is selected round robin.\" )        \\\n+          range(0, max_jint)                                                \\\n+                                                                            \\\n+  develop(int, DeoptimizeObjectsALotThreadCountAll, 1,                      \\\n+          \"The number of threads that revert optimizations based on \"       \\\n+          \"escape analysis for all threads if DeoptimizeObjectsALot \"       \\\n+          \"is enabled.\" )                                                   \\\n+          range(0, max_jint)                                                \\\n+                                                                            \\\n@@ -940,1 +964,0 @@\n-          constraint(InitialBootClassLoaderMetaspaceSizeConstraintFunc, AfterErgo)\\\n@@ -1577,0 +1600,9 @@\n+  product(ccstr, MetaspaceReclaimPolicy, \"balanced\",                        \\\n+          \"options: balanced, aggressive, none\")                            \\\n+                                                                            \\\n+  product(bool, MetaspaceGuardAllocations, false, DIAGNOSTIC,               \\\n+          \"Metapace allocations are guarded.\")                              \\\n+                                                                            \\\n+  product(bool, MetaspaceHandleDeallocations, true, DIAGNOSTIC,             \\\n+          \"Switch off Metapace deallocation handling.\")                     \\\n+                                                                            \\\n@@ -2196,0 +2228,11 @@\n+  notproduct(bool, UseDebuggerErgo, false,                                  \\\n+          \"Debugging Only: Adjust the VM to be more debugger-friendly. \"    \\\n+          \"Turns on the other UseDebuggerErgo* flags\")                      \\\n+                                                                            \\\n+  notproduct(bool, UseDebuggerErgo1, false,                                 \\\n+          \"Debugging Only: Enable workarounds for debugger induced \"        \\\n+          \"os::processor_id() >= os::processor_count() problems\")           \\\n+                                                                            \\\n+  notproduct(bool, UseDebuggerErgo2, false,                                 \\\n+          \"Debugging Only: Limit the number of spawned JVM threads\")        \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":49,"deletions":6,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -171,9 +171,0 @@\n-    if (log_is_enabled(Info, monitorinflation)) {\n-      \/\/ The ObjectMonitor subsystem uses perf counters so\n-      \/\/ do this before perfMemory_exit().\n-      \/\/ This other audit_and_print_stats() call is done at the\n-      \/\/ Debug level at a safepoint:\n-      \/\/ - for async deflation auditing:\n-      \/\/   ObjectSynchronizer::do_safepoint_work()\n-      ObjectSynchronizer::audit_and_print_stats(true \/* on_exit *\/);\n-    }\n","filename":"src\/hotspot\/share\/runtime\/init.cpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -677,33 +677,0 @@\n-\/\/ See if the thread is running inside a lazy critical native and\n-\/\/ update the thread critical count if so.  Also set a suspend flag to\n-\/\/ cause the native wrapper to return into the JVM to do the unlock\n-\/\/ once the native finishes.\n-static void check_for_lazy_critical_native(JavaThread *thread, JavaThreadState state) {\n-  if (state == _thread_in_native &&\n-      thread->has_last_Java_frame() &&\n-      thread->frame_anchor()->walkable()) {\n-    \/\/ This thread might be in a critical native nmethod so look at\n-    \/\/ the top of the stack and increment the critical count if it\n-    \/\/ is.\n-    frame wrapper_frame = thread->last_frame();\n-    CodeBlob* stub_cb = wrapper_frame.cb();\n-    if (stub_cb != NULL &&\n-        stub_cb->is_nmethod() &&\n-        stub_cb->as_nmethod_or_null()->is_lazy_critical_native()) {\n-      \/\/ A thread could potentially be in a critical native across\n-      \/\/ more than one safepoint, so only update the critical state on\n-      \/\/ the first one.  When it returns it will perform the unlock.\n-      if (!thread->do_critical_native_unlock()) {\n-#ifdef ASSERT\n-        if (!thread->in_critical()) {\n-          GCLocker::increment_debug_jni_lock_count();\n-        }\n-#endif\n-        thread->enter_critical();\n-        \/\/ Make sure the native wrapper calls back on return to\n-        \/\/ perform the needed critical unlock.\n-        thread->set_critical_native_unlock();\n-      }\n-    }\n-  }\n-}\n@@ -903,1 +870,0 @@\n-    check_for_lazy_critical_native(_thread, stable_state);\n@@ -1017,0 +983,4 @@\n+    \/\/ We have to wait if we are here because of a handshake for object deoptimization.\n+    if (self->is_obj_deopt_suspend()) {\n+      self->wait_for_object_deoptimization();\n+    }\n@@ -1037,0 +1007,4 @@\n+    \/\/ We have to wait if we are here because of a handshake for object deoptimization.\n+    if (self->is_obj_deopt_suspend()) {\n+      self->wait_for_object_deoptimization();\n+    }\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":8,"deletions":34,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -3241,30 +3241,0 @@\n-JRT_ENTRY_NO_ASYNC(void, SharedRuntime::block_for_jni_critical(JavaThread* thread))\n-  assert(thread == JavaThread::current(), \"must be\");\n-  \/\/ The code is about to enter a JNI lazy critical native method and\n-  \/\/ _needs_gc is true, so if this thread is already in a critical\n-  \/\/ section then just return, otherwise this thread should block\n-  \/\/ until needs_gc has been cleared.\n-  if (thread->in_critical()) {\n-    return;\n-  }\n-  \/\/ Lock and unlock a critical section to give the system a chance to block\n-  GCLocker::lock_critical(thread);\n-  GCLocker::unlock_critical(thread);\n-JRT_END\n-\n-JRT_LEAF(oopDesc*, SharedRuntime::pin_object(JavaThread* thread, oopDesc* obj))\n-  assert(Universe::heap()->supports_object_pinning(), \"Why we are here?\");\n-  assert(obj != NULL, \"Should not be null\");\n-  oop o(obj);\n-  o = Universe::heap()->pin_object(thread, o);\n-  assert(o != NULL, \"Should not be null\");\n-  return o;\n-JRT_END\n-\n-JRT_LEAF(void, SharedRuntime::unpin_object(JavaThread* thread, oopDesc* obj))\n-  assert(Universe::heap()->supports_object_pinning(), \"Why we are here?\");\n-  assert(obj != NULL, \"Should not be null\");\n-  oop o(obj);\n-  Universe::heap()->unpin_object(thread, o);\n-JRT_END\n-\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":0,"deletions":30,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -516,7 +516,0 @@\n-  \/\/ Block before entering a JNI critical method\n-  static void block_for_jni_critical(JavaThread* thread);\n-\n-  \/\/ Pin\/Unpin object\n-  static oopDesc* pin_object(JavaThread* thread, oopDesc* obj);\n-  static void unpin_object(JavaThread* thread, oopDesc* obj);\n-\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -149,0 +149,2 @@\n+address StubRoutines::_sha3_implCompress     = NULL;\n+address StubRoutines::_sha3_implCompressMB   = NULL;\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -231,0 +231,2 @@\n+  static address _sha3_implCompress;\n+  static address _sha3_implCompressMB;\n@@ -413,0 +415,2 @@\n+  static address sha3_implCompress()     { return _sha3_implCompress; }\n+  static address sha3_implCompressMB()   { return _sha3_implCompressMB; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -124,0 +124,1 @@\n+bool volatile ObjectSynchronizer::_is_final_audit = false;\n@@ -943,2 +944,2 @@\n-  \/\/ These are highly shared mostly-read variables.\n-  \/\/ To avoid false-sharing they need to be the sole occupants of a cache line.\n+  \/\/ This is a highly shared mostly-read variable.\n+  \/\/ To avoid false-sharing it needs to be the sole occupant of a cache line.\n@@ -946,2 +947,1 @@\n-  volatile int stw_cycle;\n-  DEFINE_PAD_MINUS_SIZE(1, OM_CACHE_LINE_SIZE, sizeof(volatile int) * 2);\n+  DEFINE_PAD_MINUS_SIZE(1, OM_CACHE_LINE_SIZE, sizeof(volatile int));\n@@ -1393,1 +1393,1 @@\n-  bool is_JavaThread = Thread::current()->is_Java_thread();\n+  Thread* self = Thread::current();\n@@ -1409,1 +1409,1 @@\n-    if (is_JavaThread) {\n+    if (self->is_Java_thread()) {\n@@ -1411,1 +1411,1 @@\n-      ThreadBlockInVM tbivm(JavaThread::current());\n+      ThreadBlockInVM tbivm(self->as_Java_thread());\n@@ -1567,1 +1567,1 @@\n-\/\/ scavenger -- deflate_monitor_list_using_JT() -- from reclaiming them\n+\/\/ scavenger -- deflate_monitor_list() -- from reclaiming them\n@@ -1686,4 +1686,3 @@\n-\/\/ deflate_global_idle_monitors_using_JT() and\n-\/\/ deflate_per_thread_idle_monitors_using_JT() (in another thread) can\n-\/\/ run at the same time as om_flush() so we have to follow a careful\n-\/\/ protocol to prevent list corruption.\n+\/\/ deflate_global_idle_monitors() and deflate_per_thread_idle_monitors()\n+\/\/ (in another thread) can run at the same time as om_flush() so we have\n+\/\/ to follow a careful protocol to prevent list corruption.\n@@ -2078,2 +2077,2 @@\n-    \/\/ exit_globals()'s call to audit_and_print_stats() is done\n-    \/\/ at the Info level and not at a safepoint.\n+    \/\/ The VMThread calls do_final_audit_and_print_stats() which calls\n+    \/\/ audit_and_print_stats() at the Info level at VM exit time.\n@@ -2084,2 +2083,2 @@\n-\/\/ Deflate the specified ObjectMonitor if not in-use using a JavaThread.\n-\/\/ Returns true if it was deflated and false otherwise.\n+\/\/ Deflate the specified ObjectMonitor if not in-use. Returns true if it\n+\/\/ was deflated and false otherwise.\n@@ -2097,4 +2096,3 @@\n-bool ObjectSynchronizer::deflate_monitor_using_JT(ObjectMonitor* mid,\n-                                                  ObjectMonitor** free_head_p,\n-                                                  ObjectMonitor** free_tail_p) {\n-  assert(Thread::current()->is_Java_thread(), \"precondition\");\n+bool ObjectSynchronizer::deflate_monitor(ObjectMonitor* mid,\n+                                         ObjectMonitor** free_head_p,\n+                                         ObjectMonitor** free_tail_p) {\n@@ -2168,3 +2166,2 @@\n-      log_trace(monitorinflation)(\"deflate_monitor_using_JT: \"\n-                                  \"object=\" INTPTR_FORMAT \", mark=\"\n-                                  INTPTR_FORMAT \", type='%s'\",\n+      log_trace(monitorinflation)(\"deflate_monitor: object=\" INTPTR_FORMAT\n+                                  \", mark=\" INTPTR_FORMAT \", type='%s'\",\n@@ -2215,2 +2212,2 @@\n-\/\/ Walk a given ObjectMonitor list and deflate idle ObjectMonitors using\n-\/\/ a JavaThread. Returns the number of deflated ObjectMonitors. The given\n+\/\/ Walk a given ObjectMonitor list and deflate idle ObjectMonitors.\n+\/\/ Returns the number of deflated ObjectMonitors. The given\n@@ -2218,2 +2215,2 @@\n-\/\/ If a safepoint has started, then we save state via saved_mid_in_use_p\n-\/\/ and return to the caller to honor the safepoint.\n+\/\/ If self is a JavaThread and a safepoint has started, then we save state\n+\/\/ via saved_mid_in_use_p and return to the caller to honor the safepoint.\n@@ -2221,7 +2218,6 @@\n-int ObjectSynchronizer::deflate_monitor_list_using_JT(ObjectMonitor** list_p,\n-                                                      int* count_p,\n-                                                      ObjectMonitor** free_head_p,\n-                                                      ObjectMonitor** free_tail_p,\n-                                                      ObjectMonitor** saved_mid_in_use_p) {\n-  JavaThread* self = JavaThread::current();\n-\n+int ObjectSynchronizer::deflate_monitor_list(Thread* self,\n+                                             ObjectMonitor** list_p,\n+                                             int* count_p,\n+                                             ObjectMonitor** free_head_p,\n+                                             ObjectMonitor** free_tail_p,\n+                                             ObjectMonitor** saved_mid_in_use_p) {\n@@ -2278,1 +2274,1 @@\n-    if (mid->is_old() && deflate_monitor_using_JT(mid, free_head_p, free_tail_p)) {\n+    if (mid->is_old() && deflate_monitor(mid, free_head_p, free_tail_p)) {\n@@ -2286,1 +2282,1 @@\n-        \/\/ in deflate_monitor_using_JT() ensure memory consistency.\n+        \/\/ in deflate_monitor() ensure memory consistency.\n@@ -2293,1 +2289,1 @@\n-        \/\/ deflate_monitor_using_JT() ensure memory consistency.\n+        \/\/ deflate_monitor() ensure memory consistency.\n@@ -2328,1 +2324,2 @@\n-      if (SafepointMechanism::should_process(self) &&\n+      if (self->is_Java_thread() &&\n+          SafepointMechanism::should_process(self->as_Java_thread()) &&\n@@ -2370,1 +2367,4 @@\n-void ObjectSynchronizer::deflate_idle_monitors_using_JT() {\n+\/\/ This function is called by the ServiceThread to deflate monitors.\n+\/\/ It is also called by do_final_audit_and_print_stats() by the VMThread.\n+void ObjectSynchronizer::deflate_idle_monitors() {\n+  Thread* self = Thread::current();\n@@ -2372,1 +2372,1 @@\n-  deflate_global_idle_monitors_using_JT();\n+  deflate_global_idle_monitors(self);\n@@ -2380,1 +2380,1 @@\n-      deflate_per_thread_idle_monitors_using_JT(jt);\n+      deflate_per_thread_idle_monitors(self, jt);\n@@ -2397,3 +2397,5 @@\n-  \/\/ The ServiceThread's async deflation request has been processed.\n-  _last_async_deflation_time_ns = os::javaTimeNanos();\n-  set_is_async_deflation_requested(false);\n+  if (self->is_Java_thread()) {\n+    \/\/ The async deflation request has been processed.\n+    _last_async_deflation_time_ns = os::javaTimeNanos();\n+    set_is_async_deflation_requested(false);\n+  }\n@@ -2427,3 +2429,6 @@\n-    \/\/ Will execute a safepoint if !ThreadLocalHandshakes:\n-    HandshakeForDeflation hfd_hc;\n-    Handshake::execute(&hfd_hc);\n+    if (self->is_Java_thread()) {\n+      \/\/ A JavaThread needs to handshake in order to safely free the\n+      \/\/ monitors that were deflated in this cycle.\n+      HandshakeForDeflation hfd_hc;\n+      Handshake::execute(&hfd_hc);\n+   }\n@@ -2438,1 +2443,1 @@\n-\/\/ Deflate global idle ObjectMonitors using a JavaThread.\n+\/\/ Deflate global idle ObjectMonitors.\n@@ -2440,4 +2445,2 @@\n-void ObjectSynchronizer::deflate_global_idle_monitors_using_JT() {\n-  JavaThread* self = JavaThread::current();\n-\n-  deflate_common_idle_monitors_using_JT(true \/* is_global *\/, self);\n+void ObjectSynchronizer::deflate_global_idle_monitors(Thread* self) {\n+  deflate_common_idle_monitors(self, true \/* is_global *\/, NULL \/* target *\/);\n@@ -2446,1 +2449,1 @@\n-\/\/ Deflate the specified JavaThread's idle ObjectMonitors using a JavaThread.\n+\/\/ Deflate the specified JavaThread's idle ObjectMonitors.\n@@ -2448,4 +2451,3 @@\n-void ObjectSynchronizer::deflate_per_thread_idle_monitors_using_JT(JavaThread* target) {\n-  assert(Thread::current()->is_Java_thread(), \"precondition\");\n-\n-  deflate_common_idle_monitors_using_JT(false \/* !is_global *\/, target);\n+void ObjectSynchronizer::deflate_per_thread_idle_monitors(Thread* self,\n+                                                          JavaThread* target) {\n+  deflate_common_idle_monitors(self, false \/* !is_global *\/, target);\n@@ -2454,1 +2456,1 @@\n-\/\/ Deflate global or per-thread idle ObjectMonitors using a JavaThread.\n+\/\/ Deflate global or per-thread idle ObjectMonitors.\n@@ -2456,3 +2458,3 @@\n-void ObjectSynchronizer::deflate_common_idle_monitors_using_JT(bool is_global, JavaThread* target) {\n-  JavaThread* self = JavaThread::current();\n-\n+void ObjectSynchronizer::deflate_common_idle_monitors(Thread* self,\n+                                                      bool is_global,\n+                                                      JavaThread* target) {\n@@ -2479,4 +2481,4 @@\n-          deflate_monitor_list_using_JT(&om_list_globals._in_use_list,\n-                                        &om_list_globals._in_use_count,\n-                                        &free_head_p, &free_tail_p,\n-                                        &saved_mid_in_use_p);\n+          deflate_monitor_list(self, &om_list_globals._in_use_list,\n+                               &om_list_globals._in_use_count,\n+                               &free_head_p, &free_tail_p,\n+                               &saved_mid_in_use_p);\n@@ -2485,3 +2487,3 @@\n-          deflate_monitor_list_using_JT(&target->om_in_use_list,\n-                                        &target->om_in_use_count, &free_head_p,\n-                                        &free_tail_p, &saved_mid_in_use_p);\n+          deflate_monitor_list(self, &target->om_in_use_list,\n+                               &target->om_in_use_count, &free_head_p,\n+                               &free_tail_p, &saved_mid_in_use_p);\n@@ -2513,1 +2515,1 @@\n-      \/\/ deflate_monitor_list_using_JT() detected a safepoint starting.\n+      \/\/ deflate_monitor_list() detected a safepoint starting.\n@@ -2521,2 +2523,4 @@\n-        assert(SafepointMechanism::should_process(self), \"sanity check\");\n-        ThreadBlockInVM blocker(self);\n+        assert(self->is_Java_thread() &&\n+               SafepointMechanism::should_process(self->as_Java_thread()),\n+               \"sanity check\");\n+        ThreadBlockInVM blocker(self->as_Java_thread());\n@@ -2971,0 +2975,21 @@\n+\/\/ Do the final audit and print of ObjectMonitor stats; must be done\n+\/\/ by the VMThread (at VM exit time).\n+void ObjectSynchronizer::do_final_audit_and_print_stats() {\n+  assert(Thread::current()->is_VM_thread(), \"sanity check\");\n+\n+  if (is_final_audit()) {  \/\/ Only do the audit once.\n+    return;\n+  }\n+  set_is_final_audit();\n+\n+  if (log_is_enabled(Info, monitorinflation)) {\n+    \/\/ Do a deflation in order to reduce the in-use monitor population\n+    \/\/ that is reported by ObjectSynchronizer::log_in_use_monitor_details()\n+    \/\/ which is called by ObjectSynchronizer::audit_and_print_stats().\n+    ObjectSynchronizer::deflate_idle_monitors();\n+    \/\/ The other audit_and_print_stats() call is done at the Debug\n+    \/\/ level at a safepoint in ObjectSynchronizer::do_safepoint_work().\n+    ObjectSynchronizer::audit_and_print_stats(true \/* on_exit *\/);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":96,"deletions":71,"binary":false,"changes":167,"status":"modified"},{"patch":"@@ -132,4 +132,6 @@\n-  static void deflate_idle_monitors_using_JT();\n-  static void deflate_global_idle_monitors_using_JT();\n-  static void deflate_per_thread_idle_monitors_using_JT(JavaThread* target);\n-  static void deflate_common_idle_monitors_using_JT(bool is_global, JavaThread* target);\n+  static void deflate_idle_monitors();\n+  static void deflate_global_idle_monitors(Thread* self);\n+  static void deflate_per_thread_idle_monitors(Thread* self,\n+                                               JavaThread* target);\n+  static void deflate_common_idle_monitors(Thread* self, bool is_global,\n+                                           JavaThread* target);\n@@ -138,9 +140,7 @@\n-  \/\/ monitors using a JavaThread.\n-  static int deflate_monitor_list_using_JT(ObjectMonitor** list_p,\n-                                           int* count_p,\n-                                           ObjectMonitor** free_head_p,\n-                                           ObjectMonitor** free_tail_p,\n-                                           ObjectMonitor** saved_mid_in_use_p);\n-  static bool deflate_monitor_using_JT(ObjectMonitor* mid,\n-                                       ObjectMonitor** free_head_p,\n-                                       ObjectMonitor** free_tail_p);\n+  \/\/ monitors.\n+  static int deflate_monitor_list(Thread* self, ObjectMonitor** list_p,\n+                                  int* count_p, ObjectMonitor** free_head_p,\n+                                  ObjectMonitor** free_tail_p,\n+                                  ObjectMonitor** saved_mid_in_use_p);\n+  static bool deflate_monitor(ObjectMonitor* mid, ObjectMonitor** free_head_p,\n+                              ObjectMonitor** free_tail_p);\n@@ -149,0 +149,2 @@\n+  static bool is_final_audit() { return _is_final_audit; }\n+  static void set_is_final_audit() { _is_final_audit = true; }\n@@ -150,1 +152,1 @@\n-  static bool request_deflate_idle_monitors();  \/\/ for whitebox test support and VM exit logging\n+  static bool request_deflate_idle_monitors();  \/\/ for whitebox test support\n@@ -172,0 +174,1 @@\n+  static void do_final_audit_and_print_stats();\n@@ -185,0 +188,1 @@\n+  static volatile bool _is_final_audit;\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":18,"deletions":14,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+#include \"prims\/jvmtiDeferredUpdates.hpp\"\n@@ -123,0 +124,1 @@\n+#include \"utilities\/spinYield.hpp\"\n@@ -264,1 +266,0 @@\n-  _num_nested_signal = 0;\n@@ -537,82 +538,1 @@\n-\/\/\n-\/\/ The bits parameter returns information about the code path through\n-\/\/ the routine. Useful for debugging:\n-\/\/\n-\/\/ set in is_ext_suspend_completed():\n-\/\/ 0x00000001 - routine was entered\n-\/\/ 0x00000010 - routine return false at end\n-\/\/ 0x00000100 - thread exited (return false)\n-\/\/ 0x00000200 - suspend request cancelled (return false)\n-\/\/ 0x00000400 - thread suspended (return true)\n-\/\/ 0x00001000 - thread is in a suspend equivalent state (return true)\n-\/\/ 0x00002000 - thread is native and walkable (return true)\n-\/\/ 0x00004000 - thread is native_trans and walkable (needed retry)\n-\/\/\n-\/\/ set in wait_for_ext_suspend_completion():\n-\/\/ 0x00010000 - routine was entered\n-\/\/ 0x00020000 - suspend request cancelled before loop (return false)\n-\/\/ 0x00040000 - thread suspended before loop (return true)\n-\/\/ 0x00080000 - suspend request cancelled in loop (return false)\n-\/\/ 0x00100000 - thread suspended in loop (return true)\n-\/\/ 0x00200000 - suspend not completed during retry loop (return false)\n-\n-\/\/ Helper class for tracing suspend wait debug bits.\n-\/\/\n-\/\/ 0x00000100 indicates that the target thread exited before it could\n-\/\/ self-suspend which is not a wait failure. 0x00000200, 0x00020000 and\n-\/\/ 0x00080000 each indicate a cancelled suspend request so they don't\n-\/\/ count as wait failures either.\n-#define DEBUG_FALSE_BITS (0x00000010 | 0x00200000)\n-\n-class TraceSuspendDebugBits : public StackObj {\n- private:\n-  JavaThread * jt;\n-  bool         is_wait;\n-  bool         called_by_wait;  \/\/ meaningful when !is_wait\n-  uint32_t *   bits;\n-\n- public:\n-  TraceSuspendDebugBits(JavaThread *_jt, bool _is_wait, bool _called_by_wait,\n-                        uint32_t *_bits) {\n-    jt             = _jt;\n-    is_wait        = _is_wait;\n-    called_by_wait = _called_by_wait;\n-    bits           = _bits;\n-  }\n-\n-  ~TraceSuspendDebugBits() {\n-    if (!is_wait) {\n-#if 1\n-      \/\/ By default, don't trace bits for is_ext_suspend_completed() calls.\n-      \/\/ That trace is very chatty.\n-      return;\n-#else\n-      if (!called_by_wait) {\n-        \/\/ If tracing for is_ext_suspend_completed() is enabled, then only\n-        \/\/ trace calls to it from wait_for_ext_suspend_completion()\n-        return;\n-      }\n-#endif\n-    }\n-\n-    if (AssertOnSuspendWaitFailure || TraceSuspendWaitFailures) {\n-      if (bits != NULL && (*bits & DEBUG_FALSE_BITS) != 0) {\n-        MutexLocker ml(Threads_lock);  \/\/ needed for get_thread_name()\n-        ResourceMark rm;\n-\n-        tty->print_cr(\n-                      \"Failed wait_for_ext_suspend_completion(thread=%s, debug_bits=%x)\",\n-                      jt->get_thread_name(), *bits);\n-\n-        guarantee(!AssertOnSuspendWaitFailure, \"external suspend wait failed\");\n-      }\n-    }\n-  }\n-};\n-#undef DEBUG_FALSE_BITS\n-\n-\n-bool JavaThread::is_ext_suspend_completed(bool called_by_wait, int delay,\n-                                          uint32_t *bits) {\n-  TraceSuspendDebugBits tsdb(this, false \/* !is_wait *\/, called_by_wait, bits);\n-\n+bool JavaThread::is_ext_suspend_completed() {\n@@ -622,2 +542,0 @@\n-  *bits |= 0x00000001;\n-\n@@ -630,1 +548,0 @@\n-      *bits |= 0x00000100;\n@@ -638,1 +555,0 @@\n-      *bits |= 0x00000200;\n@@ -644,1 +560,0 @@\n-      *bits |= 0x00000400;\n@@ -672,1 +587,0 @@\n-      *bits |= 0x00001000;\n@@ -679,2 +593,1 @@\n-      *bits |= 0x00002000;\n-    } else if (!called_by_wait && !did_trans_retry &&\n+    } else if (!did_trans_retry &&\n@@ -688,3 +601,1 @@\n-      \/\/ code check above and the self-suspend. Lucky us. If we were\n-      \/\/ called by wait_for_ext_suspend_completion(), then it\n-      \/\/ will be doing the retries so we don't have to.\n+      \/\/ code check above and the self-suspend.\n@@ -698,2 +609,0 @@\n-      *bits |= 0x00004000;\n-\n@@ -719,1 +628,1 @@\n-          SR_lock()->wait(i * delay);\n+          SR_lock()->wait(i * SuspendRetryDelay);\n@@ -721,1 +630,1 @@\n-          SR_lock()->wait_without_safepoint_check(i * delay);\n+          SR_lock()->wait_without_safepoint_check(i * SuspendRetryDelay);\n@@ -732,2 +641,0 @@\n-\n-\n@@ -737,1 +644,0 @@\n-  *bits |= 0x00000010;\n@@ -741,119 +647,0 @@\n-\/\/ Wait for an external suspend request to complete (or be cancelled).\n-\/\/ Returns true if the thread is externally suspended and false otherwise.\n-\/\/\n-bool JavaThread::wait_for_ext_suspend_completion(int retries, int delay,\n-                                                 uint32_t *bits) {\n-  TraceSuspendDebugBits tsdb(this, true \/* is_wait *\/,\n-                             false \/* !called_by_wait *\/, bits);\n-\n-  \/\/ local flag copies to minimize SR_lock hold time\n-  bool is_suspended;\n-  bool pending;\n-  uint32_t reset_bits;\n-\n-  \/\/ set a marker so is_ext_suspend_completed() knows we are the caller\n-  *bits |= 0x00010000;\n-\n-  \/\/ We use reset_bits to reinitialize the bits value at the top of\n-  \/\/ each retry loop. This allows the caller to make use of any\n-  \/\/ unused bits for their own marking purposes.\n-  reset_bits = *bits;\n-\n-  {\n-    MutexLocker ml(SR_lock(), Mutex::_no_safepoint_check_flag);\n-    is_suspended = is_ext_suspend_completed(true \/* called_by_wait *\/,\n-                                            delay, bits);\n-    pending = is_external_suspend();\n-  }\n-  \/\/ must release SR_lock to allow suspension to complete\n-\n-  if (!pending) {\n-    \/\/ A cancelled suspend request is the only false return from\n-    \/\/ is_ext_suspend_completed() that keeps us from entering the\n-    \/\/ retry loop.\n-    *bits |= 0x00020000;\n-    return false;\n-  }\n-\n-  if (is_suspended) {\n-    *bits |= 0x00040000;\n-    return true;\n-  }\n-\n-  for (int i = 1; i <= retries; i++) {\n-    *bits = reset_bits;  \/\/ reinit to only track last retry\n-\n-    \/\/ We used to do an \"os::yield_all(i)\" call here with the intention\n-    \/\/ that yielding would increase on each retry. However, the parameter\n-    \/\/ is ignored on Linux which means the yield didn't scale up. Waiting\n-    \/\/ on the SR_lock below provides a much more predictable scale up for\n-    \/\/ the delay. It also provides a simple\/direct point to check for any\n-    \/\/ safepoint requests from the VMThread\n-\n-    {\n-      Thread* t = Thread::current();\n-      MonitorLocker ml(SR_lock(),\n-                       t->is_Java_thread() ? Mutex::_safepoint_check_flag : Mutex::_no_safepoint_check_flag);\n-      \/\/ wait with safepoint check (if we're a JavaThread - the WatcherThread\n-      \/\/ can also call this)  and increase delay with each retry\n-      ml.wait(i * delay);\n-\n-      is_suspended = is_ext_suspend_completed(true \/* called_by_wait *\/,\n-                                              delay, bits);\n-\n-      \/\/ It is possible for the external suspend request to be cancelled\n-      \/\/ (by a resume) before the actual suspend operation is completed.\n-      \/\/ Refresh our local copy to see if we still need to wait.\n-      pending = is_external_suspend();\n-    }\n-\n-    if (!pending) {\n-      \/\/ A cancelled suspend request is the only false return from\n-      \/\/ is_ext_suspend_completed() that keeps us from staying in the\n-      \/\/ retry loop.\n-      *bits |= 0x00080000;\n-      return false;\n-    }\n-\n-    if (is_suspended) {\n-      *bits |= 0x00100000;\n-      return true;\n-    }\n-  } \/\/ end retry loop\n-\n-  \/\/ thread did not suspend after all our retries\n-  *bits |= 0x00200000;\n-  return false;\n-}\n-\n-\/\/ Called from API entry points which perform stack walking. If the\n-\/\/ associated JavaThread is the current thread, then wait_for_suspend\n-\/\/ is not used. Otherwise, it determines if we should wait for the\n-\/\/ \"other\" thread to complete external suspension. (NOTE: in future\n-\/\/ releases the suspension mechanism should be reimplemented so this\n-\/\/ is not necessary.)\n-\/\/\n-bool\n-JavaThread::is_thread_fully_suspended(bool wait_for_suspend, uint32_t *bits) {\n-  if (this != Thread::current()) {\n-    \/\/ \"other\" threads require special handling.\n-    if (wait_for_suspend) {\n-      \/\/ We are allowed to wait for the external suspend to complete\n-      \/\/ so give the other thread a chance to get suspended.\n-      if (!wait_for_ext_suspend_completion(SuspendRetryCount,\n-                                           SuspendRetryDelay, bits)) {\n-        \/\/ Didn't make it so let the caller know.\n-        return false;\n-      }\n-    }\n-    \/\/ We aren't allowed to wait for the external suspend to complete\n-    \/\/ so if the other thread isn't externally suspended we need to\n-    \/\/ let the caller know.\n-    else if (!is_ext_suspend_completed_with_lock(bits)) {\n-      return false;\n-    }\n-  }\n-\n-  return true;\n-}\n-\n@@ -1707,1 +1494,1 @@\n-  _deferred_locals_updates(nullptr),\n+  _jvmti_deferred_updates(nullptr),\n@@ -1908,2 +1695,2 @@\n-  GrowableArray<jvmtiDeferredLocalVariableSet*>* deferred = deferred_locals();\n-  if (deferred != NULL) {\n+  JvmtiDeferredUpdates* updates = deferred_updates();\n+  if (updates != NULL) {\n@@ -1911,8 +1698,4 @@\n-    assert(deferred->length() != 0, \"empty array!\");\n-    do {\n-      jvmtiDeferredLocalVariableSet* dlv = deferred->at(0);\n-      deferred->remove_at(0);\n-      \/\/ individual jvmtiDeferredLocalVariableSet are CHeapObj's\n-      delete dlv;\n-    } while (deferred->length() != 0);\n-    delete deferred;\n+    assert(updates->count() > 0, \"Updates holder not deleted\");\n+    \/\/ free deferred updates.\n+    delete updates;\n+    set_deferred_updates(NULL);\n@@ -2410,0 +2193,5 @@\n+  if (is_obj_deopt_suspend()) {\n+    frame_anchor()->make_walkable(this);\n+    wait_for_object_deoptimization();\n+  }\n+\n@@ -2491,1 +2279,1 @@\n-    uint32_t debug_bits = 0;\n+\n@@ -2495,2 +2283,1 @@\n-    if (is_ext_suspend_completed(false \/* !called_by_wait *\/,\n-                                 SuspendRetryDelay, &debug_bits)) {\n+    if (is_ext_suspend_completed()) {\n@@ -2550,3 +2337,1 @@\n-    \/\/ flag is not cleared until we set the ext_suspended flag so\n-    \/\/ that wait_for_ext_suspend_completion() returns consistent\n-    \/\/ results.\n+    \/\/ flag is not cleared until we set the ext_suspended flag.\n@@ -2610,3 +2395,0 @@\n-  \/\/ Since we are not using a regular thread-state transition helper here,\n-  \/\/ we must manually emit the instruction barrier after leaving a safe state.\n-  OrderAccess::cross_modify_fence();\n@@ -2618,0 +2400,57 @@\n+\/\/ Wait for another thread to perform object reallocation and relocking on behalf of\n+\/\/ this thread.\n+\/\/ This method is very similar to JavaThread::java_suspend_self_with_safepoint_check()\n+\/\/ and has the same callers. It also performs a raw thread state transition to\n+\/\/ _thread_blocked and back again to the original state before returning. The current\n+\/\/ thread is required to change to _thread_blocked in order to be seen to be\n+\/\/ safepoint\/handshake safe whilst suspended and only after becoming handshake safe,\n+\/\/ the other thread can complete the handshake used to synchronize with this thread\n+\/\/ and then perform the reallocation and relocking. We cannot use the thread state\n+\/\/ transition helpers because we arrive here in various states and also because the\n+\/\/ helpers indirectly call this method.  After leaving _thread_blocked we have to\n+\/\/ check for safepoint\/handshake, except if _thread_in_native. The thread is safe\n+\/\/ without blocking then. Allowed states are enumerated in\n+\/\/ SafepointSynchronize::block(). See also EscapeBarrier::sync_and_suspend_*()\n+\n+void JavaThread::wait_for_object_deoptimization() {\n+  assert(!has_last_Java_frame() || frame_anchor()->walkable(), \"should have walkable stack\");\n+  assert(this == Thread::current(), \"invariant\");\n+  JavaThreadState state = thread_state();\n+\n+  bool spin_wait = os::is_MP();\n+  do {\n+    set_thread_state(_thread_blocked);\n+    \/\/ Check if _external_suspend was set in the previous loop iteration.\n+    if (is_external_suspend()) {\n+      java_suspend_self();\n+    }\n+    \/\/ Wait for object deoptimization if requested.\n+    if (spin_wait) {\n+      \/\/ A single deoptimization is typically very short. Microbenchmarks\n+      \/\/ showed 5% better performance when spinning.\n+      const uint spin_limit = 10 * SpinYield::default_spin_limit;\n+      SpinYield spin(spin_limit);\n+      for (uint i = 0; is_obj_deopt_suspend() && i < spin_limit; i++) {\n+        spin.wait();\n+      }\n+      \/\/ Spin just once\n+      spin_wait = false;\n+    } else {\n+      MonitorLocker ml(this, EscapeBarrier_lock, Monitor::_no_safepoint_check_flag);\n+      if (is_obj_deopt_suspend()) {\n+        ml.wait();\n+      }\n+    }\n+    \/\/ The current thread could have been suspended again. We have to check for\n+    \/\/ suspend after restoring the saved state. Without this the current thread\n+    \/\/ might return to _thread_in_Java and execute bytecode.\n+    set_thread_state_fence(state);\n+\n+    if (state != _thread_in_native) {\n+      SafepointMechanism::process_if_requested(this);\n+    }\n+    \/\/ A handshake for obj. deoptimization suspend could have been processed so\n+    \/\/ we must check after processing.\n+  } while (is_obj_deopt_suspend() || is_external_suspend());\n+}\n+\n@@ -2647,0 +2486,4 @@\n+  if (thread->is_obj_deopt_suspend()) {\n+    thread->wait_for_object_deoptimization();\n+  }\n+\n@@ -2672,20 +2515,0 @@\n-\/\/ This is a variant of the normal\n-\/\/ check_special_condition_for_native_trans with slightly different\n-\/\/ semantics for use by critical native wrappers.  It does all the\n-\/\/ normal checks but also performs the transition back into\n-\/\/ thread_in_Java state.  This is required so that critical natives\n-\/\/ can potentially block and perform a GC if they are the last thread\n-\/\/ exiting the GCLocker.\n-void JavaThread::check_special_condition_for_native_trans_and_transition(JavaThread *thread) {\n-  check_special_condition_for_native_trans(thread);\n-\n-  \/\/ Finish the transition\n-  thread->set_thread_state(_thread_in_Java);\n-\n-  if (thread->do_critical_native_unlock()) {\n-    ThreadInVMfromJavaNoAsyncException tiv(thread);\n-    GCLocker::unlock_critical(thread);\n-    thread->clear_critical_native_unlock();\n-  }\n-}\n-\n@@ -2810,1 +2633,1 @@\n-  GrowableArray<jvmtiDeferredLocalVariableSet*>* list = deferred_locals();\n+  GrowableArray<jvmtiDeferredLocalVariableSet*>* list = JvmtiDeferredUpdates::deferred_locals(this);\n@@ -4392,0 +4215,3 @@\n+\n+  \/\/ Make new thread known to active EscapeBarrier\n+  EscapeBarrier::thread_added(p);\n@@ -4432,0 +4258,3 @@\n+\n+    \/\/ Notify threads waiting in EscapeBarriers\n+    EscapeBarrier::thread_removed(p);\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":90,"deletions":261,"binary":false,"changes":351,"status":"modified"},{"patch":"@@ -86,1 +86,1 @@\n-class jvmtiDeferredLocalVariableSet;\n+class JvmtiDeferredUpdates;\n@@ -304,2 +304,2 @@\n-    _critical_native_unlock = 0x00000002U, \/\/ Must call back to unlock JNI critical lock\n-    _trace_flag             = 0x00000004U  \/\/ call tracing backend\n+    _trace_flag             = 0x00000004U, \/\/ call tracing backend\n+    _obj_deopt              = 0x00000008U  \/\/ suspend for object reallocation and relocking for JVMTI agent\n@@ -314,2 +314,0 @@\n-  int _num_nested_signal;\n-\n@@ -319,4 +317,0 @@\n-  void enter_signal_handler() { _num_nested_signal++; }\n-  void leave_signal_handler() { _num_nested_signal--; }\n-  bool is_inside_signal_handler() const { return _num_nested_signal > 0; }\n-\n@@ -552,5 +546,0 @@\n-  bool do_critical_native_unlock() const { return (_suspend_flags & _critical_native_unlock) != 0; }\n-\n-  inline void set_critical_native_unlock();\n-  inline void clear_critical_native_unlock();\n-\n@@ -560,0 +549,3 @@\n+  inline void set_obj_deopt_flag();\n+  inline void clear_obj_deopt_flag();\n+\n@@ -634,0 +626,2 @@\n+  bool is_obj_deopt_suspend()           { return (_suspend_flags & _obj_deopt) != 0; }\n+\n@@ -1070,5 +1064,4 @@\n-  \/\/ Because deoptimization is lazy we must save jvmti requests to set locals\n-  \/\/ in compiled frames until we deoptimize and we have an interpreter frame.\n-  \/\/ This holds the pointer to array (yeah like there might be more than one) of\n-  \/\/ description of compiled vframes that have locals that need to be updated.\n-  GrowableArray<jvmtiDeferredLocalVariableSet*>* _deferred_locals_updates;\n+  \/\/ Holds updates by JVMTI agents for compiled frames that cannot be performed immediately. They\n+  \/\/ will be carried out as soon as possible which, in most cases, is just before deoptimization of\n+  \/\/ the frame, when control returns to it.\n+  JvmtiDeferredUpdates* _jvmti_deferred_updates;\n@@ -1172,1 +1165,2 @@\n-  \/\/ uniquely identify the  speculative optimization guarded by the uncommon trap\n+  \/\/ uniquely identify the speculative optimization guarded by an uncommon trap.\n+  \/\/ See JVMCINMethodData::SPECULATION_LENGTH_BITS for further details.\n@@ -1367,0 +1361,4 @@\n+  \/\/ Synchronize with another thread that is deoptimizing objects of the\n+  \/\/ current thread, i.e. reverts optimizations based on escape analysis.\n+  void wait_for_object_deoptimization();\n+\n@@ -1391,25 +1389,1 @@\n-  \/\/ Same as check_special_condition_for_native_trans but finishes the\n-  \/\/ transition into thread_in_Java mode so that it can potentially\n-  \/\/ block.\n-  static void check_special_condition_for_native_trans_and_transition(JavaThread *thread);\n-\n-  bool is_ext_suspend_completed(bool called_by_wait, int delay, uint32_t *bits);\n-  bool is_ext_suspend_completed_with_lock(uint32_t *bits) {\n-    MutexLocker ml(SR_lock(), Mutex::_no_safepoint_check_flag);\n-    \/\/ Warning: is_ext_suspend_completed() may temporarily drop the\n-    \/\/ SR_lock to allow the thread to reach a stable thread state if\n-    \/\/ it is currently in a transient thread state.\n-    return is_ext_suspend_completed(false \/* !called_by_wait *\/,\n-                                    SuspendRetryDelay, bits);\n-  }\n-\n-  \/\/ We cannot allow wait_for_ext_suspend_completion() to run forever or\n-  \/\/ we could hang. SuspendRetryCount and SuspendRetryDelay are normally\n-  \/\/ passed as the count and delay parameters. Experiments with specific\n-  \/\/ calls to wait_for_ext_suspend_completion() can be done by passing\n-  \/\/ other values in the code. Experiments with all calls can be done\n-  \/\/ via the appropriate -XX options.\n-  bool wait_for_ext_suspend_completion(int count, int delay, uint32_t *bits);\n-\n-  \/\/ test for suspend - most (all?) of these should go away\n-  bool is_thread_fully_suspended(bool wait_for_suspend, uint32_t *bits);\n+  bool is_ext_suspend_completed();\n@@ -1426,1 +1400,1 @@\n-    return (_suspend_flags & (_external_suspend JFR_ONLY(| _trace_flag))) != 0;\n+    return (_suspend_flags & (_external_suspend | _obj_deopt JFR_ONLY(| _trace_flag))) != 0;\n@@ -1499,1 +1473,1 @@\n-            is_external_suspend() || is_trace_suspend();\n+            is_external_suspend() || is_trace_suspend() || is_obj_deopt_suspend();\n@@ -1516,2 +1490,2 @@\n-  GrowableArray<jvmtiDeferredLocalVariableSet*>* deferred_locals() const { return _deferred_locals_updates; }\n-  void set_deferred_locals(GrowableArray<jvmtiDeferredLocalVariableSet *>* vf) { _deferred_locals_updates = vf; }\n+  JvmtiDeferredUpdates* deferred_updates() const      { return _jvmti_deferred_updates; }\n+  void set_deferred_updates(JvmtiDeferredUpdates* du) { _jvmti_deferred_updates = du; }\n@@ -2156,14 +2130,0 @@\n-class SignalHandlerMark: public StackObj {\n- private:\n-  Thread* _thread;\n- public:\n-  SignalHandlerMark(Thread* t) {\n-    _thread = t;\n-    if (_thread) _thread->enter_signal_handler();\n-  }\n-  ~SignalHandlerMark() {\n-    if (_thread) _thread->leave_signal_handler();\n-    _thread = NULL;\n-  }\n-};\n-\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":23,"deletions":63,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,0 +36,1 @@\n+#include \"prims\/jvmtiDeferredUpdates.hpp\"\n@@ -67,1 +68,1 @@\n-  GrowableArray<jvmtiDeferredLocalVariableSet*>* list = thread()->deferred_locals();\n+  GrowableArray<jvmtiDeferredLocalVariableSet*>* list = JvmtiDeferredUpdates::deferred_locals(thread());\n@@ -106,1 +107,1 @@\n-  GrowableArray<jvmtiDeferredLocalVariableSet*>* deferred = thread()->deferred_locals();\n+  GrowableArray<jvmtiDeferredLocalVariableSet*>* deferred = JvmtiDeferredUpdates::deferred_locals(thread());\n@@ -120,2 +121,2 @@\n-    deferred =  new(ResourceObj::C_HEAP, mtCompiler) GrowableArray<jvmtiDeferredLocalVariableSet*> (1, mtCompiler);\n-    thread()->set_deferred_locals(deferred);\n+    JvmtiDeferredUpdates::create_for(thread());\n+    deferred = JvmtiDeferredUpdates::deferred_locals(thread());\n@@ -131,0 +132,50 @@\n+\/\/ After object deoptimization, that is object reallocation and relocking, we\n+\/\/ create deferred updates for all objects in scope. No new update will be\n+\/\/ created if a deferred update already exists. It is not easy to see how this\n+\/\/ is achieved: the deoptimized objects are in the arrays returned by locals(),\n+\/\/ expressions(), and monitors(). For each object in these arrays we create a\n+\/\/ deferred updated. If an update already exists, then it will override the\n+\/\/ corresponding deoptimized object returned in one of the arrays. So the\n+\/\/ original update is kept.\n+void compiledVFrame::create_deferred_updates_after_object_deoptimization() {\n+  \/\/ locals\n+  GrowableArray<ScopeValue*>* scopeLocals = scope()->locals();\n+  StackValueCollection* lcls = locals();\n+  if (lcls != NULL) {\n+    for (int i2 = 0; i2 < lcls->size(); i2++) {\n+      StackValue* var = lcls->at(i2);\n+      if (var->type() == T_OBJECT && scopeLocals->at(i2)->is_object()) {\n+        jvalue val;\n+        val.l = cast_from_oop<jobject>(lcls->at(i2)->get_obj()());\n+        update_local(T_OBJECT, i2, val);\n+      }\n+    }\n+  }\n+\n+  \/\/ expressions\n+  GrowableArray<ScopeValue*>* scopeExpressions = scope()->expressions();\n+  StackValueCollection* exprs = expressions();\n+  if (exprs != NULL) {\n+    for (int i2 = 0; i2 < exprs->size(); i2++) {\n+      StackValue* var = exprs->at(i2);\n+      if (var->type() == T_OBJECT && scopeExpressions->at(i2)->is_object()) {\n+        jvalue val;\n+        val.l = cast_from_oop<jobject>(exprs->at(i2)->get_obj()());\n+        update_stack(T_OBJECT, i2, val);\n+      }\n+    }\n+  }\n+\n+  \/\/ monitors\n+  GrowableArray<MonitorInfo*>* mtrs = monitors();\n+  if (mtrs != NULL) {\n+    for (int i2 = 0; i2 < mtrs->length(); i2++) {\n+      if (mtrs->at(i2)->eliminated()) {\n+        assert(!mtrs->at(i2)->owner_is_scalar_replaced(),\n+               \"reallocation failure, should not update\");\n+        update_monitor(i2, mtrs->at(i2));\n+      }\n+    }\n+  }\n+}\n+\n@@ -147,1 +198,1 @@\n-  GrowableArray<jvmtiDeferredLocalVariableSet*>* list = thread()->deferred_locals();\n+  GrowableArray<jvmtiDeferredLocalVariableSet*>* list = JvmtiDeferredUpdates::deferred_locals(thread());\n@@ -221,1 +272,1 @@\n-  GrowableArray<jvmtiDeferredLocalVariableSet*>* list = thread()->deferred_locals();\n+  GrowableArray<jvmtiDeferredLocalVariableSet*>* list = JvmtiDeferredUpdates::deferred_locals(thread());\n@@ -312,0 +363,18 @@\n+bool compiledVFrame::has_ea_local_in_scope() const {\n+  if (scope() == NULL) {\n+    \/\/ native nmethod, all objs escape\n+    assert(code()->as_nmethod()->is_native_method(), \"must be native\");\n+    return false;\n+  }\n+  return (scope()->objects() != NULL) || scope()->has_ea_local_in_scope();\n+}\n+\n+bool compiledVFrame::arg_escape() const {\n+  if (scope() == NULL) {\n+    \/\/ native nmethod, all objs escape\n+    assert(code()->as_nmethod()->is_native_method(), \"must be native\");\n+    return false;\n+  }\n+  return scope()->arg_escape();\n+}\n+\n@@ -333,0 +402,1 @@\n+  _objects_are_deoptimized = false;\n@@ -428,1 +498,5 @@\n-      MonitorInfo* new_info = new MonitorInfo((oopDesc*)val->value().l, info->lock(), info->eliminated(), info->owner_is_scalar_replaced());\n+      \/\/ Originally the owner may have been scalar replaced but as an update\n+      \/\/ exists it must have been deoptimized, i.e. reallocated to the heap, and\n+      \/\/ now it is considered not to be scalar replaced.\n+      MonitorInfo* new_info = new MonitorInfo((oopDesc*)val->value().l, info->lock(),\n+                                              info->eliminated(), false);\n","filename":"src\/hotspot\/share\/runtime\/vframe_hp.cpp","additions":82,"deletions":8,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"memory\/metaspace\/metaspaceReporter.hpp\"\n@@ -187,1 +188,1 @@\n-  MetaspaceUtils::print_report(_out, _scale, _flags);\n+  metaspace::MetaspaceReporter::print_report(_out, _scale, _flags);\n@@ -434,10 +435,0 @@\n-bool VM_Exit::doit_prologue() {\n-  if (log_is_enabled(Info, monitorinflation)) {\n-    \/\/ Do a deflation in order to reduce the in-use monitor population\n-    \/\/ that is reported by ObjectSynchronizer::log_in_use_monitor_details()\n-    \/\/ at VM exit.\n-    ObjectSynchronizer::request_deflate_idle_monitors();\n-  }\n-  return true;\n-}\n-\n@@ -466,0 +457,4 @@\n+  \/\/ The ObjectMonitor subsystem uses perf counters so do this before\n+  \/\/ we call exit_globals() so we don't run afoul of perfMemory_exit().\n+  ObjectSynchronizer::do_final_audit_and_print_stats();\n+\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.cpp","additions":6,"deletions":11,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -406,1 +406,0 @@\n-  bool doit_prologue();\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -54,1 +54,0 @@\n-#include \"memory\/metaspace\/metablock.hpp\"\n@@ -907,1 +906,1 @@\n-  c2_nonstatic_field(Node,                     _flags,                                        jushort)                               \\\n+  c2_nonstatic_field(Node,                     _flags,                                        juint)                                 \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -446,0 +446,6 @@\n+  void swap(GrowableArrayWithAllocator<E, Derived>* other) {\n+    ::swap(this->_data, other->_data);\n+    ::swap(this->_len, other->_len);\n+    ::swap(this->_max, other->_max);\n+  }\n+\n@@ -698,1 +704,1 @@\n-  GrowableArrayCHeap(int initial_max) :\n+  GrowableArrayCHeap(int initial_max = 0) :\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -529,1 +529,0 @@\n-     * @spec JPMS\n@@ -1113,1 +1112,0 @@\n-     * @spec JPMS\n@@ -1229,1 +1227,0 @@\n-     * @spec JPMS\n@@ -1264,1 +1261,0 @@\n-     * @spec JPMS\n@@ -2958,1 +2954,0 @@\n-     * @spec JPMS\n@@ -3055,1 +3050,0 @@\n-     * @spec JPMS\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Class.java","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,2 +28,1 @@\n-import jdk.internal.access.JavaLangAccess;\n-import jdk.internal.access.SharedSecrets;\n+import jdk.internal.misc.CDS;\n@@ -268,1 +267,1 @@\n-        if (LambdaProxyClassArchive.isDumpArchive()) {\n+        if (CDS.isDumpingArchive()) {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/InnerClassLambdaMetafactory.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -156,1 +156,0 @@\n-     * @spec JPMS\n@@ -220,1 +219,0 @@\n-     * @spec JPMS\n@@ -1333,1 +1331,0 @@\n-         *  @spec JPMS\n@@ -1352,1 +1349,0 @@\n-         *  @spec JPMS\n@@ -1441,1 +1437,0 @@\n-         *  @spec JPMS\n@@ -1521,1 +1516,0 @@\n-         * @spec JPMS\n@@ -1668,1 +1662,0 @@\n-         * @spec JPMS\n@@ -2320,1 +2313,0 @@\n-         * @spec JPMS\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandles.java","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -78,1 +78,0 @@\n- * @spec JPMS\n@@ -123,1 +122,0 @@\n-     * @spec JPMS\n@@ -202,1 +200,0 @@\n-     * @spec JPMS\n@@ -262,1 +259,0 @@\n-     * @spec JPMS\n@@ -426,1 +422,0 @@\n-     * @spec JPMS\n@@ -463,1 +458,0 @@\n-     * @spec JPMS\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/AccessibleObject.java","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -175,1 +175,0 @@\n-     * @spec JPMS\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Constructor.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2100,1 +2100,4 @@\n-            return types.lub(condTypes.stream().map(t -> t.baseType()).collect(List.collector()));\n+            return types.lub(condTypes.stream()\n+                        .map(t -> t.baseType())\n+                        .filter(t -> !t.hasTag(BOT))\n+                        .collect(List.collector()));\n@@ -2182,0 +2185,20 @@\n+\n+                @Override\n+                public void visitClassDef(JCClassDecl that) {\n+                    if (that.sym != null) {\n+                        \/\/ Method preFlow shouldn't visit class definitions\n+                        \/\/ that have not been entered and attributed.\n+                        \/\/ See JDK-8254557 and JDK-8203277 for more details.\n+                        super.visitClassDef(that);\n+                    }\n+                }\n+\n+                @Override\n+                public void visitLambda(JCLambda that) {\n+                    if (that.type != null) {\n+                        \/\/ Method preFlow shouldn't visit lambda expressions\n+                        \/\/ that have not been entered and attributed.\n+                        \/\/ See JDK-8254557 and JDK-8203277 for more details.\n+                        super.visitLambda(that);\n+                    }\n+                }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":24,"deletions":1,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+import jdk.vm.ci.code.CompilationRequest;\n@@ -709,0 +710,13 @@\n+    static class ErrorCreatingCompiler implements JVMCICompiler {\n+        private final RuntimeException t;\n+\n+        ErrorCreatingCompiler(RuntimeException t) {\n+            this.t = t;\n+        }\n+\n+        @Override\n+        public CompilationRequestResult compileMethod(CompilationRequest request) {\n+            throw t;\n+        }\n+    }\n+\n@@ -716,2 +730,7 @@\n-                    compiler = compilerFactory.createCompiler(this);\n-                    creatingCompiler = false;\n+                    try {\n+                        compiler = compilerFactory.createCompiler(this);\n+                    } catch (RuntimeException t) {\n+                        compiler = new ErrorCreatingCompiler(t);\n+                    } finally {\n+                        creatingCompiler = false;\n+                    }\n@@ -721,0 +740,3 @@\n+        if (compiler instanceof ErrorCreatingCompiler) {\n+            throw ((ErrorCreatingCompiler) compiler).t;\n+        }\n@@ -1128,1 +1150,2 @@\n-        return compilerToVm.attachCurrentThread(asDaemon);\n+        byte[] name = IS_IN_NATIVE_IMAGE ? Thread.currentThread().getName().getBytes() : null;\n+        return compilerToVm.attachCurrentThread(name, asDaemon);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotJVMCIRuntime.java","additions":26,"deletions":3,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import jdk.vm.ci.common.JVMCIError;\n@@ -49,0 +50,3 @@\n+\n+        int speculationLengthBits = getConstant(\"JVMCINMethodData::SPECULATION_LENGTH_BITS\", Integer.class);\n+        JVMCIError.guarantee(HotSpotSpeculationEncoding.LENGTH_BITS == speculationLengthBits, \"%d != %d\", HotSpotSpeculationEncoding.LENGTH_BITS, speculationLengthBits);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -340,0 +340,7 @@\n+\n+            if (isJDK16OrHigher()) {\n+                \/\/ Added by JDK-8173585: Intrinsify StringLatin1.indexOf(char)\n+                add(toBeInvestigated,\n+                            \"java\/lang\/StringLatin1.indexOfChar([BIII)I\");\n+            }\n+\n@@ -610,0 +617,4 @@\n+\n+        if (isJDK16OrHigher()) {\n+            add(toBeInvestigated, \"sun\/security\/provider\/SHA3.\" + shaCompressName + \"([BI)V\");\n+        }\n","filename":"src\/jdk.internal.vm.compiler\/share\/classes\/org.graalvm.compiler.hotspot.test\/src\/org\/graalvm\/compiler\/hotspot\/test\/CheckGraalIntrinsics.java","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2663,1 +2663,1 @@\n-     * @see CommentUtils.dcInfoMap\n+     * @see CommentUtils#dcInfoMap\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/toolkit\/util\/Utils.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -144,0 +144,2 @@\n+compiler\/loopstripmining\/BackedgeNodeWithOutOfLoopControl.java 8255120 generic-all\n+\n@@ -260,2 +262,1 @@\n-vmTestbase\/nsk\/jvmti\/ResourceExhausted\/resexhausted003\/TestDescription.java 6606767 generic-all\n-vmTestbase\/nsk\/jvmti\/ResourceExhausted\/resexhausted004\/TestDescription.java 6606767 generic-all\n+vmTestbase\/nsk\/jvmti\/ResourceExhausted\/resexhausted004\/TestDescription.java 8253916 linux-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -87,1 +87,2 @@\n-  gtest\/GTestWrapper.java\n+  gtest\/GTestWrapper.java \\\n+  gtest\/MetaspaceGtests.java\n@@ -349,0 +350,1 @@\n+ -runtime\/cds\/appcds\/methodHandles \\\n@@ -351,0 +353,1 @@\n+ -runtime\/cds\/appcds\/BadBSM.java \\\n@@ -354,0 +357,1 @@\n+ -runtime\/cds\/appcds\/LambdaProxyClasslist.java \\\n@@ -359,0 +363,1 @@\n+ -runtime\/cds\/appcds\/StaticArchiveWithLambda.java \\\n@@ -374,0 +379,9 @@\n+# needs -nativepath:<output>\/images\/test\/hotspot\/jtreg\/native\/\n+hotspot_metaspace = \\\n+  gtest\/MetaspaceGtests.java \\\n+  gc\/metaspace \\\n+  gc\/class_unloading \\\n+  runtime\/Metaspace \\\n+  vmTestbase\/metaspace \\\n+  runtime\/SelectionResolution\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -0,0 +1,577 @@\n+\/*\n+ * Copyright (c) 2020 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test\n+ * @bug 8230956\n+ * @summary JVMTI agents can obtain references to not escaping objects using JVMTI Heap functions.\n+ *          Therefore optimizations based on escape analysis have to be reverted,\n+ *          i.e. scalar replaced objects need to be reallocated on the heap and objects with eliminated locking\n+ *          need to be relocked.\n+ * @requires ((vm.compMode == \"Xmixed\") & vm.compiler2.enabled & vm.jvmti)\n+ * @library \/test\/lib \/test\/hotspot\/jtreg\n+ * @build sun.hotspot.WhiteBox\n+ * @run driver ClassFileInstaller sun.hotspot.WhiteBox\n+ * @compile IterateHeapWithEscapeAnalysisEnabled.java\n+ *\n+ * @comment BLOCK BEGIN EXCLUSIVE TESTCASES {\n+ *\n+ *          The following test cases are executed in fresh VMs because they require that the\n+ *          capability can_tag_objects is not taken until dontinline_testMethod is jit compiled and\n+ *          an activation of the compiled version is on stack of the target thread.\n+ *\n+ *          Without JDK-8227745 these test cases require that escape analysis is disabled at\n+ *          start-up because can_tag_objects can be taken lazily, potentially after loading an\n+ *          agent dynamically by means of the attach API. Disabling escape analysis and invalidating\n+ *          compiled methods does not help then because there may be compiled frames with ea-based\n+ *          optimizations on stack. Just like in this collection of test cases.\n+ *\n+ * @run main\/othervm\/native\n+ *                  -agentlib:IterateHeapWithEscapeAnalysisEnabled\n+ *                  -XX:+UnlockDiagnosticVMOptions\n+ *                  -Xms256m -Xmx256m\n+ *                  -XX:+PrintCompilation -XX:+PrintInlining\n+ *                  -XX:+WhiteBoxAPI -Xbootclasspath\/a:.\n+ *                  -Xbatch\n+ *                  -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                  -XX:+DoEscapeAnalysis\n+ *                  IterateHeapWithEscapeAnalysisEnabled IterateOverReachableObjects\n+ * @run main\/othervm\/native\n+ *                  -agentlib:IterateHeapWithEscapeAnalysisEnabled\n+ *                  -XX:+UnlockDiagnosticVMOptions\n+ *                  -Xms256m -Xmx256m\n+ *                  -XX:+PrintCompilation -XX:+PrintInlining\n+ *                  -XX:+WhiteBoxAPI -Xbootclasspath\/a:.\n+ *                  -Xbatch\n+ *                  -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                  -XX:+DoEscapeAnalysis\n+ *                  IterateHeapWithEscapeAnalysisEnabled IterateOverHeap\n+ * @run main\/othervm\/native\n+ *                  -agentlib:IterateHeapWithEscapeAnalysisEnabled\n+ *                  -XX:+UnlockDiagnosticVMOptions\n+ *                  -Xms256m -Xmx256m\n+ *                  -XX:+PrintCompilation -XX:+PrintInlining\n+ *                  -XX:+WhiteBoxAPI -Xbootclasspath\/a:.\n+ *                  -Xbatch\n+ *                  -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                  -XX:+DoEscapeAnalysis\n+ *                  IterateHeapWithEscapeAnalysisEnabled IterateOverInstancesOfClass\n+ * @run main\/othervm\/native\n+ *                  -agentlib:IterateHeapWithEscapeAnalysisEnabled\n+ *                  -XX:+UnlockDiagnosticVMOptions\n+ *                  -Xms256m -Xmx256m\n+ *                  -XX:+PrintCompilation -XX:+PrintInlining\n+ *                  -XX:+WhiteBoxAPI -Xbootclasspath\/a:.\n+ *                  -Xbatch\n+ *                  -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                  -XX:+DoEscapeAnalysis\n+ *                  IterateHeapWithEscapeAnalysisEnabled FollowReferences\n+ * @run main\/othervm\/native\n+ *                  -agentlib:IterateHeapWithEscapeAnalysisEnabled\n+ *                  -XX:+UnlockDiagnosticVMOptions\n+ *                  -Xms256m -Xmx256m\n+ *                  -XX:+PrintCompilation -XX:+PrintInlining\n+ *                  -XX:+WhiteBoxAPI -Xbootclasspath\/a:.\n+ *                  -Xbatch\n+ *                  -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                  -XX:+DoEscapeAnalysis\n+ *                  IterateHeapWithEscapeAnalysisEnabled IterateThroughHeap\n+ *\n+ * @comment } BLOCK END EXCLUSIVE TESTCASES\n+ *\n+ * @comment BLOCK BEGIN NON EXCLUSIVE TESTCASES {\n+ *\n+ *\n+ * @comment } BLOCK END NON EXCLUSIVE TESTCASES\n+ *\/\n+\n+import compiler.whitebox.CompilerWhiteBoxTest;\n+import jdk.test.lib.Asserts;\n+import sun.hotspot.WhiteBox;\n+\n+public class IterateHeapWithEscapeAnalysisEnabled {\n+\n+    public static final WhiteBox WB = WhiteBox.getWhiteBox();\n+\n+    public static final int COMPILE_THRESHOLD = CompilerWhiteBoxTest.THRESHOLD;\n+\n+    public static native int jvmtiTagClass(Class<?> cls, long tag);\n+\n+    \/\/ Methods to tag or count instances of a given class available in JVMTI\n+    public static enum TaggingAndCountingMethods {\n+        IterateOverReachableObjects,\n+        IterateOverHeap,\n+        IterateOverInstancesOfClass,\n+        FollowReferences,\n+        IterateThroughHeap\n+    }\n+\n+    public static native int acquireCanTagObjectsCapability();\n+    public static native int registerMethod(TaggingAndCountingMethods m, String name);\n+    public static native void agentTearDown();\n+\n+    \/**\n+     * Count and tag instances of a given class.\n+     * @param cls Used by the method {@link TaggingAndCountingMethods#IterateOverInstancesOfClass} as class to count and tag instances of.\n+     *        Ignored by other counting methods.\n+     * @param clsTag Tag of the class to count and tag instances of. Used by all methods except\n+     *        {@link TaggingAndCountingMethods#IterateOverInstancesOfClass}\n+     * @param instanceTag The tag to be set for selected instances.\n+     * @param method JVMTI counting and tagging method to be used.\n+     * @return The number of instances or -1 if the call fails.\n+     *\/\n+    public static native int countAndTagInstancesOfClass(Class<?> cls, long clsTag, long instanceTag, TaggingAndCountingMethods method);\n+\n+    \/**\n+     * Get all objects tagged with the given tag.\n+     * @param tag The tag used to select objects.\n+     * @param result Selected objects are copied into this array.\n+     * @return -1 to indicated failure and 0 for success.\n+     *\/\n+    public static native int getObjectsWithTag(long tag, Object[] result);\n+\n+    public static void main(String[] args) throws Exception {\n+        try {\n+            new IterateHeapWithEscapeAnalysisEnabled().runTestCases(args);\n+        } finally {\n+            agentTearDown();\n+        }\n+    }\n+\n+    public void runTestCases(String[] args) throws Exception {\n+        \/\/ register various instance tagging and counting methods with agent\n+        for (TaggingAndCountingMethods m : TaggingAndCountingMethods.values()) {\n+            msg(\"register instance count method \" + m.name());\n+            int rc = registerMethod(m, m.name());\n+            Asserts.assertGreaterThanOrEqual(rc, 0, \"method \" + m.name() + \" is unknown to agent\");\n+        }\n+\n+        if (args.length > 0) {\n+            \/\/ EXCLUSIVE TEST CASES\n+            \/\/ cant_tag_objects is acquired after warmup. Use given tagging\/counting method.\n+            new TestCase01(true, 100, TaggingAndCountingMethods.valueOf(args[0])).run();\n+        } else {\n+            \/\/ NON-EXCLUSIVE TEST CASES\n+            \/\/ cant_tag_objects is acquired before test cases are run but still during live phase.\n+            msgHL(\"Acquire capability can_tag_objects before first test case.\");\n+            int err = acquireCanTagObjectsCapability();\n+            Asserts.assertEQ(0, err, \"acquireCanTagObjectsCapability FAILED\");\n+\n+            \/\/ run test cases\n+            for (TaggingAndCountingMethods m : TaggingAndCountingMethods.values()) {\n+                new TestCase01(false, 200, m).run();\n+            }\n+            new TestCase02a(200).run();\n+            new TestCase02b(300).run();\n+        }\n+    }\n+\n+    static class ABBox {\n+        public int aVal;\n+        public int bVal;\n+        public TestCaseBase testCase;\n+\n+        public ABBox() { \/* empty *\/ }\n+\n+        public ABBox(TestCaseBase testCase) {\n+            this.testCase = testCase;\n+        }\n+\n+        \/**\n+         * Increment {@link #aVal} and {@link #bVal} under lock. The method is supposed to\n+         * be inlined into the test method and locking is supposed to be eliminated. After\n+         * this object escaped to the JVMTI agent, the code with eliminated locking must\n+         * not be used anymore.\n+         *\/\n+        public synchronized void synchronizedSlowInc() {\n+            aVal++;\n+            testCase.waitingForCheck = true;\n+            dontinline_waitForCheck(testCase);\n+            testCase.waitingForCheck = false;\n+            bVal++;\n+        }\n+\n+        public static void dontinline_waitForCheck(TestCaseBase testCase) {\n+            if (testCase.warmUpDone) {\n+                while(!testCase.checkingNow) {\n+                    try {\n+                        Thread.sleep(50);\n+                    } catch (InterruptedException e) { \/*ign*\/ }\n+                }\n+            }\n+        }\n+\n+        \/**\n+         * This method and incrementing {@link #aVal} and {@link #bVal} are synchronized.\n+         * So {@link #aVal} and {@link #bVal} should always be equal. Unless the optimized version\n+         * of {@link #synchronizedSlowInc()} without locking is still used after this object\n+         * escaped to the JVMTI agent.\n+         * @return\n+         *\/\n+        public synchronized boolean check() {\n+            return aVal == bVal;\n+        }\n+    }\n+\n+    public static abstract class TestCaseBase implements Runnable {\n+        public final long classTag;\n+        public long instanceTag;\n+\n+        public final Class<?> taggedClass;\n+\n+        public long checkSum;\n+        public long loopCount;\n+        public volatile boolean doLoop;\n+        public volatile boolean targetIsInLoop;\n+\n+        public volatile boolean waitingForCheck;\n+        public volatile boolean checkingNow;\n+\n+        public boolean warmUpDone;\n+\n+        public TestCaseBase(long classTag, Class<?> taggedClass) {\n+            this.classTag = classTag;\n+            this.taggedClass = taggedClass;\n+        }\n+\n+        public void setUp() {\n+            \/\/ Tag the class of instances to be scalar replaced\n+            msg(\"tagging \" + taggedClass.getName() + \" with tag \" +  classTag);\n+            int err = jvmtiTagClass(taggedClass, classTag);\n+            Asserts.assertEQ(0, err, \"jvmtiTagClass FAILED\");\n+        }\n+\n+        \/\/ to be overridden by test cases\n+        abstract public void dontinline_testMethod();\n+\n+        public void warmUp() {\n+            msg(\"WarmUp: START\");\n+            int callCount = COMPILE_THRESHOLD + 1000;\n+            doLoop = true;\n+            while (callCount-- > 0) {\n+                dontinline_testMethod();\n+            }\n+            warmUpDone = true;\n+            msg(\"WarmUp: DONE\");\n+        }\n+\n+        public Object dontinline_endlessLoop(Object argEscape) {\n+            long cs = checkSum;\n+            while (loopCount-- > 0 && doLoop) {\n+                targetIsInLoop = true;\n+                checkSum += checkSum % ++cs;\n+            }\n+            loopCount = 3;\n+            targetIsInLoop = false;\n+            return argEscape;\n+        }\n+\n+        public void waitUntilTargetThreadHasEnteredEndlessLoop() {\n+            while(!targetIsInLoop) {\n+                msg(\"Target has not yet entered the loop. Sleep 100ms.\");\n+                try { Thread.sleep(100); } catch (InterruptedException e) { \/*ignore *\/ }\n+            }\n+            msg(\"Target has entered the loop.\");\n+        }\n+\n+        public void terminateEndlessLoop() throws Exception {\n+            msg(\"Terminate endless loop\");\n+            doLoop = false;\n+        }\n+    }\n+\n+    \/**\n+     * Use JVMTI heap functions associated with the elements of {@link TaggingAndCountingMethods} to\n+     * get a reference to an object allocated in {@link TestCase01#dontinline_testMethod()}. The\n+     * allocation can be eliminated \/ scalar replaced.  The test case can be run in two modes: (1)\n+     * the capability can_tag_objects which is required to use the JVMTI heap functions is taken\n+     * before the test case (2) the capability is taken after {@link TestCase01#dontinline_testMethod()}\n+     * is compiled and the target thread has an activation of it on stack.\n+     *\/\n+    public static class TestCase01 extends TestCaseBase {\n+\n+        public volatile int testMethod_result;\n+        public boolean acquireCanTagObjectsCapabilityAfterWarmup;\n+        public TaggingAndCountingMethods taggingMethod;\n+\n+        public TestCase01(boolean acquireCanTagObjectsCapabilityAfterWarmup, long classTag, TaggingAndCountingMethods taggingMethod) {\n+            super(classTag, ABBox.class);\n+            instanceTag = classTag + 1;\n+            this.acquireCanTagObjectsCapabilityAfterWarmup = acquireCanTagObjectsCapabilityAfterWarmup;\n+            this.taggingMethod = taggingMethod;\n+        }\n+\n+        @Override\n+        public void setUp() {\n+            if (!acquireCanTagObjectsCapabilityAfterWarmup) {\n+                super.setUp();\n+            }\n+        }\n+\n+        public void setUpAfterWarmUp() {\n+            if (acquireCanTagObjectsCapabilityAfterWarmup) {\n+                msg(\"Acquire capability can_tag_objects \" + (warmUpDone ? \"after\" : \"before\") + \" warmup.\");\n+                int err = acquireCanTagObjectsCapability();\n+                Asserts.assertEQ(0, err, \"acquireCanTagObjectsCapability FAILED\");\n+                super.setUp();\n+            }\n+        }\n+\n+        public void run() {\n+            try {\n+                msgHL(getClass().getName() + \": test if object that may be scalar replaced is found using \" + taggingMethod);\n+                msg(\"The capability can_tag_object is acquired \" + (acquireCanTagObjectsCapabilityAfterWarmup ? \"AFTER\" : \"BEFORE\")\n+                        + \" warmup.\");\n+                setUp();\n+                warmUp();\n+                WB.deflateIdleMonitors();\n+                WB.fullGC(); \/\/ get rid of dead instances from previous test cases\n+                runTest(taggingMethod);\n+            } catch (Exception e) {\n+                Asserts.fail(\"Unexpected Exception\", e);\n+            }\n+        }\n+\n+        public void runTest(TaggingAndCountingMethods m) throws Exception {\n+            loopCount = 1L << 62; \/\/ endless loop\n+            doLoop = true;\n+            testMethod_result = 0;\n+            Thread t1 = new Thread(() -> dontinline_testMethod(), \"Target Thread (\" + getClass().getName() + \")\");\n+            try {\n+                t1.start();\n+                try {\n+                    waitUntilTargetThreadHasEnteredEndlessLoop();\n+                    setUpAfterWarmUp();\n+                    msg(\"count and tag instances of \" + taggedClass.getName() + \" with tag \" + instanceTag + \" using JVMTI \" + m.name());\n+                    int count = countAndTagInstancesOfClass(taggedClass, classTag, instanceTag, m);\n+                    msg(\"Done. Count is \" + count);\n+                    Asserts.assertGreaterThanOrEqual(count, 0, \"countAndTagInstancesOfClass FAILED\");\n+                    Asserts.assertEQ(count, 1, \"unexpected number of instances\");\n+\n+                    ABBox[] result = new ABBox[1];\n+                    msg(\"get instances tagged with \" + instanceTag + \". The instances escape thereby.\");\n+                    int err = getObjectsWithTag(instanceTag, result);\n+                    msg(\"Done.\");\n+                    Asserts.assertEQ(0, err, \"getObjectsWithTag FAILED\");\n+\n+                    msg(\"change the now escaped instance' bVal\");\n+                    ABBox abBox = result[0];\n+                    abBox.bVal = 3;\n+                    terminateEndlessLoop();\n+\n+                    msg(\"wait until target thread has set testMethod_result\");\n+                    while (testMethod_result == 0) {\n+                        Thread.sleep(50);\n+                    }\n+                    msg(\"check if the modification of bVal is reflected in testMethod_result.\");\n+                    Asserts.assertEQ(7, testMethod_result, \" testMethod_result has wrong value\");\n+                    msg(\"ok.\");\n+                } finally {\n+                    terminateEndlessLoop();\n+                }\n+            } finally {\n+                t1.join();\n+            }\n+        }\n+\n+        @Override\n+        public void dontinline_testMethod() {\n+            ABBox ab = new ABBox();     \/\/ can be scalar replaced\n+            ab.aVal = 4;\n+            ab.bVal = 2;\n+            dontinline_endlessLoop(null);   \/\/ JVMTI agent acquires reference to ab and changes bVal\n+            testMethod_result = ab.aVal + ab.bVal;\n+        }\n+    }\n+\n+    \/**\n+     * {@link #dontinline_testMethod()} creates an ArgEscape instance of {@link TestCaseBase#taggedClass} on stack.\n+     * The jvmti agent tags all instances of this class using one of the {@link TaggingAndCountingMethods}. Then it gets the tagged\n+     * instances using <code>GetObjectsWithTags()<\/code>. This is where the ArgEscape globally escapes.\n+     * It happens at a location without eliminated locking but there is\n+     * eliminated locking following, so the compiled frame must be deoptimized. This is checked by letting the agent call the\n+     * synchronized method {@link ABBox#check()} on the escaped instance.\n+     *\/\n+    public static class TestCase02a extends TestCaseBase {\n+\n+        public long instanceTag;\n+\n+        public TestCase02a(long classTag) {\n+            super(classTag, ABBox.class);\n+            instanceTag = classTag + 1;\n+        }\n+\n+        public void run() {\n+            try {\n+                msgHL(getClass().getName() + \": test if owning frame is deoptimized if ArgEscape escapes globally\");\n+                setUp();\n+                warmUp();\n+                for (TaggingAndCountingMethods m : TaggingAndCountingMethods.values()) {\n+                    msgHL(getClass().getName() + \": Tag and Get of ArgEscapes using \" + m.name());\n+                    waitingForCheck = false;\n+                    checkingNow = false;\n+                    WB.deflateIdleMonitors();\n+                    WB.fullGC(); \/\/ get rid of dead instances from previous test cases\n+                    runTest(m);\n+                }\n+            } catch (Exception e) {\n+                Asserts.fail(\"Unexpected Exception\", e);\n+            }\n+        }\n+\n+        public void runTest(TaggingAndCountingMethods m) throws Exception {\n+            loopCount = 1L << 62; \/\/ endless loop\n+            doLoop = true;\n+            Thread t1 = new Thread(() -> dontinline_testMethod(), \"Target Thread (\" + getClass().getName() + \")\");\n+            try {\n+                t1.start();\n+                try {\n+                    waitUntilTargetThreadHasEnteredEndlessLoop();\n+                    msg(\"count and tag instances of \" + taggedClass.getName() + \" with tag \" + instanceTag + \" using JVMTI \" + m.name());\n+                    int count = countAndTagInstancesOfClass(taggedClass, classTag, instanceTag, m);\n+                    msg(\"Done. Count is \" + count);\n+                    Asserts.assertGreaterThanOrEqual(count, 0, \"countAndTagInstancesOfClass FAILED\");\n+                    Asserts.assertEQ(count, 1, \"unexpected number of instances\");\n+                } finally {\n+                    terminateEndlessLoop();\n+                }\n+\n+                ABBox[] result = new ABBox[1];\n+                msg(\"get instances tagged with \" + instanceTag);\n+                int err = getObjectsWithTag(instanceTag, result);\n+                msg(\"Done.\");\n+                Asserts.assertEQ(0, err, \"getObjectsWithTag FAILED\");\n+\n+                ABBox abBoxArgEscape = result[0];\n+                while (!waitingForCheck) {\n+                    Thread.yield();\n+                }\n+                msg(\"Check abBoxArgEscape's state is consistent\");\n+                checkingNow = true;\n+                Asserts.assertTrue(abBoxArgEscape.check(), \"Detected inconsistent state. abBoxArgEscape.aVal != abBoxArgEscape.bVal\");\n+                msg(\"Ok.\");\n+            } finally {\n+                checkingNow = true;\n+                t1.join();\n+            }\n+        }\n+\n+        @Override\n+        public void dontinline_testMethod() {\n+            ABBox ab = new ABBox(this);\n+            dontinline_endlessLoop(ab);\n+            ab.synchronizedSlowInc();\n+        }\n+    }\n+\n+    \/**\n+     * Like {@link TestCase02a}, with the exception that at the location in {@link #dontinline_testMethod()} where the\n+     * ArgEscape escapes it is not referenced by a local variable.\n+     *\/\n+    public static class TestCase02b extends TestCaseBase {\n+\n+        public long instanceTag;\n+\n+        public TestCase02b(long classTag) {\n+            super(classTag, ABBox.class);\n+            instanceTag = classTag + 1;\n+        }\n+\n+        public void run() {\n+            try {\n+                msgHL(getClass().getName() + \": test if owning frame is deoptimized if ArgEscape escapes globally\");\n+                setUp();\n+                warmUp();\n+                for (TaggingAndCountingMethods m : TaggingAndCountingMethods.values()) {\n+                    msgHL(getClass().getName() + \": Tag and Get of ArgEscapes using \" + m.name());\n+                    waitingForCheck = false;\n+                    checkingNow = false;\n+                    WB.deflateIdleMonitors();\n+                    WB.fullGC(); \/\/ get rid of dead instances from previous test cases\n+                    runTest(m);\n+                }\n+            } catch (Exception e) {\n+                Asserts.fail(\"Unexpected Exception\", e);\n+            }\n+        }\n+\n+        public void runTest(TaggingAndCountingMethods m) throws Exception {\n+            loopCount = 1L << 62; \/\/ endless loop\n+            doLoop = true;\n+            Thread t1 = new Thread(() -> dontinline_testMethod(), \"Target Thread (\" + getClass().getName() + \")\");\n+            try {\n+                t1.start();\n+                try {\n+                    waitUntilTargetThreadHasEnteredEndlessLoop();\n+                    msg(\"count and tag instances of \" + taggedClass.getName() + \" with tag \" + instanceTag + \" using JVMTI \" + m.name());\n+                    int count = countAndTagInstancesOfClass(taggedClass, classTag, instanceTag, m);\n+                    msg(\"Done. Count is \" + count);\n+                    Asserts.assertGreaterThanOrEqual(count, 0, \"countAndTagInstancesOfClass FAILED\");\n+                    Asserts.assertEQ(count, 1, \"unexpected number of instances\");\n+                } finally {\n+                    terminateEndlessLoop();\n+                }\n+\n+                ABBox[] result = new ABBox[1];\n+                msg(\"get instances tagged with \" + instanceTag);\n+                int err = getObjectsWithTag(instanceTag, result);\n+                msg(\"Done.\");\n+                Asserts.assertEQ(0, err, \"getObjectsWithTag FAILED\");\n+\n+                ABBox abBoxArgEscape = result[0];\n+                while (!waitingForCheck) {\n+                    Thread.yield();\n+                }\n+                msg(\"Check abBoxArgEscape's state is consistent\");\n+                checkingNow = true;\n+                Asserts.assertTrue(abBoxArgEscape.check(), \"Detected inconsistent state. abBoxArgEscape.aVal != abBoxArgEscape.bVal\");\n+                msg(\"Ok.\");\n+            } finally {\n+                checkingNow = true;\n+                t1.join();\n+            }\n+        }\n+\n+        @Override\n+        public void dontinline_testMethod() {\n+            \/\/ The new instance is an ArgEscape instance and escapes to the JVMTI agent\n+            \/\/ while the target thread is in the call to dontinline_endlessLoop(). At this\n+            \/\/ location there is no local variable that references the ArgEscape.\n+            ((ABBox) dontinline_endlessLoop(new ABBox(this))).synchronizedSlowInc();;\n+        }\n+    }\n+\n+    public static void msg(String m) {\n+        System.out.println();\n+        System.out.println(\"### \" + m);\n+        System.out.println();\n+    }\n+\n+    public static void msgHL(String m) {\n+        System.out.println(); System.out.println(); System.out.println();\n+        System.out.println(\"#####################################################\");\n+        System.out.println(\"### \" + m);\n+        System.out.println(\"###\");\n+        System.out.println();\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/serviceability\/jvmti\/Heap\/IterateHeapWithEscapeAnalysisEnabled.java","additions":577,"deletions":0,"binary":false,"changes":577,"status":"added"},{"patch":"@@ -0,0 +1,3143 @@\n+\/*\n+ * Copyright (c) 2020 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/**\n+ * @test\n+ * @bug 8227745\n+ * @summary Collection of test cases that check if optimizations based on escape analysis are reverted just before non-escaping objects escape through JVMTI.\n+ * @author Richard Reingruber richard DOT reingruber AT sap DOT com\n+ *\n+ * @requires ((vm.compMode == \"Xmixed\") & vm.compiler2.enabled)\n+ * @library \/test\/lib \/test\/hotspot\/jtreg\n+ *\n+ * @run build TestScaffold VMConnection TargetListener TargetAdapter sun.hotspot.WhiteBox\n+ * @run driver ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run compile -g EATests.java\n+ * @run driver EATests\n+ *                 -XX:+UnlockDiagnosticVMOptions\n+ *                 -Xms256m -Xmx256m\n+ *                 -Xbootclasspath\/a:.\n+ *                 -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                 -XX:+WhiteBoxAPI\n+ *                 -Xbatch\n+ *                 -XX:+DoEscapeAnalysis -XX:+EliminateAllocations -XX:+EliminateLocks -XX:+EliminateNestedLocks -XX:-UseBiasedLocking\n+ * @run driver EATests\n+ *                 -XX:+UnlockDiagnosticVMOptions\n+ *                 -Xms256m -Xmx256m\n+ *                 -Xbootclasspath\/a:.\n+ *                 -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                 -XX:+WhiteBoxAPI\n+ *                 -Xbatch\n+ *                 -XX:+DoEscapeAnalysis -XX:-EliminateAllocations -XX:+EliminateLocks -XX:+EliminateNestedLocks -XX:-UseBiasedLocking\n+ * @run driver EATests\n+ *                 -XX:+UnlockDiagnosticVMOptions\n+ *                 -Xms256m -Xmx256m\n+ *                 -Xbootclasspath\/a:.\n+ *                 -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                 -XX:+WhiteBoxAPI\n+ *                 -Xbatch\n+ *                 -XX:-DoEscapeAnalysis -XX:-EliminateAllocations -XX:+EliminateLocks -XX:+EliminateNestedLocks -XX:-UseBiasedLocking\n+ *\n+ * @comment Excercise -XX:+DeoptimizeObjectsALot. Mostly to prevent bit-rot because the option is meant to stress object deoptimization\n+ *          with non-synthetic workloads.\n+ * @run driver EATests\n+ *                 -XX:+UnlockDiagnosticVMOptions\n+ *                 -Xms256m -Xmx256m\n+ *                 -Xbootclasspath\/a:.\n+ *                 -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                 -XX:+WhiteBoxAPI\n+ *                 -Xbatch\n+ *                 -XX:-DoEscapeAnalysis -XX:-EliminateAllocations -XX:+EliminateLocks -XX:+EliminateNestedLocks -XX:-UseBiasedLocking\n+ *                 -XX:+IgnoreUnrecognizedVMOptions -XX:+DeoptimizeObjectsALot\n+ *\n+ *\/\n+\/**\n+ * @test\n+ * @bug 8227745\n+ *\n+ * @summary This is another configuration of EATests.java to test Graal. Some testcases are expected\n+ *          to fail because Graal does not provide all information about non-escaping objects in\n+ *          scope. These are skipped.\n+ *\n+ * @author Richard Reingruber richard DOT reingruber AT sap DOT com\n+ *\n+ * @requires ((vm.compMode == \"Xmixed\") & vm.jvmci)\n+ *\n+ * @library \/test\/lib \/test\/hotspot\/jtreg\n+ *\n+ * @run build TestScaffold VMConnection TargetListener TargetAdapter sun.hotspot.WhiteBox\n+ * @run driver ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run compile -g EATests.java\n+ *\n+ * @comment Test with Graal. Some testcases are expected to fail because Graal does not provide all information about non-escaping\n+ *          objects in scope. These are skipped.\n+ * @run driver EATests\n+ *                 -XX:+UnlockDiagnosticVMOptions\n+ *                 -Xms256m -Xmx256m\n+ *                 -Xbootclasspath\/a:.\n+ *                 -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                 -XX:+WhiteBoxAPI\n+ *                 -Xbatch\n+ *                 -XX:+UnlockExperimentalVMOptions -XX:+UseJVMCICompiler\n+ *\/\n+\n+import com.sun.jdi.*;\n+import com.sun.jdi.event.*;\n+import compiler.testlibrary.CompilerUtils;\n+import compiler.whitebox.CompilerWhiteBoxTest;\n+\n+import java.lang.reflect.Array;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+\n+import jdk.test.lib.Asserts;\n+import sun.hotspot.WhiteBox;\n+\n+\n+\/\/\n+\/\/ ANALYZING TEST FAILURES\n+\/\/\n+\/\/ - Executing just a single test case with the property EATests.onlytestcase.\n+\/\/\n+\/\/      Example: java -DEATests.onlytestcase=<test case name> ... EATests\n+\/\/\n+\/\/ - Interactive execution allows for attaching a native debugger, e.g. gdb\n+\/\/\n+\/\/      Example: java -DEATests.interactive=true ... EATests\n+\/\/\n+\/\/ - Java arguments to the test are passed as vm options to the debuggee:\n+\/\/\n+\/\/      Example: java ... EATests -XX:+UseNewCode\n+\/\/\n+\n+\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ Shared base class for test cases for both, debugger and debuggee.\n+\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+class EATestCaseBaseShared {\n+    \/\/ In interactive mode we wait for a keypress before every test case.\n+    public static final boolean INTERACTIVE =\n+            System.getProperty(\"EATests.interactive\") != null &&\n+            System.getProperty(\"EATests.interactive\").equals(\"true\");\n+\n+    \/\/ If the property is given, then just the test case it refers to is executed.\n+    \/\/ Use it to diagnose test failures.\n+    public static final String RUN_ONLY_TEST_CASE_PROPERTY = \"EATests.onlytestcase\";\n+    public static final String RUN_ONLY_TEST_CASE = System.getProperty(RUN_ONLY_TEST_CASE_PROPERTY);\n+\n+    public final String testCaseName;\n+\n+    public EATestCaseBaseShared() {\n+        String clName = getClass().getName();\n+        int tidx = clName.lastIndexOf(\"Target\");\n+        testCaseName = tidx > 0 ? clName.substring(0, tidx) : clName;\n+    }\n+\n+    public boolean shouldSkip() {\n+        return EATestCaseBaseShared.RUN_ONLY_TEST_CASE != null &&\n+               EATestCaseBaseShared.RUN_ONLY_TEST_CASE.length() > 0 &&\n+               !testCaseName.equals(EATestCaseBaseShared.RUN_ONLY_TEST_CASE);\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ Target main class, i.e. the program to be debugged.\n+\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+class EATestsTarget {\n+\n+    public static void main(String[] args) {\n+        EATestCaseBaseTarget.staticSetUp();\n+        EATestCaseBaseTarget.staticSetUpDone();\n+\n+        \/\/ Materializing test cases, i.e. reallocating objects on the heap\n+        new EAMaterializeLocalVariableUponGetTarget()                                       .run();\n+        new EAGetWithoutMaterializeTarget()                                                 .run();\n+        new EAMaterializeLocalAtObjectReturnTarget()                                        .run();\n+        new EAMaterializeLocalAtObjectPollReturnReturnTarget()                              .run();\n+        new EAMaterializeIntArrayTarget()                                                   .run();\n+        new EAMaterializeLongArrayTarget()                                                  .run();\n+        new EAMaterializeFloatArrayTarget()                                                 .run();\n+        new EAMaterializeDoubleArrayTarget()                                                .run();\n+        new EAMaterializeObjectArrayTarget()                                                .run();\n+        new EAMaterializeObjectWithConstantAndNotConstantValuesTarget()                     .run();\n+        new EAMaterializeObjReferencedBy2LocalsTarget()                                     .run();\n+        new EAMaterializeObjReferencedBy2LocalsAndModifyTarget()                            .run();\n+        new EAMaterializeObjReferencedBy2LocalsInDifferentVirtFramesTarget()                .run();\n+        new EAMaterializeObjReferencedBy2LocalsInDifferentVirtFramesAndModifyTarget()       .run();\n+        new EAMaterializeObjReferencedFromOperandStackTarget()                              .run();\n+        new EAMaterializeLocalVariableUponGetAfterSetIntegerTarget()                        .run();\n+\n+        \/\/ Relocking test cases\n+        new EARelockingSimpleTarget()                                                       .run();\n+        new EARelockingSimple_2Target()                                                     .run();\n+        new EARelockingRecursiveTarget()                                                    .run();\n+        new EARelockingNestedInflatedTarget()                                               .run();\n+        new EARelockingNestedInflated_02Target()                                            .run();\n+        new EARelockingArgEscapeLWLockedInCalleeFrameTarget()                               .run();\n+        new EARelockingArgEscapeLWLockedInCalleeFrame_2Target()                             .run();\n+        new EARelockingArgEscapeLWLockedInCalleeFrame_3Target()                             .run();\n+        new EARelockingArgEscapeLWLockedInCalleeFrame_4Target()                             .run();\n+        new EAGetOwnedMonitorsTarget()                                                      .run();\n+        new EAEntryCountTarget()                                                            .run();\n+        new EARelockingObjectCurrentlyWaitingOnTarget()                                     .run();\n+\n+        \/\/ Test cases that require deoptimization even though neither\n+        \/\/ locks nor allocations are eliminated at the point where\n+        \/\/ escape state is changed.\n+        new EADeoptFrameAfterReadLocalObject_01Target()                                     .run();\n+        new EADeoptFrameAfterReadLocalObject_01BTarget()                                    .run();\n+        new EADeoptFrameAfterReadLocalObject_02Target()                                     .run();\n+        new EADeoptFrameAfterReadLocalObject_02BTarget()                                    .run();\n+        new EADeoptFrameAfterReadLocalObject_02CTarget()                                    .run();\n+        new EADeoptFrameAfterReadLocalObject_03Target()                                     .run();\n+\n+        \/\/ PopFrame test cases\n+        new EAPopFrameNotInlinedTarget()                                                    .run();\n+        new EAPopFrameNotInlinedReallocFailureTarget()                                      .run();\n+        new EAPopInlinedMethodWithScalarReplacedObjectsReallocFailureTarget()               .run();\n+\n+        \/\/ ForceEarlyReturn test cases\n+        new EAForceEarlyReturnNotInlinedTarget()                                            .run();\n+        new EAForceEarlyReturnOfInlinedMethodWithScalarReplacedObjectsTarget()              .run();\n+        new EAForceEarlyReturnOfInlinedMethodWithScalarReplacedObjectsReallocFailureTarget().run();\n+\n+        \/\/ Instances of ReferenceType\n+        new EAGetInstancesOfReferenceTypeTarget()                                           .run();\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ Debugger main class\n+\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+public class EATests extends TestScaffold {\n+\n+    public TargetVMOptions targetVMOptions;\n+    public ThreadReference targetMainThread;\n+\n+    EATests(String args[]) {\n+        super(args);\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (EATestCaseBaseShared.RUN_ONLY_TEST_CASE != null) {\n+            args = Arrays.copyOf(args, args.length + 1);\n+            args[args.length - 1] = \"-D\" + EATestCaseBaseShared.RUN_ONLY_TEST_CASE_PROPERTY + \"=\" + EATestCaseBaseShared.RUN_ONLY_TEST_CASE;\n+        }\n+        new EATests(args).startTests();\n+    }\n+\n+    public static class TargetVMOptions {\n+\n+        public final boolean UseJVMCICompiler;\n+        public final boolean EliminateAllocations;\n+        public final boolean DeoptimizeObjectsALot;\n+        public final boolean DoEscapeAnalysis;\n+\n+        public TargetVMOptions(EATests env, ClassType testCaseBaseTargetClass) {\n+            Value val;\n+            val = testCaseBaseTargetClass.getValue(testCaseBaseTargetClass.fieldByName(\"DoEscapeAnalysis\"));\n+            DoEscapeAnalysis = ((PrimitiveValue) val).booleanValue();\n+            \/\/ Escape analysis is a prerequisite for scalar replacement (EliminateAllocations)\n+            val = testCaseBaseTargetClass.getValue(testCaseBaseTargetClass.fieldByName(\"EliminateAllocations\"));\n+            EliminateAllocations = DoEscapeAnalysis && ((PrimitiveValue) val).booleanValue();\n+            val = testCaseBaseTargetClass.getValue(testCaseBaseTargetClass.fieldByName(\"DeoptimizeObjectsALot\"));\n+            DeoptimizeObjectsALot = ((PrimitiveValue) val).booleanValue();\n+            val = testCaseBaseTargetClass.getValue(testCaseBaseTargetClass.fieldByName(\"UseJVMCICompiler\"));\n+            UseJVMCICompiler = ((PrimitiveValue) val).booleanValue();\n+        }\n+\n+    }\n+\n+    \/\/ Execute known test cases\n+    protected void runTests() throws Exception {\n+        String targetProgName = EATestsTarget.class.getName();\n+        msg(\"starting to main method in class \" +  targetProgName);\n+        startToMain(targetProgName);\n+        msg(\"resuming to EATestCaseBaseTarget.staticSetUpDone()V\");\n+        targetMainThread = resumeTo(\"EATestCaseBaseTarget\", \"staticSetUpDone\", \"()V\").thread();\n+        Location loc = targetMainThread.frame(0).location();\n+        Asserts.assertEQ(\"staticSetUpDone\", loc.method().name());\n+\n+        targetVMOptions = new TargetVMOptions(this, (ClassType) loc.declaringType());\n+\n+        \/\/ Materializing test cases, i.e. reallocating objects on the heap\n+        new EAMaterializeLocalVariableUponGet()                                       .run(this);\n+        new EAGetWithoutMaterialize()                                                 .run(this);\n+        new EAMaterializeLocalAtObjectReturn()                                        .run(this);\n+        new EAMaterializeLocalAtObjectPollReturnReturn()                              .run(this);\n+        new EAMaterializeIntArray()                                                   .run(this);\n+        new EAMaterializeLongArray()                                                  .run(this);\n+        new EAMaterializeFloatArray()                                                 .run(this);\n+        new EAMaterializeDoubleArray()                                                .run(this);\n+        new EAMaterializeObjectArray()                                                .run(this);\n+        new EAMaterializeObjectWithConstantAndNotConstantValues()                     .run(this);\n+        new EAMaterializeObjReferencedBy2Locals()                                     .run(this);\n+        new EAMaterializeObjReferencedBy2LocalsAndModify()                            .run(this);\n+        new EAMaterializeObjReferencedBy2LocalsInDifferentVirtFrames()                .run(this);\n+        new EAMaterializeObjReferencedBy2LocalsInDifferentVirtFramesAndModify()       .run(this);\n+        new EAMaterializeObjReferencedFromOperandStack()                              .run(this);\n+        new EAMaterializeLocalVariableUponGetAfterSetInteger()                        .run(this);\n+\n+        \/\/ Relocking test cases\n+        new EARelockingSimple()                                                       .run(this);\n+        new EARelockingSimple_2()                                                     .run(this);\n+        new EARelockingRecursive()                                                    .run(this);\n+        new EARelockingNestedInflated()                                               .run(this);\n+        new EARelockingNestedInflated_02()                                            .run(this);\n+        new EARelockingArgEscapeLWLockedInCalleeFrame()                               .run(this);\n+        new EARelockingArgEscapeLWLockedInCalleeFrame_2()                             .run(this);\n+        new EARelockingArgEscapeLWLockedInCalleeFrame_3()                             .run(this);\n+        new EARelockingArgEscapeLWLockedInCalleeFrame_4()                             .run(this);\n+        new EAGetOwnedMonitors()                                                      .run(this);\n+        new EAEntryCount()                                                            .run(this);\n+        new EARelockingObjectCurrentlyWaitingOn()                                     .run(this);\n+\n+        \/\/ Test cases that require deoptimization even though neither\n+        \/\/ locks nor allocations are eliminated at the point where\n+        \/\/ escape state is changed.\n+        new EADeoptFrameAfterReadLocalObject_01()                                     .run(this);\n+        new EADeoptFrameAfterReadLocalObject_01B()                                    .run(this);\n+        new EADeoptFrameAfterReadLocalObject_02()                                     .run(this);\n+        new EADeoptFrameAfterReadLocalObject_02B()                                    .run(this);\n+        new EADeoptFrameAfterReadLocalObject_02C()                                    .run(this);\n+        new EADeoptFrameAfterReadLocalObject_03()                                     .run(this);\n+\n+        \/\/ PopFrame test cases\n+        new EAPopFrameNotInlined()                                                    .run(this);\n+        new EAPopFrameNotInlinedReallocFailure()                                      .run(this);\n+        new EAPopInlinedMethodWithScalarReplacedObjectsReallocFailure()               .run(this);\n+\n+        \/\/ ForceEarlyReturn test cases\n+        new EAForceEarlyReturnNotInlined()                                            .run(this);\n+        new EAForceEarlyReturnOfInlinedMethodWithScalarReplacedObjects()              .run(this);\n+        new EAForceEarlyReturnOfInlinedMethodWithScalarReplacedObjectsReallocFailure().run(this);\n+\n+        \/\/ Instances of ReferenceType\n+        new EAGetInstancesOfReferenceType()                                           .run(this);\n+\n+        \/\/ resume the target listening for events\n+        listenUntilVMDisconnect();\n+    }\n+\n+    \/\/ Print a Message\n+    public void msg(String m) {\n+        System.out.println();\n+        System.out.println(\"###(Debugger) \" + m);\n+        System.out.println();\n+    }\n+\n+    \/\/ Highlighted message.\n+    public void msgHL(String m) {\n+        System.out.println();\n+        System.out.println();\n+        System.out.println(\"##########################################################\");\n+        System.out.println(\"### \" + m);\n+        System.out.println(\"### \");\n+        System.out.println();\n+        System.out.println();\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ Base class for debugger side of test cases.\n+\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+abstract class EATestCaseBaseDebugger  extends EATestCaseBaseShared {\n+\n+    protected EATests env;\n+\n+    public ObjectReference testCase;\n+\n+    public static final String TARGET_TESTCASE_BASE_NAME = EATestCaseBaseTarget.class.getName();\n+\n+    public static final String XYVAL_NAME = XYVal.class.getName();\n+\n+    public abstract void runTestCase() throws Exception;\n+\n+    public void run(EATests env) {\n+        this.env = env;\n+        if (shouldSkip()) {\n+            msg(\"skipping \" + testCaseName);\n+            return;\n+        }\n+        try {\n+            msgHL(\"Executing test case \" + getClass().getName());\n+            env.testFailed = false;\n+\n+            if (INTERACTIVE)\n+                env.waitForInput();\n+\n+            resumeToWarmupDone();\n+            runTestCase();\n+            Asserts.assertTrue(env.targetMainThread.isSuspended(), \"must be suspended after the testcase\");\n+            resumeToTestCaseDone();\n+            checkPostConditions();\n+        } catch (Exception e) {\n+            Asserts.fail(\"Unexpected exception in test case \" + getClass().getName(), e);\n+        }\n+    }\n+\n+    public void resumeToWarmupDone() throws Exception {\n+        msg(\"resuming to \" + TARGET_TESTCASE_BASE_NAME + \".warmupDone()V\");\n+        env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"warmupDone\", \"()V\");\n+        testCase = env.targetMainThread.frame(0).thisObject();\n+    }\n+\n+    public void resumeToTestCaseDone() {\n+        msg(\"resuming to \" + TARGET_TESTCASE_BASE_NAME + \".testCaseDone()V\");\n+        env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"testCaseDone\", \"()V\");\n+    }\n+\n+    public void checkPostConditions() throws Exception {\n+        Asserts.assertFalse(env.getExceptionCaught(), \"Uncaught exception in Debuggee\");\n+\n+        String testName = getClass().getName();\n+        if (!env.testFailed) {\n+            env.println(testName  + \": passed\");\n+        } else {\n+            throw new Exception(testName + \": failed\");\n+        }\n+    }\n+\n+    public void printStack(ThreadReference thread) throws Exception {\n+        msg(\"Debuggee Stack:\");\n+        List<StackFrame> stack_frames = thread.frames();\n+        int i = 0;\n+        for (StackFrame ff : stack_frames) {\n+            System.out.println(\"frame[\" + i++ +\"]: \" + ff.location().method() + \" (bci:\" + ff.location().codeIndex() + \")\");\n+        }\n+    }\n+\n+    public void msg(String m) {\n+        env.msg(m);\n+    }\n+\n+    public void msgHL(String m) {\n+        env.msgHL(m);\n+    }\n+\n+    \/\/ See Field Descriptors in The Java Virtual Machine Specification\n+    \/\/ (https:\/\/docs.oracle.com\/javase\/specs\/jvms\/se11\/html\/jvms-4.html#jvms-4.3.2)\n+    enum FD {\n+        I, \/\/ int\n+        J, \/\/ long\n+        F, \/\/ float\n+        D, \/\/ double\n+    }\n+\n+    \/\/ Map field descriptor to jdi type string\n+    public static final Map<FD, String> FD2JDIArrType = Map.of(FD.I, \"int[]\", FD.J, \"long[]\", FD.F, \"float[]\", FD.D, \"double[]\");\n+\n+    \/\/ Map field descriptor to PrimitiveValue getter\n+    public static final Function<PrimitiveValue, Integer> v2I = PrimitiveValue::intValue;\n+    public static final Function<PrimitiveValue, Long>    v2J = PrimitiveValue::longValue;\n+    public static final Function<PrimitiveValue, Float>   v2F = PrimitiveValue::floatValue;\n+    public static final Function<PrimitiveValue, Double>  v2D = PrimitiveValue::doubleValue;\n+    Map<FD, Function<PrimitiveValue, ?>> FD2getter = Map.of(FD.I, v2I, FD.J, v2J, FD.F, v2F, FD.D, v2D);\n+\n+    \/**\n+     * Retrieve array of primitive values referenced by a local variable in target and compare with\n+     * an array of expected values.\n+     * @param frame Frame in the target holding the local variable\n+     * @param lName Name of the local variable referencing the array to be retrieved\n+     * @param desc Array element type given as field descriptor.\n+     * @param expVals Array of expected values.\n+     * @throws Exception\n+     *\/\n+    protected void checkLocalPrimitiveArray(StackFrame frame, String lName, FD desc, Object expVals) throws Exception {\n+        String lType = FD2JDIArrType.get(desc);\n+        Asserts.assertNotNull(lType, \"jdi type not found\");\n+        Asserts.assertEQ(EATestCaseBaseTarget.TESTMETHOD_DEFAULT_NAME, frame .location().method().name());\n+        List<LocalVariable> localVars = frame.visibleVariables();\n+        msg(\"Check if the local array variable '\" + lName  + \"' in \" + EATestCaseBaseTarget.TESTMETHOD_DEFAULT_NAME + \" has the expected elements: \");\n+        boolean found = false;\n+        for (LocalVariable lv : localVars) {\n+            if (lv.name().equals(lName)) {\n+                found  = true;\n+                Value lVal = frame.getValue(lv);\n+                Asserts.assertNotNull(lVal);\n+                Asserts.assertEQ(lVal.type().name(), lType);\n+                ArrayReference aRef = (ArrayReference) lVal;\n+                Asserts.assertEQ(3, aRef.length());\n+                \/\/ now check the elements\n+                for (int i = 0; i < aRef.length(); i++) {\n+                    Object actVal = FD2getter.get(desc).apply((PrimitiveValue)aRef.getValue(i));\n+                    Object expVal = Array.get(expVals, i);\n+                    Asserts.assertEQ(expVal, actVal, \"checking element at index \" + i);\n+                }\n+            }\n+        }\n+        Asserts.assertTrue(found);\n+        msg(\"OK.\");\n+    }\n+\n+    \/**\n+     * Retrieve array of objects referenced by a local variable in target and compare with an array\n+     * of expected values.\n+     * @param frame Frame in the target holding the local variable\n+     * @param lName Name of the local variable referencing the array to be retrieved\n+     * @param lType Local type, e.g. java.lang.Long[]\n+     * @param expVals Array of expected values.\n+     * @throws Exception\n+     *\/\n+    protected void checkLocalObjectArray(StackFrame frame, String lName, String lType, ObjectReference[] expVals) throws Exception {\n+        Asserts.assertEQ(EATestCaseBaseTarget.TESTMETHOD_DEFAULT_NAME, frame .location().method().name());\n+        List<LocalVariable> localVars = frame.visibleVariables();\n+        msg(\"Check if the local array variable '\" + lName  + \"' in \" + EATestCaseBaseTarget.TESTMETHOD_DEFAULT_NAME + \" has the expected elements: \");\n+        boolean found = false;\n+        for (LocalVariable lv : localVars) {\n+            if (lv.name().equals(lName)) {\n+                found  = true;\n+                Value lVal = frame.getValue(lv);\n+                Asserts.assertNotNull(lVal);\n+                Asserts.assertEQ(lType, lVal.type().name());\n+                ArrayReference aRef = (ArrayReference) lVal;\n+                Asserts.assertEQ(3, aRef.length());\n+                \/\/ now check the elements\n+                for (int i = 0; i < aRef.length(); i++) {\n+                    ObjectReference actVal = (ObjectReference)aRef.getValue(i);\n+                    Asserts.assertSame(expVals[i], actVal, \"checking element at index \" + i);\n+                }\n+            }\n+        }\n+        Asserts.assertTrue(found);\n+        msg(\"OK.\");\n+    }\n+\n+    \/**\n+     * Retrieve a reference held by a local variable in the given frame. Check if the frame's method\n+     * is the expected method if the retrieved local value has the expected type and is not null.\n+     * @param frame The frame to retrieve the local variable value from.\n+     * @param expectedMethodName The name of the frames method should match the expectedMethodName.\n+     * @param lName The name of the local variable which is read.\n+     * @param expectedType Is the expected type of the object referenced by the local variable.\n+     * @return\n+     * @throws Exception\n+     *\/\n+    protected ObjectReference getLocalRef(StackFrame frame, String expectedMethodName, String lName, String expectedType) throws Exception {\n+        Asserts.assertEQ(expectedMethodName, frame.location().method().name());\n+        List<LocalVariable> localVars = frame.visibleVariables();\n+        msg(\"Get and check local variable '\" + lName + \"' in \" + expectedMethodName);\n+        ObjectReference lRef = null;\n+        for (LocalVariable lv : localVars) {\n+            if (lv.name().equals(lName)) {\n+                Value lVal = frame.getValue(lv);\n+                Asserts.assertNotNull(lVal);\n+                Asserts.assertEQ(expectedType, lVal.type().name());\n+                lRef = (ObjectReference) lVal;\n+                break;\n+            }\n+        }\n+        Asserts.assertNotNull(lRef, \"Local variable '\" + lName + \"' not found\");\n+        msg(\"OK.\");\n+        return lRef;\n+    }\n+\n+    \/**\n+     * Retrieve a reference held by a local variable in the given frame. Check if the frame's method\n+     * matches {@link EATestCaseBaseTarget#TESTMETHOD_DEFAULT_NAME} if the retrieved local value has\n+     * the expected type and is not null.\n+     * @param frame The frame to retrieve the local variable value from.\n+     * @param expectedMethodName The name of the frames method should match the expectedMethodName.\n+     * @param lName The name of the local variable which is read.\n+     * @param expectedType Is the expected type of the object referenced by the local variable.\n+     * @return\n+     * @throws Exception\n+     *\/\n+    protected ObjectReference getLocalRef(StackFrame frame, String lType, String lName) throws Exception {\n+        return getLocalRef(frame, EATestCaseBaseTarget.TESTMETHOD_DEFAULT_NAME, lName, lType);\n+    }\n+\n+    \/**\n+     * Set the value of a local variable in the given frame. Check if the frame's method is the expected method.\n+     * @param frame The frame holding the local variable.\n+     * @param expectedMethodName The expected name of the frame's method.\n+     * @param lName The name of the local variable to change.\n+     * @param val The new value of the local variable.\n+     * @throws Exception\n+     *\/\n+    public void setLocal(StackFrame frame, String expectedMethodName, String lName, Value val) throws Exception {\n+        Asserts.assertEQ(expectedMethodName, frame.location().method().name());\n+        List<LocalVariable> localVars = frame.visibleVariables();\n+        msg(\"Set local variable '\" + lName + \"' = \" + val + \" in \" + expectedMethodName);\n+        for (LocalVariable lv : localVars) {\n+            if (lv.name().equals(lName)) {\n+                frame.setValue(lv, val);\n+                break;\n+            }\n+        }\n+        msg(\"OK.\");\n+    }\n+\n+    \/**\n+     * Set the value of a local variable in the given frame. Check if the frame's method matches\n+     * {@link EATestCaseBaseTarget#TESTMETHOD_DEFAULT_NAME}.\n+     * @param frame The frame holding the local variable.\n+     * @param expectedMethodName The expected name of the frame's method.\n+     * @param lName The name of the local variable to change.\n+     * @param val The new value of the local variable.\n+     * @throws Exception\n+     *\/\n+    public void setLocal(StackFrame frame, String lName, Value val) throws Exception {\n+        setLocal(frame, EATestCaseBaseTarget.TESTMETHOD_DEFAULT_NAME, lName, val);\n+    }\n+\n+    \/**\n+     * Check if a field has the expected primitive value.\n+     * @param o Object holding the field.\n+     * @param desc Field descriptor.\n+     * @param fName Field name\n+     * @param expVal Expected primitive value\n+     * @throws Exception\n+     *\/\n+    protected void checkPrimitiveField(ObjectReference o, FD desc, String fName, Object expVal) throws Exception {\n+        msg(\"check field \" + fName);\n+        ReferenceType rt = o.referenceType();\n+        Field fld = rt.fieldByName(fName);\n+        Value val = o.getValue(fld);\n+        Object actVal = FD2getter.get(desc).apply((PrimitiveValue) val);\n+        Asserts.assertEQ(expVal, actVal, \"field '\" + fName + \"' has unexpected value.\");\n+        msg(\"ok\");\n+    }\n+\n+    \/**\n+     * Check if a field references the expected object.\n+     * @param obj Object holding the field.\n+     * @param fName Field name\n+     * @param expVal Object expected to be referenced by the field\n+     * @throws Exception\n+     *\/\n+    protected void checkObjField(ObjectReference obj, String fName, ObjectReference expVal) throws Exception {\n+        msg(\"check field \" + fName);\n+        ReferenceType rt = obj.referenceType();\n+        Field fld = rt.fieldByName(fName);\n+        Value actVal = obj.getValue(fld);\n+        Asserts.assertEQ(expVal, actVal, \"field '\" + fName + \"' has unexpected value.\");\n+        msg(\"ok\");\n+    }\n+\n+    protected void setField(ObjectReference obj, String fName, Value val) throws Exception {\n+        msg(\"set field \" + fName + \" = \" + val);\n+        ReferenceType rt = obj.referenceType();\n+        Field fld = rt.fieldByName(fName);\n+        obj.setValue(fld, val);\n+        msg(\"ok\");\n+    }\n+\n+    protected Value getField(ObjectReference obj, String fName) throws Exception {\n+        msg(\"get field \" + fName);\n+        ReferenceType rt = obj.referenceType();\n+        Field fld = rt.fieldByName(fName);\n+        Value val = obj.getValue(fld);\n+        msg(\"result : \" + val);\n+        return val;\n+    }\n+\n+    \/**\n+     * Free the memory consumed in the target by {@link EATestCaseBaseTarget#consumedMemory}\n+     * @throws Exception\n+     *\/\n+    public void freeAllMemory() throws Exception {\n+        msg(\"free consumed memory\");\n+        setField(testCase, \"consumedMemory\", null);\n+    }\n+\n+    \/**\n+     * @return The value of {@link EATestCaseBaseTarget#targetIsInLoop}. The target must set that field to true as soon as it\n+     *         enters the endless loop.\n+     * @throws Exception\n+     *\/\n+    public boolean targetHasEnteredEndlessLoop() throws Exception {\n+        Value v = getField(testCase, \"targetIsInLoop\");\n+        return ((PrimitiveValue) v).booleanValue();\n+    }\n+\n+    \/**\n+     * Poll {@link EATestCaseBaseTarget#targetIsInLoop} and return if it is found to be true.\n+     * @throws Exception\n+     *\/\n+    public void waitUntilTargetHasEnteredEndlessLoop() throws Exception {\n+        while(!targetHasEnteredEndlessLoop()) {\n+            msg(\"Target has not yet entered the loop. Sleep 200ms.\");\n+            try { Thread.sleep(200); } catch (InterruptedException e) { \/*ignore *\/ }\n+        }\n+    }\n+\n+    \/**\n+     * Set {@link EATestCaseBaseTarget#doLoop} to <code>false<\/code>. This will allow the target to\n+     * leave the endless loop.\n+     * @throws Exception\n+     *\/\n+    public void terminateEndlessLoop() throws Exception {\n+        msg(\"terminate loop\");\n+        setField(testCase, \"doLoop\", env.vm().mirrorOf(false));\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ Base class for debuggee side of test cases.\n+\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+abstract class EATestCaseBaseTarget extends EATestCaseBaseShared implements Runnable {\n+\n+    \/**\n+     * The target must set that field to true as soon as it enters the endless loop.\n+     *\/\n+    public volatile boolean targetIsInLoop;\n+\n+    \/**\n+     * Used for busy loops. See {@link #dontinline_endlessLoop()}.\n+     *\/\n+    public volatile long loopCount;\n+\n+    \/**\n+     * Used in {@link EATestCaseBaseDebugger#terminateEndlessLoop()} to signal target to leave the endless loop.\n+     *\/\n+    public volatile boolean doLoop;\n+\n+    public long checkSum;\n+\n+    public static final String TESTMETHOD_DEFAULT_NAME = \"dontinline_testMethod\";\n+\n+    public static final WhiteBox WB = WhiteBox.getWhiteBox();\n+\n+    public static boolean unbox(Boolean value, boolean dflt) {\n+        return value == null ? dflt : value;\n+    }\n+\n+    public static final boolean UseJVMCICompiler     = unbox(WB.getBooleanVMFlag(\"UseJVMCICompiler\"), false); \/\/ read by debugger\n+    public static final boolean DoEscapeAnalysis     = unbox(WB.getBooleanVMFlag(\"DoEscapeAnalysis\"), UseJVMCICompiler);\n+    public static final boolean EliminateAllocations = unbox(WB.getBooleanVMFlag(\"EliminateAllocations\"), UseJVMCICompiler); \/\/ read by debugger\n+    public static final boolean DeoptimizeObjectsALot   = WB.getBooleanVMFlag(\"DeoptimizeObjectsALot\");                      \/\/ read by debugger\n+    public static final long BiasedLockingBulkRebiasThreshold = WB.getIntxVMFlag(\"BiasedLockingBulkRebiasThreshold\");\n+    public static final long BiasedLockingBulkRevokeThreshold = WB.getIntxVMFlag(\"BiasedLockingBulkRevokeThreshold\");\n+\n+    public String testMethodName;\n+    public int testMethodDepth;\n+\n+    \/\/ Results produced by dontinline_testMethod()\n+    public int  iResult;\n+    public long lResult;\n+    public float  fResult;\n+    public double dResult;\n+\n+\n+    public boolean warmupDone;\n+    \/\/ With UseJVMCICompiler it is possible that a compilation is made a\n+    \/\/ background compilation even though -Xbatch is given (e.g. if JVMCI is not\n+    \/\/ yet fully initialized). Therefore it is possible that the test method has\n+    \/\/ not reached the highest compilation level after warm-up.\n+    public boolean testMethodReachedHighestCompLevel;\n+\n+    public volatile Object biasToBeRevoked;\n+\n+    \/\/ an object with an inflated monitor\n+    public static XYVal inflatedLock;\n+    public static Thread  inflatorThread;\n+    public static boolean inflatedLockIsPermanentlyInflated;\n+\n+    public static int    NOT_CONST_1I = 1;\n+    public static long   NOT_CONST_1L = 1L;\n+    public static float  NOT_CONST_1F = 1.1F;\n+    public static double NOT_CONST_1D = 1.1D;\n+\n+    public static          Long NOT_CONST_1_OBJ = Long.valueOf(1);\n+\n+\n+    public static final    Long CONST_2_OBJ     = Long.valueOf(2);\n+    public static final    Long CONST_3_OBJ     = Long.valueOf(3);\n+\n+    \/**\n+     * Main driver of a test case.\n+     * <ul>\n+     * <li> Skips test case if not selected (see {@link EATestCaseBaseShared#RUN_ONLY_TEST_CASE}\n+     * <li> Call {@link #setUp()}\n+     * <li> warm-up and compile {@link #dontinline_testMethod()} (see {@link #compileTestMethod()}\n+     * <li> calling {@link #dontinline_testMethod()}\n+     * <li> checking the result (see {@link #checkResult()}\n+     * <ul>\n+     *\/\n+    public void run() {\n+        try {\n+            if (shouldSkip()) {\n+                msg(\"skipping \" + testCaseName);\n+                return;\n+            }\n+            setUp();\n+            msg(testCaseName + \" is up and running.\");\n+            compileTestMethod();\n+            msg(testCaseName + \" warmup done.\");\n+            warmupDone();\n+            checkCompLevel();\n+            dontinline_testMethod();\n+            checkResult();\n+            msg(testCaseName + \" done.\");\n+            testCaseDone();\n+        } catch (Exception e) {\n+            Asserts.fail(\"Caught unexpected exception\", e);\n+        }\n+    }\n+\n+    public static void staticSetUp() {\n+        inflatedLock = new XYVal(1, 1);\n+        synchronized (inflatedLock) {\n+            inflatorThread = new Thread(\"Lock Inflator (test thread)\") {\n+                @Override\n+                public void run() {\n+                    synchronized (inflatedLock) {\n+                        inflatedLockIsPermanentlyInflated = true;\n+                        inflatedLock.notify(); \/\/ main thread\n+                        while (true) {\n+                            try {\n+                                \/\/ calling wait() on a monitor will cause inflation into a heavy monitor\n+                                inflatedLock.wait();\n+                            } catch (InterruptedException e) { \/* ignored *\/ }\n+                        }\n+                    }\n+                }\n+            };\n+            inflatorThread.setDaemon(true);\n+            inflatorThread.start();\n+\n+            \/\/ wait until the lock is permanently inflated by the inflatorThread\n+            while(!inflatedLockIsPermanentlyInflated) {\n+                try {\n+                    inflatedLock.wait(); \/\/ until inflated\n+                } catch (InterruptedException e1) { \/* ignored *\/ }\n+            }\n+        }\n+    }\n+\n+    \/\/ Debugger will set breakpoint here to sync with target.\n+    public static void staticSetUpDone() {\n+    }\n+\n+    public void setUp() {\n+        testMethodDepth = 1;\n+        testMethodName = TESTMETHOD_DEFAULT_NAME;\n+    }\n+\n+    public abstract void dontinline_testMethod() throws Exception;\n+\n+    public int dontinline_brkpt_iret() {\n+        dontinline_brkpt();\n+        return 42;\n+    }\n+\n+    \/**\n+     * It is a common protocol to have the debugger set a breakpoint in this method and have {@link\n+     * #dontinline_testMethod()} call it and then perform some test actions on debugger side.\n+     * After that it is checked if a frame of {@link #dontinline_testMethod()} is found at the\n+     * expected depth on stack and if it is (not) marked for deoptimization as expected.\n+     *\/\n+    public void dontinline_brkpt() {\n+        \/\/ will set breakpoint here after warmup\n+        if (warmupDone) {\n+            \/\/ check if test method is at expected depth\n+            StackTraceElement[] frames = Thread.currentThread().getStackTrace();\n+            int stackTraceDepth = testMethodDepth + 1; \/\/ ignore java.lang.Thread.getStackTrace()\n+            Asserts.assertEQ(testMethodName, frames[stackTraceDepth].getMethodName(),\n+                    testCaseName + \": test method not found at depth \" + testMethodDepth);\n+            \/\/ check if the frame is (not) deoptimized as expected\n+            if (!DeoptimizeObjectsALot) {\n+                if (testFrameShouldBeDeoptimized() && testMethodReachedHighestCompLevel) {\n+                    Asserts.assertTrue(WB.isFrameDeoptimized(testMethodDepth+1),\n+                            testCaseName + \": expected test method frame at depth \" + testMethodDepth + \" to be deoptimized\");\n+                } else {\n+                    Asserts.assertFalse(WB.isFrameDeoptimized(testMethodDepth+1),\n+                            testCaseName + \": expected test method frame at depth \" + testMethodDepth + \" not to be deoptimized\");\n+                }\n+            }\n+        }\n+    }\n+\n+    \/**\n+     * Some test cases run busy endless loops by initializing {@link #loopCount}\n+     * to {@link Long#MAX_VALUE} after warm-up and then counting down to 0 in their main test method.\n+     * During warm-up {@link #loopCount} is initialized to a small value.\n+     *\/\n+    public long dontinline_endlessLoop() {\n+        long cs = checkSum;\n+        doLoop = true;\n+        while (loopCount-- > 0 && doLoop) {\n+            targetIsInLoop = true;\n+            checkSum += checkSum % ++cs;\n+        }\n+        loopCount = 3;\n+        targetIsInLoop = false;\n+        return checkSum;\n+    }\n+\n+    public boolean testFrameShouldBeDeoptimized() {\n+        return DoEscapeAnalysis;\n+    }\n+\n+    public void warmupDone() {\n+        warmupDone = true;\n+    }\n+\n+    \/\/ Debugger will set breakpoint here to sync with target.\n+    public void testCaseDone() {\n+    }\n+\n+    public void compileTestMethod() throws Exception {\n+        int callCount = CompilerWhiteBoxTest.THRESHOLD;\n+        while (callCount-- > 0) {\n+            dontinline_testMethod();\n+        }\n+    }\n+\n+    public void checkCompLevel() {\n+        java.lang.reflect.Method m = null;\n+        try {\n+            m = getClass().getMethod(TESTMETHOD_DEFAULT_NAME);\n+        } catch (NoSuchMethodException | SecurityException e) {\n+            Asserts.fail(\"could not check compilation level of\", e);\n+        }\n+        \/\/ Background compilation (-Xbatch) cannot always be disabled with JVMCI\n+        \/\/ compiler (e.g. if JVMCI is not yet fully initialized), therefore it\n+        \/\/ is possible that due to background compilation we reach here before\n+        \/\/ the test method is compiled on the highest level.\n+        int highestLevel = CompilerUtils.getMaxCompilationLevel();\n+        int compLevel = WB.getMethodCompilationLevel(m);\n+        testMethodReachedHighestCompLevel = highestLevel == compLevel;\n+        if (!UseJVMCICompiler) {\n+            Asserts.assertEQ(highestLevel, compLevel,\n+                             m + \" not on expected compilation level\");\n+        }\n+    }\n+\n+    \/\/ to be overridden as appropriate\n+    public int getExpectedIResult() {\n+        return 0;\n+    }\n+\n+    \/\/ to be overridden as appropriate\n+    public long getExpectedLResult() {\n+        return 0;\n+    }\n+\n+    \/\/ to be overridden as appropriate\n+    public float getExpectedFResult() {\n+        return 0f;\n+    }\n+\n+    \/\/ to be overridden as appropriate\n+    public double getExpectedDResult() {\n+        return 0d;\n+    }\n+\n+    private void checkResult() {\n+        Asserts.assertEQ(getExpectedIResult(), iResult, \"checking iResult\");\n+        Asserts.assertEQ(getExpectedLResult(), lResult, \"checking lResult\");\n+        Asserts.assertEQ(getExpectedFResult(), fResult, \"checking fResult\");\n+        Asserts.assertEQ(getExpectedDResult(), dResult, \"checking dResult\");\n+    }\n+\n+    public void msg(String m) {\n+        System.out.println();\n+        System.out.println(\"###(Target) \" + m);\n+        System.out.println();\n+    }\n+\n+    \/\/ The object passed will be ArgEscape if it was NoEscape before.\n+    public final void dontinline_make_arg_escape(XYVal xy) {\n+    }\n+\n+    \/**\n+     * Call a method indirectly using reflection. The indirection is a limit for escape\n+     * analysis in the sense that the VM need not search beyond for frames that might have\n+     * an object being read by an JVMTI agent as ArgEscape.\n+     * @param receiver The receiver object of the call.\n+     * @param methodName The name of the method to be called.\n+     *\/\n+    public final void dontinline_call_with_entry_frame(Object receiver, String methodName) {\n+        Asserts.assertTrue(warmupDone, \"We want to take the slow path through jni, so don't call in warmup\");\n+\n+        Class<?> cls = receiver.getClass();\n+        Class<?>[] none = {};\n+\n+        java.lang.reflect.Method m;\n+        try {\n+            m = cls.getDeclaredMethod(methodName, none);\n+            m.invoke(receiver);\n+        } catch (Exception e) {\n+            Asserts.fail(\"Call through reflection failed\", e);\n+        }\n+    }\n+\n+    \/**\n+     * Trigger bulk rebiasing for the given class by creating new instances and calling <code> hashCode() <\/code> on them.\n+     * @param cls The class to bulk rebias\n+     *\/\n+    public void dontinline_bulkRebiasAfterWarmup(Class<?> cls) {\n+        if (warmupDone) {\n+            try {\n+                for (int i=0; i < BiasedLockingBulkRebiasThreshold+2; i++) {\n+                    biasToBeRevoked = cls.getDeclaredConstructor(int.class, int.class).newInstance(1, 1);\n+                    synchronized (biasToBeRevoked) { \/\/ bias towards current thread\n+                        checkSum++;\n+                    }\n+                    biasToBeRevoked.hashCode(); \/\/ calling hashCode triggers revocation\n+                }\n+            } catch (Throwable e) {\n+                Asserts.fail(\"failed to create new instance of \" + cls.getName(), e);\n+            }\n+        }\n+    }\n+\n+    \/**\n+     * Trigger bulk revoke of biases for the given class by creating new instances and calling <code> hashCode() <\/code> on them.\n+     * @param cls The class to bulk rebias\n+     *\/\n+    public void dontinline_bulkRevokeAfterWarmup(Class<?> cls) {\n+        if (warmupDone) {\n+            try {\n+                for (int i=0; i < BiasedLockingBulkRevokeThreshold+2; i++) {\n+                    biasToBeRevoked = cls.getDeclaredConstructor(int.class, int.class).newInstance(1, 1);\n+                    synchronized (biasToBeRevoked) { \/\/ bias towards current thread\n+                        checkSum++;\n+                    }\n+                    biasToBeRevoked.hashCode(); \/\/ calling hashCode triggers revocation\n+                }\n+            } catch (Throwable e) {\n+                Asserts.fail(\"failed to create new instance of \" + cls.getName(), e);\n+            }\n+        }\n+    }\n+\n+    static class LinkedList {\n+        LinkedList l;\n+        public long[] array;\n+        public LinkedList(LinkedList l, int size) {\n+            this.array = new long[size];\n+            this.l = l;\n+        }\n+    }\n+\n+    public LinkedList consumedMemory;\n+\n+    public void consumeAllMemory() {\n+        msg(\"consume all memory\");\n+        int size = 128 * 1024 * 1024;\n+        while(size > 0) {\n+            try {\n+                while(true) {\n+                    consumedMemory = new LinkedList(consumedMemory, size);\n+                }\n+            } catch(OutOfMemoryError oom) {\n+            }\n+            size = size \/ 2;\n+        }\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ Test Cases\n+\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/ make sure a compiled frame is not deoptimized if an escaping local is accessed\n+class EAGetWithoutMaterializeTarget extends EATestCaseBaseTarget {\n+\n+    public XYVal getAway;\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy = new XYVal(4, 2);\n+        getAway = xy;  \/\/ allocated object escapes\n+        dontinline_brkpt();\n+        iResult = xy.x + xy.y;\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 4 + 2;\n+    }\n+\n+    @Override\n+    public boolean testFrameShouldBeDeoptimized() {\n+        return false;\n+    }\n+}\n+\n+class EAGetWithoutMaterialize extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        ObjectReference o = getLocalRef(bpe.thread().frame(1), XYVAL_NAME, \"xy\");\n+        checkPrimitiveField(o, FD.I, \"x\", 4);\n+        checkPrimitiveField(o, FD.I, \"y\", 2);\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/\n+\/\/ Tests the following:\n+\/\/\n+\/\/ 1. Debugger can obtain a reference to a scalar replaced object R from java thread J.\n+\/\/    See runTestCase.\n+\/\/\n+\/\/ 2. Subsequent modifications of R by J are noticed by the debugger.\n+\/\/    See checkPostConditions.\n+\/\/\n+class EAMaterializeLocalVariableUponGet extends EATestCaseBaseDebugger {\n+\n+    private ObjectReference o;\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        \/\/ check 1.\n+        o = getLocalRef(bpe.thread().frame(1), XYVAL_NAME, \"xy\");\n+        \/\/ o is referenced in checkPostConditions() and must not be gc'ed.\n+        o.disableCollection();\n+        checkPrimitiveField(o, FD.I, \"x\", 4);\n+        checkPrimitiveField(o, FD.I, \"y\", 2);\n+    }\n+\n+    @Override\n+    public void checkPostConditions() throws Exception {\n+        super.checkPostConditions();\n+        \/\/ check 2.\n+        checkPrimitiveField(o, FD.I, \"x\", 5);\n+    }\n+}\n+\n+class EAMaterializeLocalVariableUponGetTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy = new XYVal(4, 2);\n+        dontinline_brkpt();       \/\/ Debugger obtains scalar replaced object at this point.\n+        xy.x += 1;                \/\/ Change scalar replaced object after debugger obtained a reference to it.\n+        iResult = xy.x + xy.y;\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 4 + 2 + 1;\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/ Test if an eliminated object can be reallocated in a frame with an active\n+\/\/ call that will return another object\n+class EAMaterializeLocalAtObjectReturn extends EATestCaseBaseDebugger {\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        ObjectReference o = getLocalRef(bpe.thread().frame(2), XYVAL_NAME, \"xy\");\n+        checkPrimitiveField(o, FD.I, \"x\", 4);\n+        checkPrimitiveField(o, FD.I, \"y\", 2);\n+    }\n+}\n+\n+class EAMaterializeLocalAtObjectReturnTarget extends EATestCaseBaseTarget {\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy = new XYVal(4, 2);\n+        Integer io =                 \/\/ Read xy here triggers reallocation\n+                dontinline_brkpt_return_Integer();\n+        iResult = xy.x + xy.y + io;\n+    }\n+\n+    public Integer dontinline_brkpt_return_Integer() {\n+        \/\/ We can't break directly in this method, as this results in making\n+        \/\/ the test method not entrant caused by an existing dependency\n+        dontinline_brkpt();\n+        return Integer.valueOf(23);\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 4 + 2 + 23;\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/ Test if an eliminated object can be reallocated *just* before a call returns an object.\n+\/\/ (See CompiledMethod::is_at_poll_return())\n+\/\/ Details: the callee method has just one safepoint poll at the return. The other safepoint\n+\/\/ is at the end of an iteration of the endless loop. We can detect if we suspended the target\n+\/\/ there because the local xy is out of scope there.\n+class EAMaterializeLocalAtObjectPollReturnReturn extends EATestCaseBaseDebugger {\n+    public void runTestCase() throws Exception {\n+        msg(\"Resume \" + env.targetMainThread);\n+        env.targetMainThread.resume();\n+        waitUntilTargetHasEnteredEndlessLoop();\n+        ObjectReference o = null;\n+        do {\n+            env.targetMainThread.suspend();\n+            printStack(env.targetMainThread);\n+            try {\n+                o = getLocalRef(env.targetMainThread.frame(0), XYVAL_NAME, \"xy\");\n+            } catch (Exception e) {\n+                msg(\"The local variable xy is out of scope because we suspended at the wrong bci. Resume and try again!\");\n+                env.targetMainThread.resume();\n+            }\n+        } while (o == null);\n+        checkPrimitiveField(o, FD.I, \"x\", 4);\n+        checkPrimitiveField(o, FD.I, \"y\", 2);\n+        terminateEndlessLoop();\n+    }\n+}\n+\n+class EAMaterializeLocalAtObjectPollReturnReturnTarget extends EATestCaseBaseTarget {\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        loopCount = 3;\n+        doLoop = true;\n+    }\n+\n+    public void warmupDone() {\n+        super.warmupDone();\n+        msg(\"enter 'endless' loop by setting loopCount = Long.MAX_VALUE\");\n+        loopCount = Long.MAX_VALUE; \/\/ endless loop\n+    }\n+\n+    public void dontinline_testMethod() {\n+        long result = 0;\n+        while (doLoop && loopCount-- > 0) {\n+            targetIsInLoop = true;\n+            XYVal xy = new XYVal(4, 2);\n+            Integer io =           \/\/ Read xy here triggers reallocation just before the call returns\n+                    dontinline_brkpt_return_Integer();\n+            result += xy.x + xy.y + io;\n+        }  \/\/ Here is a second safepoint. We were suspended here if xy is not in scope.\n+        targetIsInLoop = false;\n+        lResult = result;\n+    }\n+\n+    public Integer dontinline_brkpt_return_Integer() {\n+        return Integer.valueOf(23);\n+    }\n+\n+    @Override\n+    public long getExpectedLResult() {\n+        return (Long.MAX_VALUE - loopCount) * (4+2+23);\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ Test case collection that tests rematerialization of different\n+\/\/ array types where the first element is always not constant and the\n+\/\/ other elements are constants. Not constant values are stored in\n+\/\/ the stack frame for rematerialization whereas constants are kept\n+\/\/ in the debug info of the nmethod.\n+\n+class EAMaterializeIntArrayTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        int nums[] = {NOT_CONST_1I , 2, 3};\n+        dontinline_brkpt();\n+        iResult = nums[0] + nums[1] + nums[2];\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return NOT_CONST_1I + 2 + 3;\n+    }\n+}\n+\n+class EAMaterializeIntArray extends EATestCaseBaseDebugger {\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        int[] expectedVals = {1, 2, 3};\n+        checkLocalPrimitiveArray(bpe.thread().frame(1), \"nums\", FD.I, expectedVals);\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+class EAMaterializeLongArrayTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        long nums[] = {NOT_CONST_1L , 2, 3};\n+        dontinline_brkpt();\n+        lResult = nums[0] + nums[1] + nums[2];\n+    }\n+\n+    @Override\n+    public long getExpectedLResult() {\n+        return NOT_CONST_1L + 2 + 3;\n+    }\n+}\n+\n+class EAMaterializeLongArray extends EATestCaseBaseDebugger {\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        long[] expectedVals = {1, 2, 3};\n+        checkLocalPrimitiveArray(bpe.thread().frame(1), \"nums\", FD.J, expectedVals);\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+class EAMaterializeFloatArrayTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        float nums[] = {NOT_CONST_1F , 2.2f, 3.3f};\n+        dontinline_brkpt();\n+        fResult = nums[0] + nums[1] + nums[2];\n+    }\n+\n+    @Override\n+    public float getExpectedFResult() {\n+        return NOT_CONST_1F + 2.2f + 3.3f;\n+    }\n+}\n+\n+class EAMaterializeFloatArray extends EATestCaseBaseDebugger {\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        float[] expectedVals = {1.1f, 2.2f, 3.3f};\n+        checkLocalPrimitiveArray(bpe.thread().frame(1), \"nums\", FD.F, expectedVals);\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+class EAMaterializeDoubleArrayTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        double nums[] = {NOT_CONST_1D , 2.2d, 3.3d};\n+        dontinline_brkpt();\n+        dResult = nums[0] + nums[1] + nums[2];\n+    }\n+\n+    @Override\n+    public double getExpectedDResult() {\n+        return NOT_CONST_1D + 2.2d + 3.3d;\n+    }\n+}\n+\n+class EAMaterializeDoubleArray extends EATestCaseBaseDebugger {\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        double[] expectedVals = {1.1d, 2.2d, 3.3d};\n+        checkLocalPrimitiveArray(bpe.thread().frame(1), \"nums\", FD.D, expectedVals);\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+class EAMaterializeObjectArrayTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        Long nums[] = {NOT_CONST_1_OBJ , CONST_2_OBJ, CONST_3_OBJ};\n+        dontinline_brkpt();\n+        lResult = nums[0] + nums[1] + nums[2];\n+    }\n+\n+    @Override\n+    public long getExpectedLResult() {\n+        return 1 + 2 + 3;\n+    }\n+}\n+\n+class EAMaterializeObjectArray extends EATestCaseBaseDebugger {\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        ReferenceType clazz = bpe.thread().frame(0).location().declaringType();\n+        ObjectReference[] expectedVals = {\n+                (ObjectReference) clazz.getValue(clazz.fieldByName(\"NOT_CONST_1_OBJ\")),\n+                (ObjectReference) clazz.getValue(clazz.fieldByName(\"CONST_2_OBJ\")),\n+                (ObjectReference) clazz.getValue(clazz.fieldByName(\"CONST_3_OBJ\"))\n+        };\n+        checkLocalObjectArray(bpe.thread().frame(1), \"nums\", \"java.lang.Long[]\", expectedVals);\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/ Materialize an object whose fields have constant and not constant values at\n+\/\/ the point where the object is materialized.\n+class EAMaterializeObjectWithConstantAndNotConstantValuesTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        ILFDO o = new ILFDO(NOT_CONST_1I, 2,\n+                            NOT_CONST_1L, 2L,\n+                            NOT_CONST_1F, 2.1F,\n+                            NOT_CONST_1D, 2.1D,\n+                            NOT_CONST_1_OBJ, CONST_2_OBJ\n+                            );\n+        dontinline_brkpt();\n+        dResult =\n+            o.i + o.i2 + o.l + o.l2 + o.f + o.f2 + o.d + o.d2 + o.o + o.o2;\n+    }\n+\n+    @Override\n+    public double getExpectedDResult() {\n+        return NOT_CONST_1I + 2 + NOT_CONST_1L + 2L + NOT_CONST_1F + 2.1F + NOT_CONST_1D + 2.1D + NOT_CONST_1_OBJ + CONST_2_OBJ;\n+    }\n+}\n+\n+class EAMaterializeObjectWithConstantAndNotConstantValues extends EATestCaseBaseDebugger {\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        ObjectReference o = getLocalRef(bpe.thread().frame(1), \"ILFDO\", \"o\");\n+        checkPrimitiveField(o, FD.I, \"i\", 1);\n+        checkPrimitiveField(o, FD.I, \"i2\", 2);\n+        checkPrimitiveField(o, FD.J, \"l\", 1L);\n+        checkPrimitiveField(o, FD.J, \"l2\", 2L);\n+        checkPrimitiveField(o, FD.F, \"f\", 1.1f);\n+        checkPrimitiveField(o, FD.F, \"f2\", 2.1f);\n+        checkPrimitiveField(o, FD.D, \"d\", 1.1d);\n+        checkPrimitiveField(o, FD.D, \"d2\", 2.1d);\n+        ReferenceType clazz = bpe.thread().frame(1).location().declaringType();\n+        ObjectReference[] expVals = {\n+                (ObjectReference) clazz.getValue(clazz.fieldByName(\"NOT_CONST_1_OBJ\")),\n+                (ObjectReference) clazz.getValue(clazz.fieldByName(\"CONST_2_OBJ\")),\n+        };\n+        checkObjField(o, \"o\", expVals[0]);\n+        checkObjField(o, \"o2\", expVals[1]);\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/ Two local variables reference the same object.\n+\/\/ Check if the debugger obtains the same object when reading the two variables\n+class EAMaterializeObjReferencedBy2LocalsTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy = new XYVal(2, 3);\n+        XYVal alias = xy;\n+        dontinline_brkpt();\n+        iResult = xy.x + alias.x;\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 2 + 2;\n+    }\n+}\n+\n+class EAMaterializeObjReferencedBy2Locals extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        ObjectReference xy = getLocalRef(bpe.thread().frame(1), XYVAL_NAME, \"xy\");\n+        ObjectReference alias = getLocalRef(bpe.thread().frame(1), XYVAL_NAME, \"alias\");\n+        Asserts.assertSame(xy, alias, \"xy and alias are expected to reference the same object\");\n+    }\n+\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/ Two local variables reference the same object.\n+\/\/ Check if it has the expected effect in the target if the debugger modifies the object.\n+class EAMaterializeObjReferencedBy2LocalsAndModifyTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy = new XYVal(2, 3);\n+        XYVal alias = xy;\n+        dontinline_brkpt(); \/\/ debugger: alias.x = 42\n+        iResult = xy.x + alias.x;\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 42 + 42;\n+    }\n+}\n+\n+class EAMaterializeObjReferencedBy2LocalsAndModify extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        ObjectReference alias = getLocalRef(bpe.thread().frame(1), XYVAL_NAME, \"alias\");\n+        setField(alias, \"x\", env.vm().mirrorOf(42));\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/ Two local variables of the same compiled frame but in different virtual frames reference the same\n+\/\/ object.\n+\/\/ Check if the debugger obtains the same object when reading the two variables\n+class EAMaterializeObjReferencedBy2LocalsInDifferentVirtFramesTarget extends EATestCaseBaseTarget {\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy = new XYVal(2, 3);\n+        testMethod_inlined(xy);\n+        iResult += xy.x;\n+    }\n+\n+    public void testMethod_inlined(XYVal xy) {\n+        XYVal alias = xy;\n+        dontinline_brkpt();\n+        iResult = alias.x;\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 2 + 2;\n+    }\n+}\n+\n+class EAMaterializeObjReferencedBy2LocalsInDifferentVirtFrames extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        ObjectReference xy = getLocalRef(bpe.thread().frame(2), XYVAL_NAME, \"xy\");\n+        ObjectReference alias = getLocalRef(bpe.thread().frame(1), \"testMethod_inlined\", \"alias\", XYVAL_NAME);\n+        Asserts.assertSame(xy, alias, \"xy and alias are expected to reference the same object\");\n+    }\n+\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/ Two local variables of the same compiled frame but in different virtual frames reference the same\n+\/\/ object.\n+\/\/ Check if it has the expected effect in the target if the debugger modifies the object.\n+class EAMaterializeObjReferencedBy2LocalsInDifferentVirtFramesAndModifyTarget extends EATestCaseBaseTarget {\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy = new XYVal(2, 3);\n+        testMethod_inlined(xy);   \/\/ debugger: xy.x = 42\n+        iResult += xy.x;\n+    }\n+\n+    public void testMethod_inlined(XYVal xy) {\n+        XYVal alias = xy;\n+        dontinline_brkpt();\n+        iResult = alias.x;\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 42 + 42;\n+    }\n+}\n+\n+class EAMaterializeObjReferencedBy2LocalsInDifferentVirtFramesAndModify extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        ObjectReference alias = getLocalRef(bpe.thread().frame(1), \"testMethod_inlined\", \"alias\", XYVAL_NAME);\n+        setField(alias, \"x\", env.vm().mirrorOf(42));\n+    }\n+\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/ Test materialization of an object referenced only from expression stack\n+class EAMaterializeObjReferencedFromOperandStackTarget extends EATestCaseBaseTarget {\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    public void dontinline_testMethod() {\n+        @SuppressWarnings(\"unused\")\n+        XYVal xy1 = new XYVal(2, 3);\n+        \/\/ Debugger breaks in call to dontinline_brkpt_ret_100() and reads\n+        \/\/ the value of the local 'xy1'. This triggers materialization\n+        \/\/ of the object on the operand stack\n+        iResult = testMethodInlined(new XYVal(4, 2), dontinline_brkpt_ret_100());\n+    }\n+\n+    public int testMethodInlined(XYVal xy2, int dontinline_brkpt_ret_100) {\n+        return xy2.x + dontinline_brkpt_ret_100;\n+    }\n+\n+    public int dontinline_brkpt_ret_100() {\n+        dontinline_brkpt();\n+        return 100;\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 4 + 100;\n+    }\n+}\n+\n+class EAMaterializeObjReferencedFromOperandStack extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        ObjectReference xy1 = getLocalRef(bpe.thread().frame(2), XYVAL_NAME, \"xy1\");\n+        checkPrimitiveField(xy1, FD.I, \"x\", 2);\n+        checkPrimitiveField(xy1, FD.I, \"y\", 3);\n+    }\n+\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Tests a regression in the implementation by setting the value of a local int which triggers the\n+ * creation of a deferred update and then getting the reference to a scalar replaced object.  The\n+ * issue was that the scalar replaced object was not reallocated. Because of the deferred update it\n+ * was assumed that the reallocation already happened.\n+ *\/\n+class EAMaterializeLocalVariableUponGetAfterSetInteger extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        setLocal(bpe.thread().frame(1), \"i\", env.vm().mirrorOf(43));\n+        ObjectReference o = getLocalRef(bpe.thread().frame(1), XYVAL_NAME, \"xy\");\n+        checkPrimitiveField(o, FD.I, \"x\", 4);\n+        checkPrimitiveField(o, FD.I, \"y\", 2);\n+    }\n+}\n+\n+class EAMaterializeLocalVariableUponGetAfterSetIntegerTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy = new XYVal(4, 2);\n+        int i = 42;\n+        dontinline_brkpt();\n+        iResult = xy.x + xy.y + i;\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 4 + 2 + 43;\n+    }\n+\n+    @Override\n+    public boolean testFrameShouldBeDeoptimized() {\n+        return true; \/\/ setting local variable i always triggers deoptimization\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ Locking Tests\n+\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+class EARelockingSimple extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference o = getLocalRef(bpe.thread().frame(1), XYVAL_NAME, \"l1\");\n+    }\n+}\n+\n+class EARelockingSimpleTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        XYVal l1 = new XYVal(4, 2);\n+        synchronized (l1) {\n+            dontinline_brkpt();\n+        }\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Test if the bias of an object O that escapes globally is revoked correctly if local objects\n+ * escape through JVMTI. O is referenced by field l0.\n+ * This tests a regression of a previous version of the implementation.\n+ *\/\n+class EARelockingSimple_2 extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference o = getLocalRef(bpe.thread().frame(1), XYVAL_NAME, \"l1\");\n+    }\n+}\n+\n+class EARelockingSimple_2Target extends EATestCaseBaseTarget {\n+\n+    public XYVal l0;\n+\n+    public void dontinline_testMethod() {\n+        l0 = new XYVal(4, 2);         \/\/ GobalEscape\n+        XYVal l1 = new XYVal(4, 2);\n+        synchronized (l0) {\n+            synchronized (l1) {\n+                dontinline_brkpt();\n+            }\n+        }\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/ Test recursive locking\n+class EARelockingRecursiveTarget extends EATestCaseBaseTarget {\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    public void dontinline_testMethod() {\n+        XYVal l1 = new XYVal(4, 2);\n+        synchronized (l1) {\n+            testMethod_inlined(l1);\n+        }\n+    }\n+\n+    public void testMethod_inlined(XYVal l2) {\n+        synchronized (l2) {\n+            dontinline_brkpt();\n+        }\n+    }\n+}\n+\n+class EARelockingRecursive extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference o = getLocalRef(bpe.thread().frame(2), XYVAL_NAME, \"l1\");\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/ Object ref l1 is retrieved by the debugger at a location where nested locks are omitted. The\n+\/\/ accessed object is globally reachable already before the access, therefore no relocking is done.\n+class EARelockingNestedInflatedTarget extends EATestCaseBaseTarget {\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    @Override\n+    public boolean testFrameShouldBeDeoptimized() {\n+        \/\/ Access does not trigger deopt., as escape state is already global escape.\n+        return false;\n+    }\n+\n+    public void dontinline_testMethod() {\n+        XYVal l1 = inflatedLock;\n+        synchronized (l1) {\n+            testMethod_inlined(l1);\n+        }\n+    }\n+\n+    public void testMethod_inlined(XYVal l2) {\n+        synchronized (l2) {                 \/\/ eliminated nested locking\n+            dontinline_brkpt();\n+        }\n+    }\n+}\n+\n+class EARelockingNestedInflated extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference o = getLocalRef(bpe.thread().frame(2), XYVAL_NAME, \"l1\");\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Like {@link EARelockingNestedInflated} with the difference that there is\n+ * a scalar replaced object in the scope from which the object with eliminated nested locking\n+ * is read. This triggers materialization and relocking.\n+ *\/\n+class EARelockingNestedInflated_02 extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference o = getLocalRef(bpe.thread().frame(2), XYVAL_NAME, \"l1\");\n+    }\n+}\n+\n+class EARelockingNestedInflated_02Target extends EATestCaseBaseTarget {\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    public void dontinline_testMethod() {\n+        @SuppressWarnings(\"unused\")\n+        XYVal xy = new XYVal(1, 1);     \/\/ scalar replaced\n+        XYVal l1 = inflatedLock;          \/\/ read by debugger\n+        synchronized (l1) {\n+            testMethod_inlined(l1);\n+        }\n+    }\n+\n+    public void testMethod_inlined(XYVal l2) {\n+        synchronized (l2) {                 \/\/ eliminated nested locking\n+            dontinline_brkpt();\n+        }\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Checks if an eliminated lock of an ArgEscape object l1 can be relocked if\n+ * l1 is locked in a callee frame.\n+ *\/\n+class EARelockingArgEscapeLWLockedInCalleeFrame extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference o = getLocalRef(bpe.thread().frame(2), XYVAL_NAME, \"l1\");\n+    }\n+}\n+\n+class EARelockingArgEscapeLWLockedInCalleeFrameTarget extends EATestCaseBaseTarget {\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    public void dontinline_testMethod() {\n+        XYVal l1 = new XYVal(1, 1);       \/\/ ArgEscape\n+        synchronized (l1) {                   \/\/ eliminated\n+            l1.dontinline_sync_method(this);  \/\/ l1 escapes\n+        }\n+    }\n+\n+    @Override\n+    public boolean testFrameShouldBeDeoptimized() {\n+        \/\/ Graal does not provide debug info about arg escape objects, therefore the frame is not deoptimized\n+        return !UseJVMCICompiler && super.testFrameShouldBeDeoptimized();\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Similar to {@link EARelockingArgEscapeLWLockedInCalleeFrame}. In addition\n+ * the test method has got a scalar replaced object with eliminated locking.\n+ * This pattern matches a regression in the implementation.\n+ *\/\n+class EARelockingArgEscapeLWLockedInCalleeFrame_2 extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference o = getLocalRef(bpe.thread().frame(2), XYVAL_NAME, \"l1\");\n+    }\n+}\n+\n+class EARelockingArgEscapeLWLockedInCalleeFrame_2Target extends EATestCaseBaseTarget {\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    public void dontinline_testMethod() {\n+        XYVal l1 = new XYVal(1, 1);       \/\/ ArgEscape\n+        XYVal l2 = new XYVal(4, 2);       \/\/ NoEscape, scalar replaced\n+        synchronized (l1) {                   \/\/ eliminated\n+            synchronized (l2) {               \/\/ eliminated\n+                l1.dontinline_sync_method(this);  \/\/ l1 escapes\n+            }\n+        }\n+        iResult = l2.x + l2.y;\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 6;\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Similar to {@link EARelockingArgEscapeLWLockedInCalleeFrame}.\n+ * A bulk rebias operation is triggered at a position where all locks on the local object referenced\n+ * by l1 are eliminated. This leaves the object with an outdated biased locking epoch which has to be\n+ * considered when relocking.\n+ * This tests a regression in a previous version.\n+ *\/\n+class EARelockingArgEscapeLWLockedInCalleeFrame_3 extends EATestCaseBaseDebugger {\n+\n+    public static final String XYVAL_LOCAL_NAME = EARelockingArgEscapeLWLockedInCalleeFrame_3Target.XYValLocal.class.getName();\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference o = getLocalRef(bpe.thread().frame(1), XYVAL_LOCAL_NAME, \"l1\");\n+    }\n+}\n+\n+class EARelockingArgEscapeLWLockedInCalleeFrame_3Target extends EATestCaseBaseTarget {\n+\n+    \/\/ Using local type to avoid side effects on biased locking heuristics\n+    public static class XYValLocal extends XYVal {\n+        public XYValLocal(int x, int y) {\n+            super(x,y);\n+        }\n+    }\n+\n+    public void dontinline_testMethod() {\n+        XYVal l1 = new XYValLocal(1, 1);       \/\/ ArgEscape\n+        synchronized (l1) {                    \/\/ eliminated\n+            l1.dontinline_sync_method_no_brkpt(this);  \/\/ l1 escapes\n+            \/\/ trigger bulk rebias\n+            dontinline_bulkRebiasAfterWarmup(l1.getClass());\n+            \/\/ Now the epoch of l1 does not match the epoch of its class.\n+            \/\/ This has to be considered when relocking because of JVMTI access\n+            dontinline_brkpt();\n+        }\n+    }\n+\n+    @Override\n+    public boolean testFrameShouldBeDeoptimized() {\n+        \/\/ Graal does not provide debug info about arg escape objects, therefore the frame is not deoptimized\n+        return !UseJVMCICompiler && super.testFrameShouldBeDeoptimized();\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Similar to {@link EARelockingArgEscapeLWLockedInCalleeFrame_3}.\n+ * But instead of a bulk rebias a bulk revoke operation is triggered.\n+ * This leaves the object with a stale bias as the prototype header of its calls lost its bias\n+ * pattern in the bulk revoke which has to be considered during relocking.\n+ * This tests a regression in a previous version.\n+ *\/\n+class EARelockingArgEscapeLWLockedInCalleeFrame_4 extends EATestCaseBaseDebugger {\n+\n+    public static final String XYVAL_LOCAL_NAME = EARelockingArgEscapeLWLockedInCalleeFrame_4Target.XYValLocal.class.getName();\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference o = getLocalRef(bpe.thread().frame(1), XYVAL_LOCAL_NAME, \"l1\");\n+    }\n+}\n+\n+class EARelockingArgEscapeLWLockedInCalleeFrame_4Target extends EATestCaseBaseTarget {\n+\n+    \/\/ Using local type to avoid side effects on biased locking heuristics\n+    public static class XYValLocal extends XYVal {\n+        public XYValLocal(int x, int y) {\n+            super(x,y);\n+        }\n+    }\n+\n+    public void dontinline_testMethod() {\n+        XYVal l1 = new XYValLocal(1, 1);       \/\/ ArgEscape\n+        synchronized (l1) {                    \/\/ eliminated\n+            l1.dontinline_sync_method_no_brkpt(this);  \/\/ l1 escapes\n+            \/\/ trigger bulk rebias\n+            dontinline_bulkRevokeAfterWarmup(l1.getClass());\n+            \/\/ Now the epoch of l1 does not match the epoch of its class.\n+            \/\/ This has to be considered when relocking because of JVMTI access\n+            dontinline_brkpt();\n+        }\n+    }\n+\n+\n+    @Override\n+    public boolean testFrameShouldBeDeoptimized() {\n+        \/\/ Graal does not provide debug info about arg escape objects, therefore the frame is not deoptimized\n+        return !UseJVMCICompiler && super.testFrameShouldBeDeoptimized();\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Test relocking eliminated (nested) locks of an object on which the\n+ * target thread currently waits.\n+ *\/\n+class EARelockingObjectCurrentlyWaitingOn extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        env.targetMainThread.resume();\n+        boolean inWait = false;\n+        do {\n+            Thread.sleep(100);\n+            env.targetMainThread.suspend();\n+            printStack(env.targetMainThread);\n+            inWait = env.targetMainThread.frame(0).location().method().name().equals(\"wait\");\n+            if (!inWait) {\n+                msg(\"Target not yet in java.lang.Object.wait(long).\");\n+                env.targetMainThread.resume();\n+            }\n+        } while(!inWait);\n+        StackFrame testMethodFrame = env.targetMainThread.frame(4);\n+        \/\/ Access triggers relocking of all eliminated locks, including nested locks of l1 which references\n+        \/\/ the object on which the target main thread is currently waiting.\n+        ObjectReference l0 = getLocalRef(testMethodFrame, EARelockingObjectCurrentlyWaitingOnTarget.ForLocking.class.getName(), \"l0\");\n+        Asserts.assertEQ(l0.entryCount(), 1, \"wrong entry count\");\n+        ObjectReference l1 = getLocalRef(testMethodFrame, EARelockingObjectCurrentlyWaitingOnTarget.ForLocking.class.getName(), \"l1\");\n+        Asserts.assertEQ(l1.entryCount(), 0, \"wrong entry count\");\n+        setField(testCase, \"objToNotifyOn\", l1);\n+    }\n+}\n+\n+class EARelockingObjectCurrentlyWaitingOnTarget extends EATestCaseBaseTarget {\n+\n+    public static class ForLocking {\n+    }\n+\n+    public volatile Object objToNotifyOn; \/\/ debugger assigns value when notify thread should call objToNotifyOn.notifyAll()\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    @Override\n+    public void warmupDone() {\n+        super.warmupDone();\n+        Thread t = new Thread(() -> doNotify());\n+        t.start();\n+    }\n+\n+    public void doNotify() {\n+        while (objToNotifyOn == null) {\n+            try {\n+                msg(\"objToNotifyOn is still null\");\n+                Thread.sleep(100);\n+            } catch (InterruptedException e) { \/* ignored *\/ }\n+        }\n+        synchronized (objToNotifyOn) {\n+            \/\/ will be received by the target main thread waiting in dontinline_waitWhenWarmupDone\n+            msg(\"calling objToNotifyOn.notifyAll()\");\n+            objToNotifyOn.notifyAll();\n+        }\n+    }\n+\n+    @Override\n+    public boolean testFrameShouldBeDeoptimized() {\n+        return false;\n+    }\n+\n+    @Override\n+    public void dontinline_testMethod() throws Exception {\n+        ForLocking l0 = new ForLocking(); \/\/ will be scalar replaced; access triggers realloc\/relock\n+        ForLocking l1 = new ForLocking();\n+        synchronized (l0) {\n+            synchronized (l1) {\n+                testMethod_inlined(l1);\n+            }\n+        }\n+    }\n+\n+    public void testMethod_inlined(ForLocking l2) throws Exception {\n+        synchronized (l2) {                 \/\/ eliminated nested locking\n+            dontinline_waitWhenWarmupDone(l2);\n+        }\n+    }\n+\n+    public void dontinline_waitWhenWarmupDone(ForLocking l2) throws Exception {\n+        if (warmupDone) {\n+            l2.wait();\n+        }\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ Test cases that require deoptimization even though neither locks\n+\/\/ nor allocations are eliminated at the point where escape state is changed.\n+\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Let xy be NoEscape whose allocation cannot be eliminated (simulated by\n+ * -XX:-EliminateAllocations). The holding compiled frame has to be deoptimized when debugger\n+ * accesses xy because afterwards locking on xy is omitted.\n+ * Note: there are no EA based optimizations at the escape point.\n+ *\/\n+class EADeoptFrameAfterReadLocalObject_01 extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference xy = getLocalRef(bpe.thread().frame(1), XYVAL_NAME, \"xy\");\n+    }\n+}\n+\n+class EADeoptFrameAfterReadLocalObject_01Target extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy = new XYVal(1, 1);\n+        dontinline_brkpt();              \/\/ Debugger reads xy, when there are no virtual objects or eliminated locks in scope\n+        synchronized (xy) {              \/\/ Locking is eliminated.\n+            xy.x++;\n+            xy.y++;\n+        }\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Similar to {@link EADeoptFrameAfterReadLocalObject_01} with the difference that the debugger\n+ * reads xy from an inlined callee. So xy is NoEscape instead of ArgEscape.\n+ *\/\n+class EADeoptFrameAfterReadLocalObject_01BTarget extends EATestCaseBaseTarget {\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy  = new XYVal(1, 1);\n+        callee(xy);                 \/\/ Debugger acquires ref to xy from inlined callee\n+                                    \/\/ xy is NoEscape, nevertheless the object is not replaced\n+                                    \/\/ by scalars if running with -XX:-EliminateAllocations.\n+                                    \/\/ In that case there are no EA based optimizations were\n+                                    \/\/ the debugger reads the NoEscape object.\n+        synchronized (xy) {         \/\/ Locking is eliminated.\n+            xy.x++;\n+            xy.y++;\n+        }\n+    }\n+\n+    public void callee(XYVal xy) {\n+        dontinline_brkpt();              \/\/ Debugger reads xy.\n+                                         \/\/ There are no virtual objects or eliminated locks.\n+    }\n+}\n+\n+class EADeoptFrameAfterReadLocalObject_01B extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference xy = getLocalRef(bpe.thread().frame(1), \"callee\", \"xy\", XYVAL_NAME);\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Let xy be ArgEscape. The frame dontinline_testMethod() has to be deoptimized when debugger\n+ * acquires xy from dontinline_callee() because afterwards locking on xy is omitted.\n+ * Note: there are no EA based optimizations at the escape point.\n+ *\/\n+class EADeoptFrameAfterReadLocalObject_02 extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference xy = getLocalRef(bpe.thread().frame(1), \"dontinline_callee\", \"xy\", XYVAL_NAME);\n+    }\n+}\n+\n+class EADeoptFrameAfterReadLocalObject_02Target extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy  = new XYVal(1, 1);\n+        dontinline_callee(xy);      \/\/ xy is ArgEscape, debugger acquires ref to xy from callee\n+        synchronized (xy) {         \/\/ Locking is eliminated.\n+            xy.x++;\n+            xy.y++;\n+        }\n+    }\n+\n+    public void dontinline_callee(XYVal xy) {\n+        dontinline_brkpt();              \/\/ Debugger reads xy.\n+                                         \/\/ There are no virtual objects or eliminated locks.\n+    }\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    @Override\n+    public boolean testFrameShouldBeDeoptimized() {\n+        \/\/ Graal does not provide debug info about arg escape objects, therefore the frame is not deoptimized\n+        return !UseJVMCICompiler && super.testFrameShouldBeDeoptimized();\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Similar to {@link EADeoptFrameAfterReadLocalObject_02} there is an ArgEscape object xy, but in\n+ * contrast it is not in the parameter list of a call when the debugger reads an object.\n+ * Therefore the frame of the test method should not be deoptimized\n+ *\/\n+class EADeoptFrameAfterReadLocalObject_02B extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference xy = getLocalRef(bpe.thread().frame(1), \"dontinline_callee\", \"xy\", XYVAL_NAME);\n+    }\n+}\n+\n+class EADeoptFrameAfterReadLocalObject_02BTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy  = new XYVal(1, 1);\n+        dontinline_make_arg_escape(xy);  \/\/ because of this call xy is ArgEscape\n+        dontinline_callee();             \/\/ xy is ArgEscape, but not a parameter of this call\n+        synchronized (xy) {              \/\/ Locking is eliminated.\n+            xy.x++;\n+            xy.y++;\n+        }\n+    }\n+\n+    public void dontinline_callee() {\n+        @SuppressWarnings(\"unused\")\n+        XYVal xy  = new XYVal(2, 2);\n+        dontinline_brkpt();              \/\/ Debugger reads xy.\n+                                         \/\/ No need to deoptimize the caller frame\n+    }\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    @Override\n+    public boolean testFrameShouldBeDeoptimized() {\n+        return false;\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Similar to {@link EADeoptFrameAfterReadLocalObject_02} there is an ArgEscape object xy in\n+ * dontinline_testMethod() which is being passed as parameter when the debugger accesses a local object.\n+ * Nevertheless dontinline_testMethod must not be deoptimized because there is an entry frame\n+ * between it and the frame accessed by the debugger.\n+ *\/\n+class EADeoptFrameAfterReadLocalObject_02C extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference xy = getLocalRef(bpe.thread().frame(1), \"dontinline_callee_accessed_by_debugger\", \"xy\", XYVAL_NAME);\n+    }\n+}\n+\n+class EADeoptFrameAfterReadLocalObject_02CTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy  = new XYVal(1, 1);\n+        dontinline_callee(xy);           \/\/ xy is ArgEscape and being passed as parameter\n+        synchronized (xy) {              \/\/ Locking is eliminated.\n+            xy.x++;\n+            xy.y++;\n+        }\n+    }\n+\n+    public void dontinline_callee(XYVal xy) {\n+        if (warmupDone) {\n+            dontinline_call_with_entry_frame(this, \"dontinline_callee_accessed_by_debugger\");\n+        }\n+    }\n+\n+    public void dontinline_callee_accessed_by_debugger() {\n+        @SuppressWarnings(\"unused\")\n+        XYVal xy  = new XYVal(2, 2);\n+        dontinline_brkpt();              \/\/ Debugger reads xy.\n+                                         \/\/ No need to deoptimize the caller frame\n+    }\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 8;\n+    }\n+\n+    @Override\n+    public boolean testFrameShouldBeDeoptimized() {\n+        return false;\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Let xy be NoEscape whose allocation cannot be eliminated (e.g. because of\n+ * -XX:-EliminateAllocations).  The holding compiled frame has to be deoptimized when debugger\n+ * accesses xy because the following field accesses get eliminated.  Note: there are no EA based\n+ * optimizations at the escape point.\n+ *\/\n+class EADeoptFrameAfterReadLocalObject_03 extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        ObjectReference xy = getLocalRef(bpe.thread().frame(1), XYVAL_NAME, \"xy\");\n+        setField(xy, \"x\", env.vm().mirrorOf(1));\n+    }\n+}\n+\n+class EADeoptFrameAfterReadLocalObject_03Target extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy = new XYVal(0, 1);\n+        dontinline_brkpt();              \/\/ Debugger reads xy, when there are no virtual objects or\n+                                         \/\/ eliminated locks in scope and modifies xy.x\n+        iResult = xy.x + xy.y;           \/\/ Loads are replaced by constants 0 and 1.\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 1 + 1;\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ Monitor info tests\n+\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+class EAGetOwnedMonitorsTarget extends EATestCaseBaseTarget {\n+\n+    public long checkSum;\n+\n+    public void dontinline_testMethod() {\n+        XYVal l1 = new XYVal(4, 2);\n+        synchronized (l1) {\n+            dontinline_endlessLoop();\n+        }\n+    }\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+        loopCount = 3;\n+    }\n+\n+    public void warmupDone() {\n+        super.warmupDone();\n+        msg(\"enter 'endless' loop by setting loopCount = Long.MAX_VALUE\");\n+        loopCount = Long.MAX_VALUE; \/\/ endless loop\n+    }\n+}\n+\n+class EAGetOwnedMonitors extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        msg(\"resume\");\n+        env.targetMainThread.resume();\n+        waitUntilTargetHasEnteredEndlessLoop();\n+        \/\/ In contrast to JVMTI, JDWP requires a target thread to be suspended, before the owned monitors can be queried\n+        msg(\"suspend target\");\n+        env.targetMainThread.suspend();\n+        msg(\"Get owned monitors\");\n+        List<ObjectReference> monitors = env.targetMainThread.ownedMonitors();\n+        Asserts.assertEQ(monitors.size(), 1, \"unexpected number of owned monitors\");\n+        terminateEndlessLoop();\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+class EAEntryCountTarget extends EATestCaseBaseTarget {\n+\n+    public long checkSum;\n+\n+    public void dontinline_testMethod() {\n+        XYVal l1 = new XYVal(4, 2);\n+        synchronized (l1) {\n+            inline_testMethod2(l1);\n+        }\n+    }\n+\n+    public void inline_testMethod2(XYVal l1) {\n+        synchronized (l1) {\n+            dontinline_endlessLoop();\n+        }\n+    }\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+        loopCount = 3;\n+    }\n+\n+    public void warmupDone() {\n+        super.warmupDone();\n+        msg(\"enter 'endless' loop by setting loopCount = Long.MAX_VALUE\");\n+        loopCount = Long.MAX_VALUE; \/\/ endless loop\n+    }\n+}\n+\n+class EAEntryCount extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        msg(\"resume\");\n+        env.targetMainThread.resume();\n+        waitUntilTargetHasEnteredEndlessLoop();\n+        \/\/ In contrast to JVMTI, JDWP requires a target thread to be suspended, before the owned monitors can be queried\n+        msg(\"suspend target\");\n+        env.targetMainThread.suspend();\n+        msg(\"Get owned monitors\");\n+        List<ObjectReference> monitors = env.targetMainThread.ownedMonitors();\n+        Asserts.assertEQ(monitors.size(), 1, \"unexpected number of owned monitors\");\n+        msg(\"Get entry count\");\n+        int entryCount = monitors.get(0).entryCount();\n+        Asserts.assertEQ(entryCount, 2, \"wrong entry count\");\n+        terminateEndlessLoop();\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ PopFrame tests\n+\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * PopFrame into caller frame with scalar replaced objects.\n+ *\/\n+class EAPopFrameNotInlined extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        msg(\"PopFrame\");\n+        bpe.thread().popFrames(bpe.thread().frame(0));\n+        msg(\"PopFrame DONE\");\n+    }\n+\n+    @Override\n+    public boolean shouldSkip() {\n+        \/\/ And Graal currently doesn't support PopFrame\n+        return super.shouldSkip() || env.targetVMOptions.UseJVMCICompiler;\n+    }\n+}\n+\n+class EAPopFrameNotInlinedTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy = new XYVal(4, 2);\n+        dontinline_brkpt();\n+        iResult = xy.x + xy.y;\n+    }\n+\n+    @Override\n+    public boolean testFrameShouldBeDeoptimized() {\n+        \/\/ Test is only performed after the frame pop.\n+        \/\/ Then dontinline_testMethod is interpreted.\n+        return false;\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 4 + 2;\n+    }\n+\n+    @Override\n+    public boolean shouldSkip() {\n+        \/\/ And Graal currently doesn't support PopFrame\n+        return super.shouldSkip() || UseJVMCICompiler;\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Pop frames into {@link EAPopFrameNotInlinedReallocFailureTarget#dontinline_testMethod()} which\n+ * holds scalar replaced objects. In preparation of the pop frame operations the vm eagerly\n+ * reallocates scalar replaced objects to avoid failures when actually popping the frames. We provoke\n+ * a reallocation failures and expect {@link VMOutOfMemoryException}.\n+ *\/\n+class EAPopFrameNotInlinedReallocFailure extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        ThreadReference thread = bpe.thread();\n+        printStack(thread);\n+        \/\/ frame[0]: EATestCaseBaseTarget.dontinline_brkpt()\n+        \/\/ frame[1]: EAPopFrameNotInlinedReallocFailureTarget.dontinline_consume_all_memory_brkpt()\n+        \/\/ frame[2]: EAPopFrameNotInlinedReallocFailureTarget.dontinline_testMethod()\n+        \/\/ frame[3]: EATestCaseBaseTarget.run()\n+        \/\/ frame[4]: EATestsTarget.main(java.lang.String[])\n+        msg(\"PopFrame\");\n+        boolean coughtOom = false;\n+        try {\n+            \/\/ try to pop dontinline_consume_all_memory_brkpt\n+            thread.popFrames(thread.frame(1));\n+        } catch (VMOutOfMemoryException oom) {\n+            \/\/ as expected\n+            msg(\"cought OOM\");\n+            coughtOom  = true;\n+        }\n+        freeAllMemory();\n+        \/\/ We succeeded to pop just one frame. When we continue, we will call dontinline_brkpt() again.\n+        Asserts.assertTrue(coughtOom || !env.targetVMOptions.EliminateAllocations,\n+                           \"PopFrame should have triggered an OOM exception in target\");\n+        String expectedTopFrame =\n+                env.targetVMOptions.EliminateAllocations ? \"dontinline_consume_all_memory_brkpt\" : \"dontinline_testMethod\";\n+        Asserts.assertEQ(expectedTopFrame, thread.frame(0).location().method().name());\n+        printStack(thread);\n+    }\n+\n+    @Override\n+    public boolean shouldSkip() {\n+        \/\/ OOMEs because of realloc failures with DeoptimizeObjectsALot are too random.\n+        \/\/ And Graal currently doesn't provide all information about non-escaping objects in debug info\n+        return super.shouldSkip() || env.targetVMOptions.DeoptimizeObjectsALot || env.targetVMOptions.UseJVMCICompiler;\n+    }\n+}\n+\n+class EAPopFrameNotInlinedReallocFailureTarget extends EATestCaseBaseTarget {\n+\n+    public boolean doneAlready;\n+\n+    public void dontinline_testMethod() {\n+        long a[] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};                \/\/ scalar replaced\n+        Vector10 v = new Vector10(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);  \/\/ scalar replaced\n+        dontinline_consume_all_memory_brkpt();\n+        lResult = a[0] + a[1] + a[2] + a[3] + a[4] + a[5] + a[6] + a[7] + a[8] + a[9]\n+               + v.i0 + v.i1 + v.i2 + v.i3 + v.i4 + v.i5 + v.i6 + v.i7 + v.i8 + v.i9;\n+    }\n+\n+    public void dontinline_consume_all_memory_brkpt() {\n+        if (warmupDone && !doneAlready) {\n+            doneAlready = true;\n+            consumeAllMemory(); \/\/ provoke reallocation failure\n+            dontinline_brkpt();\n+        }\n+    }\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    @Override\n+    public long getExpectedLResult() {\n+        long n = 10;\n+        return 2*n*(n+1)\/2;\n+    }\n+\n+    @Override\n+    public boolean shouldSkip() {\n+        \/\/ OOMEs because of realloc failures with DeoptimizeObjectsALot are too random.\n+        \/\/ And Graal currently doesn't provide all information about non-escaping objects in debug info\n+        return super.shouldSkip() || DeoptimizeObjectsALot || UseJVMCICompiler;\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Pop inlined top frame dropping into method with scalar replaced opjects.\n+ *\/\n+class EAPopInlinedMethodWithScalarReplacedObjectsReallocFailure extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        ThreadReference thread = env.targetMainThread;\n+        thread.resume();\n+        waitUntilTargetHasEnteredEndlessLoop();\n+\n+        thread.suspend();\n+        printStack(thread);\n+        \/\/ frame[0]: EAPopInlinedMethodWithScalarReplacedObjectsReallocFailureTarget.inlinedCallForcedToReturn()\n+        \/\/ frame[1]: EAPopInlinedMethodWithScalarReplacedObjectsReallocFailureTarget.dontinline_testMethod()\n+        \/\/ frame[2]: EATestCaseBaseTarget.run()\n+\n+        msg(\"Pop Frames\");\n+        boolean coughtOom = false;\n+        try {\n+            thread.popFrames(thread.frame(0));    \/\/ Request pop frame of inlinedCallForcedToReturn()\n+                                                  \/\/ reallocation is triggered here\n+        } catch (VMOutOfMemoryException oom) {\n+            \/\/ as expected\n+            msg(\"cought OOM\");\n+            coughtOom = true;\n+        }\n+        printStack(thread);\n+        \/\/ frame[0]: EAPopInlinedMethodWithScalarReplacedObjectsReallocFailureTarget.inlinedCallForcedToReturn()\n+        \/\/ frame[1]: EAPopInlinedMethodWithScalarReplacedObjectsReallocFailureTarget.dontinline_testMethod()\n+        \/\/ frame[2]: EATestCaseBaseTarget.run()\n+\n+        freeAllMemory();\n+        setField(testCase, \"loopCount\", env.vm().mirrorOf(0)); \/\/ terminate loop\n+        Asserts.assertTrue(coughtOom || !env.targetVMOptions.EliminateAllocations,\n+                           \"PopFrame should have triggered an OOM exception in target\");\n+        String expectedTopFrame =\n+                env.targetVMOptions.EliminateAllocations ? \"inlinedCallForcedToReturn\" : \"dontinline_testMethod\";\n+        Asserts.assertEQ(expectedTopFrame, thread.frame(0).location().method().name());\n+    }\n+\n+    @Override\n+    public boolean shouldSkip() {\n+        \/\/ OOMEs because of realloc failures with DeoptimizeObjectsALot are too random.\n+        \/\/ And Graal currently doesn't provide all information about non-escaping objects in debug info\n+        return super.shouldSkip() || env.targetVMOptions.DeoptimizeObjectsALot || env.targetVMOptions.UseJVMCICompiler;\n+    }\n+}\n+\n+class EAPopInlinedMethodWithScalarReplacedObjectsReallocFailureTarget extends EATestCaseBaseTarget {\n+\n+    public long checkSum;\n+\n+    public void dontinline_testMethod() {\n+        long a[] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};                \/\/ scalar replaced\n+        Vector10 v = new Vector10(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);  \/\/ scalar replaced\n+        long l = inlinedCallForcedToReturn();\n+        lResult = a[0] + a[1] + a[2] + a[3] + a[4] + a[5] + a[6] + a[7] + a[8] + a[9]\n+               + v.i0 + v.i1 + v.i2 + v.i3 + v.i4 + v.i5 + v.i6 + v.i7 + v.i8 + v.i9;\n+    }\n+\n+    public long inlinedCallForcedToReturn() {\n+        long cs = checkSum;\n+        dontinline_consumeAllMemory();\n+        while (loopCount-- > 0) {\n+            targetIsInLoop = true;\n+            checkSum += checkSum % ++cs;\n+        }\n+        loopCount = 3;\n+        targetIsInLoop = false;\n+        return checkSum;\n+    }\n+\n+    public void dontinline_consumeAllMemory() {\n+        if (warmupDone && (loopCount > 3)) {\n+            consumeAllMemory();\n+        }\n+    }\n+\n+    @Override\n+    public long getExpectedLResult() {\n+        long n = 10;\n+        return 2*n*(n+1)\/2;\n+    }\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        loopCount = 3;\n+    }\n+\n+    public void warmupDone() {\n+        super.warmupDone();\n+        msg(\"enter 'endless' loop by setting loopCount = Long.MAX_VALUE\");\n+        loopCount = Long.MAX_VALUE; \/\/ endless loop\n+    }\n+\n+    @Override\n+    public boolean shouldSkip() {\n+        \/\/ OOMEs because of realloc failures with DeoptimizeObjectsALot are too random.\n+        \/\/ And Graal currently doesn't provide all information about non-escaping objects in debug info\n+        return super.shouldSkip() || DeoptimizeObjectsALot || UseJVMCICompiler;\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ ForceEarlyReturn tests\n+\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * ForceEarlyReturn into caller frame with scalar replaced objects.\n+ *\/\n+class EAForceEarlyReturnNotInlined extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = env.resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        ThreadReference thread = bpe.thread();\n+        printStack(thread);\n+        \/\/ frame[0]: EATestCaseBaseTarget.dontinline_brkpt()\n+        \/\/ frame[1]: EATestCaseBaseTarget.dontinline_brkpt_iret()\n+        \/\/ frame[2]: EAForceEarlyReturnNotInlinedTarget.dontinline_testMethod()\n+        \/\/ frame[3]: EATestCaseBaseTarget.run()\n+        \/\/ frame[4]: EATestsTarget.main(java.lang.String[])\n+\n+        msg(\"Step out\");\n+        env.stepOut(thread);                               \/\/ return from dontinline_brkpt\n+        printStack(thread);\n+        msg(\"ForceEarlyReturn\");\n+        thread.forceEarlyReturn(env.vm().mirrorOf(43));    \/\/ return from dontinline_brkpt_iret,\n+                                                           \/\/ does not trigger reallocation in contrast to PopFrame\n+        msg(\"Step over line\");\n+        env.stepOverLine(thread);                          \/\/ reallocation is triggered here\n+        printStack(thread);\n+        msg(\"ForceEarlyReturn DONE\");\n+    }\n+\n+    @Override\n+    public boolean shouldSkip() {\n+        \/\/ Graal currently doesn't support Force Early Return\n+        return super.shouldSkip() || env.targetVMOptions.UseJVMCICompiler;\n+    }\n+}\n+\n+class EAForceEarlyReturnNotInlinedTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy = new XYVal(4, 2);\n+        int i = dontinline_brkpt_iret();\n+        iResult = xy.x + xy.y + i;\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 4 + 2 + 43;\n+    }\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+    }\n+\n+    public boolean testFrameShouldBeDeoptimized() {\n+        return true; \/\/ because of stepping\n+    }\n+\n+    @Override\n+    public boolean shouldSkip() {\n+        \/\/ Graal currently doesn't support Force Early Return\n+        return super.shouldSkip() || UseJVMCICompiler;\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * ForceEarlyReturn at safepoint in frame with scalar replaced objects.\n+ *\/\n+class EAForceEarlyReturnOfInlinedMethodWithScalarReplacedObjects extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        ThreadReference thread = env.targetMainThread;\n+        thread.resume();\n+        waitUntilTargetHasEnteredEndlessLoop();\n+\n+        thread.suspend();\n+        printStack(thread);\n+        \/\/ frame[0]: EAForceEarlyReturnOfInlinedMethodWithScalarReplacedObjectsTarget.inlinedCallForcedToReturn()\n+        \/\/ frame[1]: EAForceEarlyReturnOfInlinedMethodWithScalarReplacedObjectsTarget.dontinline_testMethod()\n+        \/\/ frame[2]: EATestCaseBaseTarget.run()\n+\n+        msg(\"ForceEarlyReturn\");\n+        thread.forceEarlyReturn(env.vm().mirrorOf(43));    \/\/ Request force return 43 from inlinedCallForcedToReturn()\n+                                                           \/\/ reallocation is triggered here\n+        msg(\"Step over instruction to do the forced return\");\n+        env.stepOverInstruction(thread);\n+        printStack(thread);\n+        msg(\"ForceEarlyReturn DONE\");\n+    }\n+\n+    @Override\n+    public boolean shouldSkip() {\n+        \/\/ Graal currently doesn't support Force Early Return\n+        return super.shouldSkip() || env.targetVMOptions.UseJVMCICompiler;\n+    }\n+}\n+\n+class EAForceEarlyReturnOfInlinedMethodWithScalarReplacedObjectsTarget extends EATestCaseBaseTarget {\n+\n+    public int checkSum;\n+\n+    public void dontinline_testMethod() {\n+        XYVal xy = new XYVal(4, 2);\n+        int i = inlinedCallForcedToReturn();\n+        iResult = xy.x + xy.y + i;\n+    }\n+\n+    public int inlinedCallForcedToReturn() {               \/\/ forced to return 43\n+        int i = checkSum;\n+        while (loopCount-- > 0) {\n+            targetIsInLoop = true;\n+            checkSum += checkSum % ++i;\n+        }\n+        loopCount = 3;\n+        targetIsInLoop = false;\n+        return checkSum;\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 4 + 2 + 43;\n+    }\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+        loopCount = 3;\n+    }\n+\n+    public void warmupDone() {\n+        super.warmupDone();\n+        msg(\"enter 'endless' loop by setting loopCount = Long.MAX_VALUE\");\n+        loopCount = Long.MAX_VALUE; \/\/ endless loop\n+    }\n+\n+    public boolean testFrameShouldBeDeoptimized() {\n+        return true; \/\/ because of stepping\n+    }\n+\n+    @Override\n+    public boolean shouldSkip() {\n+        \/\/ Graal currently doesn't support Force Early Return\n+        return super.shouldSkip() || UseJVMCICompiler;\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * ForceEarlyReturn with reallocation failure.\n+ *\/\n+class EAForceEarlyReturnOfInlinedMethodWithScalarReplacedObjectsReallocFailure extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        ThreadReference thread = env.targetMainThread;\n+        thread.resume();\n+        waitUntilTargetHasEnteredEndlessLoop();\n+\n+        thread.suspend();\n+        printStack(thread);\n+        \/\/ frame[0]: EAForceEarlyReturnOfInlinedMethodWithScalarReplacedObjectsReallocFailureTarget.inlinedCallForcedToReturn()\n+        \/\/ frame[1]: EAForceEarlyReturnOfInlinedMethodWithScalarReplacedObjectsReallocFailureTarget.dontinline_testMethod()\n+        \/\/ frame[2]: EATestCaseBaseTarget.run()\n+\n+        msg(\"ForceEarlyReturn\");\n+        boolean coughtOom = false;\n+        try {\n+            thread.forceEarlyReturn(env.vm().mirrorOf(43));    \/\/ Request force return 43 from inlinedCallForcedToReturn()\n+                                                               \/\/ reallocation is triggered here\n+        } catch (VMOutOfMemoryException oom) {\n+            \/\/ as expected\n+            msg(\"cought OOM\");\n+            coughtOom   = true;\n+        }\n+        freeAllMemory();\n+        if (env.targetVMOptions.EliminateAllocations) {\n+            printStack(thread);\n+            Asserts.assertTrue(coughtOom, \"ForceEarlyReturn should have triggered an OOM exception in target\");\n+            msg(\"ForceEarlyReturn(2)\");\n+            thread.forceEarlyReturn(env.vm().mirrorOf(43));\n+        }\n+        msg(\"Step over instruction to do the forced return\");\n+        env.stepOverInstruction(thread);\n+        printStack(thread);\n+        msg(\"ForceEarlyReturn DONE\");\n+    }\n+\n+    @Override\n+    public boolean shouldSkip() {\n+        \/\/ OOMEs because of realloc failures with DeoptimizeObjectsALot are too random.\n+        \/\/ And Graal currently doesn't support Force Early Return\n+        return super.shouldSkip() || env.targetVMOptions.DeoptimizeObjectsALot || env.targetVMOptions.UseJVMCICompiler;\n+    }\n+}\n+\n+class EAForceEarlyReturnOfInlinedMethodWithScalarReplacedObjectsReallocFailureTarget extends EATestCaseBaseTarget {\n+\n+    public int checkSum;\n+\n+    public void dontinline_testMethod() {\n+        long a[] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};                \/\/ scalar replaced\n+        Vector10 v = new Vector10(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);  \/\/ scalar replaced\n+        long l = inlinedCallForcedToReturn();\n+        lResult = a[0] + a[1] + a[2] + a[3] + a[4] + a[5] + a[6] + a[7] + a[8] + a[9]\n+               + v.i0 + v.i1 + v.i2 + v.i3 + v.i4 + v.i5 + v.i6 + v.i7 + v.i8 + v.i9 + l;\n+    }\n+\n+    public long inlinedCallForcedToReturn() {                      \/\/ forced to return 43\n+        long cs = checkSum;\n+        dontinline_consumeAllMemory();\n+        while (loopCount-- > 0) {\n+            targetIsInLoop = true;\n+            checkSum += checkSum % ++cs;\n+        }\n+        loopCount = 3;\n+        targetIsInLoop = false;\n+        return checkSum;\n+    }\n+\n+    public void dontinline_consumeAllMemory() {\n+        if (warmupDone) {\n+            consumeAllMemory();\n+        }\n+    }\n+\n+    @Override\n+    public long getExpectedLResult() {\n+        long n = 10;\n+        return 2*n*(n+1)\/2 + 43;\n+    }\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+        loopCount = 3;\n+    }\n+\n+    public void warmupDone() {\n+        super.warmupDone();\n+        msg(\"enter 'endless' loop by setting loopCount = Long.MAX_VALUE\");\n+        loopCount = Long.MAX_VALUE; \/\/ endless loop\n+    }\n+\n+    @Override\n+    public boolean shouldSkip() {\n+        \/\/ OOMEs because of realloc failures with DeoptimizeObjectsALot are too random.\n+        \/\/ And Graal currently doesn't support Force Early Return\n+        return super.shouldSkip() || DeoptimizeObjectsALot || UseJVMCICompiler;\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ Get Instances of ReferenceType\n+\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Check if instances of a type are found even if they are scalar replaced.  To stress the\n+ * implementation a little more, the instances should be retrieved while the target is running.\n+ *\/\n+class EAGetInstancesOfReferenceType extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        printStack(env.targetMainThread);\n+        ReferenceType cls = ((ClassObjectReference)getField(testCase, \"cls\")).reflectedType();\n+        msg(\"reflected type is \" + cls);\n+        msg(\"resume\");\n+        env.targetMainThread.resume();\n+        waitUntilTargetHasEnteredEndlessLoop();\n+        \/\/ do this while thread is running!\n+        msg(\"Retrieve instances of \" + cls.name());\n+        List<ObjectReference> instances = cls.instances(10);\n+        Asserts.assertEQ(instances.size(), 3, \"unexpected number of instances of \" + cls.name());\n+        \/\/ invariant: main thread is suspended at the end of the test case\n+        msg(\"suspend\");\n+        env.targetMainThread.suspend();\n+        terminateEndlessLoop();\n+    }\n+}\n+\n+class EAGetInstancesOfReferenceTypeTarget extends EATestCaseBaseTarget {\n+\n+    public long checkSum;\n+\n+    public static Class<LocalXYVal> cls = LocalXYVal.class;\n+\n+    public static class LocalXYVal {\n+        public int x, y;\n+\n+        public LocalXYVal(int x, int y) {\n+            this.x = x; this.y = y;\n+        }\n+    }\n+\n+    @Override\n+    public void dontinline_testMethod() {\n+        LocalXYVal p1 = new LocalXYVal(4, 2);\n+        LocalXYVal p2 = new LocalXYVal(5, 3);\n+        LocalXYVal p3 = new LocalXYVal(6, 4);\n+        dontinline_endlessLoop();\n+        iResult = p1.x+p1.y + p2.x+p2.y + p3.x+p3.y;\n+    }\n+\n+    @Override\n+    public int getExpectedIResult() {\n+        return 6+8+10;\n+    }\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+        loopCount = 3;\n+    }\n+\n+    public void warmupDone() {\n+        super.warmupDone();\n+        msg(\"enter 'endless' loop by setting loopCount = Long.MAX_VALUE\");\n+        loopCount = Long.MAX_VALUE; \/\/ endless loop\n+    }\n+}\n+\n+\n+\/\/ End of test case collection\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ Helper classes\n+class XYVal {\n+\n+    public int x, y;\n+\n+    public XYVal(int x, int y) {\n+        this.x = x;\n+        this.y = y;\n+    }\n+\n+    \/**\n+     * Note that we don't use a sync block here because javac would generate an synthetic exception\n+     * handler for the synchronized block that catches Throwable E, unlocks and throws E\n+     * again. The throw bytecode causes the BCEscapeAnalyzer to set the escape state to GlobalEscape\n+     * (see comment on exception handlers in BCEscapeAnalyzer::iterate_blocks())\n+     *\/\n+    public synchronized void dontinline_sync_method(EATestCaseBaseTarget target) {\n+        target.dontinline_brkpt();\n+    }\n+\n+    \/**\n+     * Just like {@link #dontinline_sync_method(EATestCaseBaseTarget)} but without the call to\n+     * {@link EATestCaseBaseTarget#dontinline_brkpt()}.\n+     *\/\n+    public synchronized void dontinline_sync_method_no_brkpt(EATestCaseBaseTarget target) {\n+    }\n+}\n+\n+class Vector10 {\n+    int i0, i1, i2, i3, i4, i5, i6, i7, i8, i9;\n+    public Vector10(int j0, int j1, int j2, int j3, int j4, int j5, int j6, int j7, int j8, int j9) {\n+        i0=j0; i1=j1; i2=j2; i3=j3; i4=j4; i5=j5; i6=j6; i7=j7; i8=j8; i9=j9;\n+    }\n+}\n+\n+class ILFDO {\n+\n+    public int i;\n+    public int i2;\n+    public long l;\n+    public long l2;\n+    public float f;\n+    public float f2;\n+    public double d;\n+    public double d2;\n+    public Long o;\n+    public Long o2;\n+\n+    public ILFDO(int i,\n+                 int i2,\n+                 long l,\n+                 long l2,\n+                 float f,\n+                 float f2,\n+                 double d,\n+                 double d2,\n+                 Long o,\n+                 Long o2) {\n+        this.i = i;\n+        this.i2 = i2;\n+        this.l = l;\n+        this.l2 = l2;\n+        this.f = f;\n+        this.f2 = f2;\n+        this.d = d;\n+        this.d2 = d2;\n+        this.o = o;\n+        this.o2 = o2;\n+    }\n+\n+}\n","filename":"test\/jdk\/com\/sun\/jdi\/EATests.java","additions":3143,"deletions":0,"binary":false,"changes":3143,"status":"added"},{"patch":"@@ -121,0 +121,1 @@\n+        map.put(\"jdk.containerized\", this::jdkContainerized);\n@@ -552,0 +553,5 @@\n+    private String jdkContainerized() {\n+        String isEnabled = System.getenv(\"TEST_JDK_CONTAINERIZED\");\n+        return \"\" + \"true\".equalsIgnoreCase(isEnabled);\n+    }\n+\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -257,0 +257,1 @@\n+  public native boolean isFrameDeoptimized(int depth);\n@@ -413,1 +414,0 @@\n-  public native void freeMetaspace(ClassLoader classLoader, long addr, long size);\n@@ -418,0 +418,14 @@\n+  \/\/ Metaspace Arena Tests\n+  public native long createMetaspaceTestContext(long commit_limit, long reserve_limit);\n+  public native void destroyMetaspaceTestContext(long context);\n+  public native void purgeMetaspaceTestContext(long context);\n+  public native void printMetaspaceTestContext(long context);\n+  public native long getTotalCommittedWordsInMetaspaceTestContext(long context);\n+  public native long getTotalUsedWordsInMetaspaceTestContext(long context);\n+  public native long createArenaInTestContext(long context, boolean is_micro);\n+  public native void destroyMetaspaceTestArena(long arena);\n+  public native long allocateFromMetaspaceTestArena(long arena, long word_size);\n+  public native void deallocateToMetaspaceTestArena(long arena, long p, long word_size);\n+\n+  public native long maxMetaspaceAllocationSize();\n+\n","filename":"test\/lib\/sun\/hotspot\/WhiteBox.java","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"}]}